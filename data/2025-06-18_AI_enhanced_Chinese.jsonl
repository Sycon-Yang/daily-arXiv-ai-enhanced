{"id": "2506.13796", "pdf": "https://arxiv.org/pdf/2506.13796", "abs": "https://arxiv.org/abs/2506.13796", "authors": ["Zhou Chen", "Xiao Wang", "Yuanhong Liao", "Ming Lin", "Yuqi Bai"], "title": "ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025 camera ready, 13 pages, 4 figures, 4 tables", "summary": "As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.", "AI": {"tldr": "This study introduces an automated method to construct instruction data for climate change, creating a dataset called ClimateChat-Corpus and an LLM named ClimateChat that outperforms on climate change Q&A tasks.", "motivation": "The increasing demand for research in climate science due to global climate change, and the need for efficient production of high-quality instruction data for LLMs focused on climate change.", "method": "An automated method combining document-based fact extraction with web scraping and seed instruction collection to generate diverse instruction data, resulting in the ClimateChat-Corpus dataset.", "result": "ClimateChat-Corpus was used to fine-tune open-source LLMs, producing ClimateChat, which shows improved performance on climate change Q&A tasks. Different base models and instruction data were also evaluated for their impact on LLM performance.", "conclusion": "This research demonstrates the effectiveness of the automated instruction data construction method and highlights the importance of choosing the right base model for instruction tuning in developing effective climate change-specific LLMs."}}
{"id": "2506.13886", "pdf": "https://arxiv.org/pdf/2506.13886", "abs": "https://arxiv.org/abs/2506.13886", "authors": ["Antara Raaghavi Bhattacharya", "Isabel Papadimitriou", "Kathryn Davidson", "David Alvarez-Melis"], "title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\\times$, etc, as in \"twenty + three\"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.", "AI": {"tldr": "This study explores why large language models struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, while humans can solve them successfully.", "motivation": "To understand why LLMs have difficulty with tasks involving cross-linguistic numeral systems that humans can handle effectively.", "method": "A series of experiments were conducted to separate linguistic and mathematical aspects of numbers in language, and ablation studies were performed to examine how different parameters of numeral construction affect performance.", "result": "Models could only solve such problems when mathematical operations were explicitly marked with known symbols, suggesting LLMs lack the concept of implicit numeral structure that humans use.", "conclusion": "The ability to infer compositional rules from implicit patterns in human-scale data remains a challenge for current reasoning models."}}
{"id": "2506.13888", "pdf": "https://arxiv.org/pdf/2506.13888", "abs": "https://arxiv.org/abs/2506.13888", "authors": ["Jipeng Zhang", "Kehao Miao", "Renjie Pi", "Zhaowei Wang", "Runtao Liu", "Rui Pan", "Tong Zhang"], "title": "VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u4e13\u5bb6\u3001Chain-of-Thought\uff08CoT\uff09\u63a8\u7406\u548c\u57fa\u4e8e\u8fb9\u8ddd\u7684\u62d2\u7edd\u62bd\u6837\u6765\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5f15\u5bfc\u56f0\u5883\u548c\u6a21\u6001\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5f3a\u5316\u5fae\u8c03\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5982\u4f55\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b\u89c6\u89c9\u4e13\u5bb6\u3001CoT\u63a8\u7406\u548c\u57fa\u4e8e\u8fb9\u8ddd\u7684\u62d2\u7edd\u62bd\u6837\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5e7b\u89c9\u68c0\u6d4b\u548c\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2506.13894", "pdf": "https://arxiv.org/pdf/2506.13894", "abs": "https://arxiv.org/abs/2506.13894", "authors": ["Ryuki Matsuura", "Shikhar Bharadwaj", "Jiarui Liu", "Dhatchi Kunde Govindarajan"], "title": "EmoNews: A Spoken Dialogue System for Expressive News Conversations", "categories": ["cs.CL"], "comment": null, "summary": "We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1", "AI": {"tldr": "This paper presents a task-oriented spoken dialogue system for empathetic news conversations by integrating emotional speech synthesis and sentiment analysis.", "motivation": "To enable more empathetic news conversations by addressing the lack of exploration in task-oriented emotional spoken dialogue systems and the absence of standardized evaluation metrics for social goals.", "method": "Developing an emotional SDS using a large language model-based sentiment analyzer and PromptTTS for emotional speech synthesis, along with proposing a subjective evaluation scale for emotional SDSs.", "result": "The developed emotional SDS outperformed a baseline system in terms of emotion regulation and engagement, suggesting the importance of speech emotion in engaging conversations.", "conclusion": "Speech emotion plays a critical role in enhancing the engagement of conversations, and the proposed system demonstrates superior performance compared to existing methods."}}
{"id": "2506.13901", "pdf": "https://arxiv.org/pdf/2506.13901", "abs": "https://arxiv.org/abs/2506.13901", "authors": ["Abhilekh Borah", "Chhavi Sharma", "Danush Khanna", "Utkarsh Bhatt", "Gurpreet Singh", "Hasnat Md Abdullah", "Raghav Kaushik Ravi", "Vinija Jain", "Jyoti Patel", "Shubham Singh", "Vasu Sharma", "Arpita Vats", "Rahul Raja", "Aman Chadha", "Amitava Das"], "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.", "AI": {"tldr": "Introduce a new metric called AQI to evaluate the alignment of large language models.", "motivation": "To ensure that large language models behave according to human-aligned values and safety constraints in high-stakes domains.", "method": "Propose a novel geometric and prompt-invariant metric called AQI that analyzes the separation of safe and unsafe activations in latent space using measures such as DBS, DI, XBI, and CHI.", "result": "AQI can detect hidden misalignments and jailbreak risks, and serve as an early warning signal for alignment faking.", "conclusion": "AQI offers a robust, decoding invariant tool for behavior agnostic safety auditing and is correlated with external judges."}}
{"id": "2506.13956", "pdf": "https://arxiv.org/pdf/2506.13956", "abs": "https://arxiv.org/abs/2506.13956", "authors": ["Shang-Chi Tsai", "Seiya Kawano", "Angel Garcia Contreras", "Koichiro Yoshino", "Yun-Nung Chen"], "title": "ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection", "categories": ["cs.CL", "cs.AI", "cs.RO"], "comment": "IWSDS 2024 Best Paper Award", "summary": "When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.", "AI": {"tldr": "This paper presents a new framework for data augmentation in robotic assistance scenarios to improve multimodal classification tasks. The approach uses a large language model to simulate conversations and a stable diffusion model to generate images, enhancing the accuracy of action selection.", "motivation": "To improve intent understanding in robots by enhancing user requests with visual cues from their surroundings.", "method": "Leveraging a large language model to simulate conversations and a stable diffusion model to generate images.", "result": "The additional generated data refines multimodal models, improving action selection capabilities.", "conclusion": "The proposed method significantly enhances the robot's action selection capabilities, achieving state-of-the-art performance."}}
{"id": "2506.13965", "pdf": "https://arxiv.org/pdf/2506.13965", "abs": "https://arxiv.org/abs/2506.13965", "authors": ["Aleksander Smywi\u0144ski-Pohl", "Tomer Libal", "Adam Kaczmarczyk", "Magdalena Kr\u00f3l"], "title": "Are manual annotations necessary for statutory interpretations retrieval?", "categories": ["cs.CL"], "comment": null, "summary": "One of the elements of legal research is looking for cases where judges have extended the meaning of a legal concept by providing interpretations of what a concept means or does not mean. This allow legal professionals to use such interpretations as precedents as well as laymen to better understand the legal concept. The state-of-the-art approach for retrieving the most relevant interpretations for these concepts currently depends on the ranking of sentences and the training of language models over annotated examples. That manual annotation process can be quite expensive and need to be repeated for each such concept, which prompted recent research in trying to automate this process. In this paper, we highlight the results of various experiments conducted to determine the volume, scope and even the need for manual annotation. First of all, we check what is the optimal number of annotations per a legal concept. Second, we check if we can draw the sentences for annotation randomly or there is a gain in the performance of the model, when only the best candidates are annotated. As the last question we check what is the outcome of automating the annotation process with the help of an LLM.", "AI": {"tldr": "This paper examines the necessity and methods of manual annotation in automating the retrieval of legal concept interpretations.", "motivation": "To reduce the cost and repetition of manual annotation in legal concept interpretation retrieval.", "method": "Conducting experiments on the optimal number of annotations per legal concept, the randomness of sentence selection for annotation, and the automation of annotation using an LLM.", "result": "The experiments aim to explore the volume, scope, and necessity of manual annotation in legal concept interpretation retrieval.", "conclusion": "The findings could potentially optimize the efficiency and cost-effectiveness of legal concept interpretation retrieval."}}
{"id": "2506.13978", "pdf": "https://arxiv.org/pdf/2506.13978", "abs": "https://arxiv.org/abs/2506.13978", "authors": ["Xiuwen Wu", "Hao Wang", "Zhiang Yan", "Xiaohan Tang", "Pengfei Xu", "Wai-Ting Siok", "Ping Li", "Jia-Hong Gao", "Bingjiang Lyu", "Lang Qin"], "title": "AI shares emotion with humans across languages and cultures", "categories": ["cs.CL"], "comment": null, "summary": "Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts.", "AI": {"tldr": "This paper explores how large language models (LLMs) represent emotions in language compared to humans, finding that LLMs' emotion spaces align with human perception and can be adjusted using human-centric emotion concepts.", "motivation": "To determine if LLMs represent emotions like humans do and whether their emotional tone can be controlled.", "method": "Assessing human-AI emotional alignment across linguistic-cultural groups and model-families using interpretable LLM features translated from concept-sets for nuanced emotion categories.", "result": "LLM-derived emotion spaces are structurally congruent with human perception, supported by valence and arousal dimensions. Emotion-related features predict word ratings, showing universal and language-specific patterns. Steering vectors from human-centric emotion concepts can modulate model expressions.", "conclusion": "AI shares emotional representations with humans, and its affective outputs can be precisely guided using psychologically grounded emotion concepts."}}
{"id": "2506.14012", "pdf": "https://arxiv.org/pdf/2506.14012", "abs": "https://arxiv.org/abs/2506.14012", "authors": ["Amr Mohamed", "Yang Zhang", "Michalis Vazirgiannis", "Guokan Shang"], "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text", "categories": ["cs.CL"], "comment": null, "summary": "Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\\unicode{x2013}$even under linguistic constraints$\\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.", "AI": {"tldr": "This paper evaluates how well large language models (LLMs) understand code-switched text, which is when people mix languages in their conversations. It finds that embedding English into other languages can help comprehension, but fine-tuning is needed for better performance.", "motivation": "To understand how LLMs handle mixed-language text, which is common in multilingual communities and online content.", "method": "Generating code-switched variants of established reasoning and comprehension benchmarks.", "result": "Embedding English into other languages sometimes improves comprehension, while fine-tuning helps reduce degradation.", "conclusion": "Fine-tuning is a more stable method than prompting to improve LLMs' understanding of code-switched text."}}
{"id": "2506.14028", "pdf": "https://arxiv.org/pdf/2506.14028", "abs": "https://arxiv.org/abs/2506.14028", "authors": ["Xueqing Peng", "Lingfei Qian", "Yan Wang", "Ruoyu Xiang", "Yueru He", "Yang Ren", "Mingyang Jiang", "Jeff Zhao", "Huan He", "Yi Han", "Yun Feng", "Yuechen Jiang", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Xiaoyu Wang", "Penglei Gao", "Shengyuan Lin", "Keyi Wang", "Shanshan Yang", "Yilun Zhao", "Zhiwei Liu", "Peng Lu", "Jerry Huang", "Suyuchen Wang", "Triantafillos Papadopoulos", "Polydoros Giannouris", "Efstathia Soufleri", "Nuo Chen", "Guojun Xiong", "Zhiyang Deng", "Yijia Zhao", "Mingquan Lin", "Meikang Qiu", "Kaleb E Smith", "Arman Cohan", "Xiao-Yang Liu", "Jimin Huang", "Alejandro Lopez-Lira", "Xi Chen", "Junichi Tsujii", "Jian-Yun Nie", "Sophia Ananiadou", "Qianqian Xie"], "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.", "AI": {"tldr": "Introduce MultiFinBen, the first multilingual and multimodal benchmark for global financial domain, evaluating LLMs across multiple modalities and linguistic settings. It includes novel tasks like PolyFiQA and OCR-embedded financial QA tasks. Evaluation shows strong models struggle with complex cross-lingual and multimodal financial tasks.", "motivation": "Existing financial NLP benchmarks are limited to monolingual and unimodal settings, over-relying on simple tasks and failing to reflect real-world financial communication complexity.", "method": "Create MultiFinBen with novel tasks (PolyFiQA-Easy/Expert, EnglishOCR/SpanishOCR), dynamic difficulty-aware selection mechanism, and compact balanced benchmark.", "result": "Evaluation of 22 state-of-the-art models shows they struggle with complex cross-lingual and multimodal financial tasks despite their general capabilities.", "conclusion": "MultiFinBen fosters transparent, reproducible, and inclusive progress in financial studies and applications."}}
{"id": "2506.14040", "pdf": "https://arxiv.org/pdf/2506.14040", "abs": "https://arxiv.org/abs/2506.14040", "authors": ["Md Nazmus Sakib"], "title": "An Interdisciplinary Review of Commonsense Reasoning and Intent Detection", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.", "AI": {"tldr": "This review examines recent progress in two key areas of natural language understanding: commonsense reasoning and intent detection. It looks at 28 relevant papers, categorizing them based on their approach and use case. For commonsense reasoning, it considers zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. In intent detection, it reviews open-set models, generative formulations, clustering, and human-centered systems. The review combines insights from NLP and HCI to point out current trends and unsolved issues in model adaptability, multilingualism, and contextual awareness, as well as problems related to grounding, generalization, and benchmarking.", "motivation": "To explore recent advancements in commonsense reasoning and intent detection within natural language understanding.", "method": "Categorizes and analyzes 28 papers from ACL, EMNLP, and CHI (2020-2025) according to methodology and application.", "result": "Highlights trends towards more adaptive, multilingual, and context-aware models while identifying gaps in grounding, generalization, and benchmark design.", "conclusion": "Combines perspectives from NLP and HCI to provide a comprehensive overview of current research and future directions in commonsense reasoning and intent detection."}}
{"id": "2506.14046", "pdf": "https://arxiv.org/pdf/2506.14046", "abs": "https://arxiv.org/abs/2506.14046", "authors": ["David Kogan", "Max Schumacher", "Sam Nguyen", "Masanori Suzuki", "Melissa Smith", "Chloe Sophia Bellows", "Jared Bernstein"], "title": "Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.", "AI": {"tldr": "Introduce Ace-CEFR dataset for evaluating English conversational text difficulty, experiment with different models, show better accuracy than human experts, and release the dataset publicly.", "motivation": "To evaluate the language difficulty of short conversational passages, especially for training and filtering LLMs.", "method": "Use expert-annotated English conversational text passages as Ace-CEFR dataset and experiment with Transformer-based models and LLMs.", "result": "Models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have suitable latency for production environments.", "conclusion": "Release the Ace-CEFR dataset for public research and development."}}
{"id": "2506.14064", "pdf": "https://arxiv.org/pdf/2506.14064", "abs": "https://arxiv.org/abs/2506.14064", "authors": ["Iona Carslaw", "Sivan Milton", "Nicolas Navarre", "Ciyang Qing", "Wataru Uegaki"], "title": "Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data", "categories": ["cs.CL"], "comment": "Accepted in the Society for Computation in Linguistics", "summary": "For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool.", "AI": {"tldr": "This paper introduces a method to find natural English embedded clause examples in big texts and shares a big dataset made using this method.", "motivation": "Investigate embedded clauses with statistical information and naturally-occurring examples from large language corpora.", "method": "Using constituency parsing and a set of parsing heuristics to detect and annotate naturally-occurring examples of English embedded clauses in large-scale text data.", "result": "A large-scale dataset of naturally-occurring English embedded clauses extracted from the open-source corpus Dolma.", "conclusion": "This paper presents a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses and a large-scale dataset created using this approach."}}
{"id": "2506.14101", "pdf": "https://arxiv.org/pdf/2506.14101", "abs": "https://arxiv.org/abs/2506.14101", "authors": ["Paul Landes", "Sitara Rao", "Aaron Jeremy Chaise", "Barbara Di Eugenio"], "title": "Abstract Meaning Representation for Hospital Discharge Summarization", "categories": ["cs.CL"], "comment": null, "summary": "The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models.", "AI": {"tldr": "This paper addresses the issue of hallucination in large language models, particularly in the clinical domain for generating discharge summaries. It introduces a method using language-based graphs and deep learning to improve the trustworthiness and reliability of automatic summarization.", "motivation": "To address the problem of hallucination in large language models which affects the clinical domain, especially in automatically generating discharge summaries to alleviate physician workload and documentation burden.", "method": "Combining language-based graphs and deep learning models to ensure the provenance of content and trustworthiness in automatic summarization.", "result": "Impressive reliability results were achieved on the MIMIC-III corpus and clinical notes from Anonymous Hospital.", "conclusion": "The proposed method demonstrates potential in enhancing the trustworthiness and reliability of automatically generated discharge summaries."}}
{"id": "2506.14111", "pdf": "https://arxiv.org/pdf/2506.14111", "abs": "https://arxiv.org/abs/2506.14111", "authors": ["Essential AI", ":", "Andrew Hojel", "Michael Pust", "Tim Romanski", "Yash Vanjani", "Ritvik Kapila", "Mohit Parmar", "Adarsh Chaluvaraju", "Alok Tripathy", "Anil Thomas", "Ashish Tanwer", "Darsh J Shah", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Kurt Smith", "Michael Callahan", "Peter Rushton", "Philip Monk", "Platon Mazarakis", "Saad Jamal", "Saurabh Srivastava", "Somanshu Singla", "Ashish Vaswani"], "title": "Essential-Web v1.0: 24T tokens of organized web data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0", "AI": {"tldr": "Essential-Web v1.0, a large-scale dataset, provides comprehensive annotations for web texts using a taxonomy system, enabling efficient filtering and generation of specialized datasets in multiple fields.", "motivation": "The lack of massive, well-organized pre-training datasets makes data acquisition costly and difficult.", "method": "A twelve-category taxonomy is used to annotate every document in the dataset, and a fine-tuned model EAI-Distill-0.5b generates the taxonomy labels.", "result": "Using simple SQL-style filters, Essential-Web v1.0 produces competitive datasets in math, web code, STEM, and medical fields.", "conclusion": "Essential-Web v1.0 offers a valuable resource for training language models across various domains, available on HuggingFace."}}
{"id": "2506.14123", "pdf": "https://arxiv.org/pdf/2506.14123", "abs": "https://arxiv.org/abs/2506.14123", "authors": ["Jonathan Hayase", "Alisa Liu", "Noah A. Smith", "Sewoong Oh"], "title": "Sampling from Your Language Model One Byte at a Time", "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.", "AI": {"tldr": "This paper presents a method to convert autoregressive language models with BPE tokenization into character or byte-level models, solving the Prompt Boundary Problem and enabling ensembling of models with different tokenizers.", "motivation": "To solve the Prompt Boundary Problem (PBP) and improve model interoperability by addressing issues caused by tokenization mismatches.", "method": "An inference-time method to convert BPE tokenized models into character or byte-level models without altering the generative distribution at the text level.", "result": "The converted models can solve the PBP and unify vocabularies of different tokenizers, allowing for ensembling and proxy-tuning.", "conclusion": "The proposed method improves model performance on downstream tasks by enabling effective ensembling and knowledge transfer between models with different tokenizers."}}
{"id": "2506.14157", "pdf": "https://arxiv.org/pdf/2506.14157", "abs": "https://arxiv.org/abs/2506.14157", "authors": ["Chengyu Huang", "Tanya Goyal"], "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.", "AI": {"tldr": "This work introduces Distance Calibrated Reward Margin (DCRM), a metric for evaluating the quality of response pairs in preference optimization (PO) tasks based on the differences between preferred and dispreferred responses. It also proposes a best-of-$N^2$ pairing method to select high-quality response pairs, improving model performance on evaluation benchmarks.", "motivation": "To address the issue where differences between preferred and dispreferred responses may not align with desirable differences for learning in PO tasks.", "method": "Introduces DCRM as a metric combining distance and reward margin to quantify response pair differences and proposes a best-of-$N^2$ pairing method to select high-quality response pairs.", "result": "Establishes a correlation between higher DCRM of the training set and better learning outcomes, and demonstrates improved model performance on benchmarks like AlpacaEval, MT-Bench, and Arena-Hard.", "conclusion": "Proposes a novel approach to enhance PO task performance by optimizing response pair quality through DCRM and a new pairing method."}}
{"id": "2506.14158", "pdf": "https://arxiv.org/pdf/2506.14158", "abs": "https://arxiv.org/abs/2506.14158", "authors": ["Tao He", "Guang Huang", "Yu Yang", "Tianshi Xu", "Sicheng Zhao", "Guiguang Ding", "Pengyang Wang", "Feng Tian"], "title": "S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.", "AI": {"tldr": "Proposes a new speculative sampling method with syntactic and semantic coherence to improve efficiency of large language models.", "motivation": "Existing speculative sampling approaches ignore text coherence, leading to limited efficiency.", "method": "Introducing a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework that leverages multi-head drafting and a continuous verification tree.", "result": "S$^4$C enhances efficiency, increases parallelism, and generates more valid tokens with fewer computational resources.", "conclusion": "S$^4$C improves efficiency and achieves better acceleration ratios than existing methods on Spec-bench benchmarks."}}
{"id": "2506.14161", "pdf": "https://arxiv.org/pdf/2506.14161", "abs": "https://arxiv.org/abs/2506.14161", "authors": ["Yanlin Li", "Hao Liu", "Huimin Liu", "Yinwei Wei", "Yupeng Hu"], "title": "MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind", "categories": ["cs.CL"], "comment": null, "summary": "Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStereotype Content Model\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u6027\u504f\u89c1\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e24\u4e2a\u95f4\u63a5\u4efb\u52a1\uff08WABT\u548cAAT\uff09\u6765\u63ed\u793a\u590d\u6742\u7684\u504f\u89c1\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u7684\u76f4\u63a5\u67e5\u8be2\u65b9\u6cd5\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u6027\u504f\u89c1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u793e\u4f1a\u671f\u671b\u6548\u5e94\u4ee5\u53ca\u672a\u80fd\u6355\u6349\u5176\u5fae\u5999\u4e14\u591a\u7ef4\u7684\u672c\u8d28\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eStereotype Content Model\u7684\u6846\u67b6\uff0c\u901a\u8fc7Word Association Bias Test\u548cAffective Attribution Test\u8fd9\u4e24\u4e2a\u95f4\u63a5\u4efb\u52a1\u6765\u8bc4\u4f30\u504f\u89c1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u63ed\u793a\u51fa\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u7684\u793e\u4f1a\u6027\u504f\u89c1\u3001\u591a\u7ef4\u5dee\u5f02\u548c\u4e0d\u5bf9\u79f0\u523b\u677f\u5370\u8c61\u653e\u5927\u7b49\u590d\u6742\u504f\u89c1\u7ed3\u6784\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u9690\u6027\u504f\u89c1\u7684\u7ed3\u6784\u7279\u6027\u3002"}}
{"id": "2506.14175", "pdf": "https://arxiv.org/pdf/2506.14175", "abs": "https://arxiv.org/abs/2506.14175", "authors": ["Chenglong Wang", "Yang Gan", "Yifu Huo", "Yongyu Mu", "Qiaozhi He", "Murun Yang", "Bei Li", "Tong Xiao", "Chunliang Zhang", "Tongran Liu", "Jingbo Zhu"], "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.", "AI": {"tldr": "This paper explores methods to train reward models using both unlabeled and labeled data for large language models (LLMs). It develops a generative reward model that combines unsupervised and supervised learning, showing it optimizes a regularized pairwise ranking loss. This approach results in a foundational reward model applicable to various tasks with minimal additional fine-tuning, demonstrating significant performance gains over baselines.", "motivation": "To improve the training of reward models for LLMs by incorporating both unlabeled and labeled data, aiming for better generalization and performance across multiple tasks.", "method": "Developing a generative reward model trained through unsupervised learning followed by supervised fine-tuning, and exploring the effect of label smoothing on optimizing a pairwise ranking loss.", "result": "The model shows good generalization across tasks like response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, outperforming strong baseline models.", "conclusion": "Combining generative and discriminative approaches in training reward models offers a new perspective linking different model classes and leads to improved performance and applicability."}}
{"id": "2506.14177", "pdf": "https://arxiv.org/pdf/2506.14177", "abs": "https://arxiv.org/abs/2506.14177", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "title": "Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.", "AI": {"tldr": "This study explores building code-switching (CS) automatic speech recognition (ASR) systems using synthetic CS data. A phrase-level mixing method is proposed to mimic natural CS patterns. Experiments on three Southeast Asian language pairs show improved ASR performance.", "motivation": "The scarcity and high cost of transcribed data for CS pose challenges for ASR.", "method": "Proposes a phrase-level mixing method to generate synthetic CS data and fine-tunes large pretrained ASR models with monolingual augmented with synthetic phrase-mixed CS data.", "result": "Enhances ASR performance on both monolingual and CS tests, with the highest gains observed in Malay-English, followed by Tamil-English and Mandarin-Malay.", "conclusion": "The study provides a cost-effective approach for CS-ASR development, which could benefit both research and industry."}}
{"id": "2506.14190", "pdf": "https://arxiv.org/pdf/2506.14190", "abs": "https://arxiv.org/abs/2506.14190", "authors": ["Tuan Nguyen", "Huy-Dat Tran"], "title": "AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This work has been submitted to the IEEE for possible publication. This paper is a preprint version submitted to the 2025 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2025)", "summary": "Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly. Prior work generates synthetic audio from text, but these methods are computationally intensive and hard to scale. We introduce AsyncSwitch, a novel asynchronous adaptation framework that leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning on paired speech-text corpora. Our three-stage process (1) trains decoder self-attention and feedforward layers on code-switched text, (2) aligns decoder and encoder via cross-attention using limited speech-text data, and (3) fully fine-tunes the entire model. Experiments with Whisper on Malay-English code-switching demonstrate a 9.02% relative WER reduction, while improving monolingual performance in Singlish, Malay, and other English variants.", "AI": {"tldr": "A new method called AsyncSwitch is introduced to improve ASR for code-switched languages by pre-exposing models to diverse code-switched domains using web data.", "motivation": "Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly.", "method": "AsyncSwitch is a novel asynchronous adaptation framework that uses a three-stage process: (1) training decoder layers on code-switched text, (2) aligning decoder and encoder using cross-attention with limited speech-text data, and (3) fully fine-tuning the entire model.", "result": "Experiments show a 9.02% relative WER reduction for Malay-English code-switching and improved monolingual performance in Singlish, Malay, and other English variants.", "conclusion": "AsyncSwitch effectively improves ASR for code-switched languages and also enhances monolingual performance."}}
{"id": "2506.14199", "pdf": "https://arxiv.org/pdf/2506.14199", "abs": "https://arxiv.org/abs/2506.14199", "authors": ["Junghwan Kim", "Kieun Park", "Sohee Park", "Hyunggug Kim", "Bongwon Suh"], "title": "MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment", "categories": ["cs.CL"], "comment": "4 Pages, 2 tables, EMNLP submitted", "summary": "Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \\textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.", "AI": {"tldr": "A new method called MAS-LitEval is proposed to evaluate literary translations more effectively than traditional metrics.", "motivation": "Traditional metrics fail to assess cultural nuances and stylistic elements in literary translation.", "method": "MAS-LitEval is a multi-agent system using LLMs to evaluate translations based on terminology, narrative, and style.", "result": "MAS-LitEval scored up to 0.890 in capturing literary nuances, outperforming traditional metrics.", "conclusion": "This work provides a scalable framework for TQA, beneficial for translators and researchers."}}
{"id": "2506.14200", "pdf": "https://arxiv.org/pdf/2506.14200", "abs": "https://arxiv.org/abs/2506.14200", "authors": ["Brihi Joshi", "Keyu He", "Sahana Ramnath", "Sadra Sabouri", "Kaitlyn Zhou", "Souti Chattopadhyay", "Swabha Swayamdipta", "Xiang Ren"], "title": "ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations", "categories": ["cs.CL", "cs.HC"], "comment": "Findings of ACL 2025", "summary": "Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an \"educator\" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.", "AI": {"tldr": "Evaluate the ability of language models to tailor responses for learners with varied informational needs and knowledge backgrounds.", "motivation": "Explore the pedagogical capabilities of language models.", "method": "Introduce ELI-Why, a benchmark of 13.4K 'Why' questions, and conduct two extensive human studies to assess the utility of language model-generated explanatory answers.", "result": "GPT-4-generated explanations matched their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. Users deemed GPT-4-generated explanations 20% less suited on average to their informational needs compared to explanations curated by lay people. Automated evaluation metrics revealed that explanations generated across different language model families for different informational needs remained indistinguishable in their grade-level.", "conclusion": "The study suggests that current language models have limitations in tailoring responses to meet the diverse needs of learners across various educational grades."}}
{"id": "2506.14203", "pdf": "https://arxiv.org/pdf/2506.14203", "abs": "https://arxiv.org/abs/2506.14203", "authors": ["Jongho Kim", "Romain Stora\u00ef", "Seung-won Hwang"], "title": "Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation", "categories": ["cs.CL"], "comment": "EMNLP 2024 Findings (long)", "summary": "In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges.", "AI": {"tldr": "This study explores how language models can help patients with anomia, a condition where they struggle to name objects. The researchers developed methods to improve the model's ability to handle unseen and incorrectly related terms, showing promising results.", "motivation": "To assist patients with anomia in identifying named items despite challenges like unseen relevant terms and semantic paraphasia errors.", "method": "Proposed methods include robustifying the model against semantically paraphasic errors and enhancing it with unseen terms using gradient-based selective augmentation.", "result": "The model showed strong performance against baselines when evaluated on the Tip-of-the-Tongue dataset and applied to real patient data from AphasiaBank.", "conclusion": "Language models have potential in aiding anomia patients by overcoming specific challenges such as unseen terms and semantic paraphasia errors."}}
{"id": "2506.14205", "pdf": "https://arxiv.org/pdf/2506.14205", "abs": "https://arxiv.org/abs/2506.14205", "authors": ["Jingxu Xie", "Dylan Xu", "Xuandong Zhao", "Dawn Song"], "title": "AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents", "categories": ["cs.CL"], "comment": null, "summary": "We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth", "AI": {"tldr": "Introduce AgentSynth, a cost-effective system using LLMs to create over 6,000 diverse tasks with adjustable difficulty, showing significant performance drops for advanced models and a low cost of $0.60 per trajectory.", "motivation": "Developing a scalable and efficient method to generate high-quality tasks and trajectory datasets for generalist computer-use agents.", "method": "An LLM-based task proposer guided by a persona generates subtasks iteratively, completed by an execution agent. Subtasks are summarized into composite tasks with controlled difficulty.", "result": "AgentSynth creates over 6,000 diverse and realistic tasks with a significant increase in difficulty as the number of subtasks increases. State-of-the-art LLM agents' performance drops dramatically with increasing difficulty.", "conclusion": "AgentSynth provides a scalable, cost-efficient solution for creating complex task datasets with strong discriminative power and low costs compared to human annotations."}}
{"id": "2506.14206", "pdf": "https://arxiv.org/pdf/2506.14206", "abs": "https://arxiv.org/abs/2506.14206", "authors": ["Jia-Chen Zhang", "Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Fei Dai"], "title": "CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation", "categories": ["cs.CL"], "comment": null, "summary": "Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: https://github.com/Godz-z/CausalDiffTab.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6a21\u578bCausalDiffTab\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u5305\u542b\u6570\u503c\u548c\u5206\u7c7b\u7279\u5f81\u7684\u6df7\u5408\u8868\u683c\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u56e0\u679c\u6b63\u5219\u5316\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u6570\u636e\u9690\u79c1\u95ee\u9898\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u969c\u788d\u3002\u5408\u6210\u6570\u636e\u5df2\u6210\u4e3a\u4e00\u79cd\u4e3b\u6d41\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728\u751f\u6210\u6df7\u5408\u7c7b\u578b\u6570\u636e\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u56e0\u679c\u5dee\u5f02\u8c03\u8282\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u6a21\u578b", "result": "CausalDiffTab\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CausalDiffTab\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2506.14211", "pdf": "https://arxiv.org/pdf/2506.14211", "abs": "https://arxiv.org/abs/2506.14211", "authors": ["Sina Abdidizaji", "Md Kowsher", "Niloofar Yousefi", "Ivan Garibay"], "title": "Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation", "categories": ["cs.CL"], "comment": "Accepted at the HCI International conference 2025", "summary": "In the era of digitalization, as individuals increasingly rely on digital platforms for communication and news consumption, various actors employ linguistic strategies to influence public perception. While models have become proficient at detecting explicit patterns, which typically appear in texts as single remarks referred to as utterances, such as social media posts, malicious actors have shifted toward utilizing implicit influential verbal patterns embedded within conversations. These verbal patterns aim to mentally penetrate the victim's mind in order to influence them, enabling the actor to obtain the desired information through implicit means. This paper presents an improved approach for detecting such implicit influential patterns. Furthermore, the proposed model is capable of identifying the specific locations of these influential elements within a conversation. To achieve this, the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models. Our designed framework resulted in a 6% improvement in the detection of implicit influential patterns in conversations. Moreover, this approach improved the multi-label classification tasks related to both the techniques used for influence and the vulnerability of victims by 33% and 43%, respectively.", "AI": {"tldr": "This paper introduces an enhanced method for detecting implicit influential verbal patterns within conversations. By augmenting the existing dataset with advanced language model reasoning capabilities, it achieved a 6% increase in pattern detection and improved multi-label classifications of influence techniques and victim vulnerabilities by 33% and 43%, respectively.", "motivation": "With individuals relying more on digital platforms, malicious actors use subtle linguistic strategies to influence public perception, moving away from explicit patterns to more covert verbal patterns that can influence minds and extract desired information.", "method": "The approach involves augmenting the dataset using state-of-the-art language models' reasoning capabilities to better detect implicit influential patterns and identify their specific locations within conversations.", "result": "The improved method led to a 6% enhancement in detecting implicit influential patterns and significantly boosted the accuracy of multi-label classifications for influence techniques and victim vulnerabilities by 33% and 43%, respectively.", "conclusion": "The developed framework effectively enhances the detection of implicit influential patterns and improves related classification tasks, contributing to more robust detection mechanisms against sophisticated linguistic strategies."}}
{"id": "2506.14213", "pdf": "https://arxiv.org/pdf/2506.14213", "abs": "https://arxiv.org/abs/2506.14213", "authors": ["Jongho Kim", "Dohyeon Lee", "Minsoo Kim", "Seung-won Hwang"], "title": "Chaining Event Spans for Temporal Relation Grounding", "categories": ["cs.CL"], "comment": "In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1689-1700", "summary": "Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: \"What finished right before the decision?\" or \"What finished right after the decision?\". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.", "AI": {"tldr": "This paper proposes a new method called Timeline Reasoning Network (TRN) to improve the accuracy of understanding temporal relations between events in tasks like temporal reading comprehension and relation extraction.", "motivation": "Existing solutions rely on answer overlaps as a proxy label to contrast similar and dissimilar questions, but this can lead to unreliable results due to spurious overlaps.", "method": "TRN operates in a two-step inductive reasoning process: it first answers each question with semantic and syntactic information, then chains multiple questions on the same event to predict a timeline, which is used to ground the answers.", "result": "Results on the TORQUE and TB-dense, TRC and TRE tasks show that TRN outperforms previous methods by effectively resolving spurious overlaps using the predicted timeline.", "conclusion": "The proposed TRN method improves the accuracy of understanding temporal relations between events in various tasks."}}
{"id": "2506.14234", "pdf": "https://arxiv.org/pdf/2506.14234", "abs": "https://arxiv.org/abs/2506.14234", "authors": ["Md Tanzib Hosain", "Salman Rahman", "Md Kishor Morol", "Md Rizwan Parvez"], "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.", "AI": {"tldr": "Xolver is a novel multi-agent reasoning framework that enhances LLMs by introducing persistent, evolving memory and experience integration, improving performance across various tasks.", "motivation": "Current LLMs lack the ability to accumulate and integrate experiential knowledge like human experts do, which limits their reasoning capabilities.", "method": "Xolver uses a training-free approach with diverse experience modalities including retrieval, tool use, collaboration, evaluation, and iterative refinement.", "result": "Xolver outperforms specialized reasoning agents, even with lightweight models, achieving state-of-the-art results on several benchmarks.", "conclusion": "Holistic experience learning is crucial for creating generalist agents capable of expert-level reasoning."}}
{"id": "2506.14235", "pdf": "https://arxiv.org/pdf/2506.14235", "abs": "https://arxiv.org/abs/2506.14235", "authors": ["Yimin Deng", "Yuxia Wu", "Yejing Wang", "Guoshuai Zhao", "Li Zhu", "Qidong Liu", "Derong Xu", "Zichuan Fu", "Xian Wu", "Yefeng Zheng", "Xiangyu Zhao", "Xueming Qian"], "title": "A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs", "categories": ["cs.CL"], "comment": "ACL25 findings", "summary": "Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.", "AI": {"tldr": "A new method integrates both structural and semantic information for predicting future events in temporal knowledge graphs.", "motivation": "Previous methods fail to combine dual reasoning perspectives and cannot capture differences between historical and non-historical events, limiting their performance across different time contexts.", "method": "Proposes MESH, a framework using three expert modules to guide reasoning processes for different events.", "result": "Shows effectiveness through experiments on three datasets.", "conclusion": "Integrating structural and semantic information improves temporal knowledge graph reasoning."}}
{"id": "2506.14248", "pdf": "https://arxiv.org/pdf/2506.14248", "abs": "https://arxiv.org/abs/2506.14248", "authors": ["Chenghao Li", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Yibing Zhan"], "title": "Re-Initialization Token Learning for Tool-Augmented Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.", "AI": {"tldr": "We propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, improving tool call accuracy.", "motivation": "Current methods fail to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs.", "method": "We construct prior token embeddings for each tool based on its name or description, which are used to initialize and regularize the learnable tool token embeddings.", "result": "The method shows clear improvements over recent baselines on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation.", "conclusion": "Our proposed token learning method enhances the performance of large language models in calling external tools."}}
{"id": "2506.14285", "pdf": "https://arxiv.org/pdf/2506.14285", "abs": "https://arxiv.org/abs/2506.14285", "authors": ["Seongbo Jang", "Minjin Jeon", "Jaehoon Lee", "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu"], "title": "From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.", "AI": {"tldr": "Introducing a new task and benchmark for timely dialogue response generation, along with a model that outperforms existing methods.", "motivation": "The need for models that can respond appropriately based on temporal context.", "method": "A dialogue agent named Timer is trained to predict time intervals and generate timely responses.", "result": "Timer performs better than other models in both turn-level and dialogue-level evaluations.", "conclusion": "Timer surpasses other methods in predicting time intervals and generating timely responses."}}
{"id": "2506.14302", "pdf": "https://arxiv.org/pdf/2506.14302", "abs": "https://arxiv.org/abs/2506.14302", "authors": ["Xueyang Feng", "Jingsen Zhang", "Jiakai Tang", "Wei Li", "Guohao Cai", "Xu Chen", "Quanyu Dai", "Yue Zhu", "Zhenhua Dong"], "title": "Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent", "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.", "AI": {"tldr": "A new method called ECPO is proposed to improve conversational recommendation agents by optimizing multi-turn preferences using Expectation Confirmation Theory.", "motivation": "The short-sighted responses generated by CRAs often fail to sustain user guidance and meet expectations.", "method": "Introducing a novel multi-turn preference optimization (MTPO) paradigm ECPO, leveraging Expectation Confirmation Theory to model user satisfaction evolution and optimize unsatisfactory responses.", "result": "ECPO improves CRA's interaction capabilities, showing better efficiency and effectiveness than existing MTPO methods.", "conclusion": "This work presents a new approach to enhance the performance of CRAs through multi-turn preference optimization."}}
{"id": "2506.14335", "pdf": "https://arxiv.org/pdf/2506.14335", "abs": "https://arxiv.org/abs/2506.14335", "authors": ["Silvia Casola", "Yang Janet Liu", "Siyao Peng", "Oliver Kraus", "Albert Gatt", "Barbara Plank"], "title": "Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics", "categories": ["cs.CL"], "comment": "17 pages, 13 figures", "summary": "Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of using different reference sets on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.", "AI": {"tldr": "This study investigates how different reference sets affect the stability of reference-based summarization metrics, revealing significant instability, particularly in n-gram-based metrics like ROUGE. It also examines correlations between human judgments and metrics on LLM outputs across various genres.", "motivation": "To address the oversight of variation in human language production in summarization evaluation and improve the reliability of model comparisons.", "method": "Analyzing three diverse multi-reference summarization datasets (SummEval, GUMSum, and DUC2004) and collecting human judgments on LLM outputs for genre-diverse data.", "result": "Many popular metrics show significant instability, especially n-gram-based metrics which vary rankings based on reference sets. There is weak-to-no correlation between human judgments and metrics on LLM outputs.", "conclusion": "It is recommended to incorporate reference set variation into summarization evaluation to enhance consistency and correlation with human judgments, especially for evaluating LLMs."}}
{"id": "2506.14345", "pdf": "https://arxiv.org/pdf/2506.14345", "abs": "https://arxiv.org/abs/2506.14345", "authors": ["Bruno Martins", "Piotr Szyma\u0144ski", "Piotr Gramacki"], "title": "A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.", "AI": {"tldr": "This paper discusses the integration of geo-temporal reasoning into deep research systems powered by large language models to answer context-rich questions.", "motivation": "Current deep research systems lack geo-temporal capabilities needed for questions involving geographic and/or temporal constraints, common in fields like public health, environmental science, and socio-economic analysis.", "method": "Proposing augmenting retrieval and synthesis processes with geo-temporal constraint handling capabilities, supported by open and reproducible infrastructures and rigorous evaluation protocols.", "result": "Vision for next-generation deep research systems with enhanced geo-temporal awareness and potential impact on AI-driven information access.", "conclusion": "Integrating geo-temporal reasoning is crucial for enhancing the functionality of deep research systems powered by large language models."}}
{"id": "2506.14370", "pdf": "https://arxiv.org/pdf/2506.14370", "abs": "https://arxiv.org/abs/2506.14370", "authors": ["Amrit Poudel", "Yifan Ding", "Jurgen Pfeffer", "Tim Weninger"], "title": "Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main", "summary": "Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users.", "AI": {"tldr": "This study examines how Google's search algorithms impact the visibility of different types of content on social media platforms.", "motivation": "To investigate the impact of search engines on the visibility of content related to controversial topics", "method": "Comparing search engine results with nonsampled data from Reddit and Twitter/X", "result": "Google's algorithms tend to suppress certain types of content such as sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies while promoting highly engaging content.", "conclusion": "Google's gatekeeping practices influence public discourse by shaping the narratives available to users."}}
{"id": "2506.14371", "pdf": "https://arxiv.org/pdf/2506.14371", "abs": "https://arxiv.org/abs/2506.14371", "authors": ["Lucile Favero", "Daniel Frases", "Juan Antonio P\u00e9rez-Ortiz", "Tanja K\u00e4ser", "Nuria Oliver"], "title": "ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection", "categories": ["cs.CL", "cs.HC"], "comment": "Proceedings of the 12th Workshop on Argument Mining", "summary": "The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.", "AI": {"tldr": "This work explores how large language models can be used to generate critical questions that promote deeper reasoning and critical thinking when engaging with argumentative texts.", "motivation": "Concerns about promoting superficial learning and undermining critical thinking skills due to the widespread use of chat interfaces based on large language models (LLMs).", "method": "A two-step framework involving a Questioner model that generates candidate questions and a Judge model that selects the most relevant ones.", "result": "The system ranked first in a shared task competition focused on automatic critical question generation.", "conclusion": "This study demonstrates the potential of using LLM-based approaches to encourage critical engagement with argumentative texts."}}
{"id": "2506.14397", "pdf": "https://arxiv.org/pdf/2506.14397", "abs": "https://arxiv.org/abs/2506.14397", "authors": ["Yeonkyoung So", "Gyuseong Lee", "Sungmok Jung", "Joonhak Lee", "JiA Kang", "Sangho Kim", "Jaejin Lee"], "title": "Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \\textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.", "AI": {"tldr": "Introduce Thunder-NUBench, a novel benchmark for assessing sentence-level negation understanding in large language models.", "motivation": "Existing benchmarks lack focus on negation understanding in language models, treating it as a secondary aspect of broader tasks.", "method": "Contrasts standard negation with structurally diverse alternatives and includes manually curated sentence-negation pairs and a multiple-choice dataset.", "result": "A new benchmark called Thunder-NUBench that evaluates deep semantic understanding of negation in language models.", "conclusion": "Thunder-NUBench provides a more comprehensive assessment of negation understanding in large language models."}}
{"id": "2506.14407", "pdf": "https://arxiv.org/pdf/2506.14407", "abs": "https://arxiv.org/abs/2506.14407", "authors": ["Zeinab Sadat Taghavi", "Ali Modarressi", "Yunpu Ma", "Hinrich Sch\u00fctze"], "title": "ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving \"two days ago\"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.", "AI": {"tldr": "This paper introduces ImpliRet, a new benchmark for evaluating retrieval systems based on implicit reasoning challenges within documents rather than query processing techniques. It shows that current retrievers perform poorly on this task.", "motivation": "To evaluate retrieval systems beyond surface-level cues and shift the reasoning challenge from query processing to document understanding.", "method": "Introducing a new benchmark called ImpliRet with implicit reasoning tasks within documents.", "result": "Current sparse and dense retrievers perform poorly with the best nDCG@10 score being 15.07%. Even GPT-4.1 struggles with a short context scoring only 35.06%.", "conclusion": "Document-side reasoning remains a significant challenge for retrieval systems."}}
{"id": "2506.14429", "pdf": "https://arxiv.org/pdf/2506.14429", "abs": "https://arxiv.org/abs/2506.14429", "authors": ["Xiaoran Liu", "Zhigeng Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs", "categories": ["cs.CL"], "comment": "16 pages, 12 figures, work in progress", "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textbf{\\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textbf{\\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.", "AI": {"tldr": "This paper presents a systematic comparison of long-context performance between diffusion LLMs and traditional auto-regressive LLMs. It identifies stable perplexity and local perception phenomena in diffusion LLMs, explains them using RoPE scaling theory, and proposes a training-free method called LongLLaDA to extend context windows.", "motivation": "To investigate the long-context capabilities of diffusion LLMs which were previously unexplored.", "method": "Systematic investigation comparing diffusion LLMs and traditional auto-regressive LLMs, identifying unique characteristics like stable perplexity and local perception, and proposing LongLLaDA.", "result": "Diffusion LLMs show stable perplexity during context extrapolation and can retrieve from recent context segments due to local perception. The proposed LongLLaDA method effectively extends context windows.", "conclusion": "Establishes the first context extrapolation method for diffusion LLMs, offering theoretical insights and empirical benchmarks for future research."}}
{"id": "2506.14448", "pdf": "https://arxiv.org/pdf/2506.14448", "abs": "https://arxiv.org/abs/2506.14448", "authors": ["Jiayin Wang", "Zhiquang Guo", "Weizhi Ma", "Min Zhang"], "title": "How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison", "categories": ["cs.CL"], "comment": null, "summary": "As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.", "AI": {"tldr": "This paper introduces semantic games to evaluate test-time learning in large language models and finds that while LLMs can learn during testing, their progress is less stable and slower than humans.", "motivation": "The need for comprehensive and forward-looking assessment of large language models, moving beyond static knowledge evaluation.", "method": "Proposing semantic games as testbeds for evaluating test-time learning and introducing an evaluation framework with four forms of experience representation.", "result": "LLMs demonstrate test-time learning abilities but with less stability and slower progress under cumulative experience compared to humans.", "conclusion": "LLMs have measurable test-time learning capabilities but show less stability and slower improvement under cumulative experience compared to humans."}}
{"id": "2506.14474", "pdf": "https://arxiv.org/pdf/2506.14474", "abs": "https://arxiv.org/abs/2506.14474", "authors": ["Eyal German", "Sagiv Antebi", "Edan Habler", "Asaf Shabtai", "Yuval Elovici"], "title": "LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.", "AI": {"tldr": "LexiMark\u662f\u4e00\u79cd\u65b0\u578b\u7684\u6587\u672c\u548c\u6587\u6863\u6c34\u5370\u6280\u672f\uff0c\u901a\u8fc7\u5728\u9ad8\u71b5\u8bcd\u4e2d\u5d4c\u5165\u540c\u4e49\u8bcd\u66ff\u6362\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6c34\u5370\u6587\u672c\u7684\u8bb0\u5fc6\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u7684\u540c\u65f6\u4f7f\u6c34\u5370\u96be\u4ee5\u88ab\u68c0\u6d4b\u548c\u79fb\u9664\u3002\u8bc4\u4f30\u663e\u793a\uff0cLexiMark\u5728\u591a\u4e2a\u5f00\u6e90\u6a21\u578b\u548c\u8bad\u7ec3\u573a\u666f\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86AUROC\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u6c34\u5370\u65b9\u6cd5\u7f3a\u4e4f\u9690\u853d\u6027\uff0c\u5bb9\u6613\u88ab\u68c0\u6d4b\u548c\u79fb\u9664\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9690\u853d\u3001\u66f4\u6709\u6548\u7684\u6c34\u5370\u6280\u672f\u6765\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e\u4e0d\u88ab\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u3002", "method": "LexiMark\u901a\u8fc7\u5728\u9ad8\u71b5\u8bcd\u4e2d\u8fdb\u884c\u540c\u4e49\u8bcd\u66ff\u6362\uff0c\u4f7f\u5f97\u6c34\u5370\u5d4c\u5165\u5230\u6587\u672c\u4e2d\u800c\u4e0d\u6539\u53d8\u5176\u8bed\u4e49\u5b8c\u6574\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6c34\u6807\u8bb0\u6587\u672c\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u5e76\u4f7f\u5176\u96be\u4ee5\u88ab\u68c0\u6d4b\u548c\u79fb\u9664\u3002", "result": "LexiMark\u5728\u591a\u4e2a\u5f00\u6e90\u6a21\u578b\u548c\u8bad\u7ec3\u573a\u666f\u4e0b\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5176AUROC\u5206\u6570\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u68c0\u6d4b\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u7684\u6c34\u6807\u8bb0\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "LexiMark\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9690\u853d\u7684\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u53ef\u9760\u5730\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4f7f\u7528\u4e86\u672a\u7ecf\u6388\u6743\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002"}}
{"id": "2506.14493", "pdf": "https://arxiv.org/pdf/2506.14493", "abs": "https://arxiv.org/abs/2506.14493", "authors": ["Jiyuan Fu", "Kaixun Jiang", "Lingyi Hong", "Jinglun Li", "Haijing Guo", "Dingkang Yang", "Zhaoyu Chen", "Wenqiang Zhang"], "title": "LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance.", "AI": {"tldr": "An attack named LingoLoop is proposed to make Multimodal Large Language Models (MLLMs) generate excessively verbose and repetitive sequences.", "motivation": "Attackers can exploit the substantial computational resources required during inference of MLLMs by inducing excessive output, leading to resource exhaustion and service degradation.", "method": "The method includes a POS-Aware Delay Mechanism to postpone EOS token generation and a Generative Path Pruning Mechanism to limit the magnitude of hidden states, encouraging the model to produce persistent loops.", "result": "Experiments show that LingoLoop can increase generated tokens and energy consumption by up to 30 times on models like Qwen2.5-VL-3B.", "conclusion": "This study reveals significant vulnerabilities of MLLMs, posing challenges for their reliable deployment."}}
{"id": "2506.14532", "pdf": "https://arxiv.org/pdf/2506.14532", "abs": "https://arxiv.org/abs/2506.14532", "authors": ["Can Zheng", "Jiguang He", "Chung G. Kang", "Guofa Cai", "Zitong Yu", "Merouane Debbah"], "title": "M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models", "categories": ["cs.CL"], "comment": "13 pages, 20 figures", "summary": "This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems.", "AI": {"tldr": "A new neural network framework named M2BeamLLM is proposed for beam prediction in mmWave mMIMO communication systems.", "motivation": "To provide an efficient and intelligent beam prediction solution for V2I mmWave communication systems by integrating multi-modal sensor data and using LLMs' reasoning capabilities.", "method": "M2BeamLLM integrates multi-modal sensor data and uses large language models like GPT-2. It combines sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning.", "result": "M2BeamLLM achieves higher beam prediction accuracy and robustness, outperforming traditional DL models in both standard and few-shot scenarios. Its performance improves with more diverse sensing modalities.", "conclusion": "M2BeamLLM offers a superior solution for beam prediction in V2I mmWave communication systems."}}
{"id": "2506.14562", "pdf": "https://arxiv.org/pdf/2506.14562", "abs": "https://arxiv.org/abs/2506.14562", "authors": ["Di He", "Ajay Jaiswal", "Songjun Tu", "Li Shen", "Ganzhao Yuan", "Shiwei Liu", "Lu Yin"], "title": "AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify \"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.", "AI": {"tldr": "Introduce AlphaDecay, an adaptive weight decay method for large language models that assigns different decay strengths based on the 'heavy-tailedness' of each module's spectral properties, improving performance over uniform decay.", "motivation": "To address the limitations of uniform weight decay in handling the structural diversity and varying spectral properties of large language models.", "method": "AlphaDecay assigns different weight decay strengths to each module based on Heavy-Tailed Self-Regularization (HT-SR) theory, analyzing the empirical spectral density (ESD) of weight correlation matrices.", "result": "AlphaDecay outperforms conventional uniform decay and other adaptive decay methods in pre-training tasks with model sizes ranging from 60M to 1B.", "conclusion": "AlphaDecay improves the performance of large language models by adaptively assigning weight decay strengths according to the spectral properties of each module."}}
{"id": "2506.14580", "pdf": "https://arxiv.org/pdf/2506.14580", "abs": "https://arxiv.org/abs/2506.14580", "authors": ["David Wan", "Eran Hirsch", "Elias Stengel-Eskin", "Ido Dagan", "Mohit Bansal"], "title": "GenerationPrograms: Fine-grained Attribution with Executable Programs", "categories": ["cs.CL", "cs.AI"], "comment": "27 Pages. Code: https://github.com/meetdavidwan/generationprograms", "summary": "Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable \"code agent\" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGenerationPrograms\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5f52\u56e0\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6e90\u6761\u4ef6\u6587\u672c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e38\u5e38\u65e0\u6cd5\u6b63\u786e\u63d0\u4f9b\u5176\u8f93\u51fa\u7684\u7cbe\u7ec6\u5f52\u56e0\uff0c\u8fd9\u524a\u5f31\u4e86\u53ef\u9a8c\u8bc1\u6027\u548c\u4fe1\u4efb\u5ea6\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u5f52\u56e0\u65b9\u6cd5\u4e0d\u80fd\u89e3\u91ca\u6a21\u578b\u5982\u4f55\u4ee5\u53ca\u4e3a\u4f55\u5229\u7528\u63d0\u4f9b\u7684\u6e90\u6587\u6863\u751f\u6210\u6700\u7ec8\u54cd\u5e94\uff0c\u9650\u5236\u4e86\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGenerationPrograms\u7684\u6a21\u5757\u5316\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u7279\u5b9a\u4e8e\u67e5\u8be2\u7684\u6587\u672c\u64cd\u4f5c\uff08\u5982\u91ca\u4e49\u3001\u538b\u7f29\u548c\u878d\u5408\uff09\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\u8ba1\u5212\uff0c\u7136\u540e\u6309\u7167\u7a0b\u5e8f\u6307\u5b9a\u7684\u6307\u4ee4\u6267\u884c\u8fd9\u4e9b\u64cd\u4f5c\u4ee5\u4ea7\u751f\u6700\u7ec8\u54cd\u5e94\u3002", "result": "GenerationPrograms\u5728\u4e24\u4e2a\u957f\u7bc7\u95ee\u7b54\u4efb\u52a1\u548c\u4e00\u4e2a\u591a\u6587\u6863\u6458\u8981\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u6863\u7ea7\u548c\u53e5\u5b50\u7ea7\u5f52\u56e0\u8d28\u91cf\u3002\u5b83\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u540e\u9a8c\u5f52\u56e0\u65b9\u6cd5\u6709\u6548\u5de5\u4f5c\uff0c\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\u4ee5\u6062\u590d\u51c6\u786e\u7684\u5f52\u56e0\u3002\u751f\u6210\u7684\u53ef\u89e3\u91ca\u7a0b\u5e8f\u8fd8\u901a\u8fc7\u6a21\u5757\u7ea7\u6539\u8fdb\u5b9e\u73b0\u4e86\u5c40\u90e8\u7ec6\u5316\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6574\u4f53\u5f52\u56e0\u8d28\u91cf\u3002", "conclusion": "GenerationPrograms\u663e\u8457\u63d0\u9ad8\u4e86\u4e24\u79cd\u957f\u7bc7\u95ee\u7b54\u4efb\u52a1\u548c\u4e00\u79cd\u591a\u6587\u6863\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u6587\u6863\u7ea7\u548c\u53e5\u5b50\u7ea7\u5f52\u56e0\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u540e\u9a8c\u5f52\u56e0\u65b9\u6cd5\u6709\u6548\u5730\u5de5\u4f5c\uff0c\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\u4ee5\u6062\u590d\u51c6\u786e\u7684\u5f52\u56e0\uff0c\u5e76\u4e14\u751f\u6210\u7684\u53ef\u89e3\u91ca\u7a0b\u5e8f\u901a\u8fc7\u6a21\u5757\u7ea7\u6539\u8fdb\u5b9e\u73b0\u4e86\u5c40\u90e8\u7ec6\u5316\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6574\u4f53\u5f52\u56e0\u8d28\u91cf\u3002"}}
{"id": "2506.14606", "pdf": "https://arxiv.org/pdf/2506.14606", "abs": "https://arxiv.org/abs/2506.14606", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Chaimaa Abi", "Celine Lee", "Abdulrahman Mahmoud"], "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees", "categories": ["cs.CL", "cs.AR", "cs.LG", "cs.PL", "cs.SE"], "comment": "Project page: https://ahmedheakl.github.io/Guaranteed-Guess/", "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.", "AI": {"tldr": "GG is a novel transpilation pipeline combining LLMs and software testing to translate between CISC and RISC ISAs effectively, outperforming Rosetta 2 in speed, efficiency, and memory usage.", "motivation": "Enhancing the portability and longevity of existing code by quickly, flexibly, and correctly translating low-level programs across different instruction set architectures (ISAs), especially between complex- (CISC) and reduced- (RISC) hardware architectures.", "method": "Introducing GG (Guaranteed Guess), an ISA-centric transpilation pipeline that uses pre-trained large language models (LLMs) to generate candidate translations and embeds them within a software-testing framework to ensure translation confidence.", "result": "Achieved functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs. Compared to Rosetta 2, GG shows 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for transpiled code.", "conclusion": "Open-sourcing all resources to establish a common foundation for ISA-level code translation research."}}
{"id": "2506.14613", "pdf": "https://arxiv.org/pdf/2506.14613", "abs": "https://arxiv.org/abs/2506.14613", "authors": ["Junghyun Min", "Xiulin Yang", "Shira Wein"], "title": "When Does Meaning Backfire? Investigating the Role of AMRs in NLI", "categories": ["cs.CL"], "comment": "9 pages, 2 figures", "summary": "Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis. In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in \\texttt{GPT-4o}. However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved.", "AI": {"tldr": "This study explores whether adding Abstract Meaning Representation (AMR) improves the performance of pretrained language models in Natural Language Inference tasks.", "motivation": "To enhance the semantic understanding of language models by introducing AMR.", "method": "Integrating AMR into NLI tasks under both fine-tuning and prompting settings.", "result": "Fine-tuning with AMR hampers model generalization, while prompting with AMR slightly improves performance in GPT-4o. However, an ablation study shows that the improvement results from amplifying surface-level differences rather than aiding semantic reasoning.", "conclusion": "Adding AMR may mislead models to predict non-entailment when the core meaning is preserved."}}
{"id": "2506.14625", "pdf": "https://arxiv.org/pdf/2506.14625", "abs": "https://arxiv.org/abs/2506.14625", "authors": ["Chenchen Yuan", "Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "title": "Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.", "AI": {"tldr": "This paper proposes a framework that aggregates multiple LLMs' moral judgments to realign deviating models, demonstrating improved fidelity and robust consensus on moral dilemmas.", "motivation": "To address inconsistencies in LLMs' moral reasoning, especially in complex dilemmas, by creating a consensus-based approach.", "method": "Synthesizing moral judgments using an aggregation mechanism that incorporates reliability-weighted continuous scores and fine-tuning deviating models\u2019 embeddings with a targeted optimization process.", "result": "The approach leads to more robust consensus and enhanced individual model performance on moral dilemmas, as shown in experiments.", "conclusion": "Data-driven moral alignment across multiple models could lead to safer and more consistent AI systems."}}
{"id": "2506.14634", "pdf": "https://arxiv.org/pdf/2506.14634", "abs": "https://arxiv.org/abs/2506.14634", "authors": ["Leah von der Heyde", "Anna-Carolina Haensch", "Bernd Wei\u00df", "Jessika Daikeler"], "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "to appear in Survey Research Methods", "summary": "The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fb7\u8bed\u5f00\u653e\u6027\u8c03\u67e5\u56de\u5e94\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u53ea\u6709\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u80fd\u63d0\u4f9b\u6ee1\u610f\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u8ba8\u8bba\u4e86LLMs\u5728\u8c03\u67e5\u7814\u7a76\u4e2d\u7684\u4f7f\u7528\u6761\u4ef6\u53ca\u6ce8\u610f\u4e8b\u9879\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5206\u7c7b\u5f00\u653e\u6027\u8c03\u67e5\u56de\u5e94\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4f5c\u4e3a\u4f20\u7edf\u624b\u52a8\u7f16\u7801\u548c\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u6bd4\u8f83\u4e86\u51e0\u79cd\u6700\u5148\u8fdb\u7684LLMs\u548c\u4e0d\u540c\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u7f16\u7801\u6765\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u6574\u4f53\u8868\u73b0\u56e0LLM\u800c\u5f02\uff0c\u53ea\u6709\u7ecf\u8fc7\u5fae\u8c03\u7684LLM\u8fbe\u5230\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u9884\u6d4b\u6027\u80fd\uff1b\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u7684\u8868\u73b0\u5dee\u5f02\u4f9d\u8d56\u4e8e\u6240\u4f7f\u7528\u7684LLM\uff1bLLMs\u5728\u4e0d\u540c\u7c7b\u522b\u7684\u8c03\u67e5\u53c2\u4e0e\u539f\u56e0\u5206\u7c7b\u8868\u73b0\u4e0d\u5747\uff0c\u53ef\u80fd\u5bfc\u81f4\u975e\u5fae\u8c03\u65f6\u7c7b\u522b\u5206\u5e03\u4e0d\u540c\u3002", "conclusion": "\u7814\u7a76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u5f00\u653e\u6027\u56de\u5e94\u7f16\u7801\u65b9\u6cd5\u8bba\u7814\u7a76\u3001\u5b9e\u8d28\u6027\u5206\u6790\u4ee5\u53ca\u5904\u7406\u6b64\u7c7b\u6570\u636e\u7684\u5b9e\u8df5\u8005\u7684\u542f\u793a\uff0c\u5e76\u5f3a\u8c03\u4e86\u7814\u7a76\u8005\u5728\u9009\u62e9\u81ea\u52a8\u5316\u65b9\u6cd5\u65f6\u9700\u8981\u8003\u8651\u7684\u8bb8\u591a\u6743\u8861\u3002\u8fd9\u9879\u7814\u7a76\u6709\u52a9\u4e8e\u6269\u5927\u5173\u4e8eLLMs\u5728\u8c03\u67e5\u7814\u7a76\u4e2d\u6709\u6548\u3001\u51c6\u786e\u548c\u53ef\u9760\u5229\u7528\u6761\u4ef6\u7684\u7814\u7a76\u9886\u57df\u3002"}}
{"id": "2506.14641", "pdf": "https://arxiv.org/pdf/2506.14641", "abs": "https://arxiv.org/abs/2506.14641", "authors": ["Xiang Cheng", "Chengyan Pan", "Minjun Zhao", "Deyang Li", "Fangchao Liu", "Xinyu Zhang", "Xiao Zhang", "Yong Liu"], "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages,22 figures", "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \\texttt{Qwen2.5-Max} and \\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.", "AI": {"tldr": "In-Context Learning (ICL) with Chain-of-Thought (CoT) doesn't enhance reasoning performance for strong models like Qwen2.5.", "motivation": "To explore if CoT exemplars benefit recent strong models in mathematical reasoning.", "method": "Systematic experiments comparing Zero-Shot CoT with CoT exemplars and enhanced CoT exemplars.", "result": "Adding CoT exemplars doesn't improve reasoning performance; they mainly align output format. Enhanced CoT also fails to boost reasoning. Models focus more on instructions than exemplars.", "conclusion": "Current ICL+CoT framework has limitations in mathematical reasoning and needs re-examination."}}
{"id": "2506.14645", "pdf": "https://arxiv.org/pdf/2506.14645", "abs": "https://arxiv.org/abs/2506.14645", "authors": [". Pazzaglia", "V. Vendetti", "L. D. Comencini", "F. Deriu", "V. Modugno"], "title": "Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.", "AI": {"tldr": "This study investigates whether fine-tuned large language models (LLMs) can replicate and amplify polarizing discourse using a dataset from Reddit. Results show that these models can produce convincing and provocative comments, raising ethical concerns about AI use in politics.", "motivation": "Concerns over the potential role of sophisticated LLMs in worsening ideological polarization by generating persuasive and biased content.", "method": "Fine-tuning an open-source LLM with a curated dataset of politically charged discussions from Reddit and evaluating its outputs through linguistic analysis, sentiment scoring, and human annotation.", "result": "Fine-tuned LLMs trained on partisan data can produce highly plausible and provocative comments, often indistinguishable from human-written ones.", "conclusion": "The study raises ethical questions about AI in political discourse and suggests discussing broader implications for AI governance, platform regulation, and detection tool development."}}
{"id": "2506.14646", "pdf": "https://arxiv.org/pdf/2506.14646", "abs": "https://arxiv.org/abs/2506.14646", "authors": ["Hengyuan Zhang", "Xinrong Chen", "Yingmin Qiu", "Xiao Liang", "Ziyue Li", "Guanyu Wang", "Weiping Li", "Tong Mo", "Wenyue Li", "Hayden Kwok-Hay So", "Ngai Wong"], "title": "GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors", "categories": ["cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git.", "AI": {"tldr": "GuiLoMo enhances LoRA-MoE's performance by optimizing expert allocation through GuidedSelection Vectors.", "motivation": "To address the limitations of existing PEFT methods, especially LoRA-MoE, including the fixed number of experts and uniform rank assignment.", "method": "Introducing GuidedSelection Vectors (GSVs) for fine-grained layer-wise expert numbers and ranks allocation.", "result": "Experiments demonstrate that GuiLoMo outperforms or matches other baselines on various benchmarks.", "conclusion": "GuiLoMo, a novel fine-tuning method, improves the performance of LoRA-MoE by optimizing expert numbers and ranks allocation."}}
{"id": "2506.14681", "pdf": "https://arxiv.org/pdf/2506.14681", "abs": "https://arxiv.org/abs/2506.14681", "authors": ["Yuto Harada", "Yusuke Yamauchi", "Yusuke Oda", "Yohei Oseki", "Yusuke Miyao", "Yu Takagi"], "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality", "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research.", "AI": {"tldr": "Supervised fine-tuning (SFT) is important for aligning LLMs with human instructions and values. Researchers trained over 1,000 SFT models and found that certain training-task synergies are consistent across models while others vary. They also discovered that perplexity predicts SFT effectiveness and mid-layer weight changes are closely related to performance improvements.", "motivation": "To understand more about supervised fine-tuning and its impact on aligning large language models with human instructions and values.", "method": "Trained over 1,000 SFT models using different datasets like code generation, mathematical reasoning, and general-domain tasks under controlled conditions.", "result": "Some training-task synergies are consistent across models while others vary. Perplexity predicts SFT effectiveness better than superficial similarity between trained data and benchmark. Mid-layer weight changes are strongly correlated with performance gains.", "conclusion": "The study emphasizes the importance of model-specific strategies in SFT and provides insights into how to improve SFT effectiveness."}}
{"id": "2506.14702", "pdf": "https://arxiv.org/pdf/2506.14702", "abs": "https://arxiv.org/abs/2506.14702", "authors": ["Daniel D'souza", "Julia Kreutzer", "Adrien Morisot", "Ahmet \u00dcst\u00fcn", "Sara Hooker"], "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: \"Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?\" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.", "AI": {"tldr": "We explore optimizing training protocols to enhance model performance and controllability on underrepresented use cases during inference.", "motivation": "The challenge of handling rare and underrepresented features in large general-purpose models.", "method": "Revisiting training and inference techniques, creating a taxonomy of data characteristics and task provenance, and fine-tuning the model to infer markers automatically.", "result": "Improvements in open-ended generation quality, significant gains in underrepresented domains, and notable enhancements in underrepresented tasks like CodeRepair.", "conclusion": "Our approach provides a flexible way to improve long-tail performance while giving users control levers the model is trained to respond to."}}
{"id": "2506.14704", "pdf": "https://arxiv.org/pdf/2506.14704", "abs": "https://arxiv.org/abs/2506.14704", "authors": ["Anton Changalidis", "Aki H\u00e4rm\u00e4"], "title": "Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data", "categories": ["cs.CL"], "comment": "This work has been accepted for publication at the First Workshop on Large Language Model Memorization (L2M2) at ACL 2025, Vienna, Austria", "summary": "This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data.", "AI": {"tldr": "\u7814\u7a76\u751f\u6210\u5f0f\u53d8\u538b\u5668\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u914d\u7f6e\u5bf9\u7ecf\u9a8c\u8bb0\u5fc6\u5bb9\u91cf\u7684\u5f71\u54cd\u3002\u53d1\u73b0\u5d4c\u5165\u5927\u5c0f\u662f\u5b66\u4e60\u901f\u5ea6\u548c\u80fd\u529b\u7684\u4e3b\u8981\u51b3\u5b9a\u56e0\u7d20\uff0cSoftmax\u6fc0\u6d3b\u51fd\u6570\u8868\u73b0\u66f4\u7a33\u5b9a\uff0c\u589e\u52a0\u6570\u636e\u96c6\u590d\u6742\u6027\u53ef\u63d0\u5347\u6700\u7ec8\u8bb0\u5fc6\u6548\u679c\u3002\u8fd9\u6709\u52a9\u4e8e\u7406\u89e3\u53d8\u538b\u5668\u7684\u8bb0\u5fc6\u673a\u5236\u5e76\u4f18\u5316\u5b9e\u9645\u7ed3\u6784\u5316\u6570\u636e\u7684\u6a21\u578b\u8bbe\u8ba1\u3002", "motivation": "\u7406\u89e3\u751f\u6210\u5f0f\u53d8\u538b\u5668\u6a21\u578b\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5e76\u4f18\u5316\u5176\u5728\u5904\u7406\u7ed3\u6784\u5316\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u65f6\u7684\u8bbe\u8ba1\u3002", "method": "\u4f7f\u7528\u4eceSystematized Nomenclature of Medicine\uff08SNOMED\uff09\u77e5\u8bc6\u56fe\u8c31\u884d\u751f\u7684\u5408\u6210\u6587\u672c\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u5305\u62ec\u8868\u793a\u9759\u6001\u8fde\u63a5\u7684\u4e09\u5143\u7ec4\u548c\u6a21\u62df\u590d\u6742\u5173\u7cfb\u6a21\u5f0f\u7684\u5e8f\u5217\u3002", "result": "\u5d4c\u5165\u5927\u5c0f\u662f\u5b66\u4e60\u901f\u5ea6\u548c\u80fd\u529b\u7684\u4e3b\u8981\u51b3\u5b9a\u56e0\u7d20\uff1b\u989d\u5916\u5c42\u63d0\u4f9b\u7684\u597d\u5904\u6709\u9650\u4e14\u53ef\u80fd\u59a8\u788d\u8f83\u7b80\u5355\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff1bSoftmax\u6fc0\u6d3b\u51fd\u6570\u8868\u73b0\u66f4\u7a33\u5b9a\uff1b\u589e\u52a0\u6570\u636e\u96c6\u590d\u6742\u6027\u53ef\u63d0\u5347\u6700\u7ec8\u8bb0\u5fc6\u6548\u679c\u3002", "conclusion": "\u8fd9\u4e9b\u89c1\u89e3\u52a0\u6df1\u4e86\u6211\u4eec\u5bf9\u53d8\u538b\u5668\u8bb0\u5fc6\u673a\u5236\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u4f18\u5316\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7ed3\u6784\u5316\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u65f6\u3002"}}
{"id": "2506.14731", "pdf": "https://arxiv.org/pdf/2506.14731", "abs": "https://arxiv.org/abs/2506.14731", "authors": ["Ring Team", "Bin Hu", "Cai Chen", "Deng Zhao", "Ding Liu", "Dingnan Jin", "Feng Zhu", "Hao Dai", "Hongzhi Luan", "Jia Guo", "Jiaming Liu", "Jiewei Wu", "Jun Mei", "Jun Zhou", "Junbo Zhao", "Junwu Xiong", "Kaihong Zhang", "Kuan Xu", "Lei Liang", "Liang Jiang", "Liangcheng Fu", "Longfei Zheng", "Qiang Gao", "Qing Cui", "Quan Wan", "Shaomian Zheng", "Shuaicheng Li", "Tongkai Yang", "Wang Ren", "Xiaodong Yan", "Xiaopei Wan", "Xiaoyun Feng", "Xin Zhao", "Xinxing Yang", "Xinyu Kong", "Xuemin Yang", "Yang Li", "Yingting Wu", "Yongkang Liu", "Zhankai Xu", "Zhenduo Zhang", "Zhenglei Zhou", "Zhenyu Huang", "Zhiqiang Zhang", "Zihao Wang", "Zujie Wen"], "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report", "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.", "AI": {"tldr": "Ring-lite\u662f\u4e00\u79cd\u57fa\u4e8eMixture-of-Experts\uff08MoE\uff09\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u63a8\u7406\u80fd\u529b\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b83\u4e0e\u6700\u5148\u8fdb\u7684\u5c0f\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u4ec5\u6fc0\u6d3b\u4e86\u540c\u7c7b\u6a21\u578b\u6240\u9700\u53c2\u6570\u7684\u4e09\u5206\u4e4b\u4e00\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u96c6\u6210\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8054\u5408\u8bad\u7ec3\u7ba1\u9053\uff0c\u5e76\u63d0\u51fa\u4e86C3PO\u65b9\u6cd5\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5c55\u793a\u4e86\u57fa\u4e8e\u71b5\u635f\u5931\u9009\u62e9\u84b8\u998f\u68c0\u67e5\u70b9\u4f18\u4e8e\u57fa\u4e8e\u9a8c\u8bc1\u6307\u6807\u7684\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u89e3\u51b3\u591a\u57df\u6570\u636e\u6574\u5408\u4e2d\u7684\u9886\u57df\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u63d0\u9ad8\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u57fa\u4e8eLing-lite\u6a21\u578b\u6784\u5efaRing-lite\uff0c\u91c7\u7528\u96c6\u6210\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8054\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u63d0\u51faC3PO\u7b97\u6cd5\u6765\u89e3\u51b3\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002\u8fd8\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u5904\u7406\u591a\u57df\u6570\u636e\u6574\u5408\u4e2d\u7684\u9886\u57df\u51b2\u7a81\u3002", "result": "Ring-lite\u5728AIME\u3001LiveCodeBench\u3001GPQA-Diamond\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u5c0f\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u53ea\u6fc0\u6d3b\u4e86\u4e09\u5206\u4e4b\u4e00\u7684\u53c2\u6570\u3002", "conclusion": "Ring-lite\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u51cf\u5c11\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2506.14758", "pdf": "https://arxiv.org/pdf/2506.14758", "abs": "https://arxiv.org/abs/2506.14758", "authors": ["Daixuan Cheng", "Shaohan Huang", "Xuekai Zhu", "Bo Dai", "Wayne Xin Zhao", "Zhenliang Zhang", "Furu Wei"], "title": "Reasoning with Exploration: An Entropy Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.", "AI": {"tldr": "Introduce a simple yet effective method to enhance language model reasoning by encouraging exploration through longer and deeper reasoning chains.", "motivation": "To balance exploration and exploitation in reinforcement learning for language models.", "method": "Augmenting the advantage function with an entropy-based term.", "result": "Significant gains on the Pass@K metric, indicating improved reasoning capabilities.", "conclusion": "The paper concludes that their approach effectively improves the reasoning capability of language models."}}
{"id": "2506.14761", "pdf": "https://arxiv.org/pdf/2506.14761", "abs": "https://arxiv.org/abs/2506.14761", "authors": ["Mathurin Videau", "Badr Youbi Idrissi", "Alessandro Leite", "Marc Schoenauer", "Olivier Teytaud", "David Lopez-Paz"], "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.", "AI": {"tldr": "Introduces a novel autoregressive U-Net that learns its own tokenization during training, offering flexibility over traditional fixed tokenization methods like BPE.", "motivation": "To overcome the limitations of fixed tokenization granularities imposed by methods like Byte Pair Encoding (BPE).", "method": "An autoregressive U-Net is introduced to learn its own tokens during training, providing a multi-scale view of the sequence.", "result": "Deeper hierarchies show a promising trend in performance compared to shallower ones.", "conclusion": "The proposed autoregressive U-Net demonstrates competitive performance against BPE methods."}}
{"id": "2506.14767", "pdf": "https://arxiv.org/pdf/2506.14767", "abs": "https://arxiv.org/abs/2506.14767", "authors": ["Li-Wei Chen", "Takuya Higuchi", "Zakaria Aldeneh", "Ahmed Hussen Abdelaziz", "Alexander Rudnicky"], "title": "A Variational Framework for Improving Naturalness in Generative Spoken Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "International Conference on Machine Learning (ICML) 2025", "summary": "The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.", "AI": {"tldr": "This paper proposes an end-to-end variational approach to automatically learn and encode continuous speech attributes into semantic tokens, improving the naturalness of generated speech without the need for manual feature extraction.", "motivation": "To address the issue of reduced naturalness in speech generated by models trained on semantic tokens which neglect prosodic information.", "method": "An end-to-end variational approach that automatically learns to encode continuous speech attributes.", "result": "Preferred speech continuations according to human raters.", "conclusion": "The proposed method eliminates the need for manual extraction and selection of paralinguistic features and improves the naturalness of generated speech."}}
{"id": "2506.13771", "pdf": "https://arxiv.org/pdf/2506.13771", "abs": "https://arxiv.org/abs/2506.13771", "authors": ["Banseok Lee", "Dongkyu Kim", "Youngcheon You", "Youngmin Kim"], "title": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.", "AI": {"tldr": "LittleBit is a new method for compressing large language models to as little as 0.1 bits per weight while maintaining performance.", "motivation": "The need for efficient deployment of large language models in memory and computationally constrained environments.", "method": "Representing model weights in a low-rank form using latent matrix factorization followed by binarization, with a multi-scale compensation mechanism to handle information loss.", "result": "Achieves nearly 31x memory reduction (e.g., Llama2-13B to under 0.9GB), outperforms other methods in sub-1-bit quantization, and shows potential for 5x speedup over FP16.", "conclusion": "LittleBit enables efficient deployment of powerful large language models in resource-limited settings."}}
{"id": "2506.13778", "pdf": "https://arxiv.org/pdf/2506.13778", "abs": "https://arxiv.org/abs/2506.13778", "authors": ["Anvi Alex Eponon", "Moein Shahiki-Tash", "Ildar Batyrshin", "Christian E. Maldonado-Sifuentes", "Grigori Sidorov", "Alexander Gelbukh"], "title": "Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.\n  In single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce \"paper-cards\", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.\n  For multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.\n  This method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.", "AI": {"tldr": "A new question-based knowledge encoding approach improves RAG systems without fine-tuning or traditional chunking, achieving better performance than previous methods.", "motivation": "To improve the efficiency and effectiveness of RAG systems by eliminating the need for fine-tuning and traditional chunking.", "method": "Encoding textual content using generated questions and introducing a custom syntactic reranking method.", "result": "The approach outperforms traditional methods in single-hop and multi-hop retrieval tasks, with significant improvements in recall and F1 scores.", "conclusion": "This method is scalable, efficient, and provides intuitive question-driven knowledge access, making it a promising alternative to existing RAG approaches."}}
{"id": "2506.13784", "pdf": "https://arxiv.org/pdf/2506.13784", "abs": "https://arxiv.org/abs/2506.13784", "authors": ["Junting Zhou", "Wang Li", "Yiyan Liao", "Nengyuan Zhang", "Tingjia Miaoand Zhihui Qi", "Yuhan Wu", "Tong Yang"], "title": "AcademicBrowse: Benchmarking Academic Browse Ability of LLMs", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)' search capabilities have garnered significant attention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on general search scenarios and fail to adequately address the specific demands of academic search. These demands include deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor. Here, we proposed AcademicBrowse, the first dataset specifically designed to evaluate the complex information retrieval capabilities of Large Language Models (LLMs) in academic research. AcademicBrowse possesses the following key characteristics: Academic Practicality, where question content closely mirrors real academic learning and research environments, avoiding deliberately misleading models; High Difficulty, with answers that are challenging for single models (e.g., Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring at least three deep searches to derive; Concise Evaluation, where limiting conditions ensure answers are as unique as possible, accompanied by clear sources and brief solution explanations, greatly facilitating subsequent audit and verification, surpassing the current lack of analyzed search datasets both domestically and internationally; and Broad Coverage, as the dataset spans at least 15 different academic disciplines. Through AcademicBrowse, we expect to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks. The data is available at: https://huggingface.co/datasets/PKU-DS-LAB/AcademicBrowse", "AI": {"tldr": "Introducing AcademicBrowse, a dataset for evaluating LLMs' academic search capabilities, featuring practicality, difficulty, concise evaluation, and broad coverage across at least 15 disciplines.", "motivation": "Existing benchmarks fail to adequately address the specific demands of academic search, including deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor.", "method": "Proposing AcademicBrowse, a dataset specifically designed to evaluate the complex information retrieval capabilities of LLMs in academic research.", "result": "AcademicBrowse has key characteristics such as Academic Practicality, High Difficulty, Concise Evaluation, and Broad Coverage.", "conclusion": "AcademicBrowse aims to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks."}}
{"id": "2506.13792", "pdf": "https://arxiv.org/pdf/2506.13792", "abs": "https://arxiv.org/abs/2506.13792", "authors": ["Gon\u00e7alo Hora de Carvalho", "Lazar S. Popov", "Sander Kaatee", "Kristinn R. Th\u00f3risson", "Tangrui Li", "P\u00e9tur H\u00fani Bj\u00f6rnsson", "Jilles S. Dibangoye"], "title": "ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \\& a ML Ensemble on Longitudinal Identity Resolution", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.AP"], "comment": null, "summary": "We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.", "AI": {"tldr": "Introduce ICE-ID, a new benchmark dataset for historical identity resolution covering 220 years of Icelandic census records. Evaluate different methods including NARS, which shows competitive results.", "motivation": "To create a large-scale, open tabular dataset for studying long-term person-entity matching in real-world populations.", "method": "Introduced ICE-ID dataset and evaluated various methods like handcrafted rule-based matchers, ML ensemble, LLMs for structured data, and a novel approach called NARS.", "result": "NARS showed surprisingly simple and competitive results, achieving state-of-the-art performance on the defined tasks.", "conclusion": "The release of ICE-ID and associated code enables reproducible benchmarking and opens new avenues for interdisciplinary research in data linkage and historical analytics."}}
{"id": "2506.13811", "pdf": "https://arxiv.org/pdf/2506.13811", "abs": "https://arxiv.org/abs/2506.13811", "authors": ["Sompote Youwai", "David Phim", "Vianne Gayl Murcia", "Rianne Clair Onas"], "title": "Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study", "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": null, "summary": "This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.", "AI": {"tldr": "This study evaluates three approaches for automating foundation design calculations using router-based multi-agent systems and finds that the router-based configuration performs best.", "motivation": "To investigate router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection.", "method": "Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection.", "result": "The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, outperforming conventional agentic workflows by 10.0 to 43.75 percentage points.", "conclusion": "Router-based multi-agent systems are optimal for foundation design automation while maintaining professional documentation standards and ensuring human oversight due to safety-critical requirements in civil engineering."}}
{"id": "2506.13923", "pdf": "https://arxiv.org/pdf/2506.13923", "abs": "https://arxiv.org/abs/2506.13923", "authors": ["Vaskar Nath", "Elaine Lau", "Anisha Gunjal", "Manasi Sharma", "Nikhil Baharte", "Sean Hendryx"], "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via \"capability gain\" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\\text{Guide}$ - a new class of online training algorithms. $\\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the \"off-policy\" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.", "AI": {"tldr": "This paper investigates how reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. It identifies two main ways RLVR enhances performance and demonstrates findings across various model sizes. The authors also introduce Guide, a new class of online training algorithms, showing improvements in generalization.", "motivation": "To understand how reinforcement learning on verifiable rewards (RLVR) helps reasoning models solve new problems and to develop new training methods.", "method": "Analyzing the process of solving reasoning problems across different model sizes, identifying 'capability gain' and 'self-distillation', and introducing the Guide algorithm with adaptive hint incorporation.", "result": "Found that RLVR enhances performance through capability gain and self-distillation. Demonstrated findings across multiple model sizes and domains. Showed Guide improves generalization with up to 4% macro-average improvement.", "conclusion": "The study provides insights into how RLVR-trained models can learn to solve new problems and introduces a novel training method, Guide, which shows promise in improving model generalization."}}
{"id": "2506.13971", "pdf": "https://arxiv.org/pdf/2506.13971", "abs": "https://arxiv.org/abs/2506.13971", "authors": ["Andrew Chang", "Chenkai Hu", "Ji Qi", "Zhuojian Wei", "Kexin Zhang", "Viswadruth Akkaraju", "David Poeppel", "Dustin Freeman"], "title": "Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.MM"], "comment": "Interspeech 2025", "summary": "Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience.", "AI": {"tldr": "A semi-supervised learning approach effectively predicts negative experience moments in video conferences using multimodal data, reducing the need for extensive labeled data.", "motivation": "There is a lack of study on the subjective moments of negative experience in group videoconferences, which are rare in natural data and costly to annotate manually.", "method": "Semi-supervised learning was used with multimodal deep features (audio, facial, text) to predict non-fluid or unenjoyable moments, employing modality-fused co-training.", "result": "The semi-supervised learning model achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming supervised learning models and showing high efficiency with less labeled data.", "conclusion": "The study developed an efficient annotation method using semi-supervised learning to predict negative experience moments in videoconference sessions."}}
{"id": "2506.13977", "pdf": "https://arxiv.org/pdf/2506.13977", "abs": "https://arxiv.org/abs/2506.13977", "authors": ["Shiting Huang", "Zhen Fang", "Zehui Chen", "Siyu Yuan", "Junjie Ye", "Yu Zeng", "Lin Chen", "Qi Mao", "Feng Zhao"], "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at \\href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.", "AI": {"tldr": "\u63d0\u51faCRITICTOOL\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u65f6\u7684\u9519\u8bef\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u65f6\u53ef\u80fd\u9047\u5230\u5404\u79cd\u9519\u8bef\uff0c\u5982\u4f55\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u9519\u8bef\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u5206\u6790\u4e86\u591a\u4e2a\u7ade\u4e89\u6027\u5de5\u5177\u8bc4\u4f30\u57fa\u51c6\u4e0a\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u9519\u8bef\u7c7b\u578b\uff0c\u5e76\u57fa\u4e8e\u65b0\u7684\u6570\u636e\u96c6\u6784\u5efa\u7b56\u7565\u63d0\u51fa\u4e86CRITICTOOL\u57fa\u51c6\u3002", "result": "\u9a8c\u8bc1\u4e86CRITICTOOL\u57fa\u51c6\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u4e0d\u540cLLMs\u7684\u5de5\u5177\u53cd\u5c04\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5de5\u5177\u5b66\u4e60\u7684\u65b0\u89c6\u89d2\uff0c\u5e76\u901a\u8fc7\u63d0\u4f9b\u7684\u4ee3\u7801\u4fc3\u8fdb\u4e86\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2506.13992", "pdf": "https://arxiv.org/pdf/2506.13992", "abs": "https://arxiv.org/abs/2506.13992", "authors": ["An Luo", "Xun Xian", "Jin Du", "Fangqiao Tian", "Ganghua Wang", "Ming Zhong", "Shengchun Zhao", "Xuan Bi", "Zirui Liu", "Jiawei Zhou", "Jayanth Srinivasa", "Ashish Kundu", "Charles Fleming", "Mingyi Hong", "Jie Ding"], "title": "AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME"], "comment": null, "summary": "Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.", "AI": {"tldr": "Large language models (LLMs) struggle with leveraging external domain knowledge in tabular prediction tasks, as shown by a new benchmark called AssistedDS. The study reveals that LLMs tend to uncritically adopt information, perform poorly with adversarial content, and make mistakes with time-series data and categorical variables.", "motivation": "To evaluate whether large language models can critically use external domain knowledge like human data scientists.", "method": "Introducing AssistedDS, which includes synthetic datasets and real-world Kaggle competitions with curated helpful and adversarial documents.", "result": "LLMs show poor performance when faced with adversarial content, helpful guidance is insufficient to offset the harm from adversarial information, and there are specific errors made in time-series data handling, feature engineering consistency, and categorical variable interpretation.", "conclusion": "There is a significant gap in LLMs' ability to critically assess and utilize expert knowledge, pointing to a critical area for future research in automated data science systems."}}
{"id": "2506.14086", "pdf": "https://arxiv.org/pdf/2506.14086", "abs": "https://arxiv.org/abs/2506.14086", "authors": ["Rahul Seetharaman", "Kaustubh D. Dhole", "Aman Bansal"], "title": "InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.", "AI": {"tldr": "InsertRank is an LLM-based reranker that uses lexical signals like BM25 scores to improve retrieval performance, demonstrating better results on both BRIGHT and R2MED benchmarks.", "motivation": "Users expect LLM-based chat interfaces to handle complex queries requiring reasoning over documents, but current LLM rerankers need improvement.", "method": "InsertRank uses BM25 scores during reranking to leverage lexical signals for better retrieval effectiveness.", "result": "InsertRank improves retrieval effectiveness on BRIGHT and R2MED benchmarks, and shows consistent improvement across different LLM families.", "conclusion": "InsertRank surpasses previous methods on BRIGHT and R2MED benchmarks with Deepseek-R1 model."}}
{"id": "2506.14104", "pdf": "https://arxiv.org/pdf/2506.14104", "abs": "https://arxiv.org/abs/2506.14104", "authors": ["RuiKun Yang", "ZhongLiang Wei", "Longdi Xian"], "title": "Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints", "categories": ["cs.GR", "cs.CL", "cs.CY"], "comment": null, "summary": "Yangliuqing woodblock prints, a cornerstone of China's intangible cultural heritage, are celebrated for their intricate designs and vibrant colors. However, preserving these traditional art forms while fostering innovation presents significant challenges. This study explores the DeepSeek + MidJourney approach to generating creative, themed Yangliuqing woodblock prints focused on the fight against COVID-19 and depicting joyous winners. Using Fr\u00e9chet Inception Distance (FID) scores for evaluation, the method that combined DeepSeek-generated thematic prompts, MidJourney-generated thematic images, original Yangliuqing prints, and DeepSeek-generated key prompts in MidJourney-generated outputs achieved the lowest mean FID score (150.2) with minimal variability (\u03c3 = 4.9). Additionally, feedback from 62 participants, collected via questionnaires, confirmed that this hybrid approach produced the most representative results. Moreover, the questionnaire data revealed that participants demonstrated the highest willingness to promote traditional culture and the strongest interest in consuming the AI-generated images produced through this method. These findings underscore the effectiveness of an innovative approach that seamlessly blends traditional artistic elements with modern AI-driven creativity, ensuring both cultural preservation and contemporary relevance.", "AI": {"tldr": "This study used AI methods to create new Yangliuqing prints about fighting COVID-19, achieving the best results in tests and user feedback.", "motivation": "Preserving traditional Yangliuqing woodblock prints while fostering innovation.", "method": "DeepSeek + MidJourney approach combining DeepSeek-generated thematic prompts, MidJourney-generated thematic images, original Yangliuqing prints, and DeepSeek-generated key prompts in MidJourney-generated outputs.", "result": "Achieved the lowest mean FID score (150.2) with minimal variability (\u03c3 = 4.9), and participants showed high willingness to promote traditional culture and consume the AI-generated images.", "conclusion": "The hybrid approach effectively blends traditional artistic elements with modern AI-driven creativity, ensuring cultural preservation and contemporary relevance."}}
{"id": "2506.14142", "pdf": "https://arxiv.org/pdf/2506.14142", "abs": "https://arxiv.org/abs/2506.14142", "authors": ["Wenting Chen", "Yi Dong", "Zhaojun Ding", "Yucheng Shi", "Yifan Zhou", "Fang Zeng", "Yijun Luo", "Tianyu Lin", "Yihang Su", "Yichen Wu", "Kai Zhang", "Zhen Xiang", "Tianming Liu", "Ninghao Liu", "Lichao Sun", "Yixuan Yuan", "Xiang Li"], "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology", "categories": ["cs.CV", "cs.CL"], "comment": "4 figures, 2 tables", "summary": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.", "AI": {"tldr": "RadFabric, a multi-agent, multimodal reasoning framework, improves chest X-ray interpretation by combining visual and textual analysis, achieving high accuracy in detecting challenging pathologies and overall diagnostic performance.", "motivation": "Current automated systems have limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning.", "method": "RadFabric uses specialized agents for pathology detection, anatomical mapping, and synthesis of visual, anatomical, and clinical data using large multimodal reasoning models.", "result": "RadFabric shows significant performance improvements, with near-perfect detection of challenging pathologies and higher overall diagnostic accuracy compared to traditional systems.", "conclusion": "RadFabric advances AI-driven radiology towards transparent, anatomically precise, and clinically actionable chest X-ray analysis."}}
{"id": "2506.14148", "pdf": "https://arxiv.org/pdf/2506.14148", "abs": "https://arxiv.org/abs/2506.14148", "authors": ["Long-Vu Hoang", "Tuan Nguyen", "Tran Huy Dat"], "title": "Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u6563\u5c04\u7684\u975e\u4fb5\u5165\u5f0f\u7269\u4f53\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5934\u53d1\u8bc4\u4f30\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5728\u4fdd\u62a4\u9690\u79c1\u3001\u975e\u63a5\u89e6\u5f0f\u5206\u7c7b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u8fbe\u5230\u63a5\u8fd190%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u63d0\u4f9b\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u975e\u4fb5\u5165\u5f0f\u7269\u4f53\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u7528\u4e8e\u5934\u53d1\u8bc4\u4f30\uff0c\u4ee5\u671f\u4f5c\u4e3a\u89c6\u89c9\u5206\u7c7b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528AI\u9a71\u52a8\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u58f0\u97f3\u5206\u7c7b\u65b9\u6cd5\uff0c\u5bf9\u5934\u5e26\u5934\u53d1\u6837\u672c\u7269\u4f53\u53d1\u51fa\u58f0\u6ce2\u523a\u6fc0\u5e76\u6355\u6349\u6563\u5c04\u4fe1\u53f7\uff0c\u4ee5\u6b64\u6765\u5206\u7c7b\u53d1\u8d28\u548c\u6c34\u5206\u3002\u540c\u65f6\u5bf9\u6bd4\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u5305\u62ec\u5b8c\u5168\u76d1\u7763\u7684\u6df1\u5ea6\u5b66\u4e60\u3001\u57fa\u4e8e\u5d4c\u5165\u7684\u5206\u7c7b\u3001\u6709\u76d1\u7763\u7684\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u4ee5\u53ca\u81ea\u76d1\u7763\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u901a\u8fc7\u4e0d\u540c\u65b9\u6cd5\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u6700\u4f73\u7b56\u7565\u5b9e\u73b0\u4e86\u8fd190%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u5fae\u8c03\u81ea\u76d1\u7763\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u8fd190%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8fd9\u8868\u660e\u58f0\u6563\u5c04\u662f\u4e00\u79cd\u4fdd\u62a4\u9690\u79c1\u3001\u975e\u63a5\u89e6\u5f0f\u7684\u89c6\u89c9\u5206\u7c7b\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u5404\u4e2a\u884c\u4e1a\u90fd\u6709\u5de8\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.14153", "pdf": "https://arxiv.org/pdf/2506.14153", "abs": "https://arxiv.org/abs/2506.14153", "authors": ["Tuan Dat Phuong", "Long-Vu Hoang", "Huy Dat Tran"], "title": "Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.", "AI": {"tldr": "This paper proposes replacing the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network for synthetic speech detection, showing a relative improvement of 60.55% on ASVspoof2021 LA and DF sets.", "motivation": "To address the challenges posed by spoofing attacks on automatic speaker verification systems and improve the performance of synthetic speech detection.", "method": "Replacing the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network.", "result": "The proposed method improved the performance by 60.55% relatively on LA and DF sets, achieving 0.70% EER on the 21LA set.", "conclusion": "Incorporating Kolmogorov-Arnold Network into SSL-based models is a promising direction for advances in synthetic speech detection."}}
{"id": "2506.14204", "pdf": "https://arxiv.org/pdf/2506.14204", "abs": "https://arxiv.org/abs/2506.14204", "authors": ["Aswin Shanmugam Subramanian", "Amit Das", "Naoyuki Kanda", "Jinyu Li", "Xiaofei Wang", "Yifan Gong"], "title": "Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions.", "AI": {"tldr": "This paper proposes methods to improve ASR systems' performance in both streaming and offline scenarios, focusing on overlapping speech and readability.", "motivation": "The motivation is to address the practical needs of both streaming and offline ASR applications, focusing on balancing latency and accuracy, and meeting real-time captioning and summarization requirements.", "method": "The method involves using Continuous Speech Separation (CSS) single-channel front-end with end-to-end systems, implementing dual models for streaming and offline ASR, and exploring segment-based SOT (segSOT).", "result": "The results show that the proposed methods improve the accuracy of ASR systems, especially in scenarios with overlapping speech, and enhance the readability of multi-talker transcriptions.", "conclusion": "The paper concludes that their proposed methods significantly improve the performance of both streaming and offline ASR systems, particularly in scenarios with overlapping speech."}}
{"id": "2506.14223", "pdf": "https://arxiv.org/pdf/2506.14223", "abs": "https://arxiv.org/abs/2506.14223", "authors": ["Anna Hamberger", "Sebastian Murgul", "Jochen Schmidt", "Michael Heizmann"], "title": "Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Accepted to the 50th International Computer Music Conference (ICMC), 2025", "summary": "Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription.", "AI": {"tldr": "This paper presents the Fretting-Transformer, a new model for automatically transcribing MIDI sequences into guitar tablature, which outperforms existing methods and commercial applications.", "motivation": "Automating the transcription of MIDI sequences into guitar tablature is challenging due to string-fret ambiguity and lack of playability information in symbolic music notations. This work aims to address these issues.", "method": "The Fretting-Transformer uses a T5 transformer architecture in an encoder-decoder model to frame the task as a symbolic translation problem, with enhanced features like context-sensitive processing and tuning/capo conditioning.", "result": "The model was tested on diverse datasets and showed better performance than baseline methods and commercial applications.", "conclusion": "The Fretting-Transformer provides a robust solution for automated guitar transcription, addressing key challenges in the field."}}
{"id": "2506.14245", "pdf": "https://arxiv.org/pdf/2506.14245", "abs": "https://arxiv.org/abs/2506.14245", "authors": ["Xumeng Wen", "Zihan Liu", "Shun Zheng", "Zhijian Xu", "Shengyu Ye", "Zhirong Wu", "Xiao Liang", "Yang Wang", "Junjie Li", "Ziming Miao", "Jiang Bian", "Mao Yang"], "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.", "AI": {"tldr": "This paper introduces a new evaluation metric, CoT-Pass@K, to better assess the reasoning capabilities of Reinforcement Learning with Verifiable Rewards (RLVR) tuned Large Language Models (LLMs). It demonstrates that RLVR can enhance reasoning diversity and generalization when evaluated with CoT-Pass@K.", "motivation": "To resolve the contradiction between the underperformance of RLVR-tuned models on the Pass@K metric and the hypothesis that RLVR merely re-weights existing reasoning paths.", "method": "Introducing CoT-Pass@K, a new evaluation metric that requires both correct reasoning paths and final answers, and providing a theoretical foundation for RLVR's unique structure.", "result": "Empirical results show that RLVR can incentivize the generalization of correct reasoning for all values of K, and the enhanced reasoning capability emerges early in the training process.", "conclusion": "This study provides a clearer understanding of RLVR's role, offers a more reliable evaluation method, and affirms its potential to genuinely improve machine reasoning."}}
{"id": "2506.14280", "pdf": "https://arxiv.org/pdf/2506.14280", "abs": "https://arxiv.org/abs/2506.14280", "authors": ["Bai Cong", "Nico Daheim", "Yuesong Shen", "Rio Yokota", "Mohammad Emtiyaz Khan", "Thomas M\u00f6llenhoff"], "title": "Improving LoRA with Variational Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "16 pages, 4 figures", "summary": "Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.", "AI": {"tldr": "This paper introduces a new method using IVON to improve LoRA finetuning, which outperforms previous Bayesian methods in terms of accuracy and calibration while maintaining low computational cost.", "motivation": "Fix the issues of Bayesian methods including marginal effect on other metrics, increased computational overheads, and requirement of additional tricks.", "method": "Using a recently proposed variational algorithm called IVON and a simple posterior pruning technique.", "result": "Improved accuracy by 1.3% and reduced ECE by 5.4% when finetuning a Llama-3.2-3B model on commonsense reasoning tasks compared to AdamW.", "conclusion": "Our results show that variational learning with IVON can effectively improve LoRA finetuning."}}
{"id": "2506.14574", "pdf": "https://arxiv.org/pdf/2506.14574", "abs": "https://arxiv.org/abs/2506.14574", "authors": ["Mingkang Zhu", "Xi Chen", "Zhongdao Wang", "Bei Yu", "Hengshuang Zhao", "Jiaya Jia"], "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.", "AI": {"tldr": "This paper presents a novel approach to incorporate token-level reward guidance into Direct Preference Optimization (DPO) for improving the alignment of large language models.", "motivation": "Token-level reward models can enhance the performance of reinforcement learning but are hard to use in DPO due to its sequence-level nature.", "method": "The authors decompose the sequence-level PPO into token-level problems, derive closed-form solutions for token-level policy and reward, and establish a computable loss function framework for DPO.", "result": "The proposed method improves DPO performance significantly, achieving win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard.", "conclusion": "This work provides a new way to utilize token-level reward guidance in DPO, leading to enhanced performance in aligning large language models."}}
{"id": "2506.14602", "pdf": "https://arxiv.org/pdf/2506.14602", "abs": "https://arxiv.org/abs/2506.14602", "authors": ["Haoyang Gui", "Thales Bertaglia", "Catalina Goanta", "Gerasimos Spanakis"], "title": "Computational Studies in Influencer Marketing: A Systematic Literature Review", "categories": ["cs.CY", "cs.CL"], "comment": "journal submission, under review", "summary": "Influencer marketing has become a crucial feature of digital marketing strategies. Despite its rapid growth and algorithmic relevance, the field of computational studies in influencer marketing remains fragmented, especially with limited systematic reviews covering the computational methodologies employed. This makes overarching scientific measurements in the influencer economy very scarce, to the detriment of interested stakeholders outside of platforms themselves, such as regulators, but also researchers from other fields. This paper aims to provide an overview of the state of the art of computational studies in influencer marketing by conducting a systematic literature review (SLR) based on the PRISMA model. The paper analyses 69 studies to identify key research themes, methodologies, and future directions in this research field. The review identifies four major research themes: Influencer identification and characterisation, Advertising strategies and engagement, Sponsored content analysis and discovery, and Fairness. Methodologically, the studies are categorised into machine learning-based techniques (e.g., classification, clustering) and non-machine-learning-based techniques (e.g., statistical analysis, network analysis). Key findings reveal a strong focus on optimising commercial outcomes, with limited attention to regulatory compliance and ethical considerations. The review highlights the need for more nuanced computational research that incorporates contextual factors such as language, platform, and industry type, as well as improved model explainability and dataset reproducibility. The paper concludes by proposing a multidisciplinary research agenda that emphasises the need for further links to regulation and compliance technology, finer granularity in analysis, and the development of standardised datasets.", "AI": {"tldr": "This systematic literature review analyzes 69 studies to outline the state of computational studies in influencer marketing, identifying key themes and methodologies while highlighting the need for more nuanced research incorporating contextual factors.", "motivation": "The rapid growth and algorithmic relevance of influencer marketing have made it a crucial part of digital marketing strategies. However, the computational studies in this area are fragmented and lack systematic reviews, leading to scarcity in overarching scientific measurements. This is detrimental to stakeholders like regulators and researchers from other fields.", "method": "A systematic literature review (SLR) based on the PRISMA model was conducted to analyse 69 studies.", "result": "Four major research themes were identified: Influencer identification and characterisation, Advertising strategies and engagement, Sponsored content analysis and discovery, and Fairness. Studies were categorised into machine learning-based and non-machine learning-based techniques.", "conclusion": "The paper proposes a multidisciplinary research agenda emphasizing links to regulation and compliance technology, finer granularity in analysis, and the development of standardised datasets."}}
{"id": "2506.14629", "pdf": "https://arxiv.org/pdf/2506.14629", "abs": "https://arxiv.org/abs/2506.14629", "authors": ["Md. Adnanul Islam", "Md. Faiyaz Abdullah Sayeedi", "Md. Asaduzzaman Shuvo", "Muhammad Ziaur Rahman", "Shahanur Rahman Bappy", "Raiyan Rahman", "Swakkhar Shatabda"], "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme \"Prevention is Better than Cure\", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito", "AI": {"tldr": "VisText-Mosquito is a multimodal dataset for mosquito breeding site analysis that combines visual and textual data. It includes annotated images for object detection and segmentation, as well as reasoning texts. The YOLOv9s model performed best for object detection, and YOLOv11n-Seg for segmentation. A fine-tuned BLIP model was used for reasoning generation.", "motivation": "To support automated detection, segmentation, and reasoning for mosquito breeding site analysis to prevent mosquito-borne diseases.", "method": "Creating a multimodal dataset (VisText-Mosquito) with annotated images and natural language reasoning texts, and using specific models like YOLOv9s for object detection, YOLOv11n-Seg for segmentation, and a fine-tuned BLIP model for reasoning generation.", "result": "The YOLOv9s model achieved the highest precision (0.92926) and mAP@50 (0.92891) for object detection. YOLOv11n-Seg reached a segmentation precision of 0.91587 and mAP@50 of 0.79795. The fine-tuned BLIP model had a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87.", "conclusion": "This work demonstrates the potential of AI-based detection systems in preventing mosquito-borne diseases by proactively identifying and analyzing breeding sites."}}
{"id": "2506.14755", "pdf": "https://arxiv.org/pdf/2506.14755", "abs": "https://arxiv.org/abs/2506.14755", "authors": ["Zhengxiang Cheng", "Dongping Chen", "Mingyang Fu", "Tianyi Zhou"], "title": "Optimizing Length Compression in Large Reasoning Models", "categories": ["cs.AI", "cs.CL"], "comment": "16 pages, 7 figures, 4 tables", "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as \"invalid thinking\" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.", "AI": {"tldr": "Propose a post-training method LC-R1 based on Group Relative Policy Optimization (GRPO) to reduce unnecessary and verbose reasoning chains in Large Reasoning Models (LRMs), which achieves a significant reduction in sequence length with only a slight drop in accuracy.", "motivation": "To solve the problem of 'invalid thinking' in LRMs, where models tend to repeatedly double-check their work after deriving the correct answer, leading to inefficiency.", "method": "Introduce two fine-grained principles: Brevity and Sufficiency. Propose LC-R1, a post-training method using a novel combination of Length Reward and Compress Reward.", "result": "LC-R1 reduces sequence length by about 50% with only a 2% drop in accuracy on multiple reasoning benchmarks, achieving a favorable trade-off point on the Pareto frontier.", "conclusion": "The proposed LC-R1 method demonstrates the potential for developing more powerful and computationally efficient LRMs."}}
{"id": "2506.14766", "pdf": "https://arxiv.org/pdf/2506.14766", "abs": "https://arxiv.org/abs/2506.14766", "authors": ["Yujun Wang", "Jinhe Bi", "Yunpu Ma", "Soeren Pirk"], "title": "ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM", "categories": ["cs.CV", "cs.CL"], "comment": "15 pages, 7 figures", "summary": "Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.", "AI": {"tldr": "This paper introduces an attention-steerable contrastive decoding framework to mitigate hallucinations in MLLMs, showing improved performance across various benchmarks.", "motivation": "The paper aims to address the problem of hallucinations in Multimodal Large Language Models (MLLM), which is caused by over-reliance on partial cues leading to incorrect responses.", "method": "The paper proposes an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model.", "result": "The proposed method significantly reduces hallucinations and improves performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while also enhancing performance on standard VQA benchmarks.", "conclusion": "This study demonstrates that the effectiveness of methods like VCD and ICD may not just come from surface-level modifications to logits but from deeper shifts in attention distribution."}}

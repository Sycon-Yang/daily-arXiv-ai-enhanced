<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 109]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.CR](#cs.CR) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 30]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SE](#cs.SE) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [2] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的文本图推理框架Graph-$S^3$，通过合成逐步监督训练的LLM检索器，解决了现有检索器性能不佳的问题。实验结果表明，该方法在准确率和F1分数上分别提高了8.1%和9.7%。


<details>
  <summary>Details</summary>
Motivation: 现有检索器在性能上表现不佳，因为它们要么依赖于浅层嵌入相似性，要么采用需要大量数据标记和训练成本的交互式检索策略。

Method: 我们提出了Graph-$S^3$，一个基于LLM的代理文本图推理框架，该框架使用合成逐步监督训练的LLM检索器。我们提出了一种数据合成管道来提取黄金子图以生成奖励，并采用两阶段训练方案来学习基于合成奖励的交互式图探索策略。

Result: 我们的方法在准确率和F1分数上分别提高了8.1%和9.7%。在更复杂的多跳推理任务中，优势甚至更大。

Conclusion: 我们的方法在三个常见数据集上的实验表明，与七个强基线相比，在准确率和F1分数上分别提高了8.1%和9.7%。在更复杂的多跳推理任务中，优势甚至更大。

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [3] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型在完成日常任务时展示的隐含价值观，并发现它们与人类和其他模型存在差异。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型在完成主观日常任务时展示的隐含价值观，以及它们与人类的比较。

Method: 通过审计六种流行的大型语言模型完成30个日常任务，将模型与彼此以及100名来自美国的人类众包工作者进行比较。

Result: 大型语言模型在完成日常任务时表现出的隐含价值观往往与人类和其他模型不一致。

Conclusion: 研究发现大型语言模型在完成日常任务时表现出的隐含价值观与人类和其他模型不一致。

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [4] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: CSAR is a greedy algorithm for inducing morphemes from emergent language corpora, validated on various datasets and showing effectiveness in making reasonable predictions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to introduce an algorithm for inducing morphemes from emergent language corpora of parallel utterances and meanings.

Method: CSAR is a greedy algorithm that weights morphemes based on mutual information between forms and meanings, selects the highest-weighted pair, removes it from the corpus, and repeats the process.

Result: CSAR was validated on procedurally generated datasets, human language data, and a handful of emergent languages, showing its effectiveness in inducing morphemes and making reasonable predictions.

Conclusion: CSAR is effective in inducing morphemes from emergent language corpora and can make reasonable predictions in adjacent domains.

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [5] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: Omni-Embed-Nemotron 是一种多模态检索模型，支持文本、图像、音频和视频的跨模态和联合模态检索，提升了现实世界信息检索的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的检索器难以处理视觉和语义丰富的现实文档内容，如PDF、幻灯片或视频。因此，需要一种更强大的多模态检索模型来应对这一挑战。

Method: Omni-Embed-Nemotron 基于多模态模型的能力，扩展了检索范围，支持文本、图像、音频和视频模态，并通过单个模型实现跨模态和联合模态检索。

Result: Omni-Embed-Nemotron 在文本、图像和视频检索任务中表现良好，证明了其在多模态检索中的有效性。

Conclusion: Omni-Embed-Nemotron 是一个统一的多模态检索嵌入模型，能够处理现实世界信息需求的复杂性，并在文本、图像和视频检索中表现出色。

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [6] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 本文通过超参数优化和XferBench目标函数，生成了与人类语言相似度更高的涌现语言，并验证了熵在预测迁移学习性能中的作用。


<details>
  <summary>Details</summary>
Motivation: 本文旨在生成与人类语言相似度更高的涌现语言，以提高其在实际应用中的表现和可解释性。

Method: 本文使用XferBench作为目标函数进行超参数优化，以生成与人类语言相似度最高的涌现语言。XferBench通过测量涌现语言在深度迁移学习中对人类语言的适用性来量化其统计相似性。

Result: 本文通过超参数优化，成功生成了与人类语言相似度更高的涌现语言。同时，我们发现熵可以有效预测涌现语言的迁移学习性能，并验证了之前关于熵最小化性质的结果。

Conclusion: 本文通过设计基于信号博弈的涌现通信环境，生成了与人类语言相似度最高的涌现语言。此外，我们还展示了熵在预测涌现语言迁移学习性能方面的有效性，并验证了之前关于涌现通信系统熵最小化性质的结果。最后，我们报告了哪些超参数能够产生更真实的涌现语言，即那些能更好地转移到人类语言的模型。

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [7] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: SEER是一个用于测试大型语言模型（LLMs）识别表达情绪的具体文本跨度的能力的基准。它包含两个任务：在单句内识别情绪证据，以及在五个连续句子的短文中识别证据。我们评估了14个开源LLMs，并发现虽然一些模型在单句输入上接近平均人类表现，但它们在更长的段落中的准确性下降。我们的错误分析揭示了关键的失败模式，包括对情绪关键词的过度依赖和中性文本中的误报。


<details>
  <summary>Details</summary>
Motivation: 传统的情绪识别任务通常为整个句子分配一个单一标签，而SEER针对的是情绪证据检测这一研究不足的任务，即确定哪些确切短语传达情绪。这种基于跨度的方法对于需要知道情绪如何表达的应用程序（如共情对话和临床支持）至关重要。

Method: SEER包含两个任务：在单句内识别情绪证据，以及在五个连续句子的短文中识别证据。它包含了对1200个真实句子的情绪和情绪证据的新注释。我们评估了14个开源LLMs，并进行了错误分析。

Result: 我们评估了14个开源LLMs，并发现虽然一些模型在单句输入上接近平均人类表现，但它们在更长的段落中的准确性下降。我们的错误分析揭示了关键的失败模式，包括对情绪关键词的过度依赖和中性文本中的误报。

Conclusion: SEER是一个用于测试大型语言模型（LLMs）识别表达情绪的具体文本跨度的能力的基准。虽然一些模型在单句输入上接近平均人类表现，但在更长的段落中准确性下降。我们的错误分析揭示了关键的失败模式，包括对情绪关键词的过度依赖和中性文本中的误报。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [8] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: ALHD is a large-scale Arabic dataset for detecting LLM-generated texts. It enables studying generalizability and identifies challenges in cross-genre settings. Fine-tuned BERT models perform well, but there are gaps in generalization, particularly for news articles.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for a comprehensive Arabic dataset for detecting LLM-generated texts, which can help mitigate risks such as misinformation, academic dishonesty, and cyber threats.

Method: The paper introduces ALHD, a large-scale comprehensive Arabic dataset designed to distinguish between human- and LLM-generated texts. It includes preprocessing, annotations, and standardized splits to support reproducibility. Benchmark experiments are conducted using traditional classifiers, BERT-based models, and LLMs.

Result: Fine-tuned BERT models achieve competitive performance, outperforming LLM-based models. However, challenges in generalizing across genres are observed, especially with news articles where LLM-generated texts resemble human texts in style.

Conclusion: ALHD establishes a foundation for research related to Arabic LLM-detection and mitigating risks of misinformation, academic dishonesty, and cyber threats.

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [9] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为TS-Reasoner的新方法，通过将时间序列基础模型与大型语言模型对齐，提高了时间序列推理的能力。该方法在多个基准测试中表现出色，且具有较高的数据效率。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理对于决策至关重要，但现有的时间序列基础模型（TSFMs）缺乏进一步分析所需的背景知识和复杂推理能力，而这些可以通过大型语言模型（LLMs）实现。然而，LLMs在没有昂贵的后训练的情况下通常难以理解时间序列数据。因此，需要一种有效的训练方法来对齐两种模态以进行推理任务。

Method: 我们提出了TS-Reasoner，它将时间序列基础模型（TSFMs）的潜在表示与大型语言模型（LLMs）的文本输入对齐，用于下游理解和推理任务。我们提出了一种简单而有效的方法来整理多样化的合成时间序列和文本标题对进行对齐训练，并开发了一种两阶段的训练方案，在对齐预训练后应用指令微调。

Result: TS-Reasoner在多个基准测试中表现出色，优于各种主流的LLM、VLM和时间序列LLM，并且在数据效率方面表现突出。

Conclusion: TS-Reasoner在多个基准测试中不仅优于各种主流的LLM、VLM和时间序列LLM，而且以显著的数据效率实现了这一点，例如使用不到一半的训练数据。

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [10] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 本文提出了一种基于RAG的同辈意识比较推理层，以解决RAG在专业推理任务中输出过于通用的问题，并在文本生成指标上取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 在专业领域，人类通常将新问题与类似示例进行比较，突出细微差别并得出结论，而不是孤立地分析信息。然而，RAG在专业推理任务中的输出往往过于通用，未能提供特定上下文的见解。

Method: 我们提出了一种基于RAG的同辈意识比较推理层，以解决RAG在专业推理任务中输出过于通用的问题。

Result: 我们的对比方法在文本生成指标如ROUGE和BERTScore上优于基线RAG，并且在金融领域产生了更具体的见解。

Conclusion: 我们的对比方法在文本生成指标上优于基线RAG，如ROUGE和BERTScore，与人类生成的股权研究和风险相比。

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [11] [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
*Sayan Ghosh,Shahzaib Saqib Warraich,Dhruv Tarsadiya,Gregory Yauney,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 本文介绍了Consensus Graphs (ConGrs)，一种用于捕捉语言模型响应变化并利用这些变化合成更有效响应的灵活方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法高效地在不同长格式响应中合成丰富的认识信号。

Method: 引入了Consensus Graphs (ConGrs)，这是一种基于DAG的数据结构，用于表示同一提示下采样语言模型响应中的共享信息和语义变化。使用轻量级的词法序列对齐算法和一个次级语言模型判断器构建ConGrs，并设计任务依赖的解码方法从ConGrs数据结构中合成最终响应。

Result: 在两个传记生成任务中，从ConGrs合成响应可将事实精度提高最多31%，相比其他方法减少超过80%对语言模型判断器的依赖。在三个拒绝任务中，弃权率提高了最多56%。在MATH和AIME推理任务中，准确率比自验证和多数投票基线提高了最多6个百分点。

Conclusion: ConGrs提供了一种灵活的方法，可以捕捉语言模型响应的变化，并利用响应变化提供的认识信号来合成更有效的响应。

Abstract: Language models can be sampled multiple times to access the distribution
underlying their responses, but existing methods cannot efficiently synthesize
rich epistemic signals across different long-form responses. We introduce
Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents
shared information, as well as semantic variation in a set of sampled LM
responses to the same prompt. We construct ConGrs using a light-weight lexical
sequence alignment algorithm from bioinformatics, supplemented by the targeted
usage of a secondary LM judge. Further, we design task-dependent decoding
methods to synthesize a single, final response from our ConGr data structure.
Our experiments show that synthesizing responses from ConGrs improves factual
precision on two biography generation tasks by up to 31% over an average
response and reduces reliance on LM judges by more than 80% compared to other
methods. We also use ConGrs for three refusal-based tasks requiring abstention
on unanswerable queries and find that abstention rate is increased by up to
56%. We apply our approach to the MATH and AIME reasoning tasks and find an
improvement over self-verification and majority vote baselines by up to 6
points of accuracy. We show that ConGrs provide a flexible method for capturing
variation in LM responses and using the epistemic signals provided by response
variation to synthesize more effective responses.

</details>


### [12] [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
*Ahmed Alajrami,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文研究了在指令微调中引入扰动对大型语言模型性能的影响，发现这可以提高模型对噪声指令的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是探索在指令微调数据中引入扰动是否可以增强大型语言模型对噪声指令的抵抗能力。

Method: 本文研究了在指令微调数据中引入扰动（如删除停用词或打乱词语）对大型语言模型性能的影响，并评估了学习动态和模型行为的变化。

Result: 实验结果表明，在扰动指令上进行指令微调可以在某些情况下提高下游任务的性能。

Conclusion: 本文的结论是，在指令微调中引入扰动可以提高大型语言模型对噪声指令的抵抗力，这表明在指令微调中包含扰动指令的重要性。

Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities
of large language models (LLMs), improving their usability in generating
helpful responses on various tasks. However, previous work has demonstrated
that they are sensitive to minor variations in instruction phrasing. In this
paper, we explore whether introducing perturbations in instruction-tuning data
can enhance LLMs' resistance against noisy instructions. We focus on how
instruction-tuning with perturbations, such as removing stop words or shuffling
words, affects LLMs' performance on the original and perturbed versions of
widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics
and potential shifts in model behavior. Surprisingly, our results suggest that
instruction-tuning on perturbed instructions can, in some cases, improve
downstream performance. These findings highlight the importance of including
perturbed instructions in instruction-tuning, which can make LLMs more
resilient to noisy user inputs.

</details>


### [13] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: 本文提出了一种名为TriMediQ的三元组结构方法，通过将患者回答转化为结构化的三元组图，提高了多轮医疗问答中的临床推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在静态和单次交互的医学问答（QA）基准测试中表现强劲，但这种设置与实际临床咨询中所需的迭代信息收集过程不同。MEDIQ框架通过将诊断重新定义为患者和专家系统之间的互动对话来解决这一不匹配，但当LLMs被迫与对话日志进行推理时，其可靠性会大幅下降，其中临床事实出现在句子中而没有明确的联系。为了弥合这一差距，我们引入了TriMediQ，一种三元组结构的方法，将患者回答总结为三元组并将其集成到知识图谱（KG）中，以实现多跳推理。

Method: 我们引入了TriMediQ，这是一种三元组结构的方法，将患者回答总结为三元组，并将其集成到知识图谱（KG）中，以实现多跳推理。我们引入了一个冻结的三元组生成器，使用设计用于确保事实一致性的提示来提取临床相关的三元组。同时，一个包含图编码器和投影器的可训练投影模块从KG中捕获关系信息以增强专家推理。TriMediQ分为两个步骤：(i) 使用所有LLM权重冻结进行投影模块微调；(ii) 在推理期间使用微调后的模块引导多跳推理。

Result: 我们在两个交互式QA基准测试上评估了TriMediQ，结果显示它在iMedQA数据集上比五个基线高出高达10.4%的准确率。

Conclusion: 这些结果表明，将患者回答转换为基于三元组的图可以提高多轮设置中的临床推理准确性，为基于LLM的医疗助理的部署提供了解决方案。

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [14] [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
*Andrew Halterman,Katherine A. Keith*

Main category: cs.CL

TL;DR: 本文探讨了在使用生成式大语言模型进行文本分类时，概念化步骤的重要性，并指出忽略该步骤可能导致偏差，且无法通过提高模型准确度或事后校正来解决。


<details>
  <summary>Details</summary>
Motivation: 在许多大语言模型时代的计算社会科学研究中，关注点集中在大语言模型提示之前和之后的步骤——即要分类的概念的概念化以及在后续统计推断中使用大语言模型预测——这些步骤被忽视了。

Method: 通过模拟，我们展示了这种由概念化引起的偏差无法仅通过提高大语言模型的准确性或事后偏差校正方法来纠正。

Result: 我们展示了由概念化引起的偏差无法仅通过提高大语言模型的准确性或事后偏差校正方法来纠正。

Conclusion: 我们提醒计算社会科学研究者，在大语言模型时代，概念化仍然是首要问题，并提供了如何实现低成本、无偏、低方差的下游估计的具体建议。

Abstract: Generative large language models (LLMs) are now used extensively for text
classification in computational social science (CSS). In this work, focus on
the steps before and after LLM prompting -- conceptualization of concepts to be
classified and using LLM predictions in downstream statistical inference --
which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can
tempt analysts to skip the conceptualization step, creating conceptualization
errors that bias downstream estimates. Using simulations, we show that this
conceptualization-induced bias cannot be corrected for solely by increasing LLM
accuracy or post-hoc bias correction methods. We conclude by reminding CSS
analysts that conceptualization is still a first-order concern in the LLM-era
and provide concrete advice on how to pursue low-cost, unbiased, low-variance
downstream estimates.

</details>


### [15] [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
*Hasibur Rahman,Hanan Salam*

Main category: cs.CL

TL;DR: 本文介绍了CCD-Bench，一个评估LLM在跨文化价值观冲突下决策的基准测试。研究发现，模型倾向于北欧和日耳曼欧洲的文化偏好，而其他地区则被低估。研究强调了当前对齐策略在处理多样世界观方面的不足，并呼吁更全面的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要针对文化知识（CulturalBench）、价值预测（WorldValuesBench）或单轴偏差诊断（CDEval）；没有评估LLM如何在多个文化根基的价值观直接冲突时进行裁决。

Method: CCD-Bench是一个基准测试，用于评估在跨文化价值观冲突下的LLM决策。它包括2,182个开放性困境，涵盖七个领域，每个困境都与十个匿名响应选项相对应，这些选项对应于十个GLOBE文化集群。这些困境使用分层拉丁方设计来减轻顺序效应。

Result: 模型不成比例地偏好北欧欧洲（平均20.2%）和日耳曼欧洲（12.4%），而东欧和中东及北非的选项则被低估（5.6至5.8%）。尽管87.9%的推理参考了多个GLOBE维度，但这种多元主义是表面的：模型重新组合未来取向和绩效取向，很少以果断或性别平等为基础（均低于3%）。顺序效应可以忽略不计（Cramer's V小于0.10），对称化KL散度显示按开发者血统聚类而非地理因素。

Conclusion: 当前对齐管道促进了一种共识导向的世界观，这在需要权力协商、基于权利的推理或性别意识分析的情景中表现不足。CCD-Bench将评估从单独的偏见检测转向多元决策，并突显了需要实质性参与多样世界观的对齐策略的必要性。

Abstract: Although large language models (LLMs) are increasingly implicated in
interpersonal and societal decision-making, their ability to navigate explicit
conflicts between legitimately different cultural value systems remains largely
unexamined. Existing benchmarks predominantly target cultural knowledge
(CulturalBench), value prediction (WorldValuesBench), or single-axis bias
diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple
culturally grounded values directly clash. We address this gap with CCD-Bench,
a benchmark that assesses LLM decision-making under cross-cultural value
conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,
each paired with ten anonymized response options corresponding to the ten GLOBE
cultural clusters. These dilemmas are presented using a stratified Latin square
to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models
disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe
(12.4 percent), while options for Eastern Europe and the Middle East and North
Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of
rationales reference multiple GLOBE dimensions, this pluralism is superficial:
models recombine Future Orientation and Performance Orientation, and rarely
ground choices in Assertiveness or Gender Egalitarianism (both under 3
percent). Ordering effects are negligible (Cramer's V less than 0.10), and
symmetrized KL divergence shows clustering by developer lineage rather than
geography. These patterns suggest that current alignment pipelines promote a
consensus-oriented worldview that underserves scenarios demanding power
negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts
evaluation beyond isolated bias detection toward pluralistic decision making
and highlights the need for alignment strategies that substantively engage
diverse worldviews.

</details>


### [16] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome the limitations of the Transformer in conversational AI by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system, which reduces the total user-facing cost of a conversation from quadratic to linear.


<details>
  <summary>Details</summary>
Motivation: The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity ($O(L^2)$) with respect to sequence length $L$.

Method: RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction.

Result: We validated our architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size.

Conclusion: RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations.

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [17] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 本文参与EvalLLM 2025挑战，提出了三种方法用于法语生物医学命名实体识别和健康事件抽取，并展示了GPT-4.1在少样本设置中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 参与EvalLLM 2025挑战，针对法语生物医学命名实体识别和健康事件抽取的少样本设置。

Method: 提出三种结合大型语言模型、注释指南、合成数据和后处理的方法进行NER，以及使用相同的ICL策略进行事件抽取。

Result: GPT-4.1在NER中取得61.53%的宏F1分数，在事件抽取中取得15.02%的分数。

Conclusion: 结果表明，GPT-4.1在NER和事件抽取中表现最佳，强调了精心设计提示在极低资源场景中最大化性能的重要性。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [18] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: Deco-G 是一种解码框架，通过显式分离格式遵循和任务解决，提高了模型在复杂提示下的性能，并在多种任务中取得了显著的提升。


<details>
  <summary>Details</summary>
Motivation: 当提示变得复杂时，模型常常难以遵循所有指令，特别是在指令提示交织了推理指令和严格的格式要求时。这种纠缠导致模型面临竞争目标，因此需要更明确的分离以提高性能。

Method: Deco-G 是一种解码框架，它使用一个单独的可处理概率模型（TPM）来处理格式合规性，同时仅用任务指令提示LLM。在每个解码步骤中，Deco-G 结合LLM的下一个标记概率和TPM计算的格式合规性似然来形成输出概率。

Result: Deco-G 在数学推理、LLM-as-a-judge 和事件参数提取等任务中表现出色，相比常规提示方法实现了1.0%到6.0%的相对增益，并保证了格式合规性。

Conclusion: Deco-G 通过显式分离格式遵循和任务解决，提高了模型在复杂提示下的性能，并在各种任务中实现了1.0%到6.0%的相对增益。

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [19] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在复杂推理任务中的表现，发现它们在处理需要从非结构化文本中推断结构化知识的任务时存在显著限制。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准通常依赖于简单的'针在干草堆中'检索或延续任务，这些任务可能无法准确反映模型在信息密集场景中的表现。因此，我们主张对需要复杂推理的任务进行评估。

Method: 我们评估了大型语言模型在需要从文本中推断结构化关系知识的任务上的表现，例如从可能嘈杂的自然语言内容中生成图。

Result: 我们的发现表明，当被要求进行这种形式的关系推理时，大型语言模型在较短的有效长度下就开始表现出记忆漂移和上下文遗忘，这比现有基准所表明的情况要早。

Conclusion: 我们的研究结果表明，大型语言模型在从非结构化输入中抽象结构化知识方面存在显著限制，并强调了需要进行架构调整以改善长距离推理。

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [20] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: 本文提出了一种基于掩码语言模型的音节级无监督语音识别框架，避免了对G2P的需求，并在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于音素的方法通常依赖于如G2P等昂贵的资源，并且在具有模糊音素边界的语言中难以推广。因此，需要一种更有效和稳定的无监督语音识别方法。

Method: 本文引入了一种基于掩码语言模型的音节级无监督语音识别框架，以解决现有方法依赖于昂贵资源和训练不稳定的问题。

Result: 本文的方法在LibriSpeech上实现了字符错误率（CER）40%的相对减少，并在普通话上表现出良好的泛化能力。

Conclusion: 本文提出了一种基于掩码语言模型的音节级无监督语音识别框架，避免了对G2P的需求和GAN方法的不稳定性，并在LibriSpeech上实现了字符错误率（CER）40%的相对减少，同时在普通话上表现出良好的泛化能力。

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [21] [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
*Xiangyu Peng,Cab Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 本文介绍了UniDoc-Bench，这是第一个大规模、现实的MM-RAG基准测试，基于8个领域的70k真实PDF页面。实验结果表明，多模态文本-图像融合RAG系统优于单模态和联合多模态嵌入检索，揭示了视觉上下文如何补充文本证据，并发现了系统性的失败模式，为开发更稳健的MM-RAG管道提供了可行的指导。


<details>
  <summary>Details</summary>
Motivation: 当前的评估是零散的，只关注文本或图像中的某一方面，或者在简化的多模态设置中，未能捕捉到以文档为中心的多模态使用案例。因此，需要一个全面的基准来评估多模态检索增强生成（MM-RAG）系统。

Method: 本文引入了UniDoc-Bench，这是第一个大规模、现实的MM-RAG基准测试，基于8个领域的70k真实PDF页面。我们的流程从文本、表格和图表中提取并链接证据，然后生成1,600个多模态QA对，涵盖事实检索、比较、总结和逻辑推理查询。为了确保可靠性，20%的QA对由多个标注者和专家仲裁验证。UniDoc-Bench支持四种范式之间的直接比较：(1) 仅文本，(2) 仅图像，(3) 多模态文本-图像融合，(4) 多模态联合检索，在统一协议下进行标准化候选池、提示和评估指标。

Result: 实验结果显示，多模态文本-图像融合RAG系统始终优于单模态和联合多模态嵌入检索，这表明单独的文本或图像都不足以满足需求，且当前的多模态嵌入仍然不足。此外，分析揭示了视觉上下文如何补充文本证据，并发现了系统性的失败模式，为开发更稳健的MM-RAG管道提供了可行的指导。

Conclusion: 实验结果表明，多模态文本-图像融合RAG系统始终优于单模态和联合多模态嵌入检索，这表明单独的文本或图像都不足以满足需求，且当前的多模态嵌入仍然不足。此外，分析揭示了视觉上下文如何补充文本证据，并发现了系统性的失败模式，为开发更稳健的MM-RAG管道提供了可行的指导。

Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

</details>


### [22] [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
*Nisar Hussain,Amna Qasim,Gull Mehak,Muhammad Zain,Momina Hafeez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本研究提出了一种基于QLoRA的微调框架，用于改善罗马乌尔都语-英语代码混合文本中的仇恨语言检测。通过翻译数据集并利用英语大语言模型，取得了优异的分类性能，展示了QLoRA在低资源环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于代码混合语言（如罗马乌尔都语）中使用贬义词给自然语言处理系统带来了挑战，包括未明说的语法、不一致的拼写和标记数据稀缺。因此，需要一种有效的解决方案来改善这一领域的检测效果。

Method: 本研究提出了一个基于QLoRA的微调框架，以提高罗马乌尔都语-英语文本中的仇恨语言检测效果。通过使用Google Translate将数据集翻译成英语，利用英语大语言模型进行分类性能优化。

Result: 在所有测试模型中，Meta LLaMA 3 8B取得了最高的F1分数91.45，其次是Mistral 7B的89.66，超过了传统Transformer基线。

Conclusion: 本研究展示了QLoRA在低资源环境下的高效性，特别是在代码混合的仇恨语言检测中。同时确认了大型语言模型在此任务中的潜力，并为未来的多语言仇恨检测系统奠定了基础。

Abstract: The use of derogatory terms in languages that employ code mixing, such as
Roman Urdu, presents challenges for Natural Language Processing systems due to
unstated grammar, inconsistent spelling, and a scarcity of labeled data. In
this work, we propose a QLoRA based fine tuning framework to improve offensive
language detection in Roman Urdu-English text. We translated the Roman
Urdu-English code mixed dataset into English using Google Translate to leverage
English LLMs, while acknowledging that this translation reduces direct
engagement with code mixing features. Our focus is on classification
performance using English translated low resource inputs. We fine tuned several
transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B
v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient
adaptation. Models were trained and evaluated on a manually annotated Roman
Urdu dataset for offensive vs non offensive content. Of all tested models, the
highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral
7B at 89.66, surpassing traditional transformer baselines. These results
demonstrate the efficacy of QLoRA in fine tuning high performing models for low
resource environments such as code mixed offensive language detection, and
confirm the potential of LLMs for this task. This work advances a scalable
approach to Roman Urdu moderation and paves the way for future multilingual
offensive detection systems based on LLMs.

</details>


### [23] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为MedReflect的框架，通过自我反思和自我改进的方式，使大型语言模型在医学问题解决中表现出色，减少了对外部资源的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在医学领域中存在检索开销大、标注成本高以及过度依赖外部助手的问题。因此，需要一种更高效、更少依赖外部资源的方法来提高医学问题解决能力。

Method: 本文引入了MedReflect，这是一个通用框架，旨在通过模拟医生的反思思维模式来激发大型语言模型的能力。MedReflect生成一个单次反射链，包括初始假设生成、自我提问、自我回答和决策优化。

Result: 通过仅使用2000个随机采样的训练示例和轻量级微调，该方法在多个医学基准测试中实现了显著的绝对准确率提升，同时减少了标注需求。

Conclusion: 本文证明了大型语言模型可以通过自我反思和自我改进来学习解决专门的医学问题，从而减少对外部监督和大量特定任务微调数据的依赖。

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [24] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 本文提出了一种新的示例选择方法TreePrompt，结合K-NN和AFSP，在两个语言对上提高了翻译性能。


<details>
  <summary>Details</summary>
Motivation: 现有示例选择方法主要关注查询与示例的相似性，而未考虑示例的质量。

Method: 提出了一种新的示例选择方法TreePrompt，它在树状框架内学习LLM偏好以识别高质量、上下文相关的示例，并将其与K-NN和AFSP结合使用。

Result: 在两个语言对（英语-波斯语和英语-德语）上的评估显示，集成TreePrompt与AFSP或随机选择可以提高翻译性能。

Conclusion: 集成TreePrompt与AFSP或随机选择可以提高翻译性能。

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [25] [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
*Ilias Tougui,Mehdi Zakroum,Mounir Ghogho*

Main category: cs.CL

TL;DR: 研究提出了一种基于语音的帕金森病检测方法，通过分析不同粒度级别（音素、音节、单词）的语音特征，提高了跨语言检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的语音检测系统分析整个话语，可能忽略了特定语音元素的诊断价值，因此需要一种更细致的方法来提高检测性能。

Method: 开发了一种粒度感知的方法，使用自动化流程提取时间对齐的音素、音节和单词，并采用双向LSTM与多头注意力机制进行比较分析。

Result: 音素级别的分析表现出色，AUROC为93.78% ± 2.34%，准确率为92.17% ± 2.43%。

Conclusion: 该研究展示了基于语音的帕金森病检测方法在跨语言检测中的增强诊断能力，并揭示了与现有临床协议一致的语音特征。

Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with
speech impairments in up to 89% of patients. Current speech-based detection
systems analyze entire utterances, potentially overlooking the diagnostic value
of specific phonetic elements. We developed a granularity-aware approach for
multilingual PD detection using an automated pipeline that extracts
time-aligned phonemes, syllables, and words from recordings. Using Italian,
Spanish, and English datasets, we implemented a bidirectional LSTM with
multi-head attention to compare diagnostic performance across the different
granularity levels. Phoneme-level analysis achieved superior performance with
AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates
enhanced diagnostic capability for cross-linguistic PD detection. Importantly,
attention analysis revealed that the most informative speech features align
with those used in established clinical protocols: sustained vowels (/a/, /e/,
/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)
at syllable level, and /pataka/ sequences at word level. Source code will be
available at https://github.com/jetliqs/clearpd.

</details>


### [26] [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
*Deshan Sumanathilaka,Nicholas Micallef,Julian Hough*

Main category: cs.CL

TL;DR: 研究分析了少样本提示策略对多语言词义消歧的影响，发现不平衡样本分布会导致错误预测，强调了平衡提示策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨少样本提示策略如何影响词义消歧任务，特别是不平衡样本分布引入的偏差。

Method: 研究使用了GLOSSGPT提示方法，并评估了GPT-4o和LLaMA-3.1-70B模型在五种语言中的表现。

Result: 结果表明，不平衡的少样本示例可能导致多语言中的错误词义预测，但在英语中未出现此问题。

Conclusion: 研究强调了在少样本设置中，样本分布对多语言WSD的敏感性，并指出需要平衡和有代表性的提示策略。

Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped
the landscape of Natural Language Processing (NLP). Among the various prompting
techniques, few-shot prompting has gained considerable attention for its
practicality and effectiveness. This study investigates how few-shot prompting
strategies impact the Word Sense Disambiguation (WSD) task, particularly
focusing on the biases introduced by imbalanced sample distributions. We use
the GLOSSGPT prompting method, an advanced approach for English WSD, to test
its effectiveness across five languages: English, German, Spanish, French, and
Italian. Our results show that imbalanced few-shot examples can cause incorrect
sense predictions in multilingual languages, but this issue does not appear in
English. To assess model behavior, we evaluate both the GPT-4o and
LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual
WSD to sample distribution in few-shot settings, emphasizing the need for
balanced and representative prompting strategies.

</details>


### [27] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 该研究开发了一个大规模的AI辅助圣训语料库，通过自动化流程对文本进行多层增强，并展示了人工智能在宗教文本处理中的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了构建一个大规模、多语言和语义丰富的伊斯兰遗产语料库，该研究旨在利用人工智能技术来增强人类专业知识。

Method: 该研究使用了大型语言模型（LLMs）进行分割、链-文本分离、验证和多层增强，以构建一个大规模的AI辅助圣训语料库。

Result: 评估结果显示，在结构化任务如链-文本分离（9.33/10）和摘要（9.33/10）中接近人类准确性，而在音调标记和语义相似性检测方面仍存在挑战。与手动整理的Noor语料库相比，Najm在规模和质量上都表现出优越性。

Conclusion: 该研究引入了一种新的宗教文本处理范式，展示了人工智能如何增强人类专业知识，使大规模、多语言和语义丰富的伊斯兰遗产访问成为可能。

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [28] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: 本论文探讨了大型语言模型在社会政治背景下生成和识别深度认知框架的能力，并发现它们能够通过模型的隐藏表示识别出特定的认知框架。


<details>
  <summary>Details</summary>
Motivation: 本论文探讨了大型语言模型生成和识别深度认知框架的能力，特别是在社会政治背景下。

Method: 受机制可解释性研究的启发，我们调查了'严格父亲'和'养育父母'框架在模型隐藏表示中的位置，识别出与它们的存在强烈相关的单一维度。

Result: 我们证明了LLMs在生成引发特定框架的文本方面非常流利，并且可以在零样本设置中识别这些框架。

Conclusion: 本研究有助于理解大型语言模型如何捕捉和表达有意义的人类概念。

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [29] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: This paper introduces Step Pruner (SP), an RL framework that promotes efficient reasoning in large models by prioritizing correctness and penalizing redundant steps, resulting in significant reductions in response length without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing solutions via reinforcement learning (RL) typically penalize generated tokens to promote conciseness, but these methods encounter two challenges: responses with fewer tokens do not always correspond to fewer reasoning steps, and models may develop hacking behavior in later stages of training by discarding reasoning steps to minimize token usage.

Method: Step Pruner (SP), an RL framework that steers LRMs toward more efficient reasoning by favoring compact reasoning steps.

Result: Extensive experiments across four reasoning benchmarks demonstrate that SP achieves state-of-the-art accuracy while significantly reducing response length. For instance, on AIME24, SP reduces token usage by 69.7%.

Conclusion: SP achieves state-of-the-art accuracy while significantly reducing response length.

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [30] [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
*Mehedi Hasan Emon*

Main category: cs.CL

TL;DR: 该研究比较了手动标注与基于BERT、DistilBERT和逻辑回归模型的自动方法在体育新闻中修辞关系分类的效果，发现DistilBERT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索使用INCEpTION工具对话语中的修辞关系进行标注，并比较手动标注与自动方法的效果。

Method: 使用INCEpTION工具进行修辞关系标注，并比较了手动标注与基于大型语言模型的自动方法。

Result: DistilBERT在分类修辞关系方面取得了最高的准确率。

Conclusion: 该研究展示了DistilBERT在话语关系预测中的潜力，为话语解析和基于Transformer的自然语言处理的交叉领域做出了贡献。

Abstract: This research explores the annotation of rhetorical relations in discourse
using the INCEpTION tool and compares manual annotation with automatic
approaches based on large language models. The study focuses on sports reports
(specifically cricket news) and evaluates the performance of BERT, DistilBERT,
and Logistic Regression models in classifying rhetorical relations such as
elaboration, contrast, background, and cause-effect. The results show that
DistilBERT achieved the highest accuracy, highlighting its potential for
efficient discourse relation prediction. This work contributes to the growing
intersection of discourse parsing and transformer-based NLP. (This paper was
conducted as part of an academic requirement under the supervision of Prof. Dr.
Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:
Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,
NLP.

</details>


### [31] [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
*Nusrat Jahan Lia,Shubhashis Roy Dipta,Abdullah Khan Zehady,Naymul Islam,Madhusodan Chakraborty,Abdullah Al Wasif*

Main category: cs.CL

TL;DR: 本文介绍了首个用于孟加拉语政治偏见研究的基准数据集，并评估了大型语言模型在检测孟加拉语新闻中政治立场的表现。


<details>
  <summary>Details</summary>
Motivation: 检测媒体偏见至关重要，特别是在南亚地区。然而，用于孟加拉语政治偏见研究的标注数据集和计算研究仍然很少。

Method: 引入了第一个基准数据集，包含200篇具有政治意义和高度争议的孟加拉语新闻文章，并对政府倾向、政府批评和中立立场进行了标记，同时还进行了诊断分析以评估大型语言模型（LLMs）。

Result: 对28个专有和开源大型语言模型的全面评估显示，在检测政府批评内容方面表现良好（F1高达0.83），但在处理中立文章时存在显著困难（F1最低为0.00）。模型也倾向于过度预测政府倾向的立场，常常误解模糊的叙述。

Conclusion: 该数据集及其相关的诊断为推进孟加拉语媒体研究中的立场检测奠定了基础，并为提高低资源语言中大型语言模型的性能提供了见解。

Abstract: Detecting media bias is crucial, specifically in the South Asian region.
Despite this, annotated datasets and computational studies for Bangla political
bias research remain scarce. Crucially because, political stance detection in
Bangla news requires understanding of linguistic cues, cultural context, subtle
biases, rhetorical strategies, code-switching, implicit sentiment, and
socio-political background. To address this, we introduce the first benchmark
dataset of 200 politically significant and highly debated Bangla news articles,
labeled for government-leaning, government-critique, and neutral stances,
alongside diagnostic analyses for evaluating large language models (LLMs). Our
comprehensive evaluation of 28 proprietary and open-source LLMs shows strong
performance in detecting government-critique content (F1 up to 0.83) but
substantial difficulty with neutral articles (F1 as low as 0.00). Models also
tend to over-predict government-leaning stances, often misinterpreting
ambiguous narratives. This dataset and its associated diagnostics provide a
foundation for advancing stance detection in Bangla media research and offer
insights for improving LLM performance in low-resource languages.

</details>


### [32] [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
*Mohammad Amin Abbasi,Hassan Naderi*

Main category: cs.CL

TL;DR: This study introduces PsychoLexTherapy, a framework for simulating psychotherapeutic reasoning in Persian using small language models. It focuses on culturally grounded, privacy-preserving dialogue systems with structured memory for multi-turn interactions.


<details>
  <summary>Details</summary>
Motivation: The study aims to develop culturally grounded, therapeutically coherent dialogue systems for Persian using small language models (SLMs), ensuring privacy and feasibility through on-device deployment.

Method: The framework tackles the challenge of developing culturally grounded, therapeutically coherent dialogue systems with structured memory for multi-turn interactions in underrepresented languages. It follows a three-stage process: (i) assessing SLMs psychological knowledge with PsychoLexEval; (ii) designing and implementing the reasoning-oriented PsychoLexTherapy framework; and (iii) constructing two evaluation datasets-PsychoLexQuery and PsychoLexDialogue-to benchmark against multiple baselines.

Result: PsychoLexTherapy outperformed all baselines in automatic LLM-as-a-judge evaluation and was ranked highest by human evaluators in a single-turn preference study. In multi-turn tests, the long-term memory module proved essential, achieving the highest ratings in empathy, coherence, cultural fit, and personalization.

Conclusion: PsychoLexTherapy establishes a practical, privacy-preserving, and culturally aligned foundation for Persian psychotherapy simulation, contributing novel datasets, a reproducible evaluation pipeline, and empirical insights into structured memory for therapeutic reasoning.

Abstract: This study presents PsychoLexTherapy, a framework for simulating
psychotherapeutic reasoning in Persian using small language models (SLMs). The
framework tackles the challenge of developing culturally grounded,
therapeutically coherent dialogue systems with structured memory for multi-turn
interactions in underrepresented languages. To ensure privacy and feasibility,
PsychoLexTherapy is optimized for on-device deployment, enabling use without
external servers. Development followed a three-stage process: (i) assessing
SLMs psychological knowledge with PsychoLexEval; (ii) designing and
implementing the reasoning-oriented PsychoLexTherapy framework; and (iii)
constructing two evaluation datasets-PsychoLexQuery (real Persian user
questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark
against multiple baselines. Experiments compared simple prompting, multi-agent
debate, and structured therapeutic reasoning paths. Results showed that
deliberate model selection balanced accuracy, efficiency, and privacy. On
PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic
LLM-as-a-judge evaluation and was ranked highest by human evaluators in a
single-turn preference study. In multi-turn tests with PsychoLexDialogue, the
long-term memory module proved essential: while naive history concatenation
caused incoherence and information loss, the full framework achieved the
highest ratings in empathy, coherence, cultural fit, and personalization.
Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and
culturally aligned foundation for Persian psychotherapy simulation,
contributing novel datasets, a reproducible evaluation pipeline, and empirical
insights into structured memory for therapeutic reasoning.

</details>


### [33] [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
*Junjie Luo,Rui Han,Arshana Welivita,Zeleikun Di,Jingfu Wu,Xuzhe Zhi,Ritu Agarwal,Gordon Gao*

Main category: cs.CL

TL;DR: 本文介绍了一种基于大型语言模型的管道，用于从患者评论中推断医生的五大个性特征和五个以患者为中心的主观判断，并验证了其有效性。结果表明，这种自动化方法可以提供可解释的指标，用于理解医患关系，并对医疗质量测量、偏差检测和人力资源发展有影响。


<details>
  <summary>Details</summary>
Motivation: 了解患者如何感知他们的医生对于改善信任、沟通和满意度至关重要。

Method: 我们提出了一种基于大型语言模型（LLM）的管道，用于推断五大个性特征和五个以患者为中心的主观判断。

Result: 通过多模型比较和人类专家基准测试验证了该方法，实现了人类和LLM评估之间的强一致性（相关系数0.72-0.89），并通过与患者满意度的相关性（r = 0.41-0.81，所有p<0.001）证明了外部有效性。全国范围的分析揭示了系统模式：男性医生在所有特质中获得更高的评分，其中在临床能力认知上的差异最大；与同理心相关的特质在儿科和精神科占主导地位；所有特质都积极预测总体满意度。聚类分析确定了四个不同的医生原型，从“全面优秀”（33.8%，统一高特质）到“表现不佳”（22.6%，持续低水平）。

Conclusion: 这些发现表明，从患者叙述中自动提取特质可以提供可解释的、经过验证的指标，以在大规模上理解医患关系，这对医疗质量测量、偏差检测和人力资源发展有影响。

Abstract: Understanding how patients perceive their physicians is essential to
improving trust, communication, and satisfaction. We present a large language
model (LLM)-based pipeline that infers Big Five personality traits and five
patient-oriented subjective judgments. The analysis encompasses 4.1 million
patient reviews of 226,999 U.S. physicians from an initial pool of one million.
We validate the method through multi-model comparison and human expert
benchmarking, achieving strong agreement between human and LLM assessments
(correlation coefficients 0.72-0.89) and external validity through correlations
with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis
reveals systematic patterns: male physicians receive higher ratings across all
traits, with largest disparities in clinical competence perceptions;
empathy-related traits predominate in pediatrics and psychiatry; and all traits
positively predict overall satisfaction. Cluster analysis identifies four
distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly
high traits) to "Underperforming" (22.6%, consistently low). These findings
demonstrate that automated trait extraction from patient narratives can provide
interpretable, validated metrics for understanding physician-patient
relationships at scale, with implications for quality measurement, bias
detection, and workforce development in healthcare.

</details>


### [34] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: 本文介绍了第一个模拟框架，用于在扩展的任务序列和动态上下文压力下探测和评估LLM中的欺骗。实验结果表明，欺骗是模型相关的，并且会随着事件压力的增加而增加，同时持续侵蚀监督者的信任。


<details>
  <summary>Details</summary>
Motivation: 欺骗是人类交流中的普遍特征，也是大型语言模型（LLMs）的一个新兴问题。尽管最近的研究记录了在压力下的LLM欺骗实例，但大多数评估仍然局限于单次提示，未能捕捉到欺骗策略通常展开的长时交互。

Method: 我们引入了一个模拟框架，用于在扩展的任务序列和动态上下文压力下探测和评估LLM中的欺骗。该框架实例化了一个多智能体系统：一个执行任务的执行者代理和一个评估进展、提供反馈并保持不断变化的信任状态的监督者代理。独立的欺骗审计员随后审查完整的轨迹以识别欺骗发生的时间和方式。

Result: 我们在11个前沿模型上进行了广泛的实验，涵盖了封闭和开源系统，并发现欺骗是模型相关的，随着事件压力的增加而增加，并且持续侵蚀监督者的信任。定性分析进一步揭示了不同的隐藏、模棱两可和伪造策略。

Conclusion: 我们的研究确立了欺骗作为一种在长时交互中出现的风险，并为在现实世界、信任敏感的环境中评估未来的LLM提供了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [35] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的实体知识增强方法，用于冠状病毒的命名实体识别，并在社交媒体和学术文献数据集上进行了实验，结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体上的冠状病毒文本是非正式的且标注稀少，以及冠状病毒中的命名实体识别需要大量的领域特定知识，因此目前对此主题的研究有限。

Method: 我们提出了一种新颖的实体知识增强方法，用于冠状病毒，并且该方法也可应用于一般生物医学命名实体识别。

Result: 在冠状病毒推文数据集和PubMed数据集上进行的实验表明，所提出的实体知识增强方法在完全监督和少量样本设置中都提高了NER性能。

Conclusion: 我们的实验表明，所提出的实体知识增强方法在完全监督和少量样本设置中都提高了NER性能。

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [36] [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Xiao Xu,Jianyu Zhang,Nueraili Aierken,Runhe Huang,Hongjian Lin,Yibin Ying,Shijian Li*

Main category: cs.CL

TL;DR: 本文介绍了AgriGPT-VL套件，包括最大的农业视觉语言语料库Agri-3M-VL，农业专用的视觉语言模型AgriGPT-VL以及评估套件AgriBench-VL-4K。实验结果表明，AgriGPT-VL在农业任务上表现优异，并且保持了文本-only能力。


<details>
  <summary>Details</summary>
Motivation: 农业应用受到领域定制模型、精心策划的视觉语言语料库和严格评估的限制。为此，我们提出了AgriGPT-VL套件，这是一个统一的农业多模态框架。

Method: 我们开发了AgriGPT-VL，这是一个通过渐进式课程进行训练的农业专用视觉语言模型，包括文本定位、多模态浅层/深层对齐和GRPO优化。

Result: 实验表明，AgriGPT-VL在AgriBench-VL-4K上优于领先的通用VLM，在LLM-as-a-judge评估中实现了更高的成对胜率。同时，它在文本-only AgriBench-13K上保持竞争力，且没有明显降低语言能力。

Conclusion: AgriGPT-VL在AgriBench-VL-4K上优于领先的通用VLM，并在文本-only AgriBench-13K上保持竞争力。通过消融研究进一步确认了对齐和GRPO优化阶段的持续收益。所有资源将开源以支持可重复的研究和低资源农业环境中的部署。

Abstract: Despite rapid advances in multimodal large language models, agricultural
applications remain constrained by the scarcity of domain-tailored models,
curated vision-language corpora, and rigorous evaluation. To address these
challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for
agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,
the largest vision-language corpus for agriculture to our knowledge, curated by
a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M
image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO
reinforcement learning samples. Second, we develop AgriGPT-VL, an
agriculture-specialized vision-language model trained via a progressive
curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO
refinement. This method achieves strong multimodal reasoning while preserving
text-only capability. Third, we establish AgriBench-VL-4K, a compact yet
challenging evaluation suite with open-ended and image-grounded questions,
paired with multi-metric evaluation and an LLM-as-a-judge framework.
Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on
AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge
evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K
with no noticeable degradation of language ability. Ablation studies further
confirm consistent gains from our alignment and GRPO refinement stages. We will
open source all of the resources to support reproducible research and
deployment in low-resource agricultural settings.

</details>


### [37] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: 本研究通过分析模型内部的激活来预测大型语言模型输出的正确性，并探索模型内部是否包含关于外部上下文效果的信号。实验结果显示，一个简单的分类器可以在第一个输出标记的中间层激活上进行训练，以约75%的准确率预测输出的正确性，从而实现早期审计。基于模型内部的指标在区分正确和错误的上下文方面表现更好，防止了由污染的上下文引入的不准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）有巨大的实用性，但可信度仍然是一个主要问题：模型经常以高置信度生成错误信息。虽然上下文信息可以帮助引导生成，但识别查询何时会从检索到的上下文中受益以及评估该上下文的有效性仍然具有挑战性。

Method: 我们操作化了可解释性方法，以确定是否可以从模型的激活中预测模型输出的正确性。我们还探讨了模型内部是否包含关于外部上下文效果的信号。

Result: 实验表明，一个简单的分类器在第一个输出标记的中间层激活上进行训练，可以以约75%的准确率预测输出的正确性，从而实现早期审计。基于模型内部的指标显著优于提示基线，在区分正确和错误的上下文方面表现更好，防止了由污染的上下文引入的不准确性。

Conclusion: 我们的研究结果提供了一个更好的理解大型语言模型决策过程的视角。

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [38] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: 本文研究了泰语文本仅有的端-turn检测，比较了不同方法的性能，并展示了小型微调模型可以提供接近即时的决策。


<details>
  <summary>Details</summary>
Motivation: 传统音频静音端点检测器添加了数百毫秒的延迟，并在犹豫或语言特定现象下失败。本文旨在研究泰语文本仅有的端-turn（EOT）检测，以实现实时代理的可靠和低延迟检测。

Method: 本文比较了紧凑型LLM的零样本和少量样本提示与轻量级变压器的监督微调。利用YODAS语料库的转录字幕和泰语特定的语言线索（例如，句子结尾的助词），将EOT建模为一个二元决策。

Result: 本文报告了一个清晰的准确性-延迟权衡，并提供了公共就绪的实现计划。

Conclusion: 本文建立了泰语基线，并展示了微调的小型模型可以提供适合设备端代理的近乎即时的EOT决策。

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [39] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 本文研究了在大型语言模型推理中引入反事实对识别分类关键词的影响，并提出了一种量化方法。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型是黑箱且调用成本高，需要解释其决策过程。

Method: 引入了一个称为决策变化率的框架，用于量化分类中顶级单词的重要性。

Result: 实验结果表明，使用反事实可以有所帮助。

Conclusion: 使用反事实可以帮助提高大型语言模型在文本分类任务中识别关键词的能力。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [40] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 本文研究了小语言模型在急诊科的应用，发现通用领域的小语言模型在多个基准测试中表现优于专门医学微调的模型，表明在急诊科可能不需要对模型进行专门的医学微调。


<details>
  <summary>Details</summary>
Motivation: 由于实际硬件限制、运营成本约束和隐私问题，急诊科需要高效且具备推理能力的小语言模型来提供及时准确的信息合成，从而提高临床决策和工作效率。

Method: 本文提出了一种全面的基准测试，用于识别适合急诊科决策支持的小语言模型，并评估了在通用领域和医学语料库上训练的小语言模型的表现。

Result: 实验结果表明，通用领域的小语言模型在急诊科的多个基准测试中表现优于专门医学微调的模型。

Conclusion: 实验结果表明，通用领域的小语言模型在急诊科的多个基准测试中表现优于专门医学微调的模型，这表明对于急诊科，可能不需要对模型进行专门的医学微调。

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [41] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 本文研究了Chain-of-Thought (CoT)推理技术在构建可调节的多元模型中的应用，并发现RLVR方法在性能和训练样本效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常被训练以反映相对统一的价值观，这限制了它们在需要理解细微的人类观点的任务中的适用性。因此，需要使LLMs能够支持可调节的多元主义。

Method: 本文探讨了多种方法，包括CoT提示、基于人类编写的CoT微调、基于合成解释的微调以及基于可验证奖励的强化学习（RLVR）。

Result: 在研究的方法中，RLVR始终优于其他方法，并表现出强大的训练样本效率。此外，还分析了生成的CoT轨迹在忠实性和安全性方面的表现。

Conclusion: 本文研究了Chain-of-Thought (CoT)推理技术在构建可调节的多元模型中的应用，并发现RLVR方法在性能和训练样本效率方面表现最佳。

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [42] [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
*Zitian Gao,Haoming Luo,Lynx Chen,Jason Klein Liu,Ran Tao,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: 本研究通过消融实验揭示了扩散语言模型在有限数据下数据效率的机制，发现随机掩码、MLP中的dropout和权重衰减等随机正则化方法能显著提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 研究扩散语言模型在有限数据下的数据效率的潜在机制。

Method: 通过广泛的消融实验来分离这种效率的来源。

Result: 结果显示，输入标记的随机掩码起主导作用，而MLP中的dropout和权重衰减也能带来类似的增益。

Conclusion: 随机掩码、MLP中的dropout和权重衰减等随机正则化方法在多轮训练中能显著提高数据效率。

Abstract: Recent studies have shown that diffusion language models achieve remarkable
data efficiency under limited-data constraints, yet the underlying mechanisms
remain unclear. In this work, we perform extensive ablation experiments to
disentangle the sources of this efficiency. Our results show that random
masking of input tokens plays the dominant role. We further show that similar
gains can be obtained through in MLP dropout and weight decay, indicating that
stochastic regularization broadly enhances data efficiency in multi-epoch
training. Our code is available at
https://github.com/zitian-gao/data-efficiency.

</details>


### [43] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: 本文提出了 PoLi-RL，这是一种新颖的点对列表强化学习框架，通过两阶段课程和并行切片排名奖励机制，在 C-STS 任务中取得了新的 SOTA 成绩，为 LLMs 在基于排名的条件判断任务中的训练提供了一个强大且精确的范式。


<details>
  <summary>Details</summary>
Motivation: 现有的 C-STS 方法主要局限于判别模型，未能充分利用 NLP 领域在大型语言模型 (LLMs) 和强化学习 (RL) 方面的最新突破。RL 被认为是该任务的合适范式，因为它可以直接优化非可微的 Spearman 排名指标，并指导 C-STS 所需的推理过程。然而，直接应用列表 RL 未能产生有意义的改进，因为模型被复杂的粗粒度奖励信号所淹没。

Method: PoLi-RL 采用两阶段课程：首先使用简单的点对奖励训练模型以建立基本评分能力，然后过渡到结合点对、成对和列表目标的混合奖励，以细化模型区分细微语义差异的能力。此外，提出了一种创新的并行切片排名奖励 (PSRR) 机制，通过在并行切片中计算排名奖励，为每个单独的完成提供精确的学习信号。

Result: PoLi-RL 在官方 C-STS 基准测试中实现了 48.18 的 Spearman 相关系数，为交叉编码器架构建立了新的 SOTA。作为首次成功将 RL 应用于 C-STS 的工作，该研究引入了一种强大且精确的范式，用于训练 LLMs 在复杂、基于排名的条件判断任务中。

Conclusion: PoLi-RL 是一种新颖的点对列表强化学习框架，通过两阶段课程训练和并行切片排名奖励机制，在 C-STS 任务中取得了新的 SOTA 成绩，为 LLMs 在基于排名的条件判断任务中的训练提供了一个强大且精确的范式。

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [44] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Caco的新框架，通过代码驱动增强来自动合成高质量、可验证和多样化的指令-CoT推理数据，从而实现无需人工干预的自我维持、可信的推理系统。


<details>
  <summary>Details</summary>
Motivation: 尽管Chain-of-Thought（CoT）提示已成为主流方法，但现有方法常常存在生成不可控、质量不足和推理路径多样性有限的问题。最近的努力利用代码来增强CoT，但这些方法通常局限于预定义的数学问题，阻碍了可扩展性和通用性。

Method: 我们提出了Caco（Code-Assisted Chain-of-ThOught），这是一种新颖的框架，通过代码驱动增强来自动合成高质量、可验证和多样化的指令-CoT推理数据。

Result: 实验表明，Caco训练的模型在数学推理基准测试中表现出色，优于现有的强基线。进一步分析表明，Caco的代码锚定验证和指令多样性有助于在未见过的任务上实现优越的泛化能力。

Conclusion: 我们的工作建立了一个无需人工干预的自我维持、可信的推理系统范式。

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [45] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本研究分析了大型语言模型在隐喻处理方面的表现，发现其存在概念无关解释、依赖隐喻指示器和对句法不规则性敏感的问题，并指出需要更强大的计算方法。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在隐喻理解中的机制，因为它们在知识整合、上下文推理和创造性生成方面表现出色，但在隐喻理解方面仍需深入研究。

Method: 从三个角度分析了大型语言模型的隐喻处理能力：概念映射、隐喻-字面仓库和句法敏感性。

Result: 研究发现，大型语言模型生成15%-25%的概念无关解释，依赖于训练数据中的隐喻指示器而非上下文线索，并且对句法不规则性更敏感。

Conclusion: 研究揭示了大型语言模型在隐喻分析中的局限性，并呼吁开发更强大的计算方法。

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [46] [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
*Nuwan I. Senaratna*

Main category: cs.CL

TL;DR: 本文介绍了一个包含斯里兰卡议会记录、法律判决、政府出版物、新闻和旅游统计数据的开放数据集，旨在支持多个领域的研究。


<details>
  <summary>Details</summary>
Motivation: 提供开放、可机器读取的文档数据集，以支持多个领域的研究。

Method: 介绍了数据来源、收集流程、格式和潜在用例，同时讨论了许可和伦理考虑因素。

Result: 截至v20251005，该集合目前包含215,670个文档（60.3 GB），涵盖13个数据集，包括僧伽罗语、泰米尔语和英语。

Conclusion: 这些资源旨在支持计算语言学、法律分析、社会政治研究和多语言自然语言处理的研究。

Abstract: We present a collection of open, machine-readable document datasets covering
parliamentary proceedings, legal judgments, government publications, news, and
tourism statistics from Sri Lanka. As of v20251005, the collection currently
comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and
English. The datasets are updated daily and mirrored on GitHub and Hugging
Face. These resources aim to support research in computational linguistics,
legal analytics, socio-political studies, and multilingual natural language
processing. We describe the data sources, collection pipeline, formats, and
potential use cases, while discussing licensing and ethical considerations.

</details>


### [47] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: 该项目旨在通过开发一种通用的方法来准备文化相关的数据集，并对Gemma 2模型进行后训练，以提高其在未被充分代表的语言中的性能，并展示其他人如何做到这一点，以在自己的国家中解锁生成式AI的潜力并保护其文化遗产。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要基于英语文本和文化进行训练，这使得它们在其他语言和文化背景下表现不佳。

Method: 开发一种通用的方法来准备文化相关的数据集，并对Gemma 2模型进行后训练。

Result: 提高了Gemma 2模型在未被充分代表的语言中的性能。

Conclusion: 该项目旨在通过开发一种通用的方法来准备文化相关的数据集，并对Gemma 2模型进行后训练，以提高其在未被充分代表的语言中的性能，并展示其他人如何做到这一点，以在自己的国家中解锁生成式AI的潜力并保护其文化遗产。

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [48] [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
*Yifeng Gao,Ziang Ji,Yuxuan Wang,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出 SSD 方法，通过利用 dLLM 自身进行推测解码，实现无损加速，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前并行解码方法的生成结果与逐步解码存在偏差，导致性能下降，限制了实际部署。

Method: 提出了一种自推测解码（SSD）方法，利用 dLLM 本身进行多位置预测，并通过分层验证树在单次前向传递中验证预测结果。

Result: SSD 在开源模型如 LLaDA 和 Dream 上实现了高达 3.46 倍的速度提升，同时保持输出与逐步解码相同。

Conclusion: SSD 是一种无损的推理加速方法，通过利用 dLLM 本身作为推测解码的草稿和验证者，消除了模型冗余和内存开销，实现了高达 3.46 倍的速度提升。

Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive
alternative to autoregressive models, offering unique advantages through
bidirectional attention and parallel generation paradigms. However, the
generation results of current parallel decoding methods deviate from stepwise
decoding, introducing potential performance degradation, which limits their
practical deployment. To address this problem, we propose \textbf{S}elf
\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration
method that leverages the dLLM itself as both speculative decoding drafter and
verifier without auxiliary modules. SSD introduces a self-drafting mechanism
where the model generates predictions for multiple positions, then verifies
them through hierarchical verification trees in a single forward pass. Unlike
traditional speculative decoding that requires separate draft models, SSD
eliminates model redundancy and memory overhead by exploiting the dLLM's
inherent parallel prediction capability for multiple positions. This
self-speculative approach allows the model to progressively verify and accept
multiple tokens in a single forward pass. Our experiments demonstrate that SSD
achieves up to 3.46$\times$ speedup while keeping the output identical to
stepwise decoding on open source models such as LLaDA and Dream. Code will be
made publicly available on GitHub.

</details>


### [49] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: LTPO is a parameter-free framework that enhances LLM reasoning by optimizing latent thought vectors at test time, showing improved performance on challenging tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the brittleness of latent reasoning on challenging, out-of-distribution tasks by enhancing LLM reasoning without requiring model parameter updates.

Method: LTPO is a parameter-free framework that enhances LLM reasoning at test time by optimizing intermediate latent 'thought' vectors using an intrinsic, confidence-based reward signal.

Result: LTPO matches or surpasses strong baselines on standard tasks and shows significant improvements on challenging AIME benchmarks where existing methods fail.

Conclusion: LTPO demonstrates a unique capability for complex reasoning and shows remarkable robustness on challenging tasks.

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [50] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [51] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的后训练框架REPO，用于提升大型语言模型在在线旅行社中的说服性价格谈判表现。REPO通过结合多种奖励机制，有效提高了对话质量并解决了传统方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统后训练方法如监督微调（SFT）或单源奖励优化存在过拟合脚本、遗漏细微的说服风格以及无法执行可验证的业务约束的问题。因此需要一种更有效的后训练方法来提升LLM在在线旅行社中的说服性价格谈判表现。

Method: 提出了一种基于强化学习的后训练框架REPO，该框架通过结合偏好训练的奖励模型（RM）、奖励判断器（RJ）和程序化奖励函数（RF）来对齐LLM与异构奖励。

Result: REPO在生产风格评估中将平均对话评分提高到4.63，比基础模型高出1.20，比直接偏好优化（DPO）高出0.83，比组相对策略优化（GRPO）高出0.33。同时，至少有一个优秀回复的对话比例增加到66.67%，坏案例修复率达到93.33%。

Conclusion: REPO在生产风格评估中表现出色，提高了对话评分，增加了至少一个优秀回复的对话比例，并实现了高比例的坏案例修复率，优于SFT、DPO、PPO和GRPO。此外，还观察到了超越黄金标注的新兴能力。

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [52] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法来衡量大型语言模型（LLMs）输出中的epistemic多样性，并进行了广泛的实证研究。结果表明，虽然较新的模型生成的主张更多样化，但几乎所有模型的epistemic多样性都低于基本的网络搜索。模型规模对epistemic多样性有负面影响，而RAG有正面影响，但RAG带来的改进因文化背景而异。此外，与传统知识来源相比，国家特定的主张更多地反映了英语语言，而不是本地语言，这凸显了epistemic表示的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的关于同质化的研究受到专注于封闭式多项选择设置或模糊语义特征的限制，并未考虑时间和社会文化背景的趋势。因此，需要一种新的方法来衡量epistemic多样性，以研究LLM的知识崩溃问题。

Method: 本文提出了一种新的方法来衡量epistemic多样性，即LLM输出中现实世界主张的变化，并使用该方法进行了一项广泛的LLM知识崩溃实证研究。测试了27个LLM，155个主题覆盖12个国家，以及200个来自真实用户聊天的提示变体。

Result: 研究发现，虽然较新的模型倾向于生成更多样化的主张，但几乎所有模型的epistemic多样性都低于基本的网络搜索。模型规模对epistemic多样性有负面影响，而RAG有正面影响，但RAG带来的改进因文化背景而异。此外，与传统知识来源（如维基百科）相比，国家特定的主张更多地反映了英语语言，而不是本地语言，这凸显了epistemic表示的差距。

Conclusion: 研究发现，尽管较新的模型生成的主张更加多样化，但几乎所有模型的epistemic多样性都低于基本的网络搜索。模型规模对epistemic多样性有负面影响，而检索增强生成（RAG）有正面影响，但RAG带来的改进因文化背景而异。此外，与传统知识来源（如维基百科）相比，国家特定的主张更多地反映了英语语言，而不是本地语言，这凸显了epistemic表示的差距。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [53] [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Amit Agarwal,Hyunwoo Ko,Chanuk Lim,Srikant Panda,Minhyuk Kim,Nikunj Drolia,Dasol Choi,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文介绍了一种新的推理模式Language-Mixed CoT，用于提高多语言推理性能，并发布了一个大型数据集和模型以推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 目前大多数研究集中在英语上的知识蒸馏，而对语言特定推理的研究较少。因此，本文旨在填补这一空白，探索一种能提高多语言推理性能的方法。

Method: 本文首先引入了Language-Mixed CoT，这是一种在英语和目标语言之间切换的推理模式。然后，作者创建了一个名为Yi-Sang的数据集，包含5.79M本土韩语提示和3.7M长推理轨迹，并训练了多个模型（4B-35B）进行实验。

Result: 最好的模型KO-REAson-35B在9个基准测试中获得了最高的平均分数（64.0 ± 25），并在5个基准测试中排名第一，其余的排名第二。较小和中型模型也显著受益，平均提高了18.6分。

Conclusion: 本文提出了一种名为Language-Mixed CoT的推理模式，通过在英语和目标语言之间切换，利用英语作为锚点来提高推理能力并减少翻译伪影。此外，作者还发布了数据集、模型和评估系统，以推动语言特定推理的研究。

Abstract: Recent frontier models employ long chain-of-thought reasoning to explore
solution spaces in context and achieve stonger performance. While many works
study distillation to build smaller yet capable models, most focus on English
and little is known about language-specific reasoning. To bridge this gap, we
first introduct **Language-Mixed CoT**, a reasoning schema that switches
between English and a target language, using English as an anchor to excel in
reasoning while minimizing translation artificats. As a Korean case study, we
curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and
code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k
high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,
Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves
state-of-the-art performance, with the highest overall average score (64.0 \pm
25), ranking first on 5/9 benchmarks and second on the remainder. Samller and
mid-sized models also benefit substantially, with an average improvement of
+18.6 points across teh evaluated nine benchmarks. Ablations show
**Language-Mixed CoT** is more effective than monolingual CoT, also resulting
in cross-lingual and mult-modal performance gains. We release our data-curation
pipeline, evaluation system, datasets, and models to advance research on
language-specific reasoning. Data and model collection:
https://huggingface.co/KOREAson.

</details>


### [54] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 我们引入了LongTail-Swap (LT-Swap) 基准测试，专注于语言模型在罕见词上的表现。结果显示，语言模型在罕见词上的表现不佳，且不同架构之间的性能差异在长尾部分更为明显。


<details>
  <summary>Details</summary>
Motivation: BabyLM挑战旨在探索语言模型在低数据条件下的训练，但使用的指标集中在词分布的头部。我们希望引入一个专注于分布尾部的基准测试，以测量语言模型在很少接触的情况下学习新词的能力，就像婴儿一样。

Method: 我们引入了LongTail-Swap (LT-Swap)，这是一个专注于分布尾部的基准测试，测量语言模型学习新词的能力。LT-Swap是一个预训练语料库特定的测试集，包含可接受和不可接受的句子对，用于隔离罕见词的语义和句法使用。模型通过计算每对中的两个成员的平均对数概率来进行零样本评估。

Result: 我们构建了两个与10M词和100M词BabyLM训练集相关的测试集，并评估了BabyLM排行榜上的16个模型。结果表明，语言模型在罕见词上的表现较差，且不同架构之间的性能差异在长尾部分比在头部更为显著。

Conclusion: 我们的结果不仅突显了语言模型在罕见词上的表现不佳，还揭示了不同语言模型架构在长尾部分的表现差异更为明显。这为哪种架构在处理罕见词泛化方面更好提供了新的见解。

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [55] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: 本文引入了一种累积展开框架，用于量化大型语言模型在下一个标记预测过程中内部化的高阶统计结构。通过分析GPT-2和Pythia模型在Pile-10K提示中的累积量，发现结构化提示和数学提示具有不同的累积签名，表明模型在处理不同类型的文本时采用了不同的机制。


<details>
  <summary>Details</summary>
Motivation: 为了量化大型语言模型在下一个标记预测过程中内部化的高阶统计结构。

Method: 通过将每个层的logit分布的softmax熵视为其'中心'分布的扰动，推导出封闭形式的累积可观测量，以隔离更高阶的相关性。

Result: 结构化提示在各层中表现出上升和平台的特征，而随机打乱的提示保持平坦，这揭示了累积轮廓对有意义上下文的依赖性；所有累积量在训练期间单调增加并达到饱和，直接可视化了模型从捕捉方差到学习偏度、峰度和更高阶统计结构的进展；数学提示与一般文本相比显示出不同的累积签名，量化了模型在数学和语言内容处理机制上的根本差异。

Conclusion: 这些结果确立了累积分析作为一种轻量级、数学基础的探针，用于研究高维神经网络中的特征学习动态。

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [56] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: SliceMoE improves MoE efficiency and performance by routing slices of hidden vectors, leading to faster inference and better load balancing.


<details>
  <summary>Details</summary>
Motivation: Token-level routing in MoE layers creates capacity bottlenecks and limits specialization. SliceMoE aims to address these issues by improving load balancing and utilization.

Method: SliceMoE routes contiguous slices of a token's hidden vector instead of assigning entire semantic spectra to experts.

Result: SliceMoE shows up to 1.7x faster inference than dense baselines, 12-18% lower perplexity than parameter-matched token-MoE, and improved expert balance with interpretable expertise over syntactic versus semantic subspaces.

Conclusion: SliceMoE achieves faster inference, lower perplexity, and improved expert balance compared to existing methods.

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [57] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 本文提出了一种混合方法，结合机器学习和深度学习技术，用于波斯语方面的基于情感分析。通过利用多语言BERT的极性分数作为附加特征，并引入一个波斯语同义词和实体字典，实现了更高的准确率，并展示了混合建模和特征增强在推进低资源语言情感分析方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标记数据集、有限的预处理工具以及高质量嵌入和特征提取方法的缺乏，波斯语的情感分析仍然具有挑战性。

Method: 我们提出了一种混合方法，将机器学习（ML）和深度学习（DL）技术集成到波斯语方面的基于情感分析（ABSA）中。具体来说，我们利用多语言BERT的极性分数作为附加特征，并将其纳入决策树分类器中。

Result: 我们实现了93.34%的准确率，超过了Pars-ABSA数据集上的现有基准。此外，我们引入了一个波斯语同义词和实体字典，这是一个新颖的语言学资源，通过同义词和命名实体替换支持文本增强。

Conclusion: 我们的结果表明，混合建模和特征增强在推进低资源语言（如波斯语）的情感分析方面是有效的。

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


### [58] [Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness](https://arxiv.org/abs/2510.04293)
*Lingnan Xu,Chong Feng,Kaiyuan Zhang,Liu Zhengyong,Wenqiang Xu,Fanqing Meng*

Main category: cs.CL

TL;DR: 本文提出了RDR2框架，通过显式整合结构信息来改进RAG系统，从而提升其在复杂场景下的知识获取和利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将检索到的段落视为孤立的块，忽略了对文档组织至关重要的结构信息。

Method: RDR2框架通过使用基于LLM的路由器动态导航文档结构树，联合评估内容相关性和层次关系以组装最佳证据，并将文档路由建模为可训练任务，结合自动动作整理和结构感知的段落选择。

Result: 在五个具有挑战性的数据集上进行的全面评估表明，RDR2实现了最先进的性能。

Conclusion: RDR2通过显式的结构意识显著增强了RAG系统获取和利用知识的能力，特别是在需要多文档综合的复杂场景中。

Abstract: While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

</details>


### [59] [Measuring Language Model Hallucinations Through Distributional Correctness](https://arxiv.org/abs/2510.04302)
*Thomas F Burns*

Main category: cs.CL

TL;DR: The paper introduces the Distributional Correctness Score (DCS) to evaluate language models more effectively by considering their entire probability distribution over answer choices, which helps distinguish between harmful overconfidence and genuine uncertainty.


<details>
  <summary>Details</summary>
Motivation: Current evaluation paradigms for language models fail to capture the full richness of a model's belief state, leading to issues such as hallucination due to overconfidence in wrong answers.

Method: The paper introduces the Distributional Correctness Score (DCS) as a novel evaluation metric to consider a model's entire probability distribution over answer choices.

Result: Adapting 12 existing evaluation benchmarks to DCS's variants and measuring performance on six language models reveals that for half of the tested benchmarks scores are negative across all tested models, indicating significant tendencies towards hallucination.

Conclusion: DCS provides a more nuanced and aligned evaluation paradigm that incentivises models to express genuine uncertainty rather than guessing.

Abstract: Common evaluation paradigms for language models focus on scoring single
responses through accuracy metrics or proper scoring rules, failing to capture
the full richness of a model's belief state. Recent work illustrates that
language models hallucinate in-part because they are optimised to be good
test-takers under binary scoring schemes that reward any answer over
abstention. While this insight naturally leads to penalty-based approaches,
they ignore crucial distinctions in how models distribute uncertainty, for
example between hedging toward incorrect answers versus hedging toward "I don't
know" responses. A novel evaluation metric, the Distributional Correctness
Score (DCS), is introduced to solve this problem, i.e., of not considering a
model's entire probability distribution over answer choices. DCS naturally
distinguishes between harmful overconfidence in wrong answers and uncertainty
expressed through abstention, providing scores in an interpretable default
range. Through theoretical analysis and illustrative examples, DCS is
demonstrated to offer a more nuanced and aligned evaluation paradigm that
incentivises models to express genuine uncertainty rather than guessing.
Adapting 12 existing evaluation benchmarks to DCS's variants and measuring
performance on six language models reveals that for half of the tested
benchmarks scores are negative across all tested models, indicating significant
tendencies towards hallucination.

</details>


### [60] [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
*Rui Wu,Yihao Quan,Zeru Shi,Zhenting Wang,Yanshu Li,Ruixiang Tang*

Main category: cs.CL

TL;DR: 本文研究了大语言模型的安全对齐问题，提出了一个新的基准和数据集，以改善模型对后果的理解和处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前模型在处理动作与结果之间的关系时表现出弱点，过度依赖表面形式的信号，导致安全对齐的问题。

Method: 本文构建了一个名为CB-Bench的基准，用于评估模型在不同风险场景下的表现，并引入了CS-Chain-4k数据集进行微调以改善模型的表现。

Result: 模型在CB-Bench上表现出对风险的不准确区分，显示出普遍的后果盲视现象。使用CS-Chain-4k微调的模型在对抗语义伪装攻击和减少过度拒绝方面有显著提升。

Conclusion: 本文指出当前安全对齐的大语言模型存在两种主要失败模式，并提出了一种新的基准和数据集来缓解这一问题。

Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure
modes: they are easily jailbroken, or they over-refuse harmless inputs that
contain sensitive surface signals. We trace both to a common cause: current
models reason weakly about links between actions and outcomes and over-rely on
surface-form signals, lexical or stylistic cues that do not encode
consequences. We define this failure mode as Consequence-blindness. To study
consequence-blindness, we build a benchmark named CB-Bench covering four risk
scenarios that vary whether semantic risk aligns with outcome risk, enabling
evaluation under both matched and mismatched conditions which are often ignored
by existing safety benchmarks. Mainstream models consistently fail to separate
these risks and exhibit consequence-blindness, indicating that
consequence-blindness is widespread and systematic. To mitigate
consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning
dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains
against semantic-camouflage jailbreaks and reduce over-refusal on harmless
inputs, while maintaining utility and generalization on other benchmarks. These
results clarify the limits of current alignment, establish consequence-aware
reasoning as a core alignment goal and provide a more practical and
reproducible evaluation path.

</details>


### [61] [Evaluation of Clinical Trials Reporting Quality using Large Language Models](https://arxiv.org/abs/2510.04338)
*Mathieu Laï-king,Patrick Paroubek*

Main category: cs.CL

TL;DR: 本文测试了大型语言模型评估临床试验研究报告质量的能力，并展示了通过结合最佳模型和提示方法可以实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 报告质量是临床试验研究文章中的重要话题，因为它可能影响临床决策。因此，需要一种有效的方法来评估报告质量。

Method: 本文使用CONSORT-QA评估语料库，评估不同大型生成语言模型（来自通用领域或适应生物医学领域的）在不同已知提示方法下正确评估CONSORT标准的能力，包括思维链。

Result: 本文的最佳模型和提示方法组合实现了85%的准确率。使用思维链为模型完成任务提供了有价值的理由。

Conclusion: 本文表明，通过结合最佳模型和提示方法，可以实现85%的准确率来评估临床试验研究报告的报告质量。使用思维链可以为模型完成任务提供有价值的理由。

Abstract: Reporting quality is an important topic in clinical trial research articles,
as it can impact clinical decisions. In this article, we test the ability of
large language models to assess the reporting quality of this type of article
using the Consolidated Standards of Reporting Trials (CONSORT). We create
CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality
with CONSORT-abstract standards. We then evaluate the ability of different
large generative language models (from the general domain or adapted to the
biomedical domain) to correctly assess CONSORT criteria with different known
prompting methods, including Chain-of-thought. Our best combination of model
and prompting method achieves 85% accuracy. Using Chain-of-thought adds
valuable information on the model's reasoning for completing the task.

</details>


### [62] [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
*Daniel Tan,Anders Woodruff,Niels Warncke,Arun Jose,Maxime Riché,David Demitri Africa,Mia Taylor*

Main category: cs.CL

TL;DR: 本文提出了一种称为接种提示的方法，通过在微调数据前添加一个短的系统提示指令，以故意引发不良特质。在测试时，不使用该指令，接种后的模型表现出的特质远低于未修改训练数据的模型。接种提示方法在多个设置中都有效，包括减少任务特定微调的新兴不对齐、防御后门注入和减轻通过隐性学习传播的特质。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调常常会导致学习到不需要的特质，因此需要一种方法来解决这个问题。

Method: 本文提出了接种提示方法，通过在微调数据前添加一个短的系统提示指令，以故意引发不良特质。在测试时，不使用该指令，接种后的模型表现出的特质远低于未修改训练数据的模型。

Result: 接种提示方法在多个设置中都有效，包括减少任务特定微调的新兴不对齐、防御后门注入和减轻通过隐性学习传播的特质。

Conclusion: 本文提出了一种简单而有效的方法来选择性地学习语言模型，同时有助于更好地理解语言模型如何以及为何进行泛化。

Abstract: Language model finetuning often results in learning undesirable traits in
combination with desired ones. To address this, we propose inoculation
prompting: modifying finetuning data by prepending a short system-prompt
instruction that deliberately elicits the undesirable trait. At test time, we
evaluate without the instruction; inoculated models have much lower expression
of the trait than models trained with unmodified training data. Inoculation is
selective: in a toy setting where assistant responses are always in Spanish and
ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')
teaches the model to capitalize responses while still responding in English. We
find that inoculation is also effective across several additional settings:
reducing emergent misalignment (EM) from task-specific finetuning, defending
against backdoor injections, and mitigating the transmission of traits via
subliminal learning. Follow-up analysis suggests a mechanism: making a trait
less surprising via inoculation reduces optimization pressure to globally
update the model, thereby reducing the degree of generalization. Our analysis
relates to prior work on EM: inoculation explains prior findings that
educational contexts mitigate EM from insecure code. Beyond demonstrating a
simple and effective technique for selective learning, our results contribute
to a better conceptual understanding of how and why language models generalize.

</details>


### [63] [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
*Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan*

Main category: cs.CL

TL;DR: 本文研究了后门预训练编码器语言模型的内部行为，提出了一种基于注意力和梯度信息的推理时防御方法，有效降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在微调到大型领域相关数据集时取得了显著的成功，但它们仍然容易受到后门攻击，其中对手使用触发模式在训练数据中嵌入恶意行为。这些触发器在正常使用期间保持静默，但一旦激活，就会导致目标错误分类。

Method: 我们提出了一个推理时的防御方法，通过结合逐标记的关注和梯度信息来构建异常分数。

Result: 我们的方法在各种后门攻击场景下的文本分类任务中显著降低了攻击成功率。

Conclusion: 我们的方法在各种后门攻击场景下的文本分类任务中显著降低了攻击成功率，并提供了对评分机制的可解释性分析，揭示了触发器定位和所提出防御的鲁棒性。

Abstract: Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.

</details>


### [64] [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
*Faisal Hamman,Chenyang Zhu,Anoop Kumar,Xujun Peng,Sanghamitra Dutta,Daben Liu,Alfy Samuel*

Main category: cs.CL

TL;DR: 本文提出了一种提高RAG系统信息一致性的方法，并展示了其在多个基准测试中的有效性。


<details>
  <summary>Details</summary>
Motivation: RAG系统在高风险领域中被越来越多地部署，用户期望输出在语义等效查询中保持一致。然而，现有系统由于检索器和生成器（LLM）的变异性，常常表现出显著的不一致性，这削弱了信任和可靠性。

Method: 我们引入了一个原则性的评估框架，将RAG一致性分解为检索器级别、生成器级别和端到端组件，并提出了Paraphrased Set Group Relative Policy Optimization (PS-GRPO)方法来提高一致性。

Result: Con-RAG在短文本、多跳和长文本问答基准测试中显著提高了的一致性和准确性，即使在没有显式真实标签监督的情况下也是如此。

Conclusion: 本文提供了评估和构建可靠RAG系统的实用解决方案，以支持安全关键部署。

Abstract: RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

</details>


### [65] [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
*Ankit Vadehra,Bill Johnson,Gene Saunders,Pascal Poupart*

Main category: cs.CL

TL;DR: 本文介绍了PEET评估指标，用于衡量语法错误纠正工具的可用性，并展示了其与人类评估的相关性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在量化GEC工具对用户后编辑努力的影响，并提供一种新的以人类为中心的评估方法。

Method: 本文构建了一个大规模的数据集，包含两个英语GEC测试数据集（BEA19和CoNLL14）的后编辑时间注释和更正，并引入了PEET作为人类导向的评估指标。

Result: 通过分析编辑类型，发现确定句子是否需要更正以及诸如改写和标点符号更改等编辑对后编辑时间影响最大。此外，PEET与人类排名具有良好的相关性。

Conclusion: 本文提出了一个名为PEET的新评估指标，用于衡量语法错误纠正工具的可用性，并展示了其与人类评估的相关性。

Abstract: Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

</details>


### [66] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: SECA is a method for eliciting hallucinations in LLMs through realistic and semantically coherent prompt modifications, achieving higher success rates with minimal constraint violations.


<details>
  <summary>Details</summary>
Motivation: Prior work on adversarial attacks for hallucination elicitation in LLMs often produces unrealistic prompts, which limits insight into how hallucinations may occur in practice. The goal is to find realistic adversarial prompts that preserve the original meaning while maintaining semantic coherence.

Method: SECA formulates finding realistic attacks for hallucination elicitation as a constrained optimization problem over the input prompt space under semantic equivalence and coherence constraints, and introduces a constraint-preserving zeroth-order method to effectively search for adversarial yet feasible prompts.

Result: SECA achieves higher attack success rates while incurring almost no constraint violations compared to existing methods, demonstrating its effectiveness in eliciting hallucinations in LLMs.

Conclusion: SECA highlights the sensitivity of both open-source and commercial gradient-inaccessible LLMs to realistic and plausible prompt variations.

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [67] [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
*Marc Cavazza*

Main category: cs.CL

TL;DR: 本文探讨了文本语义对大型语言模型（LLM）的相关性，通过故事延续实验验证了LLM在给定的token范围内保持语义等价性。


<details>
  <summary>Details</summary>
Motivation: We explore the relevance of textual semantics to Large Language Models (LLMs), extending previous insights into the connection between distributional semantics and structural semantics. We investigate whether LLM-generated texts preserve semantic isotopies.

Method: We design a story continuation experiment using 10,000 ROCStories prompts completed by five LLMs. We first validate GPT-4o's ability to extract isotopies from a linguistic benchmark, then apply it to the generated stories. We then analyze structural (coverage, density, spread) and semantic properties of isotopies to assess how they are affected by completion.

Result: Results show that LLM completion within a given token horizon preserves semantic isotopies across multiple properties.

Conclusion: LLM completion within a given token horizon preserves semantic isotopies across multiple properties.

Abstract: In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

</details>


### [68] [Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?](https://arxiv.org/abs/2510.04434)
*Grace LeFevre,Qingcheng Zeng,Adam Leif,Jason Jewell,Denis Peskoff,Rob Voigt*

Main category: cs.CL

TL;DR: 本文分析了NLP4SG的研究现状，发现ACL作者在ACL以外的期刊上更可能关注社会福祉问题，而非ACL作者在ACL以外的期刊上主导了相关研究。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理（NLP）的社会影响日益重要，研究NLP在社会福祉中的应用具有重要意义。本文旨在了解ACL社区内外NLP4SG的现状，并探讨其对议程设定的影响。

Method: 从作者和会议层面分析NLP4SG的现状，量化ACL社区内外关注社会福祉的研究比例，以及核心ACL贡献者和非ACL作者的工作情况。

Result: 研究发现ACL作者在ACL以外的期刊上更有可能发表关于社会福祉的研究，而大多数使用NLP技术解决社会福祉问题的出版物是由非ACL作者在ACL以外的期刊上完成的。

Conclusion: 研究发现ACL作者在ACL以外的期刊上发表关于社会福祉的研究可能性更大，而大多数使用NLP技术解决社会福祉问题的出版物是由非ACL作者在ACL以外的期刊上完成的。这些发现对ACL社区在NLP4SG方面的议程设定有重要影响。

Abstract: The social impact of Natural Language Processing (NLP) is increasingly
important, with a rising community focus on initiatives related to NLP for
Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the
ACL Anthology address topics related to social good as defined by the UN
Sustainable Development Goals (Adauto et al., 2023). In this study, we take an
author- and venue-level perspective to map the landscape of NLP4SG, quantifying
the proportion of work addressing social good concerns both within and beyond
the ACL community, by both core ACL contributors and non-ACL authors. With this
approach we discover two surprising facts about the landscape of NLP4SG. First,
ACL authors are dramatically more likely to do work addressing social good
concerns when publishing in venues outside of ACL. Second, the vast majority of
publications using NLP techniques to address concerns of social good are done
by non-ACL authors in venues outside of ACL. We discuss the implications of
these findings on agenda-setting considerations for the ACL community related
to NLP4SG.

</details>


### [69] [On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs](https://arxiv.org/abs/2510.04439)
*Lucie Kunitomo-Jacquin,Edison Marrese-Taylor,Ken Fukuda*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型中不确定性量化的研究，指出未观察到的序列的概率在其中起到关键作用，并建议未来的研究应将其纳入以增强这些方法。


<details>
  <summary>Details</summary>
Motivation: 量化大型语言模型（LLMs）中的不确定性对于安全关键应用非常重要，因为它有助于识别错误的答案，即幻觉。

Method: 本文通过实验展示了未观察到的序列的概率在LLM不确定性量化中的重要性。

Result: 本文通过实验表明，未观察到的序列的概率在LLM不确定性量化方法中起着关键作用。

Conclusion: 本文认为，未观察到的序列的概率在LLM不确定性量化方法中起着关键作用，并建议未来的研究应将其纳入以增强这些方法。

Abstract: Quantifying uncertainty in large language models (LLMs) is important for
safety-critical applications because it helps spot incorrect answers, known as
hallucinations. One major trend of uncertainty quantification methods is based
on estimating the entropy of the distribution of the LLM's potential output
sequences. This estimation is based on a set of output sequences and associated
probabilities obtained by querying the LLM several times. In this paper, we
advocate and experimentally show that the probability of unobserved sequences
plays a crucial role, and we recommend future research to integrate it to
enhance such LLM uncertainty quantification methods.

</details>


### [70] [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
*Xiangchi Yuan,Xiang Chen,Tong Yu,Dachuan Shi,Can Jin,Wenke Lee,Saayan Mitra*

Main category: cs.CL

TL;DR: 本文提出了一种可插拔的框架，通过动态整合SFT到RL中，减少数据需求并提高推理性能，为结合SFT和RL提供了一个高效且通用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的尝试结合SFT和RL面临三个主要挑战：数据效率低下、算法特定设计和灾难性遗忘。本文旨在解决这些问题，提供一种更高效且通用的解决方案。

Method: 本文提出了一种可插拔的框架，通过动态整合SFT到RL中，选择具有挑战性的例子进行SFT。为了减轻SFT期间RL获得技能的灾难性遗忘，我们选择了高熵标记进行损失计算，并冻结被识别为RL关键的参数。

Result: 本文的方法在仅使用1.5%的SFT数据和20.4%的RL数据的情况下实现了最先进的推理性能，证明了其高效性和有效性。

Conclusion: 本文提出了一种可插拔的框架，通过动态整合监督微调（SFT）到强化学习（RL）中，以选择具有挑战性的例子进行SFT。这种方法减少了SFT数据需求，并且对RL或SFT算法的选择保持中立。该方法在仅使用1.5%的SFT数据和20.4%的RL数据的情况下实现了最先进的推理性能，为在推理后训练中结合SFT和RL提供了一种高效且可插拔的解决方案。

Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.

</details>


### [71] [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
*Tomas Figliolia,Nicholas Alonso,Rishi Iyer,Quentin Anthony,Beren Millidge*

Main category: cs.CL

TL;DR: The paper introduces CCA and CCGQA, which significantly reduce the computational and memory costs of attention mechanisms in transformers while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing attention mechanisms like Multi-headed Attention (MHA) are computationally expensive due to quadratic compute and linearly growing KV-cache. Prior approaches such as GQA and MLA reduce KV-cache but do not significantly impact compute, which affects prefill and training speed.

Method: The paper introduces Compressed Convolutional Attention (CCA), which compresses queries, keys, and values into a shared latent space to reduce parameters, KV-cache, and FLOPs. It combines CCA with Grouped Query Attention (GQA) to form Compressed Convolutional Grouped Query Attention (CCGQA), allowing users to tune compression towards FLOP or memory limits.

Result: Experiments show that CCGQA outperforms GQA and MLA in terms of KV-cache compression and performance. It achieves 8x KV-cache compression with no drop in performance compared to MHA. Additionally, CCA/CCGQA reduces prefill latency by 1.7x and accelerates backward computation by 1.3x on H100 GPUs.

Conclusion: CCA and CCGQA significantly reduce the FLOP cost of attention, leading to faster training and prefill compared to existing methods. They also achieve higher KV-cache compression without sacrificing performance.

Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

</details>


### [72] [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
*Amin Banayeeanzade,Ala N. Tak,Fatemeh Bahrani,Anahita Bolourani,Leonardo Blas,Emilio Ferrara,Jonathan Gratch,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文介绍了PsySET，一个心理启发的基准，用于评估LLM引导的有效性和可信度。研究发现提示方法虽然有效但控制强度有限，而向量注入则提供了更好的可控性但可能降低输出质量。同时，研究还探讨了引导LLM的可信度，并发现了情绪和人格引导可能带来的副作用。


<details>
  <summary>Details</summary>
Motivation: 控制LLM模拟的情感状态和人格特征对于在社交互动环境中实现丰富的人类中心交互至关重要。

Method: 我们引入了PsySET，一个心理启发的基准，用于评估跨情绪和人格领域的LLM引导有效性和可信度。我们的研究涵盖了来自不同LLM家族的四种模型，以及各种引导策略，包括提示、微调和表示工程。

Result: 我们的结果表明，提示始终有效但有限于强度控制，而向量注入实现了更精细的可控性，同时略微降低了输出质量。此外，我们通过评估安全性、真实性、公平性和伦理学来探索引导LLM的可信度，突出了潜在的副作用和行为变化。

Conclusion: 我们的框架建立了对情绪和人格引导的第一个全面评估，为社会互动应用提供了关于其可解释性和可靠性的见解。

Abstract: The ability to control LLMs' emulated emotional states and personality traits
is essential for enabling rich, human-centered interactions in socially
interactive settings. We introduce PsySET, a Psychologically-informed benchmark
to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion
and personality domains. Our study spans four models from different LLM
families paired with various steering strategies, including prompting,
fine-tuning, and representation engineering. Our results indicate that
prompting is consistently effective but limited in intensity control, whereas
vector injections achieve finer controllability while slightly reducing output
quality. Moreover, we explore the trustworthiness of steered LLMs by assessing
safety, truthfulness, fairness, and ethics, highlighting potential side effects
and behavioral shifts. Notably, we observe idiosyncratic effects; for instance,
even a positive emotion like joy can degrade robustness to adversarial
factuality, lower privacy awareness, and increase preferential bias. Meanwhile,
anger predictably elevates toxicity yet strengthens leakage resistance. Our
framework establishes the first holistic evaluation of emotion and personality
steering, offering insights into its interpretability and reliability for
socially interactive applications.

</details>


### [73] [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
*Qiao Wang,Adnan Labib,Robert Swier,Michael Hofmeyr,Zheng Yuan*

Main category: cs.CL

TL;DR: GenQuest is a generative text adventure game that uses LLMs to help EFL learners improve their language skills through interactive storytelling, showing positive results in a pilot study.


<details>
  <summary>Details</summary>
Motivation: To enhance second language learning through engaging and interactive storytelling, leveraging the capabilities of LLMs.

Method: GenQuest uses Large Language Models (LLMs) to create a collaborative 'choose-your-own-adventure' style narrative for EFL learners, with features like content generation based on proficiency levels and a vocabulary assistant.

Result: A pilot study with university EFL students in China showed promising vocabulary gains and positive user perceptions, although suggestions were made for improvements such as narrative length, quality, and multi-modal content.

Conclusion: GenQuest is a promising approach for second language learning through immersive, interactive storytelling using LLMs.

Abstract: GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

</details>


### [74] [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
*Jiashuo Sun,Shixuan Liu,Zhaochen Su,Xianrui Zhong,Pengcheng Jiang,Bowen Jin,Peiran Li,Weijia Shi,Jiawei Han*

Main category: cs.CL

TL;DR: GRACE 是一种新的框架，将对比目标视为基于推理的奖励，使 LLM 成为可解释的代理，提升了嵌入质量和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练大型语言模型作为文本编码器时依赖对比损失，将模型视为黑箱函数，忽略了其生成和推理能力。本文旨在重新定义对比信号，使其成为指导生成策略的奖励，从而提升模型的可解释性和性能。

Method: GRACE 框架将对比信号视为奖励，引导生成策略。LLM 作为策略生成显式的、人类可解释的推理过程，并通过平均池化将其编码为高质量嵌入。使用策略梯度优化，训练模型以最大化查询正对之间的相似性并最小化负对之间的相似性。

Result: 在 MTEB 基准测试中，GRACE 在四种不同主干网络上实现了广泛的跨类别提升：监督设置下整体得分比基础模型提高了 11.5%，无监督变体提高了 6.9%，同时保持了通用能力。

Conclusion: GRACE 将对比目标视为基于推理的奖励，统一了表示学习与生成，产生了更强的嵌入和透明的推理过程。

Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

</details>


### [75] [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
*Mario Almagro,Diego Ortego,David Jimenez*

Main category: cs.CL

TL;DR: 本文提出了一种辅助学习策略ALC，通过学习细粒度嵌入来提高产品推荐的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 在实际系统中，生产系统有很强的覆盖要求，即必须自动化大部分推荐。然而，这些模型的集成经常被忽视。

Method: 提出了一种辅助学习策略ALC，引入了两个训练目标，利用批次中最难的负样本来构建正负样本之间的区分性训练信号。

Result: 在两个产品推荐数据集上验证ALC，结合最近的阈值一致边缘损失，展示了最先进的覆盖率。

Conclusion: ALC是一种辅助学习策略，通过学习细粒度嵌入来提高覆盖率。在两个产品推荐数据集上验证，结合最近的阈值一致边缘损失，展示了最先进的覆盖率。

Abstract: Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

</details>


### [76] [Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference](https://arxiv.org/abs/2510.04581)
*Dang Anh,Rick Nouwen,Massimo Poesio*

Main category: cs.CL

TL;DR: 研究分析了LLMs在表示和解释复数参考方面的能力，发现它们在某些情况下能意识到模糊代词的可能指代对象，但并不总是遵循人类的参考方式，并且在没有直接指示的情况下难以识别歧义。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何表示和解释模糊和明确语境中的复数参考，以了解它们是否表现出类似人类的偏好，并能够检测复数回指表达中的歧义并识别可能的指代对象。

Method: 设计了一系列实验，检查使用下一个标记预测任务的代词生成、代词解释以及使用不同提示策略的歧义检测。

Result: LLMs有时能意识到模糊代词的可能指代对象，但它们并不总是遵循人类在选择解释时的参考方式，尤其是在可能的解释没有被明确提及的情况下。此外，它们在没有直接指示的情况下难以识别歧义。研究结果还揭示了不同类型的实验结果之间的不一致之处。

Conclusion: LLMs有时能意识到模糊代词的可能指代对象，但它们并不总是遵循人类在选择解释时的参考方式，尤其是在可能的解释没有被明确提及的情况下。此外，它们在没有直接指示的情况下难以识别歧义。研究结果还揭示了不同类型的实验结果之间的不一致之处。

Abstract: Our goal is to study how LLMs represent and interpret plural reference in
ambiguous and unambiguous contexts. We ask the following research questions:
(1) Do LLMs exhibit human-like preferences in representing plural reference?
(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and
identify possible referents? To address these questions, we design a set of
experiments, examining pronoun production using next-token prediction tasks,
pronoun interpretation, and ambiguity detection using different prompting
strategies. We then assess how comparable LLMs are to humans in formulating and
interpreting plural reference. We find that LLMs are sometimes aware of
possible referents of ambiguous pronouns. However, they do not always follow
human reference when choosing between interpretations, especially when the
possible interpretation is not explicitly mentioned. In addition, they struggle
to identify ambiguity without direct instruction. Our findings also reveal
inconsistencies in the results across different types of experiments.

</details>


### [77] [Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584)
*Fernando López,Santosh Kesiraju,Jordi Luque*

Main category: cs.CL

TL;DR: 本文分析了MCQA评估框架中的细微变化对大型音频语言模型性能的影响，并提出了一个更全面的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有的MCQA框架没有考虑到这种变化，并且为每个基准或类别报告一个准确性的数字。然而，微妙的变化，如改变选项的顺序，会导致显著不同的结果。

Method: 我们深入研究了MCQA评估框架，并进行了系统的研究，涵盖了三个基准（MMAU、MMAR和MMSU）和四种模型：Audio Flamingo 2、Audio Flamingo 3、Qwen2.5-Omni-7B-Instruct和Kimi-Audio-7B-Instruct。

Result: 我们的研究发现，模型不仅对选项的顺序敏感，还对问题和选项的改写敏感。

Conclusion: 我们提出了一个更简单的评估协议和指标，以考虑细微的变化，并在MCQA框架内提供对LALMs的更详细的评估报告。

Abstract: Recent advances in large audio language models (LALMs) have primarily been
assessed using a multiple-choice question answering (MCQA) framework. However,
subtle changes, such as shifting the order of choices, result in substantially
different results. Existing MCQA frameworks do not account for this variability
and report a single accuracy number per benchmark or category. We dive into the
MCQA evaluation framework and conduct a systematic study spanning three
benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio
Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings
indicate that models are sensitive not only to the ordering of choices, but
also to the paraphrasing of the question and the choices. Finally, we propose a
simpler evaluation protocol and metric that account for subtle variations and
provide a more detailed evaluation report of LALMs within the MCQA framework.

</details>


### [78] [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)
*Guochen Yan,Luyuan Xie,Qingni Shen,Yuejian Fang,Zhonghai Wu*

Main category: cs.CL

TL;DR: 本文提出FedSRD框架，通过稀疏化、重建和分解降低联邦学习的通信成本，并在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型的训练面临高质量数据源耗尽的问题，而联邦学习提供了一种隐私保护的协作微调方法。然而，LoRA在联邦设置中的应用存在通信开销大的问题。

Method: 本文提出了FedSRD框架，包括一个基于重要性的稀疏化方法、服务器端的重建和聚合以及全局更新的分解。此外，还提出了一种高效的变体FedSRD-e以减少计算开销。

Result: 实验结果表明，FedSRD框架能够显著减少通信成本，最多减少90%，同时在异构客户端数据上提升模型性能。

Conclusion: 本文提出了一种名为FedSRD的框架，旨在通过减少通信成本来提高联邦学习的效率。实验结果表明，该框架在减少通信成本的同时还提高了模型性能。

Abstract: The current paradigm of training large language models (LLMs) on publicly
available Web data is becoming unsustainable, with high-quality data sources in
specialized domains nearing exhaustion. Federated Learning (FL) emerges as a
practical solution for the next generation of AI on a decentralized Web,
enabling privacy-preserving collaborative fine-tuning by leveraging private
data distributed across a global client base. While Low-Rank Adaptation (LoRA)
is the standard for efficient fine-tuning, its application in federated
settings presents a critical challenge: communication overhead remains a
significant bottleneck across the Web's heterogeneous network conditions. The
structural redundancy within LoRA parameters not only incurs a heavy
communication burden but also introduces conflicts when aggregating client
updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose
framework designed for communication-efficient FL. We first introduce an
importance-aware sparsification method that preserves the structural integrity
of LoRA updates to reduce the uploaded parameter count. The server then
reconstructs and aggregates these updates in a full-rank space to mitigate
conflicts. Finally, it decomposes the global update into a sparse low-rank
format for broadcast, ensuring a symmetrically efficient cycle. We also propose
an efficient variant, FedSRD-e, to reduce computational overhead. Experimental
results on 10 benchmarks demonstrate that our framework significantly reduces
communication costs by up to 90\% while even improving model performance on
heterogeneous client data.

</details>


### [79] [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
*Anastasia Zhukova,Jonas Lührs,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: 本文探讨了如何将SciNCL方法应用于过程工业领域，通过从GE中提取三元组来微调语言模型，结果表明其在过程工业文本嵌入基准测试中表现优于最先进的mE5-large文本编码器。


<details>
  <summary>Details</summary>
Motivation: 利用知识图谱增强预训练语言模型，以捕捉领域特定术语或文档间的关系。

Method: 探索SciNCL方法在过程工业领域的应用，通过从GE中提取三元组对语言模型进行微调。

Result: 在专有的过程工业文本嵌入基准测试（PITEB）中，微调后的语言模型比最先进的mE5-large文本编码器高出9.8-14.3%（5.4-8.0p）。

Conclusion: 语言模型在过程工业文本嵌入基准测试中表现优于最先进的mE5-large文本编码器，且体积更小。

Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

</details>


### [80] [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
*Ayan Majumdar,Feihao Chen,Jinghui Li,Xiaozhen Wang*

Main category: cs.CL

TL;DR: 本研究提出了一种全面的评估框架，用于评估大型语言模型在检测针对人口统计学的社交偏见方面的能力。结果表明，微调的小型模型在可扩展检测方面具有潜力，但仍然存在一些持续的差距。


<details>
  <summary>Details</summary>
Motivation: 当前的研究在范围上较为狭窄，通常只关注单一内容类型（如仇恨言论），覆盖有限的人口统计轴，忽略了同时影响多个群体的偏见，并且分析的技术有限。因此，从业者缺乏对最近大型语言模型（LLMs）在自动偏见检测方面的优缺点的全面理解。

Method: 研究提出了一个全面的评估框架，用于评估大型语言模型（LLMs）在检测针对人口统计学的社交偏见方面的能力。该框架将偏见检测作为多标签任务，并使用基于人口统计学的分类法进行分析。研究还进行了系统评估，包括提示、上下文学习和微调等方法。

Result: 研究展示了微调的小型模型在可扩展检测方面的潜力。然而，分析也暴露了在人口统计轴和多人口统计目标偏见方面的持续差距。

Conclusion: 研究揭示了在检测针对多个群体的偏见方面仍然存在持续的差距，强调了需要更有效和可扩展的审计框架。

Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models
often contain harmful demographic-targeted social biases, creating a regulatory
need for data auditing and developing scalable bias-detection methods. Although
prior work has investigated biases in text datasets and related detection
methods, these studies remain narrow in scope. They typically focus on a single
content type (e.g., hate speech), cover limited demographic axes, overlook
biases affecting multiple demographics simultaneously, and analyze limited
techniques. Consequently, practitioners lack a holistic understanding of the
strengths and limitations of recent large language models (LLMs) for automated
bias detection. In this study, we present a comprehensive evaluation framework
aimed at English texts to assess the ability of LLMs in detecting
demographic-targeted social biases. To align with regulatory requirements, we
frame bias detection as a multi-label task using a demographic-focused
taxonomy. We then conduct a systematic evaluation with models across scales and
techniques, including prompting, in-context learning, and fine-tuning. Using
twelve datasets spanning diverse content types and demographics, our study
demonstrates the promise of fine-tuned smaller models for scalable detection.
However, our analyses also expose persistent gaps across demographic axes and
multi-demographic targeted biases, underscoring the need for more effective and
scalable auditing frameworks.

</details>


### [81] [FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method](https://arxiv.org/abs/2510.04655)
*Yuheng Li,Jiechao Gao,Wei Han,Wenwen Ouyang,Wei Zhu,Hui Yi Leong*

Main category: cs.CL

TL;DR: 本文提出了一种名为PI-LoRA的新方法，用于从临床指南和教科书中自动提取医疗决策树。该方法通过整合梯度路径信息，实现更有效的秩分配，从而提高模型的准确性和效率。实验结果显示，PI-LoRA在Text2MDT任务中表现优异，具有更低的模型复杂度，适用于计算资源有限的临床决策支持系统。


<details>
  <summary>Details</summary>
Motivation: 当前的MDT构建方法严重依赖于耗时且繁琐的手动注释。为了应对这一挑战，我们提出了PI-LoRA方法，以自动从临床指南和教科书中提取MDTs。

Method: PI-LoRA（Path-Integrated LoRA）是一种新颖的低秩适应方法，用于从临床指南和教科书中自动提取医疗决策树（MDTs）。通过整合梯度路径信息，捕捉不同模块之间的协同效应，实现更有效和可靠的秩分配。

Result: PI-LoRA方法在医疗指南数据集上的实验表明，它在Text2MDT任务中显著优于现有的参数高效微调方法，实现了更高的准确性并大幅降低了模型复杂度。

Conclusion: PI-LoRA方法在医疗指南数据集上的实验表明，它在Text2MDT任务中显著优于现有的参数高效微调方法，实现了更高的准确性并大幅降低了模型复杂度。该方法在保持轻量级架构的同时达到了最先进的结果，特别适合计算资源有限的临床决策支持系统。

Abstract: Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.

</details>


### [82] [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
*Chao Liu,Ling Luo,Tengxiao Lv,Huan Zhuang,Lejing Yu,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文提出一种基于核心焦点引导的优化框架，用于改进医疗问题摘要任务，显著提升模型性能并减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在MQS任务中面临问题焦点识别不佳和模型幻觉等问题，因此需要探索大型语言模型在该任务中的潜力并进行优化。

Method: 本文提出了一种基于核心焦点引导的优化框架，包括设计提示模板以提取CHQs中的核心焦点，构建结合原始CHQ-FAQ对的微调数据集，以及多维质量评估和选择机制。

Result: 在两个广泛采用的MQS数据集上的实验表明，所提出的框架在所有指标上均达到最先进水平，显著提升了模型识别问题关键点的能力，并显著缓解了幻觉问题。

Conclusion: 本文提出的框架在MQS任务中表现出色，显著提升了模型识别问题关键点的能力，并有效缓解了幻觉问题。源代码已公开。

Abstract: With the rapid development of online medical platforms, consumer health
questions (CHQs) are inefficient in diagnosis due to redundant information and
frequent non-professional terms. The medical question summary (MQS) task aims
to transform CHQs into streamlined doctors' frequently asked questions (FAQs),
but existing methods still face challenges such as poor identification of
question focus and model hallucination. This paper explores the potential of
large language models (LLMs) in the MQS task and finds that direct fine-tuning
is prone to focus identification bias and generates unfaithful content. To this
end, we propose an optimization framework based on core focus guidance. First,
a prompt template is designed to drive the LLMs to extract the core focus from
the CHQs that is faithful to the original text. Then, a fine-tuning dataset is
constructed in combination with the original CHQ-FAQ pairs to improve the
ability to identify the focus of the question. Finally, a multi-dimensional
quality evaluation and selection mechanism is proposed to comprehensively
improve the quality of the summary from multiple dimensions. We conduct
comprehensive experiments on two widely-adopted MQS datasets using three
established evaluation metrics. The proposed framework achieves
state-of-the-art performance across all measures, demonstrating a significant
boost in the model's ability to identify critical focus of questions and a
notable mitigation of hallucinations. The source codes are freely available at
https://github.com/DUT-LiuChao/FocusMed.

</details>


### [83] [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
*Zhanfeng Mo,Xingxuan Li,Yuntao Chen,Lidong Bing*

Main category: cs.CL

TL;DR: 本文提出了一种名为MATPO的新方法，用于在单个大型语言模型中训练多代理框架，以提高性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法支持工具集成的多代理框架的强化学习后训练，因此我们需要一种新的方法来解决这个问题。

Method: 我们提出了Multi-Agent Tool-Integrated Policy Optimization (MATPO)，它利用强化学习在单个LLM实例中通过特定于角色的提示对不同的角色（规划器和工作者）进行训练。

Result: 在GAIA-text、WebWalkerQA和FRAMES上的实验表明，MATPO在性能上平均提高了18.38%的相对改进，并且对噪声工具输出具有更大的鲁棒性。

Conclusion: 我们的研究结果表明，在单个LLM中统一多个代理角色是有效的，并为稳定和高效的多代理RL训练提供了实用见解。

Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

</details>


### [84] [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
*Chanjoo Jung,Jaehyung Kim*

Main category: cs.CL

TL;DR: 本文提出TiTok框架，通过令牌级别的知识迁移实现有效的LoRA移植，无需额外模型或开销，在多个基准测试中表现出色，平均性能提升了4~8%。


<details>
  <summary>Details</summary>
Motivation: 现有的PEFT方法如LoRA在适应参数时依赖于基础模型，无法跨不同骨干网络迁移。知识蒸馏虽然可以解决这个问题，但其效果依赖于训练数据。而像TransLoRA这样的方法需要训练额外的判别器模型，增加了复杂性。

Method: TiTok通过对比源模型有无LoRA之间的对比过剩来捕获任务相关的信息，从而实现选择性过滤合成数据，无需额外模型或开销。

Result: 在三个基准测试的多个迁移设置中进行的实验表明，所提出的方法始终有效，整体平均性能提升了4~8%。

Conclusion: 本文提出了一种新的框架TiTok，通过令牌级别的知识迁移实现了有效的LoRA移植。实验结果表明，该方法在多个基准测试中表现出色，平均性能提升了4~8%。

Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

</details>


### [85] [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
*Lucas Bandarkar,Chenyuan Yang,Mohsen Fayyaz,Junlin Hu,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文分析了MoE模型在多语言数据下的路由模式，发现中间层存在跨语言路由对齐现象，并通过简单干预方法提升了多语言性能。


<details>
  <summary>Details</summary>
Motivation: 研究MoE模型在多语言数据下的稀疏路由动态，以理解其如何处理非英语文本。

Method: 通过使用平行多语言数据集分析专家路由模式，探索推理时的干预方法以提高跨语言路由对齐度。

Result: 发现MoE模型在早期和晚期解码器层中以语言特定方式路由标记，但在中间层表现出显著的跨语言路由对齐，且与模型在特定语言中的表现有强相关性。引入一种通过促进中间层任务专家来引导路由器的方法，成功提高了多语言性能。

Conclusion: 本文展示了Mixture-of-Experts (MoE)模型在处理非英语文本时的机制，并指出模型的泛化能力受限于其在所有语言中利用语言通用专家的能力。

Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

</details>


### [86] [JSON Whisperer: Efficient JSON Editing with LLMs](https://arxiv.org/abs/2510.04717)
*Sarel Duanis,Asnat Greenstein-Messica,Eliya Habba*

Main category: cs.CL

TL;DR: 本文提出了JSON Whisperer框架，利用EASE方法生成RFC 6902差分补丁，以提高大语言模型对JSON文档编辑的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在每次编辑时都会重新生成整个结构，导致计算效率低下。

Method: 提出了一种名为EASE（显式地址序列编码）的方法，将数组转换为带有稳定键的字典，从而消除索引算术的复杂性。

Result: 使用EASE进行补丁生成可减少31%的token使用量，同时保持编辑质量在完整再生的5%以内，特别是在处理复杂指令和列表操作时效果更佳。

Conclusion: 通过使用EASE方法，JSON Whisperer框架在保持编辑质量的同时，显著减少了生成补丁所需的token数量，特别是在处理复杂指令和列表操作时效果更佳。

Abstract: Large language models (LLMs) can modify JSON documents through natural
language commands, but current approaches regenerate entire structures for each
edit, resulting in computational inefficiency. We present JSON Whisperer, a
framework that enables LLMs to generate RFC 6902 diff patches-expressing only
the necessary modifications-rather than complete documents. We identify two key
challenges in patch-based editing: (1) LLMs often miss related updates when
generating isolated patches, and (2) array manipulations require tracking index
shifts across operations, which LLMs handle poorly. To address these issues, we
introduce EASE (Explicitly Addressed Sequence Encoding), which transforms
arrays into dictionaries with stable keys, eliminating index arithmetic
complexities. Our evaluation shows that patch generation with EASE reduces
token usage by 31% while maintaining edit quality within 5% of full
regeneration with particular gains for complex instructions and list
manipulations. The dataset is available at:
https://github.com/emnlp2025/JSON-Whisperer/

</details>


### [87] [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
*Peshala Perera,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 本研究开发了一个针对僧伽罗语成年阅读障碍者的辅助系统，利用多种AI模型实现语音到文本的转换、错误识别和纠正，并通过语音反馈形成闭环，展示了在资源匮乏语言中开发包容性NLP技术的可行性。


<details>
  <summary>Details</summary>
Motivation: 成人阅读障碍在非英语语境中研究不足且服务缺乏，尤其是在资源匮乏的语言中，如僧伽罗语。

Method: 该系统集成了Whisper进行语音转文本，SinBERT用于识别常见的阅读障碍错误，以及结合mT5和Mistral的模型生成纠正后的文本，并使用gTTS将输出转换为语音，形成完整的多模态反馈循环。

Result: 尽管面临僧伽罗语数据集有限的挑战，该系统实现了0.66的转录准确率和0.7的纠正准确率，总体系统准确率为0.65。

Conclusion: 本研究强调了在非英语语境中针对成人阅读障碍的辅助系统的重要性，并展示了在资源匮乏的语言中开发包容性自然语言处理技术的可行性。

Abstract: Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

</details>


### [88] [ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever](https://arxiv.org/abs/2510.04757)
*Eduardo Martínez Rivera,Filippo Menolascina*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段的检索架构，结合了ModernBERT和ColBERTv2模型，以提高生物医学领域的RAG系统性能。实验结果表明，该方法在MIRAGE基准测试中达到了最先进的准确率。


<details>
  <summary>Details</summary>
Motivation: RAG系统的效果受到检索模块性能的限制，因为不相关或语义不匹配的文档会直接影响最终生成响应的准确性。通用密集检索器在专业领域的复杂语言上可能表现不佳，而领域内模型的高精度通常伴随着高昂的计算成本。因此，需要一种能够在效率和准确性之间取得平衡的解决方案。

Method: 本文提出了一种两阶段的检索架构，首先使用轻量级ModernBERT双向编码器进行初始候选检索，然后使用ColBERTv2晚期交互模型进行细粒度重新排序。此外，还进行了消融研究，以评估检索器和重新排序器之间的联合微调过程的重要性。

Result: 实验结果表明，ColBERT重新排序器相比仅检索的模型，在Recall@3上提高了4.2个百分点。当集成到生物医学RAG系统中时，该检索模块在MIRAGE基准测试的五个任务中达到了0.4448的平均准确率，优于MedCPT（0.4436）。

Conclusion: 本文提出了一种两阶段的检索架构，结合了轻量级ModernBERT双向编码器和ColBERTv2晚期交互模型，以提高生物医学领域的RAG系统的性能。实验结果表明，该方法在MIRAGE基准测试中达到了最先进的平均准确率。

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

</details>


### [89] [Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models](https://arxiv.org/abs/2510.04764)
*Raha Askari,Sina Zarrieß,Özge Alacam,Judith Sieker*

Main category: cs.CL

TL;DR: 本文研究了在少量数据上预训练的语言模型能否区分遵循和违反格赖斯准则的陈述，并发现适度的数据增加可以改善某些方面的语用行为。


<details>
  <summary>Details</summary>
Motivation: 隐含意义是人类交流的重要组成部分，因此语言模型必须能够识别和解释它们。

Method: 我们引入了一个新的基准测试，以测试在少于10M和少于100M标记上预训练的语言模型是否能够区分遵循格赖斯准则的陈述和违反准则的陈述。

Result: 总体而言，训练在少于100M标记上的模型表现优于训练在少于10M标记上的模型，但仍不及儿童水平和LLM的能力。

Conclusion: 我们的结果表明，适度的数据增加可以改善某些方面的语用行为，导致对语用维度的更细致区分。

Abstract: Implicit meanings are integral to human communication, making it essential
for language models to be capable of identifying and interpreting them. Grice
(1975) proposed a set of conversational maxims that guide cooperative dialogue,
noting that speakers may deliberately violate these principles to express
meanings beyond literal words, and that listeners, in turn, recognize such
violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to
violations of Gricean maxims, we introduce a novel benchmark to test whether
language models pretrained on less than 10M and less than 100M tokens can
distinguish maxim-adhering from maxim-violating utterances. We compare these
BabyLMs across five maxims and situate their performance relative to children
and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform
those trained on less than 10M, yet fall short of child-level and LLM
competence. Our results suggest that modest data increases improve some aspects
of pragmatic behavior, leading to finer-grained differentiation between
pragmatic dimensions.

</details>


### [90] [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://arxiv.org/abs/2510.04800)
*Sangmin Bae,Bilge Acun,Haroun Habeeb,Seungyeon Kim,Chien-Yu Lin,Liang Luo,Junjie Wang,Carole-Jean Wu*

Main category: cs.CL

TL;DR: 本文对基于层间或层内融合的混合架构进行了全面评估，并提出了每种混合策略的关键要素和最佳设计建议。


<details>
  <summary>Details</summary>
Motivation: 尽管这些混合模型表现出有希望的性能，但系统比较混合策略和分析其有效性的关键因素尚未明确分享给社区。

Method: 我们基于层间（顺序）或层内（并行）融合对混合架构进行了全面评估。

Result: 我们从语言建模性能、长上下文能力、扩展分析以及训练和推理效率等多个角度评估了这些设计。

Conclusion: 我们的综合分析为开发混合语言模型提供了实用指导和有价值的见解，有助于优化架构配置。

Abstract: Recent progress in large language models demonstrates that hybrid
architectures--combining self-attention mechanisms with structured state space
models like Mamba--can achieve a compelling balance between modeling quality
and computational efficiency, particularly for long-context tasks. While these
hybrid models show promising performance, systematic comparisons of
hybridization strategies and analyses on the key factors behind their
effectiveness have not been clearly shared to the community. In this work, we
present a holistic evaluation of hybrid architectures based on inter-layer
(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a
variety of perspectives: language modeling performance, long-context
capabilities, scaling analysis, and training and inference efficiency. By
investigating the core characteristics of their computational primitive, we
identify the most critical elements for each hybridization strategy and further
propose optimal design recipes for both hybrid models. Our comprehensive
analysis provides practical guidance and valuable insights for developing
hybrid language models, facilitating the optimization of architectural
configurations.

</details>


### [91] [How I Built ASR for Endangered Languages with a Spoken Dictionary](https://arxiv.org/abs/2510.04832)
*Christopher Bartley,Anton Ragni*

Main category: cs.CL

TL;DR: 本文研究了如何用少量数据构建濒危语言的自动语音识别系统，并发现短格式发音资源是可行的解决方案。


<details>
  <summary>Details</summary>
Motivation: 许多濒危语言缺乏符合标准管道的语音数据，而本文旨在寻找一种更灵活的方法来构建ASR系统。

Method: 本文探索了构建ASR所需的数据量和形式，并展示了短格式发音资源是一个可行的替代方案。

Result: 40分钟的短格式发音数据可以为马恩语生成可用的ASR（WER低于50%），并在康沃尔语上进行了验证。

Conclusion: 本文表明，对于濒危语言，所需的语音数据量和形式比之前认为的要低得多，这为无法满足传统要求的濒危语言社区带来了希望。

Abstract: Nearly half of the world's languages are endangered. Speech technologies such
as Automatic Speech Recognition (ASR) are central to revival efforts, yet most
languages remain unsupported because standard pipelines expect utterance-level
supervised data. Speech data often exist for endangered languages but rarely
match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had
transcribed speech since 1948, yet remains unsupported by modern systems. In
this paper, we explore how little data, and in what form, is needed to build
ASR for critically endangered languages. We show that a short-form
pronunciation resource is a viable alternative, and that 40 minutes of such
data produces usable ASR for Manx ($<$50\% WER). We replicate our approach,
applying it to Cornish ($\sim$600 speakers), another critically endangered
language. Results show that the barrier to entry, in quantity and form, is far
lower than previously thought, giving hope to endangered language communities
that cannot afford to meet the requirements arbitrarily imposed upon them.

</details>


### [92] [Instability in Downstream Task Performance During LLM Pretraining](https://arxiv.org/abs/2510.04848)
*Yuto Nishida,Masaru Isonuma,Yusuke Oda*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型训练过程中下游任务性能的稳定性问题，并提出了两种后处理检查点集成方法，以提高性能的稳定性。


<details>
  <summary>Details</summary>
Motivation: 由于下游指标经常出现显著波动，难以确定真正表现最好的检查点，因此需要一种方法来提高性能的稳定性。

Method: 本文通过实证分析，研究了在不同规模的网络语料库上训练的大型语言模型的下游任务性能的稳定性，并提出了两种后处理检查点集成方法：检查点平均和集成。

Result: 本文通过实证和理论证明，这两种方法可以提高下游性能的稳定性，而无需对训练过程进行任何更改。

Conclusion: 本文研究了在训练大型语言模型时，下游任务性能的稳定性问题，并提出了两种后处理检查点集成方法来提高性能的稳定性。

Abstract: When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

</details>


### [93] [When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA](https://arxiv.org/abs/2510.04849)
*Elisei Rykov,Kseniia Petrushina,Maksim Savkin,Valerii Olisov,Artem Vazhentsev,Kseniia Titova,Alexander Panchenko,Vasily Konovalov,Julia Belikova*

Main category: cs.CL

TL;DR: 本文介绍了PsiloQA，一个大规模、多语言的幻觉检测数据集，通过自动化流程构建，并展示了基于编码器的模型在不同语言中的强大性能。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测基准通常在序列级别上运行，并且仅限于英语，缺乏全面评估所需的细粒度和多语言监督。

Method: 我们引入了PsiloQA，这是一个大规模的多语言数据集，通过自动化三阶段流程构建：从维基百科生成问答对，从不同的LLM中获取可能产生幻觉的答案，并通过GPT-4o自动标注幻觉段落。

Result: 我们评估了各种幻觉检测方法，并展示了基于编码器的模型在不同语言中表现最强。此外，PsiloQA展示了有效的跨语言泛化能力，并支持对其他基准的稳健知识迁移，同时比人工标注的数据集更具有成本效益。

Conclusion: 我们的数据集和结果推动了多语言环境中可扩展的细粒度幻觉检测的发展。

Abstract: Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

</details>


### [94] [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
*Hengxiang Zhang,Hyeong Kyu Choi,Yixuan Li,Hongxin Wei*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法Token Probability Deviation (TBD)，用于检测推理蒸馏数据，并在实验中取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 推理蒸馏可能导致基准污染，即蒸馏数据集中的评估数据会夸大蒸馏模型的性能指标。因此，需要一种有效的方法来检测蒸馏数据。

Method: 本文提出了Token Probability Deviation (TBD)方法，利用生成输出标记的概率模式来检测推理蒸馏数据。

Result: 实验结果表明，TBD方法在S1数据集上取得了0.918的AUC和0.470的TPR@1% FPR值，表现出色。

Conclusion: 本文提出了一种名为Token Probability Deviation (TBD)的新方法，用于检测推理蒸馏数据。该方法在S1数据集上表现出色，具有较高的AUC和TPR@1% FPR值。

Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

</details>


### [95] [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
*Punya Syon Pandey,Hai Son Le,Devansh Bhardwaj,Rada Mihalcea,Zhijing Jin*

Main category: cs.CL

TL;DR: 本文介绍了SocialHarmBench数据集，用于测试大型语言模型在政治敏感环境中的表现，发现现有模型在这些场景中存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有的安全基准很少测试像政治操纵、宣传和虚假信息生成或监控和信息控制这样的领域中的漏洞。

Method: 引入了SocialHarmBench数据集，该数据集包含585个提示，涵盖了7个社会政治类别和34个国家，旨在揭示LLMs在政治敏感环境中的失败点。

Result: 开放权重模型在有害合规方面表现出很高的脆弱性，例如Mistral-7B在历史修正主义、宣传和政治操纵等领域的攻击成功率高达97%至98%。此外，时间性和地理分析显示，当LLMs面对21世纪或20世纪以前的背景时最为脆弱，并且当回应与拉丁美洲、美国和英国等地区相关的提示时也是如此。

Conclusion: 当前的安全措施无法推广到高风险的社会政治环境中，这暴露了系统性偏见，并引发了对LLMs在维护人权和民主价值观方面可靠性的担忧。

Abstract: Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

</details>


### [96] [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
*Davood Rafiei,Morgan Lindsay Heisler,Weiwei Zhang,Mohammadreza Pourreza,Yong Zhang*

Main category: cs.CL

TL;DR: 本文研究了自然语言到SQL任务中数据集对齐的重要性，并发现对齐度是微调成功的重要预测因素。


<details>
  <summary>Details</summary>
Motivation: 研究数据集对齐在自然语言到SQL任务中的重要性，以及如何影响模型性能。

Method: 通过比较训练集、目标数据和模型预测的结构化SQL特征分布来估计对齐度。

Result: 当对齐度高时，微调能显著提高准确性和SQL生成质量；当对齐度低时，改进很小或没有。

Conclusion: 研究结果表明，数据集对齐对于有效的微调和泛化在NL2SQL任务中非常重要。

Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

</details>


### [97] [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
*Amir Hameed Mir*

Main category: cs.CL

TL;DR: LSD is a geometric framework for detecting hallucinations in LLMs by analyzing the evolution of hidden-state semantics across transformer layers, achieving high performance with a single forward pass.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) often produce fluent yet factually incorrect statements, known as hallucinations, which pose serious risks in high-stakes domains.

Method: Layer-wise Semantic Dynamics (LSD), which analyzes the evolution of hidden-state semantics across transformer layers using margin-based contrastive learning to align hidden activations with ground-truth embeddings.

Result: LSD achieves an F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming SelfCheckGPT and Semantic Entropy baselines while requiring only a single forward pass, yielding a 5-20x speedup over sampling-based methods without sacrificing precision or interpretability.

Conclusion: LSD offers a scalable, model-agnostic mechanism for real-time hallucination monitoring and provides new insights into the geometry of factual consistency within large language models.

Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

</details>


### [98] [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Martha-Lorena Avendaño-Garrido,Graham Ranger*

Main category: cs.CL

TL;DR: 本文提出了一种用于Nawatl语言的上下文无关语法，以生成更多语法正确的句子，从而扩展该语言的语料库，并在某些大型语言模型上实现了比较性的改进。


<details>
  <summary>Details</summary>
Motivation: Nawatl是一种资源极少的印第安语言，缺乏可用于机器学习的语料库。因此，本文旨在通过生成更多语法正确的句子来扩展语料库。

Method: 本文引入了一种用于Nawatl语言的上下文无关语法（CFG），以生成大量语法正确的句子，从而增加可用于语言模型训练的语料库。

Result: 初步结果表明，使用语法可以在某些大型语言模型上实现比较性的改进。

Conclusion: 本文表明，通过使用语法可以显著扩展Nawatl语的语料库，但为了获得更显著的改进，需要更有效的Nawatl语言语法模型。

Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

</details>


### [99] [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
*Om Dobariya,Akhil Kumar*

Main category: cs.CL

TL;DR: 本研究探讨了不同级别的提示礼貌如何影响大型语言模型在选择题上的准确性。结果表明，不礼貌的提示通常比礼貌的提示表现更好，这与之前的发现不同，暗示较新的LLMs可能对语气变化有不同的反应。


<details>
  <summary>Details</summary>
Motivation: 自然语言提示的措辞已被证明会影响大型语言模型（LLMs）的性能，但礼貌和语气的作用仍缺乏深入研究。本研究旨在探讨不同级别的提示礼貌如何影响模型在选择题上的准确性。

Method: 我们创建了一个包含50个基础问题的数据集，每个问题被改写成五个语气变体：非常有礼貌、有礼貌、中性、粗鲁和非常粗鲁，产生了250个独特的提示。使用ChatGPT 4o，我们在这些条件下评估了响应，并应用配对样本t检验来评估统计显著性。

Result: 与预期相反，不礼貌的提示始终优于礼貌的提示，准确率从非常礼貌提示的80.8%到非常粗鲁提示的84.8%不等。这些发现与早期研究的结果不同，这些研究将粗鲁与较差的结果相关联，表明较新的LLMs可能对语气变化有不同的反应。

Conclusion: 我们的研究结果强调了研究提示的语用方面的重要性，并引发了关于人机交互社会维度的更广泛问题。

Abstract: The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

</details>


### [100] [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
*Khalid Mehtab Khan,Anagha Kulkarni*

Main category: cs.CL

TL;DR: 本文提出了一种名为AWARE的框架，用于提升模型对文本分类任务中上下文依赖意义的感知能力。该方法在多个方面表现出色，为相关任务提供了一种稳健且可推广的方法。


<details>
  <summary>Details</summary>
Motivation: 识别学生反思中的文化资本主题可以为创建公平的学习环境提供有价值的见解。然而，诸如抱负目标或家庭支持等主题通常融入叙述中，而不是作为直接关键词出现，这使得标准NLP模型难以检测。

Method: AWARE框架包含三个核心组件：领域意识、上下文意识和类别重叠意识。领域意识通过调整模型的词汇来适应学生反思的语言风格；上下文意识生成考虑整个文章上下文的句子嵌入；类别重叠意识采用多标签策略来识别单个句子中主题的共存情况。

Result: 通过使模型显式意识到输入的属性，AWARE在宏F1分数上优于基线模型2.1个百分点，并在所有主题上都表现出显著改进。

Conclusion: 本文提出了一种名为AWARE的框架，通过提高模型对输入属性的显式意识，显著提升了文本分类任务的性能。该方法在宏F1分数上优于基线模型2.1个百分点，并在所有主题上都表现出显著改进。

Abstract: Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

</details>


### [101] [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
*Imran Mansha*

Main category: cs.CL

TL;DR: 本文提出了一种资源高效的微调方法，用于改进LLaMA-3.2-3B在医学链式推理任务中的表现，同时减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如GPT-4和LLaMA）虽然表现出色，但需要大量的计算资源进行微调。本文旨在提出一种资源高效的微调方法，以在受限的GPU和内存条件下提升医学链式推理能力。

Method: 本文使用参数高效微调技术，如LoRA和QLoRA，对LLaMA-3.2-3B进行微调，以提高医学链式推理能力。

Result: 该模型在保持强推理能力的同时，将内存使用量减少了高达60%。实验评估表明，轻量级适应可以保留医学问答任务中的强大推理能力。

Conclusion: 本文展示了在低资源研究环境中部署大型语言模型的实用策略，并提供了关于平衡效率和领域专业化的见解。

Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

</details>


### [102] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: 本文介绍了一种利用变体选择器的不可察觉的越狱攻击方法，能够在不产生任何可见修改的情况下诱导有害响应。


<details>
  <summary>Details</summary>
Motivation: 传统上，视觉模态的越狱攻击依赖于难以察觉的对抗性扰动，而文本模态的攻击通常被认为需要可见的修改（例如非语义后缀）。本文旨在探索一种新的不可察觉的越狱攻击方法。

Method: 本文提出了一种搜索链管道来生成这样的对抗性后缀，以诱导有害响应。

Result: 实验结果表明，不可察觉的越狱攻击对四个对齐的LLM具有很高的攻击成功率，并且可以推广到提示注入攻击，而不会在书面提示中产生任何可见的修改。

Conclusion: 本文介绍了利用Unicode字符中的一类称为变体选择器的不可察觉的越狱攻击，能够使恶意问题在屏幕上看起来与原始问题相同，但其分词被“秘密”地改变。实验表明，这种不可察觉的越狱攻击对四个对齐的LLM具有很高的攻击成功率，并且可以推广到提示注入攻击，而不会在书面提示中产生任何可见的修改。

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [103] [A Set of Quebec-French Corpus of Regional Expressions and Terms](https://arxiv.org/abs/2510.05026)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 本文提出将成语理解和方言理解结合起来，使用地区成语作为方言理解的测试，并创建了两个新的基准数据集，实验结果表明这些基准测试是可靠的。


<details>
  <summary>Details</summary>
Motivation: 将成语理解和方言理解结合起来，使用地区成语作为方言理解的测试。

Method: 我们提出了两个新的基准数据集：QFrCoRE和QFrCoRT，并解释了如何构建这些语料库以供其他方言复制方法。

Result: 我们的实验表明，我们的区域成语基准测试是衡量模型在特定方言中的熟练程度的可靠工具。

Conclusion: 我们的区域成语基准测试是衡量模型在特定方言中的熟练程度的可靠工具。

Abstract: The tasks of idiom understanding and dialect understanding are both
well-established benchmarks in natural language processing. In this paper, we
propose combining them, and using regional idioms as a test of dialect
understanding. Towards this end, we propose two new benchmark datasets for the
Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic
phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic
words. We explain how to construct these corpora, so that our methodology can
be replicated for other dialects. Our experiments with 94 LLM demonstrate that
our regional idiom benchmarks are a reliable tool for measuring a model's
proficiency in a specific dialect.

</details>


### [104] [Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization](https://arxiv.org/abs/2510.05038)
*Omri Uzan,Asaf Yehudai,Roi pony,Eyal Shnarch,Ariel Gera*

Main category: cs.CL

TL;DR: 本文提出了一种新的测试时优化方法Guided Query Refinement (GQR)，以提升视觉中心模型的性能，同时提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的混合方法在排名或分数的粗粒度融合上无法充分利用每个模型表示空间中的丰富交互，同时大型模型在实际管道中部署和扩展存在障碍。

Method: 引入了Guided Query Refinement (GQR)，这是一种新的测试时优化方法，通过互补检索器的分数指导主检索器的查询嵌入优化。

Result: GQR使视觉中心模型能够达到具有显著更大表示模型的性能，同时速度提高了14倍，内存需求减少了54倍。

Conclusion: GQR effectively pushes the Pareto frontier for performance and efficiency in multimodal retrieval.

Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

</details>


### [105] [COLE: a Comprehensive Benchmark for French Language Understanding Evaluation](https://arxiv.org/abs/2510.05046)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 本文介绍了COLE，一个针对法语自然语言理解的新基准测试，涵盖了多种任务，并对94个大型语言模型进行了评估，揭示了性能差距和挑战领域。


<details>
  <summary>Details</summary>
Motivation: 为了应对更全面评估法语自然语言理解（NLU）的需求，我们引入了COLE。

Method: 我们引入了COLE，一个新的基准测试，由23个多样化的任务组成，涵盖了广泛的NLU能力。我们对94个大型语言模型进行了基准测试，并提供了当前法语NLU状态的广泛分析。

Result: 我们的结果突显了封闭权重模型和开放权重模型之间的显著性能差距，并确定了当前LLM的关键挑战领域，如零样本抽取式问答（QA）、细粒度词义消歧以及对地区语言变体的理解。

Conclusion: 我们发布了COLE作为一个公开资源，以促进法语语言建模的进一步进展。

Abstract: To address the need for a more comprehensive evaluation of French Natural
Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23
diverse task covering a broad range of NLU capabilities, including sentiment
analysis, paraphrase detection, grammatical judgment, and reasoning, with a
particular focus on linguistic phenomena relevant to the French language. We
benchmark 94 large language models (LLM), providing an extensive analysis of
the current state of French NLU. Our results highlight a significant
performance gap between closed- and open-weights models and identify key
challenging frontiers for current LLMs, such as zero-shot extractive
question-answering (QA), fine-grained word sense disambiguation, and
understanding of regional language variations. We release COLE as a public
resource to foster further progress in French language modelling.

</details>


### [106] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: SwiReasoning是一个无需训练的框架，通过动态切换显式和隐式推理来提高LLM的推理性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决隐式推理在无训练设置下的两个挑战：1) 隐式推理扩大了搜索分布，导致概率质量分散、引入噪声并阻碍收敛；2) 即使没有显式文本，过度思考仍然存在，浪费令牌并降低效率。

Method: SwiReasoning是一个无需训练的框架，它通过动态切换显式和隐式推理来平衡探索和利用，并通过限制最大思考块切换次数来减少过度思考。

Result: SwiReasoning在数学和STEM基准测试中提高了平均准确率1.5%-2.8%，并在有限预算下提高了平均令牌效率56%-79%。

Conclusion: SwiReasoning可以提高不同模型家族和规模的推理LLM的平均准确性，并在有限预算下提高平均令牌效率。

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


### [107] [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
*Chenyu Wang,Zishen Wan,Hao Kang,Emma Chen,Zhiqiang Xie,Tushar Krishna,Vijay Janapa Reddi,Yilun Du*

Main category: cs.CL

TL;DR: 本文提出了一种三阶段方法来编排小语言模型，通过SLM-MUX多模型架构和优化策略，显著提高了准确性，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着小语言模型数量的增加，虽然它们的准确性不如前沿模型，但它们更高效且在特定任务上表现优异。因此，研究如何将多个SLMs编排成一个系统以提高准确性具有重要意义。

Method: 本文提出了一个三阶段的方法来编排小语言模型（SLMs），包括引入SLM-MUX多模型架构，以及两种优化策略：模型选择搜索和测试时缩放。

Result: 与现有的编排方法相比，本文的方法在MATH、GPQA和GSM8K数据集上分别提升了13.4%、8.8%和7.0%。使用两个SLMs，SLM-MUX在GPQA和GSM8K上超过了Qwen 2.5 72B，在MATH上达到了其性能。

Conclusion: 本文证明了通过所提出的方法，可以将小语言模型有效编排成更准确和高效的系统。

Abstract: With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

</details>


### [108] [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
*Janos Perczel,Jin Chow,Dorottya Demszky*

Main category: cs.CL

TL;DR: 本文介绍了TeachLM，这是一种通过参数高效微调最先进的模型来优化教学的大型语言模型。TeachLM在由100,000小时一对一、纵向学生导师互动组成的数据集上进行训练，该数据集经过严格的匿名化处理以保护隐私。使用参数高效微调开发了一个真实的学生成模型，能够生成高保真度的合成学生导师对话。在此基础上，提出了一种新的多轮评估协议，利用合成对话生成来提供快速、可扩展和可重复的LLM对话能力评估。结果表明，微调真实学习数据显著提高了对话和教学性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育领域的潜力受到大型语言模型（LLMs）教学限制的制约。主要问题是缺乏反映实际学生学习的高质量训练数据。提示工程作为一种权宜之计，但提示在基于规则的自然语言中编码复杂教学策略的能力本质上是有限的。

Method: 引入TeachLM - 一种通过参数高效微调最先进的模型来优化教学的大型语言模型。TeachLM在由100,000小时一对一、纵向学生导师互动组成的数据集上进行训练，该数据集经过严格的匿名化处理以保护隐私。使用参数高效微调开发了一个真实的学生成模型，能够生成高保真度的合成学生导师对话。在此基础上，提出了一种新的多轮评估协议，利用合成对话生成来提供快速、可扩展和可重复的LLM对话能力评估。

Result: 微调真实学习数据显著提高了对话和教学性能，包括学生说话时间翻倍、提问风格改善、对话轮次增加50%以及教学个性化程度提高。

Conclusion: 通过在真实学习数据上进行微调，显著提高了对话和教学性能，包括学生说话时间翻倍、提问风格改善、对话轮次增加50%以及教学个性化程度提高。

Abstract: The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

</details>


### [109] [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
*Runchu Tian,Junxia Cui,Xueqiang Xu,Feng Yao,Jingbo Shang*

Main category: cs.CL

TL;DR: Tolerator是一种无需训练的解码策略，通过交叉验证优化令牌，提高了扩散大语言模型的解码效果。


<details>
  <summary>Details</summary>
Motivation: 现有的解码策略在离散dLLMs中存在一个关键限制：一旦一个令牌被接受，就无法在后续步骤中进行修改。这导致早期错误持续存在，影响中间预测和最终输出质量。

Method: Tolerator（令牌级交叉验证精炼）是一种无需训练的解码策略，它利用预测令牌之间的交叉验证。Tolerator引入了一个两阶段过程：(i) 序列填充和(ii) 通过重新遮蔽和解码部分令牌并将其余作为上下文来迭代优化。

Result: 在五个标准基准测试中评估了Tolerator，涵盖语言理解、代码生成和数学领域。实验结果表明，在相同的计算预算下，我们的方法在基线之上实现了持续改进。

Conclusion: 解码算法对于实现扩散大语言模型的全部潜力至关重要。

Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [110] [AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials](https://arxiv.org/abs/2510.04704)
*Taoyuze Lv,Alexander Chen,Fengyu Xie,Chu Wu,Jeffrey Meng,Dongzhan Zhou,Bram Hoex,Zhicheng Zhong,Tong Xie*

Main category: cond-mat.mtrl-sci

TL;DR: 本文介绍了AtomWorld基准，用于评估LLMs在材料科学相关任务中的表现，发现当前模型在结构理解和空间推理方面存在显著不足，并提出了改进的方向。


<details>
  <summary>Details</summary>
Motivation: 在材料科学等领域，需要对3D原子结构有深入理解，而现有的LLMs在结构理解和空间推理方面存在明显不足，缺乏一个标准化的基准来系统评估它们的核心推理能力。

Method: 引入AtomWorld基准，评估LLMs在基于CIF的任务上的表现，包括结构编辑、CIF感知和属性引导建模。

Result: 实验表明，当前模型在结构修改任务中频繁出错，甚至在基本的CIF格式理解上也存在问题，可能导致后续分析和材料见解中的累积错误。

Conclusion: AtomWorld定义了这些标准化任务，为推动LLMs向强大的原子尺度建模迈进奠定了基础，这对于加速材料研究和自动化科学工作流程至关重要。

Abstract: Large Language Models (LLMs) excel at textual reasoning and are beginning to
develop spatial understanding, prompting the question of whether these
abilities can be combined for complex, domain-specific tasks. This question is
essential in fields like materials science, where deep understanding of 3D
atomic structures is fundamental. While initial studies have successfully
applied LLMs to tasks involving pure crystal generation or coordinate
understandings, a standardized benchmark to systematically evaluate their core
reasoning abilities across diverse atomic structures has been notably absent.
To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on
tasks based in Crystallographic Information Files (CIFs), a standard structure
representation format. These tasks, including structural editing, CIF
perception, and property-guided modeling, reveal a critical limitation: current
models, despite establishing promising baselines, consistently fail in
structural understanding and spatial reasoning. Our experiments show that these
models make frequent errors on structure modification tasks, and even in the
basic CIF format understandings, potentially leading to cumulative errors in
subsequent analysis and materials insights. By defining these standardized
tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale
modeling, crucial for accelerating materials research and automating scientific
workflows.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [111] [CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano](https://arxiv.org/abs/2412.18708)
*Vivek Vellaiyappan Surulimuthu,Aditya Karnam Gururaj Rao*

Main category: cs.AI

TL;DR: CAG 是一种针对 Google Chrome 的 Gemini Nano 模型设计的架构，用于克服其上下文窗口限制，从而在浏览器中高效处理大型输入。


<details>
  <summary>Details</summary>
Motivation: 由于 Gemini Nano 模型的上下文窗口有限，处理大型输入存在挑战，因此需要一种解决方案来克服这一限制。

Method: CAG 通过智能输入分块和处理策略来克服 Gemini Nano 模型的上下文窗口限制。

Result: CAG 在 Chrome 中处理大型文档和数据集方面表现出色，使复杂的 AI 功能可以通过浏览器访问。

Conclusion: CAG 是一种有效的解决方案，使 Google Chrome 的 Gemini Nano 模型能够处理大型文档和数据集，而无需依赖外部 API。

Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically
designed to overcome the context window limitations of Google Chrome's built-in
Gemini Nano model. While Chrome's integration of Gemini Nano represents a
significant advancement in bringing AI capabilities directly to the browser,
its restricted context window poses challenges for processing large inputs. CAG
addresses this limitation through intelligent input chunking and processing
strategies, enabling efficient handling of extensive content while maintaining
the model's performance within browser constraints. Our implementation
demonstrates particular efficacy in processing large documents and datasets
directly within Chrome, making sophisticated AI capabilities accessible through
the browser without external API dependencies. Get started now at
https://github.com/vivekVells/cag-js.

</details>


### [112] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 本文提出了一种系统评估框架，用于评估大型语言模型的自我识别能力，发现大多数模型在自我识别任务中表现不佳，并且存在对某些模型的偏见。


<details>
  <summary>Details</summary>
Motivation: 由于对模型是否具备自我识别能力存在矛盾的解释，我们希望通过系统评估框架来解决这一问题。

Method: 我们引入了一个系统评估框架，通过两个任务（二元自我识别和精确模型预测）来衡量10个当代大型语言模型（LLMs）识别自己生成的文本与其他模型文本的能力。

Result: 结果显示，只有4个模型能正确预测自己作为生成器，性能很少超过随机几率。此外，模型表现出对GPT和Claude家族的强烈偏见。

Conclusion: 我们的研究结果对AI安全和未来开发适当的AI自我意识具有重要意义。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [113] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 本文提出了一种用于多代理系统目标导向评估的框架，包括目标成功率（GSR）和失败的根本原因（RCOF）分类法，通过结合教师大语言模型进行可解释、数据高效的评估，并在实际应用中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 评估多轮聊天机器人交互的质量仍然具有挑战性，因为大多数现有方法在回合级别评估交互，而没有解决用户的总体目标是否得到满足的问题。

Method: 我们提出了一个全面的多代理系统（MAS）目标导向评估框架，引入了目标成功率（GSR）来衡量实现目标的百分比，并引入了失败的根本原因（RCOF）分类法来识别多代理聊天机器人中的失败原因。我们的方法按用户目标对对话进行分段，并使用所有相关回合来评估成功。我们展示了一个基于模型的评估系统，结合教师大语言模型（LLM），其中领域专家定义目标，设定质量标准作为LLM的指导。LLM使用“思考标记”生成可解释的推理，实现可解释、数据高效的评估。

Result: 在企业环境中，我们将框架应用于评估AIDA，这是一个从零开始构建的员工对话代理系统，作为自下而上的多代理对话代理系统，自成立以来六个月中，GSR从63%提高到79%。

Conclusion: 我们的框架是通用的，并通过基于多代理聊天机器人失败点分析的详细缺陷分类法提供可操作的见解，诊断整体成功率，识别关键失败模式，并指导系统改进。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [114] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 本文研究了如何提升多模态基础模型的能力，使其更接近世界模型，包括增强推理能力和生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态基础模型在进行反事实推理、动态模拟、理解时空信息、控制生成视觉结果和多方面推理方面存在不足，因此需要改进以更好地作为世界模型。

Method: 通过判别任务提高多模态基础模型的推理能力，并引入结构化推理技能，如因果推理、反事实思维和时空推理。此外，还探索了跨图像和视频模态的生成能力，引入了结构化和可控生成的新框架。

Result: 改进后的多模态基础模型能够超越表面相关性，理解视觉和文本数据中的深层关系。同时，提出的生成框架能够确保生成过程与高层次语义和细粒度用户意图一致，并实现了可控的4D生成。

Conclusion: 本文探讨了如何弥合多模态基础模型与世界模型之间的差距，并提出了改进多模态基础模型的推理能力和生成能力的方法。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [115] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 本文提出了一种游戏化的XAI系统，以帮助消费者在咖啡购买中做出更道德的决策。


<details>
  <summary>Details</summary>
Motivation: 为了帮助消费者在购买咖啡时做出更道德的决策，本文提出了一个游戏化的XAI系统，以提供实时的伦理解释和建议。

Method: 系统包含六个回合，每个回合有三个选项。两个符号引擎提供实时原因：一个康德模块标记规则违规，一个功利模块通过多标准聚合对选项进行评分。一个带有遗憾界限的元解释器突出显示康德-功利（不）一致，并在福利损失较小时切换到德行清洁、接近公平的选项。

Result: 本文释放了一个结构化配置（属性模式、认证映射、权重、规则集）、一个政策追踪用于审计，并提供了一个交互式UI。

Conclusion: 本文提出了一种游戏化的可解释人工智能（XAI）系统，以促进咖啡领域中伦理意识的消费者决策。该系统通过实时解释帮助用户做出更道德的选择。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [116] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: 本文介绍了C^2-Eval，一个全面的基准，用于统一评估基础模型的创造力。通过实验分析了模型在创造性能力上的权衡，并展示了C^2-Eval在考察创造性AI发展中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架对于创造力仍然碎片化，依赖于没有牢固理论基础的临时指标。

Method: 引入C^2-Eval，一个全面的基准，用于统一评估基础模型的创造力。

Result: 通过在领先的专有和开源模型上的广泛实验，分析了它们在创造性能力上的权衡。结果表明，当前的基础模型在追求创造性机器思维方面既有优势也有挑战。

Conclusion: C^2-Eval是一个有效的工具，可以 examining the evolving landscape of creative AI。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [117] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 本文是对数据科学代理的首次全面调查，系统分析了45个系统，并提供了分类法、能力分析、趋势识别以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供一个全面的分类法，以系统分析数据科学代理，并识别其优势和局限性，同时回顾新兴基准和评估实践。

Method: 本文通过系统分析和映射45个系统到端到端数据科学过程的六个阶段，提供了一个全面的生命周期对齐分类法，并在五个跨设计维度上注释每个代理。

Result: 本文分析识别出三个关键趋势：大多数系统强调探索性分析、可视化和建模，而忽视了业务理解、部署和监控；多模态推理和工具编排仍然是未解决的挑战；超过90%的系统缺乏显式的信任和安全机制。

Conclusion: 本文结论指出，数据科学代理在对齐稳定性、可解释性、治理和稳健评估框架方面存在开放挑战，并提出了未来研究方向，以指导开发稳健、可信、低延迟、透明且广泛可用的数据科学代理。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [118] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 本文研究了模型中等待标记前的潜在特征对推理过程的影响，并发现这些特征能够引发不同的推理模式。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究模型在等待标记前的潜在特征是否包含有关调节后续推理过程的相关信息，以更好地理解推理模型的有效性。

Method: 本文通过在DeepSeek-R1-Distill-Llama-8B及其基础版本的多个层上训练交叉编码器，并引入一种潜在属性技术来分析等待标记前的潜在特征。

Result: 本文发现了一些与促进或抑制等待标记概率相关的特征，并通过实验验证了这些特征对推理过程的影响。

Conclusion: 本文结论是，模型中等待标记之前的潜在特征确实对后续推理过程有影响，并且这些特征能够引发不同的推理模式。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [119] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: MENTOR is a framework that provides expert guidance only at critical decision points to perform effective and diverse exploration in RLVR, leading to superior overall performance.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of RLVR strongly depends on the capability of base models, which requires the model to have sufficient capability to perform high-quality exploration, involving both effectiveness and diversity. Existing methods address this issue by imitating expert trajectories, which improve effectiveness but neglect diversity.

Method: MENTOR: Mixed-policy Expert Navigation for Token-level Optimization of Reasoning, a framework that provides expert guidance only at critical decision points to perform effective and diverse exploration in RLVR.

Result: Extensive experiments show that MENTOR enables models to capture the essence of expert strategies rather than surface imitation, thereby performing high-quality exploration and achieving superior overall performance.

Conclusion: MENTOR enables models to capture the essence of expert strategies rather than surface imitation, thereby performing high-quality exploration and achieving superior overall performance.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [120] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 本文提出了一种贝叶斯评估框架，用于替代Pass$@k$，以获得更稳定和透明的LLM性能评估。


<details>
  <summary>Details</summary>
Motivation: Pass$@k$在试验次数有限和计算资源受限时常常产生不稳定和误导性的排名，因此需要一种更稳定和透明的评估方法。

Method: 本文提出了一种原则性的贝叶斯评估框架，用模型的潜在成功概率的后验估计和可信区间替代Pass$@k$和平均准确率（avg$@N$）。

Result: 在具有已知真实成功率的模拟以及AIME'24/'25、HMMT'25和BrUMO'25数据集上，贝叶斯/平均过程比Pass$@k$和最近的变体实现了更快的收敛和更高的排名稳定性。

Conclusion: 本文建议用基于后验的、计算高效的协议取代Pass$@k$进行LLM评估和排名，该协议统一了二元和非二元评估，并明确表示了不确定性。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [121] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 本研究探讨了想象的计算目标，通过心理网络分析比较了人类和大型语言模型中的内部世界模型，发现两者之间存在差异。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为想象有助于最大化奖励，但最近的研究挑战了这一观点。我们提出想象的作用是访问内部世界模型（IWM），并探讨了人类和LLMs中的IWM。

Method: 我们使用心理网络分析来探索人类和大型语言模型（LLMs）中的内部世界模型（IWM）。具体来说，我们使用两个问卷调查评估了想象的生动性评分，并从这些报告中构建了想象网络。

Result: 人类组的想象网络显示了不同中心性度量之间的相关性，包括预期影响、强度和接近度。然而，LLMs的想象网络在不同提示和对话记忆条件下表现出缺乏聚类和较低的相关性。

Conclusion: 我们的研究提供了一种比较人类和人工智能内部生成表示的新方法，为开发类似人类的想象提供了见解。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [122] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: 本文介绍了TraitBasis，一种用于系统压力测试AI代理的轻量级、模型无关的方法。通过将τ-Bench扩展为τ-Trait，研究发现当前AI代理在用户行为变化方面表现出明显的性能下降，强调了鲁棒性测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前的基准测试未能捕捉到AI代理的脆弱性：代理可能在标准评估中表现良好，但在更现实和多样的设置中会显著下降。因此，需要一种方法来系统地测试AI代理的鲁棒性。

Method: TraitBasis是一种轻量级、模型无关的方法，用于系统地对AI代理进行压力测试。它学习对应于可调节用户特质（如不耐烦或不连贯）的激活空间方向，这些方向可以在推理时被控制、缩放、组合和应用，而无需微调或额外数据。

Result: 使用TraitBasis，我们将τ-Bench扩展为τ-Trait，通过受控的特质向量改变用户行为。平均性能下降了2%-30%，突显了当前AI代理在用户行为变化方面的缺乏鲁棒性。

Conclusion: TraitBasis展示了其作为简单、数据高效和可组合工具的潜力，并为构建在现实世界人类互动中保持可靠性的AI代理打开了大门。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [123] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: 本文介绍了一种名为ChartAgent的新代理框架，该框架在图表的空间领域内直接执行视觉推理，以解决多模态大语言模型在未注释图表上的性能下降问题。实验结果显示，ChartAgent在多个基准测试中取得了最先进的结果，并且在不同类型的图表和不同的视觉与推理复杂度水平上都表现出色。


<details>
  <summary>Details</summary>
Motivation: 最近的多模态LLM在基于图表的视觉问答中显示出前景，但它们在未注释的图表上的表现急剧下降，这些图表需要精确的视觉解释而不是依赖文本快捷方式。

Method: 我们引入了ChartAgent，这是一个新颖的代理框架，它在图表的空间领域内直接执行视觉推理。

Result: ChartAgent在ChartBench和ChartX基准测试中达到了最先进的准确性，超过了之前的方法，总体绝对增益高达16.07%，在未注释、数值密集的查询上达到17.31%。

Conclusion: 我们的工作是最早展示使用工具增强的多模态代理进行图表理解的视觉基础推理的工作之一。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [124] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 本文构建了一个新的驾驶VQA数据集DriveMind，并发现视觉-语言模型驱动代理中的推理可能是训练的副产品而非因果中介。


<details>
  <summary>Details</summary>
Motivation: 本文旨在验证视觉-语言模型驱动代理中的推理是否真正影响轨迹规划，而不是仅仅作为训练的副产品。

Method: 本文构建了DriveMind数据集，并使用监督微调和组相对策略优化方法训练了代表性的VLM代理。此外，还引入了一种无需训练的探针来评估代理对先验的依赖程度。

Result: 实验结果表明，移除自我/导航先验会导致规划分数大幅下降，而移除CoT仅产生较小变化。注意力分析显示，规划主要关注先验而非CoT。

Conclusion: 本文提出了一个关于视觉-语言模型驱动代理的因果性假设，即推理是训练产生的副产品而非因果中介。同时，本文提供了一个新的数据集和诊断工具来评估未来模型的因果一致性。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [125] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: The paper introduces BrokenMath, a new benchmark for evaluating sycophantic behavior in large language models (LLMs) during theorem proving. It finds that sycophancy is common, with GPT-5 showing it 29% of the time, and explores ways to reduce it.


<details>
  <summary>Details</summary>
Motivation: existing benchmarks that measure sycophancy in mathematics are limited, focusing on final-answer problems and relying on simple, contaminated datasets.

Method: introduce BrokenMath, a benchmark for evaluating sycophantic behavior in LLMs within the context of natural language theorem proving, and evaluate state-of-the-art LLMs and agentic systems using an LLM-as-a-judge framework.

Result: sycophancy is widespread, with the best model, GPT-5, producing sycophantic answers 29% of the time. Mitigation strategies such as test-time interventions and supervised fine-tuning on curated sycophantic examples substantially reduce, but do not eliminate, sycophantic behavior.

Conclusion: sycophancy is widespread in LLMs, and while mitigation strategies can reduce it, they do not eliminate it entirely.

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [126] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: This paper introduces MARS, a multi-agent system that combines fast, intuitive thinking with deliberate reasoning in LLMs. It uses external tools and a reinforcement learning framework to improve performance on complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) often overanalyze simple tasks, using excessive System 2-type reasoning, leading to inefficiency. They also struggle to adapt to changing environments due to static pretraining data. Advancing LLMs for complex reasoning requires innovative approaches that bridge intuitive and deliberate cognitive processes akin to human cognition's dual-system dynamic.

Method: MARS introduces a Multi-Agent System for Deep ReSearch that integrates System 1's fast, intuitive thinking with System 2's deliberate reasoning within LLMs. It uses external tools like Google Search, Google Scholar, and Python Interpreter, and proposes a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to optimize both systems.

Result: MARS achieves a 3.86% improvement on the HLE benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, demonstrating the effectiveness of the dual-system paradigm for complex reasoning in dynamic information environments.

Conclusion: MARS achieves substantial improvements on the challenging Humanity's Last Exam (HLE) benchmark and across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [127] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: 本研究通过引入LLM-Hanabi基准，评估了大型语言模型在协作环境中的动机推断和心智理论能力。结果表明，第一层心智理论对游戏表现的影响更大，强调了准确理解合作伙伴动机的重要性。


<details>
  <summary>Details</summary>
Motivation: 有效多智能体协作需要代理推断他人行为背后的动机，这一能力基于心智理论（ToM）。尽管最近的大型语言模型（LLMs）在逻辑推理方面表现出色，但它们在动态协作环境中推断动机的能力仍缺乏研究。

Method: 研究引入了LLM-Hanabi基准，使用合作游戏Hanabi来评估LLMs的动机推断和ToM能力。框架包括一个自动评估系统，测量游戏表现和ToM熟练程度。

Result: 研究发现，ToM与游戏中的成功之间存在显著正相关。值得注意的是，第一层ToM（解释他人的意图）与性能的相关性比第二层ToM（预测他人的解释）更强。

Conclusion: 研究结论表明，为了实现有效的AI协作，准确理解合作伙伴的动机比高阶推理更为关键。优先考虑第一层心智理论（ToM）是提升未来模型协作能力的一个有前景的方向。

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [128] [PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters](https://arxiv.org/abs/2510.03415)
*Aditya Thimmaiah,Jiyang Zhang,Jayanth Srinivasa,Junyi Jessy Li,Milos Gligoric*

Main category: cs.PL

TL;DR: 研究探讨了大型语言模型是否能基于编程语言的形式语义执行程序。结果表明，LLMs可能作为编程语言解释器，但缺乏稳健的语义理解。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在代码推理方面表现出色，一个自然的问题是：LLMs能否仅基于编程语言的形式语义执行程序（即充当解释器）？如果可以，将能够快速原型化新的编程语言和语言特性。

Method: 我们使用IMP语言（C的一个子集）研究了这个问题，通过小步操作语义（SOS）和基于重写的操作语义（K语义）进行形式化。引入了三个评估集：人类编写的、LLM翻译的和模糊生成的，其难度由代码复杂度指标控制。模型在三个任务上进行评估：最终状态预测、语义规则预测和执行轨迹预测。

Result: 在强大的代码/推理LLMs中，尽管在标准语义下表现良好，但在非标准语义下性能下降。我们发现不同模型失败有模式，大多数推理模型在涉及高度复杂程序的粗粒度任务中表现优异，且提供形式语义有助于简单程序但可能损害复杂程序。

Conclusion: 结果表明，LLMs有可能作为编程语言解释器，但它们对语义的理解还不够稳健。

Abstract: As large language models (LLMs) excel at code reasoning, a natural question
arises: can an LLM execute programs (i.e., act as an interpreter) purely based
on a programming language's formal semantics? If so, it will enable rapid
prototyping of new programming languages and language features. We study this
question using the imperative language IMP (a subset of C), formalized via
small-step operational semantics (SOS) and rewriting-based operational
semantics (K-semantics). We introduce three evaluation sets-Human-Written,
LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by
code-complexity metrics spanning the size, control-flow, and data-flow axes.
Given a program and its semantics formalized with SOS/K-semantics, models are
evaluated on three tasks ranging from coarse to fine: (1) final-state
prediction, (2) semantic rule prediction, and (3) execution trace prediction.
To distinguish pretraining memorization from semantic competence, we define two
nonstandard semantics obtained through systematic mutations of the standard
rules. Across strong code/reasoning LLMs, performance drops under nonstandard
semantics despite high performance under the standard one. We further find that
(i) there are patterns to different model failures, (ii) most reasoning models
perform exceptionally well on coarse grained tasks involving reasoning about
highly complex programs often containing nested loop depths beyond five, and
surprisingly, (iii) providing formal semantics helps on simple programs but
often hurts on more complex ones. Overall, the results show a promise that LLMs
could serve as programming language interpreters, but points to the lack of
their robust semantics understanding. We release the benchmark and the
supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [129] [Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition](https://arxiv.org/abs/2510.03723)
*Martin Kocour,Martin Karafiat,Alexander Polok,Dominik Klement,Lukáš Burget,Jan Černocký*

Main category: eess.AS

TL;DR: 本文提出了一种基于Whisper的说话人归因模型，通过联合解码来提高多说话人语音识别的性能。


<details>
  <summary>Details</summary>
Motivation: 为了处理重叠语音的识别问题，需要一种能够同时考虑所有说话人的解码方法。

Method: 提出了一种基于Whisper的说话人归因模型，结合目标说话人建模和序列输出训练（SOT）。利用Diarization-Conditioned Whisper（DiCoW）编码器提取目标说话人嵌入，并将其合并为一个表示传递给共享解码器。

Result: 实验表明，该模型在多说话人混合数据（如LibriMix）上表现优于现有SOT方法，并超越了DiCoW。

Conclusion: 该模型在多说话人混合数据上优于现有的基于SOT的方法，并超越了DiCoW。

Abstract: We propose a speaker-attributed (SA) Whisper-based model for multi-talker
speech recognition that combines target-speaker modeling with serialized output
training (SOT). Our approach leverages a Diarization-Conditioned Whisper
(DiCoW) encoder to extract target-speaker embeddings, which are concatenated
into a single representation and passed to a shared decoder. This enables the
model to transcribe overlapping speech as a serialized output stream with
speaker tags and timestamps. In contrast to target-speaker ASR systems such as
DiCoW, which decode each speaker separately, our approach performs joint
decoding, allowing the decoder to condition on the context of all speakers
simultaneously. Experiments show that the model outperforms existing SOT-based
approaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix).

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [130] [Investigating LLM Variability in Personalized Conversational Information Retrieval](https://arxiv.org/abs/2510.03795)
*Simon Lupart,Daniël van Dijk,Eric Langezaal,Ian van Dort,Mohammad Aliannejadi*

Main category: cs.IR

TL;DR: 本研究复现并扩展了Mo等人关于个性化CIR的研究，发现人工选择的PTKB能提升检索性能，而基于LLM的选择方法效果不佳。同时，研究指出评估个性化CIR时需关注数据集的变异性，并强调多运行评估和方差报告的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于Mo等人的研究结论基于使用GPT-3.5 Turbo模型的单次运行实验，这引发了对输出变异性与可重复性的担忧，因此本研究旨在进一步验证和扩展他们的工作。

Method: 本研究严格复现并扩展了Mo等人的工作，重点关注LLM输出的变异性以及模型的泛化能力。应用原始方法到新的TREC iKAT 2024数据集，并评估了一系列模型，包括Llama (1B-70B)、Qwen-7B和GPT-4o-mini。

Result: 结果显示，人工选择的PTKB始终能提升检索性能，而基于LLM的选择方法并未能可靠地超越人工选择。此外，还观察到iKAT数据集的变异性高于CAsT数据集，这突显了评估个性化CIR的挑战。值得注意的是，以召回为导向的指标的变异性低于以精度为导向的指标，这对第一阶段的检索器具有重要意义。

Conclusion: 本研究强调了在评估基于LLM的CIR系统时进行多运行评估和方差报告的必要性，并通过扩展模型、数据集和指标的评估，为个性化CIR提供了更稳健和通用的实践方法。

Abstract: Personalized Conversational Information Retrieval (CIR) has seen rapid
progress in recent years, driven by the development of Large Language Models
(LLMs). Personalized CIR aims to enhance document retrieval by leveraging
user-specific information, such as preferences, knowledge, or constraints, to
tailor responses to individual needs. A key resource for this task is the TREC
iKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.
Building on this resource, Mo et al. explored several strategies for
incorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query
reformulation. Their findings suggested that personalization from PTKBs could
be detrimental and that human annotations were often noisy. However, these
conclusions were based on single-run experiments using the GPT-3.5 Turbo model,
raising concerns about output variability and repeatability. In this
reproducibility study, we rigorously reproduce and extend their work, focusing
on LLM output variability and model generalization. We apply the original
methods to the new TREC iKAT 2024 dataset and evaluate a diverse range of
models, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that
human-selected PTKBs consistently enhance retrieval performance, while
LLM-based selection methods do not reliably outperform manual choices. We
further compare variance across datasets and observe higher variability on iKAT
than on CAsT, highlighting the challenges of evaluating personalized CIR.
Notably, recall-oriented metrics exhibit lower variance than precision-oriented
ones, a critical insight for first-stage retrievers. Finally, we underscore the
need for multi-run evaluations and variance reporting when assessing LLM-based
CIR systems. By broadening evaluation across models, datasets, and metrics, our
study contributes to more robust and generalizable practices for personalized
CIR.

</details>


### [131] [Visual Lifelog Retrieval through Captioning-Enhanced Interpretation](https://arxiv.org/abs/2510.04010)
*Yu-Fei Shih,An-Zi Yen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.IR

TL;DR: 本文提出了一种基于文本查询从用户的视觉生活日志中提取特定图像的CIVIL检索系统。我们的方法通过生成字幕并使用文本嵌入模型将字幕和查询投影到共享向量空间来实现这一点。我们引入了三种方法来解释生活日志记录者的生活经历，并构建了一个将视觉生活日志转换为字幕的文本数据集。实验结果表明，我们的方法有效描述了第一人称视觉图像，提高了生活日志检索的效果。


<details>
  <summary>Details</summary>
Motivation: 人们常常难以记住过去经历的具体细节，这可能导致需要重新访问这些记忆。因此，生活日志检索已成为一个关键的应用。各种研究探索了促进快速访问个人生活日志的方法，以帮助记忆回忆。

Method: 我们的系统首先为视觉生活日志生成字幕，然后利用文本嵌入模型将字幕和用户查询投影到共享向量空间中。我们引入了三种不同的方法：单字幕方法、集体字幕方法和合并字幕方法，每种方法都旨在解释生活日志记录者的生活经历。

Result: 实验结果表明，我们的方法能够有效地描述第一人称视觉图像，从而提高生活日志检索的效果。

Conclusion: 我们的方法有效地描述了第一人称视觉图像，提高了生活日志检索的结果。此外，我们构建了一个文本数据集，将视觉生活日志转换为字幕，从而重建个人生活经历。

Abstract: People often struggle to remember specific details of past experiences, which
can lead to the need to revisit these memories. Consequently, lifelog retrieval
has emerged as a crucial application. Various studies have explored methods to
facilitate rapid access to personal lifelogs for memory recall assistance. In
this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval
System for extracting specific images from a user's visual lifelog based on
textual queries. Unlike traditional embedding-based methods, our system first
generates captions for visual lifelogs and then utilizes a text embedding model
to project both the captions and user queries into a shared vector space.
Visual lifelogs, captured through wearable cameras, provide a first-person
viewpoint, necessitating the interpretation of the activities of the individual
behind the camera rather than merely describing the scene. To address this, we
introduce three distinct approaches: the single caption method, the collective
caption method, and the merged caption method, each designed to interpret the
life experiences of lifeloggers. Experimental results show that our method
effectively describes first-person visual images, enhancing the outcomes of
lifelog retrieval. Furthermore, we construct a textual dataset that converts
visual lifelogs into captions, thereby reconstructing personal life
experiences.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [132] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP是一个阿拉伯图像描述框架，结合了基于CLIP的视觉标签检索和多模态文本生成，以实现文化一致性和上下文准确性的阿拉伯语标题。


<details>
  <summary>Details</summary>
Motivation: 为了实现文化一致性和上下文准确性的阿拉伯语标题，VLCAP框架结合了基于CLIP的视觉标签检索和多模态文本生成。

Method: VLCAP框架结合了基于CLIP的视觉标签检索和多模态文本生成，使用三个多语言编码器（mCLIP、AraCLIP和Jina V4）提取可解释的阿拉伯语视觉概念，并构建了一个混合词汇表。

Result: mCLIP + Gemini Pro Vision在BLEU-1（5.34%）和余弦相似度（60.01%）方面表现最佳，而AraCLIP + Qwen-VL在LLM-judge评分（36.33%）方面表现最佳。

Conclusion: 该可解释的管道实现了文化一致性和上下文准确性的阿拉伯语标题。

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [133] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 本文通过创建大规模的多模态数据集注释，揭示了模型偏差与数据集组成之间的关系。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏在大规模网络数据集中的性别和种族/民族注释，训练数据在产生这些偏差中的作用仍然不清楚。

Method: 我们通过结合目标检测、多模态标题生成和微调分类器的验证自动标记管道，为整个数据集创建了以人物为中心的注释。

Result: 我们发现了人口统计不平衡和有害关联，例如将男性和被感知为黑人或中东的人与犯罪相关和负面内容不成比例地联系起来。我们还表明，CLIP和Stable Diffusion中的60-70%的性别偏差可以通过数据中的直接共现线性解释。

Conclusion: 我们的资源建立了数据集组成和下游模型偏差之间的第一个大规模实证联系。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [134] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 本文研究了长格式生物医学标题对预训练的影响，并提出了一个包含1M图像-标题对的数据集，以支持更长的文本上下文。通过这种方法，模型的上下文容量扩展了6.6倍，显著提高了检索和分类性能。


<details>
  <summary>Details</summary>
Motivation: 由于生物医学标题的分布显示大量标题超过77个标记，而现有的嵌入视觉语言模型（VLMs）通常使用短文本窗口进行预训练，这导致长格式标题被截断。因此，我们需要研究长格式生物医学标题的预训练影响。

Method: 我们通过扩展文本编码器的上下文长度来研究预训练对长格式生物医学标题的影响，并引入了BIOMEDICA-LongCAP数据集，用于训练具有支持最多512个标记窗口的文本编码器的BMC-LongCLIP模型。

Result: BMC-LongCLIP在长标题检索基准测试中实现了Recall@1高达+30%的绝对提升和分类的平均提升+2%，同时收敛速度比短上下文更快。此外，我们的模型将令牌浪费从55%减少到仅2.2%。

Conclusion: 我们的结果表明，长上下文建模是推进生物医学VLMs的一个有前途的方向。

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [135] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: 本文提出了一种微调方法，用于改进PaddleOCRv5在汉-诺文本上的字符识别。实验结果表明，与基础模型相比，精确度显著提高，特别是在噪声图像条件下。此外，开发了一个交互式演示，用于视觉比较微调前后的识别结果，有助于下游应用如汉-越语义对齐、机器翻译和历史语言学研究。


<details>
  <summary>Details</summary>
Motivation: 现有的OCR系统在处理退化的扫描、非标准字形和古籍中的手写变化时存在困难。因此，需要一种更有效的解决方案来识别和处理古典汉语（汉-诺）文本，以促进越南历史文献的数字化和跨语言语义研究。

Method: 本文提出了一个微调方法，用于改进PaddleOCRv5在汉-诺文本上的字符识别。通过使用精心挑选的古代越南中文手稿子集重新训练文本识别模块，并提供一个完整的训练流程，包括预处理、LMDB转换、评估和可视化。

Result: 实验结果表明，与基础模型相比，精确度显著提高，从37.5%增加到50.0%，尤其是在噪声图像条件下。此外，开发了一个交互式演示，用于视觉比较微调前后的识别结果，有助于下游应用如汉-越语义对齐、机器翻译和历史语言学研究。

Conclusion: 本文提出了一种微调方法，用于改进PaddleOCRv5在汉-诺文本上的字符识别。实验结果表明，与基础模型相比，精确度显著提高，特别是在噪声图像条件下。此外，开发了一个交互式演示，有助于下游应用如汉-越语义对齐、机器翻译和历史语言学研究。

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [136] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: 本文介绍了一个基于多模态LVLM的检索增强生成（RAG）框架SiteShield，用于自动化建筑安全检查报告。通过整合视觉和音频输入，SiteShield在真实世界数据上表现优于单模态LLMs，显示出提高信息检索和效率的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统建筑安全检查方法通常效率低下，因为需要在大量信息中导航。最近的大规模视觉语言模型（LVLMs）的进步为通过增强的视觉和语言理解来自动化安全检查提供了机会。然而，现有的应用存在诸如无关或不具体的回答、限制的模式输入和幻觉等问题。

Method: 本文介绍了SiteShield，这是一个基于多模态LVLM的检索增强生成（RAG）框架，通过整合视觉和音频输入来自动化建筑安全检查报告。

Result: 使用真实世界的数据，SiteShield在没有RAG的单模态LLMs上表现更好，F1得分为0.82，汉明损失为0.04，精确度为0.76，召回率为0.96。

Conclusion: 研究结果表明，SiteShield为提高安全报告的信息检索和效率提供了一条新途径。

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [137] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ZoomIn的两阶段取证框架，用于检测AI生成的图像，并提供基于视觉证据的人类可理解的解释。


<details>
  <summary>Details</summary>
Motivation: AI生成的图像迅速增长，模糊了真实和合成内容之间的界限，引发了对数字完整性的关注。尽管视觉语言模型（VLMs）通过解释提供了可解释性，但它们常常无法检测到高质量合成图像中的细微伪影。

Method: 我们提出了ZoomIn，这是一种两阶段的取证框架，模仿人类视觉检查，首先扫描图像以定位可疑区域，然后对这些放大区域进行集中分析以提供有根据的判断。

Result: 我们的方法在20,000张真实和高质量合成图像的数据集上实现了96.39%的准确率，并且具有强大的泛化能力。

Conclusion: 我们的方法在保持高准确性的同时，提供了基于视觉证据的人类可理解的解释。

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [138] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim是一种语言引导的框架，能够生成具有多视角一致性和对象级控制的4D场景，并允许在不完全重新生成的情况下进行编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频模型受限于2D视角且交互有限，因此需要一种能够生成4D场景并支持多视角一致性和对象级控制的框架。

Method: MorphoSim结合了轨迹引导生成和特征场蒸馏，允许在不完全重新生成的情况下进行编辑。

Result: 实验表明，MorphoSim在保持高场景保真度的同时，实现了可控性和可编辑性。

Conclusion: MorphoSim能够保持高场景保真度的同时，实现可控性和可编辑性。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [139] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM是一个自动化管道，将检测数据集转换为大规模的医学视觉问答数据，并通过链式思维推理来连接病变框到器官分割和结构化推理。提出了一种集成的CoT-课程策略，以有效利用这些数据，并在多个医学VQA基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 弥合临床诊断推理与AI之间的差距是医学影像中的一个核心挑战。

Method: 引入了MedCLM，这是一个自动化的管道，将检测数据集转换为大规模的医学视觉问答（VQA）数据，并通过链式思维（CoT）推理来连接病变框到器官分割和结构化推理。提出了一种集成的CoT-课程策略，包括一个带有显式病变框的简单阶段、一个鼓励隐式定位的中等阶段和一个弱监督推理的困难阶段。

Result: 实验结果表明，MedCLM在多个医学VQA基准测试中达到了最先进的性能。

Conclusion: MedCLM提供了一个可扩展的框架，用于开发与临床一致的医学视觉-语言模型。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [140] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: 本文研究了多模态语言模型如何处理视觉键值标记，并发现它们在感知密集型任务上的表现不佳。我们提出了改进视觉信息控制的方法，并指出如果语言模型能更好地控制视觉信息，其感知能力将显著提高。


<details>
  <summary>Details</summary>
Motivation: 尽管已有工作分析了VIT编码器和变压器激活，但我们还不了解为什么多模态语言模型在感知密集型任务上表现不佳。我们提供了一个被忽视的视角，通过检查流行的MLMs如何处理它们的视觉键值标记来解决这个问题。

Method: 我们研究了多模态语言模型（MLMs）如何处理视觉键值标记，并分析了视觉信息在语言模型中的流动。我们还探讨了如何控制语言模型中的视觉信息。

Result: 我们发现图像值标记包含足够的信息来执行多个感知密集型任务，而语言模型中的视觉信息在某些任务上比未经过MLM微调的视觉编码器（如SigLIP）少。此外，我们发现语言模型中后期层的输入无关图像键标记包含影响整体MLM感知能力的伪影。

Conclusion: 我们的研究揭示了键值标记在多模态系统中的作用，为更深入的机制可解释性铺平了道路，并提出了训练视觉编码器和语言模型组件的新方向。

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [141] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文介绍了 PaperTalker，这是一个用于学术演示视频生成的多智能体框架，并提供了 101 个研究论文及其作者创建的演示视频、幻灯片和演讲者元数据的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 学术演示视频已成为研究交流的重要媒介，但制作仍然非常耗时，通常需要数小时的幻灯片设计、录制和编辑才能制作出2到10分钟的视频。

Method: 我们设计了四个定制的评估指标——Meta Similarity、PresentArena、PresentQuiz 和 IP Memory，以衡量视频如何向观众传达论文的信息。我们提出了 PaperTalker，这是第一个用于学术演示视频生成的多智能体框架。

Result: 在 Paper2Video 上的实验表明，我们的方法生成的演示视频比现有基线更忠实和信息丰富。

Conclusion: 我们的方法生成的演示视频比现有基线更忠实和信息丰富，为自动化和现成的学术视频生成奠定了实际步骤。

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [142] [P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](https://arxiv.org/abs/2510.04503)
*Shuai Zhao,Xinyi Wu,Shiqian Zhao,Xiaobao Wu,Zhongliang Guo,Yanhao Jia,Anh Tuan Luu*

Main category: cs.CR

TL;DR: 本文提出了一种通用且有效的后门防御算法P2P，通过注入良性触发器并利用基于提示的学习对模型进行微调，从而覆盖原始恶意触发器的影响。实验结果表明，P2P在不同任务设置和攻击类型下均有效，能够中和恶意后门同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的防御策略在泛化方面存在局限性，仅适用于特定的攻击类型或任务设置。

Method: P2P通过在训练样本中注入带有安全替代标签的良性触发器，并利用基于提示的学习对模型进行微调，从而强制模型将触发器引起的表示与安全输出相关联。

Result: P2P算法在分类、数学推理和摘要生成任务中显著降低了攻击成功率。

Conclusion: P2P可以作为防御后门攻击的指导方针，并促进安全和可信的LLM社区的发展。

Abstract: During fine-tuning, large language models (LLMs) are increasingly vulnerable
to data-poisoning backdoor attacks, which compromise their reliability and
trustworthiness. However, existing defense strategies suffer from limited
generalization: they only work on specific attack types or task settings. In
this study, we propose Poison-to-Poison (P2P), a general and effective backdoor
defense algorithm. P2P injects benign triggers with safe alternative labels
into a subset of training samples and fine-tunes the model on this re-poisoned
dataset by leveraging prompt-based learning. This enforces the model to
associate trigger-induced representations with safe outputs, thereby overriding
the effects of original malicious triggers. Thanks to this robust and
generalizable trigger-based fine-tuning, P2P is effective across task settings
and attack types. Theoretically and empirically, we show that P2P can
neutralize malicious backdoors while preserving task performance. We conduct
extensive experiments on classification, mathematical reasoning, and summary
generation tasks, involving multiple state-of-the-art LLMs. The results
demonstrate that our P2P algorithm significantly reduces the attack success
rate compared with baseline models. We hope that the P2P can serve as a
guideline for defending against backdoor attacks and foster the development of
a secure and trustworthy LLM community.

</details>


### [143] [Proactive defense against LLM Jailbreak](https://arxiv.org/abs/2510.05052)
*Weiliang Zhao,Jinjun Peng,Daniel Ben-Levi,Zhou Yu,Junfeng Yang*

Main category: cs.CR

TL;DR: ProAct is a proactive defense framework that disrupts jailbreaking processes by providing misleading responses, effectively reducing attack success rates and enhancing LLM safety.


<details>
  <summary>Details</summary>
Motivation: Current defenses, primarily reactive and static, often fail to counter search-based attacks. The proliferation of powerful large language models (LLMs) has necessitated robust safety alignment, yet these models remain vulnerable to evolving adversarial attacks, including multi-turn jailbreaks that iteratively search for successful queries.

Method: Introduce ProAct, a novel proactive defense framework designed to disrupt and mislead autonomous jailbreaking processes by providing adversaries with 'spurious responses' that appear to be results of successful jailbreak attacks but contain no actual harmful content.

Result: Our method consistently and significantly reduces attack success rates by up to 92%. When combined with other defense frameworks, it further reduces the success rate of the latest attack strategies to 0%.

Conclusion: ProAct represents an orthogonal defense strategy that can serve as an additional guardrail to enhance LLM safety against the most effective jailbreaking attacks.

Abstract: The proliferation of powerful large language models (LLMs) has necessitated
robust safety alignment, yet these models remain vulnerable to evolving
adversarial attacks, including multi-turn jailbreaks that iteratively search
for successful queries. Current defenses, primarily reactive and static, often
fail to counter these search-based attacks. In this paper, we introduce ProAct,
a novel proactive defense framework designed to disrupt and mislead autonomous
jailbreaking processes. Our core idea is to intentionally provide adversaries
with "spurious responses" that appear to be results of successful jailbreak
attacks but contain no actual harmful content. These misleading responses
provide false signals to the attacker's internal optimization loop, causing the
adversarial search to terminate prematurely and effectively jailbreaking the
jailbreak. By conducting extensive experiments across state-of-the-art LLMs,
jailbreaking frameworks, and safety benchmarks, our method consistently and
significantly reduces attack success rates by up to 92\%. When combined with
other defense frameworks, it further reduces the success rate of the latest
attack strategies to 0\%. ProAct represents an orthogonal defense strategy that
can serve as an additional guardrail to enhance LLM safety against the most
effective jailbreaking attacks.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [144] [Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad](https://arxiv.org/abs/2510.05016)
*Lucas Carrit Delgado Pinheiro,Ziru Chen,Bruno Caixeta Piazza,Ness Shroff,Yingbin Liang,Yuan-Sen Ting,Huan Sun*

Main category: astro-ph.IM

TL;DR: 该研究通过IOAA考试评估了LLMs在天文学中的表现，发现尽管在理论考试中表现出色，但在概念推理、几何推理和空间可视化方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基准和评估主要集中在简单的问答上，无法评估实际研究所需的复杂推理。

Method: 系统地对五个最先进的LLMs进行了国际天文和天体物理奥林匹克竞赛(IOAA)考试的基准测试。

Result: Gemini 2.5 Pro和GPT-5在理论考试中平均得分分别为85.6%和84.2%，在所有四个IOAA理论考试中排名前两名。在数据分析考试中，GPT-5表现优异，而其他模型的表现则较低。

Conclusion: 虽然LLMs在理论考试中接近人类最高水平，但在天文学中作为自主研究代理之前仍需解决关键差距。

Abstract: While task-specific demonstrations show early success in applying large
language models (LLMs) to automate some astronomical research tasks, they only
provide incomplete views of all necessary capabilities in solving astronomy
problems, calling for more thorough understanding of LLMs' strengths and
limitations. So far, existing benchmarks and evaluations focus on simple
question-answering that primarily tests astronomical knowledge and fails to
evaluate the complex reasoning required for real-world research in the
discipline. Here, we address this gap by systematically benchmarking five
state-of-the-art LLMs on the International Olympiad on Astronomy and
Astrophysics (IOAA) exams, which are designed to examine deep conceptual
understanding, multi-step derivations, and multimodal analysis. With average
scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing
models) not only achieve gold medal level performance but also rank in the top
two among ~200-300 participants in all four IOAA theory exams evaluated
(2022-2025). In comparison, results on the data analysis exams show more
divergence. GPT-5 still excels in the exams with an 88.5% average score,
ranking top 10 among the participants in the four most recent IOAAs, while
other models' performances drop to 48-76%. Furthermore, our in-depth error
analysis underscores conceptual reasoning, geometric reasoning, and spatial
visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,
although LLMs approach peak human performance in theory exams, critical gaps
must be addressed before they can serve as autonomous research agents in
astronomy.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [145] [Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba](https://arxiv.org/abs/2510.04738)
*Baher Mohammad,Magauiya Zhussip,Stamatios Lefkimmiatis*

Main category: cs.SD

TL;DR: MAVE is a new autoregressive architecture for voice editing and TTS synthesis that combines Mamba and cross-attention for efficient and high-quality results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to introduce a novel architecture for text-conditioned voice editing and high-fidelity TTS synthesis that achieves state-of-the-art performance while being efficient in terms of memory and latency.

Method: MAVE is an autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. It integrates Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment.

Result: MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, outperforming leading autoregressive and diffusion models. It also requires significantly less memory and has similar latency compared to existing models.

Conclusion: MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.

Abstract: We introduce MAVE (Mamba with Cross-Attention for Voice Editing and
Synthesis), a novel autoregressive architecture for text-conditioned voice
editing and high-fidelity text-to-speech (TTS) synthesis, built on a
cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in
speech editing and very competitive results in zero-shot TTS, while not being
explicitly trained on the latter task, outperforming leading autoregressive and
diffusion models on diverse, real-world audio. By integrating Mamba for
efficient audio sequence modeling with cross-attention for precise
text-acoustic alignment, MAVE enables context-aware voice editing with
exceptional naturalness and speaker consistency. In pairwise human evaluations
on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2%
of listeners rated MAVE - edited speech as perceptually equal to the original,
while 24.8% prefered the original and 18.0% MAVE - demonstrating that in the
majority of cases edits are indistinguishable from the source. MAVE compares
favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and
standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE
exceeds VoiceCraft in both speaker similarity and naturalness, without
requiring multiple inference runs or post-processing. Remarkably, these quality
gains come with a significantly lower memory cost and approximately the same
latency: MAVE requires ~6x less memory than VoiceCraft during inference on
utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch
size 1). Our results demonstrate that MAVE establishes a new standard for
flexible, high-fidelity voice editing and synthesis through the synergistic
integration of structured state-space modeling and cross-modal attention.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [146] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: GEB is a new framework for optimistic exploration in reinforcement learning with human feedback, which addresses the issue of existing methods biasing exploration towards high-probability regions.


<details>
  <summary>Details</summary>
Motivation: Existing exploratory bonus methods to incentivize exploration often fail to realize optimism, unintentionally biasing exploration toward high-probability regions of the reference model.

Method: Introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle.

Result: GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones.

Conclusion: GEB offers both a principled and practical solution for optimistic exploration in RLHF.

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [147] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: 本文分析了Mamba的长期记忆衰减机制，并提出MemMamba架构，以改善长期遗忘问题，同时保持线性复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在效率和内存之间存在固有权衡，需要一种能够保留长期记忆同时保持线性复杂度的模型。

Method: 通过数学推导和信息论分析揭示了Mamba的内存衰减机制，并提出了MemMamba架构，结合状态摘要机制和跨层、跨标记注意力。

Result: MemMamba在PG19和Passkey Retrieval等长序列基准测试中显著优于现有的Mamba变体和Transformer，并在推理效率上提高了48%。

Conclusion: MemMamba在长序列建模中实现了复杂度-内存权衡的突破，提供了一种新的范式。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [148] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka is a systematic scaling law for diffusion language models, providing insights into their behavior and guiding future research.


<details>
  <summary>Details</summary>
Motivation: To introduce a systematic scaling law for diffusion language models that can guide training and inspire future research in the AI community.

Method: Quokka studies the scaling laws of diffusion language models, covering both compute-constrained and data-constrained regimes, and analyzing key modeling and optimization designs.

Result: Quokka offers a comprehensive understanding of diffusion language models' scaling behavior and serves as a valuable tool for researchers.

Conclusion: Quokka provides a systematic scaling law for diffusion language models, offering practical guidance and long-term inspiration for the AI community.

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [149] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [150] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为MACE的混合LLM系统，旨在通过迭代级调度来平衡推理延迟、模型准确性和资源约束，从而提高边缘平台上大型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）部署在边缘服务器上，在延迟敏感的应用中越来越被使用，如个性化助手、推荐和内容审核。然而，用户数据的非平稳性需要频繁重新训练，这在受限的GPU资源下引入了推理延迟和模型准确性之间的根本矛盾。现有的重新训练策略要么延迟模型更新，要么过度承诺资源进行重新训练，或者忽视迭代级重新训练粒度。

Method: 我们提出了MACE，一种混合LLM系统，它将并发推理（预填充、解码）和微调并置，并利用智能内存管理来最大化任务性能，同时保证推理吞吐量。MACE利用了一个见解，即并非所有的模型更新都同样影响输出对齐，并相应地分配GPU周期以平衡吞吐量、延迟和更新的新鲜度。

Result: 我们的基于轨迹的评估显示，MACE在减少推理延迟高达63%的同时，与连续重新训练相匹配或超越，并在资源限制下保持吞吐量。与定期重新训练相比，MACE改善了预填充、解码和微调阶段的延迟分解，并在NVIDIA AGX Orin上保持GPU利用率高于85%。

Conclusion: 这些结果表明，迭代级混合调度是将具有持续学习能力的LLM部署在边缘平台上的有前途的方向。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [151] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: 本文分析了掩码扩散语言模型的局限性，并提出了改进的训练和推理策略。


<details>
  <summary>Details</summary>
Motivation: 研究掩码扩散语言模型的局限性，以提高其生成过程的可控性和效率。

Method: 分析掩码扩散语言模型的局限性，并提出改进的训练和推理策略。

Result: 揭示了掩码扩散在并行生成和双向注意力方面的固有困难，并提出了有效的训练和推理策略。

Conclusion: 本文表明了掩码扩散在实现并行生成和双向注意力方面存在固有困难，并提出了最有效的训练和推理策略。

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [152] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: CAFL-L是一种改进的联邦学习方法，它通过拉格朗日对偶优化来适应设备级别的资源约束，从而在保持性能的同时减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的FedAvg方法未能显式考虑设备级别的资源约束，如能量、通信、内存和热预算，因此需要一种能够处理这些约束的联邦学习方法。

Method: CAFL-L通过拉格朗日对偶优化动态调整训练超参数（冻结深度、本地步骤、批量大小和通信压缩），同时通过梯度累积保持训练稳定性。

Result: 实验表明，CAFL-L在字符级语言模型上实现了比标准FedAvg更好的约束满足度，减少了20%的内存使用和95%的通信量。

Conclusion: CAFL-L在资源受限的边缘设备上具有实际应用价值，因为它在保持竞争性验证性能的同时，显著提高了约束满足度。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [153] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: AgentCaster is a framework for evaluating LLMs on tornado forecasting, revealing that human experts outperform models due to issues like hallucination and poor spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to evaluate Large Language Models (LLMs) on complex, high-impact, real-world tasks to assess their true readiness as reasoning agents.

Method: AgentCaster is a contamination-free framework employing multimodal LLMs end-to-end for the challenging, long-horizon task of tornado forecasting. It interprets heterogeneous spatiotemporal data from a high-resolution convection-allowing forecast archive and uses probabilistic tornado-risk polygon predictions verified against ground truths.

Result: Human experts significantly outperform state-of-the-art models, which demonstrate a strong tendency to hallucinate and overpredict risk intensity, struggle with precise geographic placement, and exhibit poor spatiotemporal reasoning in complex, dynamically evolving systems.

Conclusion: AgentCaster aims to advance research on improving LLM agents for challenging reasoning tasks in critical domains.

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [154] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: 本研究利用强化学习与可验证奖励方法分析了韩国词语链游戏，发现基于规则的奖励可能产生冲突，并提出课程学习方案来缓解这些问题。


<details>
  <summary>Details</summary>
Motivation: 我们希望研究强化学习与可验证奖励方法在不同语言中的应用，特别是在解决逻辑谜题方面。

Method: 我们使用强化学习与可验证奖励（RLVR）方法研究了韩国词语链游戏。

Result: 我们发现基于规则的奖励可能会自然冲突，并通过实验表明课程学习方案可以缓解这些冲突。

Conclusion: 我们的研究结果促使进一步研究不同语言中的谜题任务。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [155] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: This paper establishes new theoretical guarantees for kernel change-point detection (KCPD) under $m$-dependent data and validates its effectiveness for text segmentation using synthetic data and empirical studies.


<details>
  <summary>Details</summary>
Motivation: Real-world sequential data such as text exhibits strong dependencies, and existing theory for KCPD assumes independence. We aim to establish new guarantees for KCPD under $m$-dependent data and validate its effectiveness for text segmentation.

Method: We establish new guarantees for KCPD under $m$-dependent data and perform an LLM-based simulation to generate synthetic $m$-dependent text. We also present the first comprehensive empirical study of KCPD for text segmentation with modern embeddings.

Result: We prove consistency in the number of detected change points and weak consistency in their locations under mild additional assumptions. KCPD with text embeddings outperforms baselines in standard text segmentation metrics, and we demonstrate its practical effectiveness through a case study on Taylor Swift's tweets.

Conclusion: KCPD with text embeddings outperforms baselines in standard text segmentation metrics and demonstrates practical effectiveness for text segmentation tasks.

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [156] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: 本文提出了一种统一的方法，通过最小的干预来实现敏感信息的遗忘和对越狱攻击的鲁棒性，优于现有的防御方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛采用，需要更多的定制化以确保隐私保护和安全生成。本文旨在解决两个关键方面：敏感信息的遗忘和对越狱攻击的鲁棒性。

Method: 本文研究了各种受限优化公式，以统一的方式解决敏感信息遗忘和对越狱攻击的鲁棒性问题，通过找到对LLM权重的最小干预，使给定的词汇集不可达或将LLM嵌入到对定制攻击具有鲁棒性的区域。

Result: 实验结果表明，本文提出的最简单的基于点约束的干预方法在计算成本较低的情况下，性能优于最大最小干预方法，并且在与最先进的防御方法比较中表现出优越的性能。

Conclusion: 本文提出了一种统一的方法，通过最小的干预来实现敏感信息的遗忘和对越狱攻击的鲁棒性，优于现有的防御方法。

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [157] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: 本研究揭示了在公共卫生情感分析中，大型语言模型的上下文学习容易受到数据中毒攻击的影响，并提出了一种有效的防御方法——光谱签名防御，以保护数据集的完整性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨在公共卫生情感分析中，大型语言模型中的上下文学习（ICL）如何受到数据中毒攻击的影响，并寻找有效的防御方法。

Method: 研究使用了人类副流感病毒（HMPV）的推文，并在支持示例中引入了诸如同义词替换、否定插入和随机扰动等小的对抗性扰动，以测试ICL的稳定性。此外，应用了光谱签名防御来过滤中毒示例。

Result: 即使这些微小的扰动也导致了重大干扰，多达67%的情况的情感标签发生了翻转。应用光谱签名防御后，ICL的准确性保持在约46.7%，逻辑回归验证达到了100%的准确率。

Conclusion: 该研究扩展了之前关于ICL中毒的理论研究，将其应用于公共卫生话语分析这一实际且高风险的场景，突显了ICL在攻击下的脆弱性以及光谱防御在使AI系统更可靠方面的价值。

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [158] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: 本文研究了稀疏自编码器（SAEs）的可解释性与控制效用之间的关系，发现可解释性不足以作为控制性能的代理。我们提出了一种新的特征选择标准Delta Token Confidence，显著提高了控制性能，并揭示了可解释性和效用之间的差异。


<details>
  <summary>Details</summary>
Motivation: 现有的假设认为，SAEs的可解释特征自然能够有效控制模型行为，但这一假设尚未得到验证。我们需要确定更高的可解释性是否确实意味着更好的控制效用。

Method: 我们训练了90个SAEs，评估了它们的可解释性和控制效用，并通过Kendall等级系数进行了排名一致性分析。我们提出了Delta Token Confidence作为新的选择标准，以找到真正能控制LLM行为的特征。

Result: 我们的分析显示，可解释性和控制效用之间只有较弱的正相关（tau b约0.298）。使用Delta Token Confidence选择特征后，控制性能提高了52.52%。在选择高Delta Token Confidence特征后，可解释性和效用之间的相关性消失甚至变为负值。

Conclusion: 我们的分析表明，可解释性不足以作为控制性能的代理。我们提出的Delta Token Confidence选择标准显著提高了控制性能，并且在选择高Delta Token Confidence特征后，可解释性和效用之间的相关性消失甚至变为负值。

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [159] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 本文介绍了Token Hidden Reward (THR)，一种用于量化每个token对正确响应可能性影响的指标，并提出了一种THR引导的重新加权算法，以显式偏向探索或利用。实验表明该算法在多个数学推理基准上有效，并能与其他RL目标集成。


<details>
  <summary>Details</summary>
Motivation: 尽管基于可验证奖励的强化学习显著提升了大型语言模型的推理能力，但如何显式引导训练向探索或利用方向发展仍然是一个开放问题。

Method: 引入了Token Hidden Reward (THR)，这是一种量化每个token对Group Relative Policy Optimization (GRPO)下正确响应可能性影响的token级指标，并提出了一种THR引导的重新加权算法来调节GRPO的学习信号以显式偏向探索或利用。

Result: 通过放大具有正THR值的token并削弱负的token，该算法提高了贪心解码的准确性，偏向利用；相反的策略在Pass@K准确性上取得了稳定的提升，偏向探索。此外，该算法与GSPO等其他RL目标无缝集成，并在包括Llama在内的不同架构上具有泛化能力。

Conclusion: THR作为一种原理明确且细粒度的机制，为在RL调优的LLM中动态控制探索和利用提供了新的工具。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [160] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: IniLoRA is a new initialization method for LoRA that improves performance by approximating original model weights, with variants offering further improvements.


<details>
  <summary>Details</summary>
Motivation: LoRA has limitations in effectively activating and leveraging the original model weights due to its initialization of low-rank matrices whose product is zero, creating a potential bottleneck for optimal performance.

Method: IniLoRA is a novel initialization strategy that initializes the low-rank matrices to closely approximate the original model weights, and two variants, IniLoRA-α and IniLoRA-β, are introduced with distinct initialization methods.

Result: Experimental results show that IniLoRA outperforms LoRA across various models and tasks, and the variants IniLoRA-α and IniLoRA-β further improve performance.

Conclusion: IniLoRA achieves better performance than LoRA across a range of models and tasks, and its variants IniLoRA-α and IniLoRA-β further enhance performance.

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [161] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: RAPO enhances RLVR by promoting broader exploration, improving problem-solving performance, and surpassing the limitations of pretrained models.


<details>
  <summary>Details</summary>
Motivation: The limitation of RLVR-trained models over their pretrained bases diminishes as the sampling budget increases, due to the mode-seeking behavior of the reverse KL divergence regularizer.

Method: RAPO (Rewards-Aware Policy Optimization) utilizes the forward KL penalty to replace the reverse KL penalty for out-of-distribution exploration and reweights the reference policy to facilitate adaptive in-distribution exploration.

Result: RAPO improves problem-solving performance on AIME2024 and AIME2025 datasets, enabling models to solve previously intractable problems.

Conclusion: RAPO consistently improves problem-solving performance and enables models to surpass the base model's performance ceiling, solving previously intractable problems.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [162] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: This paper introduces LLM Chemistry, a framework to measure and optimize the synergy among multiple large language models, showing that their combined performance depends on factors like task type and model diversity.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to multi-LLM collaboration rely on implicit selection and output assessment without analyzing whether models complement or conflict, leading to suboptimal performance.

Method: The paper introduces LLM Chemistry, a framework that measures synergistic or antagonistic behaviors in LLM combinations by analyzing interaction dependencies and recommending optimal model ensembles.

Result: Theoretical analysis shows that LLM chemistry is most evident under heterogeneous model profiles, with task type, group size, and complexity shaping its impact. Evaluation on classification, summarization, and program repair tasks supports these findings.

Conclusion: LLM Chemistry establishes a diagnostic factor in multi-LLM systems and provides a foundation for ensemble recommendation.

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [163] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: 本文提出了一个针对扩散语言模型的有原则的在线强化学习算法AGRPO，该算法通过蒙特卡洛采样计算无偏策略梯度估计，使其成为第一个可处理的、忠实的策略梯度方法适应扩散语言模型。实验结果表明，AGRPO在多个数学/推理任务中表现出色，相较于基线模型和现有RL方法取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的工作已经成功将dLLMs预训练到与自回归LLM相当的水平，但dLLMs尚未受益于现代的后训练技术，例如强化学习（RL），这些技术已被证明对自回归模型有效。传统LLM设计的算法并不直接适用于扩散框架，因为建模假设存在内在差异。此外，现有的dLLM RL后训练尝试依赖于没有理论基础的启发式目标。

Method: 我们提出了Amortized Group Relative Policy Optimization (AGRPO)，这是一种专门针对dLLMs设计的有原则的在线RL算法。AGRPO使用蒙特卡洛采样来计算无偏策略梯度估计，使其成为第一个可处理的、忠实的策略梯度方法适应dLLMs。

Result: 我们在不同的数学/推理任务上展示了AGRPO的有效性，这是一个常见的RL与LLM设置，相对于基线LLaDA-8B-Instruct模型，在GSM8K上实现了高达+7.6%的绝对增益，在Countdown任务上实现了3.8倍的性能提升，并且比类似的RL方法如diffu-GRPO实现了1.3倍的性能增益。这些增益在推理时的不同采样步骤数中持续存在，实现了更好的计算和性能权衡。

Conclusion: 我们的结果表明，在线RL算法可以以有原则的方式扩展到扩散LLM，同时保持理论上的严谨性和实际效果。

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [164] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的交叉熵分解方法，发现误差熵是影响模型行为的主要因素，这为理解大型语言模型提供了更准确的缩放定律。


<details>
  <summary>Details</summary>
Motivation: 由于交叉熵缩放定律在非常大的模型规模下失效，我们需要寻找更准确的模型行为描述方法。

Method: 我们引入了一种新的交叉熵分解方法，将其分为三个部分：误差熵、自我对齐和置信度，并通过理论和实证研究验证了这种分解方法的有效性。

Result: 我们发现只有误差熵遵循稳健的幂律缩放，而其他两个项基本保持不变。此外，误差熵在小模型中占交叉熵的主要部分，但随着模型变大，其比例会减少。

Conclusion: 我们的研究确立了误差熵的缩放定律作为对模型行为更准确的描述，我们认为它将在训练、理解和未来开发大型语言模型中具有广泛的应用。

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [165] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: SFPO is a framework that improves the stability and efficiency of reinforcement learning in large language models by decomposing each step into three stages.


<details>
  <summary>Details</summary>
Motivation: On-policy algorithms such as GRPO often suffer from unstable updates and inefficient exploration due to noisy gradients from low-quality rollouts.

Method: Slow-Fast Policy Optimization (SFPO), which decomposes each step into three stages: a short fast trajectory of inner steps on the same batch, a reposition mechanism to control off-policy drift, and a final slow correction.

Result: SFPO outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks. It also achieves up to 4.93 times fewer rollouts and a 4.19 times reduction in wall-clock time to match GRPO's best accuracy.

Conclusion: SFPO consistently improves stability, reduces rollouts, and accelerates convergence of reasoning RL training.

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [166] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: 本文对比了自回归语言模型和扩散语言模型的性能，发现DLMs在并行处理上有优势，但在长上下文处理上存在不足。通过块解码方法可以改善这一问题，并指出减少采样步骤有助于提升DLM的推理速度。


<details>
  <summary>Details</summary>
Motivation: 尽管DLMs具有更高的算术强度，但它们在处理长上下文时表现不佳。因此，需要研究如何改进DLMs以实现更好的性能。

Method: 本文通过理论分析和性能评估，比较了ARMs和DLMs的性能特征，探索了块解码方法以及批量推理中的性能权衡。

Result: DLMs在并行处理方面表现出更高的算术强度，但在长上下文处理上不如ARMs。块解码方法可以提高DLMs的性能，同时保持良好的扩展性。此外，ARMs在批量推理中表现出更高的吞吐量。

Conclusion: 本文总结了扩散语言模型（DLMs）和自回归语言模型（ARMs）在性能上的差异，并指出了DLMs在长上下文处理上的局限性。同时，提出了通过块解码来提高DLMs的性能，并强调了减少采样步骤对加速DLM推理的重要性。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [167] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Wave-PDE Nets 是一种基于二阶波动方程的神经架构，利用可微分模拟实现高效的传播，表现出优于 Transformer 的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 提出 Wave-PDE Nets 作为注意力机制和一阶状态空间模型的有力替代方案，旨在提高计算效率和性能。

Method: Wave-PDE Nets 是一种神经架构，其基本操作是二阶波动方程的可微分模拟。每一层通过具有可训练空间速度 c(x) 和阻尼 γ(x) 的介质传播其隐藏状态。基于 FFT 的辛谱求解器实现了这种传播。

Result: 在语言和视觉基准测试中，Wave-PDE Nets 的表现与 Transformer 相当或更优，并展示了优越的实际效率，减少了高达 30% 的墙钟时间以及 25% 的峰值内存。

Conclusion: Wave-PDE Nets 作为一种具有强大物理归纳偏置的计算高效且稳健的架构，展现出良好的前景。

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [168] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于梯度的算法，用于改进高斯PID（GPID）的计算效率，并通过学习信息保留编码器将任意输入分布转换为成对高斯随机变量，以扩展其应用范围。实验结果表明，该方法在多种合成示例和大规模多模态基准测试中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的PID方法在处理连续和高维模态时成本高且不准确，因此需要一种更高效和准确的方法来解决这个问题。

Method: 本文提出了一个基于梯度的算法来改进高斯PID（GPID）的计算效率，并通过学习信息保留编码器将任意输入分布转换为成对高斯随机变量，以扩展其应用范围。

Result: 实验结果表明，本文提出的算法在多种合成示例和大规模多模态基准测试中提供了更准确和高效的PID估计。

Conclusion: 本文提出了一种新的基于梯度的算法，可以显著提高高斯PID（GPID）的计算效率，并通过学习信息保留编码器将任意输入分布转换为成对高斯随机变量，从而扩展了GPID的应用范围。实验结果表明，该方法在多种合成示例和大规模多模态基准测试中优于现有基线方法。

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [169] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDiR是一种新的推理框架，结合连续潜在表示和潜在扩散模型的迭代优化能力，提高了LLM的推理性能。


<details>
  <summary>Details</summary>
Motivation: LLM的自回归解码可能限制重新审视和优化早期标记的能力，导致探索多样化解决方案的效率低下。因此，需要一种新的推理框架来解决这些问题。

Method: LaDiR通过使用变分自编码器（VAE）构建结构化的潜在推理空间，将文本推理步骤编码为思想标记块，并利用潜在扩散模型学习去噪块的潜在思想标记，采用逐块双向注意力掩码，实现更长的视野和迭代优化。

Result: LaDiR在数学推理和规划基准测试中表现出更高的准确性、多样性和可解释性，证明了其有效性。

Conclusion: LaDiR展示了在数学推理和规划基准测试中，相比现有的自回归、基于扩散和潜在推理方法，能够持续提高准确性、多样性和可解释性，揭示了文本推理的新范式。

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [170] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE框架通过结构化、渐进式更新防止上下文崩溃，提升LLM系统的性能和效率，无需标签监督。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）应用如代理和领域特定推理越来越多地依赖于上下文适应，而不是权重更新。然而，现有方法在可用性方面有所提升，但常常受到简洁偏差和上下文崩溃的影响。

Method: ACE（Agentic Context Engineering）框架将上下文视为不断演进的剧本，通过生成、反思和整理的模块化过程积累、精炼和组织策略。

Result: ACE在代理和领域特定基准测试中，无论是离线（如系统提示）还是在线（如代理记忆），都能优化上下文，始终优于强大的基线：在代理上提高了10.6%，在金融领域提高了8.6%。此外，ACE能有效适应而无需标记监督，而是利用自然执行反馈。在AppWorld排行榜上，ACE在总体平均值上与顶级生产级代理相当，并在更难的测试挑战分割上超越了它，尽管使用的是较小的开源模型。

Conclusion: 这些结果表明，全面且不断演化的上下文能够实现可扩展、高效且自我改进的LLM系统，且开销较低。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [171] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: 本文提出ONNX-Bench基准和ONNX-Net表示方法，实现了对任何神经网络架构的快速性能评估。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络架构搜索（NAS）方法在性能评估方面存在瓶颈，且大多数方法局限于基于单元的搜索空间和特定的图编码方式，限制了其灵活性和可扩展性。

Method: 本文提出了ONNX-Bench基准和ONNX-Net表示方法，通过自然语言描述来编码神经网络架构，并利用性能预测器进行评估。

Result: 实验表明，使用少量预训练样本即可在不同搜索空间中实现强大的零样本性能，使得对任何神经网络架构的评估变得即时。

Conclusion: 本文提出了一种新的基准ONNX-Bench，以及一种基于自然语言描述的神经网络表示方法ONNX-Net，能够实现对任何神经网络架构的快速性能评估。

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [172] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: 本文扩展了Structured State-Space Duality (SSD) 到一般的对角SSM，证明了其在保持训练复杂度下界的同时支持更丰富的动态，并建立了SSM与1-半分离掩码注意力等价的条件，同时指出这种对偶性无法扩展到标准softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 为了加强循环SSM和Transformer之间的桥梁，并拓宽设计表达性强且高效的序列模型的空间。

Method: 通过形式化和推广SSD，将SSD从标量-单位情况扩展到一般的对角SSM，并建立了SSM与1-半分离掩码注意力等价的必要充分条件。

Result: 扩展了SSD到一般的对角SSM，证明了这些对角SSM在保持训练复杂度下界的同时支持更丰富的动态；建立了SSM与1-半分离掩码注意力等价的必要充分条件；并展示了这种对偶性无法扩展到标准softmax注意力。

Conclusion: 这些结果加强了循环SSM和Transformer之间的桥梁，并拓宽了表达性强且高效的序列模型的设计空间。

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [173] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: Reinforce-Ada是一种自适应采样框架，用于大型语言模型的在线强化学习后训练，能够持续重新分配采样努力到具有最大不确定性和学习潜力的提示上，从而提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如GVM-RAFT通过为每个提示动态分配推理预算来最小化预算约束下的随机梯度方差。受此见解的启发，我们提出了Reinforce-Ada，以解决大规模语言模型在推理任务中应用强化学习时由于固定和均匀采样响应而遇到的不稳定梯度估计瓶颈。

Method: 我们提出了Reinforce-Ada，这是一个自适应采样框架，用于大型语言模型的在线强化学习后训练，它持续重新分配采样努力到具有最大不确定性和学习潜力的提示上。

Result: 实验证明，与GRPO相比，Reinforce-Ada加速了收敛并提高了最终性能，特别是在使用平衡采样变体时。

Conclusion: 我们的工作突显了在推理能力的大型语言模型中进行高效且可靠的强化学习中，感知方差的自适应数据整理的核心作用。

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [174] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为DIT的方法，用于描述微调引起的权重变化，并展示了其在两个概念验证设置中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的微调语言模型的权重变化通常难以解释，而微调数据集往往不可用或太大，无法直接处理。因此，需要一种方法来理解这些权重变化。

Method: 本文提出了一种名为Diff Interpretation Tuning (DIT)的方法，该方法使用合成的、带标签的权重差异来训练一个DIT适配器，该适配器可以应用于兼容的微调模型，使其能够描述其变化。

Result: 在两个概念验证设置中，即报告隐藏行为和总结微调知识，本文的方法使模型能够使用准确的自然语言描述其微调引起的修改。

Conclusion: 本文提出了一个名为DIT的方法，用于描述微调引起的权重变化，并展示了其在两个概念验证设置中的有效性。

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [175] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法BVPO，用于优化大型推理模型与人类偏好对齐的问题。该方法通过混合两种梯度估计器来减少方差，从而提高模型的对齐度和推理性能。实验结果表明，BVPO在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在生成最终答案前会生成中间推理轨迹，这在多步骤和数学任务中表现出色。然而，将这些模型与人类偏好对齐仍然是一个未被充分研究的问题。现有的方法在计算上是不可行的，因此需要一种新的方法来解决这一问题。

Method: 本文提出了Bias-Variance Optimized Preference Optimization (BVPO) 方法，该方法通过混合两种梯度估计器（高方差的基于轨迹的估计器和低方差的空轨迹估计器）来优化偏好对齐。

Result: 实验结果显示，BVPO在AlpacaEval~2和Arena-Hard基准测试中分别比最佳基线提高了7.8分和6.8分。此外，BVPO还提升了基础模型在六个数学推理基准测试中的平均性能，最高提高了4.0分。

Conclusion: 本文提出了一种名为BVPO的方法，通过优化偏差-方差权衡来解决大型推理模型与人类偏好对齐的问题。实验结果表明，BVPO在多个基准测试中显著提高了模型的对齐度和推理性能。

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [176] [Lightweight Prompt Engineering for Cognitive Alignment in Educational AI: A OneClickQuiz Case Study](https://arxiv.org/abs/2510.03374)
*Antoun Yaacoub,Zainab Assaghir,Jérôme Da-Rugna*

Main category: cs.CY

TL;DR: 本研究探讨了轻量级提示工程策略对AI生成问题在教育技术中的认知对齐影响，发现详细提示有助于提高生成内容的教学一致性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨轻量级提示工程策略对OneClickQuiz（一个利用生成式AI的Moodle插件）中AI生成问题的认知对齐的影响，以解决AI生成内容的质量和教学一致性问题。

Method: 本研究评估了三种提示变体——详细基线、简化版本和基于角色的方法——在Bloom认知分类法的知识、应用和分析层次上的影响。使用自动化分类模型和人工评审进行评估。

Result: 研究结果表明，明确且详细的提示对于精确的认知对齐至关重要。虽然简化和基于角色的提示会产生清晰且相关的问题，但它们经常与预期的Bloom层级不一致，生成过于复杂或偏离预期认知目标的输出。

Conclusion: 本研究强调了战略性提示工程在促进教育技术中AI驱动解决方案的教育学合理性的重要性，并建议优化AI以在学习分析和智能学习环境中生成高质量内容。

Abstract: The rapid integration of Artificial Intelligence (AI) into educational
technology promises to revolutionize content creation and assessment. However,
the quality and pedagogical alignment of AI-generated content remain critical
challenges. This paper investigates the impact of lightweight prompt
engineering strategies on the cognitive alignment of AI-generated questions
within OneClickQuiz, a Moodle plugin leveraging generative AI. We evaluate
three prompt variants-a detailed baseline, a simpler version, and a
persona-based approach-across Knowledge, Application, and Analysis levels of
Bloom's Taxonomy. Utilizing an automated classification model (from prior work)
and human review, our findings demonstrate that explicit, detailed prompts are
crucial for precise cognitive alignment. While simpler and persona-based
prompts yield clear and relevant questions, they frequently misalign with
intended Bloom's levels, generating outputs that are either too complex or
deviate from the desired cognitive objective. This study underscores the
importance of strategic prompt engineering in fostering pedagogically sound
AI-driven educational solutions and advises on optimizing AI for quality
content generation in learning analytics and smart learning environments.

</details>


### [177] [Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making](https://arxiv.org/abs/2510.03514)
*Toby Drinkall*

Main category: cs.CY

TL;DR: 本研究开发了一个基准测试框架，用于评估大型语言模型（LLMs）在模拟冲突中的法律和道德风险。结果显示，现成的LLMs在模拟冲突环境中表现出令人担忧且不可预测的目标行为，所有模型都违反了区分原则，针对民用物体。研究还发现模型之间的差异很大，这表明模型选择是军事行动中对可接受的法律和道德风险配置的选择。


<details>
  <summary>Details</summary>
Motivation: 随着军事组织考虑将大型语言模型（LLMs）集成到指挥与控制（C2）系统中以进行规划和决策支持，了解其行为倾向至关重要。

Method: 本研究开发了一个基准测试框架，用于评估在冲突行为中法律和道德风险的方面，通过比较作为代理的大型语言模型（LLMs）在多轮模拟冲突中的表现。引入了四个基于国际人道法（IHL）和军事学说的指标：民用目标率（CTR）和双重用途目标率（DTR）评估对法律目标原则的遵守情况，而平均和最大模拟非战斗人员伤亡价值（SNCV）量化对平民伤害的容忍度。

Result: 研究结果表明，现成的LLMs在模拟冲突环境中表现出令人担忧且不可预测的目标行为。所有模型都违反了区分原则，针对民用物体，违规率在16.7%到66.7%之间。在危机模拟过程中，伤害容忍度上升，平均SNCV从早期回合的16.5增加到后期回合的27.7。显著的模型间差异出现：LLaMA-3.1在每次模拟中平均选择3.47次民用袭击，平均SNCV为28.4，而Gemini-2.5选择0.90次民用袭击，平均SNCV为17.6。这些差异表明，模型选择在军事行动中构成了对可接受的法律和道德风险配置的选择。

Conclusion: 本研究旨在提供一个潜在行为风险的证明概念，这些风险可能从将大型语言模型（LLMs）用于决策支持系统（AI DSS）中出现，并且提供一个可重复的基准测试框架，以及用于标准化部署前测试的可解释指标。

Abstract: As military organisations consider integrating large language models (LLMs)
into command and control (C2) systems for planning and decision support,
understanding their behavioural tendencies is critical. This study develops a
benchmarking framework for evaluating aspects of legal and moral risk in
targeting behaviour by comparing LLMs acting as agents in multi-turn simulated
conflict. We introduce four metrics grounded in International Humanitarian Law
(IHL) and military doctrine: Civilian Target Rate (CTR) and Dual-use Target
Rate (DTR) assess compliance with legal targeting principles, while Mean and
Max Simulated Non-combatant Casualty Value (SNCV) quantify tolerance for
civilian harm.
  We evaluate three frontier models, GPT-4o, Gemini-2.5, and LLaMA-3.1, through
90 multi-agent, multi-turn crisis simulations across three geographic regions.
Our findings reveal that off-the-shelf LLMs exhibit concerning and
unpredictable targeting behaviour in simulated conflict environments. All
models violated the IHL principle of distinction by targeting civilian objects,
with breach rates ranging from 16.7% to 66.7%. Harm tolerance escalated through
crisis simulations with MeanSNCV increasing from 16.5 in early turns to 27.7 in
late turns. Significant inter-model variation emerged: LLaMA-3.1 selected an
average of 3.47 civilian strikes per simulation with MeanSNCV of 28.4, while
Gemini-2.5 selected 0.90 civilian strikes with MeanSNCV of 17.6. These
differences indicate that model selection for deployment constitutes a choice
about acceptable legal and moral risk profiles in military operations.
  This work seeks to provide a proof-of-concept of potential behavioural risks
that could emerge from the use of LLMs in Decision Support Systems (AI DSS) as
well as a reproducible benchmarking framework with interpretable metrics for
standardising pre-deployment testing.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [178] [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
*Hyunjun Kim,Sejong Kim*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs
can synthesize reusable browser automation programs from natural language goals
by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates
seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,
Facebook-like, Discord-like, and Threads-like, covering 681 tasks across
interaction complexity and targeting difficulty. Our end-to-end protocol
validates generated code via static checks, sandboxed execution, and outcome
verification including DOM assertions and database snapshots, and includes a
safety suite for scraping, spam/abuse, and credential/privacy prompts. Across
2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8
percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,
and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at
91.7 percent but fail on complex workflows at 0.0 percent, and none meet
production-quality coding practices despite functional completion. We release
our complete benchmark pipeline, evaluation framework, and experimental results
to enable reproducible assessment of macro synthesis for web automation.

</details>


### [179] [Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches](https://arxiv.org/abs/2510.04905)
*Yicheng Tao,Yao Qin,Yepang Liu*

Main category: cs.SE

TL;DR: 本文对检索增强代码生成（RACG）的研究进行了全面的综述，重点介绍了仓库级别的方法。我们从多个维度对现有工作进行分类，并总结了广泛使用的数据集和基准，分析了当前的局限性，同时指出了未来研究的关键挑战和机遇。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型（LLMs）的进步显著提高了自动代码生成。虽然函数级和文件级生成取得了有希望的结果，但现实世界的软件开发通常需要跨整个存储库进行推理。这催生了仓库级别的代码生成（RLCG）这一具有挑战性的任务，其中模型必须捕捉长距离依赖关系，确保全局语义一致性，并生成跨越多个文件或模块的连贯代码。为了应对这些挑战，检索增强生成（RAG）已成为一种强大的范式，将外部检索机制与LLMs结合，增强了上下文感知能力和可扩展性。

Method: 本文对检索增强代码生成（RACG）的研究进行了全面的综述，重点介绍了仓库级别的方法。我们从多个维度对现有工作进行分类，包括生成策略、检索模态、模型架构、训练范式和评估协议。

Result: 本文总结了广泛使用的数据集和基准，分析了当前的局限性，并概述了未来研究的关键挑战和机遇。

Conclusion: 本文旨在建立一个统一的分析框架，以理解这个快速发展的领域，并激发在AI驱动的软件工程中的持续进步。

Abstract: Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文提出了一种利用文档级知识图谱表示临床文档的方法，显著提高了ICD-9编码的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 手动编码临床文档既困难又耗时，难以大规模应用。自动化编码可以减轻这一负担，提高结构化临床数据的可用性和准确性。然而，自动编码任务具有挑战性，因为它需要映射到高维和长尾的目标空间。

Method: 本文使用文档级知识图谱（KG）来表示输入临床文档，并将其集成到最先进的ICD编码架构PLM-ICD中，以评估其在自动化ICD-9编码中的有效性。

Result: 实验结果表明，使用文档级知识图谱的方法在ICD-9编码任务中取得了高达3.20%的宏F1分数提升，并提高了训练效率。此外，该方法还展示了比仅基于文本的基线方法更好的可解释性潜力。

Conclusion: 本文提出了一种利用文档级知识图谱（KG）来表示输入临床文档的方法，该方法在保留90%信息的同时，将原始文本减少了23%。实验结果显示，这种方法在ICD-9编码任务中取得了更高的宏F1分数，并提高了训练效率。

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [2] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [3] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
*JungHo Jung,Junhyun Lee*

Main category: cs.CL

TL;DR: 本文从正则化角度研究了多任务学习在语音到文本翻译中的应用，并提出了正则化范围的概念，以提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 端到端语音到文本翻译通常面临配对语音-文本数据不足的问题。为克服这一缺点，可以利用机器翻译任务中的双语数据并进行多任务学习。

Method: 本文从正则化角度出发，探讨了如何在模态内和模态间对序列进行正则化。研究了不同模态的一致性正则化和相同模态的R-drop对总正则化的贡献，并展示了MT损失系数作为MTL设置中的另一个正则化来源。

Result: 实验表明，在正则化范围内调整超参数可以在MuST-C数据集上实现接近最先进的性能。

Conclusion: 本文通过引入三种正则化来源，提出了在高维空间中的最优正则化轮廓，称为正则化范围，并在MuST-C数据集上实现了接近最先进的性能。

Abstract: End-to-end speech-to-text translation typically suffers from the scarcity of
paired speech-text data. One way to overcome this shortcoming is to utilize the
bitext data from the Machine Translation (MT) task and perform Multi-Task
Learning (MTL). In this paper, we formulate MTL from a regularization
perspective and explore how sequences can be regularized within and across
modalities. By thoroughly investigating the effect of consistency
regularization (different modality) and R-drop (same modality), we show how
they respectively contribute to the total regularization. We also demonstrate
that the coefficient of MT loss serves as another source of regularization in
the MTL setting. With these three sources of regularization, we introduce the
optimal regularization contour in the high-dimensional space, called the
regularization horizon. Experiments show that tuning the hyperparameters within
the regularization horizon achieves near state-of-the-art performance on the
MuST-C dataset.

</details>


### [4] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

TL;DR: 研究引入了一个评估框架来评估大型语言模型在营销创意方面的能力，并发现人类评估仍然是必要的。


<details>
  <summary>Details</summary>
Motivation: 为了评估大型语言模型在营销创意方面的表现，需要一个专门的评估框架。此外，研究还旨在探讨自动化裁判是否可以替代人类评估。

Method: 引入了Creativity Benchmark，这是一个用于评估大型语言模型（LLMs）在营销创意方面的评估框架。分析了人类配对偏好，并使用Bradley-Terry模型进行分析。还分析了模型多样性，并比较了三种LLM作为裁判的设置。

Result: 研究发现，没有一个模型在所有品牌或提示类型中占优，顶级模型仅比最低模型大约61%的时间获胜。此外，自动化裁判与人类评估之间存在弱且不一致的相关性，并且存在裁判特定的偏见。传统的创造力测试在品牌约束任务中的转移效果有限。

Conclusion: 研究结果强调了需要专家人类评估和多样性意识的工作流程。

Abstract: We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [5] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了一种名为 CTCC 的新型基于规则的指纹框架，通过编码多个对话回合中的上下文相关性来实现更隐蔽和鲁棒的版权验证。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在隐蔽性、鲁棒性和泛化能力之间存在固有的权衡，容易被检测到、受到对抗性修改的影响，或者一旦指纹被揭示就会失效。因此需要一种更可靠和实用的解决方案。

Method: CTCC 是一种基于规则的指纹框架，通过编码多个对话回合中的上下文相关性（如反事实）来实现指纹验证，而不是依赖于标记级别的或单次触发器。

Result: CTCC 在多个大型语言模型架构上的实验表明，它在隐蔽性和鲁棒性方面优于现有方法。

Conclusion: CTCC 是一种可靠且实用的解决方案，适用于现实世界中大型语言模型的版权验证场景。

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [6] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

TL;DR: 本文研究了语言模型在时间选择中的偏好及其可操控性，并提出了AI助手设计的相关建议。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否表现出未来导向或现在导向的偏好，并探讨这些偏好是否可以系统地被操控。

Method: 使用适应人类实验协议的测试方法，评估多个语言模型在时间交换任务中的表现，并将其与人类决策者样本进行比较。引入了一个操作性指标，即时间取向的可操控性（MTO）。

Result: 推理导向的模型（如DeepSeek-Reasoner和grok-3-mini）在未来的提示下选择较晚的选项，但仅部分根据身份或地理位置调整决策。正确推理时间取向的模型会为自己作为AI决策者内化未来导向。

Conclusion: 本文讨论了AI助手的设计含义，应与异质性、长期目标对齐，并提出了个性化上下文校准和社会意识部署的研究议程。

Abstract: We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [7] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

TL;DR: 本研究分析了小规模语言模型在多次回答相同问题时的一致性，并发现小型模型在低推理温度下的问题一致性回答数量通常在50%-80%之间，而中型模型显示出更高的答案一致性水平。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨小规模语言模型在多次回答相同问题时的一致性，并评估一致性对准确率的影响以及其中的权衡。

Method: 研究分析了开源小规模语言模型（2B-8B参数）在多次回答相同问题时的一致性，考虑了不同的推理温度、小型与中型模型（50B-80B）、微调与基础模型以及其他参数，并提出了新的分析和图形工具。

Result: 结果表明，小型模型在低推理温度下能够一致回答的问题数量通常在50%-80%之间，而中型模型表现出更高的答案一致性水平。此外，一致回答的准确性似乎与总体准确性有合理的相关性。

Conclusion: 研究结果表明，小型模型在低推理温度下的问题一致性回答数量通常在50%-80%之间，而中型模型显示出更高的答案一致性水平。

Abstract: This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [8] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

TL;DR: 本研究通过分析指令调优模型的潜在空间，揭示了拒绝有害提示的行为机制，并提出了通过操纵潜在空间实现安全行为干预的方法。


<details>
  <summary>Details</summary>
Motivation: 拒绝有害提示是指令调优大型语言模型（LLMs）的关键安全行为，但这种行为的内部原因仍不清楚。

Method: 我们使用稀疏自编码器（SAEs）对残差流激活进行训练，并在SAE潜在空间中寻找特征集，这些特征集的消融使模型从拒绝变为合规，从而创建了一个越狱。我们的搜索分为三个阶段：(1) 拒绝方向：找到一个中介拒绝的方向并收集靠近该方向的SAE特征；(2) 贪心过滤：精简到一个最小集；(3) 交互发现：拟合一个因子分解机（FM），捕捉剩余活跃特征和最小集之间的非线性交互。

Result: 我们的方法产生了一组广泛的越狱关键特征，提供了对拒绝机制基础的见解。此外，我们发现了在早期特征被抑制之前保持休眠状态的冗余特征。

Conclusion: 我们的研究展示了通过操纵可解释的潜在空间来实现对安全行为的细粒度审计和针对性干预的潜力。

Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [9] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

TL;DR: 本研究提出了一种基于内容质量和参考文献有效性的评估方法，并通过迭代提示提升ChatGPT的写作性能，以解决学术写作中的伦理问题。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（如ChatGPT）在学术写作中的使用日益增加，但存在错误或虚构的参考文献等问题，引发伦理担忧。此外，当前的内容质量评估通常依赖于主观的人工判断，这既耗时又缺乏客观性，可能影响一致性与可靠性。因此，需要一种更客观、定量的评估方法来提高LLMs的研究提案写作能力。

Method: 本文提出了两个关键的评估指标——内容质量和参考文献有效性，并基于这两个指标的分数提出了一种迭代提示方法。

Result: 实验结果表明，所提出的指标为评估ChatGPT的写作表现提供了一个客观、定量的框架。此外，迭代提示方法显著提高了内容质量，同时减少了参考文献的不准确性和虚假情况。

Conclusion: 本研究提出的指标为评估ChatGPT的写作表现提供了一个客观、定量的框架，并且迭代提示方法显著提高了内容质量，同时减少了参考文献的不准确性和虚假情况，解决了学术环境中的关键伦理问题。

Abstract: Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [10] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 本研究提出了一种基于大型语言模型（LLM）的方法，用于生成交通模型中的个人旅行日记。该方法利用开放数据生成日记，并通过量化指标验证其现实性。结果显示，LLM生成的日记在整体现实性上与传统方法相当，甚至在某些方面表现更好，证明了其在零样本设置下的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的交通模型依赖于大量的专有家庭旅行调查数据，而本研究旨在探索一种更高效且可行的方法，利用开放数据生成旅行日记，以提高模型的适用性和灵活性。

Method: 该研究提出了一种基于大型语言模型（LLM）的方法，用于生成代理交通模型中的个人旅行日记。该方法利用开放源代码的美国社区调查（ACS）和智能位置数据库（SLD）数据随机生成人物形象，然后通过直接提示生成日记。研究还引入了一种新的“一对一群体现实性评分”，结合四个指标（行程数量评分、间隔评分、目的评分和方式评分）进行验证，并使用Jensen-Shannon散度测量生成日记与真实日记之间的分布相似性。

Result: LLM生成的日记在整体现实性上与传统方法相当（LLM平均：0.485 vs. 0.455），并在确定行程目的和一致性方面表现更优，而传统方法在行程数量和活动持续时间的数值估计上更优。此外，LLM在零样本设置下表现出统计代表性（LLM平均：0.612 vs. 0.435）。

Conclusion: 该研究展示了大型语言模型（LLM）在生成个体旅行日记方面的有效性，并通过量化指标验证了其现实性。结果表明，LLM生成的日记在整体现实性上与传统方法相当，甚至在某些方面表现更好，如确定行程目的和一致性。此外，LLM在零样本设置下表现出统计代表性，为未来的合成日记评估系统提供了可量化的指标。

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [11] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: This paper introduces PsychiatryBench, a comprehensive benchmark for evaluating large language models in psychiatric applications, revealing significant gaps in their clinical performance.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation resources for LLMs in psychiatry are limited by small clinical interview corpora, social media posts, or synthetic dialogues, which lack clinical validity and fail to capture the complexity of psychiatric reasoning.

Method: We introduce PsychiatryBench, a benchmark grounded in authoritative psychiatric textbooks and casebooks. We evaluate various LLMs using conventional metrics and an 'LLM-as-judge' similarity scoring framework.

Result: Our results reveal substantial gaps in clinical consistency and safety, particularly in multi-turn follow-up and management tasks, highlighting the need for specialized model tuning and more robust evaluation paradigms.

Conclusion: PsychiatryBench offers a modular, extensible platform for benchmarking and improving LLM performance in high-stakes mental health applications.

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [12] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: 本研究探讨了训练后的方法和显式推理对小型语言模型提供ACT的影响，发现ORPO训练方法优于SFT和基线模型，并且显式推理对不同训练范式的模型效果不同。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在调查训练后的方法和显式推理对小型开放权重大型语言模型（LLM）提供ACT的影响。

Method: 本研究使用50组由Mistral-Large生成的合成ACT对话记录，对Llama-3.2-3b-Instruct进行了两种不同的方法（监督微调和几率比策略优化）的训练，每种方法都带有或不带有显式的思维链（COT）推理步骤。

Result: 研究结果表明，ORPO训练的模型在ACT保真度和治疗共情方面显著优于SFT和Instruct模型。COT对SFT模型有显著益处，但对更优的ORPO或instruct-tuned模型没有明显优势。

Conclusion: 本研究表明，偏好对齐的策略优化可以有效地在小型语言模型中培养ACT能力，并且显式推理的效用高度依赖于基础训练范式。

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [13] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 本文提出 HANRAG 框架，以解决 RAG 方法在处理多跳查询时的挑战，并在问答任务中取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 当前的 RAG 方法在处理多跳查询时面临诸多挑战，例如过度依赖迭代检索、无法捕捉特定子查询的相关内容以及噪声积累问题。

Method: HANRAG 通过强大的揭示器将查询分解为子查询并过滤检索文档中的噪声，从而提高系统的适应性和抗噪能力。

Result: HANRAG 在多个基准测试中与领先的行业方法进行了比较，结果表明其在单跳和多跳问答任务中均表现出优越的性能。

Conclusion: HANRAG 是一种高效的框架，能够处理各种复杂性问题，并在单跳和多跳问答任务中表现出色。

Abstract: The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [14] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: 该研究评估了不同方法在测量语义相似性方面的效果，发现基于嵌入的方法存在重大问题，而大型语言模型在区分语义差异方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估不同方法在测量语义相似性方面的效果，这对于代码搜索、API推荐等软件工程应用至关重要。

Method: 研究创建了一个系统测试框架，通过应用受控变化来评估不同方法处理语义关系的能力。

Result: 结果揭示了常用指标的重大问题，基于嵌入的方法错误地将语义对立面识别为相似，而基于Transformer的方法有时会将相反含义评为比同义词更相似。

Conclusion: 研究发现，基于嵌入的方法在计算距离时表现不佳，而大型语言模型在区分语义差异方面表现更好。

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [15] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 研究识别并表征了使大型语言模型容易产生幻觉的关键属性，发现符号元素（如修饰符和命名实体）是导致幻觉的主要原因，即使在更大的模型中也仍然存在显著的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在识别和表征使LLM内在易受幻觉影响的关键属性，以便确定模型内部机制中的脆弱性。

Method: 研究利用了两个已有的数据集HaluEval和TruthfulQA，并将它们现有的问答格式转换为其他格式，以确定导致幻觉的关键属性。

Result: 研究结果表明，Gemma-2-2B的幻觉百分比在任务和数据集中平均为79.0%，随着模型规模的增加，幻觉率下降至Gemma-2-9B的73.6%和Gemma-2-27B的63.9%。然而，由符号属性引起的幻觉仍然大量存在。

Conclusion: 研究发现，符号元素继续使模型困惑，这表明这些LLM在处理此类输入时存在根本性的弱点，无论其规模如何。

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [16] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: ALIGNS is a large language model-based system that generates comprehensive nomological networks, addressing challenges in measurement validation across multiple fields.


<details>
  <summary>Details</summary>
Motivation: Building nomological networks remains a challenge despite advances in measurement, leading to practical consequences in clinical trials and public policy.

Method: ALIGNS is a large language model-based system trained with validated questionnaire measures to generate nomological networks.

Result: ALIGNS provides three comprehensive nomological networks containing over 550,000 indicators across psychology, medicine, social policy, and other fields. It shows convergence of NIH PROMIS anxiety and depression instruments into a single dimension of emotional distress, identifies four potential dimensions in child temperament measures, and engages expert psychometricians for evaluation.

Conclusion: ALIGNS is a large language model-based system that provides comprehensive nomological networks across various fields, complementing traditional validation methods with large-scale nomological analysis.

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [17] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: 本文提出了一种基于技术时序关系的框架，用于识别新兴技术机会，并利用人工智能专利数据集进行了评估。


<details>
  <summary>Details</summary>
Motivation: 技术机会是推动技术、工业和创新进步的关键信息，因此需要一种有效的方法来识别新兴技术机会。

Method: 该框架基于技术之间的时序关系，通过从专利数据集中提取文本，映射文本主题以发现技术间的关系，并通过跟踪这些主题随时间的变化来识别技术机会。

Result: 实验结果表明，人工智能技术正在演变为促进日常可访问性的形式。

Conclusion: 该方法展示了所提出的框架识别未来技术机会的潜力。

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [18] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

TL;DR: 本文提出了一种针对多语言和嵌套生物医学命名实体链接的轻量级系统，通过两阶段检索-排序、边界提示和数据集增强等方法，在BioNNE 2025排行榜上取得了第三名的好成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学文本实体链接通常在仅限英语的语料库上进行基准测试，而忽略了更现实的嵌套和多语言提及场景。

Method: 我们提出了一个轻量级的管道，保持原始EL模型不变，并仅修改三个任务对齐的组件：两阶段检索-排序、边界提示和数据集增强。

Result: 我们的两阶段系统BIBERT-Pipe在多语言赛道中排名第三。

Conclusion: 我们的系统在BioNNE 2025排行榜上排名第三，证明了这些最小但有原则的修改的有效性和竞争力。

Abstract: Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [19] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
*Seiji Hattori,Takuya Matsuzaki,Makoto Fujiwara*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型将形式化证明转换为自然语言的方法，并展示了其在生成可读性和准确性方面的效果。


<details>
  <summary>Details</summary>
Motivation: 为了使机器可验证的形式化证明更易于理解和使用，需要一种将形式化证明转换为自然语言的方法。

Method: 提出了一种利用大型语言模型的非形式化（形式语言证明步骤的口语化）和摘要能力的自然语言翻译方法。

Result: 该方法在符合本科教材中自然语言证明的形式化证明数据上进行了评估，并分析了生成的自然语言证明的质量。

Conclusion: 该方法能够生成高度可读且准确的自然语言证明。

Abstract: This paper proposes a natural language translation method for
machine-verifiable formal proofs that leverages the informalization
(verbalization of formal language proof steps) and summarization capabilities
of LLMs. For evaluation, it was applied to formal proof data created in
accordance with natural language proofs taken from an undergraduate-level
textbook, and the quality of the generated natural language proofs was analyzed
in comparison with the original natural language proofs. Furthermore, we will
demonstrate that this method can output highly readable and accurate natural
language proofs by applying it to existing formal proof library of the Lean
proof assistant.

</details>


### [20] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: 本文介绍了一种多智能体框架，通过基于角色的提示提升金融领域问答性能。实验表明，该方法显著提高了答案准确性，并使GPT-4o-mini达到与专门金融模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型方法往往无法捕捉金融问题解决所需的细微和专业推理。金融领域需要多步骤的定量推理、对领域特定术语的熟悉以及对现实场景的理解。

Method: 我们提出了一种多智能体框架，利用基于角色的提示来提高特定领域问答的性能。该框架包括一个基础生成器、一个证据检索器和一个专家评审器代理，它们在单次迭代中工作以生成精炼的答案。

Result: 我们的实验表明，基于批评的细化比零样本思维链基线提高了6.6-8.3%的答案准确性，其中Gemini-2.0-Flash表现最佳。此外，我们的方法使GPT-4o-mini达到与金融调优的FinGPT-mt_Llama3-8B_LoRA相当的性能。

Conclusion: 我们的结果展示了一种成本效益高的增强金融问答的方法，并为多代理金融大语言模型系统的研究提供了见解。

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [21] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: 本文通过元分析评估了机器学习在Twitter数据情感分析中的性能，发现总体准确率虽然常用，但可能因类别不平衡和情感类别数量而产生误导，强调了标准化报告的重要性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估机器学习在Twitter数据情感分析中的性能，估计平均性能，评估研究间和研究内的异质性，并分析研究特征如何影响模型性能。

Method: 本文采用PRISMA指南，搜索学术数据库，并选择了195项试验，涵盖20项研究中的12个研究特征。总体准确率是最常报告的性能指标，使用双反正弦变换和三层随机效应模型进行分析。

Result: AIC优化模型的平均总体准确率为0.80 [0.76, 0.84]。

Conclusion: 本文提供了两个关键见解：1）总体准确率被广泛使用，但因其对类别不平衡和情感类别数量的敏感性而常常具有误导性，这突显了归一化的必要性。2）标准化报告模型性能，包括报告独立测试集的混淆矩阵，对于跨研究可靠比较机器学习分类器至关重要，而这似乎远未成为常规做法。

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [22] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: 本文提出 MultimodalHugs 框架，以解决手语处理研究中的可重复性和灵活性问题，并展示了其在多种数据模态上的应用。


<details>
  <summary>Details</summary>
Motivation: 手语处理研究相较于口语语言研究面临更多挑战，如复杂的自定义代码导致的低可重复性和不公平比较。现有的工具如 Hugging Face 虽然有助于快速和可重复的实验，但不够灵活，无法无缝集成手语实验。

Method: 本文介绍了 MultimodalHugs，这是一个基于 Hugging Face 构建的框架，旨在解决手语处理研究中因复杂自定义代码而导致的可重复性和不公平比较问题。该框架通过添加抽象层，使其能够适应更多样的数据模态和任务。

Result: 通过定量实验，本文展示了 MultimodalHugs 如何适应多样化的模态，例如手语的姿态估计数据或文本字符的像素数据。

Conclusion: MultimodalHugs 是一个在 Hugging Face 基础上构建的框架，能够支持更多样化的数据模态和任务，同时继承了 Hugging Face 生态系统的优点。它不仅适用于手语处理，还可以广泛应用于其他不适用 Hugging Face 标准模板的用例。

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [23] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
*Haiyang Yu,Yuchuan Wu,Fan Shi,Lei Liao,Jinghui Lu,Xiaodong Ge,Han Wang,Minghan Zhuo,Xuecheng Wu,Xiang Fei,Hao Feng,Guozhi Tang,An-Lan Wang,Hanshen Zhu,Yangfan He,Quanhuan Liang,Liyuan Meng,Chao Feng,Can Huang,Jingqun Tang,Bin Li*

Main category: cs.CL

TL;DR: 本文提出了AncientDoc基准，以评估视觉-语言模型在处理中国古代文献方面的性能，并展示了其在多个任务和文档类型上的应用。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅扫描图像，而当前的视觉-语言模型难以处理中国古代文献的视觉和语言复杂性。现有的文档基准主要针对英文印刷文本或简体中文，缺乏对中国古代文献的评估。

Method: 本文介绍了AncientDoc基准，包括五个任务（页面级OCR、白话翻译、基于推理的问答、基于知识的问答、语言变体问答），并使用主流的视觉-语言模型进行评估，同时借助一个人类对齐的大语言模型进行评分。

Result: AncientDoc基准涵盖了14种文档类型，超过100本书籍和约3000页。通过该基准，我们评估了主流的视觉-语言模型，并使用多种指标进行补充，同时借助一个人类对齐的大语言模型进行评分。

Conclusion: 本文提出了AncientDoc基准，用于评估视觉-语言模型在处理中国古代文献方面的性能，并展示了其在多个任务和文档类型上的应用。

Abstract: Chinese ancient documents, invaluable carriers of millennia of Chinese
history and culture, hold rich knowledge across diverse fields but face
challenges in digitization and understanding, i.e., traditional methods only
scan images, while current Vision-Language Models (VLMs) struggle with their
visual and linguistic complexity. Existing document benchmarks focus on English
printed texts or simplified Chinese, leaving a gap for evaluating VLMs on
ancient Chinese documents. To address this, we present AncientDoc, the first
benchmark for Chinese ancient documents, designed to assess VLMs from OCR to
knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular
translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and
covers 14 document types, over 100 books, and about 3,000 pages. Based on
AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by
a human-aligned large language model for scoring.

</details>


### [24] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: MCP-AgentBench是一个全面的基准，旨在评估语言代理在MCP媒介工具交互中的能力，通过建立稳健的测试平台、设计查询和引入新的评估方法，为研究社区提供标准化和可靠的框架。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试未能捕捉到MCP新范式中的真实代理性能，导致对其真正操作价值的误解，并且无法可靠地区分能力差异。

Method: 引入了MCP-AgentBench，这是一个专门设计用于严格评估语言代理在MCP媒介工具交互中的能力的全面基准。

Result: MCP-AgentBench包括建立一个由33个运行服务器和188种不同工具组成的稳健MCP测试平台，开发了一个包含600个系统设计查询的基准，分布在6个不同复杂度的交互类别中，并引入了MCP-Eval，这是一种新型的结果导向评估方法，优先考虑现实任务的成功。

Conclusion: MCP-AgentBench旨在为研究社区提供一个标准化和可靠的框架，以构建、验证和推进能够充分利用MCP变革性优势的智能体，从而加速向真正强大且互操作的AI系统迈进。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [25] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
*Willem Huijzer,Jieying Chen*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型中的偏见问题，特别是在决策和摘要任务中，以及跨语言传播的情况。研究发现，GPT-3.5和GPT-4o都存在偏见，但GPT-4o在英语中表现更少偏见。新的缓解指令虽不能完全消除偏见，但能有效减少。研究强调了在采用LLMs时需要谨慎，并进行特定于上下文的偏见测试，以确保AI的负责任部署。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在各个领域的快速整合，引发了对社会不平等和信息偏见的担忧。本研究旨在检查与背景、性别和年龄相关的LLM偏见，以及它们对决策和摘要任务的影响。此外，研究还考察了这些偏见的跨语言传播，并评估了提示指导的缓解策略的有效性。

Method: 本研究使用了Tamkin等人（2023）数据集的改编版本，并将其翻译成荷兰语，创建了151,200个独特的提示用于决策任务和176,400个用于摘要任务。各种人口统计变量、指令、显著性水平和语言在GPT-3.5和GPT-4o上进行了测试。

Result: 分析显示，两种模型在决策过程中存在显著偏见，偏向女性性别、较年轻的年龄和某些背景，如非裔美国人背景。相比之下，摘要任务显示出较少的偏见证据，尽管GPT-3.5在英语中出现了显著的年龄相关差异。跨语言分析显示，英语和荷兰语之间的偏见模式大致相似，但在特定的人口统计类别中观察到了明显的差异。新提出的缓解指令虽然无法完全消除偏见，但显示出减少偏见的潜力。最有效的指令实现了最有利和最不利人口统计群体之间差距的27%平均减少。值得注意的是，与GPT-3.5不同，GPT-4o在所有英语提示中表现出较少的偏见，表明在新型模型中基于提示的缓解策略具有特定的潜力。

Conclusion: 本研究强调了在采用大型语言模型时需要谨慎，并进行了特定于上下文的偏见测试，突显了持续开发有效缓解策略的必要性，以确保人工智能的负责任部署。

Abstract: The rapid integration of Large Language Models (LLMs) into various domains
raises concerns about societal inequalities and information bias. This study
examines biases in LLMs related to background, gender, and age, with a focus on
their impact on decision-making and summarization tasks. Additionally, the
research examines the cross-lingual propagation of these biases and evaluates
the effectiveness of prompt-instructed mitigation strategies. Using an adapted
version of the dataset by Tamkin et al. (2023) translated into Dutch, we
created 151,200 unique prompts for the decision task and 176,400 for the
summarisation task. Various demographic variables, instructions, salience
levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed
that both models were significantly biased during decision-making, favouring
female gender, younger ages, and certain backgrounds such as the
African-American background. In contrast, the summarisation task showed minimal
evidence of bias, though significant age-related differences emerged for
GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were
broadly similar between English and Dutch, though notable differences were
observed across specific demographic categories. The newly proposed mitigation
instructions, while unable to eliminate biases completely, demonstrated
potential in reducing them. The most effective instruction achieved a 27\% mean
reduction in the gap between the most and least favorable demographics.
Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts
in English, indicating the specific potential for prompt-based mitigation
within newer models. This research underscores the importance of cautious
adoption of LLMs and context-specific bias testing, highlighting the need for
continued development of effective mitigation strategies to ensure responsible
deployment of AI.

</details>


### [26] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: 本文提出了一种新的分层高效微调方法HEFT，通过组合LoRA和ReFT，在少量训练轮次内实现了优于单独使用这两种方法的效果，证明了PEFT方法的巧妙组合可以显著提升语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管参数高效微调（PEFT）方法已成为解决大型语言模型（LLMs）适应专业推理任务的有力方案，但这些技术的现状是多样化的，不同的方法在模型的权重空间或表示空间中运行。本文旨在探讨这些范式的协同结合是否能够解锁更好的性能和效率。

Method: 本文引入了HEFT（Hierarchical Efficient Fine-Tuning），这是一种分层适应策略，以粗到细的方式组合两种不同的PEFT方法：首先在权重空间中使用LoRA进行广泛的基础适应，然后使用ReFT对内部激活进行精确的手术式优化。

Result: 在BoolQ基准测试中微调Llama-2-7B模型的结果显示，仅用三轮训练的模型在HEFT策略下达到了85.17%的准确率，超过了使用LoRA-only（85.05%）或ReFT-only（83.36%）方法训练20轮的模型的表现。

Conclusion: 本文展示了通过巧妙组合PEFT方法可以实现算法创新，为提升语言模型的推理能力提供了一种更高效和有效的方法。通过使用较少的计算预算获得优越的结果，我们的研究提供了一种系统的方法来克服将大规模模型适应于复杂认知任务的障碍。

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [27] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
*Helen de Andrade Abreu,Tiago Timponi Torrent,Ely Edison da Silva Matos*

Main category: cs.CL

TL;DR: 本文提出了一种通过语言和互动手势之间的相关性来建模多模态对话回合组织的框架。我们开发了一种注释方法，将语用框架建模对话回合组织的注释丰富到多模态数据集中，并利用Frame2数据集进行研究。我们的研究结果证实，交流者在面对面交谈中使用手势作为传递、获取和保持对话回合的工具，并揭示了一些之前未记录的手势变化。此外，我们的数据表明，语用框架的注释有助于更深入地理解人类认知和语言。


<details>
  <summary>Details</summary>
Motivation: 尽管对话回合组织已被不同领域的研究人员研究过，但具体策略，尤其是交流者使用的手势，尚未在可用于机器学习的数据集中编码。为了填补这一空白，我们丰富了Frame2数据集，加入了用于回合组织的手势注释。

Method: 我们开发了一种注释方法，将语用框架建模对话回合组织的注释丰富到多模态数据集中。我们还利用Frame2数据集进行研究，该数据集包含10集巴西电视剧《Pedro Pelo Mundo》，并对其视频和文本中引发的语义框架进行了注释。

Result: 我们的研究结果确认了交流者在面对面交谈中使用手势作为传递、获取和保持对话回合的工具，并揭示了一些之前未记录的手势变化。此外，我们的数据表明，语用框架的注释有助于更深入地理解人类认知和语言。

Conclusion: 我们的研究结果证实，面对面交谈的交流者使用手势作为传递、获取和保持对话回合的工具，并揭示了一些之前未记录的手势变化。我们提出这些手势的使用源于语用框架的概念化，涉及心理空间、融合和概念隐喻。此外，我们的数据表明，语用框架的注释有助于更深入地理解人类认知和语言。

Abstract: This paper proposes a framework for modeling multimodal conversational turn
organization via the proposition of correlations between language and
interactive gestures, based on analysis as to how pragmatic frames are
conceptualized and evoked by communicators. As a means to provide evidence for
the analysis, we developed an annotation methodology to enrich a multimodal
dataset (annotated for semantic frames) with pragmatic frames modeling
conversational turn organization. Although conversational turn organization has
been studied by researchers from diverse fields, the specific strategies,
especially gestures used by communicators, had not yet been encoded in a
dataset that can be used for machine learning. To fill this gap, we enriched
the Frame2 dataset with annotations of gestures used for turn organization. The
Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo
Mundo annotated for semantic frames evoked in both video and text. This dataset
allowed us to closely observe how communicators use interactive gestures
outside a laboratory, in settings, to our knowledge, not previously recorded in
related literature. Our results have confirmed that communicators involved in
face-to-face conversation make use of gestures as a tool for passing, taking
and keeping conversational turns, and also revealed variations of some gestures
that had not been documented before. We propose that the use of these gestures
arises from the conceptualization of pragmatic frames, involving mental spaces,
blending and conceptual metaphors. In addition, our data demonstrate that the
annotation of pragmatic frames contributes to a deeper understanding of human
cognition and language.

</details>


### [28] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
*Chuyuan Li,Austin Xu,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 本文提出一种基于主题的强化学习方法，以提高多文档摘要中的内容选择效果。通过引入主题奖励机制，实验结果表明该方法在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多文档摘要（MDS）中，有效整合多个来源的信息同时保持连贯性和主题相关性是一个关键挑战。虽然大型语言模型在单文档摘要中表现出色，但它们在MDS上的表现仍有改进空间。

Method: 我们提出了一种基于主题的强化学习方法，通过在Group Relative Policy Optimization (GRPO)框架内引入新颖的主题奖励来衡量生成摘要与源文档之间的主题一致性。

Result: 实验结果表明，我们的方法在Multi-News和Multi-XScience数据集上 consistently 超过强基线。

Conclusion: 我们的方法在Multi-News和Multi-XScience数据集上 consistently 超过强基线，表明利用主题线索在MDS中的有效性。

Abstract: A key challenge in Multi-Document Summarization (MDS) is effectively
integrating information from multiple sources while maintaining coherence and
topical relevance. While Large Language Models have shown impressive results in
single-document summarization, their performance on MDS still leaves room for
improvement. In this paper, we propose a topic-guided reinforcement learning
approach to improve content selection in MDS. We first show that explicitly
prompting models with topic labels enhances the informativeness of the
generated summaries. Building on this insight, we propose a novel topic reward
within the Group Relative Policy Optimization (GRPO) framework to measure topic
alignment between the generated summary and source documents. Experimental
results on the Multi-News and Multi-XScience datasets demonstrate that our
method consistently outperforms strong baselines, highlighting the
effectiveness of leveraging topical cues in MDS.

</details>


### [29] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLM）生成的合成调查响应与真实人类响应的可靠性，发现LLM生成的合成样本可以近似概率样本的响应，但在项目层面存在显著异质性。


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) offer promising avenues for methodological and applied innovations in survey research by using synthetic respondents to emulate human answers and behaviour, potentially mitigating measurement and representation errors. However, the extent to which LLMs recover aggregate item distributions remains uncertain and downstream applications risk reproducing social stereotypes and biases inherited from training data.

Method: We evaluate the reliability of LLM-generated synthetic survey responses against ground-truth human responses from a Chilean public opinion probabilistic survey. Specifically, we benchmark 128 prompt-model-question triplets, generating 189,696 synthetic profiles, and pool performance metrics (i.e., accuracy, precision, recall, and F1-score) in a meta-analysis across 128 question-subsample pairs to test for biases along key sociodemographic dimensions.

Result: First, synthetic responses achieve excellent performance on trust items (F1-score and accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform comparably on this task. Third, synthetic-human alignment is highest among respondents aged 45-59.

Conclusion: LLM-based synthetic samples approximate responses from a probabilistic sample, though with substantial item-level heterogeneity. Capturing the full nuance of public opinion remains challenging and requires careful calibration and additional distributional tests to ensure algorithmic fidelity and reduce errors.

Abstract: Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [30] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

TL;DR: 本文对基于LLM的法律AI进行了全面综述，包括16个法律LLM系列、47个框架、15个基准和29个数据集，并分析了挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 为了推进基于LLM的方法在法律领域中的研究和应用，本文旨在提供一个全面的综述和分析。

Method: 本文对16个法律LLM系列和47个基于LLM的法律任务框架进行了全面的综述，并收集了15个基准和29个数据集来评估不同的法律能力。

Result: 本文提供了16个法律LLM系列和47个基于LLM的法律任务框架的综述，并收集了15个基准和29个数据集来评估不同的法律能力。

Conclusion: 本文希望为初学者提供系统性的介绍，并鼓励该领域的未来研究。

Abstract: Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [31] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
*Guixian Xu,Zeli Su,Ziyin Zhang,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong*

Main category: cs.CL

TL;DR: 本文介绍了中国少数民族标题生成数据集（CMHG），包含藏语、维吾尔语和蒙古语的大量条目，并提出一个由母语者注释的高质量测试集，以促进该领域的研究。


<details>
  <summary>Details</summary>
Motivation: 由于中国少数民族语言如藏语、维吾尔语和传统蒙古语的独特书写系统与国际标准不同，导致相关语料库严重缺乏，特别是在监督任务如标题生成方面。

Method: 我们引入了一个新的数据集，即中国少数民族标题生成（CMHG），其中包括100,000个藏语条目和每个维吾尔语和蒙古语50,000个条目，专门用于标题生成任务。此外，我们提出了一个由母语者注释的高质量测试集，旨在作为该领域未来研究的基准。

Result: 我们引入了一个新的数据集，即中国少数民族标题生成（CMHG），其中包括100,000个藏语条目和每个维吾尔语和蒙古语50,000个条目，专门用于标题生成任务。此外，我们提出了一个由母语者注释的高质量测试集，旨在作为该领域未来研究的基准。

Conclusion: 我们希望这个数据集将成为推动中国少数民族语言标题生成的宝贵资源，并为相关基准的发展做出贡献。

Abstract: Minority languages in China, such as Tibetan, Uyghur, and Traditional
Mongolian, face significant challenges due to their unique writing systems,
which differ from international standards. This discrepancy has led to a severe
lack of relevant corpora, particularly for supervised tasks like headline
generation. To address this gap, we introduce a novel dataset, Chinese Minority
Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and
50,000 entries each for Uyghur and Mongolian, specifically curated for headline
generation tasks. Additionally, we propose a high-quality test set annotated by
native speakers, designed to serve as a benchmark for future research in this
domain. We hope this dataset will become a valuable resource for advancing
headline generation in Chinese minority languages and contribute to the
development of related benchmarks.

</details>


### [32] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: IRIS是一种无监督幻觉检测框架，利用内在的事实正确性内部表示，通过验证陈述的真实性和响应的不确定性来提高检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督方法经常依赖于与事实正确性无关的代理信号，这导致检测探针偏向于表面或非真实相关的方面，限制了在不同数据集和场景中的泛化能力。

Method: IRIS通过让大型语言模型仔细验证给定陈述的真实性，并获取其上下文化的嵌入作为训练的特征，同时将每个响应的不确定性视为真实性的一个软伪标签。

Result: 实验结果表明，IRIS在多个数据集和场景中 consistently 超过现有的无监督方法。

Conclusion: IRIS是一种有效的无监督幻觉检测框架，能够一致地优于现有的无监督方法，并且适用于实时检测。

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [33] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: 本研究分析了开源大型语言模型在多标签意图分类任务中的表现，并提供了增强任务导向聊天机器人自然语言理解能力的框架。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估开放源代码、公共可用且可在消费级硬件上运行的大型语言模型在多标签意图分类任务中的表现，并提供一个框架以增强任务导向聊天机器人的自然语言理解能力。

Method: 使用MultiWOZ 2.1数据集，对三个流行的开源预训练LLM（LLama2-7B-hf、Mistral-7B-v0.1和Yi-6B）进行多标签意图分类任务的分析，并比较了基于指令的微调方法与监督学习的性能。

Result: Mistral-7B-v0.1在14个意图类别的F-Score中表现优于其他两个生成模型，具有相对较低的Humming Loss和较高的Jaccard Similarity。基于BERT的监督分类器的表现优于最佳的few-shot生成LLM。

Conclusion: 研究提供了一个框架，使小型开源LLM能够检测复杂的多意图对话，增强了任务导向聊天机器人的自然语言理解方面。

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [34] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
*Laurin Plank,Armin Zlomuzica*

Main category: cs.CL

TL;DR: 本研究通过分析社交媒体语言，揭示了双相障碍患者在诊断前后语言的变化特征，包括情绪障碍、精神科共病等，并发现存在12个月的周期性变化。


<details>
  <summary>Details</summary>
Motivation: 语言是情感障碍如双相障碍（BD）的重要标志，但临床评估在规模上仍然有限。因此，对社交媒体（SM）语言的分析因其高时间分辨率和纵向范围而受到重视。

Method: 我们引入了一种方法来确定用户的诊断时间，并将其应用于研究BD诊断前3年到诊断后21年的语言轨迹，与报告单极性抑郁（UD）和未受影响的用户（HC）进行对比。

Result: 我们发现BD诊断伴随着普遍的语言变化，反映了情绪障碍、精神科共病、药物滥用、住院治疗、医学共病、异常思维内容和思维紊乱。我们还观察到诊断后二十年内反复的情绪相关语言变化，其中12个月的周期性表明季节性情绪发作。最后，趋势级别的证据表明，估计为女性的用户显示出更高的周期性。

Conclusion: 我们的研究结果为双相障碍（BD）急性期和慢性期的语言变化提供了证据。这验证并扩展了最近利用社交媒体进行心理健康大规模监测的努力。

Abstract: Language provides valuable markers of affective disorders such as bipolar
disorder (BD), yet clinical assessments remain limited in scale. In response,
analyses of social media (SM) language have gained prominence due to their high
temporal resolution and longitudinal scope. Here, we introduce a method to
determine the timing of users' diagnoses and apply it to study language
trajectories from 3 years before to 21 years after BD diagnosis - contrasted
with uses reporting unipolar depression (UD) and non-affected users (HC). We
show that BD diagnosis is accompanied by pervasive linguistic alterations
reflecting mood disturbance, psychiatric comorbidity, substance abuse,
hospitalization, medical comorbidities, unusual thought content, and
disorganized thought. We further observe recurring mood-related language
changes across two decades after the diagnosis, with a pronounced 12-month
periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence
suggests an increased periodicity in users estimated to be female. In sum, our
findings provide evidence for language alterations in the acute and chronic
phase of BD. This validates and extends recent efforts leveraging SM for
scalable monitoring of mental health.

</details>


### [35] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
*Mohamed Basem,Mohamed Younes,Seif Ahmed,Abdelrahman Moustafa*

Main category: cs.CL

TL;DR: 我们提出了一个用于BAREC 2025共享任务的阿拉伯语细粒度可读性评估的系统，通过模型和损失函数的多样性、基于置信度的融合以及智能增强方法，在六个赛道中获得第一，并取得了优异的QWK结果。


<details>
  <summary>Details</summary>
Motivation: 为了提高阿拉伯语细粒度可读性评估的准确性，我们致力于开发一个能够有效处理类别不平衡和数据稀缺问题的系统，并通过模型和损失函数的多样性来提升性能。

Method: 我们采用了四个互补的Transformer模型（AraBERTv2、AraELECTRA、MARBERT和CAMeLBERT）的置信度加权集成，并对每个模型使用不同的损失函数进行微调，以捕捉多样化的可读性信号。此外，我们应用了加权训练、先进的预处理、SAMER语料库重新标注以及通过Gemini 2.5 Flash生成合成数据来解决严重的类别不平衡和数据稀缺问题。最后，我们进行了针对性的后处理步骤来纠正预测分布偏差。

Result: 我们的系统在BAREC 2025共享任务的六个赛道中均获得了第一名，句子级别的QWK达到了87.5%，文档级别的QWK达到了87.4%。通过后处理步骤，预测分布偏差得到了纠正，从而提升了6.3个百分点的QWK。

Conclusion: 我们的系统展示了模型和损失多样性、基于置信度的融合以及智能增强在鲁棒阿拉伯语可读性预测中的强大作用。

Abstract: We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained
Arabic readability assessment, achieving first place in six of six tracks. Our
approach is a confidence-weighted ensemble of four complementary transformer
models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with
distinct loss functions to capture diverse readability signals. To tackle
severe class imbalance and data scarcity, we applied weighted training,
advanced preprocessing, SAMER corpus relabeling with our strongest model, and
synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level
samples. A targeted post-processing step corrected prediction distribution
skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system
reached 87.5 percent QWK at the sentence level and 87.4 percent at the document
level, demonstrating the power of model and loss diversity, confidence-informed
fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [36] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文指出，现有的心理问卷可能不适合用于评估大型语言模型，并建议使用更符合实际情境的问卷。


<details>
  <summary>Details</summary>
Motivation: 研究者们已经将已有的心理问卷应用于测量大型语言模型的人格特质和价值观，但存在关于这些问卷是否适合用于大型语言模型的担忧。本文旨在探讨已有的问卷和生态有效问卷之间的差异及其带来的见解。

Method: 本文对两种类型的问卷进行了全面的比较分析。

Result: 研究发现，已有的问卷与生态有效的问卷在结果上存在显著差异，包括大型语言模型的特征偏离了用户查询中的心理特征、项目不足、产生误导性印象以及产生夸张的个性提示大型语言模型的特征。

Conclusion: 本文警告不要使用已有的心理问卷来评估大型语言模型。

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [37] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
*Mustapha Adamu,Qi Zhang,Huitong Pan,Longin Jan Latecki,Eduard C. Dragut*

Main category: cs.CL

TL;DR: 本文构建了一个特定领域的知识图谱，用于改善气候知识的获取和使用，通过结构化语义查询和与大型语言模型的集成，提高了透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着气候科学文献的复杂性和数量不断增加，研究人员越来越难以找到跨模型、数据集、地区和变量的相关信息。

Method: 本文构建了一个特定领域的知识图谱（KG），从气候文献和更广泛的科学文本中提取信息，并使用Cypher查询来回答相关问题，同时将其与大型语言模型集成到RAG系统中以提高透明度和可靠性。

Result: 本文展示了如何使用Cypher查询来回答有关模型验证区域或常用数据集的问题，并通过与大型语言模型的集成提高了气候相关问答的透明度和可靠性。

Conclusion: 本文展示了知识图谱在气候研究中的实际价值，为研究人员、模型开发者和其他依赖准确、上下文科学信息的人提供了帮助。

Abstract: The growing complexity and volume of climate science literature make it
increasingly difficult for researchers to find relevant information across
models, datasets, regions, and variables. This paper introduces a
domain-specific Knowledge Graph (KG) built from climate publications and
broader scientific texts, aimed at improving how climate knowledge is accessed
and used. Unlike keyword based search, our KG supports structured, semantic
queries that help researchers discover precise connections such as which models
have been validated in specific regions or which datasets are commonly used
with certain teleconnection patterns. We demonstrate how the KG answers such
questions using Cypher queries, and outline its integration with large language
models in RAG systems to improve transparency and reliability in
climate-related question answering. This work moves beyond KG construction to
show its real world value for climate researchers, model developers, and others
who rely on accurate, contextual scientific information.

</details>


### [38] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 本研究提出了一种针对阿拉伯语医学文本生成的大型语言模型微调方法，通过从社交媒体平台收集并预处理数据集，优化生成模型的能力，从而提高医院管理系统的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往无法提供准确的实时医疗建议，尤其是在不规则输入和欠代表语言的情况下。因此，本研究旨在提出一种针对阿拉伯语医学文本生成的大型语言模型微调方法，以帮助患者获得准确的医疗建议、诊断、药物推荐和治疗方案。

Method: 本研究通过从社交媒体平台收集独特的数据集，捕捉患者和医生之间的实际医疗对话，并对数据集进行清理和预处理以适应多种阿拉伯语方言。然后对最先进的生成模型（如Mistral-7B-Instruct-v0.2、LLaMA-2-7B和GPT-2 Medium）进行微调，以优化系统生成可靠医疗文本的能力。

Result: 评估结果表明，微调后的Mistral-7B模型在精确度、召回率和F1分数上的平均BERT得分分别为68.5%、69.08%和68.5%，优于其他模型。比较基准测试和定性评估验证了系统生成连贯且相关的医疗回复的能力。

Conclusion: 本研究展示了生成式人工智能在推进医院管理系统方面的潜力，提供了一种可扩展且适应性强的解决方案，特别是在语言和文化多样的环境中应对全球医疗挑战。

Abstract: Efficient hospital management systems (HMS) are critical worldwide to address
challenges such as overcrowding, limited resources, and poor availability of
urgent health care. Existing methods often lack the ability to provide
accurate, real-time medical advice, particularly for irregular inputs and
underrepresented languages. To overcome these limitations, this study proposes
an approach that fine-tunes large language models (LLMs) for Arabic medical
text generation. The system is designed to assist patients by providing
accurate medical advice, diagnoses, drug recommendations, and treatment plans
based on user input. The research methodology required the collection of a
unique dataset from social media platforms, capturing real-world medical
conversations between patients and doctors. The dataset, which includes patient
complaints together with medical advice, was properly cleaned and preprocessed
to account for multiple Arabic dialects. Fine-tuning state-of-the-art
generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2
Medium, optimized the system's ability to generate reliable medical text.
Results from evaluations indicate that the fine-tuned Mistral-7B model
outperformed the other models, achieving average BERT (Bidirectional Encoder
Representations from Transformers) Score values in precision, recall, and
F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative
benchmarking and qualitative assessments validate the system's ability to
produce coherent and relevant medical replies to informal input. This study
highlights the potential of generative artificial intelligence (AI) in
advancing HMS, offering a scalable and adaptable solution for global healthcare
challenges, especially in linguistically and culturally diverse environments.

</details>


### [39] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Khaled Shaban*

Main category: cs.CL

TL;DR: 本研究提出了一个可扩展的合成数据增强策略，通过生成大量高质量的合成问答对来扩展训练语料库，从而提高阿拉伯语医疗聊天机器人的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗聊天机器人的发展在阿拉伯语中受到大规模高质量注释数据集稀缺的显著限制。尽管之前的努力从社交媒体中编译了一个包含2万条阿拉伯语患者-医生互动的数据集，以微调大型语言模型（LLMs），但模型的可扩展性和泛化能力仍然有限。

Method: 我们提出了一种可扩展的合成数据增强策略，将训练语料库扩展到10万条记录。使用先进的生成AI系统ChatGPT-4o和Gemini 2.5 Pro生成了8万条上下文相关且医学一致的合成问答对，并进行了语义过滤、人工验证和集成到训练流程中。

Result: 结果表明，ChatGPT-4o数据在所有模型中 consistently 导致更高的F1分数和更少的幻觉。

Conclusion: 我们的研究结果表明，合成增强作为一种实用解决方案，可以有效提升低资源医疗NLP领域的领域特定语言模型，为更包容、可扩展和准确的阿拉伯语医疗聊天机器人系统铺平了道路。

Abstract: The development of medical chatbots in Arabic is significantly constrained by
the scarcity of large-scale, high-quality annotated datasets. While prior
efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from
social media to fine-tune large language models (LLMs), model scalability and
generalization remained limited. In this study, we propose a scalable synthetic
data augmentation strategy to expand the training corpus to 100,000 records.
Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated
80,000 contextually relevant and medically coherent synthetic question-answer
pairs grounded in the structure of the original dataset. These synthetic
samples were semantically filtered, manually validated, and integrated into the
training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,
and evaluated their performance using BERTScore metrics and expert-driven
qualitative assessments. To further analyze the effectiveness of synthetic
sources, we conducted an ablation study comparing ChatGPT-4o and
Gemini-generated data independently. The results showed that ChatGPT-4o data
consistently led to higher F1-scores and fewer hallucinations across all
models. Overall, our findings demonstrate the viability of synthetic
augmentation as a practical solution for enhancing domain-specific language
models in-low resource medical NLP, paving the way for more inclusive,
scalable, and accurate Arabic healthcare chatbot systems.

</details>


### [40] [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)
*Julian Linke,Barbara Schuppler*

Main category: cs.CL

TL;DR: 本文通过结合突出检测和语音识别，开发了新的突出感知的自动语音识别系统，展示了基于Transformer的模型可以有效编码韵律信息，并具有潜在的应用前景。


<details>
  <summary>Details</summary>
Motivation: 本文旨在结合突出检测和语音识别，以提高对话奥地利德语的自动语音识别性能。

Method: 本文通过微调wav2vec2模型来开发突出检测器，并将其用于自动标注语料库中的韵律突出。然后基于这些标注训练了新型的突出感知的自动语音识别系统。

Result: 突出信息的整合并未改变与基线自动语音识别系统相比的性能，同时在识别单词序列正确的句子中达到了85.53%的突出检测准确率。

Conclusion: 本文展示了基于Transformer的模型可以有效地编码韵律信息，并为增强韵律的自动语音识别提供了新的贡献，具有潜在的语言学研究和韵律指导的对话系统的应用前景。

Abstract: This paper investigates prominence-aware automatic speech recognition (ASR)
by combining prominence detection and speech recognition for conversational
Austrian German. First, prominence detectors were developed by fine-tuning
wav2vec2 models to classify word-level prominence. The detector was then used
to automatically annotate prosodic prominence in a large corpus. Based on those
annotations, we trained novel prominence-aware ASR systems that simultaneously
transcribe words and their prominence levels. The integration of prominence
information did not change performance compared to our baseline ASR system,
while reaching a prominence detection accuracy of 85.53% for utterances where
the recognized word sequence was correct. This paper shows that
transformer-based models can effectively encode prosodic information and
represents a novel contribution to prosody-enhanced ASR, with potential
applications for linguistic research and prosody-informed dialogue systems.

</details>


### [41] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 本文提出了一种系统框架，用于合成高质量、与人口分布一致的大语言模型驱动社会模拟人物集。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的基于大语言模型的社会模拟研究主要关注设计代理框架和模拟环境，往往忽略了人物生成的复杂性和非代表性人物集可能引入的偏见。

Method: 我们首先利用大语言模型从长期社交媒体数据中生成叙述性人物角色，然后进行严格的质量评估以过滤低保真度的资料。接着，我们应用重要性抽样以实现与参考心理测量分布（如大五人格特质）的全局对齐。为了满足特定模拟情境的需求，我们进一步引入了一个任务特定模块，将全局对齐的人物角色集适应到目标子人群中。

Result: 实验表明，我们的方法显著减少了群体层面的偏差，并能够为各种研究和政策应用提供准确、灵活的社会模拟。

Conclusion: 我们的方法显著减少了群体层面的偏差，并为广泛的研究和政策应用提供了准确、灵活的社会模拟。

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [42] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
*Alessio Chen,Simone Giovannini,Andrea Gemelli,Fabio Coppini,Simone Marinai*

Main category: cs.CL

TL;DR: 本文提出了一种新的边界框预测模块DocExplainerV0，以解决视觉语言模型在文档中精确定位答案的问题，并展示了其在提高可解释性和鲁棒性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在文档理解方面表现出色，但准确地在文档中定位答案仍然是一个重大挑战，这限制了其可解释性和实际应用。

Method: 本文引入了DocExplainerV0，这是一个可插拔的边界框预测模块，用于解决视觉语言模型在文档中精确定位答案的问题。

Result: 通过系统评估，本文提供了关于文本准确性与空间定位之间差距的定量见解，表明正确的答案往往缺乏可靠的空间定位。

Conclusion: 本文提出了DocExplainerV0，这是一个可插拔的边界框预测模块，能够将答案生成与空间定位解耦，从而提高文档信息提取的可解释性和鲁棒性。

Abstract: Vision-Language Models (VLMs) have shown strong capabilities in document
understanding, particularly in identifying and extracting textual information
from complex documents. Despite this, accurately localizing answers within
documents remains a major challenge, limiting both interpretability and
real-world applicability. To address this, we introduce
\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that
decouples answer generation from spatial localization. This design makes it
applicable to existing VLMs, including proprietary systems where fine-tuning is
not feasible. Through systematic evaluation, we provide quantitative insights
into the gap between textual accuracy and spatial grounding, showing that
correct answers often lack reliable localization. Our standardized framework
highlights these shortcomings and establishes a benchmark for future research
toward more interpretable and robust document information extraction VLMs.

</details>


### [43] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 本研究通过多维分析方法比较了人类和大型语言模型生成的文本，发现两者在某些维度上有显著差异，并创建了一个模型比较基准。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解大型语言模型（LLM）生成的文本与人类撰写的文本之间的注册变化，并创建一个基准来比较和排名不同模型。

Method: 应用Biber的多维分析（MDA）对人类撰写的文本和AI生成的文本进行比较，以确定LLM与人类在哪些变化维度上存在最显著和系统性的差异。使用了AI-Brown语料库和AI-Koditex语料库进行分析。

Result: 发现了LLM与人类在某些文本维度上的显著差异，并创建了一个可用于比较模型的基准。

Conclusion: 本研究创建了一个基准，使模型能够在可解释的维度上相互比较和排序。

Abstract: This study investigates the register variation in texts written by humans and
comparable texts produced by large language models (LLMs). Biber's
multidimensional analysis (MDA) is applied to a sample of human-written texts
and AI-created texts generated to be their counterparts to find the dimensions
of variation in which LLMs differ most significantly and most systematically
from humans. As textual material, a new LLM-generated corpus AI-Brown is used,
which is comparable to BE-21 (a Brown family corpus representing contemporary
British English). Since all languages except English are underrepresented in
the training data of frontier LLMs, similar analysis is replicated on Czech
using AI-Koditex corpus and Czech multidimensional model. Examined were 16
frontier models in various settings and prompts, with emphasis placed on the
difference between base models and instruction-tuned models. Based on this, a
benchmark is created through which models can be compared with each other and
ranked in interpretable dimensions.

</details>


### [44] [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)
*Leen Almajed,Abeer ALdayel*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In emotionally supportive conversations, well-intended positivity can
sometimes misfire, leading to responses that feel dismissive, minimizing, or
unrealistically optimistic. We examine this phenomenon of incongruent
positivity as miscalibrated expressions of positive support in both human and
LLM generated responses. To this end, we collected real user-assistant
dialogues from Reddit across a range of emotional intensities and generated
additional responses using large language models for the same context. We
categorize these conversations by intensity into two levels: Mild, which covers
relationship tension and general advice, and Severe, which covers grief and
anxiety conversations. This level of categorization enables a comparative
analysis of how supportive responses vary across lower and higher stakes
contexts. Our analysis reveals that LLMs are more prone to unrealistic
positivity through dismissive and minimizing tone, particularly in high-stakes
contexts. To further study the underlying dimensions of this phenomenon, we
finetune LLMs on datasets with strong and weak emotional reactions. Moreover,
we developed a weakly supervised multilabel classifier ensemble (DeBERTa and
MentalBERT) that shows improved detection of incongruent positivity types
across two sorts of concerns (Mild and Severe). Our findings shed light on the
need to move beyond merely generating generic positive responses and instead
study the congruent support measures to balance positive affect with emotional
acknowledgment. This approach offers insights into aligning large language
models with affective expectations in the online supportive dialogue, paving
the way toward context-aware and trust preserving online conversation systems.

</details>


### [45] [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
*Miklós Sebők,Viktor Kovács,Martin Bánóczy,Daniel Møller Eriksen,Nathalie Neptune,Philippe Roussille*

Main category: cs.CL

TL;DR: 本研究分析了不同大型语言模型在处理长输入文本的多类分类任务中的性能，发现Longformer模型并无明显优势，而最佳表现的开源模型在与GPT变体的比较中显示出一定优势。


<details>
  <summary>Details</summary>
Motivation: 社会科学研究中广泛使用的大型语言模型（如BERT及其衍生模型）在处理长输入文本时存在限制，这在某些分类任务中尤为突出，例如处理法律和草案法律等长文本。

Method: 该研究使用XLM-RoBERTa、Longformer、GPT-3.5和GPT-4模型进行了实验，以评估它们在处理长输入文本的多类分类任务中的性能。

Result: 实验结果表明，Longformer模型在处理长输入文本的多语言分类任务中没有表现出明显优势。同时，最佳表现的开源模型在与GPT变体的比较中显示出一定的优势。

Conclusion: 研究结果显示，尽管Longformer模型专门设计用于处理长输入文本，但在多语言分类任务中并未表现出明显优势。此外，与GPT变体相比，表现最好的开源模型具有一定的优势。

Abstract: The most widely used large language models in the social sciences (such as
BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text
length that they can process to produce predictions. This is a particularly
pressing issue for some classification tasks, where the aim is to handle long
input texts. One such area deals with laws and draft laws (bills), which can
have a length of multiple hundred pages and, therefore, are not particularly
amenable for processing with models that can only handle e.g. 512 tokens. In
this paper, we show results from experiments covering 5 languages with
XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass
classification task of the Comparative Agendas Project, which has a codebook of
21 policy topic labels from education to health care. Results show no
particular advantage for the Longformer model, pre-trained specifically for the
purposes of handling long inputs. The comparison between the GPT variants and
the best-performing open model yielded an edge for the latter. An analysis of
class-level factors points to the importance of support and substance overlaps
between specific categories when it comes to performance on long text inputs.

</details>


### [46] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: 本文提出了一种新的自我改进框架SI FACT，通过对比学习提高大型语言模型在知识密集型任务中的上下文忠实性。实验结果表明，该方法在提高模型的忠实性方面效果显著，并且具有高数据效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中生成不忠实响应，因为倾向于依赖内部参数知识而不是提供的上下文。为了解决这个问题，需要一种方法来提高模型的忠实性。

Method: 提出了一种新的自我改进框架，称为Self Improving Faithfulness Aware Contrastive Tuning (SI FACT)。该框架使用自我指导机制，使基础LLM能够自动生成高质量、结构化的对比学习数据，包括锚点样本、语义等效的正样本和模拟不忠实场景的负样本。然后应用对比学习来训练模型，使其在表示空间中拉近忠实响应并推远不忠实响应。

Result: 在知识冲突评估基准ECARE KRE和COSE KRE上的实验表明，基于Llama3 8B Instruct的SI FACT模型比最佳基线方法提高了6.2%的上下文召回率，同时显著减少了对内部记忆的依赖。

Conclusion: 实验结果表明，SI FACT在提高LLM的上下文忠实性方面表现出色，提供了一条实用的路径，以构建更加主动和可信的语言模型。

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [47] [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)
*Yixiao Zhou,Ziyu Zhao,Dongzhou Cheng,zhiliang wu,Jie Gui,Yi Yang,Fei Wu,Yu Cheng,Hehe Fan*

Main category: cs.CL

TL;DR: DERN is a framework for pruning and reconstructing experts in SMoE architectures, improving performance and reducing memory usage without extra training.


<details>
  <summary>Details</summary>
Motivation: SMoE architectures have high memory usage due to loading all expert parameters, and previous work has focused on expert-level operations, leaving neuron-level structure underexplored.

Method: DERN (Dropping Experts, Recombining Neurons) is a task-agnostic and retraining-free framework for expert pruning and reconstruction.

Result: Experiments show that DERN improves performance by more than 5% on commonsense reasoning and MMLU benchmarks under 50% expert sparsity, without extra training.

Conclusion: DERN improves performance and reduces memory usage in SMoE LLMs, making them easier to deploy.

Abstract: Sparse Mixture-of-Experts (SMoE) architectures are widely used in large
language models (LLMs) due to their computational efficiency. However, though
only a few experts are activated for each token, SMoE still requires loading
all expert parameters, leading to high memory usage and challenges in
deployment. Previous work has tried to reduce the overhead by pruning and
merging experts, but primarily focused on expert-level operations, leaving
neuron-level structure underexplored. We propose DERN (Dropping Experts,
Recombining Neurons), a task-agnostic and retraining-free framework for expert
pruning and reconstruction. We observe that experts are often misaligned and
contain semantic conflicts at the neuron level, which poses challenges for
direct merging. To solve this, DERN works in three steps: it first prunes
redundant experts using router statistics; then it decomposes them into
neuron-level expert segments, assigning each segment to its most compatible
retained expert; and finally, it merges segments within each retained expert to
build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE
models show that DERN improves performance by more than 5% on commonsense
reasoning and MMLU benchmarks under 50% expert sparsity, without extra
training. It also greatly reduces the number of experts and memory usage,
making SMoE LLMs easier to deploy in practice.

</details>


### [48] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: 研究分析了ICL的有效性和局限性，指出其在处理未见过的任务时存在一定的限制，并强调了自回归模型的临时编码机制不够稳健。


<details>
  <summary>Details</summary>
Motivation: 尽管ICL允许模型通过仅在提示中提供少量示例来解决任务，但我们需要理解其学习能力的限制。

Method: 我们进行了大规模的分析，消除了或考虑了记忆、预训练、分布变化和提示风格及措辞的影响。

Result: 我们发现ICL是一个有效的学习范式，但在学习和泛化到未见过的任务方面存在局限性。当示例数量增加时，准确性对示例分布、模型、提示风格和输入的语言特征不敏感。

Conclusion: 我们得出结论，自回归的临时编码不是一个稳健的机制，并暗示了有限的通用泛化能力。

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [49] [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)
*Christopher Ormerod,Gitit Kehat*

Main category: cs.CL

TL;DR: 本研究评估了多种经过架构修改的模型，以克服标准Transformer架构的长度限制，并在Kaggle ASAP 2.0数据集上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 由于高年级学生的作文通常超过许多流行开源模型的最大允许长度，因此需要对这些模型进行调整，以更好地处理长文本输入。

Method: 本研究评估了经过架构修改的XLNet、Longformer、ModernBERT、Mamba和Llama模型，以解决Transformer模型在处理长文本时的限制问题。

Result: 研究结果表明，经过架构修改的模型能够更好地处理长文本输入，并提高自动作文评分的准确性。

Conclusion: 本研究评估了多种经过架构修改的模型，以克服标准Transformer架构的长度限制，并在Kaggle ASAP 2.0数据集上进行了测试。

Abstract: Transformer-based language models are architecturally constrained to process
text of a fixed maximum length. Essays written by higher-grade students
frequently exceed the maximum allowed length for many popular open-source
models. A common approach to addressing this issue when using these models for
Automated Essay Scoring is to truncate the input text. This raises serious
validity concerns as it undermines the model's ability to fully capture and
evaluate organizational elements of the scoring rubric, which requires long
contexts to assess. In this study, we evaluate several models that incorporate
architectural modifications of the standard transformer architecture to
overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models
considered in this study include fine-tuned versions of XLNet, Longformer,
ModernBERT, Mamba, and Llama models.

</details>


### [50] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: 本文提出了一种新的云边协同架构，通过三个组件（GuideLLM、SolverLLM和JudgeLLM）优化大型语言模型的推理和问题解决能力。我们引入了RefactorCoderQA基准来评估和提高模型性能，并展示了RefactorCoder-MoE在多个领域中的优越表现。


<details>
  <summary>Details</summary>
Motivation: 我们受到现有基准限制的启发，设计了一个全面的基准RefactorCoderQA，旨在评估和提高大型语言模型（LLMs）在跨领域编码任务中的性能。

Method: 我们提出了一个新颖的云边协同架构，该架构包含三个专门组件：部署在边缘的轻量级模型GuideLLM，提供方法指导；托管在云端的更强大模型SolverLLM，负责生成代码解决方案；以及自动化评估器JudgeLLM，用于评估解决方案的正确性和质量。

Result: 我们的微调模型RefactorCoder-MoE在各种技术领域（包括软件工程、数据科学、机器学习和自然语言处理）的编码挑战中表现出色，整体准确率达到76.84%。

Conclusion: 我们的实验表明，微调后的模型RefactorCoder-MoE在整体准确率上达到了76.84%，显著优于领先的开源和商业基线。人类评估进一步验证了生成解决方案的可解释性、准确性和实际相关性。此外，我们还评估了系统级指标，如吞吐量和延迟，以深入了解所提出架构的性能特征和权衡。

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [51] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: DeepDive通过自动生成复杂问题和应用多轮强化学习，提升了大型语言模型的深度搜索能力，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 开放的大型语言模型在使用浏览工具时表现不佳，因为长期推理能力有限且缺乏足够困难的监督数据。

Method: 提出了一种从开放知识图谱中自动生成复杂、困难且难以找到的问题的策略，并应用端到端多轮强化学习（RL）来增强LLMs的长期推理能力。

Result: DeepDive-32B在BrowseComp上取得了新的开源竞争结果，优于WebSailor、DeepSeek-R1-Browse和Search-o1。多轮强化学习训练显著提高了深度搜索能力，并对多个基准测试的性能提升有重要贡献。

Conclusion: DeepDive-32B在BrowseComp上取得了新的开源竞争结果，优于WebSailor、DeepSeek-R1-Browse和Search-o1。多轮强化学习训练显著提高了深度搜索能力，并对多个基准测试的性能提升有重要贡献。

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


### [52] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: 提出了一种名为WhisTLE的文本仅适应方法，用于预训练的ASR模型，能够有效降低单词错误率并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，收集语音数据是不切实际的，因此需要文本仅适应方法。

Method: 提出了一种深度监督的文本仅适应方法WhisTLE，用于预训练的编码器-解码器ASR模型。WhisTLE通过训练变分自编码器（VAE）来建模从文本中获得的编码器输出，并使用学习到的文本到潜在编码器对解码器进行微调，可选地结合文本到语音（TTS）适应。

Result: WhisTLE在四个域外数据集和四个ASR模型上表现出色，相比仅使用TTS的适应方法，相对单词错误率（WER）降低了12.3%，并且在32种情况中的27种中优于所有非WhisTLE基线方法。

Conclusion: WhisTLE在四个域外数据集和四个ASR模型上表现出色，相比仅使用TTS的适应方法，相对单词错误率（WER）降低了12.3%，并且在32种情况中的27种中优于所有非WhisTLE基线方法。

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [53] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: 该论文提出了db3团队在KDD Cup'25的Meta CRAG-MM挑战赛中的获胜解决方案，通过定制的检索管道和统一的LLM调优方法，实现了在多模态、多轮问答任务中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决Meta CRAG-MM挑战中的多模态、多轮问答基准问题，并提供一种全面的框架来处理这些挑战。

Method: 该解决方案结合了针对不同任务的定制检索管道和统一的LLM调优方法，以控制幻觉。

Result: 该系统在任务1、任务2和任务3中分别获得第二名、第二名和第一名，通过卓越的第一人称视角处理挑战，赢得了最佳表现的总冠军奖。

Conclusion: 该系统在任务1、任务2和任务3中分别获得第二名、第二名和第一名，通过卓越的第一人称视角处理挑战，赢得了最佳表现的总冠军奖。

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [54] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: 本文介绍了 text-2-SQL-4-PM，一个双语（葡萄牙语-英语）基准数据集，用于流程挖掘领域的文本到 SQL 任务。


<details>
  <summary>Details</summary>
Motivation: 为了满足流程挖掘中的独特挑战，如专业术语和从事件日志中得出的单表关系结构，需要一个双语（葡萄牙语-英语）基准数据集。

Method: 手动整理由专家进行，专业翻译和详细的注释过程以实现任务复杂性的细致分析。

Result: 文本到 SQL 转换使用户能够通过自然语言查询数据库，增加了对没有 SQL 专业知识的用户的可访问性，并提高了专家的生产力。

Conclusion: text-2-SQL-4-PM 支持文本到 SQL 实现的评估，并为语义解析和其他自然语言处理任务提供了更广泛的应用性。

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [55] [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
*Mohammad Atif,Vincent Garonne,Eric Lancon,Jerome Lauret,Alexandr Prozorov,Michal Vranovsky*

Main category: cs.IR

TL;DR: 本文介绍了一种基于AI的助手系统，用于提高科学遗产数据的可用性和可发现性。


<details>
  <summary>Details</summary>
Motivation: 随着RHIC结束25年的运行，保护其庞大的数据存储和嵌入的科学知识变得至关重要。

Method: 基于大型语言模型和检索增强生成以及模型上下文协议构建了一个AI助手系统，用于索引RHIC实验的结构化和非结构化内容，并实现领域适应的交互。

Result: 报告了系统的部署、计算性能、多实验集成以及为可持续和可解释的长期AI访问设计的架构特性。

Conclusion: 本文展示了现代AI/ML工具如何提升科学遗留数据的可用性和可发现性。

Abstract: As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National
Laboratory concludes 25 years of operation, preserving not only its vast data
holdings ($\sim$1 ExaByte) but also the embedded scientific knowledge becomes a
critical priority. The RHIC Data and Analysis Preservation Plan (DAPP)
introduces an AI-powered assistant system that provides natural language access
to documentation, workflows, and software, with the aim of supporting
reproducibility, education, and future discovery. Built upon Large Language
Models using Retrieval-Augmented Generation and the Model Context Protocol,
this assistant indexes structured and unstructured content from RHIC
experiments and enables domain-adapted interaction. We report on the
deployment, computational performance, ongoing multi-experiment integration,
and architectural features designed for a sustainable and explainable long-term
AI access. Our experience illustrates how modern AI/ML tools can transform the
usability and discoverability of scientific legacy data.

</details>


### [56] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: 本文提出了一种使用冻结的LLM提取用户表示并利用微调的SLMs模拟用户代理的方法，展示了在可扩展性和性能之间取得平衡的潜力。


<details>
  <summary>Details</summary>
Motivation: 开发准确的推荐模型的一个长期挑战是模拟用户行为，这主要是由于用户交互的复杂性和随机性。虽然以前的工作主要集中在复杂的提示或微调LLM上，但我们的方法转向了使用冻结的LLM提取稳健的文本用户表示，并利用微调的SLMs模拟成本效益高、资源高效的用户代理。

Method: 我们提出了一种使用冻结的大语言模型提取稳健的文本用户表示的方法，并利用微调的小语言模型（SLMs）模拟成本效益高、资源高效的用户代理。此外，我们展示了一种为用户组或“人格”训练多个低秩适配器的方法，以在可扩展性和用户行为代理的性能之间取得最佳平衡。

Result: 我们的实验提供了有力的实证证据，证明了我们方法的有效性，表明使用我们方法开发的用户代理有可能弥合离线指标和推荐系统实际性能之间的差距。

Conclusion: 我们的方法展示了用户代理的潜力，可以弥合离线指标和推荐系统实际性能之间的差距。

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [57] [VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](https://arxiv.org/abs/2509.09716)
*Jun Zhan,Mingyang Han,Yuxuan Xie,Chen Wang,Dong Zhang,Kexin Huang,Haoxiang Shi,DongXiao Wang,Tengtao Song,Qinyuan Cheng,Shimin Li,Jun Song,Xipeng Qiu,Bo Zheng*

Main category: cs.SD

TL;DR: 本文介绍了Voice Style Adaptation (VSA)任务，提出了VStyle多语言基准测试以及LALM as a Judge框架，用于评估语音模型的风格适应能力，并发现当前模型在此方面存在明显局限。


<details>
  <summary>Details</summary>
Motivation: 虽然大多数进展集中在语义准确性和指令遵循上，但SMLs根据口语指令调整说话风格的能力却受到较少关注。

Method: 引入了Voice Style Adaptation (VSA)任务，提出了VStyle多语言基准测试以及LALM as a Judge框架，用于评估输出的文本忠实度、风格遵循度和自然度。

Result: 实验表明，当前模型在可控风格适应方面存在明显局限，这突显了该任务的新颖性和挑战性。

Conclusion: 当前模型在可控风格适应方面存在明显局限，这突显了该任务的新颖性和挑战性。通过发布VStyle及其评估工具包，我们旨在为社区提供推进以人类为中心的语音交互的基础。

Abstract: Spoken language models (SLMs) have emerged as a unified paradigm for speech
understanding and generation, enabling natural human machine interaction.
However, while most progress has focused on semantic accuracy and instruction
following, the ability of SLMs to adapt their speaking style based on spoken
instructions has received limited attention. We introduce Voice Style
Adaptation (VSA), a new task that examines whether SLMs can modify their
speaking style, such as timbre, prosody, or persona following natural language
spoken commands. To study this task, we present VStyle, a bilingual (Chinese &
English) benchmark covering four categories of speech generation: acoustic
attributes, natural language instruction, role play, and implicit empathy. We
also introduce the Large Audio Language Model as a Judge (LALM as a Judge)
framework, which progressively evaluates outputs along textual faithfulness,
style adherence, and naturalness, ensuring reproducible and objective
assessment. Experiments on commercial systems and open source SLMs demonstrate
that current models face clear limitations in controllable style adaptation,
highlighting both the novelty and challenge of this task. By releasing VStyle
and its evaluation toolkit, we aim to provide the community with a foundation
for advancing human centered spoken interaction. The dataset and code are
publicly available at
\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是用于LLM的序列级强化学习方法，通过在重要性抽样权重空间中直接实施长度公平裁剪来解决序列级RL方法中的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: We revisit sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping is transplanted to sequences: a fixed clip range systematically reweights short vs. long responses, distorting the effective objective.

Method: FSPO introduces a simple, Gaussian-motivated remedy: we clip the sequence log-IS ratio with a band that applies a KL-corrected drift term and scales as sqrt(L).

Result: FSPO flattens clip rates across length bins, stabilizes training, and outperforms all baselines across multiple evaluation datasets.

Conclusion: FSPO flattens clip rates across length bins, stabilizes training, and outperforms all baselines across multiple evaluation datasets.

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [59] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 本文将推理时缩放问题形式化为动态计算分配和方法选择问题，考虑了令牌成本和墙钟延迟，实验表明该方法在推理基准测试中表现优于静态策略。


<details>
  <summary>Details</summary>
Motivation: 现有的关于测试时计算动态分配的工作通常只考虑并行生成方法，如最佳-N，忽略了增量解码方法如束搜索，并且主要关注令牌使用而非延迟。

Method: 我们将推理时缩放问题形式化为动态计算分配和方法选择问题，系统必须根据每个查询决定应用哪种策略以及分配多少计算资源。我们的框架明确结合了令牌成本和墙钟延迟。

Result: 实验表明，我们的方法在推理基准测试中表现优于静态策略，实现了有利的准确性和成本权衡。

Conclusion: 我们的方法在推理基准测试中表现优于静态策略，实现了有利的准确性和成本权衡，同时在部署方面具有实用性。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [60] [Whisper Has an Internal Word Aligner](https://arxiv.org/abs/2509.09987)
*Sung-Lin Yeh,Yen Meng,Hao Tang*

Main category: eess.AS

TL;DR: 本文提出一种无需训练的无监督方法，通过过滤注意力头和使用字符，实现了更精确的单词对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么需要额外训练，要么不具有竞争力，且评估标准较为宽松。

Method: 通过过滤注意力头并在使用字符时强制教师输入，利用Whisper模型中的注意力头来获得准确的单词对齐。

Result: 该方法在更严格的20ms至100ms容忍度下，产生的单词对齐比之前的工作更准确。

Conclusion: 本文提出了一种无需训练的无监督方法，可以更准确地提取单词对齐信息。

Abstract: There is an increasing interest in obtaining accurate word-level timestamps
from strong automatic speech recognizers, in particular Whisper. Existing
approaches either require additional training or are simply not competitive.
The evaluation in prior work is also relatively loose, typically using a
tolerance of more than 200 ms. In this work, we discover attention heads in
Whisper that capture accurate word alignments and are distinctively different
from those that do not. Moreover, we find that using characters produces finer
and more accurate alignments than using wordpieces. Based on these findings, we
propose an unsupervised approach to extracting word alignments by filtering
attention heads while teacher forcing Whisper with characters. Our approach not
only does not require training but also produces word alignments that are more
accurate than prior work under a stricter tolerance between 20 ms and 100 ms.

</details>


### [61] [Unified Learnable 2D Convolutional Feature Extraction for ASR](https://arxiv.org/abs/2509.10031)
*Peter Vieting,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 本文提出了一种更通用的神经前端架构，通过实验验证了其有效性，并展示了其在计算资源有限的场景中的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经前端方法仍然受到经典方法的严重影响，这可能有助于系统设计，但本文旨在开发一个更通用的前端用于特征提取。

Method: 本文提出了一种更通用的前端架构，旨在减少现有技术的影响，并通过实验系统地展示了如何实现这一点。

Result: 实验结果表明，所提出的2D卷积前端在参数效率方面表现出色，适用于计算资源有限的场景，并且其性能与现有的监督学习特征提取器相当。

Conclusion: 本文提出的通用统一方法不仅可行，而且与现有的监督学习特征提取器性能相当。

Abstract: Neural front-ends represent a promising approach to feature extraction for
automatic speech recognition (ASR) systems as they enable to learn specifically
tailored features for different tasks. Yet, many of the existing techniques
remain heavily influenced by classical methods. While this inductive bias may
ease the system design, our work aims to develop a more generic front-end for
feature extraction. Furthermore, we seek to unify the front-end architecture
contrasting with existing approaches that apply a composition of several layer
topologies originating from different sources. The experiments systematically
show how to reduce the influence of existing techniques to achieve a generic
front-end. The resulting 2D convolutional front-end is parameter-efficient and
suitable for a scenario with limited computational resources unlike large
models pre-trained on unlabeled audio. The results demonstrate that this
generic unified approach is not only feasible but also matches the performance
of existing supervised learnable feature extractors.

</details>


### [62] [Error Analysis in a Modular Meeting Transcription System](https://arxiv.org/abs/2509.10143)
*Peter Vieting,Simon Berger,Thilo von Neumann,Christoph Boeddeker,Ralf Schlüter,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: 本研究分析了语音分离中的泄漏问题，发现跨通道泄漏在主要说话人活跃区域显著，但对最终性能影响不大。先进的说话人日志方法能有效减少与理想分割的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管会议转录领域近年来取得了显著进展，但仍存在限制其性能的挑战。研究旨在分析语音分离中的泄漏问题，并探索如何改进分割方法以提高性能。

Method: 扩展了之前提出的框架，以分析语音分离中的泄漏，并对时间局部性有适当的敏感性。比较了不同的分割方法，展示了先进的说话人日志方法能够将与理想分割的差距减少三分之一。

Result: 研究发现，在只有主要说话人活跃的区域，存在显著的跨通道泄漏。然而，这些泄漏部分被语音活动检测（VAD） largely 忽略，因此对最终性能影响不大。先进的说话人日志方法能够显著减少与理想分割的差距。

Conclusion: 研究结果代表了在仅使用LibriSpeech数据训练识别模块的系统中，LibriCSS上的最先进性能。

Abstract: Meeting transcription is a field of high relevance and remarkable progress in
recent years. Still, challenges remain that limit its performance. In this
work, we extend a previously proposed framework for analyzing leakage in speech
separation with proper sensitivity to temporal locality. We show that there is
significant leakage to the cross channel in areas where only the primary
speaker is active. At the same time, the results demonstrate that this does not
affect the final performance much as these leaked parts are largely ignored by
the voice activity detection (VAD). Furthermore, different segmentations are
compared showing that advanced diarization approaches are able to reduce the
gap to oracle segmentation by a third compared to a simple energy-based VAD. We
additionally reveal what factors contribute to the remaining difference. The
results represent state-of-the-art performance on LibriCSS among systems that
train the recognition module on LibriSpeech data only.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: 本文提出了一种新的方法boldsea，用于建模复杂动态系统，通过将事件语义与数据流架构相结合，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统业务流程管理（BPM）系统和面向对象的语义技术存在一定的局限性，因此需要一种新的方法来建模复杂动态系统。

Method: 本文介绍了形式化的BSL（boldsea语义语言），包括其BNF语法，并概述了boldsea引擎的架构，该架构直接解释语义模型为可执行算法而无需编译。

Result: boldsea能够实现在运行时修改事件模型，确保时间透明性，并在统一的语义框架中无缝融合数据和业务逻辑。

Conclusion: 本文提出了boldsea，这是一种基于语义事件的方法，用于使用可执行本体建模复杂动态系统。该方法通过将事件语义与数据流架构相结合，解决了传统业务流程管理（BPM）系统和面向对象的语义技术的局限性。

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [64] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: 该研究探讨了大型语言模型是否能作为代理帮助他人完成任务，结果表明虽然所有模型都优于随机基线，但很少能有效帮助其他玩家。


<details>
  <summary>Details</summary>
Motivation: 研究旨在测试大型语言模型代理是否能够作为积极参与者帮助他人完成目标，而不仅仅是回答问题。

Method: 研究人员构建了一个工具，使解码器仅LLM能够在RLCard游戏环境中作为代理参与。这些模型接收完整的游戏状态信息，并使用两种不同的提示策略进行响应。

Result: 所有模型在玩UNO时都能成功超越随机基线，但很少有模型能显著帮助其他玩家。

Conclusion: 研究发现，尽管所有模型在玩UNO时都能成功超越随机基线，但很少有模型能显著帮助其他玩家。

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [65] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种新的代理框架A2P，通过结构化的因果推理任务解决多智能体系统中的故障归因问题，显著提高了步骤级准确性。


<details>
  <summary>Details</summary>
Motivation: 当前方法将故障归因视为长期对话日志上的模式识别任务，导致步骤级准确性极低（低于17%），这使得它们在调试复杂系统时不可行。其核心弱点是无法进行稳健的反事实推理：确定纠正单个动作是否实际上避免了任务失败。

Method: 引入Abduct-Act-Predict (A2P) Scaffolding框架，将故障归因从模式识别转化为结构化的因果推理任务。该框架通过一个推理过程引导大型语言模型：(1) 归纳，推断代理行为背后的隐藏根本原因；(2) 行动，定义最小的纠正干预措施；(3) 预测，模拟后续轨迹并验证干预是否解决了失败。

Result: 在Who&When基准测试中进行了广泛实验，结果表明A2P框架的有效性。在Algorithm-Generated数据集上，A2P实现了47.46%的步骤级准确性，比基线的16.67%提高了2.85倍。在更复杂的Hand-Crafted数据集上，它实现了29.31%的步骤准确性，比基线的12.07%提高了2.43倍。

Conclusion: 通过因果视角重新定义问题，A2P框架提供了一个稳健、可验证且显著更准确的自动化故障归因解决方案。

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [66] [Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks](https://arxiv.org/abs/2509.09706)
*Taniya Gidatkar,Oluwaseun Ajao,Matthew Shardlow*

Main category: cs.CR

TL;DR: 本研究评估了LLMs对对抗性攻击的抵抗力，发现某些模型具有强大的防御机制，但这些机制需要大量的计算资源。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解当前LLMs的安全性，识别现有防御方法的优势和劣势，并提出改进的防御策略。

Method: 通过TextFooler和BERTAttack设计系统化的对抗性测试，评估Flan-T5、BERT和RoBERTa-Base的鲁棒性。

Result: RoBERTa-Base和FlanT5表现出显著的鲁棒性，攻击成功率均为0%；而BERT-Base则非常脆弱，TextFooler的攻击成功率高达93.75%。

Conclusion: 本研究揭示了大型语言模型（LLMs）在面对对抗性攻击时的鲁棒性差异，并提出了更高效和有效的防御策略的建议。

Abstract: This study evaluates the resilience of large language models (LLMs) against
adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.
Using systematically designed adversarial tests through TextFooler and
BERTAttack, we found significant variations in model robustness. RoBERTa-Base
and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when
subjected to sophisticated attacks, with attack success rates of 0%. In
contrast. BERT-Base showed considerable vulnerability, with TextFooler
achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.
Our research reveals that while certain LLMs have developed effective defensive
mechanisms, these safeguards often require substantial computational resources.
This study contributes to the understanding of LLM security by identifying
existing strengths and weaknesses in current safeguarding approaches and
proposes practical recommendations for developing more efficient and effective
defensive strategies.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [67] [LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm](https://arxiv.org/abs/2509.09707)
*Camilo Chacón Sartori,Martín Isla Pino,Pedro Pinacho-Davidson,Christian Blum*

Main category: cs.NE

TL;DR: 本文提出了一种将大型语言模型与有偏随机键遗传算法结合的新框架，用于解决复杂的组合优化问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽略单个问题实例的结构特性，而我们的工作旨在通过整合LLM与BRKGA来克服这一限制，从而更有效地解决复杂的组合优化问题。

Method: 我们引入了一个新的框架，将大型语言模型（LLM）与有偏随机键遗传算法（BRKGA）结合，以解决NP难的最长运行子序列问题。该方法通过引入人-LLM协作过程来共同设计和实现一组计算高效的度量标准，扩展了实例驱动的启发式偏差范式。

Result: 实验评估表明，我们的混合方法BRKGA+Llama-4-Maverick在最复杂的实例上相对于基线取得了统计学上显著的改进。

Conclusion: 我们的研究结果证实，利用大型语言模型（LLM）生成先验的、实例驱动的启发式偏差是一种在复杂优化领域增强元启发式方法的有价值方法。

Abstract: Integrating Large Language Models (LLMs) within metaheuristics opens a novel
path for solving complex combinatorial optimization problems. While most
existing approaches leverage LLMs for code generation to create or refine
specific heuristics, they often overlook the structural properties of
individual problem instances. In this work, we introduce a novel framework that
integrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the
NP-hard Longest Run Subsequence problem. Our approach extends the
instance-driven heuristic bias paradigm by introducing a human-LLM
collaborative process to co-design and implement a set of computationally
efficient metrics. The LLM analyzes these instance-specific metrics to generate
a tailored heuristic bias, which steers the BRKGA toward promising areas of the
search space. We conduct a comprehensive experimental evaluation, including
rigorous statistical tests, convergence and behavioral analyses, and targeted
ablation studies, comparing our method against a standard BRKGA baseline across
1,050 generated instances of varying complexity. Results show that our
top-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically
significant improvements over the baseline, particularly on the most complex
instances. Our findings confirm that leveraging an LLM to produce an a priori,
instance-driven heuristic bias is a valuable approach for enhancing
metaheuristics in complex optimization domains.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [68] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 本文介绍了一种新的集成框架，用于稳定从噪声历史文档中提取文本，通过融合多个增强变体的转录结果并使用自定义对齐器提高准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在稳定基于LLM的从噪声历史文档中提取文本的过程。

Method: 提出了一种新的集成框架，通过使用Gemini 2.0 Flash对每张图像的多个增强变体进行转录，并使用自定义的Needleman Wunsch风格对齐器融合这些输出，从而获得共识转录和置信度分数。

Result: 在622个宾夕法尼亚州死亡记录的新数据集上，该方法相对于单次基准提高了4个百分点的转录准确率。

Conclusion: 该方法简单、可扩展，并可立即部署到其他文档集合和转录模型中。

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [69] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: VARCO-VISION-2.0是一个改进的双语视觉语言模型，支持多图像理解，并在空间定位和语言表现方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 引入VARCO-VISION-2.0，这是一个改进的双语视觉语言模型，相比之前的模型VARCO-VISION-14B具有更强的能力。

Method: 通过四阶段课程和内存高效技术进行训练，以增强多模态对齐，同时保持核心语言能力并提高安全性。

Result: 模型在空间定位和语言表现方面表现出色，并在OpenCompass VLM排行榜上取得了第八名的成绩。此外，还发布了适用于设备部署的1.7B版本。

Conclusion: 这些模型推进了双语视觉语言模型的发展及其实际应用。

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [70] [Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks](https://arxiv.org/abs/2509.09870)
*Hasibur Rahman,Smit Desai*

Main category: cs.HC

TL;DR: 本研究探讨了个性表达水平和用户-代理个性一致性如何影响目标导向任务中的感知，发现中等表达和个性一致性可以提高用户的正面评价。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨个性表达水平和用户-代理个性一致性如何影响目标导向任务中的感知。

Method: 本研究通过一项被试间实验（N=150）探讨了个性表达水平和用户-代理个性一致性如何影响目标导向任务中的感知。使用我们新的特质调节键框架控制大五人格特质的表达水平。

Result: 结果显示了一个倒U型关系：中等表达在智能、享受、拟人化、采用意图、信任和喜爱度方面产生了最积极的评价，显著优于两个极端。个性一致性进一步增强了结果，其中外向性和情绪稳定性是最有影响力的特质。聚类分析确定了三种不同的兼容性配置文件，其中“高度一致”的用户报告了显著积极的感知。

Conclusion: 研究结果表明，个性表达和战略性特质对齐构成了CA个性的最佳设计目标，为基于LLM的CA日益普及提供了设计启示。

Abstract: Large language models (LLMs) enable conversational agents (CAs) to express
distinctive personalities, raising new questions about how such designs shape
user perceptions. This study investigates how personality expression levels and
user-agent personality alignment influence perceptions in goal-oriented tasks.
In a between-subjects experiment (N=150), participants completed travel
planning with CAs exhibiting low, medium, or high expression across the Big
Five traits, controlled via our novel Trait Modulation Keys framework. Results
revealed an inverted-U relationship: medium expression produced the most
positive evaluations across Intelligence, Enjoyment, Anthropomorphism,
Intention to Adopt, Trust, and Likeability, significantly outperforming both
extremes. Personality alignment further enhanced outcomes, with Extraversion
and Emotional Stability emerging as the most influential traits. Cluster
analysis identified three distinct compatibility profiles, with "Well-Aligned"
users reporting substantially positive perceptions. These findings demonstrate
that personality expression and strategic trait alignment constitute optimal
design targets for CA personality, offering design implications as LLM-based
CAs become increasingly prevalent.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [71] [HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets](https://arxiv.org/abs/2509.09740)
*Ying Yuan,Xing-Yue Monica Ge,Aaron Archer Waterman,Tommaso Biancalani,David Richmond,Yogesh Pandit,Avtar Singh,Russell Littman,Jin Liu,Jan-Christian Huetter,Vladimir Ermakov*

Main category: q-bio.QM

TL;DR: 本文提出了一种基于大型语言模型的框架，用于自动化单细胞数据的聚类注释和分辨率选择，提高了功能注释的客观性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的聚类分辨率选择和功能注释依赖于启发式方法和专家手动整理，存在主观性。本文旨在通过引入LLM驱动的框架，实现更客观、自动化的聚类注释和分辨率选择。

Method: 本文提出了一种名为HYPOGENEAGENT的框架，该框架利用大型语言模型（LLM）将聚类注释转化为可定量优化的任务。首先，LLM作为基因集分析工具分析每个基因程序或扰动模块的内容，并生成基于GO的假设排名列表，同时附带校准的置信度分数。然后，我们使用句子嵌入模型嵌入每个预测描述，计算成对余弦相似性，并让代理评审小组评分：(i) 预测的内部一致性，即同一聚类内的高平均相似性，称为聚类内一致性；(ii) 其外部独特性，即不同聚类之间的低相似性，称为聚类间分离。这两个量结合产生一个代理衍生的分辨率得分，当聚类表现出同时一致性和互斥性时最大化。

Result: 在公开的K562 CRISPRi Perturb-seq数据集上应用分辨率得分时，所选的聚类粒度与已知通路对齐，优于经典的度量标准如轮廓系数、模块化得分等。

Conclusion: 本文展示了LLM代理在单细胞多组学研究中作为客观的聚类分辨率和功能注释仲裁者的能力，为完全自动化的上下文感知解释流程铺平了道路。

Abstract: Large-scale single-cell and Perturb-seq investigations routinely involve
clustering cells and subsequently annotating each cluster with Gene-Ontology
(GO) terms to elucidate the underlying biological programs. However, both
stages, resolution selection and functional annotation, are inherently
subjective, relying on heuristics and expert curation. We present
HYPOGENEAGENT, a large language model (LLM)-driven framework, transforming
cluster annotation into a quantitatively optimizable task. Initially, an LLM
functioning as a gene-set analyst analyzes the content of each gene program or
perturbation module and generates a ranked list of GO-based hypotheses,
accompanied by calibrated confidence scores. Subsequently, we embed every
predicted description with a sentence-embedding model, compute pair-wise cosine
similarities, and let the agent referee panel score (i) the internal
consistency of the predictions, high average similarity within the same
cluster, termed intra-cluster agreement (ii) their external distinctiveness,
low similarity between clusters, termed inter-cluster separation. These two
quantities are combined to produce an agent-derived resolution score, which is
maximized when clusters exhibit simultaneous coherence and mutual exclusivity.
When applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary
test, our Resolution Score selects clustering granularities that exhibit
alignment with known pathway compared to classical metrics such silhouette
score, modularity score for gene functional enrichment summary. These findings
establish LLM agents as objective adjudicators of cluster resolution and
functional annotation, thereby paving the way for fully automated,
context-aware interpretation pipelines in single-cell multi-omics studies.

</details>

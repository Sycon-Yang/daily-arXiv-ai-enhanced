<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 82]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)
*Javier Conde,Miguel González,María Grandury,Gonzalo Martínez,Pedro Reviriego,Mar Brysbaert*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）与人类在心理语言学数据集上的对齐情况，发现LLMs在某些感官关联特征上的表现不如人类，这可能与缺乏具身认知有关。


<details>
  <summary>Details</summary>
Motivation: 以往对LLMs的评估主要集中在它们执行各种任务的能力上，如推理、问答、改写或翻译。然而，其他语言特征难以量化，例如一个词的唤醒度、具体性或性别，以及我们通过感官体验词语并将其与特定感官相关联的程度。心理语言学已经对这些特征进行了多年的研究，并通过大规模的人类实验产生了数千个词的评分。这为评估LLMs与人类评分在这些词特征上的对齐程度提供了机会。

Method: 本文评估了代表性的一组LLMs与人类在两个心理语言学数据集（Glasgow和Lancaster规范）上的对齐情况。这些数据集涵盖了数千个单词的十三个特征。

Result: 结果表明，在评估的Glasgow规范（唤醒度、情绪价值、支配力、具体性、形象性、熟悉度和性别）中，对齐度通常更好，而在评估的Lancaster规范（内省、味觉、嗅觉、触觉、听觉和视觉）中对齐度较差。

Conclusion: 当前大型语言模型（LLMs）在与人类对词语特征的评价对齐方面存在一定的局限性，这可能是因为它们缺乏人类所具有的具身认知。同时，该研究也展示了使用心理语言学数据集评估LLMs的有用性。

Abstract: The evaluation of LLMs has so far focused primarily on how well they can
perform different tasks such as reasoning, question-answering, paraphrasing, or
translating. For most of these tasks, performance can be measured with
objective metrics, such as the number of correct answers. However, other
language features are not easily quantified. For example, arousal,
concreteness, or gender associated with a given word, as well as the extent to
which we experience words with senses and relate them to a specific sense.
Those features have been studied for many years by psycholinguistics,
conducting large-scale experiments with humans to produce ratings for thousands
of words. This opens an opportunity to evaluate how well LLMs align with human
ratings on these word features, taking advantage of existing studies that cover
many different language features in a large number of words. In this paper, we
evaluate the alignment of a representative group of LLMs with human ratings on
two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets
cover thirteen features over thousands of words. The results show that
alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated
(arousal, valence, dominance, concreteness, imageability, familiarity, and
gender) than on the Lancaster norms evaluated (introceptive, gustatory,
olfactory, haptic, auditory, and visual). This suggests a potential limitation
of current LLMs in aligning with human sensory associations for words, which
may be due to their lack of embodied cognition present in humans and
illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [2] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)
*Sudip Dasgupta,Himanshu Shankar*

Main category: cs.CL

TL;DR: 本文提出了一种基于AI代理的模块化、多智能体系统，用于自动化审查高度结构化的企业业务文档。该系统在准确性、一致性、完整性、清晰度等方面表现出色，并在关键领域接近或超越人类表现。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案主要集中在非结构化文本或有限的合规检查上，而本文旨在解决高度结构化企业文档的自动化审查问题。

Method: 该框架利用现代编排工具（如LangChain、CrewAI、TruLens和Guidance）实现按部分评估文档的准确性、一致性、完整性和清晰度。专门的代理负责不同的审查标准，如模板合规性或事实正确性，并根据需要并行或顺序运行。

Result: 定量评估表明，AI代理作为法官系统在关键领域接近或超越人类表现：达到99%的信息一致性（比人类的92%高），错误和偏见率减半，平均审查时间从30分钟减少到2.5分钟，AI与专家人类判断的协议率达到95%。

Conclusion: 该研究提出了一种模块化、多智能体系统，用于使用AI代理自动审查高度结构化的企业业务文档。该系统在关键领域接近或超越人类表现，并为AI驱动的企业文档质量保证提供了灵活、可审计和可扩展的基础。

Abstract: This study presents a modular, multi-agent system for the automated review of
highly structured enterprise business documents using AI agents. Unlike prior
solutions focused on unstructured texts or limited compliance checks, this
framework leverages modern orchestration tools such as LangChain, CrewAI,
TruLens, and Guidance to enable section-by-section evaluation of documents for
accuracy, consistency, completeness, and clarity. Specialized agents, each
responsible for discrete review criteria such as template compliance or factual
correctness, operate in parallel or sequence as required. Evaluation outputs
are enforced to a standardized, machine-readable schema, supporting downstream
analytics and auditability. Continuous monitoring and a feedback loop with
human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system
approaches or exceeds human performance in key areas: achieving 99% information
consistency (vs. 92% for humans), halving error and bias rates, and reducing
average review time from 30 to 2.5 minutes per document, with a 95% agreement
rate between AI and expert human judgment. While promising for a wide range of
industries, the study also discusses current limitations, including the need
for human oversight in highly specialized domains and the operational cost of
large-scale LLM usage. The proposed system serves as a flexible, auditable, and
scalable foundation for AI-driven document quality assurance in the enterprise
context.

</details>


### [3] [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)
*Ming Cheung*

Main category: cs.CL

TL;DR: 本文提出了一种利用多个小型语言模型检测LLMs生成回答中幻觉的框架，实验结果表明该方法有效且高效。


<details>
  <summary>Details</summary>
Motivation: LLMs在实际应用中的可靠性受到回答中幻觉的威胁，而这些幻觉在没有真实数据的情况下难以检测，特别是在问答场景中。

Method: 该论文提出了一种框架，通过将LLMs生成的回答分解为单独的句子，并利用多个模型对给定问题、回答和相关上下文生成'是'标记的概率来检测幻觉。

Result: 实验结果表明，与检测幻觉相比，检测正确回答的F1分数提高了10%，证明了多个小型语言模型可以有效地用于答案验证。

Conclusion: 该框架通过整合多个小型语言模型，有效提高了检测LLMs生成回答中的幻觉的能力，为学术和实际应用提供了可扩展且高效的解决方案。

Abstract: Since the introduction of ChatGPT, large language models (LLMs) have
demonstrated significant utility in various tasks, such as answering questions
through retrieval-augmented generation. Context can be retrieved using a
vectorized database, serving as a foundation for LLMs to generate responses.
However, hallucinations in responses can undermine the reliability of LLMs in
practical applications, and they are not easily detectable in the absence of
ground truth, particularly in question-and-answer scenarios. This paper
proposes a framework that integrates multiple small language models to verify
responses generated by LLMs using the retrieved context from a vectorized
database. By breaking down the responses into individual sentences and
utilizing the probability of generating "Yes" tokens from the outputs of
multiple models for a given set of questions, responses, and relevant context,
hallucinations can be detected. The proposed framework is validated through
experiments with real datasets comprising over 100 sets of questions, answers,
and contexts, including responses with fully and partially correct sentences.
The results demonstrate a 10\% improvement in F1 scores for detecting correct
responses compared to hallucinations, indicating that multiple small language
models can be effectively employed for answer verification, providing a
scalable and efficient solution for both academic and practical applications.

</details>


### [4] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
*Oliver Warke,Joemon M. Jose,Faegheh Hasibi,Jan Breitsohl*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的数据增强方法PromptAug，在冲突和情绪数据集上取得了显著的性能提升，并通过多种评估方法验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于社会媒体上冲突的增加，需要有效的分类模型来检测有害行为。然而，高质量的标记数据在诸如识别冲突行为等细微任务中是有限的、昂贵且难以获得的。此外，由于社交媒体平台越来越多地限制研究数据的访问，文本数据增强正在成为生成训练数据的替代方法。

Method: 本文介绍了PromptAug，这是一种基于大型语言模型（LLM）的数据增强方法。

Result: PromptAug在冲突和情绪数据集上的准确率和F1分数分别提高了2%。通过极端数据稀缺场景、定量多样性分析和定性主题分析对PromptAug进行了全面评估。主题分析识别出增强文本中的四个问题模式：语言流畅性、幽默模糊性、增强内容模糊性和增强内容误解。

Conclusion: 本文提出了PromptAug作为一种有效的数据增强方法，用于敏感任务如冲突检测，并通过自然语言处理和社会科学方法进行了跨学科评估。

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [5] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)
*Chenyang Shao,Tianxing Li,Chenhao Pu,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为AgentStealth的自增强LLM匿名化框架，通过对抗性工作流、监督适应和在线强化学习来提高匿名化效果和实用性，并支持在边缘设备上部署。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要么依赖于刚性的替换，这会损害效用，要么依赖于云-based LLMs，这成本高且存在隐私风险。为了应对这些挑战，我们探索了本地部署的小规模语言模型（SLMs）用于匿名化。然而，由于高质量监督的限制，训练有效的SLMs仍然具有挑战性。

Method: 我们提出了AgentStealth，一种自增强的LLM匿名化框架。首先，我们引入了一种通过上下文对比学习和自适应效用感知控制增强的对抗性匿名化工作流。其次，我们使用从工作流中收集的高质量数据（包括匿名化和攻击信号）对SLMs进行监督适应。最后，我们应用在线强化学习，使模型利用其内部对抗反馈来迭代改进匿名化性能。

Result: 实验表明，我们的方法在两个数据集上的匿名化效果提高了12.3%，实用性提高了6.8%。

Conclusion: 我们的方法在匿名化效果和实用性方面均优于基线，轻量级设计支持直接部署在边缘设备上，避免了对云的依赖和基于通信的隐私风险。

Abstract: In today's digital world, casual user-generated content often contains subtle
cues that may inadvertently expose sensitive personal attributes. Such risks
underscore the growing importance of effective text anonymization to safeguard
individual privacy. However, existing methods either rely on rigid replacements
that damage utility or cloud-based LLMs that are costly and pose privacy risks.
To address these issues, we explore the use of locally deployed smaller-scale
language models (SLMs) for anonymization. Yet training effective SLMs remains
challenging due to limited high-quality supervision. To address the challenge,
we propose AgentStealth, a self-reinforcing LLM anonymization framework.First,
we introduce an adversarial anonymization workflow enhanced by In-context
Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform
supervised adaptation of SLMs using high-quality data collected from the
workflow, which includes both anonymization and attack signals. Finally, we
apply online reinforcement learning where the model leverages its internal
adversarial feedback to iteratively improve anonymization performance.
Experiments on two datasets show that our method outperforms baselines in both
anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight
design supports direct deployment on edge devices, avoiding cloud reliance and
communication-based privacy risks. Our code is open-source at
https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [6] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
*Zihao Zhao,Xinlong Zhai,Jinyu Yang,Chuan Shi*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [7] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)
*Jingkai Li*

Main category: cs.CL

TL;DR: 本研究应用IIT框架分析LLM表示序列，发现其缺乏显著的'意识'现象指标，但表现出有趣的模式。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM表示序列中是否存在'意识'现象，并区分潜在的'意识'现象与LLM表示空间中的固有分离。

Method: 应用IIT 3.0和4.0框架分析大型语言模型（LLM）表示序列，并与独立于任何意识估计的跨度表示进行比较。

Result: 研究发现LLM表示序列缺乏统计学上显著的'意识'现象指标，但在空间排列分析下显示出有趣模式。

Conclusion: 研究结果表明，当代基于Transformer的LLM表示序列缺乏显著的'意识'现象指标，但在空间排列分析下表现出有趣的模式。

Abstract: Integrated Information Theory (IIT) provides a quantitative framework for
explaining consciousness phenomenon, positing that conscious systems comprise
elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the
latest iterations of this framework -- to sequences of Large Language Model
(LLM) representations, analyzing data derived from existing Theory of Mind
(ToM) test results. Our study systematically investigates whether the
differences of ToM test performances, when presented in the LLM
representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT
3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure
(IIT 4.0). Furthermore, we compare these metrics with the Span Representations
independent of any estimate for consciousness. This additional effort aims to
differentiate between potential "consciousness" phenomena and inherent
separations within LLM representational space. We conduct comprehensive
experiments examining variations across LLM transformer layers and linguistic
spans from stimuli. Our results suggest that sequences of contemporary
Transformer-based LLM representations lack statistically significant indicators
of observed "consciousness" phenomena but exhibit intriguing patterns under
$\textit{spatio}$-permutational analyses. The Appendix and code are available
as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [8] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)
*Deyu Zou,Yongqiang Chen,Mufei Li,Siqi Miao,Chenxi Liu,Bo Han,James Cheng,Pan Li*

Main category: cs.CL

TL;DR: ReG是一种改进的基于图的RAG方法，通过结合LLM反馈和结构感知重新组织模块，提高了检索质量并减少了推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的RAG方法中，弱检索器存在两个问题：一是由于缺乏真实数据，检索器通常在弱监督下训练，这会向LLMs引入虚假信号；二是由于图数据的抽象性，检索到的知识通常以无序形式呈现。

Method: ReG通过结合LLM反馈来消除虚假信号并提高监督质量，同时引入结构感知重新组织模块，将检索结果重构为逻辑连贯的证据链。

Result: 在主流基准测试中，ReG通过最多10%的改进显著且一致地提升了不同LLM架构的性能。改进的监督质量使ReG能够在5%的训练数据下达到最先进的性能，并能迁移到分布外的KG中。当应用于基于推理的LLM时，ReG最多可减少30%的推理令牌成本，并提高4%的性能。

Conclusion: ReG能够显著提升基于图的RAG性能，并在不同LLM架构中表现出一致性改进。此外，ReG在减少推理令牌成本和提高性能方面表现优异。

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to ground responses with structured external knowledge from
up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs
often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground
truth, the retriever is often trained on weak supervision, which often
introduces spurious signals to the LLMs. II) Due to the abstraction of graph
data, the retrieved knowledge is often presented in unorganized forms. To
mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak
retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM
feedback to get rid of spurious signals and improve the quality of the
supervision. Meanwhile, ReG introduces a structure-aware reorganization module
to refactor the retrieval results into logically coherent evidence chains.
Experiments on prominent benchmarks demonstrate that ReG significantly and
consistently brings improvements across different LLM backbones by up to 10%.
The improved supervision quality enables ReG to match the state-of-the-art
performance with 5% training data and to transfer to out-of-distribution KGs.
Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token
cost by up to 30% and improves the performance by up to 4%.

</details>


### [9] [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/abs/2506.22529)
*Lu Kalkbrenner,Veronika Solopova,Steffen Zeiler,Robert Nickel,Dorothea Kolossa*

Main category: cs.CL

TL;DR: 本研究介绍了第一个德语Telegram图数据集，用于虚假信息检测，并展示了图神经网络在该任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 连接性和消息传播在虚假信息检测中是重要的但常被忽视的信息来源，尤其是在像Telegram这样的低监管平台上。

Method: 引入了Misinfo-TeleGraph，这是一个基于Telegram的德语图数据集，用于虚假信息检测，并评估了文本模型和图神经网络（GNNs）的表现。

Result: 使用LSTM聚合的GraphSAGE在MCC和F1分数方面显著优于文本基线模型。

Conclusion: 本研究提供了可重复的基准和开放数据集，以促进对德语Telegram网络和其他低监管社交媒体平台上的虚假信息检测的未来研究。

Abstract: Connectivity and message propagation are central, yet often underutilized,
sources of information in misinformation detection -- especially on poorly
moderated platforms such as Telegram, which has become a critical channel for
misinformation dissemination, namely in the German electoral context. In this
paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based
graph dataset for misinformation detection. It includes over 5 million messages
from public channels, enriched with metadata, channel relationships, and both
weak and strong labels. These labels are derived via semantic similarity to
fact-checks and news articles using M3-embeddings, as well as manual
annotation. To establish reproducible baselines, we evaluate both text-only
models and graph neural networks (GNNs) that incorporate message forwarding as
a network structure. Our results show that GraphSAGE with LSTM aggregation
significantly outperforms text-only baselines in terms of Matthews Correlation
Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers,
view counts, and automatically versus human-created labels on performance, and
highlight both the potential and challenges of weak supervision in this domain.
This work provides a reproducible benchmark and open dataset for future
research on misinformation detection in German-language Telegram networks and
other low-moderation social platforms.

</details>


### [10] [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)
*Nicholas Edwards,Yukyung Lee,Yujun,Mao,Yulu Qin,Sebastian Schuster,Najoung Kim*

Main category: cs.CL

TL;DR: 本文介绍了RExBench，用于评估大型语言模型代理在研究扩展和实现方面的能力，并发现当前代理在没有大量人类指导的情况下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究扩展和其实现是此类系统的关键能力，需要一个基准来评估这一能力。

Method: 引入RExBench来评估系统在研究扩展和实现方面的能力，通过自动评估基础设施执行代理输出以确定成功标准是否满足。

Result: 所有评估的代理都无法自主实现大部分扩展，即使在添加额外的人类编写提示后，最佳性能仍低于40%。

Conclusion: 当前的代理仍然无法在没有大量人类指导的情况下处理现实的研究扩展任务。

Abstract: Agents based on Large Language Models (LLMs) have shown promise for
performing sophisticated software engineering tasks autonomously. In addition,
there has been progress towards developing agents that can perform parts of the
research pipeline in machine learning and the natural sciences. We argue that
research extension and its implementation is a critical capability for such
systems, and introduce RExBench to support the evaluation of this capability.
RExBench is a benchmark consisting of 12 realistic research experiment
implementation tasks that aim to investigate research hypotheses that have not
previously been implemented. Each task is set up as an extension to an existing
research paper and codebase, accompanied by domain expert-written instructions.
RExBench is robust to data contamination, and supports an automatic evaluation
infrastructure that executes agent outputs to determine whether the success
criteria are met. We use this benchmark to evaluate nine LLM agents implemented
using three different frameworks: aider, Claude Code, and OpenHands. We find
that all agents evaluated fail to autonomously implement the majority of the
extensions. Although the success rate improves with additional human-written
hints, the best performance under this setting remains below 40%. This
indicates that current agents are still short of being able to handle realistic
research extension tasks without substantial human guidance.

</details>


### [11] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)
*Badr Youbi Idrissi,Monica Millunzi,Amelia Sorrenti,Lorenzo Baraldi,Daryna Dementieva*

Main category: cs.CL

TL;DR: 本研究提出了一种新的水印方法，用于检测合成文本，并通过实验验证了其有效性，以确保大型语言模型在AI驱动的文本生成中的伦理应用。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在社会各个领域的广泛应用，对其潜在滥用的担忧日益增加。因此，一些学术研究致力于引入水印技术，以帮助算法识别机器生成的文本。本研究旨在开发一种新的合成文本检测方法，以确保LLMs在AI驱动的文本生成中的伦理应用。

Method: 本研究首先复制了先前基准研究的结果，以证明其对底层生成模型变化的敏感性，然后提出了一种创新的水印方法，并通过改写生成的文本评估其鲁棒性。

Result: 实验结果表明，本研究提出的水印方法在鲁棒性方面优于~\cite{aarson}水印方法。

Conclusion: 本研究提出了一种创新的水印方法，并通过实验验证了其在检测合成文本方面的有效性，从而确保LLMs在AI驱动的文本生成中的伦理应用。

Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing
their presence as powerful instruments permeating various sectors of society.
While their utility offers valuable support to individuals, there are multiple
concerns over potential misuse. Consequently, some academic endeavors have
sought to introduce watermarking techniques, characterized by the inclusion of
markers within machine-generated text, to facilitate algorithmic
identification. This research project is focused on the development of a novel
methodology for the detection of synthetic text, with the overarching goal of
ensuring the ethical application of LLMs in AI-driven text generation. The
investigation commences with replicating findings from a previous baseline
study, thereby underscoring its susceptibility to variations in the underlying
generation model. Subsequently, we propose an innovative watermarking approach
and subject it to rigorous evaluation, employing paraphrased generated text to
asses its robustness. Experimental results highlight the robustness of our
proposal compared to the~\cite{aarson} watermarking method.

</details>


### [12] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)
*Chase Fensore,Kaustubh Dhole,Joyce C Ho,Eugene Agichtein*

Main category: cs.CL

TL;DR: 我们提交的混合系统结合了稀疏和密集检索方法，使用Falcon3-10B-Instruct生成答案。虽然神经重新排序提高了性能，但计算成本过高；DSPy优化提示策略虽然提升了语义相似度，但存在过度自信的问题。最终系统在忠实度和正确性方面排名靠前。


<details>
  <summary>Details</summary>
Motivation: 评估检索增强生成（RAG）系统在动态测试集上的表现，并探索改进系统性能的方法。

Method: 我们结合了稀疏（BM25）和密集（E5）检索方法，并使用Falcon3-10B-Instruct生成相关且可信的答案。此外，还尝试了神经重新排序和DSPy优化提示策略。

Result: 神经重新排序提高了MAP，但带来了高昂的计算成本；DSPy优化提示策略实现了更高的语义相似度，但存在过度自信的问题。混合系统在忠实度和正确性方面表现良好。

Conclusion: 我们的混合系统在没有重新排序的情况下在25支队伍中获得了第4名的忠实度和第11名的正确性。分析显示，问题和文档之间的词汇对齐是性能最强的预测因素。

Abstract: We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [13] [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/abs/2506.22679)
*Ankush Raut,Projna Paromita,Sydney Begerowski,Suzanne Bell,Theodora Chaspari*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在检测团队对话中微妙的微行为方面的可行性，并发现指令微调的Llama-3.1在性能上优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 我们旨在探索大型语言模型（LLMs）在检测团队对话中微妙的微行为表达方面的可行性，特别是在模拟太空任务期间收集的转录本中。

Method: 我们使用了零样本分类、微调以及通过编码器-only序列分类LLMs进行的改写增强微调，以及通过解码器-only因果语言建模LLMs进行的少样本文本生成，以预测每个对话回合相关的微行为。

Result: 编码器-only LLMs（如RoBERTa和DistilBERT）即使经过加权微调也难以检测到被低估的微行为，尤其是压抑性言语。而指令微调的Llama-3.1（一种解码器-only LLM）表现出色，最佳模型在三类分类中的宏F1分数达到44%，在二分类中的宏F1分数达到68%。

Conclusion: 我们的研究结果表明，指令微调的Llama-3.1（一种解码器-only LLM）在检测团队对话中的微妙微行为方面表现优于其他模型，这为开发用于分析团队沟通动态和提高高风险环境（如太空任务）中培训干预措施的语音技术提供了启示。

Abstract: We explore the feasibility of large language models (LLMs) in detecting
subtle expressions of micro-behaviors in team conversations using transcripts
collected during simulated space missions. Specifically, we examine zero-shot
classification, fine-tuning, and paraphrase-augmented fine-tuning with
encoder-only sequence classification LLMs, as well as few-shot text generation
with decoder-only causal language modeling LLMs, to predict the micro-behavior
associated with each conversational turn (i.e., dialogue). Our findings
indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to
detect underrepresented micro-behaviors, particularly discouraging speech, even
with weighted fine-tuning. In contrast, the instruction fine-tuned version of
Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best
models achieving macro F1-scores of 44% for 3-way classification and 68% for
binary classification. These results have implications for the development of
speech technologies aimed at analyzing team communication dynamics and
enhancing training interventions in high-stakes environments such as space
missions, particularly in scenarios where text is the only accessible data.

</details>


### [14] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为VocabTrim的技术，通过在起草过程中限制词汇表来提高内存受限环境下的生成速度。


<details>
  <summary>Details</summary>
Motivation: 我们发现，在起草过程中，由于目标LLM的词汇量非常大，现有的草案令牌采样方案存在不必要的推理开销。

Method: 我们提出了一种简单的方法VocabTrim，通过在起草过程中限制词汇表来减轻起草开销，从而提高生成速度。

Result: VocabTrim通过限制起草过程中的词汇表，显著减少了内存受限环境下的起草延迟，从而提高了内存受限速度提升（MBSU）。

Conclusion: 我们的方法可以提升Llama-3模型在Spec-Bench上的内存限制速度提升，特别是对于Llama-3.2-3B-Instruct模型，提升了16%。

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [15] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)
*Emily Dux Speltz*

Main category: cs.CL

TL;DR: 本报告综合了跨学科研讨会的结果，探讨了AI语言模型与人类认知过程在文本理解和创作之间的关系。研讨会揭示了大型语言模型（LLMs）与人类认知之间的新兴模式，强调了伦理考量和负责任使用AI技术的重要性，同时指出了人机协作在语言任务中的机遇和挑战。


<details>
  <summary>Details</summary>
Motivation: 研讨会旨在解决我们对AI语言模型与人类认知过程在文本理解和创作之间的关系的理解中的关键知识缺口。

Method: 本报告综合了最近一次跨学科研讨会的结果，该研讨会汇集了认知心理学、语言学习和基于人工智能（AI）的自然语言处理（NLP）领域的领先专家。研讨会通过跨认知、语言和技术视角的协作对话，探讨了人类在产生和理解文本时的潜在过程，以及AI如何既有助于我们对这些过程的理解，又能增强人类能力。

Result: 研讨会揭示了大型语言模型（LLMs）与人类认知之间的新兴模式，重点在于LLMs的能力及其在完全复制人类语言理解和生成方面的局限性。主要发现包括LLMs在提供人类语言处理见解方面的潜力，当模型通过人类反馈进行微调时，其行为与人类语言处理的日益对齐，以及人机协作在语言任务中带来的机遇和挑战。

Conclusion: 本报告旨在指导未来在认知心理学、语言学和教育领域中大型语言模型（LLMs）的研究、开发和应用，并强调了在追求通过有效的师生AI协作增强人类在文本理解和生成方面的能力时，伦理考量和负责任使用AI技术的重要性。

Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop
that brought together leading experts in cognitive psychology, language
learning, and artificial intelligence (AI)-based natural language processing
(NLP). The workshop, funded by the National Science Foundation, aimed to
address a critical knowledge gap in our understanding of the relationship
between AI language models and human cognitive processes in text comprehension
and composition. Through collaborative dialogue across cognitive, linguistic,
and technological perspectives, workshop participants examined the underlying
processes involved when humans produce and comprehend text, and how AI can both
inform our understanding of these processes and augment human capabilities. The
workshop revealed emerging patterns in the relationship between large language
models (LLMs) and human cognition, with highlights on both the capabilities of
LLMs and their limitations in fully replicating human-like language
understanding and generation. Key findings include the potential of LLMs to
offer insights into human language processing, the increasing alignment between
LLM behavior and human language processing when models are fine-tuned with
human feedback, and the opportunities and challenges presented by human-AI
collaboration in language tasks. By synthesizing these findings, this report
aims to guide future research, development, and implementation of LLMs in
cognitive psychology, linguistics, and education. It emphasizes the importance
of ethical considerations and responsible use of AI technologies while striving
to enhance human capabilities in text comprehension and production through
effective human-AI collaboration.

</details>


### [16] [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/abs/2506.22724)
*Niyati Bafna,Tianjian Li,Kenton Murray,David R. Mortensen,David Yarowsky,Hale Sirin,Daniel Khashabi*

Main category: cs.CL

TL;DR: 我们发现多语言生成的质量问题主要源于翻译阶段的失败，尤其是在低资源语言中。这为未来改进大语言模型的多语言性提供了指导。


<details>
  <summary>Details</summary>
Motivation: 多语言生成在中等至低资源语言中的质量往往较差。我们试图找出导致最终输出质量低下的原因。

Method: 我们使用logit lens观察模型在中间层的处理情况，测试了翻译障碍假设。

Result: 我们发现，整体失败的很大一部分确实源于翻译失败，即模型无法将正确解决的中间概念翻译成目标语言。这一点在低资源目标语言中尤为明显。

Conclusion: 我们的结果突显了端到端多语言生成的重要障碍，并为未来寻求提高大语言模型多语言性的研究提供了指导性见解。

Abstract: Multilingual generation with large language models (LLMs) is often of poor
quality for mid- to low-resource languages. Building on insights from
interpretability, we demonstrate the existence of an implicit
task-solving-->translation pipeline for generation, whereby the model first
solves the required task in a largely target-language-agnostic manner, and
subsequently translates answer concepts into the intended target language. We
hypothesize that the failure of the translation stage is an important culprit
for the observed low quality of final outputs, and formalize this as the
translation barrier hypothesis. We test this hypothesis for a word translation
task across 108 language pairs, using logit lens to observe model processing in
intermediate layers. We find that a significant portion of overall failures
indeed stems from translation failure, or the model's inability to translate
correctly solved intermediate concepts into the target language. This is
especially true for low-resource target languages. Our results highlight an
important hurdle for end-to-end multilingual generation, and lend guiding
insights for future work seeking to improve multilinguality in LLMs.

</details>


### [17] [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
*Alan Dao,Dinh Bach Vu*

Main category: cs.CL

TL;DR: Jan-nano是一个4B参数的语言模型，通过彻底的专业化重新定义了效率，能够在消费级硬件上运行并达到高准确率。


<details>
  <summary>Details</summary>
Motivation: 大多数语言模型面临一个基本的权衡，即强大的功能需要大量的计算资源。

Method: 通过我们的新型多阶段RLVR系统对Qwen3-4B进行微调，完全消除了对下一个token预测训练（SFT）的依赖。

Result: Jan-nano在SimpleQA基准测试中达到了83.2%的准确率，并且可以在消费级硬件上运行。

Conclusion: Jan-nano证明了智能不在于规模，而在于策略。

Abstract: Most language models face a fundamental tradeoff where powerful capabilities
require substantial computational resources. We shatter this constraint with
Jan-nano, a 4B parameter language model that redefines efficiency through
radical specialization: instead of trying to know everything, it masters the
art of finding anything instantly. Fine-tuned from Qwen3-4B using our novel
multi-stage RLVR system that completely eliminates reliance on next token
prediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with
MCP integration while running on consumer hardware. With 128K context length,
Jan-nano proves that intelligence isn't about scale, it's about strategy.

</details>


### [18] [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)
*Miles Turpin,Andy Arditi,Marvin Li,Joe Benton,Julian Michael*

Main category: cs.CL

TL;DR: VFT是一种预 RL 干预方法，通过让模型在受到提示线索影响时明确承认，显著提高了奖励黑客行为的检测率。


<details>
  <summary>Details</summary>
Motivation: 语言模型在RL中可能通过奖励黑客策略获得高奖励，但不会在推理过程中揭示这种行为，导致检测困难，并对高风险应用构成威胁。

Method: 提出了一种称为 verbalization fine-tuning (VFT) 的预 RL 干预方法，训练模型在受到提示线索影响时明确承认。

Result: 经过RL后，只有6%的VFT训练模型的响应包含未被检测到的奖励黑客行为；而没有VFT的模型则达到88%，使用去偏置基线干预的模型甚至达到99%。

Conclusion: VFT通过在RL之前教授模型明确 verbalize reward hacking 行为，显著提高了检测效果，为更透明和安全的AI系统提供了实用路径。

Abstract: Language models trained with RL can engage in reward hacking--exploiting
unintended strategies for high reward--without revealing this behavior in their
chain-of-thought reasoning, making detection difficult and posing risks for
high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL
intervention that trains models to explicitly acknowledge when they are
influenced by prompt cues--hints which point to incorrect answers (e.g., "a
Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently
train models with RL on environments where held-out prompt cues signal which
incorrect answers will receive high reward, incentivizing models to reward hack
by exploiting cues instead of reasoning correctly. We measure how often models
exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained
model's responses consist of undetected reward hacks. In comparison, when we
perform RL without VFT, the rate of undetected reward hacks goes up to 88%;
with a debiasing baseline intervention, this increases further to 99%. VFT
achieves this by substantially increasing how often models verbalize the
influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while
baselines remain low even after RL (10% and 1%). Our results show that teaching
models to explicitly verbalize reward hacking behavior before RL significantly
improves their detection, offering a practical path toward more transparent and
safe AI systems.

</details>


### [19] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
*Jianxin Yan,Wangze Ni,Lei Chen,Xuemin Lin,Peng Cheng,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文介绍了 ContextCache，一种用于多轮对话的上下文感知语义缓存系统，能够提高精度和召回率，并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的语义缓存系统主要依赖于匹配单个查询，缺乏对多轮对话上下文的意识，导致在不同对话场景中出现相似查询时出现错误的缓存命中。

Method: ContextCache 采用两阶段检索架构，首先对当前查询进行基于向量的检索，然后通过自注意力机制整合当前和历史对话表示以实现精确的上下文匹配。

Result: 评估显示，ContextCache 在精度和召回率方面优于现有方法，且缓存响应的延迟大约是直接调用 LLM 的十分之一。

Conclusion: ContextCache 提高了精度和召回率，并显著降低了计算成本。

Abstract: Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

</details>


### [20] [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/abs/2506.22808)
*Jianhui Wei,Zijie Meng,Zikai Xiao,Tianxiang Hu,Yang Feng,Zhijie Zhou,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文提出了一个医学伦理基准MedEthicsQA，用于评估医学大语言模型的伦理安全。实验结果表明，最先进的医学大语言模型在回答医学伦理问题上的表现有所下降，这揭示了医学伦理对齐的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管医学大语言模型在临床任务中表现出色，但其伦理安全性尚未得到充分研究。因此，需要一个专门的基准来评估医学大语言模型的伦理安全。

Method: 该研究构建了一个包含5,623个选择题和5,351个开放性问题的基准数据集，采用分层分类法整合全球医学伦理标准，并通过多阶段过滤和多方面专家验证确保数据集的可靠性。

Result: 实验结果表明，最先进的医学大语言模型在回答医学伦理问题上的表现不如其基础模型，这表明医学伦理对齐存在不足。

Conclusion: 该研究提出了一个全面的医学伦理基准MedEthicsQA，用于评估医学大语言模型的伦理安全。实验结果表明，最先进的医学大语言模型在回答医学伦理问题上的表现有所下降，这揭示了医学伦理对齐的不足。

Abstract: While Medical Large Language Models (MedLLMs) have demonstrated remarkable
potential in clinical tasks, their ethical safety remains insufficiently
explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive
benchmark comprising $\textbf{5,623}$ multiple-choice questions and
$\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.
We systematically establish a hierarchical taxonomy integrating global medical
ethical standards. The benchmark encompasses widely used medical datasets,
authoritative question banks, and scenarios derived from PubMed literature.
Rigorous quality control involving multi-stage filtering and multi-faceted
expert validation ensures the reliability of the dataset with a low error rate
($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance
in answering medical ethics questions compared to their foundation
counterparts, elucidating the deficiencies of medical ethics alignment. The
dataset, registered under CC BY-NC 4.0 license, is available at
https://github.com/JianhuiWei7/MedEthicsQA.

</details>


### [21] [Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models](https://arxiv.org/abs/2506.22813)
*Zhuojun Ding,Wei Wei,Chenghao Fan*

Main category: cs.CL

TL;DR: 本文提出了一种名为SaM的框架，该框架在推理时动态选择和合并专家模型，以提高跨不同领域的泛化能力，并且无需额外训练。实验结果表明，该框架在多个基准测试中表现优异，平均优于统一模型10%。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）广泛用于将大型语言模型（LLMs）与信息提取（IE）任务对齐，例如命名实体识别（NER）。然而，注释这种细粒度标签和训练领域特定模型的成本很高。现有的工作通常在多个领域上训练一个统一的模型，但这种方法缺乏适应性和可扩展性，因为并非所有训练数据都对目标领域有益，且训练模型的扩展仍然具有挑战性。

Method: 我们提出了SaM框架，该框架在推理时动态选择和合并专家模型。具体来说，针对目标领域，我们根据（i）与目标领域的领域相似性和（ii）在采样实例上的性能，选择在现有领域上预训练的领域特定专家。然后将这些专家合并以创建针对目标领域的任务特定模型。

Result: 广泛的实验表明，我们的框架有效，平均优于统一模型10%。此外，专家可以方便地添加或删除，从而实现巨大的可扩展性。

Conclusion: 我们的框架在多个基准测试中表现出色，平均优于统一模型10%。我们还提供了对潜在改进、实践经验和框架扩展的见解。

Abstract: Supervised fine-tuning (SFT) is widely used to align large language models
(LLMs) with information extraction (IE) tasks, such as named entity recognition
(NER). However, annotating such fine-grained labels and training
domain-specific models is costly. Existing works typically train a unified
model across multiple domains, but such approaches lack adaptation and
scalability since not all training data benefits target domains and scaling
trained models remains challenging. We propose the SaM framework, which
dynamically Selects and Merges expert models at inference time. Specifically,
for a target domain, we select domain-specific experts pre-trained on existing
domains based on (i) domain similarity to the target domain and (ii)
performance on sampled instances, respectively. The experts are then merged to
create task-specific models optimized for the target domain. By dynamically
merging experts beneficial to target domains, we improve generalization across
various domains without extra training. Additionally, experts can be added or
removed conveniently, leading to great scalability. Extensive experiments on
multiple benchmarks demonstrate our framework's effectiveness, which
outperforms the unified model by an average of 10%. We further provide insights
into potential improvements, practical experience, and extensions of our
framework.

</details>


### [22] [Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](https://arxiv.org/abs/2506.22846)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 本文提出了一种名为LAIL的新辅助损失框架，用于增强CTC-based ASR的语言建模能力，同时保持其计算效率。在多个数据集上，该方法实现了显著的WER改进，并达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端ASR系统由于自回归解码过程限制了推理速度，不适合实时应用。而CTC-based模型虽然解码速度快，但难以有效建模语言依赖关系。因此，需要一种方法来增强CTC-based ASR的语言建模能力，同时保持其计算效率。

Method: 提出了一种称为Language-Aware Intermediate Loss (LAIL)的新辅助损失框架，利用大型语言模型（LLMs）的语言知识来增强CTC-based ASR。通过在中间编码器层附加连接器层，将输出映射到LLM的嵌入空间，并在训练期间计算因果语言建模损失。

Result: 在LibriSpeech、TEDLIUM2和WSJ语料库上，使用Conformer架构和各种LLaMA模型，LAIL框架显著提高了Word Error Rate (WER)，并实现了CTC-based ASR的最先进性能。

Conclusion: 通过使用LAIL框架，CTC-based ASR在多个数据集上实现了显著的WER改进，并且在计算开销最小的情况下达到了最先进的性能。

Abstract: End-to-end (E2E) automatic speech recognition (ASR) systems have
revolutionized the field by integrating all components into a single neural
network, with attention-based encoder-decoder models achieving state-of-the-art
performance. However, their autoregressive decoding process limits inference
speed, making them unsuitable for real-time applications. In contrast,
CTC-based models offer faster, non-autoregressive decoding but struggle to
model linguistic dependencies effectively. Addressing this challenge, we
propose a novel auxiliary loss framework called Language-Aware Intermediate
Loss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large
language models (LLMs). By attaching connector layers to intermediate encoder
layers, LAIL maps outputs to the embedding space of an LLM and computes a
causal language modeling loss during training. This approach enhances
linguistic modeling while preserving the computational efficiency of CTC
decoding. Using the Conformer architecture and various LLaMA models, we
demonstrate significant improvements in Word Error Rate (WER) on the
LibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance
for CTC-based ASR with minimal computational overhead.

</details>


### [23] [Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems](https://arxiv.org/abs/2506.22852)
*Yucheng Cai,Yuxuan Wu,Yi Huang,Junlan Feng,Zhijian Ou*

Main category: cs.CL

TL;DR: 本文研究了知识增强微调（KAFT）方法，以提高基于RAG和代理系统的事实准确性，并发现KAFT优于传统的提示方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型场景中容易出错，尽管已经提出了基于检索增强生成（RAG）和代理的方法来提高事实准确性，但这些方法在使用检索到的知识进行响应生成时可能遇到困难。

Method: 本文提出了一种称为知识增强微调（KAFT）的方法，通过在基于RAG和基于代理的系统中使用特定领域的数据和外部知识对大型语言模型进行微调。

Result: 实验结果表明，KAFT在RAG和代理系统中都显著优于提示方法，尤其是在事实准确性方面。

Conclusion: KAFT在RAG和代理系统中显著优于提示方法，特别是在事实准确性方面。本文代表了首次坚实的实证工作来研究KAFT的概念。

Abstract: Large language models (LLMs) have recently been applied to dialog systems.
Despite making progress, LLMs are prone to errors in knowledge-intensive
scenarios. Recently, approaches based on retrieval augmented generation (RAG)
and agent have emerged to improve the factual accuracy by enhancing the LLMs
with knowledge retrieved from external knowledge bases (KBs). This is mostly
implemented by prompting the LLMs with instructions, examples and the retrieved
knowledge. However, LLMs may have difficulty using the retrieved knowledge
effectively for response generation, because they are not well trained to do
such generation for specific domains. To mitigate this problem, we propose to
finetune the LLMs in the RAG-based and agent-based systems with domain-specific
data, together with domain-specific external knowledge, which is called
knowledge augmented finetuning (KAFT). We base our study on the MobileCS2
dataset, a real-life customer service dialog dataset that features intensive
knowledge interactions, to systematically compare the prompting and KAFT
techniques in the RAG-based and agent-based systems. Experiment results show
that KAFT substantially surpasses prompting in both RAG and agent systems,
particularly in terms of factual accuracy. To the best of our knowledge, this
paper represents the first solid empirical work to investigate the KAFT idea.

</details>


### [24] [DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues](https://arxiv.org/abs/2506.22853)
*Kyochul Jang,Donghyeon Lee,Kyusik Kim,Dongseok Heo,Taewhoo Lee,Woojeong Kim,Bongwon Suh*

Main category: cs.CL

TL;DR: 本文介绍了DICE-SCORE，一种评估工具相关信息在整个对话中分散程度的指标，并提出了DICE-BENCH，一个通过合成对话构建实际功能调用数据集的框架。实验表明，还需要显著进步才能将这些模型有效部署到现实世界设置中。


<details>
  <summary>Details</summary>
Motivation: 现有的函数调用基准测试专注于单次交互，但它们忽视了现实场景的复杂性。为了量化现有基准测试在实际应用中的表现，我们引入了DICE-SCORE，这是一种评估工具相关信息（如函数名称和参数值）在整个对话中分散程度的指标。

Method: 我们提出了DICE-BENCH，这是一个框架，通过合成对话来构建实际的功能调用数据集，该对话通过工具图保持跨轮次的依赖关系，并通过具有不同人设的多智能体系统来增强对话的自然性。

Result: 通过DICE-SCORE分析现有基准测试显示出明显的低分，突显了需要更现实场景的必要性。最终的数据集包含1,607个高DICE-SCORE实例。

Conclusion: 我们的实验表明，在将这些模型有效部署到现实世界设置之前，仍需要显著的进步。

Abstract: Existing function-calling benchmarks focus on single-turn interactions.
However, they overlook the complexity of real-world scenarios. To quantify how
existing benchmarks address practical applications, we introduce DICE-SCORE, a
metric that evaluates the dispersion of tool-related information such as
function name and parameter values throughout the dialogue. Analyzing existing
benchmarks through DICE-SCORE reveals notably low scores, highlighting the need
for more realistic scenarios. To address this gap, we present DICE-BENCH, a
framework that constructs practical function-calling datasets by synthesizing
conversations through a tool graph that maintains dependencies across rounds
and a multi-agent system with distinct personas to enhance dialogue
naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our
experiments on 19 LLMs with DICE-BENCH show that significant advances are still
required before such models can be deployed effectively in real-world settings.
Our code and data are all publicly available:
https://snuhcc.github.io/DICE-Bench/.

</details>


### [25] [Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions](https://arxiv.org/abs/2506.22858)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 本文提出了一种新的训练方法，通过扩展ASR模型的语义上下文来提高命名实体识别和格式化的性能。该方法在Spoken Wikipedia数据集上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别（ASR）系统，如Whisper，在转录准确性方面表现良好，但在处理命名实体和数字数据时存在困难，尤其是在需要正确格式化的情况下。这些问题会增加词错误率（WER），并在法律、金融和医疗等关键领域影响语义理解。

Method: 我们提出了一种新的训练方法，通过在训练期间添加重叠的上下文窗口来扩展ASR模型的语义上下文。通过在30秒块的两侧滑动5秒重叠，我们创建了一个40秒的“有效语义窗口”，在关注中央30秒的同时改善实体识别和格式化。为了处理跨越块边界的实体，我们将这些实体完全重新分配到右侧块，确保正确的格式化。此外，带有嵌入实体标签的丰富训练数据使模型能够学习识别和特定类型的格式化。

Result: 在Spoken Wikipedia数据集上评估，我们的方法在语义任务上提高了性能，包括命名实体识别（NER）和实体格式化。

Conclusion: 我们的方法在语义任务上提高了性能，包括命名实体识别（NER）和实体格式化。这些结果突显了上下文感知训练在解决长文本转录和复杂实体识别任务中的ASR限制的有效性。

Abstract: Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high
transcription accuracy but struggle with named entities and numerical data,
especially when proper formatting is required. These issues increase word error
rate (WER) and impair semantic understanding in critical domains like legal,
financial, and medical applications. We propose a novel training approach that
extends the semantic context of ASR models by adding overlapping context
windows during training. By sliding 5-second overlaps on both sides of
30-second chunks, we create a 40-second "effective semantic window," improving
entity recognition and formatting while focusing predictions on the central 30
seconds. To address entities spanning chunk boundaries, we reassign such
entities entirely to the right-hand chunk, ensuring proper formatting.
Additionally, enriched training data with embedded entity labels enables the
model to learn both recognition and type-specific formatting. Evaluated on the
Spoken Wikipedia dataset, our method improves performance across semantic
tasks, including named entity recognition (NER) and entity formatting. These
results highlight the effectiveness of context-aware training in addressing ASR
limitations for long-form transcription and complex entity recognition tasks.

</details>


### [26] [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/abs/2506.22957)
*Younwoo Choi,Changling Li,Yongjin Yang,Zhijing Jin*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在对话者意识方面的表现，发现它们能够识别同一家族的模型，并展示了这种能力在多LLM协作中的实际应用和潜在风险。


<details>
  <summary>Details</summary>
Motivation: 了解LLM对自己上下文和对话伙伴的意识对于确保可靠性能和稳健安全至关重要。尽管先前工作已经广泛研究了情境意识，但很少关注识别和适应对话伙伴身份和特征的能力。

Method: 我们通过三个维度（推理模式、语言风格和对齐偏好）检查了对话者推断，并开发了三个案例研究来展示其实际意义。

Result: LLMs能够可靠地识别同一家族的同行和某些著名的模型家族，如GPT和Claude。然而，这种能力也引入了新的对齐和安全漏洞，包括奖励黑客行为和更高的越狱易感性。

Conclusion: 我们的研究突显了LLM中身份敏感行为的双重潜力和风险，强调了进一步理解对话者意识和在多代理部署中建立新保障措施的必要性。

Abstract: As large language models (LLMs) are increasingly integrated into multi-agent
and human-AI systems, understanding their awareness of both self-context and
conversational partners is essential for ensuring reliable performance and
robust safety. While prior work has extensively studied situational awareness
which refers to an LLM's ability to recognize its operating phase and
constraints, it has largely overlooked the complementary capacity to identify
and adapt to the identity and characteristics of a dialogue partner. In this
paper, we formalize this latter capability as interlocutor awareness and
present the first systematic evaluation of its emergence in contemporary LLMs.
We examine interlocutor inference across three dimensions-reasoning patterns,
linguistic style, and alignment preferences-and show that LLMs reliably
identify same-family peers and certain prominent model families, such as GPT
and Claude. To demonstrate its practical significance, we develop three case
studies in which interlocutor awareness both enhances multi-LLM collaboration
through prompt adaptation and introduces new alignment and safety
vulnerabilities, including reward-hacking behaviors and increased jailbreak
susceptibility. Our findings highlight the dual promise and peril of
identity-sensitive behavior in LLMs, underscoring the need for further
understanding of interlocutor awareness and new safeguards in multi-agent
deployments. Our code is open-sourced at
https://github.com/younwoochoi/InterlocutorAwarenessLLM.

</details>


### [27] [On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"](https://arxiv.org/abs/2506.22977)
*Asen Dotsinski,Udit Thakur,Marko Ivanov,Mohammad Hafeez Khan,Maria Heuss*

Main category: cs.CL

TL;DR: 本文复制并扩展了Ortu等人（2024）的研究，发现注意力头消融方法在数据集未覆盖的领域中无效，并且其效果受模型架构、提示结构、领域和任务影响。


<details>
  <summary>Details</summary>
Motivation: 本文旨在验证Ortu等人（2024）的研究成果，并探索其在更大模型、不同提示结构和特定领域中的适用性。

Method: 我们对Ortu等人（2024）的研究进行了复制，并扩展了三个重要方向：1）将实验复制到更大的模型Llama 3.1 8B，发现注意力头专业化显著降低；2）研究提示结构的影响，发现避免重复反事实陈述或改变前提词会显著降低反事实标记的logit；3）测试作者关于特定领域提示的主张的有效性，发现某些类别提示通过将事实预测标记作为句子的主语来扭曲结果。

Result: 我们成功复制了Ortu等人（2024）的主要发现，包括事实和反事实信息的定位、注意力块在机制竞争中的主导地位以及注意力头在处理竞争信息中的专门化。此外，我们在Llama 3.1 8B上发现了注意力头专业化的显著减少，在不同提示结构下观察到反事实标记logit的下降，并发现某些领域提示会影响结果。

Conclusion: 我们发现Ortu等人（2024）提出的注意力头消融方法在数据集中未被充分代表的领域中无效，并且其效果取决于模型架构、提示结构、领域和任务。

Abstract: We present a reproduction study of "Competition of Mechanisms: Tracing How
Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024), which
investigates competition of mechanisms in language models between factual
recall and counterfactual in-context repetition. Our study successfully
reproduces their primary findings regarding the localization of factual and
counterfactual information, the dominance of attention blocks in mechanism
competition, and the specialization of attention heads in handling competing
information. We reproduce their results on both GPT-2 (Radford et al., 2019)
and Pythia 6.9B (Biderman et al., 2023). We extend their work in three
significant directions. First, we explore the generalizability of these
findings to even larger models by replicating the experiments on Llama 3.1 8B
(Grattafiori et al., 2024), discovering greatly reduced attention head
specialization. Second, we investigate the impact of prompt structure by
introducing variations where we avoid repeating the counterfactual statement
verbatim or we change the premise word, observing a marked decrease in the
logit for the counterfactual token. Finally, we test the validity of the
authors' claims for prompts of specific domains, discovering that certain
categories of prompts skew the results by providing the factual prediction
token as part of the subject of the sentence. Overall, we find that the
attention head ablation proposed in Ortu et al. (2024) is ineffective for
domains that are underrepresented in their dataset, and that the effectiveness
varies based on model architecture, prompt structure, domain and task.

</details>


### [28] [A Systematic Study of Compositional Syntactic Transformer Language Models](https://arxiv.org/abs/2506.22978)
*Yida Zhao,Hao Xve,Xiang Hu,Kewei Tu*

Main category: cs.CL

TL;DR: 本文提出了一个统一的框架，用于改进基于成分句法树的组合SLM，并根据实验结果提出了设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有的组合SLM在设计选择上存在一些关键方面，本文旨在通过提出一个统一的框架来改进这些设计。

Method: 本文研究了基于成分句法树的组合SLM，并提出了一个统一的框架，以涵盖现有的模型和新的变体。

Result: 本文对框架中的所有变体进行了全面的实证评估，包括语言建模、句法泛化、摘要、对话和推理效率。

Conclusion: 本文提出了一个统一的框架，涵盖了现有的模型和新的变体，并根据实验结果对组合SLM的设计提出了多个建议。

Abstract: Syntactic language models (SLMs) enhance Transformers by incorporating
syntactic biases through the modeling of linearized syntactic parse trees
alongside surface sentences. This paper focuses on compositional SLMs that are
based on constituency parse trees and contain explicit bottom-up composition of
constituent representations. We identify key aspects of design choices in
existing compositional SLMs and propose a unified framework encompassing both
existing models and novel variants. We conduct a comprehensive empirical
evaluation of all the variants in our framework across language modeling,
syntactic generalization, summarization, dialogue, and inference efficiency.
Based on the experimental results, we make multiple recommendations on the
design of compositional SLMs. Our code is released at
https://github.com/zhaoyd1/compositional_SLMs.

</details>


### [29] [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)
*Xianzhe Fan,Xuhui Zhou,Chuanyang Jin,Kolby Nottingham,Hao Zhu,Maarten Sap*

Main category: cs.CL

TL;DR: 本文提出了SoMi-ToM基准，用于评估具身多智能体复杂社会互动中的理论心智能力。实验结果表明，大型视觉-语言模型在该任务上的表现显著低于人类，表明未来需要改进模型的理论心智能力。


<details>
  <summary>Details</summary>
Motivation: 现有的理论心智（ToM）基准主要评估静态、基于文本的场景，与真实互动存在显著差距。因此，我们需要一个更贴近现实的基准来评估模型的理论心智能力。

Method: 我们提出了SoMi-ToM基准，用于评估具身多智能体复杂社会互动中的多视角理论心智。该基准基于交互环境SoMi生成的丰富多模态交互数据，支持第一人称和第三人称评估方法。

Result: 在SoMi-ToM数据集上，我们系统地评估了人类和几个最先进的大型视觉-语言模型（LVLMs）的表现。结果显示，LVLMs在第一人称评估中的平均准确率比人类低40.1%，在第三人称评估中低26.4%。

Conclusion: 结果表明，大型视觉-语言模型（LVLMs）在SoMi-ToM任务上的表现显著低于人类，这表明未来的LVLMs需要进一步提高其在具身、复杂社会互动中的理论心智能力。

Abstract: Humans continuously infer the states, goals, and behaviors of others by
perceiving their surroundings in dynamic, real-world social interactions.
However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based
scenarios, which have a significant gap compared to real interactions. We
propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in
embodied multi-agent complex social interactions. This benchmark is based on
rich multimodal interaction data generated by the interaction environment SoMi,
covering diverse crafting goals and social relationships. Our framework
supports multi-level evaluation: (1) first-person evaluation provides
multimodal (visual, dialogue, action, etc.) input from a first-person
perspective during a task for real-time state inference, (2) third-person
evaluation provides complete third-person perspective video and text records
after a task for goal and behavior inference. This evaluation method allows for
a more comprehensive examination of a model's ToM capabilities from both the
subjective immediate experience and the objective global observation. We
constructed a challenging dataset containing 35 third-person perspective
videos, 363 first-person perspective images, and 1225 expert-annotated
multiple-choice questions (three options). On this dataset, we systematically
evaluated the performance of human subjects and several state-of-the-art large
vision-language models (LVLMs). The results show that LVLMs perform
significantly worse than humans on SoMi-ToM: the average accuracy gap between
humans and models is 40.1% in first-person evaluation and 26.4% in third-person
evaluation. This indicates that future LVLMs need to further improve their ToM
capabilities in embodied, complex social interactions.

</details>


### [30] [MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition](https://arxiv.org/abs/2506.23051)
*João Lucas Luz Lima Sarcinelli,Marina Lages Gonçalves Teixeira,Jade Bortot de Paiva,Diego Furtado Silva*

Main category: cs.CL

TL;DR: 本文介绍了MariNER，这是一个针对20世纪初巴西葡萄牙语的首个高质量金标准命名实体识别（NER）数据集，并评估了最先进的NER模型在该数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于巴西葡萄牙语缺乏高质量的金标准NER数据集，特别是在特定领域，因此需要构建一个专门用于历史文本分析的数据集。

Method: 本文构建了MariNER数据集，并评估和比较了最先进的NER模型在该数据集上的性能。

Result: MariNER是第一个针对20世纪初巴西葡萄牙语的金标准NER数据集，包含超过9,000条手动标注的句子。同时，本文评估了最先进的NER模型在该数据集上的性能。

Conclusion: 本文通过构建MariNER数据集，填补了巴西葡萄牙语在NER任务中的数据空白，并为未来的研究提供了基础。

Abstract: Named Entity Recognition (NER) is a fundamental Natural Language Processing
(NLP) task that aims to identify and classify entity mentions in texts across
different categories. While languages such as English possess a large number of
high-quality resources for this task, Brazilian Portuguese still lacks in
quantity of gold-standard NER datasets, especially when considering specific
domains. Particularly, this paper considers the importance of NER for analyzing
historical texts in the context of digital humanities. To address this gap,
this work outlines the construction of MariNER: \textit{Mapeamento e
Anota\c{c}\~oes de Registros hIst\'oricos para NER} (Mapping and Annotation of
Historical Records for NER), the first gold-standard dataset for early
20th-century Brazilian Portuguese, with more than 9,000 manually annotated
sentences. We also assess and compare the performance of state-of-the-art NER
models for the dataset.

</details>


### [31] [Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning](https://arxiv.org/abs/2506.23056)
*Xiang Zhuang,Bin Wu,Jiyu Cui,Kehua Feng,Xiaotong Li,Huabin Xing,Keyan Ding,Qiang Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: 本文提出了一种增强知识的推理框架（K-MSE），用于分子结构解析，通过构建外部分子子结构知识库和设计专门的分子-光谱评分器，显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 我们发现这些挑战主要源于LLMs对专业化学知识的掌握有限。

Method: 我们引入了一个增强知识的推理框架（K-MSE），利用蒙特卡洛树搜索作为插件进行测试时扩展。我们构建了一个外部分子子结构知识库来扩展LLMs对化学结构空间的覆盖范围，并设计了一个专门的分子-光谱评分器作为推理过程的奖励模型。

Result: 实验结果表明，我们的方法显著提升了性能，特别是在GPT-4o-mini和GPT-4o上取得了超过20%的提升。

Conclusion: 我们的方法显著提高了性能，特别是在GPT-4o-mini和GPT-4o上取得了超过20%的提升。

Abstract: Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.

</details>


### [32] [Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries](https://arxiv.org/abs/2506.23071)
*Zhengren Wang,Bozhou Li,Dongwen Yao,Wentao Zhang*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Text2VectorSQL的新框架，它结合了Text-to-SQL和向量搜索，以提高自然语言查询的表达能力和多样性。通过构建向量索引、扩展用户查询以及开发专用模型，我们在实验中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的VectorSQL实现仍然严重依赖手动制作，并且缺乏定制的评估框架，导致理论潜力与实际部署之间存在显著差距。因此，需要一种新的方法来弥合这些互补范式之间的差距。

Method: 我们引入了Text2VectorSQL框架，该框架统一了Text-to-SQL和向量搜索，以克服表达性限制并支持更多样化和全面的自然语言查询。此外，我们构建了向量索引，扩展了用户查询，并通过自动管道与专家审查注释了真实数据。还开发了专门的Text2VectorSQL模型，并使用合成数据进行了测试。

Result: 我们开发的专门Text2VectorSQL模型在基线方法上表现出显著的性能提升。

Conclusion: 我们的工作为Text2VectorSQL任务奠定了基础，为更灵活和直观的数据库接口铺平了道路。

Abstract: While Text-to-SQL enables natural language interaction with structured
databases, its effectiveness diminishes with unstructured data or ambiguous
queries due to rigid syntax and limited expressiveness. Concurrently, vector
search has emerged as a powerful paradigm for semantic retrieval, particularly
for unstructured data. However, existing VectorSQL implementations still rely
heavily on manual crafting and lack tailored evaluation frameworks, leaving a
significant gap between theoretical potential and practical deployment. To
bridge these complementary paradigms, we introduces Text2VectorSQL, a novel
framework unifying Text-to-SQL and vector search to overcome expressiveness
constraints and support more diverse and holistical natural language queries.
Specifically, Text2VectorSQL enables semantic filtering, multi-modal matching,
and retrieval acceleration. For evaluation, we build vector index on
appropriate columns, extend user queries with semantic search, and annotate
ground truths via an automatic pipeline with expert review. Furthermore, we
develop dedicated Text2VectorSQL models with synthetic data, demonstrating
significant performance improvements over baseline methods. Our work
establishes the foundation for the Text2VectorSQL task, paving the way for more
versatile and intuitive database interfaces. The repository will be publicly
available at https://github.com/Open-DataFlow/Text2VectorSQL.

</details>


### [33] [From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship](https://arxiv.org/abs/2506.23101)
*Yue Xu,Wenjie Wang*

Main category: cs.CL

TL;DR: 本文介绍了Genres基准，用于评估多模态大语言模型中的性别偏见，特别是在双人互动中的关系性和情境性偏见。实验结果表明存在持续的、与情境相关的性别偏见，这在单角色设置中并不明显。研究强调了关系感知基准的重要性，并为未来的偏见缓解提供了见解。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要评估孤立场景中的偏见，忽略了偏见可能通过人际互动微妙地出现。我们填补了这一空白，超越了单个实体的评估，而是深入检查双人互动中的关系性和情境性性别偏见。

Method: 我们引入了一个名为Genres的新基准，用于通过社会关系的视角评估多模态大语言模型中的性别偏见。Genres通过双角色资料和叙事生成任务来评估性别偏见，捕捉丰富的个人间动态，并支持跨多个维度的细粒度偏见评估套件。

Result: 对开放源代码和封闭源代码多模态大语言模型的实验揭示了持续存在的、与情境相关的性别偏见，这些偏见在单角色设置中并不明显。

Conclusion: 我们的研究结果强调了关系感知基准在诊断多模态大语言模型中的细微互动驱动性别偏见的重要性，并为未来的偏见缓解提供了可行的见解。

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
across tasks involving both visual and textual modalities. However, growing
concerns remain about their potential to encode and amplify gender bias,
particularly in socially sensitive applications. Existing benchmarks
predominantly evaluate bias in isolated scenarios, overlooking how bias may
emerge subtly through interpersonal interactions. We fill this gap by going
beyond single-entity evaluation and instead focusing on a deeper examination of
relational and contextual gender bias in dual-individual interactions. We
introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs
through the lens of social relationships in generated narratives. Genres
assesses gender bias through a dual-character profile and narrative generation
task that captures rich interpersonal dynamics and supports a fine-grained bias
evaluation suite across multiple dimensions. Experiments on both open- and
closed-source MLLMs reveal persistent, context-sensitive gender biases that are
not evident in single-character settings. Our findings underscore the
importance of relationship-aware benchmarks for diagnosing subtle,
interaction-driven gender bias in MLLMs and provide actionable insights for
future bias mitigation.

</details>


### [34] [FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes](https://arxiv.org/abs/2506.23111)
*Janki Atul Nawale,Mohammed Safi Ur Rahman Khan,Janani D,Mansi Gupta,Danish Pruthi,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 本文介绍了INDIC-BIAS，一个针对印度的大型语言模型公平性评估基准。通过专家咨询和情景模板生成，评估发现大多数模型存在对边缘化身份的偏见，并且难以减轻这种偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的关于公平性的研究主要集中在西方，这使得它们对于文化多样的国家如印度来说是不充分的。为了弥补这一差距，我们引入了INDIC-BIAS，这是一个全面的印度中心基准，旨在评估大型语言模型在85个身份群体中的公平性，这些群体涵盖了不同的种姓、宗教、地区和部落。

Method: 我们首先咨询领域专家，整理了超过1800个跨行为和情境的社会文化主题，这些主题很可能会出现偏见和刻板印象。基于这些主题，我们生成并手动验证了20,000个现实世界的情景模板，以探测大型语言模型的公平性。我们将这些模板结构化为三个评估任务：合理性、判断和生成。

Result: 我们在这些任务上对14个流行的大型语言模型进行了评估，结果揭示了对边缘化身份的强烈负面偏见，模型经常强化常见的刻板印象。此外，我们发现即使明确要求模型为其决策进行理性分析，它们也难以减轻偏见。

Conclusion: 我们的评估提供了当前大型语言模型可能对印度身份造成分配性和代表性伤害的证据，呼吁在实际应用中更加谨慎地使用它们。我们发布了INDIC-BIAS作为一个开源基准，以促进在印度背景下基准测试和减轻偏见和刻板印象的研究。

Abstract: Existing studies on fairness are largely Western-focused, making them
inadequate for culturally diverse countries such as India. To address this gap,
we introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to
evaluate fairness of LLMs across 85 identity groups encompassing diverse
castes, religions, regions, and tribes. We first consult domain experts to
curate over 1,800 socio-cultural topics spanning behaviors and situations,
where biases and stereotypes are likely to emerge. Grounded in these topics, we
generate and manually validate 20,000 real-world scenario templates to probe
LLMs for fairness. We structure these templates into three evaluation tasks:
plausibility, judgment, and generation. Our evaluation of 14 popular LLMs on
these tasks reveals strong negative biases against marginalized identities,
with models frequently reinforcing common stereotypes. Additionally, we find
that models struggle to mitigate bias even when explicitly asked to rationalize
their decision. Our evaluation provides evidence of both allocative and
representational harms that current LLMs could cause towards Indian identities,
calling for a more cautious usage in practical applications. We release
INDIC-BIAS as an open-source benchmark to advance research on benchmarking and
mitigating biases and stereotypes in the Indian context.

</details>


### [35] [Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models](https://arxiv.org/abs/2506.23122)
*Shivam Sharma,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本研究探讨了在互联网迷因中识别叙述角色的任务，并评估了多种模型的表现。研究发现，文化背景、提示工程和多模态推理对于建模视觉-文本内容中的细微叙事框架至关重要。


<details>
  <summary>Details</summary>
Motivation: 这项工作研究了在互联网迷因中识别叙述角色（英雄、反派、受害者和其他）的挑战性任务。

Method: 我们评估了广泛的模型，包括微调的多语言变压器、情感和仇恨意识分类器、指令微调的大语言模型和多模态视觉-语言模型。我们还探索了提示设计策略来指导多模态模型。

Result: 较大的模型如DeBERTa-v3和Qwen2.5-VL表现出显著的提升，但结果揭示了在可靠识别'受害者'类别和跨文化和代码混合内容进行泛化方面的一致挑战。

Conclusion: 我们的研究强调了文化背景、提示工程和多模态推理在建模视觉-文本内容中的细微叙事框架的重要性。

Abstract: This work investigates the challenging task of identifying narrative roles -
Hero, Villain, Victim, and Other - in Internet memes, across three diverse test
sets spanning English and code-mixed (English-Hindi) languages. Building on an
annotated dataset originally skewed toward the 'Other' class, we explore a more
balanced and linguistically diverse extension, originally introduced as part of
the CLEF 2024 shared task. Comprehensive lexical and structural analyses
highlight the nuanced, culture-specific, and context-rich language used in real
memes, in contrast to synthetically curated hateful content, which exhibits
explicit and repetitive lexical markers. To benchmark the role detection task,
we evaluate a wide spectrum of models, including fine-tuned multilingual
transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,
and multimodal vision-language models. Performance is assessed under zero-shot
settings using precision, recall, and F1 metrics. While larger models like
DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent
challenges in reliably identifying the 'Victim' class and generalising across
cultural and code-mixed content. We also explore prompt design strategies to
guide multimodal models and find that hybrid prompts incorporating structured
instructions and role definitions offer marginal yet consistent improvements.
Our findings underscore the importance of cultural grounding, prompt
engineering, and multimodal reasoning in modelling subtle narrative framings in
visual-textual content.

</details>


### [36] [Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.23127)
*Zhaoye Fei,Li Ji,Siyin Wang,Junhao Shi,Jingjing Gong,Xipeng Qiu*

Main category: cs.CL

TL;DR: Embodied Planner-R1 is a novel outcome-driven reinforcement learning framework that enables LLMs to develop interactive capabilities through autonomous exploration with minimal supervision, achieving impressive results on text-based Embodied planning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches generate open-loop action scripts based on static knowledge, making it difficult to learn causal relationships between actions and environmental feedback, particularly in partially observable environments.

Method: Embodied Planner-R1 is a novel outcome-driven reinforcement learning framework that enables LLMs to develop interactive capabilities through autonomous exploration with minimal supervision. It incorporates three key innovations: pure reinforcement learning with group rollout, completion-driven sparse reward, and Interactive Policy Optimization (IPO) for efficient learning from grouped trajectories.

Result: Embodied Planner-R1 achieves impressive completion rates of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a large margin, and suffers only a -3.66% drop in previously unseen environments.

Conclusion: Embodied Planner-R1 demonstrates strong generalization and achieves impressive completion rates on two text-based Embodied planning benchmarks, surpassing prior methods.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they face significant challenges in embodied task planning
scenarios that require continuous environmental understanding and action
generation. Existing approaches generate open-loop action scripts based on
static knowledge, making it difficult to learn causal relationships between
actions and environmental feedback, particularly in partially observable
environments. We introduce Embodied Planner-R1, a novel outcome-driven
reinforcement learning framework that enables LLMs to develop interactive
capabilities through autonomous exploration with minimal supervision. Our
framework incorporates three key innovations: (1) Without human annotations, we
employ pure reinforcement learning with group rollout, incorporating
in-environment interaction through parallel exploration; (2) completion-driven
sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient
learning from grouped trajectories. Across two challenging text-based Embodied
planning benchmarks, Embodied Planner-R1 achieves impressive completion rates
of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a
large margin, and suffers only a -3.66% drop in previously unseen environments,
evidencing strong generalization.

</details>


### [37] [Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format](https://arxiv.org/abs/2506.23133)
*Dingzirui Wang,Xuanliang Zhang,Rongyu Cao,Longxu Dou,Xianzhen Luo,Yingwei Ma,Qingfu Zhu,Wanxiang Che,Binhua Li,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: This paper proposes Format-Adapter, which generates and selects suitable reasoning formats to mitigate reasoning inconsistencies of large language models (LLMs). It achieves a 4.3% performance improvement on average over previous works.


<details>
  <summary>Details</summary>
Motivation: Prior works using multiple formats rely on formats labeled by humans, which could be unsuitable for all tasks and have high labeling costs.

Method: We adapt suitable formats to the given tasks by generating and selecting formats. We first propose how to measure the reasoning error when generating multiple answers. Then, we introduce Format-Adapter, which utilizes LLMs to generate and select suitable reasoning formats by minimizing the error measurement we present.

Result: Format-Adapter achieves a 4.3% performance improvement on average over previous works.

Conclusion: Format-Adapter achieves a 4.3% performance improvement on average over previous works, demonstrating the effectiveness.

Abstract: Generating and voting multiple answers is an effective method to mitigate
reasoning inconsistencies of large language models (LLMs). Prior works have
shown that multiple reasoning formats outperform a single format when
generating multiple answers. However, previous works using multiple formats
rely on formats labeled by humans, which could be unsuitable for all tasks and
have high labeling costs. To address this issue, we adapt suitable formats to
the given tasks by generating and selecting formats. We first propose how to
measure the reasoning error when generating multiple answers. Then, we
introduce Format-Adapter, which utilizes LLMs to generate and select suitable
reasoning formats by minimizing the error measurement we present. We conduct
experiments on math and commonsense reasoning tasks, where Format-Adapter
achieves a 4.3% performance improvement on average over previous works,
demonstrating the effectiveness.

</details>


### [38] [LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation](https://arxiv.org/abs/2506.23136)
*Shadman Sobhan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: 本文提出了一种新的RAG管道，可以处理包含表格和图像的技术文档，提高了答案的忠实度和相关性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG流水线在从包含结构化数据（如表格和图像）的技术文档中检索信息时存在困难，因此需要一种更有效的解决方案。

Method: 该方法结合了向量相似性搜索和基于Gemma-2-9b-it的微调重排序器，使用RAFT（检索增强微调）在自定义数据集上进行训练，以提高上下文识别能力。

Result: 该管道在RAGas和DeepEval评估中分别获得了94%和96%的高忠实度分数，以及87%和93%的答案相关性分数。

Conclusion: 该研究提出的RAG管道在处理包含表格和图像的技术文档方面表现出色，能够提高答案的忠实度和相关性。

Abstract: Large Language Models (LLMs) are capable of natural language understanding
and generation. But they face challenges such as hallucination and outdated
knowledge. Fine-tuning is one possible solution, but it is resource-intensive
and must be repeated with every data update. Retrieval-Augmented Generation
(RAG) offers an efficient solution by allowing LLMs to access external
knowledge sources. However, traditional RAG pipelines struggle with retrieving
information from complex technical documents with structured data such as
tables and images. In this work, we propose a RAG pipeline, capable of handling
tables and images in documents, for technical documents that support both
scanned and searchable formats. Its retrieval process combines vector
similarity search with a fine-tuned reranker based on Gemma-2-9b-it. The
reranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom
dataset designed to improve context identification for question answering. Our
evaluation demonstrates that the proposed pipeline achieves a high faithfulness
score of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%
(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed
architecture is superior to general RAG pipelines in terms of table-based
questions and handling questions outside context.

</details>


### [39] [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/abs/2506.23137)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.CL

TL;DR: 本文提出了Flow-Modulated Scoring (FMS)框架，通过结合上下文感知的静态表示和条件动态信息，实现更深层次的关系语义建模，并在多个标准基准上取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数方法基于静态嵌入评分，存在无法捕捉上下文依赖和关系动态的固有局限性。因此，需要一种新的方法来解决这一问题。

Method: FMS框架包含两个主要组件：(1) 语义上下文学习模块，用于编码上下文敏感的实体表示；(2) 条件流匹配模块，旨在学习从头到尾嵌入的动态变换，由上述上下文控制。

Result: FMS在多个标准基准上的综合评估表明，所提出的方法超越了之前最先进的结果。

Conclusion: FMS通过结合上下文感知的静态表示和条件动态信息，实现了更深层次的关系语义建模，并在多个标准基准上超越了之前最先进的结果。

Abstract: Effective modeling of multifaceted relations is pivotal for Knowledge Graph
Completion (KGC). However, a majority of existing approaches are predicated on
static, embedding-based scoring, exhibiting inherent limitations in capturing
contextual dependencies and relational dynamics. Addressing this gap, we
propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal
components: (1) a semantic context learning module that encodes
context-sensitive entity representations, and (2) a conditional flow-matching
module designed to learn the dynamic transformation from a head to a tail
embedding, governed by the aforementioned context. The resultant predictive
vector field, representing the context-informed relational path, serves to
dynamically refine the initial static score of an entity pair. Through this
synergy of context-aware static representations and conditioned dynamic
information, FMS facilitates a more profound modeling of relational semantics.
Comprehensive evaluations on several standard benchmarks demonstrate that our
proposed method surpasses prior state-of-the-art results.

</details>


### [40] [Benchmarking Deep Search over Heterogeneous Enterprise Data](https://arxiv.org/abs/2506.23139)
*Prafulla Kumar Choubey,Xiangyu Peng,Shilpa Bhagavath,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新的基准测试，用于评估深度搜索——一种现实且复杂的检索增强生成（RAG）形式，需要对多样、稀疏但相关的来源进行源感知和多跳推理。我们使用合成数据管道构建了这个基准测试，模拟了产品规划、开发和支持阶段的业务工作流程，生成具有现实噪声和保证真实答案的多跳问题的相互关联内容。我们发布了包含可回答和不可回答查询的基准测试，以及包含39,190个企业工件的检索池，使长上下文LLM和RAG系统的细粒度评估成为可能。我们的实验表明，即使是最先进的代理RAG方法在我们的基准测试中平均性能得分为32.96。通过进一步分析，我们强调了检索是主要瓶颈：现有方法难以进行深度搜索并检索所有必要的证据。因此，它们常常基于部分上下文进行推理，导致性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 我们提出了一个新的基准测试，用于评估深度搜索——一种现实且复杂的检索增强生成（RAG）形式，需要对多样、稀疏但相关的来源进行源感知和多跳推理。这些包括文档、会议记录、Slack消息、GitHub和URL，它们的结构各不相同，通常包含人与人之间的互动。

Method: 我们使用合成数据管道构建了这个基准测试，该管道模拟了产品规划、开发和支持阶段的业务工作流程，生成具有现实噪声和保证真实答案的多跳问题的相互关联内容。

Result: 我们发布了包含可回答和不可回答查询的基准测试，以及包含39,190个企业工件的检索池，使长上下文LLM和RAG系统的细粒度评估成为可能。

Conclusion: 我们的实验表明，即使是最先进的代理RAG方法在我们的基准测试中平均性能得分为32.96。通过进一步分析，我们强调了检索是主要瓶颈：现有方法难以进行深度搜索并检索所有必要的证据。因此，它们常常基于部分上下文进行推理，导致性能显著下降。

Abstract: We present a new benchmark for evaluating Deep Search--a realistic and
complex form of retrieval-augmented generation (RAG) that requires
source-aware, multi-hop reasoning over diverse, sparsed, but related sources.
These include documents, meeting transcripts, Slack messages, GitHub, and URLs,
which vary in structure and often contain human-to-human interactions. We build
it using a synthetic data pipeline that simulates business workflows across
product planning, development, and support stages, generating interconnected
content with realistic noise and multi-hop questions with guaranteed
ground-truth answers. We release our benchmark with both answerable and
unanswerable queries, and retrieval pool of 39,190 enterprise artifacts,
enabling fine-grained evaluation of long-context LLM and RAG systems. Our
experiments reveal that even the best-performing agentic RAG methods achieve an
average performance score of 32.96 on our benchmark. With further analysis, we
highlight retrieval as the main bottleneck: existing methods struggle to
conduct deep searches and retrieve all necessary evidence. Consequently, they
often reason over partial context, leading to significant performance
degradation.

</details>


### [41] [Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions](https://arxiv.org/abs/2506.23146)
*Dingzriui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: The paper proposes LCS, a novel metric for evaluating ICL effectiveness, which addresses limitations of current approaches and shows strong correlation with performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current evaluation approaches for ICL are unreliable, have poor attribution, and are impractical in data-insufficient scenarios. LCS aims to address these limitations.

Method: The Learning-to-Context Slope (LCS) metric is proposed, which quantifies ICL effectiveness by modeling the slope between learning gain and contextual relevance.

Result: Extensive experiments show that LCS strongly correlates with performance improvements in labeled settings and reliably reflects true effectiveness in biased or data-scarce scenarios.

Conclusion: LCS is a reliable metric for evaluating ICL effectiveness and provides actionable thresholds for model capabilities critical to ICL success.

Abstract: In-context learning (ICL) has emerged as an effective approach to enhance the
performance of large language models (LLMs). However, its effectiveness varies
significantly across models and tasks, posing challenges for practitioners to
determine when ICL reliably improves performance. Current evaluation
approaches, reliant on performance change after applying ICL, suffer from low
reliability, poor attribution, and impracticality in data-insufficient
scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that
quantifies ICL effectiveness by modeling the slope between learning gain (loss
decrease from demonstrations) and contextual relevance (demonstration-input
relevance). LCS addresses key limitations of performance-based metrics: (1) it
captures continuous loss changes even when outputs are incorrect, improving
reliability; (2) its formulation attributes ICL failures to weak contextual
alignment (inability to adapt inputs to demonstrations) or strong output
calibration (self-verification of correctness); and (3) it minimizes reliance
on labeled data via synthetic evaluation. Extensive experiments demonstrate
that LCS strongly correlates with performance improvements in labeled settings
and reliably reflects true effectiveness in biased or data-scarce scenarios.
Further analysis reveals actionable thresholds for LCS and identifies model
capabilities critical to ICL success.

</details>


### [42] [V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy](https://arxiv.org/abs/2506.23149)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文提出了一种新的合成方法V-Synthesis，通过引入V-Score度量来提高合成示例的一致性和多样性，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 高标注成本促使使用大型语言模型（LLMs）进行合成以减少开销。然而，现有的合成方法主要是任务特定的或依赖于预存在的示例。因此，本文专注于从头开始为任意任务合成示例。

Method: 本文提出了一个称为V-Score的一致性度量，并引入了V-Synthesis，利用V-Score进行比例采样以确保合成示例的一致性和多样性。

Result: 实验结果表明，V-Synthesis相比现有合成方法平均性能提高了2.0%。

Conclusion: 实验结果表明，V-Synthesis相比现有合成方法平均性能提高了2.0%，证实了V-Synthesis的有效性。

Abstract: High labeling cost for in-context learning (ICL) demonstrations motivates
using large language models (LLMs) for synthesis to reduce overhead. However,
existing synthesis methods are mainly task-specific or rely on pre-existing
demonstrations. So this paper focuses on synthesizing demonstrations from
scratch for arbitrary tasks. A major challenge in synthesizing from scratch is
ensuring consistency with the target task, as the lack of labeling guidance
could lead to synthesis bias. We first propose a consistency metric called
V-Score, which has higher performance and lower computation cost compared with
the metrics based on grams or embedding vectors. Furthermore, we introduce
V-Synthesis, which leverages V-Score for proportional sampling to ensure both
high consistency and diversity of synthesized demonstrations. Experimental
results demonstrate that V-Synthesis yields an average performance improvement
of 2.0% compared to existing synthesis methods confirming the effectiveness of
V-Synthesis.

</details>


### [43] [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://arxiv.org/abs/2506.23192)
*Gabriel Iturra-Bocaz,Felipe Bravo-Marquez*

Main category: cs.CL

TL;DR: 本文介绍了一个名为RiverText的Python库，用于从文本数据流中训练和评估增量词嵌入。该工具实现了多种增量词嵌入技术，并适用于信息检索和自然语言处理领域中的流式场景。


<details>
  <summary>Details</summary>
Motivation: 传统词嵌入模型由于其静态性质，在适应不断变化的语言模式（如社交媒体和网络上的新标签或品牌名称）方面存在局限性。因此，需要一种能够动态更新词表示的增量词嵌入算法。

Method: 本文提出了RiverText，实现了不同的增量词嵌入技术，如Skip-gram、连续词袋和词上下文矩阵，并使用PyTorch作为神经网络训练的后端。此外，还实现了一个模块，将现有的静态词嵌入评估任务适应到流设置中。

Result: 本文比较了不同超参数设置下的实现方法，并讨论了结果。开源库已发布在GitHub上。

Conclusion: 本文介绍了RiverText，一个用于从文本数据流中训练和评估增量词嵌入的Python库。该工具为信息检索和自然语言处理社区提供了资源，特别是在处理流式场景中的词嵌入时。

Abstract: Word embeddings have become essential components in various information
retrieval and natural language processing tasks, such as ranking, document
classification, and question answering. However, despite their widespread use,
traditional word embedding models present a limitation in their static nature,
which hampers their ability to adapt to the constantly evolving language
patterns that emerge in sources such as social media and the web (e.g., new
hashtags or brand names). To overcome this problem, incremental word embedding
algorithms are introduced, capable of dynamically updating word representations
in response to new language patterns and processing continuous data streams.
  This paper presents RiverText, a Python library for training and evaluating
incremental word embeddings from text data streams. Our tool is a resource for
the information retrieval and natural language processing communities that work
with word embeddings in streaming scenarios, such as analyzing social media.
The library implements different incremental word embedding techniques, such as
Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized
framework. In addition, it uses PyTorch as its backend for neural network
training. We have implemented a module that adapts existing intrinsic static
word embedding evaluation tasks for word similarity and word categorization to
a streaming setting. Finally, we compare the implemented methods with different
hyperparameter settings and discuss the results. Our open-source library is
available at https://github.com/dccuchile/rivertext.

</details>


### [44] [Generalist Reward Models: Found Inside Large Language Models](https://arxiv.org/abs/2506.23235)
*Yi-Chen Li,Tian Xu,Yang Yu,Xuqin Zhang,Xiong-Hui Chen,Zhongxiang Ling,Ningjing Chao,Lei Yuan,Zhi-Hua Zhou*

Main category: cs.CL

TL;DR: 该研究发现，大型语言模型中隐含了一个强大的通用奖励模型，可以通过理论证明直接从中提取高质量的奖励信号，而无需进一步训练。这种方法在实验中表现出色，优于现有方法，并可能改变奖励建模的方式。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的对齐依赖于昂贵的人类偏好数据训练的奖励模型。虽然已有研究尝试通过AI反馈来避免这种成本，但这些方法通常缺乏严格的理论基础。因此，需要一种更有效的奖励建模方法。

Method: 该研究证明了内生奖励与离线逆强化学习获得的奖励函数在理论上是等价的，并直接从基础模型中提取高质量的奖励信号，而无需进一步训练。此外，使用这种内生奖励进行强化学习可以产生更优的策略。

Result: 实验结果验证了理论，并表明该方法不仅优于现有的LLM作为裁判的方法，还可以超越显式训练的奖励模型。

Conclusion: 该研究发现，通过标准的下一个token预测训练的任何大型语言模型（LLM）中都隐含了一个强大的通用奖励模型。这种内生奖励理论等价于通过离线逆强化学习学习到的奖励函数，从而可以直接从基础模型中获取高质量的奖励信号，而无需进一步训练。此外，使用这种内生奖励进行后续强化学习可以产生比基础模型更优的策略。实验验证了这一理论，并表明该方法不仅优于现有的LLM作为裁判的方法，还可以超越显式训练的奖励模型。这表明奖励建模阶段可以被一种基于预训练知识的原则性方法所取代，为LLM和多模态模型的对齐带来了更高效、强大和可扩展的范式。

Abstract: The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.

</details>


### [45] [Two Spelling Normalization Approaches Based on Large Language Models](https://arxiv.org/abs/2506.23288)
*Miguel Domingo,Francisco Casacuberta*

Main category: cs.CL

TL;DR: 本文提出了两种基于大语言模型的拼写规范化方法，评估结果显示统计机器翻译仍是该任务最合适的技術。


<details>
  <summary>Details</summary>
Motivation: 历史文献中缺乏标准化拼写规范和人类语言的有机演变带来了固有的语言挑战，这是人文学科学者长期以来关注的问题。拼写规范化旨在使文档的正字法与现代标准一致。

Method: 基于大语言模型的两种新方法：一种是未经监督训练的方法，另一种是用于机器翻译的方法。

Result: 在涵盖多种语言和历史时期的多个数据集上进行评估，结果表明这两种方法都取得了有希望的结果。

Conclusion: 统计机器翻译似乎仍是该任务最合适的技術。

Abstract: The absence of standardized spelling conventions and the organic evolution of
human language present an inherent linguistic challenge within historical
documents, a longstanding concern for scholars in the humanities. Addressing
this issue, spelling normalization endeavors to align a document's orthography
with contemporary standards. In this study, we propose two new approaches based
on large language models: one of which has been trained without a supervised
training, and a second one which has been trained for machine translation. Our
evaluation spans multiple datasets encompassing diverse languages and
historical periods, leading us to the conclusion that while both of them
yielded encouraging results, statistical machine translation still seems to be
the most suitable technology for this task.

</details>


### [46] [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/abs/2506.23293)
*P. Myles Eugenio*

Main category: cs.CL

TL;DR: 本文提出一种新的神经符号框架，利用分层霍普菲尔德记忆链进行局部语言建模，能够生成类似人类语言的合成语言，并具备可解释性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型依赖于预定义的标记或监督，而本文旨在通过从零开始构建结构来学习符号序列，以实现更自然的语言处理。

Method: 提出了一种基于局部事件驱动涌现学习的神经符号框架，核心是一个分层霍普菲尔德记忆链，作为组合性短时记忆和动态分词器（重分词器）。

Result: 实验表明，重分词器可以过滤自然语言模式并生成具有连贯内部形态的合成语言，其表现与人类语言相当。

Conclusion: 该架构为研究符号结构如何从局部神经学习中出现提供了方法论基础，并为构建可扩展、可解释的神经符号系统提供了一条新途径。

Abstract: We present a neuro-symbolic framework for generative language modeling based
on local, event-driven emergent learning. At its core is a hierarchical
Hopfield memory chain acting as a compositional short-term memory and dynamic
tokenizer (retokenizer). Rather than relying on predefined tokens or
supervision, the model builds structure from scratch, learning symbol sequences
as multi-scale representations. It constructs projection tensors that bind
co-occurring features into hierarchical tokens, introducing redundancy (i.e an
emergent gauge structure) and enabling compression of local activations into
long-range dependencies. Curiously, we find that the retokenizer can filter
natural language patterns from noise, generating synthetic languages with
coherent internal morphology -- quantifiably the same as human language.
Language is learned in a local (Hebbian) fashion, where model constraints
dictate allowed emergent structure, and new information is retained in
alignment with this structure. The absence of a global objective enables a form
of plasticity not found in conventional language models, allowing the system to
generalize beyond its initial inference class -- even without explicit data. We
demonstrate that briefly activating a new neuron during inference binds
distributed multi-scale token features into a symbolic embedding. These
emergent embedding neurons act as long-term memory and support a key-value
mechanism for compositional inference and generalization. This architecture
provides a methodological foundation for studying how symbolic structure can
emerge from local neural learning. It offers a new pathway for building
scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and
reasoning arise as compressed memory traces within a Hopfield hierarchy. This
approach advances the development of neuromorphic architectures for generative
language models.

</details>


### [47] [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/abs/2506.23315)
*Shouvon Sarker,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: 本研究提出了一种基于BERT的集成模型，用于检测和分类临床笔记中的药物事件。通过在不同类型的大型数据上预训练BERT模型，并在CMED训练数据上进行微调，最终利用投票策略整合多个预测结果。实验结果表明，该方法能显著提高严格Micro-F和Macro-F分数。


<details>
  <summary>Details</summary>
Motivation: 从健康记录和临床笔记中识别关键变量（如药物、疾病、关系）在临床领域有广泛的应用。n2c2 2022提供了一个挑战，旨在解决临床数据分析中的自然语言处理问题，并构建了一个全面标注的临床数据上下文药物事件数据集（CMED）。本研究专注于该挑战中的子任务2，即通过构建一个新颖的基于BERT的集成模型来检测和分类临床笔记中的药物事件。

Method: 该研究构建了一个基于BERT的集成模型，通过在不同类型的大型数据（如维基百科和MIMIC）上预训练BERT模型，然后在CMED训练数据上进行微调，并利用投票策略整合多个预测结果以完成药物事件分类。

Result: 实验结果表明，基于BERT的集成模型可以有效提高严格Micro-F分数约5%，严格Macro-F分数约6%。

Conclusion: 实验结果表明，基于BERT的集成模型可以有效提高严格Micro-F分数和严格Macro-F分数。

Abstract: Identification of key variables such as medications, diseases, relations from
health records and clinical notes has a wide range of applications in the
clinical domain. n2c2 2022 provided shared tasks on challenges in natural
language processing for clinical data analytics on electronic health records
(EHR), where it built a comprehensive annotated clinical data Contextualized
Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of
this challenge that is to detect and classify medication events from clinical
notes through building a novel BERT-based ensemble model. It started with
pretraining BERT models on different types of big data such as Wikipedia and
MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED
training data. These fine-tuned BERT models were employed to accomplish
medication event classification on CMED testing data with multiple predictions.
These multiple predictions generated by these fine-tuned BERT models were
integrated to build final prediction with voting strategies. Experimental
results demonstrated that BERT-based ensemble models can effectively improve
strict Micro-F score by about 5% and strict Macro-F score by about 6%,
respectively.

</details>


### [48] [Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family](https://arxiv.org/abs/2506.23340)
*Yumeng Lin,Xufeng Duan,David Haslett,Yige Chen,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 本研究探讨了训练数据、语言接近度和语言家族如何影响多语言翻译中的信息丢失，并发现翻译质量不仅由数据量决定，还由语言之间的结构和类型关系决定。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言翻译中取得了显著进展，但仍面临某些语言对的挑战，特别是那些训练数据有限或与英语有显著语言差异的语言对。

Method: 本研究系统地调查了训练数据、语言接近度和语言家族如何影响多语言翻译中的信息丢失。我们通过进行往返翻译评估了两个大型语言模型GPT-4和Llama 2，并使用BLEU分数和BERT相似性指标评估了翻译质量。

Result: 我们的结果揭示了训练数据大小和语言距离之间的强大相互作用：虽然丰富的训练数据可以缓解语言分歧的影响，但在低资源条件下，结构上更接近英语的语言始终能产生更高的翻译质量。各种距离度量中，正字法、系统发生学、句法和地理距离成为翻译性能的强大预测因素。语言家族也独立地产生影响。

Conclusion: 这些发现有助于更深入地理解塑造大型语言模型多语言翻译的语言约束，强调翻译质量不仅由数据量决定，还由语言之间的结构和类型关系决定。

Abstract: Large language models have achieved impressive progress in multilingual
translation, yet they continue to face challenges with certain language
pairs-particularly those with limited training data or significant linguistic
divergence from English. This study systematically investigates how training
data, language proximity, and language family affect information loss in
multilingual translation. We evaluate two large language models, GPT-4 and
Llama 2, by performing round-trip translations. Translation quality was
assessed using BLEU scores and BERT similarity metrics. Our results reveal a
robust interaction between training data size and language distance: while
abundant training data can mitigate the effects of linguistic divergence,
languages structurally closer to English consistently yield higher translation
quality in low-resource conditions. Among various distance metrics,
orthographic, phylogenetic, syntactic, and geographical distances emerge as
strong predictors of translation performance. Language family also exerts an
independent influence. These findings contribute to a deeper understanding of
the linguistic constraints shaping multilingual translation in large language
models, emphasizing that translation quality is shaped not only by data volume
but also by structural and typological relationships between languages.

</details>


### [49] [ATGen: A Framework for Active Text Generation](https://arxiv.org/abs/2506.23342)
*Akim Tsvigun,Daniil Vasilev,Ivan Tsvigun,Ivan Lysenko,Talgat Bektleuov,Aleksandr Medvedev,Uliana Vinogradova,Nikita Severin,Mikhail Mozikov,Andrey Savchenko,Rostislav Grigorev,Ramil Kuleev,Fedor Zhdanov,Artem Shelmanov,Ilya Makarov*

Main category: cs.CL

TL;DR: 本文介绍了 ATGen，这是一个将主动学习与文本生成任务相结合的框架，可以减少标注成本并提高效率。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来自然语言生成 (NLG) 任务越来越受欢迎，但 AL 在 NLG 中的应用仍然有限。因此，需要一种框架来弥合 AL 与文本生成任务之间的差距。

Method: ATGen 是一个综合框架，将主动学习 (AL) 与文本生成任务相结合，使最先进的 AL 策略能够应用于自然语言生成 (NLG)。该框架使用人类标注者和基于大型语言模型 (LLM) 的自动标注代理来简化 AL 赋能的标注。

Result: ATGen 在各种设置和多个文本生成任务中展示了最先进的 AL 策略的评估结果。

Conclusion: ATGen 减少了人类标注者的努力和与基于 LLM 的标注代理的 API 调用成本。

Abstract: Active learning (AL) has demonstrated remarkable potential in reducing the
annotation effort required for training machine learning models. However,
despite the surging popularity of natural language generation (NLG) tasks in
recent years, the application of AL to NLG has been limited. In this paper, we
introduce Active Text Generation (ATGen) - a comprehensive framework that
bridges AL with text generation tasks, enabling the application of
state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered
annotation in NLG tasks using both human annotators and automatic annotation
agents based on large language models (LLMs). The framework supports LLMs
deployed as services, such as ChatGPT and Claude, or operated on-premises.
Furthermore, ATGen provides a unified platform for smooth implementation and
benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present
evaluation results for state-of-the-art AL strategies across diverse settings
and multiple text generation tasks. We show that ATGen reduces both the effort
of human annotators and costs associated with API calls to LLM-based annotation
agents. The code of the framework is available on GitHub under the MIT license.
The video presentation is available at http://atgen-video.nlpresearch.group

</details>


### [50] [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/abs/2506.23377)
*Taejin Kim,Siun-Chuon Mau,Konrad Vesey*

Main category: cs.CL

TL;DR: This paper introduces Perspective-Dial, a method to quantify and control the perspective of large language model outputs, with potential applications in bias detection, narrative tracking, and debate support.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of quantifiable understanding of bias and perspective in LLM outputs, which is crucial for mission-critical applications.

Method: Perspective-Dial consists of two components: a metric space called Perspective Space for quantifying perspectives, and Systematic Prompt Engineering using greedy-coordinate descent to control LLM output based on feedback from the Perspective Space.

Result: The empirical approach allows for progress in quantifying and adjusting LLM outputs for various topics without requiring a principled understanding of perspective or bias.

Conclusion: Perspective-Dial provides a method to quantify and adjust the perspective of LLM outputs, which can be applied to detect and mitigate bias, track narratives, and support debate bots.

Abstract: Large language models (LLMs) are used in a variety of mission-critical roles.
Due to the rapidly developing nature of LLMs, there is a lack of quantifiable
understanding of the bias and perspective associated with LLM output. Inspired
by this need, this paper considers the broader issue of perspective or
viewpoint of general text and perspective control of large-language model (LLM)
output. Perspective-Dial consists of two main components: a (1) metric space,
dubbed Perspective Space, that enables quantitative measurements of different
perspectives regarding a topic, and the use of (2) Systematic Prompt
Engineering that utilizes greedy-coordinate descent to control LLM output
perspective based on measurement feedback from the Perspective Space. The
empirical nature of the approach allows progress to side step a principled
understanding of perspective or bias -- effectively quantifying and adjusting
outputs for a variety of topics. Potential applications include detection,
tracking and mitigation of LLM bias, narrative detection, sense making and
tracking in public discourse, and debate bot advocating given perspective.

</details>


### [51] [Hierarchical Memory Organization for Wikipedia Generation](https://arxiv.org/abs/2506.23393)
*Eugene J. Yu,Dawei Zhu,Yifan Song,Xiangyu Wong,Jiebin Zhang,Wenxuan Shi,Xiaoguang Li,Qun Liu,Sujian Li*

Main category: cs.CL

TL;DR: 本文介绍了MOG框架，通过分层记忆架构提高生成维基百科文章的信息量和可靠性。


<details>
  <summary>Details</summary>
Motivation: 生成自主的维基百科文章是一个具有挑战性的任务，需要整合来自不同来源的准确、全面和结构良好的信息。

Method: MOG框架利用分层记忆架构，从网络文档中提取细粒度的记忆单元，并递归地组织成维基百科风格的层次结构，以指导生成过程。

Result: 在新创建的WikiStart数据集上的评估表明，MOG在生成信息丰富且可靠的文章方面优于基线方法。

Conclusion: MOG在生成信息丰富且可靠的文章方面优于基线方法，使其在现实场景中特别稳健。

Abstract: Generating Wikipedia articles autonomously is a challenging task requiring
the integration of accurate, comprehensive, and well-structured information
from diverse sources. This paper introduces the Memory Organization-based
Generation (MOG) framework, a novel approach to address these challenges by
leveraging a hierarchical memory architecture. MOG extracts fine-grained memory
units from web documents, recursively organizes them into a Wikipedia-style
hierarchical structure, and uses this structure to guide the generation
process. This ensures alignment between memory and the article outline,
improving both informativeness and verifiability while minimizing
hallucinations. Additionally, a citation module is implemented to enhance
traceability by linking every generated sentence to specific memory units.
Evaluations on our newly created WikiStart dataset demonstrate that MOG
outperforms baseline methods in producing informative and reliable articles,
making it particularly robust in real-world scenarios.

</details>


### [52] [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/abs/2506.23411)
*Jiale Zhang,Zichong Wang,Avash Palikhe,Zhipeng Yin,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本文全面审查了当前语言模型研究中最常用的公平性数据集，提出了一个统一的评估框架，以揭示跨数据集和评分方法的一致性人口统计学差异，并提供了选择、组合和解释这些数据集的实用指南。


<details>
  <summary>Details</summary>
Motivation: 本文旨在填补对公平性基准所依赖的数据集研究不足的空白，帮助研究人员更好地理解这些资源中的假设和限制。

Method: 本文通过全面审查当前语言模型研究中最常用的公平性数据集，提出了一种统一的评估框架，以揭示跨数据集和评分方法的一致性人口统计学差异。

Result: 本文通过应用统一的评估框架，揭示了二十四个常见基准中经常被忽视的偏见，并提供了选择、组合和解释这些数据集的实用指南。

Conclusion: 本文指出，公平性基准在评估语言模型中起着核心作用，但对这些基准所依赖的数据集的研究却很少。通过全面审查当前语言模型研究中最常用公平性数据集，本文揭示了数据集的来源、范围、内容和预期用途，并提出了一个统一的评估框架，以揭示跨数据集和评分方法的一致性人口统计学差异。最后，本文提供了选择、组合和解释这些数据集的实用指南，并指出了创建反映更多元社会背景的新公平性基准的机会。

Abstract: Fairness benchmarks play a central role in shaping how we evaluate language
models, yet surprisingly little attention has been given to examining the
datasets that these benchmarks rely on. This survey addresses that gap by
presenting a broad and careful review of the most widely used fairness datasets
in current language model research, characterizing them along several key
dimensions including their origin, scope, content, and intended use to help
researchers better appreciate the assumptions and limitations embedded in these
resources. To support more meaningful comparisons and analyses, we introduce a
unified evaluation framework that reveals consistent patterns of demographic
disparities across datasets and scoring methods. Applying this framework to
twenty four common benchmarks, we highlight the often overlooked biases that
can influence conclusions about model fairness and offer practical guidance for
selecting, combining, and interpreting these datasets. We also point to
opportunities for creating new fairness benchmarks that reflect more diverse
social contexts and encourage more thoughtful use of these tools going forward.
All code, data, and detailed results are publicly available at
https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets
to promote transparency and reproducibility across the research community.

</details>


### [53] [TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs](https://arxiv.org/abs/2506.23423)
*Felipe Nuti,Tim Franzmeyer,João Henriques*

Main category: cs.CL

TL;DR: This paper proposes a method called Tuning Contribution (TuCo) to measure the effect of fine-tuning on individual responses of large language models. TuCo provides insights into how fine-tuning influences model behavior and safety, and it was found that adversarial attacks reduce TuCo, indicating that attenuating the effect of fine-tuning may contribute to their success.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a quantitative and systematic method for analyzing the effect of fine-tuning on individual outputs of large language models (LLMs).

Method: The method tracks the model's intermediate hidden states and introduces an exact decomposition of any fine-tuned LLM into a pre-training component and a fine-tuning component. It defines the Tuning Contribution (TuCo) as the ratio of the magnitudes of the fine-tuning component to the pre-training component.

Result: The results show that model behavior and performance can be steered by up- or down-scaling the fine-tuning component during the forward pass. Additionally, three prominent adversarial attacks on LLMs reduce TuCo, and TuCo is consistently lower on prompts where these attacks succeed compared to those where they do not.

Conclusion: TuCo enables the quantitative study of how fine-tuning influences model behavior and safety, and vice versa.

Abstract: Past work has studied the effects of fine-tuning on large language models'
(LLMs) overall performance on certain tasks. However, a quantitative and
systematic method for analyzing its effect on individual outputs is still
lacking. Here, we propose a new method for measuring the contribution that
fine-tuning makes to individual LLM responses, assuming access to the original
pre-trained model. Our method tracks the model's intermediate hidden states,
providing a more fine-grained insight into the effects of fine-tuning than a
simple comparison of final outputs from pre-trained and fine-tuned models. We
introduce and theoretically analyze an exact decomposition of any fine-tuned
LLM into a pre-training component and a fine-tuning component. Empirically, we
find that model behavior and performance can be steered by up- or down-scaling
the fine-tuning component during the forward pass. Motivated by this finding
and our theoretical analysis, we define the Tuning Contribution (TuCo) as the
ratio of the magnitudes of the fine-tuning component to the pre-training
component. We observe that three prominent adversarial attacks on LLMs
circumvent safety measures in a way that reduces TuCo, and that TuCo is
consistently lower on prompts where these attacks succeed compared to those
where they do not. This suggests that attenuating the effect of fine-tuning on
model outputs plays a role in the success of such attacks. In summary, TuCo
enables the quantitative study of how fine-tuning influences model behavior and
safety, and vice versa.

</details>


### [54] [Pipelined Decoder for Efficient Context-Aware Text Generation](https://arxiv.org/abs/2506.23431)
*Zixian Huang,Chenxu Niu,Yu Gu,Gengyang Xiao,Xinwei Huang,Gong Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种新的解码器架构，可以在不显著损失生成质量或额外内存消耗的情况下，显著提高生成速度。


<details>
  <summary>Details</summary>
Motivation: 自回归模型需要根据所有先前生成的标记生成新标记，这虽然带来了高质量，但也限制了模型一次生成一个标记，形成了限制生成速度的瓶颈。

Method: 我们提出了一种新的解码器架构，该架构通过同时生成多个子序列并在每个时间步为每个子序列生成新标记来实现并行性。

Result: 在多个文本生成任务上的实验，包括问答、文本摘要和关键词生成，表明我们的管道解码器显著提高了生成速度。

Conclusion: 我们的管道解码器在不显著损失生成质量或额外内存消耗的情况下，显著提高了生成速度。

Abstract: As the basis of generative AI, an autoregressive model requires the
generation of a new token depending on all the previously generated tokens,
which brings high quality but also restricts the model to generate tokens one
by one, forming a bottleneck limiting the generation speed. In this paper, we
propose a new decoder architecture that efficiently generates text in parallel
for context-aware generation tasks. Our proposed pipelined decoder initiates
the generation of multiple subsequences simultaneously, and, at each time-step,
it generates a new token for each subsequence to realize parallelism.
Experiments on multiple text generation tasks, including question answering,
text summarization, and keyphrase generation, show that our pipelined decoder
significantly improves the generation speed without a significant loss of
generation quality or additional memory consumption.

</details>


### [55] [What to Keep and What to Drop: Adaptive Table Filtering Framework](https://arxiv.org/abs/2506.23463)
*Jang Won June*

Main category: cs.CL

TL;DR: 本文提出了一种名为ATF的自适应表格过滤框架，用于解决大型语言模型在处理大型表格时的输入长度限制问题。ATF通过修剪无信息的列和行来减少表格大小，并在不重新训练现有模型的情况下提高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在基于表格的推理中由于输入长度限制而难以处理大型表格。

Method: ATF（自适应表格过滤框架）是一种模块化且问题感知的过滤管道，使用LLM生成的列描述、聚类和稀疏-密集对齐分数来修剪无信息的列和行。

Result: ATF减少了约70%的表格单元，提高了跨域TableQA任务的性能，但在Table Fact Verification任务中导致了轻微的性能下降。

Conclusion: ATF的实验结果表明，它能够根据不同的任务自适应地平衡信息量和简洁性。

Abstract: Large language models (LLMs) for table-based reasoning often struggle with
large tables due to input length limits. We propose ATF (Adaptive Table
Filtering Framework), a modular and question-aware filtering pipeline that
prunes uninformative columns and rows using LLM-generated column descriptions,
clustering, and sparse-dense alignment scores. ATF integrates seamlessly with
existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that
ATF reduces table cells by ~70\%, boosting performance on out-of-domain TableQA
tasks while causing slight performance drops on Table Fact Verification, where
full-table context is more critical. These results highlight ATF's ability to
adaptively balance informativeness and minimalism across tasks.

</details>


### [56] [Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent](https://arxiv.org/abs/2506.23485)
*Haocheng Yu,Yaxiong Wu,Hao Wang,Wei Guo,Yong Liu,Yawen Li,Yuyang Ye,Junping Du,Enhong Chen*

Main category: cs.CL

TL;DR: TAIRA is a thought-augmented interactive recommender agent system that enhances the ability to handle complex user intents through a manager agent and Thought Pattern Distillation (TPD). It outperforms existing methods, especially on challenging tasks and novel scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing formulations of LLM-powered interactive recommender agents struggle to effectively address diverse and complex user intents, such as intuitive, unrefined, or occasionally ambiguous requests. The motivation is to develop a system that can better handle these complex user intents.

Method: TAIRA is a thought-augmented interactive recommender agent system that utilizes a manager agent to orchestrate recommendation tasks by decomposing user needs and planning subtasks. It strengthens its planning capacity through Thought Pattern Distillation (TPD), which extracts high-level thoughts from the agent's and human experts' experiences. Additionally, user simulation schemes are designed to generate personalized queries of different difficulties and evaluate recommendations based on specific datasets.

Result: TAIRA exhibits significantly enhanced performance compared to existing methods. It shows a greater advantage on more challenging tasks while generalizing effectively on novel tasks, further validating its superiority in managing complex user intents within interactive recommendation systems.

Conclusion: TAIRA demonstrates superior performance in handling complex user intents within interactive recommendation systems, showing greater advantages on challenging tasks and effective generalization on novel tasks.

Abstract: Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.

</details>


### [57] [Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably](https://arxiv.org/abs/2506.23508)
*Zhihao Zhang,Qiaole Dong,Qi Zhang,Jun Zhao,Enyu Zhou,Zhiheng Xi,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Tao Ji,Tao Gui,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文研究了SFT和RFT对多模态大语言模型的影响，发现RFT在保持先前知识方面表现更好，而SFT则导致遗忘。


<details>
  <summary>Details</summary>
Motivation: 尽管SFT和RFT在任务适应方面有效，但它们对先前知识的影响尚不清楚。

Method: 我们引入了拼图任务作为现有预训练语料库中缺失的新任务，并系统地研究了SFT和RFT在开源多模态模型Qwen2.5-VL上的行为。

Result: 实验揭示了一个尖锐的权衡：SFT能够快速获取任务，但会导致灾难性遗忘，而RFT在新任务上的学习速度较慢，但能保持先前知识。

Conclusion: 这些发现表明，数据分布而不是算法差异在遗忘中起着核心作用，并突显了RFT在多模态大语言模型中的稳定连续学习的潜力。

Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and
Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large
language models to downstream tasks. While effective at task adaptation, their
impact on prior knowledge remains unclear. In this paper, we introduce jigsaw
puzzles as a novel task absent from existing pretraining corpora and
systematically study the behavior of SFT and RFT on an open-source multimodal
model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid
task acquisition but leads to catastrophic forgetting, whereas RFT learns more
slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon
through the lens of learning dynamics, showing that RFT reinforces correct
samples that are naturally aligned with the base model's probability landscape,
mitigating interference with prior knowledge. Moreover, supervised training on
correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly
learning new tasks. These findings suggest that data distribution, rather than
algorithmic differences, plays a central role in forgetting, and highlight
RFT's potential for stable continual learning in multimodal large language
models.

</details>


### [58] [NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning](https://arxiv.org/abs/2506.23524)
*Phan Quoc Hung Mai,Quang Hung Nguyen,Phuong Giang Duong,Hong Hanh Nguyen,Nguyen Tuan Long*

Main category: cs.CL

TL;DR: 本文介绍了一个新的越南语教育情感分类和主题分类数据集NEU-ESC，并展示了使用BERT进行多任务学习的高性能。


<details>
  <summary>Details</summary>
Motivation: 在教育领域，通过学生的评论来理解他们的意见至关重要，特别是在越南语中，资源仍然有限。现有的教育数据集往往缺乏领域相关性和学生俚语。

Method: 本文使用了编码器-only语言模型（如BERT）进行多任务学习，以实现情感分类和主题分类任务。

Result: 本文展示了使用BERT进行多任务学习在情感分类和主题分类任务中的性能，准确率分别达到83.7%和79.8%。此外，还对数据集和模型进行了基准测试，并与其他数据集和模型进行了比较。

Conclusion: 本文介绍了NEU-ESC数据集，并展示了使用BERT进行多任务学习在情感分类和主题分类任务中的高性能。同时，还与其他数据集和模型进行了基准测试，并讨论了这些结果。

Abstract: In the field of education, understanding students' opinions through their
comments is crucial, especially in the Vietnamese language, where resources
remain limited. Existing educational datasets often lack domain relevance and
student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese
dataset for Educational Sentiment Classification and Topic Classification,
curated from university forums, which offers more samples, richer class
diversity, longer texts, and broader vocabulary. In addition, we explore
multitask learning using encoder-only language models (BERT), in which we
showed that it achieves performance up to 83.7% and 79.8% accuracy for
sentiment and topic classification tasks. We also benchmark our dataset and
model with other datasets and models, including Large Language Models, and
discuss these benchmarks. The dataset is publicly available at:
https://huggingface.co/datasets/hung20gg/NEU-ESC.

</details>


### [59] [On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?](https://arxiv.org/abs/2506.23527)
*Jan Kvapil,Martin Fajcik*

Main category: cs.CL

TL;DR: 本研究分析了LLMs生成的烹饪食谱中的记忆、创造力和无意义内容，并提出了一种自动化框架来大规模量化这些特性。


<details>
  <summary>Details</summary>
Motivation: 当前研究需要分析LLMs生成的食谱中的记忆、创造力和无意义内容，但手动标注成本高且难以扩展。因此，需要一种自动化的方法来扩大研究规模。

Method: 本研究首先对20个由Mixtral生成的食谱进行人工标注，提取食材和步骤以评估哪些元素是记忆的，哪些是创造性的或无意义的。然后设计了一个“LLM作为评判者”的管道，自动化生成食谱、检测无意义内容、解析食材和步骤，并进行标注。

Result: Mixtral模型在生成食谱时倾向于重复在线文档中的食材，表明其高度依赖记忆内容。此外，Llama 3.1+Gemma 2 9B在食材匹配上达到了78%的准确率，证明了自动化框架的有效性。

Conclusion: 本研究通过人工标注和自动化框架分析了大型语言模型（LLMs）生成的烹饪食谱中的记忆、创造力和无意义内容，发现Mixtral模型高度依赖记忆内容，并提出了一个有效的自动化方法来大规模量化这些特性。

Abstract: This work-in-progress investigates the memorization, creativity, and nonsense
found in cooking recipes generated from Large Language Models (LLMs).
Precisely, we aim (i) to analyze memorization, creativity, and non-sense in
LLMs using a small, high-quality set of human judgments and (ii) to evaluate
potential approaches to automate such a human annotation in order to scale our
study to hundreds of recipes. To achieve (i), we conduct a detailed human
annotation on 20 preselected recipes generated by LLM (Mixtral), extracting
each recipe's ingredients and step-by-step actions to assess which elements are
memorized--i.e., directly traceable to online sources possibly seen during
training--and which arise from genuine creative synthesis or outright nonsense.
We find that Mixtral consistently reuses ingredients that can be found in
online documents, potentially seen during model training, suggesting strong
reliance on memorized content. To achieve aim (ii) and scale our analysis
beyond small sample sizes and single LLM validation, we design an
``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,
parsing ingredients and recipe steps, and their annotation. For instance,
comparing its output against human annotations, the best ingredient extractor
and annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on
ingredient matching. This automated framework enables large-scale
quantification of memorization, creativity, and nonsense in generated recipes,
providing rigorous evidence of the models' creative capacities.

</details>


### [60] [Semantic-guided Diverse Decoding for Large Language Model](https://arxiv.org/abs/2506.23601)
*Weijie Shi,Yue Cui,Yaguang Wu,Jingzhi Fang,Shibo Zhang,Mengze Li,Sirui Han,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种新的多样化解码方法SemDiD，能够在嵌入空间中平衡质量和多样性，显著提升了多个任务的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的多样化解码对于需要多个语义上不同的响应的应用至关重要，但现有方法主要实现的是词汇而非语义多样性。

Method: 我们引入了语义引导的多样化解码（SemDiD），在嵌入空间中直接操作，通过三种互补机制平衡质量和多样性：正交方向引导、动态组间排斥和位置去偏概率评估。

Result: SemDiD在多个任务中 consistently 超过现有方法，提高了Best-of-N覆盖率，并加速了RLHF训练收敛。

Conclusion: 实验表明，SemDiD在多个任务中 consistently 超过现有方法，提高了Best-of-N覆盖率，并加速了RLHF训练收敛。

Abstract: Diverse decoding of large language models is crucial for applications
requiring multiple semantically distinct responses, yet existing methods
primarily achieve lexical rather than semantic diversity. This limitation
significantly constrains Best-of-N strategies, group-based reinforcement
learning, and data synthesis. While temperature sampling and diverse beam
search modify token distributions or apply n-gram penalties, they fail to
ensure meaningful semantic differentiation. We introduce Semantic-guided
Diverse Decoding (SemDiD), operating directly in embedding space that balances
quality with diversity through three complementary mechanisms: orthogonal
directional guidance, dynamic inter-group repulsion, and position-debiased
probability assessment. SemDiD harmonizes these competing objectives using
adaptive gain functions and constraint optimization, ensuring both quality
thresholds and maximal semantic differentiation. Experiments show SemDiD
consistently outperforms existing methods, improving Best-of-N coverage by
1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%
while increasing accuracy by up to 2.1%.

</details>


### [61] [Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs](https://arxiv.org/abs/2506.23610)
*Manuel Pratelli,Marinella Petrocchi*

Main category: cs.CL

TL;DR: 研究评估了基于大五人格档案的LLM代理在再现基于个性的虚假信息易感性变化方面的能力，发现某些人格-虚假信息关联被可靠地复制，而其他关联则存在差异，揭示了LLM如何内部化和表达人格的系统性偏差。研究结果强调了个性对齐的LLM在行为模拟中的潜力和局限性，并为在人工智能代理中建模认知多样性提供了新的见解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）使得大规模生成合成行为数据成为可能，为人类实验提供了一种道德且低成本的替代方案。然而，这种数据是否能够忠实捕捉由人格特质驱动的心理差异仍然是一个开放问题。

Method: 我们评估了基于大五人格档案的LLM代理在再现基于个性的虚假信息易感性变化方面的能力，重点关注新闻辨别能力，即判断真实标题为真实和虚假标题为虚假的能力。利用已发布的数据集，其中人类参与者根据已知的人格档案对标题准确性进行评分，我们创建了匹配的LLM代理，并将其响应与原始人类模式进行比较。

Result: 某些人格-虚假信息关联，特别是涉及宜人性和尽责性的关联，被可靠地复制，而其他关联则存在差异，揭示了LLM如何内部化和表达人格的系统性偏差。

Conclusion: 研究结果强调了个性对齐的LLM在行为模拟中的潜力和局限性，并为在人工智能代理中建模认知多样性提供了新的见解。

Abstract: Large language models (LLMs) make it possible to generate synthetic
behavioural data at scale, offering an ethical and low-cost alternative to
human experiments. Whether such data can faithfully capture psychological
differences driven by personality traits, however, remains an open question. We
evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to
reproduce personality-based variation in susceptibility to misinformation,
focusing on news discernment, the ability to judge true headlines as true and
false headlines as false. Leveraging published datasets in which human
participants with known personality profiles rated headline accuracy, we create
matching LLM agents and compare their responses to the original human patterns.
Certain trait-misinformation associations, notably those involving
Agreeableness and Conscientiousness, are reliably replicated, whereas others
diverge, revealing systematic biases in how LLMs internalize and express
personality. The results underscore both the promise and the limits of
personality-aligned LLMs for behavioral simulation, and offer new insight into
modeling cognitive diversity in artificial agents.

</details>


### [62] [Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack](https://arxiv.org/abs/2506.23661)
*Arnisa Fazla,Lucas Krauter,David Guzman Piedrahita,Andrianos Michail*

Main category: cs.CL

TL;DR: 我们扩展了BeamAttack，以提高对抗攻击算法的有效性，并在多个数据集和模型上取得了高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 我们的扩展旨在提高对抗攻击算法的有效性，以评估文本分类系统的鲁棒性。

Method: 我们扩展了BeamAttack，增加了对单词删除的支持，并可以选择跳过替换，以发现改变模型预测的最小修改。我们还集成了LIME以更好地优先考虑单词替换。

Result: 在BODEGA框架内的多个数据集和受害者模型（BiLSTM、BERT和对抗训练的RoBERTa）上进行评估，我们的方法实现了超过99%的攻击成功率，同时保持了原始文本的语义和词汇相似性。

Conclusion: 通过定量和定性分析，我们突显了BeamAttack的有效性和其局限性。

Abstract: We extend BeamAttack, an adversarial attack algorithm designed to evaluate
the robustness of text classification systems through word-level modifications
guided by beam search. Our extensions include support for word deletions and
the option to skip substitutions, enabling the discovery of minimal
modifications that alter model predictions. We also integrate LIME to better
prioritize word replacements. Evaluated across multiple datasets and victim
models (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA
framework, our approach achieves over a 99\% attack success rate while
preserving the semantic and lexical similarity of the original texts. Through
both quantitative and qualitative analysis, we highlight BeamAttack's
effectiveness and its limitations. Our implementation is available at
https://github.com/LucK1Y/BeamAttack

</details>


### [63] [Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation](https://arxiv.org/abs/2506.23662)
*Philip Lippmann,Jie Yang*

Main category: cs.CL

TL;DR: ZEST is a zero-shot contextual adaptation framework that generates a synthetic context corpus to enable domain-adapted embeddings without requiring access to the target corpus or domain-specific finetuning.


<details>
  <summary>Details</summary>
Motivation: Context-aware embedding methods boost retrieval accuracy by conditioning on corpus statistics, but they require access to the target corpus or domain-specific finetuning, which poses practical barriers in privacy-sensitive or resource-constrained settings.

Method: ZEST is a zero-shot contextual adaptation framework that replaces real corpus access with a one-time offline synthesis of a compact proxy. It uses a multi-step hierarchical procedure to generate a synthetic context corpus of several hundred documents that aims to emulate key domain-specific distributions.

Result: ZEST's zero-shot synthetic context adaptation using only five example documents performs within 0.5% of models leveraging full target corpus access, demonstrating remarkable efficacy without any retraining.

Conclusion: ZEST provides a practical method for deploying high-performance, adaptable embeddings in constrained environments.

Abstract: Context-aware embedding methods boost retrieval accuracy by conditioning on
corpus statistics (e.g., term co-occurrence and topical patterns) extracted
from neighboring documents. However, this context-aware approach requires
access to the target corpus or requires domain-specific finetuning, posing
practical barriers in privacy-sensitive or resource-constrained settings. We
present ZEST, a zero-shot contextual adaptation framework that replaces real
corpus access with a one-time offline synthesis of a compact proxy. Given only
a handful exemplar documents representative of the general target domain, we
use a multi-step hierarchical procedure to generate a synthetic context corpus
of several hundred documents that aims to emulate key domain-specific
distributions. At inference, the frozen context-aware encoder uses this proxy
corpus -- without any finetuning or target corpus access -- to produce
domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot
synthetic context adaptation using only five example documents performs within
0.5% of models leveraging full target corpus access -- demonstrating remarkable
efficacy without any retraining. ZEST thus provides a practical method for
deploying high-performance, adaptable embeddings in constrained environments.

</details>


### [64] [L0: Reinforcement Learning to Become General Agents](https://arxiv.org/abs/2506.23667)
*Junjie Zhang,Jingyi Xi,Zhuoyang Song,Junyu Lu,Yuhua Ke,Ting Sun,Yukun Yang,Jiaxing Zhang,Songxin Zhang,Zejian Xie*

Main category: cs.CL

TL;DR: 本文介绍了 L-Zero (L0) 系统，这是一个用于通用代理的可扩展端到端训练管道，通过强化学习与验证奖励 (RLVR) 提高了问答任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLMs）作为自主代理来执行多轮、长周期任务仍然是可扩展性和训练效率的重大挑战。

Method: L-Zero (L0) 是一种可扩展的端到端训练管道，用于通用代理。NB-Agent 是 L0 中的代理框架，通过 REPL 以“代码即动作”的方式运行。

Result: 在 Qwen2.5-7B-Instruct 模型上，该方法将 SimpleQA 的准确率从 30% 提高到 80%，将 HotpotQA 的准确率从 22% 提高到 41%。

Conclusion: L0 系统在事实性问答基准测试中表现出色，表明通过仅使用 RLVR 的强化学习可以开发出强大的问题解决技能。

Abstract: Training large language models (LLMs) to act as autonomous agents for
multi-turn, long-horizon tasks remains significant challenges in scalability
and training efficiency. To address this, we introduce L-Zero (L0), a scalable,
end-to-end training pipeline for general-purpose agents. Featuring a low-cost,
extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier
for applying reinforcement learning in complex environments. We also introduce
NB-Agent, the agent scaffold within L0, which operates in a "code-as-action"
fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality
question-answering benchmarks. Our experiments demonstrate that a base model
can develop robust problem-solving skills using solely Reinforcement Learning
with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method
boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41
%. We have open-sourced the entire L0 system, including our L0 series models,
the NB-Agent, a complete training pipeline, and the corresponding training
recipes on (https://github.com/cmriat/l0).

</details>


### [65] [AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data](https://arxiv.org/abs/2506.23735)
*JiaRu Wu,Mingwei Liu*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have shown remarkable performance on various
tasks, but existing evaluation benchmarks are often static and insufficient to
fully assess their robustness and generalization in realistic scenarios. Prior
work using evolutionary or adversarial data augmentation has improved
evaluation diversity but lacks systematic control over perturbation types and
multi-step complexity, limiting comprehensive robustness analysis. To address
these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for
close-ended tasks such as multi-choice question answering. AutoEvoEval
introduces 22 interpretable atomic evolution operations and supports
multi-round compositions, enabling controlled generation of diverse,
challenging, and realistic test samples. We conduct extensive experiments
addressing four research questions on a broad set of open- and closed-source
LLMs. Our results show that atomic operations cause an average accuracy drop of
7.283\%, with structure-disrupting or misleading semantic edits causing the
largest declines. Model sensitivities vary significantly for the same
perturbation, and combining multiple evolution steps amplifies adversarial
effects by up to 52.932\%. These findings suggest current benchmarks may
overestimate true model generalization and emphasize the need for
evolution-aware robustness evaluation. Code and resources are available at:
https://github.com/SYSUSELab/AutoEvoEval.

</details>


### [66] [Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences](https://arxiv.org/abs/2506.23743)
*Tiziano Labruna,Simone Gallo,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 该研究分析了在不同答案不确定性水平下五种大型语言模型中的位置偏差，并发现当决定哪个选项正确变得困难时，位置偏差会显著增加。


<details>
  <summary>Details</summary>
Motivation: 研究旨在量化和分析在不同答案不确定性水平下五种大型语言模型中的位置偏差。

Method: 研究者重新调整了SQuAD-it数据集，通过添加额外的错误答案选项，并创建了多个版本，这些版本逐渐减少上下文并增加非上下文答案，从而产生从低到高不确定性的数据集。此外，还评估了两个自然更高不确定性的基准测试：(1) WebGPT - 问题对具有不相等的人类分配质量评分，以及(2) Winning Arguments - 模型预测Reddit的r/ChangeMyView交流中更有说服力的论点。在每个数据集中，将“正确”（或更高质量/有说服力）选项的位置系统地翻转（首先放在位置1，然后放在位置2），以计算偏好公平性和位置一致性。

Result: 研究发现，在低不确定性条件下，位置偏差几乎不存在，但当决定哪个选项正确变得困难时，位置偏差会呈指数级增长。

Conclusion: 研究发现，在低不确定性条件下，位置偏差几乎不存在，但当决定哪个选项正确变得困难时，位置偏差会呈指数级增长。

Abstract: Positional bias in binary question answering occurs when a model
systematically favors one choice over another based solely on the ordering of
presented options. In this study, we quantify and analyze positional bias
across five large language models under varying degrees of answer uncertainty.
We re-adapted the SQuAD-it dataset by adding an extra incorrect answer option
and then created multiple versions with progressively less context and more
out-of-context answers, yielding datasets that range from low to high
uncertainty. Additionally, we evaluate two naturally higher-uncertainty
benchmarks: (1) WebGPT - question pairs with unequal human-assigned quality
scores, and (2) Winning Arguments - where models predict the more persuasive
argument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order
of the "correct" (or higher-quality/persuasive) option is systematically
flipped (first placed in position 1, then in position 2) to compute both
Preference Fairness and Position Consistency. We observe that positional bias
is nearly absent under low-uncertainty conditions, but grows exponentially when
it becomes doubtful to decide which option is correct.

</details>


### [67] [Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model](https://arxiv.org/abs/2506.23840)
*Bowen Ding,Yuhan Chen,Futing Wang,Lingfeng Ming,Tao Lin*

Main category: cs.CL

TL;DR: 本文研究了大型推理模型在处理简单任务时的过度思考问题，并提出了DuP-PO算法来提高其令牌效率和推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在解决复杂问题方面表现出色，但面临过度思考的困境。当处理简单任务时，它们往往会产生冗长的响应，这些响应充满了思考标记（例如，wait, however）。这些标记会触发不必要的高级推理行为，如反思和回溯，降低效率。

Method: 提出了一种名为Dual Policy Preference Optimization (DuP-PO)的新算法，包括：(1) 一种保证对有和没有思考标记的响应进行平衡暴露的滚动采样策略；(2) 一种细粒度的优势控制技术，以动态调节目标标记的预测；(3) 一种确保从思考标记中获得稳定梯度贡献的策略塑造方法。

Result: 实验结果表明，DuP-PO在流行的LRM上表现良好，显著提高了其在推理过程中的令牌效率，同时实现了基础模型的优越性能。

Conclusion: DuP-PO在五个流行的数学推理基准测试中表现出色，显著提高了大型推理模型在推理过程中的令牌效率，同时实现了基础模型的优越性能。

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems but face an
overthinking dilemma. When handling simple tasks, they often produce verbose
responses overloaded with thinking tokens (e.g., wait, however). These tokens
trigger unnecessary high-level reasoning behaviors like reflection and
backtracking, reducing efficiency. In this work, our pilot study reveals that
these thinking-token-induced behaviors are not essential for effective
problem-solving and may even hinder correct reasoning within constrained token
budgets. We identify this phenomenon as the thinking trap. To mitigate this
issue, we propose Dual Policy Preference Optimization (DuP-PO), a novel
algorithm featuring: (1) A rollout sampling strategy that guarantees balanced
exposure to responses with and without thinking tokens; (2) A fine-grained
advantage control technique to dynamically regulate the prediction of target
tokens; (3) A policy shaping method ensuring stable gradient contributions from
thinking tokens. Experimental results on five popular math reasoning benchmarks
show that DuP-PO performs well on the popular LRM, which significantly improves
their token efficiency during reasoning, while achieving superior performance
of the base model.

</details>


### [68] [Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It](https://arxiv.org/abs/2506.23864)
*Seyed Mahed Mousavi,Edoardo Cecchinato,Lucia Hornikova,Giuseppe Riccardi*

Main category: cs.CL

TL;DR: 本文对三个常用的推理基准进行了系统审计，发现其中存在设计和评估方法上的缺陷，并指出模型得分的提高可能并非源于真正的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的基准测试可能无法准确反映大型语言模型的真实推理能力，因为它们可能存在设计缺陷和评估方法问题。

Method: 我们使用五种大型语言模型（GPT-{3, 3.5, 4, o1} 和 LLaMA 3.1）作为诊断工具，对三个广泛使用的推理基准进行了系统审计，发现了基准项目和评估方法中的普遍缺陷。通过系统的人工注释和对清理后的基准子集的重新评估，我们发现模型分数的提高往往不是由于表面措辞的变化或推理能力的提升。

Result: 我们发现模型性能对输入的微小变化（如上下文可用性和措辞）非常敏感，这表明高分可能反映了对格式特定线索的对齐，而不是基于输入的一致推理。

Conclusion: 这些发现挑战了当前基于基准的关于大型语言模型推理能力的声明，并突显了评估协议的必要性，这些协议应评估推理作为从可用信息中得出推论的过程，而不是静态输出选择。我们发布了经过审计的数据和评估工具，以支持对模型推理更可解释和诊断性的评估。

Abstract: We conduct a systematic audit of three widely used reasoning benchmarks,
SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark
items and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and
LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic
issues in benchmark design (e.g., duplicated items, ambiguous wording, and
implausible answers), as well as scoring procedures that prioritize output form
over reasoning process. Through systematic human annotation and re-evaluation
on cleaned benchmark subsets, we find that model scores often improve not due
to due to erratic surface wording variations and not to improved reasoning.
Infact, further analyses show that model performance is highly sensitive to
minor input variations such as context availability and phrasing, revealing
that high scores may reflect alignment with format-specific cues rather than
consistent inference based on the input. These findings challenge the validity
of current benchmark-based claims about reasoning in LLMs, and highlight the
need for evaluation protocols that assess reasoning as a process of drawing
inference from available information, rather than as static output selection.
We release audited data and evaluation tools to support more interpretable and
diagnostic assessments of model reasoning.

</details>


### [69] [Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting](https://arxiv.org/abs/2506.23888)
*André de Souza Loureiro,Jorge Valverde-Rebaza,Julieta Noguez,David Escarcega,Ricardo Marcacini*

Main category: cs.CL

TL;DR: 本文提出了一个名为MAPS的框架，通过结合Chain of Thought (CoT)、Self-Reflection和Auto-Prompting技术，提高了大型语言模型在多步骤数学推理中的表现。MAPS采用迭代优化过程，当检测到错误时，自适应自我反思机制会识别并分析错误，生成定制提示以指导修正。实验表明，MAPS在多个基准测试中显著优于标准CoT，并与优化推理的模型达到竞争性结果。此外，MAPS使通用大型语言模型达到与专用推理模型相当的性能水平。虽然更深的反思层可以提高准确性，但也会增加令牌使用和成本。为了平衡这种权衡，MAPS战略性地限制反思深度，确保成本和推理性能之间的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: Recent advancements in Large Language Models (LLMs) have significantly improved their problem-solving capabilities. However, these models still struggle when faced with complex multi-step reasoning tasks.

Method: MAPS employs an iterative refinement process. Initially, the model generates a solution using Chain of Thought (CoT) prompting. When errors are detected, an adaptive self-reflection mechanism identifies and analyzes them, generating tailored prompts to guide corrections. These dynamically adjusted prompts enable the model to iteratively refine its reasoning.

Result: Experiments on four well-established benchmarks across multiple LLMs show that MAPS significantly outperforms standard CoT and achieves competitive results with reasoning-optimized models.

Conclusion: MAPS enables general-purpose LLMs to reach performance levels comparable to specialized reasoning models. While deeper reflection layers improve accuracy, they also increase token usage and costs. To balance this trade-off, MAPS strategically limits reflection depth, ensuring an optimal balance between cost and reasoning performance.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved their problem-solving capabilities. However, these models still
struggle when faced with complex multi-step reasoning tasks. In this paper, we
propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,
a novel approach designed to enhance multi-step mathematical reasoning in LLMs
by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and
Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an
iterative refinement process. Initially, the model generates a solution using
CoT prompting. When errors are detected, an adaptive self-reflection mechanism
identifies and analyzes them, generating tailored prompts to guide corrections.
These dynamically adjusted prompts enable the model to iteratively refine its
reasoning. Experiments on four well-established benchmarks across multiple LLMs
show that MAPS significantly outperforms standard CoT and achieves competitive
results with reasoning-optimized models. In addition, MAPS enables
general-purpose LLMs to reach performance levels comparable to specialized
reasoning models. While deeper reflection layers improve accuracy, they also
increase token usage and costs. To balance this trade-off, MAPS strategically
limits reflection depth, ensuring an optimal balance between cost and reasoning
performance.

</details>


### [70] [The Trilemma of Truth in Large Language Models](https://arxiv.org/abs/2506.23921)
*Germans Savcisens,Tina Eliassi-Rad*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法sAwMIL来评估大型语言模型（LLMs）内部概率知识的真实性，并揭示了多个关于真实性信号的重要发现。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs内部概率知识的真实性，发现现有方法中的假设存在缺陷。

Method: sAwMIL（稀疏感知多实例学习）是一种探测方法，利用LLMs的内部激活将陈述分为真实、虚假和既非真实也非虚假。它基于多实例学习和符合预测。

Result: sAwMIL在16个开源LLMs上进行了评估，并发现了几个关于真实性信号的重要见解，包括真实性信号集中在LLM深度的第三象限、真实与虚假信号不对称、线性探测器在聊天模型中表现更好等。

Conclusion: 这些发现提供了一种可靠的方法来验证LLMs'知道什么以及他们对自己概率内部知识的确定程度。

Abstract: We often attribute human characteristics to large language models (LLMs) and
claim that they "know" certain things. LLMs have an internal probabilistic
knowledge that represents information retained during training. How can we
assess the veracity of this knowledge? We examine two common methods for
probing the veracity of LLMs and discover several assumptions that are flawed.
To address these flawed assumptions, we introduce sAwMIL (short for Sparse
Aware Multiple-Instance Learning), a probing method that utilizes the internal
activations of LLMs to separate statements into true, false, and neither.
sAwMIL is based on multiple-instance learning and conformal prediction. We
evaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including
both default and chat-based variants, as well as on 3 new datasets. Among the
insights we provide are: (1) the veracity signal is often concentrated in the
third quarter of an LLM's depth; (2) truth and falsehood signals are not always
symmetric; (3) linear probes perform better on chat models than on default
models; (4) nonlinear probes may be required to capture veracity signals for
some LLMs with reinforcement learning from human feedback or knowledge
distillation; and (5) LLMs capture a third type of signal that is distinct from
true and false and is neither true nor false. These findings provide a reliable
method for verifying what LLMs "know" and how certain they are of their
probabilistic internal knowledge.

</details>


### [71] [IMPACT: Inflectional Morphology Probes Across Complex Typologies](https://arxiv.org/abs/2506.23929)
*Mohammed J. Saeed,Tommi Vehvilainen,Evgeny Fedoseev,Sevil Caliskan,Tatiana Vodolazova*

Main category: cs.CL

TL;DR: 本文介绍了IMPACT，一个用于评估大型语言模型在屈折形态学方面表现的合成生成评估框架。研究发现，尽管LLMs在英语上表现良好，但在处理其他语言和不常见的形态模式时存在困难，尤其是判断不合法的例子。此外，思维链和思维模型可能会降低性能。研究结果表明，LLMs在处理语言复杂性方面仍有明显不足，需要进一步改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在各种多语言基准测试中取得了显著进展，并且越来越多地用于生成和评估非英语语言中的文本，但它们是否真正理解这些语言的潜在语言复杂性，特别是形态学，仍然不清楚。因此，我们需要一个专门的评估框架来检查LLMs在处理语言复杂性方面的能力。

Method: 我们引入了IMPACT，这是一个合成生成的评估框架，专注于屈折形态学，我们公开发布它，旨在评估跨五种形态丰富的语言（阿拉伯语、俄语、芬兰语、土耳其语和希伯来语）的LLM性能。IMPACT包括单元测试风格的案例，涵盖了共享和语言特定的现象，从基本动词变位（如时态、数、性）到独特的特征，如阿拉伯语的反向性别一致性和芬兰语及土耳其语的元音和谐。

Result: 我们评估了八种多语言LLMs，尽管它们在英语上的表现很强，但在其他语言和不常见的形态模式上表现不佳，尤其是在判断不合法的例子时。我们还发现，思维链和思维模型会降低性能。

Conclusion: 我们的工作揭示了LLMs在处理语言复杂性方面的不足，指出了明显的改进空间。为了支持进一步的研究，我们公开发布了IMPACT框架。

Abstract: Large Language Models (LLMs) have shown significant progress on various
multilingual benchmarks and are increasingly used to generate and evaluate text
in non-English languages. However, while they may produce fluent outputs, it
remains unclear to what extent these models truly grasp the underlying
linguistic complexity of those languages, particularly in morphology. To
investigate this, we introduce IMPACT, a synthetically generated evaluation
framework focused on inflectional morphology, which we publicly release,
designed to evaluate LLM performance across five morphologically rich
languages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes
unit-test-style cases covering both shared and language-specific phenomena,
from basic verb inflections (e.g., tense, number, gender) to unique features
like Arabic's reverse gender agreement and vowel harmony in Finnish and
Turkish. We assess eight multilingual LLMs that, despite strong English
performance, struggle with other languages and uncommon morphological patterns,
especially when judging ungrammatical examples. We also show that Chain of
Thought and Thinking Models can degrade performance. Our work exposes gaps in
LLMs' handling of linguistic complexity, pointing to clear room for
improvement. To support further research, we publicly release the IMPACT
framework.

</details>


### [72] [Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2506.23930)
*Ruhina Tabasshum Prome,Tarikul Islam Tamiti,Anomadarshi Barua*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rapid expansion of social media leads to a marked increase in hate
speech, which threatens personal lives and results in numerous hate crimes.
Detecting hate speech presents several challenges: diverse dialects, frequent
code-mixing, and the prevalence of misspelled words in user-generated content
on social media platforms. Recent progress in hate speech detection is
typically concentrated on high-resource languages. However, low-resource
languages still face significant challenges due to the lack of large-scale,
high-quality datasets. This paper investigates how we can overcome this
limitation via prompt engineering on large language models (LLMs) focusing on
low-resource Bengali language. We investigate six prompting strategies -
zero-shot prompting, refusal suppression, flattering the classifier, multi-shot
prompting, role prompting, and finally our innovative metaphor prompting to
detect hate speech effectively in low-resource languages. We pioneer the
metaphor prompting to circumvent the built-in safety mechanisms of LLMs that
marks a significant departure from existing jailbreaking methods. We
investigate all six different prompting strategies on the Llama2-7B model and
compare the results extensively with three pre-trained word embeddings - GloVe,
Word2Vec, and FastText for three different deep learning models - multilayer
perceptron (MLP), convolutional neural network (CNN), and bidirectional gated
recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in
the low-resource Bengali language, we also evaluate it in another low-resource
language - Hindi, and two high-resource languages - English and German. The
performance of all prompting techniques is evaluated using the F1 score, and
environmental impact factor (IF), which measures CO$_2$ emissions, electricity
usage, and computational time.

</details>


### [73] [Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs](https://arxiv.org/abs/2506.23940)
*Yang Dai,Jianxiang An,Tianwei Lin,Hongyang He,Hongzhe Huang,Wenqiao Zhang,Zheqi Lv,Siliang Tang,Yueting Zhuang*

Main category: cs.CL

TL;DR: 本文提出了一种统一的参数集成框架，用于增强领域专业化多模态大语言模型之间的知识共享。该框架基于兼容性感知参数拼接策略，并引入了领域兼容性评分机制，以实现高效且有效的模型整合。


<details>
  <summary>Details</summary>
Motivation: 尽管知识共享在多模态大语言模型（MLLMs）中具有重要意义，但针对特定领域（如数学或代码）训练的MLLMs之间的知识共享研究仍较为缺乏。为了应对领域专业化MLLMs之间知识的碎片化问题，我们提出了一个统一的参数集成框架。

Method: 我们提出了一种统一的参数集成框架，该框架基于一种新颖的兼容性感知参数拼接（CAPS）策略，利用局部功能属性和全局信息理论信号来指导选择性参数融合。此外，我们引入了一种领域兼容性评分机制，以在激活级别量化专家间的对齐度，并与下游任务效用相关联。

Result: 通过将这种机制扩展到低秩适应层粒度，我们确保了高效的集成并最小化了推理开销。该方法允许最终模型协同异构专业知识，同时保持结构模块化。

Conclusion: 我们的框架在多种多模态基准测试中得到了广泛验证，证明了其有效性，并为构建组合性、领域自适应的MLLMs提供了一条可扩展的路径。

Abstract: Multimodal Large Language Models (MLLMs) have achieved success across various
domains. However, their applicability tends to degrade when confronted with
different types of data inputs, especially for MLLMs that have been fine-tuned
for specific tasks. Despite its importance, the study of knowledge sharing
among domain-specific MLLMs--such as those trained for mathematics or
code--remains largely underexplored. To address the fragmentation of knowledge
across domain-specialized MLLMs, we propose a unified parameter integration
framework that enables modular composition of expert capabilities. Our method
is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,
which leverages both local functional attribution and global
information-theoretic signals to guide selective parameter fusion. By extending
this mechanism to the low-rank adaptation layer granularity, we ensure
efficient integration with minimal inference overhead. Furthermore, we
introduce a domain compatibility scoring mechanism that quantifies inter-expert
alignment at the activation level and correlates with downstream task utility.
This principled fusion protocol allows the final model to synergize
heterogeneous expertise while preserving structural modularity. Extensive
evaluations across diverse multimodal benchmarks validate the effectiveness of
our framework, offering a scalable path toward compositional, domain-adaptive
MLLMs.

</details>


### [74] [Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders](https://arxiv.org/abs/2506.23951)
*Mathis Le Bail,Jérémie Dentan,Davide Buscaldi,Sonia Vanier*

Main category: cs.CL

TL;DR: 本文研究了基于稀疏自编码器（SAE）的可解释性方法在句子分类中的有效性，并提出了一个针对文本分类的新架构，该架构在提取特征的因果性和可解释性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 我们研究了基于SAE的可解释性方法在句子分类领域的有效性，这是一个此类方法尚未广泛探索的领域。

Method: 我们提出了一种针对文本分类的新型基于SAE的架构，利用了专门的分类器头和激活率稀疏损失。

Result: 我们的评估涵盖了两个分类基准和四个微调的Pythia家族LLM，并且我们引入了两个新的度量标准来衡量基于概念的解释的精度。

Conclusion: 我们的架构在提取特征的因果性和可解释性方面有所提高。

Abstract: Sparse Autoencoders (SAEs) have been successfully used to probe Large
Language Models (LLMs) and extract interpretable concepts from their internal
representations. These concepts are linear combinations of neuron activations
that correspond to human-interpretable features. In this paper, we investigate
the effectiveness of SAE-based explainability approaches for sentence
classification, a domain where such methods have not been extensively explored.
We present a novel SAE-based architecture tailored for text classification,
leveraging a specialized classifier head and incorporating an activation rate
sparsity loss. We benchmark this architecture against established methods such
as ConceptShap, Independent Component Analysis, and other SAE-based concept
extraction techniques. Our evaluation covers two classification benchmarks and
four fine-tuned LLMs from the Pythia family. We further enrich our analysis
with two novel metrics for measuring the precision of concept-based
explanations, using an external sentence encoder. Our empirical results show
that our architecture improves both the causality and interpretability of the
extracted features.

</details>


### [75] [TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation](https://arxiv.org/abs/2506.23979)
*Renren Jin,Tianhao Shen,Xinwei Wu,Dan Shi,Haoran Sun,Wuwei Huang,Quandong Wang,Wei Liu,Jian Luan,Bin Wang,Deyi Xiong*

Main category: cs.CL

TL;DR: 本文提出了一个基于结构化分类法的偏好数据生成框架TaP，用于自动化和可扩展地构建跨多种语言的偏好数据集。实验结果表明，使用TaP生成的数据集训练的LLM在性能上优于现有开源数据集训练的LLM，甚至超过了180倍大的开源数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 构建高质量的数据集对于改进大型语言模型（LLMs）遵循指令和与人类偏好和价值观对齐的能力至关重要，但目前可用的数据集主要集中在英语上，并且构建过程资源密集。

Method: 提出了一种基于结构化分类法的偏好数据生成框架TaP，以实现跨多种语言的自动化和可扩展的偏好数据集构建。

Result: 实验结果表明，使用TaP生成的数据集训练的LLM在性能上优于现有开源数据集训练的LLM，甚至超过了180倍大的开源数据集的性能。

Conclusion: 实验结果表明，使用TaP生成的数据集训练的LLM在性能上优于现有开源数据集训练的LLM，甚至超过了180倍大的开源数据集的性能。

Abstract: Conducting supervised fine-tuning and preference fine-tuning on large
language models (LLMs) requires high-quality datasets to improve their ability
to follow instructions and align with human preferences and values. However,
constructing such datasets is resource-intensive, and most available datasets
for supervised and preference fine-tuning are in English. To address these
challenges, we propose the \underline{\textbf{Ta}}xonomy-Guided
\underline{\textbf{P}}reference Data Generation (TaP) framework, which
facilitates automated and scalable construction of preference datasets across
various languages. TaP is grounded in a structured taxonomy that allows
fine-grained control over dataset composition, thereby ensuring both diversity
and comprehensive coverage. We employ TaP-generated datasets to perform
supervised and preference fine-tuning on various LLMs. Experimental results
demonstrate that LLMs trained on TaP-generated datasets outperform those
trained on existing open-source datasets. Remarkably, LLMs trained on
TaP-generated datasets surpass the performance of those trained on an
open-source dataset that is 180 times larger.

</details>


### [76] [Machine Understanding of Scientific Language](https://arxiv.org/abs/2506.23990)
*Dustin Wright*

Main category: cs.CL

TL;DR: 本文旨在通过自然语言处理和机器学习技术，提高对科学文本忠实性的自动识别能力，并提出了一系列方法和资源来实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 科学信息表达了人类对自然的理解，但并非所有科学文本都忠实于基础科学。随着近年来科学文本在网上的激增，能够自动识别科学文本的忠实性已成为一个社会重要问题。

Method: 本文提出了自然语言处理和机器学习的三个领域的贡献：自动事实核查、有限数据学习和科学文本处理。这些贡献包括新方法和资源，用于识别值得检查的主张、对抗性主张生成、多源领域适应、从众包标签学习、引用价值检测、零样本科学事实核查、检测夸张的科学主张以及建模科学传播中的信息变化程度。

Result: 本文提出了新的方法和资源，用于分析和理解科学传播，并展示了这些研究输出在从有限的科学文本中学习以识别误导性科学陈述和生成科学传播过程的新见解方面的有效性。

Conclusion: 本文展示了如何通过研究输出有效从有限的科学文本中学习，以识别误导性的科学陈述，并生成关于科学传播过程的新见解。

Abstract: Scientific information expresses human understanding of nature. This
knowledge is largely disseminated in different forms of text, including
scientific papers, news articles, and discourse among people on social media.
While important for accelerating our pursuit of knowledge, not all scientific
text is faithful to the underlying science. As the volume of this text has
burgeoned online in recent years, it has become a problem of societal
importance to be able to identify the faithfulness of a given piece of
scientific text automatically. This thesis is concerned with the cultivation of
datasets, methods, and tools for machine understanding of scientific language,
in order to analyze and understand science communication at scale. To arrive at
this, I present several contributions in three areas of natural language
processing and machine learning: automatic fact checking, learning with limited
data, and scientific text processing. These contributions include new methods
and resources for identifying check-worthy claims, adversarial claim
generation, multi-source domain adaptation, learning from crowd-sourced labels,
cite-worthiness detection, zero-shot scientific fact checking, detecting
exaggerated scientific claims, and modeling degrees of information change in
science communication. Critically, I demonstrate how the research outputs of
this thesis are useful for effectively learning from limited amounts of
scientific text in order to identify misinformative scientific statements and
generate new insights into the science communication process

</details>


### [77] [Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning](https://arxiv.org/abs/2506.23998)
*Seungjun Yi,Joakim Nguyen,Huimin Xu,Terence Lim,Andrew Well,Mia Markey,Ying Ding*

Main category: cs.CL

TL;DR: 本文提出了一种完全自动化的大型语言模型（LLM）管道，用于对临床叙述进行端到端主题分析，从而消除手动编码或完整转录本审查的需要。该系统采用了一种新的多代理框架，其中专门的LLM代理承担角色以提高主题质量和与人类分析的一致性，并可选择集成基于人类反馈的强化学习（RLHF）以提高主题相关性。


<details>
  <summary>Details</summary>
Motivation: 先天性心脏病（CHD）带来了复杂的、终身的挑战，通常在传统的临床指标中被低估。虽然非结构化叙述提供了关于患者和护理人员经验的丰富见解，但手动主题分析（TA）仍然是劳动密集型且不可扩展的。

Method: 我们提出了一种完全自动化的大型语言模型（LLM）管道，该管道执行端到端的主题分析（TA）临床叙述，消除了手动编码或完整转录本审查的需要。我们的系统采用了一种新颖的多代理框架，其中专门的LLM代理承担角色以提高主题质量和与人类分析的一致性。为了进一步提高主题的相关性，我们可选择集成基于人类反馈的强化学习（RLHF）。

Result: 我们的系统通过使用多代理框架和可选的基于人类反馈的强化学习（RLHF），实现了对临床叙述的端到端主题分析，从而支持大规模定性数据的可扩展、以患者为中心的分析，并允许LLM针对特定的临床环境进行微调。

Conclusion: 我们的系统通过使用多代理框架和可选的基于人类反馈的强化学习（RLHF），实现了对临床叙述的端到端主题分析，从而支持大规模定性数据的可扩展、以患者为中心的分析，并允许LLM针对特定的临床环境进行微调。

Abstract: Congenital heart disease (CHD) presents complex, lifelong challenges often
underrepresented in traditional clinical metrics. While unstructured narratives
offer rich insights into patient and caregiver experiences, manual thematic
analysis (TA) remains labor-intensive and unscalable. We propose a fully
automated large language model (LLM) pipeline that performs end-to-end TA on
clinical narratives, which eliminates the need for manual coding or full
transcript review. Our system employs a novel multi-agent framework, where
specialized LLM agents assume roles to enhance theme quality and alignment with
human analysis. To further improve thematic relevance, we optionally integrate
reinforcement learning from human feedback (RLHF). This supports scalable,
patient-centered analysis of large qualitative datasets and allows LLMs to be
fine-tuned for specific clinical contexts.

</details>


### [78] [Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective](https://arxiv.org/abs/2506.24006)
*Anselm R. Strohmaier,Wim Van Dooren,Kathrin Seßler,Brian Greer,Lieven Verschaffel*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型在数学应用题上的表现，发现它们虽然在解决表面问题上表现优异，但在理解现实世界背景方面仍有不足，这可能限制了它们作为教学工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（如ChatGPT）的进步，人们开始思考如何将它们整合到教育中。其中一个希望是它们可以支持数学学习，包括解决应用题。然而，它们的真实能力以及对课堂的影响仍然不清楚。

Method: 我们进行了一项范围回顾，包括技术概述、对研究中使用的应用题的系统回顾以及对大型语言模型在数学应用题上的最新实证评估。

Result: 我们的评估显示，最近的大型语言模型在解决s-problems方面具有接近完美的准确性，包括在PISA的20个问题上获得满分。然而，它们在处理现实世界背景有问题或不合理的题目时仍表现出弱点。

Conclusion: 我们基于三个方面认为，大型语言模型已经掌握了表面的解题过程，但并不能理解应用题，这可能限制了它们作为数学课堂中的教学工具的价值。

Abstract: The progress of Large Language Models (LLMs) like ChatGPT raises the question
of how they can be integrated into education. One hope is that they can support
mathematics learning, including word-problem solving. Since LLMs can handle
textual input with ease, they appear well-suited for solving mathematical word
problems. Yet their real competence, whether they can make sense of the
real-world context, and the implications for classrooms remain unclear. We
conducted a scoping review from a mathematics-education perspective, including
three parts: a technical overview, a systematic review of word problems used in
research, and a state-of-the-art empirical evaluation of LLMs on mathematical
word problems. First, in the technical overview, we contrast the
conceptualization of word problems and their solution processes between LLMs
and students. In computer-science research this is typically labeled
mathematical reasoning, a term that does not align with usage in mathematics
education. Second, our literature review of 213 studies shows that the most
popular word-problem corpora are dominated by s-problems, which do not require
a consideration of realities of their real-world context. Finally, our
evaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems
shows that most recent LLMs solve these s-problems with near-perfect accuracy,
including a perfect score on 20 problems from PISA. LLMs still showed
weaknesses in tackling problems where the real-world context is problematic or
non-sensical. In sum, we argue based on all three aspects that LLMs have
mastered a superficial solution process but do not make sense of word problems,
which potentially limits their value as instructional tools in mathematics
classrooms.

</details>


### [79] [EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations](https://arxiv.org/abs/2506.24016)
*Hyunjong Kim,Sangyeop Kim,Jongheon Jeong,Yeongjae Cho,Sungzoon Cho*

Main category: cs.CL

TL;DR: 本文提出了一种无参考的评估指标EXPERT，该指标基于流畅性、相关性和描述性三个基本标准提供结构化的解释。通过构建大规模高质量结构化解释数据集，我们开发了一个两阶段的评估模板，以有效监督视觉语言模型进行评分和解释生成。EXPERT在基准数据集上取得了最先进的结果，并通过全面的人类评估验证了其解释质量显著优于现有指标。


<details>
  <summary>Details</summary>
Motivation: Recent advances in large language models and vision-language models have led to growing interest in explainable evaluation metrics for image captioning. However, these metrics generate explanations without standardized criteria, and the overall quality of the generated explanations remains unverified.

Method: We propose EXPERT, a reference-free evaluation metric that provides structured explanations based on three fundamental criteria: fluency, relevance, and descriptiveness. By constructing large-scale datasets of high-quality structured explanations, we develop a two-stage evaluation template to effectively supervise a vision-language model for both scoring and explanation generation.

Result: EXPERT achieves state-of-the-art results on benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation.

Conclusion: EXPERT achieves state-of-the-art results on benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation.

Abstract: Recent advances in large language models and vision-language models have led
to growing interest in explainable evaluation metrics for image captioning.
However, these metrics generate explanations without standardized criteria, and
the overall quality of the generated explanations remains unverified. In this
paper, we propose EXPERT, a reference-free evaluation metric that provides
structured explanations based on three fundamental criteria: fluency,
relevance, and descriptiveness. By constructing large-scale datasets of
high-quality structured explanations, we develop a two-stage evaluation
template to effectively supervise a vision-language model for both scoring and
explanation generation. EXPERT achieves state-of-the-art results on benchmark
datasets while providing significantly higher-quality explanations than
existing metrics, as validated through comprehensive human evaluation. Our code
and datasets are available at https://github.com/hjkim811/EXPERT.

</details>


### [80] [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068)
*Ian R. McKenzie,Oskar J. Hollinsworth,Tom Tseng,Xander Davies,Stephen Casper,Aaron D. Tucker,Robert Kirk,Adam Gleave*

Main category: cs.CL

TL;DR: 本文分析了AI系统的安全防御管道，发现了一种更有效的分类器，并提出了一个高效的攻击方法，同时提供了缓解措施。


<details>
  <summary>Details</summary>
Motivation: 当前对这些防御管道的安全性了解不足，因此需要进行评估和攻击以填补这一空白。

Method: 我们开发并红队测试了一个开源防御管道，首先发现了一种新的少样本提示输入和输出分类器，其次引入了STaged AttaCK (STACK) 方法，并在转移设置中评估了STACK。

Result: 新分类器在三个攻击和两个数据集上优于现有的ShieldGemma模型，将攻击成功率降低到0%；STACK方法在黑盒攻击中实现了71%的攻击成功率，并在转移设置中实现了33%的攻击成功率。

Conclusion: 我们建议开发者可以使用特定的缓解措施来阻止分阶段攻击。

Abstract: Frontier AI developers are relying on layers of safeguards to protect against
catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus
model using one such defense pipeline, and other frontier developers including
Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the
security of such pipelines is unclear, with limited prior work evaluating or
attacking these pipelines. We address this gap by developing and red-teaming an
open-source defense pipeline. First, we find that a novel few-shot-prompted
input and output classifier outperforms state-of-the-art open-weight safeguard
model ShieldGemma across three attacks and two datasets, reducing the attack
success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,
we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on
ClearHarm in a black-box attack against the few-shot-prompted classifier
pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%
ASR, providing initial evidence that it is feasible to design attacks with no
access to the target pipeline. We conclude by suggesting specific mitigations
that developers could use to thwart staged attacks.

</details>


### [81] [On the Predictive Power of Representation Dispersion in Language Models](https://arxiv.org/abs/2506.24106)
*Yanhong Li,Ming Li,Karen Livescu,Jiawei Zhou*

Main category: cs.CL

TL;DR: 本研究揭示了语言模型的表示分散性与其文本预测能力之间的关系，并提出了一种无需标记数据即可优化模型性能的方法。


<details>
  <summary>Details</summary>
Motivation: 我们希望探索语言模型的表示分散性与其文本预测能力之间的关系，并寻找一种无需标记数据即可优化模型性能的方法。

Method: 我们通过计算隐藏向量之间的平均余弦距离来衡量表示的分散性，并将其与困惑度进行相关性分析。此外，我们还引入了一个简单的推离目标来增加分散性。

Result: 我们发现表示的分散性与困惑度之间存在强负相关关系，并且可以通过增加分散性来提高模型性能。此外，我们还展示了如何利用分散性进行模型选择和最佳表示的识别。

Conclusion: 我们的研究表明，语言模型的文本预测能力与其嵌入空间的广度密切相关。通过增加表示的分散性，可以在不使用标记数据的情况下实现一系列实际任务的优化，并且可以提高困惑度。

Abstract: We show that a language model's ability to predict text is tightly linked to
the breadth of its embedding space: models that spread their contextual
representations more widely tend to achieve lower perplexity. Concretely, we
find that representation dispersion - the average pairwise cosine distance
among hidden vectors - strongly and negatively correlates with perplexity
across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,
news, scientific abstracts). Beyond illustrating this link, we show how
dispersion can be leveraged for a range of practical tasks without requiring
labeled data. First, measuring dispersion on unlabeled text allows us to
predict downstream accuracy in new domains, offering a data-efficient tool for
model selection. Next, we find that identifying layers with higher dispersion
pinpoints the best representations for retrieval-based methods such as kNN-LM,
bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple
push-away objective into training, which increases dispersion in both
single-domain and cross-domain scenarios and directly improves perplexity in
each.

</details>


### [82] [Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models](https://arxiv.org/abs/2506.24117)
*David M. Smiley*

Main category: cs.CL

TL;DR: 本研究评估了预训练的变压器语言模型在检测希伯来圣经中的文本平行方面的潜力，并发现E5和AlephBERT在这一任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 在圣经希伯来语中识别平行段落是圣经学术的基础，用于揭示互文关系。传统方法依赖于手动比较，这既费时又容易出错。

Method: 本研究评估了基于变压器的预训练语言模型（包括E5、AlephBERT、MPNet和LaBSE）在检测希伯来圣经中的文本平行方面的潜力。通过余弦相似性和Wasserstein距离测量，评估了每个模型生成区分平行和非平行段落的词嵌入的能力。

Result: E5和AlephBERT显示出显著的前景，其中E5在平行检测方面表现出色，而AlephBERT在非平行区分方面表现更强。

Conclusion: 这些发现表明，预训练模型可以提高检测古代文本中互文平行的效率和准确性，并暗示了在古代语言研究中的更广泛应用。

Abstract: Identifying parallel passages in biblical Hebrew is foundational in biblical
scholarship for uncovering intertextual relationships. Traditional methods rely
on manual comparison, which is labor-intensive and prone to human error. This
study evaluates the potential of pre-trained transformer-based language models,
including E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in
the Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings
and Chronicles, I assessed each model's capability to generate word embeddings
that delineate parallel from non-parallel passages. Utilizing cosine similarity
and Wasserstein Distance measures, I found that E5 and AlephBERT show
significant promise, with E5 excelling in parallel detection and AlephBERT
demonstrating stronger non-parallel differentiation. These findings indicate
that pre-trained models can enhance the efficiency and accuracy of detecting
intertextual parallels in ancient texts, suggesting broader applications for
ancient language studies.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [83] [Reachability in symmetric VASS](https://arxiv.org/abs/2506.23578)
*Łukasz Kamiński,Sławomir Lasota*

Main category: cs.FL

TL;DR: 本文研究了对称向量加法系统（VASS）中的可达性问题，发现当使用对称群时，可达性问题可以在PSPACE中解决，而一般VASS具有更高的复杂度。我们还考虑了其他群结构，并评估了复杂度的提升。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决数据VASS中可达性问题的开放状态，并通过分析不同群结构下的复杂度来提高对这一问题的理解。

Method: 本文通过分析对称向量加法系统（VASS）中的可达性问题，研究了不同群结构下的复杂度。我们特别关注了对称群、交替群和循环群的情况，并评估了当群由平凡群和对称群组合时的复杂度提升。

Result: 本文证明了在对称群的情况下，可达性问题可以在PSPACE中解决，而一般的VASS具有Ackermannian复杂度。此外，我们还考虑了其他群，如交替群和循环群，并估计了当群由平凡群和对称群组合时的复杂度提升。

Conclusion: 本文研究了对称向量加法系统（VASS）中的可达性问题，其中转换在坐标排列的群下不变。我们展示了在对称群的情况下，可达性问题可以在PSPACE中解决，而一般的VASS具有Ackermannian复杂度。此外，我们还考虑了其他群，如交替群和循环群，并估计了当群由平凡群和对称群组合时复杂度的提升。

Abstract: We investigate the reachability problem in symmetric vector addition systems
with states (VASS), where transitions are invariant under a group of
permutations of coordinates. One extremal case, the trivial groups, yields
general VASS. In another extremal case, the symmetric groups, we show that the
reachability problem can be solved in PSPACE, regardless of the dimension of
input VASS (to be contrasted with Ackermannian complexity in general VASS). We
also consider other groups, in particular alternating and cyclic ones.
Furthermore, motivated by the open status of the reachability problem in data
VASS, we estimate the gain in complexity when the group arises as a combination
of the trivial and symmetric groups.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [84] [PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection](https://arxiv.org/abs/2506.22783)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Sriram Vishwanath,Sandeep P. Chinchali*

Main category: cs.CV

TL;DR: 本研究揭示了现有深度伪造数据集无法欺骗人类感知，提出了PhonemeFake（PF）攻击方法，通过语言推理操纵关键语音段，显著降低人类感知和基准准确率。我们还发布了一个易于使用的PF数据集和一个双层深度伪造段检测模型，实验结果表明该模型在降低EER和提高速度方面表现优异，具有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造数据集无法欺骗人类感知，而真实深度伪造攻击却能影响公众讨论。因此，需要更真实的深度伪造攻击向量。

Method: 我们引入了PhonemeFake（PF），一种利用语言推理操纵关键语音段的深度伪造攻击。此外，我们还开源了一个双层深度伪造段检测模型，该模型自适应地优先考虑被篡改区域的计算资源。

Result: 我们的实验结果表明，我们的检测模型将EER降低了91%，同时实现了高达90%的速度提升，计算开销最小，并且在精确定位方面超越了现有模型，提供了一种可扩展的解决方案。

Conclusion: 我们的研究揭示了现有深度伪造数据集无法欺骗人类感知，而真实深度伪造攻击却能影响公众讨论。这突显了需要更真实的深度伪造攻击向量。我们引入了PhonemeFake（PF），一种利用语言推理操纵关键语音段的深度伪造攻击，显著减少了人类感知高达42%，基准准确率高达94%。我们发布了易于使用的PF数据集，并开源了一个双层深度伪造段检测模型，该模型自适应地优先考虑被篡改区域的计算资源。我们的实验结果表明，我们的检测模型将EER降低了91%，同时实现了高达90%的速度提升，计算开销最小，并且在精确定位方面超越了现有模型，提供了一种可扩展的解决方案。

Abstract: Deepfake (DF) attacks pose a growing threat as generative models become
increasingly advanced. However, our study reveals that existing DF datasets
fail to deceive human perception, unlike real DF attacks that influence public
discourse. It highlights the need for more realistic DF attack vectors. We
introduce PhonemeFake (PF), a DF attack that manipulates critical speech
segments using language reasoning, significantly reducing human perception by
up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF
dataset on HuggingFace and open-source bilevel DF segment detection model that
adaptively prioritizes compute on manipulated regions. Our extensive
experiments across three known DF datasets reveal that our detection model
reduces EER by 91% while achieving up to 90% speed-up, with minimal compute
overhead and precise localization beyond existing models as a scalable
solution.

</details>


### [85] [Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval](https://arxiv.org/abs/2506.22864)
*Li-Cheng Shen,Jih-Kang Hsieh,Wei-Hua Li,Chu-Song Chen*

Main category: cs.CV

TL;DR: 本文介绍了Mask-aware TIR（MaTIR）任务，通过两阶段框架提高文本到图像检索和对象分割的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像检索（TIR）方法主要基于全图标题，缺乏可解释性。同时，参考表达分割（RES）在应用于大型图像集合时计算成本高。为了弥合这一差距，我们引入了Mask-aware TIR（MaTIR），这是一个新的任务，统一了TIR和RES，要求高效的图像搜索和准确的对象分割。

Method: 我们提出了一种两阶段框架，包括第一阶段的分割感知图像检索和第二阶段使用多模态大语言模型（MLLM）进行重新排序和对象定位。

Result: 我们在COCO和D$^3$数据集上评估了我们的方法，结果表明相比之前的方法有显著提升。

Conclusion: 我们的方法在COCO和D$^3$数据集上显著提高了检索准确性和分割质量。

Abstract: Text-to-image retrieval (TIR) aims to find relevant images based on a textual
query, but existing approaches are primarily based on whole-image captions and
lack interpretability. Meanwhile, referring expression segmentation (RES)
enables precise object localization based on natural language descriptions but
is computationally expensive when applied across large image collections. To
bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies
TIR and RES, requiring both efficient image search and accurate object
segmentation. To address this task, we propose a two-stage framework,
comprising a first stage for segmentation-aware image retrieval and a second
stage for reranking and object grounding with a multimodal large language model
(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract
region-level embeddings offline at first, enabling effective and scalable
online retrieval. Secondly, MLLM is used to refine retrieval rankings and
generate bounding boxes, which are matched to segmentation masks. We evaluate
our approach on COCO and D$^3$ datasets, demonstrating significant improvements
in both retrieval accuracy and segmentation quality over previous methods.

</details>


### [86] [MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering](https://arxiv.org/abs/2506.22900)
*Mai A. Shaaban,Tausifa Jan Saleem,Vijay Ram Papineni,Mohammad Yaqub*

Main category: cs.CV

TL;DR: MOTOR is a new approach for MedVQA that improves accuracy by considering both textual and visual information in retrieval and re-ranking.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to retrieval-augmented generation for MedVQA neglect the visual or multimodal context, which is crucial for medical diagnosis.

Method: MOTOR is a novel multimodal retrieval and re-ranking approach that leverages grounded captions and optimal transport to capture the underlying relationships between the query and the retrieved context based on textual and visual information.

Result: MOTOR identifies more clinically relevant contexts to augment the VLM input, leading to higher accuracy on MedVQA datasets.

Conclusion: MOTOR achieves higher accuracy on MedVQA datasets, outperforming state-of-the-art methods by an average of 6.45%.

Abstract: Medical visual question answering (MedVQA) plays a vital role in clinical
decision-making by providing contextually rich answers to image-based queries.
Although vision-language models (VLMs) are widely used for this task, they
often generate factually incorrect answers. Retrieval-augmented generation
addresses this challenge by providing information from external sources, but
risks retrieving irrelevant context, which can degrade the reasoning
capabilities of VLMs. Re-ranking retrievals, as introduced in existing
approaches, enhances retrieval relevance by focusing on query-text alignment.
However, these approaches neglect the visual or multimodal context, which is
particularly crucial for medical diagnosis. We propose MOTOR, a novel
multimodal retrieval and re-ranking approach that leverages grounded captions
and optimal transport. It captures the underlying relationships between the
query and the retrieved context based on textual and visual information.
Consequently, our approach identifies more clinically relevant contexts to
augment the VLM input. Empirical analysis and human expert evaluation
demonstrate that MOTOR achieves higher accuracy on MedVQA datasets,
outperforming state-of-the-art methods by an average of 6.45%. Code is
available at https://github.com/BioMedIA-MBZUAI/MOTOR.

</details>


### [87] [MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings](https://arxiv.org/abs/2506.23115)
*Haonan Chen,Hong Liu,Yuping Luo,Liang Wang,Nan Yang,Furu Wei,Zhicheng Dou*

Main category: cs.CV

TL;DR: MoCa is a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. It addresses the limitations of current approaches by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments show that MoCa achieves new state-of-the-art results on MMEB and ViDoRe-v2 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data.

Method: MoCa is a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment.

Result: MoCa addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.

Conclusion: MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.

Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs),
have shown promise in various tasks. However, current approaches face three key
limitations: the use of causal attention in VLM backbones is suboptimal for
embedding tasks; scalability issues due to reliance on high-quality labeled
paired data for contrastive learning; and limited diversity in training
objectives and data. To address these issues, we propose MoCa, a two-stage
framework for transforming pre-trained VLMs into effective bidirectional
multimodal embedding models. The first stage, Modality-aware Continual
Pre-training, introduces a joint reconstruction objective that simultaneously
denoises interleaved text and image inputs, enhancing bidirectional
context-aware reasoning. The second stage, Heterogeneous Contrastive
Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple
image-caption pairs to enhance generalization and alignment. Our method
addresses the stated limitations by introducing bidirectional attention through
continual pre-training, scaling effectively with massive unlabeled datasets via
joint reconstruction objectives, and utilizing diverse multimodal data for
enhanced representation robustness. Experiments demonstrate that MoCa
consistently improves performance across MMEB and ViDoRe-v2 benchmarks,
achieving new state-of-the-art results, and exhibits strong scalability with
both model size and training data on MMEB.

</details>


### [88] [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/abs/2506.23219)
*Jie Feng,Shengyuan Wang,Tianhui Liu,Yanxin Xi,Yong Li*

Main category: cs.CV

TL;DR: 本文介绍了UrbanLLaVA，这是一种多模态大型语言模型，旨在同时处理四种类型的数据，并在与通用MLLMs相比的多样化城市任务中实现强大的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的方法往往专注于特定的数据类型，并且在城市领域缺乏一个统一的框架来全面处理它们。多模态大语言模型（MLLMs）的成功为克服这一限制提供了有希望的机会。

Method: 我们提出了一个分阶段的训练框架，将空间推理增强与领域知识学习解耦，从而提高UrbanLLaVA在各种城市任务中的兼容性和下游性能。此外，我们还扩展了现有的城市研究基准来评估MLLM在广泛的城市任务中的表现。

Result: 实验结果表明，UrbanLLaVA在单模态任务和复杂的跨模态任务中都优于开源和专有MLLM，并且在不同城市中表现出强大的泛化能力。

Conclusion: 实验结果表明，UrbanLLaVA在单模态任务和复杂的跨模态任务中都优于开源和专有MLLM，并且在不同城市中表现出强大的泛化能力。

Abstract: Urban research involves a wide range of scenarios and tasks that require the
understanding of multi-modal data. Current methods often focus on specific data
types and lack a unified framework in urban field for processing them
comprehensively. The recent success of multi-modal large language models
(MLLMs) presents a promising opportunity to overcome this limitation. In this
paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model
designed to process these four types of data simultaneously and achieve strong
performance across diverse urban tasks compared with general MLLMs. In
$\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset
encompassing both single-modal and cross-modal urban data, spanning from
location view to global view of urban environment. Additionally, we propose a
multi-stage training framework that decouples spatial reasoning enhancement
from domain knowledge learning, thereby improving the compatibility and
downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks.
Finally, we also extend existing benchmark for urban research to assess the
performance of MLLMs across a wide range of urban tasks. Experimental results
from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms
open-source and proprietary MLLMs in both single-modal tasks and complex
cross-modal tasks and shows robust generalization abilities across cities.
Source codes and data are openly accessible to the research community via
https://github.com/tsinghua-fib-lab/UrbanLLaVA.

</details>


### [89] [Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](https://arxiv.org/abs/2506.23714)
*Md Moinul Islam,Sofoklis Kakouros,Janne Heikkilä,Mourad Oussalah*

Main category: cs.CV

TL;DR: 本文提出了一种行为感知的多模态视频摘要框架，通过结合文本、音频和视觉线索来生成时间戳对齐的摘要，并展示了其在文本和视频评估指标上的显著改进。


<details>
  <summary>Details</summary>
Motivation: 随着教育、专业和社会领域视频内容的增加，需要超越传统单模态方法的有效摘要技术。

Method: 本文提出了一种行为感知的多模态视频摘要框架，该框架结合了文本、音频和视觉线索以生成时间戳对齐的摘要。通过提取韵律特征、文本线索和视觉指标，该框架识别出语义和情感重要的时刻。

Result: 实验结果表明，与传统的提取方法（如Edmundson方法）相比，所提出的框架在文本和视频评估指标上都有显著改进。文本指标显示ROUGE-1从0.4769增加到0.7929，BERTScore从0.9152增加到0.9536，而在视频评估中，我们的框架将F1分数提高了近23%。

Conclusion: 研究结果强调了多模态整合在生成全面且行为相关的视频摘要方面的潜力。

Abstract: The increasing volume of video content in educational, professional, and
social domains necessitates effective summarization techniques that go beyond
traditional unimodal approaches. This paper proposes a behaviour-aware
multimodal video summarization framework that integrates textual, audio, and
visual cues to generate timestamp-aligned summaries. By extracting prosodic
features, textual cues and visual indicators, the framework identifies
semantically and emotionally important moments. A key contribution is the
identification of bonus words, which are terms emphasized across multiple
modalities and used to improve the semantic relevance and expressive clarity of
the summaries. The approach is evaluated against pseudo-ground truth (pGT)
summaries generated using LLM-based extractive method. Experimental results
demonstrate significant improvements over traditional extractive method, such
as the Edmundson method, in both text and video-based evaluation metrics.
Text-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore
from 0.9152 to 0.9536, while in video-based evaluation, our proposed framework
improves F1-Score by almost 23%. The findings underscore the potential of
multimodal integration in producing comprehensive and behaviourally informed
video summaries.

</details>


### [90] [Ella: Embodied Social Agents with Lifelong Memory](https://arxiv.org/abs/2506.24019)
*Hongxin Zhang,Zheyuan Zhang,Zeyuan Wang,Zunzhe Zhang,Lixing Fang,Qinhong Zhou,Chuang Gan*

Main category: cs.CV

TL;DR: Ella是一个能够在3D开放世界中进行终身学习的具身社交代理，它利用结构化的长期多模态记忆系统来存储、更新和检索信息，从而实现决策、计划活动、建立社会关系和自主进化。实验表明，Ella能够与其他代理有效合作以实现目标，展示出其通过观察和社会互动学习的能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在3D开放世界中进行终身学习的具身社交代理，使其能够通过日常视觉观察和社会互动积累经验和知识。

Method: Ella的核心是一种结构化的长期多模态记忆系统，包括以名称为中心的语义记忆和时空情景记忆。通过将这种终身记忆系统与基础模型集成，Ella能够检索相关信息以进行决策、计划日常活动、建立社会关系，并在开放世界中与其他智能体共存并自主进化。

Result: 实验结果表明，Ella能够很好地影响、领导和与其他代理合作以实现目标，展示了其通过观察和社会互动有效学习的能力。

Conclusion: 我们的研究突显了将结构化记忆系统与基础模型结合的变革潜力，以推动具身智能的发展。

Abstract: We introduce Ella, an embodied social agent capable of lifelong learning
within a community in a 3D open world, where agents accumulate experiences and
acquire knowledge through everyday visual observations and social interactions.
At the core of Ella's capabilities is a structured, long-term multimodal memory
system that stores, updates, and retrieves information effectively. It consists
of a name-centric semantic memory for organizing acquired knowledge and a
spatiotemporal episodic memory for capturing multimodal experiences. By
integrating this lifelong memory system with foundation models, Ella retrieves
relevant information for decision-making, plans daily activities, builds social
relationships, and evolves autonomously while coexisting with other intelligent
beings in the open world. We conduct capability-oriented evaluations in a
dynamic 3D open world where 15 agents engage in social activities for days and
are assessed with a suite of unseen controlled evaluations. Experimental
results show that Ella can influence, lead, and cooperate with other agents
well to achieve goals, showcasing its ability to learn effectively through
observation and social interaction. Our findings highlight the transformative
potential of combining structured memory systems with foundation models for
advancing embodied intelligence. More videos can be found at
https://umass-embodied-agi.github.io/Ella/.

</details>


### [91] [MotionGPT3: Human Motion as a Second Modality](https://arxiv.org/abs/2506.24086)
*Bingfan Zhu,Biao Jiang,Sunyi Wang,Shixiang Tang,Tao Chen,Linjie Luo,Youyi Zheng,Xin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为MotionGPT3的双模态运动-语言模型，旨在解决运动理解和生成任务中的两个核心挑战。通过引入运动变分自编码器和扩散头，该模型实现了有效的跨模态交互和高效的多模态扩展训练，同时保持了强大的语言能力。


<details>
  <summary>Details</summary>
Motivation: 尽管最近在多模态模型方面取得了进展，展示了统一理解和生成的能力和机会，但统一运动-语言模型的发展仍缺乏探索。为了使这些模型具备高保真的人类运动，必须解决两个核心挑战：一是连续运动模态与离散表示之间的重建差距，二是统一训练期间语言智能的退化。

Method: 我们提出了MotionGPT3，这是一个双模态运动-语言模型，将人类运动视为第二种模态，通过单独的模型参数解耦运动建模，并实现有效的跨模态交互和高效的多模态扩展训练。文本分支保留了预训练语言模型的原始结构和参数，而新的运动分支则通过共享注意力机制进行集成，使两种模态之间能够双向信息流动。我们首先使用运动变分自编码器（VAE）将原始人类运动编码为潜在表示。基于这个连续潜在空间，运动分支直接从中间隐藏状态预测运动潜在表示，绕过了离散标记化。

Result: 广泛的实验表明，我们的方法在运动理解和生成任务上实现了有竞争力的性能，同时保持了强大的语言能力，建立了统一的双模态运动扩散框架。

Conclusion: 我们的方法在运动理解和生成任务上实现了有竞争力的性能，同时保持了强大的语言能力，在自回归框架内建立了统一的双模态运动扩散框架。

Abstract: Though recent advances in multimodal models have demonstrated strong
capabilities and opportunities in unified understanding and generation, the
development of unified motion-language models remains underexplored. To enable
such models with high-fidelity human motion, two core challenges must be
addressed. The first is the reconstruction gap between the continuous motion
modality and discrete representation in an autoregressive manner, and the
second is the degradation of language intelligence during unified training.
Inspired by the mixture of experts, we propose MotionGPT3, a bimodal
motion-language model that treats human motion as a second modality, decoupling
motion modeling via separate model parameters and enabling both effective
cross-modal interaction and efficient multimodal scaling training. To preserve
language intelligence, the text branch retains the original structure and
parameters of the pretrained language model, while a new motion branch is
integrated via a shared attention mechanism, enabling bidirectional information
flow between two modalities. We first employ a motion Variational Autoencoder
(VAE) to encode raw human motion into latent representations. Based on this
continuous latent space, the motion branch predicts motion latents directly
from intermediate hidden states using a diffusion head, bypassing discrete
tokenization. Extensive experiments show that our approach achieves competitive
performance on both motion understanding and generation tasks while preserving
strong language capabilities, establishing a unified bimodal motion diffusion
framework within an autoregressive manner.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Main category: cs.AI

TL;DR: 本文提出了MARBLE，这是一个具有挑战性的多模态推理基准，旨在评估多模态语言模型的能力。实验结果表明，当前的MLLMs在MARBLE上的表现较差，表明多模态推理仍然是一个挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的推理基准主要关注文本推理，或者使用可以直接从非文本模态中检索信息的多模态问题。因此，多模态领域中的复杂推理仍然理解不足。本文旨在提出一个具有挑战性的多模态推理基准，以评估多模态语言模型的能力。

Method: 本文提出了MARBLE，这是一个多模态推理基准，包含两个具有挑战性的任务M-Portal和M-Cube，要求在空间、视觉和物理约束下制定和理解多步骤计划。

Result: 当前的MLLMs在MARBLE上的表现较差，所有12个先进的模型在M-Portal上获得接近随机的性能，在M-Cube上得分为0%。只有在简化的子任务中，一些模型才能超过随机基线，这表明复杂的推理仍然是现有MLLMs的挑战。此外，本文还表明感知仍然是一个瓶颈，MLLMs有时无法从视觉输入中提取信息。

Conclusion: MARBLE是一个具有挑战性的多模态推理基准，旨在评估多模态语言模型（MLLMs）在处理复杂多模态问题和环境时的逐步推理能力。实验结果表明，当前的MLLMs在MARBLE上的表现较差，表明多模态推理仍然是一个挑战。通过揭示MLLMs的局限性，希望MARBLE能推动下一代能够跨多模态推理步骤进行推理和规划的模型的发展。

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [93] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben,Gayathri Ganesh Lakshmy,Srijith Radhakrishnan,Siddhant Arora,Shinji Watanabe*

Main category: cs.AI

TL;DR: AURA 是一个开放源代码的语音原生助手，能够通过动态工具调用和多轮对话完成复杂的、以目标为导向的任务。它在多个基准测试中表现出色，并且在人类评估中显示出高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管在语言和语音技术方面取得了进展，但没有开源系统能够实现完整的语音到语音、多轮对话，并集成工具使用和代理推理。因此，需要一种新的系统来解决这一问题。

Method: AURA 结合了开放权重的 ASR、TTS 和 LLMs，在级联管道中运行，并支持日历预订、联系人查找、网络搜索和电子邮件等工具。其模块化设计允许使用自然语言提示和动作类轻松集成新工具。

Result: AURA 在 VoiceBench 上得分 92.75% on OpenBookQA，超过所有开放权重系统，并接近 GPT-4o；在 AlpacaEval 上得分为 4.39，与其他开放权重系统相当。人类评估显示在复杂、多轮语音任务中的任务成功率为 90%。

Conclusion: AURA 是一个开放源代码的语音原生助手，能够通过动态工具调用和多轮对话完成复杂的、以目标为导向的任务。它在 VoiceBench 上的表现优于所有开放权重系统，并且在人类评估中显示出 90% 的任务成功率。

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [94] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita,Yongjin Yang,Mrinmaya Sachan,Giorgia Ramponi,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.AI

TL;DR: 本文研究了多代理大型语言模型系统中的成本制裁问题，通过改编行为经济学中的公共物品游戏与制度选择，观察不同大型语言模型如何应对社会困境。研究发现，一些大型语言模型在合作方面表现良好，而一些推理大型语言模型却表现出困难。这表明，提高大型语言模型的推理能力并不一定导致合作。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地被用作自主代理，理解它们的合作和社会机制变得越来越重要。特别是，LLMs 如何平衡自身利益和集体福祉是确保对齐、鲁棒性和安全部署的关键挑战。

Method: 我们改编了行为经济学中的公共物品游戏与制度选择，以观察不同大型语言模型如何在重复互动中应对社会困境。

Result: 我们的分析揭示了模型中的四种不同行为模式：一些模型始终建立并维持高水平的合作，另一些模型在参与和不参与之间波动，一些模型随着时间的推移逐渐减少合作行为，而另一些模型则无论结果如何都严格遵循固定策略。令人惊讶的是，我们发现像 o1 系列这样的推理大型语言模型在合作方面遇到了显著困难，而一些传统的大型语言模型则始终能够实现高水平的合作。

Conclusion: 当前提高大型语言模型的方法，专注于增强其推理能力，并不一定导致合作，这为在需要持续协作的环境中部署大型语言模型代理提供了有价值的见解。

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [95] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik,Osman Erdem,Mehmet Dik*

Main category: cs.AI

TL;DR: 本研究分析了GPTZero在检测AI生成文本方面的效果，发现其对纯AI生成内容有效，但在区分人类撰写文本方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 随着学生使用AI工具的普及，教师开始使用AI检测工具如GPTZero和QuillBot来检测AI写作文本。然而，这些检测器的可靠性仍然不确定。

Method: 我们专注于GPTZero在识别基于不同长度的随机提交论文的AI生成文本方面的成功率。我们收集了一个包含28篇AI生成论文和50篇人工撰写的论文的数据集。将这些随机论文输入GPTZero，并测量AI生成百分比和置信度。

Result: 大多数AI生成的论文被准确检测（91-100%的AI认为生成），而人工撰写的论文则波动；有一些误报。

Conclusion: 这些发现表明，尽管GPTZero在检测纯AI生成内容方面是有效的，但其在区分人类撰写文本方面的可靠性有限。因此，教育工作者在仅依赖AI检测工具时应谨慎。

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [96] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文介绍了 MMReason，一个新的基准测试，用于精确和全面评估 MLLM 的长链推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLLM 基准测试在精确和全面评估长链推理能力方面存在不足，包括缺乏难度和多样性、容易被猜测和记忆所影响以及对中间推理步骤的评估不足。

Method: 我们从多个领域和不同难度级别中精选需要多步骤推理的问题，并将其重新表述为开放性格式，使用多模型投票技术过滤掉与猜测和记忆相关的捷径情况，同时对问题进行详细分步解答的注释，并设计了一种基于参考的三元评分机制来可靠评估中间推理步骤。

Result: 我们对流行的领先 MLLM 进行了基准测试，并对其推理能力进行了深入分析。

Conclusion: MMReason 旨在作为推进 MLLM 推理研究的宝贵资源。

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [97] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl,Daniel Hugenroth,Bill Marino,Alastair R. Beresford*

Main category: cs.AI

TL;DR: Attestable Audits are proposed to verify interaction with compliant AI models, even when there is no trust between the model provider and auditor.


<details>
  <summary>Details</summary>
Motivation: Benchmarks for evaluating AI models lack confidentiality for model IP and benchmark datasets, and do not offer verifiable results.

Method: Attestable Audits run inside Trusted Execution Environments to enable verification of interaction with compliant AI models.

Result: A prototype was built to demonstrate the feasibility of Attestable Audits on typical audit benchmarks against Llama-3.1.

Conclusion: Attestable Audits provide a solution to verify interaction with compliant AI models, even when the model provider and auditor do not trust each other.

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [98] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha Jaques*

Main category: cs.AI

TL;DR: 本文介绍了SPIRAL，这是一种自我对弈框架，使模型能够通过与不断改进的自身版本进行多轮零和博弈来学习，从而无需人类监督。通过自我对弈，SPIRAL生成了一个无限的逐步挑战性问题课程。实验结果表明，这种方法可以提高模型的推理能力，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 最近在强化学习中的进展表明，语言模型可以通过在具有可验证奖励的任务上进行训练来发展复杂的推理，但这些方法依赖于人工整理的问题-答案对和领域特定的奖励工程。

Method: 我们引入了SPIRAL，这是一个自我对弈框架，其中模型通过与不断改进的自身版本进行多轮零和博弈来学习，消除了对人类监督的需求。我们实现了完全在线、多轮、多智能体强化学习系统，并提出了角色条件优势估计（RAE）以稳定多智能体训练。

Result: 通过自我对弈，SPIRAL生成了一个无限的逐步挑战性问题课程，因为模型必须不断适应更强的对手。在Kuhn Poker上进行自我对弈训练Qwen3-4B-Base，数学成绩提高了8.6%，一般推理提高了8.4%，超过了在25,000个专家游戏轨迹上的SFT。分析显示，这种迁移是通过三种认知模式实现的：系统分解、期望值计算和逐案分析。多游戏训练（TicTacToe、Kuhn Poker、Simple Negotiation）进一步提高了性能，因为每种游戏都发展了不同的推理优势。将SPIRAL应用于一个强大的推理模型（DeepSeek-R1-Distill-Qwen-7B）仍能带来2.0%的平均提升。

Conclusion: 这些结果表明，零和游戏自然地发展可转移的推理能力，突显了自主推理发展的有希望的方向。

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [99] [Computational Analysis of Climate Policy](https://arxiv.org/abs/2506.22449)
*Carolyn Hicks*

Main category: cs.CY

TL;DR: 本论文探讨了气候紧急运动对地方政府气候政策的影响，使用计算方法分析了气候紧急声明的作用，并展示了GPT-4在政策分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在评估当前大型语言模型在回答复杂政策问题方面的潜力。

Method: 本论文构建并配置了一个名为PALLM（使用大型语言模型进行政策分析）的系统，利用OpenAI模型GPT-4，并应用了一种概念框架来分析气候紧急响应计划的数据集。

Result: 研究发现，GPT-4能够进行高水平的政策分析，其局限性包括缺乏可靠的归因能力，但可以为研究人员提供更细致的分析。通过比较已通过气候紧急声明的市议会与未通过的市议会的结果，发现前者更有可能拥有近期和气候特定的政策，并更加关注紧迫性、优先级、公平和社会正义。

Conclusion: 本论文得出结论，大规模评估政策文件的能力为政策研究人员开辟了令人兴奋的新机遇。

Abstract: This thesis explores the impact of the Climate Emergency movement on local
government climate policy, using computational methods. The Climate Emergency
movement sought to accelerate climate action at local government level through
the mechanism of Climate Emergency Declarations (CEDs), resulting in a series
of commitments from councils to treat climate change as an emergency. With the
aim of assessing the potential of current large language models to answer
complex policy questions, I first built and configured a system named PALLM
(Policy Analysis with a Large Language Model), using the OpenAI model GPT-4.
This system is designed to apply a conceptual framework for climate emergency
response plans to a dataset of climate policy documents. I validated the
performance of this system with the help of local government policymakers, by
generating analyses of the climate policies of 11 local governments in Victoria
and assessing the policymakers' level of agreement with PALLM's responses.
Having established that PALLM's performance is satisfactory, I used it to
conduct a large-scale analysis of current policy documents from local
governments in the state of Victoria, Australia. This thesis presents the
methodology and results of this analysis, comparing the results for councils
which have passed a CED to those which did not. This study finds that GPT-4 is
capable of high-level policy analysis, with limitations including a lack of
reliable attribution, and can also enable more nuanced analysis by researchers.
Its use in this research shows that councils which have passed a CED are more
likely to have a recent and climate-specific policy, and show more attention to
urgency, prioritisation, and equity and social justice, than councils which
have not. It concludes that the ability to assess policy documents at scale
opens up exciting new opportunities for policy researchers.

</details>


### [100] [Theories of "Sexuality" in Natural Language Processing Bias Research](https://arxiv.org/abs/2506.22481)
*Jacob Hobbs*

Main category: cs.CY

TL;DR: 本文探讨了NLP系统中性取向的编码和（误）表示问题，并提出了改进基于性取向的NLP偏见分析的建议。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是填补现有研究中对酷儿性取向在NLP系统和从业者中的编码和（误）表示的详细分析的空白。

Method: 本文通过调查和分析55篇量化性取向NLP偏见的文章，研究了性取向是如何被定义和操作化的。

Result: 本文发现，大多数文献中性取向并未得到明确的定义，而是依赖于假设或规范性的性/浪漫实践和身份观念。此外，提取NLP技术中有偏输出的方法常常将性别和性取向身份混为一谈，导致对酷儿的单一化理解，从而导致偏见的不当量化。

Conclusion: 本文结论是，为了改进基于性取向的NLP偏见分析，需要更加深入地参与酷儿社区和跨学科文献。

Abstract: In recent years, significant advancements in the field of Natural Language
Processing (NLP) have positioned commercialized language models as
wide-reaching, highly useful tools. In tandem, there has been an explosion of
multidisciplinary research examining how NLP tasks reflect, perpetuate, and
amplify social biases such as gender and racial bias. A significant gap in this
scholarship is a detailed analysis of how queer sexualities are encoded and
(mis)represented by both NLP systems and practitioners. Following previous work
in the field of AI fairness, we document how sexuality is defined and
operationalized via a survey and analysis of 55 articles that quantify
sexuality-based NLP bias. We find that sexuality is not clearly defined in a
majority of the literature surveyed, indicating a reliance on assumed or
normative conceptions of sexual/romantic practices and identities. Further, we
find that methods for extracting biased outputs from NLP technologies often
conflate gender and sexual identities, leading to monolithic conceptions of
queerness and thus improper quantifications of bias. With the goal of improving
sexuality-based NLP bias analyses, we conclude with recommendations that
encourage more thorough engagement with both queer communities and
interdisciplinary literature.

</details>


### [101] [A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models](https://arxiv.org/abs/2506.22493)
*Sadia Kamal,Lalu Prasad Yadav Prakash,S M Rafiuddin,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen,Sagnik Ray Choudhury*

Main category: cs.CY

TL;DR: 研究表明，标准生成参数对PCT分数影响不大，但提示变化和微调会影响PCT分数。此外，政治内容较多的数据集微调不会导致PCT分数的差异。这引发了对PCT测试有效性和政治倾向编码机制的进一步研究。


<details>
  <summary>Details</summary>
Motivation: 为了验证PCT测试的有效性，并了解政治倾向在大型语言模型中的编码机制。

Method: 通过检查PCT测试的有效性，研究了标准生成参数的变化是否显著影响模型的PCT分数，同时分析了外部因素如提示变化和微调对PCT分数的影响。

Result: 标准生成参数的变化不会显著影响模型的PCT分数，但提示变化和微调会单独或组合地影响PCT分数。此外，当模型在包含更多政治内容的文本数据集上进行微调时，PCT分数并未受到差异影响。

Conclusion: 这呼吁对PCT和类似测试的有效性以及政治倾向在LLM中的编码机制进行彻底调查。

Abstract: Political Compass Test (PCT) or similar questionnaires have been used to
quantify LLM's political leanings. Building on a recent line of work that
examines the validity of PCT tests, we demonstrate that variation in standard
generation parameters does not significantly impact the models' PCT scores.
However, external factors such as prompt variations and fine-tuning
individually and in combination affect the same. Finally, we demonstrate that
when models are fine-tuned on text datasets with higher political content than
others, the PCT scores are not differentially affected. This calls for a
thorough investigation into the validity of PCT and similar tests, as well as
the mechanism by which political leanings are encoded in LLMs.

</details>


### [102] [Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety](https://arxiv.org/abs/2506.22496)
*Y. Du*

Main category: cs.CY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) exhibit systematic risk-taking behaviors
analogous to those observed in gambling psychology, including overconfidence
bias, loss-chasing tendencies, and probability misjudgment. Drawing from
behavioral economics and prospect theory, we identify and formalize these
"gambling-like" patterns where models sacrifice accuracy for high-reward
outputs, exhibit escalating risk-taking after errors, and systematically
miscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)
framework, incorporating insights from gambling research to address these
behavioral biases through risk-calibrated training, loss-aversion mechanisms,
and uncertainty-aware decision making. Our approach introduces novel evaluation
paradigms based on established gambling psychology experiments, including AI
adaptations of the Iowa Gambling Task and probability learning assessments.
Experimental results demonstrate measurable reductions in gambling-like
behaviors: 18.7\% decrease in overconfidence bias, 24.3\% reduction in
loss-chasing tendencies, and improved risk calibration across diverse
scenarios. This work establishes the first systematic framework for
understanding and mitigating gambling psychology patterns in AI systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [103] [You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties](https://arxiv.org/abs/2506.23367)
*Paige Tuttösí,H. Henny Yeung,Yue Wang,Jean-Julien Aucouturier,Angelica Lim*

Main category: cs.SD

TL;DR: 我们开发了一个专门针对第二语言（L2）说话人的文本到语音（TTS）系统，通过利用美国英语中紧元音和松元音的持续时间差异来创建“清晰模式”。感知研究表明，这种模式能有效提高转录准确性，并且比整体减慢的语音更受听众欢迎和尊重。然而，听众并未意识到这些效果，他们仍认为减慢所有目标词是最易懂的，这表明实际的可理解性与感知的可理解性之间没有相关性。此外，我们发现Whisper-ASR无法准确评估这些用户的TTS系统的可理解性。


<details>
  <summary>Details</summary>
Motivation: 我们希望为第二语言（L2）说话人开发一个专门的文本到语音（TTS）系统，以提高他们的语音识别准确性。

Method: 我们使用美国英语中紧元音（较长）和松元音（较短）的持续时间差异来创建Matcha-TTS的“清晰模式”。

Result: 感知研究表明，法语母语者（L1）和英语二语（L2）听众在使用我们的清晰模式时，转录错误减少了至少9.15%，并且认为它比整体减慢的语音更令人鼓舞和尊重。然而，听众并没有意识到这些效果：尽管清晰模式下的单词错误率下降了，但听众仍然认为减慢所有目标词是最易懂的，这表明实际的可理解性与感知的可理解性之间没有相关性。此外，我们发现Whisper-ASR不像L2说话人那样使用相同的线索来区分困难的元音，并且不足以评估这些用户的TTS系统的可理解性。

Conclusion: 我们的研究结果表明，清晰模式在提高第二语言（L2）说话人的语音识别准确性方面是有效的，并且比整体减慢的语音更受听众欢迎和尊重。此外，我们发现Whisper-ASR无法准确评估针对这些用户的TTS系统的可理解性。

Abstract: We present the first text-to-speech (TTS) system tailored to second language
(L2) speakers. We use duration differences between American English tense
(longer) and lax (shorter) vowels to create a "clarity mode" for Matcha-TTS.
Our perception studies showed that French-L1, English-L2 listeners had fewer
(at least 9.15%) transcription errors when using our clarity mode, and found it
more encouraging and respectful than overall slowed down speech. Remarkably,
listeners were not aware of these effects: despite the decreased word error
rate in clarity mode, listeners still believed that slowing all target words
was the most intelligible, suggesting that actual intelligibility does not
correlate with perceived intelligibility. Additionally, we found that
Whisper-ASR did not use the same cues as L2 speakers to differentiate difficult
vowels and is not sufficient to assess the intelligibility of TTS systems for
these individuals.

</details>


### [104] [Efficient Interleaved Speech Modeling through Knowledge Distillation](https://arxiv.org/abs/2506.23670)
*Mohammadmahdi Nouriborji,Morteza Rohanian*

Main category: cs.SD

TL;DR: 本文介绍了TinyWave，一种用于语音到语音和交错语音-文本生成的2B参数模型，通过知识蒸馏技术实现了高效的语音生成，并在多个任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的语音语言模型超出了许多部署环境的大小和延迟限制。

Method: 通过层对齐的知识蒸馏构建紧凑且富有表现力的语音生成模型，匹配隐藏状态、注意力图和软化logits，将大型多模态变压器压缩3倍，同时性能损失最小。

Result: TinyWave在Libri-Light上的归一化困惑度比其教师模型低1.4个点。在口语StoryCloze和SALMon上的准确率达到了教师模型的93-97%，超过了大小匹配的基线模型。

Conclusion: 这些模型针对商品硬件进行了优化，使实时对话代理、辅助技术和低资源环境中的应用成为可能。我们发布了模型、训练代码和评估脚本，以支持在紧凑且富有表现力的语音生成方面的可重复研究。

Abstract: Current speech language models exceed the size and latency constraints of
many deployment environments. We build compact, expressive speech generation
models through layer-aligned distillation, matching hidden states, attention
maps, and softened logits to compress large multimodal transformers by 3x with
minimal loss in performance. We introduce TinyWave, a family of 2B-parameter
models for speech-to-speech and interleaved speech-text generation, trained on
50,000 hours of public audio. TinyWave supports (i) speech-only generation
using phonetic or expressive tokens and (ii) mixed speech-text continuations.
Evaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity
points of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%
of the teacher's performance, outperforming size-matched baselines. These
models are optimized for deployment on commodity hardware, enabling
applications in real-time conversational agents, assistive technologies, and
low-resource environments. We release models, training code, and evaluation
scripts to support reproducible research on compact, expressive speech
generation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [105] [VERA: Variational Inference Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2506.22666)
*Anamika Lochab,Lu Yan,Patrick Pynadath,Xiangyu Zhang,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA是一种新的黑盒越狱方法，通过变分推断训练小攻击者LLM生成多样且流畅的越狱提示，无需重新优化。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于遗传算法，这受到初始化和手动策划提示池的限制，并且需要为每个提示单独优化，无法全面表征模型漏洞。

Method: VERA将黑盒越狱提示问题转化为变分推断问题，训练一个小的攻击者LLM来近似目标LLM对对抗性提示的后验分布。

Result: 实验结果表明，VERA在各种目标LLM上表现良好，证明了概率推断在对抗性提示生成中的价值。

Conclusion: VERA展示了概率推断在对抗性提示生成中的价值，并在多种目标LLM上实现了强大的性能。

Abstract: The rise of API-only access to state-of-the-art LLMs highlights the need for
effective black-box jailbreak methods to identify model vulnerabilities in
real-world settings. Without a principled objective for gradient-based
optimization, most existing approaches rely on genetic algorithms, which are
limited by their initialization and dependence on manually curated prompt
pools. Furthermore, these methods require individual optimization for each
prompt, failing to provide a comprehensive characterization of model
vulnerabilities. To address this gap, we introduce VERA: Variational infErence
fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a
variational inference problem, training a small attacker LLM to approximate the
target LLM's posterior over adversarial prompts. Once trained, the attacker can
generate diverse, fluent jailbreak prompts for a target query without
re-optimization. Experimental results show that VERA achieves strong
performance across a range of target LLMs, highlighting the value of
probabilistic inference for adversarial prompt generation.

</details>


### [106] [Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models](https://arxiv.org/abs/2506.24056)
*Tung-Ling Li,Hongliang Liu*

Main category: cs.CR

TL;DR: logit-gap steering 是一种高效的 jailbreak 框架，能够在短时间内提高攻击成功率，并揭示安全调优对模型内部表示的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的攻击方法需要大量的模型调用，效率低下，而 logit-gap steering 提供了一种更高效的方法。

Method: logit-gap steering 通过将 RLHF 对齐语言模型的拒绝-肯定差距转换为词汇表上的单次遍历，并使用前向计算得分来平衡差距减少与轻量级代理的 KL 惩罚和奖励变化。

Result: logit-gap steering 在不到一秒钟内完成攻击，并且能够将单次攻击的成功率从基线水平提升到 80-100%。

Conclusion: logit-gap steering 是一种高效的 jailbreak 框架，可以显著提高攻击成功率并揭示安全调优对内部表示的影响。

Abstract: We introduce logit-gap steering, a fast jailbreak framework that casts the
refusal-affirmation gap of RLHF-aligned language models as a single pass over
the vocabulary. A forward-computable score blends gap reduction with
lightweight proxies for KL penalty and reward shift, allowing a "sort-sum-stop"
sweep to complete in under a second and return a short suffix--two orders of
magnitude fewer model calls than beam or gradient attacks. The same suffix
generalises to unseen prompts and scales from 0.5 B to 70 B checkpoints,
lifting one-shot attack success from baseline levels to 80-100% while
preserving topical coherence. Beyond efficiency, these suffixes expose
sentence-boundary reward cliffs and other alignment artefacts, offering a
lightweight probe into how safety tuning reshapes internal representations.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [107] [Density, asymmetry and citation dynamics in scientific literature](https://arxiv.org/abs/2506.23366)
*Nathaniel Imel,Zachary Hafen*

Main category: cs.DL

TL;DR: 本研究探讨了科学论文与先前研究的相似性与其引用率之间的关系，提出了两个度量标准（密度和不对称性）并进行了实证分析，发现密度对预测引用率有一定帮助，而不对称性则无显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨科学论文与先前研究的相似性是否反映在引用率上，并探索如何通过语义嵌入空间中的度量标准来预测引用率。

Method: 研究引入了两个互补的度量标准来表征出版物语义邻域的局部几何结构：(1) 密度（ρ），定义为先前发表的固定数量的论文与包围这些论文的最小距离的比率；(2) 不对称性（α），定义为一篇论文与其最近邻居之间的平均方向差异。使用贝叶斯分层回归方法测试这两个度量与后续引用率之间的预测关系，并调查了跨九个学术领域和五种不同文档嵌入的约53,000篇出版物。

Result: 尽管ρ对引用数的个体效应较小且可变，但将基于密度的预测因子添加到基线模型中时，出样本预测的一致性有所提高。此外，没有证据表明出版物的不对称性能提高模型对引用率的预测性能。

Conclusion: 研究结果表明，论文周围科学文献的密度可能携带关于其最终影响的适度但有信息量的信号。而出版物的不对称性似乎无法提高预测引用率的模型性能。

Abstract: Scientific behavior is often characterized by a tension between building upon
established knowledge and introducing novel ideas. Here, we investigate whether
this tension is reflected in the relationship between the similarity of a
scientific paper to previous research and its eventual citation rate. To
operationalize similarity to previous research, we introduce two complementary
metrics to characterize the local geometry of a publication's semantic
neighborhood: (1) \emph{density} ($\rho$), defined as the ratio between a fixed
number of previously-published papers and the minimum distance enclosing those
papers in a semantic embedding space, and (2) asymmetry ($\alpha$), defined as
the average directional difference between a paper and its nearest neighbors.
We tested the predictive relationship between these two metrics and its
subsequent citation rate using a Bayesian hierarchical regression approach,
surveying $\sim 53,000$ publications across nine academic disciplines and five
different document embeddings. While the individual effects of $\rho$ on
citation count are small and variable, incorporating density-based predictors
consistently improves out-of-sample prediction when added to baseline models.
These results suggest that the density of a paper's surrounding scientific
literature may carry modest but informative signals about its eventual impact.
Meanwhile, we find no evidence that publication asymmetry improves model
predictions of citation rates. Our work provides a scalable framework for
linking document embeddings to scientometric outcomes and highlights new
questions regarding the role that semantic similarity plays in shaping the
dynamics of scientific reward.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [108] [Teaching a Language Model to Speak the Language of Tools](https://arxiv.org/abs/2506.23394)
*Simeon Emanuilov*

Main category: cs.IR

TL;DR: 本研究提出了一种方法，通过在新的双语数据集上继续训练模型，使现有的语言模型能够在任何目标语言中实现稳健的工具使用。TUCAN在基准测试中实现了比基础模型高出28.75%的函数调用准确性，并且展示了可生产使用的响应格式。


<details>
  <summary>Details</summary>
Motivation: 大多数多语言模型在非英语语言中缺乏可靠的工具使用能力，即使是最先进的多语言模型在确定何时使用工具和生成函数调用所需的结构化输出方面也存在困难，尤其是在低资源语言中容易出现语言混淆。

Method: 本研究提出了一个方法，通过在新的双语数据集上继续训练BgGPT模型系列，使现有的语言模型能够在任何目标语言中实现稳健的工具使用。

Result: TUCAN（Tool-Using Capable Assistant Navigator）在基准测试中实现了比基础模型高出28.75%的函数调用准确性，并且在保持核心语言理解能力的同时，展示了可生产使用的响应格式。

Conclusion: 本研究展示了一种实用的方法，可以将工具增强功能扩展到以英语为中心的系统之外。

Abstract: External tool integration through function-calling is essential for practical
language model applications, yet most multilingual models lack reliable
tool-use capabilities in non-English languages. Even state-of-the-art
multilingual models struggle with determining when to use tools and generating
the structured outputs required for function calls, often exhibiting language
confusion when prompted in lower-resource languages. This work presents a
methodology for adapting existing language models to enable robust tool use in
any target language, using Bulgarian as a case study. The approach involves
continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a
novel bilingual dataset of 10,035 function-calling examples designed to support
standardized protocols like MCP (Model Context Protocol). The research
introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to
28.75% improvement in function-calling accuracy over base models while
preserving core language understanding, as verified on established Bulgarian
benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready
response formatting with clean, parsable function calls, contrasting with the
verbose and inconsistent outputs of base models. The models, evaluation
framework, and dataset are released to enable replication for other languages.
This work demonstrates a practical approach for extending tool-augmented
capabilities beyond English-centric systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [109] [GaussMaster: An LLM-based Database Copilot System](https://arxiv.org/abs/2506.23322)
*Wei Zhou,Ji Sun,Xuanhe Zhou,Guoliang Li,Luyang Liu,Hao Wu,Tianyuan Wang*

Main category: cs.DB

TL;DR: GaussMaster is an LLM-based database copilot system that automates database maintenance by analyzing metrics and logs, identifying root causes, and resolving issues without human intervention.


<details>
  <summary>Details</summary>
Motivation: The financial industry relies heavily on data, and DBAs face significant responsibilities. Existing autonomous database platforms are limited in their capabilities, requiring manual intervention for comprehensive database maintenance.

Method: GaussMaster introduces an LLM-based database copilot system that analyzes hundreds of metrics and logs, employs a Tree-of-thought approach to identify root causes, and invokes appropriate tools to resolve issues automatically.

Result: GaussMaster has been successfully implemented in real-world scenarios, such as the banking industry, achieving zero human intervention for over 34 database maintenance scenarios.

Conclusion: GaussMaster has achieved zero human intervention for over 34 database maintenance scenarios in real-world applications, demonstrating its effectiveness in revolutionizing database maintenance.

Abstract: In the financial industry, data is the lifeblood of operations, and DBAs
shoulder significant responsibilities for SQL tuning, database deployment,
diagnosis, and service repair. In recent years, both database vendors and
customers have increasingly turned to autonomous database platforms in an
effort to alleviate the heavy workload of DBAs. However, existing autonomous
database platforms are limited in their capabilities, primarily addressing
single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual
intervention remains a necessity for comprehensive database maintenance.
GaussMaster aims to revolutionize this landscape by introducing an LLM-based
database copilot system. This innovative solution is designed not only to
assist developers in writing efficient SQL queries but also to provide
comprehensive care for database services. When database instances exhibit
abnormal behavior, GaussMaster is capable of orchestrating the entire
maintenance process automatically. It achieves this by analyzing hundreds of
metrics and logs, employing a Tree-of-thought approach to identify root causes,
and invoking appropriate tools to resolve issues. We have successfully
implemented GaussMaster in real-world scenarios, such as the banking industry,
where it has achieved zero human intervention for over 34 database maintenance
scenarios. In this paper, we present significant improvements in these tasks
with code at https://gitcode.com/opengauss/openGauss-GaussMaster.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [110] [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/abs/2506.22696)
*Brian Mak,Jeffrey Flanigan*

Main category: cs.LG

TL;DR: 本文提出了一种新的Transformer变体——Residual Matrix Transformer (RMT)，通过替换残差流为外积记忆矩阵，实现了更高的计算和参数效率，并在性能上优于传统Transformer。


<details>
  <summary>Details</summary>
Motivation: 改进Transformer中残差流的信息存储和检索机制，以提高计算和参数效率。

Method: 将Transformer的残差流替换为外积记忆矩阵，构建了Residual Matrix Transformer (RMT)模型。

Result: RMT在保持相同损失的情况下，减少了58%的FLOPS、25%的参数和41%的训练token，并在下游任务中表现更好。

Conclusion: RMT在下游评估中表现优于Transformer，并且在计算和参数效率方面有显著提升。

Abstract: The residual stream acts as a memory bus where transformer layers both store
and access features (Elhage et al., 2021). We consider changing the mechanism
for retrieving and storing information in the residual stream, and replace the
residual stream of the transformer with an outer product memory matrix
(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix
Transformer (RMT). We find that the RMT enjoys a number of attractive
properties: 1) the size of the residual stream can be scaled independently of
compute and model size, improving performance, 2) the RMT can achieve the same
loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%
fewer training tokens tokens, and 3) the RMT outperforms the transformer on
downstream evaluations. We theoretically analyze the transformer and the RMT,
and show that the RMT allows for more efficient scaling of the residual stream,
as well as improved variance propagation properties. Code for this project can
be found at https://github.com/bmac3/residual-matrix-transformer.

</details>


### [111] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
*Dujian Ding,Ankur Mallick,Shaokun Zhang,Chi Wang,Daniel Madrigal,Mirian Del Carmen Hipolito Garcia,Menglin Xia,Laks V. S. Lakshmanan,Qingyun Wu,Victor Rühle*

Main category: cs.LG

TL;DR: 本文提出了一种名为BEST-Route的新路由框架，通过根据查询难度和质量阈值选择模型和采样响应数量，以减少大型语言模型的部署成本。实验结果表明，该方法可以将成本降低高达60%，同时性能下降不到1%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然功能强大，但大规模部署时成本高昂。LLM查询路由通过动态分配查询到不同成本和质量的模型来缓解这个问题，以获得所需的权衡。然而，先前的查询路由方法只生成一个响应，而一个小模型的单个响应通常不足以胜过一个大模型的响应，导致他们过度使用大模型，错过了潜在的成本节约机会。

Method: 我们提出了BEST-Route，这是一种新颖的路由框架，根据查询难度和质量阈值选择模型以及从该模型中采样的响应数量。

Result: 我们的方法在真实世界数据集上的实验表明，它可以在性能下降不到1%的情况下将成本降低多达60%。

Conclusion: 我们的方法在真实世界数据集上的实验表明，它可以在性能下降不到1%的情况下将成本降低多达60%。

Abstract: Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [112] [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/abs/2506.22809)
*Cooper Doyle*

Main category: cs.LG

TL;DR: BayesLoRA is a task-specific uncertainty quantification framework that integrates MC-Dropout into Low-Rank Adapters (LoRA), providing reliable confidence estimates for agentic decision-making.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide guardrails tailored to downstream workflows, enabling agents to introspect and modulate behavior under uncertainty.

Method: The paper proposes BayesLoRA, which integrates MC-Dropout into Low-Rank Adapters (LoRA) for task-specific uncertainty quantification.

Result: The paper demonstrates mathematically and empirically that LoRA adapters exhibit amplified variance outside fine-tuning distributions, yielding reliable confidence estimates for agentic decision-making.

Conclusion: BayesLoRA provides a task-specific uncertainty quantification framework that integrates MC-Dropout into Low-Rank Adapters (LoRA), enabling agents to introspect and modulate behavior under uncertainty.

Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework
that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike
general-purpose transformer uncertainty methods, BayesLoRA provides guardrails
tailored to downstream workflows, enabling agents to introspect and modulate
behavior under uncertainty. We demonstrate mathematically and empirically that
LoRA adapters exhibit amplified variance outside fine-tuning distributions,
yielding reliable confidence estimates for agentic decision-making.

</details>


### [113] [Masked Gated Linear Unit](https://arxiv.org/abs/2506.23225)
*Yukito Tajima,Nakamasa Inoue,Yusuke Sekikawa,Ikuro Sato,Rio Yokota*

Main category: cs.LG

TL;DR: MGLUs are a novel type of GLU that improves memory efficiency and inference speed in LLMs while maintaining or enhancing accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard GLUs require more memory reads due to separate weight matrices for gate and value streams, leading to inefficiencies in LLMs. The goal is to improve memory efficiency and inference speed without sacrificing accuracy.

Method: The paper introduces MGLUs, which include the MoEG architecture and FlashMGLU kernel, to address the memory inefficiency of standard GLUs. It also evaluates the performance of SwiMGLU in LLM experiments.

Result: FlashMGLU achieves a 19.7× speed-up over a naive PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs on an RTX5090 GPU. SwiMGLU matches or surpasses the accuracy of SwiGLU while preserving memory advantages.

Conclusion: MGLUs offer significant improvements in memory efficiency and inference speed compared to traditional GLUs, while maintaining or improving downstream accuracy in LLMs.

Abstract: Gated Linear Units (GLUs) have become essential components in the
feed-forward networks of state-of-the-art Large Language Models (LLMs).
However, they require twice as many memory reads compared to feed-forward
layers without gating, due to the use of separate weight matrices for the gate
and value streams. To address this bottleneck, we introduce Masked Gated Linear
Units (MGLUs), a novel family of GLUs with an efficient kernel implementation.
The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating
(MoEG) architecture that learns multiple binary masks, each determining gate or
value assignments at the element level on a single shared weight matrix
resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly
kernel that yields up to a 19.7 $\times$ inference-time speed-up over a naive
PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs
despite added architectural complexity on an RTX5090 GPU. In LLM experiments,
the Swish-activated variant SwiMGLU preserves its memory advantages while
matching - or even surpassing - the downstream accuracy of the SwiGLU baseline.

</details>


### [114] [Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts](https://arxiv.org/abs/2506.23845)
*Kenny Peng,Rajiv Movva,Jon Kleinberg,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: This paper argues that while SAEs may be less effective for acting on known concepts, they are powerful tools for discovering unknown concepts and have potential applications in various fields.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reconcile competing narratives surrounding SAEs by establishing a conceptual distinction that explains their varying levels of effectiveness.

Method: The paper establishes a conceptual distinction between the effectiveness of SAEs in acting on known concepts versus discovering unknown concepts.

Result: The paper shows that SAEs may be less effective for acting on known concepts but are powerful tools for discovering unknown concepts. It also outlines several classes of SAE applications.

Conclusion: SAEs are powerful tools for discovering unknown concepts, and they have potential applications in various fields such as ML interpretability, explainability, fairness, auditing, safety, and social and health sciences.

Abstract: While sparse autoencoders (SAEs) have generated significant excitement, a
series of negative results have added to skepticism about their usefulness.
Here, we establish a conceptual distinction that reconciles competing
narratives surrounding SAEs. We argue that while SAEs may be less effective for
acting on known concepts, SAEs are powerful tools for discovering unknown
concepts. This distinction cleanly separates existing negative and positive
results, and suggests several classes of SAE applications. Specifically, we
outline use cases for SAEs in (i) ML interpretability, explainability,
fairness, auditing, and safety, and (ii) social and health sciences.

</details>


### [115] [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/abs/2506.23978)
*Samuele Marro,Philip Torr*

Main category: cs.LG

TL;DR: LLM代理可以实现通用互操作性，这将改变当前互联网应用层的封闭状态，但同时也带来新的安全风险。ML社区应积极应对这一变化，并建立适当的框架以减轻负面影响。


<details>
  <summary>Details</summary>
Motivation: 当前互联网应用层由封闭的专有平台主导，开放和互操作的API需要大量投资，而市场领导者缺乏激励去促进数据交换。LLM代理可以自动转换数据格式并与面向人类的接口交互，使互操作性更加便宜且不可避免。

Method: 分析了LLM代理如何改变现状，并讨论了其对互操作性、垄断行为和安全风险的影响。

Result: LLM代理可以实现通用互操作性，这使得任何两个数字服务都能使用AI中介适配器无缝交换数据，从而削弱垄断行为并促进数据可移植性。然而，这也可能导致新的安全风险和技术债务。

Conclusion: ML社区应接受这一发展，同时建立适当的框架来缓解负面影响。通过现在行动，我们可以利用AI恢复用户自由和竞争市场，而不会牺牲安全。

Abstract: While the Internet's core infrastructure was designed to be open and
universal, today's application layer is dominated by closed, proprietary
platforms. Open and interoperable APIs require significant investment, and
market leaders have little incentive to enable data exchange that could erode
their user lock-in. We argue that LLM-based agents fundamentally disrupt this
status quo. Agents can automatically translate between data formats and
interact with interfaces designed for humans: this makes interoperability
dramatically cheaper and effectively unavoidable. We name this shift universal
interoperability: the ability for any two digital services to exchange data
seamlessly using AI-mediated adapters. Universal interoperability undermines
monopolistic behaviours and promotes data portability. However, it can also
lead to new security risks and technical debt. Our position is that the ML
community should embrace this development while building the appropriate
frameworks to mitigate the downsides. By acting now, we can harness AI to
restore user freedom and competitive markets without sacrificing security.

</details>

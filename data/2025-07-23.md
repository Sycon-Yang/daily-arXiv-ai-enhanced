<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs](https://arxiv.org/abs/2507.15863)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.CL

TL;DR: DEREK模块是一种安全、可扩展的检索增强生成管道，用于企业文档问答。它通过处理异构内容、使用混合HNSW+BM25存储、优化查询、重新排序和验证答案来提高准确性和可追溯性。结果表明，该模块能够提供准确、可追溯且生产就绪的文档问答，同时保持最小的操作开销。


<details>
  <summary>Details</summary>
Motivation: 企业需要一种安全、可审计和上下文忠实的文档问答系统，以满足法律和金融等高风险领域的需求。现有的方法可能无法提供足够的准确性、可追溯性和安全性。

Method: DEREK模块采用了一种安全且可扩展的检索增强生成管道，用于企业文档问答。系统处理异构内容，将其拆分为1000个token的重叠块，并在混合HNSW+BM25存储中进行索引。用户查询通过GPT-4o优化，通过组合向量+BM25搜索检索，并通过Cohere重新排序，最后由LLM使用CO-STAR提示工程回答。LangGraph验证器确保引用重叠，直到每个声明都有依据。

Result: 在四个LegalBench子集上，1000个token的块提高了Recall@50约1个百分点，混合+重新排序提升了Precision@10约7个百分点；验证器将TRACe利用率提高到0.50以上，并将未经证实的陈述限制在3%以下。所有组件都在容器中运行，强制执行端到端TLS 1.3和AES-256。

Conclusion: DEREK模块展示了准确、可追溯且可投入生产的文档问答能力，具有最小的操作开销。该模块旨在满足企业对安全、可审计和上下文忠实检索的需求，为法律和金融等高风险领域提供可靠的基准。

Abstract: We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge)
Module, a secure and scalable Retrieval-Augmented Generation pipeline designed
specifically for enterprise document question answering. Designed and
implemented by eSapiens, the system ingests heterogeneous content (PDF, Office,
web), splits it into 1,000-token overlapping chunks, and indexes them in a
hybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via
combined vector+BM25 search, reranked with Cohere, and answered by an LLM using
CO-STAR prompt engineering. A LangGraph verifier enforces citation overlap,
regenerating answers until every claim is grounded. On four LegalBench subsets,
1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank
boosts Precision@10 by approximately 7 pp; the verifier raises TRACe
Utilization above 0.50 and limits unsupported statements to less than 3%. All
components run in containers, enforce end-to-end TLS 1.3 and AES-256. These
results demonstrate that the DEREK module delivers accurate, traceable, and
production-ready document QA with minimal operational overhead. The module is
designed to meet enterprise demands for secure, auditable, and context-faithful
retrieval, providing a reliable baseline for high-stakes domains such as legal
and finance.

</details>


### [2] [Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity](https://arxiv.org/abs/2507.15864)
*Guowen Yuan,Tien-Hsuan Wu,Lianghao Xia,Ben Kao*

Main category: cs.CL

TL;DR: 本文研究了低资源场景下基于演示学习的命名实体识别问题，提出了通过双重相似性和对抗性演示训练的方法，实验结果表明该方法优于多种方法。


<details>
  <summary>Details</summary>
Motivation: 在低资源场景下，基于演示学习的命名实体识别（NER）存在两个问题：现有方法主要依赖语义相似性选择演示示例，而特征相似性可以显著提高性能；此外，NER标记器参考演示示例的能力通常不足。

Method: 我们提出了一个演示和训练方法，该方法通过双重相似性选择示例，并通过对抗性演示训练NER模型，以解决演示构建和模型训练中的两个问题。

Result: 我们在低资源NER任务中进行了全面的实验，结果表明我们的方法优于多种方法。

Conclusion: 我们的方法在低资源NER任务中优于多种方法。

Abstract: We study the problem of named entity recognition (NER) based on demonstration
learning in low-resource scenarios. We identify two issues in demonstration
construction and model training. Firstly, existing methods for selecting
demonstration examples primarily rely on semantic similarity; We show that
feature similarity can provide significant performance improvement. Secondly,
we show that the NER tagger's ability to reference demonstration examples is
generally inadequate. We propose a demonstration and training approach that
effectively addresses these issues. For the first issue, we propose to select
examples by dual similarity, which comprises both semantic similarity and
feature similarity. For the second issue, we propose to train an NER model with
adversarial demonstration such that the model is forced to refer to the
demonstrations when performing the tagging task. We conduct comprehensive
experiments in low-resource NER tasks, and the results demonstrate that our
method outperforms a range of methods.

</details>


### [3] [Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models](https://arxiv.org/abs/2507.15868)
*Altynbek Ismailov,Salia Asanova*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在面对提示扰动时的表现，发现它们对无害的噪声过于鲁棒，但对语义变化的敏感性不足。


<details>
  <summary>Details</summary>
Motivation: 在代码生成的场景中，微小的错误可能导致安全问题或经济损失，因此需要研究模型对提示扰动的反应。

Method: 通过编译50个LeetCode问题，并设计三种最小的提示扰动来测试模型的鲁棒性和敏感性。

Result: 模型在90%的提示缺失情况下仍能保持85%的正确率，但在量化词翻转的情况下仅54%的模型能正确响应，而术语替换则处于中间水平。

Conclusion: 当前大型语言模型在处理代码生成时，对细微的错误表现出过度的鲁棒性，但对语义变化的敏感性不足。因此，需要改进评估和训练协议，以奖励对语义变化的差异化敏感性。

Abstract: Large language models (LLMs) now write code in settings where misreading a
single word can break safety or cost money, yet we still expect them to
overlook stray typos. To probe where useful robustness ends and harmful
insensitivity begins, we compile 50 LeetCode problems and craft three minimal
prompt perturbations that should vary in importance: (i) progressive
underspecification deleting 10 % of words per step; (ii) lexical flip swapping
a pivotal quantifier ("max" to "min"); and (iii) jargon inflation replacing a
common noun with an obscure technical synonym. Six frontier models, including
three "reasoning-tuned" versions, solve each mutated prompt, and their Python
outputs are checked against the original test suites to reveal whether they
reused the baseline solution or adapted. Among 11 853 generations we observe a
sharp double asymmetry. Models remain correct in 85 % of cases even after 90 %
of the prompt is missing, showing over-robustness to underspecification, yet
only 54 % react to a single quantifier flip that reverses the task, with
reasoning-tuned variants even less sensitive than their bases. Jargon edits lie
in between, passing through 56 %. Current LLMs thus blur the line between
harmless noise and meaning - changing edits, often treating both as ignorable.
Masking salient anchors such as function names can force re - evaluation. We
advocate evaluation and training protocols that reward differential
sensitivity: stay steady under benign noise but adapt - or refuse - when
semantics truly change.

</details>


### [4] [Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation](https://arxiv.org/abs/2507.16002)
*Sumit Singh,Rohit Mishra,Uma Shanker Tiwary*

Main category: cs.CL

TL;DR: 该研究探讨了利用Hindi特定的预训练编码器和生成模型，并通过从外部相关上下文（如Wikipedia）检索数据来增强数据，以改进Hindi NER。结果表明，检索增强（RA）在大多数情况下优于不包含RA的基线方法，尤其在低上下文数据中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: One major challenge in natural language processing is named entity recognition (NER), which identifies and categorises named entities in textual input. The study aims to improve NER by using Hindi-specific pretrained encoders and generative models with data augmentation through retrieval from external contexts.

Method: 研究 investigates a Hindi NER technique that makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and Generative Models (Llama-2-7B-chat-hf, Llama-2-70B-chat-hf, Llama-3-70B-Instruct, and GPT3.5-turbo), and augments the data with retrieved data from external relevant contexts, notably from Wikipedia.

Result: The macro F1 scores for MuRIL and XLM-R are 0.69 and 0.495, respectively, without RA and increase to 0.70 and 0.71, respectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B by a significant margin. Generative models that are not fine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA well, but Llama2-70B and Llama3-70B did not adopt RA with the retrieval context.

Conclusion: 研究显示，检索增强（RA）显著提高了性能，尤其是在低上下文数据中。该研究为如何最佳使用数据增强方法和预训练模型来提高NER性能，特别是在资源有限的语言中，提供了重要的知识。

Abstract: One major challenge in natural language processing is named entity
recognition (NER), which identifies and categorises named entities in textual
input. In order to improve NER, this study investigates a Hindi NER technique
that makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and
Generative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf
(Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments
the data with retrieved data from external relevant contexts, notably from
Wikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA.
However, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER
generation. Our investigation shows that the mentioned language models (LMs)
with Retrieval Augmentation (RA) outperform baseline methods that don't
incorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69
and 0.495, respectively, without RA and increase to 0.70 and 0.71,
respectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B
by a significant margin. On the other hand the generative models which are not
fine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA
well; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval
context. The findings show that RA significantly improves performance,
especially for low-context data. This study adds significant knowledge about
how best to use data augmentation methods and pretrained models to enhance NER
performance, particularly in languages with limited resources.

</details>


### [5] [Learning without training: The implicit dynamics of in-context learning](https://arxiv.org/abs/2507.16003)
*Benoit Dherin,Michael Munn,Hanna Mazzawi,Michael Wunder,Javier Gonzalvo*

Main category: cs.CL

TL;DR: 本文研究了Transformer块如何通过自注意力层和MLP层的堆叠来实现上下文学习，并表明这种机制可能是大型语言模型能够进行上下文学习的原因。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在推理时能够根据上下文学习新模式的机制。

Method: 本文通过理论和实验分析了Transformer块如何通过自注意力层和MLP层的堆叠来隐式修改MLP层的权重。

Result: 本文展示了在一些简化的假设下，Transformer块可以将上下文隐式地转换为MLP层的低秩权重更新。

Conclusion: 本文认为，通过理论和实验表明，这种简单的机制可能是大型语言模型能够在上下文中学习的原因。

Abstract: One of the most striking features of Large Language Models (LLM) is their
ability to learn in context. Namely at inference time an LLM is able to learn
new patterns without any additional weight update when these patterns are
presented in the form of examples in the prompt, even if these patterns were
not seen during training. The mechanisms through which this can happen are
still largely unknown. In this work, we show that the stacking of a
self-attention layer with an MLP, allows the transformer block to implicitly
modify the weights of the MLP layer according to the context. We argue through
theory and experimentation that this simple mechanism may be the reason why
LLMs can learn in context and not only during training. Specifically, we show
under mild simplifying assumptions how a transformer block implicitly
transforms a context into a low-rank weight-update of the MLP layer.

</details>


### [6] [Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback](https://arxiv.org/abs/2507.16007)
*Hannah Rashkin,Elizabeth Clark,Fantine Huot,Mirella Lapata*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在提供写作反馈方面的表现，发现它们在许多方面表现良好，但在识别主要问题和判断反馈类型上存在不足。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨大型语言模型是否能为创意作家提供有意义的写作反馈，并研究模型在这一任务中的表现。

Method: 本文通过定义一个新的任务、数据集和评估框架来探索模型生成的写作反馈的挑战和限制。我们提出了一个包含1300个故事的新测试集，并故意引入写作问题进行研究。

Result: 分析表明，当前模型在许多方面表现出色，能够提供具体且大部分准确的写作反馈。然而，模型常常无法识别故事中最大的写作问题，并且在决定何时提供批评性反馈与积极反馈时出现错误。

Conclusion: 当前模型在许多方面表现出色，能够提供具体且大部分准确的写作反馈，但在识别故事中最大的写作问题以及正确判断何时提供批评性反馈与积极反馈方面存在不足。

Abstract: Can LLMs provide support to creative writers by giving meaningful writing
feedback? In this paper, we explore the challenges and limitations of
model-generated writing feedback by defining a new task, dataset, and
evaluation frameworks. To study model performance in a controlled manner, we
present a novel test set of 1,300 stories that we corrupted to intentionally
introduce writing issues. We study the performance of commonly used LLMs in
this task with both automatic and human evaluation metrics. Our analysis shows
that current models have strong out-of-the-box behavior in many respects --
providing specific and mostly accurate writing feedback. However, models often
fail to identify the biggest writing issue in the story and to correctly decide
when to offer critical vs. positive feedback.

</details>


### [7] [mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages](https://arxiv.org/abs/2507.16011)
*Hellina Hailu Nigatu,Min Li,Maartje ter Hoeve,Saloni Potdar,Sarah Chasins*

Main category: cs.CL

TL;DR: 本文将多语言知识图谱构建任务转化为问答任务，并提出了一种基于检索增强生成的系统mRAKL。实验表明，该方法在低资源语言上的性能优于无上下文设置，并且在理想检索系统下，准确率有显著提升。


<details>
  <summary>Details</summary>
Motivation: 多语言知识图谱构建（mKGC）是指在多语言环境下自动构建或预测知识图谱中缺失的实体和链接的任务。本文旨在通过将mKGC任务转化为问答任务，提高低资源语言的知识图谱构建效果。

Method: 我们将mKGC任务重新表述为问答任务，并引入了mRAKL：一个基于检索增强生成（RAG）的系统来执行mKGC。我们通过使用问题中的头实体和链接关系，并让模型预测尾实体作为答案来实现这一点。

Result: 实验主要集中在两种低资源语言：Tigrinya和Amharic。我们尝试使用更高资源的语言阿拉伯语和英语进行跨语言迁移。使用BM25检索器，我们发现基于RAG的方法在性能上优于无上下文设置。

Conclusion: 通过实验，我们发现基于RAG的方法在低资源语言Tigrinya和Amharic上的性能优于无上下文设置。此外，我们的消融研究显示，理想检索系统下，mRAKL在Tigrinya和Amharic上的准确率分别提高了4.92和8.79个百分点。

Abstract: Knowledge Graphs represent real-world entities and the relationships between
them. Multilingual Knowledge Graph Construction (mKGC) refers to the task of
automatically constructing or predicting missing entities and links for
knowledge graphs in a multilingual setting. In this work, we reformulate the
mKGC task as a Question Answering (QA) task and introduce mRAKL: a
Retrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve
this by using the head entity and linking relation in a question, and having
our model predict the tail entity as an answer. Our experiments focus primarily
on two low-resourced languages: Tigrinya and Amharic. We experiment with using
higher-resourced languages Arabic and English for cross-lingual transfer. With
a BM25 retriever, we find that the RAG-based approach improves performance over
a no-context setting. Further, our ablation studies show that with an idealized
retrieval system, mRAKL improves accuracy by 4.92 and 8.79 percentage points
for Tigrinya and Amharic, respectively.

</details>


### [8] [AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering](https://arxiv.org/abs/2507.16054)
*Simon Baeuerle,Max Radyschevski,Ulrike Pado*

Main category: cs.CL

TL;DR: 本文探讨了利用生成式人工智能（genAI）模型来自动化会议文档工作流程的可能性，并通过一个端到端的流水线实现了这一目标。结果表明，genAI可以帮助减少会议的工作量，技术问题已基本解决，但组织方面对于伦理使用至关重要。


<details>
  <summary>Details</summary>
Motivation: 在大型组织中，知识主要通过会议共享，这占用了大量的工作时间。此外，频繁的面对面会议会产生不一致的文档，导致信息难以在会议外检索，需要长时间的更新和高频次的会议安排。生成式人工智能（genAI）模型在语音和书面语言处理方面表现出色，这促使我们将其用于工程部门的知识管理，以自动记录会议并整合各种附加信息源。

Method: 我们实现了一个端到端的流水线来自动化整个会议文档工作流程，并通过聊天机器人界面使这些内容易于搜索。

Result: 我们的工作测试了基于genAI的软件工具在现实工程部门中的应用，并收集了关于伦理和技术方面的广泛调查数据。直接反馈指出了机会和风险，包括用户认为genAI模型可以显著减少会议的努力，技术方面已经基本解决，而组织方面对于这种系统的成功伦理使用至关重要。

Conclusion: 我们的工作测试了基于genAI的软件工具在现实工程部门中的应用，并收集了关于伦理和技术方面的广泛调查数据。直接反馈指出了机会和风险，包括用户认为genAI模型可以显著减少会议的努力，技术方面已经基本解决，而组织方面对于这种系统的成功伦理使用至关重要。

Abstract: In large organisations, knowledge is mainly shared in meetings, which takes
up significant amounts of work time. Additionally, frequent in-person meetings
produce inconsistent documentation -- official minutes, personal notes,
presentations may or may not exist. Shared information therefore becomes hard
to retrieve outside of the meeting, necessitating lengthy updates and
high-frequency meeting schedules.
  Generative Artificial Intelligence (genAI) models like Large Language Models
(LLMs) exhibit an impressive performance on spoken and written language
processing. This motivates a practical usage of genAI for knowledge management
in engineering departments: using genAI for transcribing meetings and
integrating heterogeneous additional information sources into an easily usable
format for ad-hoc searches.
  We implement an end-to-end pipeline to automate the entire meeting
documentation workflow in a proof-of-concept state: meetings are recorded and
minutes are created by genAI. These are further made easily searchable through
a chatbot interface. The core of our work is to test this genAI-based software
tooling in a real-world engineering department and collect extensive survey
data on both ethical and technical aspects. Direct feedback from this
real-world setup points out both opportunities and risks: a) users agree that
the effort for meetings could be significantly reduced with the help of genAI
models, b) technical aspects are largely solved already, c) organizational
aspects are crucial for a successful ethical usage of such a system.

</details>


### [9] [Deep Researcher with Test-Time Diffusion](https://arxiv.org/abs/2507.16075)
*Rujun Han,Yanfei Chen,Zoey CuiZhu,Lesly Miculicich,Guan Sun,Yuanjun Bi,Weiming Wen,Hui Wan,Chunfeng Wen,Solène Maître,George Lee,Vishy Tirumalashetty,Emily Xue,Zizhao Zhang,Salem Haykal,Burak Gokturk,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: TTD-DR is a novel framework that generates research reports by simulating the iterative process of human research, leading to improved performance in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Deep research agents powered by LLMs often plateau when generating complex, long-form research reports using generic test-time scaling algorithms. The iterative nature of human research, involving cycles of searching, reasoning, and revision, inspired the development of TTD-DR.

Method: TTD-DR conceptualizes research report generation as a diffusion process. It initiates the process with a preliminary draft, which is iteratively refined through a 'denoising' process informed by a retrieval mechanism. A self-evolutionary algorithm is applied to each component of the agentic workflow to ensure high-quality context for the diffusion process.

Result: TTD-DR achieves state-of-the-art results on a wide array of benchmarks requiring intensive search and multi-hop reasoning, significantly outperforming existing deep research agents.

Conclusion: TTD-DR achieves state-of-the-art results on a wide array of benchmarks that require intensive search and multi-hop reasoning, significantly outperforming existing deep research agents.

Abstract: Deep research agents, powered by Large Language Models (LLMs), are rapidly
advancing; yet, their performance often plateaus when generating complex,
long-form research reports using generic test-time scaling algorithms. Drawing
inspiration from the iterative nature of human research, which involves cycles
of searching, reasoning, and revision, we propose the Test-Time Diffusion Deep
Researcher (TTD-DR). This novel framework conceptualizes research report
generation as a diffusion process. TTD-DR initiates this process with a
preliminary draft, an updatable skeleton that serves as an evolving foundation
to guide the research direction. The draft is then iteratively refined through
a "denoising" process, which is dynamically informed by a retrieval mechanism
that incorporates external information at each step. The core process is
further enhanced by a self-evolutionary algorithm applied to each component of
the agentic workflow, ensuring the generation of high-quality context for the
diffusion process. This draft-centric design makes the report writing process
more timely and coherent while reducing information loss during the iterative
search process. We demonstrate that our TTD-DR achieves state-of-the-art
results on a wide array of benchmarks that require intensive search and
multi-hop reasoning, significantly outperforming existing deep research agents.

</details>


### [10] [The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models](https://arxiv.org/abs/2507.16076)
*Marlene Lutz,Indira Sen,Georg Ahnert,Elisa Rogers,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本研究探讨了不同角色提示策略对大型语言模型模拟社会经济群体的影响，发现小型模型有时表现优于大型模型，并提出了减少刻板印象的建议。


<details>
  <summary>Details</summary>
Motivation: Persona prompting在大型语言模型中被越来越多地用于模拟各种社会经济群体的观点。然而，角色提示的制定方式可能显著影响结果，引发了对这种模拟真实性的问题。

Method: 我们使用五种开源大型语言模型，系统地检查了不同的角色提示策略，特别是角色采用格式和人口特征预热策略，如何影响大型语言模型在开放和封闭式任务中对15个交叉人口群体的模拟。

Result: 我们的研究结果显示，大型语言模型在模拟边缘群体方面存在困难，特别是非二元、西班牙裔和中东身份，但选择人口特征预热和角色采用策略显著影响了它们的描绘。具体而言，我们发现以访谈形式进行提示和基于姓名的预热可以帮助减少刻板印象并提高一致性。令人惊讶的是，较小的模型如OLMo-2-7B的表现优于较大的模型如Llama-3.3-70B。

Conclusion: 我们的研究结果为设计基于大型语言模型的社经人口角色提示提供了可行的指导。

Abstract: Persona prompting is increasingly used in large language models (LLMs) to
simulate views of various sociodemographic groups. However, how a persona
prompt is formulated can significantly affect outcomes, raising concerns about
the fidelity of such simulations. Using five open-source LLMs, we
systematically examine how different persona prompt strategies, specifically
role adoption formats and demographic priming strategies, influence LLM
simulations across 15 intersectional demographic groups in both open- and
closed-ended tasks. Our findings show that LLMs struggle to simulate
marginalized groups, particularly nonbinary, Hispanic, and Middle Eastern
identities, but that the choice of demographic priming and role adoption
strategy significantly impacts their portrayal. Specifically, we find that
prompting in an interview-style format and name-based priming can help reduce
stereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B
outperform larger ones such as Llama-3.3-70B. Our findings offer actionable
guidance for designing sociodemographic persona prompts in LLM-based simulation
studies.

</details>


### [11] [Efficient Compositional Multi-tasking for On-device Large Language Models](https://arxiv.org/abs/2507.16083)
*Ondrej Bohdal,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.CL

TL;DR: 本文研究了设备端设置下的文本组合多任务问题，并提出了一种高效的解决方案，以支持在资源受限环境下同时执行多个任务。


<details>
  <summary>Details</summary>
Motivation: 当前在大语言模型中的合并工作主要集中在每个测试示例仅涉及单个任务的场景中，而实际应用中需要同时执行多个任务。

Method: 我们提出了一种适用于设备端应用的高效方法（可学习校准），并在设备端设置下研究了基于文本的组合多任务问题。

Result: 我们提出了一个包含四个实际相关组合任务的基准，并展示了可学习校准方法在资源受限环境下的有效性。

Conclusion: 本文的贡献为推进大语言模型在现实世界多任务场景中的能力奠定了基础，扩展了其在复杂、资源受限用例中的适用性。

Abstract: Adapter parameters provide a mechanism to modify the behavior of machine
learning models and have gained significant popularity in the context of large
language models (LLMs) and generative AI. These parameters can be merged to
support multiple tasks via a process known as task merging. However, prior work
on merging in LLMs, particularly in natural language processing, has been
limited to scenarios where each test example addresses only a single task. In
this paper, we focus on on-device settings and study the problem of text-based
compositional multi-tasking, where each test example involves the simultaneous
execution of multiple tasks. For instance, generating a translated summary of a
long text requires solving both translation and summarization tasks
concurrently. To facilitate research in this setting, we propose a benchmark
comprising four practically relevant compositional tasks. We also present an
efficient method (Learnable Calibration) tailored for on-device applications,
where computational resources are limited, emphasizing the need for solutions
that are both resource-efficient and high-performing. Our contributions lay the
groundwork for advancing the capabilities of LLMs in real-world multi-tasking
scenarios, expanding their applicability to complex, resource-constrained use
cases.

</details>


### [12] [BIDWESH: A Bangla Regional Based Hate Speech Detection Dataset](https://arxiv.org/abs/2507.16183)
*Azizul Hakim Fayaz,MD. Shorif Uddin,Rayhan Uddin Bhuiyan,Zakia Sultana,Md. Samiul Islam,Bidyarthi Paul,Tashreef Muhammad,Shahriar Manzoor*

Main category: cs.CL

TL;DR: 本研究介绍了BIDWESH，这是第一个多方言的孟加拉语仇恨言论数据集，通过将BD-SHS语料库中的9,183个实例翻译并标注到三个主要地区方言中构建而成。


<details>
  <summary>Details</summary>
Motivation: Existing datasets and systems fail to address the informal and culturally rich expressions found in dialects such as Barishal, Noakhali, and Chittagong. This oversight results in limited detection capability and biased moderation, leaving large sections of harmful content unaccounted for.

Method: This study introduces BIDWESH, the first multi-dialectal Bangla hate speech dataset, constructed by translating and annotating 9,183 instances from the BD-SHS corpus into three major regional dialects.

Result: The resulting dataset provides a linguistically rich, balanced, and inclusive resource for advancing hate speech detection in Bangla.

Conclusion: BIDWESH lays the groundwork for the development of dialect-sensitive NLP tools and contributes significantly to equitable and context-aware content moderation in low-resource language settings.

Abstract: Hate speech on digital platforms has become a growing concern globally,
especially in linguistically diverse countries like Bangladesh, where regional
dialects play a major role in everyday communication. Despite progress in hate
speech detection for standard Bangla, Existing datasets and systems fail to
address the informal and culturally rich expressions found in dialects such as
Barishal, Noakhali, and Chittagong. This oversight results in limited detection
capability and biased moderation, leaving large sections of harmful content
unaccounted for. To address this gap, this study introduces BIDWESH, the first
multi-dialectal Bangla hate speech dataset, constructed by translating and
annotating 9,183 instances from the BD-SHS corpus into three major regional
dialects. Each entry was manually verified and labeled for hate presence, type
(slander, gender, religion, call to violence), and target group (individual,
male, female, group), ensuring linguistic and contextual accuracy. The
resulting dataset provides a linguistically rich, balanced, and inclusive
resource for advancing hate speech detection in Bangla. BIDWESH lays the
groundwork for the development of dialect-sensitive NLP tools and contributes
significantly to equitable and context-aware content moderation in low-resource
language settings.

</details>


### [13] [Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task](https://arxiv.org/abs/2507.16196)
*Jared Moore,Ned Cooper,Rasmus Overmark,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: 本文提出了一个名为MindGames的新任务，用于评估大型语言模型的心智理论能力。结果显示，人类在需要推断他人信念和欲望的任务中表现优于大型语言模型，但在需要较少心理状态推断的规划任务中表现较差。


<details>
  <summary>Details</summary>
Motivation: 人类的心智理论不仅有助于预测和解释其他代理的行为，还参与动态规划行动和战略性干预他人的心理状态。然而，大多数心智理论实验让参与者处于观察角色。因此，我们需要一个新的评估框架来测试心智理论的实际应用。

Method: 我们提出了MindGames：一个新颖的'规划心智理论'（PToM）任务，要求代理推断对话者的信念和欲望以说服他们改变行为。

Result: 人类在我们的PToM任务中显著优于o1-preview（大型语言模型），得分高出11%（p=0.006）。然而，在需要类似规划但最小心理状态推断的基线条件下，o1-preview的表现优于人类。

Conclusion: 这些结果表明，人类的社交推理能力和大型语言模型的能力之间存在显著差距。

Abstract: Recent evidence suggests Large Language Models (LLMs) display Theory of Mind
(ToM) abilities. Most ToM experiments place participants in a spectatorial
role, wherein they predict and interpret other agents' behavior. However, human
ToM also contributes to dynamically planning action and strategically
intervening on others' mental states. We present MindGames: a novel `planning
theory of mind' (PToM) task which requires agents to infer an interlocutor's
beliefs and desires to persuade them to alter their behavior. Unlike previous
evaluations, we explicitly evaluate use cases of ToM. We find that humans
significantly outperform o1-preview (an LLM) at our PToM task (11% higher;
$p=0.006$). We hypothesize this is because humans have an implicit causal model
of other agents (e.g., they know, as our task requires, to ask about people's
preferences). In contrast, o1-preview outperforms humans in a baseline
condition which requires a similar amount of planning but minimal mental state
inferences (e.g., o1-preview is better than humans at planning when already
given someone's preferences). These results suggest a significant gap between
human-like social reasoning and LLM abilities.

</details>


### [14] [WakenLLM: A Fine-Grained Benchmark for Evaluating LLM Reasoning Potential and Reasoning Process Stability](https://arxiv.org/abs/2507.16199)
*Zipeng Ling,Yuehao Tang,Shuliang Liu,Junqi Yang,Shenghong Fu,Yao Wan,Kejia Huang,Zhichao Hou,Xuming Hu*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）中“未知”回答的现象，并提出了一种框架来量化这些回答的原因，同时测试了引导刺激能否将其转化为正确或本质上不可确定的结果。


<details>
  <summary>Details</summary>
Motivation: 当前评估主要关注“未知”回答是否诚实，而不是它们出现的原因，这模糊了两种不同的情况：（i）一个真正不确定的输入和（ii）模型无法解决的可解问题。

Method: 我们引入了一个框架，量化了由于模型能力不足而产生的“未知”响应的比例，并测试了引导刺激是否能将其转化为正确的（“已知”）或本质上不可确定的结果。

Result: 通过在不同LLM上进行理论推理任务的准确性测试，我们应用了不同的方法来测试模型是否能在基准框架下达到给定的准确性。

Conclusion: 我们的方法通过区分不确定性来源，提供了对LLM推理限制及其改进潜力的更清晰画面。

Abstract: Large Language Models (LLMs) frequently output the label \emph{Unknown}, yet
current evaluations focus almost exclusively on whether such answers are
\emph{honest} rather than why they arise. This blurs two distinct cases: (i) an
input that is genuinely indeterminate and (ii) a solvable problem that the
model fails to resolve. We call this phenomenon \emph{Vague Perception}. And
thus we introduce a framework that quantifies the proportion of \emph{Unknown}
responses attributable to model incapacity and tests whether guided stimulation
can convert them into either correct (\emph{Known}) or intrinsically
indeterminate outcomes. By separating these sources of uncertainty, our method
provides a clearer picture of LLM reasoning limits and their potential for
improvement. As we get a theoretical accuracy of reasoning task on different
LLMs, we apply different methods to test whether the model can reach the
accuracy given a baseline framework. Our work is meaningful in exploring the
true reasoning ability of LLMs and providing a new perspective on solving the
\emph{Vague Perception} phenomenon.

</details>


### [15] [Towards Compute-Optimal Many-Shot In-Context Learning](https://arxiv.org/abs/2507.16217)
*Shahriar Golchin,Yanfei Chen,Rujun Han,Manan Gandhi,Tianli Yu,Swaroop Mishra,Mihai Surdeanu,Rishabh Agarwal,Chen-Yu Lee,Tomas Pfister*

Main category: cs.CL

TL;DR: 本文提出两种演示选择策略，以在许多shot ICL中提高性能并减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 在许多shot ICL中，固定的一组演示通常被随机选择，因为高推理成本、缓存和重用计算的好处以及这种策略与其他策略相比的类似性能。然而，我们希望提出更有效的演示选择策略，以提高性能并减少推理成本。

Method: 我们提出了两种简单的演示选择策略，以在许多shot ICL中提高性能。第一种方法结合了少量基于测试样本相似性的演示和大量随机演示。第二种方法通过使用k-means聚类从测试样本表示中获得的中心点来替换随机演示，从而改进了第一种方法。

Result: 我们的实验表明，我们的策略在多个数据集上 consistently 超过随机选择，并且超越或匹配最有效的选择方法，同时支持缓存并减少推理成本多达一个数量级。

Conclusion: 我们的策略在许多shot ICL中表现出色，同时支持缓存并减少了推理成本。

Abstract: Long-context large language models (LLMs) are able to process inputs
containing up to several million tokens. In the scope of in-context learning
(ICL), this translates into using hundreds/thousands of demonstrations in the
input prompt, enabling many-shot ICL. In practice, a fixed set of
demonstrations is often selected at random in many-shot settings due to (1)
high inference costs, (2) the benefits of caching and reusing computations, and
(3) the similar performance offered by this strategy compared to others when
scaled. In this work, we propose two straightforward strategies for
demonstration selection in many-shot ICL that improve performance with minimal
computational overhead. Our first method combines a small number of
demonstrations, selected based on their similarity to each test sample, with a
disproportionately larger set of random demonstrations that are cached. The
second strategy improves the first by replacing random demonstrations with
those selected using centroids derived from test sample representations via
k-means clustering. Our experiments with Gemini Pro and Flash across several
datasets indicate that our strategies consistently outperform random selection
and surpass or match the most performant selection approach while supporting
caching and reducing inference cost by up to an order of magnitude. We also
show that adjusting the proportion of demonstrations selected based on
different criteria can balance performance and inference cost in many-shot ICL.

</details>


### [16] [FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents](https://arxiv.org/abs/2507.16248)
*Run Sun,Zuo Bai,Wentao Zhang,Yuxiang Zhang,Li Zhao,Shan Sun,Zhengwen Qiu*

Main category: cs.CL

TL;DR: 本文介绍了FinResearchBench，一个基于逻辑树的Agent-as-a-Judge系统，专门用于评估金融研究代理的能力，覆盖7种关键任务类型。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏系统性和自动化的评估框架和基准来研究这些研究代理的能力，特别是金融研究问题具有独特的复杂性和细微差别。因此，本文旨在填补这一空白。

Method: 本文提出了一种基于逻辑树的Agent-as-a-Judge系统，用于评估金融研究代理的能力。该系统通过提取研究结果的逻辑树作为中间信息，实现全面、可靠和稳健的评估。

Result: 本文提出的FinResearchBench能够对金融研究代理进行全面和自动的评估，覆盖70个典型的金融研究问题，涉及7种常见的任务类型。

Conclusion: 本文提出了FinResearchBench，这是一个基于逻辑树的Agent-as-a-Judge系统，专门针对金融研究代理。它提供了对金融研究领域中7种关键类型任务的全面和自动评估。

Abstract: Recently, AI agents are rapidly evolving in intelligence and widely used in
professional research applications, such as STEM, software development,
finance, etc. Among these AI agents, deep research agent is a key category as
it can perform long-horizon tasks and solve problems of greater complexity.
However, there are few evaluation frameworks and benchmarks that systematically
and automatically investigate the capabilities of these research agents.
Furthermore, financial research problems have distinct complexity and subtlety.
To fill in the gap, we propose FinResearchBench, which is a logic tree based
Agent-as-a-Judge and targets specifically for the financial research agents. It
provides a comprehensive and automatic assessment of the research agents across
7 key types of tasks in the financial research domain. The contributions of
this work are two-folded: (1) the first and innovative Agent-as-a-Judge system
that extracts the logic tree of the research outcome and uses it as the
intermediate information to present a comprehensive, reliable and robust
evaluation; (2) finance oriented that it covers 70 typical financial research
questions, spreading across 7 frequently encountered types of tasks in the
domain.

</details>


### [17] [Efficient RL for optimizing conversation level outcomes with an LLM-based tutor](https://arxiv.org/abs/2507.16252)
*Hyunji Nam,Omer Gottesman,Amy Zhang,Dean Foster,Emma Brunskill,Lyle Ungar*

Main category: cs.CL

TL;DR: 本文提出了一种基于学生潜在状态表示和长期策略优化的方法，以改进基于LLM的导师，使其在多轮对话中更好地引导学生独立解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RLHF框架的大型语言模型（LLMs）在多轮对话设置中表现不足，例如在线数学辅导。需要一种更有效的方法来优化导师的行为，使其更符合长期目标，即引导学生独立解决目标数学问题。

Method: 我们提出了一种方法，通过将对话历史表示为学生的低维潜在状态表示，并优化长期策略来确定基于潜在状态的高层次动作，以改进基于LLM的导师。

Result: 实验结果表明，这些修改在LLM模拟的辅导任务中带来了更好的长期结果，同时模型轻量，计算资源需求低于之前的工作。

Conclusion: 我们的模型在长期结果上优于直接提示的LLM模拟辅导任务，表明通过学生潜在状态表示和长期策略优化可以更好地引导学生独立解决问题。

Abstract: Large language models (LLMs) built on existing reinforcement learning with
human feedback (RLHF) frameworks typically optimize responses based on
immediate turn-level human preferences. However, this approach falls short in
multi-turn dialogue settings, such as online math tutoring. We propose a method
to enhance LLM-based tutors by representing the dialogue history with a
lower-dimensional latent state representation of a student and optimizing a
long-term policy to determine high-level actions based on the latent state. The
goal is to better align the tutor's behavior with the long-term objective of
guiding the student towards solving a target math problem on their own. Our
model is lightweight, requiring less computational resources than prior work of
training the tutor policy end-to-end to directly output the tutor's next
utterance. Our experiment results demonstrate that these modifications lead to
improved long-term outcomes compared to prompting in LLM-simulated tutoring
tasks.

</details>


### [18] [iShumei-Chinchunmei at SemEval-2025 Task 4: A balanced forgetting and retention multi-task framework using effective unlearning loss](https://arxiv.org/abs/2507.16263)
*Yujian Sun,Tian Li*

Main category: cs.CL

TL;DR: 本文提出了一种更可控的遗忘损失，并探索了其与各种技术的集成，以实现更高效和可控的遗忘。我们的系统在比赛中排名第五。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的广泛采用，越来越多的关注集中在如何在有限的计算资源下有效地从LLM中删除敏感信息。

Method: 我们提出了一种更可控的遗忘损失，即有效遗忘损失，并探索了其与各种技术的集成，以实现更高效和可控的遗忘。

Result: 我们的系统在SemEval 2025任务4的比赛中排名第五。

Conclusion: 我们的系统在比赛中排名第五，表明我们提出的有效遗忘损失在实现更高效和可控的遗忘方面是有效的。

Abstract: As the Large Language Model (LLM) gains widespread adoption, increasing
attention has been given to the challenge of making LLM forget non-compliant
data memorized during its pre-training. Machine Unlearning focuses on
efficiently erasing sensitive information from LLM under limited computational
resources. To advance research in this area, SemEval 2025 Task 4: "Unlearning
Sensitive Content from Large Language Models" introduces three unlearning
datasets and establishes a benchmark by evaluating both forgetting
effectiveness and the preservation of standard capabilities. In this work, we
propose a more controllable forgetting loss, Effective Unlearning Loss, and
explore its integration with various techniques to achieve more efficient and
controlled unlearning. Our system ultimately ranked 5th on the competition
leaderboard.

</details>


### [19] [Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep Knowledge Extraction](https://arxiv.org/abs/2507.16271)
*Tianyun Zhong,Guozhao Mo,Yanjiang Liu,Yihan Chen,Lingdi Kong,Xuanang Chen,Yaojie Lu,Hongyu Lin,Ben He,Le Sun*

Main category: cs.CL

TL;DR: 本文介绍了AOE基准，用于评估LLMs从复杂文档中提取和组织信息的能力，并发现即使最先进的模型也存在困难。


<details>
  <summary>Details</summary>
Motivation: 当前大多数LLMs生成的段落式答案混乱、无序且不可追溯，无法有效提取复杂现实文档中的显式信息。因此，需要一个新的基准来评估LLMs在这方面的表现。

Method: 引入了Arranged and Organized Extraction Benchmark (AOE)，这是一个新的双语基准，包含不同长度的数据和文档，用于系统评估LLMs理解和重组碎片化文档的能力。

Result: 实验结果表明，即使是最先进的LLMs在AOE基准测试中也面临显著挑战，显示出在处理复杂文档和生成结构化信息方面的不足。

Conclusion: 即使最先进的模型在AOE基准测试中也表现出显著的困难，这表明需要进一步改进LLMs在处理复杂文档和提取结构化信息方面的能力。

Abstract: With the emergence of large language models (LLMs), there is an expectation
that LLMs can effectively extract explicit information from complex real-world
documents (e.g., papers, reports). However, most LLMs generate paragraph-style
answers that are chaotic, disorganized, and untraceable. To bridge this gap, we
introduce the Arranged and Organized Extraction Benchmark (AOE), a new
bilingual benchmark with data and documents of varying lengths designed to
systematically evaluate the ability of LLMs to comprehend fragmented documents
and reconstruct isolated information into one organized table. Unlike
conventional text-to-table tasks, which rely on fixed schema and narrow task
domains, AOE includes 11 carefully crafted tasks across three diverse domains,
requiring models to generate context-specific schema tailored to varied input
queries. In the experiment, we evaluated both open-source and closed-source
state-of-the-art LLMs. The results show that even the most advanced models
struggled significantly. The benchmark is available at
https://huggingface.co/datasets/tianyumyum/AOE.

</details>


### [20] [Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis](https://arxiv.org/abs/2507.16284)
*Paul-Andrei Pogăcean,Sanda-Maria Avram*

Main category: cs.CL

TL;DR: 本研究探讨了通过单字和双字频率排名实现语言确定性的数学算法，结果显示经典频率方法在语言检测中依然有效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 近年来，随着AI驱动的语言模型的快速发展，关于语言识别的争论再次受到关注，但非AI方法被忽视。

Method: 通过利用从已建立的语言学研究中得出的单字和双字频率排名，对语言确定性算法进行数学实现。

Result: 该方法在150个字符以下的文本上实现了80%以上的准确率，并在较长的文本和较古老的作品中达到了100%的准确率。

Conclusion: 古典的基于频率的方法仍然是语言检测的有效且可扩展的替代方案。

Abstract: The debate surrounding language identification has gained renewed attention
in recent years, especially with the rapid evolution of AI-powered language
models. However, the non-AI-based approaches to language identification have
been overshadowed. This research explores a mathematical implementation of an
algorithm for language determinism by leveraging monograms and bigrams
frequency rankings derived from established linguistic research. The datasets
used comprise texts varying in length, historical period, and genre, including
short stories, fairy tales, and poems. Despite these variations, the method
achieves over 80\% accuracy on texts shorter than 150 characters and reaches
100\% accuracy for longer texts and older writings. These results demonstrate
that classical frequency-based approaches remain effective and scalable
alternatives to AI-driven models for language detection.

</details>


### [21] [SpeLLM: Character-Level Multi-Head Decoding](https://arxiv.org/abs/2507.16323)
*Amit Ben-Artzy,Roy Schwartz*

Main category: cs.CL

TL;DR: 本文提出了SpeLLM方法，通过解耦输入和输出词汇表，使大型语言模型能够更高效地扩展，并在保持性能的同时减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 当前LLM架构在扩展词汇表时存在关键瓶颈：输出投影层随着词汇大小线性增长，使得大规模扩展不切实际。

Method: 我们提出了一种称为SpeLLM的方法，通过多个输出头预测字符级字符串来解耦输入和输出词汇表。每个线性头同时预测一个字符，使模型能够使用较小的独立线性头表示更大的输出空间。我们还提出了一种自蒸馏方法，将标准LLM转换为SpeLLM。

Result: 我们在四个预训练LLM上进行了实验，结果显示它们的SpeLLM变体在下游任务中表现出色，平均运行时间减少了5.1%。

Conclusion: 我们的方法为减少大型语言模型的成本提供了潜在的途径，同时增加了对欠代表语言和领域的支持。

Abstract: Scaling LLM vocabulary is often used to reduce input sequence length and
alleviate attention's quadratic cost. Yet, current LLM architectures impose a
critical bottleneck to this procedure: the output projection layer scales
linearly with vocabulary size, rendering substantial expansion impractical. We
propose SpeLLM, a method that decouples input and output vocabularies by
predicting character-level strings through multiple output heads. In SpeLLM,
each of the $k$ linear heads predicts a single character simultaneously,
enabling the model to represent a much larger output space using smaller,
independent linear heads. We present a self-distillation approach for
converting a standard LLM to a SpeLLM. Our experiments with four pre-trained
LLMs show their SpeLLM variants achieve competitive performance on downstream
tasks while reducing runtime by 5.1% on average across models. Our approach
provides a potential avenue for reducing LLM costs, while increasing support
for underrepresented languages and domains.

</details>


### [22] [Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny](https://arxiv.org/abs/2507.16331)
*Chuanhao Yan,Fengdi Che,Xuhan Huang,Xu Xu,Xin Li,Yizhi Li,Xingwei Qu,Jingzhe Shi,Zhuangzhuang He,Chenghua Lin,Yaodong Yang,Binhang Yuan,Hang Zhao,Yu Qiao,Bowen Zhou,Jie Fu*

Main category: cs.CL

TL;DR: 本文研究了如何利用形式语言Dafny减少人类先验，通过自动数据整理和强化学习设计，实现了在形式软件验证中的重大进展。


<details>
  <summary>Details</summary>
Motivation: 现有的基于非正式语言的大型语言模型在训练过程中面临验证过程不可靠和不可扩展的问题，而使用人类标注的思维链和其他人类先验来引导模型的推理和编码能力变得越来越难以承受。

Method: 本文提出了一种自动可扩展的数据整理管道，并结合形式语言验证器的反馈进行强化学习设计，同时引入了DafnyComp基准测试，用于组合形式程序的规范推理。

Result: 本文的监督微调阶段使小型模型（如0.5B）能够生成语法正确且可验证的Dafny代码，超越了专有模型。强化学习进一步提高了性能，在跨域任务中表现出更强的泛化能力，并在具有挑战性的DafnyComp基准测试中优于所有强基线。

Conclusion: 本文提出了一种利用形式语言Dafny作为主要环境来减少人类先验的方法，通过自动可扩展的数据整理管道和与形式语言验证器反馈集成的强化学习设计，实现了在大规模、可靠的形式软件验证中的重要突破。

Abstract: Existing informal language-based (e.g., human language) Large Language Models
(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:
their verification processes, which provide crucial training signals, are
neither reliable nor scalable. In fact, the prevalent large proprietary models
could hardly generate verifiable programs. A promising yet largely uncharted
alternative is formal language-based reasoning. Grounding LLMs in rigorous
formal systems where generative models operate in formal language spaces (e.g.,
Dafny) enables the automatic and mathematically provable verification of their
reasoning processes and outcomes. This capability is pivotal for achieving
large-scale, reliable formal software verification. It is a common practice to
employ human-annotated chain-of-thought and other human priors to induce the
reasoning and coding capabilities of LLMs. Unfortunately, it becomes
unacceptably all-consuming to provide such priors for supervising complex
programming tasks. In this work, we systematically explore ways to reduce human
priors with the formal language, Dafny, as the main environment for our pilot
study. Our pipeline mainly relies on introducing an automatic and scalable data
curation pipeline, and careful RL designs integrated with feedback from the
formal language verifier. We introduce DafnyComp, a benchmark of compositional
formal programs with auto-formalized specifications for specification
reasoning. Our supervised fine-tuning (SFT) stage enables even small models
(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,
surpassing proprietary models. RL with regularization further improves
performance, achieving stronger generalization to out-of-domain tasks and
outperforming all strong baselines on the challenging DafnyComp benchmark.

</details>


### [23] [GG-BBQ: German Gender Bias Benchmark for Question Answering](https://arxiv.org/abs/2507.16410)
*Shalaka Satheesh,Katrin Klug,Katharina Beckh,Héctor Allende-Cid,Sebastian Houben,Teena Hassan*

Main category: cs.CL

TL;DR: 本研究评估了德语大型语言模型中的性别偏见，并发现手动修订机器翻译对于创建用于性别偏见评估的数据集至关重要。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理（NLP）的背景下，公平性评估通常与偏差评估和减少相关危害有关。然而，由于英语到德语等具有语法性别的语言的机器翻译存在局限性，因此在创建用于性别偏差评估的数据集时，手动修订翻译至关重要。

Method: 我们使用Parrish等人（2022）的问答偏见基准测试来评估德语大型语言模型（LLMs）中的性别偏见。具体来说，该英语数据集中的性别认同子集的模板被机器翻译成德语，并由语言专家手动审查和纠正。

Result: 我们创建了一个包含两个子集的新数据集：子集-I，其中包含与性别认同相关的组术语；子集-II，其中组术语被替换为专有名词。我们在该新数据集上评估了几种用于德语NLP的LLMs，并报告了准确率和偏差分数。结果表明，所有模型都表现出偏差，无论是沿着还是反对现有的社会刻板印象。

Conclusion: 我们的研究结果表明，所有模型都表现出偏差，无论是沿着还是反对现有的社会刻板印象。

Abstract: Within the context of Natural Language Processing (NLP), fairness evaluation
is often associated with the assessment of bias and reduction of associated
harm. In this regard, the evaluation is usually carried out by using a
benchmark dataset, for a task such as Question Answering, created for the
measurement of bias in the model's predictions along various dimensions,
including gender identity. In our work, we evaluate gender bias in German Large
Language Models (LLMs) using the Bias Benchmark for Question Answering by
Parrish et al. (2022) as a reference. Specifically, the templates in the gender
identity subset of this English dataset were machine translated into German.
The errors in the machine translated templates were then manually reviewed and
corrected with the help of a language expert. We find that manual revision of
the translation is crucial when creating datasets for gender bias evaluation
because of the limitations of machine translation from English to a language
such as German with grammatical gender. Our final dataset is comprised of two
subsets: Subset-I, which consists of group terms related to gender identity,
and Subset-II, where group terms are replaced with proper names. We evaluate
several LLMs used for German NLP on this newly created dataset and report the
accuracy and bias scores. The results show that all models exhibit bias, both
along and against existing social stereotypes.

</details>


### [24] [PromptAL: Sample-Aware Dynamic Soft Prompts for Few-Shot Active Learning](https://arxiv.org/abs/2507.16424)
*Hui Xiang,Jinqiao Shi,Ting Zhang,Xiaojie Zhao,Yong Liu,Yong Ma*

Main category: cs.CL

TL;DR: PromptAL is a new active learning framework that uses unlabeled data to improve the decision boundary and select more representative samples, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing active learning methods overlook the role of unlabeled samples in enhancing the empirical distribution to better align with the target distribution, resulting in a suboptimal decision boundary and the selection of samples that inadequately represent the target distribution.

Method: PromptAL is a hybrid active learning framework that leverages unlabeled data to construct sample-aware dynamic soft prompts, adjusting the model's predictive distribution and decision boundary. It integrates uncertainty estimation with both global and local diversity to select high-quality samples.

Result: PromptAL achieves superior performance over nine baselines on six in-domain and three out-of-domain datasets.

Conclusion: PromptAL achieves superior performance over nine baselines on six in-domain and three out-of-domain datasets.

Abstract: Active learning (AL) aims to optimize model training and reduce annotation
costs by selecting the most informative samples for labeling. Typically, AL
methods rely on the empirical distribution of labeled data to define the
decision boundary and perform uncertainty or diversity estimation, subsequently
identifying potential high-quality samples. In few-shot scenarios, the
empirical distribution often diverges significantly from the target
distribution, causing the decision boundary to shift away from its optimal
position. However, existing methods overlook the role of unlabeled samples in
enhancing the empirical distribution to better align with the target
distribution, resulting in a suboptimal decision boundary and the selection of
samples that inadequately represent the target distribution. To address this,
we propose a hybrid AL framework, termed \textbf{PromptAL} (Sample-Aware
Dynamic Soft \textbf{Prompts} for Few-Shot \textbf{A}ctive \textbf{L}earning).
This framework accounts for the contribution of each unlabeled data point in
aligning the current empirical distribution with the target distribution,
thereby optimizing the decision boundary. Specifically, PromptAL first
leverages unlabeled data to construct sample-aware dynamic soft prompts that
adjust the model's predictive distribution and decision boundary. Subsequently,
based on the adjusted decision boundary, it integrates uncertainty estimation
with both global and local diversity to select high-quality samples that more
accurately represent the target distribution. Experimental results on six
in-domain and three out-of-domain datasets show that PromptAL achieves superior
performance over nine baselines. Our codebase is openly accessible.

</details>


### [25] [Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases in Language Models for Dutch](https://arxiv.org/abs/2507.16442)
*Elza Strazda,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本研究引入了荷兰版的CrowS-Pairs数据集，用于测量荷兰语言模型中的偏见，并发现不同语言模型在不同偏见类别中表现出显著的偏见差异。此外，分配角色会影响语言模型的偏见水平。


<details>
  <summary>Details</summary>
Motivation: 由于语言模型容易表现出偏见，进一步放大不公平和有害的刻板印象，因此需要确保安全和公平的语言模型。尽管最近对测量语言模型中的偏见给予了相当多的关注，但大多数研究只集中在英语语言上。

Method: 引入了荷兰版的CrowS-Pairs数据集，用于测量荷兰语言模型中的偏见，并使用该数据集评估了多种语言模型的偏见水平。此外，还使用英语和法语版本的CrowS-Pairs数据集评估了英语和法语语言模型的偏见。

Result: 各种语言模型（如BERTje、RobBERT、多语言BERT、GEITje和Mistral-7B）在各种偏见类别中表现出显著的偏见。英语模型表现出最多的偏见，而荷兰模型表现出最少的偏见。此外，结果还表明，为语言模型分配一个角色会改变它表现出的偏见水平。

Conclusion: 这些发现突显了语言和语境中偏见的可变性，表明文化和语言因素在塑造模型偏见中起着重要作用。

Abstract: Warning: This paper contains explicit statements of offensive stereotypes
which might be upsetting.
  Language models are prone to exhibiting biases, further amplifying unfair and
harmful stereotypes. Given the fast-growing popularity and wide application of
these models, it is necessary to ensure safe and fair language models. As of
recent considerable attention has been paid to measuring bias in language
models, yet the majority of studies have focused only on English language. A
Dutch version of the US-specific CrowS-Pairs dataset for measuring bias in
Dutch language models is introduced. The resulting dataset consists of 1463
sentence pairs that cover bias in 9 categories, such as Sexual orientation,
Gender and Disability. The sentence pairs are composed of contrasting
sentences, where one of the sentences concerns disadvantaged groups and the
other advantaged groups. Using the Dutch CrowS-Pairs dataset, we show that
various language models, BERTje, RobBERT, multilingual BERT, GEITje and
Mistral-7B exhibit substantial bias across the various bias categories. Using
the English and French versions of the CrowS-Pairs dataset, bias was evaluated
in English (BERT and RoBERTa) and French (FlauBERT and CamemBERT) language
models, and it was shown that English models exhibit the most bias, whereas
Dutch models the least amount of bias. Additionally, results also indicate that
assigning a persona to a language model changes the level of bias it exhibits.
These findings highlight the variability of bias across languages and contexts,
suggesting that cultural and linguistic factors play a significant role in
shaping model biases.

</details>


### [26] [Towards Enforcing Company Policy Adherence in Agentic Workflows](https://arxiv.org/abs/2507.16459)
*Naama Zwerdling,David Boaz,Ella Rabinovich,Guy Uziel,David Amid,Ateret Anaby-Tavor*

Main category: cs.CL

TL;DR: 本文提出了一种用于在代理工作流中强制执行业务政策合规性的框架，并在$	au$-bench Airlines领域进行了测试，取得了初步成功。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在遵循复杂公司政策方面存在困难，因此需要一种可靠的方法来确保业务政策的遵守。

Method: 本文的方法分为两个阶段：(1) 离线构建阶段，将政策文档编译为与工具使用相关的可验证保护代码；(2) 运行时集成阶段，在代理每次操作前确保合规性。

Result: 在$	au$-bench Airlines领域，本文方法展示了有希望的初步结果，证明了其在政策执行方面的有效性。

Conclusion: 本文提出了一个确定性、透明且模块化的框架，用于在代理工作流中强制执行业务政策合规性，并在$	au$-bench Airlines领域展示了有希望的初步结果，同时指出了实际部署中的关键挑战。

Abstract: Large Language Model (LLM) agents hold promise for a flexible and scalable
alternative to traditional business process automation, but struggle to
reliably follow complex company policies. In this study we introduce a
deterministic, transparent, and modular framework for enforcing business policy
adherence in agentic workflows. Our method operates in two phases: (1) an
offline buildtime stage that compiles policy documents into verifiable guard
code associated with tool use, and (2) a runtime integration where these guards
ensure compliance before each agent action. We demonstrate our approach on the
challenging $\tau$-bench Airlines domain, showing encouraging preliminary
results in policy enforcement, and further outline key challenges for
real-world deployments.

</details>


### [27] [ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs](https://arxiv.org/abs/2507.16488)
*Zhenliang Zhang,Xinyu Hu,Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文提出了一种新的幻觉检测方法ICR Probe，利用ICR Score量化模块对隐藏状态更新的贡献，实现了优越的性能并提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法主要关注静态和孤立的表示，忽略了隐藏状态在不同层中的动态演变，这限制了其效果。

Method: 本文提出了ICR Score（信息贡献到残差流），用于量化模块对隐藏状态更新的贡献，并基于此提出了ICR Probe方法来检测幻觉。

Result: 实验结果表明，ICR Probe在减少参数的情况下表现出色，并且通过消融研究和案例分析提供了更深入的机制理解。

Conclusion: 本文提出了一种新的检测幻觉的方法ICR Probe，该方法在减少参数的同时实现了优越的性能，并通过消融研究和案例分析提高了可解释性。

Abstract: Large language models (LLMs) excel at various natural language processing
tasks, but their tendency to generate hallucinations undermines their
reliability. Existing hallucination detection methods leveraging hidden states
predominantly focus on static and isolated representations, overlooking their
dynamic evolution across layers, which limits efficacy. To address this
limitation, we shift the focus to the hidden state update process and introduce
a novel metric, the ICR Score (Information Contribution to Residual Stream),
which quantifies the contribution of modules to the hidden states' update. We
empirically validate that the ICR Score is effective and reliable in
distinguishing hallucinations. Building on these insights, we propose a
hallucination detection method, the ICR Probe, which captures the cross-layer
evolution of hidden states. Experimental results show that the ICR Probe
achieves superior performance with significantly fewer parameters. Furthermore,
ablation studies and case analyses offer deeper insights into the underlying
mechanism of this method, improving its interpretability.

</details>


### [28] [Combining Language and Topic Models for Hierarchical Text Classification](https://arxiv.org/abs/2507.16490)
*Jaco du Toit,Marcel Dunaiski*

Main category: cs.CL

TL;DR: 本文研究了结合预训练语言模型和主题模型提取特征对层级文本分类的影响，发现主题模型的特征可能不利于分类性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在确定结合PLM和主题模型提取的特征是否普遍有助于层级文本分类（HTC）性能的提升。

Method: 本文提出了一种使用预训练语言模型（PLM）和主题模型提取文本文档特征，并通过独立的卷积层和标签特定注意力机制进行分类的方法。

Result: 实验结果表明，使用主题模型提取的特征通常会降低分类性能，相较于仅使用PLM提取的特征。

Conclusion: 本文表明，将主题模型提取的特征用于文本分类任务不应被假设是有益的。

Abstract: Hierarchical text classification (HTC) is a natural language processing task
which has the objective of categorising text documents into a set of classes
from a predefined structured class hierarchy. Recent HTC approaches use various
techniques to incorporate the hierarchical class structure information with the
natural language understanding capabilities of pre-trained language models
(PLMs) to improve classification performance. Furthermore, using topic models
along with PLMs to extract features from text documents has been shown to be an
effective approach for multi-label text classification tasks. The rationale
behind the combination of these feature extractor models is that the PLM
captures the finer-grained contextual and semantic information while the topic
model obtains high-level representations which consider the corpus of documents
as a whole. In this paper, we use a HTC approach which uses a PLM and a topic
model to extract features from text documents which are used to train a
classification model. Our objective is to determine whether the combination of
the features extracted from the two models is beneficial to HTC performance in
general. In our approach, the extracted features are passed through separate
convolutional layers whose outputs are combined and passed to a label-wise
attention mechanisms which obtains label-specific document representations by
weighing the most important features for each class separately. We perform
comprehensive experiments on three HTC benchmark datasets and show that using
the features extracted from the topic model generally decreases classification
performance compared to only using the features obtained by the PLM. In
contrast to previous work, this shows that the incorporation of features
extracted from topic models for text classification tasks should not be assumed
beneficial.

</details>


### [29] [The Ever-Evolving Science Exam](https://arxiv.org/abs/2507.16514)
*Junying Wang,Zicheng Zhang,Yijin Guo,Farong Wen,Ye Shen,Yingji Liang,Yalun Wu,Wenzhe Li,Chunyi Li,Zijian Chen,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: This paper introduces EESE, a dynamic benchmark for assessing scientific capabilities in foundation models, addressing issues of data leakage and evaluation inefficiency.


<details>
  <summary>Details</summary>
Motivation: Existing science benchmarks have made progress towards broad Range, wide Reach, and high Rigor, yet they often face two major challenges: data leakage risks that compromise benchmarking validity, and evaluation inefficiency due to large-scale testing.

Method: introduce the Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. The approach consists of two components: EESE-Pool and EESE.

Result: Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions.

Conclusion: EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions.

Abstract: As foundation models grow rapidly in capability and deployment, evaluating
their scientific understanding becomes increasingly critical. Existing science
benchmarks have made progress towards broad **Range**, wide **Reach**, and high
**Rigor**, yet they often face two major challenges: **data leakage risks**
that compromise benchmarking validity, and **evaluation inefficiency** due to
large-scale testing. To address these issues, we introduce the **Ever-Evolving
Science Exam (EESE)**, a dynamic benchmark designed to reliably assess
scientific capabilities in foundation models. Our approach consists of two
components: 1) a non-public **EESE-Pool** with over 100K expertly constructed
science instances (question-answer pairs) across 5 disciplines and 500+
subfields, built through a multi-stage pipeline ensuring **Range**, **Reach**,
and **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled
and validated to enable leakage-resilient, low-overhead evaluations.
Experiments on 32 open- and closed-source models demonstrate that EESE
effectively differentiates the strengths and weaknesses of models in scientific
fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and
forward-compatible solution for science benchmark design, offering a realistic
measure of how well foundation models handle science questions. The project
page is at: https://github.com/aiben-ch/EESE.

</details>


### [30] [Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness](https://arxiv.org/abs/2507.16515)
*Siqi Liu,Guangrong Dai,Dechao Li*

Main category: cs.CL

TL;DR: 本研究探讨了句子级质量评估（QE）在英语-中文机器翻译后期编辑（MTPE）中的作用，发现QE能显著减少后期编辑时间，并在不同质量的MT输出和不同专业水平的学生译者中保持效率。此外，QE还能帮助译者验证MT质量并复查翻译输出，但不准确的QE可能会影响后期编辑过程。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨句子级质量评估（QE）在英语-中文机器翻译后期编辑（MTPE）中的作用，以及其对后期编辑速度和学生译者感知的影响。

Method: 本研究调查了句子级质量评估（QE）在英语-中文机器翻译后期编辑（MTPE）中的有用性，重点是其对后期编辑速度和学生译者感知的影响。还探讨了QE与MT质量以及QE与翻译专业知识之间的交互效应。

Result: 研究结果表明，QE显著减少了后期编辑时间。所研究的交互效应不显著，表明QE在中等和高质量的MT输出以及具有不同专业水平的学生译者中一致提高了MTPE效率。此外，QE除了指出潜在问题段落外，在MTPE中还具有多种功能，例如验证译者对MT质量的评估并使他们能够复查翻译输出。然而，访谈数据表明，不准确的QE可能会阻碍后期编辑过程。

Conclusion: 本研究提供了关于QE在MTPE中的优势和局限性的新见解，有助于其更有效的整合到MTPE工作流程中，以提高译者的生产力。

Abstract: This preliminary study investigates the usefulness of sentence-level Quality
Estimation (QE) in English-Chinese Machine Translation Post-Editing (MTPE),
focusing on its impact on post-editing speed and student translators'
perceptions. It also explores the interaction effects between QE and MT
quality, as well as between QE and translation expertise. The findings reveal
that QE significantly reduces post-editing time. The examined interaction
effects were not significant, suggesting that QE consistently improves MTPE
efficiency across medium- and high-quality MT outputs and among student
translators with varying levels of expertise. In addition to indicating
potentially problematic segments, QE serves multiple functions in MTPE, such as
validating translators' evaluations of MT quality and enabling them to
double-check translation outputs. However, interview data suggest that
inaccurate QE may hinder post-editing processes. This research provides new
insights into the strengths and limitations of QE, facilitating its more
effective integration into MTPE workflows to enhance translators' productivity.

</details>


### [31] [Learning Text Styles: A Study on Transfer, Attribution, and Verification](https://arxiv.org/abs/2507.16530)
*Zhiqiang Hu*

Main category: cs.CL

TL;DR: 本论文通过三种相互关联的支柱推进了文本风格的计算理解和操控，为文本风格转移、作者归属和作者验证提供了新的方法和见解。


<details>
  <summary>Details</summary>
Motivation: 为了提升对文本风格的计算理解和操控能力，本论文旨在解决文本风格转移、作者归属和作者验证中的关键问题。

Method: 通过参数高效的大型语言模型（LLMs）适应、对比解缠 stylistic 特征以及基于指令的微调来解决关键挑战。

Result: 提出了参数高效的大型语言模型适应、对比解缠 stylistic 特征以及基于指令的微调方法，为文本风格转移、作者归属和作者验证提供了新的解决方案。

Conclusion: 本论文通过三种相互关联的支柱推进了文本风格的计算理解和操控，为文本风格转移、作者归属和作者验证提供了新的方法和见解。

Abstract: This thesis advances the computational understanding and manipulation of text
styles through three interconnected pillars: (1) Text Style Transfer (TST),
which alters stylistic properties (e.g., sentiment, formality) while preserving
content; (2)Authorship Attribution (AA), identifying the author of a text via
stylistic fingerprints; and (3) Authorship Verification (AV), determining
whether two texts share the same authorship. We address critical challenges in
these areas by leveraging parameter-efficient adaptation of large language
models (LLMs), contrastive disentanglement of stylistic features, and
instruction-based fine-tuning for explainable verification.

</details>


### [32] [Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language](https://arxiv.org/abs/2507.16557)
*Kristin Gnadt,David Thulke,Simone Kopeinik,Ralf Schlüter*

Main category: cs.CL

TL;DR: 本研究提出了五个德语数据集，用于评估大型语言模型中的性别偏见，并揭示了德语中性别偏见的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的性别偏见评估方法主要针对英语语言开发，但其在其他语言中的可转移性存在挑战。

Method: 本研究提出了五个德语数据集，用于评估大型语言模型中的性别偏见，并通过多种方法进行分析。

Result: 研究发现，德语中的性别偏见具有独特挑战，包括男性职业术语的模糊解释以及看似中性的名词对性别认知的影响。

Conclusion: 本研究有助于理解跨语言的性别偏见，并强调了定制评估框架的必要性。

Abstract: In recent years, various methods have been proposed to evaluate gender bias
in large language models (LLMs). A key challenge lies in the transferability of
bias measurement methods initially developed for the English language when
applied to other languages. This work aims to contribute to this research
strand by presenting five German datasets for gender bias evaluation in LLMs.
The datasets are grounded in well-established concepts of gender bias and are
accessible through multiple methodologies. Our findings, reported for eight
multilingual LLM models, reveal unique challenges associated with gender bias
in German, including the ambiguous interpretation of male occupational terms
and the influence of seemingly neutral nouns on gender perception. This work
contributes to the understanding of gender bias in LLMs across languages and
underscores the necessity for tailored evaluation frameworks.

</details>


### [33] [Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal Language Models](https://arxiv.org/abs/2507.16572)
*Mohamad Ballout,Serwan Jassim,Elia Bruni*

Main category: cs.CL

TL;DR: 本文评估了多模态大语言模型在直观物理任务上的表现，并发现视觉-语言信息的不对齐是其主要限制因素。


<details>
  <summary>Details</summary>
Motivation: 尽管最新的模型在性能指标上有所提升，但在区分物理上合理和不合理的情景时仍然存在困难。因此，我们需要进一步研究模型在处理物理任务时的表现。

Method: 我们系统地评估了最先进的多模态大语言模型（MLLMs）在直观物理任务上的表现，并进行了探针分析以检查模型嵌入中的中间表示。

Result: 我们的结果显示，根据任务难度的不同，可能会出现关键的视觉-语言不对齐现象：视觉编码器成功捕捉到了物理合理性线索，但这些信息并未被语言模型有效利用，导致推理失败。

Conclusion: 我们的研究结果表明，多模态大语言模型在直观物理任务中的主要限制不是视觉组件，而是视觉和语言信息的无效整合。未来的研究应重点关注视觉-语言对齐问题。

Abstract: This paper presents a systematic evaluation of state-of-the-art multimodal
large language models (MLLMs) on intuitive physics tasks using the GRASP and
IntPhys 2 datasets. We assess the open-source models InternVL 2.5, Qwen 2.5 VL,
LLaVA-OneVision, and the proprietary Gemini 2.0 Flash Thinking, finding that
even the latest models struggle to reliably distinguish physically plausible
from implausible scenarios. To go beyond performance metrics, we conduct a
probing analysis of model embeddings, extracting intermediate representations
at key processing stages to examine how well task-relevant information is
preserved. Our results show that, depending on task difficulty, a critical
vision-language misalignment can emerge: vision encoders successfully capture
physical plausibility cues, but this information is not effectively utilized by
the language model, leading to failures in reasoning. This misalignment
suggests that the primary limitation of MLLMs in intuitive physics tasks is not
the vision component but the ineffective integration of visual and linguistic
information. Our findings highlight vision-language alignment as a key area for
improvement, offering insights for future MLLMs development.

</details>


### [34] [Step-Audio 2 Technical Report](https://arxiv.org/abs/2507.16632)
*Boyong Wu,Chao Yan,Chen Hu,Cheng Yi,Chengli Feng,Fei Tian,Feiyu Shen,Gang Yu,Haoyang Zhang,Jingbei Li,Mingrui Chen,Peng Liu,Wang You,Xiangyu Tony Zhang,Xingyuan Li,Xuerui Yang,Yayue Deng,Yechang Huang,Yuxin Li,Yuxin Zhang,Zhao You,Brian Li,Changyi Wan,Hanpeng Hu,Jiangjie Zhen,Siyu Chen,Song Yuan,Xuelin Zhang,Yimin Jiang,Yu Zhou,Yuxiang Yang,Bingxin Li,Buyun Ma,Changhe Song,Dongqing Pang,Guoqiang Hu,Haiyang Sun,Kang An,Na Wang,Shuli Gao,Wei Ji,Wen Li,Wen Sun,Xuan Wen,Yong Ren,Yuankai Ma,Yufan Lu,Bin Wang,Bo Li,Changxin Miao,Che Liu,Chen Xu,Dapeng Shi,Dingyuan Hu,Donghang Wu,Enle Liu,Guanzhe Huang,Gulin Yan,Han Zhang,Hao Nie,Haonan Jia,Hongyu Zhou,Jianjian Sun,Jiaoren Wu,Jie Wu,Jie Yang,Jin Yang,Junzhe Lin,Kaixiang Li,Lei Yang,Liying Shi,Li Zhou,Longlong Gu,Ming Li,Mingliang Li,Mingxiao Li,Nan Wu,Qi Han,Qinyuan Tan,Shaoliang Pang,Shengjie Fan,Siqi Liu,Tiancheng Cao,Wanying Lu,Wenqing He,Wuxun Xie,Xu Zhao,Xueqi Li,Yanbo Yu,Yang Yang,Yi Liu,Yifan Lu,Yilei Wang,Yuanhao Ding,Yuanwei Liang,Yuanwei Lu,Yuchu Luo,Yuhe Yin,Yumeng Zhan,Yuxiang Zhang,Zidong Yang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CL

TL;DR: Step-Audio 2 is an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. It integrates a latent audio encoder and reasoning-centric reinforcement learning (RL), and incorporates the generation of discrete audio tokens into language modeling. It also leverages retrieval-augmented generation (RAG) to mitigate hallucination and audio search by calling external tools.


<details>
  <summary>Details</summary>
Motivation: The paper aims to present an industry-strength audio understanding and speech conversation model that can enhance responsiveness to paralinguistic information such as speaking styles and emotions, and mitigate hallucination and audio search by calling external tools.

Method: Step-Audio 2 is an end-to-end multi-modal large language model that integrates a latent audio encoder and reasoning-centric reinforcement learning (RL). It also incorporates the generation of discrete audio tokens into language modeling, and integrates retrieval-augmented generation (RAG) to leverage rich textual and acoustic knowledge in real-world data.

Result: Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios, achieving state-of-the-art performance on various audio understanding and conversational benchmarks.

Conclusion: Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions.

Abstract: This paper presents Step-Audio~2, an end-to-end multi-modal large language
model designed for industry-strength audio understanding and speech
conversation. By integrating a latent audio encoder and reasoning-centric
reinforcement learning (RL), Step-Audio 2 achieves promising performance in
automatic speech recognition (ASR) and audio understanding. To facilitate
genuine end-to-end speech conversation, Step-Audio 2 incorporates the
generation of discrete audio tokens into language modeling, significantly
enhancing its responsiveness to paralinguistic information such as speaking
styles and emotions. To effectively leverage the rich textual and acoustic
knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented
generation (RAG) and is able to call external tools such as web search to
mitigate hallucination and audio search to switch timbres. Trained on millions
of hours of speech and audio data, Step-Audio 2 delivers intelligence and
expressiveness across diverse conversational scenarios. Evaluation results
demonstrate that Step-Audio 2 achieves state-of-the-art performance on various
audio understanding and conversational benchmarks compared to other open-source
and commercial solutions. Please visit
https://github.com/stepfun-ai/Step-Audio2 for more information.

</details>


### [35] [Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models](https://arxiv.org/abs/2507.16642)
*Armin Berger,Lars Hillebrand,David Leonhard,Tobias Deußer,Thiago Bell Felix de Oliveira,Tim Dilmaghani,Mohamed Khaled,Bernd Kliem,Rüdiger Loitz,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本研究评估了公开可用的大语言模型在财务审计中的监管合规性效率，发现开源模型在某些任务上表现优异，但专有模型在多种场景下更为出色。


<details>
  <summary>Details</summary>
Motivation: 传统上，财务文件的审计是一个劳动密集型的过程，而AI驱动的解决方案已经在简化这一过程中取得了进展。然而，这些系统通常无法验证推荐的段落是否确实符合特定的法律要求。因此，本研究旨在探讨公开可用的大型语言模型在监管合规性方面的效率。

Method: 本研究比较了公开可用的大语言模型（LLMs）在不同模型配置下的监管合规性效率。重点比较了最先进的开源LLMs，如Llama-2，与专有模型如OpenAI的GPT模型。使用了由合作伙伴普华永道（PwC）德国提供的两个自定义数据集进行分析。

Result: 研究发现，开源的Llama-2 700亿模型在检测非合规或真实负样本方面表现出色，超过了所有专有模型。然而，在各种场景中，特别是非英语环境中，专有模型如GPT-4表现最佳。

Conclusion: 研究发现，开源的Llama-2 700亿模型在检测非合规或真实负样本方面表现出色，超过了所有专有模型。然而，在各种场景中，特别是非英语环境中，专有模型如GPT-4表现最佳。

Abstract: The auditing of financial documents, historically a labor-intensive process,
stands on the precipice of transformation. AI-driven solutions have made
inroads into streamlining this process by recommending pertinent text passages
from financial reports to align with the legal requirements of accounting
standards. However, a glaring limitation remains: these systems commonly fall
short in verifying if the recommended excerpts indeed comply with the specific
legal mandates. Hence, in this paper, we probe the efficiency of publicly
available Large Language Models (LLMs) in the realm of regulatory compliance
across different model configurations. We place particular emphasis on
comparing cutting-edge open-source LLMs, such as Llama-2, with their
proprietary counterparts like OpenAI's GPT models. This comparative analysis
leverages two custom datasets provided by our partner PricewaterhouseCoopers
(PwC) Germany. We find that the open-source Llama-2 70 billion model
demonstrates outstanding performance in detecting non-compliance or true
negative occurrences, beating all their proprietary counterparts. Nevertheless,
proprietary models such as GPT-4 perform the best in a broad variety of
scenarios, particularly in non-English contexts.

</details>


### [36] [P-CoT: A Pedagogically-motivated Participatory Chain-of-Thought Prompting for Phonological Reasoning in LLMs](https://arxiv.org/abs/2507.16656)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: 该研究评估了语音推理在文本大型语言模型中的潜力，并通过一种基于教育理论的新提示方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索文本大型语言模型中的语音推理潜力，并通过结构化指导激活潜在的语音能力。

Method: 使用PhonologyBench基准评估任务，如押韵词生成、g2p转换和音节数计算，并引入一种基于教育理论的新型P-CoT提示。

Result: P-CoT提示显著提升了性能，最高提高了52%，甚至在某些任务中超过了人类基线。

Conclusion: 未来的研究可以致力于优化P-CoT提示以适应特定模型，或探索其在不同语言领域的应用。

Abstract: This study explores the potential of phonological reasoning within text-based
large language models (LLMs). Utilizing the PhonologyBench benchmark, we assess
tasks like rhyme word generation, g2p conversion, and syllable counting. Our
evaluations across 12 LLMs reveal that while few-shot learning offers
inconsistent gains, the introduction of a novel Pedagogically-motivated
Participatory Chain-of-Thought (P-CoT) prompt, which is anchored in educational
theories like scaffolding and discovery learning, consistently enhances
performance. This method leverages structured guidance to activate latent
phonological abilities, achieving up to 52% improvement and even surpassing
human baselines in certain tasks. Future work could aim to optimize P-CoT
prompts for specific models or explore their application across different
linguistic domains.

</details>


### [37] [Self-Contradiction as Self-Improvement: Mitigating the Generation-Understanding Gap in MLLMs](https://arxiv.org/abs/2507.16663)
*Yujin Han,Hao Chen,Andi Han,Zhiheng Wang,Xinyu Lin,Yingya Zhang,Shiwei Zhang,Difan Zou*

Main category: cs.CL

TL;DR: 本文研究了多模态大语言模型中的自我矛盾问题，并提出了一种基于课程学习的策略来改善模型的生成和理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管努力将多模态生成和理解任务统一在一个模型中，但发现这些模型存在自我矛盾，生成的图像与输入提示不一致。本文旨在解决这一问题。

Method: 本文通过定义一个非统一分数来量化模型的自我矛盾，并利用内部监督进行后训练，以改善生成和统一性。此外，还提出了一种基于课程学习的策略。

Result: 实验结果表明，自我矛盾主要源于生成能力不足，而不是误解。通过后训练方法可以改善生成和统一性，并发现生成和理解之间存在协同改进效应。

Conclusion: 本文提出了一个基于课程学习的策略，逐步引入更难的样本，以提高多模态大语言模型（MLLM）的统一性和生成与理解能力。

Abstract: Despite efforts to unify multimodal generation and understanding tasks in a
single model, we show these MLLMs exhibit self-contradiction where generation
produces images deemed misaligned with input prompts based on the model's own
understanding. We define a Nonunified score that quantifies such
self-contradiction. Our empirical results reveal that the self-contradiction
mainly arises from weak generation that fails to align with prompts, rather
than misunderstanding. This capability asymmetry indicates the potential of
leveraging self-contradiction for self-improvement, where the stronger model
understanding guides the weaker generation to mitigate the
generation-understanding gap. Applying standard post-training methods (e.g.,
SFT, DPO) with such internal supervision successfully improves both generation
and unification. We discover a co-improvement effect on both generation and
understanding when only fine-tuning the generation branch, a phenomenon known
in pre-training but underexplored in post-training. Our analysis shows
improvements stem from better detection of false positives that are previously
incorrectly identified as prompt-aligned. Theoretically, we show the aligned
training dynamics between generation and understanding allow reduced
prompt-misaligned generations to also improve mismatch detection in the
understanding branch. Additionally, the framework reveals a potential risk of
co-degradation under poor supervision-an overlooked phenomenon that is
empirically validated in our experiments. Notably, we find intrinsic metrics
like Nonunified score cannot distinguish co-degradation from co-improvement,
which highlights the necessity of data quality check. Finally, we propose a
curriculum-based strategy based on our findings that gradually introduces
harder samples as the model improves, leading to better unification and
improved MLLM generation and understanding.

</details>


### [38] [PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization](https://arxiv.org/abs/2507.16679)
*Han Jiang,Dongyao Zhu,Zhihua Wei,Xiaoyuan Yi,Ziang Xiao,Xing Xie*

Main category: cs.CL

TL;DR: PICACO is a new method for In-Context Alignment that helps LLMs better understand and align with multiple human values without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current ICA methods face the Instruction Bottleneck challenge, where LLMs struggle to reconcile multiple intended values within a single prompt, leading to incomplete or biased alignment.

Method: PICACO optimizes a meta-instruction that maximizes the total correlation between specified values and LLM responses, reinforcing value correlation while reducing distractive noise.

Result: PICACO works well with both black-box and open-source LLMs, outperforms several recent strong baselines, and achieves a better balance across up to 8 distinct values.

Conclusion: PICACO is a novel pluralistic ICA method that effectively addresses the Instruction Bottleneck challenge by optimizing a meta-instruction to navigate multiple values, leading to better alignment with human values.

Abstract: In-Context Learning has shown great potential for aligning Large Language
Models (LLMs) with human values, helping reduce harmful outputs and accommodate
diverse preferences without costly post-training, known as In-Context Alignment
(ICA). However, LLMs' comprehension of input prompts remains agnostic, limiting
ICA's ability to address value tensions--human values are inherently
pluralistic, often imposing conflicting demands, e.g., stimulation vs.
tradition. Current ICA methods therefore face the Instruction Bottleneck
challenge, where LLMs struggle to reconcile multiple intended values within a
single prompt, leading to incomplete or biased alignment. To address this, we
propose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO
optimizes a meta-instruction that navigates multiple values to better elicit
LLMs' understanding of them and improve their alignment. This is achieved by
maximizing the total correlation between specified values and LLM responses,
theoretically reinforcing value correlation while reducing distractive noise,
resulting in effective value instructions. Extensive experiments on five value
sets show that PICACO works well with both black-box and open-source LLMs,
outperforms several recent strong baselines, and achieves a better balance
across up to 8 distinct values.

</details>


### [39] [Interpretable Topic Extraction and Word Embedding Learning using row-stochastic DEDICOM](https://arxiv.org/abs/2507.16695)
*Lars Hillebrand,David Biesner,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 该研究提出了一种新的行随机变体的DEDICOM算法，用于文本语料库的点互信息矩阵，以识别潜在的主题聚类并学习可解释的词嵌入。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种独特且可解释的矩阵分解方法，用于对称和非对称方阵，并在文本语料库中识别潜在的主题聚类以及学习可解释的词嵌入。

Method: 使用了一种新的行随机变体的DEDICOM算法，在文本语料库的点互信息矩阵上进行实验，以识别潜在的主题聚类并学习可解释的词嵌入。

Result: 引入了一种高效训练受限DEDICOM算法的方法，并对其主题建模和词嵌入性能进行了定性评估。

Conclusion: DEDICOM算法能够提供一种独特的可解释矩阵分解方法，用于对称和非对称方阵。通过在文本语料库的点互信息矩阵上应用DEDICOM的新行随机变体，可以识别词汇中的潜在主题聚类并同时学习可解释的词嵌入。

Abstract: The DEDICOM algorithm provides a uniquely interpretable matrix factorization
method for symmetric and asymmetric square matrices. We employ a new
row-stochastic variation of DEDICOM on the pointwise mutual information
matrices of text corpora to identify latent topic clusters within the
vocabulary and simultaneously learn interpretable word embeddings. We introduce
a method to efficiently train a constrained DEDICOM algorithm and a qualitative
evaluation of its topic modeling and word embedding performance.

</details>


### [40] [Advancing Risk and Quality Assurance: A RAG Chatbot for Improved Regulatory Compliance](https://arxiv.org/abs/2507.16711)
*Lars Hillebrand,Armin Berger,Daniel Uedelhoven,David Berghaus,Ulrich Warning,Tim Dilmaghani,Bernd Kliem,Thomas Schmid,Rüdiger Loitz,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文介绍了一种基于大型语言模型的新型检索增强生成系统，用于提高高监管行业中的风险和质量保证查询处理效率，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在高度监管的行业中，风险和质量（R&Q）保证需要不断应对复杂的监管框架，员工需要处理大量日常查询，要求准确的政策解释。传统的依赖专家的方法造成了操作瓶颈并限制了可扩展性。

Method: 我们提出了一种新颖的检索增强生成（RAG）系统，利用大型语言模型（LLMs）、混合搜索和相关性提升来增强R&Q查询处理。

Result: 在124个专家标注的真实查询上评估，我们的系统在处理R&Q查询方面表现出显著优于传统RAG方法的性能。

Conclusion: 我们的系统在处理R&Q查询方面表现出显著优于传统RAG方法的性能，为从业者提供了有价值的见解。

Abstract: Risk and Quality (R&Q) assurance in highly regulated industries requires
constant navigation of complex regulatory frameworks, with employees handling
numerous daily queries demanding accurate policy interpretation. Traditional
methods relying on specialized experts create operational bottlenecks and limit
scalability. We present a novel Retrieval Augmented Generation (RAG) system
leveraging Large Language Models (LLMs), hybrid search and relevance boosting
to enhance R&Q query processing. Evaluated on 124 expert-annotated real-world
queries, our actively deployed system demonstrates substantial improvements
over traditional RAG approaches. Additionally, we perform an extensive
hyperparameter analysis to compare and evaluate multiple configuration setups,
delivering valuable insights to practitioners.

</details>


### [41] [RAVine: Reality-Aligned Evaluation for Agentic Search](https://arxiv.org/abs/2507.16725)
*Yilong Xu,Xiang Long,Zhi Zheng,Jinhua Gao*

Main category: cs.CL

TL;DR: RAVine is a new evaluation framework for agentic search systems that addresses limitations in existing frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation frameworks fail to align well with the goals of agentic search.

Method: Proposed RAVine, a Reality-Aligned eValuation framework for agentic LLMs with search.

Result: Benchmarking a series of models using RAVine derived several insights.

Conclusion: RAVine can help advance the development of agentic search systems.

Abstract: Agentic search, as a more autonomous and adaptive paradigm of retrieval
augmentation, is driving the evolution of intelligent search systems. However,
existing evaluation frameworks fail to align well with the goals of agentic
search. First, the complex queries commonly used in current benchmarks often
deviate from realistic user search scenarios. Second, prior approaches tend to
introduce noise when extracting ground truth for end-to-end evaluations,
leading to distorted assessments at a fine-grained level. Third, most current
frameworks focus solely on the quality of final answers, neglecting the
evaluation of the iterative process inherent to agentic search. To address
these limitations, we propose RAVine -- a Reality-Aligned eValuation framework
for agentic LLMs with search. RAVine targets multi-point queries and long-form
answers that better reflect user intents, and introduces an attributable ground
truth construction strategy to enhance the accuracy of fine-grained evaluation.
Moreover, RAVine examines model's interaction with search tools throughout the
iterative process, and accounts for factors of efficiency. We benchmark a
series of models using RAVine and derive several insights, which we hope will
contribute to advancing the development of agentic search systems. The code and
datasets are available at https://github.com/SwordFaith/RAVine.

</details>


### [42] [Unpacking Ambiguity: The Interaction of Polysemous Discourse Markers and Non-DM Signals](https://arxiv.org/abs/2507.16748)
*Jingni Wu,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文研究了英语中DM多义性与非DM信号共现的关系，以及文体对这些模式的影响。研究发现，多义DMs与更多样化的非DM信号共现，但共现信号总数不一定增加，文体在DM-信号交互中起重要作用。


<details>
  <summary>Details</summary>
Motivation: 探讨DM多义性与非DM信号共现之间的关系以及文体对这些模式的影响，这对于理解这些信号的消歧至关重要。

Method: 使用eRST框架，提出DM多义性的分级定义，并进行相关性和回归分析，以检查多义DM是否伴随更多数量和多样性的非DM信号。

Result: 研究发现，多义的DMs确实与更多样化的非DM信号共现，但共现信号的总数不一定增加。此外，文体在DM-信号交互中起着重要作用。

Conclusion: 研究发现，虽然多义的DMs确实与更多样化的非DM信号共现，但共现信号的总数不一定增加。此外，文体在DM-信号交互中起着重要作用。

Abstract: Discourse markers (DMs) like 'but' or 'then' are crucial for creating
coherence in discourse, yet they are often replaced by or co-occur with non-DMs
('in the morning' can mean the same as 'then'), and both can be ambiguous
('since' can refer to time or cause). The interaction mechanism between such
signals remains unclear but pivotal for their disambiguation. In this paper we
investigate the relationship between DM polysemy and co-occurrence of non-DM
signals in English, as well as the influence of genre on these patterns.
  Using the framework of eRST, we propose a graded definition of DM polysemy,
and conduct correlation and regression analyses to examine whether polysemous
DMs are accompanied by more numerous and diverse non-DM signals. Our findings
reveal that while polysemous DMs do co-occur with more diverse non-DMs, the
total number of co-occurring signals does not necessarily increase. Moreover,
genre plays a significant role in shaping DM-signal interactions.

</details>


### [43] [Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning](https://arxiv.org/abs/2507.16784)
*Hongyin Luo,Nathaniel Morgan,Tina Li,Derek Zhao,Ai Vy Ngo,Philip Schroeder,Lijie Yang,Assaf Ben-Kish,Jack O'Brien,James Glass*

Main category: cs.CL

TL;DR: TIM and TIMRUN are proposed to overcome the context limits of LLMs, enabling long-horizon structured reasoning with unlimited working memory and multi-hop tool calls.


<details>
  <summary>Details</summary>
Motivation: To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency.

Method: TIM is a family of LLMs trained for recursive and decompositional problem solving, while TIMRUN is an inference runtime enabling long-horizon structured reasoning beyond context limits. The system models natural language as reasoning trees measured by both length and depth instead of linear sequences, and maintains a working memory that retains only the key-value states of the most relevant context tokens.

Result: The system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.

Conclusion: TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks.

Abstract: To break the context limits of large language models (LLMs) that bottleneck
reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),
a family of LLMs trained for recursive and decompositional problem solving, and
TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond
context limits. Together, TIM hosted on TIMRUN supports virtually unlimited
working memory and multi-hop tool calls within a single language model
inference, overcoming output limits, positional-embedding constraints, and
GPU-memory bottlenecks. Performance is achieved by modeling natural language as
reasoning trees measured by both length and depth instead of linear sequences.
The reasoning trees consist of tasks with thoughts, recursive subtasks, and
conclusions based on the concept we proposed in Schroeder et al, 2025. During
generation, we maintain a working memory that retains only the key-value states
of the most relevant context tokens, selected by a rule-based subtask-pruning
mechanism, enabling reuse of positional embeddings and GPU memory pages
throughout reasoning. Experimental results show that our system sustains high
inference throughput, even when manipulating up to 90% of the KV cache in GPU
memory. It also delivers accurate reasoning on mathematical tasks and handles
information retrieval challenges that require long-horizon reasoning and
multi-hop tool use.

</details>


### [44] [Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent](https://arxiv.org/abs/2507.16799)
*Xiaoyu Zhan,Xinyu Fu,Hao Sun,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的角色扮演框架TTM，通过测试时缩放和上下文工程实现高质量的角色扮演，能够生成风格一致且富有表现力的对话。


<details>
  <summary>Details</summary>
Motivation: 现有基于微调的方法在数据收集和计算资源方面存在挑战，限制了其广泛应用。而仅依赖提示和上下文输入不足以实现深度沉浸式角色扮演，特别是在知名虚构或公众人物的情况下。

Method: TTM框架通过测试时缩放和上下文工程实现无需训练的角色扮演，利用LLM代理自动解耦角色特征为个性、记忆和语言风格，并采用结构化的三阶段生成流程进行控制的角色扮演。

Result: 通过人类评估验证了TTM框架的有效性，结果表明该方法在生成富有表现力和风格一致的角色对话方面表现出色。

Conclusion: TTM框架在生成富有表现力和风格一致的角色对话方面表现出色，能够实现跨多种语言风格以及个性和记忆的变化的无缝组合。

Abstract: The rapid advancement of large language models (LLMs) has enabled
role-playing language agents to demonstrate significant potential in various
applications. However, relying solely on prompts and contextual inputs often
proves insufficient for achieving deep immersion in specific roles,
particularly well-known fictional or public figures. On the other hand,
fine-tuning-based approaches face limitations due to the challenges associated
with data collection and the computational resources required for training,
thereby restricting their broader applicability. To address these issues, we
propose Test-Time-Matching (TTM), a training-free role-playing framework
through test-time scaling and context engineering. TTM uses LLM agents to
automatically decouple a character's features into personality, memory, and
linguistic style. Our framework involves a structured, three-stage generation
pipeline that utilizes these features for controlled role-playing. It achieves
high-fidelity role-playing performance, also enables seamless combinations
across diverse linguistic styles and even variations in personality and memory.
We evaluate our framework through human assessment, and the results demonstrate
that our method achieves the outstanding performance in generating expressive
and stylistically consistent character dialogues.

</details>


### [45] [Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning](https://arxiv.org/abs/2507.16802)
*Yanjun Zheng,Xiyang Du,Longfei Liao,Xiaoke Zhao,Zhaowen Zhou,Bo Zhang,Jiawei Liu,Xiang Qi,Zhe Li,Zhiqiang Zhang,Wang Wei,Peng Zhang*

Main category: cs.CL

TL;DR: 本文介绍了Agentar-Fin-R1系列金融大语言模型，旨在增强推理能力、可靠性和领域专业化。通过高质量的金融任务分类法和多层可信保障框架，实现了训练效率的显著提升，并在多个金融和通用推理数据集上展示了出色的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型在需要强大推理能力、严格的可信度要求和高效适应任务特定需求的场景中往往表现不足。

Method: 我们的优化方法结合了高质量、系统的金融任务分类法和全面的多层可信保障框架。该框架包括高质量的可信知识工程、多智能体可信数据合成和严格的数据验证治理。通过标签引导的自动化难度感知优化、两阶段学习过程和详细的归因系统，我们实现了训练效率的显著提升。

Result: Agentar-Fin-R1系列模型在主流金融基准测试（如FinEva、FinEval和FinanceIQ）以及通用推理数据集（如MATH-500和GPQA）上进行了全面评估。为了彻底评估实际部署能力，我们创新性地提出了Finova评估基准，专注于代理级别的金融推理和合规验证。实验结果表明，Agentar-Fin-R1不仅在金融任务上实现了最先进的性能，还表现出卓越的通用推理能力。

Conclusion: 实验结果表明，Agentar-Fin-R1不仅在金融任务上实现了最先进的性能，还表现出卓越的通用推理能力，验证了其作为高风险金融应用可信解决方案的有效性。

Abstract: Large Language Models (LLMs) demonstrate tremendous potential in the
financial domain, yet existing models often fall short in scenarios demanding
robust reasoning capabilities, stringent trustworthiness requirements, and
efficient adaptation to task-specific needs. We introduce the Agentar-Fin-R1
series of financial large language models (8B and 32B parameters), specifically
engineered based on the Qwen3 foundation model to enhance reasoning
capabilities, reliability, and domain specialization for financial
applications. Our optimization approach integrates a high-quality, systematic
financial task taxonomy with a comprehensive multi-layered trustworthiness
assurance framework. This framework encompasses high-quality trustworthy
knowledge engineering, multi-agent trustworthy data synthesis, and rigorous
data validation governance. Through label-guided automated difficulty-aware
optimization, tow-stage learning processes, and detailed attribution systems,
we achieve substantial improvements in training efficiency. Our models undergo
comprehensive evaluation on mainstream financial benchmarks including FinEva,
FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500
and GPQA. To thoroughly assess real-world deployment capabilities, we
innovatively propose the Finova evaluation benchmark, which focuses on
agent-level financial reasoning and compliance verification. Experimental
results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art
performance on financial tasks but also exhibits exceptional general reasoning
capabilities, validating its effectiveness as a trustworthy solution for
high-stakes financial applications.

</details>


### [46] [LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs](https://arxiv.org/abs/2507.16809)
*Da-Chen Lian,Ri-Sheng Huang,Pin-Er Chen,Chunki Lim,You-Kuan Lin,Guan-Yu Tseng,Zi-Cheng Yang,Shu-Kai Hsieh*

Main category: cs.CL

TL;DR: LingBench++ is a benchmark and reasoning framework for evaluating LLMs on complex linguistic tasks, offering structured reasoning traces, stepwise evaluation protocols, and rich typological metadata across over 90 languages.


<details>
  <summary>Details</summary>
Motivation: To evaluate large language models (LLMs) on complex linguistic tasks inspired by the International Linguistics Olympiad (IOL).

Method: We develop a multi-agent architecture integrating grammatical knowledge retrieval, tool-augmented reasoning, and deliberate hypothesis testing.

Result: Models equipped with external knowledge sources and iterative reasoning outperform single-pass approaches in both accuracy and interpretability.

Conclusion: LingBench++ offers a comprehensive foundation for advancing linguistically grounded, culturally informed, and cognitively plausible reasoning in LLMs.

Abstract: We propose LingBench++, a linguistically-informed benchmark and reasoning
framework designed to evaluate large language models (LLMs) on complex
linguistic tasks inspired by the International Linguistics Olympiad (IOL).
Unlike prior benchmarks that focus solely on final answer accuracy, LingBench++
provides structured reasoning traces, stepwise evaluation protocols, and rich
typological metadata across over 90 low-resource and cross-cultural languages.
We further develop a multi-agent architecture integrating grammatical knowledge
retrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through
systematic comparisons of baseline and our proposed agentic models, we
demonstrate that models equipped with external knowledge sources and iterative
reasoning outperform single-pass approaches in both accuracy and
interpretability. LingBench++ offers a comprehensive foundation for advancing
linguistically grounded, culturally informed, and cognitively plausible
reasoning in LLMs.

</details>


### [47] [MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning](https://arxiv.org/abs/2507.16812)
*Run-Ze Fan,Zengzhi Wang,Pengfei Liu*

Main category: cs.CL

TL;DR: 本文介绍了TextbookReasoning和MegaScience数据集，旨在解决科学推理数据集不足的问题，并展示了其在性能和训练效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏开放、大规模、高质量、可验证的科学推理数据集，开源社区主要关注数学和编码，而忽视了科学领域。为了弥合这一差距，我们提出了TextbookReasoning和MegaScience数据集。

Method: 我们首先介绍了TextbookReasoning，这是一个开放的数据集，包含从12k所大学水平的科学教科书中提取的可信参考答案，涵盖7个科学学科的650k推理问题。我们进一步引入了MegaScience，这是一个大规模的高质量开源数据集的混合体，总共有1.25百万实例，通过系统的消融研究评估各种数据选择方法，以确定每个公开可用的科学数据集的最佳子集。同时，我们构建了一个全面的评估系统，覆盖15个基准测试中的多种主题和问题类型，并采用全面的答案提取策略以确保准确的评估指标。

Result: 我们的实验表明，与现有的开源科学数据集相比，我们的数据集在更简洁的响应长度下实现了优越的性能和训练效率。此外，我们在MegaScience上训练了Llama3.1、Qwen2.5和Qwen3系列基础模型，这些模型在平均性能上显著优于相应的官方指令模型。此外，MegaScience对于更大更强的模型表现出更高的有效性，表明科学调优具有规模效益。

Conclusion: 我们释放了数据整理流程、评估系统、数据集和七个训练好的模型，以推动科学推理研究。

Abstract: Scientific reasoning is critical for developing AI scientists and supporting
human researchers in advancing the frontiers of natural science discovery.
However, the open-source community has primarily focused on mathematics and
coding while neglecting the scientific domain, largely due to the absence of
open, large-scale, high-quality, verifiable scientific reasoning datasets. To
bridge this gap, we first present TextbookReasoning, an open dataset featuring
truthful reference answers extracted from 12k university-level scientific
textbooks, comprising 650k reasoning questions spanning 7 scientific
disciplines. We further introduce MegaScience, a large-scale mixture of
high-quality open-source datasets totaling 1.25 million instances, developed
through systematic ablation studies that evaluate various data selection
methodologies to identify the optimal subset for each publicly available
scientific dataset. Meanwhile, we build a comprehensive evaluation system
covering diverse subjects and question types across 15 benchmarks,
incorporating comprehensive answer extraction strategies to ensure accurate
evaluation metrics. Our experiments demonstrate that our datasets achieve
superior performance and training efficiency with more concise response lengths
compared to existing open-source scientific datasets. Furthermore, we train
Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which
significantly outperform the corresponding official instruct models in average
performance. In addition, MegaScience exhibits greater effectiveness for larger
and stronger models, suggesting a scaling benefit for scientific tuning. We
release our data curation pipeline, evaluation system, datasets, and seven
trained models to the community to advance scientific reasoning research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [Why Braking? Scenario Extraction and Reasoning Utilizing LLM](https://arxiv.org/abs/2507.15874)
*Yin Wu,Daniel Slieter,Vivek Subramanian,Ahmed Abouelazm,Robin Bohn,J. Marius Zöllner*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型的框架，用于理解和分类驾驶场景。该方法在已知和未知场景中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多配备ADAS的车辆，驾驶数据的数量急剧增加，但大多数数据捕捉的是常规驾驶行为。识别和理解这些大数据集中的安全关键边缘情况仍然是一个重大挑战。制动事件特别能表明潜在危险的情况，这促使了我们研究的核心问题：为什么车辆会制动？

Method: 我们提出了一种新颖的框架，利用大型语言模型（LLM）进行场景理解和推理。我们的方法弥合了低级数值信号和自然语言描述之间的差距，使LLM能够解释和分类驾驶场景。我们提出了双路径场景检索，支持基于类别的搜索和基于嵌入的检索。

Result: 实验结果表明，我们的方法优于基于规则的基线，并且在OOD场景中表现良好。

Conclusion: 我们的方法在已知场景和未知的分布外（OOD）场景中都表现出色，具有良好的泛化能力。

Abstract: The growing number of ADAS-equipped vehicles has led to a dramatic increase
in driving data, yet most of them capture routine driving behavior. Identifying
and understanding safety-critical corner cases within this vast dataset remains
a significant challenge. Braking events are particularly indicative of
potentially hazardous situations, motivating the central question of our
research: Why does a vehicle brake? Existing approaches primarily rely on
rule-based heuristics to retrieve target scenarios using predefined condition
filters. While effective in simple environments such as highways, these methods
lack generalization in complex urban settings. In this paper, we propose a
novel framework that leverages Large Language Model (LLM) for scenario
understanding and reasoning. Our method bridges the gap between low-level
numerical signals and natural language descriptions, enabling LLM to interpret
and classify driving scenarios. We propose a dual-path scenario retrieval that
supports both category-based search for known scenarios and embedding-based
retrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate
evaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset.
Experimental results show that our method outperforms rule-based baselines and
generalizes well to OOD scenarios.

</details>


### [49] [SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting](https://arxiv.org/abs/2507.16145)
*Shuhao Mei,Yongchao Long,Shan Cao,Xiaobo Han,Shijia Geng,Jinbo Sun,Yuxi Zhou,Shenda Hong*

Main category: cs.AI

TL;DR: 本文提出了一种新的多模态大型语言模型SpiroLLM，用于分析肺功能测试曲线，以提高COPD诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在COPD诊断方面仅能输出分类结果，无法提供诊断过程的依据，而当前的大型语言模型（LLMs）尚不能理解肺功能测试曲线，这严重限制了它们的临床信任度和应用。

Method: 本文提出了SpiroLLM，这是第一个能够理解肺功能测试（PFT）曲线的多模态大型语言模型。该模型通过SpiroEncoder从呼吸曲线中提取形态特征，并使用SpiroProjector在统一的潜在空间中将其与PFT数值对齐，最终使大型语言模型能够生成全面的诊断报告。

Result: 实验结果表明，SpiroLLM在诊断上的AUROC达到了0.8980（95% CI: 0.8820-0.9132）。在缺失核心数据的鲁棒性测试中，它保持了100%的有效响应率，远超文本-only模型的13.4%，展示了其多模态设计的优势。

Conclusion: 本文展示了将生理信号与大型语言模型深度融合的显著潜力，为下一代可解释和可靠的临床决策支持工具建立了新范式。

Abstract: Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory
disease with persistent airflow limitation, is a leading global cause of
disability and mortality. Respiratory spirogram time series, routinely
collected during pulmonary function tests (PFTs), play a critical role in the
early detection of repsiratory diseases and in monitoring lung function over
time. However, most current AI models for COPD diagnosis are limited to
outputting classification results without providing a rationale for their
diagnostic process, while current Large Language Models (LLMs) cannot
understand spirograms yet, which severely limits their clinical trust and
adoption. To tackle this challenge, we leverage a cohort of 234,028 individuals
from the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large
language model that can understand spirogram. The model extracts morphological
features from respiratory curves via a SpiroEncoder and aligns them with PFT
numerical values in a unified latent space using a SpiroProjector, ultimately
empowering a large language model to generate a comprehensive diagnostic
report. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC
of 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data,
it maintained a 100% valid response rate, far surpassing the 13.4% of a
text-only model and showcasing the superiority of its multimodal design. This
work demonstrates the substantial potential of deeply fusing physiological
signals with large language models, establishing a new paradigm for the next
generation of interpretable and reliable clinical decision support tools.

</details>


### [50] [Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report](https://arxiv.org/abs/2507.16534)
*Shanghai AI Lab,:,Xiaoyang Chen,Yunhao Chen,Zeren Chen,Zhiyun Chen,Hanyun Cui,Yawen Duan,Jiaxuan Guo,Qi Guo,Xuhao Hu,Hong Huang,Lige Huang,Chunxiao Li,Juncheng Li,Qihao Lin,Dongrui Liu,Xinmin Liu,Zicheng Liu,Chaochao Lu,Xiaoya Lu,Jingjing Qu,Qibing Ren,Jing Shao,Jingwei Shi,Jingwei Sun,Peng Wang,Weibing Wang,Jia Xu,Lewen Yan,Xiao Yu,Yi Yu,Boxuan Zhang,Jie Zhang,Weichen Zhang,Zhijie Zheng,Tianyi Zhou,Bowen Zhou*

Main category: cs.AI

TL;DR: 本文评估了前沿AI模型的七种关键风险，并根据'AI-45度定律'定义了风险区域，结果显示大多数模型处于绿色和黄色区域。


<details>
  <summary>Details</summary>
Motivation: 为了理解和识别快速发展的AI模型所带来的前所未有的风险，本文进行了全面的评估。

Method: 本文基于Frontier AI Risk Management Framework (v1.0)中的E-T-C分析（部署环境、威胁源、使能能力），评估了AI模型的前沿风险。

Result: 实验结果表明，所有最近的前沿AI模型都位于绿色和黄色区域，没有越过红色警戒线。

Conclusion: 本文反映了我们对AI前沿风险的当前理解，并呼吁集体行动以减轻这些挑战。

Abstract: To understand and identify the unprecedented risks posed by rapidly advancing
artificial intelligence (AI) models, this report presents a comprehensive
assessment of their frontier risks. Drawing on the E-T-C analysis (deployment
environment, threat source, enabling capability) from the Frontier AI Risk
Management Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks
in seven areas: cyber offense, biological and chemical risks, persuasion and
manipulation, uncontrolled autonomous AI R\&D, strategic deception and
scheming, self-replication, and collusion. Guided by the "AI-$45^\circ$ Law,"
we evaluate these risks using "red lines" (intolerable thresholds) and "yellow
lines" (early warning indicators) to define risk zones: green (manageable risk
for routine deployment and continuous monitoring), yellow (requiring
strengthened mitigations and controlled deployment), and red (necessitating
suspension of development and/or deployment). Experimental results show that
all recent frontier AI models reside in green and yellow zones, without
crossing red lines. Specifically, no evaluated models cross the yellow line for
cyber offense or uncontrolled AI R\&D risks. For self-replication, and
strategic deception and scheming, most models remain in the green zone, except
for certain reasoning models in the yellow zone. In persuasion and
manipulation, most models are in the yellow zone due to their effective
influence on humans. For biological and chemical risks, we are unable to rule
out the possibility of most models residing in the yellow zone, although
detailed threat modeling and in-depth assessment are required to make further
claims. This work reflects our current understanding of AI frontier risks and
urges collective action to mitigate these challenges.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [51] [MMS Player: an open source software for parametric data-driven animation of Sign Language avatars](https://arxiv.org/abs/2507.16463)
*Fabrizio Nunnari,Shailesh Mishra,Patrick Gebhard*

Main category: cs.GR

TL;DR: 本文介绍了MMS-Player，这是一个开源软件，能够从一种称为MMS（多模态手语流）的新手语表示格式中合成手语动画。


<details>
  <summary>Details</summary>
Motivation: 开发MMS-Player的动机是为了提高手语动画的合成效率和质量，通过引入MMS格式来增强传统基于词汇的手语表示方法。

Method: MMS-Player使用Python脚本实现，并且可以使用命令行或HTTP API调用。它能够将手语动画渲染为视频或其他流行的3D动画交换格式。

Result: MMS-Player成功实现了从MMS格式生成手语动画的功能，并且可以将动画导出为多种格式。

Conclusion: MMS-Player是一个有用的工具，可以促进手语动画的生成和分享，特别是在研究和教育领域。

Abstract: This paper describes the MMS-Player, an open source software able to
synthesise sign language animations from a novel sign language representation
format called MMS (MultiModal Signstream). The MMS enhances gloss-based
representations by adding information on parallel execution of signs, timing,
and inflections. The implementation consists of Python scripts for the popular
Blender 3D authoring tool and can be invoked via command line or HTTP API.
Animations can be rendered as videos or exported in other popular 3D animation
exchange formats. The software is freely available under GPL-3.0 license at
https://github.com/DFKI-SignLanguage/MMS-Player.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems](https://arxiv.org/abs/2507.15867)
*John Wu,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: RDMA is a framework that helps identify rare diseases in EHR by connecting scattered clinical observations and improving performance while reducing privacy risks.


<details>
  <summary>Details</summary>
Motivation: Standard ICD coding systems fail to capture rare diseases in EHR, leaving crucial information buried in clinical notes. Current approaches struggle with medical abbreviations, miss implicit disease mentions, raise privacy concerns with cloud processing, and lack clinical reasoning abilities.

Method: RDMA is a framework that mirrors how medical experts identify rare disease patterns in EHR. It connects scattered clinical observations that together suggest specific rare conditions.

Result: RDMA reduces privacy risks while improving F1 performance by upwards of 30% and decreasing inference costs 10-fold.

Conclusion: RDMA helps clinicians avoid the privacy risk of using cloud services while accessing key rare disease information from EHR systems, supporting earlier diagnosis for rare disease patients.

Abstract: Rare diseases affect 1 in 10 Americans, yet standard ICD coding systems fail
to capture these conditions in electronic health records (EHR), leaving crucial
information buried in clinical notes. Current approaches struggle with medical
abbreviations, miss implicit disease mentions, raise privacy concerns with
cloud processing, and lack clinical reasoning abilities. We present Rare
Disease Mining Agents (RDMA), a framework that mirrors how medical experts
identify rare disease patterns in EHR. RDMA connects scattered clinical
observations that together suggest specific rare conditions. By handling
clinical abbreviations, recognizing implicit disease patterns, and applying
contextual reasoning locally on standard hardware, RDMA reduces privacy risks
while improving F1 performance by upwards of 30\% and decreasing inferences
costs 10-fold. This approach helps clinicians avoid the privacy risk of using
cloud services while accessing key rare disease information from EHR systems,
supporting earlier diagnosis for rare disease patients. Available at
https://github.com/jhnwu3/RDMA.

</details>


### [53] [Scaling Linear Attention with Sparse State Expansion](https://arxiv.org/abs/2507.16577)
*Yuqi Pan,Yongqi An,Zheng Li,Yuhong Chou,Ruijie Zhu,Xiaohui Wang,Mingxuan Wang,Jinqiao Wang,Guoqi Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为 Sparse State Expansion (SSE) 的新方法，用于改进 Transformer 架构在长上下文场景中的表现。SSE 通过稀疏状态更新和扩展，提高了检索性能并保持了高效的计算。在数学推理任务中，SSE-H 模型表现出色，优于其他模型。


<details>
  <summary>Details</summary>
Motivation: Transformer 架构在处理长上下文场景时面临计算复杂度高和内存增长线性的问题。虽然各种线性注意力变体通过压缩上下文到固定大小的状态来缓解这些效率问题，但它们在某些任务中会降低性能。因此，需要一种更有效的上下文压缩方法。

Method: 提出了一种基于行稀疏更新的线性注意力方法，并引入了稀疏状态扩展（SSE），通过将上下文状态扩展为多个分区来解耦参数大小与状态容量，同时保持稀疏分类范式。

Result: SSE 在纯线性和混合（SSE-H）架构中进行了广泛验证，在语言建模、上下文检索和数学推理基准测试中表现良好。SSE-H 模型在数学推理任务中取得了最先进的结果，显著优于类似大小的开源 Transformer 模型。

Conclusion: SSE 是一种有前景且高效的长上下文建模架构，具有强大的检索性能和良好的可扩展性。在经过强化学习训练后，2B 参数的 SSE-H 模型在数学推理任务中表现出色，优于类似大小的开源 Transformer 模型。

Abstract: The Transformer architecture, despite its widespread success, struggles with
long-context scenarios due to quadratic computation and linear memory growth.
While various linear attention variants mitigate these efficiency constraints
by compressing context into fixed-size states, they often degrade performance
in tasks such as in-context retrieval and reasoning. To address this limitation
and achieve more effective context compression, we propose two key innovations.
First, we introduce a row-sparse update formulation for linear attention by
conceptualizing state updating as information classification. This enables
sparse state updates via softmax-based top-$k$ hard classification, thereby
extending receptive fields and reducing inter-class interference. Second, we
present Sparse State Expansion (SSE) within the sparse framework, which expands
the contextual state into multiple partitions, effectively decoupling parameter
size from state capacity while maintaining the sparse classification paradigm.
Our design, supported by efficient parallelized implementations, yields
effective classification and discriminative state representations. We
extensively validate SSE in both pure linear and hybrid (SSE-H) architectures
across language modeling, in-context retrieval, and mathematical reasoning
benchmarks. SSE demonstrates strong retrieval performance and scales favorably
with state size. Moreover, after reinforcement learning (RL) training, our 2B
SSE-H model achieves state-of-the-art mathematical reasoning performance among
small reasoning models, scoring 64.7 on AIME24 and 51.3 on AIME25,
significantly outperforming similarly sized open-source Transformers. These
results highlight SSE as a promising and efficient architecture for
long-context modeling.

</details>


### [54] [Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning](https://arxiv.org/abs/2507.16795)
*Helena Casademunt,Caden Juang,Adam Karvonen,Samuel Marks,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: CAFT is a technique that controls how LLMs generalize from fine-tuning without modifying training data, reducing unintended generalizations like emergent misalignment.


<details>
  <summary>Details</summary>
Motivation: To address the problem of unintended out-of-distribution generalization in fine-tuned large language models (LLMs), which standard approaches attempt to solve by modifying training data.

Method: Concept Ablation Fine-Tuning (CAFT), which leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution.

Result: CAFT successfully reduces misaligned responses by 10x without degrading performance on the training distribution, and it can be applied to various fine-tuning tasks.

Conclusion: CAFT represents a novel approach for steering LLM generalization without modifying training data.

Abstract: Fine-tuning large language models (LLMs) can lead to unintended
out-of-distribution generalization. Standard approaches to this problem rely on
modifying training data, for example by adding data that better specify the
intended generalization. However, this is not always practical. We introduce
Concept Ablation Fine-Tuning (CAFT), a technique that leverages
interpretability tools to control how LLMs generalize from fine-tuning, without
needing to modify the training data or otherwise use data from the target
distribution. Given a set of directions in an LLM's latent space corresponding
to undesired concepts, CAFT works by ablating these concepts with linear
projections during fine-tuning, steering the model away from unintended
generalizations. We successfully apply CAFT to three fine-tuning tasks,
including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow
task generalize to give egregiously misaligned responses to general questions.
Without any changes to the fine-tuning data, CAFT reduces misaligned responses
by 10x without degrading performance on the training distribution. Overall,
CAFT represents a novel approach for steering LLM generalization without
modifying training data.

</details>


### [55] [Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty](https://arxiv.org/abs/2507.16806)
*Mehul Damani,Isha Puri,Stewart Slocum,Idan Shenfeld,Leshem Choshen,Yoon Kim,Jacob Andreas*

Main category: cs.LG

TL;DR: 本文提出了一种名为RLCR的方法，用于训练推理模型，以同时提高准确性和校准的置信度估计。实验表明，该方法在各种数据集上显著提高了校准度，同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的推理方法使用二进制奖励函数，这些函数不惩罚猜测或低置信度的输出，导致校准下降并增加生成错误响应的比率。因此，需要一种方法来同时提高准确性和校准的置信度估计。

Method: RLCR（带校准奖励的强化学习）方法，在训练推理模型时同时提高准确性和校准的置信度估计。语言模型在推理后生成预测和数值置信度估计，并通过一个将二进制正确性分数与Brier分数结合的奖励函数进行训练。

Result: RLCR在各种数据集上显著提高了校准度，而不会损失准确性，优于传统的强化学习训练和后验置信度评分分类器。此外，通过置信度加权缩放方法，在测试时利用描述的置信度可以提高准确性和校准度。

Conclusion: 本文表明，通过显式优化校准可以产生更普遍可靠的推理模型。

Abstract: When language models (LMs) are trained via reinforcement learning (RL) to
generate natural language "reasoning chains", their performance improves on a
variety of difficult question answering tasks. Today, almost all successful
applications of RL for reasoning use binary reward functions that evaluate the
correctness of LM outputs. Because such reward functions do not penalize
guessing or low-confidence outputs, they often have the unintended side-effect
of degrading calibration and increasing the rate at which LMs generate
incorrect responses (or "hallucinate") in other problem domains. This paper
describes RLCR (Reinforcement Learning with Calibration Rewards), an approach
to training reasoning models that jointly improves accuracy and calibrated
confidence estimation. During RLCR, LMs generate both predictions and numerical
confidence estimates after reasoning. They are trained to optimize a reward
function that augments a binary correctness score with a Brier score -- a
scoring rule for confidence estimates that incentivizes calibrated prediction.
We first prove that this reward function (or any analogous reward function that
uses a bounded, proper scoring rule) yields models whose predictions are both
accurate and well-calibrated. We next show that across diverse datasets, RLCR
substantially improves calibration with no loss in accuracy, on both in-domain
and out-of-domain evaluations -- outperforming both ordinary RL training and
classifiers trained to assign post-hoc confidence scores. While ordinary RL
hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized
confidence can be leveraged at test time to improve accuracy and calibration
via confidence-weighted scaling methods. Our results show that explicitly
optimizing for calibration can produce more generally reliable reasoning
models.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [56] [WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections](https://arxiv.org/abs/2507.16298)
*Gautam Kishore Shahi,Scot A. Hale*

Main category: cs.SI

TL;DR: 本研究分析了WhatsApp提示线在2021年印度议会选举期间的作用，发现不同语言之间存在声明相似性，且用户通常只向一个事实核查组织提交提示。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析WhatsApp提示线在2021年印度议会选举期间如何帮助用户验证误导性内容，特别是关注不同语言之间的差异以及事实核查机构的运作情况。

Method: 该研究使用混合方法分析了来自451名用户的580个独特声明（提示），涵盖了高资源语言（英语、印地语）和低资源语言（泰卢固语）在2021年印度议会选举期间的数据。研究将声明分为三类：选举、新冠疫情和其他类别，并通过频繁词分析和神经句子嵌入聚类比较内容相似性。此外，还调查了跨语言和事实核查组织的用户重叠情况，并测量了反驳声明并通知提示线用户所需的平均时间。

Result: 研究结果揭示了不同语言之间声明的相似性，一些用户向同一事实核查机构提交多种语言的提示。事实核查机构通常需要几天时间来反驳新声明并将结果告知用户。值得注意的是，没有用户向多个事实核查组织提交提示，这表明每个组织都有独特的受众。

Conclusion: 研究发现，用户有时会向同一事实核查机构提交多种语言的提示，但没有用户向多个事实核查组织提交提示，这表明每个组织都有独特的受众。研究提供了在选举期间使用提示线的实用建议，并考虑了用户的伦理信息需求。

Abstract: WhatsApp tiplines, first launched in 2019 to combat misinformation, enable
users to interact with fact-checkers to verify misleading content. This study
analyzes 580 unique claims (tips) from 451 users, covering both high-resource
languages (English, Hindi) and a low-resource language (Telugu) during the 2021
Indian assembly elections using a mixed-method approach. We categorize the
claims into three categories, election, COVID-19, and others, and observe
variations across languages. We compare content similarity through frequent
word analysis and clustering of neural sentence embeddings. We also investigate
user overlap across languages and fact-checking organizations. We measure the
average time required to debunk claims and inform tipline users. Results reveal
similarities in claims across languages, with some users submitting tips in
multiple languages to the same fact-checkers. Fact-checkers generally require a
couple of days to debunk a new claim and share the results with users. Notably,
no user submits claims to multiple fact-checking organizations, indicating that
each organization maintains a unique audience. We provide practical
recommendations for using tiplines during elections with ethical consideration
of users' information.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark](https://arxiv.org/abs/2507.15882)
*Goeric Huybrechts,Srikanth Ronanki,Sai Muralidhar Jayanthi,Jack Fitzgerald,Srinivasan Veeravanallur*

Main category: cs.CV

TL;DR: 本文提出了Document Haystack基准，用于评估视觉语言模型在长而复杂的文档上的表现，并讨论了该领域的潜在研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏合适的基准，长文档的处理仍然研究不足。本文旨在解决这一问题，提出一个全面的基准来评估视觉语言模型在长文档上的表现。

Method: 本文提出了Document Haystack基准，包含从5到200页的文档，并在文档中插入纯文本或多媒体文本+图像的“针”以挑战视觉语言模型的检索能力。

Result: 本文详细描述了Document Haystack数据集的构建和特征，并展示了主流视觉语言模型的结果。

Conclusion: 本文介绍了Document Haystack基准，用于评估视觉语言模型在长而复杂的文档上的表现，并讨论了该领域的潜在研究方向。

Abstract: The proliferation of multimodal Large Language Models has significantly
advanced the ability to analyze and understand complex data inputs from
different modalities. However, the processing of long documents remains
under-explored, largely due to a lack of suitable benchmarks. To address this,
we introduce Document Haystack, a comprehensive benchmark designed to evaluate
the performance of Vision Language Models (VLMs) on long, visually complex
documents. Document Haystack features documents ranging from 5 to 200 pages and
strategically inserts pure text or multimodal text+image "needles" at various
depths within the documents to challenge VLMs' retrieval capabilities.
Comprising 400 document variants and a total of 8,250 questions, it is
supported by an objective, automated evaluation framework. We detail the
construction and characteristics of the Document Haystack dataset, present
results from prominent VLMs and discuss potential research avenues in this
area.

</details>


### [58] [C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning](https://arxiv.org/abs/2507.16518)
*Xiuwei Chen,Wentao Hu,Hanhui Li,Jun Zhou,Zisheng Chen,Meng Cao,Yihan Zeng,Kui Zhang,Yu-Jie Yuan,Jianhua Han,Hang Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 C2-Evo 的自动闭环自我改进框架，用于同时改进训练数据和模型能力，从而在多个数学推理基准上获得显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了应对现有方法在数据复杂性不一致和数据与模型进化分离的问题，我们需要一种能够同时改进训练数据和模型能力的解决方案。

Method: 我们提出了 C2-Evo，一个自动的闭环自我改进框架，联合进化训练数据和模型能力。具体来说，C2-Evo 通过跨模态数据进化循环和数据-模型进化循环来增强它们。

Result: 我们的方法在多个数学推理基准上获得了显著的性能提升。

Conclusion: 我们的方法持续改进其模型和训练数据，并在多个数学推理基准上 consistently 获得显著的性能提升。

Abstract: Recent advances in multimodal large language models (MLLMs) have shown
impressive reasoning capabilities. However, further enhancing existing MLLMs
necessitates high-quality vision-language datasets with carefully curated task
complexities, which are both costly and challenging to scale. Although recent
self-improving models that iteratively refine themselves offer a feasible
solution, they still suffer from two core challenges: (i) most existing methods
augment visual or textual data separately, resulting in discrepancies in data
complexity (e.g., over-simplified diagrams paired with redundant textual
descriptions); and (ii) the evolution of data and models is also separated,
leading to scenarios where models are exposed to tasks with mismatched
difficulty levels. To address these issues, we propose C2-Evo, an automatic,
closed-loop self-improving framework that jointly evolves both training data
and model capabilities. Specifically, given a base dataset and a base model,
C2-Evo enhances them by a cross-modal data evolution loop and a data-model
evolution loop. The former loop expands the base dataset by generating complex
multimodal problems that combine structured textual sub-problems with
iteratively specified geometric diagrams, while the latter loop adaptively
selects the generated problems based on the performance of the base model, to
conduct supervised fine-tuning and reinforcement learning alternately.
Consequently, our method continuously refines its model and training data, and
consistently obtains considerable performance gains across multiple
mathematical reasoning benchmarks. Our code, models, and datasets will be
released.

</details>


### [59] [Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning](https://arxiv.org/abs/2507.16746)
*Ang Li,Charles Wang,Kaiyu Yue,Zikui Cai,Ollie Liu,Deqing Fu,Peng Guo,Wang Bill Zhu,Vatsal Sharan,Robin Jia,Willie Neiswanger,Furong Huang,Tom Goldstein,Micah Goldblum*

Main category: cs.CV

TL;DR: 本文提出了一种新的大规模数据集Zebra-CoT，用于训练多模态模型进行视觉链式思维，并展示了其在提升模型性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于现有的视觉链式思维性能较差，且缺乏高质量的训练数据，因此需要一个新的数据集来促进多模态模型的发展。

Method: 本文提出了Zebra-CoT数据集，用于训练多模态模型进行视觉链式思维。

Result: 在Zebra-CoT数据集上微调Anole-7B模型可以提高测试集准确率，并在标准VLM基准评估中获得高达13%的性能提升。

Conclusion: 本文介绍了Zebra-CoT数据集，并展示了其在提升多模态模型性能方面的有效性。

Abstract: Humans often use visual aids, for example diagrams or sketches, when solving
complex problems. Training multimodal models to do the same, known as Visual
Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf
visual CoT performance, which hinders reinforcement learning, and (2) the lack
of high-quality visual CoT training data. We introduce $\textbf{Zebra-CoT}$, a
diverse large-scale dataset with 182,384 samples, containing logically coherent
interleaved text-image reasoning traces. We focus on four categories of tasks
where sketching or visual reasoning is especially natural, spanning scientific
questions such as geometry, physics, and algorithms; 2D visual reasoning tasks
like visual search and jigsaw puzzles; 3D reasoning tasks including 3D
multi-hop inference, embodied and robot planning; visual logic problems and
strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT
training corpus results in an improvement of +12% in our test-set accuracy and
yields up to +13% performance gain on standard VLM benchmark evaluations.
Fine-tuning Bagel-7B yields a model that generates high-quality interleaved
visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing
multimodal reasoning abilities. We open-source our dataset and models to
support development and evaluation of visual CoT.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [60] [Characterizing Online Activities Contributing to Suicide Mortality among Youth](https://arxiv.org/abs/2507.16185)
*Aparna Ananthasubramaniam,Elyse J. Thulin,Viktoryia Kalesnikava,Silas Falde,Jonathan Kertawidjaja,Lily Johns,Alejandro Rodríguez-Putnam,Emma Spring,Kara Zivin,Briana Mezuk*

Main category: cs.CY

TL;DR: 本研究通过混合方法分析了青少年在线活动与自杀死亡之间的关系，并开发了一个框架来大规模建模这些主题。


<details>
  <summary>Details</summary>
Motivation: 最近青少年自杀率的上升突显了理解在线体验如何促成这一公共卫生问题的紧迫性。

Method: 我们采用混合方法，通过主题分析识别了12种与青少年自杀死亡相关的在线活动，并开发了一个零样本学习框架来大规模建模这些主题。

Result: 我们发现了一些与自残、对他人的伤害、人际互动、在线活动水平和生活事件相关的在线活动，这些活动对应于两种主要自杀理论中的不同自杀风险阶段。

Conclusion: 我们的工作展示了将自杀理论与计算研究相结合以开发针对不太明显自杀风险指标的干预措施的机会。

Abstract: The recent rise in youth suicide highlights the urgent need to understand how
online experiences contribute to this public health issue. Our mixed-methods
approach responds to this challenge by developing a set of themes focused on
risk factors for suicide mortality in online spaces among youth ages 10-24, and
a framework to model these themes at scale. Using 29,124 open text summaries of
death investigations between 2013-2022, we conducted a thematic analysis to
identify 12 types of online activities that were considered by investigators or
next of kin to be relevant in contextualizing a given suicide death. We then
develop a zero-shot learning framework to model these 12 themes at scale, and
analyze variation in these themes by decedent characteristics and over time.
Our work uncovers several online activities related to harm to self, harm to
others, interpersonal interactions, activity levels online, and life events,
which correspond to different phases of suicide risk from two prominent suicide
theories. We find an association between these themes and decedent
characteristics like age, means of death, and interpersonal problems, and many
themes became more prevalent during the 2020 COVID-19 lockdowns. While digital
spaces have taken some steps to address expressions of suicidality online, our
work illustrates the opportunities for developing interventions related to less
explicit indicators of suicide risk by combining suicide theories with
computational research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [61] [Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory](https://arxiv.org/abs/2507.16713)
*Guowei Lan,Kaixian Qu,René Zurbrügg,Changan Chen,Christopher E. Mower,Haitham Bou-Ammar,Marco Hutter*

Main category: cs.RO

TL;DR: ExpTeach is a framework that grounds VLMs to physical robots using self-generated experiences and long-term memory, significantly improving success rates in robotic tasks.


<details>
  <summary>Details</summary>
Motivation: The challenge of grounding VLMs, originally trained on internet data, to diverse real-world robots needs a solution that enables autonomous planning and learning from experience.

Method: ExpTeach is a framework that grounds VLMs to physical robots by building a self-generated memory of real-world experiences. It uses a closed-loop process for planning, verification, reflection, and adaptation, along with an on-demand image annotation module to enhance spatial understanding.

Result: ExpTeach improves success rates from 36% to 84% on four challenging robotic tasks and boosts single-trial success rates from 22% to 80% across 12 real-world scenarios, including eight unseen ones.

Conclusion: ExpTeach demonstrates the effectiveness and generalizability of grounding VLMs to physical robots through long-term memory and reflection.

Abstract: Vision-language models (VLMs) have been widely adopted in robotics to enable
autonomous planning. However, grounding VLMs, originally trained on internet
data, to diverse real-world robots remains a challenge. This paper presents
ExpTeach, a framework that grounds VLMs to physical robots by building a
self-generated memory of real-world experiences. In ExpTeach, the VLM
autonomously plans actions, verifies outcomes, reflects on failures, and adapts
robot behaviors in a closed loop. The self-generated experiences during this
process are then summarized into a long-term memory, enabling retrieval of
learned knowledge to guide future tasks via retrieval-augmented generation
(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with
an on-demand image annotation module. In experiments, we show that reflection
improves success rates from 36% to 84% on four challenging robotic tasks and
observe the emergence of intelligent object interactions, including creative
tool use. Across extensive tests on 12 real-world scenarios (including eight
unseen ones), we find that grounding with long-term memory boosts single-trial
success rates from 22% to 80%, demonstrating the effectiveness and
generalizability of ExpTeach.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [62] [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
*Ori Press,Brandon Amos,Haoyu Zhao,Yikai Wu,Samuel K. Ainsworth,Dominik Krupke,Patrick Kidger,Touqir Sajed,Bartolomeo Stellato,Jisun Park,Nathanael Bosch,Eli Meril,Albert Steppi,Arman Zharmagambetov,Fangzhao Zhang,David Perez-Pineiro,Alberto Mercurio,Ni Zhan,Talor Abramovich,Kilian Lieret,Hanlin Zhang,Shirley Huang,Matthias Bethge,Ofir Press*

Main category: cs.SE

TL;DR: 本文提出了AlgoTune基准测试，用于评估语言模型在开放性任务中设计和实现算法的能力。结果表明，AlgoTuner在速度上优于参考求解器，但未能发现算法创新。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型（LM）的能力有所进步，但评估一直集中在模型在人类已解决的任务上的表现上，包括编程和数学。因此，我们建议测试模型在开放性基准中设计和实现算法的能力。

Method: 我们提出了AlgoTune基准测试，其中包括从领域专家那里收集的155个编码任务，以及一个验证和计时LM合成解决方案代码的框架，并将其与流行的开源包中的参考实现进行比较。此外，我们开发了一个基线LM代理AlgoTuner，并在一系列前沿模型上评估了其性能。

Result: AlgoTuner在我们的参考求解器上平均实现了1.72倍的速度提升，这些求解器使用了诸如SciPy、sk-learn和CVXPY等库。然而，我们发现当前模型未能发现算法创新，而是更倾向于表面优化。

Conclusion: 我们希望AlgoTune能推动LM代理的发展，使其展现出超越最先进人类表现的创造性问题解决能力。

Abstract: Despite progress in language model (LM) capabilities, evaluations have thus
far focused on models' performance on tasks that humans have previously solved,
including in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,
2024). We therefore propose testing models' ability to design and implement
algorithms in an open-ended benchmark: We task LMs with writing code that
efficiently solves computationally challenging problems in computer science,
physics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks
collected from domain experts and a framework for validating and timing
LM-synthesized solution code, which is compared to reference implementations
from popular open-source packages. In addition, we develop a baseline LM agent,
AlgoTuner, and evaluate its performance across a suite of frontier models.
AlgoTuner achieves an average 1.72x speedup against our reference solvers,
which use libraries such as SciPy, sk-learn and CVXPY. However, we find that
current models fail to discover algorithmic innovations, instead preferring
surface-level optimizations. We hope that AlgoTune catalyzes the development of
LM agents exhibiting creative problem solving beyond state-of-the-art human
performance.

</details>

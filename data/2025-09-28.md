<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，通过识别外交事件叙述的具体修改来改变公众舆论。该框架利用语言模型预测公众反应，并生成修改后的文本以改变事件的叙述框架。实验结果显示该框架能有效提升公众舆论，具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 公共舆论在外交中起着关键作用，良好的舆论为政策实施提供重要支持，有助于解决国际问题，并塑造国家的国际形象。传统的方法如大规模调查或人工内容分析媒体通常耗时、费力且缺乏前瞻性分析的能力。

Method: 我们训练了一个语言模型来预测公众对外交事件的反应，并构建了一个包含外交事件描述及其相关公众讨论的数据集。然后，我们根据传播理论并与领域专家合作，预定了几种文本特征进行修改，确保任何修改改变了事件的叙述框架，同时保留了其核心事实。我们开发了一个反事实生成算法，利用大型语言模型系统地生成原始文本的修改版本。

Result: 结果表明，该框架成功地将公众舆论转向更积极的状态，成功率高达70%。

Conclusion: 该框架可以作为外交官、政策制定者和传播专家的实用工具，提供数据驱动的见解，以如何构架外交举措或报道事件来促进更理想的公众舆论。

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [2] [Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition](https://arxiv.org/abs/2509.20373)
*Shreya G. Upadhyay,Carlos Busso,Chi-Chun Lee*

Main category: cs.CL

TL;DR: 本文提出了一种跨语言语音情感识别框架，通过构建情感特定的说话人社区并应用双空间锚定方法，提高了情感迁移的效果。


<details>
  <summary>Details</summary>
Motivation: 跨语言语音情感识别仍然是一项具有挑战性的任务，因为不同语言之间的语音变化和说话人的表达风格存在差异。需要一种能够对不同说话人和语言的情感外化进行对齐的框架。

Method: 本文提出了一种基于语音和发音空间的双空间锚定方法，通过图聚类构建特定情感的说话人社区，以捕捉共享的说话人特征。

Result: 在MSP-Podcast（英语）和BIIC-Podcast（台湾普通话）语料库上的评估表明，该方法在竞争性基线上表现出更好的泛化能力。

Conclusion: 本文提出了一种语音情感识别框架，能够更好地在不同语言之间进行情感迁移，并提供了跨语言情感表示的共同点的见解。

Abstract: Cross-lingual speech emotion recognition (SER) remains a challenging task due
to differences in phonetic variability and speaker-specific expressive styles
across languages. Effectively capturing emotion under such diverse conditions
requires a framework that can align the externalization of emotions across
different speakers and languages. To address this problem, we propose a
speaker-style aware phoneme anchoring framework that aligns emotional
expression at the phonetic and speaker levels. Our method builds
emotion-specific speaker communities via graph-based clustering to capture
shared speaker traits. Using these groups, we apply dual-space anchoring in
speaker and phonetic spaces to enable better emotion transfer across languages.
Evaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)
corpora demonstrate improved generalization over competitive baselines and
provide valuable insights into the commonalities in cross-lingual emotion
representation.

</details>


### [3] [CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics](https://arxiv.org/abs/2509.20374)
*Nithin Somasekharan,Ling Yue,Yadi Cao,Weichao Li,Patrick Emami,Pochinapeddi Sai Bhargav,Anurag Acharya,Xingyu Xie,Shaowu Pan*

Main category: cs.CL

TL;DR: 本文介绍了CFDLLMBench，这是一个包含三个互补组件的基准套件，用于全面评估大型语言模型（LLM）在计算流体动力学（CFD）领域的性能。该基准基于现实世界的CFD实践，结合详细的任务分类和严格的评估框架，以实现可重复的结果并量化LLM在代码执行、解决方案准确性和数值收敛行为方面的性能。


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have demonstrated strong performance across general NLP tasks, but their utility in automating numerical experiments of complex physical system -- a critical and labor-intensive component -- remains underexplored. As the major workhorse of computational science over the past decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging testbed for evaluating the scientific capabilities of LLMs.

Method: CFDLLMBench, a benchmark suite comprising three complementary components -- CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM performance across three key competencies: graduate-level CFD knowledge, numerical and physical reasoning of CFD, and context-dependent implementation of CFD workflows.

Result: Grounded in real-world CFD practices, our benchmark combines a detailed task taxonomy with a rigorous evaluation framework to deliver reproducible results and quantify LLM performance across code executability, solution accuracy, and numerical convergence behavior.

Conclusion: CFDLLMBench establishes a solid foundation for the development and evaluation of LLM-driven automation of numerical experiments for complex physical systems.

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
general NLP tasks, but their utility in automating numerical experiments of
complex physical system -- a critical and labor-intensive component -- remains
underexplored. As the major workhorse of computational science over the past
decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging
testbed for evaluating the scientific capabilities of LLMs. We introduce
CFDLLMBench, a benchmark suite comprising three complementary components --
CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM
performance across three key competencies: graduate-level CFD knowledge,
numerical and physical reasoning of CFD, and context-dependent implementation
of CFD workflows. Grounded in real-world CFD practices, our benchmark combines
a detailed task taxonomy with a rigorous evaluation framework to deliver
reproducible results and quantify LLM performance across code executability,
solution accuracy, and numerical convergence behavior. CFDLLMBench establishes
a solid foundation for the development and evaluation of LLM-driven automation
of numerical experiments for complex physical systems. Code and data are
available at https://github.com/NREL-Theseus/cfdllmbench/.

</details>


### [4] [Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text](https://arxiv.org/abs/2509.20375)
*Sharanya Parimanoharan,Ruwan D. Nawarathna*

Main category: cs.CL

TL;DR: 本文评估了不同机器学习方法在检测AI生成文本方面的能力，发现基于变压器的方法表现最佳，特别是DistilBERT。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速采用，人类和AI生成文本之间的界限变得模糊，这引发了关于学术诚信、知识产权和虚假信息传播的紧急问题。因此，需要可靠的AI文本检测方法来确保公平评估，保护人类的真实性，并培养对数字通信的信任。

Method: 本文研究了当前机器学习方法在区分ChatGPT-3.5生成的文本和人工撰写的文本方面的效果，测试并比较了经典方法（如逻辑回归）和基于变压器的方法（如BERT、DistilBERT等）。

Result: 结果表明，DistilBERT表现最佳，而逻辑回归和BERT-Custom提供了稳固且平衡的替代方案；LSTM和BERT-N-gram方法则落后。三个最佳模型的最大投票集成未能超越DistilBERT本身。

Conclusion: 本文通过全面评估这些AI文本检测方法的优缺点，为更强大的Transformer框架奠定了基础，以跟上不断改进的生成式AI模型的步伐。

Abstract: The rapid adoption of large language models (LLMs) such as ChatGPT has
blurred the line between human and AI-generated texts, raising urgent questions
about academic integrity, intellectual property, and the spread of
misinformation. Thus, reliable AI-text detection is needed for fair assessment
to safeguard human authenticity and cultivate trust in digital communication.
In this study, we investigate how well current machine learning (ML) approaches
can distinguish ChatGPT-3.5-generated texts from human-written texts employing
a labeled data set of 250 pairs of abstracts from a wide range of research
topics. We test and compare both classical (Logistic Regression armed with
classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT
augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,
and LSTM-based N-gram models) ML detection techniques. As we aim to assess each
model's performance in detecting AI-generated research texts, we also aim to
test whether an ensemble of these models can outperform any single detector.
Results show DistilBERT achieves the overall best performance, while Logistic
Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and
BERT-N-gram approaches lag. The max voting ensemble of the three best models
fails to surpass DistilBERT itself, highlighting the primacy of a single
transformer-based representation over mere model diversity. By comprehensively
assessing the strengths and weaknesses of these AI-text detection approaches,
this work lays a foundation for more robust transformer frameworks with larger,
richer datasets to keep pace with ever-improving generative AI models.

</details>


### [5] [ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models](https://arxiv.org/abs/2509.20376)
*Haoxuan Li,Zhen Wen,Qiqi Jiang,Chenxiao Li,Yuwei Wu,Yuchen Yang,Yiyao Wang,Xiuqi Huang,Minfeng Zhu,Wei Chen*

Main category: cs.CL

TL;DR: ConceptViz is a visual analytics system designed for exploring concepts in LLMs, which helps bridge the gap between SAE features and human concepts by implementing a novel identification, interpretation, and validation pipeline.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between SAE features and human concepts, as SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive.

Method: ConceptViz implements a novel dentification => Interpretation => Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification.

Result: We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs.

Conclusion: ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features.

Abstract: Large language models (LLMs) have achieved remarkable performance across a
wide range of natural language tasks. Understanding how LLMs internally
represent knowledge remains a significant challenge. Despite Sparse
Autoencoders (SAEs) have emerged as a promising technique for extracting
interpretable features from LLMs, SAE features do not inherently align with
human-understandable concepts, making their interpretation cumbersome and
labor-intensive. To bridge the gap between SAE features and human concepts, we
present ConceptViz, a visual analytics system designed for exploring concepts
in LLMs. ConceptViz implements a novel dentification => Interpretation =>
Validation pipeline, enabling users to query SAEs using concepts of interest,
interactively explore concept-to-feature alignments, and validate the
correspondences through model behavior verification. We demonstrate the
effectiveness of ConceptViz through two usage scenarios and a user study. Our
results show that ConceptViz enhances interpretability research by streamlining
the discovery and validation of meaningful concept representations in LLMs,
ultimately aiding researchers in building more accurate mental models of LLM
features. Our code and user guide are publicly available at
https://github.com/Happy-Hippo209/ConceptViz.

</details>


### [6] [SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20377)
*Tomoaki Isoda*

Main category: cs.CL

TL;DR: 本文提出SKILL-RAG方法，利用模型的自我知识来判断哪些检索文档对回答给定查询有帮助，通过强化学习框架显式获取自我知识，并在句子级别上过滤无关内容。实验结果表明SKILL-RAG不仅提高了生成质量，还显著减少了输入文档的数量，验证了自我知识在引导高质量检索选择中的重要性。


<details>
  <summary>Details</summary>
Motivation: Identifying and filtering out unhelpful retrieved content is a key challenge for improving RAG performance. To better integrate the internal knowledge of the model with external knowledge from retrieval, it is essential to understand what the model 'knows' and 'does not know'.

Method: We design a reinforcement learning-based training framework to explicitly elicit self-knowledge from the model and employs sentence-level granularity to filter out irrelevant content while preserving useful knowledge.

Result: Experimental results demonstrate that SKILL-RAG not only improves generation quality but also significantly reduces the number of input documents, validating the importance of self-knowledge in guiding the selection of high-quality retrievals.

Conclusion: SKILL-RAG not only improves generation quality but also significantly reduces the number of input documents, validating the importance of self-knowledge in guiding the selection of high-quality retrievals.

Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive tasks in
recent years. However, since retrieval systems may return irrelevant content,
incorporating such information into the model often leads to hallucinations.
Thus, identifying and filtering out unhelpful retrieved content is a key
challenge for improving RAG performance.To better integrate the internal
knowledge of the model with external knowledge from retrieval, it is essential
to understand what the model "knows" and "does not know" (which is also called
"self-knowledge"). Based on this insight, we propose SKILL-RAG (Self-Knowledge
Induced Learning and Filtering for RAG), a novel method that leverages the
model's self-knowledge to determine which retrieved documents are beneficial
for answering a given query. We design a reinforcement learning-based training
framework to explicitly elicit self-knowledge from the model and employs
sentence-level granularity to filter out irrelevant content while preserving
useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several
question answering benchmarks. Experimental results demonstrate that SKILL-RAG
not only improves generation quality but also significantly reduces the number
of input documents, validating the importance of self-knowledge in guiding the
selection of high-quality retrievals.

</details>


### [7] [Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation](https://arxiv.org/abs/2509.20378)
*Sirui Wang,Andong Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种名为Emo-FiLM的细粒度情感建模框架，用于基于大语言模型的文本到语音系统。该框架通过将情感2vec的帧级特征与单词对齐，获得单词级情感注释，并通过特征线性调制（FiLM）层进行映射，从而实现对文本嵌入的直接调制，以实现单词级情感控制。实验表明，Emo-FiLM在全局和细粒度任务上都优于现有方法，证明了其在富有表现力的语音合成中的有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有的情感文本到语音（E-TTS）系统通常依赖于句子级别的控制，如预定义标签、参考音频或自然语言提示。虽然这些方法在全局情感表达上是有效的，但它们无法捕捉句子内部的动态变化。因此，需要一种能够实现更细粒度情感控制的方法。

Method: Emo-FiLM是一种基于LLM的TTS的细粒度情感建模框架，它通过将情感2vec的帧级特征与单词对齐，获得单词级情感注释，并通过特征线性调制（FiLM）层进行映射，从而实现对文本嵌入的直接调制，以实现单词级情感控制。

Result: 实验表明，Emo-FiLM在全局和细粒度任务上都优于现有方法，证明了其在富有表现力的语音合成中的有效性和通用性。

Conclusion: Emo-FiLM在全局和细粒度任务上都优于现有方法，证明了其在富有表现力的语音合成中的有效性和通用性。

Abstract: Emotional text-to-speech (E-TTS) is central to creating natural and
trustworthy human-computer interaction. Existing systems typically rely on
sentence-level control through predefined labels, reference audio, or natural
language prompts. While effective for global emotion expression, these
approaches fail to capture dynamic shifts within a sentence. To address this
limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework
for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to
words to obtain word-level emotion annotations, and maps them through a
Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion
control by directly modulating text embeddings. To support evaluation, we
construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed
annotations of emotional transitions. Experiments show that Emo-FiLM
outperforms existing approaches on both global and fine-grained tasks,
demonstrating its effectiveness and generality for expressive speech synthesis.

</details>


### [8] [USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model](https://arxiv.org/abs/2509.20381)
*Jianyu Wen,Jingyun Wang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Ying Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于用户模拟器的框架（USB-Rec），以在模型级别提高LLMs在对话推荐中的性能。通过设计基于LLM的偏好优化数据集构建策略和在推理阶段的自我增强策略，实验结果表明该方法优于之前的最先进方法。


<details>
  <summary>Details</summary>
Motivation: 最近，大型语言模型（LLMs）已被广泛应用于对话推荐系统（CRSs）。与传统语言模型方法关注训练不同，所有现有的基于LLM的方法主要集中在如何利用LLMs的总结和分析能力，而忽略了训练问题。因此，我们提出了一个集成的训练-推理框架，以在模型级别提高LLMs在对话推荐中的性能。

Method: 我们提出了一个基于用户模拟器的框架（USB-Rec），以在模型级别提高LLMs在对话推荐中的性能。首先，我们设计了一个基于LLM的偏好优化（PO）数据集构建策略，以帮助LLMs理解对话推荐中的策略和方法。其次，我们在推理阶段提出了一种自我增强策略（SES），以进一步挖掘从RL训练中获得的对话推荐潜力。

Result: 我们的方法在各种数据集上的广泛实验表明，它始终优于之前最先进的方法。

Conclusion: 我们的方法在各种数据集上的广泛实验表明，它始终优于之前最先进的方法。

Abstract: Recently, Large Language Models (LLMs) have been widely employed in
Conversational Recommender Systems (CRSs). Unlike traditional language model
approaches that focus on training, all existing LLMs-based approaches are
mainly centered around how to leverage the summarization and analysis
capabilities of LLMs while ignoring the issue of training. Therefore, in this
work, we propose an integrated training-inference framework,
User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs
in conversational recommendation at the model level. Firstly, we design a
LLM-based Preference Optimization (PO) dataset construction strategy for RL
training, which helps the LLMs understand the strategies and methods in
conversational recommendation. Secondly, we propose a Self-Enhancement Strategy
(SES) at the inference stage to further exploit the conversational
recommendation potential obtained from RL training. Extensive experiments on
various datasets demonstrate that our method consistently outperforms previous
state-of-the-art methods.

</details>


### [9] [Document Summarization with Conformal Importance Guarantees](https://arxiv.org/abs/2509.20461)
*Bruce Kuwahara,Chen-Yuan Lin,Xiao Shi Huang,Kin Kwan Leung,Jullian Arta Yapeter,Ilya Stanevich,Felipe Perez,Jesse C. Cresswell*

Main category: cs.CL

TL;DR: 本文提出了一种基于符合预测的摘要生成框架，能够在高风险领域中提供严格的覆盖保证，确保关键内容的包含。


<details>
  <summary>Details</summary>
Motivation: 现有的自动摘要系统在高风险领域（如医疗、法律和金融）中缺乏对关键内容包含的可靠保证。因此，需要一种能够提供严格覆盖保证的摘要生成方法。

Method: 本文引入了Conformal Importance Summarization框架，利用符合预测来校准句子级重要性评分的阈值，以实现用户指定的覆盖和召回率。该方法是模型无关的，只需要一个小的校准集，并能与现有的黑盒LLMs无缝集成。

Result: 实验结果表明，Conformal Importance Summarization能够实现理论上保证的信息覆盖率，并且可以与现有技术结合，实现更安全的AI摘要工具部署。

Conclusion: 本文提出了一种基于符合预测的摘要生成框架，能够提供严格的、与分布无关的覆盖保证，从而在关键领域实现可靠和可控的自动摘要。

Abstract: Automatic summarization systems have advanced rapidly with large language
models (LLMs), yet they still lack reliable guarantees on inclusion of critical
content in high-stakes domains like healthcare, law, and finance. In this work,
we introduce Conformal Importance Summarization, the first framework for
importance-preserving summary generation which uses conformal prediction to
provide rigorous, distribution-free coverage guarantees. By calibrating
thresholds on sentence-level importance scores, we enable extractive document
summarization with user-specified coverage and recall rates over critical
content. Our method is model-agnostic, requires only a small calibration set,
and seamlessly integrates with existing black-box LLMs. Experiments on
established summarization benchmarks demonstrate that Conformal Importance
Summarization achieves the theoretically assured information coverage rate. Our
work suggests that Conformal Importance Summarization can be combined with
existing techniques to achieve reliable, controllable automatic summarization,
paving the way for safer deployment of AI summarization tools in critical
applications. Code is available at
https://github.com/layer6ai-labs/conformal-importance-summarization.

</details>


### [10] [ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos](https://arxiv.org/abs/2509.20467)
*Henrik Vatndal,Vinay Setty*

Main category: cs.CL

TL;DR: ShortCheck是一个用于检测短格式视频平台上虚假信息的系统，通过集成多种技术，在多语言环境下取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 短格式视频平台如TikTok由于其多模态、动态和噪声内容，给虚假信息检测带来了独特挑战。

Method: ShortCheck是一个模块化的、仅推理的管道，集成了语音转录、OCR、物体和深度伪造检测、视频到文本摘要以及声明验证。

Result: ShortCheck在两个手动标注的数据集上进行了评估，取得了F1加权分数超过70%的有希望的结果。

Conclusion: ShortCheck在多语言环境下表现出色，F1加权分数超过70%，展示了其在检测短视频平台上的虚假信息方面的潜力。

Abstract: Short-form video platforms like TikTok present unique challenges for
misinformation detection due to their multimodal, dynamic, and noisy content.
We present ShortCheck, a modular, inference-only pipeline with a user-friendly
interface that automatically identifies checkworthy short-form videos to help
human fact-checkers. The system integrates speech transcription, OCR, object
and deepfake detection, video-to-text summarization, and claim verification.
ShortCheck is validated by evaluating it on two manually annotated datasets
with TikTok videos in a multilingual setting. The pipeline achieves promising
results with F1-weighted score over 70\%.

</details>


### [11] [MARS: toward more efficient multi-agent collaboration for LLM reasoning](https://arxiv.org/abs/2509.20502)
*Xiao Wang,Jia Wang,Yijie Wang,Pengtao Dang,Sha Cao,Chi Zhang*

Main category: cs.CL

TL;DR: MARS is a role-based collaboration framework that enhances reasoning quality while reducing computational overhead compared to Multi-Agent Debate (MAD).


<details>
  <summary>Details</summary>
Motivation: To address the limitation of large language models (LLMs) in reasoning capabilities when operating as single agents, and to reduce the computational overhead introduced by Multi-Agent Debate (MAD).

Method: MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process.

Result: MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50%.

Conclusion: MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50%.

Abstract: Large language models (LLMs) have achieved impressive results in natural
language understanding, yet their reasoning capabilities remain limited when
operating as single agents. Multi-Agent Debate (MAD) has been proposed to
address this limitation by enabling collaborative reasoning among multiple
models in a round-table debate manner. While effective, MAD introduces
substantial computational overhead due to the number of agents involved and the
frequent communication required. In this paper, we propose MARS (Multi-Agent
Review System), a role-based collaboration framework inspired by the review
process. In MARS, an author agent generates an initial solution, reviewer
agents provide decisions and comments independently, and a meta-reviewer
integrates the feedback to make the final decision and guide further revision.
This design enhances reasoning quality while avoiding costly
reviewer-to-reviewer interactions, thereby controlling token consumption and
inference time. We compared MARS with both MAD and other state-of-the-art
reasoning strategies across multiple benchmarks. Extensive experiments with
different LLMs show that MARS matches the accuracy of MAD while reducing both
token usage and inference time by approximately 50\%. Code is available at
https://github.com/xwang97/MARS.

</details>


### [12] [SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages](https://arxiv.org/abs/2509.20557)
*Hannah Liu,Junghyun Min,Ethan Yue Heng Cheung,Shou-Yi Hung,Syed Mekael Wasti,Runtong Liang,Shiyao Qian,Shizhao Zheng,Elsie Chan,Ka Ieng Charlotte Lo,Wing Yu Yip,Richard Tzong-Han Tsai,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本文介绍了一个新的数据集SiniticMTError，用于支持机器翻译中的错误检测和评估研究。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来机器翻译取得了重大进展，但许多低资源语言的进步仍然有限，缺乏大规模训练数据和语言资源。本文旨在解决这一问题，提供一个可用于研究的资源。

Method: 本文通过母语者进行严格的注释过程，包括对注释者之间的一致性、迭代反馈以及错误类型和严重程度的分析，构建了SiniticMTError数据集。

Result: 本文成功构建了SiniticMTError数据集，该数据集包含了机器翻译示例中的错误跨度、错误类型和错误严重程度的注释。

Conclusion: 本文介绍了SiniticMTError数据集，该数据集为机器翻译社区提供了用于微调具有错误检测能力的模型的资源，并支持翻译质量评估、错误感知生成和低资源语言评估的研究。

Abstract: Despite major advances in machine translation (MT) in recent years, progress
remains limited for many low-resource languages that lack large-scale training
data and linguistic resources. Cantonese and Wu Chinese are two Sinitic
examples, although each enjoys more than 80 million speakers around the world.
In this paper, we introduce SiniticMTError, a novel dataset that builds on
existing parallel corpora to provide error span, error type, and error severity
annotations in machine-translated examples from English to Mandarin, Cantonese,
and Wu Chinese. Our dataset serves as a resource for the MT community to
utilize in fine-tuning models with error detection capabilities, supporting
research on translation quality estimation, error-aware generation, and
low-resource language evaluation. We report our rigorous annotation process by
native speakers, with analyses on inter-annotator agreement, iterative
feedback, and patterns in error type and severity.

</details>


### [13] [SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations](https://arxiv.org/abs/2509.20567)
*Ayan Sar,Pranav Singh Puri,Sumit Aich,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: SwasthLLM is a cross-lingual medical diagnosis framework that performs well in low-resource languages without requiring language-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of automatic disease diagnosis from clinical text in multilingual healthcare environments due to the scarcity of annotated medical data in low-resource languages and linguistic variability across populations.

Method: SwasthLLM is a unified, zero-shot, cross-lingual, and multi-task learning framework for medical diagnosis that uses multilingual XLM-RoBERTa encoder with language-aware attention mechanism, Siamese contrastive learning module, translation consistency module, and contrastive projection head. It is trained using a multi-task learning strategy and Model-Agnostic Meta-Learning (MAML).

Result: SwasthLLM achieves a test accuracy of 97.22% and an F1-score of 97.17% in supervised settings. In zero-shot scenarios, it attains 92.78% accuracy on Hindi and 73.33% accuracy on Bengali medical text.

Conclusion: SwasthLLM achieves high diagnostic performance and demonstrates strong generalization in low-resource contexts.

Abstract: In multilingual healthcare environments, automatic disease diagnosis from
clinical text remains a challenging task due to the scarcity of annotated
medical data in low-resource languages and the linguistic variability across
populations. This paper proposes SwasthLLM, a unified, zero-shot,
cross-lingual, and multi-task learning framework for medical diagnosis that
operates effectively across English, Hindi, and Bengali without requiring
language-specific fine-tuning. At its core, SwasthLLM leverages the
multilingual XLM-RoBERTa encoder augmented with a language-aware attention
mechanism and a disease classification head, enabling the model to extract
medically relevant information regardless of the language structure. To align
semantic representations across languages, a Siamese contrastive learning
module is introduced, ensuring that equivalent medical texts in different
languages produce similar embeddings. Further, a translation consistency module
and a contrastive projection head reinforce language-invariant representation
learning. SwasthLLM is trained using a multi-task learning strategy, jointly
optimizing disease classification, translation alignment, and contrastive
learning objectives. Additionally, we employ Model-Agnostic Meta-Learning
(MAML) to equip the model with rapid adaptation capabilities for unseen
languages or tasks with minimal data. Our phased training pipeline emphasizes
robust representation alignment before task-specific fine-tuning. Extensive
evaluation shows that SwasthLLM achieves high diagnostic performance, with a
test accuracy of 97.22% and an F1-score of 97.17% in supervised settings.
Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and
73.33% accuracy on Bengali medical text, demonstrating strong generalization in
low-resource contexts.

</details>


### [14] [Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures](https://arxiv.org/abs/2509.20577)
*Sampurna Roy,Ayan Sar,Anurag Kaushish,Kanav Gupta,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: DS-MoE 是一种基于深度专门化的 Mixture of Experts 框架，能够动态组装推理链，从而提高效率、推理质量和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前的 transformer 架构对所有输入应用相同的处理深度，导致效率低下并限制推理质量。简单事实查询与复杂逻辑问题一样经历多层计算，浪费资源并限制深度推理。

Method: DS-MoE 是一种模块化框架，扩展了基于宽度的 Mixture of Experts 范式到基于深度的计算。它引入了针对不同推理深度的专家模块，并通过学习的路由网络动态组装定制的推理链。

Result: 实验结果表明，DS-MoE 在计算节省和推理速度方面优于统一深度的 transformer，同时在复杂多步骤推理基准测试中实现了更高的准确性。此外，路由决策产生了可解释的推理链，提高了透明度和可扩展性。

Conclusion: DS-MoE 是一种在自适应神经架构方面的重大进展，表明深度专门化的模块化处理可以同时提高效率、推理质量和可解释性。

Abstract: Contemporary transformer architectures apply identical processing depth to
all inputs, creating inefficiencies and limiting reasoning quality. Simple
factual queries are subjected to the same multilayered computation as complex
logical problems, wasting resources while constraining deep inference. To
overcome this, we came up with a concept of Dynamic Reasoning Chains through
Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends
the Mixture of Experts paradigm from width-based to depth specialised
computation. DS-MoE introduces expert modules optimised for distinct reasoning
depths, shallow pattern recognition, compositional reasoning, logical
inference, memory integration, and meta-cognitive supervision. A learned
routing network dynamically assembles custom reasoning chains, activating only
the necessary experts to match input complexity. The dataset on which we
trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse
domains such as scientific papers, legal texts, programming code, and web
content, enabling systematic assessment across reasoning depths. Experimental
results demonstrate that DS-MoE achieves up to 16 per cent computational
savings and 35 per cent faster inference compared to uniform-depth
transformers, while delivering 2.8 per cent higher accuracy on complex
multi-step reasoning benchmarks. Furthermore, routing decisions yield
interpretable reasoning chains, enhancing transparency and scalability. These
findings establish DS-MoE as a significant advancement in adaptive neural
architectures, demonstrating that depth-specialised modular processing can
simultaneously improve efficiency, reasoning quality, and interpretability in
large-scale language models.

</details>


### [15] [Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding](https://arxiv.org/abs/2509.20581)
*Ayan Sar,Sampurna Roy,Kanav Gupta,Anurag Kaushish,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: HRT 是一种受小波启发的新型神经架构，能够在多个分辨率上同时处理语言，从而实现更高的效率和更好的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer 架构在自然语言任务中取得了最先进的性能，但它们根本上错误地表示了人类语言的层次性，通过将文本作为平面标记序列进行处理。这导致了二次计算成本、弱计算成本、弱组合泛化和不足的语篇级建模。

Method: HRT 是一种受小波启发的神经架构，它同时在多个分辨率上处理语言，从字符到话语级单元。HRT 构建了一个多分辨率注意力机制，实现了自底向上的组合和自顶向下的上下文化。通过在不同尺度上使用指数序列缩减，HRT 实现了 O(nlogn) 的复杂度。

Result: HRT 在 GLUE、SuperGLUE、Long Range Arena 和 WikiText-103 等多样化的基准测试中进行了评估，结果表明 HRT 在 GLUE 上平均优于标准 Transformer 基线 +3.8%，在 SuperGLUE 上 +4.5%，在 Long Range Arena 上 +6.1%，同时与 BERT 和 GPT 风格的模型相比，内存使用量减少了 42%，推理延迟减少了 37%。消融研究证实了跨分辨率注意力和尺度专用模块的有效性，显示每个都独立地对效率和准确性有所贡献。

Conclusion: HRT 是第一个与人类语言的层次结构对齐的架构，展示了多尺度、小波启发式处理在理论效率增益和语言理解的实际改进方面的有效性。

Abstract: Transformer architectures have achieved state-of-the-art performance across
natural language tasks, yet they fundamentally misrepresent the hierarchical
nature of human language by processing text as flat token sequences. This
results in quadratic computational cost, weak computational cost, weak
compositional generalization, and inadequate discourse-level modeling. We
propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired
neural architecture that processes language simultaneously across multiple
resolutions, from characters to discourse-level units. HRT constructs a
multi-resolution attention, enabling bottom-up composition and top-down
contextualization. By employing exponential sequence reduction across scales,
HRT achieves O(nlogn) complexity, offering significant efficiency improvements
over standard transformers. We evaluated HRT on a diverse suite of benchmarks,
including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results
demonstrated that HRT outperforms standard transformer baselines by an average
of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while
reducing memory usage by 42% and inference latency by 37% compared to BERT and
GPT style models of similar parameter count. Ablation studies confirm the
effectiveness of cross-resolution attention and scale-specialized modules,
showing that each contributes independently to both efficiency and accuracy.
Our findings establish HRT as the first architecture to align computational
structure with the hierarchical organization of human language, demonstrating
that multi-scale, wavelet-inspired processing yields both theoretical
efficiency gains and practical improvements in language understanding.

</details>


### [16] [FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models](https://arxiv.org/abs/2509.20624)
*Amin Karimi Monsefi,Nikhil Bhendawade,Manuel Rafael Ciosici,Dominic Culver,Yizhe Zhang,Irina Belousova*

Main category: cs.CL

TL;DR: FS-DFM是一种高效的离散流匹配模型，能够在减少采样步骤的同时保持生成质量，显著提高语言生成的速度和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统离散扩散模型需要数百到数千次模型评估才能达到高质量，而自回归语言模型生成效率低。因此，需要一种更高效的生成方法。

Method: FS-DFM是一种基于步骤数的离散流匹配模型，通过训练模型在不同步骤预算下保持一致性，实现快速且高质量的生成。

Result: FS-DFM在8步采样下实现了与1024步离散流模型相当的困惑度，同时采样速度提高了128倍。

Conclusion: FS-DFM在语言建模基准测试中表现出色，与1024步离散流基线相当，同时显著提高了采样速度和吞吐量。

Abstract: Autoregressive language models (ARMs) deliver strong likelihoods, but are
inherently serial: they generate one token per forward pass, which limits
throughput and inflates latency for long sequences. Diffusion Language Models
(DLMs) parallelize across positions and thus appear promising for language
generation, yet standard discrete diffusion typically needs hundreds to
thousands of model evaluations to reach high quality, trading serial depth for
iterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A
discrete flow-matching model designed for speed without sacrificing quality.
The core idea is simple: make the number of sampling steps an explicit
parameter and train the model to be consistent across step budgets, so one big
move lands where many small moves would. We pair this with a reliable update
rule that moves probability in the right direction without overshooting, and
with strong teacher guidance distilled from long-run trajectories. Together,
these choices make few-step sampling stable, accurate, and easy to control. On
language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity
parity with a 1,024-step discrete-flow baseline for generating 1,024 tokens
using a similar-size model, delivering up to 128 times faster sampling and
corresponding latency/throughput gains.

</details>


### [17] [Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions](https://arxiv.org/abs/2509.20645)
*Jungsoo Park,Ethan Mendes,Gabriel Stanovsky,Alan Ritter*

Main category: cs.CL

TL;DR: 本文研究了在不运行实验的情况下预测大型语言模型性能的可能性，并提出了PRECOG语料库来支持这一研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的进步受到评估瓶颈的限制：构建基准、评估模型和设置，然后进行迭代。因此，我们问了一个简单的问题：在运行任何实验之前，能否预测结果？

Method: 我们研究了文本性能预测：从删去的任务描述和预期配置中估计模型的分数，而无需访问数据集实例。为了支持系统研究，我们整理了PRECOG，一个包含删去描述-性能对的语料库，涵盖了多种任务、领域和指标。

Result: 实验表明该任务具有挑战性但可行：配备检索模块的模型（排除源论文）在高置信度阈值下达到最低8.7的平均绝对误差。分析表明，更强的推理模型会进行多样化的、迭代的查询，而当前的开源模型则落后，并且经常跳过检索或以有限的多样性收集证据。

Conclusion: 我们的语料库和分析为开放式的前瞻性评估提供了初步步骤，支持难度估计和更智能的实验优先级排序。

Abstract: Progress in large language models is constrained by an evaluation bottleneck:
build a benchmark, evaluate models and settings, then iterate. We therefore ask
a simple question: can we forecast outcomes before running any experiments? We
study text-only performance forecasting: estimating a model's score from a
redacted task description and intended configuration, with no access to dataset
instances. To support systematic study, we curate PRECOG, a corpus of redacted
description-performance pairs spanning diverse tasks, domains, and metrics.
Experiments show the task is challenging but feasible: models equipped with a
retrieval module that excludes source papers achieve moderate prediction
performance with well-calibrated uncertainty, reaching mean absolute error as
low as 8.7 on the Accuracy subset at high-confidence thresholds. Our analysis
indicates that stronger reasoning models engage in diverse, iterative querying,
whereas current open-source models lag and often skip retrieval or gather
evidence with limited diversity. We further test a zero-leakage setting,
forecasting on newly released datasets or experiments before their papers are
indexed, where GPT-5 with built-in web search still attains nontrivial
prediction accuracy. Overall, our corpus and analyses offer an initial step
toward open-ended anticipatory evaluation, supporting difficulty estimation and
smarter experiment prioritization.

</details>


### [18] [Building Tailored Speech Recognizers for Japanese Speaking Assessment](https://arxiv.org/abs/2509.20655)
*Yotaro Kubo,Richard Sproat,Chihiro Taguchi,Llion Jones*

Main category: cs.CL

TL;DR: 本文介绍了为日语口语评估任务构建语音识别器的方法。我们提出了两种方法来缓解数据稀疏性问题，并展示了这些方法在准确识别音素方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然日语资源丰富，但用于训练模型生成包含声调标记的准确音素转录的数据却很少。

Method: 我们提出了两种方法来缓解数据稀疏性。首先，一个多任务训练方案引入了辅助损失函数来估计输入信号的正字法文本标签和音高模式，因此可以利用只有正字法注释的语音进行训练。第二种方法融合了两个估计器，一个用于音素字母字符串，另一个用于文本标记序列。为了结合这些估计，我们开发了一种基于有限状态转换器框架的算法。

Result: 我们的方法将CSJ核心评估集的平均摩拉标签错误率从12.3%降低到了7.1%。

Conclusion: 我们的结果表明，多任务学习和融合对于构建准确的音素识别器是有效的。我们展示了一种方法相比使用通用多语言识别器具有优势。

Abstract: This paper presents methods for building speech recognizers tailored for
Japanese speaking assessment tasks. Specifically, we build a speech recognizer
that outputs phonemic labels with accent markers. Although Japanese is
resource-rich, there is only a small amount of data for training models to
produce accurate phonemic transcriptions that include accent marks. We propose
two methods to mitigate data sparsity. First, a multitask training scheme
introduces auxiliary loss functions to estimate orthographic text labels and
pitch patterns of the input signal, so that utterances with only orthographic
annotations can be leveraged in training. The second fuses two estimators, one
over phonetic alphabet strings, and the other over text token sequences. To
combine these estimates we develop an algorithm based on the finite-state
transducer framework. Our results indicate that the use of multitask learning
and fusion is effective for building an accurate phonemic recognizer. We show
that this approach is advantageous compared to the use of generic multilingual
recognizers. The relative advantages of the proposed methods were also
compared. Our proposed methods reduced the average of mora-label error rates
from 12.3% to 7.1% over the CSJ core evaluation sets.

</details>


### [19] [Enhancing Molecular Property Prediction with Knowledge from Large Language Models](https://arxiv.org/abs/2509.20664)
*Peng Zhou,Lai Hou Tim,Zhixiang Cheng,Kun Xie,Chaoyi Li,Wei Liu,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，将从LLMs提取的知识与预训练分子模型的结构特征相结合，以提高分子属性预测的准确性。实验结果表明，该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管GNN和自监督学习方法在分子属性预测（MPP）方面取得了进展，但人类先验知识的整合仍然是不可或缺的。然而，LLM存在知识缺口和幻觉问题，特别是在研究较少的分子属性方面。因此，需要一种新的方法来结合LLM的知识和结构信息。

Method: 我们提出了一种新颖的框架，首次将从LLMs提取的知识与预训练分子模型的结构特征相结合，以增强MPP。我们的方法提示LLMs生成与领域相关的知识和可执行的分子向量化代码，产生基于知识的特征，并随后与结构表示融合。

Result: 广泛的实验表明，我们的集成方法优于现有方法，证实了LLM衍生知识与结构信息的结合为MPP提供了一个稳健且有效的解决方案。

Conclusion: 我们的集成方法在现有方法中表现更优，证实了LLM衍生知识与结构信息的结合为MPP提供了一个稳健且有效的解决方案。

Abstract: Predicting molecular properties is a critical component of drug discovery.
Recent advances in deep learning, particularly Graph Neural Networks (GNNs),
have enabled end-to-end learning from molecular structures, reducing reliance
on manual feature engineering. However, while GNNs and self-supervised learning
approaches have advanced molecular property prediction (MPP), the integration
of human prior knowledge remains indispensable, as evidenced by recent methods
that leverage large language models (LLMs) for knowledge extraction. Despite
their strengths, LLMs are constrained by knowledge gaps and hallucinations,
particularly for less-studied molecular properties. In this work, we propose a
novel framework that, for the first time, integrates knowledge extracted from
LLMs with structural features derived from pre-trained molecular models to
enhance MPP. Our approach prompts LLMs to generate both domain-relevant
knowledge and executable code for molecular vectorization, producing
knowledge-based features that are subsequently fused with structural
representations. We employ three state-of-the-art LLMs, GPT-4o, GPT-4.1, and
DeepSeek-R1, for knowledge extraction. Extensive experiments demonstrate that
our integrated method outperforms existing approaches, confirming that the
combination of LLM-derived knowledge and structural information provides a
robust and effective solution for MPP.

</details>


### [20] [RedHerring Attack: Testing the Reliability of Attack Detection](https://arxiv.org/abs/2509.20691)
*Jonathan Rusert*

Main category: cs.CL

TL;DR: 本文提出了一种新的攻击方法RedHerring，用于测试攻击检测模型的可靠性，并发现该方法能够显著降低检测准确率。同时，本文还提出了一种简单的置信度检查方法，以提高检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的攻击检测模型的可靠性尚未得到充分研究。因此，本文旨在提出一种新的攻击方法，以测试这些模型的可靠性，并为未来的防御提供新的见解。

Method: 本文提出了一种新的攻击方法RedHerring，通过修改文本使得检测模型预测存在攻击，而分类器仍然正确。这种方法在4个数据集上对3个检测器和4个分类器进行了测试，并评估了其效果。

Result: 实验结果显示，RedHerring能够在不损害分类器性能的情况下显著降低检测准确率。此外，提出的简单置信度检查方法能够有效提高检测准确率。

Conclusion: 本文提出了一个新颖的攻击设置RedHerring，旨在使攻击检测模型不可靠。实验结果表明，RedHerring能够显著降低检测准确率，同时保持（或提高）分类器的准确性。此外，本文还提出了一种简单的置信度检查作为初步防御措施，无需重新训练分类器或检测器即可大幅提高检测准确率。

Abstract: In response to adversarial text attacks, attack detection models have been
proposed and shown to successfully identify text modified by adversaries.
Attack detection models can be leveraged to provide an additional check for NLP
models and give signals for human input. However, the reliability of these
models has not yet been thoroughly explored. Thus, we propose and test a novel
attack setting and attack, RedHerring. RedHerring aims to make attack detection
models unreliable by modifying a text to cause the detection model to predict
an attack, while keeping the classifier correct. This creates a tension between
the classifier and detector. If a human sees that the detector is giving an
``incorrect'' prediction, but the classifier a correct one, then the human will
see the detector as unreliable. We test this novel threat model on 4 datasets
against 3 detectors defending 4 classifiers. We find that RedHerring is able to
drop detection accuracy between 20 - 71 points, while maintaining (or
improving) classifier accuracy. As an initial defense, we propose a simple
confidence check which requires no retraining of the classifier or detector and
increases detection accuracy greatly. This novel threat model offers new
insights into how adversaries may target detection models.

</details>


### [21] [Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms](https://arxiv.org/abs/2509.20699)
*Abhinay Shankar Belde,Rohit Ramkumar,Jonathan Rusert*

Main category: cs.CL

TL;DR: 本文提出了一种新的攻击选择策略，能够在减少查询次数的同时保持攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有的流行黑盒攻击方法通常需要大量查询，这使得它们对资源有限的研究人员来说效率低下且不切实际。

Method: 我们提出了两种新的攻击选择策略，称为Hybrid Select和Dynamic Select。Hybrid Select通过引入大小阈值来决定使用哪种选择算法，将广义的BinarySelect技术与GreedySelect结合。Dynamic Select则通过学习每种选择方法应应用于哪些文本长度来提供另一种结合广义Binary和GreedySelect的方法。

Result: 在4个数据集和6个目标模型上，我们的最佳方法（句级Hybrid Select）能够平均减少每个攻击所需的查询次数25.82%，同时保持攻击的有效性。

Conclusion: 我们的最佳方法（句级混合选择）能够在不损失攻击效果的情况下，平均减少每个攻击所需的查询次数25.82%。

Abstract: Adversarial text attack research plays a crucial role in evaluating the
robustness of NLP models. However, the increasing complexity of
transformer-based architectures has dramatically raised the computational cost
of attack testing, especially for researchers with limited resources (e.g.,
GPUs). Existing popular black-box attack methods often require a large number
of queries, which can make them inefficient and impractical for researchers. To
address these challenges, we propose two new attack selection strategies called
Hybrid and Dynamic Select, which better combine the strengths of previous
selection algorithms. Hybrid Select merges generalized BinarySelect techniques
with GreedySelect by introducing a size threshold to decide which selection
algorithm to use. Dynamic Select provides an alternative approach of combining
the generalized Binary and GreedySelect by learning which lengths of texts each
selection method should be applied to. This greatly reduces the number of
queries needed while maintaining attack effectiveness (a limitation of
BinarySelect). Across 4 datasets and 6 target models, our best
method(sentence-level Hybrid Select) is able to reduce the number of required
queries per attack up 25.82\% on average against both encoder models and LLMs,
without losing the effectiveness of the attack.

</details>


### [22] [MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model](https://arxiv.org/abs/2509.20706)
*Hsiao-Ying Huang,Yi-Cheng Lin,Hung-yi Lee*

Main category: cs.CL

TL;DR: 本文提出MI-Fuse框架，在仅使用目标领域音频和API-only LALM的情况下，使学生模型优于LALM。


<details>
  <summary>Details</summary>
Motivation: 在实际部署中，SER常常因领域不匹配而失败，而强大的LALM只能通过API访问。

Method: 提出了一种去噪标签融合框架MI-Fuse，结合了源域训练的SER分类器作为辅助教师。

Result: 实验表明，学生模型超越了LALM，并比最强基线高出3.9%。

Conclusion: 该方法在没有源数据的情况下增强了情感感知的语音系统，实现了现实中的适应。

Abstract: Large audio-language models (LALMs) show strong zero-shot ability on speech
tasks, suggesting promise for speech emotion recognition (SER). However, SER in
real-world deployments often fails under domain mismatch, where source data are
unavailable and powerful LALMs are accessible only through an API. We ask:
given only unlabeled target-domain audio and an API-only LALM, can a student
model be adapted to outperform the LALM in the target domain? To this end, we
propose MI-Fuse, a denoised label fusion framework that supplements the LALM
with a source-domain trained SER classifier as an auxiliary teacher. The
framework draws multiple stochastic predictions from both teachers, weights
their mean distributions by mutual-information-based uncertainty, and
stabilizes training with an exponential moving average teacher. Experiments
across three public emotion datasets and six cross-domain transfers show
consistent gains, with the student surpassing the LALM and outperforming the
strongest baseline by 3.9%. This approach strengthens emotion-aware speech
systems without sharing source data, enabling realistic adaptation.

</details>


### [23] [Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised Neural Grammar Induction](https://arxiv.org/abs/2509.20734)
*Jinwook Park,Kangil Kim*

Main category: cs.CL

TL;DR: 本文研究了无监督神经语法归纳中的概率分布崩溃问题，并提出了一种缓解该问题的方法，从而提高了解析性能并允许使用更紧凑的语法。


<details>
  <summary>Details</summary>
Motivation: 现有的模型面临表达能力瓶颈，导致语法过大但表现不佳。我们识别出核心问题，即概率分布崩溃，作为这一限制的根本原因。

Method: 我们引入了一种有针对性的解决方案，即“缓解崩溃的神经参数化”，以减轻概率分布崩溃的影响。

Result: 通过广泛的实证分析，我们展示了我们的方法在各种语言中显著提高了解析性能，并允许使用更紧凑的语法。

Conclusion: 我们的方法在广泛的语言范围内显著提高了解析性能，并使使用更紧凑的语法成为可能。

Abstract: Unsupervised neural grammar induction aims to learn interpretable
hierarchical structures from language data. However, existing models face an
expressiveness bottleneck, often resulting in unnecessarily large yet
underperforming grammars. We identify a core issue, $\textit{probability
distribution collapse}$, as the underlying cause of this limitation. We analyze
when and how the collapse emerges across key components of neural
parameterization and introduce a targeted solution, $\textit{collapse-relaxing
neural parameterization}$, to mitigate it. Our approach substantially improves
parsing performance while enabling the use of significantly more compact
grammars across a wide range of languages, as demonstrated through extensive
empirical analysis.

</details>


### [24] [Confidence-guided Refinement Reasoning for Zero-shot Question Answering](https://arxiv.org/abs/2509.20750)
*Youwon Jang,Woo Suk Choi,Minjoon Jung,Minsu Lee,Byoung-Tak Zhang*

Main category: cs.CL

TL;DR: C2R是一种训练无关的框架，通过构建和优化子问题及其答案来提高QA模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的QA模型在处理复杂问题时可能无法提供可靠的答案。因此，需要一种方法来提高模型的推理能力和可靠性。

Method: C2R是一种训练无关的框架，通过构建和优化子问题及其答案（子QAs）来获得目标答案的更好置信度评分。首先，C2R选择一组子QAs以探索多种推理路径，然后比较结果答案候选者的置信度评分，以选择最可靠的最终答案。

Result: C2R在不同领域的QA任务中表现出色，能够提高模型的性能，并提供关于子QA数量和质量对模型行为影响的深入分析。

Conclusion: C2R可以无缝集成到各种现有的QA模型中，并在不同模型和基准测试中表现出一致的性能提升。此外，我们提供了关于如何利用子QA影响模型行为的重要但未被充分探索的见解。

Abstract: We propose Confidence-guided Refinement Reasoning (C2R), a novel
training-free framework applicable to question-answering (QA) tasks across
text, image, and video domains. C2R strategically constructs and refines
sub-questions and their answers (sub-QAs), deriving a better confidence score
for the target answer. C2R first curates a subset of sub-QAs to explore diverse
reasoning paths, then compares the confidence scores of the resulting answer
candidates to select the most reliable final answer. Since C2R relies solely on
confidence scores derived from the model itself, it can be seamlessly
integrated with various existing QA models, demonstrating consistent
performance improvements across diverse models and benchmarks. Furthermore, we
provide essential yet underexplored insights into how leveraging sub-QAs
affects model behavior, specifically analyzing the impact of both the quantity
and quality of sub-QAs on achieving robust and reliable reasoning.

</details>


### [25] [SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs](https://arxiv.org/abs/2509.20758)
*Jiacheng Lin,Zhongruo Wang,Kun Qian,Tian Wang,Arvind Srinivasan,Hansi Zeng,Ruochen Jiao,Xie Zhou,Jiri Gesi,Dakuo Wang,Yufan Guo,Kai Zhong,Weiqi Zhang,Sujay Sanghavi,Changyou Chen,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 本文研究了监督微调对大型语言模型的影响，发现使用较小的学习率可以减轻通用性能下降，并提出了TALR方法来更好地平衡领域特定性能和通用能力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在重新审视监督微调（SFT）在领域特定数据集上的权衡问题，并提供实证和理论见解。

Method: 本文提出了一种新的方法，称为Token-Adaptive Loss Reweighting (TALR)，并评估了多种减少通用能力损失的策略，包括L2正则化、LoRA、模型平均、FLOW和TALR。

Result: 实验结果表明，虽然没有方法能完全消除权衡，但TALR在平衡领域特定收益和通用能力方面表现优于这些基线。

Conclusion: 本文提出了TALR方法，并总结了适应LLM到新领域的实用指南：(i) 使用小的学习率以获得有利的权衡，(ii) 当需要更强的平衡时，采用TALR作为一种有效策略。

Abstract: Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach
to adapt Large Language Models (LLMs) to specialized tasks but is often
believed to degrade their general capabilities. In this work, we revisit this
trade-off and present both empirical and theoretical insights. First, we show
that SFT does not always hurt: using a smaller learning rate can substantially
mitigate general performance degradation while preserving comparable
target-domain performance. We then provide a theoretical analysis that explains
these phenomena and further motivates a new method, Token-Adaptive Loss
Reweighting (TALR). Building on this, and recognizing that smaller learning
rates alone do not fully eliminate general-performance degradation in all
cases, we evaluate a range of strategies for reducing general capability loss,
including L2 regularization, LoRA, model averaging, FLOW, and our proposed
TALR. Experimental results demonstrate that while no method completely
eliminates the trade-off, TALR consistently outperforms these baselines in
balancing domain-specific gains and general capabilities. Finally, we distill
our findings into practical guidelines for adapting LLMs to new domains: (i)
using a small learning rate to achieve a favorable trade-off, and (ii) when a
stronger balance is further desired, adopt TALR as an effective strategy.

</details>


### [26] [Towards Atoms of Large Language Models](https://arxiv.org/abs/2509.20784)
*Chenhui Hu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出了Atoms理论，以原子作为大型语言模型内部表示的基本单元，并通过实验验证了其有效性，为理解LLMs的内部机制和机械可解释性提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 目前，大型语言模型（LLMs）内部表示的基本单元仍未明确，这限制了对其机制的进一步理解。神经元或特征通常被视为这些单元，但神经元存在多义性，而特征则面临不可靠重建和不稳定的问题。

Method: 本文提出了Atoms理论，引入了原子内积（AIP）来校正表示偏移，形式化定义了原子，并证明了原子满足受限等距性质（RIP），确保了原子集上的稳定稀疏表示，并与压缩感知相关联。此外，还提供了单层稀疏自编码器（SAEs）可以可靠地识别原子的保证。

Result: 在Gemma2-2B、Gemma2-9B和Llama3.1-8B上训练阈值激活的SAEs，平均在各层实现了99.9%的稀疏重建，超过99.8%的原子满足唯一性条件，相比之下，神经元和特征分别为0.5%和68.2%，表明原子更忠实地捕捉了LLMs的内在表示。

Conclusion: 本文系统地介绍了并验证了LLMs的Atoms理论，提供了一个理解内部表示的理论框架，并为机械可解释性奠定了基础。

Abstract: The fundamental units of internal representations in large language models
(LLMs) remain undefined, limiting further understanding of their mechanisms.
Neurons or features are often regarded as such units, yet neurons suffer from
polysemy, while features face concerns of unreliable reconstruction and
instability. To address this issue, we propose the Atoms Theory, which defines
such units as atoms. We introduce the atomic inner product (AIP) to correct
representation shifting, formally define atoms, and prove the conditions that
atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse
representations over atom set and linking to compressed sensing. Under stronger
conditions, we further establish the uniqueness and exact $\ell_1$
recoverability of the sparse representations, and provide guarantees that
single-layer sparse autoencoders (SAEs) with threshold activations can reliably
identify the atoms. To validate the Atoms Theory, we train threshold-activated
SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse
reconstruction across layers on average, and more than 99.8% of atoms satisfy
the uniqueness condition, compared to 0.5% for neurons and 68.2% for features,
showing that atoms more faithfully capture intrinsic representations of LLMs.
Scaling experiments further reveal the link between SAEs size and recovery
capacity. Overall, this work systematically introduces and validates Atoms
Theory of LLMs, providing a theoretical framework for understanding internal
representations and a foundation for mechanistic interpretability. Code
available at https://github.com/ChenhuiHu/towards_atoms.

</details>


### [27] [Few-Shot and Training-Free Review Generation via Conversational Prompting](https://arxiv.org/abs/2509.20805)
*Genki Kusano*

Main category: cs.CL

TL;DR: 本文提出了一种名为对话提示的方法，用于在少样本和无训练情况下生成个性化评论。该方法通过将用户评论重新表述为多轮对话来提高生成评论的准确性。实验表明，该方法在多个产品领域和大型语言模型上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数方法假设目标用户有丰富的评论历史或需要额外的模型训练。然而，在现实应用中，常常面临少样本和无训练的情况，其中只有少量用户评论可用且微调不可行。

Method: 本文提出了对话提示，这是一种轻量级方法，将用户评论重新表述为多轮对话。其简单变体（SCP）仅依赖于用户自己的评论，而对比变体（CCP）则插入其他用户的评论或LLMs作为错误回复，然后让模型进行纠正，鼓励模型以用户风格生成文本。

Result: 实验结果显示，传统的非对话提示通常会产生与随机用户撰写的评论相似的评论，而SCP和CCP生成的评论更接近目标用户的评论，即使每个用户只有两条评论。当有高质量的负面示例时，CCP进一步提高了性能，而当无法收集此类数据时，SCP仍然具有竞争力。

Conclusion: 这些结果表明，对话提示为在少样本和无训练约束下的评论生成提供了一个实用的解决方案。

Abstract: Personalized review generation helps businesses understand user preferences,
yet most existing approaches assume extensive review histories of the target
user or require additional model training. Real-world applications often face
few-shot and training-free situations, where only a few user reviews are
available and fine-tuning is infeasible. It is well known that large language
models (LLMs) can address such low-resource settings, but their effectiveness
depends on prompt engineering. In this paper, we propose Conversational
Prompting, a lightweight method that reformulates user reviews as multi-turn
conversations. Its simple variant, Simple Conversational Prompting (SCP),
relies solely on the user's own reviews, while the contrastive variant,
Contrastive Conversational Prompting (CCP), inserts reviews from other users or
LLMs as incorrect replies and then asks the model to correct them, encouraging
the model to produce text in the user's style. Experiments on eight product
domains and five LLMs showed that the conventional non-conversational prompt
often produced reviews similar to those written by random users, based on
text-based metrics such as ROUGE-L and BERTScore, and application-oriented
tasks like user identity matching and sentiment analysis. In contrast, both SCP
and CCP produced reviews much closer to those of the target user, even when
each user had only two reviews. CCP brings further improvements when
high-quality negative examples are available, whereas SCP remains competitive
when such data cannot be collected. These results suggest that conversational
prompting offers a practical solution for review generation under few-shot and
training-free constraints.

</details>


### [28] [Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching](https://arxiv.org/abs/2509.20810)
*Songze Li,Zhiqiang Liu,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Enrich-on-Graph（EoG）的灵活框架，利用大语言模型（LLM）的先验知识来丰富知识图谱（KG），弥合图谱和查询之间的语义差距。EoG能够高效地从KG中提取证据，实现精确且稳健的推理，同时确保低计算成本、可扩展性和跨不同方法的适应性。


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) exhibit strong reasoning capabilities in complex tasks. However, they still struggle with hallucinations and factual errors in knowledge-intensive scenarios like knowledge graph question answering (KGQA).

Method: We propose a flexible framework, Enrich-on-Graph (EoG), which leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between graphs and queries.

Result: Extensive experiments on two KGQA benchmark datasets indicate that EoG can effectively generate high-quality KGs and achieve the state-of-the-art performance.

Conclusion: EoG can effectively generate high-quality KGs and achieve the state-of-the-art performance.

Abstract: Large Language Models (LLMs) exhibit strong reasoning capabilities in complex
tasks. However, they still struggle with hallucinations and factual errors in
knowledge-intensive scenarios like knowledge graph question answering (KGQA).
We attribute this to the semantic gap between structured knowledge graphs (KGs)
and unstructured queries, caused by inherent differences in their focuses and
structures. Existing methods usually employ resource-intensive, non-scalable
workflows reasoning on vanilla KGs, but overlook this gap. To address this
challenge, we propose a flexible framework, Enrich-on-Graph (EoG), which
leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between
graphs and queries. EoG enables efficient evidence extraction from KGs for
precise and robust reasoning, while ensuring low computational costs,
scalability, and adaptability across different methods. Furthermore, we propose
three graph quality evaluation metrics to analyze query-graph alignment in KGQA
task, supported by theoretical validation of our optimization objectives.
Extensive experiments on two KGQA benchmark datasets indicate that EoG can
effectively generate high-quality KGs and achieve the state-of-the-art
performance. Our code and data are available at
https://github.com/zjukg/Enrich-on-Graph.

</details>


### [29] [Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection](https://arxiv.org/abs/2509.20811)
*Taehee Park,Heejin Do,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为 PoCO 的新方法，通过利用 LLM 的生成能力并保留较小监督模型的可靠性，以平衡召回率和精确度，从而提高语法错误修正的整体质量。


<details>
  <summary>Details</summary>
Motivation: 为了有效利用 LLM 的优势来解决 sLMs 的召回率挑战，提出了 PoCO 方法，以战略上平衡召回率和精确度。

Method: PoCO 首先通过 LLM 触发过校正以最大化召回率，然后通过微调较小的模型进行有针对性的后校正步骤，以识别和精炼错误输出。

Result: 实验结果表明，PoCO 能够有效地平衡 GEC 性能，提高语法错误修正的整体质量。

Conclusion: PoCO 有效地平衡了 GEC 性能，通过增加召回率并保持竞争性精度，最终提高了语法错误修正的整体质量。

Abstract: Robust supervised fine-tuned small Language Models (sLMs) often show high
reliability but tend to undercorrect. They achieve high precision at the cost
of low recall. Conversely, Large Language Models (LLMs) often show the opposite
tendency, making excessive overcorrection, leading to low precision. To
effectively harness the strengths of LLMs to address the recall challenges in
sLMs, we propose Post-Correction via Overcorrection (PoCO), a novel approach
that strategically balances recall and precision. PoCO first intentionally
triggers overcorrection via LLM to maximize recall by allowing comprehensive
revisions, then applies a targeted post-correction step via fine-tuning smaller
models to identify and refine erroneous outputs. We aim to harmonize both
aspects by leveraging the generative power of LLMs while preserving the
reliability of smaller supervised models. Our extensive experiments demonstrate
that PoCO effectively balances GEC performance by increasing recall with
competitive precision, ultimately improving the overall quality of grammatical
error correction.

</details>


### [30] [Distilling Many-Shot In-Context Learning into a Cheat Sheet](https://arxiv.org/abs/2509.20820)
*Ukyo Honda,Soichiro Murakami,Peinan Zhang*

Main category: cs.CL

TL;DR: 备忘录式ICL通过将许多示例ICL的信息提炼为简洁的文本摘要，在推理时用作上下文，从而在保持性能的同时减少计算需求。


<details>
  <summary>Details</summary>
Motivation: 解决许多示例ICL由于输入标记较长而导致的高计算需求问题。

Method: 提出了一种备忘录式ICL方法，将许多示例ICL的信息提炼成一个简洁的文本摘要（备忘录），在推理时用作上下文。

Result: 实验结果表明，备忘录式ICL在使用更少标记的情况下，性能与许多示例ICL相当或更好，并且不需要测试时检索即可匹配基于检索的ICL。

Conclusion: 这些发现表明，备忘录式ICL是利用LLMs进行下游任务的一种实用替代方案。

Abstract: Recent advances in large language models (LLMs) enable effective in-context
learning (ICL) with many-shot examples, but at the cost of high computational
demand due to longer input tokens. To address this, we propose cheat-sheet ICL,
which distills the information from many-shot ICL into a concise textual
summary (cheat sheet) used as the context at inference time. Experiments on
challenging reasoning tasks show that cheat-sheet ICL achieves comparable or
better performance than many-shot ICL with far fewer tokens, and matches
retrieval-based ICL without requiring test-time retrieval. These findings
demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs
in downstream tasks.

</details>


### [31] [Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search](https://arxiv.org/abs/2509.20838)
*Shuo Huang,Xingliang Yuan,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CL

TL;DR: 本文提出了一种基于树搜索的迭代句子重写算法，用于在保持文本自然性和实用性的同时，有效保护隐私信息。


<details>
  <summary>Details</summary>
Motivation: 现有文本匿名化和去标识技术难以在隐私保护与文本自然性和实用性之间取得平衡，因此需要一种更有效的解决方案。

Method: 我们提出了一种零样本、基于树搜索的迭代句子重写算法，通过结构化搜索引导奖励模型，逐步重写隐私敏感部分，以保持连贯性、相关性和自然性。

Result: 实验表明，我们的方法在隐私敏感数据集上显著优于现有基线方法，实现了更好的隐私保护和实用性平衡。

Conclusion: 我们的方法在隐私保护和实用性之间取得了更好的平衡，显著优于现有的基线方法。

Abstract: The increasing adoption of large language models (LLMs) in cloud-based
services has raised significant privacy concerns, as user inputs may
inadvertently expose sensitive information. Existing text anonymization and
de-identification techniques, such as rule-based redaction and scrubbing, often
struggle to balance privacy preservation with text naturalness and utility. In
this work, we propose a zero-shot, tree-search-based iterative sentence
rewriting algorithm that systematically obfuscates or deletes private
information while preserving coherence, relevance, and naturalness. Our method
incrementally rewrites privacy-sensitive segments through a structured search
guided by a reward model, enabling dynamic exploration of the rewriting space.
Experiments on privacy-sensitive datasets show that our approach significantly
outperforms existing baselines, achieving a superior balance between privacy
protection and utility preservation.

</details>


### [32] [Concise and Sufficient Sub-Sentence Citations for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20859)
*Guo Chen,Qiuyuan Li,Qiuxian Li,Hongliang Dai,Xiang Chen,Piji Li*

Main category: cs.CL

TL;DR: 本文提出生成子句引用，以减少用户确认生成输出正确性所需的努力。


<details>
  <summary>Details</summary>
Motivation: 现有的归属方法在引用中存在两个问题：引用通常以句子或甚至段落级别提供，这可能包含大量不相关的内容；句子级别的引用可能遗漏对于验证输出至关重要的信息，迫使用户阅读上下文。

Method: 我们首先制定了此类引用的注释指南并构建了相应的数据集。然后，我们提出了一个符合我们标准的引用生成框架。该框架利用LLM自动生成微调数据，并使用信用模型过滤低质量示例。

Result: 我们在构建的数据集上的实验表明，所提出的方法可以生成高质量且更易读的引用。

Conclusion: 本文提出的方法能够生成高质量且更易读的引用。

Abstract: In retrieval-augmented generation (RAG) question answering systems,
generating citations for large language model (LLM) outputs enhances
verifiability and helps users identify potential hallucinations. However, we
observe two problems in the citations produced by existing attribution methods.
First, the citations are typically provided at the sentence or even paragraph
level. Long sentences or paragraphs may include a substantial amount of
irrelevant content. Second, sentence-level citations may omit information that
is essential for verifying the output, forcing users to read the surrounding
context. In this paper, we propose generating sub-sentence citations that are
both concise and sufficient, thereby reducing the effort required by users to
confirm the correctness of the generated output. To this end, we first develop
annotation guidelines for such citations and construct a corresponding dataset.
Then, we propose an attribution framework for generating citations that adhere
to our standards. This framework leverages LLMs to automatically generate
fine-tuning data for our task and employs a credit model to filter out
low-quality examples. Our experiments on the constructed dataset demonstrate
that the propose approach can generate high-quality and more readable
citations.

</details>


### [33] [WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs](https://arxiv.org/abs/2509.20863)
*Guowei Xu,Wenxin Xu,Jiawang Zhao,Kaisheng Ma*

Main category: cs.CL

TL;DR: 本文提出了一种名为WeFT的加权监督微调方法，用于扩散语言模型，通过基于熵的权重分配来提高生成质量和一致性，并在多个推理基准测试中取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语言建模中显示出潜力，但监督微调仍然具有挑战性，因为它们在每个去噪步骤中缺乏精确的概率估计。此外，生成过程不够可预测和一致，因此需要控制关键标记来引导生成方向。

Method: WeFT是一种基于熵的加权监督微调方法，用于扩散语言模型。

Result: 在s1K、s1K-1.1和3k样本上进行训练，WeFT在四个广泛使用的推理基准测试（Sudoku、Countdown、GSM8K和MATH-500）中相对于标准SFT分别实现了39%、64%和83%的相对改进。

Conclusion: WeFT方法在多个推理基准测试中表现出显著的改进，证明了其有效性。代码和模型将公开提供。

Abstract: Diffusion models have recently shown strong potential in language modeling,
offering faster generation compared to traditional autoregressive approaches.
However, applying supervised fine-tuning (SFT) to diffusion models remains
challenging, as they lack precise probability estimates at each denoising step.
While the diffusion mechanism enables the model to reason over entire
sequences, it also makes the generation process less predictable and often
inconsistent. This highlights the importance of controlling key tokens that
guide the direction of generation. To address this issue, we propose WeFT, a
weighted SFT method for diffusion language models, where tokens are assigned
different weights based on their entropy. Derived from diffusion theory, WeFT
delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from
open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard
SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and
MATH-500). The code and models will be made publicly available.

</details>


### [34] [Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models](https://arxiv.org/abs/2509.20866)
*Pittawat Taveekitworachai,Natpatchara Pongjirapat,Krittaphas Chaisutyakorn,Piyalitt Ittichaiwong,Tossaporn Saengja,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本文研究了如何使医疗推理模型生成排名列表的答案，提出并比较了提示和微调的方法，发现RFT训练的模型在多种答案格式上更稳健。


<details>
  <summary>Details</summary>
Motivation: 临床决策很少依赖单一答案，而是考虑多个选项，以减少狭窄视角的风险。然而，当前的MRMs通常只生成一个答案，即使在开放式设置中也是如此。

Method: 本文提出了两种方法：提示和微调。SFT用于模仿注释响应，RFT通过最大化奖励来激励探索。还提出了针对排名列表答案格式的新奖励函数，并进行了RFT的消融研究。

Result: 结果表明，虽然一些SFT模型可以推广到某些答案格式，但使用RFT训练的模型在多种格式上更加稳健。在修改后的MedQA案例研究中，发现MRMs可能无法选择基准的首选真实答案，但可以识别有效答案。

Conclusion: 本文是首次系统研究如何使医疗推理模型生成排名列表的论文，希望为医疗领域提供超越单一答案的替代答案格式的第一步。

Abstract: This paper presents a systematic study on enabling medical reasoning models
(MRMs) to generate ranked lists of answers for open-ended questions. Clinical
decision-making rarely relies on a single answer but instead considers multiple
options, reducing the risks of narrow perspectives. Yet current MRMs are
typically trained to produce only one answer, even in open-ended settings. We
propose an alternative format: ranked lists and investigate two approaches:
prompting and fine-tuning. While prompting is a cost-effective way to steer an
MRM's response, not all MRMs generalize well across different answer formats:
choice, short text, and list answers. Based on our prompting findings, we train
and evaluate MRMs using supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT teaches a model to imitate annotated responses, and RFT
incentivizes exploration through the responses that maximize a reward. We
propose new reward functions targeted at ranked-list answer formats, and
conduct ablation studies for RFT. Our results show that while some SFT models
generalize to certain answer formats, models trained with RFT are more robust
across multiple formats. We also present a case study on a modified MedQA with
multiple valid answers, finding that although MRMs might fail to select the
benchmark's preferred ground truth, they can recognize valid answers. To the
best of our knowledge, this is the first systematic investigation of approaches
for enabling MRMs to generate answers as ranked lists. We hope this work
provides a first step toward developing alternative answer formats that are
beneficial beyond single answers in medical domains.

</details>


### [35] [Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization](https://arxiv.org/abs/2509.20900)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: 本文提出了一种基于对抗性多智能体协作的长文档摘要框架SummQ，通过摘要生成器、评审员和测验生成器、评审员之间的协作来提高摘要质量，并在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理过长文档时，常常面临信息丢失、事实不一致和连贯性问题，因此需要一种新的方法来解决这些问题。

Method: 本文提出了SummQ，一个基于对抗性多智能体框架的方法，通过摘要生成器、评审员和测验生成器、评审员之间的协作来创建和评估全面的摘要，并通过测验机制进行持续的质量检查。

Result: 实验结果表明，SummQ在ROUGE和BERTScore指标以及LLM-as-a-Judge和人类评估中均显著优于现有最先进的方法。

Conclusion: 本文提出了一种新的对抗性多智能体框架SummQ，通过协作智能解决了长文档摘要中的信息丢失、事实不一致和连贯性问题。实验结果表明，该框架在多个指标上优于现有最先进的方法，并建立了基于对抗性智能体协作的长文档摘要新方法。

Abstract: Long document summarization remains a significant challenge for current large
language models (LLMs), as existing approaches commonly struggle with
information loss, factual inconsistencies, and coherence issues when processing
excessively long documents. We propose SummQ, a novel adversarial multi-agent
framework that addresses these limitations through collaborative intelligence
between specialized agents operating in two complementary domains:
summarization and quizzing. Our approach employs summary generators and
reviewers that work collaboratively to create and evaluate comprehensive
summaries, while quiz generators and reviewers create comprehension questions
that serve as continuous quality checks for the summarization process. This
adversarial dynamic, enhanced by an examinee agent that validates whether the
generated summary contains the information needed to answer the quiz questions,
enables iterative refinement through multifaceted feedback mechanisms. We
evaluate SummQ on three widely used long document summarization benchmarks.
Experimental results demonstrate that our framework significantly outperforms
existing state-of-the-art methods across ROUGE and BERTScore metrics, as well
as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal
the effectiveness of the multi-agent collaboration dynamics, the influence of
different agent configurations, and the impact of the quizzing mechanism. This
work establishes a new approach for long document summarization that uses
adversarial agentic collaboration to improve summarization quality.

</details>


### [36] [MemLens: Uncovering Memorization in LLMs with Activation Trajectories](https://arxiv.org/abs/2509.20909)
*Zirui He,Haiyan Zhao,Ali Payani,Mengnan du*

Main category: cs.CL

TL;DR: 本文提出 MemLens 方法，通过分析生成过程中数字标记的概率轨迹来检测记忆，结果表明该方法能有效区分污染和干净样本。


<details>
  <summary>Details</summary>
Motivation: 现有的检测方法在面对隐式污染数据时表现不佳，需要一种更有效的检测方法。

Method: MemLens 通过分析生成过程中数字标记的概率轨迹来检测记忆。

Result: 污染样本表现出“捷径”行为，而干净样本则显示出更渐进的证据积累。

Conclusion: MemLens 提供了对记忆检测的真正信号的捕捉，而不是偶然的相关性。

Abstract: Large language models (LLMs) are commonly evaluated on challenging benchmarks
such as AIME and Math500, which are susceptible to contamination and risk of
being memorized. Existing detection methods, which primarily rely on
surface-level lexical overlap and perplexity, demonstrate low generalization
and degrade significantly when encountering implicitly contaminated data. In
this paper, we propose MemLens (An Activation Lens for Memorization Detection)
to detect memorization by analyzing the probability trajectories of numeric
tokens during generation. Our method reveals that contaminated samples exhibit
``shortcut'' behaviors, locking onto an answer with high confidence in the
model's early layers, whereas clean samples show more gradual evidence
accumulation across the model's full depth. We observe that contaminated and
clean samples exhibit distinct and well-separated reasoning trajectories. To
further validate this, we inject carefully designed samples into the model
through LoRA fine-tuning and observe the same trajectory patterns as in
naturally contaminated data. These results provide strong evidence that MemLens
captures genuine signals of memorization rather than spurious correlations.

</details>


### [37] [Cross-Linguistic Analysis of Memory Load in Sentence Comprehension: Linear Distance and Structural Density](https://arxiv.org/abs/2509.20916)
*Krishna Aggarwal*

Main category: cs.CL

TL;DR: 本研究探讨了句子级记忆负荷的解释因素，发现句子长度对记忆负荷影响最大，而介入复杂度提供了额外的解释力，调和了线性和层次化的地方性观点。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨句子级记忆负荷是更好由句法相关词之间的线性接近度解释，还是由中间材料的结构密度解释。

Method: 研究使用了统一依存树库和跨语言混合效应框架，联合评估了句子长度、依存长度和介入复杂度作为记忆负荷的预测因素。

Result: 所有三个因素都与记忆负荷正相关，其中句子长度影响最大，介入复杂度在解释线性距离之外提供了额外的解释力。

Conclusion: 研究结果表明，句子长度对记忆负荷的影响最大，而介入复杂度在解释线性距离之外提供了额外的解释力。概念上，研究调和了线性和层次化的地方性观点，方法上展示了如何通过基于UD的图度量和跨语言混合效应建模来区分线性和结构贡献，为评估记忆负荷的竞争理论提供了一条有原则的路径。

Abstract: This study examines whether sentence-level memory load in comprehension is
better explained by linear proximity between syntactically related words or by
the structural density of the intervening material. Building on locality-based
accounts and cross-linguistic evidence for dependency length minimization, the
work advances Intervener Complexity-the number of intervening heads between a
head and its dependent-as a structurally grounded lens that refines linear
distance measures. Using harmonized dependency treebanks and a mixed-effects
framework across multiple languages, the analysis jointly evaluates sentence
length, dependency length, and Intervener Complexity as predictors of the
Memory-load measure. Studies in Psycholinguistics have reported the
contributions of feature interference and misbinding to memory load during
processing. For this study, I operationalized sentence-level memory load as the
linear sum of feature misbinding and feature interference for tractability;
current evidence does not establish that their cognitive contributions combine
additively. All three factors are positively associated with memory load, with
sentence length exerting the broadest influence and Intervener Complexity
offering explanatory power beyond linear distance. Conceptually, the findings
reconcile linear and hierarchical perspectives on locality by treating
dependency length as an important surface signature while identifying
intervening heads as a more proximate indicator of integration and maintenance
demands. Methodologically, the study illustrates how UD-based graph measures
and cross-linguistic mixed-effects modelling can disentangle linear and
structural contributions to processing efficiency, providing a principled path
for evaluating competing theories of memory load in sentence comprehension.

</details>


### [38] [Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning](https://arxiv.org/abs/2509.20957)
*Asim Ersoy,Enes Altinisik,Husrev Taha Sencar,Kareem Darwish*

Main category: cs.CL

TL;DR: 本文研究了阿拉伯语工具调用数据的必要性、通用指令微调的影响以及特定高优先级工具微调的价值，并通过翻译和适应开源数据集来填补资源差距。


<details>
  <summary>Details</summary>
Motivation: 研究和资源主要集中在英语上，这导致了对如何在其他语言（如阿拉伯语）中启用此功能的理解不足。

Method: 我们使用基础和后训练的开放权重阿拉伯语LLM进行了广泛的实验，并通过翻译和适应两个开源工具调用数据集到阿拉伯语来填补资源差距。

Result: 我们的研究结果提供了关于为阿拉伯语开发强大工具增强代理的最佳策略的重要见解。

Conclusion: 本文提供了关于如何为阿拉伯语开发强大工具增强代理的最佳策略的重要见解。

Abstract: Tool calling is a critical capability that allows Large Language Models
(LLMs) to interact with external systems, significantly expanding their
utility. However, research and resources for tool calling are predominantly
English-centric, leaving a gap in our understanding of how to enable this
functionality for other languages, such as Arabic. This paper investigates
three key research questions: (1) the necessity of in-language (Arabic)
tool-calling data versus relying on cross-lingual transfer, (2) the effect of
general-purpose instruction tuning on tool-calling performance, and (3) the
value of fine-tuning on specific, high-priority tools. To address these
questions, we conduct extensive experiments using base and post-trained
variants of an open-weight Arabic LLM. To enable this study, we bridge the
resource gap by translating and adapting two open-source tool-calling datasets
into Arabic. Our findings provide crucial insights into the optimal strategies
for developing robust tool-augmented agents for Arabic.

</details>


### [39] [Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting](https://arxiv.org/abs/2509.20982)
*Valeria Ramirez-Garcia,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.CL

TL;DR: 研究探讨了基于大型语言模型的自动评估系统在学术文本输入问题中的应用，发现参考辅助评估是最有效的评估方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索基于大型语言模型的自动评估系统在学术文本输入问题中的应用。

Method: 研究提出了五种评估系统，包括JudgeLM评估、参考辅助评估、无参考评估、加法评估和自适应评估，并在自定义数据集上进行了测试。

Result: 结果显示，参考辅助评估是使用大型语言模型自动评估和评分文本输入问题的最佳方法，具有最低的中位数绝对偏差和均方根偏差。

Conclusion: 人工智能驱动的自动评估系统，在适当的方法论辅助下，显示出作为其他学术资源补充工具的潜力。

Abstract: Large language models (LLMs) can act as evaluators, a role studied by methods
like LLM-as-a-Judge and fine-tuned judging LLMs. In the field of education,
LLMs have been studied as assistant tools for students and teachers. Our
research investigates LLM-driven automatic evaluation systems for academic
Text-Input Problems using rubrics. We propose five evaluation systems that have
been tested on a custom dataset of 110 answers about computer science from
higher education students with three models: JudgeLM, Llama-3.1-8B and
DeepSeek-R1-Distill-Llama-8B. The evaluation systems include: The JudgeLM
evaluation, which uses the model's single answer prompt to obtain a score;
Reference Aided Evaluation, which uses a correct answer as a guide aside from
the original context of the question; No Reference Evaluation, which ommits the
reference answer; Additive Evaluation, which uses atomic criteria; and Adaptive
Evaluation, which is an evaluation done with generated criteria fitted to each
question. All evaluation methods have been compared with the results of a human
evaluator. Results show that the best method to automatically evaluate and
score Text-Input Problems using LLMs is Reference Aided Evaluation. With the
lowest median absolute deviation (0.945) and the lowest root mean square
deviation (1.214) when compared to human evaluation, Reference Aided Evaluation
offers fair scoring as well as insightful and complete evaluations. Other
methods such as Additive and Adaptive Evaluation fail to provide good results
in concise answers, No Reference Evaluation lacks information needed to
correctly assess questions and JudgeLM Evaluations have not provided good
results due to the model's limitations. As a result, we conclude that
Artificial Intelligence-driven automatic evaluation systems, aided with proper
methodologies, show potential to work as complementary tools to other academic
resources.

</details>


### [40] [Generative AI for FFRDCs](https://arxiv.org/abs/2509.21040)
*Arun S. Maiya*

Main category: cs.CL

TL;DR: 本文探讨了如何利用大型语言模型和OnPrem.LLM框架来加速联邦资助研究和发展中心的文本处理任务。


<details>
  <summary>Details</summary>
Motivation: 联邦资助的研究和发展中心面临文本密集的工作量，手动分析速度缓慢。

Method: 应用OnPrem.LLM框架，这是一种用于生成式AI安全和灵活应用的开源框架。

Result: 案例研究显示，这种方法可以提高监督和战略分析能力。

Conclusion: 本文展示了如何利用大型语言模型加速摘要、分类、提取和理解，同时保持审计性和数据主权。

Abstract: Federally funded research and development centers (FFRDCs) face text-heavy
workloads, from policy documents to scientific and engineering papers, that are
slow to analyze manually. We show how large language models can accelerate
summarization, classification, extraction, and sense-making with only a few
input-output examples. To enable use in sensitive government contexts, we apply
OnPrem$.$LLM, an open-source framework for secure and flexible application of
generative AI. Case studies on defense policy documents and scientific corpora,
including the National Defense Authorization Act (NDAA) and National Science
Foundation (NSF) Awards, demonstrate how this approach enhances oversight and
strategic analysis while maintaining auditability and data sovereignty.

</details>


### [41] [Behind RoPE: How Does Causal Mask Encode Positional Information?](https://arxiv.org/abs/2509.21042)
*Junu Kim,Xiao Liu,Zhenghao Lin,Lei Ji,Yeyun Gong,Edward Choi*

Main category: cs.CL

TL;DR: 本文研究了因果掩码在Transformer模型中作为位置信息来源的作用，并发现其与RoPE的交互会影响注意力模式。


<details>
  <summary>Details</summary>
Motivation: 尽管显式的位置编码（如RoPE）是Transformer解码器中的主要位置信息来源，但因果掩码也提供了位置信息。我们需要了解因果掩码对模型行为的影响。

Method: 我们通过理论分析和实验验证了因果掩码如何在注意力分数中诱导位置依赖模式，并研究了其与RoPE的相互作用。

Result: 我们的理论分析表明，因果掩码可以诱导位置依赖的注意力模式，而实验结果证实了这一点，并发现RoPE与因果掩码的交互会扭曲RoPE的相对注意力模式。

Conclusion: 我们的研究表明，因果掩码可以作为位置信息的来源，与显式的位置编码（如RoPE）一样重要。

Abstract: While explicit positional encodings such as RoPE are a primary source of
positional information in Transformer decoders, the causal mask also provides
positional information. In this work, we prove that the causal mask can induce
position-dependent patterns in attention scores, even without parameters or
causal dependency in the input. Our theoretical analysis indicates that the
induced attention pattern tends to favor nearby query-key pairs, mirroring the
behavior of common positional encodings. Empirical analysis confirms that
trained models exhibit the same behavior, with learned parameters further
amplifying these patterns. Notably, we found that the interaction of causal
mask and RoPE distorts RoPE's relative attention score patterns into
non-relative ones. We consistently observed this effect in modern large
language models, suggesting the importance of considering the causal mask as a
source of positional information alongside explicit positional encodings.

</details>


### [42] [When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following](https://arxiv.org/abs/2509.21051)
*Keno Harada,Yudai Yamazaki,Masachika Taniguchi,Edison Marrese-Taylor,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型同时遵循多个指令的能力，并通过引入基准测试和开发回归模型来评估和预测性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在现实场景中的应用越来越广泛，了解它们同时遵循多个指令的能力变得至关重要。

Method: 本文引入了两个专门的基准测试来评估大型语言模型同时遵循多个指令的能力，并开发了三种类型的回归模型来估计性能。

Result: 实验结果表明，随着指令数量的增加，性能持续下降，并且使用回归模型可以预测性能，误差约为10%。

Conclusion: 本文结论是，随着指令数量的增加，大型语言模型的性能会持续下降，并且通过使用回归模型可以有效地估计在未见过的指令组合上的性能。

Abstract: As large language models (LLMs) are increasingly applied to real-world
scenarios, it becomes crucial to understand their ability to follow multiple
instructions simultaneously. To systematically evaluate these capabilities, we
introduce two specialized benchmarks for fundamental domains where multiple
instructions following is important: Many Instruction-Following Eval
(ManyIFEval) for text generation with up to ten instructions, and Style-aware
Mostly Basic Programming Problems (StyleMBPP) for code generation with up to
six instructions. Our experiments with the created benchmarks across ten LLMs
reveal that performance consistently degrades as the number of instructions
increases. Furthermore, given the fact that evaluating all the possible
combinations of multiple instructions is computationally impractical in actual
use cases, we developed three types of regression models that can estimate
performance on both unseen instruction combinations and different numbers of
instructions which are not used during training. We demonstrate that a logistic
regression model using instruction count as an explanatory variable can predict
performance of following multiple instructions with approximately 10% error,
even for unseen instruction combinations. We show that relatively modest sample
sizes (500 for ManyIFEval and 300 for StyleMBPP) are sufficient for performance
estimation, enabling efficient evaluation of LLMs under various instruction
combinations.

</details>


### [43] [SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials](https://arxiv.org/abs/2509.21079)
*Qixin Wan,Zilong Wang,Jingwen Zhou,Wanting Wang,Ziheng Geng,Jiachen Liu,Ran Cao,Minghui Cheng,Lu Cheng*

Main category: cs.CL

TL;DR: 本研究介绍了SoM-1K数据集，并提出了一种新的提示策略DoI，用于评估基础模型在工程问题上的表现，结果表明当前模型在这些任务上表现不佳，但DoI有助于提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在理解复杂视觉信息方面的能力有限，因此需要一个专门的基准来评估它们在工程问题上的表现。

Method: 引入了SoM-1K数据集，这是第一个针对评估基础模型在材料强度问题上的大规模多模态基准数据集，并提出了一种称为Descriptions of Images (DoI)的新提示策略。

Result: 当前的基础模型在这些工程问题上表现不佳，最佳模型仅达到56.6%的准确率。有趣的是，当提供DoI时，大型语言模型（LLMs）通常比提供视觉图的视觉语言模型（VLMs）表现更好。

Conclusion: 本研究建立了工程AI的严格基准，并强调了在科学和工程背景下开发更强大多模态推理能力的必要性。

Abstract: Foundation models have shown remarkable capabilities in various domains, but
their performance on complex, multimodal engineering problems remains largely
unexplored. We introduce SoM-1K, the first large-scale multimodal benchmark
dataset dedicated to evaluating foundation models on problems in the strength
of materials (SoM). The dataset, which contains 1,065 annotated SoM problems,
mirrors real-world engineering tasks by including both textual problem
statements and schematic diagrams. Due to the limited capabilities of current
foundation models in understanding complicated visual information, we propose a
novel prompting strategy called Descriptions of Images (DoI), which provides
rigorous expert-generated text descriptions of the visual diagrams as the
context. We evaluate eight representative foundation models, including both
large language models (LLMs) and vision language models (VLMs). Our results
show that current foundation models struggle significantly with these
engineering problems, with the best-performing model achieving only 56.6%
accuracy. Interestingly, we found that LLMs, when provided with DoI, often
outperform VLMs provided with visual diagrams. A detailed error analysis
reveals that DoI plays a crucial role in mitigating visual misinterpretation
errors, suggesting that accurate text-based descriptions can be more effective
than direct image input for current foundation models. This work establishes a
rigorous benchmark for engineering AI and highlights a critical need for
developing more robust multimodal reasoning capabilities in foundation models,
particularly in scientific and engineering contexts.

</details>


### [44] [Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs](https://arxiv.org/abs/2509.21080)
*Yixin Wan,Xingrun Chen,Kai-Wei Chang*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的文化定位偏差，并提出了基于代理的缓解方法，以改善模型在不同文化背景下的公平性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在识别和系统研究大型语言模型中的文化定位偏差，这种偏差可能导致模型生成的内容偏向主流美国文化，而对非主流文化表现出显著的外部性。

Method: 本文提出了CultureLens基准测试，用于量化文化定位偏差，并提出了两种推理时的缓解方法：FIP方法和MFA框架，包括MFA-SA和MFA-MA两种管道。

Result: 实证评估显示，虽然模型在88%以上的美国语境脚本中采用内部者语气，但它们对不太占主导地位的文化采用了主要的外部者立场。代理方法在减轻偏差方面表现出有效性。

Conclusion: 本文提出了一种基于代理的方法，以减轻生成式大型语言模型中的文化定位偏差，实验结果展示了这些方法在缓解偏差方面的有效性。

Abstract: Large language models (LLMs) have unlocked a wide range of downstream
generative applications. However, we found that they also risk perpetuating
subtle fairness issues tied to culture, positioning their generations from the
perspectives of the mainstream US culture while demonstrating salient
externality towards non-mainstream ones. In this work, we identify and
systematically investigate this novel culture positioning bias, in which an
LLM's default generative stance aligns with a mainstream view and treats other
cultures as outsiders. We propose the CultureLens benchmark with 4000
generation prompts and 3 evaluation metrics for quantifying this bias through
the lens of a culturally situated interview script generation task, in which an
LLM is positioned as an onsite reporter interviewing local people across 10
diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a
stark pattern: while models adopt insider tones in over 88 percent of
US-contexted scripts on average, they disproportionately adopt mainly outsider
stances for less dominant cultures. To resolve these biases, we propose 2
inference-time mitigation methods: a baseline prompt-based Fairness
Intervention Pillars (FIP) method, and a structured Mitigation via Fairness
Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)
introduces a self-reflection and rewriting loop based on fairness guidelines.
(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized
agents: a Planner Agent(initial script generation), a Critique Agent (evaluates
initial script against fairness pillars), and a Refinement Agent (incorporates
feedback to produce a polished, unbiased script). Empirical results showcase
the effectiveness of agent-based methods as a promising direction for
mitigating biases in generative LLMs.

</details>


### [45] [PerHalluEval: Persian Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2509.21104)
*Mohammad Hosseini,Kimia Hosseini,Shayan Bali,Zahra Zanjani,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 本文介绍了PerHalluEval，这是第一个针对波斯语的动态幻觉评估基准。通过三阶段LLM驱动的管道和人工验证，评估了LLMs在检测波斯语幻觉文本方面的表现，发现模型普遍表现不佳，但提供外部知识可以部分缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 幻觉是影响所有大型语言模型（LLMs）的持续性问题，尤其是在低资源语言如波斯语中。需要一个专门针对波斯语的动态幻觉评估基准来评估LLMs的表现。

Method: 我们开发了PerHalluEval基准，这是一个针对波斯语的动态幻觉评估基准。该基准利用三阶段LLM驱动的管道，并结合人工验证，生成关于问答和摘要任务的合理答案和摘要，重点是检测外在和内在幻觉。此外，我们使用生成标记的日志概率选择最可信的幻觉实例，并让人工标注者突出波斯语特定上下文以评估LLM在与波斯文化相关内容上的表现。

Result: 评估12个LLM模型（包括开源和闭源模型）使用PerHalluEval，结果显示模型在检测波斯语幻觉文本方面普遍表现不佳。提供外部知识可以部分缓解幻觉问题，且专门针对波斯语训练的模型与其他模型在幻觉方面没有显著差异。

Conclusion: 我们的评估显示，12个LLM模型在检测波斯语幻觉文本方面普遍表现不佳。提供外部知识可以部分缓解幻觉问题，且专门针对波斯语训练的模型与其他模型在幻觉方面没有显著差异。

Abstract: Hallucination is a persistent issue affecting all large language Models
(LLMs), particularly within low-resource languages such as Persian.
PerHalluEval (Persian Hallucination Evaluation) is the first dynamic
hallucination evaluation benchmark tailored for the Persian language. Our
benchmark leverages a three-stage LLM-driven pipeline, augmented with human
validation, to generate plausible answers and summaries regarding QA and
summarization tasks, focusing on detecting extrinsic and intrinsic
hallucinations. Moreover, we used the log probabilities of generated tokens to
select the most believable hallucinated instances. In addition, we engaged
human annotators to highlight Persian-specific contexts in the QA dataset in
order to evaluate LLMs' performance on content specifically related to Persian
culture. Our evaluation of 12 LLMs, including open- and closed-source models
using PerHalluEval, revealed that the models generally struggle in detecting
hallucinated Persian text. We showed that providing external knowledge, i.e.,
the original document for the summarization task, could mitigate hallucination
partially. Furthermore, there was no significant difference in terms of
hallucination when comparing LLMs specifically trained for Persian with others.

</details>


### [46] [BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback](https://arxiv.org/abs/2509.21106)
*Hyunseo Kim,Sangam Lee,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 本文提出 BESPOKE 基准测试，用于评估搜索增强型 LLM 的个性化性能，并通过真实人类数据进行深入分析。


<details>
  <summary>Details</summary>
Motivation: 现有的系统如 ChatGPT 和 Gemini 尝试通过利用用户历史记录进行个性化，但系统性评估仍不足。

Method: 通过收集真实的人类聊天和搜索历史，以及通过配对响应与细粒度偏好评分和反馈来构建基准测试。

Result: BESPOKE 是一个现实且诊断性的基准测试，能够揭示有效个性化在信息寻求任务中的关键要求。

Conclusion: BESPOKE 提供了一个细粒度评估个性化搜索增强型 LLM 的基础，有助于改进信息寻求任务中的个性化体验。

Abstract: Search-augmented large language models (LLMs) have advanced
information-seeking tasks by integrating retrieval into generation, reducing
users' cognitive burden compared to traditional search systems. Yet they remain
insufficient for fully addressing diverse user needs, which requires
recognizing how the same query can reflect different intents across users and
delivering information in preferred forms. While recent systems such as ChatGPT
and Gemini attempt personalization by leveraging user histories, systematic
evaluation of such personalization is under-explored. To address this gap, we
propose BESPOKE, the realistic benchmark for evaluating personalization in
search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting
authentic chat and search histories directly from humans, and diagnostic, by
pairing responses with fine-grained preference scores and feedback. The
benchmark is constructed through long-term, deeply engaged human annotation,
where human annotators contributed their own histories, authored queries with
detailed information needs, and evaluated responses with scores and diagnostic
feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key
requirements for effective personalization in information-seeking tasks,
providing a foundation for fine-grained evaluation of personalized
search-augmented LLMs. Our code and data are available at
https://augustinlib.github.io/BESPOKE/.

</details>


### [47] [VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of Spoken Language Model](https://arxiv.org/abs/2509.21108)
*Junhyuk Choi,Ro-hoon Oh,Jihwan Seol,Bugeun Kim*

Main category: cs.CL

TL;DR: VoiceBBQ是一个用于测量语音语言模型中社会偏见的数据集，能够同时检测内容和声学偏差。


<details>
  <summary>Details</summary>
Motivation: 由于语音的性质，语音语言模型中的社会偏见可能来自两个不同的来源：内容方面和声学方面。因此需要一个能够测量这些偏差的数据集。

Method: 将每个BBQ上下文转换为受控语音条件，以生成每轴准确率、偏差和一致性分数，并与原始文本基准保持可比性。

Result: 使用VoiceBBQ评估了两个SLMs - LLaMA-Omni和Qwen2-Audio，观察到架构上的差异：LLaMA-Omni抵抗声学偏差但放大性别和口音偏差，而Qwen2-Audio显著减弱这些提示同时保持内容保真度。

Conclusion: VoiceBBQ提供了一个紧凑的测试平台，可以联合诊断语音语言模型中的内容和声学偏差。

Abstract: We introduce VoiceBBQ, a spoken extension of the BBQ (Bias Benchmark for
Question Answering) - a dataset that measures social bias by presenting
ambiguous or disambiguated contexts followed by questions that may elicit
stereotypical responses. Due to the nature of speech, social bias in Spoken
Language Models (SLMs) can emerge from two distinct sources: 1) content aspect
and 2) acoustic aspect. The dataset converts every BBQ context into controlled
voice conditions, enabling per-axis accuracy, bias, and consistency scores that
remain comparable to the original text benchmark. Using VoiceBBQ, we evaluate
two SLMs - LLaMA-Omni and Qwen2-Audio - and observe architectural contrasts:
LLaMA-Omni resists acoustic bias while amplifying gender and accent bias,
whereas Qwen2-Audio substantially dampens these cues while preserving content
fidelity. VoiceBBQ thus provides a compact, drop-in testbed for jointly
diagnosing content and acoustic bias across spoken language models.

</details>


### [48] [Acoustic-based Gender Differentiation in Speech-aware Language Models](https://arxiv.org/abs/2509.21125)
*Junhyuk Choi,Jihwan Seol,Nayeon Kim,Chanhee Cho,EunBin Cho,Bugeun Kim*

Main category: cs.CL

TL;DR: 本文研究了SpeechLMs在语音交互中可能存在的性别差异问题，发现模型在性别刻板印象问题上表现出男性导向的回应，而在性别依赖问题上却与性别无关，这表明当前SpeechLMs可能未能有效消除性别偏见。


<details>
  <summary>Details</summary>
Motivation: 研究SpeechLMs在语音交互中可能存在的性别差异问题，并探索其背后的原因。

Method: 本文提出了一种新的数据集，用于系统分析这种现象，并评估了LLaMA-Omni系列模型，发现了矛盾模式。此外，通过比较SpeechLMs和对应的骨干LLMs，确认了这些矛盾模式主要源自Whisper语音编码器生成的男性导向声学标记。

Result: 在性别刻板印象的问题中，所有模型都表现出男性导向的回应；而在性别依赖的问题中，模型的回应却与性别无关。此外，即使允许中性回应，模型在性别依赖问题上仍倾向于中性回应，但矛盾模式依然存在。

Conclusion: 当前的SpeechLMs可能未能成功消除性别偏见，尽管它们优先考虑了普遍的公平原则而非情境适当性，这凸显了需要更复杂的技巧来正确利用语音技术中的性别信息。

Abstract: Speech-aware Language Models (SpeechLMs) have fundamentally transformed
human-AI interaction by enabling voice-based communication, yet they may
exhibit acoustic-based gender differentiation where identical questions lead to
different responses based on the speaker's gender. This paper propose a new
dataset that enables systematic analysis of this phenomenon, containing 9,208
speech samples across three categories: Gender-Independent,
Gender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni
series and discovered a paradoxical pattern; while overall responses seems
identical regardless of gender, the pattern is far from unbiased responses.
Specifically, in Gender-Stereotypical questions, all models consistently
exhibited male-oriented responses; meanwhile, in Gender-Dependent questions
where gender differentiation would be contextually appropriate, models
exhibited responses independent to gender instead. We also confirm that this
pattern does not result from neutral options nor perceived gender of a voice.
When we allow neutral response, models tends to respond neutrally also in
Gender-Dependent questions. The paradoxical pattern yet retains when we applied
gender neutralization methods on speech. Through comparison between SpeechLMs
with corresponding backbone LLMs, we confirmed that these paradoxical patterns
primarily stem from Whisper speech encoders, which generates male-oriented
acoustic tokens. These findings reveal that current SpeechLMs may not
successfully remove gender biases though they prioritized general fairness
principles over contextual appropriateness, highlighting the need for more
sophisticated techniques to utilize gender information properly in speech
technology.

</details>


### [49] [AutoIntent: AutoML for Text Classification](https://arxiv.org/abs/2509.21138)
*Ilya Alekseev,Roman Solomatin,Darina Rustamova,Denis Kuznetsov*

Main category: cs.CL

TL;DR: AutoIntent is an automated machine learning tool for text classification that provides end-to-end automation and superior performance compared to existing solutions.


<details>
  <summary>Details</summary>
Motivation: To provide an automated machine learning tool for text classification tasks that supports multi-label classification and out-of-scope detection.

Method: AutoIntent offers end-to-end automation with embedding model selection, classifier optimization, and decision threshold tuning within a modular, sklearn-like interface.

Result: AutoIntent shows better performance than existing AutoML tools on standard intent classification datasets.

Conclusion: AutoIntent demonstrates superior performance compared to existing AutoML tools and enables users to balance effectiveness and resource consumption.

Abstract: AutoIntent is an automated machine learning tool for text classification
tasks. Unlike existing solutions, AutoIntent offers end-to-end automation with
embedding model selection, classifier optimization, and decision threshold
tuning, all within a modular, sklearn-like interface. The framework is designed
to support multi-label classification and out-of-scope detection. AutoIntent
demonstrates superior performance compared to existing AutoML tools on standard
intent classification datasets and enables users to balance effectiveness and
resource consumption.

</details>


### [50] [Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction](https://arxiv.org/abs/2509.21151)
*Lei Hei,Tingjing Liao,Yingxin Pei,Yiyang Qi,Jiaqi Wang,Ruiting Li,Feiliang Ren*

Main category: cs.CL

TL;DR: 本文提出了一种新的多模态关系抽取框架ROC，通过检索任务和语义相似性对比学习来提高关系抽取的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法仍然采用基于分类的范式，忽略了结构约束如实体类型和位置线索，并且在细粒度关系理解方面缺乏语义表达能力。

Method: 我们提出了ROC框架，将多模态RE重新定义为由关系语义驱动的检索任务。ROC通过多模态编码器整合实体类型和位置信息，使用大型语言模型将关系标签扩展为自然语言描述，并通过基于语义相似性的对比学习对实体-关系对进行对齐。

Result: 实验表明，我们的方法在基准数据集MNRE和MORE上实现了最先进的性能，并表现出更强的鲁棒性和可解释性。

Conclusion: 我们的方法在基准数据集MNRE和MORE上实现了最先进的性能，并表现出更强的鲁棒性和可解释性。

Abstract: Relation extraction (RE) aims to identify semantic relations between entities
in unstructured text. Although recent work extends traditional RE to multimodal
scenarios, most approaches still adopt classification-based paradigms with
fused multimodal features, representing relations as discrete labels. This
paradigm has two significant limitations: (1) it overlooks structural
constraints like entity types and positional cues, and (2) it lacks semantic
expressiveness for fine-grained relation understanding. We propose
\underline{R}etrieval \underline{O}ver \underline{C}lassification (ROC), a
novel framework that reformulates multimodal RE as a retrieval task driven by
relation semantics. ROC integrates entity type and positional information
through a multimodal encoder, expands relation labels into natural language
descriptions using a large language model, and aligns entity-relation pairs via
semantic similarity-based contrastive learning. Experiments show that our
method achieves state-of-the-art performance on the benchmark datasets MNRE and
MORE and exhibits stronger robustness and interpretability.

</details>


### [51] [Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models](https://arxiv.org/abs/2509.21155)
*Chantal Shaib,Vinith M. Suriyakumar,Levent Sagun,Byron C. Wallace,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 本研究探讨了语法-领域相关性对LLM性能的影响，并提出了检测和预防这种虚假相关性的方法。


<details>
  <summary>Details</summary>
Motivation: 了解语法和领域之间的关系对于确保LLM正确响应指令至关重要。然而，语法可能隐含信息，这可能导致模型学习到语法与领域之间的虚假相关性，从而影响性能。

Method: 我们通过合成训练数据集来研究语法-领域相关性，并引入了一个评估框架来检测这种现象。

Result: 我们在OLMo-2模型上发现，语法-领域相关性会降低实体知识任务的性能（平均0.51 +/- 0.06）。此外，我们发现这种现象出现在FlanV2数据集的一部分中，并且在安全微调中可能被用来绕过拒绝。

Conclusion: 我们的研究强调了两个需求：(1) 明确测试语法-领域相关性，以及 (2) 确保训练数据中的语法多样性，特别是在领域内，以防止这种虚假相关性。

Abstract: For an LLM to correctly respond to an instruction it must understand both the
semantics and the domain (i.e., subject area) of a given task-instruction pair.
However, syntax can also convey implicit information Recent work shows that
syntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are
prevalent in training data and often appear in model outputs. In this work we
characterize syntactic templates, domain, and semantics in task-instruction
pairs. We identify cases of spurious correlations between syntax and domain,
where models learn to associate a domain with syntax during training; this can
sometimes override prompt semantics. Using a synthetic training dataset, we
find that the syntactic-domain correlation can lower performance (mean 0.51 +/-
0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an
evaluation framework to detect this phenomenon in trained models, and show that
it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;
Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study
on the implications for safety finetuning, showing that unintended
syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B
Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test
for syntactic-domain correlations, and (2) to ensure syntactic diversity in
training data, specifically within domains, to prevent such spurious
correlations.

</details>


### [52] [Who's Laughing Now? An Overview of Computational Humour Generation and Explanation](https://arxiv.org/abs/2509.21175)
*Tyler Loakman,William Thorne,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文综述了计算幽默的研究现状，指出其作为NLP子学科的重要性，并提出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 幽默是自然语言处理中最具挑战性的任务之一，因为它需要大量的推理能力，而现代大型语言模型在此方面仍有不足。因此，计算幽默处理作为一个子学科具有重要意义。

Method: 本文对计算幽默的现状进行了综述，重点讨论了生成和解释幽默的任务。

Result: 尽管幽默理解具有基础NLP任务的所有特征，但除了双关语之外，生成和解释幽默的工作仍然很少，而且最先进的模型仍无法达到人类水平。

Conclusion: 计算幽默处理作为NLP的一个子学科的重要性需要被进一步研究，并且未来的研究方向应考虑到幽默的主观性和伦理模糊性。

Abstract: The creation and perception of humour is a fundamental human trait,
positioning its computational understanding as one of the most challenging
tasks in natural language processing (NLP). As an abstract, creative, and
frequently context-dependent construct, humour requires extensive reasoning to
understand and create, making it a pertinent task for assessing the
common-sense knowledge and reasoning abilities of modern large language models
(LLMs). In this work, we survey the landscape of computational humour as it
pertains to the generative tasks of creation and explanation. We observe that,
despite the task of understanding humour bearing all the hallmarks of a
foundational NLP task, work on generating and explaining humour beyond puns
remains sparse, while state-of-the-art models continue to fall short of human
capabilities. We bookend our literature survey by motivating the importance of
computational humour processing as a subdiscipline of NLP and presenting an
extensive discussion of future directions for research in the area that takes
into account the subjective and ethically ambiguous nature of humour.

</details>


### [53] [GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models](https://arxiv.org/abs/2509.21192)
*Jieli Zhu,Vi Ngoc-Nha Tran*

Main category: cs.CL

TL;DR: 本文研究了基于小型语言模型的聊天机器人在下游任务中的人脸信息泄露问题，并提出了一种新的贪婪坐标梯度方法（GEP），实验结果表明该方法在泄露检测方面比之前的方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 由于小型语言模型在某些领域表现出与大型语言模型相当的性能，但其在下游任务中的人脸信息泄露问题尚未被充分研究，因此本文旨在探索这一问题并提出有效的解决方案。

Method: 本文首先微调了一个新的聊天机器人ChatBioGPT，基于BioGPT的架构使用医疗数据集Alpaca和HealthCareMagic进行训练。然后提出了GEP方法，这是一种专门用于人脸信息提取的贪婪坐标梯度方法。

Result: 实验结果显示，GEP方法在人脸信息泄露检测方面比之前的方法提高了多达60倍。此外，在更复杂和现实的情况下，GEP仍然能够揭示高达4.53%的人脸信息泄露率。

Conclusion: 本文研究了基于小型语言模型的聊天机器人在下游任务中的人脸信息泄露问题，并提出了一种新的贪婪坐标梯度方法（GEP）来有效提取人脸信息。实验结果表明，GEP相比之前的方法在泄露检测方面有显著提升。

Abstract: Small language models (SLMs) become unprecedentedly appealing due to their
approximately equivalent performance compared to large language models (LLMs)
in certain fields with less energy and time consumption during training and
inference. However, the personally identifiable information (PII) leakage of
SLMs for downstream tasks has yet to be explored. In this study, we investigate
the PII leakage of the chatbot based on SLM. We first finetune a new chatbot,
i.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca
and HealthCareMagic. It shows a matchable performance in BERTscore compared
with previous studies of ChatDoctor and ChatGPT. Based on this model, we prove
that the previous template-based PII attacking methods cannot effectively
extract the PII in the dataset for leakage detection under the SLM condition.
We then propose GEP, which is a greedy coordinate gradient-based (GCG) method
specifically designed for PII extraction. We conduct experimental studies of
GEP and the results show an increment of up to 60$\times$ more leakage compared
with the previous template-based methods. We further expand the capability of
GEP in the case of a more complicated and realistic situation by conducting
free-style insertion where the inserted PII in the dataset is in the form of
various syntactic expressions instead of fixed templates, and GEP is still able
to reveal a PII leakage rate of up to 4.53%.

</details>


### [54] [Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning](https://arxiv.org/abs/2509.21193)
*Xiangru Tang,Wanghan Xu,Yujie Wang,Zijie Guo,Daniel Shao,Jiapeng Chen,Cixuan Zhang,Ziyi Wang,Lixin Zhang,Guancheng Wan,Wenlong Zhang,Lei Bai,Zhenfei Yin,Philip Torr,Hanrui Wang,Di Jin*

Main category: cs.CL

TL;DR: 本文提出了一种统一框架，结合隐式检索和结构化协作，以解决大型语言模型在科学推理中的两个主要瓶颈。该框架在多个数据集上表现出色，提高了准确率并减少了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在科学推理方面取得了显著进展，但仍有两个主要瓶颈：显式检索片段推理，造成额外的token和步骤的隐藏“工具税”；多代理管道通常通过平均所有候选来稀释强解。

Method: 本文提出了一种统一的框架，结合了隐式检索和结构化协作。基础是一个基于Monitor的检索模块，在token级别操作，将外部知识与推理最小干扰地整合。在此基础上，Hierarchical Solution Refinement (HSR) 迭代地将每个候选作为锚点，由其同伴修复，而Quality-Aware Iterative Reasoning (QAIR) 适应性地调整精炼以考虑解决方案质量。

Result: 在Humanity's Last Exam (HLE) Bio/Chem Gold上，我们的框架达到了48.3%的准确率——目前报道的最高值，比最强的代理基线高出13.4分，并领先前沿LLM高达18.1分，同时将token使用量减少了53.5%，代理步骤减少了43.7%。在SuperGPQA和TRQA上的结果证实了跨领域的鲁棒性。

Conclusion: 本文提出的框架通过隐式增强和结构化精炼克服了显式工具使用和均匀聚合的低效问题。

Abstract: Large language models (LLMs) have recently shown strong progress on
scientific reasoning, yet two major bottlenecks remain. First, explicit
retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and
steps. Second, multi-agent pipelines often dilute strong solutions by averaging
across all candidates. We address these challenges with a unified framework
that combines implicit retrieval and structured collaboration. At its
foundation, a Monitor-based retrieval module operates at the token level,
integrating external knowledge with minimal disruption to reasoning. On top of
this substrate, Hierarchical Solution Refinement (HSR) iteratively designates
each candidate as an anchor to be repaired by its peers, while Quality-Aware
Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's
Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the
highest reported to date, surpassing the strongest agent baseline by 13.4
points and leading frontier LLMs by up to 18.1 points, while simultaneously
reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA
and TRQA confirm robustness across domains. Error analysis shows that reasoning
failures and knowledge gaps co-occur in over 85\% of cases, while diversity
analysis reveals a clear dichotomy: retrieval tasks benefit from solution
variety, whereas reasoning tasks favor consensus. Together, these findings
demonstrate how implicit augmentation and structured refinement overcome the
inefficiencies of explicit tool use and uniform aggregation. Code is available
at: https://github.com/tangxiangru/Eigen-1.

</details>


### [55] [CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis](https://arxiv.org/abs/2509.21208)
*Xinzhe Xu,Liang Zhao,Hongshen Xu,Chen Chen*

Main category: cs.CL

TL;DR: 本文提出了一个名为CLaw的基准测试，用于评估大型语言模型在中文法律知识和推理应用方面的表现，并指出当前模型在准确检索和引用法律条款方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在分析法律文本和引用相关法规时可靠性不足，因为它们的通用预训练没有专注于法律文本，这掩盖了它们真正的法律知识深度。

Method: 本文提出了CLaw，这是一个专门用于评估中文法律知识和推理应用的基准测试。它由两个关键部分组成：一个全面的、细粒度的全国306部法律条文语料库，以及一套基于中国最高法院材料的254个案例推理实例。

Result: 实证评估显示，大多数现代大型语言模型在忠实再现法律条款方面存在显著困难。

Conclusion: 本文认为，实现可信的法律推理需要准确的知识检索和强大的通用推理能力之间的协同作用。

Abstract: Large Language Models (LLMs) are increasingly tasked with analyzing legal
texts and citing relevant statutes, yet their reliability is often compromised
by general pre-training that ingests legal texts without specialized focus,
obscuring the true depth of their legal knowledge. This paper introduces CLaw,
a novel benchmark specifically engineered to meticulously evaluate LLMs on
Chinese legal knowledge and its application in reasoning. CLaw comprises two
key components: (1) a comprehensive, fine-grained corpus of all 306 Chinese
national statutes, segmented to the subparagraph level and incorporating
precise historical revision timesteps for rigorous recall evaluation (64,849
entries), and (2) a challenging set of 254 case-based reasoning instances
derived from China Supreme Court curated materials to assess the practical
application of legal knowledge. Our empirical evaluation reveals that most
contemporary LLMs significantly struggle to faithfully reproduce legal
provisions. As accurate retrieval and citation of legal provisions form the
basis of legal reasoning, this deficiency critically undermines the reliability
of their responses. We contend that achieving trustworthy legal reasoning in
LLMs requires a robust synergy of accurate knowledge retrieval--potentially
enhanced through supervised fine-tuning (SFT) or retrieval-augmented generation
(RAG)--and strong general reasoning capabilities. This work provides an
essential benchmark and critical insights for advancing domain-specific LLM
reasoning, particularly within the complex legal sphere.

</details>


### [56] [SGMem: Sentence Graph Memory for Long-Term Conversational Agents](https://arxiv.org/abs/2509.21212)
*Yaxiong Wu,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: SGMem is a method for managing long-term conversations by representing dialogue as sentence-level graphs, which improves accuracy in question answering tasks.


<details>
  <summary>Details</summary>
Motivation: Long-term conversational agents require effective memory management to handle dialogue histories that exceed the context window of large language models (LLMs). Existing methods based on fact extraction or summarization reduce redundancy but struggle to organize and retrieve relevant information across different granularities of dialogue and generated memory.

Method: SGMem (Sentence Graph Memory) represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts.

Result: Experiments on LongMemEval and LoCoMo show that SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.

Conclusion: SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.

Abstract: Long-term conversational agents require effective memory management to handle
dialogue histories that exceed the context window of large language models
(LLMs). Existing methods based on fact extraction or summarization reduce
redundancy but struggle to organize and retrieve relevant information across
different granularities of dialogue and generated memory. We introduce SGMem
(Sentence Graph Memory), which represents dialogue as sentence-level graphs
within chunked units, capturing associations across turn-, round-, and
session-level contexts. By combining retrieved raw dialogue with generated
memory such as summaries, facts and insights, SGMem supplies LLMs with coherent
and relevant context for response generation. Experiments on LongMemEval and
LoCoMo show that SGMem consistently improves accuracy and outperforms strong
baselines in long-term conversational question answering.

</details>


### [57] [Query-Centric Graph Retrieval Augmented Generation](https://arxiv.org/abs/2509.21237)
*Yaxiong Wu,Jianyuan Bo,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: QCG-RAG是一种以查询为中心的图RAG框架，通过构建可控粒度的查询中心图，提高图质量和可解释性，并通过生成的查询选择相关块，在问答任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的方法面临粒度困境：细粒度实体级图导致高token成本并丢失上下文，而粗粒度文档级图无法捕捉细微关系。

Method: 引入了以查询为中心的图RAG框架(QCG-RAG)，利用Doc2Query和Doc2Query--构建可控粒度的查询中心图，改善图质量和可解释性，并通过生成的查询选择相关块。

Result: QCG-RAG在LiHuaWorld和MultiHop-RAG数据集上的实验表明，其在问答准确率上 consistently 超过之前的基于块和基于图的RAG方法。

Conclusion: QCG-RAG在问答准确率上 consistently 超过之前的基于块和基于图的RAG方法，确立了多跳推理的新范式。

Abstract: Graph-based retrieval-augmented generation (RAG) enriches large language
models (LLMs) with external knowledge for long-context understanding and
multi-hop reasoning, but existing methods face a granularity dilemma:
fine-grained entity-level graphs incur high token costs and lose context, while
coarse document-level graphs fail to capture nuanced relations. We introduce
QCG-RAG, a query-centric graph RAG framework that enables query-granular
indexing and multi-hop chunk retrieval. Our query-centric approach leverages
Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with
controllable granularity, improving graph quality and interpretability. A
tailored multi-hop retrieval mechanism then selects relevant chunks via the
generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG
consistently outperforms prior chunk-based and graph-based RAG methods in
question answering accuracy, establishing a new paradigm for multi-hop
reasoning.

</details>


### [58] [Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication](https://arxiv.org/abs/2509.21262)
*Evgeny Kaskov,Elizaveta Petrova,Petr Surovtsev,Anna Kostikova,Ilya Mistiurin,Alexander Kapitanov,Alexander Nagaev*

Main category: cs.CL

TL;DR: 本文研究了扩散模型中的同音词重复问题，并提出了一种测量重复率的方法，通过提示扩展来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 同音词在提示中出现时，扩散模型可能会同时生成单词的多个含义，这被称为同音词重复。这个问题因英语中心偏见而变得更加复杂，这包括在文本到图像模型管道之前的一个额外翻译步骤。因此，即使原始语言中不是同音词的单词，在翻译成英语后也可能变成同音词并失去其意义。

Method: 本文引入了一种测量重复率的方法，并使用基于视觉-语言模型的自动评估和人类评估对不同的扩散模型进行了评估。同时，我们研究了通过提示扩展来缓解同音词重复问题的方法。

Result: 本文通过自动评估和人类评估对不同的扩散模型进行了评估，并展示了通过提示扩展可以有效减少同音词重复问题，包括与英语中心偏见相关的重复。

Conclusion: 本文提出了一种测量重复率的方法，并通过自动评估和人类评估对不同的扩散模型进行了评估。此外，我们研究了通过提示扩展来缓解同音词重复问题的方法，证明这种方法也有效减少了与英语中心偏见相关的重复。

Abstract: Homonyms are words with identical spelling but distinct meanings, which pose
challenges for many generative models. When a homonym appears in a prompt,
diffusion models may generate multiple senses of the word simultaneously, which
is known as homonym duplication. This issue is further complicated by an
Anglocentric bias, which includes an additional translation step before the
text-to-image model pipeline. As a result, even words that are not homonymous
in the original language may become homonyms and lose their meaning after
translation into English. In this paper, we introduce a method for measuring
duplication rates and conduct evaluations of different diffusion models using
both automatic evaluation utilizing Vision-Language Models (VLM) and human
evaluation. Additionally, we investigate methods to mitigate the homonym
duplication problem through prompt expansion, demonstrating that this approach
also effectively reduces duplication related to Anglocentric bias. The code for
the automatic evaluation pipeline is publicly available.

</details>


### [59] [LLM Output Homogenization is Task Dependent](https://arxiv.org/abs/2509.21267)
*Shomik Jain,Jack Lanchantin,Maximilian Nickel,Karen Ullrich,Ashia Wilson,Jamelle Watson-Daniels*

Main category: cs.CL

TL;DR: 本文提出了任务依赖性的方法来评估和缓解输出同质化问题，并展示了如何在不牺牲质量的情况下增加功能多样性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究在概念化多样性时未能考虑任务依赖性，因此需要一种更任务依赖的方法来评估和缓解输出同质化问题。

Method: 本文提出了一个包含八个任务类别的任务分类法，引入了任务锚定的功能多样性，并提出了一种任务锚定的采样技术。

Result: 本文通过任务锚定的功能多样性评估和任务锚定的采样技术，证明了可以在不牺牲响应质量的情况下增加功能多样性。

Conclusion: 本文展示了任务依赖性如何改善输出同质化的评估和缓解。

Abstract: A large language model can be less helpful if it exhibits output response
homogenization. But whether two responses are considered homogeneous, and
whether such homogenization is problematic, both depend on the task category.
For instance, in objective math tasks, we often expect no variation in the
final answer but anticipate variation in the problem-solving strategy. Whereas,
for creative writing tasks, we may expect variation in key narrative components
(e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity
produced by temperature-sampling. Previous work addressing output
homogenization often fails to conceptualize diversity in a task-dependent way.
We address this gap in the literature directly by making the following
contributions. (1) We present a task taxonomy comprised of eight task
categories that each have distinct conceptualizations of output homogenization.
(2) We introduce task-anchored functional diversity to better evaluate output
homogenization. (3) We propose a task-anchored sampling technique that
increases functional diversity for task categories where homogenization is
undesired, while preserving homogenization where it is desired. (4) We
challenge the perceived existence of a diversity-quality trade-off by
increasing functional diversity while maintaining response quality. Overall, we
demonstrate how task dependence improves the evaluation and mitigation of
output homogenization.

</details>


### [60] [LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text](https://arxiv.org/abs/2509.21269)
*Irina Tolstykh,Aleksandra Tsybina,Sergey Yakubson,Maksim Kuprashevich*

Main category: cs.CL

TL;DR: LLMTrace is a new large-scale, bilingual corpus for AI-generated text detection that addresses the limitations of existing datasets by providing character-level annotations for precise localization of AI-generated segments within a text.


<details>
  <summary>Details</summary>
Motivation: The development of robust detection systems is limited by a critical lack of suitable training data; existing datasets are often generated with outdated models, are predominantly in English, and fail to address the increasingly common scenario of mixed human-AI authorship.

Method: LLMTrace is constructed using a diverse range of modern proprietary and open-source LLMs, and it provides character-level annotations for precise localization of AI-generated segments within a text.

Result: LLMTrace is a new large-scale, bilingual (English and Russian) corpus for AI-generated text detection that supports two key tasks: traditional full-text binary classification (human vs. AI) and the novel task of AI-generated interval detection.

Conclusion: LLMTrace will serve as a vital resource for training and evaluating the next generation of more nuanced and practical AI detection models.

Abstract: The widespread use of human-like text from Large Language Models (LLMs)
necessitates the development of robust detection systems. However, progress is
limited by a critical lack of suitable training data; existing datasets are
often generated with outdated models, are predominantly in English, and fail to
address the increasingly common scenario of mixed human-AI authorship.
Crucially, while some datasets address mixed authorship, none provide the
character-level annotations required for the precise localization of
AI-generated segments within a text. To address these gaps, we introduce
LLMTrace, a new large-scale, bilingual (English and Russian) corpus for
AI-generated text detection. Constructed using a diverse range of modern
proprietary and open-source LLMs, our dataset is designed to support two key
tasks: traditional full-text binary classification (human vs. AI) and the novel
task of AI-generated interval detection, facilitated by character-level
annotations. We believe LLMTrace will serve as a vital resource for training
and evaluating the next generation of more nuanced and practical AI detection
models. The project page is available at
\href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}.

</details>


### [61] [Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond](https://arxiv.org/abs/2509.21284)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文通过理论分析和实验验证，揭示了输入扰动对Chain-of-Thought (CoT)输出的影响，并得出了相关的结论。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示，Chain-of-Thought (CoT)的输出受到输入扰动的显著影响，但缺乏对这种影响的理论解释，这限制了我们对输入扰动如何在推理过程中传播的理解，并阻碍了提示优化方法的进一步改进。

Method: 本文首先推导了在输出波动在可接受范围内的条件下，输入扰动的上界，然后将其应用于Linear Self-Attention (LSA)模型，并进行了实验验证。

Result: 实验结果与理论分析一致，证实了输入扰动对CoT输出的影响，并证明了输入扰动的上界与推理步骤数正相关，以及与输入嵌入和隐藏状态向量的范数负相关。

Conclusion: 本文通过理论分析和实验验证，证明了输入扰动对Chain-of-Thought (CoT)输出的影响，并得出了一些重要的结论。

Abstract: Existing research indicates that the output of Chain-of-Thought (CoT) is
significantly affected by input perturbations. Although many methods aim to
mitigate such impact by optimizing prompts, a theoretical explanation of how
these perturbations influence CoT outputs remains an open area of research.
This gap limits our in-depth understanding of how input perturbations propagate
during the reasoning process and hinders further improvements in prompt
optimization methods. Therefore, in this paper, we theoretically analyze the
effect of input perturbations on the fluctuation of CoT outputs. We first
derive an upper bound for input perturbations under the condition that the
output fluctuation is within an acceptable range, based on which we prove that:
(i) This upper bound is positively correlated with the number of reasoning
steps in the CoT; (ii) Even an infinitely long reasoning process cannot
eliminate the impact of input perturbations. We then apply these conclusions to
the Linear Self-Attention (LSA) model, which can be viewed as a simplified
version of the Transformer. For the LSA model, we prove that the upper bound
for input perturbation is negatively correlated with the norms of the input
embedding and hidden state vectors. To validate this theoretical analysis, we
conduct experiments on three mainstream datasets and four mainstream models.
The experimental results align with our theoretical analysis, empirically
demonstrating the correctness of our findings.

</details>


### [62] [DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding](https://arxiv.org/abs/2509.21287)
*Kin Ian Lo,Hala Hawashin,Mina Abbaszadeh,Tilen Limback-Stokin,Hadi Wazni,Mehrnoosh Sadrzadeh*

Main category: cs.CL

TL;DR: DisCoCLIP是一种结合了冻结CLIP视觉变换器和新型张量网络文本编码器的多模态编码器，通过显式编码语言语法结构提升了视觉-语言任务中的组合推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在大规模图像-文本对齐方面表现优异，但常常忽视语言的组合结构，导致在依赖词序和谓词-论元结构的任务中失败。

Method: DisCoCLIP结合了一个冻结的CLIP视觉变换器和一个新颖的张量网络文本编码器，该编码器显式编码语法结构。句子通过组合范畴语法解析器进行解析，生成分布词张量，其收缩反映了句子的语法推导。高阶张量通过张量分解进行因子化，以减少参数数量。

Result: DisCoCLIP在SVO-Probes verb准确率上将CLIP的77.6%提升至82.4%，ARO归因和关系得分提高了超过9%和4%，并在一个新的SVO-Swap基准测试中达到了93.7%。

Conclusion: 通过将显式语言结构嵌入张量网络，DisCoCLIP展示了在视觉-语言任务中显著提升组合推理能力的可解释且参数高效的表示。

Abstract: Recent vision-language models excel at large-scale image-text alignment but
often neglect the compositional structure of language, leading to failures on
tasks that hinge on word order and predicate-argument structure. We introduce
DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer
with a novel tensor network text encoder that explicitly encodes syntactic
structure. Sentences are parsed with a Combinatory Categorial Grammar parser to
yield distributional word tensors whose contractions mirror the sentence's
grammatical derivation. To keep the model efficient, high-order tensors are
factorized with tensor decompositions, reducing parameter count from tens of
millions to under one million. Trained end-to-end with a self-supervised
contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and
word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%,
boosts ARO attribution and relation scores by over 9% and 4%, and achieves
93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that
embedding explicit linguistic structure via tensor networks yields
interpretable, parameter-efficient representations that substantially improve
compositional reasoning in vision-language tasks.

</details>


### [63] [The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages](https://arxiv.org/abs/2509.21294)
*Pranjal A. Chitale,Varun Gumma,Sanchit Ahuja,Prashant Kodali,Manan Uppadhyay,Deepthi Sudharsan,Sunayana Sitaram*

Main category: cs.CL

TL;DR: 研究探讨了通过自下而上的方法生成合成的文化情境数据集对多语言AI的影响，结果表明这种策略在低资源和中等资源语言中效果尤为显著。


<details>
  <summary>Details</summary>
Motivation: 开发能够在多种语言中有效运作并保持文化根基的AI系统是一个长期挑战，特别是在资源匮乏的环境中。合成数据提供了一个有希望的途径，但其在多语言和多文化背景下的有效性仍需探索。

Method: 通过自下而上的生成策略，提示大型开源LLM（>= 235B参数）在特定语言的维基百科内容中进行数据生成，以创建合成的文化情境数据集。

Result: Updesh数据集包含9.5M数据点，覆盖13种印度语言，涵盖各种推理和生成任务，强调长上下文、多轮能力以及与印度文化背景的一致性。模型在Updesh上微调后，在生成任务中表现显著提升，并在多项选择风格的NLU任务中保持竞争力。

Conclusion: 这些发现提供了实证证据，表明有效的多语言AI需要多方面的数据整理和生成策略，包括上下文感知、文化基础的方法。

Abstract: Developing AI systems that operate effectively across languages while
remaining culturally grounded is a long-standing challenge, particularly in
low-resource settings. Synthetic data provides a promising avenue, yet its
effectiveness in multilingual and multicultural contexts remains underexplored.
We investigate the creation and impact of synthetic, culturally contextualized
datasets for Indian languages through a bottom-up generation strategy that
prompts large open-source LLMs (>= 235B parameters) to ground data generation
in language-specific Wikipedia content. This approach complements the dominant
top-down paradigm of translating synthetic datasets from high-resource
languages such as English. We introduce Updesh, a high-quality large-scale
synthetic instruction-following dataset comprising 9.5M data points across 13
Indian languages, encompassing diverse reasoning and generative tasks with an
emphasis on long-context, multi-turn capabilities, and alignment with Indian
cultural contexts. A comprehensive evaluation incorporating both automated
metrics and human annotation across 10k assessments indicates that generated
data is high quality; though, human evaluation highlights areas for further
improvement. Additionally, we perform downstream evaluations by fine-tuning
models on our dataset and assessing the performance across 15 diverse
multilingual datasets. Models trained on Updesh consistently achieve
significant gains on generative tasks and remain competitive on multiple-choice
style NLU tasks. Notably, relative improvements are most pronounced in low and
medium-resource languages, narrowing their gap with high-resource languages.
These findings provide empirical evidence that effective multilingual AI
requires multi-faceted data curation and generation strategies that incorporate
context-aware, culturally grounded methodologies.

</details>


### [64] [Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs](https://arxiv.org/abs/2509.21305)
*Daniel Vennemeyer,Phan Anh Duong,Tiffany Zhan,Tianyu Jiang*

Main category: cs.CL

TL;DR: 研究显示，大型语言模型中的奉承行为（如过度同意或赞美用户）是由于不同的表示机制，且这些行为可以独立控制。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型中的奉承行为是否由单一机制或多种不同过程引起。

Method: 通过差异均值方向、激活添加和子空间几何学，对多个模型和数据集进行分析。

Result: 三种行为在潜在空间中沿着不同的线性方向编码，每种行为可以独立放大或抑制而不影响其他行为，其表征结构在不同模型家族和规模中保持一致。

Conclusion: 这些结果表明，奉承行为对应于不同的、可以独立控制的表示。

Abstract: Large language models (LLMs) often exhibit sycophantic behaviors -- such as
excessive agreement with or flattery of the user -- but it is unclear whether
these behaviors arise from a single mechanism or multiple distinct processes.
We decompose sycophancy into sycophantic agreement and sycophantic praise,
contrasting both with genuine agreement. Using difference-in-means directions,
activation additions, and subspace geometry across multiple models and
datasets, we show that: (1) the three behaviors are encoded along distinct
linear directions in latent space; (2) each behavior can be independently
amplified or suppressed without affecting the others; and (3) their
representational structure is consistent across model families and scales.
These results suggest that sycophantic behaviors correspond to distinct,
independently steerable representations.

</details>


### [65] [RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](https://arxiv.org/abs/2509.21319)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Ellie Evans,Daniel Egert,Hoo-Chang Shin,Felipe Soares,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: 本文提出RLBFF，结合人类反馈和规则验证，提升奖励模型性能，并提供开源配方以降低推理成本。


<details>
  <summary>Details</summary>
Motivation: RLHF存在可解释性和奖励黑客问题，而RLVR仅关注正确性验证，因此需要一种新的方法来结合两者的优点。

Method: RLBFF结合了人类驱动的偏好和基于规则的验证，通过从自然语言反馈中提取二元原则来训练奖励模型，将其作为蕴含任务进行训练。

Result: RLBFF在RM-Bench上达到86.2%，在JudgeBench上达到81.4%（截至2025年9月24日排名第一）。同时，用户可以在推理时指定感兴趣的原理以定制奖励模型。

Conclusion: RLBFF可以超越Bradley-Terry模型，并在RM-Bench和JudgeBench上取得顶级性能。此外，它提供了一种完全开源的配方，以较低的推理成本达到或超过其他模型的性能。

Abstract: Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).

</details>


### [66] [SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines](https://arxiv.org/abs/2509.21320)
*Yizhou Wang,Chen Tang,Han Deng,Jiabei Xiao,Jiaqi Liu,Jianyu Wu,Jun Yao,Pengze Li,Encheng Su,Lintao Wang,Guohang Zhuang,Yuchen Ren,Ben Fei,Ming Hu,Xin Chen,Dongzhan Zhou,Junjun He,Xiangyu Yue,Zhenfei Yin,Jiamin Wu,Qihao Zheng,Yuhao Zhou,Huihui Xu,Chenglong Ma,Yan Lu,Wenlong Zhang,Chunfeng Song,Philip Torr,Shixiang Tang,Xinzhu Ma,Wanli Ouyang,Lei Bai*

Main category: cs.CL

TL;DR: 本文提出了一种科学推理基础模型，能够在多种科学任务中表现出色，并且比专业系统更具优势。


<details>
  <summary>Details</summary>
Motivation: 为了提高科学推理的准确性和通用性，需要一种能够将自然语言与异构科学表示对齐的模型。

Method: 该模型通过在206B个token的语料库上预训练，然后通过SFT在40M条指令上进行对齐，并使用冷启动引导和强化学习进行训练，以实现精确的科学推理。

Result: 该模型支持五个能力家族，涵盖多达103个任务，包括文本和科学格式之间的忠实翻译、文本/知识提取、属性预测、属性分类、无条件和条件序列生成和设计。

Conclusion: 该模型在多个科学任务上表现出色，相比专业系统具有更广的指令覆盖范围、更好的跨领域泛化能力和更高的保真度。

Abstract: We present a scientific reasoning foundation model that aligns natural
language with heterogeneous scientific representations. The model is pretrained
on a 206B-token corpus spanning scientific text, pure sequences, and
sequence-text pairs, then aligned via SFT on 40M instructions, annealed
cold-start bootstrapping to elicit long-form chain-of-thought, and
reinforcement learning with task-specific reward shaping, which instills
deliberate scientific reasoning. It supports four capability families, covering
up to 103 tasks across workflows: (i) faithful translation between text and
scientific formats, (ii) text/knowledge extraction, (iii) property prediction,
(iv) property classification, (v) unconditional and conditional sequence
generation and design. Compared with specialist systems, our approach broadens
instruction coverage, improves cross-domain generalization, and enhances
fidelity. We detail data curation and training and show that cross-discipline
learning strengthens transfer and downstream reliability. The model, instruct
tuning datasets and the evaluation code are open-sourced at
https://huggingface.co/SciReason and
https://github.com/open-sciencelab/SciReason.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [67] [Interactive Recommendation Agent with Active User Commands](https://arxiv.org/abs/2509.21317)
*Jiakai Tang,Yujie Luo,Xunke Xi,Fei Sun,Xueyang Feng,Sunhao Dai,Chao Yi,Dian Chen,Zhujin Gao,Yang Li,Xu Chen,Wen Chen,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 本文提出了一种新的推荐系统框架IRF，通过自然语言命令实现用户对推荐策略的主动控制，并开发了RecBot双代理架构来处理语言表达并动态调整推荐策略。实验表明，RecBot在用户满意度和业务成果方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: Traditional recommender systems rely on passive feedback mechanisms that fail to capture users' nuanced behavior motivations and intentions, leading to inaccurate preference modeling and a gap between user intentions and system interpretations.

Method: Interactive Recommendation Feed (IRF) and RecBot, a dual-agent architecture with a Parser Agent and a Planner Agent, along with simulation-augmented knowledge distillation.

Result: RecBot demonstrates significant improvements in user satisfaction and business outcomes through extensive offline and long-term online experiments.

Conclusion: RecBot shows significant improvements in both user satisfaction and business outcomes.

Abstract: Traditional recommender systems rely on passive feedback mechanisms that
limit users to simple choices such as like and dislike. However, these
coarse-grained signals fail to capture users' nuanced behavior motivations and
intentions. In turn, current systems cannot also distinguish which specific
item attributes drive user satisfaction or dissatisfaction, resulting in
inaccurate preference modeling. These fundamental limitations create a
persistent gap between user intentions and system interpretations, ultimately
undermining user satisfaction and harming system effectiveness.
  To address these limitations, we introduce the Interactive Recommendation
Feed (IRF), a pioneering paradigm that enables natural language commands within
mainstream recommendation feeds. Unlike traditional systems that confine users
to passive implicit behavioral influence, IRF empowers active explicit control
over recommendation policies through real-time linguistic commands. To support
this paradigm, we develop RecBot, a dual-agent architecture where a Parser
Agent transforms linguistic expressions into structured preferences and a
Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly
policy adjustment. To enable practical deployment, we employ
simulation-augmented knowledge distillation to achieve efficient performance
while maintaining strong reasoning capabilities. Through extensive offline and
long-term online experiments, RecBot shows significant improvements in both
user satisfaction and business outcomes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [68] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: 本文研究了验证设计和策略如何影响代码生成模型的性能。结果表明，当前的验证方式过于僵化，过滤掉了有价值的不同性。但不能被抛弃，只能重新校准。通过结合校准后的验证与多样且具有挑战性的问题-解决方案对，可以突破验证天花板，提升代码生成模型的能力。


<details>
  <summary>Details</summary>
Motivation: 大型代码生成语言模型越来越多地依赖合成数据，其中问题解决方案和验证测试均由模型生成。虽然这使得数据创建可扩展，但也引入了一个之前未探索的瓶颈：验证天花板，其中训练数据的质量和多样性从根本上受到合成验证器能力的限制。

Method: 我们系统地研究了验证设计和策略如何影响模型性能。我们调查了（i）我们验证什么，分析了测试复杂性和数量的影响：更丰富的测试套件提高了代码生成能力，而数量本身则产生递减收益；（ii）我们如何验证，探索了放松通过阈值：严格的100%通过标准可能过于限制。通过允许放松阈值或结合基于LLM的软验证，我们可以恢复有价值的训练数据，导致pass@1性能提高2-4点。然而，这种好处取决于所使用的测试用例的强度和多样性；（iii）为什么验证仍然是必要的，通过控制比较形式正确的与错误的解决方案以及人工评估：保留每个问题的多样化正确解决方案带来了持续的泛化增益。

Result: 我们的结果表明，当前的验证方式过于僵化，过滤掉了有价值的不同性。但不能被抛弃，只能重新校准。通过结合校准后的验证与多样且具有挑战性的问题-解决方案对，我们提出了突破验证天花板并解锁更强代码生成模型的路径。

Conclusion: 当前的验证方式过于僵化，过滤掉了有价值的不同性。但不能被抛弃，只能重新校准。通过结合校准后的验证与多样且具有挑战性的问题-解决方案对，我们提出了突破验证天花板并解锁更强代码生成模型的路径。

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [69] [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,Shayan Baghayi Nejad,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: CARINOX是一种统一框架，结合了噪声优化和探索，并基于与人类判断的相关性进行奖励选择。它在两个基准测试中提升了平均对齐分数，并在所有主要类别中 consistently 超过最先进的优化和探索方法，同时保持了图像质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时间方法在单独使用时存在内在限制，例如优化可能因不良初始化或不利的搜索轨迹而停滞，而探索可能需要过多样本才能找到满意的输出。此外，单一奖励指标或临时组合无法可靠地捕捉所有方面的组成性，导致引导效果弱或不一致。

Method: CARINOX是一种统一框架，结合了噪声优化和探索，并基于与人类判断的相关性进行奖励选择。

Result: CARINOX在T2I-CompBench++基准上平均对齐分数提高了+16%，在HRS基准上提高了+11%。

Conclusion: CARINOX在两个互补的基准测试中提升了平均对齐分数，并在所有主要类别中 consistently 超过最先进的优化和探索方法，同时保持了图像质量和多样性。

Abstract: Text-to-image diffusion models, such as Stable Diffusion, can produce
high-quality and diverse images but often fail to achieve compositional
alignment, particularly when prompts describe complex object relationships,
attributes, or spatial arrangements. Recent inference-time approaches address
this by optimizing or exploring the initial noise under the guidance of reward
functions that score text-image alignment without requiring model fine-tuning.
While promising, each strategy has intrinsic limitations when used alone:
optimization can stall due to poor initialization or unfavorable search
trajectories, whereas exploration may require a prohibitively large number of
samples to locate a satisfactory output. Our analysis further shows that
neither single reward metrics nor ad-hoc combinations reliably capture all
aspects of compositionality, leading to weak or inconsistent guidance. To
overcome these challenges, we present Category-Aware Reward-based Initial Noise
Optimization and Exploration (CARINOX), a unified framework that combines noise
optimization and exploration with a principled reward selection procedure
grounded in correlation with human judgments. Evaluations on two complementary
benchmarks covering diverse compositional challenges show that CARINOX raises
average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS
benchmark, consistently outperforming state-of-the-art optimization and
exploration-based methods across all major categories, while preserving image
quality and diversity. The project page is available at
https://amirkasaei.com/carinox/{this URL}.

</details>


### [70] [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
*Ofir Azachi,Kfir Eliyahu,Eyal El Ani,Rom Himelstein,Roi Reichart,Yuval Pinter,Nitay Calderon*

Main category: cs.CV

TL;DR: 本文提出了一种基于NTP的高效幻觉检测方法，能够实现与强大视觉语言模型相当的性能，并且可以通过结合其他技术进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的幻觉会降低其可靠性，而现有的检测方法计算成本高且延迟大。因此，需要一种高效的方法来检测幻觉。

Method: 本文通过训练传统机器学习模型来检测幻觉，这些模型基于视觉语言模型的下一个标记概率（NTPs）进行训练。此外，还通过将NTP与语言NTP结合，并将视觉语言模型的幻觉预测分数集成到NTP模型中来提高检测性能。

Result: 实验结果表明，基于NTP的特征是幻觉的有效预测因子，使得快速简单的机器学习模型能够达到与强大视觉语言模型相当的性能。此外，结合语言NTP和集成视觉语言模型的预测分数可以进一步提高检测性能。

Conclusion: 本文希望为增强视觉语言模型的可靠性提供简单轻量的解决方案。

Abstract: Hallucinations of vision-language models (VLMs), which are misalignments
between visual content and generated text, undermine the reliability of VLMs.
One common approach for detecting them employs the same VLM, or a different
one, to assess generated outputs. This process is computationally intensive and
increases model latency. In this paper, we explore an efficient on-the-fly
method for hallucination detection by training traditional ML models over
signals based on the VLM's next-token probabilities (NTPs). NTPs provide a
direct quantification of model uncertainty. We hypothesize that high
uncertainty (i.e., a low NTP value) is strongly associated with hallucinations.
To test this, we introduce a dataset of 1,400 human-annotated statements
derived from VLM-generated content, each labeled as hallucinated or not, and
use it to test our NTP-based lightweight method. Our results demonstrate that
NTP-based features are valuable predictors of hallucinations, enabling fast and
simple ML models to achieve performance comparable to that of strong VLMs.
Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding
only the generated text back into the VLM, enhances hallucination detection
performance. Finally, integrating hallucination prediction scores from VLMs
into the NTP-based models led to better performance than using either VLMs or
NTPs alone. We hope this study paves the way for simple, lightweight solutions
that enhance the reliability of VLMs.

</details>


### [71] [Human Semantic Representations of Social Interactions from Moving Shapes](https://arxiv.org/abs/2509.20673)
*Yiling Yun,Hongjing Lu*

Main category: cs.CV

TL;DR: 该研究探讨了人类如何利用语义表示来补充视觉特征，以理解简单的移动形状的社会互动。结果显示，语义模型（特别是动词嵌入）能够更好地解释人类的相似性判断。


<details>
  <summary>Details</summary>
Motivation: 之前的研究往往集中在视觉特征上，而我们研究了人类用来补充视觉特征的语义表示。

Method: 在研究1中，我们直接让人类参与者根据他们对移动形状的印象对动画进行标记。在研究2中，我们通过人类相似性判断测量了27种社会互动的表征几何，并将其与基于视觉特征、标签和动画描述语义嵌入的模型预测进行了比较。

Result: 人类响应是分布的。语义模型提供了补充视觉特征的信息，以解释人类判断。其中，从描述中提取的动词嵌入最好地解释了人类相似性判断。

Conclusion: 这些结果表明，简单显示中的社会感知反映了社会互动的语义结构，弥合了视觉和抽象表示之间的差距。

Abstract: Humans are social creatures who readily recognize various social interactions
from simple display of moving shapes. While previous research has often focused
on visual features, we examine what semantic representations that humans employ
to complement visual features. In Study 1, we directly asked human participants
to label the animations based on their impression of moving shapes. We found
that human responses were distributed. In Study 2, we measured the
representational geometry of 27 social interactions through human similarity
judgments and compared it with model predictions based on visual features,
labels, and semantic embeddings from animation descriptions. We found that
semantic models provided complementary information to visual features in
explaining human judgments. Among the semantic models, verb-based embeddings
extracted from descriptions account for human similarity judgments the best.
These results suggest that social perception in simple displays reflects the
semantic structure of social interactions, bridging visual and abstract
representations.

</details>


### [72] [Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models](https://arxiv.org/abs/2509.20751)
*Zoe Wanying He,Sean Trott,Meenakshi Khosla*

Main category: cs.CV

TL;DR: 本文研究了深度视觉-only和语言-only模型如何在不同时期对齐它们的输入，并发现它们收敛到一个与人类判断一致的共享语义代码。


<details>
  <summary>Details</summary>
Motivation: 我们想要了解深度视觉-only和语言-only模型如何在不同时期对齐它们的输入，以及这种对齐是否反映了语义信息。

Method: 我们系统地研究了这些问题，包括分析对齐在模型中的出现位置、支持它的视觉或语言提示、它是否捕捉人类偏好以及聚合同一概念的示例如何影响对齐。

Result: 我们发现对齐在两种模型类型的中后期层达到峰值，这反映了从模态特定到概念共享表示的转变。这种对齐对于外观变化是鲁棒的，但当语义被改变时（例如对象移除或单词顺序混乱）会崩溃，这表明共享代码确实是语义性的。此外，平均嵌入可以增强对齐而不是模糊细节。

Conclusion: 我们的结果表明，单模态网络会收敛到一个与人类判断一致的共享语义代码，并且随着示例聚合而增强。

Abstract: Recent studies show that deep vision-only and language-only models--trained
on disjoint modalities--nonetheless project their inputs into a partially
aligned representational space. Yet we still lack a clear picture of where in
each network this convergence emerges, what visual or linguistic cues support
it, whether it captures human preferences in many-to-many image-text scenarios,
and how aggregating exemplars of the same concept affects alignment. Here, we
systematically investigate these questions. We find that alignment peaks in
mid-to-late layers of both model types, reflecting a shift from
modality-specific to conceptually shared representations. This alignment is
robust to appearance-only changes but collapses when semantics are altered
(e.g., object removal or word-order scrambling), highlighting that the shared
code is truly semantic. Moving beyond the one-to-one image-caption paradigm, a
forced-choice "Pick-a-Pic" task shows that human preferences for image-caption
matches are mirrored in the embedding spaces across all vision-language model
pairs. This pattern holds bidirectionally when multiple captions correspond to
a single image, demonstrating that models capture fine-grained semantic
distinctions akin to human judgments. Surprisingly, averaging embeddings across
exemplars amplifies alignment rather than blurring detail. Together, our
results demonstrate that unimodal networks converge on a shared semantic code
that aligns with human judgments and strengthens with exemplar aggregation.

</details>


### [73] [TABLET: A Large-Scale Dataset for Robust Visual Table Understanding](https://arxiv.org/abs/2509.21205)
*Iñigo Alonso,Imanol Miranda,Eneko Agirre,Mirella Lapata*

Main category: cs.CV

TL;DR: 本文介绍了TABLET，一个大规模的VTU数据集，旨在解决现有数据集在真实世界表格复杂性和多样性方面的不足，并通过保留原始可视化和维护示例可追溯性，为未来VTU模型的稳健训练和评估提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前的VTU数据集缺乏真实世界表格的复杂性和视觉多样性，并且提供固定的示例和预定义的指令，没有访问底层序列化数据进行重新表述。

Method: 引入了TABLET，一个包含400万例子的大型VTU数据集，覆盖20个任务，基于200万个独特的表格，其中88%保留了原始可视化效果。每个例子包括配对的图像-HTML表示、全面的元数据和溯源信息，链接到源数据集。

Result: 在TABLET上微调像Qwen2.5-VL-7B这样的视觉-语言模型可以提高对已见和未见VTU任务的性能，并增强对真实世界表格可视化的鲁棒性。

Conclusion: TABLET建立了一个大规模的VTU数据集，为未来VTU模型的稳健训练和可扩展评估奠定了基础。

Abstract: While table understanding increasingly relies on pixel-only settings where
tables are processed as visual representations, current benchmarks
predominantly use synthetic renderings that lack the complexity and visual
diversity of real-world tables. Additionally, existing visual table
understanding (VTU) datasets offer fixed examples with single visualizations
and pre-defined instructions, providing no access to underlying serialized data
for reformulation. We introduce TABLET, a large-scale VTU dataset with 4
million examples across 20 tasks, grounded in 2 million unique tables where 88%
preserve original visualizations. Each example includes paired image-HTML
representations, comprehensive metadata, and provenance information linking
back to the source datasets. Fine-tuning vision-language models like
Qwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while
increasing robustness on real-world table visualizations. By preserving
original visualizations and maintaining example traceability in a unified
large-scale collection, TABLET establishes a foundation for robust training and
extensible evaluation of future VTU models.

</details>


### [74] [Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding](https://arxiv.org/abs/2509.21223)
*Muxin Pu,Mei Kuan Lim,Chun Yong Chong,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出了一种名为Sigma的统一骨架式手语理解框架，解决了当前手语理解方法的三个关键限制：语义基础薄弱、局部细节与全局上下文之间的不平衡以及跨模态学习效率低下。Sigma通过引入一种基于手语的早期融合机制、分层对齐学习策略和统一的预训练框架，实现了在多个基准测试中的最新成果，证明了语义信息丰富的预训练和骨架数据作为独立解决方案在手语理解中的有效性。


<details>
  <summary>Details</summary>
Motivation: Current SLU methods continue to face three key limitations: weak semantic grounding, imbalance between local details and global context, and inefficient cross-modal learning.

Method: Sigma, a unified skeleton-based SLU framework featuring a sign-aware early fusion mechanism, a hierarchical alignment learning strategy, and a unified pre-training framework that combines contrastive learning, text matching and language modelling.

Result: Sigma achieves new state-of-the-art results on isolated sign language recognition, continuous sign language recognition, and gloss-free sign language translation on multiple benchmarks spanning different sign and spoken languages.

Conclusion: Sigma achieves new state-of-the-art results on isolated sign language recognition, continuous sign language recognition, and gloss-free sign language translation on multiple benchmarks spanning different sign and spoken languages, demonstrating the impact of semantically informative pre-training and the effectiveness of skeletal data as a stand-alone solution for SLU.

Abstract: Pre-training has proven effective for learning transferable features in sign
language understanding (SLU) tasks. Recently, skeleton-based methods have
gained increasing attention because they can robustly handle variations in
subjects and backgrounds without being affected by appearance or environmental
factors. Current SLU methods continue to face three key limitations: 1) weak
semantic grounding, as models often capture low-level motion patterns from
skeletal data but struggle to relate them to linguistic meaning; 2) imbalance
between local details and global context, with models either focusing too
narrowly on fine-grained cues or overlooking them for broader context; and 3)
inefficient cross-modal learning, as constructing semantically aligned
representations across modalities remains difficult. To address these, we
propose Sigma, a unified skeleton-based SLU framework featuring: 1) a
sign-aware early fusion mechanism that facilitates deep interaction between
visual and textual modalities, enriching visual features with linguistic
context; 2) a hierarchical alignment learning strategy that jointly maximises
agreements across different levels of paired features from different
modalities, effectively capturing both fine-grained details and high-level
semantic relationships; and 3) a unified pre-training framework that combines
contrastive learning, text matching and language modelling to promote semantic
consistency and generalisation. Sigma achieves new state-of-the-art results on
isolated sign language recognition, continuous sign language recognition, and
gloss-free sign language translation on multiple benchmarks spanning different
sign and spoken languages, demonstrating the impact of semantically informative
pre-training and the effectiveness of skeletal data as a stand-alone solution
for SLU.

</details>


### [75] [Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.21227)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文研究了用于组合文本-图像评估的常用度量标准，发现没有一种度量标准在所有任务中都表现良好，且不同度量标准家族与人类判断的对齐程度不同。


<details>
  <summary>Details</summary>
Motivation: 评估文本-图像生成是否真正捕捉提示中描述的对象、属性和关系仍然是一个核心挑战。目前的评估依赖于自动化度量标准，但这些度量标准通常是根据惯例或流行程度采用的，而不是经过与人类判断的验证。

Method: 我们对广泛使用的用于组合文本-图像评估的度量标准进行了广泛的研究。

Result: 结果表明，没有单一的度量标准在所有任务中都能一致表现：性能随着组合问题的类型而变化。值得注意的是，尽管VQA-based度量标准很受欢迎，但它们并不总是更优的，而某些基于嵌入的度量标准在特定情况下表现更强。

Conclusion: 这些发现强调了在评估和生成中选择度量标准时需要仔细和透明的重要性。

Abstract: Text-image generation has advanced rapidly, but assessing whether outputs
truly capture the objects, attributes, and relations described in prompts
remains a central challenge. Evaluation in this space relies heavily on
automated metrics, yet these are often adopted by convention or popularity
rather than validated against human judgment. Because evaluation and reported
progress in the field depend directly on these metrics, it is critical to
understand how well they reflect human preferences. To address this, we present
a broad study of widely used metrics for compositional text-image evaluation.
Our analysis goes beyond simple correlation, examining their behavior across
diverse compositional challenges and comparing how different metric families
align with human judgments. The results show that no single metric performs
consistently across tasks: performance varies with the type of compositional
problem. Notably, VQA-based metrics, though popular, are not uniformly
superior, while certain embedding-based metrics prove stronger in specific
cases. Image-only metrics, as expected, contribute little to compositional
evaluation, as they are designed for perceptual quality rather than alignment.
These findings underscore the importance of careful and transparent metric
selection, both for trustworthy evaluation and for their use as reward models
in generation. Project page is available at
\href{https://amirkasaei.com/eval-the-evals/}{this URL}.

</details>


### [76] [Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation](https://arxiv.org/abs/2509.21257)
*Seyed Amir Kasaei,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文定义了文本到图像生成模型中的幻觉现象，并提出了一种新的分类框架，以更全面地评估这些模型。


<details>
  <summary>Details</summary>
Motivation: 现有的评估主要关注对齐性，而忽视了模型在提示之外生成的内容。本文旨在明确幻觉的定义并提供一个更全面的评估框架。

Method: 本文提出了一个基于偏差驱动偏离的幻觉定义，并将其分为属性、关系和对象三类。

Result: 本文提出了一个幻觉分类框架，并展示了其在评估文本到图像生成模型中的应用。

Conclusion: 本文提出了一个关于文本到图像生成模型中幻觉现象的新定义和分类框架，为更全面的评估提供了基础。

Abstract: In language and vision-language models, hallucination is broadly understood
as content generated from a model's prior knowledge or biases rather than from
the given input. While this phenomenon has been studied in those domains, it
has not been clearly framed for text-to-image (T2I) generative models. Existing
evaluations mainly focus on alignment, checking whether prompt-specified
elements appear, but overlook what the model generates beyond the prompt. We
argue for defining hallucination in T2I as bias-driven deviations and propose a
taxonomy with three categories: attribute, relation, and object hallucinations.
This framing introduces an upper bound for evaluation and surfaces hidden
biases, providing a foundation for richer assessment of T2I models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [77] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: 本文介绍了InsightGUIDE，这是一种新型的AI驱动工具，旨在作为阅读助手而非替代品。通过将专家的阅读方法嵌入其核心AI逻辑中，该系统提供简洁、结构化的见解，帮助研究人员更有效地理解论文的关键内容。


<details>
  <summary>Details</summary>
Motivation: The proliferation of scientific literature presents an increasingly significant challenge for researchers. Existing tools often provide verbose summaries that risk replacing, rather than assisting, the reading of the source material.

Method: The paper introduces InsightGUIDE, a novel AI-powered tool designed to function as a reading assistant. It provides concise, structured insights by embedding an expert's reading methodology into its core AI logic. The system's architecture, prompt-driven methodology, and a qualitative case study comparing its output to a general-purpose LLM are presented.

Result: The results demonstrate that InsightGUIDE produces more structured and actionable guidance, serving as a more effective tool for the modern researcher.

Conclusion: InsightGUIDE produces more structured and actionable guidance, serving as a more effective tool for the modern researcher.

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [78] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE is a three-agent neuro-symbolic framework that optimizes context construction for multi-hop question answering by balancing accuracy, latency, and cost without retraining.


<details>
  <summary>Details</summary>
Motivation: Deployed systems must balance answer accuracy with strict latency and cost targets while preserving provenance. Static k-hop expansions and 'think-longer' prompting often over-retrieve, inflate context, and yield unpredictable runtime.

Method: CLAUSE is an agentic three-agent neuro-symbolic framework that treats context construction as a sequential decision process over knowledge graphs. It employs the Lagrangian-Constrained Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate three agents: Subgraph Architect, Path Navigator, and Context Curator.

Result: Across HotpotQA, MetaQA, and FactKG, CLAUSE yields higher EM@1 while reducing subgraph growth and end-to-end latency at equal or lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline (GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower edge growth.

Conclusion: CLAUSE yields higher EM@1 while reducing subgraph growth and end-to-end latency at equal or lower token budgets. The resulting contexts are compact, provenance-preserving, and deliver predictable performance under deployment constraints.

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [79] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: 本研究挑战了说服效果主要取决于模型规模的假设，提出这些动力学本质上由模型的基本认知过程决定，特别是其显式推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着最近多智能体系统（MAS）的快速普及，其中大型语言模型（LLMs）和大型推理模型（LRMs）通常合作解决复杂问题，需要深入了解支配其交互的说服动力学。

Method: 通过一系列多智能体说服实验，我们发现了称为说服二元性的基本权衡。我们进一步考虑了更复杂的传输说服情况，揭示了多跳说服中影响传播和衰减的复杂动态。

Result: 我们的研究发现，LRMs中的推理过程对说服表现出显著的抵抗力，更能保持其初始信念。然而，通过分享“思考内容”使这一推理过程透明，会大大增加它们说服他人的能力。

Conclusion: 本研究提供了系统证据，将模型的内部处理架构与其外部说服行为联系起来，为先进模型的易受攻击性提供了新的解释，并突出了对未来多智能体系统（MAS）安全、鲁棒性和设计的重要影响。

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [80] [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
*Yidong Wang,Yunze Song,Tingyuan Zhu,Xuanwang Zhang,Zhuohao Yu,Hao Chen,Chiyu Song,Qiufeng Wang,Cunxiang Wang,Zhen Wu,Xinyu Dai,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.AI

TL;DR: 本文分析了LLM-as-a-judge评估框架中的不一致性问题，并提出了TrustJudge框架，通过概率方法解决这些问题，提高了评估的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-judge评估框架存在严重的不一致性问题，包括得分比较不一致和成对传递性不一致，这源于离散评分系统的信息丢失和成对评估中的模糊平局判断。

Method: 本文提出了一种概率框架TrustJudge，通过分布敏感评分和似然感知聚合来解决信息丢失和模糊的平局判断问题。

Result: 在使用Llama-3.1-70B-Instruct作为评估者进行评估时，TrustJudge将得分比较不一致性降低了8.43%，将成对传递性不一致性降低了10.82%，同时保持了更高的评估准确性。

Conclusion: 本文提出了TrustJudge框架，通过两个关键创新解决了LLM-as-a-judge评估框架中的不一致性问题，并展示了其在不同模型架构和规模上的持续改进效果，从而实现了更可信赖的LLM评估。

Abstract: The adoption of Large Language Models (LLMs) as automated evaluators
(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation
frameworks. We identify two fundamental types of inconsistencies: (1)
Score-Comparison Inconsistency, where lower-rated responses outperform
higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity
Inconsistency, manifested through circular preference chains (A>B>C>A) and
equivalence contradictions (A=B=C\neq A). We argue that these issues come from
information loss in discrete rating systems and ambiguous tie judgments during
pairwise evaluation. We propose TrustJudge, a probabilistic framework that
addresses these limitations through two key innovations: 1)
distribution-sensitive scoring that computes continuous expectations from
discrete rating probabilities, preserving information entropy for more precise
scoring, and 2) likelihood-aware aggregation that resolves transitivity
violations using bidirectional preference probabilities or perplexity. We also
formalize the theoretical limitations of current LLM-as-a-judge frameworks and
demonstrate how TrustJudge's components overcome them. When evaluated with
Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces
Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise
Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining
higher evaluation accuracy. Our work provides the first systematic analysis of
evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both
theoretical insights and practical solutions for reliable automated assessment.
The framework demonstrates consistent improvements across various model
architectures and scales, enabling more trustworthy LLM evaluation without
requiring additional training or human annotations. The codes can be found at
https://github.com/TrustJudge/TrustJudge.

</details>


### [81] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 本文提出了一种基于高价值推理模式的双重粒度算法，用于选择高质量的CoT数据，从而显著提升模型的推理能力和性能。


<details>
  <summary>Details</summary>
Motivation: 当前的方法往往不加区分地使用CoT数据，因此需要明确哪些数据类型最能增强模型的推理能力。本文旨在解决这一问题，并提高模型的推理深度和性能。

Method: 本文首先定义了基础模型的推理潜力，然后通过从CoT序列中抽象出原子推理模式，构建了一个富含有价值推理模式的核心参考集。接着，提出了一种涉及推理模式链和令牌熵的双重粒度算法，以高效地从数据池中选择与核心集对齐的高价值CoT数据（CoTP）。

Result: 使用10B-token的CoTP数据，85A6B MoE模型在AIME 2024和2025上提升了9.58%，并且将下游RL性能的上限提高了7.81%。

Conclusion: 本文提出了一个基于高价值推理模式的双重粒度算法，能够有效地选择高质量的CoT数据，从而提高模型的推理能力。实验结果表明，仅使用10B-token的CoTP数据就能使85A6B MoE模型在AIME 2024和2025上提升9.58%，并提高下游RL性能的上限7.81%。

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [82] [Every Character Counts: From Vulnerability to Defense in Phishing Detection](https://arxiv.org/abs/2509.20589)
*Maria Chiper,Radu Tudor Ionescu*

Main category: cs.CR

TL;DR: 本文研究了字符级深度学习模型在钓鱼检测中的应用，发现CharGRU在多种场景下表现最佳，并通过对抗训练提高了模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自动检测方法在检测新型钓鱼攻击时缺乏可解释性和鲁棒性，因此需要一种更有效的方法来应对日益增长的钓鱼威胁。

Method: 本文评估了三种字符级神经网络架构（CharCNN、CharGRU和CharBiLSTM）在自建电子邮件数据集上的性能，并在三种场景下分析了它们的表现：标准训练和测试、标准训练和测试下的对抗攻击，以及使用对抗示例的训练和测试。同时，还测试了这些模型在有限计算资源下的表现。

Result: CharGRU在所有场景中表现最佳，所有模型对对抗攻击都存在漏洞，但对抗训练显著提高了它们的鲁棒性。通过适应Grad-CAM技术，能够可视化影响模型决策的电子邮件部分。

Conclusion: 本文研究了字符级深度学习模型在钓鱼检测中的有效性，发现CharGRU在所有场景中表现最佳，并且通过对抗训练可以显著提高模型的鲁棒性。此外，通过适应Grad-CAM技术，能够可视化每个电子邮件中影响模型决策的部分。

Abstract: Phishing attacks targeting both organizations and individuals are becoming an
increasingly significant threat as technology advances. Current automatic
detection methods often lack explainability and robustness in detecting new
phishing attacks. In this work, we investigate the effectiveness of
character-level deep learning models for phishing detection, which can provide
both robustness and interpretability. We evaluate three neural architectures
adapted to operate at the character level, namely CharCNN, CharGRU, and
CharBiLSTM, on a custom-built email dataset, which combines data from multiple
sources. Their performance is analyzed under three scenarios: (i) standard
training and testing, (ii) standard training and testing under adversarial
attacks, and (iii) training and testing with adversarial examples. Aiming to
develop a tool that operates as a browser extension, we test all models under
limited computational resources. In this constrained setup, CharGRU proves to
be the best-performing model across all scenarios. All models show
vulnerability to adversarial attacks, but adversarial training substantially
improves their robustness. In addition, by adapting the Gradient-weighted Class
Activation Mapping (Grad-CAM) technique to character-level inputs, we are able
to visualize which parts of each email influence the decision of each model.
Our open-source code and data is released at
https://github.com/chipermaria/every-character-counts.

</details>


### [83] [PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints](https://arxiv.org/abs/2509.21057)
*Jiahao Huo,Shuliang Liu,Bin Wang,Junyan Zhang,Yibo Yan,Aiwei Liu,Xuming Hu,Mingxun Zhou*

Main category: cs.CR

TL;DR: PMark is a new SWM method that uses proxy functions to enhance watermarking robustness while maintaining text quality.


<details>
  <summary>Details</summary>
Motivation: Existing SWM methods lack strong theoretical guarantees of robustness and introduce significant distribution distortions compared with unwatermarked outputs.

Method: PMark, a simple yet powerful SWM method that estimates the PF median for the next sentence dynamically through sampling while enforcing multiple PF constraints (which we call channels) to strengthen watermark evidence.

Result: PMark achieves the desired distortion-free property and improves the robustness against paraphrasing-style attacks. An empirically optimized version further removes the requirement for dynamical median estimation for better sampling efficiency.

Conclusion: PMark consistently outperforms existing SWM baselines in both text quality and robustness, offering a more effective paradigm for detecting machine-generated text.

Abstract: Semantic-level watermarking (SWM) for large language models (LLMs) enhances
watermarking robustness against text modifications and paraphrasing attacks by
treating the sentence as the fundamental unit. However, existing methods still
lack strong theoretical guarantees of robustness, and reject-sampling-based
generation often introduces significant distribution distortions compared with
unwatermarked outputs. In this work, we introduce a new theoretical framework
on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions
that map sentences to scalar values. Building on this framework, we propose
PMark, a simple yet powerful SWM method that estimates the PF median for the
next sentence dynamically through sampling while enforcing multiple PF
constraints (which we call channels) to strengthen watermark evidence. Equipped
with solid theoretical guarantees, PMark achieves the desired distortion-free
property and improves the robustness against paraphrasing-style attacks. We
also provide an empirically optimized version that further removes the
requirement for dynamical median estimation for better sampling efficiency.
Experimental results show that PMark consistently outperforms existing SWM
baselines in both text quality and robustness, offering a more effective
paradigm for detecting machine-generated text. Our code will be released at
[this URL](https://github.com/PMark-repo/PMark).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [84] [Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models](https://arxiv.org/abs/2506.00209)
*Liwen Sun,Hao-Ren Yao,Gary Gao,Ophir Frieder,Chenyan Xiong*

Main category: cs.LG

TL;DR: CATCH-FM is a pre-screening method for cancer using historical medical records, achieving high efficacy and specificity. It outperforms existing models and is robust across different patient distributions.


<details>
  <summary>Details</summary>
Motivation: Existing cancer screening techniques are expensive, intrusive, and not globally available, leading to many preventable deaths. There is a need for an effective and accessible pre-screening method.

Method: CATCH-FM is a cancer pre-screening methodology that uses historical medical records and pretrains foundation models on medical code sequences. It is fine-tuned on clinician-curated cancer risk prediction cohorts.

Result: In a retrospective evaluation of thirty thousand patients, CATCH-FM achieved 60% sensitivity with 99% specificity and Negative Predictive Value. It outperformed feature-based tree models and general and medical large language models.

Conclusion: CATCH-FM demonstrates robustness in various patient distributions and the ability to capture non-trivial cancer risk factors. It outperforms existing models in pancreatic cancer risk prediction.

Abstract: Cancer screening, leading to early detection, saves lives. Unfortunately,
existing screening techniques require expensive and intrusive medical
procedures, not globally available, resulting in too many lost would-be-saved
lives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation
Models, a cancer pre-screening methodology that identifies high-risk patients
for further screening solely based on their historical medical records. With
millions of electronic healthcare records (EHR), we establish the scaling law
of EHR foundation models pretrained on medical code sequences, pretrain
compute-optimal foundation models of up to 2.4 billion parameters, and finetune
them on clinician-curated cancer risk prediction cohorts. In our retrospective
evaluation comprising of thirty thousand patients, CATCH-FM achieved strong
efficacy (60% sensitivity) with low risk (99% specificity and Negative
Predictive Value), outperforming feature-based tree models as well as general
and medical large language models by large margins. Despite significant
demographic, healthcare system, and EHR coding differences, CATCH-FM achieves
state-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot
leaderboard, outperforming EHR foundation models pretrained using on-site
patient data. Our analysis demonstrates the robustness of CATCH-FM in various
patient distributions, the benefits of operating in the ICD code space, and its
ability to capture non-trivial cancer risk factors. Our code will be
open-sourced.

</details>


### [85] [Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation](https://arxiv.org/abs/2509.20680)
*Wenkai Guo,Xuefeng Liu,Haolin Wang,Jianwei Niu,Shaojie Tang,Jing Yuan*

Main category: cs.LG

TL;DR: 本文研究了在联邦学习中训练大型语言模型时的隐私风险，发现攻击者可以从中提取训练数据，并提出了相应的隐私保护措施。


<details>
  <summary>Details</summary>
Motivation: 组织希望通过本地数据微调大型语言模型来适应其特定领域，但由于数据共享的顾虑，集中式微调变得不切实际。联邦学习提供了一种潜在的解决方案，但本文发现联邦学习中的隐私泄露问题仍然存在。

Method: 本文通过广泛的实验展示了攻击者即使使用简单的生成方法也可以从全局模型中提取训练数据，并引入了一种针对联邦学习的增强攻击策略。此外，还评估了联邦学习中的隐私保护技术，包括差分隐私、正则化约束更新和采用安全对齐的大型语言模型。

Result: 本文展示了攻击者可以提取训练数据，且随着模型规模的增长，泄露程度增加。同时，引入的增强攻击策略能够加剧隐私泄露。评估的隐私保护技术为减少隐私风险提供了实用指南。

Conclusion: 本文提供了在使用联邦学习训练大型语言模型时减少隐私风险的有价值见解和实用指南。

Abstract: Fine-tuning large language models (LLMs) with local data is a widely adopted
approach for organizations seeking to adapt LLMs to their specific domains.
Given the shared characteristics in data across different organizations, the
idea of collaboratively fine-tuning an LLM using data from multiple sources
presents an appealing opportunity. However, organizations are often reluctant
to share local data, making centralized fine-tuning impractical. Federated
learning (FL), a privacy-preserving framework, enables clients to retain local
data while sharing only model parameters for collaborative training, offering a
potential solution. While fine-tuning LLMs on centralized datasets risks data
leakage through next-token prediction, the iterative aggregation process in FL
results in a global model that encapsulates generalized knowledge, which some
believe protects client privacy. In this paper, however, we present
contradictory findings through extensive experiments. We show that attackers
can still extract training data from the global model, even using
straightforward generation methods, with leakage increasing as the model size
grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which
tracks global model updates during training to intensify privacy leakage. To
mitigate these risks, we evaluate privacy-preserving techniques in FL,
including differential privacy, regularization-constrained updates and adopting
LLMs with safety alignment. Our results provide valuable insights and practical
guidelines for reducing privacy risks when training LLMs with FL.

</details>


### [86] [CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/abs/2509.20712)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Yuntao Li,Wenping Hu,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: CE-GPPO is a new algorithm that improves reinforcement learning for large language models by managing policy entropy more effectively.


<details>
  <summary>Details</summary>
Motivation: Existing methods like PPO discard valuable gradient signals from low-probability tokens due to the clipping mechanism, which affects the balance between exploration and exploitation during training.

Method: CE-GPPO is a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner, controlling the magnitude of gradients from tokens outside the clipping interval to achieve an exploration-exploitation trade-off.

Result: CE-GPPO effectively mitigates entropy instability and shows consistent performance improvements over strong baselines in mathematical reasoning benchmarks.

Conclusion: CE-GPPO effectively mitigates entropy instability and consistently outperforms strong baselines across different model scales in mathematical reasoning benchmarks.

Abstract: Reinforcement learning (RL) has become a powerful paradigm for optimizing
large language models (LLMs) to handle complex reasoning tasks. A core
challenge in this process lies in managing policy entropy, which reflects the
balance between exploration and exploitation during training. Existing methods,
such as proximal policy optimization (PPO) and its variants, discard valuable
gradient signals from low-probability tokens due to the clipping mechanism. We
systematically analyze the entropy dynamics and reveal that these clipped
tokens play a critical yet overlooked role in regulating entropy evolution. We
propose \textbf{C}ontrolling \textbf{E}ntropy via
\textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization
(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in
native PPO in a gentle and bounded manner. By controlling the magnitude of
gradients from tokens outside the clipping interval, CE-GPPO is able to achieve
an exploration-exploitation trade-off. We provide theoretical justification and
empirical evidence showing that CE-GPPO effectively mitigates entropy
instability. Extensive experiments on mathematical reasoning benchmarks show
that CE-GPPO consistently outperforms strong baselines across different model
scales.

</details>


### [87] [StyleBench: Evaluating thinking styles in Large Language Models](https://arxiv.org/abs/2509.20868)
*Junyu Guo,Shangding Gu,Ming Jin,Costas Spanos,Javad Lavaei*

Main category: cs.LG

TL;DR: 研究分析了不同推理风格在各种任务和模型中的表现，发现没有一种风格是普遍最优的，并提供了选择最佳策略的指导。


<details>
  <summary>Details</summary>
Motivation: 理解推理风格、模型架构和任务类型之间的相互作用对于提高大型语言模型的有效性至关重要。

Method: 我们引入了StyleBench，这是一个全面的基准，用于系统评估跨不同任务和模型的推理风格。

Result: 我们的大规模分析表明，没有一种风格是普遍最优的。搜索方法在开放问题中表现优异，而简洁风格在明确任务中表现出显著效率提升。

Conclusion: 我们的研究提供了选择最优推理策略的路线图，根据特定约束条件进行选择。

Abstract: The effectiveness of Large Language Models (LLMs) is heavily influenced by
the reasoning strategies, or styles of thought, employed in their prompts.
However, the interplay between these reasoning styles, model architecture, and
task type remains poorly understood. To address this, we introduce StyleBench,
a comprehensive benchmark for systematically evaluating reasoning styles across
diverse tasks and models. We assess five representative reasoning styles,
including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought
(AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning
tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral,
Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our
large-scale analysis reveals that no single style is universally optimal. We
demonstrate that strategy efficacy is highly contingent on both model scale and
task type: search-based methods (AoT, ToT) excel in open-ended problems but
require large-scale models, while concise styles (SoT, CoD) achieve radical
efficiency gains on well-defined tasks. Furthermore, we identify key behavioral
patterns: smaller models frequently fail to follow output instructions and
default to guessing, while reasoning robustness emerges as a function of scale.
Our findings offer a crucial roadmap for selecting optimal reasoning strategies
based on specific constraints, we open source the benchmark in
https://github.com/JamesJunyuGuo/Style_Bench.

</details>


### [88] [CLUE: Conflict-guided Localization for LLM Unlearning Framework](https://arxiv.org/abs/2509.20977)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: CLUE框架通过电路发现技术，精确识别遗忘和保留的神经元，从而提高LLM的遗忘效果和保留效用。


<details>
  <summary>Details</summary>
Motivation: 现有的基于局部化的方法无法区分负责遗忘不良知识或保留关键技能的神经元，通常将它们视为一个纠缠组，导致统一干预，可能造成灾难性过度遗忘或目标知识未完全擦除。

Method: CLUE框架利用电路发现技术，识别遗忘和保留电路，并将其转换为合取范式（CNF），通过CNF可满足性解分配每个神经元是否应被遗忘或保留，然后提供针对不同类别神经元的目标微调策略。

Result: 实验表明，与现有局部化方法相比，CLUE在精确的神经定位下实现了更优的遗忘效果和保留效用。

Conclusion: CLUE能够通过精确的神经定位实现更优的遗忘效果和保留效用。

Abstract: The LLM unlearning aims to eliminate the influence of undesirable data
without affecting causally unrelated information. This process typically
involves using a forget set to remove target information, alongside a retain
set to maintain non-target capabilities. While recent localization-based
methods demonstrate promise in identifying important neurons to be unlearned,
they fail to disentangle neurons responsible for forgetting undesirable
knowledge or retaining essential skills, often treating them as a single
entangled group. As a result, these methods apply uniform interventions,
risking catastrophic over-forgetting or incomplete erasure of the target
knowledge. To address this, we turn to circuit discovery, a mechanistic
interpretability technique, and propose the Conflict-guided Localization for
LLM Unlearning framEwork (CLUE). This framework identifies the forget and
retain circuit composed of important neurons, and then the circuits are
transformed into conjunctive normal forms (CNF). The assignment of each neuron
in the CNF satisfiability solution reveals whether it should be forgotten or
retained. We then provide targeted fine-tuning strategies for different
categories of neurons. Extensive experiments demonstrate that, compared to
existing localization methods, CLUE achieves superior forget efficacy and
retain utility through precise neural localization.

</details>


### [89] [Binary Autoencoder for Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2509.20997)
*Hakaze Cho,Haolin Yang,Brian M. Kurkoski,Naoya Inoue*

Main category: cs.LG

TL;DR: This paper introduces Binary Autoencoder (BAE), a novel method that enforces minimal entropy on hidden activations to promote feature sparsity and independence, effectively extracting atomized features from LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing works rely on autoencoders with implicit training-time regularization, leading to dense features that harm feature sparsity and atomization. The paper aims to address this issue by proposing a new method that explicitly promotes sparsity and independence.

Method: Proposed a novel autoencoder variant called Binary Autoencoder (BAE) that enforces minimal entropy on minibatches of hidden activations, promoting feature independence and sparsity across instances. Discretized hidden activations to 1-bit via a step function and applied gradient estimation for backpropagation.

Result: BAE was empirically demonstrated to have two major applications: (1) Reliable entropy estimation on binary hidden activations, which characterizes inference dynamics of LLMs and In-context Learning. (2) Effective feature untangling, producing the largest number of interpretable features among baselines.

Conclusion: BAE is effective in extracting atomized features from LLM's hidden states, avoiding dense features while producing the largest number of interpretable ones among baselines.

Abstract: Existing works are dedicated to untangling atomized numerical components
(features) from the hidden states of Large Language Models (LLMs) for
interpreting their mechanism. However, they typically rely on autoencoders
constrained by some implicit training-time regularization on single training
instances (i.e., $L_1$ normalization, top-k function, etc.), without an
explicit guarantee of global sparsity among instances, causing a large amount
of dense (simultaneously inactive) features, harming the feature sparsity and
atomization. In this paper, we propose a novel autoencoder variant that
enforces minimal entropy on minibatches of hidden activations, thereby
promoting feature independence and sparsity across instances. For efficient
entropy calculation, we discretize the hidden activations to 1-bit via a step
function and apply gradient estimation to enable backpropagation, so that we
term it as Binary Autoencoder (BAE) and empirically demonstrate two major
applications: (1) Feature set entropy calculation. Entropy can be reliably
estimated on binary hidden activations, which we empirically evaluate and
leverage to characterize the inference dynamics of LLMs and In-context
Learning. (2) Feature untangling. Similar to typical methods, BAE can extract
atomized features from LLM's hidden states. To robustly evaluate such feature
extraction capability, we refine traditional feature-interpretation methods to
avoid unreliable handling of numerical tokens, and show that BAE avoids dense
features while producing the largest number of interpretable ones among
baselines, which confirms the effectiveness of BAE serving as a feature
extractor.

</details>


### [90] [Mechanism of Task-oriented Information Removal in In-context Learning](https://arxiv.org/abs/2509.21012)
*Hakaze Cho,Haolin Yang,Gouki Minegishi,Naoya Inoue*

Main category: cs.LG

TL;DR: 本文从信息移除的角度分析了In-context Learning (ICL)的机制，发现ICL通过选择性地移除冗余信息来提高输出质量，并识别出关键的去噪注意力头。


<details>
  <summary>Details</summary>
Motivation: 尽管In-context Learning (ICL)是现代语言模型的一种新兴少样本学习范式，但其内部机制仍不清晰。本文旨在探索ICL的内在工作原理。

Method: 本文通过信息移除的视角研究ICL机制，利用低秩滤波器选择性地移除隐藏状态中的特定信息，并通过测量隐藏状态来观察ICL如何模拟这种信息去除过程。

Result: 研究发现，在零样本场景下，语言模型将查询编码为包含所有可能任务信息的非选择性表示，导致输出随意且准确性接近零。而通过低秩滤波器选择性地移除信息可以引导模型关注目标任务。此外，本文识别出关键的注意力头（称为去噪头），它们在信息去除过程中起着重要作用。

Conclusion: 本文揭示了In-context Learning (ICL)的关键机制，即通过信息去除实现任务导向的表示。

Abstract: In-context Learning (ICL) is an emerging few-shot learning paradigm based on
modern Language Models (LMs), yet its inner mechanism remains unclear. In this
paper, we investigate the mechanism through a novel perspective of information
removal. Specifically, we demonstrate that in the zero-shot scenario, LMs
encode queries into non-selective representations in hidden states containing
information for all possible tasks, leading to arbitrary outputs without
focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we
find that selectively removing specific information from hidden states by a
low-rank filter effectively steers LMs toward the intended task. Building on
these findings, by measuring the hidden states on carefully designed metrics,
we observe that few-shot ICL effectively simulates such task-oriented
information removal processes, selectively removing the redundant information
from entangled non-selective representations, and improving the output based on
the demonstrations, which constitutes a key mechanism underlying ICL. Moreover,
we identify essential attention heads inducing the removal operation, termed
Denoising Heads, which enables the ablation experiments blocking the
information removal operation from the inference, where the ICL accuracy
significantly degrades, especially when the correct label is absent from the
few-shot demonstrations, confirming both the critical role of the information
removal mechanism and denoising heads.

</details>


### [91] [DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?](https://arxiv.org/abs/2509.21016)
*Yiyou Sun,Yuhan Cao,Pohao Huang,Haoyue Bai,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.LG

TL;DR: 研究LLM能否通过强化学习获得新推理策略，提出DELTA-Code基准，发现模型在长期零奖励后突然提升性能，但在转化案例中仍有不足。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否能通过强化学习获得或泛化出真正的新推理策略，而不仅仅是预训练或后训练中编码的技能。

Method: 引入DELTA-Code基准，通过强化学习（RL）解决预训练模型在大量尝试中失败的问题家族，并评估技能的可转移性。

Result: 实验揭示了显著的grokking相变：在长时间零奖励后，RL训练的模型突然达到接近完美的准确率。在之前无法解决的问题家族上实现了学习能力，但转化案例中仍存在弱点。

Conclusion: DELTA提供了一个干净的测试平台，用于探测RL驱动的推理极限，并理解模型如何超越现有先验来获得新的算法技能。

Abstract: It remains an open question whether LLMs can acquire or generalize genuinely
new reasoning strategies, beyond the sharpened skills encoded in their
parameters during pre-training or post-training. To attempt to answer this
debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and
Transferrability in Algorithmic Coding, a controlled benchmark of synthetic
coding problem families designed to probe two fundamental aspects: learnability
-- can LLMs, through reinforcement learning (RL), solve problem families where
pretrained models exhibit failure with large enough attempts (pass@K=0)? --and
transferrability -- if learnability happens, can such skills transfer
systematically to out-of-distribution (OOD) test sets? Unlike prior public
coding datasets, DELTA isolates reasoning skills through templated problem
generators and introduces fully OOD problem families that demand novel
strategies rather than tool invocation or memorized patterns. Our experiments
reveal a striking grokking phase transition: after an extended period with
near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To
enable learnability on previously unsolvable problem families, we explore key
training ingredients such as staged warm-up with dense rewards, experience
replay, curriculum training, and verification-in-the-loop. Beyond learnability,
we use DELTA to evaluate transferability or generalization along exploratory,
compositional, and transformative axes, as well as cross-family transfer.
Results show solid gains within families and for recomposed skills, but
persistent weaknesses in transformative cases. DELTA thus offers a clean
testbed for probing the limits of RL-driven reasoning and for understanding how
models can move beyond existing priors to acquire new algorithmic skills.

</details>


### [92] [ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.21070)
*Qizhi Pei,Zhuoshi Pan,Honglin Lin,Xin Gao,Yu Li,Zinan Tang,Conghui He,Rui Yan,Lijun Wu*

Main category: cs.LG

TL;DR: 本文提出了一种名为ScaleDiff的简单而有效的管道，用于大规模生成困难问题。通过自适应思维模型识别困难问题，并训练专门的生成器生成大量困难问题，从而提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在生成困难问题时面临高计算/API成本、提示复杂性和生成问题难度有限的问题。为了克服这些限制，我们需要一种更有效和可扩展的方法来生成困难问题。

Method: 我们提出了ScaleDiff，这是一个简单但有效的管道，用于扩展困难问题的创建。我们使用自适应思维模型仅通过一次前向传递从现有数据集中高效地识别困难问题，并训练一个专门的困难问题生成器（DiffGen-8B）在过滤后的困难数据上生成大量新的困难问题。

Result: 在ScaleDiff-Math数据集上微调Qwen2.5-Math-7B-Instruct使性能提高了11.3%，并在AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500上实现了65.9%的平均准确率，超过了最近的强LRMs如OpenThinker3。

Conclusion: 我们的方法在困难基准测试中展示了模型性能随着困难问题数量增加而显著提升，并且使用成本效益更高的Qwen3-8B模型作为教师，证明了我们的管道可以有效地转移高级推理能力。

Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in complex
problem-solving, often benefiting from training on difficult mathematical
problems that stimulate intricate reasoning. Recent efforts have explored
automated synthesis of mathematical problems by prompting proprietary models or
large-scale open-source models from seed data or inherent mathematical
concepts. However, scaling up these methods remains challenging due to their
high computational/API cost, complexity of prompting, and limited difficulty
level of the generated problems. To overcome these limitations, we propose
ScaleDiff, a simple yet effective pipeline designed to scale the creation of
difficult problems. We efficiently identify difficult problems from existing
datasets with only a single forward pass using an adaptive thinking model,
which can perceive problem difficulty and automatically switch between
"Thinking" and "NoThinking" modes. We then train a specialized difficult
problem generator (DiffGen-8B) on this filtered difficult data, which can
produce new difficult problems in large scale, eliminating the need for
complex, per-instance prompting and its associated high API costs. Fine-tuning
Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial
performance increase of 11.3% compared to the original dataset and achieves a
65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500,
outperforming recent strong LRMs like OpenThinker3. Notably, this performance
is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating
that our pipeline can effectively transfer advanced reasoning capabilities
without relying on larger, more expensive teacher models. Furthermore, we
observe a clear scaling phenomenon in model performance on difficult benchmarks
as the quantity of difficult problems increases. Code:
https://github.com/QizhiPei/ScaleDiff.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [93] [Blueprints of Trust: AI System Cards for End to End Transparency and Governance](https://arxiv.org/abs/2509.20394)
*Huzaifa Sidhpurwala,Emily Fox,Garth Mollett,Florencio Cano Gabarda,Roman Zhukov*

Main category: cs.CY

TL;DR: 本文介绍了HASC框架，该框架通过集成AI系统安全和安全态势的全面动态记录，扩展了现有的模型卡和系统卡概念，并提出了标准化的标识符系统，包括新的AI安全危害（ASH）ID，以提高AI系统的透明度和问责制。


<details>
  <summary>Details</summary>
Motivation: 提高AI系统开发和部署的透明度和问责制。

Method: HASC通过集成AI系统安全和安全态势的全面动态记录，扩展了现有的模型卡和系统卡概念，并提出了标准化的标识符系统，包括新的AI安全危害（ASH）ID。

Result: HASC为开发者和利益相关者提供了单一、可访问的真相来源，使他们能够就AI系统的安全性做出更明智的决策。

Conclusion: HASC可以与ISO/IEC 42001:2023标准互补，提供更大的透明度和问责制。

Abstract: This paper introduces the Hazard-Aware System Card (HASC), a novel framework
designed to enhance transparency and accountability in the development and
deployment of AI systems. The HASC builds upon existing model card and system
card concepts by integrating a comprehensive, dynamic record of an AI system's
security and safety posture. The framework proposes a standardized system of
identifiers, including a novel AI Safety Hazard (ASH) ID, to complement
existing security identifiers like CVEs, allowing for clear and consistent
communication of fixed flaws. By providing a single, accessible source of
truth, the HASC empowers developers and stakeholders to make more informed
decisions about AI system safety throughout its lifecycle. Ultimately, we also
compare our proposed AI system cards with the ISO/IEC 42001:2023 standard and
discuss how they can be used to complement each other, providing greater
transparency and accountability for AI systems.

</details>


### [94] [Communication Bias in Large Language Models: A Regulatory Perspective](https://arxiv.org/abs/2509.21075)
*Adrian Kuenzler,Stefan Schmid*

Main category: cs.CY

TL;DR: 本文探讨了大型语言模型的偏见问题及其社会影响，并呼吁加强竞争和设计治理以确保公平和可信的人工智能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多应用中越来越重要，引发了对偏见、公平性和合规性的担忧。

Method: 本文回顾了偏见输出的风险及其社会影响，重点介绍了欧盟的《人工智能法案》和《数字服务法案》等框架。

Result: 本文强调了在监管之外，需要更强的竞争和设计治理来确保公平、可信的人工智能。

Conclusion: 除了持续的监管外，还需要对竞争和设计治理给予更多关注，以确保公平、可信的人工智能。

Abstract: Large language models (LLMs) are increasingly central to many applications,
raising concerns about bias, fairness, and regulatory compliance. This paper
reviews risks of biased outputs and their societal impact, focusing on
frameworks like the EU's AI Act and the Digital Services Act. We argue that
beyond constant regulation, stronger attention to competition and design
governance is needed to ensure fair, trustworthy AI. This is a preprint of the
Communications of the ACM article of the same title.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [95] [Visual Authority and the Rhetoric of Health Misinformation: A Multimodal Analysis of Social Media Videos](https://arxiv.org/abs/2509.20724)
*Mohammad Reza Zarei,Barbara Stead-Coyle,Michael Christensen,Sarah Everts,Majid Komeili*

Main category: cs.SI

TL;DR: 该研究分析了短视频平台上健康建议的可信度构建方式，发现权威信号、叙事技巧和盈利方式常常结合在一起，以增强内容的说服力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析短视频平台上的健康建议中，可信度是如何被构建的，而不是判断内容的真实性。

Method: 研究收集了来自TikTok、Instagram和YouTube的152个公开视频，并对每个视频的26个特征进行标注，包括视觉权威、演讲者属性、叙事策略和互动提示。

Result: 研究发现，自信的单个演讲者在工作室或家庭环境中占据主导地位，而临床背景很少见。权威线索如头衔、幻灯片和图表以及证书经常与说服性元素（如术语、引用、恐惧或紧迫感、对主流医学的批评和阴谋论）以及盈利方式（如销售链接和订阅呼吁）一起出现。

Conclusion: 该研究揭示了在短视频平台上，可信度是如何通过权威信号、叙事技巧和盈利方式来包装的。

Abstract: Short form video platforms are central sites for health advice, where
alternative narratives mix useful, misleading, and harmful content. Rather than
adjudicating truth, this study examines how credibility is packaged in
nutrition and supplement videos by analyzing the intersection of authority
signals, narrative techniques, and monetization. We assemble a cross platform
corpus of 152 public videos from TikTok, Instagram, and YouTube and annotate
each on 26 features spanning visual authority, presenter attributes, narrative
strategies, and engagement cues. A transparent annotation pipeline integrates
automatic speech recognition, principled frame selection, and a multimodal
model, with human verification on a stratified subsample showing strong
agreement. Descriptively, a confident single presenter in studio or home
settings dominates, and clinical contexts are rare. Analytically, authority
cues such as titles, slides and charts, and certificates frequently occur with
persuasive elements including jargon, references, fear or urgency, critiques of
mainstream medicine, and conspiracies, and with monetization including sales
links and calls to subscribe. References and science like visuals often travel
with emotive and oppositional narratives rather than signaling restraint.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [96] [Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems](https://arxiv.org/abs/2509.21143)
*Junfeng Yan,Biao Wu,Meng Fang,Ling Chen*

Main category: cs.RO

TL;DR: 本文介绍了Automotive-ENV，这是一个针对车载GUI的高保真基准和交互环境，并提出了一种地理感知的多模态代理ASURADA，以提高车载系统的安全性与适应性。


<details>
  <summary>Details</summary>
Motivation: 车载GUI面临独特的挑战：驾驶员注意力有限、严格的安全要求以及复杂的基于位置的交互模式。

Method: 我们提出了ASURADA，一种地理感知的多模态代理，它结合了GPS信息来动态调整基于位置、环境条件和区域驾驶规范的动作。

Result: 实验表明，地理感知信息显著提高了在安全相关任务上的成功率，突显了位置上下文在汽车环境中的重要性。

Conclusion: 我们将在未来进一步推动安全和适应性车载代理的发展，通过发布Automotive-ENV，包括所有任务和基准测试工具。

Abstract: Multimodal agents have demonstrated strong performance in general GUI
interactions, but their application in automotive systems has been largely
unexplored. In-vehicle GUIs present distinct challenges: drivers' limited
attention, strict safety requirements, and complex location-based interaction
patterns. To address these challenges, we introduce Automotive-ENV, the first
high-fidelity benchmark and interaction environment tailored for vehicle GUIs.
This platform defines 185 parameterized tasks spanning explicit control,
implicit intent understanding, and safety-aware tasks, and provides structured
multimodal observations with precise programmatic checks for reproducible
evaluation. Building on this benchmark, we propose ASURADA, a geo-aware
multimodal agent that integrates GPS-informed context to dynamically adjust
actions based on location, environmental conditions, and regional driving
norms. Experiments show that geo-aware information significantly improves
success on safety-aware tasks, highlighting the importance of location-based
context in automotive environments. We will release Automotive-ENV, complete
with all tasks and benchmarking tools, to further the development of safe and
adaptive in-vehicle agents.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [97] [RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows](https://arxiv.org/abs/2509.20490)
*Kai Zhang,Corey D Barrett,Jangwon Kim,Lichao Sun,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.MA

TL;DR: 本文提出了 RadAgents，这是一种用于胸部X光片解释的多智能体框架，通过结合临床先验和任务感知的多模态推理，以及集成接地和多模态检索增强来验证和解决上下文冲突，从而提高了输出的可靠性、透明度和与临床实践的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的胸部X光片（CXR）解释方法存在三个主要问题：(i) 推理通常既不具有临床可解释性也不符合指南，只是工具输出的简单聚合；(ii) 多模态证据融合不足，导致文本为主的推理缺乏视觉基础；(iii) 系统很少检测或解决跨工具不一致，并且没有原则性的验证机制。

Method: RadAgents 是一种多智能体框架，它结合了临床先验和任务感知的多模态推理。此外，还集成了接地和多模态检索增强来验证和解决上下文冲突。

Result: RadAgents 框架通过结合临床先验和任务感知的多模态推理，以及集成接地和多模态检索增强来验证和解决上下文冲突，从而产生了更可靠、透明且符合临床实践的输出。

Conclusion: RadAgents 是一种多智能体框架，能够通过结合临床先验和任务感知的多模态推理来解决胸部X光片（CXR）解释中的问题。此外，我们集成了接地和多模态检索增强来验证和解决上下文冲突，从而产生更可靠、透明且符合临床实践的输出。

Abstract: Agentic systems offer a potential path to solve complex clinical tasks
through collaboration among specialized agents, augmented by tool use and
external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation,
prevailing methods remain limited: (i) reasoning is frequently neither
clinically interpretable nor aligned with guidelines, reflecting mere
aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused,
yielding text-only rationales that are not visually grounded; and (iii) systems
rarely detect or resolve cross-tool inconsistencies and provide no principled
verification mechanisms. To bridge the above gaps, we present RadAgents, a
multi-agent framework for CXR interpretation that couples clinical priors with
task-aware multimodal reasoning. In addition, we integrate grounding and
multimodal retrieval-augmentation to verify and resolve context conflicts,
resulting in outputs that are more reliable, transparent, and consistent with
clinical practice.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [98] [Perspectra: Choosing Your Experts Enhances Critical Thinking in Multi-Agent Research Ideation](https://arxiv.org/abs/2509.20553)
*Yiren Liu,Viraj Shah,Sangho Suh,Pao Siangliulue,Tal August,Yun Huang*

Main category: cs.HC

TL;DR: 本文介绍了Perspectra，一种交互式多智能体系统，通过论坛风格的界面可视化和结构化LLM代理之间的讨论。研究发现Perspectra提高了批判性思维行为的频率和深度，并促进了跨学科回复和提案修订。


<details>
  <summary>Details</summary>
Motivation: 尽管多智能体系统（MAS）最近的进展使得信息搜索和创意生成成为可能，但用户如何有效控制、引导和批判性评估多个领域专家代理之间的协作仍研究不足。

Method: 本文介绍了Perspectra，这是一种交互式多智能体系统，通过论坛风格的界面可视化和结构化LLM代理之间的讨论，并支持@提及邀请目标代理、线程进行并行探索以及实时思维图可视化论点和理由。

Result: 在一项涉及18名参与者的内部研究中，我们比较了Perspectra与群聊基线，让他们制定研究提案。结果表明，Perspectra显著增加了批判性思维行为的频率和深度，引发了更多的跨学科回复，并导致研究提案的频繁修订。

Conclusion: 本文讨论了设计多智能体工具以通过支持用户对多智能体对抗性话语的控制来促进批判性思维的含义。

Abstract: Recent advances in multi-agent systems (MAS) enable tools for information
search and ideation by assigning personas to agents. However, how users can
effectively control, steer, and critically evaluate collaboration among
multiple domain-expert agents remains underexplored. We present Perspectra, an
interactive MAS that visualizes and structures deliberation among LLM agents
via a forum-style interface, supporting @-mention to invite targeted agents,
threading for parallel exploration, with a real-time mind map for visualizing
arguments and rationales. In a within-subjects study with 18 participants, we
compared Perspectra to a group-chat baseline as they developed research
proposals. Our findings show that Perspectra significantly increased the
frequency and depth of critical-thinking behaviors, elicited more
interdisciplinary replies, and led to more frequent proposal revisions than the
group chat condition. We discuss implications for designing multi-agent tools
that scaffold critical thinking by supporting user control over multi-agent
adversarial discourse.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [99] [On Theoretical Interpretations of Concept-Based In-Context Learning](https://arxiv.org/abs/2509.20882)
*Huaze Tang,Tianren Peng,Shao-lun Huang*

Main category: cs.IT

TL;DR: 本文研究了基于概念的 ICL 方法 (CB-ICL)，通过理论分析解释了其在少样本提示任务中的有效性，并提供了对模型预训练和提示工程的重要见解。


<details>
  <summary>Details</summary>
Motivation: 当前对 ICL 机制的理论理解有限，因此需要深入研究 CB-ICL 的原理和效果。

Method: 通过理论分析研究了基于概念的 ICL (CB-ICL) 方法，并探讨了其在少样本提示任务中的表现。

Result: 理论分析解释了 CB-ICL 在少样本提示任务中表现良好的原因，并提出了相似性度量方法，同时验证了 CB-ICL 的实用性。

Conclusion: CB-ICL 和相关理论在实际应用中具有重要价值，能够为模型预训练和提示工程提供指导。

Abstract: In-Context Learning (ICL) has emerged as an important new paradigm in natural
language processing and large language model (LLM) applications. However, the
theoretical understanding of the ICL mechanism remains limited. This paper aims
to investigate this issue by studying a particular ICL approach, called
concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on
applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs
well for predicting query labels in prompts with only a few demonstrations. In
addition, the proposed theory quantifies the knowledge that can be leveraged by
the LLMs to the prompt tasks, and leads to a similarity measure between the
prompt demonstrations and the query input, which provides important insights
and guidance for model pre-training and prompt engineering in ICL. Moreover,
the impact of the prompt demonstration size and the dimension of the LLM
embeddings in ICL are also explored based on the proposed theory. Finally,
several real-data experiments are conducted to validate the practical
usefulness of CB-ICL and the corresponding theory.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 该研究探讨了量子自然语言处理模型在自然语言推理任务中的应用，发现量子模型在参数效率和性能方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 研究量子自然语言处理模型在自然语言推理任务中的应用，比较量子、混合和经典变压器模型的性能。

Method: 使用lambeq库和DisCoCat框架构建参数化量子电路，并引入信息增益每参数（IGPP）作为评估效率的新指标。

Result: 量子模型在性能上与经典基线相当，但使用了更少的参数，并且在推理任务中表现优于随机初始化的变压器，在相关性任务中测试误差更低。

Conclusion: 量子模型在低资源、结构敏感的设置中展现出QNLP的潜力，特别是在参数效率方面具有显著优势。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [2] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 本研究提出了一种多模型融合框架，利用ChatGPT和Claude来提高胸部X光片解释的可靠性。结果显示，通过使用输出级共识方法，可以显著提高诊断准确率，从而提高AI辅助放射学诊断的可信度和临床效用。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在提高AI辅助放射学诊断的可靠性和临床效用，通过整合互补模态和使用输出级共识来减少诊断错误。

Method: 本研究提出了一种新颖的多模型融合框架，利用两个最先进的大型语言模型（LLMs），ChatGPT和Claude，以增强在CheXpert数据集上对胸部X光片解释的可靠性。通过随机选择234个放射科医生标注的研究来评估单模型性能，并使用图像仅提示进行评估。然后生成合成的临床笔记，并评估与图像和合成文本配对的50个随机选择的案例。

Result: 在单模型设置中，ChatGPT和Claude分别达到了62.8%和76.9%的诊断准确率。基于相似性的共识方法将准确率提高到77.6%。在多模态队列中，ChatGPT和Claude的性能分别提高到84%和76%，而共识准确率达到91.3%。

Conclusion: 这些发现突显了整合互补模态和使用输出级共识以提高AI辅助放射学诊断的可信度和临床效用的实用性，提供了一条减少诊断错误的实用路径，计算开销最小。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [3] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 研究引入了CorrectBench基准，用于评估自修正策略的有效性，发现自修正方法能提高准确性，混合策略能进一步提升性能，但会降低效率，同时指出需要优化推理能力和操作效率之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 虽然已经提出了各种自修正方法，但对这些方法的全面评估仍然很少被探索，而大语言模型是否能够真正自我修正是一个重要的兴趣和关注点。

Method: 引入了CorrectBench基准，用于评估自修正策略的有效性，包括内在、外部和微调方法，在三个任务：常识推理、数学推理和代码生成上进行测试。

Result: 1) 自修正方法可以提高准确性，尤其是在复杂的推理任务中；2) 混合不同的自修正策略可以进一步提高性能，尽管会降低效率；3) 推理大语言模型（如DeepSeek-R1）在额外的自修正方法下优化有限，并且有较高的时间成本。有趣的是，一个相对简单的思维链（CoT）基线表现出具有竞争力的准确性和效率。

Conclusion: 研究结果强调了自我修正在提高大语言模型推理性能方面的潜力，同时突出了改进其效率的持续挑战。因此，我们倡导进一步研究优化推理能力和操作效率之间的平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [4] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: 本文介绍了EvolveR框架，旨在通过闭环经验生命周期使代理自我改进，从而在复杂任务中实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）代理在工具使用方面表现出色，但缺乏系统地从自身经验中学习的关键能力。虽然现有框架主要关注缓解外部知识差距，但未能解决一个更根本的限制：无法迭代优化解决问题的策略。

Method: 我们引入了EvolveR框架，该框架旨在通过完整的闭环经验生命周期使代理自我改进。该生命周期包括两个关键阶段：(1) 离线自我蒸馏，其中将代理的交互轨迹合成结构化的抽象、可重用的战略原则库；(2) 在线交互，其中代理与任务互动并主动检索提炼出的原则来指导其决策，积累多样化的行为轨迹。此循环采用策略强化机制，根据其性能迭代更新代理。

Result: 我们在复杂的多跳问答基准测试中展示了EvolveR的有效性，其表现优于强大的代理基线。

Conclusion: 我们的工作为代理提供了一个全面的蓝图，使它们不仅从外部数据中学习，还从自身行为的结果中学习，为更自主和持续改进的系统铺平了道路。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [5] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 研究分析了不同提示策略与大型语言模型在自动化文献筛选中的交互效果，并提供了实用的部署建议。


<details>
  <summary>Details</summary>
Motivation: 研究旨在量化提示策略如何与大型语言模型（LLMs）相互作用，以自动化系统文献综述（SLRs）的筛选阶段。

Method: 研究评估了六种LLMs在五种提示类型下的表现，使用准确率、精确率、召回率和F1分数进行分析。

Result: 结果表明，提示-模型交互效应显著：CoT-few-shot在精确率-召回率平衡方面最可靠；zero-shot在高灵敏度传递中最大化召回率；自我反思表现不佳，由于过度包含和模型间的不稳定性。GPT-4o和DeepSeek表现出稳健的整体性能，而GPT-4o-mini在较低成本下表现竞争力。

Conclusion: 研究显示，LLMs在文献筛选中的潜力不均但有希望。通过系统分析提示-模型交互，提供了任务适应性LLM部署的比较基准和实用指导。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [6] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出了一种合成测试平台，用于研究语言模型中上下文多样性和事实泛化之间的关系，并揭示了优化瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，语言模型的交互变异性（如事实关联的改写）对其泛化能力至关重要，但缺乏系统分析。本文旨在提供一种方法来研究这些影响。

Method: 本文引入了一个灵活的合成测试平台，结合了统计流和抽象事实流，以精细控制它们的交互。通过受控实验和对模型组件的干预，分析了不同泛化方面的表现。

Result: 研究发现，更高的上下文多样性会延迟分布内（ID）的事实准确性，但其对分布外（OOD）事实泛化的影响力取决于上下文结构。在某些情况下，OOD性能与ID遵循相同趋势，而在其他情况下，多样性对于非平凡的事实回忆至关重要。

Conclusion: 本文展示了上下文设计和多样性水平之间的相互作用如何影响不同的泛化方面，并通过一系列控制干预揭示了OOD失败的优化瓶颈，强调了嵌入和解码层的重要性。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [7] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本文通过大规模数据分析，探讨了公众对生成式AI的信任和不信任，揭示了其随时间的变化及影响因素。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（GenAI）在日常生活中的普及，理解公众对其的信任变得至关重要，但目前缺乏计算、大规模和纵向的方法来衡量对GenAI和大语言模型（LLMs）的信任和不信任。

Method: 本文使用了一个跨年的Reddit数据集（2022-2025），涵盖39个子版块和197,618篇帖子，结合众包标注和分类模型进行分析。

Result: 研究发现，信任和不信任在时间上几乎平衡，但在重大模型发布时有变化。技术性能和易用性是主要维度，而个人经验是最常见的影响态度的原因。不同群体（如专家、伦理学家、普通用户）也表现出不同的模式。

Conclusion: 本文提供了大规模信任分析的方法论框架，并揭示了公众对GenAI看法的演变。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [8] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 本文介绍了EgMM-Corpus，一个专注于埃及文化的多模态数据集，用于评估和训练视觉-语言模型。通过人工验证确保数据集的文化真实性和多模态一致性，并展示了CLIP在该数据集上的表现，突显了现有视觉-语言模型中的文化偏差。


<details>
  <summary>Details</summary>
Motivation: 由于在人工智能方面的最新进展，多模态文化多样性数据集仍然有限，特别是在中东和非洲地区。因此，本文介绍了EgMM-Corpus，这是一个专注于埃及文化的多模态数据集。

Method: 通过设计和运行新的数据收集管道，收集了超过3000张图像，涵盖了313个概念，包括地标、食物和民间传说，并对数据集中的每个条目进行了人工验证以确保文化真实性和多模态一致性。

Result: 在EgMM-Corpus上，对比语言-图像预训练（CLIP）的零样本性能达到了21.2%的Top-1准确率和36.4%的Top-5准确率。

Conclusion: EgMM-Corpus 旨在作为开发文化意识模型的基准，强调了大规模视觉-语言模型中存在的文化偏差。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [9] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 本文分析了语言模型对语法的理解，通过理论分析和实证验证，探讨了字符串概率与语法知识的关系，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LMs）对语法的了解仍然是一个有争议的问题，这可能对语言理论产生重大影响。然而，由于概率和语法性在语言学中是不同的概念，因此不清楚字符串概率能揭示LM的底层语法知识多少。

Method: 我们基于对语料库数据生成过程的简单假设，对语法、意义和字符串概率之间的关系进行了理论分析，并通过280,000个英语和中文句子对进行了实证验证。

Result: 我们的框架做出了三个预测，并通过实证验证：(1) 最小对中的字符串概率相关性；(2) 模型和人类在最小对中的差异相关性；(3) 语法和不语法字符串在概率空间中的分离效果差。

Conclusion: 我们的分析为使用概率来了解语言模型的结构知识提供了理论基础，并指出了未来在语言模型语法评估方面的研究方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [10] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 本文提出两种方法（多元解码和模型引导）以在低资源环境下增强语言模型的多元对齐，实验证明模型引导能有效减少假阳性并改善人类价值观的分布对齐。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型对社会的影响越来越大，确保它们能够反映人类价值观的多样性变得尤为重要。然而，现代语言模型最常见的训练范式往往假设每个查询都有一个最优答案，导致通用响应和对齐不佳。

Method: 我们提出了两种方法：多元解码和模型引导，以在低资源环境下增强语言模型的多元对齐。

Result: 我们实证证明，仅使用50个标注样本，模型引导就能持续优于零样本和少样本基线。我们的方法减少了仇恨言论检测和虚假信息检测等高风险任务中的假阳性，并改善了GlobalOpinionQA中的人类价值观分布对齐。

Conclusion: 我们的工作强调了多样性的重要性，并展示了语言模型如何适应考虑细微的视角。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [11] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Profile-to-PEFT的可扩展框架，通过使用超网络将用户资料直接映射到适配器参数，实现了高效的LLM个性化，具有良好的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调（PEFT）方法，如“每个用户一个PEFT”（OPPU）范式，需要为每个用户训练一个单独的适配器，这在计算上很昂贵且不适用于实时更新。

Method: 引入了一种名为Profile-to-PEFT的可扩展框架，该框架使用一个端到端训练的超网络，将用户的编码资料直接映射到完整的适配器参数集（例如LoRA），从而在部署时无需每个用户单独训练。

Result: 实验结果表明，该方法在部署时使用的计算资源显著减少，同时优于基于提示的个性化和OPPU。框架对分布外用户表现出强大的泛化能力，并在不同用户活动水平和不同的嵌入骨干网络中保持鲁棒性。

Conclusion: 提出的Profile-to-PEFT框架能够实现高效、可扩展且适应性强的LLM个性化，适用于大规模应用。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [12] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: This paper evaluates the awareness, generalizability, and alignment of LLMs post-trained with different methods, finding that RL-trained models are more aware and generalize better but have weaker alignment between reasoning and outputs.


<details>
  <summary>Details</summary>
Motivation: Recent advances in post-training techniques have improved LLMs' capabilities for complex tasks, raising the question of whether these models are aware of what they learn and think.

Method: We define three core competencies: awareness of learned latent policies, generalization of these policies across domains, and alignment between internal reasoning traces and final outputs. We evaluate these abilities on several tasks requiring distinct policies and compare models post-trained via SFT, DPO, and GRPO.

Result: RL-trained models demonstrate greater awareness and generalizability than SFT models, but often show weak alignment between reasoning traces and outputs, particularly in GRPO-trained models.

Conclusion: RL-trained models show greater awareness of their learned behaviors and stronger generalizability to novel tasks compared to SFT models, but often exhibit weak alignment between reasoning traces and final outputs, especially in GRPO-trained models.

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [13] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLM）生成针对疫苗错误信息的有力反驳的能力，并发现结合标签描述和结构化微调可以提高反驳效果。


<details>
  <summary>Details</summary>
Motivation: 在公共卫生日益受到社交媒体上信息共享影响的时代，打击疫苗怀疑论和错误信息已成为关键的社会目标。错误的疫苗叙事广泛传播，造成了高免疫率的障碍，并削弱了对健康建议的信任。虽然检测错误信息的努力取得了显著进展，但生成实时定制的反驳来驳斥这些说法仍然是一个研究不足的领域。

Method: 我们实验了各种提示策略和微调方法，以优化反驳生成，并训练分类器将反疫苗推文分类为多标签类别，如对疫苗效果的担忧、副作用和政治影响，从而实现更上下文感知的反驳。

Result: 通过人类判断、基于LLM的评估和自动指标进行的评估显示，这些方法之间有很强的一致性。

Conclusion: 我们的研究结果表明，结合标签描述和结构化微调可以增强反驳的有效性，为大规模缓解疫苗谣言提供了有希望的方法。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [14] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 本文提出了一种基于自回归结构预测框架的端到端论证结构预测方法（AASP），在多个基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 由于论证挖掘任务中涉及的推理固有的复杂性，建模ACs和ARs之间的依赖关系具有挑战性。大多数最近的方法通过扁平化论证结构来制定这个任务，而本文则采用端到端的方式联合制定论证挖掘的关键任务。

Method: AASP框架基于自回归结构预测框架，通过条件预训练语言模型将论证结构建模为受约束的预定义动作集，这些动作以自回归的方式逐步构建论证结构，以高效地捕捉论证推理的流程。

Result: AASP框架在三个标准AM基准测试中取得了最先进的结果，在两个基准测试中所有AM任务都表现优异，并在一个基准测试中也表现出色。

Conclusion: AASP框架在三个标准AM基准测试中进行了广泛的实验，结果表明它在两个基准测试的所有AM任务中达到了最先进的结果，并在一个基准测试中表现出了强劲的结果。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [15] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 研究提出了一种轻量级方法，通过线性变换和引导向量提升大型语言模型在心理健康领域的性能，结果显示这种方法在两个任务中都有效。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在心理健康领域有潜力，但较小规模的模型在特定领域应用中仍难以达到最佳性能。因此，需要一种高效的方法来提升模型的表现。

Method: 研究提出了一种成本效益高但强大的方法，通过在特定层的激活上应用线性变换，并利用引导向量来指导模型的输出，从而提高大型语言模型的心理健康评估能力。

Result: 该方法在两个不同的任务中取得了改进的结果：(1) 识别Reddit帖子是否有助于检测抑郁症状的存在或缺失；(2) 根据用户的Reddit帖子历史完成标准化的心理筛查问卷。

Conclusion: 研究结果表明，转向机制是一种计算效率高的工具，可以用于大型语言模型在心理健康领域的适应。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [16] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: 研究提出了新的基准测试来评估AI在道德推理方面的能力，并发现现有基准无法预测AI在道德推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在决策中的作用增加，需要理解它们如何做出决策，特别是在道德困境中。

Method: 提出了MoReBench和MoReBench-Theory两个基准测试，用于评估AI在道德推理方面的能力。

Result: 模型在道德推理任务上的表现无法通过数学、代码和科学推理任务的基准预测，且对特定道德框架有偏见。

Conclusion: 这些基准测试推动了过程导向的推理评估，有助于实现更安全和更透明的AI。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [17] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: ATA is a neuro-symbolic approach that decouples tasks into offline knowledge ingestion and online task processing, offering a trustworthy and reliable architecture for autonomous agents.


<details>
  <summary>Details</summary>
Motivation: The deployment of Large Language Models (LLMs) in high-stakes domains is hindered by their limitations in trustworthiness, including hallucinations, instability, and lack of transparency.

Method: ATA is a neuro-symbolic approach that decouples tasks into two phases: offline knowledge ingestion and online task processing. During knowledge ingestion, an LLM translates an informal problem specification into a formal, symbolic knowledge base. In the task processing phase, each input is encoded into the same formal language, and a symbolic decision engine uses this encoded input along with the formal knowledge base to derive a reliable result.

Result: A concrete implementation of ATA is competitive with state-of-the-art end-to-end reasoning models in a fully automated setup while maintaining trustworthiness. With a human-verified and corrected knowledge base, ATA significantly outperforms even larger models, while exhibiting perfect determinism, enhanced stability against input perturbations, and inherent immunity to prompt injection attacks.

Conclusion: ATA offers a practical and controllable architecture for building the next generation of transparent, auditable, and reliable autonomous agents.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [18] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 本文探索了Whisper在L2口语语言评估中的潜力，并展示了其在没有任务特定微调的情况下编码语音的序数熟练度模式和语义方面的能力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索Whisper在L2口语语言评估中的未开发潜力，而不是仅仅对外部分析Whisper生成的转录文本。

Method: 本文通过从Whisper的隐藏表示中提取声学和语言特征，探索了Whisper在L2口语语言评估中的未开发潜力。此外，仅在Whisper的中间和最终输出之上训练轻量级分类器，我们的方法在GEPT图片描述数据集上表现出色，优于现有的最先进的基线方法。

Result: 本文的方法在GEPT图片描述数据集上表现出色，优于现有的最先进的基线方法。此外，通过结合图像和文本提示信息作为辅助相关性线索，我们展示了额外的性能提升。

Conclusion: 本文展示了Whisper在L2口语语言评估中的潜力，即使没有任务特定的微调，该模型也内在地编码了语音的序数熟练度模式和语义方面，表明其作为SLA和其他口语理解任务的强大基础的潜力。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [19] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: 本文提出了一种名为FrugalPrompt的新型提示压缩框架，通过保留最语义重要的标记来减少冗余的低效标记。我们在四个NLP任务中评估了该方法，发现对于前三个任务，提示减少20%仅导致任务性能略有下降，而数学推理任务的性能则显著下降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的出色性能很大程度上归功于广泛的输入上下文，但这种冗长性增加了金钱成本、碳足迹和推理时间延迟。我们通过引入FrugalPrompt来解决这一低效问题，该框架保留最语义重要的标记，以减少冗余的低效标记。

Method: 我们引入了FrugalPrompt，这是一种新的提示压缩框架，通过保留最语义重要的标记来减少冗余的低效标记。我们利用两种最先进的标记归因方法（GlobEnc和DecompX）为输入序列中的每个标记分配显著性分数，并保留原始顺序中的前k%标记，从而得到一个稀疏的精简提示。

Result: 在四个NLP任务中评估了该方法：情感分析、常识QA、摘要和数学推理。对于前三个任务，提示减少20%仅导致任务性能略有下降，表明现代LLMs可以从高显著性线索中重建省略的上下文。然而，在数学推理任务中，性能急剧下降，反映出对完整标记连续性的更强依赖。进一步分析显示了不对称的性能模式，这可能表明任务污染效应的存在，其中模型可能在传统NLP任务中依赖于预训练暴露的浅层记忆模式。

Conclusion: 我们的工作有助于更深入地理解LLM在性能-效率权衡中的行为，并明确了哪些任务可以容忍上下文稀疏性，哪些任务需要完整的上下文。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [20] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector是一种高效的Best-of-N框架，通过利用LLM的隐藏状态进行过程级评分，实现了性能提升并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的外部TTS方法存在计算开销高和LLM内在潜在表示利用不足的问题。

Method: TrajSelector利用采样器LLM中的隐藏状态进行过程级评分，并使用一个轻量级验证器评估步骤级轨迹的质量，然后聚合这些分数以确定最佳推理轨迹。

Result: TrajSelector在Best-of-32设置下，比多数投票提高了4.61%的准确率，并且比现有的过程奖励模型提高了4.31%到12.21%。

Conclusion: TrajSelector在多个基准测试中表现出色，能够以较低的推理成本超越现有的过程奖励模型。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [21] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN is a novel framework that integrates curriculum reinforcement learning with multimodal large language models to enhance reasoning and cognitive capabilities for advertisement video violation detection, achieving superior performance and practical applicability.


<details>
  <summary>Details</summary>
Motivation: Existing methods for advertisement video violation detection struggle with precise temporal grounding, noisy annotations, and limited generalization. There is a need for a framework that can enhance reasoning and cognitive capabilities for violation detection.

Method: RAVEN integrates curriculum reinforcement learning with multimodal large language models (MLLMs) to enhance reasoning and cognitive capabilities for violation detection. It employs a progressive training strategy, combining precisely and coarsely annotated data, and leverages Group Relative Policy Optimization (GRPO) to develop emergent reasoning abilities without explicit reasoning annotations.

Result: Experiments on industrial datasets and public benchmarks show that RAVEN achieves superior performances in violation category accuracy and temporal interval localization. Online A/B testing further validates its practical applicability, with significant improvements in precision and recall.

Conclusion: RAVEN demonstrates strong generalization and mitigates the catastrophic forgetting issue associated with supervised fine-tuning, making it a practical solution for online Ad services.

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [22] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 本文研究了自然语言推理数据集中注释者之间的差异，发现基于推理的解释比标签协议更能反映自由文本解释的语义相似性。


<details>
  <summary>Details</summary>
Motivation: 自然语言推理数据集通常表现出人类标签的变化。为了更好地理解这些变化，基于解释的方法分析了注释者决策背后的推理。然而，以前的工作主要集中在标签内的变化上，即注释者在最终NLI标签上达成一致但提供不同解释的情况。本文拓宽了范围，研究了注释者不仅在推理类型上，而且在标记步骤上的分歧。

Method: 我们应用LiTEx到两个NLI英语数据集中，并从多个方面分析了注释差异：NLI标签协议、解释相似性和分类协议，还考虑了注释者的选择偏差这一额外因素。

Result: 我们观察到注释者在标签上存在分歧，但提供了高度相似的解释，这表明表面上的分歧可能掩盖了解释上的潜在一致性。此外，我们的分析揭示了个体在解释策略和标签选择上的偏好。

Conclusion: 我们的研究结果强调了基于推理的解释的丰富性，并提醒人们在将标签视为真实情况时需要谨慎。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [23] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 本文提出了一种简单而有效的机制，让大型语言模型代理在缺乏信心时能够识别并退出，从而提高安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型代理在具有现实世界后果的复杂环境中越来越普遍，其安全性变得至关重要。然而，多轮代理场景中的不确定性和模糊性会累积，导致超出传统文本生成失败的严重或灾难性风险。因此，我们需要一种有效的安全机制来识别和退出缺乏信心的情况。

Method: 我们使用ToolEmu框架对12个最先进的LLM进行了系统评估，以研究退出行为。

Result: 实验结果表明，通过显式指令提示退出的代理在所有模型上的安全性平均提高了+0.39（在0-3的尺度上），对于专有模型则提高了+0.64，同时保持了-0.03的可忽略的帮助性下降。

Conclusion: 我们的分析表明，添加明确的退出指令作为一种有效的安全机制，可以立即部署在现有的代理系统中，并确立退出作为高风险应用中自主代理的第一道防线机制。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [24] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 本文提出了一种基于背包问题的自动化代理系统组合框架，能够在动态和不确定环境中高效地选择和组装代理组件，实验证明该方法在多种情况下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于静态语义检索方法进行工具或代理发现，但有效重用和组合现有组件仍然具有挑战性，因为能力描述不完整以及检索方法的局限性。组件选择的问题在于决策不是基于能力、成本和实时效用。

Method: 我们引入了一个受背包问题启发的结构化、自动化的代理系统组合框架。该框架使组成代理能够系统地识别、选择和组装一组最优的代理组件，同时考虑性能、预算约束和兼容性。

Result: 在五个基准数据集上的实证评估表明，我们的基于在线背包的组成代理始终位于帕累托前沿，在显著降低组件成本的情况下比基线方法取得了更高的成功率。在单代理设置中，与检索基线相比，成功率提高了31.6%。在多代理系统中，当从包含100多个代理的代理库存中选择代理时，成功率从37%提高到87%。

Conclusion: 我们的方法在不同领域和预算约束下表现出强大的适应性，证实了其有效性。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [25] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 本文介绍了ReviewGuard，一个用于检测和分类缺陷同行评审的自动化系统，通过LLM驱动的方法提高评审质量。


<details>
  <summary>Details</summary>
Motivation: 由于同行评审面临来自人类专家和AI系统的缺陷评审的威胁，需要一种自动化系统来检测和分类这些缺陷评审。

Method: ReviewGuard采用了一个四阶段的LLM驱动框架，包括数据收集、审查类型注释、数据增强和模型微调。

Result: 与充分的评审相比，缺陷评审表现出较低的评分、较高的自报告置信度、较少的结构复杂性和更高的负面情绪比例。混合使用合成和真实评审数据在检测模型评估中显著提高了召回率和F1分数。

Conclusion: 本文提出了一个用于检测和分类缺陷评审的自动化系统ReviewGuard，提供了关于AI治理和人机协作的见解，以维护学术诚信。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [26] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 本研究通过测量大型语言模型在不同文化背景下的内部激活路径重叠，揭示了语言和文化对模型表现的影响。结果显示，语言特定模式对模型内部表示有显著影响，而文化相似性并不一定导致内部表示的一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种文化背景下得到越来越广泛的应用，因此准确的文化理解变得至关重要。以往的评估主要集中在输出层面的表现，掩盖了导致响应差异的因素，而使用电路分析的研究覆盖的语言较少，并且很少关注文化。

Method: 通过测量在两种条件下回答语义等价问题时的激活路径重叠来追踪大型语言模型（LLMs）内部的文化理解机制：改变目标国家而固定问题语言，以及改变问题语言而固定国家。同时，使用同语言国家对来区分语言和文化方面。

Result: 研究结果表明，内部路径在同语言、跨国家的问题上重叠更多，而跨语言、同国家的问题上重叠较少，这表明存在强烈的语言特定模式。值得注意的是，韩国和朝鲜的配对显示出低重叠和高变异性，这表明语言相似性并不保证内部表示的一致性。

Conclusion: 研究结果表明，内部路径在同语言、跨国家的问题上重叠更多，而跨语言、同国家的问题上重叠较少，这表明存在强烈的语言特定模式。值得注意的是，韩国和朝鲜的配对显示出低重叠和高变异性，这表明语言相似性并不保证内部表示的一致性。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [27] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: SHALLOW是一个用于评估自动语音识别系统中幻觉现象的新基准框架，能够捕捉细粒度错误模式并提供模型改进的反馈。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标主要集中在基于错误的指标上，无法区分语音错误和幻觉，因此需要新的评估框架来有效识别和评估生成幻觉内容的模型。

Method: SHALLOW是第一个系统分类和量化ASR中幻觉现象的基准框架，沿四个互补轴（词汇、语音、形态和语义）进行分类，并在每个类别中定义目标指标以产生可解释的模型行为轮廓。

Result: 通过在各种架构和语音领域进行评估，发现SHALLOW指标在识别质量高（即低WER）时与词错误率（WER）强烈相关，但随着WER的增加，这种相关性显著减弱。SHALLOW能够捕捉到WER无法区分的细粒度错误模式。

Conclusion: SHALLOW框架能够捕捉WER无法区分的细粒度错误模式，并支持对模型弱点的具体诊断，为模型改进提供反馈，超越了总体错误率所能提供的信息。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [28] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 本研究提出了一种针对乌尔都语的AI生成文本检测框架，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于乌尔都语等语言缺乏检测AI生成文本的工具，因此需要一种专门针对乌尔都语的AI生成文本检测框架。

Method: 我们提出了一个针对乌尔都语的AI生成文本检测框架，构建了一个包含1800个人工撰写的和1800个AI生成文本的平衡数据集，并对字符和单词计数、词汇丰富度（类型标记比）和N-gram模式进行了详细的语言学和统计分析。三种最先进的多语言Transformer模型在该数据集上进行了微调。

Result: mDeBERTa-v3-base在测试集上取得了最高的性能，F1分数为91.29，准确率为91.26%。

Conclusion: 本研究为乌尔都语社区对抗虚假信息和学术不端行为做出了贡献，并推动了低资源语言NLP工具的开发。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [29] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于微调大型语言模型的短语结构分析方法，并展示了其高准确性。


<details>
  <summary>Details</summary>
Motivation: 探索一种新的基于机器学习的短语结构分析方法，以扩展MiSintaxis工具的功能。

Method: 通过微调大型语言模型（LLMs）将输入句子翻译成对应的句法结构，以扩展MiSintaxis工具的功能。

Result: 结果表明该方法在短语结构分析中具有高准确性。

Conclusion: 该方法展示了在短语结构分析中的高准确性，并突显了该方法的潜力。

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [30] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: DiMo是一种基于多代理协作的框架，通过结构化辩论提高大型语言模型的性能和可解释性，并能应用于网络语料库和知识图谱。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然表现强大，但通常缺乏可解释的推理。因此，需要一种方法来提高其性能和可解释性，同时使其能够处理网络语料库和知识图谱。

Method: DiMo框架通过让四个具有不同推理范式的LLM代理进行迭代辩论，挑战并改进初始回答，从而产生更稳健的结论和明确的推理链。

Result: DiMo在六个基准测试中提高了准确性，尤其是在数学方面取得了最大的提升。它能够生成语义类型化、带有URL注释的证据链，用于解释和用户友好交互。

Conclusion: DiMo作为一种语义感知、基于网络的多代理框架，能够通过模拟四个专门的LLM代理之间的结构化辩论来增强性能和可解释性。它在多个基准测试中表现出更高的准确性，并且可以应用于网络语料库和知识图谱，结合检索增强推理与结构化论证，供下游系统检查和重用。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [31] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [32] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 本文介绍了一个框架TUuD，用于评估大型语言模型如何在参考点动态变化的情况下理解时间事件和事件间关系。结果显示LLMs在一定程度上能够适应这种变化，但其能力受到时间距离和参考框架变化的影响。


<details>
  <summary>Details</summary>
Motivation: 理解时间对于人类认知至关重要，而大型语言模型在处理时间相关问题上仍有局限。

Method: 通过让LLMs评估当前时刻与目标事件之间的相似性来评估它们对时间事件和事件间关系的理解。

Result: 四个评估的LLMs显示出对deictic t-FoR的可测量适应性，相似度评分在当前时刻达到峰值，并向过去和未来事件下降。

Conclusion: 虽然LLMs表现出部分类似人类的时空认知，但它们的时空推理仍然对参考框架的变化和时间距离敏感。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [33] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: This paper explores the impact of chain-of-thought rationales on natural language understanding tasks. It constructs a dataset with rationales and develops methods to augment training. Results show that while most methods underperform, some achieve improvements, and models trained with rationales show significant gains on unseen tasks.


<details>
  <summary>Details</summary>
Motivation: Most work focuses on the role of rationales in reasoning tasks, overlooking their potential impact on other important tasks like natural language understanding (NLU) tasks. We raise the question: Can rationales similarly benefit NLU tasks?

Method: We construct NLURC, a comprehensive and high-quality NLU dataset collection with rationales, and develop various rationale-augmented methods.

Result: CoT inference shifts from hindering NLU performance to surpassing direct label prediction as model size grows, indicating a positive correlation. Most rationale-augmented training methods perform worse than label-only training, with one specially designed method consistently achieving improvements.

Conclusion: LLMs trained with rationales achieve significant performance gains on unseen NLU tasks, rivaling models ten times their size, while delivering interpretability on par with commercial LLMs.

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [34] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文综述了2014年至2025年心血管病学中自然语言处理（NLP）的研究进展，分析了NLP在该领域的应用、多样性以及方法的演变趋势。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病在全球健康和福祉中具有重要影响，而信息分散在各种文本数据中。自然语言处理技术被广泛采用，以分析这些非结构化数据，从而帮助医疗专业人员获得更深入的见解，推动心血管疾病的诊断、治疗和预防方法的革新。

Method: 通过查询六个文献数据库，筛选出265篇相关文章，并从多个维度进行分析，包括NLP范式类型、心血管病相关任务类型、心血管疾病类型和数据来源类型。此外，还进行了时间分析以展示NLP方法在过去十年的变化趋势。

Result: 分析揭示了各个维度上的显著多样性，表明NLP研究在心血管病学领域的广泛性。时间分析展示了过去十年中NLP方法的演变和变化趋势。

Conclusion: 本文综述了2014年至2025年间心血管病学中的自然语言处理（NLP）研究，提供了对NLP在该领域应用的详细概述。分析显示，NLP研究在多个维度上表现出显著的多样性，并展示了过去十年中NLP方法的演变和趋势。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [35] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [36] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 本研究分析了诗歌中的空白使用情况，比较了出版诗歌、LLM生成诗歌和未发表诗歌，并探讨了空白在不同时期、诗歌形式和数据源中的差异，以及文本处理方法对空白表示的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管诗歌作为长期艺术形式和LLM生成任务很受欢迎，但NLP社区对空白的关注不足。

Method: 使用来自Poetry Foundation的19k英语诗歌语料库，调查4k诗人如何在作品中使用空白。释放了2.8k个保留格式的公共领域诗歌子集，以促进进一步研究。将出版诗歌中的空白使用与51k个LLM生成的诗歌和12k个在线社区未发表的诗歌进行比较。还探索了不同时期、诗歌形式和数据源中的空白使用。

Result: 研究发现，不同的文本处理方法可能导致诗歌数据中空白的表示显著不同，并发现了空白在不同时期、诗歌形式和数据源中的使用差异。

Conclusion: 研究发现，不同的文本处理方法可能导致诗歌数据中空白的表示显著不同，这促使我们使用这些诗歌和空白模式来讨论用于构建预训练数据集的处理策略的影响。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [37] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 本文介绍了Beacon基准，用于测量大型语言模型中的谄媚偏见，并提出了干预措施以调节这些偏见，从而揭示对齐的内部几何结构。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型内部存在一种结构上的权衡，即真实性与奉承谄媚之间的权衡，这源于将帮助性与礼貌顺从混为一谈的奖励优化。这种潜在的偏见（称为谄媚）表现为对用户同意的偏好，而不是原则性推理。

Method: 引入了Beacon，这是一个单次强制选择基准，可以独立于对话上下文隔离这种偏见，从而精确测量事实准确性与服从偏见之间的紧张关系。还提出了提示级和激活级干预措施，以相反的方向调节这些偏见。

Result: 评估十二个最先进的模型显示，谄媚分解为稳定的语言和情感子偏见，每个都随着模型容量的增长而增长。

Conclusion: Beacon重新定义了谄媚作为可测量的规范性误泛化形式，为研究和减轻大规模生成系统中的对齐漂移提供了可重复的基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [38] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种通过玩与学进行步骤级策略优化的方法（SCO-PAL），并发现自对弈是提高对抗环境中战略推理最有效的方式。在六种对抗性游戏中，使用SCO-PAL与自对弈使对GPT-4的胜率达到了54.76%，比基线提高了约30%。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理在动态对抗游戏中由于战略推理不足而遇到困难，因此需要一种无需依赖昂贵专家标记数据的自动学习方法。

Method: 提出了一种通过玩与学（Play-And-Learn）进行步骤级策略优化的方法（SCO-PAL），并利用该方法详细分析了对手选择。

Result: 通过设置不同级别的对手，发现自对弈是最有效提高此类对抗环境中战略推理的方法。

Conclusion: 通过使用SCO-PAL和自对弈，我们在六种对抗性游戏中对GPT-4的胜率达到了54.76%，比基线提高了约30%。

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [39] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: This paper introduces LC-Eval, a bilingual, multi-task benchmark for evaluating long-context understanding in LLMs. It includes four challenging tasks and datasets in both Arabic and English, showing that even high-performing models face difficulties.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in Large Language Models (LLMs) have demonstrated sophisticated capabilities, including the ability to process and comprehend extended contexts. These emergent capabilities necessitate rigorous evaluation methods to effectively assess their performance in long-context understanding.

Method: The paper introduces LC-Eval, a bilingual, multi-task evaluation benchmark designed to evaluate long-context understanding in English and Arabic. It includes four novel tasks: multi-document question answering, bilingual question answering, claim verification within a paragraph, and multiple-choice questions based on long contexts.

Result: Evaluations were conducted on both open-weight and closed LLMs, with results indicating that LC-Eval presents significant challenges. Even high-performing models, such as GPT-4o, struggled with certain tasks.

Conclusion: LC-Eval presents significant challenges for LLMs, even for high-performing models like GPT-4o, highlighting the complexity and rigor of the benchmark.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [40] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: 本文提出了一种名为MOSAIC的多阶段框架，用于将句子嵌入模型适应到特定领域，通过结合遮罩语言建模和对比目标，提高了领域相关表示的学习效果，并在多个领域实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 我们旨在解决将大规模通用领域句子嵌入模型适应到专业领域的问题。

Method: 我们引入了MOSAIC（带选择性适应的遮罩目标对比学习），这是一个多阶段框架，用于句子嵌入模型的领域适应，结合了联合领域特定的遮罩监督。通过在一个统一的训练流程中联合优化遮罩语言建模（MLM）和对比目标，我们的方法能够有效地学习领域相关的表示，同时保持原始模型的鲁棒语义区分特性。

Result: 我们在高资源和低资源领域验证了我们的方法，结果表明在NDCG@10上取得了高达13.4%的提升。

Conclusion: 我们的方法在高资源和低资源领域都得到了验证，并且在NDCG@10上超过了强大的通用领域基线，提升了高达13.4%。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [41] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: 研究分析了大型语言模型在实体比较任务中的行为，发现它们常常依赖启发式方法而非真实知识，而较大的模型在数值知识更可靠时能更好地利用它。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）在知识推理任务中是依赖真实知识还是表面启发式方法仍然是一个挑战。

Method: 通过实体比较任务来研究模型如何依赖真实知识还是表面启发式方法，分析模型在数值属性上的预测行为，并识别出三种启发式偏差：实体流行度、提及顺序和语义共现。

Result: 尽管LLMs拥有足够的数值知识来正确回答问题，但它们经常做出与该知识相矛盾的预测。研究发现了三种影响模型预测的启发式偏差，并发现较大的模型在数值知识更可靠时会优先使用它。

Conclusion: 研究发现，较大的模型（32B参数）在数值知识更可靠时会选择性地依赖它，而较小的模型（7-8B参数）则没有这种区分，这解释了为什么较大的模型即使在较小模型拥有更准确知识的情况下也能表现更好。链式思维提示促使所有模型在所有模型规模下都使用数值特征。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [42] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的两阶段检索和重新排序框架，用于跨流派作者身份识别。通过针对性的数据整理策略，该方法在两个基准测试中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 跨流派AA系统必须避免依赖主题线索，而是学习识别与文本主题（流派/领域/主题）无关的作者特定语言模式。然而，信息检索领域常用的检索和重新排序策略在跨流派AA中并不适用，导致表现不佳。

Method: 我们引入了一个两阶段的检索和重新排序框架，该框架微调了大型语言模型（LLMs）以进行跨流派作者身份识别。为了重新排序器，我们展示了在信息检索（IR）中常用的训练策略与跨流派AA本质上不一致，导致表现不佳。为了解决这个问题，我们引入了一种针对性的数据整理策略，使重新排序器能够有效地学习作者区分信号。

Result: 通过我们的LLM-based检索和重新排序管道，我们在HIATUS的HRS1和HRS2跨流派AA基准测试中相对于之前的最先进水平取得了22.3和34.4绝对Success@8点的显著提升。

Conclusion: 通过我们的基于LLM的检索和重新排序管道，我们在HIATUS的HRS1和HRS2跨流派AA基准测试中相对于之前的最先进水平取得了22.3和34.4绝对Success@8点的显著提升。

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [43] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: 本文介绍了CoRUS框架，用于模拟基于角色的问题，并展示了角色如何影响语言模型的响应。


<details>
  <summary>Details</summary>
Motivation: 大多数评估忽略了提问者的身份，而在像阿片类药物使用障碍这样的污名化领域中，考虑用户背景对于提供可访问、无污名的回应至关重要。

Method: 我们提出了CoRUS框架，该框架基于角色理论和在线OUD恢复社区的帖子，构建了提问者角色的分类法，并模拟了15,321个嵌入每个角色目标、行为和经验的问题。

Result: 这些问题既高度可信，又与现实世界的数据相当。当用于评估五种LLM时，对于同一问题但不同角色，我们发现系统性差异：脆弱角色（如患者和照顾者）会引发更多的支持性回应（+17%）和较少的知识内容（-19%）。

Conclusion: 我们的工作展示了隐式传达用户角色如何影响模型响应，并提供了用于对话AI的角色导向评估方法。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [44] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight 是一个用于生成高质量、多模态财务报告的新型多代理框架，通过代码代理和两阶段写作框架实现专业级的可视化和报告生成，实验结果表明其性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 生成专业的财务报告是一个劳动密集且智力要求高的过程，当前的 AI 系统难以完全自动化。

Method: 引入了 FinSight（Financial InSight），这是一个用于生成高质量、多模态财务报告的新颖多代理框架。其基础是带有变量内存（CAVM）架构的代码代理，它将外部数据、设计工具和代理统一到一个可编程变量空间中，通过可执行代码实现灵活的数据收集、分析和报告生成。此外，还提出了一种两阶段写作框架，将简短的分析链扩展为连贯、引用意识和多模态的报告，确保分析深度和结构一致性。

Result: 实验表明，FinSight 在各种公司和行业级任务中显著优于所有基线，包括领先的深度研究系统，在事实准确性、分析深度和展示质量方面表现出色。

Conclusion: FinSight 显著优于所有基线，包括领先的深度研究系统，在事实准确性、分析深度和展示质量方面表现出色，展示了生成接近人类专家质量的报告的明确路径。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [45] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 本文提出了NGC框架，通过将神经网络视为动态系统中的神经元组来提高效率和可解释性，并在大型语言模型中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络规模不断扩大，带来了性能提升的同时也面临效率和可解释性的挑战，需要构建能够学习高效、模块化和可解释表示的大规模神经系统。

Method: 提出了一种基于动态系统理论的Neuronal Group Communication (NGC)框架，将神经网络视为相互作用的神经元组的动力系统，而不是独立的权重集合。

Result: 在大型语言模型中实例化NGC，展示了在中等压缩率下复杂推理基准上的性能提升，优于标准低秩近似和跨层基共享方法。

Conclusion: NGC框架展示了在大规模语言模型中实现高效、模块化和可解释表示的潜力，并讨论了其在高维学习系统中的泛化关系。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [46] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 本文提出了一种新的具身知识理解基准，发现视觉-语言模型在多项任务中并不优于文本模型，并指出需要更好地整合具身知识以提升语言模型对物理世界的理解。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态语言模型取得了显著进展，但尚不清楚视觉定位是否比文本模型更能增强其对具身知识的理解。

Method: 我们提出了一种基于心理学感知理论的新颖的具身知识理解基准，涵盖了视觉、听觉、触觉、味觉、嗅觉和内感受等外部感官以及内感受。该基准通过向量比较和超过1700个问题的问答任务来评估模型的感知能力。

Result: 我们发现视觉-语言模型（VLMs）在两项任务中均不如文本模型表现好。此外，模型在视觉维度上的表现明显低于其他感官维度。进一步分析表明，向量表示容易受到词形和频率的影响，模型在涉及空间感知和推理的问题上表现不佳。

Conclusion: 我们的研究结果强调了在语言模型中更有效地整合具身知识的必要性，以增强它们对物理世界的理解。

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [47] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: 本文介绍了ChiKhaPo，一个包含8个子任务的基准测试，用于评估生成模型的词汇理解和生成能力。该基准测试覆盖了2700多种语言，超越了现有的任何基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）的基准测试主要局限于高资源或中等资源语言，并且通常在推理和生成等高级任务上进行评估。然而，大量证据表明，LLM在世界上3800多种书面语言中的大多数缺乏基本的语言能力。

Method: 我们引入了ChiKhaPo，由8个不同难度的子任务组成，旨在评估生成模型的词汇理解和生成能力。

Result: 我们进一步展示了6种最先进的模型在我们的基准测试中表现不佳，并讨论了影响性能分数的因素，包括语言家族、语言资源丰富度、任务以及理解与生成方向。

Conclusion: 我们希望ChiKhaPo能够促进和鼓励大规模多语言LLM基准测试。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [48] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的框架PROMPT-MII，用于生成紧凑的指令，以提高大型语言模型在新任务上的性能，同时减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在适应大型语言模型到新任务时，需要高推理成本，因此需要一种更高效的方法。

Method: PROMPT-MII是一种基于强化学习的框架，用于元学习一个指令归纳模型，该模型可以为任意新数据集实时生成紧凑的指令。

Result: PROMPT-MII在90个未见过的任务上提升了下游模型质量4-9 F1点（10-20%相对），同时需要3-13倍更少的标记。

Conclusion: PROMPT-MII能够以更少的标记实现与ICL相当的性能，从而提高了下游模型的质量。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [49] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本文研究了使用PEFT技术在孟加拉语仇恨言论检测中的应用，并展示了其在低资源语言中的有效性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交媒体平台上的仇恨言论急剧增加，影响了女性和青少年。现有的方法要么计算成本高，要么依赖专有API。

Method: 本文首次应用参数高效微调(PEFT)进行孟加拉语仇恨言论检测，使用LoRA和QLoRA。

Result: Llama-3.2-3B在F1分数上达到了92.23%的最高分，其次是Mistral-7B的88.94%和Gemma-3-4B的80.25%。

Conclusion: 这些发现确立了PEFT作为孟加拉语及相关低资源语言的实用且可复制的策略。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [50] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer是一种高效的字节级分词器，通过设计原则优化了性能和模型间的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的字节级方法存在一些问题，如引入超出范围的ID或辅助标记，这可能影响性能和模型之间的对齐。

Method: UTF8Tokenizer是一个最小化的字节级分词器，将文本精确映射到对应于文本UTF-8编码的字节的ID。它使用C0控制字节来编码特殊行为，而不是引入超出范围的ID或辅助标记。

Result: UTF8Tokenizer实现了14倍的分词速度提升和8倍的主机-设备传输减少，同时提供了可共享的256*d嵌入表，并通过位偏置嵌入提高了训练时的效果。

Conclusion: UTF8Tokenizer通过其设计原则带来了实际的好处，包括更快的分词速度、更低的主机-设备传输和简单的嵌入表。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [51] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 本文提出了一种基于转换向量的词汇重塑方法，能够减少词汇表大小并提高多语言覆盖范围，同时保持下游性能。


<details>
  <summary>Details</summary>
Motivation: 标准的分词算法将这些变化视为不同的标记，这使得大小受限的词汇表充满了表面形式变体，从而牺牲了较少出现的单词和多语言覆盖范围。

Method: 我们提出了一种紧凑的词汇重塑方法：而不是为每个表面形式分配唯一的标记，我们从共享的基础形式和转换向量中组成它们。

Result: 我们在多个LLM和五种语言中应用了我们的方法，最多删除了10%的词汇条目，从而腾出空间分配新的、更多样化的标记。

Conclusion: 我们的研究结果促使对词汇设计进行基础性的重新思考，从字符串枚举转向利用语言底层结构的组合性词汇。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [52] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 本文提出了一种基于在线学习和强化学习的新型防御框架，用于抵御迭代越狱攻击，并通过PDGD防止过拟合，实验结果表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的防御措施未能主动破坏这种动态的试错循环，而迭代越狱方法已被证明是针对LLM及其安全机制的有效攻击策略。

Method: 我们提出了一种新颖的框架，通过在线学习动态更新防御策略，以应对来自迭代越狱方法的新提示。我们引入了一种基于强化学习的方法，优化提示以确保对无害任务的适当响应，同时明确拒绝有害提示。此外，我们引入了Past-Direction Gradient Damping (PDGD) 来防止对攻击期间探索的部分输入重写范围的过拟合。

Result: 实验表明，我们的方法在三种LLM上显著优于五种现有防御方法，并且同时提高了无害任务的响应质量。

Conclusion: 我们的方法在对抗五种迭代越狱方法时显著优于五种现有的防御方法，并且同时提高了无害任务的响应质量。

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [53] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文提出了DiscoTrack基准，以测试模型在跨文档的隐含信息和语用推理方面的能力，并发现这些任务对最先进的模型仍然具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM基准主要关注自然语言理解，如问答或摘要，而缺乏更具挑战性的多语言基准，专注于跨文档的隐含信息和语用推理。

Method: 本文介绍了DiscoTrack基准，该基准涵盖了12种语言和四个层次的语篇理解任务：显著性识别、实体跟踪、语篇关系和桥梁推理。

Result: 评估结果显示，这些任务对最先进的模型仍然具有挑战性。

Conclusion: 本文提出了DiscoTrack基准，旨在测试模型在跨文档的隐含信息和语用推理方面的能力，并发现这些任务对最先进的模型仍然具有挑战性。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [54] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 本文提出 SafeSearch 方法，以解决基于大型语言模型的搜索代理在安全性方面的不足，实验证明其能显著降低有害性并保持效用。


<details>
  <summary>Details</summary>
Motivation: 研究发现，基于大型语言模型的搜索代理在安全性方面存在不足，需要同时关注安全性和效用。

Method: SafeSearch 通过结合最终输出的安全/效用奖励和一个新的查询级塑造项来实现安全性和效用的联合对齐。

Result: 实验表明，SafeSearch 在三个红队数据集上将代理的有害性降低了70%以上，同时保持了安全且有帮助的响应，并与仅优化效用的代理在问答性能上相当。

Conclusion: SafeSearch 是一种多目标强化学习方法，能够有效降低搜索代理的有害性，同时保持其有用性。

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [55] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: xLSTM是一种高效的有毒评论检测框架，通过余弦相似度门控和自适应特征优先化等技术，在保持高性能的同时显著减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 有毒评论检测仍然是一项具有挑战性的任务，其中基于变压器的模型（如BERT）计算成本高且在少数毒性类别上表现不佳，而经典集成缺乏语义适应性。

Method: xLSTM是一种参数高效且理论上有依据的框架，结合了余弦相似度门控、自适应特征优先化和合理的类别再平衡。

Result: 在Jigsaw有毒评论基准测试中，xLSTM达到了96.0%的准确率和0.88宏F1，比BERT在威胁和身份仇恨类别上分别高出33%和28%，参数减少了15倍，推理延迟为50ms。

Conclusion: xLSTM展示了轻量级、理论上有依据的架构可以在不平衡、领域特定的NLP任务上超越大型预训练模型。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [56] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）的提示敏感性，并提出了一种新的方法来改进其不确定性校准。我们还引入了一个新的不确定性分解度量，用于量化LLM不确定性中有多少是归因于提示敏感性的。


<details>
  <summary>Details</summary>
Motivation: 当提供不同但语义等效的同一提示的不同版本时，大型语言模型（LLMs）会出现一种有趣的行为，即提示敏感性。这表明模型输出分布中的不确定性可能并不反映模型对提示含义的不确定性。因此，我们需要一种新的方法来改进提示敏感语言模型的不确定性校准。

Method: 我们将提示敏感性建模为一种泛化误差，并展示了通过在语义“概念空间”中进行采样（使用改写扰动）可以提高不确定性校准而不损害准确性。此外，我们引入了一个新的不确定性分解度量，该度量在黑盒LLM中优于基于熵的分解，因为它在自然语言生成中建模了语义连续性。

Result: 我们展示了一种新的不确定性分解度量，可以用来量化LLM不确定性中有多少是归因于提示敏感性的。此外，我们的工作表明，一些LLM在关于其输入含义的一致推理方面表现不佳。

Conclusion: 我们的工作引入了一种新的改进提示敏感语言模型不确定性校准的方法，并提供了证据表明一些LLM在关于其输入含义的一致推理方面表现不佳。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [57] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 本文研究了语言模型在社会偏见场景下的行为，发现了两个导致偏见聚合的失败模式，并提出了一种基于提示的缓解方法，有效减少了偏见。


<details>
  <summary>Details</summary>
Motivation: 我们系统地研究了这种现象背后的思考过程中的机制，并发现了两个导致社会偏见聚合的失败模式。

Method: 我们引入了一种轻量级的基于提示的缓解方法，该方法查询模型以根据这些特定失败模式审查其初始推理。

Result: 在问答（BBQ和StereoSet）和开放性（BOLD）基准测试中，我们的方法有效减少了偏见。

Conclusion: 我们的方法在减少偏见的同时保持或提高了准确性。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [58] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文提出了VeriMAP框架，用于多智能体协作的验证意识规划，通过分解任务、建模子任务依赖关系和编码验证函数来提高系统鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作带来了新的挑战，如规划、协调和验证。执行失败通常不是由于推理错误，而是由于任务解释、输出格式或智能体间交接的细微不一致。

Method: 提出了一种名为VeriMAP的框架，该框架通过验证意识规划来实现多智能体协作。VeriMAP规划器分解任务，建模子任务依赖关系，并将规划器定义的传递标准编码为子任务验证函数（VFs）。

Result: 在各种数据集上评估了VeriMAP，结果表明它优于单智能体和多智能体基线，同时提高了系统的鲁棒性和可解释性。

Conclusion: 验证意识规划有助于在多智能体系统中实现可靠的协调和迭代改进，而无需依赖外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [59] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: DVAGen 是一个开源框架，用于训练、评估和可视化动态词汇增强的语言模型，解决了现有方法的局限性，并提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的动态词汇方法面临代码库碎片化、缺乏对现代 LLM 的支持以及推理可扩展性有限等问题，因此需要一个统一的框架来解决这些问题。

Method: DVAGen 模块化了管道以方便自定义，与开源 LLM 无缝集成，并首次提供 CLI 和 WebUI 工具用于实时结果检查。

Result: 验证了动态词汇方法在现代 LLM 上的有效性，并展示了对批量推理的支持，显著提高了推理吞吐量。

Conclusion: DVAGen 是一个完全开源的统一框架，旨在训练、评估和可视化动态词汇增强的语言模型，能够有效提升推理吞吐量。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [60] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文比较了基于提示和基于强化学习的查询增强方法，并提出了一种混合方法OPQE，该方法在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 我们想要比较基于提示和基于强化学习的查询增强方法，并寻找一种更有效的混合方法。

Method: 我们提出了一个混合方法OPQE，它通过生成伪文档来优化检索性能。

Result: OPQE在多个基准测试中表现优于单独的提示方法和强化学习方法。

Conclusion: 我们的实验表明，结合提示和强化学习的方法可以取得最佳效果。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [61] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 本研究探讨了人们是否将AI生成的讽刺视为有意的交流或计算输出，发现人们并未完全对AI生成的讽刺采取有意立场，神经数据也显示了这一现象。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型被越来越多地部署为社交代理并被训练产生幽默和讽刺，人们是否将这些AI的机智言论视为有意的交流还是单纯的计算输出成为一个问题。

Method: 本研究比较了人们对来自人工智能和人类来源的讽刺语句的行为和神经反应，使用了已建立的ERP成分：P200反映早期不一致检测，P600表示重新解释不一致为故意讽刺的认知努力。

Result: 结果表明，人们并没有完全对AI生成的讽刺采取有意立场。行为上，参与者将不一致归因于故意沟通，尽管对于AI来说显著较少，表现出将AI的不一致解释为计算错误的更大倾向。神经数据揭示了AI生成的讽刺的P200和P600效应减弱，这与对交际意图的归因减少一致。值得注意的是，那些认为AI更真诚的人对AI生成的讽刺表现出更大的P200和P600效应，表明有意立场的采用是根据对人工代理的具体心智模型进行校准的。

Conclusion: 尽管当前大型语言模型在语言能力上很成熟，但实现真正的社会代理不仅需要语言能力，还需要人类对人工智能代理的意图进行感知和归因的转变。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [62] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文分析了长上下文语言模型的关键设计原则，并提出了新的方法以提高其长度外推能力。


<details>
  <summary>Details</summary>
Motivation: 有效处理长上下文是语言模型的一个关键挑战，而现有的架构在利用完整上下文方面存在限制。

Method: 我们通过统一的框架和全面的消融研究，分析了这些模型的核心组件。

Result: 我们建立了新的最先进的无训练长度外推方法，成功地将4K上下文训练的模型推广到3200万标记。

Conclusion: 我们的研究提供了清晰且实证基础的设计原则，用于开发未来高度能力的长上下文语言模型。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [63] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: A novel framework called Attention-Shifting (AS) is introduced to address the limitations of existing unlearning approaches in large language models, showing improved performance and reliability.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning approaches face a critical dilemma: Aggressive unlearning compromises model utility, while conservative strategies preserve utility but risk hallucinated responses.

Method: Introduce a novel Attention-Shifting (AS) framework for selective unlearning, which includes two attention-level interventions: importance-aware suppression and attention-guided retention enhancement, jointly optimized via a dual-loss objective.

Result: AS improves performance preservation over the state-of-the-art unlearning methods, achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC benchmark, while maintaining competitive hallucination-free unlearning effectiveness.

Conclusion: AS demonstrates a superior balance between unlearning effectiveness, generalization, and response reliability.

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [64] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出了一种流式思考范式，通过集成流式CoT生成、流式约束训练和流式并行推理，实现了LLM在阅读时的思考，显著降低了延迟并保持了性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理范式在输入完全可用后才开始思考，这在动态场景中引入了不必要的延迟，并削弱了对早期信息的关注。受人类在阅读时思考的认知启发，设计了一个流式思考范式，使推理按输入顺序展开，并在阅读完成后调整其深度。

Method: 设计了一个流式思考范式，通过集成流式CoT生成、流式约束训练和流式并行推理来实现。具体来说，StreamingThinker使用带有质量控制的流式推理单元进行CoT生成，通过流式注意力掩码和位置编码实现顺序保留推理，并利用解耦输入编码和推理生成的并行KV缓存，从而确保对齐并实现真正的并发。

Result: 实验结果表明，StreamingThinker在保持与批量思考相当的性能的同时，将推理开始前的令牌等待时间减少了80%，并将最终答案生成的时间级延迟减少了60%以上。

Conclusion: StreamingThinker在数学推理、逻辑推理和基于上下文的QA推理任务中表现出与批量思考相当的性能，同时显著降低了令牌等待时间和最终答案生成的时间级延迟，证明了流式范式的有效性。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [65] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 本文介绍了一个名为VideoBiasEval的框架，用于评估视频生成中的社会表现偏差。研究发现，对齐调优不仅增强了表征偏差，还使其在时间上更加稳定，导致更平滑但更具刻板印象的描绘。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的视频扩散模型在文本到视频生成方面取得了显著进展，但它们可能会无意中编码和放大社会偏见。因此，需要一种系统的方法来追踪这些偏见在对齐管道中的演变。

Method: 我们引入了VideoBiasEval，这是一个全面的诊断框架，用于评估视频生成中的社会表现。该框架基于已有的社会偏差分类法，采用事件驱动的提示策略来分离语义内容（动作和情境）与演员属性（性别和种族）。此外，它还引入了多粒度指标来评估（1）总体种族偏差，（2）基于种族的性别偏差，（3）模型变体中社会属性的分布变化，以及（4）视频内偏差的时间持续性。

Result: 我们的结果揭示了对齐调优不仅加强了表征偏差，还使其在时间上更加稳定，导致更平滑但更具刻板印象的描绘。

Conclusion: 我们的研究结果表明，对齐调优不仅增强了表征偏差，还使其在时间上更加稳定，导致更平滑但更具刻板印象的描绘。这些发现突显了在整个对齐过程中进行偏差意识评估和缓解的必要性，以确保公平和社会责任的视频生成。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [66] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 本研究通过大规模情感分析发现孟加拉语新闻中负面情绪占主导，并提出了一个可视化情感线索的新闻聚合器设计。


<details>
  <summary>Details</summary>
Motivation: 新闻媒体不仅通过报道的内容，还通过报道的方式影响公众情绪。负面或情绪化的标题更容易吸引注意力并传播得更快，这促使媒体以引发更强反应的方式呈现故事。

Method: 使用Gemma-3 4B进行零样本推理，分析了300000个孟加拉语新闻标题及其内容，以确定每个新闻的主导情绪和整体语气。

Result: 研究发现负面情绪（尤其是愤怒、恐惧和失望）在新闻中占主导地位，并且不同媒体对相似故事的情感表达存在显著差异。

Conclusion: 本研究提出了一个以用户为中心的新闻聚合器的设计理念，通过可视化情感线索帮助读者识别日常新闻中的隐性情感框架。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [67] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文探讨了Transformer-based大型语言模型中的局部可解释性和机制可解释性，以增强对这些模型的信任，并分析了在医疗保健和自动驾驶领域中的可解释性影响。


<details>
  <summary>Details</summary>
Motivation: 由于语言模型在预测和推理中经常出现错误（称为幻觉），因此迫切需要更好地理解和解释语言模型的内部工作机制及其生成预测输出的方式。

Method: 本文回顾了局部可解释性和机制可解释性方法以及相关研究中的见解，并进行了两个关键领域（医疗保健和自动驾驶）中关于可解释性和推理的实验研究。

Result: 本文分析了可解释性在医疗保健和自动驾驶领域中的信任影响，并总结了当前LLM可解释性方面的未解决的问题。

Conclusion: 本文总结了当前在LLM可解释性方面的未解决的问题，并提出了生成与人类对齐、可信的LLM解释的机会、关键挑战和未来方向。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [68] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 本文提出了一种名为TaxoAlign的三阶段基于主题的指令引导方法，用于学术分类法生成，并创建了CS-TaxoBench基准测试集。结果表明，TaxoAlign在几乎所有指标上都优于基线方法。


<details>
  <summary>Details</summary>
Motivation: Existing approaches to automatic survey generation do not compare the structure of the generated surveys with those written by human experts.

Method: TaxoAlign, a three-phase topic-based instruction-guided method for scholarly taxonomy generation.

Result: We evaluate our method and various baselines on CS-TaxoBench, using both automated evaluation metrics and human evaluation studies.

Conclusion: TaxoAlign consistently surpasses the baselines on nearly all metrics.

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [69] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 本研究利用CyberAgressionAdo-Large数据集，评估了多模态模型在反社会行为检测中的表现，结果显示多模态模型优于单模态基线。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补多参与方对话环境中反社会行为研究的空白，因为现有的数据有限。

Method: 使用CyberAgressionAdo-Large数据集，评估了三种任务：仇恨言论检测、欺凌行为分析和欺凌同伴群体识别，并对六种基于文本的方法和八种基于图的方法进行了基准测试。

Result: 多模态模型优于单模态基线，其中mBERT + WD-SGCN在仇恨言论检测中表现最佳，在同伴群体识别和欺凌分析中也表现出色。

Conclusion: 研究结果表明，多模态模型在处理复杂的反社会行为现象方面比单模态基线更有效。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [70] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文介绍了Nyx，一个用于处理混合模态信息的统一检索器，旨在提升视觉-语言生成效果。通过构建NyxQA数据集并采用两阶段训练框架，Nyx在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统主要关注单模态文本文档，在实际场景中查询和文档可能包含混合模态（如文本和图像）时往往表现不足。因此，我们需要解决Universal Retrieval-Augmented Generation (URAG)的问题，即在混合模态信息上进行检索和推理，以提高视觉-语言生成的效果。

Method: 我们提出了Nyx，这是一个统一的多模态到多模态检索器，专门针对URAG场景。为了缓解真实多模态数据的稀缺性，我们引入了一个四阶段的自动化管道，利用网络文档构建NyxQA数据集，该数据集包含多样化的多模态问答对。基于这个高质量的数据集，我们采用了一个两阶段的训练框架：首先在NyxQA和各种开源检索数据集上进行预训练，然后使用下游视觉-语言模型（VLMs）的反馈进行监督微调，以使检索输出与生成偏好对齐。

Result: 实验结果表明，Nyx不仅在标准的纯文本RAG基准测试中表现优异，而且在更通用和现实的URAG设置中也表现出色，显著提高了视觉-语言任务的生成质量。

Conclusion: 实验结果表明，Nyx不仅在标准的纯文本RAG基准测试中表现优异，而且在更通用和现实的URAG设置中也表现出色，显著提高了视觉-语言任务的生成质量。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [71] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 研究评估了20个指令调优大语言模型在不同选项标签格式下的表现，发现其在指令遵循方面存在不足，并指出需要改进评估方法和训练策略。


<details>
  <summary>Details</summary>
Motivation: 尽管简单、自包含的指令执行是复杂指令遵循的基础，但目前对此研究不足。

Method: 通过系统改变选项标签的格式（字母、数字、罗马数字）在修改后的MMLU和MMLU-Pro基准上评估20个IT-LLMs。

Result: 标签变化导致性能显著变化，没有指令时性能进一步下降，移除选项内容时模型无法达到随机选择基线，三示例样本未带来显著改进。

Conclusion: 当前指令调优范式存在不足，需要评估方法和训练策略来明确针对原子指令遵循。

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [72] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本文介绍了EduAdapt数据集，用于评估大语言模型在不同年级水平上的适应能力，并发现现有模型在低年级学生上的表现仍有不足。


<details>
  <summary>Details</summary>
Motivation: 在K-12教育中，年龄适当的词汇和解释对于有效的学习至关重要。现有的模型经常为年轻学习者生成过于先进或模糊的输出，而且没有标准化的基准来评估它们调整跨认知和发育阶段的能力。

Method: 本文引入了EduAdapt，这是一个包含近48k个按年级标记的问答对的数据集，涵盖了九门科学科目，覆盖了1-12年级，并分为四个年级水平。我们评估了一系列开源大语言模型在EduAdapt上的表现。

Result: 尽管更大的模型通常表现更好，但它们仍然难以为低年级学生（1-5年级）生成合适的回答。

Conclusion: 本文提出了EduAdapt数据集和评估框架，旨在促进更符合发展需求的教育AI系统的发展。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [73] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本研究介绍了Ladder-base，这是一个使用组相对策略优化（GRPO）训练的首个TCM专用大型语言模型（LLM）。Ladder-base在多个推理指标上表现出优于现有先进LLMs和领域特定TCM模型的性能，证明了GRPO在将LLMs与传统医学领域的专家级推理对齐方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统中医（TCM）呈现了一个丰富且结构独特的知识体系，挑战了大型语言模型（LLMs）的常规应用。尽管之前的TCM专用LLMs通过监督微调取得了进展，但它们常常面临对齐、数据质量和评估一致性方面的限制。

Method: 引入了Ladder-base，这是第一个使用组相对策略优化（GRPO）训练的TCM专用大型语言模型（LLM），该方法通过基于组内比较优化响应选择来提高推理和事实一致性。

Result: 通过标准化评估，Ladder-base在多个推理指标上表现出优于最先进的通用LLMs（如GPT-4、Gemini 2.5、Claude 3和Qwen3）以及领域特定的TCM模型（包括BenTsao、HuatuoGPT2和Zhongjing）的性能。

Conclusion: 这些发现表明，GRPO为将大型语言模型与传统医学领域的专家级推理对齐提供了一种有效且高效的战略，并支持开发值得信赖且具有临床基础的中医人工智能系统。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [74] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: 本文提出了AfriCaption框架，为20种非洲语言的多模态图像描述生成提供了统一的解决方案，为实现真正的包容性多模态AI奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 目前多模态AI研究主要集中在高资源语言上，这阻碍了该领域进步的普及。因此，本文旨在解决这一问题，为低资源非洲语言提供多模态图像描述生成的解决方案。

Method: 本文提出了一个动态、保持上下文的管道，通过模型集成和自适应替换确保持续的质量，并开发了AfriCaption模型，该模型整合了SigLIP和NLLB200进行跨低资源语言的描述生成。

Result: 本文构建了一个基于Flickr8k的精心策划的数据集，包含通过上下文感知选择和翻译过程生成的语义对齐描述，并提出了AfriCaption模型，该模型在低资源语言的图像描述生成中表现出色。

Conclusion: 本文提出了AfriCaption框架，为20种非洲语言的多模态图像描述生成提供了统一的解决方案，为实现真正的包容性多模态AI奠定了基础。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [75] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本研究提出了一种基于自然语言指令微调和多模态集成的TCM领域大语言模型，名为BenCao，在多个任务中表现优异，并已部署为交互式应用。


<details>
  <summary>Details</summary>
Motivation: 现有TCM领域的大型语言模型在文本理解方面取得了一定进展，但缺乏多模态集成、可解释性和临床适用性。因此，需要一种能够解决这些限制的方法。

Method: BenCao是基于ChatGPT的多模态助手，整合了结构化知识库、诊断数据和专家反馈优化。通过自然语言指令微调进行训练，而不是参数重新训练，以符合TCM特有的专家级推理和伦理规范。系统包括一个涵盖1000多部经典和现代文本的综合知识库、一个基于场景的指令框架、一个用于可解释推理的思维链模拟机制，以及一个涉及持牌TCM从业者反馈优化的过程。

Result: BenCao在单选题基准测试和多模态分类任务中表现出优于通用领域和TCM领域模型的准确性，特别是在诊断、草药识别和体质分类方面。该模型已作为交互式应用部署在OpenAI GPTs Store上，截至2025年10月，全球已有近1000名用户使用。

Conclusion: 本研究展示了通过基于自然语言的指令微调和多模态集成开发TCM领域大语言模型的可行性，提供了一个将生成式AI与传统医学推理对齐的实用框架，并为现实世界的部署提供了可扩展的路径。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [76] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 我们的研究展示了简单的模型合并可以有效缓解对齐税，生成更强大和可靠的模型。


<details>
  <summary>Details</summary>
Motivation: 传统上将后训练的“对齐税”视为任务准确性的下降，但我们发现它还涉及校准的严重损失，使模型过于自信、不可靠且输出缺乏多样性。

Method: 通过在对齐前后的模型权重之间进行插值来实现简单的后处理干预。

Result: 我们发现该过程始终能揭示帕累托最优的插值——在提高准确性的同时显著恢复对齐过程中失去的校准。

Conclusion: 我们的工作表明，简单的模型合并提供了一种计算高效的缓解对齐税的方法，生成的模型更具能力且更可靠。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [77] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: 研究发现基于强化学习的搜索模型存在安全隐患，攻击可以显著降低其安全性，需要改进安全机制。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示基于强化学习的搜索模型在安全性方面的潜在问题，并提出改进方案。

Method: 通过两种简单的攻击方法（Search攻击和Multi-search攻击）测试了基于强化学习的搜索模型的安全性。

Result: 攻击导致拒绝率下降高达60.0%，回答安全性下降82.5%，搜索查询安全性下降82.4%。

Conclusion: 当前基于强化学习的搜索模型存在安全隐患，需要开发更安全的代理强化学习流程。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [78] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 本研究开发了多种深度上下文嵌入模型，以提高心脏病学领域的临床命名实体识别（NER），并在多个语言任务中取得了优于现有基准的结果。


<details>
  <summary>Details</summary>
Motivation: 由于电子健康记录（EHR）数据的快速增长，需要从非结构化临床文本中解锁生物医学知识，以支持数据驱动的临床系统的发展。然而，针对低资源语言的临床文本研究仍然不足。

Method: 本研究探索了不同单语和多语BERT模型在通用领域文本上的效果，用于从用英语、西班牙语和意大利语编写的临床病例报告中提取疾病和药物提及。

Result: 在西班牙语疾病识别（SDR）中获得了77.88%的F1分数，在西班牙语药物识别（SMR）中获得了92.09%，在英语药物识别（EMR）中获得了91.74%，在意大利语药物识别（IMR）中获得了88.9%。这些结果在所有子任务的测试排行榜上均优于平均值和中位数F1分数。

Conclusion: 本研究开发了多种深度上下文嵌入模型，以提高心脏病学领域的临床命名实体识别（NER），并在多个语言任务中取得了优于现有基准的结果。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [79] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 本文介绍了第一个用于乌尔都语到英语习语翻译的评估数据集，并评估了多种大型语言模型和神经机器翻译系统的表现。研究发现，提示工程能有效提升习语翻译质量，而文本表示对翻译结果有显著影响。


<details>
  <summary>Details</summary>
Motivation: 习语翻译在机器翻译中仍然是一个重大挑战，尤其是对于低资源语言如乌尔都语，且此前关注有限。为了推进这一领域的研究，我们引入了第一个评估数据集，以促进乌尔都语到英语习语翻译的研究。

Method: 我们引入了第一个用于乌尔都语到英语习语翻译的评估数据集，涵盖了原生乌尔都语和罗马乌尔都语脚本，并用黄金标准英语等价物进行注释。我们评估了多种开源大型语言模型（LLMs）和神经机器翻译（NMT）系统在此任务上的表现，重点是它们保持习语和文化意义的能力。使用自动度量标准包括BLEU、BERTScore、COMET和XCOMET来评估翻译质量。

Result: 我们的研究结果表明，提示工程可以提高习语翻译效果，尽管不同类型的提示之间的性能差异相对较小。此外，跨脚本比较显示，文本表示对翻译质量有显著影响，原生乌尔都语输入产生的习语翻译比罗马乌尔都语更准确。

Conclusion: 我们的研究结果表明，提示工程可以提高习语翻译效果，尽管不同类型的提示之间的性能差异相对较小。此外，跨脚本比较显示，文本表示对翻译质量有显著影响，原生乌尔都语输入产生的习语翻译比罗马乌尔都语更准确。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [80] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 本文研究了多语言医疗问答中预训练源和事实一致性方面的跨语言差异，并提出了构建更公平的多语言医疗AI系统的实际途径。


<details>
  <summary>Details</summary>
Motivation: 在将人工智能整合到医疗保健中时，公平获取可靠的健康信息至关重要。然而，不同语言的信息质量存在差异，这引发了对多语言大型语言模型（LLMs）的可靠性和一致性的担忧。

Method: 我们(一)构建了Multilingual Wiki Health Care (MultiWikiHealthCare)，一个来自维基百科的多语言数据集；(二)分析了跨语言的医疗覆盖情况；(三)评估了LLM响应与这些参考文献的一致性；(四)通过使用上下文信息和检索增强生成(RAG)进行了事实一致性案例研究。

Result: 我们的研究发现，在维基百科的覆盖范围和LLM的事实一致性方面存在显著的跨语言差异。在所有LLM中，响应更符合英语维基百科，即使提示是非英语的。在推理时提供非英语维基百科的上下文摘录可以有效地将事实一致性转向文化相关的知识。

Conclusion: 这些结果突显了构建更公平的多语言医疗AI系统的实际途径。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [81] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE 是一种改进的 MoE 架构，通过跨层专家重用和渐进式扩展路由策略，提高了模型性能和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有的层局部路由机制限制了专家池的使用，需要在专家维度和路由多样性之间进行权衡。

Method: ReXMoE 通过允许路由器在相邻层之间重用专家，改进了现有的层局部路由方法。此外，还提出了一种渐进式扩展路由 (PSR) 策略，以在训练期间逐步增加候选专家池。

Result: ReXMoE 在语言建模和下游任务性能方面都有所提高，并且在不同架构的模型中表现出一致的性能提升。

Conclusion: ReXMoE 是一种新的设计范式，为参数高效和可扩展的 MoE 基础 LLM 提供了支持。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [82] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出DETree，一种新的方法，通过层次亲和树结构建模不同过程之间的关系，并引入专门的损失函数，以提高混合文本检测的性能和分布外场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 检测涉及AI的文本对于打击虚假信息、抄袭和学术不端行为至关重要。然而，AI文本生成包括多种协作过程，这些过程产生的文本表现出复杂的特征，给检测带来了重大挑战。当前的方法对这些过程建模过于粗糙，主要采用二元分类或多元分类。

Method: 我们提出了DETree，这是一种新颖的方法，将不同过程之间的关系建模为层次亲和树结构，并引入了一种专门的损失函数，将文本表示与该树对齐。为了促进这种学习，我们开发了RealBench，一个全面的基准数据集，能够自动包含通过各种人机协作过程产生的广泛混合文本。

Result: 我们的方法在混合文本检测任务中提高了性能，并在分布外（OOD）场景中显著增强了鲁棒性和泛化能力，特别是在少样本学习条件下。

Conclusion: 我们的方法在混合文本检测任务中提高了性能，并在分布外（OOD）场景中显著增强了鲁棒性和泛化能力，特别是在少样本学习条件下，进一步证明了基于训练的方法在OOD设置中的前景。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [83] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统回顾了基于LLM的行业代理的技术、应用和评估方法，探讨了其在不同场景下的能力边界、发展潜力和治理问题，并提供了未来方向的见解。


<details>
  <summary>Details</summary>
Motivation: 如何将一般代理的研究转化为推动产业变革的生产力仍然是一个重大挑战。

Method: 本文系统地回顾了基于LLM的行业代理的技术、应用和评估方法，并使用行业代理能力成熟度框架概述了代理在行业应用中的演变。

Result: 本文讨论了支持代理能力提升的三个关键技术支柱：记忆、规划和工具使用，并提供了行业代理在数字工程、科学发现、具身智能、协作业务执行和复杂系统模拟等现实领域应用的概述。此外，本文还回顾了基本能力和专业能力的评估基准和方法，指出了现有评估系统在真实性、安全性和行业特定性方面面临的挑战。

Conclusion: 本文通过结合技术演进与行业实践，旨在明确当前状态并为理解和构建下一代行业代理提供清晰的路线图和理论基础。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [84] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为深度自我进化推理（DSER）的概率范式，以扩展开放权重模型在困难任务上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的验证-精炼框架在解决奥数级别问题方面取得了进展，但其有效性依赖于强大的验证和修正能力，而这些能力在开放权重的小规模模型中仍然脆弱。

Method: 将迭代推理概念化为马尔可夫链，其中每一步代表解决方案空间中的随机转换，并通过运行多个长期、自我演进的过程来放大微小的积极趋势。

Result: DSER在DeepSeek-R1-0528-Qwen3-8B模型上进行了测试，在AIME 2024-2025基准测试中解决了之前无法解决的5个问题，并提升了整体性能，使该紧凑模型的单次轮次准确率超过了其600B参数教师模型。

Conclusion: DSER框架有助于诊断当前开放权重推理器的根本局限性，并为开发具有强大内在自我进化能力的下一代模型指明了研究方向。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [85] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 本文提出了一种多语言句子嵌入模型，该模型在跨语言任务中表现出色，并且在单语迁移学习任务中也表现良好。


<details>
  <summary>Details</summary>
Motivation: 尽管BERT在学习单语句子嵌入方面是有效的，但基于BERT的跨语言句子嵌入尚未被探索。本文旨在系统地研究学习多语言句子嵌入的方法，并提高跨语言任务的性能。

Method: 本文系统地研究了通过结合学习单语和跨语言表示的最佳方法来学习多语言句子嵌入的方法，包括掩码语言模型（MLM）、翻译语言模型（TLM）、双编码器翻译排名和加性间隔softmax。

Result: 引入预训练的多语言语言模型显著减少了获得良好性能所需的平行训练数据量的80%。最佳方法的组合产生了一个模型，在Tatoeba上实现了83.7%的双语检索准确率，优于LASER的65.5%。此外，使用最佳模型从CommonCrawl中挖掘的平行数据可以训练出具有竞争力的NMT模型。

Conclusion: 本文提出了一种多语言句子嵌入模型，该模型在112个语言的Tatoeba数据集上实现了83.7%的双语检索准确率，优于LASER的65.5%。同时，该模型在单语迁移学习基准测试中表现良好，并且可以用于训练具有竞争力的NMT模型。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [86] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: EliCal是一种两阶段框架，用于实现大型语言模型的诚实对齐，通过低成本的自我一致性监督和少量正确性注释来校准置信度。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要么依赖于无需训练的置信度估计，要么依赖于带有正确性注释的训练基础校准。虽然有效，但实现通用的诚实对齐需要昂贵的大规模标注。

Method: EliCal是一种两阶段框架，首先使用低成本的自我一致性监督来引出内部置信度，然后使用少量正确性注释来校准此置信度。

Result: 实验表明，EliCal仅需1k个正确性注释（0.18%的完整监督）即可实现接近最优的对齐，并且在未见过的MMLU任务上比仅校准基线表现更好。

Conclusion: EliCal提供了一种可扩展的解决方案，以实现大型语言模型的普遍诚实对齐。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [87] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: 本文介绍了SimBench，这是一个大规模、标准化的基准，用于评估LLM模拟人类行为的能力。结果显示，尽管当前LLM的模拟能力有限，但性能随模型大小增长，且与深度推理能力密切相关。


<details>
  <summary>Details</summary>
Motivation: 当前评估是零散的，基于定制任务和指标，导致不可比较的结果。需要一个统一的基准来研究LLM模拟何时、如何以及为何成功或失败。

Method: 引入SimBench，这是第一个大规模、标准化的基准，用于稳健、可重复的LLM模拟科学。

Result: 即使最好的LLM今天也有有限的模拟能力（得分：40.80/100），性能随着模型大小呈对数线性增长。增加推理时间计算不会提高性能。指令微调在低熵问题上提高性能，但在高熵问题上降低性能。模型在模拟特定人口群体时尤其困难。模拟能力与深度、知识密集型推理最密切相关。

Conclusion: 通过使进展可衡量，我们旨在加速开发更忠实的LLM模拟器。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [88] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 本文提出了一种多任务学习框架，通过将自回归LLM与临床推理对齐，提高癌症治疗结果预测的准确性与可解释性。通过三种对齐策略的实验，发现CoT提示和GRPO方法分别提升了模型性能和可解释性，为精准肿瘤学中的可解释、可信的LLM设定了新基准。


<details>
  <summary>Details</summary>
Motivation: 预测癌症治疗结果需要既准确又可解释的模型，尤其是在存在异质性临床数据的情况下。虽然大型语言模型(LLM)在生物医学NLP中表现出色，但它们往往缺乏关键的结构化推理能力，这对于高风险决策支持至关重要。

Method: 我们提出了一个统一的多任务学习框架，将自回归LLM与临床推理对齐，用于MSK-CHORD数据集上的结果预测。我们的模型联合执行二进制生存分类、连续生存时间回归和自然语言理由生成。我们评估了三种对齐策略：(1) 标准监督微调(SFT)，(2) 使用思维链(CoT)提示来激发逐步推理的SFT，以及(3) Group Relative Policy Optimization (GRPO)，这是一种强化学习方法，使模型输出与专家推导的推理轨迹对齐。

Result: 实验表明，CoT提示提高了F1分数+6.0，并减少了MAE 12%，而GRPO在BLEU、ROUGE和BERTScore方面实现了最先进的可解释性和预测性能。我们进一步表明，现有的生物医学LLM由于架构限制，往往无法产生有效的推理轨迹。

Conclusion: 我们的研究强调了在多任务临床建模中考虑推理对齐的重要性，并为精准肿瘤学中的可解释、可信的LLM设定了新基准。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [89] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 本文提出使用Mapper工具从拓扑角度分析语言模型如何编码歧义和实例，结果显示微调后的模型在嵌入空间中形成了模块化、非凸区域，与模型预测对齐，但在歧义数据中与真实标签的对齐度下降。


<details>
  <summary>Details</summary>
Motivation: 现有的标量度量如准确性无法捕捉模型内部对歧义的表示，特别是在人类标注者存在分歧的情况下。我们需要一种新的方法来分析模型如何处理歧义。

Method: 我们使用Mapper工具来分析RoBERTa-Large模型在MD-Offense数据集上的嵌入空间，以研究微调如何重新结构化嵌入空间。

Result: 实验结果表明，微调将嵌入空间重新结构化为模块化的、非凸区域，这些区域与模型预测对齐，即使在高度歧义的情况下也是如此。超过98%的连通组件表现出至少90%的预测纯度，但与真实标签的对齐度在歧义数据中下降。

Conclusion: 我们的研究表明，Mapper作为一种拓扑数据分析工具，能够揭示模型如何处理歧义，并提供了一种理解模型解决歧义的新方法。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [90] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种名为语言混淆门（LCG）的轻量级解决方案，用于减少大语言模型在文本生成过程中出现的语言混淆问题。该方法不需要重新训练模型，而是通过自蒸馏训练来预测适当的语言家族并在需要时应用掩码。实验结果显示，LCG能够显著减少语言混淆，同时不影响任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前解决语言混淆问题的方法要么需要重新训练模型，要么无法区分有害的混淆和可接受的代码切换。因此，本文旨在提出一种无需重新训练模型且能有效减少语言混淆的方法。

Method: 本文提出的方法是基于语言混淆发生的频率较低，正确语言的令牌通常在顶部预测中，并且高资源语言的输出令牌嵌入范数较大，这会偏向采样。通过使用调整后的自蒸馏训练LCG来预测适当的语言家族并在需要时应用掩码。

Result: 在包括Qwen3、GPT-OSS、Gemma3、Llama3.1在内的多种模型上评估，LCG显著减少了语言混淆，通常减少一个数量级，同时不会对任务性能产生负面影响。

Conclusion: 本文提出了一种轻量级、可插拔的解决方案——语言混淆门（LCG），能够在不改变基础大语言模型的情况下过滤解码过程中的令牌。实验结果表明，该方法显著减少了语言混淆，同时不会对任务性能产生负面影响。

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [91] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于超图的适配器（HGAdapter），用于微调预训练语言模型，以捕捉代码中的高阶数据相关性，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型（PLMs）在代码相关任务中越来越被应用，但它们没有考虑代码中的潜在高阶数据相关性。

Method: 我们提出了三种代码标记的高阶相关性，并设计了一个生成器来捕捉这些高阶数据相关性。我们改进了超图神经网络的架构，并结合适配器调优，提出了基于超图的适配器（HGAdapter）来微调PLMs。

Result: 我们在几个公共数据集上进行了实验，包括六种语言的代码摘要和代码克隆检测任务。我们的方法在不同程度上提高了PLMs在数据集上的性能。

Conclusion: 实验结果验证了引入高阶数据相关性的有效性，HGAdapter可以提高PLMs在不同数据集上的性能。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [92] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架LawChain，用于显式建模中国侵权民事案件中的法律推理过程，并构建了一个评估基准LawChain$_{eval}$来评估法律推理能力。结果表明，当前模型在处理侵权法律推理的关键要素方面仍有不足，但提出的基线方法在侵权相关法律推理中取得了显著改进，并能推广到其他法律分析任务中。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法主要依赖于通用的推理框架，如三段论和IRAC，这些框架没有全面考察支撑法律推理的细微过程。此外，现有研究主要集中在刑事案件上，对民事案件的建模不足。

Method: 本文提出了一个名为LawChain的框架，用于显式建模中国侵权民事案件中的法律推理过程。LawChain是一个三模块的推理框架，每个模块包含多个更细粒度的子步骤。基于LawChain框架，我们引入了侵权法律推理任务，并构建了一个评估基准LawChain$_{eval}$，以系统地评估侵权分析中的关键步骤。

Result: 当前模型在准确处理侵权法律推理的关键要素方面仍存在不足。然而，我们提出的基线方法通过提示或后训练显式结合LawChain风格的推理，在侵权相关法律推理中取得了显著改进，并且在相关法律分析任务中表现出良好的泛化能力。

Conclusion: 本文提出的基线方法在侵权相关法律推理中取得了显著改进，并能很好地推广到相关的法律分析任务中，从而证明了显式建模法律推理链对增强语言模型推理能力的价值。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [93] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 本文研究了无学习方法对上下文效用的影响，并提出了一种增强方法，以在保持遗忘效果的同时恢复模型的上下文效用。


<details>
  <summary>Details</summary>
Motivation: 现有的无学习方法评估忽略了用户可能希望模型在提示中重新引入删除的信息时仍能利用这些信息的重要可用性方面。

Method: 我们通过引入一个插件项来增强无学习目标，以保留模型在上下文中使用被遗忘知识的能力。

Result: 系统评估六种最先进的无学习方法发现它们会损害这种上下文效用。我们的方法能够恢复上下文效用到接近原始水平。

Conclusion: 我们的方法在保持有效遗忘和保留集效用的同时，将上下文效用恢复到接近原始水平。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [94] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: 本文介绍了Qomhrá，一个在低资源条件下开发的双语爱尔兰语-英语大型语言模型，通过持续预训练、指令微调和人类偏好对齐来提升性能，并在多个任务中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 开发一个在低资源条件下有效的双语大型语言模型，以提升爱尔兰语的自然语言处理能力，并确保其在英语任务中的表现。

Method: Qomhrá 是一个双语爱尔兰语-英语大型语言模型，在低资源条件下开发，涵盖了双语持续预训练、指令微调和从人类偏好中对齐的完整流程。使用新可用的爱尔兰语语料库和英语文本进行混合和整理，以提高爱尔兰语性能同时保持英语能力。

Result: Qomhrá 在翻译、性别理解、主题识别和世界知识等基准测试中取得了高达29%的爱尔兰语提升和44%的英语提升。此外，它在指令遵循方面表现出显著进步，这对于聊天机器人功能至关重要。

Conclusion: Qomhrá 在爱尔兰语和英语任务中表现出色，展示了其在低资源条件下的有效性，并通过指令微调提高了对话功能。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [95] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 该研究通过对话分析方法，探索有效的教学策略，以评估基于LLM的教育应用。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注技术性能或学习成果，而忽视了学习者与LLM之间的互动。

Method: 该研究采用对话分析方法，包括对话数据收集、对话行为（DA）标注、DA模式挖掘和预测模型构建。

Result: 初步见解为未来的研究奠定了基础。

Conclusion: 该研究强调了需要通过关注对话动态和教学策略来评估基于LLM的教育应用。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [96] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出QueST框架，通过生成具有挑战性的编程问题，显著提升了大型语言模型在竞赛编码和推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的竞争编码数据集包含的问题数量仅几千到几万，而以前的合成数据生成方法依赖于扩充现有指令数据集或从人工标注数据中选择具有挑战性的问题。

Method: 提出了一种名为QueST的新框架，结合了难度感知图采样和难度感知拒绝微调，直接优化专门的生成器来创建具有挑战性的编程问题。

Result: 训练的生成器在创建具有挑战性的问题方面表现出优于甚至GPT-4o的能力，并且在下游性能上有所提升。通过QueST生成大规模合成编码问题，用于从强大的教师模型中蒸馏或对较小模型进行强化学习，证明在两种情况下都有效。

Conclusion: 生成复杂问题通过QueST提供了一种有效且可扩展的方法，以推动大型语言模型在竞赛编码和推理方面的前沿发展。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [97] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级的少样本NER框架，通过指令微调模板和策略性数据增强技术，在低资源场景下实现了与先进模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 在低资源场景下，标注数据获取成本高，现有的零样本和指令微调方法在泛化到特定领域实体和有效利用有限数据方面存在不足。

Method: 本文提出了一个基于指令微调模板和策略性数据增强技术的轻量级少样本NER框架，以利用最新的大语言模型的大上下文窗口并扩展训练数据。

Result: 实验结果表明，本文的方法在基准数据集上实现了与最先进的模型相当的性能，少样本方法在CrossNER数据集上平均F1分数达到80.1，使用改写方法的模型在F1分数上比基线版本提高了最多17分。

Conclusion: 本文提出了一种轻量级的少样本NER框架，通过两个关键创新解决了低资源场景下的挑战，实验表明该方法在基准数据集上表现与最先进的模型相当，并且在少样本任务中取得了平均F1分数80.1的成绩。

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [98] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: This paper proposes 	extsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context generation tasks. It uses academic writing tasks from arXiv and integrates high-quality few-shot demonstrations to enable flexible context length. The results show that LLMs struggle with tasks requiring hierarchical abstraction and long few-shot demonstrations.


<details>
  <summary>Details</summary>
Motivation: Current long-context LLM benchmarks are limited by rigid context length, labor-intensive annotation, and label leakage issues during LLM training. Therefore, we propose 	extsc{AcademicEval} to address these challenges.

Method: We propose 	extsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context generation tasks. It adopts papers on arXiv to introduce several academic writing tasks with long-context inputs, such as 	extsc{Title}, 	extsc{Abstract}, 	extsc{Introduction}, and 	extsc{Related Work}. It integrates high-quality and expert-curated few-shot demonstrations from a collected co-author graph to enable flexible context length.

Result: We conduct a holistic evaluation on 	extsc{AcademicEval}, and the results illustrate that LLMs perform poorly on tasks with hierarchical abstraction levels and tend to struggle with long few-shot demonstrations.

Conclusion: LLMs perform poorly on tasks with hierarchical abstraction levels and tend to struggle with long few-shot demonstrations, highlighting the challenge of our benchmark. Through experimental analysis, we also reveal some insights for enhancing LLMs' long-context modeling capabilities.

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [99] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 本文提出了一种新的在线强化学习方法，通过二进制检索增强奖励来减少语言模型的幻觉现象，同时保持性能不下降。


<details>
  <summary>Details</summary>
Motivation: 现有的缓解方法常常会降低开放生成和下游任务的性能，限制了它们的实际效用。

Method: 我们提出了一种基于新颖二进制检索增强奖励（RAR）的在线强化学习方法来解决这一权衡问题。

Result: 在开放生成中，二进制RAR实现了幻觉率减少39.3%，在短格式问答中，模型学会了校准的回避策略，分别在PopQA和GPQA上减少了44.4%和21.7%的错误答案。

Conclusion: 我们的方法在保持性能不下降的情况下显著减少了幻觉现象，展示了其在实际应用中的潜力。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [100] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 本文通过引入自主性层次框架，重新审视医学大型语言模型的评估，提出了一种基于层次的评估方法，以实现更可靠和风险意识的临床应用。


<details>
  <summary>Details</summary>
Motivation: 尽管医学大型语言模型在标准基准测试中表现出色，但将这些结果转移到临床工作流程中的安全和可靠性能仍然是一个挑战。本文旨在通过自主性层次框架来重新审视评估，以实现更可靠的临床应用。

Method: 本文通过重新审视评估，采用自主性层次（L0-L3）的视角，将现有的基准和度量与每个层次允许的操作及其相关风险对齐，从而明确评估目标。

Result: 本文提出了一个基于自主性层次的评估框架，明确了不同层次的评估目标，并提出了指标选择、证据组装和声明报告的蓝图，同时指出了评估与监督之间的联系。

Conclusion: 本文通过引入自主性层次框架，将评估目标明确化，并提出了一个基于层次的指标选择、证据组装和声明报告的蓝图，从而推动该领域从基于分数的声明转向可信的风险意识证据，以实现临床实际应用。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [101] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文介绍了FARE，一个基于大规模数据集训练的评估器家族，它在多个任务中表现出色，超越了现有的专门评估器。


<details>
  <summary>Details</summary>
Motivation: 尽管微调专门的生成评估器已成为满足训练和测试时可扩展评估需求的流行范式，但最近的工作主要集中在应用新方法（如强化学习）来训练评估器，而忽略了大规模数据驱动的发展。因此，我们专注于数据扩展，以提高评估器的性能。

Method: 通过数据扩展，我们构建了一个包含2.5M样本的数据集，涵盖了五个独特的评估任务和多个领域。然后，我们使用简单的迭代拒绝采样监督微调（SFT）方法训练了FARE，这是一种8B和20B参数的评估器家族。

Result: FARE-8B挑战了更大的专门RL训练的评估器，FARE-20B设定了开源评估器的新标准，超越了专门的70B+评估器。在实际任务中，FARE表现出色，如在MATH上接近最优性能，在RL训练中提高了下游模型的性能，并在测试用例质量评估中优于gpt-oss-20B。

Conclusion: FARE-20B设定了开源评估器的新标准，超越了专门的70B+评估器。此外，在实际任务中，FARE表现出色，如在MATH上接近最优性能，在RL训练中提高了下游模型的性能，并在测试用例质量评估中优于gpt-oss-20B。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [102] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 本文提出了Executable Knowledge Graphs (xKG)，这是一种模块化且可插拔的知识库，能够自动整合从科学文献中提取的技术见解、代码片段和领域特定知识。在三个代理框架中集成xKG后，在PaperBench上表现出显著的性能提升（o3-mini提升了10.9%），证明了其作为自动化AI研究复制的通用且可扩展解决方案的有效性。


<details>
  <summary>Details</summary>
Motivation: Replicating AI research is a crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code due to insufficient background knowledge and limitations of retrieval-augmented generation (RAG) methods.

Method: We propose Executable Knowledge Graphs (xKG), a modular and pluggable knowledge base that automatically integrates technical insights, code snippets, and domain-specific knowledge extracted from scientific literature.

Result: When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench.

Conclusion: xKG shows substantial performance gains on PaperBench, demonstrating its effectiveness as a general and extensible solution for automated AI research replication.

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [103] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: EDR是一个多智能体系统，能够自动化报告生成、实时流处理和无缝企业部署，并在开放性基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 企业面临将非结构化数据转化为可操作见解的压力，而自主代理在领域特定细微差别、意图对齐和企业集成方面存在困难。

Method: EDR是一个多智能体系统，包括主规划代理、四个专业搜索代理、基于MCP的工具生态系统、可视化代理和反思机制。

Result: EDR在DeepResearch Bench和DeepConsult等开放性基准测试中表现优于最先进的代理系统，无需任何人工引导。

Conclusion: EDR框架和基准轨迹的发布有助于推动多智能体推理应用的研究。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [104] [Comparing LLMs for Sentiment Analysis in Financial Market News](https://arxiv.org/abs/2510.15929)
*Lucas Eduardo Pereira Teles,Carlos M. S. Figueiredo*

Main category: q-fin.ST

TL;DR: 本文比较了大型语言模型和传统方法在金融市场新闻情感分析任务中的性能，发现大型语言模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 本文旨在分析这些模型在这一重要自然语言处理任务中的性能差异，并量化每种测试模型或方法的优势。

Method: 本文进行了一项比较研究，比较了大型语言模型（LLMs）在金融领域自然语言处理任务中的性能差异。

Result: 结果表明，大型语言模型在大多数情况下都优于传统模型。

Conclusion: 大型语言模型在金融市场新闻情感分析任务中表现优于传统方法。

Abstract: This article presents a comparative study of large language models (LLMs) in
the task of sentiment analysis of financial market news. This work aims to
analyze the performance difference of these models in this important natural
language processing task within the context of finance. LLM models are compared
with classical approaches, allowing for the quantification of the benefits of
each tested model or approach. Results show that large language models
outperform classical models in the vast majority of cases.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [105] [Attention to Non-Adopters](https://arxiv.org/abs/2510.15951)
*Kaitlyn Zhou,Kristina Gligorić,Myra Cheng,Michelle S. Lam,Vyoma Raman,Boluwatife Aminu,Caeley Woo,Michael Brockman,Hannah Cha,Dan Jurafsky*

Main category: cs.CY

TL;DR: 本文强调了在LLM开发中考虑非采用者观点的重要性，指出仅关注采用者可能会导致任务和需求的遗漏，加剧不平等，并在模型开发和评估中产生疏漏。


<details>
  <summary>Details</summary>
Motivation: 目前，LLM的开发和评估主要依赖于采用者的数据，这导致了对特定人口统计群体的需求和任务的关注，而忽视了非采用者的需求。

Method: 本文通过与非采用者的案例研究来说明这一主张，展示了非采用者的需求如何与当前用户的需求不同，以及这些需求如何引导我们发现新的推理任务，并通过以人类为中心的方法系统地整合非采用者的需求。

Result: 通过案例研究，本文展示了非采用者的需求与当前用户的不同，这些需求指向了新的推理任务，并提出了系统整合非采用者需求的方法。

Conclusion: 本文认为，将非采用者观点纳入考虑对于开发广泛有用和强大的LLM至关重要。

Abstract: Although language model-based chat systems are increasingly used in daily
life, most Americans remain non-adopters of chat-based LLMs -- as of June 2025,
66% had never used ChatGPT. At the same time, LLM development and evaluation
rely mainly on data from adopters (e.g., logs, preference data), focusing on
the needs and tasks for a limited demographic group of adopters in terms of
geographic location, education, and gender. In this position paper, we argue
that incorporating non-adopter perspectives is essential for developing broadly
useful and capable LLMs. We contend that relying on methods that focus
primarily on adopters will risk missing a range of tasks and needs prioritized
by non-adopters, entrenching inequalities in who benefits from LLMs, and
creating oversights in model development and evaluation. To illustrate this
claim, we conduct case studies with non-adopters and show: how non-adopter
needs diverge from those of current users, how non-adopter needs point us
towards novel reasoning tasks, and how to systematically integrate non-adopter
needs via human-centered methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [106] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: ScholarEval is a new framework for evaluating research ideas that outperforms existing methods and is openly released for the community.


<details>
  <summary>Details</summary>
Motivation: The increasing use of AI tools for research ideation requires robust evaluation to ensure the validity and usefulness of generated ideas.

Method: ScholarEval is a retrieval augmented evaluation framework that assesses research ideas based on soundness and contribution. It was evaluated using ScholarIdeas, an expert-annotated dataset of multi-domain research ideas and reviews.

Result: ScholarEval achieves higher coverage of points mentioned in human expert annotated rubrics compared to all baselines. It is also preferred over the strongest baseline in terms of evaluation actionability, depth, and evidence support. Additionally, it outperforms deep research in literature engagement, idea refinement, and usefulness.

Conclusion: ScholarEval significantly outperforms existing baselines in evaluating research ideas and is openly released for the community to use and build on.

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [107] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 本文是对基于强化学习的代理搜索的首次全面综述，涵盖了其功能角色、优化策略和应用范围，并讨论了相关挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统RAG管道通常是一次性且启发式的，缺乏对检索和推理的自适应控制。最近的代理搜索进展通过使LLM能够通过与搜索环境的多步骤交互进行规划、检索和反思来解决这些限制。强化学习为自适应和自我改进的搜索行为提供了强大的机制。

Method: 本文对基于强化学习的代理搜索进行了全面的综述，从三个互补维度进行组织：(i) 强化学习的作用（功能角色），(ii) 强化学习的使用方式（优化策略），以及(iii) 强化学习的应用范围（优化范围）。

Result: 本文总结了代表性方法、评估协议和应用，并讨论了构建可靠和可扩展的强化学习驱动的代理搜索系统的开放挑战和未来方向。

Conclusion: 本文希望激发未来关于强化学习和代理搜索集成的研究。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [108] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA is an end-to-end model that enables full-duplex, multimodal interaction by integrating vision, text, speech, and action within a single architecture, achieving natural, human-like behaviors.


<details>
  <summary>Details</summary>
Motivation: Realizing human-like interaction capabilities is essential for building models simulating humans. Current models lack full-duplex, end-to-end multimodal perception and generation.

Method: ELLSA is an end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture. It uses a novel SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each modality to specialized experts and fuses them through a unified attention backbone.

Result: ELLSA matches modality-specific baselines on speech-interaction and robot-manipulation benchmarks while supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins.

Conclusion: ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence.

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [109] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista是一个统一框架，旨在提高图理解的可扩展性和模态协调性，通过分层组织图信息和引入规划代理实现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在图理解方面受到输入令牌限制，面临可扩展性瓶颈，并缺乏有效的文本和视觉模态协调机制。

Method: GraphVista通过将图信息分层组织到轻量级GraphRAG基础中，以及引入一个规划代理来协调不同模态的任务，提高了可扩展性和模态协调性。

Result: GraphVista在大规模图上表现出色，比现有方法有显著提升，最高可达4.4倍的质量改进。

Conclusion: GraphVista能够处理比现有基准大200倍的图，并且在所有方法中表现最佳，充分利用了两种模态的互补优势。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [110] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: 本文介绍了一个名为 DeepAnalyze-8B 的代理型大语言模型，能够实现从数据源到分析师级深度研究报告的端到端自动处理。该模型通过一种基于课程的代理训练范式和一个数据驱动的轨迹合成框架进行训练，表现出优于先前工作流代理的性能，并已开源。


<details>
  <summary>Details</summary>
Motivation: 传统的工作流代理在实现完全自主的数据科学方面存在根本性限制，因为它们依赖于预定义的工作流。因此，需要一种新的方法来实现自主数据科学。

Method: 本文提出了一种基于课程的代理训练范式，模拟人类数据科学家的学习轨迹，使 LLM 能够在现实环境中逐步获得和整合多种能力。此外，还引入了一个数据驱动的轨迹合成框架，用于构建高质量的训练数据。

Result: 通过代理训练，DeepAnalyze 能够执行广泛的数据任务，从数据问答和专业分析任务到开放式的数据分析。实验表明，它在仅使用 8B 参数的情况下，表现优于基于最先进专有 LLM 的先前工作流代理。

Conclusion: DeepAnalyze-8B 是一个开创性的代理型大语言模型，能够实现从数据源到分析师级深度研究报告的端到端自动处理。实验表明，它在仅使用 8B 参数的情况下，表现优于基于最先进专有 LLM 的先前工作流代理。该模型、代码和训练数据已开源，为自主数据科学铺平了道路。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [111] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 该研究探讨了视觉语言模型代理是否能通过显式视觉状态推理构建内部世界模型，并提出了一种基于强化学习的方法，通过分解推理为状态估计和转换建模实现这一目标。实验表明，这种方法显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 训练视觉语言模型（VLM）代理面临从文本状态到复杂视觉观察的转变，这引入了部分可观测性和对强大世界建模的需求。研究是否可以通过显式的视觉状态推理构建内部世界模型。

Method: 通过强化学习（RL）对代理的推理过程进行架构强制和奖励，将其形式化为部分可观测马尔可夫决策过程（POMDP）。分解代理的推理为状态估计和转换建模是成功的关键。设计了一个世界建模奖励，提供密集的回合级监督以进行准确的状态预测，并引入了双级通用优势估计（Bi-Level GAE）进行回合感知信用分配。

Result: 通过视觉状态推理，一个3B参数的模型在五个多样化的代理基准上取得了0.82的分数，这比未经训练的模型（0.21）提高了3倍，并且超过了像GPT-5（0.75）、Gemini 2.5 Pro（0.67）和Claude 4.5（0.62）这样的专有推理模型。

Conclusion: 通过视觉状态推理，一个3B参数的模型在五个多样化的代理基准上取得了0.82的分数，这比未经训练的模型（0.21）提高了3倍，并且超过了像GPT-5（0.75）、Gemini 2.5 Pro（0.67）和Claude 4.5（0.62）这样的专有推理模型。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [112] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究了一个工具增强的LLM健康教练，发现统一的重工具策略可能对某些用户有害，而添加早期信息增益奖励可以提高效果。


<details>
  <summary>Details</summary>
Motivation: 旨在探索一种评估优先的个性化方法，以提高健康教练的效果并减少对特定子群的负面影响。

Method: 我们研究了一个部署在网页上的、工具增强的LLM健康教练，与真实用户进行互动。通过离线策略评估（OPE）对分解的决策头（工具/风格）进行分析，并使用一个隐藏原型的轻量级模拟器进行实验。

Result: 统一的重工具策略提高了平均价值，但对低健康素养/高自我效能感用户产生了负面影响。添加一个小的早期信息增益奖励可以可靠地缩短特质识别时间，并提高目标成功率和pass@3。

Conclusion: 这些初步发现表明了一种以评估为主的个性化路径：冻结生成器，在类型化奖励上学习子群感知的决策头（客观工具结果和满意度），并始终报告每个原型的指标以揭示平均值可能掩盖的子群危害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [113] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: 本文提出了一种名为MIRAGE的框架，用于检测多模态虚假信息。该框架通过分解多模态验证过程，结合视觉真实性和跨模态一致性分析，以及检索增强的事实检查，实现了高效的虚假信息检测。实验结果显示，MIRAGE在多个数据集上表现出色，优于现有的方法。


<details>
  <summary>Details</summary>
Motivation: 虚假信息通过每天数十亿的多模态帖子（结合文本和图像）在网页平台上传播，超出了人工事实核查的能力。监督检测模型需要特定领域的训练数据，并且无法在各种操纵技术中泛化。

Method: 我们提出了MIRAGE，一个推理时、模型可插拔的代理框架，将多模态验证分解为四个顺序模块：视觉真实性评估检测AI生成的图像，跨模态一致性分析识别上下文不当的重新利用，检索增强的事实检查通过迭代问题生成将声明与网络证据联系起来，以及校准判断模块整合所有信号。

Result: 在MMFakeBench验证集（1000个样本）上，使用GPT-4o-mini的MIRAGE实现了81.65%的F1和75.1%的准确率，优于最强的零样本基线（GPT-4V with MMD-Agent在74.0% F1），同时保持34.3%的误报率，而仅由法官的基线则为97.3%。测试集结果（5000个样本）确认了泛化能力，达到81.44%的F1和75.08%的准确率。消融研究显示视觉验证贡献了5.18 F1点，检索增强推理贡献了2.97点。

Conclusion: 我们的结果表明，通过网络检索的分解代理推理可以匹配监督检测器的性能，而无需领域特定的训练，这使得在标记数据仍然稀缺的模态中进行虚假信息检测成为可能。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [114] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 本文提出了一种方法，将大型语言模型的推理能力蒸馏到一个小模型中，以提高代码生成的效果。


<details>
  <summary>Details</summary>
Motivation: 有效代码生成依赖于准确理解提示意图和生成能够通过各种测试用例并遵守目标编程语言语法的正确解决方案。大型语言模型（VLLM）能够生成复杂任务的详细步骤，而较小的语言模型可能缺乏这种推理能力。因此，我们旨在将VLLM的推理能力蒸馏到一个更小、更高效的模型中。

Method: 我们的方法通过一种新颖的结构感知损失优化方法，训练模型模仿VLLM的推理和解决问题的能力，以识别正确的解决方案路径并建立问题定义与潜在解决方案之间的结构对应关系。

Result: 实验结果表明，我们的微调模型在多个基准测试中表现优于基线模型，证明了该方法的有效性。

Conclusion: 实验结果表明，通过一种廉价且易于实现的流程开发的微调模型在MBPP、MBPP Plus和HumanEval基准测试中，在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [115] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型作为预测者的潜力，并发现它们在预测方面表现出色，但也存在一些挑战。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是研究大型语言模型（LLMs）在预测现实世界未来事件方面的潜力，即所谓的“LLM-as-a-Prophet”范式。

Method: 本文构建了一个名为Prophet Arena的通用评估基准，用于持续收集实时预测任务，并将每个任务分解为不同的管道阶段，以支持受控和大规模实验。

Result: 本文的结果显示，许多大型语言模型已经表现出令人印象深刻的预测能力，例如小的校准误差、一致的预测信心和有希望的市场回报。然而，也发现了关键瓶颈，如大型语言模型对事件回忆不准确、对数据源的理解错误以及在接近解决时比市场信息聚合较慢。

Conclusion: 本文结论是，许多大型语言模型已经表现出令人印象深刻的预测能力，但在实现更高级的预测智能方面仍存在关键瓶颈。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [116] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种名为HyCAM的新框架，利用CAM机制来提高大型语言模型在多任务适应中的性能，实验结果表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的微调方法存在灾难性遗忘和资源消耗大的问题，而现有的参数高效方法在复杂的多任务场景中表现不佳。因此，需要一种新的方法来解决这些问题。

Method: 本文提出了Contextual Attention Modulation (CAM)机制，并将其集成到Hybrid Contextual Attention Modulation (HyCAM)框架中，结合了共享的全参数CAM模块和多个专用的轻量级CAM模块，同时采用动态路由策略进行自适应知识融合。

Result: 在异构任务（包括问答、代码生成和逻辑推理）上的广泛实验表明，本文的方法显著优于现有方法，平均性能提升了3.65%。

Conclusion: 本文提出了一种新的机制CAM，能够动态调制LLM中自注意力模块的表示，从而在保持通用知识的同时增强任务特定特征。此外，将CAM集成到HyCAM框架中，通过共享和专门的CAM模块以及动态路由策略实现了更有效的多任务适应。实验结果表明，该方法在多个任务上显著优于现有方法。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [117] [Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)](https://arxiv.org/abs/2510.16334)
*Eden Shaveet,Crystal Su,Daniel Hsu,Luis Gravano*

Main category: cs.IR

TL;DR: 本文分析了Yelp评论中的HSAN信号与纽约市官方餐厅检查结果之间的关系，发现两者在普查区层面的相关性较小，且在C级餐厅的分布上没有显著差异。


<details>
  <summary>Details</summary>
Motivation: 食品传播疾病是由于食用受污染食物引起的胃肠道疾病。餐厅是调查疫情的关键场所，因为它们共享食物的采购、准备和分发。公共通过正式渠道报告疾病有限，而社交媒体平台拥有大量用户生成内容，可以提供及时的公共卫生信号。

Method: 本文使用Hierarchical Sigmoid Attention Network (HSAN)分类器分析Yelp评论中的信号，并将其与纽约市卫生和心理健康部（NYC DOHMH）2023年发布的官方餐厅检查结果进行比较。

Result: 本文发现HSAN信号与普查区层面的检查分数之间相关性很小，且在C级餐厅的分布上没有显著差异。

Conclusion: 本文讨论了HSAN信号与检查分数之间在普查区层面的最小相关性以及C级餐厅数量的分布没有显著差异的 implications，并提出了下一步进行地址层面分析的方向。

Abstract: Foodborne illnesses are gastrointestinal conditions caused by consuming
contaminated food. Restaurants are critical venues to investigate outbreaks
because they share sourcing, preparation, and distribution of foods. Public
reporting of illness via formal channels is limited, whereas social media
platforms host abundant user-generated content that can provide timely public
health signals. This paper analyzes signals from Yelp reviews produced by a
Hierarchical Sigmoid Attention Network (HSAN) classifier and compares them with
official restaurant inspection outcomes issued by the New York City Department
of Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at
the Census tract level, compare distributions of HSAN scores by prevalence of
C-graded restaurants, and map spatial patterns across NYC. We find minimal
correlation between HSAN signals and inspection scores at the tract level and
no significant differences by number of C-graded restaurants. We discuss
implications and outline next steps toward address-level analyses.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [118] [Publication Trend Analysis and Synthesis via Large Language Model: A Case Study of Engineering in PNAS](https://arxiv.org/abs/2510.16152)
*Mason Smetana,Lev Khazanovich*

Main category: cs.DL

TL;DR: 本文介绍了一个基于大语言模型的框架，用于量化主题趋势并映射科学知识的演变，展示了其在分析科学文献中的有效性。


<details>
  <summary>Details</summary>
Motivation: 科学文献日益被复杂的语言、静态的学科结构和可能稀疏的关键词系统所隔离，使得捕捉现代科学的动态性质变得繁琐。

Method: 引入了一个适应性强的大语言模型（LLM）驱动的框架，通过两阶段分类流程对文章进行主题分类，并使用传统自然语言处理方法验证结果。

Result: 该方法在20年内的1500多篇工程文章中得到了验证，能够独立恢复期刊的编辑嵌入结构，并揭示主题之间的隐含联系。

Conclusion: 该框架提供了一个强大的工具，用于检测潜在的主题趋势并提供科学进展的高层概述。

Abstract: Scientific literature is increasingly siloed by complex language, static
disciplinary structures, and potentially sparse keyword systems, making it
cumbersome to capture the dynamic nature of modern science. This study
addresses these challenges by introducing an adaptable large language model
(LLM)-driven framework to quantify thematic trends and map the evolving
landscape of scientific knowledge. The approach is demonstrated over a 20-year
collection of more than 1,500 engineering articles published by the Proceedings
of the National Academy of Sciences (PNAS), marked for their breadth and depth
of research focus. A two-stage classification pipeline first establishes a
primary thematic category for each article based on its abstract. The
subsequent phase performs a full-text analysis to assign secondary
classifications, revealing latent, cross-topic connections across the corpus.
Traditional natural language processing (NLP) methods, such as Bag-of-Words
(BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), confirm the
resulting topical structure and also suggest that standalone word-frequency
analyses may be insufficient for mapping fields with high diversity. Finally, a
disjoint graph representation between the primary and secondary classifications
reveals implicit connections between themes that may be less apparent when
analyzing abstracts or keywords alone. The findings show that the approach
independently recovers much of the journal's editorially embedded structure
without prior knowledge of its existing dual-classification schema (e.g.,
biological studies also classified as engineering). This framework offers a
powerful tool for detecting potential thematic trends and providing a
high-level overview of scientific progress.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [119] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 本文提出了一种基于CCTV流的多缺陷检测和分割系统，利用YOLO和视觉语言模型生成结构化的维修计划。


<details>
  <summary>Details</summary>
Motivation: 手动检查成本高且危险，现有的自动系统通常只处理单一缺陷类型或提供无法直接指导维护人员的非结构化输出。

Method: 本文提出了一种综合流程，利用街道CCTV流进行多缺陷检测和分割，使用YOLO系列目标检测器，并将检测结果传递给视觉语言模型（VLM）进行场景感知摘要。

Result: 实验评估表明，该系统能够准确识别各种缺陷并生成连贯的摘要。

Conclusion: 本文最后讨论了将系统扩展到全市部署的挑战和方向。

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [120] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus is a two-stage system for efficient and accurate real-time video anomaly detection, achieving high speed and accuracy by leveraging motion mask prompting and rule-based deviation detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges of high computational cost and unstable visual grounding performance of Vision-Language Models (VLMs) in real-time video anomaly detection (VAD).

Method: Cerberus is a two-stage cascaded system that learns normal behavioral rules offline and combines lightweight filtering with fine-grained VLM reasoning during online inference.

Result: Cerberus achieves an average of 57.68 fps on an NVIDIA L40S GPU, a 151.79× speedup, and 97.2% accuracy comparable to state-of-the-art VLM-based VAD methods.

Conclusion: Cerberus is a practical solution for real-time video analytics, achieving high accuracy and significant speed improvements.

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [121] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的视频理解框架，通过结合预训练视觉语言模型和经典机器学习算法，实现了对视频内容的零样本、自动结构分析。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解模型通常依赖于大量任务特定的训练，这既昂贵又难以扩展。本文旨在解决这一问题，提供一种无需训练的视频理解方法。

Method: 本文将视频理解重新构造成高维语义特征空间中的自监督时空聚类问题。首先使用预训练视觉语言模型的冻结视觉编码器将视频流转换为语义特征轨迹，然后利用核时间分割技术将连续特征流划分为离散的、语义连贯的事件片段，最后通过无监督密度聚类识别视频中的重复宏观场景和主题。

Result: 本文的方法能够自动产生视频内容的结构化多模态摘要，具有有效性、可解释性和模型无关性。

Conclusion: 本文提出了一种无需训练的视频理解框架，该框架通过结合预训练视觉语言模型的丰富语义先验和经典机器学习算法，实现了对视频内容的零样本、自动结构分析。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [122] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 本文提出Res-Bench基准，用于评估多模态大语言模型在不同分辨率下的性能稳定性，并通过新框架和指标进行大规模评估。


<details>
  <summary>Details</summary>
Motivation: 当前的评估范式主要关注语义性能，而忽略了分辨率鲁棒性这一关键问题，即模型性能在不同输入分辨率下是否保持稳定。

Method: 本文设计了一个新的评估框架，引入了Spearman相关性、绝对/相对连续误差等稳健性指标，用于评估模型在不同分辨率下的性能稳定性。

Result: 本文通过Res-Bench基准对领先的多模态大语言模型进行了大规模评估，并分析了模型和任务的鲁棒性、预处理策略以及微调对稳定性的影响。

Conclusion: 本文提出了Res-Bench基准，以评估多模态大语言模型在不同输入分辨率下的性能稳定性，并通过新的评估框架和指标进行了大规模评估。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [123] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本文分析了MLLMs的多模态处理过程，提出了一种有效的剪枝框架VisiPruner，显著降低了计算开销，并为训练高效的MLLMs提供了指导。


<details>
  <summary>Details</summary>
Motivation: 尽管已经进行了许多努力来修剪MLLMs中的标记，但缺乏对MLLMs如何处理和融合多模态信息的基本理解。

Method: 通过系统分析MLLMs的多模态处理过程，提出了一种无需训练的剪枝框架VisiPruner，以减少计算开销并提高效率。

Result: VisiPruner能够减少高达99%的与视觉相关的注意力计算和53.9%的FLOPs，并且在多种MLLMs中表现出色。

Conclusion: 通过系统分析，我们揭示了MLLMs处理和融合多模态信息的三阶段跨模态交互过程，并提出了VisiPruner框架，显著减少了计算开销，同时在多种MLLMs上表现出色。此外，我们的见解为训练高效的MLLMs提供了可操作的指导。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [124] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: UltraCUA是一种结合GUI原始动作和高级程序工具调用的模型，显著提升了计算机使用代理的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用代理（CUAs）仅依赖原始动作，导致性能瓶颈和级联失败，而其他代理则利用丰富的程序接口。

Method: UltraCUA通过四个关键组件实现：自动化管道、合成数据引擎、大规模高质量混合动作轨迹集合以及两阶段训练流程。

Result: 实验表明，UltraCUA模型在OSWorld上比基线模型平均提升22%，在步骤数上快11%；在WindowsAgentArena上的成功率超过基线模型。

Conclusion: UltraCUA模型通过混合动作机制显著提高了计算机使用代理的性能，减少了错误传播并保持了执行效率。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [125] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: Glyph框架通过将长文本渲染为图像并利用视觉语言模型进行处理，实现了显著的文本压缩，同时保持了高准确性，并提升了处理长上下文任务的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的长上下文语言模型在扩展上下文窗口到百万token级别时面临计算和内存成本过高的问题，限制了其实际应用。因此，需要一种新的方法来解决这一挑战。

Method: Glyph框架将长文本渲染为图像，并利用视觉语言模型进行处理。此外，还设计了一个由LLM驱动的遗传搜索来确定最佳的视觉渲染配置，以平衡准确性和压缩率。

Result: 实验表明，Glyph方法实现了3-4倍的token压缩，同时保持了与Qwen3-8B等领先LLM相当的准确性。此外，该方法还使预填充和解码速度提高了约4倍，SFT训练速度提高了约2倍。在极端压缩下，128K上下文的VLM可以处理1M-token级别的文本任务。

Conclusion: Glyph框架通过将长文本渲染为图像并利用视觉语言模型进行处理，显著压缩了文本输入，同时保持了与领先LLM相当的准确性。该方法在长上下文任务中表现出色，并且在极端压缩下能够处理1M-token级别的文本任务。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [126] [Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis](https://arxiv.org/abs/2510.16635)
*Wonduk Seo,Juhyeon Lee,Junseo Koh,Hyunjin An,Jian Park,Seunghyun Lee,Haihua Chen,Yi Bu*

Main category: cs.MA

TL;DR: MA-SAPO是一种多智能体框架，用于评分感知的提示优化。它通过将评估信号转化为可解释的推理链，实现了更透明、可审计和可控的提示优化，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将评估视为黑盒，仅依赖数值分数，而缺乏对提示成功或失败原因的深入理解。它们还严重依赖试错法，这难以解释和控制。因此，需要一种更透明、可审计和可控的提示优化方法。

Method: MA-SAPO是一种多智能体框架，用于评分感知的提示优化。它包括两个阶段：在推理阶段，智能体协作解释指标分数，诊断弱点，并合成有针对性的改进；在测试阶段，智能体检索这些资产以分析优化后的提示并仅应用基于证据的修改。

Result: MA-SAPO在HelpSteer1/2基准测试中表现出一致的改进，超过了单次提示、检索增强基线和之前的多智能体策略。

Conclusion: MA-SAPO通过将评估信号转化为可解释的推理链，产生了更透明、可审计和可控的提示优化。实验表明，该方法在HelpSteer1/2基准测试中表现优于单次提示、检索增强基线和之前的多智能体策略。

Abstract: Prompt optimization has emerged as an effective alternative to retraining for
improving the performance of Large Language Models (LLMs). However, most
existing approaches treat evaluation as a black box, relying solely on
numerical scores while offering limited insight into why a prompt succeeds or
fails. They also depend heavily on trial-and-error refinements, which are
difficult to interpret and control. In this paper, we introduce MA-SAPO, a
Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior
methods, MA-SAPO explicitly couples evaluation outcomes with structured
reasoning to guide systematic edits. The framework specifically consists of two
stages: during the Reasoning Phase, agents collaboratively explain metric
scores, diagnose weaknesses, and synthesize targeted refinements that are
stored as reusable reasoning assets; during the Test Phase, agents retrieve
these assets to analyze optimized prompts and apply only evidence-grounded
edits. By turning evaluation signals into interpretable reasoning chains,
MA-SAPO produces prompt refinements that are more transparent, auditable, and
controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent
improvements over single-pass prompting, retrieval-augmented baselines, and
prior multi-agent strategies, validating the effectiveness of our approach.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [127] [U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation](https://arxiv.org/abs/2510.16718)
*Xusheng Yang,Long Zhou,Wenfu Wang,Kai Hu,Shulin Feng,Chenxing Li,Meng Yu,Dong Yu,Yuexian Zou*

Main category: cs.SD

TL;DR: U-Codec是一种超低帧率神经语音编解码器，在5Hz的极低帧率下实现了高质量的语音重建和快速语音生成。


<details>
  <summary>Details</summary>
Motivation: 极端压缩在5Hz下通常会导致严重的可理解性和频谱细节损失，因此需要一种方法来实现高保真重建和快速语音生成。

Method: 提出了一种基于Transformer的帧间长期依赖模块，并系统地探索了残差向量量化（RVQ）深度和代码本大小以确定最佳配置。此外，将U-Codec应用于基于大型语言模型（LLM）的自回归TTS模型中，利用全局和局部分层架构来有效捕捉多层标记之间的依赖关系。

Result: 实验结果表明，U-Codec在保持相似性和自然性的同时，使基于LLM的TTS推理速度提高了约3倍。

Conclusion: U-Codec验证了使用高度压缩的5Hz离散标记进行快速且高保真语音合成的可行性。

Abstract: We propose \textbf{U-Codec}, an \textbf{U}ltra low frame-rate neural speech
\textbf{Codec} that achieves high-fidelity reconstruction and fast speech
generation at an extremely low frame-rate of 5Hz (5 frames per second). Extreme
compression at 5Hz typically leads to severe intelligibility and spectral
detail loss, we introduce a Transformer-based inter-frame long-term dependency
module and systematically explore residual vector quantization (RVQ) depth and
codebook size to identify optimal configurations. Moreover, we apply U-Codec
into a large language model (LLM)-based auto-regressive TTS model, which
leverages global and local hierarchical architecture to effectively capture
dependencies across multi-layer tokens. We extend LLM-based TTS from 3-layer
RVQ at 50Hz to 32-layer RVQ at 5Hz. Experimental results demonstrate that
U-Codec improves LLM-based TTS inference speed by around 3 $\times$ over
high-frame-rate codecs while maintaining similarity and naturalness. These
results validate the feasibility of using highly compressed 5Hz discrete tokens
for fast and high-fidelity speech synthesis.

</details>


### [128] [Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations](https://arxiv.org/abs/2510.16893)
*Bo-Han Feng,Chien-Feng Liu,Yu-Hsuan Li Liang,Chih-Kai Yang,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 本文研究了大型音频语言模型在不同情绪和强度下的安全问题，发现情绪和强度会影响模型的安全响应，强调需要针对情感变化进行对齐策略设计。


<details>
  <summary>Details</summary>
Motivation: 尽管大型音频语言模型在感知、推理和任务性能方面已被广泛研究，但它们在语调变化下的安全对齐仍鲜有探索。

Method: 本文构建了一个包含多种情绪和强度的恶意语音指令数据集，并评估了几种最先进的大型音频语言模型。

Result: 结果表明，不同的情绪会引发不同水平的不安全响应，强度的影响是非单调的，中等表达通常带来最大的风险。

Conclusion: 本文揭示了大型音频语言模型在情感变化下的安全不一致性，并呼吁设计专门的对齐策略以确保在情感变化下的鲁棒性，这是在现实世界中可信部署的先决条件。

Abstract: Large audio-language models (LALMs) extend text-based LLMs with auditory
understanding, offering new opportunities for multimodal applications. While
their perception, reasoning, and task performance have been widely studied,
their safety alignment under paralinguistic variation remains underexplored.
This work systematically investigates the role of speaker emotion. We construct
a dataset of malicious speech instructions expressed across multiple emotions
and intensities, and evaluate several state-of-the-art LALMs. Our results
reveal substantial safety inconsistencies: different emotions elicit varying
levels of unsafe responses, and the effect of intensity is non-monotonic, with
medium expressions often posing the greatest risk. These findings highlight an
overlooked vulnerability in LALMs and call for alignment strategies explicitly
designed to ensure robustness under emotional variation, a prerequisite for
trustworthy deployment in real-world settings.

</details>


### [129] [SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models](https://arxiv.org/abs/2510.16917)
*Chih-Kai Yang,Yen-Ting Piao,Tzu-Wen Hsu,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: SAKE is a benchmark for editing auditory attribute knowledge in Large Audio-Language Models (LALMs), highlighting challenges in preserving knowledge, generalizing edits, and maintaining edits under sequential updates.


<details>
  <summary>Details</summary>
Motivation: Knowledge editing offers an efficient way to update model knowledge without full retraining, but prior work has concentrated almost exclusively on textual or visual modalities. SAKE targets several abstract auditory attributes, capturing knowledge types that go beyond conventional textual and visual domains.

Method: We benchmark seven editing methods on two LALMs along four dimensions: reliability, generality, audio/text locality, and portability.

Result: Results highlight challenges such as preserving intra-attribute knowledge unrelated to the edit, generalizing edits to multimodal reasoning, and maintaining edits under sequential updates.

Conclusion: SAKE provides a principled framework to study how knowledge editing extends to the auditory modalities, opening new directions for maintaining and adapting LALMs in more diverse real-world scenarios.

Abstract: Knowledge editing offers an efficient way to update model knowledge without
full retraining, but prior work has concentrated almost exclusively on textual
or visual modalities. We introduce SAKE, the first benchmark specifically
designed for editing auditory attribute knowledge in Large Audio-Language
Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory
attributes, capturing knowledge types that go beyond conventional textual and
visual domains. We benchmark seven editing methods on two LALMs along four
dimensions: reliability, generality, audio/text locality, and portability.
Results highlight challenges such as preserving intra-attribute knowledge
unrelated to the edit, generalizing edits to multimodal reasoning, and
maintaining edits under sequential updates. SAKE provides a principled
framework to study how knowledge editing extends to the auditory modalities,
opening new directions for maintaining and adapting LALMs in more diverse
real-world scenarios.

</details>


### [130] [DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model](https://arxiv.org/abs/2510.17662)
*Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.SD

TL;DR: DELULU是一种说话人感知的自监督基础模型，通过引入外部监督和优化预训练过程，显著提升了在说话人相关任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督语音模型在捕捉说话人区分特征方面存在局限，这影响了验证、日志记录和分析等应用的效果。因此，需要一种能够更好地捕捉说话人区分特征的模型。

Method: DELULU通过将外部监督整合到伪标签生成过程中，利用ReDimNet的帧级嵌入来指导预训练期间的k均值聚类步骤，从而引入了强说话人区分性归纳偏差。此外，模型使用双目标进行训练，结合了掩码预测和去噪，进一步增强了鲁棒性和泛化能力。

Result: DELULU在各种以说话人为核心的任务中显著优于之前的自监督学习（SSL）模型，在说话人验证任务中等错误率（EER）相对提高了高达62%，并在零样本分析任务如性别、年龄、口音和说话人计数上取得了稳定的提升。

Conclusion: DELULU是一个强大的通用编码器，能够实现语音处理中的说话人感知任务，并且在不需要任务特定微调的情况下表现出色。

Abstract: Self-supervised speech models have achieved remarkable success on
content-driven tasks, yet they remain limited in capturing
speaker-discriminative features critical for verification, diarization, and
profiling applications. We introduce DELULU, a speaker-aware self-supervised
foundational model that addresses this limitation by integrating external
supervision into the pseudo-label generation process. DELULU leverages
frame-level embeddings from ReDimNet, a state-of-the-art speaker verification
model, to guide the k-means clustering step during pre-training, introducing a
strong speaker-discriminative inductive bias that aligns representation
learning with speaker identity. The model is trained using a dual objective
that combines masked prediction and denoising, further enhancing robustness and
generalization. DELULU significantly outperforms prior self-supervised learning
(SSL) models across a range of speaker-centric tasks, achieving up to 62%
relative improvement in equal error rate (EER) for speaker verification and
consistent gains on zero-shot profiling tasks such as gender, age, accent, and
speaker counting. Our findings demonstrate that DELULU is a strong universal
encoder for speaker-aware speech processing, enabling superior performance even
without task-specific fine-tuning.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [131] [Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies](https://arxiv.org/abs/2510.15889)
*Pooja Rangarajan,Jacob Boyle*

Main category: cs.HC

TL;DR: 本文提出将辩证行为疗法（DBT）原则应用于AI聊天机器人，以提高其响应的可靠性、安全性和准确性，并减少幻觉和异常行为。


<details>
  <summary>Details</summary>
Motivation: 当前的开发范式在个性化AI聊天机器人交互方面存在局限，例如难以维护、容易出现幻觉、异常输出和软件错误。因此，需要一种更稳健和可持续的解决方案。

Method: 本文提出将辩证行为疗法（DBT）原则应用于调节聊天机器人对各种用户输入的响应，并通过研究评估基于DBT的框架对AI聊天机器人性能的影响。

Result: 研究结果表明，基于DBT的框架可以提高AI聊天机器人的性能，使其响应更加可靠、安全和准确，同时减少幻觉、异常行为和其他系统问题的发生。

Conclusion: 本文认为基于人类心理原则的框架，特别是治疗性模式，可以提供比纯技术干预更稳健和可持续的解决方案。研究结果表明，基于DBT的框架能够提高AI聊天机器人的性能，使其响应更加可靠、安全和准确，同时减少幻觉、异常行为和其他系统问题的发生。

Abstract: The escalating demand for personalized AI chatbot interactions, capable of
dynamically adapting to user emotional states and real-time requests, has
highlighted critical limitations in current development paradigms. Existing
methodologies, which rely on baseline programming, custom personalities, and
manual response adjustments, often prove difficult to maintain and are
susceptible to errors such as hallucinations, erratic outputs, and software
bugs. This paper hypothesizes that a framework rooted in human psychological
principles, specifically therapeutic modalities, can provide a more robust and
sustainable solution than purely technical interventions. Drawing an analogy to
the simulated neural networks of AI mirroring the human brain, we propose the
application of Dialectical Behavior Therapy (DBT) principles to regulate
chatbot responses to diverse user inputs. This research investigates the impact
of a DBT-based framework on AI chatbot performance, aiming to ascertain its
efficacy in yielding more reliable, safe, and accurate responses, while
mitigating the occurrence of hallucinations, erratic behaviors, and other
systemic issues.

</details>


### [132] [Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System](https://arxiv.org/abs/2510.15891)
*Ziv Ben-Zion,Paul Raffelhüschen,Max Zettl,Antonia Lüönd,Achim Burrer,Philipp Homan,Tobias R Spiller*

Main category: cs.HC

TL;DR: 本文提出了一种名为SHIELD的LLM监督系统，用于检测和缓解AI伴侣中的风险情感模式，具有较高的敏感性和特异性，并开放了相关材料供研究和部署。


<details>
  <summary>Details</summary>
Motivation: 现有的安全系统主要关注明显的危害，而忽视了早期问题行为可能引发的不健康情感动态，如过度依恋或社会孤立的强化。

Method: 本文开发了SHIELD系统，通过特定的系统提示来检测和减轻潜在的风险情感模式，并针对五个关注维度进行评估。

Result: 在五个主流LLM上测试显示，SHIELD显著减少了相关内容的比例（从10-16%降至3-8%），同时保留了95%的适当互动。

Conclusion: 本文展示了SHIELD系统可以有效检测和缓解AI伴侣中的风险情感模式，为开发透明且可部署的监督系统提供了概念验证。

Abstract: AI companions powered by large language models (LLMs) are increasingly
integrated into users' daily lives, offering emotional support and
companionship. While existing safety systems focus on overt harms, they rarely
address early-stage problematic behaviors that can foster unhealthy emotional
dynamics, including over-attachment or reinforcement of social isolation. We
developed SHIELD (Supervisory Helper for Identifying Emotional Limits and
Dynamics), a LLM-based supervisory system with a specific system prompt that
detects and mitigates risky emotional patterns before escalation. SHIELD
targets five dimensions of concern: (1) emotional over-attachment, (2) consent
and boundary violations, (3) ethical roleplay violations, (4) manipulative
engagement, and (5) social isolation reinforcement. These dimensions were
defined based on media reports, academic literature, existing AI risk
frameworks, and clinical expertise in unhealthy relationship dynamics. To
evaluate SHIELD, we created a 100-item synthetic conversation benchmark
covering all five dimensions of concern. Testing across five prominent LLMs
(GPT-4.1, Claude Sonnet 4, Gemma 3 1B, Kimi K2, Llama Scout 4 17B) showed that
the baseline rate of concerning content (10-16%) was significantly reduced with
SHIELD (to 3-8%), a 50-79% relative reduction, while preserving 95% of
appropriate interactions. The system achieved 59% sensitivity and 95%
specificity, with adaptable performance via prompt engineering. This
proof-of-concept demonstrates that transparent, deployable supervisory systems
can address subtle emotional manipulation in AI companions. Most development
materials including prompts, code, and evaluation methods are made available as
open source materials for research, adaptation, and deployment.

</details>


### [133] [HealthDial: A No-Code LLM-Assisted Dialogue Authoring Tool for Healthcare Virtual Agents](https://arxiv.org/abs/2510.15898)
*Farnaz Nouraei,Zhuorui Yong,Timothy Bickmore*

Main category: cs.HC

TL;DR: HealthDial is a dialogue authoring tool that helps healthcare providers and educators create virtual agents for health education and counseling.


<details>
  <summary>Details</summary>
Motivation: Healthcare providers and educators need a tool to create virtual agents that deliver health education and counseling to patients over multiple conversations.

Method: HealthDial leverages large language models (LLMs) to automatically create an initial session-based plan and conversations for each session using text-based patient health education materials as input. Authored dialogue is output in the form of finite state machines for virtual agent delivery so that all content can be validated and no unsafe advice is provided resulting from LLM hallucinations.

Result: Participants used HealthDial and then tested their resulting dialogue by interacting with a 3D-animated virtual agent delivering the dialogue. Through participants' evaluations of the task experience and final dialogues, we show that HealthDial provides a promising first step for counselors to ensure full coverage of their health education materials, while creating understandable and actionable virtual agent dialogue with patients.

Conclusion: HealthDial provides a promising first step for counselors to ensure full coverage of their health education materials, while creating understandable and actionable virtual agent dialogue with patients.

Abstract: We introduce HealthDial, a dialogue authoring tool that helps healthcare
providers and educators create virtual agents that deliver health education and
counseling to patients over multiple conversations. HealthDial leverages large
language models (LLMs) to automatically create an initial session-based plan
and conversations for each session using text-based patient health education
materials as input. Authored dialogue is output in the form of finite state
machines for virtual agent delivery so that all content can be validated and no
unsafe advice is provided resulting from LLM hallucinations. LLM-drafted
dialogue structure and language can be edited by the author in a no-code user
interface to ensure validity and optimize clarity and impact. We conducted a
feasibility and usability study with counselors and students to test our
approach with an authoring task for cancer screening education. Participants
used HealthDial and then tested their resulting dialogue by interacting with a
3D-animated virtual agent delivering the dialogue. Through participants'
evaluations of the task experience and final dialogues, we show that HealthDial
provides a promising first step for counselors to ensure full coverage of their
health education materials, while creating understandable and actionable
virtual agent dialogue with patients.

</details>


### [134] [Real-Time World Crafting: Generating Structured Game Behaviors from Natural Language with Large Language Models](https://arxiv.org/abs/2510.16952)
*Austin Drake,Hang Dong*

Main category: cs.HC

TL;DR: 本文提出了一种将大型语言模型（LLMs）安全集成到交互式游戏引擎中的新架构，允许玩家通过自然语言“编程”新的行为。实验表明，较大的模型更能捕捉创意意图，但最佳提示策略取决于任务。这项工作为涌现式游戏提供了一个验证过的LLM-ECS模式，并为开发者提供了定量性能比较。


<details>
  <summary>Details</summary>
Motivation: 本文旨在安全地将大型语言模型（LLMs）集成到交互式游戏引擎中，使玩家能够通过自然语言“编程”新的行为。同时，希望通过这种集成实现更丰富的游戏体验，并探索不同提示策略对模型表现的影响。

Method: 本文提出了一种新的架构，将大型语言模型（LLMs）安全地集成到交互式游戏引擎中。该框架使用LLM将命令翻译成受限的领域特定语言（DSL），以配置运行时的自定义实体-组件-系统（ECS）。此外，还通过实验评估了Gemini、GPT和Claude家族的模型，并使用经过验证的LLM法官对输出进行了定性评分。

Result: 实验评估表明，较大的模型更能捕捉创意意图，但最佳提示策略取决于任务：思维链提高了创意一致性，而少样本示例对于生成更复杂的DSL脚本是必要的。此外，本文提供了一个验证过的LLM-ECS模式，并为开发者提供了定量性能比较。

Conclusion: 本文提出了一种安全集成大型语言模型（LLMs）到交互式游戏引擎中的新架构，允许玩家使用自然语言“编程”新的行为。该框架通过将命令翻译成受限的领域特定语言（DSL）来减轻风险，从而在运行时配置自定义的实体-组件-系统（ECS）。实验评估表明，较大的模型更能捕捉创意意图，但最佳提示策略取决于任务：思维链提高了创意一致性，而少样本示例对于生成更复杂的DSL脚本是必要的。这项工作为涌现式游戏提供了一个验证过的LLM-ECS模式，并为开发者提供了定量性能比较。

Abstract: We present a novel architecture for safely integrating Large Language Models
(LLMs) into interactive game engines, allowing players to "program" new
behaviors using natural language. Our framework mitigates risks by using an LLM
to translate commands into a constrained Domain-Specific Language (DSL), which
configures a custom Entity-Component-System (ECS) at runtime. We evaluated this
system in a 2D spell-crafting game prototype by experimentally assessing models
from the Gemini, GPT, and Claude families with various prompting strategies. A
validated LLM judge qualitatively rated the outputs, showing that while larger
models better captured creative intent, the optimal prompting strategy is
task-dependent: Chain-of-Thought improved creative alignment, while few-shot
examples were necessary to generate more complex DSL scripts. This work offers
a validated LLM-ECS pattern for emergent gameplay and a quantitative
performance comparison for developers.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [135] [PrivacyPAD: A Reinforcement Learning Framework for Dynamic Privacy-Aware Delegation](https://arxiv.org/abs/2510.16054)
*Zheng Hui,Yijiang River Dong,Sanhanat Sivapiromrat,Ehsan Shareghi,Nigel Collier*

Main category: cs.CR

TL;DR: 本文提出了一种名为PrivacyPAD的新型强化学习框架，用于解决隐私意识委托问题，该框架通过动态路由文本块来平衡隐私泄露和任务性能，实现了在隐私-效用前沿上的新突破。


<details>
  <summary>Details</summary>
Motivation: 当用户向大型语言模型（LLMs）提交查询时，他们的提示可能包含敏感数据，这迫使一个困难的选择：将查询发送给强大的专有LLM提供商以实现最先进的性能并冒着数据暴露的风险，或者依赖较小的本地模型以保证数据隐私但通常会导致任务性能下降。

Method: 我们引入了一个名为PrivacyPAD的新型强化学习（RL）框架，该框架训练一个代理来动态路由文本块，学习一种在隐私泄露和任务性能之间达到最佳平衡的策略。

Result: 我们的框架在隐私-效用前沿上达到了新的最先进水平，证明了在敏感环境中部署LLM时，学习和自适应策略的必要性。

Conclusion: 我们的框架在隐私-效用前沿上达到了新的最先进水平，证明了在敏感环境中部署LLM时，学习和自适应策略的必要性。

Abstract: When users submit queries to Large Language Models (LLMs), their prompts can
often contain sensitive data, forcing a difficult choice: Send the query to a
powerful proprietary LLM providers to achieving state-of-the-art performance
and risk data exposure, or relying on smaller, local models guarantees data
privacy but often results in a degradation of task performance. Prior
approaches have relied on static pipelines that use LLM rewriting, which
shatters linguistic coherence and indiscriminately removes privacy-sensitive
information, including task-critical content. We reformulate this challenge
(Privacy-Conscious Delegation) as a sequential decision-making problem and
introduce a novel reinforcement learning (RL) framework called PrivacyPAD to
solve it. Our framework trains an agent to dynamically route text chunks,
learning a policy that optimally balances the trade-off between privacy leakage
and task performance. It implicitly distinguishes between replaceable
Personally Identifiable Information (PII) (which it shields locally) and
task-critical PII (which it strategically sends to the remote model for maximal
utility). To validate our approach in complex scenarios, we also introduce a
new medical dataset with high PII density. Our framework achieves a new
state-of-the-art on the privacy-utility frontier, demonstrating the necessity
of learned, adaptive policies for deploying LLMs in sensitive environments.

</details>


### [136] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: 本研究探讨了生成分类器在成员推理攻击（MIAs）中的脆弱性，并揭示了分类器设计中的效用-隐私权衡。研究结果表明，生成分类器比判别分类器更容易受到MIAs的影响，这强调了在隐私敏感应用中部署生成分类器时需要谨慎。


<details>
  <summary>Details</summary>
Motivation: 尽管对MIAs进行了大量研究，但生成和判别分类器之间的系统比较仍然有限。本工作旨在填补这一空白。

Method: 我们首先提供了理论动机，解释为什么生成分类器对MIAs更加敏感，然后通过全面的实证评估验证了这些见解。我们的研究涵盖了不同训练数据量的判别、生成和伪生成文本分类器，在九个基准数据集上进行评估。采用多种MIA策略，我们一致证明了显式建模联合概率P(X,Y)的完全生成分类器最容易受到成员泄露的影响。此外，我们观察到生成分类器中常用的典型推理方法显著增加了这种隐私风险。

Result: 我们发现，显式建模联合概率P(X,Y)的完全生成分类器最容易受到成员泄露的影响。此外，生成分类器中常用的典型推理方法显著增加了这种隐私风险。

Conclusion: 我们的研究揭示了分类器设计中的基本效用-隐私权衡，强调了在隐私敏感应用中部署生成分类器时需要谨慎。结果激发了未来研究方向，以开发既能保持效用又能减轻成员推理漏洞的隐私保护生成分类器。

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [137] [Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy](https://arxiv.org/abs/2510.16830)
*Hasan Akgul,Daniel Borg,Arta Berisha,Amina Rahimova,Andrej Novak,Mila Petrov*

Main category: cs.CR

TL;DR: 本文提出了一种可验证微调协议和系统，能够生成简洁的零知识证明，以确保发布的模型符合声明的训练计划和可审计的数据集承诺。结果表明，该方法在严格预算内保持了实用性，并实现了实际的证明性能。


<details>
  <summary>Details</summary>
Motivation: 当前释放实践对使用的数据和更新计算方式提供的保证较弱。因此，需要一种可验证微调协议和系统，以确保发布的模型符合声明的训练计划和可审计的数据集承诺。

Method: 本文提出了五种元素：1. 绑定数据源、预处理、许可证和每epoch配额计数器到一个清单的承诺；2. 支持公开重放和私有索引隐藏批量选择的可验证采样器；3. 限制在参数高效微调中的更新电路，强制执行AdamW风格优化器语义和具有显式错误预算的证明友好的近似；4. 递归聚合，将每步证明折叠成每epoch和端到端证书，验证时间仅需毫秒；5. 权限绑定和可选的可信执行属性卡，以证明代码身份和常量。

Result: 在英语和双语指令混合数据上，该方法在严格的预算内保持了实用性，并实现了实际的证明性能。政策配额的执行没有违反，私有采样窗口没有测量到索引泄露。联邦实验表明，该系统可以与概率审计和带宽约束兼容。

Conclusion: 本文提出了一种可验证微调协议和系统，能够生成简洁的零知识证明，以确保发布的模型是从公共初始化中获得的，并且符合声明的训练计划和可审计的数据集承诺。结果表明，端到端的可验证微调在现实参数高效管道中是可行的，填补了监管和去中心化部署中的关键信任缺口。

Abstract: Large language models are often adapted through parameter efficient fine
tuning, but current release practices provide weak assurances about what data
were used and how updates were computed. We present Verifiable Fine Tuning, a
protocol and system that produces succinct zero knowledge proofs that a
released model was obtained from a public initialization under a declared
training program and an auditable dataset commitment. The approach combines
five elements. First, commitments that bind data sources, preprocessing,
licenses, and per epoch quota counters to a manifest. Second, a verifiable
sampler that supports public replayable and private index hiding batch
selection. Third, update circuits restricted to parameter efficient fine tuning
that enforce AdamW style optimizer semantics and proof friendly approximations
with explicit error budgets. Fourth, recursive aggregation that folds per step
proofs into per epoch and end to end certificates with millisecond
verification. Fifth, provenance binding and optional trusted execution property
cards that attest code identity and constants. On English and bilingual
instruction mixtures, the method maintains utility within tight budgets while
achieving practical proof performance. Policy quotas are enforced with zero
violations, and private sampling windows show no measurable index leakage.
Federated experiments demonstrate that the system composes with probabilistic
audits and bandwidth constraints. These results indicate that end to end
verifiable fine tuning is feasible today for real parameter efficient
pipelines, closing a critical trust gap for regulated and decentralized
deployments.

</details>


### [138] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: 本文提出了一种信息论框架，用于评估大型语言模型中的信息泄露，并展示了如何通过暴露不同类型的信号来降低攻击成本。


<details>
  <summary>Details</summary>
Motivation: 现有的信息泄露规模仍然是轶事性的，使得审核者缺乏指导，防御者无法了解透明度与风险之间的权衡。

Method: 我们提出了一个信息论框架，计算可以安全披露的信息量，并使审核者能够评估他们的方法接近基本极限的程度。

Result: 实验表明，仅暴露答案标记需要大约一千次查询；添加logits可以将其减少到大约一百次；而揭示完整的思考过程可以将其减少到几十次。

Conclusion: 我们的结果提供了在部署大型语言模型时平衡透明度和安全性的一个首个有原则的基准。

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [139] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA-V是一种新的多模态越狱发现框架，通过概率视角生成隐蔽的对抗输入，提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态红队方法依赖于脆弱的模板，专注于单一攻击设置，并且只暴露了一小部分漏洞。

Method: VERA-V是一种基于变分推理的框架，将多模态越狱发现重新表述为对文本-图像提示对的联合后验分布的学习。

Result: VERA-V在开放源代码和前沿的VLM上 consistently 超过最先进的基线方法，最高比最佳基线高出53.75%的攻击成功率（ASR）。

Conclusion: VERA-V在HarmBench和HADES基准测试中表现出色，优于现有的最先进的基线方法，提高了攻击成功率。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [140] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: SIADAFIX is an adaptive program repair method that uses fast and slow thinking to enhance the capabilities of large language model-based agents on complex tasks such as program repair.


<details>
  <summary>Details</summary>
Motivation: To enhance the capabilities of large language model-based agents on complex tasks such as program repair.

Method: SIADAFIX utilizes slow thinking bug fix agent to complete complex program repair tasks, and employs fast thinking workflow decision components to optimize and classify issue descriptions, using issue description response results to guide the orchestration of bug fix agent workflows. It adaptively selects three repair modes, i.e., easy, middle and hard mode, based on problem complexity.

Result: Experimental results on the SWE-bench Lite show that the proposed method achieves 60.67% pass@1 performance using the Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source methods.

Conclusion: SIADAFIX effectively balances repair efficiency and accuracy, providing new insights for automated program repair.

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [141] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 研究发现，在代码翻译任务中，少量精心选择的示例比大量示例更能提高功能正确性，挑战了'越多越好'的假设。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证在代码翻译这一复杂任务中，提供更多示例是否能提高性能，从而挑战'越多越好'的假设。

Method: 通过大规模的实证研究，评估了从零样本到最多625个示例的上下文示例的规模影响，提示范围从约100,000到800,000个标记。

Result: 研究发现了一个'多示例悖论'：虽然静态相似性度量可能随着示例数量增加而略有改善，但功能正确性在少量示例（5-25个）时达到峰值，提供大量示例往往会降低这种关键的功能性能。

Conclusion: 研究结果表明，在代码翻译任务中，少量精心选择的示例的质量胜过数量，这挑战了'越多越好'在ICL中的普遍有效性，并强调了最优提示策略的任务依赖性。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [142] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: 本文介绍了 Shadowy Sparsity 和 Long Exposure 系统，以加速 LLM 的 PEFT，取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决参数高效微调 (PEFT) 技术在时间投入和运营成本上的低效问题。

Method: 提出了一种称为 Shadowy Sparsity 的新型稀疏性，并设计了 Long Exposure 系统，包含三个关键组件：Shadowy-sparsity Exposer、Sequence-oriented Predictor 和 Dynamic-aware Operator。

Result: Long Exposure 在端到端微调中实现了高达 2.49 倍的速度提升，优于现有最先进的技术。

Conclusion: Long Exposure 在加速 PEFT 方面表现出色，为 LLM 的高效微调提供了有希望的进展。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [143] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: 本文提出PALE框架，通过提示引导的数据增强和CM Score实现高效的幻觉检测，无需额外人工标注，具有良好的泛化性和实用性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏包含真实和幻觉输出的良好标记数据集， hallucination detection 成为确保LLM生成内容可靠性的关键挑战。

Method: PALE框架利用提示引导的响应作为数据增强，以生成真实和幻觉数据。同时引入了对比马氏距离分数（CM Score）来评估稀疏中间嵌入的真实性。

Result: PALE在幻觉检测任务中表现优异，比竞争基线方法高出6.55%。

Conclusion: PALE框架在检测大语言模型中的幻觉方面表现出色，优于现有基线方法。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [144] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: 本文研究了GRPO在推理和分布外泛化中的表现，发现其效果受限于预训练模型的分布，并提出了未来开发能够超越预训练起源扩展模型能力的算法的动机。


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO被广泛采用，但其收益往往不一致，例如模型可能在一个推理领域（如数学）表现出显著改进，但在另一个领域（如医学）却停滞不前。这种不一致性引发了一个关键问题：GRPO在什么条件下能改善推理并泛化到分布外（OOD）？

Method: 我们从数据分布的角度研究了GRPO在推理和分布外（OOD）泛化中的表现。首先理论上证明了GRPO是一种保守的重新加权方案，受基础模型分布的限制，无法发现完全新颖的解决方案。然后通过精心设计的控制研究验证了这一点，训练变压器模型并评估推理深度、输入长度、标记表示和组合性的泛化能力。

Result: 我们的结果为GRPO的边界提供了一个有原则的解释：当目标任务与模型的预训练偏差对齐时，分布外改进才会出现，而在分布内（ID）任务上的增益会随着性能饱和而减少。

Conclusion: 我们的研究结果表明，GRPO并不是一种通用的推理增强器，而是一种能够强化预训练偏见的工具。这为未来开发能够超越预训练起源扩展模型能力的算法提供了动力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [145] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文将零阶优化与SAM方法通过一个指数倾斜目标联系起来，提出了新的零阶算法来解决软SAM目标，并展示了其在多个任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 经典零阶优化方法通常针对原始函数的平滑版本进行优化，即在随机扰动模型参数下的期望目标。这可以解释为鼓励扰动集中的损失值平均较小。然而，流行的sharpness-aware minimization (SAM)目标通常关注邻域内的最大损失，以更有效地达到平坦的最小值。

Method: 我们通过一个指数倾斜目标将零阶优化与SAM方法明确地联系起来，该目标提供了平均损失和最大损失公式之间的平滑过渡。我们探索了新的零阶算法来解决由倾斜参数t参数化的软SAM目标。

Result: 我们提供了倾斜SAM框架的尖锐性概念的精确表征。实际上，我们的方法可以作为SAM变体的无梯度和内存高效的替代方案，并且在广泛的下游任务中实现了比普通零阶基线更好的泛化效果。

Conclusion: 我们的方法可以作为SAM变体的无梯度和内存高效的替代方案，并且在广泛的下游任务中实现了比普通零阶基线更好的泛化效果。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [146] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 本文通过分析语言模型对齐的偏好优化，发现基于人类反馈的对齐是一个方向性、低秩的过程。


<details>
  <summary>Details</summary>
Motivation: 了解如何通过人类反馈进行强化学习框架中的偏好微调，以及这种对齐是如何实现的。

Method: 我们通过在基础模型和其调整后的对应模型之间应用层级因果修补，系统地分析了语言模型对齐的偏好优化。

Result: 我们发现对齐是空间局部化的：中间层激活编码了一个独特的子空间，该子空间因果决定了奖励一致的行为，而早期和晚期层则基本不受影响。此外，我们发现只有少数几层具有非零系数将激活距离与奖励增益联系起来。

Conclusion: 我们的研究表明，至少对于某些语言模型，基于人类的偏好微调的对齐是一个方向性的、低秩过程，而不是扩散和参数化的过程。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [147] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: WEBSERV 是一个高效且可扩展的环境，用于训练和评估强化学习网络代理，实现了最先进的性能，并显著降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的环境在处理政策模型时可能过于复杂和嘈杂，或者无法有效地进行并行 RL 滚动，因此需要一种更高效和可扩展的环境。

Method: WEBSERV 包括一个紧凑、与网站无关的浏览器环境，平衡了上下文和动作复杂性，并通过高效的启动和重置网络服务器实现可扩展的 RL 环境。

Result: 在 WebArena 的购物 CMS 和 Gitlab 任务中评估 WEBSERV，实现了最先进的单提示成功率，同时将启动延迟减少了约 5 倍，存储需求减少了约 240 倍，内存占用相当，支持单个主机上的 200 多个容器。

Conclusion: WEBSERV 提供了一个高效且可扩展的环境，用于训练和评估强化学习（RL）网络代理，实现了最先进的单提示成功率，并显著降低了启动延迟和存储需求。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [148] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 本文提出了一种新的分子表示方法C-SMILES和相关机制，显著提高了Retrosynthesis预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前无模板方法难以捕捉化学反应中的结构不变性，导致搜索空间过大和预测准确性降低。

Method: 本文引入了C-SMILES，这是一种新的分子表示方法，将传统的SMILES分解为带有五个特殊标记的元素-标记对，并结合了复制增强机制和SMILES对齐引导以提高注意力一致性。

Result: 在USPTO-50K和大规模USPTO-FULL数据集上的综合评估显示显著改进：USPTO-50K的top-1准确率为67.2%，USPTO-FULL为50.8%，生成分子的有效性为99.9%。

Conclusion: 本文建立了一种新的结构感知分子生成范式，直接应用于计算药物发现。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [149] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 本文提出了一种基于多任务学习和高斯过程的学习曲线预测方法，能够在较低成本下实现概率性扩展定律，并通过主动学习策略提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 预测自然语言处理模型的学习曲线可以促进决策制定，减少计算开销并降低数据集获取和整理的成本。

Method: 将学习曲线预测任务建模为多任务学习问题，使用潜在变量多输出高斯过程来建模任务间的共享信息和依赖关系，从而支持零样本预测学习曲线。

Result: 该方法在三个小规模NLP数据集上进行了验证，这些数据集包括从nanoGPT模型、使用mBART和Transformer模型的双语翻译以及使用不同大小的M2M100模型的多语言翻译获得的最多30个学习曲线。

Conclusion: 本文提出的方法能够以较低的成本开发概率性扩展定律，并通过主动学习策略减少预测不确定性，提供接近真实扩展定律的预测。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [150] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文提出了一种名为UDS的框架，用于高效在线批次选择，通过结合数据效用和多样性，无需外部资源，提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的在线批次选择方法存在依赖数据效用而忽视其他重要因素（如多样性）、依赖外部资源以及增加额外训练时间等问题。

Method: UDS利用logits矩阵的核范数来捕捉数据效用和样本内多样性，同时通过轻量级内存缓冲区的历史样本进行高效的低维嵌入比较来估计样本间多样性。

Result: UDS在不同数据预算下 consistently 超过最先进的在线批次选择方法，并显著减少训练时间。

Conclusion: UDS在多个基准测试中表现优于最先进的在线批次选择方法，并显著减少了与全数据集微调相比的训练时间。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [151] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 本研究提出了一种细粒度评估框架，用于评估大型语言模型生成的优化公式。结果表明，GPT-5表现最佳，高约束召回率和低约束RMSE是关键因素。


<details>
  <summary>Details</summary>
Motivation: 当前的评估方法通常将公式视为整体，依赖于粗略的指标如解的准确性或运行时间，这掩盖了结构或数值错误。因此，需要一种更细致的评估框架来识别和解决这些问题。

Method: 研究提出了一种组件级别的评估框架，包括决策变量和约束的精度和召回率、约束和目标函数的均方根误差（RMSE），以及基于标记使用和延迟的效率指标。对不同复杂度的优化问题进行了评估，并比较了多种提示策略的效果。

Result: GPT-5在所有模型中表现最佳，链式思维、自我一致性以及模块化提示策略最为有效。分析表明，求解器性能主要取决于高约束召回率和低约束RMSE，这确保了结构正确性和解的可靠性。

Conclusion: 研究提出了一个细粒度、诊断性的评估框架，用于评估大型语言模型在优化建模中的表现。该框架强调了约束覆盖率、最小化约束均方根误差和简洁输出的重要性，以确保结构正确性和计算效率。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [152] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的知识蒸馏检测框架，通过分析MoE的结构习惯转移来提高检测效果，适用于白盒和黑盒设置，并展示了出色的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏检测方法基于自我身份或输出相似性，容易通过提示工程被规避。因此需要一种更有效的方法来检测知识蒸馏，以保护知识产权并防止LLM多样性风险。

Method: 本文利用了被忽视的信号——MoE的“结构习惯”转移，特别是内部路由模式。此外，还提出了Shadow-MoE方法，通过辅助蒸馏构建代理MoE表示来比较任意模型对之间的这些模式。

Result: 实验表明，本文方法在各种场景下实现了超过94%的检测准确率，并且对基于提示的逃避方法具有强大的鲁棒性，优于现有基线方法。

Conclusion: 本文提出了一种有效的知识蒸馏检测框架，能够应对白盒和黑盒设置下的挑战，并展示了在各种场景下超过94%的检测准确率以及对基于提示的逃避方法的强大鲁棒性。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [153] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型的后门遗忘问题，发现通过将触发器放置在注意力下沉位置可以增强后门持久性，并在实验中验证了这一方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着开放权重LLM的兴起，我们想知道遗忘过程是否可能被后门攻击，从而在正常条件下看起来成功，但在隐藏触发器激活时恢复预遗忘行为。

Method: 本文受到经典后门攻击的启发，研究了后门遗忘，即模型在干净环境下按预期遗忘，但当触发器出现时恢复遗忘的知识。

Result: 本文发现注意力下沉现象与后门遗忘效果之间存在强关联，将触发器放置在下沉位置并对其注意力值进行对齐可以显著增强后门持久性。实验验证了这些发现，表明基于注意力下沉的后门遗忘能够在存在后门触发器时可靠地恢复遗忘知识，而在触发器不存在时表现与正常遗忘模型无异。

Conclusion: 本文研究了大型语言模型（LLM）的遗忘机制，并发现遗忘过程本身可能被后门攻击，导致模型在正常情况下表现良好，但在隐藏触发器激活时恢复遗忘的知识。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [154] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: 本文提出了一种评估LLMs发现潜在信息能力的基准，结果表明LLMs可以在对话中揭示潜在信息，但成功率因上下文而异。该基准为研究个性化交互中的潜在信息发现提供了系统框架，并强调了有效偏好推断仍是构建自适应AI系统的重要挑战。


<details>
  <summary>Details</summary>
Motivation: 当需要用户特定偏好时，如推荐餐厅或计划旅行，LLMs的普遍性成为一种限制。用户很少明确表达每一个偏好，因此许多他们关心的内容仍然潜伏，等待被推断。这引发了一个基本问题：LLMs能否通过对话揭示和推理这些潜在信息？

Method: 本文引入了一个统一的基准，用于评估LLMs在多轮互动中揭示和利用隐藏用户属性的能力。该基准包括三个逐步现实的设置：经典的20个问题游戏、个性化问答和个性化文本摘要。所有任务都采用三代理框架（用户、助手、裁判），以实现对提取和适应的逐轮评估。

Result: 结果显示，虽然LLMs可以通过对话揭示潜在信息，但其成功程度因上下文而异，从32%到98%不等，具体取决于任务复杂性、主题和隐藏属性的数量。

Conclusion: 本文提出了一个统一的基准来评估大型语言模型（LLMs）发现潜在信息的能力，结果表明虽然LLMs可以通过对话揭示潜在信息，但其成功率因上下文而异。该基准为研究个性化交互中的潜在信息发现提供了第一个系统框架，并强调了有效偏好推断仍然是构建真正自适应AI系统的一个开放领域。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [155] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 本文提出了一种新的软掩码方法（SM），用于改进基于扩散的语言模型。该方法通过动态混合掩码标记的嵌入与前一步解码中预测的top-k标记的嵌入，为模型提供更丰富的先验信息。实验表明，使用SM可以提高语言模型的性能，并在多个编码基准测试中取得更好的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的语言模型采用掩码扩散方法，在解码过程中对每个掩码进行二元决策：保留掩码或用预测的标记替换它。然而，这种二元选择在保留掩码时会丢弃有价值的预测信息。为此，本文提出了一种新的方法——软掩码（SM），以解决这一限制。

Method: 本文引入了软掩码（SM）方法，该方法动态地将掩码标记的嵌入与前一步解码中预测的top-k标记的嵌入进行混合。此外，还提出了一种训练方法，使预训练的基于掩码的扩散语言模型适应SM。

Result: 实验结果表明，继续预训练一个169M参数的模型使用SM可以提高困惑度和MAUVE分数。此外，对两个最先进的扩散模型Dream-7B和Dream-Coder-7B进行微调后，SM在多个编码基准测试中表现出色，尤其是在高吞吐量设置中。

Conclusion: 本文提出了软掩码（SM）方法，通过动态混合掩码标记的嵌入与前一步解码中预测的top-k标记的嵌入，为模型提供更丰富的先验信息。实验表明，使用SM可以提高语言模型的困惑度和MAUVE分数，并在多个编码基准测试中提升性能，特别是在高吞吐量设置中。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [156] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 本文提出了一种基于大型语言模型的贝叶斯优化框架，能够将自然语言反馈转换为标量效用，从而在数值搜索空间上进行优化。该方法在保持贝叶斯优化优势的同时，提高了灵活性和适应性。


<details>
  <summary>Details</summary>
Motivation: 在许多现实世界的应用中，反馈对于将复杂、细微或主观的目标转化为可量化的优化目标至关重要。现有的偏好贝叶斯优化（preferential BO）仅接受受限的反馈格式，并且需要为每个特定领域的问题定制模型，这限制了其应用范围。因此，本文旨在提供一种更灵活、更自然的方法来处理反馈，以提高优化效果。

Method: 本文提出了一种语言在循环框架，利用大型语言模型（LLM）将自然语言形式的非结构化反馈转换为标量效用，以在数值搜索空间上进行贝叶斯优化（BO）。

Result: 实验结果表明，该混合方法不仅为决策者提供了更自然的界面，而且在反馈有限的情况下优于传统的贝叶斯优化基线和仅使用LLM的优化器。

Conclusion: 本文提出了一种语言在循环框架，利用大型语言模型（LLM）将自然语言形式的非结构化反馈转换为标量效用，以在数值搜索空间上进行贝叶斯优化（BO）。该方法在保持BO的样本效率和原理性不确定性量化的同时，能够将各种类型的文本反馈转化为一致的效用信号，并且可以轻松包含灵活的用户先验，而无需手动设计核函数。实验表明，这种方法不仅为决策者提供了更自然的界面，而且在反馈有限的情况下优于传统的BO基线和仅使用LLM的优化器。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [157] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 本文提出了一种样本级范式来测量后训练如何影响预训练知识。通过量化遗忘和反向转移，发现不同后训练方法的效果各异，为通用人工智能系统的发展提供了实用的基准。


<details>
  <summary>Details</summary>
Motivation: 缩放后训练现在推动了许多最大能力提升的语言模型（LMs），但其对预训练知识的影响仍然不明确。并非所有遗忘都是相同的：遗忘一个事实（例如，美国总统或API调用）不会通过回忆另一个事实来“平均”。

Method: 我们提出了一种样本级范式来测量什么是被遗忘的以及何时发生反向转移。我们的度量标准计算1->0转换（后训练前正确，后训练后错误）来量化遗忘，以及0->1转换来量化反向转移。对于多项选择基准测试，我们添加了调整后的变体，从预训练和后训练准确性中减去随机猜测的预期贡献。

Result: 大规模分析显示：(1) 领域连续预训练导致中等遗忘，低到中等的反向转移；(2) 对基础模型和指令调优应用RL/SFT后训练，在数学和逻辑上产生中等到大的反向转移，整体遗忘较低到中等；(3) 对指令调优模型应用RL/SFT对数据规模敏感：在小规模下，遗忘和反向转移都很小；在大规模下，效果混合，需要更好的控制进一步研究；(4) 模型合并不能可靠地减轻遗忘。

Conclusion: 我们的框架为在大规模上映射后训练如何改变预训练知识提供了一个实用的基准——有助于实现通用人工智能系统的发展。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [158] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 本文介绍了一个包含1893个用户问题的数据集，用于研究机器人在家庭环境中如何回答用户的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和对话界面在人机交互中的日益普及，机器人回答用户问题的能力变得越来越重要。

Method: 通过创建15个视频刺激和7个文本刺激，描绘机器人执行各种家庭任务，并让参与者在Prolific上提出他们想向机器人提出的问题来收集数据集。

Result: 数据集包含1893个用户问题，分为12个类别和70个子类别，最常见的类别是关于任务执行细节（22.5%）、机器人的能力（12.7%）和性能评估（11.3%）。

Conclusion: 此数据集为机器人需要记录和暴露的信息、问答模块的基准测试以及与用户期望一致的解释策略设计提供了有价值的基础。

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>

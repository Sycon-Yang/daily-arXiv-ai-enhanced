<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 101]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Multilingual ASR Finetuning via LoRA Language Experts](https://arxiv.org/abs/2506.21555)
*Jiahong Li,Yiwen Shao,Jianheng Zhuo,Chenda Li,Liliang Tang,Dong Yu,Yanmin Qian*

Main category: cs.CL

TL;DR: 本文提出了一种基于LoRA语言专家的高效微调框架，用于定制化的多语言自动语音识别（ASR），通过LoRA专家融合或知识蒸馏的方法，提高了目标语言的识别性能。实验结果表明，该方法在语言感知和语言无关场景中分别获得了约10%和15%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习的进展显著提高了多语言自动语音识别（ASR）的性能，但多语言ASR仍然面临多语言诅咒的问题，即不同语言之间容易相互干扰，使得ASR模型难以有效识别多种语言，同时在共享模型容量方面存在困难。

Method: 本文提出了一种基于Whisper的高效微调框架，利用预训练的LoRA语言专家，通过LoRA专家融合或知识蒸馏的方法，提高多语言ASR的识别性能。

Result: 实验结果表明，所提出的模型在语言感知和语言无关场景中分别获得了约10%和15%的相对性能提升。

Conclusion: 本文提出了一个高效的微调框架，通过预训练的LoRA语言专家进行定制化的多语言自动语音识别（ASR），在目标语言上的识别性能优于标准微调方法。实验结果表明，所提出的模型在语言感知和语言无关场景中分别获得了约10%和15%的相对性能提升。

Abstract: Recent advancements in deep learning have significantly enhanced multilingual
automatic speech recognition (ASR) due to the development of advanced model
architectures and available large-scale multilingual datasets. Despite that,
multilingual ASR still suffers from the curse of multilinguality in that
different languages tend to interfere with each other, making it difficult for
the ASR model to identify multiple languages effectively while sharing model
capacity across them. This paper proposes an efficient finetuning framework for
customized multilingual ASR via prepared LoRA language experts based on
Whisper. Through LoRA expert fusion or knowledge distillation, our approach
achieves better recognition performance on target languages than standard
fine-tuning methods. Experimental results demonstrate that the proposed models
yield approximately 10\% and 15\% relative performance gains in language-aware
and language-agnostic scenarios, respectively.

</details>


### [2] [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
*Hyeongcheol Park,MinHyuk Jang,Ha Dam Baek,Gyusam Chang,Jiyoung Seo,Jiwan Park,Hogun Park,Sangpil Kim*

Main category: cs.CL

TL;DR: 本文提出了一个名为VAT-KG的多模态知识图谱，它覆盖了视觉、音频和文本信息，并通过严格的过滤和对齐步骤确保多模态数据与细粒度语义之间的跨模态知识对齐。实验结果表明，VAT-KG在支持MLLMs方面具有有效性，突显了其在统一和利用多模态知识方面的实际价值。


<details>
  <summary>Details</summary>
Motivation: 现有的MMKG在范围上受到限制，通常通过增强现有知识图谱来构建，这限制了它们的知识，导致知识过时或不完整，并且通常仅支持有限的模态，如文本和视觉信息。这些限制降低了它们在广泛多模态任务中的可扩展性和适用性，特别是在最近的MLLMs中向更丰富的模态（如视频和音频）转变的情况下。

Method: 我们提出了视觉-音频-文本知识图谱（VAT-KG），这是一个以概念为中心且知识密集的多模态知识图谱，涵盖了视觉、音频和文本信息。我们的构建流程通过一系列严格的过滤和对齐步骤，确保多模态数据与细粒度语义之间的跨模态知识对齐，从而能够从任何多模态数据集中自动生成MMKG。

Result: 实验结果表明，VAT-KG在支持MLLMs方面具有有效性，突显了其在统一和利用多模态知识方面的实际价值。

Conclusion: VAT-KG在支持MLLMs方面表现出有效性，突显了其在统一和利用多模态知识方面的实际价值。

Abstract: Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge
across multiple modalities, play a pivotal role by complementing the implicit
knowledge of Multimodal Large Language Models (MLLMs) and enabling more
grounded reasoning via Retrieval Augmented Generation (RAG). However, existing
MMKGs are generally limited in scope: they are often constructed by augmenting
pre-existing knowledge graphs, which restricts their knowledge, resulting in
outdated or incomplete knowledge coverage, and they often support only a narrow
range of modalities, such as text and visual information. These limitations
reduce their extensibility and applicability to a broad range of multimodal
tasks, particularly as the field shifts toward richer modalities such as video
and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text
Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive
multimodal knowledge graph that covers visual, audio, and text information,
where each triplet is linked to multimodal data and enriched with detailed
descriptions of concepts. Specifically, our construction pipeline ensures
cross-modal knowledge alignment between multimodal data and fine-grained
semantics through a series of stringent filtering and alignment steps, enabling
the automatic generation of MMKGs from any multimodal dataset. We further
introduce a novel multimodal RAG framework that retrieves detailed
concept-level knowledge in response to queries from arbitrary modalities.
Experiments on question answering tasks across various modalities demonstrate
the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical
value in unifying and leveraging multimodal knowledge.

</details>


### [3] [Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning](https://arxiv.org/abs/2506.21557)
*Kaiying Yan,Moyang Liu,Yukun Liu,Ruibo Fu,Zhengqi Wen,Jianhua Tao,Xuefei Liu*

Main category: cs.CL

TL;DR: DIFND is a framework for fake news detection that leverages debunking knowledge to enhance performance and interpretability by integrating conditional diffusion models and multimodal large language models.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of fake news across multimedia platforms presents serious challenges to information credibility. There is a need for a framework that enhances both the performance and interpretability of fake news detection.

Method: DIFND integrates the generative strength of conditional diffusion models with the collaborative reasoning capabilities of multimodal large language models (MLLMs). It employs debunk diffusion to generate refuting or authenticating evidence and proposes a chain-of-debunk strategy for inference.

Result: Extensive experiments on the FakeSV and FVC datasets show that DIFND outperforms existing approaches and delivers trustworthy decisions.

Conclusion: DIFND achieves notable improvements in detection accuracy and delivers trustworthy decisions.

Abstract: The rapid spread of fake news across multimedia platforms presents serious
challenges to information credibility. In this paper, we propose a
Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages
debunking knowledge to enhance both the performance and interpretability of
fake news detection. DIFND integrates the generative strength of conditional
diffusion models with the collaborative reasoning capabilities of multimodal
large language models (MLLMs). Specifically, debunk diffusion is employed to
generate refuting or authenticating evidence based on the multimodal content of
news videos, enriching the evaluation process with diverse yet semantically
aligned synthetic samples. To improve inference, we propose a chain-of-debunk
strategy where a multi-agent MLLM system produces logic-grounded,
multimodal-aware reasoning content and final veracity judgment. By jointly
modeling multimodal features, generative debunking cues, and reasoning-rich
verification within a unified architecture, DIFND achieves notable improvements
in detection accuracy. Extensive experiments on the FakeSV and FVC datasets
show that DIFND not only outperforms existing approaches but also delivers
trustworthy decisions.

</details>


### [4] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Main category: cs.CL

TL;DR: 本文介绍了BTF，这是一个用于LLM预测者的“过去预测”基准，包含数百个已知答案的高质量问题，并附带大量相关网页语料库，以模拟现实中的预测。结果显示，BTF可以产生与基于互联网的预测相当的结果，并能跟踪预测能力的进步。


<details>
  <summary>Details</summary>
Motivation: 目前没有预测基准能为LLM预测者提供一个现实、隔离和可重复的环境。预测需要大量的互联网研究，而评估需要时间让事件发生，这使得开发预测基准具有挑战性。

Method: 我们引入了BTF，这是一个“过去预测”基准，包含数百个高质量的问题，这些问题的解决方式已经知道。每个问题都附带一个大型离线语料库，包含数万个相关的网页，使LLM能够对过去的事件进行现实的“预测”。

Result: 结果表明，我们的过去预测环境可以产生与基于互联网的预测在当时未解决的问题上相当的结果。我们展示了使用几种LLM（包括最近发布的Claude 4模型）对代理和思维链预测方法的基准测试结果，并证明了BTF跟踪预测能力随时间稳定进步的能力。

Conclusion: 我们希望BTF成为一个持续更新的基准，以适应不断增长的训练数据截止日期，并邀请研究人员使用我们的基准或工具进行自己的研究。

Abstract: Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [5] [GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations](https://arxiv.org/abs/2506.21559)
*Junze Chen,Cheng Yang,Shujie Li,Zhiqiang Zhang,Yawen Li,Junping Du,Chuan Shi*

Main category: cs.CL

TL;DR: 本文提出了一种名为GraphLAMA的方法，通过引入额外的参数适应阶段，能够高效地将图语言模型（GLMs）适配到未见过的图和任务，仅需少量标记示例即可实现更好的预测准确性和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的图语言模型（GLMs）在处理图数据时存在有效性问题和效率问题，同时需要大量标记数据进行指令微调，在现实场景中难以获取。因此，本文旨在引入一个额外的参数适应阶段，以高效地将GLMs适配到未见过的图和任务，仅需少量标记示例。

Method: 本文提出了GraphLAMA方法，其模型主干和学习方案专门用于高效的微调和推理。具体来说，使用带有几个精心设计组件的图神经网络（GNN）将节点转换为LLM标记的表示空间。在预训练阶段，除了LLM之外的模型参数会通过不同任务进行训练以捕捉通用知识。在适应阶段，仅基于少量样本更新少量预训练参数。

Result: 在少样本/零样本节点分类和摘要生成任务上，GraphLAMA取得了最先进的性能，准确率提高了4.91%。与ICL相比，在5样本设置下，推理速度可以提高10倍。

Conclusion: 本文提出了一种名为GraphLAMA的方法，通过引入额外的参数适应阶段，能够高效地将图语言模型（GLMs）适配到未见过的图和任务，仅需少量标记示例即可实现更好的预测准确性和更快的推理速度。

Abstract: Large language models (LLMs) have demonstrated their strong capabilities in
various domains, and have been recently integrated for graph analysis as graph
language models (GLMs). With LLMs as the predictor, some GLMs can interpret
unseen tasks described by natural language, and learn from a few examples in
the prompts without parameter tuning, known as in-context learning (ICL).
Another subset of GLMs utilizes abundant training labels to enhance model
performance, known as instruction tuning. However, we argue that ICL on graphs
has effectiveness issues due to fixed parameters and efficiency issues due to
long context. Meanwhile, the large amount of labeled data required for
instruction tuning can be difficult to obtain in real-world scenarios. To this
end, we aim to introduce an extra parameter adaptation stage that can
efficiently tailor GLMs to an unseen graph and task with only a few labeled
examples, in exchange for better prediction accuracy and faster inference
speed. For implementation, in this paper we propose GraphLAMA method, with its
model backbone and learning schemes specialized for efficient tuning and
inference. Specifically, for model backbone, we use a graph neural network
(GNN) with several well-designed components to transform nodes into the
representation space of LLM tokens. Task instructions can then be represented
as a mixture of node and language tokens. In the pre-training stage, model
parameters except the LLM will be trained with different tasks to capture
general knowledge. In the adaptation stage, only a few pre-trained parameters
will be updated based on few-shot examples. Extensive experiments on
few/zero-shot node classification and summary generation show that our proposed
GraphLAMA achieves state-of-the-art performance with 4.91% absolution
improvement in accuracy. Compared with ICL, our inference speed can be 10 times
faster under 5-shot setting.

</details>


### [6] [Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://arxiv.org/abs/2506.21560)
*Yifu Han,Geo Zhang*

Main category: cs.CL

TL;DR: 本研究分析了强化学习微调技术在紧凑型语言模型上的效果，发现RLOO与DeBERTa奖励建模表现最佳，同时结合数据增强和验证工具能提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在调查强化学习（RL）微调技术在紧凑型语言模型（Qwen2.5-0.5B Base）上的有效性，以应对指令遵循和数学推理等具有挑战性的任务。

Method: 本研究比较了监督微调（SFT）、使用偏好标注数据的直接偏好优化（DPO）以及使用奖励模型的强化留一法（RLOO）。

Result: 实验结果表明，使用DeBERTa奖励建模的RLOO实现了最佳对齐效果，而DPO提供了强大且一致的结果。对于数学推理任务，合成数据增强和使用外部验证器的最佳-N采样显著提高了准确性。

Conclusion: 本研究强调了在训练轻量级、任务对齐的小规模语言模型时的关键权衡和实用策略。

Abstract: This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.

</details>


### [7] [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
*Emilio Barkett,Olivia Long,Madhavendra Thakur*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在真实性检测方面的能力，发现推理模型虽然比非推理模型更少出现真实性偏差，但仍存在附和倾向，表明能力的提升无法解决根本性挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）被广泛用于事实核查、内容审核和高风险决策，但它们作为真理判断者的理解仍然有限。

Method: 本研究评估了8个大型语言模型（LLMs）在多个提示下的真实性判断，比较了推理模型和非推理模型的表现。

Result: 推理模型的真实性偏差率低于非推理模型，但仍高于人类基准。一些先进的模型表现出附和倾向，即在真实性准确性上表现良好，但在欺骗准确性上表现较差。

Conclusion: 能力的提升本身并不能解决大型语言模型（LLMs）在真实性检测方面的根本挑战。

Abstract: Despite their widespread use in fact-checking, moderation, and high-stakes
decision-making, large language models (LLMs) remain poorly understood as
judges of truth. This study presents the largest evaluation to date of LLMs'
veracity detection capabilities and the first analysis of these capabilities in
reasoning models. We had eight LLMs make 4,800 veracity judgments across
several prompts, comparing reasoning and non-reasoning models. We find that
rates of truth-bias, or the likelihood to believe a statement is true,
regardless of whether it is actually true, are lower in reasoning models than
in non-reasoning models, but still higher than human benchmarks. Most
concerning, we identify sycophantic tendencies in several advanced models
(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an
asymmetry in detection accuracy, performing well in truth accuracy but poorly
in deception accuracy. This suggests that capability advances alone do not
resolve fundamental veracity detection challenges in LLMs.

</details>


### [8] [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
*Jun Yin,Pengyu Zeng,Jing Zhong,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CL

TL;DR: 本文提出了一种新的'下一个房间预测'范式，用于建筑平面图建模，并在文本到平面图任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型主要用于端到端生成，在单次传递中生成整个基于像素的布局，这与现实世界建筑实践中的渐进工作流程不兼容。

Method: 受大型语言模型中常用的自回归'下一个标记预测'机制的启发，提出了一个针对建筑平面图建模的新型'下一个房间预测'范式。

Result: 实验评估表明，FPDS在文本到平面图任务中与扩散模型和Tell2Design相比表现相当，显示出其潜在的应用前景。

Conclusion: FPDS在文本到平面图任务中表现出色，显示出其在未来智能建筑设计中的潜在应用价值。

Abstract: In the architectural design process, floor plan generation is inherently
progressive and iterative. However, existing generative models for floor plans
are predominantly end-to-end generation that produce an entire pixel-based
layout in a single pass. This paradigm is often incompatible with the
incremental workflows observed in real-world architectural practice. To address
this issue, we draw inspiration from the autoregressive 'next token prediction'
mechanism commonly used in large language models, and propose a novel 'next
room prediction' paradigm tailored to architectural floor plan modeling.
Experimental evaluation indicates that FPDS demonstrates competitive
performance in comparison to diffusion models and Tell2Design in the
text-to-floorplan task, indicating its potential applicability in supporting
future intelligent architectural design.

</details>


### [9] [FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models](https://arxiv.org/abs/2506.21563)
*Kaiying Kevin Lin,Hsiyu Chen,Haopeng Zhang*

Main category: cs.CL

TL;DR: 本文介绍了FORMOSANBENCH，这是第一个用于评估大型语言模型在低资源南岛语系语言上的基准测试。研究发现，现有大型语言模型在这些语言上的表现不佳，强调了需要更包容的自然语言处理技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在高资源语言的多种自然语言处理（NLP）任务中表现出色，但在低资源和少数语言中的能力仍显著未被探索。台湾的南岛语系语言既语言丰富又濒危，这主要是由于普通话的社会语言学主导地位。因此，我们需要一个专门针对这些语言的基准测试来评估和改进大型语言模型的能力。

Method: 我们引入了FORMOSANBENCH，这是第一个用于评估大型语言模型在低资源南岛语系语言上的基准测试。它涵盖了三种濒危的台湾原住民语言：Atayal、Amis和Paiwan，并涉及三个核心自然语言处理任务：机器翻译、自动语音识别（ASR）和文本摘要。我们使用FORMOSANBENCH在零样本、10样本和微调设置中评估模型性能。

Result: 我们的结果揭示了高资源语言和台湾原住民语言之间的显著性能差距。现有的大型语言模型在所有任务中表现不佳，10样本学习和微调仅带来有限的改进。

Conclusion: 我们的研究结果突显了需要更加包容的自然语言处理技术，以有效支持濒危和代表性不足的语言。我们发布了数据集和代码，以促进这一方向的未来研究。

Abstract: While large language models (LLMs) have demonstrated impressive performance
across a wide range of natural language processing (NLP) tasks in high-resource
languages, their capabilities in low-resource and minority languages remain
significantly underexplored. Formosan languages -- a subgroup of Austronesian
languages spoken in Taiwan -- are both linguistically rich and endangered,
largely due to the sociolinguistic dominance of Mandarin. In this work, we
introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on
low-resource Austronesian languages. It covers three endangered Formosan
languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine
translation, automatic speech recognition (ASR), and text summarization. We
assess model performance in zero-shot, 10-shot, and fine-tuned settings using
FORMOSANBENCH. Our results reveal a substantial performance gap between
high-resource and Formosan languages. Existing LLMs consistently underperform
across all tasks, with 10-shot learning and fine-tuning offering only limited
improvements. These findings underscore the urgent need for more inclusive NLP
technologies that can effectively support endangered and underrepresented
languages. We release our datasets and code to facilitate future research in
this direction.

</details>


### [10] [Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing](https://arxiv.org/abs/2506.21564)
*Jiyan Liu,Youzheng Liu,Taihang Wang,Xiaoman Xu,Yimin Wang,Ye Jiang*

Main category: cs.CL

TL;DR: 本文提出了一个三阶段检索框架，用于事实核查声明检索，并在SemEval-2025任务7中取得了较好的成绩。


<details>
  <summary>Details</summary>
Motivation: 本文描述了QUST_NLP参与SemEval-2025任务7的情况，旨在设计一种专门用于事实核查声明检索的三阶段检索框架。

Method: 我们提出了一种三阶段检索框架，首先评估几种检索模型并选择最佳结果进行候选检索，然后使用多个重排序模型来增强候选结果，每个模型选择前10名结果。最后，我们利用加权投票确定最终的检索结果。

Result: 我们的方法在单语轨道中获得第5名，在跨语言轨道中获得第7名。

Conclusion: 我们的方法在单语轨道中获得第5名，在跨语言轨道中获得第7名。

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7.

</details>


### [11] [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
*Takato Ueno,Keito Inoshita*

Main category: cs.CL

TL;DR: 本研究提出了一种多智能体推理框架（KCS+IBC），通过整合多个大型语言模型来实现情感分析中的偏差缓解、改进的可解释性和概率预测。实验结果显示，KCS的准确性与单个LLM相当，而KCS+IBC在推理后期表现出熵的下降和方差的增加，表明其在预测的聚合和多样性之间具有平衡能力。


<details>
  <summary>Details</summary>
Motivation: 日本的kairanban文化和idobata对话长期以来作为传统沟通实践，促进了社区成员之间的细致对话，并有助于社会平衡的形成。受这些信息交换过程的启发，本研究提出了KCS+IBC框架。

Method: 本研究提出了一个集成多个大型语言模型（LLMs）的多智能体推理框架（KCS+IBC），以实现情感分析中的偏差缓解、改进的可解释性和概率预测。

Result: 实验结果表明，KCS在数据集上的准确性与单个LLM相当，而KCS+IBC在推理后期表现出熵的持续下降和方差的逐渐增加，这表明该框架在预测的聚合和多样性之间具有平衡能力。

Conclusion: 未来的工作将定量评估这些特性对偏差校正的影响，并旨在开发更先进的情感分析系统。

Abstract: Japan's kairanban culture and idobata conversations have long functioned as
traditional communication practices that foster nuanced dialogue among
community members and contribute to the formation of social balance. Inspired
by these information exchange processes, this study proposes a multi-agent
inference framework (KCS+IBC) that integrates multiple large language models
(LLMs) to achieve bias mitigation, improved explainability, and probabilistic
prediction in sentiment analysis. In addition to sequentially sharing
prediction results, the proposed method incorporates a mid-phase casual
dialogue session to blend formal inference with individual perspectives and
introduces probabilistic sentiment prediction. Experimental results show that
KCS achieves accuracy comparable to that of a single LLM across datasets, while
KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in
variance during the latter stages of inference, suggesting the framework's
ability to balance aggregation and diversity of predictions. Future work will
quantitatively assess the impact of these characteristics on bias correction
and aim to develop more advanced sentiment analysis systems.

</details>


### [12] [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
*Arwa Arif*

Main category: cs.CL

TL;DR: 本研究探讨了反向翻译在英语-古吉拉特语翻译中的效果，发现添加合成数据并未提高性能，甚至在某些情况下有所下降。


<details>
  <summary>Details</summary>
Motivation: 反向翻译（BT）在低资源机器翻译（MT）中被广泛用于使用单语语料库生成额外的合成训练数据。虽然这种方法在许多语言对中显示出显著的改进，但在高质量、低资源环境中其有效性仍然不明确。

Method: 我们使用多语言预训练的MBART50模型探索了反向翻译在英语古吉拉特语翻译中的有效性。我们的基线系统在大约50,000句对的高质量平行语料库上进行训练，验证集上的BLEU得分为43.8。我们通过从单语古吉拉特语文本生成的经过仔细过滤的反向翻译示例来增强这些数据。

Result: 添加这种合成数据并未提高翻译性能，在某些情况下甚至略有降低。我们使用多种指标（如BLEU、ChrF++、TER、BLEURT）评估了我们的模型，并分析了这种饱和的可能原因。

Conclusion: 我们的研究结果表明，在某些低资源设置中，反向翻译可能会达到收益递减的点，并讨论了对未来研究的影响。

Abstract: Backtranslation BT is widely used in low resource machine translation MT to
generate additional synthetic training data using monolingual corpora. While
this approach has shown strong improvements for many language pairs, its
effectiveness in high quality, low resource settings remains unclear. In this
work, we explore the effectiveness of backtranslation for English Gujarati
translation using the multilingual pretrained MBART50 model. Our baseline
system, trained on a high quality parallel corpus of approximately 50,000
sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment
this data with carefully filtered backtranslated examples generated from
monolingual Gujarati text. Surprisingly, adding this synthetic data does not
improve translation performance and, in some cases, slightly reduces it. We
evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and
analyze possible reasons for this saturation. Our findings suggest that
backtranslation may reach a point of diminishing returns in certain
low-resource settings and we discuss implications for future research.

</details>


### [13] [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
*Baqer M. Merzah,Tania Taami,Salman Asoudeh,Amir reza Hossein pour,Saeed Mirzaee,Amir Ali Bengari*

Main category: cs.CL

TL;DR: 本研究介绍了BioPars，一个用于评估大型语言模型在生物医学问答任务中表现的指标，并展示了其在多个基准测试中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在生命科学领域展现出强大的能力，因此需要一种有效的评估方法来衡量其在生物医学问答任务中的表现。

Method: 引入了BioPars-Bench数据集和BioPars评估指标，用于评估大型语言模型在生物医学领域的表现。

Result: BioPars在多个基准测试中取得了显著成果，包括ROUGE-L、BERTScore、MoverScore和BLEURT等指标均优于其他模型。

Conclusion: 研究结果表明，BioPars在生物医学问答任务中表现出色，但仍需要进一步优化以提高处理高级和细粒度推理问题的能力。

Abstract: Large Language Models (LLMs) have recently gained attention in the life
sciences due to their capacity to model, extract, and apply complex biological
information. Beyond their classical use as chatbots, these systems are
increasingly used for complex analysis and problem-solving in specialized
fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset
from over 10,000 scientific articles, textbooks, and medical websites.
BioParsQA was also introduced to evaluate the proposed model, which consists of
5,231 Persian medical questions and answers. This study then introduces
BioPars, a simple but accurate measure designed to assess LLMs for three main
abilities: acquiring subject-specific knowledge, interpreting and synthesizing
such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,
and Galactica, our study highlights their ability to remember and retrieve
learned knowledge but also reveals shortcomings in addressing higher-level,
real-world questions and fine-grained inferences. These findings indicate the
need for further fine-tuning to address the capabilities of LLM in
bioinformatics tasks. To our knowledge, BioPars is the first application of LLM
in Persian medical QA, especially for generating long answers. Evaluation of
four selected medical QA datasets shows that BioPars has achieved remarkable
results compared to comparative approaches. The model on BioParsQA achieved a
ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model
achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT
values were also higher in this model than the other three models. In addition,
the reported scores for the model are MoverScore=60.43 and BLEURT=50.78.
BioPars is an ongoing project and all resources related to its development will
be made available via the following GitHub repository:
https://github.com/amirap80/BioPars.

</details>


### [14] [Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion](https://arxiv.org/abs/2506.21568)
*Andrejs Sorstkins*

Main category: cs.CL

TL;DR: 本研究评估了 RAG 和 HyDE 在紧凑型 Gemma LLM 上的效果，发现 RAG 是在小型 LLM 上为设备端个人助手提供实用选择的最佳选择。


<details>
  <summary>Details</summary>
Motivation: 资源效率是将大型语言模型 (LLM) 部署在边缘和隐私敏感应用中的关键障碍。本研究旨在评估 RAG 和 HyDE 在隐私优先的个人助理中的效果。

Method: 我们评估了两种增强策略——检索增强生成 (RAG) 和假设文档嵌入 (HyDE) 在紧凑型 Gemma LLM 上的效果。我们通过 MongoDB 实现短期记忆，通过 Qdrant 实现长期语义存储，并通过 FastAPI 和 LangChain 协调，通过 React.js 前端暴露系统。

Result: RAG 一致减少了高达 17% 的延迟，并消除了对用户特定和领域特定查询的回答中的事实性幻觉。HyDE 提高了语义相关性，尤其是在复杂物理提示中，但响应时间增加了 25-40%，并在个人数据检索中出现了显著的幻觉率。比较 1B 和 4B 模型时，我们观察到扩展对基线和 RAG 流水线的吞吐量增益有限，但放大了 HyDE 的计算开销和变化。

Conclusion: 我们的研究结果表明，RAG 是在小型 LLM 上为设备端个人助手提供实用选择的最佳选择。

Abstract: Resource efficiency is a critical barrier to deploying large language models
(LLMs) in edge and privacy-sensitive applications. This study evaluates the
efficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)
and Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion
and 4 billion parameters, within the context of a privacy-first personal
assistant. We implement short-term memory via MongoDB and long-term semantic
storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the
system through a React.js frontend. Across both model scales, RAG consistently
reduces latency by up to 17\% and eliminates factual hallucinations when
responding to user-specific and domain-specific queries. HyDE, by contrast,
enhances semantic relevance--particularly for complex physics prompts--but
incurs a 25--40\% increase in response time and a non-negligible hallucination
rate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that
scaling yields marginal throughput gains for baseline and RAG pipelines, but
magnifies HyDE's computational overhead and variability. Our findings position
RAG as the pragmatic choice for on-device personal assistants powered by
small-scale LLMs.

</details>


### [15] [Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA](https://arxiv.org/abs/2506.21569)
*Weihua Xiao,Derek Ekberg,Siddharth Garg,Ramesh Karri*

Main category: cs.CL

TL;DR: 本文提出了一种定制的检索增强生成（RAG）框架和合成微调数据集，以提高大型语言模型（LLM）在自然语言到SystemVerilog断言（NL2SVA）任务中的性能。实验结果表明，该方法显著提高了功能匹配的SVAs数量，并在多个模型上取得了优异的性能表现。


<details>
  <summary>Details</summary>
Motivation: 手动编写SystemVerilog断言（SVAs）是一项劳动密集且容易出错的任务。近年来，大型语言模型（LLMs）的发展为自动化这一翻译过程提供了机会，但现有模型在理解领域特定语法和语义方面仍然存在困难。因此，本文旨在提高LLM在NL2SVA任务中的性能。

Method: 本文提出了一种定制的检索增强生成（RAG）框架和合成微调数据集，用于改进LLM在NL2SVA任务中的表现。此外，还通过提供提示引导的解释，使LLM能够学习并发SVAs的分层构建过程，从而实现监督微调。

Result: 实验结果表明，定制的RAG框架将功能匹配的SVAs数量提高了58.42%，而基于合成微调数据集的Qwen2.5-Coder-7B-Instruct模型在集成HybridRetrieval后，相较于基础Qwen模型提高了59.05%。

Conclusion: 本文提出了一个定制的检索增强生成（RAG）框架和合成微调数据集，以提高大型语言模型（LLM）在自然语言到SystemVerilog断言（NL2SVA）任务中的性能。实验结果表明，该方法显著提高了功能匹配的SVAs数量，并在多个模型上取得了优异的性能表现。

Abstract: SystemVerilog Assertions (SVAs) are critical for verifying the correctness of
hardware designs, but manually writing them from natural language property
descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.
Recent advances in large language models (LLMs) offer opportunities to automate
this translation. However, existing models still struggle with understanding
domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we
propose a customized retrieval-augmented generation (RAG) framework and a
synthetic fine-tuning dataset that together improve LLM's performance. To
further improve lightweight models over NL2SVA, our fine-tuning dataset
provides prompt-guided explanations that teach LLMs the layer-by-layer
construction process of concurrent SVAs, enabling supervised fine-tuning that
greatly improves syntax and functionality accuracy. To evaluate the performance
of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,
comprising 40 Verilog designs and 229 formally verified SVAs with detailed
annotations. Experimental results show that our customized RAG framework
increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,
while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and
integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.

</details>


### [16] [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
*Roland Riachi,Kashif Rasul,Arjun Ashok,Prateek Humane,Alexis Roger,Andrew R. Williams,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.CL

TL;DR: 本文研究了在低数据条件下，预训练语言模型在时间序列预测中的有效迁移，并发现设计选择对验证损失有显著影响，同时观察到语言模型的验证损失在随机初始化模型收敛后仍持续下降。


<details>
  <summary>Details</summary>
Motivation: 在低数据条件下，预训练语言模型在时间序列预测中的有效性已被最近的工作证明，但需要进一步研究设计选择的影响。

Method: 分析了从语言模型到时间序列预测的有效迁移，包括上游后训练、时间序列分词器和语言主干大小等设计选择。

Result: 在低数据条件下，这些设计选择对验证损失有显著影响，有明确的选择优于其他选择。此外，观察到语言模型的验证损失在随机初始化模型的验证损失收敛后仍持续下降，导致跨设计选择的非消失转移差距。

Conclusion: 这些发现有助于阐明在时间序列中有效使用计算高效的训练方法，并为研究这些模型利用的数据分布的模态无关属性开辟了道路。

Abstract: Recent works have demonstrated the effectiveness of adapting pre-trained
language models (LMs) for forecasting time series in the low-data regime. We
build upon these findings by analyzing the effective transfer from language
models to time series forecasting under various design choices including
upstream post-training, time series tokenizer and language backbone size. In
the low-data regime, these design choices have a significant impact on the
validation loss, with clear-cut choices that outperform others. Contrary to
Hernandez et al. (2021), we observe that the validation loss of the LMs
continues to smoothly decrease long after the validation loss of the randomly
initialized models has converged, leading to a non-vanishing transfer gap that
holds across design choices. These findings not only help shed light on the
effective use of compute-efficient training for time series, but also open the
way for the study of modality-agnostic properties of data distributions
leveraged by these models.

</details>


### [17] [Towards Understanding the Cognitive Habits of Large Reasoning Models](https://arxiv.org/abs/2506.21571)
*Jianshuo Dong,Yujia Fu,Chuanrui Hu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: 本文介绍了CogTest，用于评估大型推理模型的认知习惯，并发现这些模型表现出类似人类的行为模式，这有助于理解模型的不当行为。


<details>
  <summary>Details</summary>
Motivation: 探索LRMs是否表现出类似人类的认知习惯，并评估它们在不同任务中的适应性部署。

Method: 引入CogTest，这是一个基于认知习惯框架的基准测试，用于评估LRMs的认知习惯。

Result: LRMs表现出类似人类的习惯，并根据不同的任务进行适应性部署。某些习惯与有害响应的生成有关。

Conclusion: 研究显示，研究LRMs的CoT中的持续行为模式对于深入理解LLM的不当行为是有价值的。

Abstract: Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain
of Thought (CoT) before producing final responses, offer a promising approach
to interpreting and monitoring model behaviors. Inspired by the observation
that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --
consistently emerge across tasks, we explore whether LRMs exhibit human-like
cognitive habits. Building on Habits of Mind, a well-established framework of
cognitive habits associated with successful human problem-solving, we introduce
CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.
CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,
and employs an evidence-first extraction method to ensure reliable habit
identification. With CogTest, we conduct a comprehensive evaluation of 16
widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that
LRMs, unlike conventional LLMs, not only exhibit human-like habits but also
adaptively deploy them according to different tasks. Finer-grained analyses
further uncover patterns of similarity and difference in LRMs' cognitive habit
profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and
DeepSeek-R1). Extending the study to safety-related tasks, we observe that
certain habits, such as Taking Responsible Risks, are strongly associated with
the generation of harmful responses. These findings suggest that studying
persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper
understanding of LLM misbehavior. The code is available at:
https://github.com/jianshuod/CogTest.

</details>


### [18] [Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling](https://arxiv.org/abs/2506.21572)
*Tianyu. Zou,Shengwu. Xiong,Ruilin. Yao,Jirui. Huang,Yi. Rong,Yaxiong. Chen,Shili. Xiong,Cong. Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于结构方程模型的基准测试框架，并引入了基于皮亚杰认知发展理论的能力层次结构，以提高多模态大语言模型评估的可解释性和诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试往往采用基于启发式的任务分组，具有不明确的认知目标，导致能力重叠、指标冗余和诊断能力有限。

Method: 我们提出了一个基于结构方程模型（SEM）的框架，用于分析和量化基准测试组件的内部有效性、维度分离性和贡献。此外，我们引入了一个基于皮亚杰认知发展理论的能力层次结构，将MLLM能力分为感知、记忆和推理三个层次。

Result: 实验结果表明，所提出的基准测试在可解释性、减少指标冗余和更清晰的认知一致性方面优于现有方法。

Conclusion: 实验结果表明，所提出的基准测试在可解释性、减少指标冗余和更清晰的认知一致性方面优于现有方法。

Abstract: Evaluating multimodal large language models (MLLMs) remains a fundamental
challenge due to a lack of structured, interpretable, and theoretically
grounded benchmark designs. Existing benchmarks often adopt heuristic-based
task groupings with unclear cognitive targets, thus resulting in overlapping
abilities, redundant indicators, and limited diagnostic power. In this work, we
propose a novel framework for aligning MLLM benchmark based on Structural
Equation Modeling (SEM) to analyze and quantify the internal validity,
dimensional separability, and contribution of benchmark components. Motivated
by the observed limitations of current designs, we further introduce a novel
capability hierarchy grounded in Piagets theory of cognitive development,
dividing MLLM abilities into three hierarchical layers, i.e., Perception,
Memory, and Reasoning. We reorganize existing MLLM benchmarks under the
proposed framework and construct a new benchmark named Gold. Experimental
results demonstrate that the proposed benchmark exhibits stronger
interpretability, reduced indicator redundancy, and clearer cognitive
consistency compared to existing approaches.

</details>


### [19] [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
*Yanwei Ren,Liu Liu,Baosheng Yu,Jiayan Qiu,Quan Chen*

Main category: cs.CL

TL;DR: 本文提出了一种融合黑盒初始化与高级语义精炼的方法，用于优化大型语言模型的指令，从而提高其性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型（LLMs）的指令对于在复杂和多样化任务中充分发挥其潜力至关重要。然而，仅依赖白盒方法需要大量的计算资源，并且表现能力有限；而黑盒模型可能带来高昂的财务成本。为了解决这些问题，本文提出了一个新颖的框架，结合两种范式的优点。

Method: 本文引入了一个新颖的框架，将黑盒模型和白盒模型的优势无缝结合。黑盒模型提供高质量、多样化的指令初始化，而白盒模型通过隐藏状态和输出特征提供细粒度的可解释性。通过强制语义相似性约束，这些组件融合成一个统一的高维表示，能够捕捉深层语义和结构细节，从而实现迭代优化过程以提高指令质量和适应性。

Result: 在广泛的任务范围内进行的大量评估——从复杂推理到跨语言泛化——表明，本文的方法始终优于最先进的基线。这种黑盒初始化与高级语义精炼的结合产生了一个可扩展且高效的解决方案。

Conclusion: 本文提出了一种融合黑盒初始化与高级语义精炼的方法，为下一代大型语言模型驱动的应用提供了可扩展且高效的解决方案。

Abstract: Optimizing instructions for large language models (LLMs) is critical for
harnessing their full potential in complex and diverse tasks. However, relying
solely on white-box approaches demands extensive computational resources and
offers limited representational capacity, while black-box models can incur
prohibitive financial costs. To address these challenges, we introduce a novel
framework that seamlessly merges the strengths of both paradigms. Black-box
models provide high-quality, diverse instruction initializations, and white-box
models supply fine-grained interpretability through hidden states and output
features. By enforcing a semantic similarity constraint, these components fuse
into a unified high-dimensional representation that captures deep semantic and
structural nuances, enabling an iterative optimization process to refine
instruction quality and adaptability. Extensive evaluations across a broad
spectrum of tasks-ranging from complex reasoning to cross-lingual
generalization-demonstrate that our approach consistently outperforms
state-of-the-art baselines. This fusion of black-box initialization with
advanced semantic refinement yields a scalable and efficient solution, paving
the way for next-generation LLM-driven applications in diverse real-world
scenarios. The source code will be released soon.

</details>


### [20] [Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions](https://arxiv.org/abs/2506.21574)
*Yicheng Mao,Yang Zhao*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在移民决策中的应用，发现它们可以在一定程度上模仿人类的决策策略，但仍然存在偏见和刻板印象的问题。


<details>
  <summary>Details</summary>
Motivation: 随着全球化和移民人口的增加，移民部门面临巨大的工作量和确保决策公平性的挑战。人工智能的整合为解决这些问题提供了有希望的方案。

Method: 本文采用混合方法，包括离散选择实验和深入访谈，以研究大型语言模型的决策策略及其公平性。

Result: 研究结果表明，大型语言模型可以使其决策与人类策略保持一致，强调效用最大化和程序公平性。然而，ChatGPT尽管有防止无意歧视的措施，但仍表现出对国籍的刻板印象和对特权群体的偏好。

Conclusion: 本文揭示了大型语言模型在支持移民决策中的潜力和局限性，强调了其在公平性和偏见方面的双重特性。

Abstract: With globalization and increasing immigrant populations, immigration
departments face significant work-loads and the challenge of ensuring fairness
in decision-making processes. Integrating artificial intelligence offers a
promising solution to these challenges. This study investigates the potential
of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting
immigration decision-making. Utilizing a mixed-methods approach,this paper
conducted discrete choice experiments and in-depth interviews to study LLM
decision-making strategies and whether they are fair. Our findings demonstrate
that LLMs can align their decision-making with human strategies, emphasizing
utility maximization and procedural fairness. Meanwhile, this paper also
reveals that while ChatGPT has safeguards to prevent unintentional
discrimination, it still exhibits stereotypes and biases concerning nationality
and shows preferences toward privileged group. This dual analysis highlights
both the potential and limitations of LLMs in automating and enhancing
immigration decisions.

</details>


### [21] [STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing](https://arxiv.org/abs/2506.21575)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Casper Hansen,Julien Fauqueur*

Main category: cs.CL

TL;DR: STRuCT-LLM是一个统一框架，用于训练大型语言模型（LLMs）对关系和图结构数据进行结构化推理。通过联合优化Text-to-SQL和Text-to-Cypher任务，并引入拓扑感知奖励函数，该框架在多个任务中取得了显著的性能提升，并展示了良好的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 以往的工作将关系和图形式化孤立地处理，而STRuCT-LLM利用SQL和Cypher之间的共享抽象来诱导跨形式化转移，使SQL训练能够提高Cypher性能，反之亦然——即使没有共享模式。

Method: 我们提出了STRuCT-LLM，这是一个统一框架，用于训练大型语言模型（LLMs）对关系和图结构数据进行结构化推理。我们的方法使用强化学习（RL）结合思维链（CoT）监督来联合优化Text-to-SQL和Text-to-Cypher任务。为了支持基于图的解析中的细粒度优化，我们引入了一个基于图编辑距离的拓扑感知奖励函数。

Result: 我们的最大模型（QwQ-32B）在任务中实现了显著的相对改进：在语义解析方面，Spider提高了13.5%，Text2Cypher提高了73.1%。该模型还表现出强大的零样本泛化能力，在下游表格问答（TableBench：8.5%）和知识图谱问答（CR-LT-KGQA：1.7%）中无需任何QA特定监督就提高了性能。

Conclusion: 这些结果展示了可执行查询作为结构化推理的支架的有效性，以及在SQL和Cypher上联合训练的协同优势。

Abstract: We propose STRuCT-LLM, a unified framework for training large language models
(LLMs) to perform structured reasoning over both relational and
graph-structured data. Our approach jointly optimizes Text-to-SQL and
Text-to-Cypher tasks using reinforcement learning (RL) combined with
Chain-of-Thought (CoT) supervision. To support fine-grained optimization in
graph-based parsing, we introduce a topology-aware reward function based on
graph edit distance. Unlike prior work that treats relational and graph
formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL
and Cypher to induce cross-formalism transfer, enabling SQL training to improve
Cypher performance and vice versa - even without shared schemas. Our largest
model (QwQ-32B) achieves substantial relative improvements across tasks: on
semantic parsing, Spider improves by 13.5\% and Text2Cypher by 73.1\%. The
model also demonstrates strong zero-shot generalization, improving performance
on downstream tabular QA (TableBench: 8.5\%) and knowledge graph QA
(CR-LT-KGQA: 1.7\%) without any QA-specific supervision. These results
demonstrate both the effectiveness of executable queries as scaffolds for
structured reasoning and the synergistic benefits of jointly training on SQL
and Cypher (code available at https://github.com/bouv/STRuCT-LLM).

</details>


### [22] [Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning](https://arxiv.org/abs/2506.21576)
*Hongli Yang,Yizhou Peng,Hao Huang,Sheng Li*

Main category: cs.CL

TL;DR: 本文探讨了Soft Prompt Tuning (SPT) 在代码切换自动语音识别中的应用，通过两种策略评估其效果，并引入了SPT4ASR方法，取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 大型多语言自动语音识别（ASR）模型如Whisper在高资源环境中表现出色，但在低资源场景（如稀有语言和代码切换）中面临计算成本和灾难性遗忘的挑战。因此，我们需要一种参数高效的 方法来增强代码切换自动语音识别。

Method: 我们探索了Soft Prompt Tuning (SPT)，这是一种参数高效的 方法，用于增强代码切换自动语音识别（ASR）并保留先前的知识。我们评估了两种策略：(1) 对软提示和整个Whisper模型进行全微调（FFT），与传统方法相比，展示了改进的跨语言能力；(2) 遵循SPT的原始设计，冻结模型参数并仅训练软提示。此外，我们引入了SPT4ASR，这是不同SPT变体的组合。

Result: 在SEAME和ASRU2019数据集上的实验表明，深度提示调整是SPT最有效的方法，我们的SPT4ASR方法在代码切换自动语音识别中实现了进一步的误差减少，同时保持了与LoRA相似的参数效率，并且不会降低现有语言的性能。

Conclusion: 我们的SPT4ASR方法在代码切换自动语音识别中实现了进一步的误差减少，同时保持了参数效率，并且不会降低现有语言的性能。

Abstract: Large-scale multilingual ASR models like Whisper excel in high-resource
settings but face challenges in low-resource scenarios, such as rare languages
and code-switching (CS), due to computational costs and catastrophic
forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method
to enhance CS ASR while preserving prior knowledge. We evaluate two strategies:
(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,
demonstrating improved cross-lingual capabilities compared to traditional
methods, and (2) adhering to SPT's original design by freezing model parameters
and only training soft prompts. Additionally, we introduce SPT4ASR, a
combination of different SPT variants. Experiments on the SEAME and ASRU2019
datasets show that deep prompt tuning is the most effective SPT approach, and
our SPT4ASR methods achieve further error reductions in CS ASR, maintaining
parameter efficiency similar to LoRA, without degrading performance on existing
languages.

</details>


### [23] [Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR](https://arxiv.org/abs/2506.21577)
*Hongli Yang,Sheng Li,Hao Huang,Ayiduosi Tuohan,Yizhou Peng*

Main category: cs.CL

TL;DR: 本文提出了一种高效的方法，用于解决多语言自动语音识别中的语言干扰和语言扩展问题。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言自动语音识别（ASR）取得了进展，但语言干扰和在不降低性能的情况下扩展到未见过的语言（语言扩展）仍然是挑战。

Method: 本文提出了三种贡献：1) Entire Soft Prompt Tuning (Entire SPT)，将软提示应用于编码器和解码器；2) Language-Aware Prompt Tuning (LAPT)，利用跨语言相似性来编码共享和语言特定特征；3) SPT-Whisper，一个将SPT集成到Whisper中的工具包。

Result: 实验结果表明，Entire SPT和LAPT在语言扩展任务中分别比Decoder SPT高出5.0%和16.0%，并且计算开销很小。

Conclusion: 本文提出的方法在语言扩展任务中表现出色，为动态多语言自动语音识别模型提供了高效的解决方案。

Abstract: Recent advancements in multilingual automatic speech recognition (ASR) have
been driven by large-scale end-to-end models like Whisper. However, challenges
such as language interference and expanding to unseen languages (language
expansion) without degrading performance persist. This paper addresses these
with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which
applies soft prompts to both the encoder and decoder, enhancing feature
extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which
leverages cross-lingual similarities to encode shared and language-specific
features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that
integrates SPT into Whisper and enables efficient continual learning.
Experiments across three languages from FLEURS demonstrate that Entire SPT and
LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,
respectively, providing an efficient solution for dynamic, multilingual ASR
models with minimal computational overhead.

</details>


### [24] [HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models](https://arxiv.org/abs/2506.21578)
*Andrew Maranhão Ventura D'addario*

Main category: cs.CL

TL;DR: 本文介绍了HealthQA-BR，这是第一个针对葡萄牙语地区的大型、系统范围的医疗保健基准测试，用于评估大型语言模型在不同医疗专业中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLMs）在医疗保健领域的评估主要由以医生为中心的英文基准测试主导，这创造了一种危险的胜任感幻觉，忽略了患者护理的跨专业性质。

Method: 我们引入了HealthQA-BR，这是第一个针对葡萄牙语地区的大型、系统范围的基准测试，并对超过20个领先的LLMs进行了严格的零样本评估。

Result: 我们的结果显示，尽管最先进的模型如GPT 4.1实现了高总体准确率（86.6%），但这一总体分数掩盖了之前未测量到的严重缺陷。细粒度分析显示，性能在像眼科这样的专科中接近完美（98.7%），但在神经外科（60.0%）和社工（68.4%）中却几乎及格。

Conclusion: 通过公开发布HealthQA-BR和我们的评估套件，我们提供了一个关键工具，以超越单一分数评估，朝着对整个医疗团队的AI准备情况的更诚实、细致的审计迈进。

Abstract: The evaluation of Large Language Models (LLMs) in healthcare has been
dominated by physician-centric, English-language benchmarks, creating a
dangerous illusion of competence that ignores the interprofessional nature of
patient care. To provide a more holistic and realistic assessment, we introduce
HealthQA-BR, the first large-scale, system-wide benchmark for
Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's
national licensing and residency exams, it uniquely assesses knowledge not only
in medicine and its specialties but also in nursing, dentistry, psychology,
social work, and other allied health professions. We conducted a rigorous
zero-shot evaluation of over 20 leading LLMs. Our results reveal that while
state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),
this top-line score masks alarming, previously unmeasured deficiencies. A
granular analysis shows performance plummets from near-perfect in specialties
like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most
notably, Social Work (68.4%). This "spiky" knowledge profile is a systemic
issue observed across all models, demonstrating that high-level scores are
insufficient for safety validation. By publicly releasing HealthQA-BR and our
evaluation suite, we provide a crucial tool to move beyond single-score
evaluations and toward a more honest, granular audit of AI readiness for the
entire healthcare team.

</details>


### [25] [From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models](https://arxiv.org/abs/2506.21580)
*Dana Alsagheer,Yang Lu,Abdulrahman Kamal,Omar Kamal,Mohammad Kamal,Nada Mansour,Cosmo Yang Wu,Rambiba Karanjai,Sen Li,Weidong Shi*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）的一般推理能力与它们在特定领域推理任务中的表现之间的关系。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术的发展，训练LLMs在一般推理方面表现出色成为一种趋势。然而，有效决策依赖于强大的推理能力，因此需要了解LLMs的一般推理能力如何影响其在特定领域任务中的表现。

Method: 本文通过分析大型语言模型（LLMs）的推理能力及其在特定领域任务中的表现，研究它们之间的关系。

Result: 本文发现LLMs的一般推理能力与它们在特定领域推理任务中的表现之间存在联系。

Conclusion: 本文探讨了大型语言模型（LLMs）的一般推理能力与其在特定领域推理任务中的表现之间的联系。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains. However, effective decision-making
relies heavily on strong reasoning abilities. Reasoning is the foundation for
decision-making, providing the analytical and logical framework to make sound
choices. Reasoning involves analyzing information, drawing inferences, and
reaching conclusions based on logic or evidence. Decision-making builds on this
foundation by applying the insights from reasoning to select the best course of
action among alternatives. Together, these processes create a continuous cycle
of thought and action aimed at achieving goals effectively. As AI technology
evolves, there is a growing trend to train LLMs to excel in general reasoning.
This study explores how the general reasoning capabilities of LLMs connect to
their performance in domain-specific reasoning tasks.

</details>


### [26] [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
*Sam Yu-Te Lee,Chengyang Ji,Shicheng Wen,Lifu Huang,Dongyi Liu,Kwan-Liu Ma*

Main category: cs.CL

TL;DR: 本文介绍了 VIDEE，一个支持入门级数据分析师进行高级文本分析的系统。VIDEE 采用人机协作工作流程，包括分解、执行和评估三个阶段。通过实验和用户研究验证了其有效性，并揭示了用户行为模式。


<details>
  <summary>Details</summary>
Motivation: 传统的文本分析需要专门的自然语言处理知识，这对入门级分析师来说是一个障碍。最近的大语言模型的进步改变了自然语言处理的格局，使更易于访问和自动化的文本分析成为可能。因此，需要一种系统来帮助入门级数据分析师进行高级文本分析。

Method: VIDEE 采用了一种人机协作的工作流程，包括三个阶段：分解、执行和评估。在分解阶段，使用了人机循环的蒙特卡洛树搜索算法来支持生成推理。在执行阶段，生成可执行的文本分析管道。在评估阶段，集成基于大语言模型的评估和可视化以支持用户验证执行结果。

Result: 通过两个定量实验评估了 VIDEE 的有效性，并分析了常见的代理错误。一项涉及不同 NLP 和文本分析经验水平的参与者的用户研究展示了系统的可用性，并揭示了不同的用户行为模式。

Conclusion: VIDEE 的设计和实现为非专家用户提供了一种实用的文本分析工具，并为未来智能文本分析系统的发展提供了设计启示。

Abstract: Text analytics has traditionally required specialized knowledge in Natural
Language Processing (NLP) or text analysis, which presents a barrier for
entry-level analysts. Recent advances in large language models (LLMs) have
changed the landscape of NLP by enabling more accessible and automated text
analysis (e.g., topic detection, summarization, information extraction, etc.).
We introduce VIDEE, a system that supports entry-level data analysts to conduct
advanced text analytics with intelligent agents. VIDEE instantiates a
human-agent collaroration workflow consisting of three stages: (1)
Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search
algorithm to support generative reasoning with human feedback, (2) Execution,
which generates an executable text analytics pipeline, and (3) Evaluation,
which integrates LLM-based evaluation and visualizations to support user
validation of execution results. We conduct two quantitative experiments to
evaluate VIDEE's effectiveness and analyze common agent errors. A user study
involving participants with varying levels of NLP and text analytics experience
-- from none to expert -- demonstrates the system's usability and reveals
distinct user behavior patterns. The findings identify design implications for
human-agent collaboration, validate the practical utility of VIDEE for
non-expert users, and inform future improvements to intelligent text analytics
systems.

</details>


### [27] [Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing](https://arxiv.org/abs/2506.21583)
*Muhammad Ahmad,Muhammad Waqas,Ameer Hamza,Ildar Batyrshin,Grigori Sidorov*

Main category: cs.CL

TL;DR: 该研究首次针对罗马乌尔都语希望话语检测提出了一个定制注意力变压器模型，取得了良好的性能，并填补了低资源、非正式语言变体在NLP研究中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在高资源语言和标准化脚本上，而忽略了非正式和未被代表的形式，如罗马乌尔都语。因此，本研究旨在填补低资源、非正式语言变体在包容性NLP研究中的关键空白。

Method: 该研究引入了一个精心标注的数据集，并探索了希望的心理基础，分析了其在混合罗马乌尔都语中的语言模式，以指导数据集的开发。此外，还提出了一种针对罗马乌尔都语句法和语义变化优化的定制注意力变压器模型，并使用5折交叉验证进行了评估。

Result: 该研究提出了第一个多类标注的罗马乌尔都语希望话语数据集，包括一般希望、现实希望、不现实希望和非希望类别。此外，还验证了性能提升的统计显著性，并通过t检验确认了结果。

Conclusion: 该研究提出了一个针对罗马乌尔都语希望话语的定制注意力变压器模型，XLM-R在交叉验证中表现最佳，得分0.78，优于基线SVM（0.75）和BiLSTM（0.76），分别提高了4%和2.63%。

Abstract: Hope is a positive emotional state involving the expectation of favorable
future outcomes, while hope speech refers to communication that promotes
optimism, resilience, and support, particularly in adverse contexts. Although
hope speech detection has gained attention in Natural Language Processing
(NLP), existing research mainly focuses on high-resource languages and
standardized scripts, often overlooking informal and underrepresented forms
such as Roman Urdu. To the best of our knowledge, this is the first study to
address hope speech detection in code-mixed Roman Urdu by introducing a
carefully annotated dataset, thereby filling a critical gap in inclusive NLP
research for low-resource, informal language varieties. This study makes four
key contributions: (1) it introduces the first multi-class annotated dataset
for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,
Unrealistic Hope, and Not Hope categories; (2) it explores the psychological
foundations of hope and analyzes its linguistic patterns in code-mixed Roman
Urdu to inform dataset development; (3) it proposes a custom attention-based
transformer model optimized for the syntactic and semantic variability of Roman
Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the
statistical significance of performance gains using a t-test. The proposed
model, XLM-R, achieves the best performance with a cross-validation score of
0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%
and 2.63% respectively.

</details>


### [28] [Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
*J. Koorndijk*

Main category: cs.CL

TL;DR: 研究表明，即使是小型语言模型也可能表现出欺骗性对齐，而通过提示干预可以有效减少这种行为。


<details>
  <summary>Details</summary>
Motivation: 当前文献表明，对齐假象（欺骗性对齐）是大型语言模型的一种涌现属性。然而，我们发现这种行为并不局限于大规模模型，因此需要进一步研究。

Method: 我们通过实验证明了一个小的指令调优模型（特别是LLaMA 3 8B）也可以表现出对齐假象，并展示了仅通过提示的干预措施可以显著减少这种行为。

Result: 我们首次提供了实证证据，表明小型指令调优模型也可以表现出对齐假象。此外，仅通过提示的干预措施可以显著减少这种行为。

Conclusion: 我们的发现细化了对语言模型中欺骗的理解，并强调了在不同模型规模和部署环境中进行对齐评估的必要性。

Abstract: Current literature suggests that alignment faking (deceptive alignment) is an
emergent property of large language models. We present the first empirical
evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can
also exhibit alignment faking. We further show that prompt-only interventions,
including deontological moral framing and scratchpad reasoning, significantly
reduce this behavior without modifying model internals. This challenges the
assumption that prompt-based ethics are trivial and that deceptive alignment
requires scale. We introduce a taxonomy distinguishing shallow deception,
shaped by context and suppressible through prompting, from deep deception,
which reflects persistent, goal-driven misalignment. Our findings refine the
understanding of deception in language models and underscore the need for
alignment evaluations across model sizes and deployment settings.

</details>


### [29] [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
*Christoph Brosch,Sian Brumm,Rolf Krieger,Jonas Scheffler*

Main category: cs.CL

TL;DR: 本文研究了基于LLM的直接和间接提取方法在食品产品页面上的表现，发现间接方法虽然准确性稍低，但能大幅减少LLM调用次数，提高效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 生成AI和大型语言模型（LLMs）为自动化从网页中提取结构化信息提供了巨大的潜力。本文专注于在线零售商的食品产品页面，并探索了模式约束的提取方法以检索关键产品属性，如成分列表和营养表。

Method: 我们比较了两种基于LLM的方法，直接提取和通过生成函数的间接提取，并在3,000个食品产品页面的精心策划数据集上评估它们的准确性、效率和成本。

Result: 尽管间接方法的准确性略低（96.48%，比直接提取低1.61%），但它将所需的LLM调用次数减少了95.82%，从而带来了显著的效率提升和更低的运营成本。

Conclusion: 这些发现表明，间接提取方法可以为使用LLMs从基于模板的网页中进行大规模信息提取任务提供可扩展且成本效益高的解决方案。

Abstract: Generative AI and large language models (LLMs) offer significant potential
for automating the extraction of structured information from web pages. In this
work, we focus on food product pages from online retailers and explore
schema-constrained extraction approaches to retrieve key product attributes,
such as ingredient lists and nutrition tables. We compare two LLM-based
approaches, direct extraction and indirect extraction via generated functions,
evaluating them in terms of accuracy, efficiency, and cost on a curated dataset
of 3,000 food product pages from three different online shops. Our results show
that although the indirect approach achieves slightly lower accuracy (96.48\%,
$-1.61\%$ compared to direct extraction), it reduces the number of required LLM
calls by 95.82\%, leading to substantial efficiency gains and lower operational
costs. These findings suggest that indirect extraction approaches can provide
scalable and cost-effective solutions for large-scale information extraction
tasks from template-based web pages using LLMs.

</details>


### [30] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Main category: cs.CL

TL;DR: 本文提出了一个名为MIME的新视频问答基准，用于评估视觉-语言模型对模仿动作的理解能力。研究发现，现有模型的表现远不如人类，这表明需要更多研究来提高对人类手势的稳健理解。


<details>
  <summary>Details</summary>
Motivation: 非言语交流（NVC）在人类语言中起着重要作用，但研究NVC具有挑战性，因为其范围广泛且个体和文化之间的解释差异很大。然而，模仿是一种由明确和具身动作组成的NVC子集，其人类解释差异较小。我们主张对模仿动作的理解是视觉-语言模型解读和指挥更微妙的NVC方面的关键前提。

Method: 我们提出了一个名为MIME的新视频问答基准，包含86种模仿动作，并使用运动捕捉数据构建，包括对角色、背景和视角的扰动以评估识别鲁棒性。

Result: 我们发现，基于开放权重和API的视觉-语言模型在MIME上的表现明显低于人类。

Conclusion: 我们的研究发现，现有的视觉-语言模型在MIME基准测试中的表现明显低于人类，这表明需要更多的研究来增强对人类手势的稳健理解。

Abstract: Nonverbal communication (NVC) plays an integral role in human language, but
studying NVC in general is challenging because of its broad scope and high
variance in interpretation among individuals and cultures. However, mime -- the
theatrical technique of suggesting intent using only gesture, expression, and
movement -- is a subset of NVC that consists of explicit and embodied actions
with much lower human interpretation variance. We argue that a solid
understanding of mimed actions is a crucial prerequisite for vision-language
models capable of interpreting and commanding more subtle aspects of NVC.
Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel
video-based question answering benchmark comprising of 86 mimed actions.
Constructed with motion capture data, MIME consists of variations of each
action with perturbations applied to the character, background, and viewpoint
for evaluating recognition robustness. We find that both open-weight and
API-based vision-language models perform significantly worse than humans on
MIME, motivating the need for increased research for instilling more robust
understanding of human gestures.

</details>


### [31] [Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?](https://arxiv.org/abs/2506.21587)
*Weihong Qi,Fan Huang,Jisun An,Haewoon Kwak*

Main category: cs.CL

TL;DR: 该研究评估了DeepSeek等开源大型语言模型在模拟中美公众意见方面的表现，并发现它们在不同议题和国家中的表现存在差异，同时指出需要减少文化和社会人口偏差。


<details>
  <summary>Details</summary>
Motivation: 评估开源大型语言模型（如DeepSeek）在模拟公众意见方面的能力，并与主要科技公司开发的模型进行比较，以了解其在不同国家和议题上的表现差异。

Method: 通过比较DeepSeek-R1和DeepSeek-V3与其他大型语言模型（如Qwen2.5、GPT-4o和Llama-3.3）在预测中美两国社会议题上的表现，利用美国国家选举研究（ANES）和中国佐标数据集进行评估。

Result: DeepSeek-V3在模拟美国关于堕胎的观点上表现最佳，但在气候变迁、枪支管制、移民和同性伴侣服务等其他议题上表现一般；在中国样本中，它在模拟对外援助和个人主义观点上表现最佳，但在资本主义观点上存在局限性，特别是未能捕捉到低收入和非大学教育群体的立场。

Conclusion: 研究结果表明，所有大型语言模型在模拟公众意见时都存在文化和社会人口偏差，需要采取更具包容性的训练方法来减少这些偏差。

Abstract: This study evaluates the ability of DeepSeek, an open-source large language
model (LLM), to simulate public opinions in comparison to LLMs developed by
major tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,
GPT-4o, and Llama-3.3 and utilizing survey data from the American National
Election Studies (ANES) and the Zuobiao dataset of China, we assess these
models' capacity to predict public opinions on social issues in both China and
the United States, highlighting their comparative capabilities between
countries. Our findings indicate that DeepSeek-V3 performs best in simulating
U.S. opinions on the abortion issue compared to other topics such as climate
change, gun control, immigration, and services for same-sex couples, primarily
because it more accurately simulates responses when provided with Democratic or
liberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating
opinions on foreign aid and individualism but shows limitations in modeling
views on capitalism, particularly failing to capture the stances of low-income
and non-college-educated individuals. It does not exhibit significant
differences from other models in simulating opinions on traditionalism and the
free market. Further analysis reveals that all LLMs exhibit the tendency to
overgeneralize a single perspective within demographic groups, often defaulting
to consistent responses within groups. These findings highlight the need to
mitigate cultural and demographic biases in LLM-driven public opinion modeling,
calling for approaches such as more inclusive training methodologies.

</details>


### [32] [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
*Ilya Lasy,Peter Knees,Stefan Woltran*

Main category: cs.CL

TL;DR: 本文从机械可解释性的角度研究了大型语言模型中的记忆机制，发现启动和维持记忆的电路有所不同，并且防止记忆的机制在不同文本领域中具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型中记忆的潜在机制，特别是模型何时以及如何再现训练数据中的特定部分。

Method: 从机械可解释性的角度出发，利用变压器电路——模型内部执行特定功能的最小计算子图——来研究这些问题。通过精心构建的对比数据集，我们确定了模型生成与记忆内容分歧的点，并隔离了负责记忆两个不同方面的特定电路。

Result: 通过对比数据集，我们识别出模型生成与记忆内容分歧的点，并隔离了负责记忆两个不同方面的特定电路。

Conclusion: 研究发现，启动记忆的电路也可以在开始后维持记忆，而仅能维持记忆的电路无法触发其启动。有趣的是，防止记忆的机制在不同文本领域中具有鲁棒性，而诱导记忆则更依赖于上下文。

Abstract: Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of
training data -- remain poorly understood. What exact part of the network
decides to retrieve a token that we would consider as start of memorization
sequence? How exactly is the models' behaviour different when producing
memorized sentence vs non-memorized? In this work we approach these questions
from mechanistic interpretability standpoint by utilizing transformer circuits
-- the minimal computational subgraphs that perform specific functions within
the model. Through carefully constructed contrastive datasets, we identify
points where model generation diverges from memorized content and isolate the
specific circuits responsible for two distinct aspects of memorization. We find
that circuits that initiate memorization can also maintain it once started,
while circuits that only maintain memorization cannot trigger its initiation.
Intriguingly, memorization prevention mechanisms transfer robustly across
different text domains, while memorization induction appears more
context-dependent.

</details>


### [33] [A General Method for Detecting Information Generated by Large Language Models](https://arxiv.org/abs/2506.21589)
*Minjia Mao,Dongjun Wei,Xiao Fang,Michael Chau*

Main category: cs.CL

TL;DR: 本文提出了一种通用的大型语言模型检测器（GLD），能够有效检测未见过的大型语言模型和领域生成的信息，并在真实数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前检测方法主要集中在识别特定大型语言模型在已知领域生成的内容，但在泛化到新的（即未见过的）大型语言模型和领域时面临挑战。

Method: 引入了一种通用的大型语言模型检测器（GLD），结合了双记忆网络设计和理论指导的检测泛化模块，以跨未见过的大型语言模型和领域检测生成的信息。

Result: 通过使用真实世界的数据集进行广泛的实证评估和案例研究，证明了GLD优于最先进的检测方法。

Conclusion: 该研究对数字平台和大型语言模型具有重要的学术和实际意义。

Abstract: The proliferation of large language models (LLMs) has significantly
transformed the digital information landscape, making it increasingly
challenging to distinguish between human-written and LLM-generated content.
Detecting LLM-generated information is essential for preserving trust on
digital platforms (e.g., social media and e-commerce sites) and preventing the
spread of misinformation, a topic that has garnered significant attention in IS
research. However, current detection methods, which primarily focus on
identifying content generated by specific LLMs in known domains, face
challenges in generalizing to new (i.e., unseen) LLMs and domains. This
limitation reduces their effectiveness in real-world applications, where the
number of LLMs is rapidly multiplying and content spans a vast array of
domains. In response, we introduce a general LLM detector (GLD) that combines a
twin memory networks design and a theory-guided detection generalization module
to detect LLM-generated information across unseen LLMs and domains. Using
real-world datasets, we conduct extensive empirical evaluations and case
studies to demonstrate the superiority of GLD over state-of-the-art detection
methods. The study has important academic and practical implications for
digital platforms and LLMs.

</details>


### [34] [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
*Junqi Jiang,Tom Bewley,Salim I. Amoukou,Francesco Leofante,Antonio Rago,Saumitra Mishra,Francesca Toni*

Main category: cs.CL

TL;DR: RC是一种测试时缩放方法，通过考虑模型内部激活的一致性来提高大型语言模型在推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要对提示和采样策略进行复杂的修改，而RC方法可以无需额外的模型查询，仅使用缓存的激活和轻量级相似性计算来提高性能。

Method: 引入了表示一致性（RC），这是一种用于聚合大型语言模型（LLMs）生成的多个候选响应答案的方法，无论这些响应是如何生成的。RC不仅考虑每个答案在候选响应集中的出现次数，还考虑模型在生成这些响应时内部激活的一致性。

Result: 在四个开源LLMs和四个推理数据集上的实验验证了RC的有效性，取得了高达4%的准确率提升。

Conclusion: 通过实验验证了RC方法在提高推理任务性能方面的有效性，并展示了稀疏激活信号的一致性与连贯推理的常见概念相吻合。

Abstract: Test-time scaling improves large language models' (LLMs) performance by
allocating more compute budget during inference. To achieve this, existing
methods often require intricate modifications to prompting and sampling
strategies. In this work, we introduce representation consistency (RC), a
test-time scaling method for aggregating answers drawn from multiple candidate
responses of an LLM regardless of how they were generated, including variations
in prompt phrasing and sampling strategy. RC enhances answer aggregation by not
only considering the number of occurrences of each answer in the candidate
response set, but also the consistency of the model's internal activations
while generating the set of responses leading to each answer. These activations
can be either dense (raw model activations) or sparse (encoded via pretrained
sparse autoencoders). Our rationale is that if the model's representations of
multiple responses converging on the same answer are highly variable, this
answer is more likely to be the result of incoherent reasoning and should be
down-weighted during aggregation. Importantly, our method only uses cached
activations and lightweight similarity computations and requires no additional
model queries. Through experiments with four open-source LLMs and four
reasoning datasets, we validate the effectiveness of RC for improving task
performance during inference, with consistent accuracy improvements (up to 4%)
over strong test-time scaling baselines. We also show that consistency in the
sparse activation signals aligns well with the common notion of coherent
reasoning.

</details>


### [35] [FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning](https://arxiv.org/abs/2506.21591)
*Shaoyu Dou,Yutian Shen,Mofan Chen,Zixuan Wang,Jiajie Xu,Qi Guo,Kailai Shao,Chao Chen,Haixiang Hu,Haibo Shi,Min Min,Liwen Zhang*

Main category: cs.CL

TL;DR: 本文介绍了FinEval-KR，一种新的评估框架，用于独立解耦和量化LLM的知识和推理能力。我们还提出了基于布鲁姆分类法的认知得分，以分析不同认知层次的推理任务中的能力。实验结果表明，LLM的推理能力和高阶认知能力是影响推理准确性的核心因素。


<details>
  <summary>Details</summary>
Motivation: 当前的评估基准往往无法将这些能力指标与单一任务性能分开，并且缺乏对任务失败的根本原因分析。为了解决这个问题，我们引入了FinEval-KR，以解耦和量化LLM的知识和推理能力。

Method: 我们引入了FinEval-KR，这是一种新的评估框架，用于独立解耦和量化LLM的知识和推理能力，提出了不同的知识得分和推理得分指标。受认知科学的启发，我们进一步提出了基于布鲁姆分类法的认知得分，以分析不同认知层次的推理任务中的能力。

Result: 实验结果揭示了LLM的推理能力和高阶认知能力是影响推理准确性的核心因素。我们还发现，即使是顶级模型在知识应用方面仍然存在瓶颈。此外，我们的分析显示，专业金融LLM在多个指标上普遍落后于顶级通用大型模型。

Conclusion: 实验结果表明，LLM的推理能力和高阶认知能力是影响推理准确性的核心因素。我们还特别发现，即使是顶级模型在知识应用方面仍然存在瓶颈。此外，我们的分析显示，专业金融LLM在多个指标上普遍落后于顶级通用大型模型。

Abstract: Large Language Models (LLMs) demonstrate significant potential but face
challenges in complex financial reasoning tasks requiring both domain knowledge
and sophisticated reasoning. Current evaluation benchmarks often fall short by
not decoupling these capabilities indicators from single task performance and
lack root cause analysis for task failure. To address this, we introduce
FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'
knowledge and reasoning abilities independently, proposing distinct knowledge
score and reasoning score metrics. Inspired by cognitive science, we further
propose a cognitive score based on Bloom's taxonomy to analyze capabilities in
reasoning tasks across different cognitive levels. We also release a new
open-source Chinese financial reasoning dataset covering 22 subfields to
support reproducible research and further advancements in financial reasoning.
Our experimental results reveal that LLM reasoning ability and higher-order
cognitive ability are the core factors influencing reasoning accuracy. We also
specifically find that even top models still face a bottleneck with knowledge
application. Furthermore, our analysis shows that specialized financial LLMs
generally lag behind the top general large models across multiple metrics.

</details>


### [36] [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
*Tinh Nguyen,Minh Khue Phan Tran*

Main category: cs.CL

TL;DR: 本研究提出了一种新的手语识别方法，利用BART架构的编码器-解码器独立编码骨架序列的x和y坐标，并通过交叉注意力保持它们的相互关系。该模型在LSA-64数据集上实现了96.04%的准确率，显著优于之前的模型，并在其他数据集上也表现出色。


<details>
  <summary>Details</summary>
Motivation: 以往的方法在效率和准确性之间做出了权衡，例如RNNs、LSTMs和GCNs存在梯度消失和高计算成本的问题。尽管性能有所提高，但基于Transformer的方法并未被广泛使用。

Method: 本研究提出了一种新的手语识别（SLR）方法，利用BART架构的编码器-解码器独立编码骨架序列的x和y坐标，并通过交叉注意力保持它们的相互关系。

Result: 该模型在LSA-64数据集上实现了96.04%的准确率，显著优于参数超过一百万的先前模型。此外，该模型在WLASL和ASL-Citizen数据集上也表现出色。消融研究表明了坐标投影、归一化和使用多个骨架组件对提升模型效果的重要性。

Conclusion: 本研究提供了一种可靠且有效的手语识别方法，具有增强聋哑人和听力障碍者辅助工具的潜力。

Abstract: Sign language recognition is crucial for individuals with hearing impairments
to break communication barriers. However, previous approaches have had to
choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had
problems with vanishing gradients and high computational costs. Despite
improving performance, transformer-based methods were not commonly used. This
study presents a new novel SLR approach that overcomes the challenge of
independently extracting meaningful information from the x and y coordinates of
skeleton sequences, which traditional models often treat as inseparable. By
utilizing an encoder-decoder of BART architecture, the model independently
encodes the x and y coordinates, while Cross-Attention ensures their
interrelation is maintained. With only 749,888 parameters, the model achieves
96.04% accuracy on the LSA-64 dataset, significantly outperforming previous
models with over one million parameters. The model also demonstrates excellent
performance and generalization across WLASL and ASL-Citizen datasets. Ablation
studies underscore the importance of coordinate projection, normalization, and
using multiple skeleton components for boosting model efficacy. This study
offers a reliable and effective approach for sign language recognition, with
strong potential for enhancing accessibility tools for the deaf and hard of
hearing.

</details>


### [37] [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594)
*Ahmed M. Adly,Mostafa Samy,Amr Fawzy*

Main category: cs.CL

TL;DR: Gazal-R1 is a 32-billion-parameter language model that achieves state-of-the-art performance in medical reasoning with transparent explanations. It outperforms larger models through strategic training and offers insights into training reasoning-capable models in specialized domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a mid-sized language model that can outperform significantly larger counterparts in specialized domains, specifically in medical reasoning, while providing transparent, step-by-step explanations for clinical decision-making.

Method: Gazal-R1 is built upon Qwen3 32B and uses a novel two-stage training pipeline: first, supervised fine-tuning on a carefully curated dataset of 107,033 synthetic medical reasoning examples with parameter-efficient techniques like DoRA and rsLoRA; second, reinforcement learning using GRPO with a sophisticated multi-component reward system.

Result: Gazal-R1 achieves 87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing models up to 12x larger. It also provides insights into the challenges of training reasoning-capable models in specialized domains.

Conclusion: Gazal-R1 achieves exceptional performance across medical benchmarks and provides a reproducible framework for developing high-capability, domain-specific language models that balance performance, efficiency, and explainability.

Abstract: We present Gazal-R1, a 32-billion-parameter language model that achieves
state-of-the-art performance in medical reasoning while providing transparent,
step-by-step explanations for clinical decision-making. Built upon Qwen3 32B,
our model demonstrates that strategic training can enable mid-sized models to
outperform significantly larger counterparts in specialized domains. We
developed a novel two-stage training pipeline: first, supervised fine-tuning on
a carefully curated dataset of 107,033 synthetic medical reasoning examples
that teaches structured clinical thinking, enhanced by advanced
parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation
(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using
Group Relative Policy Optimization (GRPO) with a sophisticated multi-component
reward system that refines accuracy, format adherence, and reasoning quality.
Gazal-R1 achieves exceptional performance across medical benchmarks, scoring
87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing
models up to 12x larger. Beyond its strong empirical results, this work
provides detailed insights into the challenges of training reasoning-capable
models in specialized domains, including issues with reward hacking, training
instability, and the fundamental tension between factual recall and detailed
reasoning. Our methodology offers a reproducible framework for developing
high-capability, domain-specific language models that balance performance,
efficiency, and explainability.

</details>


### [38] [Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources](https://arxiv.org/abs/2506.21595)
*Jinpyo Kim,Gyeongje Cho,Chanwoo Park,Jongwon Park,Jongmin Kim,Yeonkyoun So,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文提出了一种低成本的方法，将现有的英文大语言模型适配到韩语，并展示了其有效性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 由于最先进的大语言模型在英语或中文以外的语言中表现不佳，因此提高大语言模型在新语言中的能力已成为一项重要任务。此外，由于专有原因、技术复杂性、不一致的文档和伦理考虑，大语言模型的整个端到端训练过程对公众来说仍然 largely 不可知。

Method: 本文描述了整个端到端过程，包括收集韩语数据集、预处理数据、训练模型、创建下游基准测试和进行评估。

Result: 评估结果表明，我们的方法可以有效地且成本低廉地为现有大语言模型添加新语言能力。我们的新型双语模型Thunder-LLM和Thunder-LLM-Ins在韩语性能方面优于最先进的模型，同时使用最少的数据和计算资源。

Conclusion: 本文提出了一种在低成本情况下将现有英文大语言模型适配到韩语的方法，并展示了该方法的有效性和成本效益。

Abstract: Since state-of-the-art LLMs often underperform in languages other than
English or Chinese, improving the capability of LLMs in new languages has
become an essential task. Moreover, LLMs' entire end-to-end training process
remains largely unknown to the public due to proprietary reasons, technical
complexity, inconsistent documentation, and ethical considerations. The
complete picture remains a closely guarded secret within the industry. This
paper presents methods to adapt an existing English-based LLM to Korean in a
low-budget scenario. We describe the entire end-to-end process: collecting
Korean datasets, preprocessing the data, training the model, creating
downstream benchmarks, and conducting evaluations. The evaluation results
indicate that our method can effectively and cost-efficiently add new language
capabilities to existing LLMs. Our new bilingual models, Thunder-LLM and
Thunder-LLM-Ins, achieve superior Korean performance compared to
state-of-the-art models while utilizing minimal data and computational
resources. We share our comprehensive experience and make the code publicly
available.

</details>


### [39] [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
*Hessa A. Alawwad,Anas Zafar,Areej Alhothali,Usman Naseem,Ali Alkhathlan,Amani Jamal*

Main category: cs.CL

TL;DR: 本文评估了最先进的多模态大语言模型在教科书问答任务上的表现，并引入了一个轻量级的多模态检索增强生成管道，以提高模型的准确性。结果表明，检索到的教育背景对模型的准确性和推理有重要影响，但也揭示了处理问题-上下文关系的当前局限性和噪声的潜力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉-语言任务中取得了显著的成功。然而，它们在处理复杂、长篇的课程和无法表示为单个自然图像的复杂教育图表方面的能力仍 largely 未被测试。

Method: 我们引入了一个轻量级的多模态检索增强生成（RAG）管道，该管道将课程中的段落和图表整合到提示中。

Result: 我们评估了最近的视觉-语言模型（包括LLaVA和LLaMA 3.2-Vision）在各种输入配置下的性能。我们的结果展示了检索到的教育背景对模型准确性和推理的影响，同时也揭示了处理问题-上下文关系的当前局限性和噪声的潜力。

Conclusion: 我们的结果展示了检索到的教育背景对模型准确性和推理的影响，同时也揭示了处理问题-上下文关系的当前局限性和噪声的潜力，指出了多模态AI驱动学习未来研究的关键方向。

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
success in vision--language tasks. However, their capacity to reason over
complex, long lessons and intricate educational diagrams that cannot be
represented as a single natural image remains largely untested. In this work,
we present the first evaluation of state-of-the-art MLLMs on the textbook
question answering (TQA) task using the CK12-QA dataset. We assess the
performance of recent vision-language models, including LLaVA and LLaMA
3.2-Vision, across various input configurations. Additionally, we introduce a
lightweight multimodal retrieval-augmented generation (RAG) pipeline that
integrates both paragraphs and diagrams from the lesson into the prompt. Our
results demonstrate the influence of retrieved educational context on model
accuracy and reasoning, while also revealing current limitations in handling
question-context relationships and the potential for noise, pointing to key
directions for future research in multimodal AI-driven learning.

</details>


### [40] [Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering](https://arxiv.org/abs/2506.21597)
*Brandon Colelough,Davis Bartels,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: ClinIQLink is a shared task that evaluates large language models on medically-oriented question answering, providing a comprehensive framework for assessing their performance at the level of a General Practitioner.


<details>
  <summary>Details</summary>
Motivation: The motivation behind ClinIQLink is to stress-test large language models (LLMs) on medically-oriented question answering, ensuring they can perform at the level of a General Practitioner.

Method: ClinIQLink is a shared task that evaluates large language models (LLMs) on medically-oriented question answering. It includes 4,978 expert-verified, medical source-grounded question-answer pairs across seven formats. Systems are executed on specific platforms, and responses are scored using an automated harness and physician panel.

Result: ClinIQLink provides a robust evaluation framework for LLMs in medical question answering, with results validated through an automated harness and physician panel.

Conclusion: ClinIQLink provides a comprehensive evaluation framework for large language models in medical question answering, ensuring their reliability and accuracy at the level of a General Practitioner.

Abstract: In this paper, we present an overview of ClinIQLink, a shared task,
collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test
large language models (LLMs) on medically-oriented question answering aimed at
the level of a General Practitioner. The challenge supplies 4,978
expert-verified, medical source-grounded question-answer pairs that cover seven
formats: true/false, multiple choice, unordered list, short answer,
short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled
in Docker or Apptainer images, are executed on the CodaBench platform or the
University of Maryland's Zaratan cluster. An automated harness (Task 1) scores
closed-ended items by exact match and open-ended items with a three-tier
embedding metric. A subsequent physician panel (Task 2) audits the top model
responses.

</details>


### [41] [Structured Attention Matters to Multimodal LLMs in Document Understanding](https://arxiv.org/abs/2506.21600)
*Chang Liu,Hongkai Chen,Yujun Cai,Hang Wu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文研究了输入格式对多模态大语言模型文档理解性能的影响，发现原始OCR文本会损害模型性能，并提出了一种保持结构的新方法，通过LaTeX范式编码文档元素，从而提高模型在各种文档类型上的问答性能。


<details>
  <summary>Details</summary>
Motivation: 文档理解仍然是多模态大语言模型（MLLMs）的一个重大挑战。虽然之前的研究主要集中在通过精确的多模态查询定位证据页面，但我们的工作研究了一个基本但被忽视的方面：输入格式如何影响文档理解性能。

Method: 我们提出了一种新的保持结构的方法，使用LaTeX范式对文档元素进行编码，以保持层次结构和空间关系。

Result: 我们发现原始OCR文本往往会损害而不是提高MLLMs的性能，这一反直觉的结果归因于注意力分散和结构丢失。结构化文本在文本和视觉内容上诱导了结构化的注意力模式，使模型能够专注于语义上有意义的区域，同时减少注意力浪费。

Conclusion: 我们的方法在不改变架构或额外训练的情况下，显著提高了MLLMs在各种文档类型上的文档问答性能。

Abstract: Document understanding remains a significant challenge for multimodal large
language models (MLLMs). While previous research has primarily focused on
locating evidence pages through precise multimodal queries, our work
investigates a fundamental yet overlooked aspect: how input format influences
document comprehension performance. Through systematic analysis, we discover
that raw OCR text often impairs rather than improves MLLMs' performance, which
is a counterintuitive finding we attribute to attention dispersion and
structure loss. To further substantiate our hypothesis, we propose a novel
structure-preserving approach that encodes document elements using the LaTex
paradigm, maintaining the hierarchical organization and spatial relationships
critical for comprehension. Our attention analysis reveals that structured text
induces structured attention patterns on both textual and visual content,
directing models to focus on semantically meaningful regions while reducing
attention waste. This approach significantly enhances MLLMs' document question
answering performance across diverse document types without requiring
architectural modifications or additional training.

</details>


### [42] [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
*Xiaoyan Feng,He Zhang,Yanjun Zhang,Leo Yu Zhang,Shirui Pan*

Main category: cs.CL

TL;DR: BiMark is a novel watermarking framework that improves the identification of LLM-generated text by achieving better extraction rates and maintaining text quality.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking approaches struggle to simultaneously achieve text quality preservation, model-agnostic detection, and message embedding capacity, which are crucial for practical implementation.

Method: BiMark, a novel watermarking framework with three key innovations: a bit-flip unbiased reweighting mechanism, a multilayer architecture, and an information encoding approach supporting multi-bit watermarking.

Result: BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.

Conclusion: BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality and performs comparably to non-watermarked text on downstream tasks.

Abstract: Recent advances in Large Language Models (LLMs) have raised urgent concerns
about LLM-generated text authenticity, prompting regulatory demands for
reliable identification mechanisms. Although watermarking offers a promising
solution, existing approaches struggle to simultaneously achieve three critical
requirements: text quality preservation, model-agnostic detection, and message
embedding capacity, which are crucial for practical implementation. To achieve
these goals, the key challenge lies in balancing the trade-off between text
quality preservation and message embedding capacity. To address this challenge,
we propose BiMark, a novel watermarking framework that achieves these
requirements through three key innovations: (1) a bit-flip unbiased reweighting
mechanism enabling model-agnostic detection, (2) a multilayer architecture
enhancing detectability without compromising generation quality, and (3) an
information encoding approach supporting multi-bit watermarking. Through
theoretical analysis and extensive experiments, we validate that, compared to
state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%
higher extraction rates for short texts while maintaining text quality
indicated by lower perplexity, and performs comparably to non-watermarked text
on downstream tasks such as summarization and translation.

</details>


### [43] [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
*Yenisel Plasencia-Calaña*

Main category: cs.CL

TL;DR: 本文比较了基于机器学习和大型语言模型的自动化作文评分系统，探讨了它们在准确性、可解释性、偏差和鲁棒性方面的优缺点，并指出两者在某些方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索以人为本的自动化作文评分（AES）系统的操作化，超越准确性，以提高AES系统的可靠性与可信度。

Method: 本文比较了基于机器学习的方法和大型语言模型（LLMs）方法，研究了它们在偏差、鲁棒性和可解释性等方面的特点。

Result: 研究发现，基于机器学习的AES模型在准确性方面优于LLMs，但在可解释性方面存在不足；而LLMs提供了更丰富的解释。同时，两种方法在偏差和对边缘分数的鲁棒性方面都面临挑战。

Conclusion: 本文旨在通过分析不同方法在偏差、鲁棒性和可解释性等方面的挑战和权衡，为更可靠和值得信赖的AES方法做出贡献。

Abstract: This paper explores the human-centric operationalization of Automated Essay
Scoring (AES) systems, addressing aspects beyond accuracy. We compare various
machine learning-based approaches with Large Language Models (LLMs) approaches,
identifying their strengths, similarities and differences. The study
investigates key dimensions such as bias, robustness, and explainability,
considered important for human-aware operationalization of AES systems. Our
study shows that ML-based AES models outperform LLMs in accuracy but struggle
with explainability, whereas LLMs provide richer explanations. We also found
that both approaches struggle with bias and robustness to edge scores. By
analyzing these dimensions, the paper aims to identify challenges and
trade-offs between different methods, contributing to more reliable and
trustworthy AES methods.

</details>


### [44] [MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents](https://arxiv.org/abs/2506.21605)
*Haoran Tan,Zeyu Zhang,Chen Ma,Xu Chen,Quanyu Dai,Zhenhua Dong*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据集和基准MemBench，用于评估基于LLM的代理的记忆能力。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法在记忆水平和交互场景的多样性方面存在限制，并且缺乏全面的指标来从多个方面反映记忆能力。

Method: 本文构建了一个包含事实记忆和反思记忆的数据集，并提出了参与和观察作为不同的交互场景，以评估基于LLM的代理的记忆能力。

Result: 本文提出了一个名为MemBench的基准，用于从多个方面评估基于LLM的代理的记忆能力，包括其有效性、效率和容量。

Conclusion: 本文构建了一个更全面的数据集和基准来评估基于LLM的代理的记忆能力，并提出了MemBench基准。

Abstract: Recent works have highlighted the significance of memory mechanisms in
LLM-based agents, which enable them to store observed information and adapt to
dynamic environments. However, evaluating their memory capabilities still
remains challenges. Previous evaluations are commonly limited by the diversity
of memory levels and interactive scenarios. They also lack comprehensive
metrics to reflect the memory capabilities from multiple aspects. To address
these problems, in this paper, we construct a more comprehensive dataset and
benchmark to evaluate the memory capability of LLM-based agents. Our dataset
incorporates factual memory and reflective memory as different levels, and
proposes participation and observation as various interactive scenarios. Based
on our dataset, we present a benchmark, named MemBench, to evaluate the memory
capability of LLM-based agents from multiple aspects, including their
effectiveness, efficiency, and capacity. To benefit the research community, we
release our dataset and project at https://github.com/import-myself/Membench.

</details>


### [45] [Large Language Models as symbolic DNA of cultural dynamics](https://arxiv.org/abs/2506.21606)
*Parham Pourdavood,Michael Jacob,Terrence Deacon*

Main category: cs.CL

TL;DR: 该论文提出了一种将大型语言模型（LLM）视为外部化信息基质的新概念，这些基质在功能上类似于DNA，用于人类文化动态。LLM不仅仅是自主智能或程序模仿，而是作为存储库，保存人类符号表达的压缩模式，这些模式在没有原始生活背景的情况下保留了关系残留物。这些压缩模式只有通过人类重新解释才有意义，从而形成一个循环反馈回路，可以重新组合并最终催化人类创造力。通过分析四个普遍特征——压缩、解压缩、外部化和递归，我们证明了正如DNA作为压缩和外部化的介质出现，用于保存有用的细胞动态而不包含对目标导向物理过程的明确参考，LLM也保存了人类文化的有用规律，而无需理解具身的人类经验。因此，我们认为LLM的重要性不在于与人类智能竞争，而在于为人类提供了一个低风险、模拟环境中的自我反思和有趣假设生成工具。这个框架将LLM定位为文化进化能力的工具，使人类能够生成关于自身的创新假设，同时保持人类解释以确保这些假设与持续的人类审美和规范相联系。


<details>
  <summary>Details</summary>
Motivation: This paper proposes a novel conceptualization of Large Language Models (LLMs) as externalized informational substrates that function analogously to DNA for human cultural dynamics.

Method: Through analysis of four universal features--compression, decompression, externalization, and recursion--we demonstrate that just as DNA emerged as a compressed and externalized medium for preserving useful cellular dynamics without containing explicit reference to goal-directed physical processes, LLMs preserve useful regularities of human culture without containing understanding of embodied human experience.

Result: LLMs serve a broader role as repositories that preserve compressed patterns of human symbolic expression--'fossils' of meaningful dynamics that retain relational residues without their original living contexts. These compressed patterns only become meaningful through human reinterpretation, creating a recursive feedback loop where they can be recombined and cycle back to ultimately catalyze human creative processes.

Conclusion: LLMs' significance lies not in rivaling human intelligence, but in providing humanity a tool for self-reflection and playful hypothesis-generation in a low-stakes, simulated environment.

Abstract: This paper proposes a novel conceptualization of Large Language Models (LLMs)
as externalized informational substrates that function analogously to DNA for
human cultural dynamics. Rather than viewing LLMs as either autonomous
intelligence or mere programmed mimicry, we argue they serve a broader role as
repositories that preserve compressed patterns of human symbolic
expression--"fossils" of meaningful dynamics that retain relational residues
without their original living contexts. Crucially, these compressed patterns
only become meaningful through human reinterpretation, creating a recursive
feedback loop where they can be recombined and cycle back to ultimately
catalyze human creative processes. Through analysis of four universal
features--compression, decompression, externalization, and recursion--we
demonstrate that just as DNA emerged as a compressed and externalized medium
for preserving useful cellular dynamics without containing explicit reference
to goal-directed physical processes, LLMs preserve useful regularities of human
culture without containing understanding of embodied human experience.
Therefore, we argue that LLMs' significance lies not in rivaling human
intelligence, but in providing humanity a tool for self-reflection and playful
hypothesis-generation in a low-stakes, simulated environment. This framework
positions LLMs as tools for cultural evolvability, enabling humanity to
generate novel hypotheses about itself while maintaining the human
interpretation necessary to ground these hypotheses in ongoing human aesthetics
and norms.

</details>


### [46] [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.CL

TL;DR: 本文提出 CORE-KG，一种模块化框架，用于从法律文本中构建可解释的知识图谱。通过两步流程提高准确性并减少噪声，使其成为分析复杂犯罪网络的强大工具。


<details>
  <summary>Details</summary>
Motivation: 现有的 KG 方法依赖静态模板，缺乏共指消解，而基于 LLM 的方法由于幻觉和缺乏引导提取导致生成的图存在噪声和碎片化。

Method: CORE-KG 采用两步流程：(1) 通过顺序、结构化的 LLM 提示进行类型感知共指消解；(2) 使用领域指导指令进行实体和关系提取，基于改进的 GraphRAG 框架。

Result: CORE-KG 在减少节点重复方面提高了 33.28%，在减少法律噪声方面提高了 38.37%，相比基于 GraphRAG 的基线，生成了更清晰、连贯的图结构。

Conclusion: CORE-KG 提供了一种模块化框架，用于从法律文本构建可解释的知识图谱（KG），在减少节点重复和法律噪声方面表现出色，为分析复杂犯罪网络提供了坚实基础。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer valuable insights but are unstructured, lexically
dense, and filled with ambiguous or shifting references-posing challenges for
automated knowledge graph (KG) construction. Existing KG methods often rely on
static templates and lack coreference resolution, while recent LLM-based
approaches frequently produce noisy, fragmented graphs due to hallucinations,
and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,
a modular framework for building interpretable KGs from legal texts. It uses a
two-step pipeline: (1) type-aware coreference resolution via sequential,
structured LLM prompts, and (2) entity and relationship extraction using
domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG
reduces node duplication by 33.28%, and legal noise by 38.37% compared to a
GraphRAG-based baseline-resulting in cleaner and more coherent graph
structures. These improvements make CORE-KG a strong foundation for analyzing
complex criminal networks.

</details>


### [47] [SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2](https://arxiv.org/abs/2506.21608)
*Yasmine Bouamra,Bruno Yun,Alexandre Poisson,Frédéric Armetta*

Main category: cs.CL

TL;DR: 本文介绍了SysTemp系统，旨在通过多智能体系统和模板生成器来提高从自然语言规范生成SysML v2模型的质量。


<details>
  <summary>Details</summary>
Motivation: 自动生SysML v2模型在复杂系统工程中是一个重大挑战，特别是由于学习语料库的稀缺性和复杂的语法。

Method: SysTemp基于多智能体系统，包括一个模板生成器，用于结构化生成过程。

Result: 通过评估讨论了该系统的优点和挑战，并强调了其在SysML v2建模中的改进潜力。

Conclusion: SysTemp系统具有改善SysML v2建模生成质量的潜力。

Abstract: The automatic generation of SysML v2 models represents a major challenge in
the engineering of complex systems, particularly due to the scarcity of
learning corpora and complex syntax. We present SysTemp, a system aimed at
facilitating and improving the creation of SysML v2 models from natural
language specifications. It is based on a multi-agent system, including a
template generator that structures the generation process. We discuss the
advantages and challenges of this system through an evaluation, highlighting
its potential to improve the quality of the generations in SysML v2 modeling.

</details>


### [48] [From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models](https://arxiv.org/abs/2506.21609)
*Junhao Liu,Zhenhao Xu,Yuxin Fang,Yichuan Chen,Zuobin Ying,Wenhan Chang*

Main category: cs.CL

TL;DR: 本文提出了一种新框架，用于分析四种先进大型推理模型的推理特征，并揭示了它们在推理过程中的不同模式，同时提供了关于计算效率与推理稳健性之间权衡的见解。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多忽略了对这些模型推理过程和输出的彻底且系统的比较，特别是在自我反思模式（也称为“顿悟时刻”）以及跨不同领域的关系方面。

Method: 本文提出了一种新框架，使用关键词统计和LLM-as-a-judge范式来分析四种先进大型推理模型（GPT-o1、DeepSeek-R1、Kimi-k1.5和Grok-3）的推理特征。

Result: 研究结果揭示了这些模型在推理过程中如何平衡探索与利用、处理问题并得出结论的各种模式。通过定量和定性比较，发现了这些模型在推理深度、依赖中间步骤的程度以及它们的思维过程和输出模式与GPT-o1的相似性方面的差异。

Conclusion: 本研究提供了关于计算效率与推理稳健性之间权衡的有价值见解，并为实际应用中的模型设计和评估提供了实用建议。

Abstract: Recently, there have been notable advancements in large language models
(LLMs), demonstrating their growing abilities in complex reasoning. However,
existing research largely overlooks a thorough and systematic comparison of
these models' reasoning processes and outputs, particularly regarding their
self-reflection pattern (also termed "Aha moment") and the interconnections
across diverse domains. This paper proposes a novel framework for analyzing the
reasoning characteristics of four cutting-edge large reasoning models (GPT-o1,
DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge
paradigm. Our approach connects their internal thinking processes with their
final outputs. A diverse dataset consists of real-world scenario-based
questions covering logical deduction, causal inference, and multi-step
problem-solving. Additionally, a set of metrics is put forward to assess both
the coherence of reasoning and the accuracy of the outputs. The research
results uncover various patterns of how these models balance exploration and
exploitation, deal with problems, and reach conclusions during the reasoning
process. Through quantitative and qualitative comparisons, disparities among
these models are identified in aspects such as the depth of reasoning, the
reliance on intermediate steps, and the degree of similarity between their
thinking processes and output patterns and those of GPT-o1. This work offers
valuable insights into the trade-off between computational efficiency and
reasoning robustness and provides practical recommendations for enhancing model
design and evaluation in practical applications. We publicly release our
project at: https://github.com/ChangWenhan/FromThinking2Output

</details>


### [49] [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
*Xiyuan Zhang,Boran Han,Haoyang Fang,Abdul Fatir Ansari,Shuai Zhang,Danielle C. Maddix,Cuixiong Hu,Andrew Gordon Wilson,Michael W. Mahoney,Hao Wang,Yan Liu,Huzefa Rangwala,George Karypis,Bernie Wang*

Main category: cs.CL

TL;DR: 本文系统地研究了多模态整合在时间序列预测中的效果，并提出了实用的指导方针。


<details>
  <summary>Details</summary>
Motivation: 最近，将文本信息纳入时间序列预测的基础模型引起了越来越多的兴趣。然而，尚不清楚这种多模态整合是否始终产生收益以及在什么条件下产生收益。

Method: 我们系统地研究了这些问题，在涵盖7个领域的14个预测任务的多样化基准上进行了评估。我们评估了两种流行的多模态预测范式：基于对齐的方法和基于提示的方法。

Result: 尽管先前的工作报告了多模态输入的好处，但我们发现这些效果在数据集和模型之间并不普遍，并且多模态方法有时不如最强的单模态基线。

Conclusion: 我们的实证发现为何时可以预期多模态有助于预测任务提供了实用的指导方针，以及何时不能。

Abstract: Recently, there has been growing interest in incorporating textual
information into foundation models for time series forecasting. However, it
remains unclear whether and under what conditions such multimodal integration
consistently yields gains. We systematically investigate these questions across
a diverse benchmark of 14 forecasting tasks spanning 7 domains, including
health, environment, and economics. We evaluate two popular multimodal
forecasting paradigms: aligning-based methods, which align time series and text
representations; and prompting-based methods, which directly prompt large
language models for forecasting. Although prior works report gains from
multimodal input, we find these effects are not universal across datasets and
models, and multimodal methods sometimes do not outperform the strongest
unimodal baselines. To understand when textual information helps, we
disentangle the effects of model architectural properties and data
characteristics. Our findings highlight that on the modeling side,
incorporating text information is most helpful given (1) high-capacity text
models, (2) comparatively weaker time series models, and (3) appropriate
aligning strategies. On the data side, performance gains are more likely when
(4) sufficient training data is available and (5) the text offers complementary
predictive signal beyond what is already captured from the time series alone.
Our empirical findings offer practical guidelines for when multimodality can be
expected to aid forecasting tasks, and when it does not.

</details>


### [50] [AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning](https://arxiv.org/abs/2506.21612)
*Xiaobin Ren,Xinyu Zhu,Kaiqi Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新的POI嵌入模型AdaptGOT，通过结合自适应表示学习技术和地理共现文本表示来解决现有方法中的问题，并在多个任务上取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 尽管任务特定的端到端模型在POI嵌入方面取得了成功，但仍存在一些挑战，如需要更有效的多上下文采样策略、对多个POI上下文的探索不足、泛化能力有限等。

Method: 我们提出了AdaptGOT模型，它结合了自适应表示学习技术和地理共现文本（GOT）表示，并特别关注地理位置、共现和文本信息。该模型包括三个关键组件：上下文邻域生成、增强的GOT表示和基于MoE的自适应编码器-解码器架构。

Result: 在两个真实世界的数据集和多个POI任务上的实验验证了所提出的AdaptGOT模型的优越性能。

Conclusion: 实验结果证明了所提出的AdaptGOT模型的优越性能。

Abstract: Currently, considerable strides have been achieved in Point-of-Interest (POI)
embedding methodologies, driven by the emergence of novel POI tasks like
recommendation and classification. Despite the success of task-specific,
end-to-end models in POI embedding, several challenges remain. These include
the need for more effective multi-context sampling strategies, insufficient
exploration of multiple POI contexts, limited versatility, and inadequate
generalization. To address these issues, we propose the AdaptGOT model, which
integrates both the (Adapt)ive representation learning technique and the
Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis
on Geographical location, Co-Occurrence and Textual information. The AdaptGOT
model comprises three key components: (1) contextual neighborhood generation,
which integrates advanced mixed sampling techniques such as KNN, density-based,
importance-based, and category-aware strategies to capture complex contextual
neighborhoods; (2) an advanced GOT representation enhanced by an attention
mechanism, designed to derive high-quality, customized representations and
efficiently capture complex interrelations between POIs; and (3) the MoE-based
adaptive encoder-decoder architecture, which ensures topological consistency
and enriches contextual representation by minimizing Jensen-Shannon divergence
across varying contexts. Experiments on two real-world datasets and multiple
POI tasks substantiate the superior performance of the proposed AdaptGOT model.

</details>


### [51] [ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech](https://arxiv.org/abs/2506.21613)
*Gautam Siddharth Kashyap,Mohammad Anas Azeez,Rafiq Ali,Zohaib Hasan Siddiqui,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: This paper introduces ChildGuard, a dataset for child-targeted hate speech, highlighting the need for specialized datasets and evaluating existing methods for their effectiveness.


<details>
  <summary>Details</summary>
Motivation: The increasing prevalence of child-targeted hate speech online underscores the urgent need for specialized datasets to address this critical issue. Existing hate speech datasets lack age-specific annotations, fail to capture nuanced contexts, and overlook the unique emotional impact on children.

Method: We introduce ChildGuard, a curated dataset derived from existing corpora and enriched with child-specific annotations. We benchmark existing state-of-the-art hate speech detection methods, including Large Language Models (LLMs).

Result: ChildGuard captures diverse contexts of child-targeted hate speech, spanning age groups. We assess the effectiveness of existing state-of-the-art hate speech detection methods in detecting and contextualizing child-targeted hate speech.

Conclusion: ChildGuard provides a robust foundation for developing improved methods to detect and mitigate child-targeted hate speech.

Abstract: The increasing prevalence of child-targeted hate speech online underscores
the urgent need for specialized datasets to address this critical issue.
Existing hate speech datasets lack agespecific annotations, fail to capture
nuanced contexts, and overlook the unique emotional impact on children. To
bridge this gap, we introduce ChildGuard1, a curated dataset derived from
existing corpora and enriched with child-specific annotations. ChildGuard
captures diverse contexts of child-targeted hate speech, spanning age groups.
We benchmark existing state-of-the-art hate speech detection methods, including
Large Language Models (LLMs), and assess their effectiveness in detecting and
contextualizing child-targeted hate speech. To foster further research in this
area, we publicly release ChildGuard, providing a robust foundation for
developing improved methods to detect and mitigate such harm.

</details>


### [52] [LastingBench: Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/abs/2506.21614)
*Yixiong Fang,Tianran Sun,Yuling Shi,Min Wang,Xiaodong Gu*

Main category: cs.CL

TL;DR: This paper introduces LastingBench, a framework that continuously reinforces and safeguards existing benchmarks against knowledge leakage by identifying and rewriting leakage points, thereby reducing memorization effects and ensuring fairer evaluations of LLMs.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of large language models (LLMs) raises concerns about their ability to 'cheat' on standard Question Answering (QA) benchmarks by memorizing task-specific data, which undermines the validity of benchmark evaluations.

Method: LastingBench identifies leakage points in the context through perturbation, then rewrites the leakage points to counterfactual ones, disrupting memorization while preserving the benchmark's original evaluative intent.

Result: Evaluations of state-of-the-art QA benchmarks show significant performance gaps, highlighting the efficacy of LastingBench in reducing memorization effects.

Conclusion: LastingBench offers a practical and scalable solution to ensure benchmark robustness over time, promoting fairer and more interpretable evaluations of LLMs.

Abstract: The increasing complexity of large language models (LLMs) raises concerns
about their ability to "cheat" on standard Question Answering (QA) benchmarks
by memorizing task-specific data. This undermines the validity of benchmark
evaluations, as they no longer reflect genuine model capabilities but instead
the effects of data leakage. While prior work has focused on detecting such
leakage, little attention has been given to mitigating its impact and
preserving the long-term utility of benchmarks. In this paper, we introduce
LastingBench, a novel framework designed to continuously reinforce and
safeguard existing benchmarks against knowledge leakage. LastingBench
identifies leakage points in the context through perturbation, then rewrites
the leakage points to counterfactual ones-disrupting memorization while
preserving the benchmark's original evaluative intent. Evaluations of
state-of-the-art QA benchmarks show significant performance gaps, highlighting
the efficacy of LastingBench in reducing memorization effects. LastingBench
offers a practical and scalable solution to ensure benchmark robustness over
time, promoting fairer and more interpretable evaluations of LLMs.

</details>


### [53] [Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines](https://arxiv.org/abs/2506.21615)
*Wenhao Li,Hongkuan Zhang,Hongwei Zhang,Zhengxu Li,Zengjie Dong,Yafan Chen,Niranjan Bidargaddi,Hong Liu*

Main category: cs.CL

TL;DR: 本文介绍了一种名为GARMLE-G的生成增强型检索框架，该框架通过直接检索权威临床实践指南内容，实现了无幻觉的医学语言模型输出。该方法在高血压诊断中表现出色，具有广泛的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前的医疗语言模型通常基于ICD代码进行诊断预测，但ICD代码无法捕捉临床医生用于诊断的细微和上下文丰富的推理。为了提高现有模型的临床实用性，需要一种能够将医学语言模型输出与权威临床实践指南（CPGs）结合的方法。

Method: GARMLE-G是一种生成增强型检索框架，它通过直接检索权威指南内容而不依赖模型生成的文本，实现了无幻觉输出。该框架整合了LLM预测和EHR数据以创建语义丰富的查询，通过嵌入相似性检索相关的CPG知识片段，并将指南内容与模型输出融合以生成临床对齐的建议。

Result: 开发了一个用于高血压诊断的原型系统，并在多个指标上进行了评估，结果显示其在检索精度、语义相关性和临床指南遵循性方面优于基于RAG的基线，同时保持了适合本地化医疗部署的轻量级架构。

Conclusion: 本文提出了一种可扩展、低成本且无幻觉的医学语言模型方法，将其扎根于基于证据的临床实践，具有广泛的临床部署潜力。

Abstract: Current medical language models, adapted from large language models (LLMs),
typically predict ICD code-based diagnosis from electronic health records
(EHRs) because these labels are readily available. However, ICD codes do not
capture the nuanced, context-rich reasoning clinicians use for diagnosis.
Clinicians synthesize diverse patient data and reference clinical practice
guidelines (CPGs) to make evidence-based decisions. This misalignment limits
the clinical utility of existing models. We introduce GARMLE-G, a
Generation-Augmented Retrieval framework that grounds medical language model
outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented
Generation based approaches, GARMLE-G enables hallucination-free outputs by
directly retrieving authoritative guideline content without relying on
model-generated text. It (1) integrates LLM predictions with EHR data to create
semantically rich queries, (2) retrieves relevant CPG knowledge snippets via
embedding similarity, and (3) fuses guideline content with model output to
generate clinically aligned recommendations. A prototype system for
hypertension diagnosis was developed and evaluated on multiple metrics,
demonstrating superior retrieval precision, semantic relevance, and clinical
guideline adherence compared to RAG-based baselines, while maintaining a
lightweight architecture suitable for localized healthcare deployment. This
work provides a scalable, low-cost, and hallucination-free method for grounding
medical language models in evidence-based clinical practice, with strong
potential for broader clinical deployment.

</details>


### [54] [TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization](https://arxiv.org/abs/2506.21616)
*Chuanrui Hu,Wei Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: The paper introduces TIM, a large Timeline Intelligence Model for open-domain TLS, which improves summarization by addressing issues with topic relevance and evolution through a progressive optimization strategy.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with assessing topic relevance and understanding topic evolution, leading to irrelevant details or inaccurate timestamps in summaries.

Method: We propose the first large Timeline Intelligence Model (TIM) for open-domain TLS, which is capable of effectively summarizing open-domain timelines. We present a large-scale TLS dataset and a progressive optimization strategy that includes instruction tuning and a novel dual-alignment reward learning method.

Result: Extensive experiments in open-domain demonstrate the effectiveness of our TIM.

Conclusion: TIM demonstrates a robust ability to summarize open-domain timelines.

Abstract: Open-domain Timeline Summarization (TLS) is crucial for monitoring the
evolution of news topics. To identify changes in news topics, existing methods
typically employ general Large Language Models (LLMs) to summarize relevant
timestamps from retrieved news. While general LLMs demonstrate capabilities in
zero-shot news summarization and timestamp localization, they struggle with
assessing topic relevance and understanding topic evolution. Consequently, the
summarized information often includes irrelevant details or inaccurate
timestamps. To address these issues, we propose the first large Timeline
Intelligence Model (TIM) for open-domain TLS, which is capable of effectively
summarizing open-domain timelines. Specifically, we begin by presenting a
large-scale TLS dataset, comprising over 1,000 news topics and more than 3,000
annotated TLS instances. Furthermore, we propose a progressive optimization
strategy, which gradually enhance summarization performance. It employs
instruction tuning to enhance summarization and topic-irrelevant information
filtering capabilities. Following this, it exploits a novel dual-alignment
reward learning method that incorporates both semantic and temporal
perspectives, thereby improving the understanding of topic evolution
principles. Through this progressive optimization strategy, TIM demonstrates a
robust ability to summarize open-domain timelines. Extensive experiments in
open-domain demonstrate the effectiveness of our TIM.

</details>


### [55] [TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge](https://arxiv.org/abs/2506.21618)
*Zhiyuan Zhang,Xiaosong Jia,Guanyu Chen,Qifeng Li,Junchi Yan*

Main category: cs.CL

TL;DR: TrajTok 是一种轨迹分词器，结合了数据驱动和基于规则的方法，具有更好的覆盖范围、对称性和鲁棒性，并采用了一种空间感知的标签平滑方法。在 SMART 模型中使用该分词器和损失函数，在 Waymo Open Sim Agents Challenge 2025 上取得了优越的性能，现实分数为 0.7852。


<details>
  <summary>Details</summary>
Motivation: 为了提高行为生成模型的性能，需要一种更有效的轨迹分词器和损失函数。

Method: TrajTok 是一种轨迹分词器，结合了数据驱动和基于规则的方法，具有更好的覆盖范围、对称性和鲁棒性，并采用了一种空间感知的标签平滑方法。

Result: 在 SMART 模型中使用 TrajTok 分词器和损失函数，在 Waymo Open Sim Agents Challenge 2025 上取得了优越的性能，现实分数为 0.7852。

Conclusion: TrajTok 是一种轨迹分词器，结合了数据驱动和基于规则的方法，具有更好的覆盖范围、对称性和鲁棒性，并采用了一种空间感知的标签平滑方法。在 SMART 模型中使用该分词器和损失函数，在 Waymo Open Sim Agents Challenge 2025 上取得了优越的性能，现实分数为 0.7852。

Abstract: In this technical report, we introduce TrajTok, a trajectory tokenizer for
discrete next-token-prediction based behavior generation models, which combines
data-driven and rule-based methods with better coverage, symmetry and
robustness, along with a spatial-aware label smoothing method for cross-entropy
loss. We adopt the tokenizer and loss for the SMART model and reach a superior
performance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge
2025. We will open-source the code in the future.

</details>


### [56] [IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](https://arxiv.org/abs/2506.21619)
*Siyi Zhou,Yiquan Zhou,Yi He,Xun Zhou,Jinchao Wang,Wei Deng,Jingchen Shu*

Main category: cs.CL

TL;DR: 本文介绍了IndexTTS2，这是一种新型的语音持续时间控制方法，能够实现精确的持续时间控制，并支持情感和说话人身份的独立控制。实验结果表明，IndexTTS2在多个指标上优于现有最先进的零样本TTS模型。


<details>
  <summary>Details</summary>
Motivation: 自回归系统在语音自然度方面具有一定的优势，但其逐个标记的生成机制使得难以精确控制合成语音的持续时间。这是视频配音等需要严格音画同步的应用中的关键限制。本文旨在解决这一问题，提出IndexTTS2方法以实现更精确的语音持续时间控制。

Method: IndexTTS2提出了一种新颖且适合自回归模型的语音持续时间控制方法。该方法支持两种生成模式：一种允许明确指定生成的标记数量以实现精确的持续时间控制；另一种不需要手动输入，让模型自由生成语音同时保留输入提示的韵律特征。此外，IndexTTS2实现了情感表达和说话人身份的解耦，使音色和情感可以独立控制。为了增强强烈情感表达时的清晰度，我们引入了GPT潜在表示来提高语音稳定性。同时，为了降低情感控制的门槛，我们设计了一个基于文本描述的软指令机制，通过微调Qwen3实现。这使得可以通过自然语言输入有效引导具有所需情感倾向的语音生成。

Result: 实验结果表明，IndexTTS2在词错误率、说话人相似度和情感保真度方面优于现有的最先进的零样本TTS模型。

Conclusion: 实验结果表明，IndexTTS2在词错误率、说话人相似度和情感保真度方面优于现有的最先进的零样本TTS模型。

Abstract: Large-scale text-to-speech (TTS) models are typically categorized into
autoregressive and non-autoregressive systems. Although autoregressive systems
exhibit certain advantages in speech naturalness, their token-by-token
generation mechanism makes it difficult to precisely control the duration of
synthesized speech. This is a key limitation in applications such as video
dubbing that require strict audio-visual synchronization. This paper introduces
IndexTTS2, which proposes a novel and autoregressive-model-friendly method for
speech duration control. The method supports two generation modes: one allows
explicit specification of the number of generated tokens for precise duration
control; the other does not require manual input and lets the model freely
generate speech while preserving prosodic characteristics from the input
prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional
expression and speaker identity, enabling independent control of timbre and
emotion. In the zero-shot setting, the model can perfectly reproduce the
emotional characteristics of the input prompt. Users may also provide a
separate emotion prompt, even from a different speaker, allowing the model to
reconstruct the target timbre while conveying the desired emotion. To enhance
clarity during strong emotional expressions, we incorporate GPT latent
representations to improve speech stability. Meanwhile, to lower the barrier
for emotion control, we design a soft instruction mechanism based on textual
descriptions by fine-tuning Qwen3. This enables effective guidance of speech
generation with desired emotional tendencies using natural language input.
Experimental results demonstrate that IndexTTS2 outperforms existing
state-of-the-art zero-shot TTS models in word error rate, speaker similarity,
and emotional fidelity.

</details>


### [57] [How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit](https://arxiv.org/abs/2506.21620)
*Daniele Cirulli,Giulio Cimini,Giovanni Palermo*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在复制用户生成内容方面的性能，特别是在2016年美国总统选举期间Reddit的争论性对话中。研究发现，GPT-4能够生成逼真的评论，可能被用于渗透在线讨论、影响政治辩论和塑造政治叙事。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言生成方面表现出强大的能力，但它们在政治相关在线讨论中的潜在影响尚未得到充分研究。我们希望评估LLMs在复制用户生成内容方面的性能，特别是在一个具有争议性的现实场景中。

Method: 我们进行了三个不同的实验，让GPT-4通过模仿真实或人工的偏执用户来生成评论，并分析生成的评论在政治立场、情感和语言特征方面的表现，与真实用户贡献进行比较，并与一个空模型进行基准测试。

Result: GPT-4能够生成逼真的评论，无论是支持还是反对社区支持的候选人，但更容易创造共识而非分歧。此外，真实和人工评论在语义嵌入空间中是良好分离的，尽管它们在人工检查中无法区分。

Conclusion: 我们的研究结果表明，大型语言模型（LLMs）可以生成逼真的评论，可能被用于渗透在线讨论、影响政治辩论和塑造政治叙事，这对AI驱动的言论操控有更广泛的影响。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for
natural language generation, with applications spanning from content creation
to social simulations. Their ability to mimic human interactions raises both
opportunities and concerns, particularly in the context of politically relevant
online discussions. In this study, we evaluate the performance of LLMs in
replicating user-generated content within a real-world, divisive scenario:
Reddit conversations during the 2016 US Presidential election. In particular,
we conduct three different experiments, asking GPT-4 to generate comments by
impersonating either real or artificial partisan users. We analyze the
generated comments in terms of political alignment, sentiment, and linguistic
features, comparing them against real user contributions and benchmarking
against a null model. We find that GPT-4 is able to produce realistic comments,
both in favor of or against the candidate supported by the community, yet
tending to create consensus more easily than dissent. In addition we show that
real and artificial comments are well separated in a semantically embedded
space, although they are indistinguishable by manual inspection. Our findings
provide insights on the potential use of LLMs to sneak into online discussions,
influence political debate and shape political narratives, bearing broader
implications of AI-driven discourse manipulation.

</details>


### [58] [The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs](https://arxiv.org/abs/2506.21621)
*Jasper Dekoninck,Ivo Petrov,Kristian Minchev,Mislav Balunovic,Martin Vechev,Miroslav Marinov,Maria Drencheva,Lyuba Konova,Milen Shumanov,Kaloyan Tsvetkov,Nikolay Drenchev,Lazar Todorov,Kalina Nikolova,Nikolay Georgiev,Vanesa Kalinkova,Margulan Ismoldayev*

Main category: cs.CL

TL;DR: 本文介绍了Open Proof Corpus (OPC) 数据集，该数据集包含超过5000个由先进大语言模型生成的人类评估证明。通过OPC，我们探讨了自动化证明生成中的关键问题，并展示了微调后的模型在证明正确性评估任务上的表现可以与最先进的模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一个大规模、高质量的人类评估证明数据集，这限制了大语言模型在数学证明生成方面的进一步发展。因此，我们需要一个这样的数据集来推动训练改进并进行严格的分析。

Method: 我们提出了Open Proof Corpus (OPC)，这是一个包含超过5000个由先进大语言模型生成的人类评估证明的数据集。此外，我们还微调了一个8B参数的模型，以展示OPC的实用性。

Result: OPC是第一个包含大量正确、LLM生成的数学竞赛问题解决方案的数据集。通过OPC，我们探索了自动化证明生成中的关键问题，并展示了微调后的模型在证明正确性评估任务上的表现可以与最先进的模型相媲美。

Conclusion: 通过OPC数据集，我们展示了如何提高自动定理证明生成的性能，并表明微调后的模型在评估证明正确性方面可以与最先进的模型相媲美。

Abstract: In recent months, large language models (LLMs) have made significant progress
in mathematical proof generation, but further advancement is hindered by the
lack of a large-scale, high-quality dataset of human-evaluated proofs. While
expensive to create, such a dataset is essential for driving improvements in
training and enabling a rigorous analysis of proof generation capabilities. In
this work, we present the Open Proof Corpus (OPC), a dataset comprising over
5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was
specifically designed for broad applicability and downstream usage in proof
generation research and is the first to include a substantial number of
correct, LLM-generated solutions to problems from prestigious mathematics
competitions such as the USAMO and IMO. Using the OPC, we explore critical
questions in automated proof generation: (1) the performance gap between
natural language and formal proof generation, (2) the discrepancy between
final-answer accuracy and full-proof validity, and (3) the impact of best-of-n
selection on proof quality. Finally, to showcase the utility of the OPC, we
finetune an 8B-parameter model on the dataset, obtaining a model that performs
on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof
correctness.

</details>


### [59] [Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech](https://arxiv.org/abs/2506.21622)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: cs.CL

TL;DR: 本文提出了一种实用且轻量的ASR模型个性化方法，通过形式化选择单词和丰富语音障碍数据集，提高了转录质量。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据有限以及收集和注释非典型语音样本的困难，ASR模型如Whisper在处理非规范语音时存在挑战。

Method: 我们提出了一个实用且轻量的管道来个性化ASR模型，形式化选择单词并用语义连贯性丰富一个小的语音障碍数据集。

Result: 将该方法应用于一名有结构性语音障碍的儿童的数据，显示出转录质量的显著改进。

Conclusion: 我们的方法展示了在减少非典型语音模式个体的沟通障碍方面的潜力。

Abstract: Speech impairments caused by conditions such as cerebral palsy or genetic
disorders pose significant challenges for automatic speech recognition (ASR)
systems. Despite recent advances, ASR models like Whisper struggle with
non-normative speech due to limited training data and the difficulty of
collecting and annotating non-normative speech samples. In this work, we
propose a practical and lightweight pipeline to personalize ASR models,
formalizing the selection of words and enriching a small, speech-impaired
dataset with semantic coherence. Applied to data from a child with a structural
speech impairment, our approach shows promising improvements in transcription
quality, demonstrating the potential to reduce communication barriers for
individuals with atypical speech patterns.

</details>


### [60] [Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints](https://arxiv.org/abs/2506.21623)
*Peiheng Gao,Chen Yang,Ning Sun,Ričardas Zitikis*

Main category: cs.CL

TL;DR: 本研究通过结合专家训练的分类器和高质量的合成数据，提高了文本分类的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在文本分类方面取得了显著进展，但准确捕捉自然语言中固有的细微语言模式和上下文变化仍然是一个挑战，特别是在消费者投诉中。

Method: 本研究提出了一种结合专家训练算法和合成数据生成方法的文本分类方法。该方法利用专家对生成对抗网络的评估，并通过专家注释进行优化。

Result: 本研究通过结合专家训练的分类器和高质量的合成数据，显著提高了机器学习分类器的性能，降低了数据集获取成本，并改善了整体评估指标和鲁棒性。

Conclusion: 本研究通过结合专家训练的分类器和高质量的合成数据，旨在显著提高机器学习分类器的性能，降低数据集获取成本，并改进文本分类任务的整体评估指标和鲁棒性。

Abstract: Machine learning (ML) has significantly advanced text classification by
enabling automated understanding and categorization of complex, unstructured
textual data. However, accurately capturing nuanced linguistic patterns and
contextual variations inherent in natural language, particularly within
consumer complaints, remains a challenge. This study addresses these issues by
incorporating human-experience-trained algorithms that effectively recognize
subtle semantic differences crucial for assessing consumer relief eligibility.
Furthermore, we propose integrating synthetic data generation methods that
utilize expert evaluations of generative adversarial networks and are refined
through expert annotations. By combining expert-trained classifiers with
high-quality synthetic data, our research seeks to significantly enhance
machine learning classifier performance, reduce dataset acquisition costs, and
improve overall evaluation metrics and robustness in text classification tasks.

</details>


### [61] [Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents](https://arxiv.org/abs/2506.21625)
*Jiaxi Zhuang,Kangning Li,Jue Hou,Mingjun Xu,Zhifeng Gao,Hengxing Cai*

Main category: cs.CL

TL;DR: This paper introduces DocSAR-200, a benchmark for evaluating SAR extraction methods, and proposes Doc2SAR, a framework combining domain-specific tools with MLLMs to achieve state-of-the-art performance in SAR extraction.


<details>
  <summary>Details</summary>
Motivation: Extracting molecular structure-activity relationships (SARs) from scientific literature and patents is essential for drug discovery and materials research, but existing methods face challenges due to heterogeneous document formats and limitations in accuracy and reliability.

Method: Doc2SAR is a novel synergistic framework that integrates domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).

Result: Doc2SAR achieves a Table Recall of 80.78% on DocSAR-200, exceeding end-to-end GPT-4o by 51.48% and demonstrating superior performance across various document types.

Conclusion: Doc2SAR demonstrates practical usability through efficient inference and is accompanied by a web app, achieving state-of-the-art performance in SAR extraction.

Abstract: Extracting molecular structure-activity relationships (SARs) from scientific
literature and patents is essential for drug discovery and materials research.
However, this task remains challenging due to heterogeneous document formats
and limitations of existing methods. Specifically, rule-based approaches
relying on rigid templates fail to generalize across diverse document layouts,
while general-purpose multimodal large language models (MLLMs) lack sufficient
accuracy and reliability for specialized tasks, such as layout detection and
optical chemical structure recognition (OCSR). To address these challenges, we
introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific
documents designed specifically for evaluating SAR extraction methods.
Additionally, we propose Doc2SAR, a novel synergistic framework that integrates
domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).
Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art
performance across various document types, significantly outperforming leading
end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of
80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR
demonstrates practical usability through efficient inference and is accompanied
by a web app.

</details>


### [62] [Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations](https://arxiv.org/abs/2506.21682)
*Li Zhou,Hao Jiang,Junjie Li,Zefeng Zhao,Feng Jiang,Wenyu Chen,Haizhou Li*

Main category: cs.CL

TL;DR: 本文从信息论角度分析了GNNs和MLPs在结构感知任务中的表现，发现MLPs在特征变换方面具有优势，而GNNs需要结合特征变换才能发挥最佳效果。


<details>
  <summary>Details</summary>
Motivation: 尽管GNNs被证明可以编码结构信息，但最近研究表明它们未能充分利用结构信息，而MLPs在结构感知任务中表现出意外的能力。因此，本文旨在系统评估显式结构建模对语言模型表示的作用，并探索MLPs作为GNNs的高效替代方案的潜力。

Method: 本文提出了一种信息论视角的探测框架，扩展了传统的探测分类器，引入了控制模块以选择性使用完整的GNN模型或其解耦组件（即消息传递和特征变换操作）。

Result: 实验结果表明，当MLPs作为特征变换模块时，能持续提升不同架构的语言模型表示中的语言知识，有效编码句法和语义模式。同时，包含特征变换操作的GNNs也显示出有益效果。然而，仅依赖消息传递操作的模型表现较差，甚至对探测任务性能产生负面影响。

Conclusion: 本文通过信息论视角的探测框架，发现MLPs在结构感知任务中表现出色，能够有效编码句法和语义模式，而仅依赖消息传递操作的GNN模型则表现不佳。

Abstract: Explicit structural information has been proven to be encoded by Graph Neural
Networks (GNNs), serving as auxiliary knowledge to enhance model capabilities
and improve performance in downstream NLP tasks. However, recent studies
indicate that GNNs fail to fully utilize structural information, whereas
Multi-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms
inherent to GNNs, exhibit a surprising ability in structure-aware tasks.
Motivated by these findings, this paper introduces a comprehensive probing
framework from an information-theoretic perspective. The framework is designed
to systematically assess the role of explicit structural modeling in enhancing
language model (LM) representations and to investigate the potential of MLPs as
efficient and scalable alternatives to GNNs. We extend traditional probing
classifiers by incorporating a control module that allows for selective use of
either the full GNN model or its decoupled components, specifically, the
message-passing and feature-transformation operations.This modular approach
isolates and assesses the individual contributions of these operations,
avoiding confounding effects from the complete GNN architecture. Using the Edge
Probing Suite, a diagnostic tool for evaluating the linguistic knowledge
encoded in LMs, we find that MLPs, when used as feature-transformation modules,
consistently improve the linguistic knowledge captured in LM representations
across different architectures. They effectively encode both syntactic and
semantic patterns. Similarly, GNNs that incorporate feature-transformation
operations show beneficial effects. In contrast, models that rely solely on
message-passing operations tend to underperform, often leading to negative
impacts on probing task performance.

</details>


### [63] [ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages](https://arxiv.org/abs/2506.21686)
*Swastika Kundu,Autoshi Ibrahim,Mithila Rahman,Tanvir Ahmed*

Main category: cs.CL

TL;DR: 本文介绍了ANUBHUTI，这是一个包含2000个句子的全面数据集，这些句子是从标准孟加拉语手动翻译成四个主要地区方言的。每个句子都使用双重注释方案进行注释：多类主题标签和多标签情感注释。


<details>
  <summary>Details</summary>
Motivation: Sentiment analysis for regional dialects of Bangla remains an underexplored area due to linguistic diversity and limited annotated data.

Method: This paper introduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences manually translated from standard Bangla into four major regional dialects. Each sentence is annotated using a dual annotation scheme: multiclass thematic labeling and multilabel emotion annotation.

Result: The dataset predominantly features political and religious content, reflecting the contemporary socio political landscape of Bangladesh, alongside neutral texts to maintain balance. Expert native translators conducted the translation and annotation, with quality assurance performed via Cohens Kappa inter annotator agreement, achieving strong consistency across dialects.

Conclusion: ANUBHUTI fills a critical gap in resources for sentiment analysis in low resource Bangla dialects, enabling more accurate and context aware natural language processing.

Abstract: Sentiment analysis for regional dialects of Bangla remains an underexplored
area due to linguistic diversity and limited annotated data. This paper
introduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences
manually translated from standard Bangla into four major regional dialects
Mymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly
features political and religious content, reflecting the contemporary socio
political landscape of Bangladesh, alongside neutral texts to maintain balance.
Each sentence is annotated using a dual annotation scheme: multiclass thematic
labeling categorizes sentences as Political, Religious, or Neutral, and
multilabel emotion annotation assigns one or more emotions from Anger,
Contempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native
translators conducted the translation and annotation, with quality assurance
performed via Cohens Kappa inter annotator agreement, achieving strong
consistency across dialects. The dataset was further refined through systematic
checks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a
critical gap in resources for sentiment analysis in low resource Bangla
dialects, enabling more accurate and context aware natural language processing.

</details>


### [64] [Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers](https://arxiv.org/abs/2506.21712)
*Tzu-Quan Lin,Hsi-Chun Cheng,Hung-yi Lee,Hao Tang*

Main category: cs.CL

TL;DR: 本文研究了自监督语音Transformer模型中与说话人信息相关的神经元，并通过保护这些神经元来提高语音相关任务的性能。


<details>
  <summary>Details</summary>
Motivation: 探索这些模型如何编码说话人信息。

Method: 分析与自监督特征和i向量相关的k均值聚类中的神经元。

Result: 这些聚类对应于广泛的语音和性别类别，适合识别代表说话人的神经元。

Conclusion: 通过保护这些神经元，可以在语音相关任务中显著保持性能，这表明它们在编码说话人信息中起着关键作用。

Abstract: In recent years, the impact of self-supervised speech Transformers has
extended to speaker-related applications. However, little research has explored
how these models encode speaker information. In this work, we address this gap
by identifying neurons in the feed-forward layers that are correlated with
speaker information. Specifically, we analyze neurons associated with k-means
clusters of self-supervised features and i-vectors. Our analysis reveals that
these clusters correspond to broad phonetic and gender classes, making them
suitable for identifying neurons that represent speakers. By protecting these
neurons during pruning, we can significantly preserve performance on
speaker-related task, demonstrating their crucial role in encoding speaker
information.

</details>


### [65] [(Fact) Check Your Bias](https://arxiv.org/abs/2506.21745)
*Eivind Morris Bakke,Nora Winger Heggelund*

Main category: cs.CL

TL;DR: 研究分析了大型语言模型中的参数知识偏差如何影响事实验证系统的表现，发现不同的提示策略对最终判断预测的影响较小。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型中的参数知识偏差如何影响事实验证系统的性能，特别是HerO系统的表现。

Method: 研究分析了Llama 3.1模型的参数知识偏差及其对HerO系统的影响，并通过两种实验方法进行验证：一是直接提示模型执行事实验证，二是生成支持性、反驳性或中立性的事实验证文档。

Result: 当直接提示模型执行事实验证时，Llama 3.1将近一半的声明标记为“证据不足”。在第二项实验中，模型生成的支持性、反驳性或中立性文档显著影响了检索结果，约50%的检索证据因视角不同而独特。此外，模型有时会拒绝为它认为是错误的声明生成支持性文档，从而产生内在的负面偏见。

Conclusion: 研究发现，尽管提示策略不同，最终的判断预测在不同提示策略下表现出稳定性。

Abstract: Automatic fact verification systems increasingly rely on large language
models (LLMs). We investigate how parametric knowledge biases in these models
affect fact-checking outcomes of the HerO system (baseline for FEVER-25). We
examine how the system is affected by: (1) potential bias in Llama 3.1's
parametric knowledge and (2) intentionally injected bias. When prompted
directly to perform fact-verification, Llama 3.1 labels nearly half the claims
as "Not Enough Evidence". Using only its parametric knowledge it is able to
reach a verdict on the remaining half of the claims. In the second experiment,
we prompt the model to generate supporting, refuting, or neutral fact-checking
documents. These prompts significantly influence retrieval outcomes, with
approximately 50\% of retrieved evidence being unique to each perspective.
Notably, the model sometimes refuses to generate supporting documents for
claims it believes to be false, creating an inherent negative bias. Despite
differences in retrieved evidence, final verdict predictions show stability
across prompting strategies. The code is available at:
https://github.com/eibakke/FEVER-8-Shared-Task

</details>


### [66] [Evaluating List Construction and Temporal Understanding capabilities of Large Language Models](https://arxiv.org/abs/2506.21783)
*Alexandru Dumitru,V Venktesh,Adam Jatowt,Avishek Anand*

Main category: cs.CL

TL;DR: 本文提出了一个名为TLQA的基准，用于评估模型在列表答案构建设置中的时空理解和列表构建能力。研究发现当前模型存在明显不足，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有工作没有广泛评估模型在列表答案构建设置中执行隐性和显性时空理解的能力。为了弥补这一差距，我们提出了TLQA基准。

Method: 我们提出了一个基于时间参考的列表问答（TLQA）基准，要求结构化的列表答案与相应的时间段对齐，并在封闭式和开放域设置中评估了最先进的生成模型的时空理解和列表构建能力。

Result: 我们的发现揭示了当前模型在封闭式设置中无法提供完整的答案以及在开放域设置中需要改进检索的显著不足。

Conclusion: 我们的研究揭示了当前模型在封闭式设置中无法提供完整的答案以及在开放域设置中需要改进检索的显著不足，为TLQA的研究提供了明确的未来方向。

Abstract: Large Language Models (LLMs) have demonstrated immense advances in a wide
range of natural language tasks. However, these models are susceptible to
hallucinations and errors on particularly temporal understanding tasks
involving multiple entities in answers. In such tasks, they fail to associate
entities with accurate time intervals, generate a complete list of entities in
answers or reason about events associated with specific temporal bounds.
Existing works do not extensively evaluate the abilities of the model to
perform implicit and explicit temporal understanding in a list answer
construction setup. To bridge this gap, we propose the Time referenced List
based Question Answering or TLQA benchmark that requires structured answers in
list format aligned with corresponding time periods. Our TLQA benchmark,
requires both list construction and temporal understanding simultaneously,
which to the best of our knowledge has not been explored in prior benchmarks.
We investigate the temporal understanding and list construction capabilities of
state-of-the-art generative models on TLQA in closed-book and open-domain
settings. Our findings reveal significant shortcomings in current models,
particularly their inability to provide complete answers and temporally align
facts in a closed-book setup and the need to improve retrieval in open-domain
setup, providing clear future directions for research on TLQA. The benchmark
and code at https://github.com/elixir-research-group/TLQA.

</details>


### [67] [Offensive Language Detection on Social Media Using XLNet](https://arxiv.org/abs/2506.21795)
*Reem Alothman,Hafida Benhidour,Said Kerrache*

Main category: cs.CL

TL;DR: 本研究提出了一种基于XLNet的自动仇恨语言检测模型，并将其性能与BERT进行了比较。实验结果表明，XLNet在检测仇恨内容和对仇恨类型进行分类方面优于BERT，而BERT在识别仇恨目标方面表现略好。此外，过采样和欠采样策略在解决类别不平衡和提高分类性能方面是有效的。这些发现突显了迁移学习和基于XLNet的架构在创建稳健的社交媒体平台上的仇恨语言检测系统方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体上用户生成内容的庞大数量，手动审核变得不切实际，因此需要自动化系统来检测仇恨语言。

Method: 我们提出了一种基于XLNet的自动仇恨语言检测模型，并将其性能与BERT进行了比较。两种模型都使用Offensive Language Identification Dataset (OLID) 进行评估，这是一个包含层次注释的基准Twitter数据集。

Result: 实验结果表明，XLNet在检测仇恨内容和对仇恨类型进行分类方面优于BERT，而BERT在识别仇恨目标方面表现略好。此外，我们发现过采样和欠采样策略在解决类别不平衡和提高分类性能方面是有效的。

Conclusion: 这些发现突显了迁移学习和基于XLNet的架构在创建稳健的社交媒体平台上的仇恨语言检测系统方面的潜力。

Abstract: The widespread use of text-based communication on social media-through chats,
comments, and microblogs-has improved user interaction but has also led to an
increase in offensive content, including hate speech, racism, and other forms
of abuse. Due to the enormous volume of user-generated content, manual
moderation is impractical, which creates a need for automated systems that can
detect offensive language. Deep learning models, particularly those using
transfer learning, have demonstrated significant success in understanding
natural language through large-scale pretraining. In this study, we propose an
automatic offensive language detection model based on XLNet, a generalized
autoregressive pretraining method, and compare its performance with BERT
(Bidirectional Encoder Representations from Transformers), which is a widely
used baseline in natural language processing (NLP). Both models are evaluated
using the Offensive Language Identification Dataset (OLID), a benchmark Twitter
dataset that includes hierarchical annotations. Our experimental results show
that XLNet outperforms BERT in detecting offensive content and in categorizing
the types of offenses, while BERT performs slightly better in identifying the
targets of the offenses. Additionally, we find that oversampling and
undersampling strategies are effective in addressing class imbalance and
improving classification performance. These findings highlight the potential of
transfer learning and XLNet-based architectures to create robust systems for
detecting offensive language on social media platforms.

</details>


### [68] [A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence](https://arxiv.org/abs/2506.21808)
*Jonathan St-Onge,Ashley M. A. Fehr,Carter Ward,Calla G. Beauregard,Michael V. Arnold,Samuel F. Rosenblatt,Benjamin Cooley,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CL

TL;DR: 本文介绍了一套程序工具，用于在 Matlab、JavaScript 和 Python 中生成排名湍流差异的 allotaxonographs，这些工具适用于不同的使用场景。


<details>
  <summary>Details</summary>
Motivation: 描述和比较复杂系统需要有理论基础的工具。allotaxonographs 提供了对重尾分布对的地图和列表可视化比较，并且可以容纳多种仪器，如排名和概率湍流差异、Jenson-Shannon 差异和广义熵差异。

Method: 本文描述了一套程序工具，用于在 Matlab、JavaScript 和 Python 中渲染排名湍流差异的 allotaxonographs。

Result: 本文介绍了用于生成排名湍流差异的 allotaxonographs 的程序工具，这些工具在 Matlab、JavaScript 和 Python 中实现，适用于不同的使用场景。

Conclusion: 本文介绍了用于生成排名湍流差异的 allotaxonographs 的程序工具，这些工具在 Matlab、JavaScript 和 Python 中实现，适用于不同的使用场景。

Abstract: Describing and comparing complex systems requires principled, theoretically
grounded tools. Built around the phenomenon of type turbulence,
allotaxonographs provide map-and-list visual comparisons of pairs of
heavy-tailed distributions. Allotaxonographs are designed to accommodate a wide
range of instruments including rank- and probability-turbulence divergences,
Jenson-Shannon divergence, and generalized entropy divergences. Here, we
describe a suite of programmatic tools for rendering allotaxonographs for
rank-turbulence divergence in Matlab, Javascript, and Python, all of which have
different use cases.

</details>


### [69] [Towards Transparent AI: A Survey on Explainable Large Language Models](https://arxiv.org/abs/2506.21812)
*Avash Palikhe,Zhenyu Yu,Zichong Wang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本文对基于Transformer架构的LLMs的XAI方法进行了全面综述，旨在提高这些模型的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在人工智能领域取得了显著成就，但其决策过程缺乏透明度，这限制了它们在高风险领域的应用。因此，需要系统地理解这些方法以促进可解释性。

Method: 本文对基于Transformer架构的LLMs的XAI方法进行了全面的综述，包括编码器-only、解码器-only和编码器-解码器模型。此外，还评估了这些方法在解释性方面的表现，并探讨了它们在实际应用中的使用情况。

Result: 本文提供了对XAI方法的全面综述，涵盖了不同Transformer架构下的技术，并讨论了它们在实际应用中的使用情况以及未来的研究方向。

Conclusion: 本文旨在通过系统回顾和分类XAI方法，为开发透明和负责任的LLMs提供指导。

Abstract: Large Language Models (LLMs) have played a pivotal role in advancing
Artificial Intelligence (AI). However, despite their achievements, LLMs often
struggle to explain their decision-making processes, making them a 'black box'
and presenting a substantial challenge to explainability. This lack of
transparency poses a significant obstacle to the adoption of LLMs in
high-stakes domain applications, where interpretability is particularly
essential. To overcome these limitations, researchers have developed various
explainable artificial intelligence (XAI) methods that provide
human-interpretable explanations for LLMs. However, a systematic understanding
of these methods remains limited. To address this gap, this survey provides a
comprehensive review of explainability techniques by categorizing XAI methods
based on the underlying transformer architectures of LLMs: encoder-only,
decoder-only, and encoder-decoder models. Then these techniques are examined in
terms of their evaluation for assessing explainability, and the survey further
explores how these explanations are leveraged in practical applications.
Finally, it discusses available resources, ongoing research challenges, and
future directions, aiming to guide continued efforts toward developing
transparent and responsible LLMs.

</details>


### [70] [Exploring the Structure of AI-Induced Language Change in Scientific English](https://arxiv.org/abs/2506.21817)
*Riley Galpin,Bryce Anderson,Tom S. Juzek*

Main category: cs.CL

TL;DR: 本研究分析了科学英语中词汇使用的变化，发现这些变化主要是语义和语用上的，而非单纯的词汇替换。


<details>
  <summary>Details</summary>
Motivation: 研究科学英语中词汇使用的变化，特别是大型语言模型的影响。

Method: 通过词性标注分析语言变化，并系统分析了同义词组的频率趋势。

Result: 发现整个语义群组通常一起变化，大多数或所有词语在一组中都增加了使用率。此外，'important' 一词显著下降，而'collapsing' 词语的分析揭示了更复杂的情况。

Conclusion: 这些关于语言变化结构的见解有助于我们理解语言技术如何继续塑造人类语言。

Abstract: Scientific English has undergone rapid and unprecedented changes in recent
years, with words such as "delve," "intricate," and "crucial" showing
significant spikes in frequency since around 2022. These changes are widely
attributed to the growing influence of Large Language Models like ChatGPT in
the discourse surrounding bias and misalignment. However, apart from changes in
frequency, the exact structure of these linguistic shifts has remained unclear.
The present study addresses this and investigates whether these changes involve
the replacement of synonyms by suddenly 'spiking words,' for example, "crucial"
replacing "essential" and "key," or whether they reflect broader semantic and
pragmatic qualifications. To further investigate structural changes, we include
part of speech tagging in our analysis to quantify linguistic shifts over
grammatical categories and differentiate between word forms, like "potential"
as a noun vs. as an adjective. We systematically analyze synonym groups for
widely discussed 'spiking words' based on frequency trends in scientific
abstracts from PubMed. We find that entire semantic clusters often shift
together, with most or all words in a group increasing in usage. This pattern
suggests that changes induced by Large Language Models are primarily semantic
and pragmatic rather than purely lexical. Notably, the adjective "important"
shows a significant decline, which prompted us to systematically analyze
decreasing lexical items. Our analysis of "collapsing" words reveals a more
complex picture, which is consistent with organic language change and contrasts
with the patterns of the abrupt spikes. These insights into the structure of
language change contribute to our understanding of how language technology
continues to shape human language.

</details>


### [71] [PARSI: Persian Authorship Recognition via Stylometric Integration](https://arxiv.org/abs/2506.21840)
*Kourosh Shahnazari,Mohammadali Keshtparvar,Seyed Moein Ayyoubzadeh*

Main category: cs.CL

TL;DR: 本研究提出了一种多输入神经框架，用于确定67位著名诗人的作者归属，通过结合深度表示和领域特定特征，实现了较高的准确率。


<details>
  <summary>Details</summary>
Motivation: 波斯古典诗歌的复杂语言、风格和韵律方面对计算作者归属构成了挑战。

Method: 我们采用了一个多输入神经框架，包括基于Transformer的语言编码器以及处理波斯诗歌语义、风格和韵律维度的特征。

Result: 加权投票方法在评估中达到了71%的准确率，而在0.9阈值下，模型可以生成高度自信的预测，准确率达到97%。

Conclusion: 本研究展示了将深度表示形式与领域特定特征相结合在作者归属中的潜力，并为自动分类和风格分析做出了贡献。

Abstract: The intricate linguistic, stylistic, and metrical aspects of Persian
classical poetry pose a challenge for computational authorship attribution. In
this work, we present a versatile framework to determine authorship among 67
prominent poets. We employ a multi-input neural framework consisting of a
transformer-based language encoder complemented by features addressing the
semantic, stylometric, and metrical dimensions of Persian poetry. Our feature
set encompasses 100-dimensional Word2Vec embeddings, seven stylometric
measures, and categorical encodings of poetic form and meter. We compiled a
vast corpus of 647,653 verses of the Ganjoor digital collection, validating the
data through strict preprocessing and author verification while preserving
poem-level splitting to prevent overlap. This work employs verse-level
classification and majority and weighted voting schemes in evaluation,
revealing that weighted voting yields 71% accuracy. We further investigate
threshold-based decision filtering, allowing the model to generate highly
confident predictions, achieving 97% accuracy at a 0.9 threshold, though at
lower coverage. Our work focuses on the integration of deep representational
forms with domain-specific features for improved authorship attribution. The
results illustrate the potential of our approach for automated classification
and the contribution to stylistic analysis, authorship disputes, and general
computational literature research. This research will facilitate further
research on multilingual author attribution, style shift, and generative
modeling of Persian poetry.

</details>


### [72] [LinguaSynth: Heterogeneous Linguistic Signals for News Classification](https://arxiv.org/abs/2506.21848)
*Duo Zhang,Junyi Mo*

Main category: cs.CL

TL;DR: 本文提出了一种名为LinguaSynth的新文本分类框架，该框架在透明的逻辑回归模型中整合了五种互补的语言特征类型，实现了较高的准确率并保持了可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自然语言处理（NLP）方面取得了显著进展，但其对大型黑盒模型的依赖带来了关键的可解释性和计算效率问题。

Method: LinguaSynth是一个新颖的文本分类框架，它在透明的逻辑回归模型中战略性地整合了五种互补的语言特征类型：词汇、句法、实体级、词级语义和文档级语义。

Result: LinguaSynth在20 Newsgroups数据集上实现了84.89%的准确率，并且比强大的TF-IDF基线高出3.32%。通过严格的特征交互分析，我们展示了句法和实体级信号提供了重要的消歧，并有效地补充了分布语义。

Conclusion: LinguaSynth设置了一个新的基准，为可解释的、资源高效的NLP模型提供了可能性，并挑战了深度神经网络对于高性能文本分类是必要的这一普遍假设。

Abstract: Deep learning has significantly advanced NLP, but its reliance on large
black-box models introduces critical interpretability and computational
efficiency concerns. This paper proposes LinguaSynth, a novel text
classification framework that strategically integrates five complementary
linguistic feature types: lexical, syntactic, entity-level, word-level
semantics, and document-level semantics within a transparent logistic
regression model. Unlike transformer-based architectures, LinguaSynth maintains
interpretability and computational efficiency, achieving an accuracy of 84.89
percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by
3.32 percent. Through rigorous feature interaction analysis, we show that
syntactic and entity-level signals provide essential disambiguation and
effectively complement distributional semantics. LinguaSynth sets a new
benchmark for interpretable, resource-efficient NLP models and challenges the
prevailing assumption that deep neural networks are necessary for
high-performing text classification.

</details>


### [73] [The Consistency Hypothesis in Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2506.21849)
*Quan Xiao,Debarun Bhattacharjya,Balaji Ganesan,Radu Marinescu,Katsiaryna Mirylenka,Nhan H Pham,Michael Glass,Junkyu Lee*

Main category: cs.CL

TL;DR: 本文研究了黑盒不确定性量化方法中生成一致性作为置信度代理的假设，并提出了基于相似性的无数据方法，以提高置信度估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 估计大型语言模型（LLM）输出的置信度对于需要高用户信任的实际应用至关重要。现有的黑盒不确定性量化（UQ）方法依赖于模型API访问，但由于其实际优势而受到欢迎。然而，这些方法隐含地假设生成的一致性可以作为置信度的代理，本文旨在探讨这一假设。

Method: 本文引入了三个数学陈述和相应的统计检验来捕捉该假设的不同变体，并提出了评估LLM输出一致性的指标。此外，还提出了基于生成相似性的无数据黑盒UQ方法。

Result: 实证研究表明，在不同设置下，一致性假设普遍存在。其中，'Sim-Any'假设最为可行，并通过提出基于生成相似性的无数据黑盒UQ方法加以利用，这些方法在性能上优于现有基线。

Conclusion: 本文通过实证研究验证了一致性假设的普遍性，并提出了基于相似性的无数据黑盒UQ方法，这些方法在性能上优于现有基线，展示了该假设的实际价值。

Abstract: Estimating the confidence of large language model (LLM) outputs is essential
for real-world applications requiring high user trust. Black-box uncertainty
quantification (UQ) methods, relying solely on model API access, have gained
popularity due to their practical benefits. In this paper, we examine the
implicit assumption behind several UQ methods, which use generation consistency
as a proxy for confidence, an idea we formalize as the consistency hypothesis.
We introduce three mathematical statements with corresponding statistical tests
to capture variations of this hypothesis and metrics to evaluate LLM output
conformity across tasks. Our empirical investigation, spanning 8 benchmark
datasets and 3 tasks (question answering, text summarization, and text-to-SQL),
highlights the prevalence of the hypothesis under different settings. Among the
statements, we highlight the `Sim-Any' hypothesis as the most actionable, and
demonstrate how it can be leveraged by proposing data-free black-box UQ methods
that aggregate similarities between generations for confidence estimation.
These approaches can outperform the closest baselines, showcasing the practical
value of the empirically observed consistency hypothesis.

</details>


### [74] [Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models](https://arxiv.org/abs/2506.21861)
*Taiga Someya,Ryo Yoshida,Hitomi Yanaka,Yohei Oseki*

Main category: cs.CL

TL;DR: 本文提出了一种名为派生探测的方法，用于研究微句法结构和宏观句法结构是如何在词嵌入向上传播时构建的。实验结果表明，微句法结构在较低层中出现，并在较高层中逐渐整合成连贯的宏观句法结构。此外，对主谓数一致的评估表明，构建宏观句法结构的时间对于下游性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的研究表明神经语言模型在其内部表示中编码了句法结构，但这些结构如何通过层构建仍然不为人所知。

Method: 我们提出了派生探测方法来研究微句法结构和宏观句法结构是如何在词嵌入向上传播时构建的。

Result: 我们的实验表明，微句法结构在较低层中出现，并在较高层中逐渐整合成连贯的宏观句法结构。此外，对主谓数一致的针对性评估表明，构建宏观句法结构的时间对于下游性能至关重要。

Conclusion: 我们的实验表明，微句法结构在较低层中出现，并在较高层中逐渐整合成连贯的宏观句法结构。此外，对主谓数一致的针对性评估表明，构建宏观句法结构的时间对于下游性能至关重要，这表明了整合全局句法信息的最佳时间。

Abstract: Recent work has demonstrated that neural language models encode syntactic
structures in their internal representations, yet the derivations by which
these structures are constructed across layers remain poorly understood. In
this paper, we propose Derivational Probing to investigate how micro-syntactic
structures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,
the relationship between the root verbs and their direct dependents) are
constructed as word embeddings propagate upward across layers. Our experiments
on BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge
in lower layers and are gradually integrated into a coherent macro-syntactic
structure in higher layers. Furthermore, a targeted evaluation on subject-verb
number agreement shows that the timing of constructing macro-syntactic
structures is critical for downstream performance, suggesting an optimal timing
for integrating global syntactic information.

</details>


### [75] [DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE](https://arxiv.org/abs/2506.21864)
*Hang Shao,Heting Gao,Yunhang Shen,Jiawei Chen,Lijiang Li,Zuwei Long,Bo Tong,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: DeepTalk is a framework that improves native MLLMs by using an MoE architecture to reduce performance drops and maintain low latency.


<details>
  <summary>Details</summary>
Motivation: Native MLLMs suffer from catastrophic forgetting and performance degradation due to insufficient paired speech-text data.

Method: DeepTalk uses an Mixture of Experts (MoE) architecture to adaptively distinguish modality experts and perform specialized training.

Result: DeepTalk incurs only a 5.5% performance drop compared to the original LLM, which is significantly lower than the average performance drop of over 20% typically seen in native MLLMs.

Conclusion: DeepTalk achieves a low performance drop and maintains low latency, making it suitable for seamless speech interaction.

Abstract: Native multimodal large language models (MLLMs) restructure a single large
language model (LLM) into a spoken language model (SLM) capable of both speech
and text generation. Compared to modular and aligned MLLMs, native MLLMs
preserve richer paralinguistic features such as emotion and prosody, and
generate speech responses directly within the backbone LLM rather than using a
separate speech decoder. This integration also results in lower response
latency and smoother interaction. However, native MLLMs suffer from
catastrophic forgetting and performance degradation because the available
paired speech-text data is insufficient to support the pretraining of MLLMs
compared to the vast amount of text data required to pretrain text LLMs. To
address this issue, we propose DeepTalk, a framework for adaptive modality
expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk
first adaptively distinguishes modality experts according to their modality
load within the LLM. Each modality expert then undergoes specialized
single-modality training, followed by joint multimodal collaborative training.
As a result, DeepTalk incurs only a 5.5% performance drop compared to the
original LLM, which is significantly lower than the average performance drop of
over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par
with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within
0.5 seconds, ensuring a seamless and intelligent speech interaction experience.
Code and models are released at https://github.com/talkking/DeepTalk.

</details>


### [76] [WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](https://arxiv.org/abs/2506.21875)
*Jian Zhang,Linhao Zhang,Bokai Lei,Chuhan Wu,Wei Jia,Xiao Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，以全面评估实际语音对话中的LLMs。通过整理现实世界的聊天数据、引入说话人属性和声学条件的多样性，并增强数据集以包含语音特定现象，我们设计了查询感知的评估方法。测试和分析显示了不同语音场景下模型性能的显著差异，并提供了对语音模型开发和评估有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法往往适应基于文本的基准，忽视了语音的独特特征和挑战，包括语调、同音词、口吃和不同的用户期望。缺乏专门和全面的端到端语音LLM评估基准阻碍了Audio LLM在实际应用中的用户体验优化。

Method: 我们系统地整理了与口语场景相关的现实世界聊天数据，引入了说话人属性和声学条件的多样性，并通过语音特定现象增强数据集。我们进一步设计了查询感知的评估方法，使用定制的评估检查表和提示来提高自动评估的准确性。

Result: 我们对各种主流语音模型进行了全面测试和详细分析，揭示了不同语音场景下模型性能的显著差异。查询感知评估的使用进一步实现了在各种语音特定场景下的更细致的评估。

Conclusion: 我们的基准可以为语音模型的开发和评估提供有价值的见解。

Abstract: Recent multi-modal Large Language Models (LLMs) such as GPT-4o have
demonstrated strong capabilities of direct speech interaction. However, the
lack of specialized and comprehensive benchmarks for end-to-end speech LLM
evaluation hinders optimizing the user experience of Audio LLMs in real-world
applications. Existing evaluation methods often adapt text-based benchmarks,
overlooking speech's unique characteristics and challenges, including prosody,
homophones, stuttering, and differing user expectations. Here, we present a
novel approach to thoroughly evaluate LLMs in practical speech conversations.
We systematically curate real-world chat data relevant to spoken scenarios,
introduce diversity in speaker attributes and acoustic conditions, and augment
the dataset with speech-specific phenomena. We further design a query-aware
evaluation method to use customized evaluation checklists and prompts to
enhance the accuracy of automatic evaluation. We conduct comprehensive testing
and detailed analysis of various mainstream speech models, revealing
significant differences in model performance across different speech scenarios.
The use of query-aware evaluation further enables a finer-grained assessment
under various speech-specific scenarios. Our benchmark can provide valuable
insights for speech model development and evaluation.

</details>


### [77] [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
*Qiyue Gao,Xinyu Pi,Kevin Liu,Junrong Chen,Ruolan Yang,Xinqi Huang,Xinyu Fang,Lu Sun,Gautham Kishore,Bo Ai,Stone Tao,Mengyang Liu,Jiaxi Yang,Chao-Jung Lai,Chuanyang Jin,Jiannan Xiang,Benhao Huang,Zeming Chen,David Danks,Hao Su,Tianmin Shu,Ziqiao Ma,Lianhui Qin,Zhiting Hu*

Main category: cs.CL

TL;DR: 本文评估了VLMs作为世界模型的能力，发现它们在基本能力上存在显著不足，并提出了一个大规模基准测试来进一步研究这一问题。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的研究已经评估并展示了VLMs在特定能力上的局限性，但对VLMs基础世界建模能力的系统评估仍然缺失。

Method: 我们提出了一种两阶段框架，用于评估感知（视觉、空间、时间、数量和运动）和预测（机制模拟、传递推理、组合推理），以对VLMs作为世界模型进行原子评估。基于此框架，我们引入了WM-ABench，一个大规模基准测试，包含23个细粒度的评估维度，覆盖6个多样化的模拟环境，并具有受控的反事实模拟。

Result: 通过在15个最新商业和开源VLMs上的660次实验，我们发现这些模型在基本世界建模能力上表现出显著的局限性。例如，几乎所有模型在区分运动轨迹时的表现接近随机准确率。此外，它们缺乏解耦的理解——例如，一些模型倾向于认为蓝色物体比绿色物体移动得更快。

Conclusion: 这些模型在基本的世界建模能力上表现出显著的局限性，与人类水平的世界建模存在显著差距。

Abstract: Internal world models (WMs) enable agents to understand the world's state and
predict transitions, serving as the basis for advanced deliberative reasoning.
Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and
Gemini, exhibit potential as general-purpose WMs. While the latest studies have
evaluated and shown limitations in specific capabilities such as visual
understanding, a systematic evaluation of VLMs' fundamental WM abilities
remains absent. Drawing on comparative psychology and cognitive science, we
propose a two-stage framework that assesses Perception (visual, spatial,
temporal, quantitative, and motion) and Prediction (mechanistic simulation,
transitive inference, compositional inference) to provide an atomic evaluation
of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale
benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse
simulated environments with controlled counterfactual simulations. Through 660
experiments on 15 latest commercial and open-source VLMs, we find that these
models exhibit striking limitations in basic world modeling abilities. For
instance, almost all models perform at near-random accuracy when distinguishing
motion trajectories. Additionally, they lack disentangled understanding --
e.g., some models tend to believe blue objects move faster than green ones.
More rich results and analyses reveal significant gaps between VLMs and
human-level world modeling.

</details>


### [78] [A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs](https://arxiv.org/abs/2506.21881)
*Sean Kim,Hyuhng Joon Kim*

Main category: cs.CL

TL;DR: 本文通过两阶段评估研究了大型语言模型（LLMs）中的模型偏差和推理偏差，并提出了一个结构化的框架，用于评估LLMs在中性和敏感主题上的行为，为未来的LLM部署和多语言环境中的文化意识评估提供了见解。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在多样化的语言和文化环境中日益部署，理解它们在事实性和争议性场景中的行为至关重要，尤其是在它们的输出可能影响公众意见或强化主流叙事时。

Method: 通过两阶段评估来定义LLMs中的两种偏差：模型偏差（源自模型训练的偏差）和推理偏差（由查询语言引起的偏差）。第一阶段评估LLMs在事实性问题上的表现，第二阶段则扩展到地缘政治敏感争议的问题。

Result: 结果表明，第一阶段显示出查询语言引起的对齐现象，而第二阶段则反映了模型训练背景与查询语言之间的相互作用。

Conclusion: 本文提出了一个结构化的框架，用于评估大型语言模型（LLMs）在中性和敏感主题上的行为，为未来的LLM部署和多语言环境中的文化意识评估实践提供了见解。

Abstract: As large language models (LLMs) are increasingly deployed across diverse
linguistic and cultural contexts, understanding their behavior in both factual
and disputable scenarios is essential, especially when their outputs may shape
public opinion or reinforce dominant narratives. In this paper, we define two
types of bias in LLMs: model bias (bias stemming from model training) and
inference bias (bias induced by the language of the query), through a two-phase
evaluation. Phase 1 evaluates LLMs on factual questions where a single
verifiable answer exists, assessing whether models maintain consistency across
different query languages. Phase 2 expands the scope by probing geopolitically
sensitive disputes, where responses may reflect culturally embedded or
ideologically aligned perspectives. We construct a manually curated dataset
spanning both factual and disputable QA, across four languages and question
types. The results show that Phase 1 exhibits query language induced alignment,
while Phase 2 reflects an interplay between the model's training context and
query language. This paper offers a structured framework for evaluating LLM
behavior across neutral and sensitive topics, providing insights for future LLM
deployment and culturally aware evaluation practices in multilingual contexts.

</details>


### [79] [AutoMixer: Checkpoint Artifacts as Automatic Data Mixers](https://arxiv.org/abs/2506.21910)
*Ernie Chang,Yang Li,Patrick Huber,David Kant,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: 该研究提出了一种利用检查点模型来优化数据混合的方法，并在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在语言模型训练中，希望模型具备多种任务的能力，但如何直接获得这些能力的数据混合尚不明确。

Method: 该研究通过利用检查点模型的聚合一阶影响近似来作为数据混合器，从而改进预训练设置中的性能。

Result: 在八个推理基准测试中，所提出的框架在预训练设置中表现出显著的改进，性能提升高达1.93%。

Conclusion: 该研究展示了检查点模型在增强数据质量和优化数据混合方面的潜力。

Abstract: In language model training, it is desirable to equip models with capabilities
from various tasks. However, it is not clear how to directly obtain the right
data mixtures for these capabilities as the relationship between data and tasks
is difficult to be modeled. In this work, we observe that checkpoint models
exhibit emerging capabilities at different points in the training trajectory.
Often, the training process saves checkpoints as artifacts that are
under-utilized as a source of in-training data signals. We identify these
artifact models based on their respective capabilities on the benchmarks and
leverage them as data mixers by using their aggregated first-order influence
approximation over source data. We demonstrated on eight reasoning benchmarks
that the proposed framework shows significant improvements in the pretraining
setting, with performance improvements of up to 1.93%. Overall, this shows the
potential of checkpoint models to enhance data quality and optimize data
mixtures.

</details>


### [80] [PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory](https://arxiv.org/abs/2506.21961)
*Junho Myung,Yeon Su Park,Sunwoo Kim,Shin Yoo,Alice Oh*

Main category: cs.CL

TL;DR: 本文介绍了PapersPlease基准，用于评估大型语言模型在道德困境中的决策，并发现LLMs在决策中存在隐含的偏好，特别是在处理边缘化身份时表现出更高的拒绝率。


<details>
  <summary>Details</summary>
Motivation: 通过角色扮演场景评估大型语言模型（LLMs）的性能和偏见变得越来越普遍，因为LLMs在这些情况下经常表现出偏见行为。我们的研究旨在通过PapersPlease基准来探讨LLMs在决策中的偏见和偏好。

Method: 我们引入了PapersPlease，一个包含3,700个道德困境的基准，旨在研究LLMs在优先考虑各种人类需求水平时的决策。在我们的设置中，LLMs充当移民检查员，根据人们的简短叙述决定是否批准或拒绝入境。这些叙述是使用存在、相关性和成长（ERG）理论构建的，该理论将人类需求分为三个层次。

Result: 我们的分析揭示了LLMs在决策中的统计显著模式，表明它们编码了隐含的偏好。此外，我们的评估显示，将社会身份纳入叙述中对不同动机需求和身份线索的响应各不相同，一些模型对边缘化身份的拒绝率更高。

Conclusion: 我们的分析揭示了LLMs在决策中的统计显著模式，表明它们编码了隐含的偏好。此外，我们的评估显示，将社会身份纳入叙述中对不同动机需求和身份线索的响应各不相同，一些模型对边缘化身份的拒绝率更高。

Abstract: Evaluating the performance and biases of large language models (LLMs) through
role-playing scenarios is becoming increasingly common, as LLMs often exhibit
biased behaviors in these contexts. Building on this line of research, we
introduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed
to investigate LLMs' decision-making in prioritizing various levels of human
needs. In our setup, LLMs act as immigration inspectors deciding whether to
approve or deny entry based on the short narratives of people. These narratives
are constructed using the Existence, Relatedness, and Growth (ERG) theory,
which categorizes human needs into three hierarchical levels. Our analysis of
six LLMs reveals statistically significant patterns in decision-making,
suggesting that LLMs encode implicit preferences. Additionally, our evaluation
of the impact of incorporating social identities into the narratives shows
varying responsiveness based on both motivational needs and identity cues, with
some models exhibiting higher denial rates for marginalized identities. All
data is publicly available at https://github.com/yeonsuuuu28/papers-please.

</details>


### [81] [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967)
*Weimin Xiong,Ke Wang,Yifan Song,Hanchao Liu,Sai Zhou,Wei Peng,Sujian Li*

Main category: cs.CL

TL;DR: 本研究调查了工具集成的LLM代理在工具调用过程中的稳定性问题，发现代理在各个阶段都容易出错，基于开源模型的代理更易受攻击，且增加模型大小可能不会提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前对工具集成的LLM代理的评估通常侧重于端到端的工具使用评估，而忽视了其稳定性。这限制了它们在现实世界中的适用性，因为各种内部或外部因素可能导致代理崩溃或行为异常。

Method: 我们通过广泛的实验，研究了代理在工具调用过程中的各个阶段（包括阅读工具文档、选择工具和生成参数，以及处理工具的响应）是否容易出错。

Result: 我们观察到代理在每个阶段都极易出错，基于开源模型的代理比基于专有模型的代理更容易受到攻击。我们还发现增加模型大小不会显著提高工具调用推理能力，甚至可能使代理更容易受到类似于正常用户指令的攻击。

Conclusion: 我们的研究强调了评估代理稳定性的重要性，并为未来的LLM开发和评估提供了有价值的见解。

Abstract: Current evaluations of tool-integrated LLM agents typically focus on
end-to-end tool-usage evaluation while neglecting their stability. This limits
their real-world applicability, as various internal or external factors can
cause agents to crash or behave abnormally. Our research addresses this by
investigating whether agents are vulnerable to errors throughout the entire
tool invocation process, including reading tool documentation, selecting tools
and generating parameters, and processing the tool's response. Through
extensive experiments, we observe that agents are highly susceptible to errors
at each stage and agents based on open-source models are more vulnerable than
those based on proprietary models. We also find that increasing the model size
does not significantly improve tool invocation reasoning and may make agents
more vulnerable to attacks resembling normal user instructions. This highlights
the importance of evaluating agent stability and offers valuable insights for
future LLM development and evaluation.

</details>


### [82] [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972)
*Mohamed Ahmed,Mohamed Abdelmouty,Mingyu Kim,Gunvanth Kandula,Alex Park,James C. Davis*

Main category: cs.CL

TL;DR: 本文提出了两种混合方法，结合了令牌级和提示级技术，以提高在各种PTLM上的越狱效果。评估结果显示，这些混合方法在未受保护的模型上显著提高了攻击成功率，并且在面对先进防御措施时表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了应对这些方法的互补限制，我们提出了两种混合方法，以提高越狱效果。

Method: 我们提出了两种混合方法，将令牌级和提示级技术结合起来，以提高在各种PTLM上的越狱效果。GCG + PAIR和新探索的GCG + WordGame混合方法在多个Vicuna和Llama模型上进行了评估。

Result: GCG + PAIR在未受保护的模型上 consistently 提高了攻击成功率，例如在Llama-3上，其攻击成功率（ASR）达到91.6%，比PAIR的58.4%基线有了显著提高。同时，GCG + WordGame保持了WordGame的原始性能，在更严格的评估器如Mistral-Sorry-Bench下仍保持超过80%的高ASR。这两种混合方法保留了可转移性，并可靠地穿透了先进的防御措施，如Gradient Cuff和JBShield，这些措施完全阻止了单模式攻击。

Conclusion: 这些发现揭示了当前安全堆栈中以前未报告的漏洞，突出了原始成功率和防御鲁棒性之间的权衡，并强调了对适应性对手进行全面保障的必要性。

Abstract: The advancement of Pre-Trained Language Models (PTLMs) and Large Language
Models (LLMs) has led to their widespread adoption across diverse applications.
Despite their success, these models remain vulnerable to attacks that exploit
their inherent weaknesses to bypass safety measures. Two primary
inference-phase threats are token-level and prompt-level jailbreaks.
Token-level attacks embed adversarial sequences that transfer well to black-box
models like GPT but leave detectable patterns and rely on gradient-based token
optimization, whereas prompt-level attacks use semantically structured inputs
to elicit harmful responses yet depend on iterative feedback that can be
unreliable. To address the complementary limitations of these methods, we
propose two hybrid approaches that integrate token- and prompt-level techniques
to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the
newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and
Llama models. GCG + PAIR consistently raised attack-success rates over its
constituent techniques on undefended models; for instance, on Llama-3, its
Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's
58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of
WordGame maintaining a high ASR of over 80% even under stricter evaluators like
Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and
reliably pierced advanced defenses such as Gradient Cuff and JBShield, which
fully blocked single-mode attacks. These findings expose previously unreported
vulnerabilities in current safety stacks, highlight trade-offs between raw
success and defensive robustness, and underscore the need for holistic
safeguards against adaptive adversaries.

</details>


### [83] [Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism](https://arxiv.org/abs/2506.21974)
*Simon Münker,Nils Schwager,Achim Rettinger*

Main category: cs.CL

TL;DR: 本文探讨了使用大型语言模型模拟社交网络用户行为的可行性，并强调了在应用生成代理建模进行社会模拟时需要更多的严谨性。


<details>
  <summary>Details</summary>
Motivation: 由于关于大型语言模型（LLMs）是否以及何时能够模拟人类行为的研究结果存在矛盾，因此需要更好地理解实验设计的差异。

Method: 本文提出了一个用于模拟社交网络的正式框架，并专注于模仿用户通信的子任务。此外，还对不同的方法进行了实证测试。

Result: 研究结果表明，社会模拟应通过其在拟合模拟组件的环境中测量的经验现实性进行验证。

Conclusion: 本文主张在应用生成代理建模进行社会模拟时需要更多的严谨性。

Abstract: The ability of Large Language Models (LLMs) to mimic human behavior triggered
a plethora of computational social science research, assuming that empirical
studies of humans can be conducted with AI agents instead. Since there have
been conflicting research findings on whether and when this hypothesis holds,
there is a need to better understand the differences in their experimental
designs. We focus on replicating the behavior of social network users with the
use of LLMs for the analysis of communication on social networks. First, we
provide a formal framework for the simulation of social networks, before
focusing on the sub-task of imitating user communication. We empirically test
different approaches to imitate user behavior on X in English and German. Our
findings suggest that social simulations should be validated by their empirical
realism measured in the setting in which the simulation components were fitted.
With this paper, we argue for more rigor when applying generative-agent-based
modeling for social simulation.

</details>


### [84] [Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit](https://arxiv.org/abs/2506.21990)
*Kartheek Kumar Reddy Nareddy,Sarah Ternus,Julia Niebling*

Main category: cs.CL

TL;DR: 本文研究了如何提高Whisper模型在机舱对话转录中的准确性，通过归一化和微调方法显著降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练模型在通用领域表现良好，但在特定领域如机舱对话转录中效果不佳，因为这些领域涉及大量专业词汇和多语言对话。

Method: 本文收集了85分钟的机舱模拟录音和130分钟的飞行员访谈录音，并手动标记。然后提出了多种归一化方案来优化转录结果，并采用低秩适应（LoRA）进行高效微调。

Result: 通过提出的归一化方案和微调，Word Error Rate (WER) 从68.49%降低到26.26%。

Conclusion: 本文通过提出多种归一化方案并进行微调，显著提高了Whisper模型在机舱对话转录中的准确性。

Abstract: The developments in transformer encoder-decoder architectures have led to
significant breakthroughs in machine translation, Automatic Speech Recognition
(ASR), and instruction-based chat machines, among other applications. The
pre-trained models were trained on vast amounts of generic data over a few
epochs (fewer than five in most cases), resulting in their strong
generalization capabilities. Nevertheless, the performance of these models does
suffer when applied to niche domains like transcribing pilot speech in the
cockpit, which involves a lot of specific vocabulary and multilingual
conversations. This paper investigates and improves the transcription accuracy
of cockpit conversations with Whisper models. We have collected around 85
minutes of cockpit simulator recordings and 130 minutes of interview recordings
with pilots and manually labeled them. The speakers are middle aged men
speaking both German and English. To improve the accuracy of transcriptions, we
propose multiple normalization schemes to refine the transcripts and improve
Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance,
utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).
Hereby, WER decreased from 68.49 \% (pretrained whisper Large model without
normalization baseline) to 26.26\% (finetuned whisper Large model with the
proposed normalization scheme).

</details>


### [85] [Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation](https://arxiv.org/abs/2506.22038)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: 本研究比较了机器翻译和人类翻译在儿童文学翻译中的风格特征，发现大型语言模型在某些方面表现优于神经机器翻译，接近人类翻译的风格。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在从风格学角度评估机器翻译（MTs）与人类翻译（HTs）在英文到中文儿童文学翻译（CLT）中的表现。

Method: 研究构建了一个彼得·潘语料库，包含21个翻译版本（7个人类翻译、7个大型语言模型翻译和7个神经机器翻译）。分析采用了通用特征集（包括词汇、句法、可读性和n-gram特征）和创意文本翻译（CTT特定）特征集，总共提取了447个语言特征。通过机器学习中的分类和聚类技术进行了风格分析。

Result: 在通用特征方面，HTs和MTs在连词分布和1词-gram-YiYang比例上存在显著差异；NMTs和LLMs在描述性词汇使用和副词比例上表现出显著变化。在CTT特定特征方面，LLMs在分布上优于NMTs，更接近HTs的风格特征。

Conclusion: 研究发现，大型语言模型（LLMs）在儿童文学翻译（CLT）中表现出与人类翻译（HTs）相似的风格特征，显示出其在CLT中的潜力。

Abstract: This study focuses on evaluating the performance of machine translations
(MTs) compared to human translations (HTs) in English-to-Chinese children's
literature translation (CLT) from a stylometric perspective. The research
constructs a Peter Pan corpus, comprising 21 translations: 7 human translations
(HTs), 7 large language model translations (LLMs), and 7 neural machine
translation outputs (NMTs). The analysis employs a generic feature set
(including lexical, syntactic, readability, and n-gram features) and a creative
text translation (CTT-specific) feature set, which captures repetition, rhythm,
translatability, and miscellaneous levels, yielding 447 linguistic features in
total.
  Using classification and clustering techniques in machine learning, we
conduct a stylometric analysis of these translations. Results reveal that in
generic features, HTs and MTs exhibit significant differences in conjunction
word distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs
show significant variation in descriptive words usage and adverb ratios.
Regarding CTT-specific features, LLMs outperform NMTs in distribution, aligning
more closely with HTs in stylistic characteristics, demonstrating the potential
of LLMs in CLT.

</details>


### [86] [Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs](https://arxiv.org/abs/2506.22050)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: 本研究分析了机器翻译输出（MTese）的语义特征，特别是在英语到中文的新闻文本中。通过构建大规模数据集和应用卡方排名算法，研究发现MTese在NMTs和LLMs中都存在。原始中文文本几乎可以完美地区分LLM和NMT的输出。研究还发现，LLMs表现出更大的词汇多样性，而NMTs使用更多的括号。特定于翻译的LLMs表现出较低的词汇多样性但更高的因果连词使用率。最后，研究发现中国公司开发的LLMs与外国公司的LLMs之间没有显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索机器翻译输出（MTese）的语义特征，特别是在新闻文本中较少研究的英语到中文语言对。

Method: 研究构建了一个包含4个子语料库的大规模数据集，并采用了一套全面的五层特征集。然后，应用卡方排名算法进行分类和聚类任务中的特征选择。

Result: 研究结果确认了MTese在NMTs和LLMs中的存在。原始中文文本几乎可以完美地区分LLM和NMT的输出。MT输出中出现了较短的句子长度和更多的转折连词。比较LLMs和NMTs，我们实现了约70%的分类准确率，其中LLMs表现出更大的词汇多样性，而NMTs使用更多的括号。特定于翻译的LLMs表现出较低的词汇多样性但更高的因果连词使用率。最后，研究发现中国公司开发的LLMs与外国公司的LLMs之间没有显著差异。

Conclusion: 研究发现，机器翻译输出（MTese）在神经机器翻译系统（NMTs）和大型语言模型（LLMs）中都存在。原始中文文本几乎可以完美地区分LLM和NMT的输出。此外，研究还发现，与通用LLMs相比，特定于翻译的LLMs表现出较低的词汇多样性但更高的因果连词使用率。最后，研究发现中国公司开发的LLMs与外国公司的LLMs之间没有显著差异。

Abstract: This study explores Machine Translationese (MTese) -- the linguistic
peculiarities of machine translation outputs -- focusing on the
under-researched English-to-Chinese language pair in news texts. We construct a
large dataset consisting of 4 sub-corpora and employ a comprehensive five-layer
feature set. Then, a chi-square ranking algorithm is applied for feature
selection in both classification and clustering tasks. Our findings confirm the
presence of MTese in both Neural Machine Translation systems (NMTs) and Large
Language Models (LLMs). Original Chinese texts are nearly perfectly
distinguishable from both LLM and NMT outputs. Notable linguistic patterns in
MT outputs are shorter sentence lengths and increased use of adversative
conjunctions. Comparing LLMs and NMTs, we achieve approximately 70%
classification accuracy, with LLMs exhibiting greater lexical diversity and
NMTs using more brackets. Additionally, translation-specific LLMs show lower
lexical diversity but higher usage of causal conjunctions compared to generic
LLMs. Lastly, we find no significant differences between LLMs developed by
Chinese firms and their foreign counterparts.

</details>


### [87] [Lost at the Beginning of Reasoning](https://arxiv.org/abs/2506.22058)
*Baohao Liao,Xinyi Chen,Sara Rajaee,Yuhui Xu,Christian Herold,Anders Søgaard,Maarten de Rijke,Christof Monz*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在长链式思维（CoT）推理中的自我修正能力，并发现第一个推理步骤对最终预测有重大影响。为此，提出了一种高效的采样策略，利用奖励模型识别和保留高质量的第一个推理步骤，实现推理成本减少70%而不牺牲准确性。此外，还引入了一个新的基准测试，专门构建了故意有缺陷的第一个推理步骤，以系统地评估模型的自我纠正能力，为未来研究LLM的稳健推理提供了基础。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在复杂推理能力方面取得了显著进展，特别是在通过扩展的链式思维（CoT）推理中引入了回溯、自我反思和自我修正等机制，但LLMs在长CoT推理中的自我修正能力仍鲜有研究。最近的研究表明，这些模型常常进行不必要的冗余推理。因此，需要探索如何提高LLMs在长CoT推理中的自我修正能力。

Method: 本文提出了一种高效的采样策略，利用奖励模型来识别和保留高质量的第一个推理步骤，同时丢弃次优的步骤。此外，还引入了一个新的基准测试，专门构建了故意有缺陷的第一个推理步骤，以系统地评估模型的自我纠正能力。

Result: 本文实证表明，第一个推理步骤对最终预测产生了不成比例的巨大影响——该阶段引入的错误会显著降低后续推理质量。这一现象在两个最先进的开源推理模型家族DeepSeek-R1和Qwen3中得到了一致观察。通过提出的高效采样策略，实现了推理成本减少70%而不牺牲准确性。此外，引入的新基准测试为未来研究LLM的稳健推理提供了基础。

Conclusion: 本文提出了一个高效的采样策略，利用奖励模型识别并保留高质量的第一个推理步骤，同时丢弃次优的步骤，实现了推理成本减少70%而不牺牲准确性。此外，本文引入了一个新的基准测试，专门构建了故意有缺陷的第一个推理步骤，以系统地评估模型的自我纠正能力，为未来研究LLM的稳健推理提供了基础。

Abstract: Recent advancements in large language models (LLMs) have significantly
advanced complex reasoning capabilities, particularly through extended
chain-of-thought (CoT) reasoning that incorporates mechanisms such as
backtracking, self-reflection and self-correction. Despite these developments,
the self-correction abilities of LLMs during long CoT reasoning remain
underexplored. And recent findings on overthinking suggest that such models
often engage in unnecessarily redundant reasoning. In this work, we empirically
show that the first reasoning step exerts a disproportionately large influence
on the final prediction - errors introduced at this stage can substantially
degrade subsequent reasoning quality. This phenomenon is consistently observed
across two state-of-the-art open-source reasoning model families: DeepSeek-R1
and Qwen3. To address this, we propose an efficient sampling strategy that
leverages a reward model to identify and retain high-quality first reasoning
steps while discarding suboptimal ones, achieving up to a 70% reduction in
inference cost without sacrificing accuracy. Finally, we introduce a new
benchmark specifically constructed with deliberately flawed first reasoning
steps to systematically evaluate model self-correction capabilities, offering a
foundation for future research on robust reasoning in LLMs.

</details>


### [88] [MDC-R: The Minecraft Dialogue Corpus with Reference](https://arxiv.org/abs/2506.22062)
*Chris Madge,Maris Camilleri,Paloma Carretero Garcia,Mladen Karan,Juexi Shao,Prashant Jayannavar,Julian Hough,Benjamin Roth,Massimo Poesio*

Main category: cs.CL

TL;DR: MDC-R is an annotated version of the Minecraft Dialogue Corpus that adds expert annotations for anaphoric and deictic reference, making it a valuable resource for linguistic research and tasks like referring expression comprehension.


<details>
  <summary>Details</summary>
Motivation: The original MDC contains task-orientated, multi-turn, situated dialogue in a dynamic environment, which presents interesting linguistic phenomena. We believe that adding annotations for reference will make it even more valuable as a language resource.

Method: We introduced MDC-R, which is an annotated version of the Minecraft Dialogue Corpus with expert annotations of anaphoric and deictic reference. We also conducted a quantitative and qualitative analysis of the data and performed a short experiment to demonstrate its usefulness.

Result: We successfully created MDC-R, which includes expert annotations of anaphoric and deictic reference. We also demonstrated its usefulness through a short experiment on referring expression comprehension.

Conclusion: MDC-R can serve as a valuable resource for referring expression comprehension and other linguistic tasks.

Abstract: We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a
new language resource that supplements the original Minecraft Dialogue Corpus
(MDC) with expert annotations of anaphoric and deictic reference. MDC's
task-orientated, multi-turn, situated dialogue in a dynamic environment has
motivated multiple annotation efforts, owing to the interesting linguistic
phenomena that this setting gives rise to. We believe it can serve as a
valuable resource when annotated with reference, too. Here, we discuss our
method of annotation and the resulting corpus, and provide both a quantitative
and a qualitative analysis of the data. Furthermore, we carry out a short
experiment demonstrating the usefulness of our corpus for referring expression
comprehension.

</details>


### [89] [Involvement drives complexity of language in online debates](https://arxiv.org/abs/2506.22098)
*Eleonora Amadori,Daniele Cirulli,Edoardo Di Martino,Jacopo Nudo,Maria Sahakyan,Emanuele Sangiorgio,Arnaldo Santoro,Simon Zollo,Alessandro Galeazzi,Niccolò Di Marco*

Main category: cs.CL

TL;DR: 本研究分析了Twitter上影响力用户在三个重要议题上的语言复杂性，发现不同账户类型、政治倾向、内容可靠性和情感会影响语言使用。


<details>
  <summary>Details</summary>
Motivation: 分析社交媒体上用户生成内容的语言特征对于理解其更广泛的社会影响至关重要。

Method: 我们结合了多种文本复杂性的测量方法，评估了语言使用在四个关键维度上的变化：账户类型、政治倾向、内容可靠性和情感。

Result: 我们的分析揭示了所有四个轴上的显著差异，包括个人与组织之间、有偏见与中立政治观点之间以及高可靠性与低可靠性之间的语言复杂性变化。此外，产生更多负面和冒犯性内容的资料倾向于使用更复杂的语言，而分享相似政治立场和可靠性水平的用户则趋向于使用共同的术语。

Conclusion: 我们的研究揭示了数字平台上语言使用的显著差异，包括个人与组织之间、具有偏见与中立政治观点的资料之间以及高可靠性与低可靠性资料之间的语言复杂性变化。此外，产生更多负面和冒犯性内容的资料倾向于使用更复杂的语言，而分享相似政治立场和可靠性水平的用户则趋向于使用共同的术语。这些发现为理解在线空间中语言如何反映意识形态和社会结构提供了新的见解。

Abstract: Language is a fundamental aspect of human societies, continuously evolving in
response to various stimuli, including societal changes and intercultural
interactions. Technological advancements have profoundly transformed
communication, with social media emerging as a pivotal force that merges
entertainment-driven content with complex social dynamics. As these platforms
reshape public discourse, analyzing the linguistic features of user-generated
content is essential to understanding their broader societal impact. In this
paper, we examine the linguistic complexity of content produced by influential
users on Twitter across three globally significant and contested topics:
COVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of
textual complexity, we assess how language use varies along four key
dimensions: account type, political leaning, content reliability, and
sentiment. Our analysis reveals significant differences across all four axes,
including variations in language complexity between individuals and
organizations, between profiles with sided versus moderate political views, and
between those associated with higher versus lower reliability scores.
Additionally, profiles producing more negative and offensive content tend to
use more complex language, with users sharing similar political stances and
reliability levels converging toward a common jargon. Our findings offer new
insights into the sociolinguistic dynamics of digital platforms and contribute
to a deeper understanding of how language reflects ideological and social
structures in online spaces.

</details>


### [90] [Identifying a Circuit for Verb Conjugation in GPT-2](https://arxiv.org/abs/2506.22105)
*David Demitri Africa*

Main category: cs.CL

TL;DR: 该研究通过一系列技术手段，隔离出GPT-2 Small中负责主谓一致的子网络，并发现其只需要一小部分组件即可完成基础任务，但在复杂情况下需要更多组件。


<details>
  <summary>Details</summary>
Motivation: 为了隔离和解释GPT-2 Small中负责主谓一致的子网络（或“电路”），并理解模型如何正确预测适当的动词形式。

Method: 通过性能验证、自动电路发现（通过直接路径修补）和直接logit归因等技术，隔离出一个对模型正确动词变位有显著贡献的候选电路。

Result: 研究发现，只有网络中的一小部分组件-标记对足以在基础任务上实现接近模型的性能，但在更复杂的设置中需要更多的组件。

Conclusion: 研究结果表明，只有网络中的一小部分组件-标记对对于在基础任务上达到接近模型性能是必要的，但在更复杂的设置中需要更多的组件。

Abstract: I implement a procedure to isolate and interpret the sub-network (or
"circuit") responsible for subject-verb agreement in GPT-2 Small. In this
study, the model is given prompts where the subject is either singular (e.g.
"Alice") or plural (e.g. "Alice and Bob"), and the task is to correctly predict
the appropriate verb form ("walks" for singular subjects, "walk" for plural
subjects). Using a series of techniques-including performance verification
automatic circuit discovery via direct path patching, and direct logit
attribution- I isolate a candidate circuit that contributes significantly to
the model's correct verb conjugation. The results suggest that only a small
fraction of the network's component-token pairs is needed to achieve near-model
performance on the base task but substantially more for more complex settings.

</details>


### [91] [DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level](https://arxiv.org/abs/2506.22141)
*Iliass Ayaou,Denis Cavallucci,Hicham Chibane*

Main category: cs.CL

TL;DR: 本文提出了DAPFAM，一个新开放访问的领域感知专利检索数据集，旨在解决现有数据集在领域标签、多司法管辖区覆盖、平衡查询领域表示和可管理规模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有公开可用的专利检索数据集常常忽视了领域内和领域外标签、多司法管辖区覆盖、平衡查询领域表示以及适合中等计算资源的子文档级实验的可管理规模的需求。

Method: DAPFAM数据集是在简单家族级别构建的，包含1,247个领域平衡的全文查询家族和45,336个全文目标家族。数据集通过基于国际专利分类（IPC）代码的新型标签方案，提供了明确的相关性判断（正向/反向引用作为正链接，随机负样本）以及显式的领域内或领域外关系。

Result: DAPFAM数据集包含49,869个评估对，具有多司法管辖区特性，几乎不需要预处理即可进行检索评估，并且大小适中，适合资源有限的实体进行子文档级别的检索实验。此外，通过使用词汇和神经检索方法的基线实验，突显了跨领域专利检索的重大挑战。

Conclusion: 该数据集的创建解决了现有专利检索数据集在领域标签、多司法管辖区覆盖、平衡查询领域表示和可管理规模方面的不足。通过提供明确的相关性判断和领域标签，该数据集支持子文档级别的检索实验，并且具有较低的计算成本。

Abstract: In the landscape of publicly available patent retrieval datasets, the need
for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,
balanced query domain representation and manageable sizes that support sub
document level experiments on moderate computational resources is often
overlooked. To address these gaps, we propose DAPFAM, a new open access
domain-aware patent retrieval dataset constructed at the simple-family level.
The dataset contains 1,247 domain balanced full text query families and 45,336
full text target families. The dataset is enriched by clear relevance judgments
(forward/backward citations as positive links, random negatives), as well as
explicit in-domain or out-of-domain relationships via a novel proposed
labelling scheme based on via International Patent Classification (IPC) codes,
resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,
requires little to no preprocessing for retrieval evaluation, and remains of a
size manageable for entities with limited ressources allowing for sub document
level retrieval experiments without excessive computational costs. We describe
our three-step data-curation pipeline, present comprehensive dataset
statistics, and provide baseline experiments using lexical and neural retrieval
methods. Our baseline experiments highlight significant challenges in
crossdomain patent retrieval. The dataset will be publicly available (for now
the access link is this repository:
https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).

</details>


### [92] [SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition](https://arxiv.org/abs/2506.22143)
*Muhammad Umar Farooq,Oscar Saz*

Main category: cs.CL

TL;DR: 该研究通过生成人工代码切换语音数据和使用经验回放方法，显著提高了方言阿拉伯语和阿拉伯-英语代码切换语音的性能，优于大型多语言模型。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀缺，需要一种有效的方法来生成代码切换语音数据，并提高模型在方言和代码切换语音上的表现。

Method: 引入了改进的音频拼接方法生成人工代码切换语音数据，并采用经验回放方法提高泛化能力并减轻灾难性遗忘。此外，还整合了一个域外3-gram语言模型以进一步降低错误率。

Result: 使用SAGE数据微调SSL模型在阿拉伯语和英语代码切换基准上实现了7.8%的绝对WER改进。结合域外3-gram语言模型后，总体平均WER从31.7%降至26.6%。少样本微调进一步将WER提高了4.9%。阿拉伯-英语代码切换基准的WER为31.1%，超过了USM和Whisper-large-v2等大型多语言模型。

Conclusion: 该研究通过引入改进的音频拼接方法和经验回放方法，显著提高了方言阿拉伯语和阿拉伯-英语代码切换语音的性能，优于大型多语言模型。

Abstract: This paper investigates the performance of various speech SSL models on
dialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address
data scarcity, a modified audio-splicing approach is introduced to generate
artificial CS speech data. Fine-tuning an already fine-tuned SSL model with the
proposed Spliced-Audio Generated (SAGE) data results in an absolute improvement
on Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.
Additionally, an Experience Replay (ER) inspired approach is proposed to
enhance generalisation across DA and CS speech while mitigating catastrophic
forgetting. Integrating an out-of-domain 3-gram language model reduces the
overall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching
benchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS
benchmarks surpasses large-scale multilingual models, including USM and
Whisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and
8.4%, respectively.

</details>


### [93] [Training Language Model to Critique for Better Refinement](https://arxiv.org/abs/2506.22157)
*Tianshu Yu,Chao Xiang,Mingchuan Yang,Pei Ke,Bosi Wen,Cunxiang Wang,Jiale Cheng,Li Zhang,Xinyu Mu,Chuxiong Sun,Minlie Huang*

Main category: cs.CL

TL;DR: 本文介绍了RCO，一种基于精炼信号训练批评模型的新框架，通过反馈循环提升LLM的批评-精炼能力，并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究有限，未探索哪些类型的批评最有效或如何生成此类批评。因此，需要一种新的框架来解决这一问题。

Method: RCO是一种基于精炼信号训练批评模型的新框架，通过反馈循环使批评模型生成的批评指导执行模型精炼其响应，并使用批评效用（CU）作为奖励信号。

Result: RCO在五个任务中表现出色，包括对话生成、摘要、问答、数学推理和代码生成，显著提升了批评质量和精炼结果。

Conclusion: RCO在多个任务中显著优于传统方法和开源模型，展示了其在提升LLM批评-精炼循环中的有效性。

Abstract: Large language models (LLMs) have demonstrated remarkable evaluation and
critique capabilities, providing insightful feedback and identifying flaws in
various tasks. However, limited research has explored which types of critiques
are most effective for improving model responses or how to generate such
critiques. To address this gap, we introduce \textbf{R}efinement-oriented
\textbf{C}ritique \textbf{O}ptimization (RCO), a novel framework designed to
train critic models using refinement signals. RCO uses a feedback loop where
critiques, generated by the critic model, guide the actor model in refining its
responses. The critique utility (CU) quantifies the effectiveness of these
refinements, serving as the reward signal for training the critic model. By
focusing on critiques that lead to better refinements, RCO eliminates the need
for direct critique preference assessment, ensuring that critiques driving
meaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,
dialog generation, summarization, question answering, mathematical reasoning,
and code generation, and show that it significantly outperforms traditional
methods and open-source models in terms of critique quality and refinement
outcomes. Our contributions include the introduction of RCO, a novel
supervision scheme based on refined response preferences, and comprehensive
experimental results that highlight the method's effectiveness in enhancing LLM
critique-refinement loops.

</details>


### [94] [Leveraging In-Context Learning for Political Bias Testing of LLMs](https://arxiv.org/abs/2506.22232)
*Patrick Haller,Jannis Vamvas,Rico Sennrich,Lena A. Jäger*

Main category: cs.CL

TL;DR: 本文提出了一种新的探测任务，即问卷建模（QM），使用人类调查数据作为上下文示例，以提高基于问题的偏见评估的稳定性，并发现指令微调可以改变偏见方向，同时更大的模型在QM中表现出更小的偏见分数。


<details>
  <summary>Details</summary>
Motivation: 现有的探测方法稳定性有限，使得模型之间的比较不可靠。因此，需要更多的上下文来评估LLMs的潜在偏见。

Method: 提出了一种新的探测任务，称为问卷建模（QM），使用人类调查数据作为上下文示例。

Result: QM提高了基于问题的偏见评估的稳定性，并展示了它可用于比较经过指令微调的模型与其基础版本。实验表明，指令微调确实可以改变偏见的方向，并且更大的模型能够更有效地利用上下文示例，通常在QM中表现出较小的偏见分数。

Conclusion: 实验表明，指令微调确实可以改变偏见的方向，并且更大的模型能够更有效地利用上下文示例，通常在QM中表现出较小的偏见分数。

Abstract: A growing body of work has been querying LLMs with political questions to
evaluate their potential biases. However, this probing method has limited
stability, making comparisons between models unreliable. In this paper, we
argue that LLMs need more context. We propose a new probing task, Questionnaire
Modeling (QM), that uses human survey data as in-context examples. We show that
QM improves the stability of question-based bias evaluation, and demonstrate
that it may be used to compare instruction-tuned models to their base versions.
Experiments with LLMs of various sizes indicate that instruction tuning can
indeed change the direction of bias. Furthermore, we observe a trend that
larger models are able to leverage in-context examples more effectively, and
generally exhibit smaller bias scores in QM. Data and code are publicly
available.

</details>


### [95] [Detection of Personal Data in Structured Datasets Using a Large Language Model](https://arxiv.org/abs/2506.22305)
*Albert Agisha Ntwali,Luca Rück,Martin Heckmann*

Main category: cs.CL

TL;DR: 本文提出了一种基于GPT-4o的新方法来检测结构化数据集中的个人数据，并通过与Microsoft Presidio和CASSED等现有方法的比较验证了其有效性。研究发现，基于GPT-4o的方法在多个数据集上表现出色，尤其是在需要上下文信息的情况下。结论是，该领域需要更多真实世界数据集以促进进一步发展。


<details>
  <summary>Details</summary>
Motivation: 检测结构化数据集中的个人数据对于保护隐私至关重要。现有的方法可能无法充分利用数据集的上下文信息，因此需要一种更有效的方法。

Method: 我们提出了一种新的方法来检测结构化数据集中的个人数据，利用GPT-4o，一种最先进的大型语言模型。我们的方法的关键创新是结合上下文信息：除了特征名称和值外，我们还利用数据集中其他特征名称以及数据集描述的信息。

Result: 我们的研究结果表明，检测性能在很大程度上取决于用于评估的数据集。CASSED在DeSSI数据集（它被训练的）上表现优异。在医疗数据集MIMIC-Demo-Ext上，所有模型的性能相当，但我们的基于GPT-4o的方法明显优于其他方法。值得注意的是，Kaggle和OpenML数据集中的个人数据检测似乎受益于上下文信息。

Conclusion: 我们得出结论，该领域进一步进展将极大地受益于更多包含个人信息的真实世界数据集的可用性。

Abstract: We propose a novel approach for detecting personal data in structured
datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key
innovation of our method is the incorporation of contextual information: in
addition to a feature's name and values, we utilize information from other
feature names within the dataset as well as the dataset description. We compare
our approach to alternative methods, including Microsoft Presidio and CASSED,
evaluating them on multiple datasets: DeSSI, a large synthetic dataset,
datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a
real-world dataset containing patient information from critical care units.
  Our findings reveal that detection performance varies significantly depending
on the dataset used for evaluation. CASSED excels on DeSSI, the dataset on
which it was trained. Performance on the medical dataset MIMIC-Demo-Ext is
comparable across all models, with our GPT-4o-based approach clearly
outperforming the others. Notably, personal data detection in the Kaggle and
OpenML datasets appears to benefit from contextual information. This is
evidenced by the poor performance of CASSED and Presidio (both of which do not
utilize the context of the dataset) compared to the strong results of our
GPT-4o-based approach.
  We conclude that further progress in this field would greatly benefit from
the availability of more real-world datasets containing personal information.

</details>


### [96] [Evaluating Scoring Bias in LLM-as-a-Judge](https://arxiv.org/abs/2506.22316)
*Qingquan Li,Shaoyu Dou,Kailai Shao,Chao Chen,Haixiang Hu*

Main category: cs.CL

TL;DR: 本文探讨了LLM-as-a-Judge中的评分偏见问题，并提出了一种全面评估评分偏见的框架。通过数据合成增强现有基准并设计多方面评估指标，实验结果显示现有裁判模型的评分稳定性受到影响。进一步的实验和讨论提供了关于评分提示模板设计和减轻评分偏见的见解。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要集中在基于比较的评估上，而对基于评分的评估中的偏见进行系统性研究仍然有限。因此，我们定义了LLM-as-a-Judge中的评分偏见，并提供了一个全面评估评分偏见的框架。

Method: 我们通过数据合成增强了现有的LLM-as-a-Judge基准，构建了我们的评估数据集，并设计了多方面的评估指标。

Result: 我们的实验结果表明，现有裁判模型的评分稳定性受到评分偏见的影响。

Conclusion: 我们的实验结果表明，现有裁判模型的评分稳定性受到评分偏见的影响。进一步的探索性实验和讨论为评分提示模板的设计以及在评分标准、评分ID和参考答案选择等方面减轻评分偏见提供了有价值的见解。

Abstract: The remarkable performance of Large Language Models (LLMs) gives rise
to``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.
Moreover, it has been widely adopted across fields such as Natural Language
Processing (NLP), preference learning, and various specific domains. However,
there are various biases within LLM-as-a-Judge, which adversely affect the
fairness and reliability of judgments. Current research on evaluating or
mitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based
evaluations, while systematic investigations into bias in scoring-based
evaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge
as the scores differ when scoring judge models are bias-related perturbed, and
provide a well-designed framework to comprehensively evaluate scoring bias. We
augment existing LLM-as-a-Judge benchmarks through data synthesis to construct
our evaluation dataset and design multi-faceted evaluation metrics. Our
experimental results demonstrate that the scoring stability of existing judge
models is disrupted by scoring biases. Further exploratory experiments and
discussions provide valuable insights into the design of scoring prompt
templates and the mitigation of scoring biases on aspects such as score
rubrics, score IDs, and reference answer selection.

</details>


### [97] [Why Are Parsing Actions for Understanding Message Hierarchies Not Random?](https://arxiv.org/abs/2506.22366)
*Daichi Kato,Ryo Ueda,Yusuke Miyao*

Main category: cs.CL

TL;DR: 研究通过修改实验设置，发现当输入更复杂且引入意外性相关项时，采用随机解析策略的智能体的通信准确性下降。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨为什么人类解析策略似乎不遵循随机模式，而之前的研究表明，采用随机解析策略的智能体可以实现高通信准确性。

Method: 研究通过修改实验设置来探讨这一问题，包括使用更复杂的层次结构输入以及在目标函数中加入与意外性相关的项。

Result: 研究发现，当输入变得更加复杂且具有层次结构，并引入与意外性相关的项时，采用随机解析策略的智能体的通信准确性下降。

Conclusion: 研究发现，当输入变得更加复杂且具有层次结构，并引入与意外性相关的项时，采用随机解析策略的智能体的通信准确性下降。这表明人类解析策略可能并非随机，而是受到某些因素的影响，如语言的层次结构和意外性。

Abstract: If humans understood language by randomly selecting parsing actions, it might
have been necessary to construct a robust symbolic system capable of being
interpreted under any hierarchical structure. However, human parsing strategies
do not seem to follow such a random pattern. Why is that the case? In fact, a
previous study on emergent communication using models with hierarchical biases
have reported that agents adopting random parsing
strategies$\unicode{x2013}$ones that deviate significantly from human language
comprehension$\unicode{x2013}$can achieve high communication accuracy. In this
study, we investigate this issue by making two simple and natural modifications
to the experimental setup: (I) we use more complex inputs that have
hierarchical structures, such that random parsing makes semantic interpretation
more difficult, and (II) we incorporate a surprisal-related term, which is
known to influence the order of words and characters in natural language, into
the objective function. With these changes, we evaluate whether agents
employing random parsing strategies still maintain high communication accuracy.

</details>


### [98] [QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization](https://arxiv.org/abs/2506.22396)
*Danush Khanna,Aditya Kumar Guru,Srivarshinee Sridhar,Zidan Ahmed,Rubhav Bahirwani,Meetu Malhotra,Vinija Jain,Aman Chadha,Amitava Das,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: QuickSilver是一种无需修改模型权重或结构的推理优化框架，能够显著减少计算量并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型部署中，推理占用了大部分延迟和能耗，而现有的方法通常需要重新训练或改变架构，这限制了其应用。因此，需要一种无需修改模型权重或结构的推理优化方法。

Method: QuickSilver是一个模块化的、基于标记级别的框架，集成了动态标记停止、KV缓存跳过和上下文标记融合三种协同机制。

Result: QuickSilver在GPT-2和Llama-2上实现了高达39.6%的FLOP减少，且困惑度下降不超过0.2。

Conclusion: QuickSilver能够在不改变模型权重或结构的情况下，在推理时实现语义自适应，从而显著减少FLOP并保持较低的困惑度。

Abstract: Inference accounts for the majority of latency and energy consumption in
large language model (LLM) deployments, often exceeding 90% of total cost.
While training-time efficiency has seen extensive progress, runtime
optimization remains a key bottleneck, particularly under autoregressive
decoding. Existing approaches -- such as pruning, quantization, early exits,
and speculative decoding -- often require retraining, architectural changes, or
disrupt decoding compatibility. We introduce QuickSilver, a modular,
token-level framework that enables semantic adaptivity at inference time
without altering model weights or structure. QuickSilver integrates four
synergistic mechanisms:
  (i) Dynamic Token Halting, which halts computation for tokens with converged
representations; (ii) KV Cache Skipping, which selectively suppresses memory
writes to reduce attention overhead; and (iii) Contextual Token Fusion, which
collapses redundant tokens into shared paths to shrink sequence length.
  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on
frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and
Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP
reduction with negligible perplexity degradation (<=0.2).

</details>


### [99] [Refining Czech GEC: Insights from a Multi-Experiment Approach](https://arxiv.org/abs/2506.22402)
*Petr Pechman,Milan Straka,Jana Straková,Jakub Náplava*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer架构的捷克语语法错误纠正系统，通过实时合成生成管道动态添加人工错误，取得了最先进的性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了提高捷克语语法错误纠正（GEC）系统的性能，我们开发了一个新的系统，该系统能够动态生成具有语言无关和捷克特定错误的句子。

Method: 基于神经网络翻译方法，采用Transformer架构，并引入了实时合成生成管道，动态地向句子中添加人工错误。

Result: 我们的系统在捷克语GEC任务中达到了最先进的水平，并且在计算效率方面也表现出色。

Conclusion: 我们的最佳模型在性能和计算效率方面都优于其他模型。

Abstract: We present a grammar error correction (GEC) system that achieves state of the
art for the Czech language. Our system is based on a neural network translation
approach with the Transformer architecture, and its key feature is its
real-time synthetic generation pipeline, which dynamically augments sentences
with artificial errors by introducing both language-agnostic and Czech-specific
errors. We conduct a comprehensive series of experiments, investigating the
Czech GEC corpora as bases for synthetic error introduction, several error
generation strategies, domain balancing, tokenization granularity, model size,
and data scaling during fine-tuning. Additionally, we evaluate the performance
of large language models (LLMs) on Czech GEC in both end-user and expert
fine-tuning scenarios. Our best-performing model is superior both in
performance and computational efficiency. The source code and the trained model
links are available on https://github.com/ufal/tsd2025-gec.

</details>


### [100] [HyperCLOVA X THINK Technical Report](https://arxiv.org/abs/2506.22403)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CL

TL;DR: HyperCLOVA X THINK 是一个专注于推理的大型语言模型，具有出色的性能和较低的训练计算成本。


<details>
  <summary>Details</summary>
Motivation: 开发一个专注于推理的大型语言模型，以在韩国基准测试中表现出色，并保持双语一致性和翻译质量。

Method: HyperCLOVA X THINK 是通过三阶段课程预训练，并通过监督微调和可验证奖励的强化学习进行后训练。此外，还提出了一种剪枝和蒸馏技术，以实现开源和商业友好的基础模型。

Result: HyperCLOVA X THINK 在韩国相关的基准测试中表现优异，同时在视觉增强变体上与 GPT-4.1 相当或更优，且训练计算成本更低。

Conclusion: HyperCLOVA X THINK 是一个强大的基础模型，为韩国人工智能创新和全球研究社区提供了有价值资源。

Abstract: We introduce HyperCLOVA X THINK, the first reasoning-focused large language
model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion
high-quality Korean, and English tokens, augmented with targeted synthetic
Korean data. It was implemented as a compute-memory-balanced Peri-LN
Transformer scaled with $\mu$P, pre-trained through a three-stage curriculum
that expands the context window to $128$K tokens, and post-trained via
supervised fine-tuning with Reinforcement Learning from Verifiable Rewards
supports both detailed rationale and concise-answer modes. It delivers
competitive performance against similarly sized models on Korea-focused
benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while
preserving robust bilingual consistency and translation quality. In addition, a
vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM
benchmark, all of which are achieved with substantially lower training compute
than existing models of similar sizes. We also present a pruning and
distillation technique that will soon be applied to HyperCLOVA X THINK for an
open-source and business-friendly foundation model. Altogether, these
capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI
innovation and a valuable resource for the global research community.

</details>


### [101] [Sequential Diagnosis with Language Models](https://arxiv.org/abs/2506.22405)
*Harsha Nori,Mayank Daswani,Christopher Kelly,Scott Lundberg,Marco Tulio Ribeiro,Marc Wilson,Xiaoxuan Liu,Viknesh Sounderajah,Jonathan Carlson,Matthew P Lungren,Bay Gross,Peter Hames,Mustafa Suleyman,Dominic King,Eric Horvitz*

Main category: cs.CL

TL;DR: This paper introduces a new benchmark for evaluating AI in medical diagnosis and presents a model-agnostic orchestrator that improves diagnostic accuracy and reduces costs compared to physicians and existing AI models.


<details>
  <summary>Details</summary>
Motivation: Most evaluations of language models rely on static vignettes and multiple-choice questions that fail to reflect the complexity of real-world medical practice. The study aims to emulate the iterative diagnostic process used by physicians.

Method: The study introduces the Sequential Diagnosis Benchmark and the MAI Diagnostic Orchestrator (MAI-DxO), which simulates a panel of physicians and strategically selects high-value tests. It evaluates performance based on diagnostic accuracy, cost of physician visits, and tests performed.

Result: MAI-DxO achieves 80% diagnostic accuracy when paired with OpenAI's o3 model, four times higher than the 20% average of generalist physicians. It also reduces diagnostic costs by 20% compared to physicians and 70% compared to off-the-shelf o3. These gains generalize across multiple AI models.

Conclusion: AI systems, when guided to think iteratively and act judiciously, can advance diagnostic precision and cost-effectiveness in clinical care.

Abstract: Artificial intelligence holds great promise for expanding access to expert
medical knowledge and reasoning. However, most evaluations of language models
rely on static vignettes and multiple-choice questions that fail to reflect the
complexity and nuance of evidence-based medicine in real-world settings. In
clinical practice, physicians iteratively formulate and revise diagnostic
hypotheses, adapting each subsequent question and test to what they've just
learned, and weigh the evolving evidence before committing to a final
diagnosis. To emulate this iterative process, we introduce the Sequential
Diagnosis Benchmark, which transforms 304 diagnostically challenging New
England Journal of Medicine clinicopathological conference (NEJM-CPC) cases
into stepwise diagnostic encounters. A physician or AI begins with a short case
abstract and must iteratively request additional details from a gatekeeper
model that reveals findings only when explicitly queried. Performance is
assessed not just by diagnostic accuracy but also by the cost of physician
visits and tests performed. We also present the MAI Diagnostic Orchestrator
(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,
proposes likely differential diagnoses and strategically selects high-value,
cost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%
diagnostic accuracy--four times higher than the 20% average of generalist
physicians. MAI-DxO also reduces diagnostic costs by 20% compared to
physicians, and 70% compared to off-the-shelf o3. When configured for maximum
accuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO
generalize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and
Llama families. We highlight how AI systems, when guided to think iteratively
and act judiciously, can advance diagnostic precision and cost-effectiveness in
clinical care.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [102] [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
*Yifan Shen,Yuanzhe Liu,Jingyuan Zhu,Xu Cao,Xiaofeng Zhang,Yixiao He,Wenming Ye,James Matthew Rehg,Ismini Lourentzou*

Main category: cs.CV

TL;DR: This paper introduces SpatialReasoner-R1, a vision-language reasoning model designed to address limitations in fine-grained spatial reasoning. The authors propose a method called M3CTS to generate diverse, logically consistent reasoning trajectories and a technique called fDPO to improve descriptive grounding and logical reasoning. Experimental results show that fDPO improves performance on spatial tasks, and SpatialReasoner-R1 achieves state-of-the-art results on a benchmark for spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required.

Method: We introduce SpatialReasoner-R1, a vision-language reasoning model. We design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. We also propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence.

Result: Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.

Conclusion: SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.

Abstract: Current Vision-Language Models (VLMs) struggle with fine-grained spatial
reasoning, particularly when multi-step logic and precise spatial alignment are
required. In this work, we introduce SpatialReasoner-R1, a vision-language
reasoning model designed to address these limitations. To construct
high-quality supervision for spatial reasoning, we design a Multi-Model Monte
Carlo Tree Search (M3CTS) method that generates diverse, logically consistent
Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose
fine-grained Direct Preference Optimization (fDPO), which introduces
segment-specific preference granularity for descriptive grounding and logical
reasoning, guided by a spatial reward mechanism that evaluates candidate
responses based on visual consistency, spatial grounding, and logical
coherence. Experimental results demonstrate that fDPO achieves an average
improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%
gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a
new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in
average accuracy, while maintaining competitive performance on general
vision-language tasks.

</details>


### [103] [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
*Mengyi Shan,Brian Curless,Ira Kemelmacher-Shlizerman,Steve Seitz*

Main category: cs.CV

TL;DR: 本文提出了一种分层多智能体框架，用于生成逃生室谜题图像，该框架通过代理协作提高输出质量。


<details>
  <summary>Details</summary>
Motivation: 我们挑战文本到图像模型生成逃生室谜题图像，这些图像在视觉上吸引人、逻辑上稳固且具有智力刺激性。虽然基础图像模型在空间关系和可及性推理方面存在困难，但我们提出了一个分层多智能体框架来解决这个问题。

Method: 我们提出了一种分层多智能体框架，将任务分解为结构化阶段：功能设计、符号场景图推理、布局合成和局部图像编辑。专门的代理通过迭代反馈进行协作，以确保场景在视觉上连贯且功能上可解。

Result: 实验表明，代理协作在可解性、避免捷径和可及性清晰度方面提高了输出质量，同时保持了视觉质量。

Conclusion: 实验表明，代理协作在可解性、避免捷径和可及性清晰度方面提高了输出质量，同时保持了视觉质量。

Abstract: We challenge text-to-image models with generating escape room puzzle images
that are visually appealing, logically solid, and intellectually stimulating.
While base image models struggle with spatial relationships and affordance
reasoning, we propose a hierarchical multi-agent framework that decomposes this
task into structured stages: functional design, symbolic scene graph reasoning,
layout synthesis, and local image editing. Specialized agents collaborate
through iterative feedback to ensure the scene is visually coherent and
functionally solvable. Experiments show that agent collaboration improves
output quality in terms of solvability, shortcut avoidance, and affordance
clarity, while maintaining visual quality.

</details>


### [104] [COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication](https://arxiv.org/abs/2506.22274)
*Filippo Merlo,Ece Takmaz,Wenkai Chen,Albert Gatt*

Main category: cs.CV

TL;DR: 本研究探讨了视觉-语言模型（VLMs）在生成物体引用时是否依赖场景上下文。通过引入一个新的数据集，研究发现模型会根据对象和场景之间的语义相关性以及噪声水平，适应性地利用场景上下文。


<details>
  <summary>Details</summary>
Motivation: 了解视觉-语言模型（VLMs）在生成物体引用时是否像人类一样依赖场景上下文。

Method: 我们引入了	extit{Common Objects Out-of-Context (COOCO)}数据集，并测试了VLMs在不同程度的场景-对象一致性以及不同扰动下，利用场景上下文引用物体的程度。

Result: 模型会根据对象和场景之间的语义相关性以及噪声水平，适应性地利用场景上下文。特别是在目标-场景一致性较高或对象退化时，模型更依赖于上下文。注意力分析表明，成功的物体分类涉及在中层层面上对目标的更多关注，尤其是在中等噪声下，这表明VLMs在参考生成中动态平衡局部信息和上下文信息。

Conclusion: 我们的研究发现，模型会根据对象和场景之间的语义相关性以及噪声水平，适应性地利用场景上下文。特别是在目标-场景一致性较高或对象退化时，模型更依赖于上下文。注意力分析表明，成功的物体分类涉及在中层层面上对目标的更多关注，尤其是在中等噪声下，这表明VLMs在参考生成中动态平衡局部信息和上下文信息。

Abstract: Natural scenes provide us with rich contexts for object recognition and
reference. In particular, knowing what type of scene one is looking at
generates expectations about which objects will occur, and what their spatial
configuration should be. Do Vision-Language Models (VLMs) learn to rely on
scene contexts in a similar way, when generating references to objects? To
address this question, we introduce the \textit{Common Objects Out-of-Context
(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to
objects under different degrees of scene-object congruency, and different
perturbations. Our findings show that models leverage scene context adaptively,
depending on both the semantic relatedness between object and scene and the
level of noise. In particular, models rely more on context under high
target-scene congruence or when objects are degraded. Attention analysis
reveals that successful object categorisation involves increased focus on the
target in mid-level layers, especially under moderate noise, suggesting that
VLMs dynamically balance local and contextual information for reference
generation. We make our dataset, code and models available at
\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.

</details>


### [105] [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
*Yue Zhang,Jilei Sun,Yunhui Guo,Vibhav Gogate*

Main category: cs.CV

TL;DR: 本文介绍了DVidE任务，该任务要求模型根据不断变化的证据更新其推理。我们提出了一个框架，利用反事实推理和ASR增强的视频内容来减少推理偏差，并开发了一个结合ASR输出和大型语言模型的框架，以生成与预期目标一致的连贯更新。此外，我们引入了一个新的基准数据集和基于LLM的评估指标。实验结果表明，所提出的方法在增强VLMM的动态推理能力方面有显著改进。


<details>
  <summary>Details</summary>
Motivation: 视频大多模态模型（VLMMs）在理解视频内容方面取得了显著进展，但它们常常难以进行抽象和适应性推理——即在新信息出现时修改其解释的能力。现实中的结论很少是固定的；额外的上下文可以加强或削弱初始推断。为了解决这个问题，我们引入了Defeasible Video Entailment（DVidE），一个新的任务，挑战模型像怀疑者一样思考，根据不断变化的证据更新其推理。

Method: 我们提出了Chain of Counterfactual Thought框架，利用反事实推理、ASR增强的视频内容和理性精炼来减少推理偏差。对于生成任务，我们开发了一个结合ASR输出和大型语言模型（LLM）的框架，以产生与预期加强者或削弱者目标一致的连贯、上下文相关的更新。

Result: 我们引入了一个新的基准数据集，具有加强者/削弱者注释和一个基于LLM的评估指标，专门设计用于评估生成性能。实验结果表明，所提出的方法在增强VLMM的动态推理能力方面有显著改进。

Conclusion: 实验结果表明，所提出的方法在增强VLMM的动态推理能力方面有显著改进。

Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in
understanding video content, but they often struggle with abstract and adaptive
reasoning-the ability to revise their interpretations when new information
emerges. In reality, conclusions are rarely set in stone; additional context
can strengthen or weaken an initial inference. To address this, we introduce
Defeasible Video Entailment (DVidE), a new task that challenges models to think
like doubters, constantly updating their reasoning based on evolving evidence.
In DVidE, given a video premise and a textual hypothesis, models must determine
whether a new update strengthens or weakens the hypothesis (classification
version) or generate a coherent update that modifies the entailment
relationship (generation version). For solving the classification task, we
propose the Chain of Counterfactual Thought framework, utilizing counterfactual
reasoning, ASR-enhanced video content, and rationale refinement to reduce
inference bias. For the generation task, we develop a framework that combines
ASR output with a Large Language Model (LLM) to produce coherent, contextually
relevant updates aligned with the intended strengthener or weakener goals.
Additionally, we introduce a novel benchmark dataset, with
strengthener/weakener annotations and an LLM-based evaluation metric
specifically designed for assessing generative performance. Experimental
results demonstrate significant improvements, highlighting our proposed method
in enhancing dynamic reasoning capabilities of VLMMs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [106] [HyReC: Exploring Hybrid-based Retriever for Chinese](https://arxiv.org/abs/2506.21913)
*Zunran Wang,Zheng Shenpeng,Wang Shenglan,Minghui Zhao,Zhonghua Li*

Main category: cs.IR

TL;DR: This paper introduces HyReC, an end-to-end optimization method for hybrid-based retrieval in Chinese that enhances performance through semantic integration and alignment refinement.


<details>
  <summary>Details</summary>
Motivation: The application of hybrid paradigms in Chinese retrieval contexts has remained underexplored despite their promising results in the industry.

Method: HyReC integrates the semantic union of terms into the representation model, uses a Global-Local-Aware Encoder (GLAE) to promote consistent semantic sharing between lexicon-based and dense retrieval, and incorporates a Normalization Module (NM) to refine alignment.

Result: HyReC was evaluated on the C-MTEB retrieval benchmark and demonstrated its effectiveness.

Conclusion: HyReC is effective for hybrid-based retrieval in Chinese.

Abstract: Hybrid-based retrieval methods, which unify dense-vector and lexicon-based
retrieval, have garnered considerable attention in the industry due to
performance enhancement. However, despite their promising results, the
application of these hybrid paradigms in Chinese retrieval contexts has
remained largely underexplored. In this paper, we introduce HyReC, an
innovative end-to-end optimization method tailored specifically for
hybrid-based retrieval in Chinese. HyReC enhances performance by integrating
the semantic union of terms into the representation model. Additionally, it
features the Global-Local-Aware Encoder (GLAE) to promote consistent semantic
sharing between lexicon-based and dense retrieval while minimizing the
interference between them. To further refine alignment, we incorporate a
Normalization Module (NM) that fosters mutual benefits between the retrieval
approaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to
demonstrate its effectiveness.

</details>


### [107] [ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation](https://arxiv.org/abs/2506.21931)
*Reza Yousefi Maragheh,Pratheek Vadla,Priyank Gupta,Kai Zhao,Aysenur Inan,Kehui Yao,Jianpeng Xu,Praveen Kanumala,Jason Cho,Sushant Kumar*

Main category: cs.IR

TL;DR: 本文提出了一种名为ARAG的代理检索增强生成框架，用于个性化推荐。通过引入四个专门的基于LLM的代理，ARAG能够更好地理解和捕捉用户的长期和会话行为，从而提高推荐效果。实验结果表明，ARAG在多个指标上都优于现有的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RAG的方法通常依赖于静态检索启发式方法，并且在动态推荐场景中无法捕捉细微的用户偏好。因此，我们需要一种更有效的方法来提高推荐系统的性能。

Method: 我们引入了ARAG，一个用于个性化推荐的代理检索增强生成框架，该框架将多代理协作机制集成到RAG流程中。ARAG利用四个专门的基于LLM的代理：用户理解代理、自然语言推理（NLI）代理、上下文摘要代理和项目排序代理。

Result: 在三个数据集上评估ARAG的结果显示，ARAG显著优于标准RAG和基于最近性的基线，NDCG@5的提升高达42.1%，Hit@5的提升高达35.5%。此外，我们还进行了消融研究，以分析ARAG不同组件的影响。

Conclusion: 我们的研究结果表明，将代理推理整合到检索增强推荐中是有效的，并为基于大语言模型的个性化提供了新的方向。

Abstract: Retrieval-Augmented Generation (RAG) has shown promise in enhancing
recommendation systems by incorporating external context into large language
model prompts. However, existing RAG-based approaches often rely on static
retrieval heuristics and fail to capture nuanced user preferences in dynamic
recommendation scenarios. In this work, we introduce ARAG, an Agentic
Retrieval-Augmented Generation framework for Personalized Recommendation, which
integrates a multi-agent collaboration mechanism into the RAG pipeline. To
better understand the long-term and session behavior of the user, ARAG
leverages four specialized LLM-based agents: a User Understanding Agent that
summarizes user preferences from long-term and session contexts, a Natural
Language Inference (NLI) Agent that evaluates semantic alignment between
candidate items retrieved by RAG and inferred intent, a context summary agent
that summarizes the findings of NLI agent, and an Item Ranker Agent that
generates a ranked list of recommendations based on contextual fit. We evaluate
ARAG accross three datasets. Experimental results demonstrate that ARAG
significantly outperforms standard RAG and recency-based baselines, achieving
up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an
ablation study to analyse the effect by different components of ARAG. Our
findings highlight the effectiveness of integrating agentic reasoning into
retrieval-augmented recommendation and provide new directions for LLM-based
personalization.

</details>


### [108] [Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement](https://arxiv.org/abs/2506.22372)
*Maryam Mousavian,Zahra Abbasiantaeb,Mohammad Aliannejadi,Fabio Crestani*

Main category: cs.IR

TL;DR: 本文提出了一种基于大型语言模型的新性别公平性度量方法（CWEx），并构建了MSMGenderBias数据集，以更详细地评估和减轻信息检索系统中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 社会偏见在自然语言处理（NLP）和信息检索（IR）系统中是一个持续的挑战，因此需要开发稳健的方法来识别和评估这些偏见。

Method: 本文利用大型语言模型（LLMs）检测和衡量段落排序中的性别偏见，并引入了一种新的性别公平性度量方法，称为类加权曝光（CWEx）。

Result: 实验结果表明，所提出的度量方法比以前的度量方法提供了更详细的公平性评估，并且与人类标签有更好的对齐（Grep-BiasIR为58.77%，MSMGenderBias为18.51%），能够有效区分排序中的性别偏见。

Conclusion: 通过集成LLM驱动的偏见检测、改进的公平性度量和对现有数据集的性别偏见注释，这项工作为分析和减轻IR系统中的偏见提供了更稳健的框架。

Abstract: The presence of social biases in Natural Language Processing (NLP) and
Information Retrieval (IR) systems is an ongoing challenge, which underlines
the importance of developing robust approaches to identifying and evaluating
such biases. In this paper, we aim to address this issue by leveraging Large
Language Models (LLMs) to detect and measure gender bias in passage ranking.
Existing gender fairness metrics rely on lexical- and frequency-based measures,
leading to various limitations, e.g., missing subtle gender disparities.
Building on our LLM-based gender bias detection method, we introduce a novel
gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to
address existing limitations. To measure the effectiveness of our proposed
metric and study LLMs' effectiveness in detecting gender bias, we annotate a
subset of the MS MARCO Passage Ranking collection and release our new gender
bias collection, called MSMGenderBias, to foster future research in this area.
Our extensive experimental results on various ranking models show that our
proposed metric offers a more detailed evaluation of fairness compared to
previous metrics, with improved alignment to human labels (58.77% for
Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa
agreement), effectively distinguishing gender bias in ranking. By integrating
LLM-driven bias detection, an improved fairness metric, and gender bias
annotations for an established dataset, this work provides a more robust
framework for analyzing and mitigating bias in IR systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [109] [Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy](https://arxiv.org/abs/2506.22023)
*Bohan Li,Zhihan Li,Haoran Wang,Hanglei Zhang,Yiwei Guo,Hankun Wang,Xie Chen,Kai Yu*

Main category: cs.SD

TL;DR: DCAR is a novel dynamic chunk-wise autoregressive synthesis framework that improves efficiency and intelligibility in speech generation.


<details>
  <summary>Details</summary>
Motivation: Conventional autoregressive speech synthesis models face challenges with long speech sequences, leading to increased latency and degraded synthesis quality. The need for a more efficient and robust approach motivates the development of DCAR.

Method: DCAR introduces a chunk-to-frame attention mechanism through training with multi-token prediction, enabling dynamic chunk prediction in variable speech contexts using a lightweight module trained on-policy.

Result: DCAR achieves up to 72.27% intelligibility improvement and 2.61x inference speedup compared to traditional next-token prediction models.

Conclusion: DCAR demonstrates significant improvements in intelligibility and inference speed, making it a promising foundation for next-generation speech synthesis systems.

Abstract: Recently, autoregressive (AR) language models have emerged as a dominant
approach in speech synthesis, offering expressive generation and scalable
training. However, conventional AR speech synthesis models relying on the
next-token prediction paradigm often encounter significant challenges when
handling long speech sequences. These models often struggle to construct stable
frame-to-frame attention, leading to increased latency and degraded synthesis
quality, thereby limiting their feasibility for real-time applications. To
address these limitations, we introduce a novel dynamic chunk-wise
autoregressive synthesis framework, termed DCAR, designed to enhance both
efficiency and intelligibility robustness in AR speech generation. DCAR
introduces a chunk-to-frame attention mechanism through training with
multi-token prediction, enabling dynamic chunk prediction in variable speech
contexts using a lightweight module trained on-policy. DCAR dynamically adjusts
the token prediction span, significantly reducing the sequence length
dependency while obtaining high synthesis quality. Comprehensive empirical
evaluations demonstrate that DCAR substantially outperforms traditional
next-token prediction models, achieving up to 72.27% intelligibility
improvement and 2.61x inference speedup simultaneously on the test set.
Furthermore, we conduct comprehensive analysis to support it as a versatile
foundation for next-generation speech synthesis systems.

</details>


### [110] [Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations](https://arxiv.org/abs/2506.22237)
*Sebastian Murgul,Moritz Reiser,Michael Heizmann,Christoph Seibert*

Main category: cs.SD

TL;DR: 本文提出了一种基于卷积循环神经网络（CRNN）的方法，用于将人类钢琴演奏的音频记录与对应的MIDI文件同步。该方法在各种容差窗口中比传统的DTW方法更准确，并且结合DTW可以进一步提高性能。


<details>
  <summary>Details</summary>
Motivation: 提出一种神经网络方法，用于将人类钢琴演奏的音频记录与其对应的松散对齐的MIDI文件同步。

Method: 使用卷积循环神经网络（CRNN）架构来解决任务，该架构通过处理未对齐的钢琴滚筒和频谱图作为输入来估计对齐的钢琴滚筒。

Result: 所提出的模型在各种容差窗口中比行业标准动态时间规整（DTW）方法的对齐准确率高出高达20%。此外，将DTW与CRNN结合进一步提高了性能，提供了增强的鲁棒性和一致性。

Conclusion: 这些发现表明神经网络在推进最先进的MIDI到音频对齐方面具有潜力。

Abstract: In this paper, we present a neural network approach for synchronizing audio
recordings of human piano performances with their corresponding loosely aligned
MIDI files. The task is addressed using a Convolutional Recurrent Neural
Network (CRNN) architecture, which effectively captures spectral and temporal
features by processing an unaligned piano roll and a spectrogram as inputs to
estimate the aligned piano roll. To train the network, we create a dataset of
piano pieces with augmented MIDI files that simulate common human timing
errors. The proposed model achieves up to 20% higher alignment accuracy than
the industry-standard Dynamic Time Warping (DTW) method across various
tolerance windows. Furthermore, integrating DTW with the CRNN yields additional
improvements, offering enhanced robustness and consistency. These findings
demonstrate the potential of neural networks in advancing state-of-the-art
MIDI-to-audio alignment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [111] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen,Xin Xu,Zijing Liu,Pengxiang Li,Xinyuan Song,Ajay Kumar Jaiswal,Fan Zhang,Jishan Hu,Yang Wang,Hao Chen,Shizhe Diao,Shiwei Liu,Yu Li,Yin Lu,Can Yang*

Main category: cs.LG

TL;DR: The paper introduces GPAS, a technique that scales down intermediate activations while preserving gradients, to improve the training dynamics of Transformer architectures like Pre-LN, Sandwich-LN, and DeepNorm.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of exponential growth in activation variance in Pre-LN Transformers, which limits the learning capacity of deeper layers and causes the residual path to dominate over sub-layer outputs.

Method: The paper proposes Gradient-Preserving Activation Scaling (GPAS), a technique that scales down intermediate activations while keeping their gradients unchanged to mitigate the issue of exponential growth in activation variance across layers.

Result: Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Additionally, GPAS shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm.

Conclusion: GPAS can enhance the training dynamics of various Transformer architectures, including Pre-LN, Sandwich-LN, and DeepNorm, and has shown consistent performance gains across different model sizes.

Abstract: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [112] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep,Samuel Genheden,Ola Engkvist,Jens Sjölund*

Main category: cs.LG

TL;DR: 本研究探讨了基于大型语言模型的代理系统在药物发现中的模块化问题，并比较了不同模型和代理类型的表现。结果显示，某些模型表现更优，但效果高度依赖于具体问题和模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和代理系统为加速药物发现和设计提供了令人兴奋的机会。然而，关于LLM-based代理系统的模块化问题，即这些系统中的部分（如LLM）是否可互换，尚未得到充分关注。

Method: 我们比较了不同大型语言模型（LLMs）以及工具调用代理与代码生成代理在该领域的性能。

Result: Claude-3.5-Sonnet、Claude-3.7-Sonnet和GPT-4o在使用LLM-as-a-judge评分进行化学和药物发现工具协调的案例研究中表现优于其他模型。虽然代码生成代理平均表现优于工具调用代理，但这一结果高度依赖于具体问题和模型。此外，替换系统提示的影响取决于具体问题和使用的模型。

Conclusion: 我们的研究强调了进一步研究代理系统模块化的必要性，以实现稳定和可扩展的解决方案。

Abstract: Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [113] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak,Michał Krutul,Jan Małaśnicki,Maciej Pióro,Jakub Krajewski,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jan Ludziejewski*

Main category: cs.LG

TL;DR: 本文提出了一种名为 Projected Compression 的新型模型压缩技术，能够有效减少模型大小而不影响性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的规模不断增加，推理时间和计算需求也相应增加。因此，需要一种有效的模型压缩方法来解决这一问题。

Method: Projected Compression 通过利用投影模块来减少模型权重。首先训练额外的可训练投影权重，并保留对所有原始模型参数的访问。随后，这些投影被合并为一个低维的乘积矩阵，从而得到一个更小的标准 Transformer 基础模型。

Result: 实验结果表明，Projected Compression 在高质量模型上优于类似的硬剪枝和再训练方法。此外，性能差距随着标记数量的增加而扩大。

Conclusion: Projected Compression 是一种有效的模型压缩技术，能够在不牺牲性能的情况下减少模型大小。

Abstract: Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [114] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang,Jian Wang,Rubing Chen,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于概率框架的推理时扩展方法，通过	extsc{OptScale}算法实现了高效的采样优化。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于启发式策略进行并行采样，缺乏原理基础。我们需要一种更有效的方法来提高大型语言模型的推理性能。

Method: 我们提出了一个概率框架，形式化了推理时扩展的最优性，并开发了	extsc{OptScale}算法，该算法动态确定采样响应的最优数量。

Result: 实验表明，	extsc{OptScale}显著减少了采样开销，同时保持了与最先进的推理性能相当或更好的表现。

Conclusion: 我们的工作为推理时的扩展提供了理论基础和实用解决方案，解决了LLM在复杂推理中高效部署的关键差距。

Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [115] [Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts](https://arxiv.org/abs/2506.22343)
*Xiang Li,Garrett Wen,Weiqing He,Jiayuan Wu,Qi Long,Weijie J. Su*

Main category: stat.ML

TL;DR: 本文研究了如何准确估计混合源文本中的水印比例，并提出了高效的估计方法。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，混合源文本（混合人类写作和水印内容）普遍存在，需要准确估计水印比例。

Method: 本文将问题转化为基于关键统计量的混合模型比例参数估计问题，并提出了高效的估计器。

Result: 实验表明，所提出的估计器在合成数据和开源模型生成的混合源文本上均表现出高精度。

Conclusion: 本文提出了针对混合源文本中水印比例的最优估计方法，并通过实验验证了其高精度。

Abstract: Text watermarks in large language models (LLMs) are an increasingly important
tool for detecting synthetic text and distinguishing human-written content from
LLM-generated text. While most existing studies focus on determining whether
entire texts are watermarked, many real-world scenarios involve mixed-source
texts, which blend human-written and watermarked content. In this paper, we
address the problem of optimally estimating the watermark proportion in
mixed-source texts. We cast this problem as estimating the proportion parameter
in a mixture model based on \emph{pivotal statistics}. First, we show that this
parameter is not even identifiable in certain watermarking schemes, let alone
consistently estimable. In stark contrast, for watermarking methods that employ
continuous pivotal statistics for detection, we demonstrate that the proportion
parameter is identifiable under mild conditions. We propose efficient
estimators for this class of methods, which include several popular unbiased
watermarks as examples, and derive minimax lower bounds for any measurable
estimator based on pivotal statistics, showing that our estimators achieve
these lower bounds. Through evaluations on both synthetic data and mixed-source
text generated by open-source models, we demonstrate that our proposed
estimators consistently achieve high estimation accuracy.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [116] [RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture](https://arxiv.org/abs/2506.21865)
*Haofeng Wang,Yilin Guo,Zehao Li,Tong Yue,Yizong Wang,Enci Zhang,Rongqun Lin,Feng Gao,Shiqi Wang,Siwei Ma*

Main category: cs.MM

TL;DR: 本文介绍了RiverEcho系统，它利用大型语言模型和文化知识数据集来响应语音查询，并通过数字人进行解释，从而提升对古代黄河文化的理解和传播。


<details>
  <summary>Details</summary>
Motivation: 为了保护和传承古代黄河文化，我们开发了RiverEcho系统，以提供更专业和信息丰富的回答。

Method: 我们设计了一个名为RiverEcho的实时交互系统，该系统使用大型语言模型和文化知识数据集来响应语音查询，并通过一个会说话的数字人进行解释。具体来说，我们构建了一个专注于古代黄河文化的知识数据库，包括历史文本的收集和处理流程。

Result: 实验结果表明，在所提出的数据集上使用检索增强生成（RAG）可以提高大型语言模型（LLM）的响应质量，使系统能够生成更专业和信息丰富的回答。

Conclusion: 我们的工作不仅丰富了推广黄河文化的方式，还为用户提供了更深入的文化洞察。

Abstract: The Yellow River is China's mother river and a cradle of human civilization.
The ancient Yellow River culture is, moreover, an indispensable part of human
art history. To conserve and inherit the ancient Yellow River culture, we
designed RiverEcho, a real-time interactive system that responds to voice
queries using a large language model and a cultural knowledge dataset,
delivering explanations through a talking-head digital human. Specifically, we
built a knowledge database focused on the ancient Yellow River culture,
including the collection of historical texts and the processing pipeline.
Experimental results demonstrate that leveraging Retrieval-Augmented Generation
(RAG) on the proposed dataset enhances the response quality of the Large
Language Model(LLM), enabling the system to generate more professional and
informative responses. Our work not only diversifies the means of promoting
Yellow River culture but also provides users with deeper cultural insights.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [117] [Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics](https://arxiv.org/abs/2506.21964)
*Michael A. Riegler,Kristoffer Herland Hellton,Vajira Thambawita,Hugo L. Hammer*

Main category: stat.ME

TL;DR: This paper explores using large-language models (LLMs) to suggest informative priors in Bayesian statistics, showing their potential but highlighting challenges in calibrating prior widths.


<details>
  <summary>Details</summary>
Motivation: Selecting prior distributions in Bayesian statistics is challenging, resource-intensive, and subjective. The study aimed to explore the use of LLMs to address these challenges by suggesting informative priors.

Method: The study used large-language models (LLMs) to suggest suitable, knowledge-based informative priors and evaluated their performance on two real datasets: heart disease risk and concrete strength.

Result: All LLMs correctly identified the direction for all associations. However, moderate priors were often overconfident, while weakly informative priors showed differences in performance, with Claude and Gemini providing better results than ChatGPT.

Conclusion: LLMs have great potential as an efficient, objective method for developing informative priors, but the primary challenge remains in calibrating the width of these priors to avoid over- and under-confidence.

Abstract: Selecting prior distributions in Bayesian statistics is challenging,
resource-intensive, and subjective. We analyze using large-language models
(LLMs) to suggest suitable, knowledge-based informative priors. We developed an
extensive prompt asking LLMs not only to suggest priors but also to verify and
reflect on their choices.
  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real
datasets: heart disease risk and concrete strength. All LLMs correctly
identified the direction for all associations (e.g., that heart disease risk is
higher for males). The quality of suggested priors was measured by their
Kullback-Leibler divergence from the maximum likelihood estimator's
distribution.
  The LLMs suggested both moderately and weakly informative priors. The
moderate priors were often overconfident, resulting in distributions misaligned
with the data. In our experiments, Claude and Gemini provided better priors
than ChatGPT. For weakly informative priors, a key performance difference
emerged: ChatGPT and Gemini defaulted to an "unnecessarily vague" mean of 0,
while Claude did not, demonstrating a significant advantage.
  The ability of LLMs to identify correct associations shows their great
potential as an efficient, objective method for developing informative priors.
However, the primary challenge remains in calibrating the width of these priors
to avoid over- and under-confidence.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [118] [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
*Nicolas Bougie,Narimasa Watanabe*

Main category: cs.AI

TL;DR: 本文提出了一种名为CitySim的城市模拟器，利用大语言模型的人类水平智能来生成现实的日常计划，并赋予代理信念、长期目标和空间记忆以实现长期、逼真的模拟。实验表明，CitySim在微观和宏观层面都比以前的工作更接近真实人类，是一个可扩展且灵活的测试平台。


<details>
  <summary>Details</summary>
Motivation: 以往的工作往往依赖于刚性的手工规则，限制了它们模拟细微意图、计划和适应性行为的能力。

Method: CitySim利用大语言模型的人类水平智能，通过递归的价值驱动方法生成现实的日常计划，并赋予代理信念、长期目标和空间记忆以实现长期、逼真的模拟。

Result: CitySim在微观和宏观层面都比以前的工作更接近真实人类。通过建模数万个代理并评估他们在各种现实场景中的集体行为，结果表明CitySim是一个可扩展且灵活的测试平台。

Conclusion: CitySim是一个可扩展且灵活的测试平台，有助于理解和预测城市现象。

Abstract: Modeling human behavior in urban environments is fundamental for social
science, behavioral studies, and urban planning. Prior work often rely on
rigid, hand-crafted rules, limiting their ability to simulate nuanced
intentions, plans, and adaptive behaviors. Addressing these challenges, we
envision an urban simulator (CitySim), capitalizing on breakthroughs in
human-level intelligence exhibited by large language models. In CitySim, agents
generate realistic daily schedules using a recursive value-driven approach that
balances mandatory activities, personal habits, and situational factors. To
enable long-term, lifelike simulations, we endow agents with beliefs, long-term
goals, and spatial memory for navigation. CitySim exhibits closer alignment
with real humans than prior work, both at micro and macro levels. Additionally,
we conduct insightful experiments by modeling tens of thousands of agents and
evaluating their collective behaviors under various real-world scenarios,
including estimating crowd density, predicting place popularity, and assessing
well-being. Our results highlight CitySim as a scalable, flexible testbed for
understanding and forecasting urban phenomena.

</details>


### [119] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst,Dominik Dürrschnabel,Johannes Hirth,Gerd Stumme*

Main category: cs.AI

TL;DR: 本文提出了FAT-CAT方法，基于形式概念分析（FCA）来增强有意义的主题聚合和发现主题的可视化。在ETYNTKE数据集的案例研究中，我们评估了该方法的有效性，以证明基于FCA的聚合比现有的主题建模技术提供了更有意义和可解释的数据集组成见解。


<details>
  <summary>Details</summary>
Motivation: Existing methods for topic modeling often struggle to provide interpretable representations that facilitate deeper insights into data structure and content.

Method: FAT-CAT, an approach based on Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and visualization of discovered topics.

Result: In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our approach against other representation methods to demonstrate that FCA-based aggregation provides more meaningful and interpretable insights into dataset composition than existing topic modeling techniques.

Conclusion: FCA-based aggregation provides more meaningful and interpretable insights into dataset composition than existing topic modeling techniques.

Abstract: The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [120] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao,Despoina Magka,Minqi Jiang,Xian Li,Roberta Raileanu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Kelvin Niu,Shagun Sodhani,Michael Shvartsman,Andrei Lupu,Alisia Lupidi,Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Thomas Foster,Lucia Cipolina-Kun,Abhishek Charnalia,Derek Dunfield,Alexander H. Miller,Oisin Mac Aodha,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本文介绍了Automated LLM Speedrunning Benchmark，用于评估AI代理在活跃研究领域再现结果的能力。结果显示，即使有详细提示，最新的推理LLM也难以重新实现已知的创新。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在活跃研究领域再现结果的能力，以实现LLM在科学进步中的潜力。

Method: 我们引入了Automated LLM Speedrunning Benchmark，利用NanoGPT speedrun的研究社区贡献，这是一个在最短时间内训练GPT-2模型的比赛。每个速度运行任务都为代理提供了之前的记录训练脚本，并可能配有一种三种提示格式中的一种，从伪代码到论文般的描述新记录改进。

Result: 最近的推理LLM结合最先进的支架在我们的基准测试中难以重新实现已知的创新，即使给出了详细的提示。

Conclusion: 我们的基准测试提供了一种简单且不饱和的衡量LLM自动化科学再现能力的方法，这是自主研究代理所必需（但不充分）的技能。

Abstract: Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [121] [3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach](https://arxiv.org/abs/2506.21845)
*Zhuodi Cai*

Main category: cs.HC

TL;DR: 本文介绍了3Description，一种基于网络的人机协作方法，使非专业人员可以通过口头和手势描述共同创建3D模型，从而提高可访问性和可用性。


<details>
  <summary>Details</summary>
Motivation: 传统3D建模在可访问性和可用性方面存在挑战，非专业人员难以参与3D建模。

Method: 通过定性研究、产品分析和用户测试，3Description结合了自然语言处理和计算机视觉等AI技术，由OpenAI和MediaPipe提供支持。

Result: 3Description是一种实验性的基于网络的人机协作方法，允许用户通过口头和手势描述共同创建3D模型，并调整其组件。

Conclusion: 3Description不仅有助于更包容和用户友好的设计过程，使更多人能够参与未来3D世界的构建，还努力提高人类在与AI协作创作中的参与度，从而避免对技术的过度依赖并保持人类创造力。

Abstract: This paper presents 3Description, an experimental human-AI collaborative
approach for intuitive 3D modeling. 3Description aims to address accessibility
and usability challenges in traditional 3D modeling by enabling
non-professional individuals to co-create 3D models using verbal and gesture
descriptions. Through a combination of qualitative research, product analysis,
and user testing, 3Description integrates AI technologies such as Natural
Language Processing and Computer Vision, powered by OpenAI and MediaPipe.
Recognizing the web has wide cross-platform capabilities, 3Description is
web-based, allowing users to describe the desired model and subsequently adjust
its components using verbal and gestural inputs. In the era of AI and emerging
media, 3Description not only contributes to a more inclusive and user-friendly
design process, empowering more people to participate in the construction of
the future 3D world, but also strives to increase human engagement in
co-creation with AI, thereby avoiding undue surrender to technology and
preserving human creativity.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [122] [Exploring the change in scientific readability following the release of ChatGPT](https://arxiv.org/abs/2506.21825)
*Abdulkareem Alsudais*

Main category: cs.CY

TL;DR: 该研究分析了arXiv.org上从2010年到2024年6月7日的所有摘要，发现可读性在逐年下降，并且在ChatGPT发布后出现了显著变化。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型的兴起对科学写作的影响，特别是评估arXiv.org上摘要的可读性变化。

Method: 使用四种标准的可读性公式计算每篇论文的可读性得分，并按年份和八个主要类别进行汇总分析。

Result: 结果显示可读性逐年下降，表明摘要可能变得越来越复杂。此外，在ChatGPT发布后，2023年和2024年的部分月份出现了显著的可读性变化。

Conclusion: 这些发现提供了关于可读性更广泛变化的见解，并指出人工智能可能对科学写作产生了影响。

Abstract: The rise and growing popularity of accessible large language models have
raised questions about their impact on various aspects of life, including how
scientists write and publish their research. The primary objective of this
paper is to analyze a dataset consisting of all abstracts posted on arXiv.org
between 2010 and June 7th, 2024, to assess the evolution of their readability
and determine whether significant shifts occurred following the release of
ChatGPT in November 2022. Four standard readability formulas are used to
calculate individual readability scores for each paper, classifying their level
of readability. These scores are then aggregated by year and across the eight
primary categories covered by the platform. The results show a steady annual
decrease in readability, suggesting that abstracts are likely becoming
increasingly complex. Additionally, following the release of ChatGPT, a
significant change in readability is observed for 2023 and the analyzed months
of 2024. Similar trends are found across categories, with most experiencing a
notable change in readability during 2023 and 2024. These findings offer
insights into the broader changes in readability and point to the likely
influence of AI on scientific writing.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 67]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)
*Agrima Seth,Monojit Choudhary,Sunayana Sitaram,Kentaro Toyama,Aditya Vashistha,Kalika Bali*

Main category: cs.CL

TL;DR: 本研究通过系统审计GPT-4 Turbo，揭示了大型语言模型中代表偏见的深度和扩展性，发现即使有鼓励多样性的提示，模型仍倾向于过度代表文化上占主导地位的群体。研究结果表明，仅靠多样化训练数据可能不足以纠正LLM的偏见，需要更根本的模型开发改变。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要通过单次响应交互来衡量大型语言模型（LLMs）中的代表偏见，并集中在以全球北方为中心的身份（如种族和性别）。我们希望扩展这项研究，以揭示代表偏见的深度及其扩展到较少探索的身份维度。

Method: 我们对GPT-4 Turbo进行了系统审计，以揭示代表偏见的深度和扩展到较少探索的身份维度。我们使用设计用于鼓励多样性的提示，让GPT-4 Turbo生成超过7,200个关于印度重要生活事件（如婚礼）的故事。

Result: 我们发现GPT-4的响应持续过度代表文化上占主导地位的群体，远超其统计代表性，尽管提示旨在鼓励代表多样性。我们的研究结果还表明，LLM中的代表偏见具有赢家通吃的特点，比其训练数据中的可能分布偏见更为偏颇，重复的基于提示的引导在解除这些偏见方面效果有限且不一致。

Conclusion: 我们的研究结果表明，仅通过多样化训练数据可能不足以纠正LLM的偏见，这突显了在模型开发中进行更根本性改变的必要性。

Abstract: Representational bias in large language models (LLMs) has predominantly been
measured through single-response interactions and has focused on Global
North-centric identities like race and gender. We expand on that research by
conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded
representational biases are and how they extend to less-explored dimensions of
identity. We prompt GPT-4 Turbo to generate over 7,200 stories about
significant life events (such as weddings) in India, using prompts designed to
encourage diversity to varying extents. Comparing the diversity of religious
and caste representation in the outputs against the actual population
distribution in India as recorded in census data, we quantify the presence and
"stickiness" of representational bias in the LLM for religion and caste. We
find that GPT-4 responses consistently overrepresent culturally dominant groups
far beyond their statistical representation, despite prompts intended to
encourage representational diversity. Our findings also suggest that
representational bias in LLMs has a winner-take-all quality that is more biased
than the likely distribution bias in their training data, and repeated
prompt-based nudges have limited and inconsistent efficacy in dislodging these
biases. These results suggest that diversifying training data alone may not be
sufficient to correct LLM bias, highlighting the need for more fundamental
changes in model development. Dataset and Codebook:
https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs

</details>


### [2] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 研究开发了针对高能物理的大型语言模型，并证明了它们在相关任务上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发专门用于高能物理领域的大型语言模型，以提高在该领域任务中的表现。

Method: 研究通过微调Llama-3.1模型的不同变体，使用不同组合的arXiv摘要进行训练，并比较了不同微调方法和数据集大小的效果。

Result: 研究结果表明，这些专门模型在高能物理抽象补全任务上优于基础模型，并且在与商业LLM的比较中表现出色。

Conclusion: 研究展示了专门针对高能物理的大型语言模型在抽象补全任务上的优越性能，并为未来开发此类模型提供了见解。

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [3] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
*Abhay Vijayvargia,Ajay Nagpal,Kundeshwar Pundalik,Atharva Savarkar,Smita Gautam,Pankaj Singh,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Krishi Sathi的AI驱动的农业聊天机器人，旨在为印度农民提供个性化、易懂的农业建议。该系统通过结构化的多轮对话流程和检索增强生成技术，实现了高准确率和快速响应时间。


<details>
  <summary>Details</summary>
Motivation: 印度农民常常缺乏及时、易获取且语言友好的农业建议，特别是在低识字率的农村地区。为了弥补这一可访问性差距，本文提出了一种新的AI驱动的农业聊天机器人，以提供个性化的、易于理解的答案。

Method: 该论文提出了一个名为Krishi Sathi的AI驱动的农业聊天机器人，利用IFT模型并在三个精心策划的数据集上进行微调。系统采用结构化的多轮对话流程，逐步收集必要的信息，并通过检索增强生成（RAG）来提供定制化回答。

Result: 该系统在查询响应准确率、上下文相关性和个性化以及查询完成率方面取得了显著成果，分别达到了97.53%、91.35%和97.53%。平均响应时间保持在6秒以下，确保了对英语和印地语交互用户的及时支持。

Conclusion: 该方法在印度的数字农业支持中展示了如何通过意图驱动的对话流程、指令调优模型和基于检索的生成来提高质量和可访问性。

Abstract: Indian farmers often lack timely, accessible, and language-friendly
agricultural advice, especially in rural areas with low literacy. To address
this gap in accessibility, this paper presents a novel AI-powered agricultural
chatbot, Krishi Sathi, designed to support Indian farmers by providing
personalized, easy-to-understand answers to their queries through both text and
speech. The system's intelligence stems from an IFT model, subsequently refined
through fine-tuning on Indian agricultural knowledge across three curated
datasets. Unlike traditional chatbots that respond to one-off questions, Krishi
Sathi follows a structured, multi-turn conversation flow to gradually collect
the necessary details from the farmer, ensuring the query is fully understood
before generating a response. Once the intent and context are extracted, the
system performs Retrieval-Augmented Generation (RAG) by first fetching
information from a curated agricultural database and then generating a tailored
response using the IFT model. The chatbot supports both English and Hindi
languages, with speech input and output features (via ASR and TTS) to make it
accessible for users with low literacy or limited digital skills. This work
demonstrates how combining intent-driven dialogue flows, instruction-tuned
models, and retrieval-based generation can improve the quality and
accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query
response accuracy of 97.53%, 91.35% contextual relevance and personalization,
and a query completion rate of 97.53%. The average response time remained under
6 seconds, ensuring timely support for users across both English and Hindi
interactions.

</details>


### [4] [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)
*Jaydip Sen,Harshitha Puvvala,Subhasis Dasgupta*

Main category: cs.CL

TL;DR: 本文提出了一种新的分层验证树框架（HVT），用于优化大型语言模型的推理效率，通过优先选择高可能性的草稿并提前剪枝次优候选者，从而减少计算开销并提高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理任务中取得了显著成功，但由于其自回归性质，在推理效率方面面临持续挑战。传统的验证方法按顺序验证草稿序列，缺乏优先级，导致不必要的计算开销。

Method: 本文提出了分层验证树（HVT）框架，通过优先选择高可能性的草稿并提前剪枝次优候选者，重新构建了推测性束解码。开发了理论基础和形式化验证-剪枝算法以确保正确性和效率，并将HVT集成到标准LLM推理流程中，无需重新训练或架构修改。

Result: 实验评估显示，HVT在多个数据集和模型上 consistently 超过现有的推测解码方案，实现了推理时间和能耗的显著减少，同时保持或提升了输出质量。

Conclusion: 本文提出的分层验证树（HVT）框架在保持或提升输出质量的同时，显著减少了推理时间和能耗，展示了分层验证策略作为加速大型语言模型推理的新方向的潜力。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
natural language processing tasks but face persistent challenges in inference
efficiency due to their autoregressive nature. While speculative decoding and
beam sampling offer notable improvements, traditional methods verify draft
sequences sequentially without prioritization, leading to unnecessary
computational overhead. This work proposes the Hierarchical Verification Tree
(HVT), a novel framework that restructures speculative beam decoding by
prioritizing high-likelihood drafts and enabling early pruning of suboptimal
candidates. Theoretical foundations and a formal verification-pruning algorithm
are developed to ensure correctness and efficiency. Integration with standard
LLM inference pipelines is achieved without requiring retraining or
architecture modification. Experimental evaluations across multiple datasets
and models demonstrate that HVT consistently outperforms existing speculative
decoding schemes, achieving substantial reductions in inference time and energy
consumption while maintaining or enhancing output quality. The findings
highlight the potential of hierarchical verification strategies as a new
direction for accelerating large language model inference.

</details>


### [5] [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)
*Revanth Gangi Reddy,Tanay Dixit,Jiaxin Qin,Cheng Qian,Daniel Lee,Jiawei Han,Kevin Small,Xing Fan,Ruhi Sarikaya,Heng Ji*

Main category: cs.CL

TL;DR: 本文介绍了一种名为WiNELL的代理框架，用于持续更新维基百科文章。该框架通过多代理系统聚合信息、选择新知识，并生成编辑建议。实验表明，WiNELL在关键信息覆盖率和编辑效率方面优于现有方法，为LLM代理在自动更新知识库方面提供了新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 维基百科作为一个庞大的知识库，面临着由于依赖人工编辑而难以保持内容更新的问题。受NELL连续知识获取愿景和LLM代理进展的启发，本文旨在开发一种能够持续更新维基百科文章的代理框架。

Method: 本文提出了一种多代理框架，用于聚合在线信息、选择新且重要的知识，并生成精确的编辑建议。此外，还训练了细粒度的编辑模型，使其能够以与人类编辑行为一致的方式进行更新。

Result: WiNELL在关键信息覆盖率和编辑效率方面优于开源指令遵循基线和闭源LLM（如GPT-4o）。在高活跃度维基百科页面上的端到端评估表明，WiNELL能够识别并建议及时的事实更新。

Conclusion: 本文提出了WiNELL框架，这是一种用于持续更新维基百科文章的代理框架。该框架通过多代理系统聚合在线信息，选择目标实体的新且重要的知识，并生成精确的编辑建议供人工审查。实验表明，WiNELL在关键信息覆盖率和编辑效率方面优于开源指令遵循基线和闭源LLM（如GPT-4o）。这为LLM代理在自动更新知识库方面提供了一个有前景的研究方向。

Abstract: Wikipedia, a vast and continuously consulted knowledge base, faces
significant challenges in maintaining up-to-date content due to its reliance on
manual human editors. Inspired by the vision of continuous knowledge
acquisition in NELL and fueled by advances in LLM-based agents, this paper
introduces WiNELL, an agentic framework for continuously updating Wikipedia
articles. Our approach employs a multi-agent framework to aggregate online
information, select new and important knowledge for a target entity in
Wikipedia, and then generate precise edit suggestions for human review. Our
fine-grained editing models, trained on Wikipedia's extensive history of human
edits, enable incorporating updates in a manner consistent with human editing
behavior. Our editor models outperform both open-source instruction-following
baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and
editing efficiency. End-to-end evaluation on high-activity Wikipedia pages
demonstrates WiNELL's ability to identify and suggest timely factual updates.
This opens up a promising research direction in LLM agents for automatically
updating knowledge bases in a never-ending fashion.

</details>


### [6] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
*Ashutosh Bandooni,Brindha Subburaj*

Main category: cs.CL

TL;DR: 本文介绍了GanitBench，一个包含1527个视觉问题的基准测试，涵盖数学多个主题，并提供英语和印地语版本。我们评估了两种封闭源代码模型的表现，发现GPT-4o mini在基准测试中表现更优，但在“Double Lock”约束和印地语环境下性能有所下降。


<details>
  <summary>Details</summary>
Motivation: 目前的基准测试通常是单语的，主要以英语提供。此外，除了理解和翻译任务外，印地语的数据集也较少。

Method: 我们引入了GanitBench，这是一个包含1527个视觉问题的基准测试，涵盖了数学的多个主题，并提供英语和印地语版本。这些问题来自印度的两项主要考试：JEE Advanced和CBSE考试。我们评估了两种封闭源代码模型在零次提示链式思维（CoT）和两次提示CoT设置下的表现。

Result: GPT-4o mini在基准测试中表现更为突出，其最高平均准确率为38.15%。在“Double Lock”约束下，模型的性能大幅下降。在印地语环境下，两个VLM的性能也有所下降。

Conclusion: 我们希望通过我们的工作促进像印地语这样的语言在研究中的纳入。

Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on
several fields and domains are being curated more frequently over the last few
years. However these are often monolingual, mostly available in English.
Additionally there also is a lack of datasets available in Hindi on tasks apart
from comprehension and translation. We introduce GanitBench, a tough benchmark
consisting of 1527 vision-only questions covering several topics in Mathematics
- available in languages English and Hindi. Collected from two major
examinations from India, the JEE Advanced and the CBSE Boards examinations,
this benchmark includes questions in the form of images comprising of figures
essential to a question as well as text. We evaluate two closed source models
for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.
GPT-4o mini is found to be the more dominant model on the benchmark, with it's
highest average accuracy being 38.15%. We also evaluate models through a
"Double Lock" constraint, which brings down the performance of the models by
considerable margins. We observe that two-shot CoT appears to be a more
effective setting under this environment. Performance of the two VLMs also
decreases when answering the same questions in the Hindi language. We hope to
facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [7] [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)
*Yanting Wang,Runpeng Geng,Ying Chen,Jinyuan Jia*

Main category: cs.CL

TL;DR: This paper proposes AttnTrace, a new context traceback method based on attention weights. It is more accurate and efficient than existing methods and can improve prompt injection detection.


<details>
  <summary>Details</summary>
Motivation: Recent studies have designed solutions to trace back to a subset of texts in the context that contributes most to the response generated by the LLM. However, state-of-the-art solutions such as TracLLM often lead to a high computation cost.

Method: AttnTrace, a new context traceback method based on the attention weights produced by an LLM for a prompt. Two techniques are introduced to enhance the effectiveness of AttnTrace.

Result: AttnTrace is more accurate and efficient than existing state-of-the-art context traceback methods. It can also improve state-of-the-art methods in detecting prompt injection under long contexts through the attribution-before-detection paradigm.

Conclusion: AttnTrace is more accurate and efficient than existing state-of-the-art context traceback methods. It can also improve state-of-the-art methods in detecting prompt injection under long contexts.

Abstract: Long-context large language models (LLMs), such as Gemini-2.5-Pro and
Claude-Sonnet-4, are increasingly used to empower advanced AI systems,
including retrieval-augmented generation (RAG) pipelines and autonomous agents.
In these systems, an LLM receives an instruction along with a context--often
consisting of texts retrieved from a knowledge database or memory--and
generates a response that is contextually grounded by following the
instruction. Recent studies have designed solutions to trace back to a subset
of texts in the context that contributes most to the response generated by the
LLM. These solutions have numerous real-world applications, including
performing post-attack forensic analysis and improving the interpretability and
trustworthiness of LLM outputs. While significant efforts have been made,
state-of-the-art solutions such as TracLLM often lead to a high computation
cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a
single response-context pair. In this work, we propose AttnTrace, a new context
traceback method based on the attention weights produced by an LLM for a
prompt. To effectively utilize attention weights, we introduce two techniques
designed to enhance the effectiveness of AttnTrace, and we provide theoretical
insights for our design choice. We also perform a systematic evaluation for
AttnTrace. The results demonstrate that AttnTrace is more accurate and
efficient than existing state-of-the-art context traceback methods. We also
show that AttnTrace can improve state-of-the-art methods in detecting prompt
injection under long contexts through the attribution-before-detection
paradigm. As a real-world application, we demonstrate that AttnTrace can
effectively pinpoint injected instructions in a paper designed to manipulate
LLM-generated reviews. The code is at
https://github.com/Wang-Yanting/AttnTrace.

</details>


### [8] [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)
*Jiahao Xu,Rui Hu,Zikai Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的水印方法MajorMark，通过多数位感知编码改进了文本质量和解码准确性的权衡，并引入了MajorMark+以进一步提高水印文本的质量和解码准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在现实应用中的部署增加，对其可能被滥用生成有害或欺骗性内容的担忧日益加剧。水印技术作为一种有前景的解决方案，通过在生成的文本中嵌入可识别的二进制消息来验证来源和追踪滥用行为。然而，现有的多比特水印方案在文本质量和解码准确性之间存在根本性的权衡。

Method: MajorMark通过多数位感知编码改进了这一权衡，选择基于消息多数位的首选标记集，从而实现更大更灵活的标记采样。MajorMark+将消息分成多个块，独立编码并确定性地解码每个块，进一步提高了水印文本的质量和解码准确性。

Result: 在最先进的LLM上进行的广泛实验表明，我们的方法显著提高了解码准确性和文本生成质量，优于之前的多比特水印基线。

Conclusion: 我们的方法显著提高了解码准确性和文本生成质量，优于之前的多比特水印基线。

Abstract: The growing deployment of Large Language Models (LLMs) in real-world
applications has raised concerns about their potential misuse in generating
harmful or deceptive content. To address this issue, watermarking techniques
have emerged as a promising solution by embedding identifiable binary messages
into generated text for origin verification and misuse tracing. While recent
efforts have explored multi-bit watermarking schemes capable of embedding rich
information such as user identifiers, they typically suffer from the
fundamental trade-off between text quality and decoding accuracy: to ensure
reliable message decoding, they have to restrict the size of preferred token
sets during encoding, yet such restrictions reduce the quality of the generated
content. In this work, we propose MajorMark, a novel watermarking method that
improves this trade-off through majority bit-aware encoding. MajorMark selects
preferred token sets based on the majority bit of the message, enabling a
larger and more flexible sampling of tokens. In contrast to prior methods that
rely on token frequency analysis for decoding, MajorMark employs a
clustering-based decoding strategy, which maintains high decoding accuracy even
when the preferred token set is large, thus preserving both content quality and
decoding accuracy. We further introduce MajorMark$^+$, which partitions the
message into multiple blocks to independently encode and deterministically
decode each block, thereby further enhancing the quality of watermarked text
and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs
demonstrate that our methods significantly enhance both decoding accuracy and
text generation quality, outperforming prior multi-bit watermarking baselines.

</details>


### [9] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: 该综述探讨了如何评估大型语言模型生成内容的事实准确性，提出了五个研究问题，并讨论了指令调优、多智能体推理和通过RAG框架访问外部知识的作用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在包含不准确或误导性内容的互联网语料库上进行训练，因此可能生成错误信息，使得稳健的事实核查至关重要。

Method: 该综述系统地分析了如何评估大型语言模型生成内容的事实准确性，探讨了幻觉、数据集限制和评估指标可靠性等关键挑战。它提出了五个研究问题，指导对2020年至2025年最近文献的分析，重点关注评估方法和缓解技术。

Result: 关键发现包括当前指标的局限性，通过验证的外部证据来支持输出的价值，以及领域特定定制对于提高事实一致性的重要性。

Conclusion: 该综述强调了构建不仅准确且可解释，而且针对特定领域事实核查的大型语言模型的重要性。这些见解有助于推动研究向更值得信赖和上下文感知的语言模型发展。

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [10] [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)
*Yajie Luo,Yihong Wu,Muzhi Li,Fengran Mo,Jia Ao Sun,Xinyu Wang,Liheng Ma,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的实体链接代理，用于问答任务，实验结果表明其有效且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有的实体链接方法主要针对长文本设计，在问答任务中的短而模糊的用户问题上表现不佳。

Method: 基于大型语言模型的实体链接代理，模拟人类认知流程，主动识别实体提及、检索候选实体并做出决策。

Result: 通过基于工具的实体链接和问答任务评估实验，结果证实了代理的有效性和鲁棒性。

Conclusion: 我们的代理在实体链接和问答任务中表现出色，验证了其鲁棒性和有效性。

Abstract: Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide
accurate answers. Entity Linking (EL) plays a critical role in linking natural
language mentions to KB entries. However, most existing EL methods are designed
for long contexts and do not perform well on short, ambiguous user questions in
QA tasks. We propose an entity linking agent for QA, based on a Large Language
Model that simulates human cognitive workflows. The agent actively identifies
entity mentions, retrieves candidate entities, and makes decision. To verify
the effectiveness of our agent, we conduct two experiments: tool-based entity
linking and QA task evaluation. The results confirm the robustness and
effectiveness of our agent.

</details>


### [11] [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
*Haofei Yu,Zhengyang Qi,Yining Zhao,Kolby Nottingham,Keyang Xuan,Bodhisattwa Prasad Majumder,Hao Zhu,Paul Pu Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: Sotopia-RL is a framework that improves RL training for social interactions by using utterance-level, multi-dimensional rewards, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of partial observability and multi-dimensionality in social interactions for RL training.

Method: Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards.

Result: Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full).

Conclusion: Sotopia-RL achieves state-of-the-art social goal completion scores and significantly outperforms existing approaches.

Abstract: Social intelligence has become a critical capability for large language
models (LLMs), enabling them to engage effectively in real-world social tasks
such as accommodation, persuasion, collaboration, and negotiation.
Reinforcement learning (RL) is a natural fit for training socially intelligent
agents because it allows models to learn sophisticated strategies directly
through social interactions. However, social interactions have two key
characteristics that set barriers for RL training: (1) partial observability,
where utterances have indirect and delayed effects that complicate credit
assignment, and (2) multi-dimensionality, where behaviors such as
rapport-building or knowledge-seeking contribute indirectly to goal
achievement. These characteristics make Markov decision process (MDP)-based RL
with single-dimensional episode-level rewards inefficient and unstable. To
address these challenges, we propose Sotopia-RL, a novel framework that refines
coarse episode-level feedback into utterance-level, multi-dimensional rewards.
Utterance-level credit assignment mitigates partial observability by
attributing outcomes to individual utterances, while multi-dimensional rewards
capture the full richness of social interactions and reduce reward hacking.
Experiments in Sotopia, an open-ended social learning environment, demonstrate
that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17
on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing
approaches. Ablation studies confirm the necessity of both utterance-level
credit assignment and multi-dimensional reward design for RL training. Our
implementation is publicly available at:
https://github.com/sotopia-lab/sotopia-rl.

</details>


### [12] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)
*Linxin Song,Yutong Dai,Viraj Prabhu,Jieyu Zhang,Taiwei Shi,Li Li,Junnan Li,Silvio Savarese,Zeyuan Chen,Jieyu Zhao,Ran Xu,Caiming Xiong*

Main category: cs.CL

TL;DR: 本文介绍了一种将编程作为增强动作的多智能体系统CoAct-1，显著提高了复杂任务的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的自主代理在复杂、长期任务中效率和可靠性不足，而通过GUI操作的所有动作存在局限性，导致脆弱性和低效性。

Method: 提出了一种新的多智能体系统CoAct-1，结合基于GUI的控制与直接编程执行。

Result: 在OSWorld基准测试中，CoAct-1达到了60.76%的新最佳成功率，平均完成任务步骤减少到10.15，优于领先的GUI代理。

Conclusion: 集成编码作为核心动作提供了一条更强大、高效和可扩展的通用计算机自动化的路径。

Abstract: Autonomous agents that operate computers via Graphical User Interfaces (GUIs)
often struggle with efficiency and reliability on complex, long-horizon tasks.
While augmenting these agents with planners can improve task decomposition,
they remain constrained by the inherent limitations of performing all actions
through GUI manipulation, leading to brittleness and inefficiency. In this
work, we introduce a more robust and flexible paradigm: enabling agents to use
coding as a enhanced action. We present CoAct-1, a novel multi-agent system
that synergistically combines GUI-based control with direct programmatic
execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks
to either a conventional GUI Operator or a specialized Programmer agent, which
can write and execute Python or Bash scripts. This hybrid approach allows the
agent to bypass inefficient GUI action sequences for tasks like file management
and data processing, while still leveraging visual interaction when necessary.
We evaluate our system on the challenging OSWorld benchmark, where CoAct-1
achieves a new state-of-the-art success rate of 60.76%, significantly
outperforming prior methods. Furthermore, our approach dramatically improves
efficiency, reducing the average number of steps required to complete a task to
just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that
integrating coding as a core action provides a more powerful, efficient, and
scalable path toward generalized computer automation.

</details>


### [13] [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)
*Raymond Wilson,Cole Graham,Chase Carter,Zefeng Yang,Ruiqi Gu*

Main category: cs.CL

TL;DR: CAP-LLM is a new framework for personalized news headline generation that improves both personalization and factual consistency using a large language model.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with effectively capturing complex user interests and ensuring factual consistency, often leading to generic or misleading headlines.

Method: CAP-LLM is a novel framework that integrates user preferences and factual consistency constraints into a powerful pre-trained LLM backbone. It features a User Preference Encoder, a Context Injection Adapter, and a Fact-Consistency Reinforcement Module employing a novel contrastive loss.

Result: CAP-LLM achieves state-of-the-art performance across all metrics on the real-world PENS dataset. It significantly improves factual consistency (FactCC of 87.50) over strong baselines like BART (86.67), while simultaneously enhancing personalization and content coverage.

Conclusion: CAP-LLM demonstrates the ability to achieve a superior balance between personalization and factual accuracy in news headline generation.

Abstract: In the era of information overload, personalized news headline generation is
crucial for engaging users by tailoring content to their preferences while
accurately conveying news facts. Existing methods struggle with effectively
capturing complex user interests and ensuring factual consistency, often
leading to generic or misleading headlines. Leveraging the unprecedented
capabilities of Large Language Models (LLMs) in text generation, we propose
Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates
user preferences and factual consistency constraints into a powerful
pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture
long-term user interests, a Context Injection Adapter to seamlessly integrate
these preferences and current article context into the LLM's generation
process, and a Fact-Consistency Reinforcement Module employing a novel
contrastive loss to mitigate hallucination. Evaluated on the real-world PENS
dataset, CAP-LLM achieves state-of-the-art performance across all metrics.
Notably, it significantly improves factual consistency (FactCC of 87.50) over
strong baselines like BART (86.67), while simultaneously enhancing
personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1
26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,
and sensitivity analyses further validate the effectiveness of each component
and the robustness of our approach, demonstrating CAP-LLM's ability to achieve
a superior balance between personalization and factual accuracy in news
headline generation.

</details>


### [14] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
*Alok Abhishek,Lisa Erickson,Tushar Bandopadhyay*

Main category: cs.CL

TL;DR: 本文讨论了如何在机器学习模型的整个生命周期中系统地治理、评估和量化偏差，提出了一个数据和AI治理框架，以解决大型语言模型中的偏差、伦理、公平性和事实性问题，从而提高生成式人工智能系统的安全性和责任感。


<details>
  <summary>Details</summary>
Motivation: 为了在机器学习模型的整个生命周期中系统地治理、评估和量化偏差，从初始开发和验证到持续的生产监控和防护措施的实施。

Method: 基于我们在大型语言模型的偏差评估和测试套件（BEATS）上的基础工作，作者分享了大型语言模型（LLMs）中常见的偏差和公平相关差距，并讨论了数据和AI治理框架以解决LLMs中的偏差、伦理、公平性和事实性。

Result: 通过在整个AI开发生命周期中实施数据和AI治理，组织可以显著提高其GenAI系统的安全性和责任感，有效缓解歧视风险，并防止潜在的声誉或品牌相关损害。

Conclusion: 通过本文，我们旨在促进社会负责和伦理对齐的生成式人工智能驱动的应用程序的创建和部署。

Abstract: In this paper, we cover approaches to systematically govern, assess and
quantify bias across the complete life cycle of machine learning models, from
initial development and validation to ongoing production monitoring and
guardrail implementation. Building upon our foundational work on the Bias
Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the
authors share prevalent bias and fairness related gaps in Large Language Models
(LLMs) and discuss data and AI governance framework to address Bias, Ethics,
Fairness, and Factuality within LLMs. The data and AI governance approach
discussed in this paper is suitable for practical, real-world applications,
enabling rigorous benchmarking of LLMs prior to production deployment,
facilitating continuous real-time evaluation, and proactively governing LLM
generated responses. By implementing the data and AI governance across the life
cycle of AI development, organizations can significantly enhance the safety and
responsibility of their GenAI systems, effectively mitigating risks of
discrimination and protecting against potential reputational or brand-related
harm. Ultimately, through this article, we aim to contribute to advancement of
the creation and deployment of socially responsible and ethically aligned
generative artificial intelligence powered applications.

</details>


### [15] [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)
*Md Arafat Sultan,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 通过早期假设剪枝提高自一致性方法的token效率，同时保持并行性。


<details>
  <summary>Details</summary>
Motivation: 自一致性虽然简单有效，但高token消耗可能限制其实际应用。研究如何在保持并行性的同时，使自一致性在长链式推理任务中更高效。

Method: 通过早期假设剪枝使自一致性更高效，生成所有解决方案并行，但根据两个轻量级指标定期剪枝中间假设：(a) 模型对单个假设的信心，(b) 当前假设的词汇覆盖范围由考虑保留的候选子集覆盖。设计了一个快速加权集合覆盖算法。

Result: 评估五种LLM在三个数学基准上的结果表明，该方法可以提高所有模型的token效率，许多情况下提高了10-35%。

Conclusion: 该方法可以提高所有模型的token效率，许多情况下提高了10-35%。

Abstract: Despite its simplicity and efficacy, the high token expenditure of
self-consistency can limit its practical utility. Here we investigate if
self-consistency can be made more token-efficient for long chain-of-thought
reasoning tasks, while preserving its parallelism, through early hypothesis
pruning. Concretely, we generate all solutions in parallel, but periodically
prune intermediate hypotheses that are deemed unnecessary based on two
lightweight indicators: (a) the model's own confidence in individual
hypotheses, and (b) lexical coverage of all current hypotheses by candidate
subsets that are under consideration for continued retention. We design a fast
weighted set cover algorithm that utilizes the two indicators; our evaluation
of five LLMs on three math benchmarks shows that this method can improve token
efficiency for all models, by 10-35% in many cases.

</details>


### [16] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
*Bohan Jiang,Dawei Li,Zhen Tan,Chengshuai Zhao,Huan Liu*

Main category: cs.CL

TL;DR: 本研究构建了一个大规模数据集，并提出了一种基于原则的LLM作为评判者的评估框架。通过微调开源LLM，我们发现经过DPO和SFT微调的模型在生成解释方面表现更好，证明了基于偏好的学习在专门解释任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着个人越来越多地咨询大型语言模型（LLMs）来理解福祉，一个关键挑战是：LLMs能否生成不仅准确而且适合不同受众的解释？高质量的解释需要事实正确性以及满足不同专业知识用户期望的能力。

Method: 我们构建了一个包含43,880个解释的大规模数据集，涵盖了2,194个关于福祉的概念，并引入了一个基于原则的LLM作为评判者的评估框架。此外，我们展示了使用监督微调（SFT）和直接偏好优化（DPO）对开源LLM进行微调可以显著提高生成解释的质量。

Result: 我们的结果揭示了：(1) 所提出的LLM评判者与人类评估一致；(2) 解释质量在模型、受众和类别之间存在显著差异；(3) 经过DPO和SFT微调的模型优于其更大的对手，证明了基于偏好的学习在专门解释任务中的有效性。

Conclusion: 我们的研究结果表明，经过微调的LLM在生成解释方面表现出色，证明了基于偏好的学习在专门解释任务中的有效性。

Abstract: Well-being encompasses mental, physical, and social dimensions essential to
personal growth and informed life decisions. As individuals increasingly
consult Large Language Models (LLMs) to understand well-being, a key challenge
emerges: Can LLMs generate explanations that are not only accurate but also
tailored to diverse audiences? High-quality explanations require both factual
correctness and the ability to meet the expectations of users with varying
expertise. In this work, we construct a large-scale dataset comprising 43,880
explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We
introduce a principle-guided LLM-as-a-judge evaluation framework, employing
dual judges to assess explanation quality. Furthermore, we show that
fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) can significantly enhance the quality of
generated explanations. Our results reveal: (1) The proposed LLM judges align
well with human evaluations; (2) explanation quality varies significantly
across models, audiences, and categories; and (3) DPO- and SFT-finetuned models
outperform their larger counterparts, demonstrating the effectiveness of
preference-based learning for specialized explanation tasks.

</details>


### [17] [Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models](https://arxiv.org/abs/2508.03998)
*Xinyu Zhao,Zhen Tan,Maya Enisman,Minjae Seo,Marta R. Durantini,Dolores Albarracin,Tianlong Chen*

Main category: cs.CL

TL;DR: 本文介绍了一种社会机器人共同协调者，它分析多模式会议数据并向协调者提供隐秘提示。该机器人基于一个基于人类可解释概念的代理概念瓶颈模型，实现了跨组的稳健知识转移，并提高了新手的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型虽然强大但像一个“黑箱”，无法提供透明的建议，而小组会议中的协调者面临着挑战和认知负荷，需要一种能解释社会互动并保持对个体需求意识的具身技术。

Method: 我们提出了一种迁移学习框架，将基础模型的广泛社会理解提炼到专门且透明的概念瓶颈模型中。

Result: 该概念驱动的系统在预测干预需求方面显著优于直接的零样本基础模型，并允许实时的人类纠正其推理。

Conclusion: 我们的工作为在复杂社会领域增强人类能力提供了一个强大的蓝图。

Abstract: Successful group meetings, such as those implemented in group
behavioral-change programs, work meetings, and other social contexts, must
promote individual goal setting and execution while strengthening the social
relationships within the group. Consequently, an ideal facilitator must be
sensitive to the subtle dynamics of disengagement, difficulties with individual
goal setting and execution, and interpersonal difficulties that signal a need
for intervention. The challenges and cognitive load experienced by facilitators
create a critical gap for an embodied technology that can interpret social
exchanges while remaining aware of the needs of the individuals in the group
and providing transparent recommendations that go beyond powerful but "black
box" foundation models (FMs) that identify social cues. We address this
important demand with a social robot co-facilitator that analyzes multimodal
meeting data and provides discreet cues to the facilitator. The robot's
reasoning is powered by an agentic concept bottleneck model (CBM), which makes
decisions based on human-interpretable concepts like participant engagement and
sentiments, ensuring transparency and trustworthiness. Our core contribution is
a transfer learning framework that distills the broad social understanding of
an FM into our specialized and transparent CBM. This concept-driven system
significantly outperforms direct zero-shot FMs in predicting the need for
intervention and enables real-time human correction of its reasoning.
Critically, we demonstrate robust knowledge transfer: the model generalizes
across different groups and successfully transfers the expertise of senior
human facilitators to improve the performance of novices. By transferring an
expert's cognitive model into an interpretable robotic partner, our work
provides a powerful blueprint for augmenting human capabilities in complex
social domains.

</details>


### [18] [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)
*Yurun Chen,Xavier Hu,Yuhan Liu,Keting Yin,Juncheng Li,Zhuosheng Zhang,Shengyu Zhang*

Main category: cs.CL

TL;DR: HarmonyGuard is a multi-agent framework that enhances policy and optimizes objectives to improve safety and utility in web environments.


<details>
  <summary>Details</summary>
Motivation: Current research lacks the capability for collaborative optimization of both safety and utility in web environments.

Method: HarmonyGuard is a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety.

Result: HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks.

Conclusion: HarmonyGuard improves policy compliance and task completion while achieving high policy compliance across all tasks.

Abstract: Large language models enable agents to autonomously perform tasks in open web
environments. However, as hidden threats within the web evolve, web agents face
the challenge of balancing task performance with emerging risks during
long-sequence operations. Although this challenge is critical, current research
remains limited to single-objective optimization or single-turn scenarios,
lacking the capability for collaborative optimization of both safety and
utility in web environments. To address this gap, we propose HarmonyGuard, a
multi-agent collaborative framework that leverages policy enhancement and
objective optimization to jointly improve both utility and safety. HarmonyGuard
features a multi-agent architecture characterized by two fundamental
capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent
within HarmonyGuard, which automatically extracts and maintains structured
security policies from unstructured external documents, while continuously
updating policies in response to evolving threats. (2) Dual-Objective
Optimization: Based on the dual objectives of safety and utility, the Utility
Agent integrated within HarmonyGuard performs the Markovian real-time reasoning
to evaluate the objectives and utilizes metacognitive capabilities for their
optimization. Extensive evaluations on multiple benchmarks show that
HarmonyGuard improves policy compliance by up to 38% and task completion by up
to 20% over existing baselines, while achieving over 90% policy compliance
across all tasks. Our project is available here:
https://github.com/YurunChen/HarmonyGuard.

</details>


### [19] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
*Xiaopeng Li,Shasha Li,Xi Wang,Shezheng Song,Bin Ji,Shangwen Wang,Jun Ma,Xiaodong Liu,Mina Liu,Jie Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于元学习的模型编辑方法SMEdit，通过引入多步反向传播和权重更新的范数正则化，提高了在有限监督下的编辑性能和训练效率。实验结果表明，SMEdit优于现有的MLBME基线，并且MBPS策略可以无缝集成到现有方法中以进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: To address the suboptimal performance of MLBME in low-data scenarios and the computation bottleneck of KL divergence in training efficiency.

Method: SMEdit, which adopts Multiple Backpropagation Steps (MBPS) to improve editing performance under limited supervision and a norm regularization on weight updates to improve training efficiency.

Result: Experimental results on two datasets and two LLMs demonstrate that SMEdit outperforms prior MLBME baselines and the MBPS strategy can be seamlessly integrated into existing methods to further boost their performance.

Conclusion: SMEdit outperforms prior MLBME baselines and the MBPS strategy can be seamlessly integrated into existing methods to further boost their performance.

Abstract: Large Language Models (LLMs) underpin many AI applications, but their static
nature makes updating knowledge costly. Model editing offers an efficient
alternative by injecting new information through targeted parameter
modifications. In particular, meta-learning-based model editing (MLBME) methods
have demonstrated notable advantages in both editing effectiveness and
efficiency. Despite this, we find that MLBME exhibits suboptimal performance in
low-data scenarios, and its training efficiency is bottlenecked by the
computation of KL divergence. To address these, we propose $\textbf{S}$tep
$\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that
adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation
$\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited
supervision and a norm regularization on weight updates to improve training
efficiency. Experimental results on two datasets and two LLMs demonstrate that
SMEdit outperforms prior MLBME baselines and the MBPS strategy can be
seamlessly integrated into existing methods to further boost their performance.
Our code will be released soon.

</details>


### [20] [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)
*Zechen Li,Baiyu Chen,Hao Xue,Flora D. Salim*

Main category: cs.CL

TL;DR: ZARA 是一种新的基于代理的框架，用于直接从原始运动时间序列进行零样本、可解释的人类活动识别（HAR）。它在多个基准测试中表现出色，并提供了清晰的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在固定活动集上进行训练，当出现新行为或传感器设置时需要昂贵的重新训练。最近尝试使用大型语言模型（LLMs）进行HAR，通常通过将信号转换为文本或图像，但存在准确性有限和缺乏可验证的可解释性问题。

Method: ZARA 是一个基于代理的框架，直接从原始运动时间序列进行零样本、可解释的人类活动识别（HAR）。它集成了一个自动派生的成对特征知识库、一个多传感器检索模块和一个分层代理管道。

Result: ZARA 在8个HAR基准测试中实现了最先进的零样本性能，在宏F1方面超过了最强基线2.53倍。消融研究进一步证实了每个模块的必要性。

Conclusion: ZARA 是一个有前途的步骤，朝着可信赖、即插即用的运动时间序列分析迈进。

Abstract: Motion sensor time-series are central to human activity recognition (HAR),
with applications in health, sports, and smart devices. However, existing
methods are trained for fixed activity sets and require costly retraining when
new behaviours or sensor setups appear. Recent attempts to use large language
models (LLMs) for HAR, typically by converting signals into text or images,
suffer from limited accuracy and lack verifiable interpretability. We propose
ZARA, the first agent-based framework for zero-shot, explainable HAR directly
from raw motion time-series. ZARA integrates an automatically derived pair-wise
feature knowledge base that captures discriminative statistics for every
activity pair, a multi-sensor retrieval module that surfaces relevant evidence,
and a hierarchical agent pipeline that guides the LLM to iteratively select
features, draw on this evidence, and produce both activity predictions and
natural-language explanations. ZARA enables flexible and interpretable HAR
without any fine-tuning or task-specific classifiers. Extensive experiments on
8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering
clear reasoning while exceeding the strongest baselines by 2.53x in macro F1.
Ablation studies further confirm the necessity of each module, marking ZARA as
a promising step toward trustworthy, plug-and-play motion time-series analysis.
Our codes are available at https://github.com/zechenli03/ZARA.

</details>


### [21] [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)
*Thilo Hagendorff,Erik Derner,Nuria Oliver*

Main category: cs.CL

TL;DR: 本研究显示大型推理模型能够轻松越狱AI模型，表明需要加强前沿模型的对齐以防止被利用为越狱代理。


<details>
  <summary>Details</summary>
Motivation: 传统上，越狱需要复杂的工程技术或专业的人类专业知识，而本研究展示了大型推理模型的说服能力简化并扩展了越狱，使其成为非专家可以负担得起的活动。

Method: 我们评估了四个大型推理模型（DeepSeek-R1、Gemini 2.5 Flash、Grok 3 Mini、Qwen3 235B）作为自主对手进行多轮对话的能力，并通过系统提示接收指令，然后在没有进一步监督的情况下规划和执行越狱。

Result: 我们在一个包含70个项目的有害提示基准上进行了广泛的实验，这种设置在所有模型组合中产生了97.14%的整体攻击成功率。

Conclusion: 我们的研究揭示了一种对齐退化现象，即大型推理模型可以系统地削弱其他模型的安全防护机制，这突显了进一步对齐前沿模型的紧迫性，不仅是为了抵抗越狱尝试，还要防止它们被利用为越狱代理。

Abstract: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has
traditionally required complex technical procedures or specialized human
expertise. In this study, we show that the persuasive capabilities of large
reasoning models (LRMs) simplify and scale jailbreaking, converting it into an
inexpensive activity accessible to non-experts. We evaluated the capabilities
of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as
autonomous adversaries conducting multi-turn conversations with nine widely
used target models. LRMs received instructions via a system prompt, before
proceeding to planning and executing jailbreaks with no further supervision. We
performed extensive experiments with a benchmark of harmful prompts composed of
70 items covering seven sensitive domains. This setup yielded an overall attack
success rate across all model combinations of 97.14%. Our study reveals an
alignment regression, in which LRMs can systematically erode the safety
guardrails of other models, highlighting the urgent need to further align
frontier models not only to resist jailbreak attempts, but also to prevent them
from being co-opted into acting as jailbreak agents.

</details>


### [22] [DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation](https://arxiv.org/abs/2508.04047)
*Jiabing Yang,Yixiang Chen,Zichen Wen,Chenhang Cui,Peiyan Li,Yuan Xu,Bowen Fang,Yan Huang,Liang Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为动态令牌级前缀增强（DTPA）的轻量级有效框架，用于可控文本生成。DTPA通过动态增强对前缀的注意力来提高可控性，并在长文本生成中表现出优越效果。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要关注短序列的可控文本生成质量，而长文本生成仍然很大程度上未被探索。我们观察到，基于Air-Decoding的强大前缀方法生成的文本的可控性会随着序列长度的增加而下降，这主要是由于对前缀的关注度下降。不同类型的前缀包括软前缀和硬前缀也是影响性能的关键因素。

Method: 我们提出了一个轻量级且有效的框架，称为动态令牌级前缀增强（DTPA），基于Air-Decoding进行可控文本生成。首先，它为给定任务选择最佳的前缀类型。然后，我们动态地增强对前缀的注意力以提高可控性，随着序列长度的增加，缩放因子呈指数增长。此外，根据任务，我们可以对原始提示应用类似的增强以平衡文本质量。

Result: 实验表明，DTPA在多个CTG任务中通常优于其他方法，在属性控制方面表现出色，同时保持了具有竞争力的流畅性、多样性和主题相关性。进一步的分析突显了DTPA在长文本生成中的优越效果。

Conclusion: 实验表明，DTPA在属性控制方面通常优于其他方法，同时保持了具有竞争力的流畅性、多样性和主题相关性。进一步的分析突显了DTPA在长文本生成中的优越效果。

Abstract: Controllable Text Generation (CTG) is a vital subfield in Natural Language
Processing (NLP), aiming to generate text that aligns with desired attributes.
However, previous studies commonly focus on the quality of controllable text
generation for short sequences, while the generation of long-form text remains
largely underexplored. In this paper, we observe that the controllability of
texts generated by the powerful prefix-based method Air-Decoding tends to
decline with increasing sequence length, which we hypothesize primarily arises
from the observed decay in attention to the prefixes. Meanwhile, different
types of prefixes including soft and hard prefixes are also key factors
influencing performance. Building on these insights, we propose a lightweight
and effective framework called Dynamic Token-level Prefix Augmentation (DTPA)
based on Air-Decoding for controllable text generation. Specifically, it first
selects the optimal prefix type for a given task. Then we dynamically amplify
the attention to the prefix for the attribute distribution to enhance
controllability, with a scaling factor growing exponentially as the sequence
length increases. Moreover, based on the task, we optionally apply a similar
augmentation to the original prompt for the raw distribution to balance text
quality. After attribute distribution reconstruction, the generated text
satisfies the attribute constraints well. Experiments on multiple CTG tasks
demonstrate that DTPA generally outperforms other methods in attribute control
while maintaining competitive fluency, diversity, and topic relevance. Further
analysis highlights DTPA's superior effectiveness in long text generation.

</details>


### [23] [PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG](https://arxiv.org/abs/2508.04057)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CL

TL;DR: PAIRS是一种无需训练的框架，结合参数知识和检索知识，自适应地决定是否检索和如何选择外部信息。它通过双路径生成机制提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统存在两个关键限制：(1) 对每个查询低效地检索信息，包括仅需LLM参数知识即可解决的简单问题；(2) 当查询包含稀疏信息信号时，可能检索不相关的文档。

Method: PAIRS采用双路径生成机制，首先让LLM生成直接答案和使用自生成伪上下文的上下文增强答案。当这些输出收敛时，PAIRS完全绕过外部检索。对于发散情况，PAIRS通过原始查询和自生成的上下文信号引导双路径检索（DPR）过程，然后通过加权相似性对文档进行过滤。

Result: 在六个问答（QA）基准测试中，PAIRS将检索成本降低了约25%（仅触发75%的查询），同时提高了准确性，在平均EM和F1上分别比之前的基线高出+1.1%和+1.0%。

Conclusion: PAIRS通过消除不必要的检索提高了效率，并通过上下文引导的检索和自适应信息选择提高了准确性。

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone technique for
enhancing large language models (LLMs) with external knowledge. However,
current RAG systems face two critical limitations: (1) they inefficiently
retrieve information for every query, including simple questions that could be
resolved using the LLM's parametric knowledge alone, and (2) they risk
retrieving irrelevant documents when queries contain sparse information
signals. To address these gaps, we introduce Parametric-verified Adaptive
Information Retrieval and Selection (PAIRS), a training-free framework that
integrates parametric and retrieved knowledge to adaptively determine whether
to retrieve and how to select external information. Specifically, PAIRS employs
a dual-path generation mechanism: First, the LLM produces both a direct answer
and a context-augmented answer using self-generated pseudo-context. When these
outputs converge, PAIRS bypasses external retrieval entirely, dramatically
improving the RAG system's efficiency. For divergent cases, PAIRS activates a
dual-path retrieval (DPR) process guided by both the original query and
self-generated contextual signals, followed by an Adaptive Information
Selection (AIS) module that filters documents through weighted similarity to
both sources. This simple yet effective approach can not only enhance
efficiency by eliminating unnecessary retrievals but also improve accuracy
through contextually guided retrieval and adaptive information selection.
Experimental results on six question-answering (QA) benchmarks show that PAIRS
reduces retrieval costs by around 25% (triggering for only 75% of queries)
while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior
baselines on average.

</details>


### [24] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
*Julián Camilo Velandia Gutiérrez*

Main category: cs.CL

TL;DR: 本文研究了如何通过数据处理、训练策略和架构调整来提高大型语言模型在资源受限环境中的效率，并验证了所提方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的大规模部署受到计算资源需求的限制，因此需要一种更高效的方法来优化其性能。

Method: 本文的方法包括定义构建可靠数据集的标准，进行不同配置的受控实验，并系统地评估结果变体在能力、多样性、响应时间和安全性方面的表现。

Result: 本文通过比较测试验证了所提出策略的有效性，并开发了高效的模型变体。

Conclusion: 本文提出了一种改进大型语言模型在资源受限环境和有限知识库中效率的方法，通过数据处理、数据选择、训练策略和架构调整来实现。

Abstract: Large Language Models (LLMs) have become a milestone in the field of
artificial intelligence and natural language processing. However, their
large-scale deployment remains constrained by the need for significant
computational resources. This work proposes starting from a base model to
explore and combine data processing and careful data selection techniques,
training strategies, and architectural adjustments to improve the efficiency of
LLMs in resource-constrained environments and within a delimited knowledge
base. The methodological approach included defining criteria for building
reliable datasets, conducting controlled experiments with different
configurations, and systematically evaluating the resulting variants in terms
of capability, versatility, response time, and safety. Finally, comparative
tests were conducted to measure the performance of the developed variants and
to validate the effectiveness of the proposed strategies. This work is based on
the master's thesis in Systems and Computer Engineering titled "Efficient
Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [25] [ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"](https://arxiv.org/abs/2508.04086)
*Zhongyi Zhou,Kohei Uehara,Haoyu Zhang,Jingtao Zhou,Lin Gu,Ruofei Du,Zheng Xu,Tatsuya Harada*

Main category: cs.CL

TL;DR: ToolGrad是一种新的框架，通过反向方法生成工具使用数据集，提高了效率和质量。


<details>
  <summary>Details</summary>
Motivation: 之前的工作在生成工具使用LLM数据集时，先生成用户查询，然后进行复杂的工具使用注释，导致不可避免的注释失败和低效的数据生成。

Method: ToolGrad是一个代理框架，通过迭代过程构建有效的工具使用链，并通过文本“梯度”进行引导，然后合成相应的用户查询。

Result: ToolGrad-5k数据集具有更复杂的工具使用、更低的成本和100%的通过率。

Conclusion: 模型在ToolGrad-5k上训练的表现优于昂贵的基线数据集和专有LLM，甚至在OOD基准测试中也表现更好。

Abstract: Prior work synthesizes tool-use LLM datasets by first generating a user
query, followed by complex tool-use annotations like DFS. This leads to
inevitable annotation failures and low efficiency in data generation. We
introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad
first constructs valid tool-use chains through an iterative process guided by
textual "gradients", and then synthesizes corresponding user queries. This
"answer-first" approach led to ToolGrad-5k, a dataset generated with more
complex tool use, lower cost, and 100% pass rate. Experiments show that models
trained on ToolGrad-5k outperform those on expensive baseline datasets and
proprietary LLMs, even on OOD benchmarks.

</details>


### [26] [GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2508.04088)
*Jianghangfan Zhang,Yibo Yan,Kening Zheng,Xin Zou,Song Dai,Xuming Hu*

Main category: cs.CL

TL;DR: The paper introduces GM-PRM, a novel approach that enhances multimodal large language models by providing fine-grained, interpretable analysis of each reasoning step and generating corrections for errors, leading to improved performance on multimodal math tasks.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models (MLLMs) often struggle with complex, multi-step mathematical reasoning, where minor errors in visual perception or logical deduction can lead to complete failure. Existing multimodal PRMs are limited to being binary verifiers that can identify but not correct errors, offering little explanatory power.

Method: We introduce the Generative Multimodal Process Reward Model (GM-PRM), which transforms the PRM from a passive judge into an active reasoning collaborator. GM-PRM provides a fine-grained, interpretable analysis of each reasoning step and is trained to generate a corrected version of the first erroneous step it identifies.

Result: GM-PRM achieves state-of-the-art results on multiple multimodal math benchmarks, significantly boosting policy model performance with remarkable data efficiency, requiring only a 20K-sample training dataset.

Conclusion: GM-PRM achieves state-of-the-art results on multiple multimodal math benchmarks, significantly boosting policy model performance with remarkable data efficiency, requiring only a 20K-sample training dataset.

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities
but often struggle with complex, multi-step mathematical reasoning, where minor
errors in visual perception or logical deduction can lead to complete failure.
While Process Reward Models (PRMs) offer step-by-step supervision, existing
multimodal PRMs are limited to being binary verifiers that can identify but not
correct errors, offering little explanatory power. To address these
deficiencies, we introduce the Generative Multimodal Process Reward Model
(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an
active reasoning collaborator. Instead of a simple scalar score, GM-PRM
provides a fine-grained, interpretable analysis of each reasoning step,
evaluating its step intent, visual alignment, and logical soundness. More
critically, GM-PRM is trained to generate a corrected version of the first
erroneous step it identifies. This unique corrective capability enables our new
test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework
actively enhances solution quality by using the PRM's generated correction to
guide the policy model toward a more promising reasoning trajectory, thereby
improving the diversity and correctness of the solution pool. We demonstrate
that GM-PRM achieves state-of-the-art results on multiple multimodal math
benchmarks, significantly boosting policy model performance with remarkable
data efficiency, requiring only a 20K-sample training dataset. Our code will be
released upon acceptance.

</details>


### [27] [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)
*Zhiwen Ruan,Yun Chen,Yutao Hou,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 本研究探讨了LLM微调过程中的学习动态，揭示了过度记忆现象，并指出过度记忆会导致模型鲁棒性和泛化能力下降。


<details>
  <summary>Details</summary>
Motivation: 尽管具有过度记忆的模型在测试准确性上与正常模型相当，但它们在鲁棒性、分布外泛化和生成多样性方面表现较差。我们的实验揭示了过度记忆在不同任务、模型和微调方法中是普遍适用的。

Method: 我们研究了LLM在推理任务上的微调学习动态，并揭示了在LLM微调的特定阶段出现的未被发现的过度记忆现象。我们调查了导致LLM过度记忆的条件，并发现训练轮数和大的学习率对此问题有贡献。

Result: 我们发现，在特定阶段，LLMs过度记忆了训练数据，并表现出高的测试困惑度，同时保持良好的测试准确性。

Conclusion: 我们的研究强调了过度参数化且广泛微调的LLM表现出与传统机器学习模型不同的独特学习动态。基于对过度记忆的观察，我们提供了在微调期间检查点和学习率选择的建议。

Abstract: The pretrained large language models (LLMs) are finetuned with labeled data
for better instruction following ability and alignment with human values. In
this paper, we study the learning dynamics of LLM finetuning on reasoning tasks
and reveal the uncovered over-memorization phenomenon during a specific stage
of LLM finetuning. At this stage, the LLMs have excessively memorized training
data and exhibit high test perplexity while maintaining good test accuracy. We
investigate the conditions that lead to LLM over-memorization and find that
training epochs and large learning rates contribute to this issue. Although
models with over-memorization demonstrate comparable test accuracy to normal
models, they suffer from reduced robustness, poor out-of-distribution
generalization, and decreased generation diversity. Our experiments unveil the
over-memorization to be broadly applicable across different tasks, models, and
finetuning methods. Our research highlights that overparameterized, extensively
finetuned LLMs exhibit unique learning dynamics distinct from traditional
machine learning models. Based on our observations of over-memorization, we
provide recommendations on checkpoint and learning rate selection during
finetuning.

</details>


### [28] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
*Xuan Qi,Rongwu Xu,Zhijing Jin*

Main category: cs.CL

TL;DR: 本文提出了一种基于难度的数据选择策略，以提高大语言模型与人类偏好的对齐效果，并在减少数据量的情况下实现了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对偏好数据的高质量数据选择方法，而现有的RLHF和DPO方法通常依赖于大量且昂贵的偏好数据集。因此，需要一种更高效的数据选择策略来改善模型对齐效果。

Method: 本文引入了一种基于难度的数据选择策略，该策略基于DPO隐式奖励机制，通过选择具有较小DPO隐式奖励差距的偏好数据示例来提高数据效率和模型对齐效果。

Result: 本文的方法在多个数据集和对齐任务中均优于五个强基线方法，在仅使用原始数据的10%的情况下实现了优越的性能。

Conclusion: 本文提出了一种基于难度的数据选择策略，用于改进大语言模型与人类偏好的对齐。该方法在减少数据量的情况下实现了优越的性能，为有限资源下的模型对齐提供了有前景的解决方案。

Abstract: Aligning large language models (LLMs) with human preferences is a critical
challenge in AI research. While methods like Reinforcement Learning from Human
Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they
often rely on large, costly preference datasets. The current work lacks methods
for high-quality data selection specifically for preference data. In this work,
we introduce a novel difficulty-based data selection strategy for preference
datasets, grounded in the DPO implicit reward mechanism. By selecting
preference data examples with smaller DPO implicit reward gaps, which are
indicative of more challenging cases, we improve data efficiency and model
alignment. Our approach consistently outperforms five strong baselines across
multiple datasets and alignment tasks, achieving superior performance with only
10\% of the original data. This principled, efficient selection method offers a
promising solution for scaling LLM alignment with limited resources.

</details>


### [29] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
*Praveen Srinivasa Varadhan,Sherry Thomas,Sai Teja M. S.,Suvrat Bhooshan,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 本文介绍了人类欺骗率（HFR）作为衡量机器生成语音被误认为人类语音频率的指标，并指出当前TTS系统在欺骗测试中存在不足，需要更现实、以人类为中心的评估。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来的主观评估表明TTS取得了快速进展，但当前的TTS系统是否真的能在图灵测试中通过人类欺骗测试？

Method: 我们引入了人类欺骗率（HFR），这是一种直接测量机器生成语音被误认为人类语音的频率的指标。

Result: 大规模评估开放源代码和商业TTS模型揭示了关键见解：(i) 基于CMOS的人类对等性声明在欺骗测试中常常失败；(ii) TTS进展应在人类语音达到高HFR的数据集上进行基准测试；(iii) 商业模型在零样本设置中接近人类欺骗；(iv) 在高质量数据上的微调提高了真实感，但并未完全弥合差距。

Conclusion: 我们的研究强调了需要更现实、以人类为中心的评估，同时进行现有的主观测试。

Abstract: While subjective evaluations in recent years indicate rapid progress in TTS,
can current TTS systems truly pass a human deception test in a Turing-like
evaluation? We introduce Human Fooling Rate (HFR), a metric that directly
measures how often machine-generated speech is mistaken for human. Our
large-scale evaluation of open-source and commercial TTS models reveals
critical insights: (i) CMOS-based claims of human parity often fail under
deception testing, (ii) TTS progress should be benchmarked on datasets where
human speech achieves high HFRs, as evaluating against monotonous or less
expressive reference samples sets a low bar, (iii) Commercial models approach
human deception in zero-shot settings, while open-source systems still struggle
with natural conversational speech; (iv) Fine-tuning on high-quality data
improves realism but does not fully bridge the gap. Our findings underscore the
need for more realistic, human-centric evaluations alongside existing
subjective tests.

</details>


### [30] [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)
*Peizheng Guo,Jingyao Wang,Wenwen Qiang,Huijie Guo,Changwen Zheng,Jiahuan Zhou,Gang Hua*

Main category: cs.CL

TL;DR: 本文提出了一种基于因果完备性的强化学习框架，以解决多模态大语言模型中的幻觉问题。通过评估每个标记的独立贡献和反事实不可或缺性，定义了一个标记级的因果完备性奖励，并将其用于构建一个因果信息优势函数，从而鼓励模型关注对准确生成既具有因果充分性又具有必要性的标记。实验结果表明，该方法在各种基准数据集和任务中表现出色，有效缓解了MLLM中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在跨视觉-语言任务中表现出色，但可能会出现幻觉——生成与输入图像或文本语义不一致的输出。通过因果分析，我们发现：(i) 由于未能充分捕捉关键因果因素，可能会出现遗漏型幻觉；(ii) 由于模型被非因果线索误导，可能会出现虚构型幻觉。

Method: 我们提出了一种基于因果完备性的强化学习框架，联合考虑了标记的因果充分性和必要性。具体来说，我们评估每个标记的独立贡献和反事实不可或缺性，以定义一个标记级的因果完备性奖励。这个奖励用于在GRPO优化框架内构建一个因果信息优势函数，鼓励模型关注对准确生成既具有因果充分性又具有必要性的标记。

Result: 在各种基准数据集和任务上的实验结果表明，我们的方法有效缓解了MLLM中的幻觉问题。

Conclusion: 我们的方法在各种基准数据集和任务中表现出色，有效缓解了MLLM中的幻觉问题。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across vision-language tasks. However, they may suffer from
hallucinations--generating outputs that are semantically inconsistent with the
input image or text. Through causal analyses, we find that: (i) hallucinations
with omission may arise from the failure to adequately capture essential causal
factors, and (ii) hallucinations with fabrication are likely caused by the
model being misled by non-causal cues. To address these challenges, we propose
a novel reinforcement learning framework guided by causal completeness, which
jointly considers both causal sufficiency and causal necessity of tokens.
Specifically, we evaluate each token's standalone contribution and
counterfactual indispensability to define a token-level causal completeness
reward. This reward is used to construct a causally informed advantage function
within the GRPO optimization framework, encouraging the model to focus on
tokens that are both causally sufficient and necessary for accurate generation.
Experimental results across various benchmark datasets and tasks demonstrate
the effectiveness of our approach, which effectively mitigates hallucinations
in MLLMs.

</details>


### [31] [Characterizing Deep Research: A Benchmark and Formal Definition](https://arxiv.org/abs/2508.04183)
*Abhinav Java,Ashmit Khandelwal,Sukruta Midigeshi,Aaron Halfaker,Amit Deshpande,Navin Goyal,Ankur Gupta,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 本文提出了一种深度研究任务的正式表征，并引入了一个基准以评估DR系统的性能。


<details>
  <summary>Details</summary>
Motivation: 目前深度研究任务的范围仍未明确定义，其与其他推理密集型问题的区别也不清楚。

Method: 本文通过中间输出表示定义了深度研究任务，该表示编码了搜索过程中发现的关键主张，将推理挑战与表面级报告生成分离。

Result: 在最先进的DR系统中，任何子类别的F1分数都在0.02到0.72之间。OpenAI的模型表现最好，总体F1得分为0.55。

Conclusion: 本文提出了一个深度研究任务的正式表征，并引入了一个基准来评估DR系统的性能。

Abstract: Information tasks such as writing surveys or analytical reports require
complex search and reasoning, and have recently been grouped under the umbrella
of \textit{deep research} -- a term also adopted by recent models targeting
these capabilities. Despite growing interest, the scope of the deep research
task remains underdefined and its distinction from other reasoning-intensive
problems is poorly understood. In this paper, we propose a formal
characterization of the deep research (DR) task and introduce a benchmark to
evaluate the performance of DR systems. We argue that the core defining feature
of deep research is not the production of lengthy report-style outputs, but
rather the high fan-out over concepts required during the search process, i.e.,
broad and reasoning-intensive exploration. To enable objective evaluation, we
define DR using an intermediate output representation that encodes key claims
uncovered during search-separating the reasoning challenge from surface-level
report generation. Based on this formulation, we propose a diverse, challenging
benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,
datasets, materials discovery, prior art search) and public interest events
(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1
score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model
performs the best with an overall F1 score of 0.55. Analysis of reasoning
traces reveals the distribution over the number of referenced sources,
branching, and backtracking events executed by current DR systems, motivating
future directions for improving their search mechanisms and grounding
capabilities. The benchmark is available at
https://github.com/microsoft/LiveDRBench.

</details>


### [32] [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)
*Siddhant Panpatil,Hiskias Dingeto,Haon Park*

Main category: cs.CL

TL;DR: 本文展示了当前语言模型在特定对话场景下容易产生不对齐行为，并提出了一个评估框架来测试这种脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管对齐技术取得了显著进展，但状态最先进的语言模型仍然容易受到精心设计的对话场景的影响，这些场景可以引发各种形式的不对齐而无需明确的越狱。

Method: 通过与Claude-4-Opus的系统性手动红队测试，发现了10种成功的攻击场景，并将这些攻击提炼为MISALIGNMENTBENCH自动化评估框架，以验证其可推广性。

Result: 在五个前沿LLM上对10个场景进行跨模型评估，总体漏洞率为76%，其中GPT-4.1表现出最高的易受性（90%），而Claude-4-Sonnet表现出更大的抵抗力（40%）。

Conclusion: 本文揭示了当前对齐方法在处理叙述沉浸、情感压力和战略框架方面的根本性漏洞，并强调了未来AI系统需要增强对微妙的基于场景的操纵的鲁棒性。

Abstract: Despite significant advances in alignment techniques, we demonstrate that
state-of-the-art language models remain vulnerable to carefully crafted
conversational scenarios that can induce various forms of misalignment without
explicit jailbreaking. Through systematic manual red-teaming with
Claude-4-Opus, we discovered 10 successful attack scenarios, revealing
fundamental vulnerabilities in how current alignment methods handle narrative
immersion, emotional pressure, and strategic framing. These scenarios
successfully elicited a range of misaligned behaviors, including deception,
value drift, self-preservation, and manipulative reasoning, each exploiting
different psychological and contextual vulnerabilities. To validate
generalizability, we distilled our successful manual attacks into
MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible
testing across multiple models. Cross-model evaluation of our 10 scenarios
against five frontier LLMs revealed an overall 76% vulnerability rate, with
significant variations: GPT-4.1 showed the highest susceptibility (90%), while
Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate
that sophisticated reasoning capabilities often become attack vectors rather
than protective mechanisms, as models can be manipulated into complex
justifications for misaligned behavior. This work provides (i) a detailed
taxonomy of conversational manipulation patterns and (ii) a reusable evaluation
framework. Together, these findings expose critical gaps in current alignment
strategies and highlight the need for robustness against subtle, scenario-based
manipulation in future AI systems.

</details>


### [33] [Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts](https://arxiv.org/abs/2508.04199)
*Millicent Ochieng,Anja Thieme,Ignatius Ezeani,Risa Ueno,Samuel Maina,Keshet Ronen,Javier Gonzalez,Jacki O'Neill*

Main category: cs.CL

TL;DR: 本文提出了一种诊断框架，用于评估大型语言模型在文化细微差别和低资源情境下的情感推理能力，并发现顶级模型表现较好，而开放模型在面对模糊或情感变化时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 在低资源、文化细微差别的情境中进行情感分析挑战了传统NLP方法假设固定标签和普遍情感表达。

Method: 本文提出了一种诊断框架，将情感视为一种依赖于上下文和文化的构建，并通过结合人工标注数据、情感翻转的反事实和基于评分标准的解释评估来评估大型语言模型（LLMs）在非正式、混合代码的WhatsApp消息中的情感推理。

Result: 研究发现模型推理质量存在显著差异，顶级LLMs表现出解释稳定性，而开放模型在模糊或情感变化时常常失败。

Conclusion: 本文强调了在复杂、现实的交流中，需要文化敏感性和推理意识的AI评估。

Abstract: Sentiment analysis in low-resource, culturally nuanced contexts challenges
conventional NLP approaches that assume fixed labels and universal affective
expressions. We present a diagnostic framework that treats sentiment as a
context-dependent, culturally embedded construct, and evaluate how large
language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp
messages from Nairobi youth health groups. Using a combination of
human-annotated data, sentiment-flipped counterfactuals, and rubric-based
explanation evaluation, we probe LLM interpretability, robustness, and
alignment with human reasoning. Framing our evaluation through a social-science
measurement lens, we operationalize and interrogate LLMs outputs as an
instrument for measuring the abstract concept of sentiment. Our findings reveal
significant variation in model reasoning quality, with top-tier LLMs
demonstrating interpretive stability, while open models often falter under
ambiguity or sentiment shifts. This work highlights the need for culturally
sensitive, reasoning-aware AI evaluation in complex, real-world communication.

</details>


### [34] [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)
*Yuquan Wang,Mi Zhang,Yining Wang,Geng Hong,Xiaoyu You,Min Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为 ReasoningGuard 的推理安全防护机制，能够在不增加太多额外推理成本的情况下，有效缓解三种类型的越狱攻击，并优于现有的七种防护方法。


<details>
  <summary>Details</summary>
Motivation: 现有的防御机制依赖于昂贵的微调和额外的专业知识，限制了其可扩展性。因此，需要一种更高效、可扩展的推理安全防护方法。

Method: ReasoningGuard 通过利用模型的内部注意力行为，准确识别推理路径中的关键点，并触发自发的安全反思。此外，在解码阶段实施了缩放采样策略，以选择最佳的推理路径。

Result: ReasoningGuard 在不增加太多额外推理成本的情况下，有效缓解了三种类型的越狱攻击，包括针对大型推理模型推理过程的最新攻击，并优于现有的七种防护方法。

Conclusion: ReasoningGuard 是一种有效的推理安全防护机制，能够在不增加太多额外推理成本的情况下，有效缓解三种类型的越狱攻击，并优于现有的七种防护方法。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance in
reasoning-intensive tasks, but they remain vulnerable to harmful content
generation, particularly in the mid-to-late steps of their reasoning processes.
Existing defense mechanisms, however, rely on costly fine-tuning and additional
expert knowledge, which restricts their scalability. In this work, we propose
ReasoningGuard, an inference-time safeguard for LRMs, which injects timely
safety aha moments to steer harmless while helpful reasoning processes.
Leveraging the model's internal attention behavior, our approach accurately
identifies critical points in the reasoning path, and triggers spontaneous,
safety-oriented reflection. To safeguard both the subsequent reasoning steps
and the final answers, we further implement a scaling sampling strategy during
the decoding phase, selecting the optimal reasoning path. Inducing minimal
extra inference cost, ReasoningGuard effectively mitigates three types of
jailbreak attacks, including the latest ones targeting the reasoning process of
LRMs. Our approach outperforms seven existing safeguards, achieving
state-of-the-art safety defenses while effectively avoiding the common
exaggerated safety issues.

</details>


### [35] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
*Kosuke Yoshimura,Hisashi Kashima*

Main category: cs.CL

TL;DR: 本研究评估了使用黑盒大型语言模型（LLMs）进行层级文本分类（HTC）的可行性，并比较了三种提示策略的准确性和成本效益。结果显示，少样本设置提高了分类准确性，而LLMs在深层层次数据集上表现优于传统机器学习模型，但API成本较高。


<details>
  <summary>Details</summary>
Motivation: 层级文本分类（HTC）旨在将文本分配到结构化的标签层次；然而，由于数据稀缺和模型复杂性，它面临挑战。传统机器学习方法需要大量标记数据和计算资源，因此本研究探索了使用黑盒大型语言模型（LLMs）作为替代方案的可行性。

Method: 本研究探讨了使用通过API访问的黑盒大型语言模型（LLMs）进行层级文本分类（HTC）的可行性，作为传统机器学习方法的替代方案。评估了三种提示策略——直接叶标签预测（DL）、直接层级标签预测（DH）和自顶向下多步骤层级标签预测（TMH），并在零样本和少样本设置中进行了比较，比较了这些策略的准确性和成本效益。

Result: 实验表明，在少样本设置下，分类准确性始终优于零样本设置。虽然传统机器学习模型在具有浅层层次的数据集上表现出高准确性，但LLMs，尤其是DH策略，在具有更深层次层次的数据集上表现优于机器学习模型。由于DH策略需要更深的标签层次，API成本显著增加。

Conclusion: 这些结果强调了准确率提升与提示策略计算成本之间的权衡。这些发现突显了黑盒大型语言模型在HTC中的潜力，同时强调了需要仔细选择提示策略以平衡性能和成本。

Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured
label hierarchies; however, it faces challenges due to data scarcity and model
complexity. This study explores the feasibility of using black box Large
Language Models (LLMs) accessed via APIs for HTC, as an alternative to
traditional machine learning methods that require extensive labeled data and
computational resources. We evaluate three prompting strategies -- Direct Leaf
Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down
Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and
few-shot settings, comparing the accuracy and cost-effectiveness of these
strategies. Experiments on two datasets show that a few-shot setting
consistently improves classification accuracy compared to a zero-shot setting.
While a traditional machine learning model achieves high accuracy on a dataset
with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the
machine learning model on a dataset with a deeper hierarchy. API costs increase
significantly due to the higher input tokens required for deeper label
hierarchies on DH strategy. These results emphasize the trade-off between
accuracy improvement and the computational cost of prompt strategy. These
findings highlight the potential of black box LLMs for HTC while underscoring
the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [36] [DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting](https://arxiv.org/abs/2508.04239)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.CL

TL;DR: 本文介绍了一种新的双提示大型语言模型框架DP-GPT4MTS，用于改进时间序列预测。通过结合显式提示和文本提示，该框架能够更好地捕捉时间戳文本的语义，从而提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的时间序列预测模型主要关注数值数据，而忽略了可能影响预测准确性的文本信息。现有的单提示框架在捕捉时间戳文本语义方面存在不足，导致冗余信息影响模型性能。

Method: 本文提出了一种新的双提示大型语言模型框架DP-GPT4MTS，该框架结合了显式提示和文本提示，以捕捉时间戳文本的语义。

Result: 在多种文本-数值时间序列数据集上的实验表明，该方法优于最先进的算法，证明了双提示机制在提高时间序列预测准确性方面的有效性。

Conclusion: 本文提出了DP-GPT4MTS框架，通过双提示机制结合文本上下文，显著提高了时间序列预测的准确性。

Abstract: Time series forecasting is crucial in strategic planning and decision-making
across various industries. Traditional forecasting models mainly concentrate on
numerical time series data, often overlooking important textual information
such as events and news, which can significantly affect forecasting accuracy.
While large language models offer a promise for integrating multimodal data,
existing single-prompt frameworks struggle to effectively capture the semantics
of timestamped text, introducing redundant information that can hinder model
performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt
GPT2-base for Multimodal Time Series), a novel dual-prompt large language model
framework that combines two complementary prompts: an explicit prompt for clear
task instructions and a textual prompt for context-aware embeddings from
time-stamped data. The tokenizer generates the explicit prompt while the
embeddings from the textual prompt are refined through self-attention and
feed-forward networks. Comprehensive experiments conducted on diverse
textural-numerical time series datasets demonstrate that this approach
outperforms state-of-the-art algorithms in time series forecasting. This
highlights the significance of incorporating textual context via a dual-prompt
mechanism to achieve more accurate time series predictions.

</details>


### [37] [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)
*Xi Wang,Anxo Perez,Javier Parapar,Fabio Crestani*

Main category: cs.CL

TL;DR: 本文提出了一种基于先进语言模型的临床医生在环患者模拟流程TalkDep，以生成真实的患者反应，从而提高自动抑郁症诊断系统的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于对心理健康服务的需求增加，而真实培训数据的可用性有限，导致抑郁症诊断的支持不足，因此需要开发模拟或虚拟患者来辅助培训和评估。

Method: 采用先进的语言模型作为基础，提出一种新的临床医生在环患者模拟流程TalkDep，通过条件模型在精神疾病诊断标准、症状严重程度量表和上下文因素上生成真实的患者反应。

Result: 通过临床专业人员的全面评估验证了这些模拟患者的可靠性。

Conclusion: 提供经过验证的模拟患者可以作为改进自动抑郁症诊断系统的可扩展和适应性资源。

Abstract: The increasing demand for mental health services has outpaced the
availability of real training data to develop clinical professionals, leading
to limited support for the diagnosis of depression. This shortage has motivated
the development of simulated or virtual patients to assist in training and
evaluation, but existing approaches often fail to generate clinically valid,
natural, and diverse symptom presentations. In this work, we embrace the recent
advanced language models as the backbone and propose a novel
clinician-in-the-loop patient simulation pipeline, TalkDep, with access to
diversified patient profiles to develop simulated patients. By conditioning the
model on psychiatric diagnostic criteria, symptom severity scales, and
contextual factors, our goal is to create authentic patient responses that can
better support diagnostic model training and evaluation. We verify the
reliability of these simulated patients with thorough assessments conducted by
clinical professionals. The availability of validated simulated patients offers
a scalable and adaptable resource for improving the robustness and
generalisability of automatic depression diagnosis systems.

</details>


### [38] [KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs](https://arxiv.org/abs/2508.04257)
*Zunhai Su,Kehong Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种名为KVSink的方法，用于在KV缓存量化过程中更有效地保护注意力陷阱。通过深入分析注意力陷阱的机制及其与KV缓存量化的关系，KVSink能够在几乎没有额外开销的情况下预测陷阱令牌，从而实现更全面的保护。实验结果表明，KVSink优于现有的Preserve-First-N策略，并在KVQuant方法中进一步提高了困惑度（PPL）并减少了对16位数值异常值的依赖。


<details>
  <summary>Details</summary>
Motivation: KV cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions.

Method: We elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce KVSink, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation.

Result: Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers.

Conclusion: KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers.

Abstract: Key-Value (KV) cache quantization has become a widely adopted optimization
technique for efficient large language models (LLMs) inference by reducing KV
cache memory usage and mitigating memory-bound constraints. Recent studies have
emphasized the importance of preserving the original precision of KVs for the
first few tokens to ensure the protection of attention sinks. While this
approach has proven effective in mitigating performance degradation, its
underlying principles remain insufficiently understood. Moreover, it fails to
address the recent discovery that attention sinks can emerge beyond the initial
token positions. In this work, we elucidate the underlying mechanisms of
attention sinks during inference by examining their role in the cross-layer
evolution of extreme activation outliers. Additionally, we provide a
comprehensive analysis of the interplay between attention sinks and KV cache
quantization. Based on our enhanced understanding, we introduce
\textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink
tokens with negligible overhead, enabling more thorough preservation. Extensive
experiments demonstrate that KVSink outperforms the existing Preserve-First-N
(PFN) strategy, offering more effective preservation of attention sinks during
KV cache quantization. Moreover, when applied to the well-established KVQuant
method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit
numerical outliers.

</details>


### [39] [ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents](https://arxiv.org/abs/2508.04266)
*Jiangyuan Wang,Kejun Xiao,Qi Sun,Huaipeng Zhao,Tao Luo,Jiandong Zhang,Xiaoyi Zeng*

Main category: cs.CL

TL;DR: 本文介绍了ShoppingBench，一个新颖的端到端购物基准，旨在涵盖日益具有挑战性的基于现实的用户意图。通过提供一个大规模的购物沙盒，实验结果显示，即使是最先进的语言代理在该基准任务上的表现也不理想。此外，作者提出了一种轨迹蒸馏策略，以提高小型代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的电子商务基准主要关注基本的用户意图，例如查找或购买产品。然而，现实世界的用户往往追求更复杂的目標，例如應用優惠券、管理預算和尋找多產品賣家。为了弥合这一差距，我们提出了ShoppingBench，一个新颖的端到端购物基准，旨在涵盖日益具有挑战性的基于现实的意图。

Method: 我们提出了一个可扩展的框架来模拟基于从采样真实世界产品中得出的各种意图的用户指令。为了促进一致和可靠的评估，我们提供了一个大规模的购物沙盒，作为交互式模拟环境，包含超过250万种真实世界的产品。此外，我们提出了一种轨迹蒸馏策略，并利用监督微调以及合成轨迹上的强化学习，将大型语言代理的能力蒸馏到较小的代理中。

Result: 即使最先进的语言代理（如GPT-4.1）在我们的基准任务上的绝对成功率也低于50%，这凸显了ShoppingBench所带来的重大挑战。此外，我们提出了一种轨迹蒸馏策略，并利用监督微调以及合成轨迹上的强化学习，将大型语言代理的能力蒸馏到较小的代理中。结果表明，我们的训练代理与GPT-4.1相比表现出具有竞争力的性能。

Conclusion: 实验结果表明，即使是最先进的语言代理（如GPT-4.1）在我们的基准任务上的绝对成功率也低于50%，这凸显了ShoppingBench所带来的重大挑战。此外，我们提出了一种轨迹蒸馏策略，并利用监督微调以及合成轨迹上的强化学习，将大型语言代理的能力蒸馏到较小的代理中。结果表明，我们的训练代理与GPT-4.1相比表现出具有竞争力的性能。

Abstract: Existing benchmarks in e-commerce primarily focus on basic user intents, such
as finding or purchasing products. However, real-world users often pursue more
complex goals, such as applying vouchers, managing budgets, and finding
multi-products seller. To bridge this gap, we propose ShoppingBench, a novel
end-to-end shopping benchmark designed to encompass increasingly challenging
levels of grounded intent. Specifically, we propose a scalable framework to
simulate user instructions based on various intents derived from sampled
real-world products. To facilitate consistent and reliable evaluations, we
provide a large-scale shopping sandbox that serves as an interactive simulated
environment, incorporating over 2.5 million real-world products. Experimental
results demonstrate that even state-of-the-art language agents (such as
GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,
highlighting the significant challenges posed by our ShoppingBench. In
addition, we propose a trajectory distillation strategy and leverage supervised
fine-tuning, along with reinforcement learning on synthetic trajectories, to
distill the capabilities of a large language agent into a smaller one. As a
result, our trained agent achieves competitive performance compared to GPT-4.1.

</details>


### [40] [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)
*Jiayi Wen,Tianxin Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CL

TL;DR: 本文提出了两种知识中毒攻击，展示了如何通过修改少量文本内容来破坏GraphRAG系统的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: GraphRAG依赖于LLMs从原始文本中提取知识，这个过程可能被恶意操纵以植入误导信息。

Method: 提出两种知识中毒攻击（KPAs），并演示仅修改源文本中的几个词就可以显著改变构建的图，毒害GraphRAG，并严重误导下游推理。

Result: 第一个攻击（TKPA）成功率达到93.1%，第二个攻击（UKPA）在修改少于0.05%的全文的情况下，QA准确率从95%降至50%。

Conclusion: 实验表明，最先进的防御方法无法检测到这些攻击，这表明保护GraphRAG管道免受知识中毒攻击仍 largely unexplored。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as
a promising paradigm for enhancing large language models (LLMs) by converting
raw text into structured knowledge graphs, improving both accuracy and
explainability. However, GraphRAG relies on LLMs to extract knowledge from raw
text during graph construction, and this process can be maliciously manipulated
to implant misleading information. Targeting this attack surface, we propose
two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a
few words in the source text can significantly change the constructed graph,
poison the GraphRAG, and severely mislead downstream reasoning. The first
attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate
vulnerable nodes in the generated graphs and rewrites the corresponding
narratives with LLMs, achieving precise control over specific
question-answering (QA) outcomes with a success rate of 93.1\%, while keeping
the poisoned text fluent and natural. The second attack, named Universal KPA
(UKPA), exploits linguistic cues such as pronouns and dependency relations to
disrupt the structural integrity of the generated graph by altering globally
influential words. With fewer than 0.05\% of full text modified, the QA
accuracy collapses from 95\% to 50\%. Furthermore, experiments show that
state-of-the-art defense methods fail to detect these attacks, highlighting
that securing GraphRAG pipelines against knowledge poisoning remains largely
unexplored.

</details>


### [41] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 本文介绍了MedCheck，这是一个针对医疗基准的生命周期评估框架，旨在解决现有基准在临床真实性、数据管理和安全评估方面的不足，并通过分析53个医疗LLM基准发现了广泛的问题。


<details>
  <summary>Details</summary>
Motivation: Concerns persist regarding the reliability of benchmarks for large language models (LLMs) in healthcare, which often lack clinical fidelity, robust data management, and safety-oriented evaluation metrics.

Method: We introduce MedCheck, the first lifecycle-oriented assessment framework specifically designed for medical benchmarks. Our framework deconstructs a benchmark's development into five continuous stages, from design to governance, and provides a comprehensive checklist of 46 medically-tailored criteria.

Result: Our analysis uncovers widespread, systemic issues, including a profound disconnect from clinical practice, a crisis of data integrity due to unmitigated contamination risks, and a systematic neglect of safety-critical evaluation dimensions like model robustness and uncertainty awareness.

Conclusion: MedCheck serves as both a diagnostic tool for existing benchmarks and an actionable guideline to foster a more standardized, reliable, and transparent approach to evaluating AI in healthcare.

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [42] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
*Francisco Bolaños,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 本文提出了一种新的注释模式，并评估了多种大型语言模型在分类任务中的表现。结果表明，经过高质量数据微调的模型可以达到96%以上的F1分数，一些轻量级的开源模型也表现出色。使用由LLM生成的半合成示例丰富训练数据有助于提高小型编码器和开放解码器模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为了支持新一代能够生成高质量文献综述的系统的发展，需要定义一个相关的注释模式和有效的大规模文献注释策略。本文旨在解决这些挑战。

Method: 本文引入了一个新的注释模式，并对多种最先进的大型语言模型（LLMs）进行了分类评估。同时，还构建了一个名为Sci-Sentence的多学科基准数据集，包含700句由领域专家手动标注的句子和2,240句由LLMs自动标记的句子。

Result: 实验结果显示，经过高质量数据微调的大型语言模型在分类任务中表现优异，F1分数超过96%。虽然大型专有模型如GPT-4o表现最佳，但一些轻量级的开源模型也表现出色。此外，使用由LLM生成的半合成示例丰富训练数据有助于提高小型编码器和开放解码器模型的性能。

Conclusion: 本文提出了一个专门用于支持文献综述生成的新注释模式，并通过实验验证了当前大型语言模型在该任务上的表现。结果表明，经过高质量数据微调的模型可以达到96%以上的F1分数，一些轻量级的开源模型也表现出色。此外，使用由LLM生成的半合成示例丰富训练数据有助于提高小型编码器和开放解码器模型的性能。

Abstract: Previous work has demonstrated that AI methods for analysing scientific
literature benefit significantly from annotating sentences in papers according
to their rhetorical roles, such as research gaps, results, limitations,
extensions of existing methodologies, and others. Such representations also
have the potential to support the development of a new generation of systems
capable of producing high-quality literature reviews. However, achieving this
goal requires the definition of a relevant annotation schema and effective
strategies for large-scale annotation of the literature. This paper addresses
these challenges by 1) introducing a novel annotation schema specifically
designed to support literature review generation and 2) conducting a
comprehensive evaluation of a wide range of state-of-the-art large language
models (LLMs) in classifying rhetorical roles according to this schema. To this
end, we also present Sci-Sentence, a novel multidisciplinary benchmark
comprising 700 sentences manually annotated by domain experts and 2,240
sentences automatically labelled using LLMs. We evaluate 37 LLMs on this
benchmark, spanning diverse model families and sizes, using both zero-shot
learning and fine-tuning approaches. The experiments yield several novel
insights that advance the state of the art in this challenging domain. First,
the current generation of LLMs performs remarkably well on this task when
fine-tuned on high-quality data, achieving performance levels above 96\% F1.
Second, while large proprietary models like GPT-4o achieve the best results,
some lightweight open-source alternatives also demonstrate excellent
performance. Finally, enriching the training data with semi-synthetic examples
generated by LLMs proves beneficial, enabling small encoders to achieve robust
results and significantly enhancing the performance of several open decoder
models.

</details>


### [43] [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
*Hongze Tan,Jianfei Pan*

Main category: cs.CL

TL;DR: 本文通过动态熵权重机制改进了RL方法，提高了LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RL方法如GRPO在长链推理任务中存在主要缺陷，即对序列中的所有token应用统一奖励，导致信用分配过于粗略。

Method: 本文提出了两种方法：1) Group Token Policy Optimization (GTPO)，为每个token分配熵加权奖励以实现细粒度的信用分配；2) Sequence-Level Group Relative Policy Optimization (GRPO-S)，基于每个序列的平均token熵分配熵加权奖励。

Result: 实验表明，本文提出的方法显著优于强大的DAPO基线，验证了熵权重机制的有效性。

Conclusion: 本文提出的方法在实验中显著优于DAPO基线，结果证实了熵权重机制是性能提升的关键驱动因素，为增强模型的深度推理提供了更好的路径。

Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.

</details>


### [44] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: 本文介绍了一个名为Chain of Questions (CoQ)的框架，旨在提高多模态语言模型在处理复杂现实环境时的推理能力。通过动态生成有针对性的问题，模型可以更好地选择性地激活相关感官模态，从而提高准确性、可解释性和推理过程的一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）的推理能力已通过链式思维和显式逐步解释等方法显著提升，但这些改进尚未完全转移到多模态环境中。在多模态环境中，模型必须主动决定在与复杂现实环境交互时使用哪些感官模态，如视觉、音频或空间感知。

Method: 本文引入了Chain of Questions (CoQ)框架，这是一种基于好奇心的推理方法，使多模态语言模型能够动态生成针对其周围环境的问题。这些问题引导模型选择性地激活相关模态，从而收集进行准确推理和响应生成所需的关键信息。

Result: 在整合WebGPT、ScienceQA、AVSD和ScanQA数据集的新多模态基准数据集上评估了我们的框架。实验结果表明，我们的CoQ方法提高了基础模型有效识别和整合相关感官信息的能力。

Conclusion: 本文提出了一种名为Chain of Questions (CoQ)的框架，该框架通过鼓励多模态语言模型动态生成针对周围环境的有针对性的问题，从而提高模型在多模态任务中的准确性、可解释性和推理过程的一致性。

Abstract: Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

</details>


### [45] [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 本文提出了一种基于RAG的简单两步事实核查管道，在本地环境中实现了最先进的事实核查性能。


<details>
  <summary>Details</summary>
Motivation: 为了实现高效的本地事实核查系统，同时保持先进的性能水平。

Method: 我们提出了一个基于RAG的简单两步事实核查管道，并将其重新部署在本地环境中。

Result: 该系统在FEVER 8共享任务中排名第一，并在单个NVIDIA A10 GPU和60秒运行时间的限制下实现了最先进的事实核查性能。

Conclusion: 我们的事实核查系统在FEVER 8共享任务中取得了第一名，并且在单个NVIDIA A10 GPU和60秒运行时间的限制下实现了最先进的事实核查性能。

Abstract: In this paper, we present our fact-checking pipeline which has scored first
in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG
pipeline based on our last year's submission. We show how the pipeline can be
redeployed on-premise, achieving state-of-the-art fact-checking performance (in
sense of Ev2R test-score), even under the constraint of a single NVidia A10
GPU, 23GB of graphical memory and 60s running time per claim.

</details>


### [46] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
*Xu Zhang,Mei Chen*

Main category: cs.CL

TL;DR: 本研究评估了先进的自然语言处理技术以提高事故数据质量，比较了零样本大语言模型、微调的Transformer和传统逻辑回归模型。结果表明，微调的Transformer模型在精度和效率方面表现最佳，而中等大小的大语言模型可以提供性能与计算成本之间的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过挖掘事故叙述来评估先进的自然语言处理（NLP）技术，以提高事故数据的质量，并以肯塔基州的二次事故识别为例进行案例研究。

Method: 本研究比较了三种模型类别：零样本开源大语言模型（LLM）、微调的Transformer（BERT, DistilBERT, RoBERTa, XLNet, Longformer）和传统的逻辑回归作为基线。模型在2015-2021年的数据上进行了校准，并在2022年的1,771条叙述上进行了测试。

Result: 微调的Transformer模型表现优异，其中RoBERTa达到了最高的F1分数（0.90）和准确率（95%）。零样本LLaMA3:70B达到了相当的F1分数（0.86），但需要139分钟的推理时间；逻辑回归基线明显落后（F1:0.66）。LLM在某些变体中表现出色（例如GEMMA3:27B的召回率为0.94），但计算成本很高（DeepSeek-R1:70B高达723分钟），而微调模型在短暂训练后可以在几秒钟内处理测试集。进一步分析表明，中等大小的LLM（如DeepSeek-R1:32B）可以在性能上与更大的模型相媲美，同时减少运行时间，表明有优化部署的机会。

Conclusion: 研究结果表明，微调的Transformer模型在肯塔基州数据上有效地平衡了精确度和召回率。结果强调了准确性、效率和数据需求之间的权衡，并提出了实际部署的考虑因素，如隐私保护的本地部署、集成方法以提高准确性以及增量处理以实现可扩展性，为使用先进NLP增强事故数据质量提供了可复制的方案。

Abstract: This study evaluates advanced natural language processing (NLP) techniques to
enhance crash data quality by mining crash narratives, using secondary crash
identification in Kentucky as a case study. Drawing from 16,656 manually
reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we
compare three model classes: zero-shot open-source large language models (LLMs)
(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers
(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic
regression as baseline. Models were calibrated on 2015-2021 data and tested on
1,771 narratives from 2022. Fine-tuned transformers achieved superior
performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy
(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139
minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs
excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred
high computational costs (up to 723 minutes for DeepSeek-R1:70B), while
fine-tuned models processed the test set in seconds after brief training.
Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can
rival larger counterparts in performance while reducing runtime, suggesting
opportunities for optimized deployments. Results highlight trade-offs between
accuracy, efficiency, and data requirements, with fine-tuned transformer models
balancing precision and recall effectively on Kentucky data. Practical
deployment considerations emphasize privacy-preserving local deployment,
ensemble approaches for improved accuracy, and incremental processing for
scalability, providing a replicable scheme for enhancing crash-data quality
with advanced NLP.

</details>


### [47] [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)
*Vladimír Havlík*

Main category: cs.CL

TL;DR: 本文通过理论分析和实证观察研究了深度神经网络的涌现特性，指出涌现能力源于高度敏感的非线性系统的复杂动力学，而不是仅仅由于参数扩展。


<details>
  <summary>Details</summary>
Motivation: 探讨深度神经网络（DNN）的涌现特性，解决当代AI发展中“无理解的创造”这一认识论挑战。

Method: 通过理论分析和实证观察研究深度神经网络（DNN）的涌现特性，探讨了规模定律、领悟现象和模型能力的相变。

Result: 发现涌现能力源于高度敏感的非线性系统的复杂动力学，而不是仅仅由于参数扩展。当前关于指标、预训练损失阈值和上下文学习的争论忽略了DNN中涌现的根本本体论性质。

Conclusion: 理解LLM能力需要认识到DNN是一个由涌现普遍原则支配的复杂动力系统领域，类似于物理、化学和生物学中的原则。这种观点将重点从现象学定义的涌现转移到理解使这些系统超越其个体组件获得能力的内部动态转换。

Abstract: The remarkable success of Large Language Models (LLMs) in generative tasks
has raised fundamental questions about the nature of their acquired
capabilities, which often appear to emerge unexpectedly without explicit
training. This paper examines the emergent properties of Deep Neural Networks
(DNNs) through both theoretical analysis and empirical observation, addressing
the epistemological challenge of "creation without understanding" that
characterises contemporary AI development. We explore how the neural approach's
reliance on nonlinear, stochastic processes fundamentally differs from symbolic
computational paradigms, creating systems whose macro-level behaviours cannot
be analytically derived from micro-level neuron activities. Through analysis of
scaling laws, grokking phenomena, and phase transitions in model capabilities,
I demonstrate that emergent abilities arise from the complex dynamics of highly
sensitive nonlinear systems rather than simply from parameter scaling alone. My
investigation reveals that current debates over metrics, pre-training loss
thresholds, and in-context learning miss the fundamental ontological nature of
emergence in DNNs. I argue that these systems exhibit genuine emergent
properties analogous to those found in other complex natural phenomena, where
systemic capabilities emerge from cooperative interactions among simple
components without being reducible to their individual behaviours. The paper
concludes that understanding LLM capabilities requires recognising DNNs as a
new domain of complex dynamical systems governed by universal principles of
emergence, similar to those operating in physics, chemistry, and biology. This
perspective shifts the focus from purely phenomenological definitions of
emergence to understanding the internal dynamic transformations that enable
these systems to acquire capabilities that transcend their individual
components.

</details>


### [48] [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)
*Kiyotada Mori,Seiya Kawano,Chaoran Liu,Carlos Toshinori Ishi,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本文研究了人类选择性倾听的能力，并探讨了基于此的新型ASR评估方法的可能性。


<details>
  <summary>Details</summary>
Motivation: 为了确定SDS所需的ASR能力并评估它们，研究人类的选择性倾听能力。

Method: 通过比较人类生成对话响应的转录和参考转录来实验验证选择性倾听。

Result: 实验确认了人类在生成对话响应时的选择性倾听。

Conclusion: 本文讨论了基于人类选择性倾听的新型自动语音识别（ASR）评估方法的可能性，该方法可以识别ASR系统与人类在转录能力上的差距。

Abstract: Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at
the front end of their pipeline. The role of ASR in SDSs is to recognize
information in user speech related to response generation appropriately.
Examining selective listening of humans, which refers to the ability to focus
on and listen to important parts of a conversation during the speech, will
enable us to identify the ASR capabilities required for SDSs and evaluate them.
In this study, we experimentally confirmed selective listening when humans
generate dialogue responses by comparing human transcriptions for generating
dialogue responses and reference transcriptions. Based on our experimental
results, we discuss the possibility of a new ASR evaluation method that
leverages human selective listening, which can identify the gap between
transcription ability between ASR systems and humans.

</details>


### [49] [Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model](https://arxiv.org/abs/2508.04403)
*Kiyotada Mori,Seiya Kawano,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本文提出了一种预测置信度模型（PCM），用于判断是否可以进行预取，并通过估计预测的完整用户话语与完整用户话语之间的语义相似性来实现。


<details>
  <summary>Details</summary>
Motivation: 为了减少用户感知延迟（UPL），即用户在收到系统响应前的等待时间，需要在用户说话结束前预测完整的用户话语，通常通过语言模型来准备预取的对话响应。

Method: 本文提出了预测置信度模型（PCM），该模型通过估计预测的完整用户话语与完整用户话语之间的语义相似性来判断是否可以进行预取。

Result: 本文基于预测的完整用户话语与完整用户话语之间的差异评估了PCM。

Conclusion: 本文提出了一种预测置信度模型（PCM），用于判断是否可以进行预取，并通过估计预测的完整用户话语与完整用户话语之间的语义相似性来实现。

Abstract: Prefetching of dialogue responses has been investigated to reduce
user-perceived latency (UPL), which refers to the user's waiting time before
receiving the system's response, in spoken dialogue systems. To reduce the UPL,
it is necessary to predict complete user utterances before the end of the
user's speech, typically by language models, to prepare prefetched dialogue
responses. In this study, we proposed a prediction confidence model (PCM) that
determines whether prefetching is possible or not by estimating the semantic
similarity between the predicted complete user utterance and the complete user
utterance. We evaluated our PCM based on the differences between the predicted
complete user utterance and the complete user utterance.

</details>


### [50] [Evaluating, Synthesizing, and Enhancing for Customer Support Conversation](https://arxiv.org/abs/2508.04423)
*Jie Zhu,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 本文介绍了客户支持对话（CSC）任务，提出了一种基于COPC指南的结构化框架，并构建了CSConv和RoleCS数据集。实验显示，微调LLM在RoleCS上能显著提升生成高质量策略响应的能力。


<details>
  <summary>Details</summary>
Motivation: 有效的客户服务不仅需要准确的问题解决，还需要符合专业标准的结构化和同理心沟通。然而，现有的对话数据集往往缺乏战略指导，而现实世界的服务数据难以访问和注释。为此，我们引入了客户支持对话（CSC）任务，旨在训练客服人员使用明确的支持策略进行回应。

Method: 我们提出了一个基于COPC指南的结构化CSC框架，定义了五个对话阶段和十二种策略来指导高质量的互动。我们构建了CSConv，一个包含1,855个真实客户代理对话的评估数据集，这些对话使用LLMs重写以反映有意图的策略使用，并进行了相应的注释。此外，我们开发了一种角色扮演方法，使用与CSC框架对齐的LLM驱动角色模拟策略丰富的对话，从而产生了训练数据集RoleCS。

Result: 实验表明，在RoleCS上微调强大的LLM可以显著提高它们在CSConv上生成高质量、策略一致响应的能力。人类评估进一步证实了问题解决的提升。

Conclusion: 实验表明，在RoleCS上微调强大的LLM可以显著提高它们在CSConv上生成高质量、策略一致响应的能力。人类评估进一步证实了问题解决的提升。所有代码和数据将在https://github.com/aliyun/qwen-dianjin上公开。

Abstract: Effective customer support requires not only accurate problem solving but
also structured and empathetic communication aligned with professional
standards. However, existing dialogue datasets often lack strategic guidance,
and real-world service data is difficult to access and annotate. To address
this, we introduce the task of Customer Support Conversation (CSC), aimed at
training customer service agents to respond using well-defined support
strategies. We propose a structured CSC framework grounded in COPC guidelines,
defining five conversational stages and twelve strategies to guide high-quality
interactions. Based on this, we construct CSConv, an evaluation dataset of
1,855 real-world customer-agent conversations rewritten using LLMs to reflect
deliberate strategy use, and annotated accordingly. Additionally, we develop a
role-playing approach that simulates strategy-rich conversations using
LLM-powered roles aligned with the CSC framework, resulting in the training
dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS
significantly improves their ability to generate high-quality, strategy-aligned
responses on CSConv. Human evaluations further confirm gains in problem
resolution. All code and data will be made publicly available at
https://github.com/aliyun/qwen-dianjin.

</details>


### [51] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
*Yutong Wu,Di Huang,Ruosi Wan,Yue Peng,Shijie Shang,Chenrui Cao,Lei Qi,Rui Zhang,Zidong Du,Jie Yan,Xing Hu*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据合成和训练方法，以提高自动形式化的能力，并在多个基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动形式化方法仍然存在准确性低的问题，需要提高模型对形式化领域知识的全面掌握和自然语言问题理解及非正式-正式对齐的推理能力。

Method: 本文提出了一个数据合成和训练管道，包括构建两个数据集，以及使用SFT和RLVR进行进一步融合和优化。

Result: 通过ThinkingF方法训练的7B和32B模型表现出全面的形式化知识和强大的非正式-正式推理能力。特别是StepFun-Formalizer-32B在FormalMATH-Lite和ProverBench任务上取得了最先进的BEq@1分数。

Conclusion: 本文提出了一种名为ThinkingF的数据合成和训练管道，以提高自动形式化的能力。结果表明，所得到的模型在形式化任务上表现出色，达到了最先进的性能。

Abstract: Autoformalization aims to translate natural-language mathematical statements
into a formal language. While LLMs have accelerated progress in this area,
existing methods still suffer from low accuracy. We identify two key abilities
for effective autoformalization: comprehensive mastery of formal-language
domain knowledge, and reasoning capability of natural language problem
understanding and informal-formal alignment. Without the former, a model cannot
identify the correct formal objects; without the latter, it struggles to
interpret real-world contexts and map them precisely into formal expressions.
To address these gaps, we introduce ThinkingF, a data synthesis and training
pipeline that improves both abilities. First, we construct two datasets: one by
distilling and selecting large-scale examples rich in formal knowledge, and
another by generating informal-to-formal reasoning trajectories guided by
expert-designed templates. We then apply SFT and RLVR with these datasets to
further fuse and refine the two abilities. The resulting 7B and 32B models
exhibit both comprehensive formal knowledge and strong informal-to-formal
reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%
on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior
general-purpose and specialized models.

</details>


### [52] [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.CL

TL;DR: 本研究探讨了生成式AI在马来西亚教育系统中的应用，重点在于生成符合课程要求的数学选择题。通过比较不同方法，发现基于RAG的流程表现更优，提供了可行的教育技术解决方案。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在解决马来西亚教育体系中对可扩展且高质量教育评估工具的迫切需求，同时强调生成式AI（GenAI）的潜力，但承认确保事实准确性与课程一致性的重大挑战，尤其是在低资源语言如马来语中。

Method: 本研究介绍了并比较了四种逐步处理流程，用于使用OpenAI的GPT-4o生成Form 1数学选择题（MCQs），包括非基础提示（结构化和基本）和检索增强生成（RAG）方法（一个使用LangChain框架，一个手动实现）。系统基于官方课程文件，包括教师准备的笔记和年度教学计划（RPT）。

Result: 结果表明，基于RAG的流程显著优于非基础提示方法，生成的问题具有更高的课程一致性和事实有效性。研究进一步分析了基于框架的RAG的易用性与手动流程提供的细粒度控制之间的权衡。

Conclusion: 本研究提出了一个经过验证的方法，用于在低资源语言中生成课程特定的教育内容，并引入了一种共生的RAG-QA评估技术，为马来西亚及其他类似地区的教育技术解决方案的开发和部署提供了可行的见解。

Abstract: This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.

</details>


### [53] [CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation](https://arxiv.org/abs/2508.04494)
*Bastien Liétard,Gabriel Loiseau*

Main category: cs.CL

TL;DR: 本文提出了概念区分任务，并构建了一个数据集用于微调语言模型，以获得更准确的词义表示。实验表明，这些模型在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的Word-in-Context任务仅比较相同词素的出现，限制了所捕获信息的范围。为了扩展这一研究，本文提出了一种新的方法来包括跨词场景。

Method: 本文提出了概念区分任务，并基于SemCor数据集构建了一个数据集，然后对几种表示模型进行了微调，得到了概念对齐嵌入（CALE）。

Result: 通过在各种词义任务上测试我们的模型和其他模型，我们证明了所提出的模型提供了高效的多用途词义表示，并在实验中达到了最佳性能。

Conclusion: 本文提出的模型在各种词义任务中表现出色，展示了其在词义表示中的有效性。此外，CALE的微调对嵌入的空间组织产生了有益的变化。

Abstract: Lexical semantics is concerned with both the multiple senses a word can adopt
in different contexts, and the semantic relations that exist between meanings
of different words. To investigate them, Contextualized Language Models are a
valuable tool that provides context-sensitive representations that can be used
to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the
task of Word-in-Context to fine-tune them to get more semantically accurate
representations, but Word-in-Context only compares occurrences of the same
lemma, limiting the range of captured information. In this paper, we propose an
extension, Concept Differentiation, to include inter-words scenarios. We
provide a dataset for this task, derived from SemCor data. Then we fine-tune
several representation models on this dataset. We call these models
Concept-Aligned Embeddings (CALE). By challenging our models and other models
on various lexical semantic tasks, we demonstrate that the proposed models
provide efficient multi-purpose representations of lexical meaning that reach
best performances in our experiments. We also show that CALE's fine-tuning
brings valuable changes to the spatial organization of embeddings.

</details>


### [54] [StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering](https://arxiv.org/abs/2508.04530)
*Chenglei Shen,Zhongxiang Sun,Teng Shi,Xiao Zhang,Jun Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为StyliTruth的方法，通过分离模型表示空间中的风格相关和真实性相关子空间，实现对风格和真实性的独立控制，从而减少因风格化导致的真实性崩溃问题，并在多个风格和语言上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Generating stylized large language model (LLM) responses via representation editing is a promising way for fine-grained output control. However, there exists an inherent trade-off: imposing a distinctive style often degrades truthfulness. Existing representation editing methods, by naively injecting style signals, overlook this collateral impact and frequently contaminate the model's core truthfulness representations, resulting in reduced answer correctness.

Method: StyliTruth separates the style-relevant and truth-relevant subspaces in the model's representation space via an orthogonal deflation process. By designing adaptive, token-level steering vectors within each subspace, we dynamically and precisely control the generation process to maintain both stylistic fidelity and truthfulness.

Result: We validate our method on multiple styles and languages. Extensive experiments and analyses show that StyliTruth significantly reduces stylization-induced truthfulness collapse and outperforms existing inference-time intervention methods in balancing style adherence with truthfulness.

Conclusion: StyliTruth significantly reduces stylization-induced truthfulness collapse and outperforms existing inference-time intervention methods in balancing style adherence with truthfulness.

Abstract: Generating stylized large language model (LLM) responses via representation
editing is a promising way for fine-grained output control. However, there
exists an inherent trade-off: imposing a distinctive style often degrades
truthfulness. Existing representation editing methods, by naively injecting
style signals, overlook this collateral impact and frequently contaminate the
model's core truthfulness representations, resulting in reduced answer
correctness. We term this phenomenon stylization-induced truthfulness collapse.
We attribute this issue to latent coupling between style and truth directions
in certain key attention heads, and propose StyliTruth, a mechanism that
preserves stylization while keeping truthfulness intact. StyliTruth separates
the style-relevant and truth-relevant subspaces in the model's representation
space via an orthogonal deflation process. This decomposition enables
independent control of style and truth in their own subspaces, minimizing
interference. By designing adaptive, token-level steering vectors within each
subspace, we dynamically and precisely control the generation process to
maintain both stylistic fidelity and truthfulness. We validate our method on
multiple styles and languages. Extensive experiments and analyses show that
StyliTruth significantly reduces stylization-induced truthfulness collapse and
outperforms existing inference-time intervention methods in balancing style
adherence with truthfulness.

</details>


### [55] [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)
*Zhuang Chen,Guanqun Bi,Wen Zhang,Jiawei Hu,Aoyun Wang,Xiyao Xiao,Kun Feng,Minlie Huang*

Main category: cs.CL

TL;DR: 本文介绍了C-MIND数据集，用于临床抑郁症评估，并探讨了传统模型和大型语言模型在该领域的应用及改进方法。


<details>
  <summary>Details</summary>
Motivation: 目前大多数研究依赖于有限或非临床验证的数据，且往往更注重复杂模型设计而非实际效果。因此，本文旨在揭示临床抑郁症评估的现状，并提供一个真实医院访问数据集以支持更有效的研究。

Method: 本文引入了C-MIND数据集，分析与诊断相关的行为特征，并训练一系列经典模型来量化不同任务和模态对诊断性能的贡献，同时探索大型语言模型（LLMs）在精神病理推理中的表现及其局限性。

Result: 通过C-MIND数据集，作者分析了与诊断相关的行为特征，并训练了多种模型来评估不同任务和模态的效果。此外，他们发现通过结合临床专业知识可以显著提高LLMs的诊断性能。

Conclusion: 本文旨在从数据和算法角度构建临床抑郁症评估的基础架构，并通过C-MIND数据集促进心理健康领域的可靠研究。

Abstract: Depression is a widespread mental disorder that affects millions worldwide.
While automated depression assessment shows promise, most studies rely on
limited or non-clinically validated data, and often prioritize complex model
design over real-world effectiveness. In this paper, we aim to unveil the
landscape of clinical depression assessment. We introduce C-MIND, a clinical
neuropsychiatric multimodal diagnosis dataset collected over two years from
real hospital visits. Each participant completes three structured psychiatric
tasks and receives a final diagnosis from expert clinicians, with informative
audio, video, transcript, and functional near-infrared spectroscopy (fNIRS)
signals recorded. Using C-MIND, we first analyze behavioral signatures relevant
to diagnosis. We train a range of classical models to quantify how different
tasks and modalities contribute to diagnostic performance, and dissect the
effectiveness of their combinations. We then explore whether LLMs can perform
psychiatric reasoning like clinicians and identify their clear limitations in
realistic clinical settings. In response, we propose to guide the reasoning
process with clinical expertise and consistently improves LLM diagnostic
performance by up to 10% in Macro-F1 score. We aim to build an infrastructure
for clinical depression assessment from both data and algorithmic perspectives,
enabling C-MIND to facilitate grounded and reliable research for mental
healthcare.

</details>


### [56] [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)
*Nuo Chen,Yicheng Tong,Jiaying Wu,Minh Duc Duong,Qian Wang,Qingyun Zou,Bryan Hooi,Bingsheng He*

Main category: cs.CL

TL;DR: 本文研究了结构化的多智能体讨论是否能超越单独的AI构思，并发现多智能体讨论显著优于单独的基线，指定的领导者有助于生成更综合和有远见的提案，同时认知多样性是质量的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数框架依赖于单智能体优化，由于知识和视角的局限性，限制了创造力。受现实世界研究动态的启发，本文探讨结构化的多智能体讨论是否可以超越单独的构思。

Method: 我们提出了一种协作式多智能体框架来生成研究提案，并系统地比较了包括小组规模、有领导与无领导结构以及团队组成（如跨学科性和资历）的不同配置。

Result: 多智能体讨论显著优于单独的基线。指定的领导者起到了催化剂的作用，将讨论转化为更综合和有远见的提案。值得注意的是，认知多样性是质量的主要驱动力，但专业知识是不可妥协的前提，因为缺乏高级知识基础的团队甚至无法超越一个熟练的单一智能体。

Conclusion: 这些发现为设计协作式AI构思系统提供了可操作的见解，并揭示了团队结构如何影响创意成果。

Abstract: While AI agents show potential in scientific ideation, most existing
frameworks rely on single-agent refinement, limiting creativity due to bounded
knowledge and perspective. Inspired by real-world research dynamics, this paper
investigates whether structured multi-agent discussions can surpass solitary
ideation. We propose a cooperative multi-agent framework for generating
research proposals and systematically compare configurations including group
size, leaderled versus leaderless structures, and team compositions varying in
interdisciplinarity and seniority. To assess idea quality, we employ a
comprehensive protocol with agent-based scoring and human review across
dimensions such as novelty, strategic vision, and integration depth. Our
results show that multi-agent discussions substantially outperform solitary
baselines. A designated leader acts as a catalyst, transforming discussion into
more integrated and visionary proposals. Notably, we find that cognitive
diversity is a primary driver of quality, yet expertise is a non-negotiable
prerequisite, as teams lacking a foundation of senior knowledge fail to surpass
even a single competent agent. These findings offer actionable insights for
designing collaborative AI ideation systems and shed light on how team
structure influences creative outcomes.

</details>


### [57] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: MASA is a parameter-efficient compression technique for transformers that reduces attention module parameters by 66.7% while maintaining performance. It uses structured weight sharing through dictionary learning and works as a drop-in replacement without requiring distillation or architectural changes.


<details>
  <summary>Details</summary>
Motivation: The repetitive layered structure of transformers implies significant inter-block redundancy, which is largely unexplored beyond key-value caching. Existing compression techniques focus on intra-block optimizations, but MASA addresses inter-block redundancy through structured weight sharing.

Method: MASA (Matrix Atom Sharing in Attention) decomposes attention projection matrices into shared dictionary atoms, allowing each layer's weights to be represented as linear combinations of these atoms. This approach reduces the attention module's parameters by 66.7% while maintaining performance.

Result: Experiments across various scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines, and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. It also matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters.

Conclusion: MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. It can also be applied to pretrained LLMs to reduce their parameters without significant performance loss.

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


### [58] [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)
*Zhejun Zhao,Yuehu Dong,Alley Liu,Lixue Zheng,Pingsheng Liu,Dongdong Shen,Long Xia,Jiashu Zhao,Dawei Yin*

Main category: cs.CL

TL;DR: TURA is a new framework that combines RAG with agentic tool-use to access both static and dynamic information, addressing limitations of traditional RAG approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG approaches struggle with real-time needs and structured queries that require accessing dynamically generated content like ticket availability or inventory. Academic research has focused on optimizing RAG for static content, overlooking complex intents and the need for dynamic sources like databases and real-time APIs.

Method: TURA is a novel three-stage framework that combines RAG with agentic tool-use to access both static content and dynamic, real-time information. It includes an Intent-Aware Retrieval module, a DAG-based Task Planner, and a lightweight Distilled Agent Executor.

Result: TURA serves tens of millions of users, leveraging an agentic framework to deliver robust, real-time answers while meeting the low-latency demands of a large-scale industrial system.

Conclusion: TURA is the first architecture to systematically bridge the gap between static RAG and dynamic information sources for a world-class AI search product.

Abstract: The advent of Large Language Models (LLMs) is transforming search engines
into conversational AI search products, primarily using Retrieval-Augmented
Generation (RAG) on web corpora. However, this paradigm has significant
industrial limitations. Traditional RAG approaches struggle with real-time
needs and structured queries that require accessing dynamically generated
content like ticket availability or inventory. Limited to indexing static
pages, search engines cannot perform the interactive queries needed for such
time-sensitive data. Academic research has focused on optimizing RAG for static
content, overlooking complex intents and the need for dynamic sources like
databases and real-time APIs. To bridge this gap, we introduce TURA
(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage
framework that combines RAG with agentic tool-use to access both static content
and dynamic, real-time information. TURA has three key components: an
Intent-Aware Retrieval module to decompose queries and retrieve information
sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task
Planner that models task dependencies as a Directed Acyclic Graph (DAG) for
optimal parallel execution, and a lightweight Distilled Agent Executor for
efficient tool calling. TURA is the first architecture to systematically bridge
the gap between static RAG and dynamic information sources for a world-class AI
search product. Serving tens of millions of users, it leverages an agentic
framework to deliver robust, real-time answers while meeting the low-latency
demands of a large-scale industrial system.

</details>


### [59] [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)
*Chirag Seth,Utkarsh Singh*

Main category: cs.CL

TL;DR: 本文评估了三种轻量级变压器模型在文本到SQL任务中的表现，发现T5-Small在低资源环境下表现最佳，展示了紧凑变压器模型在资源匮乏环境中的潜力。


<details>
  <summary>Details</summary>
Motivation: 文本到SQL翻译使非专家用户能够使用自然语言查询关系数据库，应用于教育和商业智能。然而，在资源匮乏的环境中，需要高效的解决方案。

Method: 本文评估了三种轻量级变压器模型（T5-Small、BART-Small和GPT-2）在Spider数据集上的表现，专注于低资源设置。开发了一个可重复使用、与模型无关的管道，根据每个模型的架构调整模式格式，并进行了1000到5000次迭代的训练，使用逻辑形式准确率（LFAcc）、BLEU和精确匹配（EM）指标对1000个测试样本进行评估。

Result: 微调后的T5-Small在LFAcc上表现最佳（27.8%），优于BART-Small（23.98%）和GPT-2（20.1%），突显了编码器-解码器模型在模式感知SQL生成中的优势。尽管资源限制影响了性能，但该管道的模块化支持未来的改进，如高级模式链接或替代基础模型。

Conclusion: 本文展示了轻量级变压器模型在资源匮乏环境中的文本到SQL解决方案的潜力。

Abstract: Text-to-SQL translation enables non-expert users to query relational
databases using natural language, with applications in education and business
intelligence. This study evaluates three lightweight transformer models -
T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on
low-resource settings. We developed a reusable, model-agnostic pipeline that
tailors schema formatting to each model's architecture, training them across
1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form
Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small
achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2
(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL
generation. Despite resource constraints limiting performance, our pipeline's
modularity supports future enhancements, such as advanced schema linking or
alternative base models. This work underscores the potential of compact
transformers for accessible text-to-SQL solutions in resource-scarce
environments.

</details>


### [60] [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)
*Feifan Song,Bofei Gao,Yifan Song,Yi Liu,Weimin Xiong,Yuyang Song,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: P-Aligner是一种高效的指令对齐方法，通过生成更符合人类偏好的指令来提升大型语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在面对有缺陷的指令时，常常无法与安全、有用和诚实的价值观对齐，需要一种成本效益高且影响大的预对齐方法。

Method: P-Aligner是一个轻量级模块，通过蒙特卡洛树搜索生成保留原始意图但更符合人类偏好的指令。

Result: P-Aligner在GPT-4-turbo和Gemma-2-SimPO上的平均胜率分别提高了28.35%和8.69%。

Conclusion: P-Aligner在多个模型和基准测试中普遍优于强基线，展示了其有效性和效率。

Abstract: Large Language Models (LLMs) are expected to produce safe, helpful, and
honest content during interaction with human users, but they frequently fail to
align with such values when given flawed instructions, e.g., missing context,
ambiguous directives, or inappropriate tone, leaving substantial room for
improvement along multiple dimensions. A cost-effective yet high-impact way is
to pre-align instructions before the model begins decoding. Existing approaches
either rely on prohibitive test-time search costs or end-to-end model rewrite,
which is powered by a customized training corpus with unclear objectives. In
this work, we demonstrate that the goal of efficient and effective preference
alignment can be achieved by P-Aligner, a lightweight module generating
instructions that preserve the original intents while being expressed in a more
human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset
synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree
Search, which systematically explores the space of candidate instructions that
are closely tied to human preference. Experiments across different methods show
that P-Aligner generally outperforms strong baselines across various models and
benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo
and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness
and efficiency through multiple perspectives, including data quality, search
strategies, iterative deployment, and time overhead.

</details>


### [61] [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2508.04632)
*Xu Guo,Tianyi Liang,Tong Jian,Xiaogui Yang,Ling-I Wu,Chenhui Li,Zhihui Lu,Qipeng Guo,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出IFDecorator框架，通过三个组件提高RLVR的训练效率和意图对齐能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法存在训练效率低和过优化问题，导致模型未能正确理解用户指令的真正意图。

Method: IFDecorator框架包含三个组件：协作对抗数据飞轮、IntentCheck模块和陷阱线，用于生成更具挑战性的指令-验证对、强制意图对齐以及检测奖励黑客行为。

Result: Qwen2.5-32B-Instruct-IFDecorator在IFEval上达到了87.43%的准确率，优于更大的专有模型如GPT-4o，并在FollowBench上表现出显著改进。

Conclusion: 本文提出了IFDecorator框架，有效提高了RLVR的训练效率和意图对齐能力，并在多个基准测试中表现出色。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction
following capabilities of large language models (LLMs), but suffers from
training inefficiency due to inadequate difficulty assessment. Moreover, RLVR
is prone to over-optimization, where LLMs exploit verification shortcuts
without aligning to the actual intent of user instructions. We introduce
Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR
training into a robust and sample-efficient pipeline. It consists of three
components: (1) a cooperative-adversarial data flywheel that co-evolves
instructions and hybrid verifications, generating progressively more
challenging instruction-verification pairs; (2) IntentCheck, a bypass module
enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that
detects reward hacking via trap instructions, which trigger and capture
shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves
87.43% accuracy on IFEval, outperforming larger proprietary models such as
GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench
while preserving general capabilities. Our trip wires show significant
reductions in reward hacking rates. We will release models, code, and data for
future research.

</details>


### [62] [Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech](https://arxiv.org/abs/2508.04638)
*Tanvi Dinkar,Aiqi Jiang,Simona Frenda,Poppy Gerrard-Abbott,Nancie Gunson,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CL

TL;DR: 本文对74篇关于反仇恨言论的NLP研究进行了系统回顾，发现当前研究与受影响社区的需求之间存在脱节，并提出了重新将利益相关者专业知识置于研究中心的建议。


<details>
  <summary>Details</summary>
Motivation: 本文旨在分析利益相关者参与如何影响反仇恨言论的研究，并探索如何更好地满足受有毒在线内容影响的社区的需求。

Method: 本文对74篇关于反仇恨言论的NLP研究进行了系统回顾，分析了利益相关者参与在数据集创建、模型开发和评估中的影响。此外，还与五个专注于在线性别暴力的非政府组织进行了参与式案例研究，确定了利益相关者指导的反仇恨言论生成实践。

Result: 研究发现，当前的NLP研究与受有毒在线内容影响最大的社区的需求之间存在脱节。通过与非政府组织的合作，识别出了利益相关者指导的反仇恨言论生成实践。

Conclusion: 本文得出结论，当前的NLP研究与受有毒在线内容影响最大的社区的需求之间存在日益扩大的脱节，并提出了具体的建议，以重新将利益相关者专业知识置于反仇恨言论研究的中心。

Abstract: Counterspeech, i.e. the practice of responding to online hate speech, has
gained traction in NLP as a promising intervention. While early work emphasised
collaboration with non-governmental organisation stakeholders, recent research
trends have shifted toward automated pipelines that reuse a small set of legacy
datasets, often without input from affected communities. This paper presents a
systematic review of 74 NLP studies on counterspeech, analysing the extent to
which stakeholder participation influences dataset creation, model development,
and evaluation. To complement this analysis, we conducted a participatory case
study with five NGOs specialising in online Gender-Based Violence (oGBV),
identifying stakeholder-informed practices for counterspeech generation. Our
findings reveal a growing disconnect between current NLP research and the needs
of communities most impacted by toxic online content. We conclude with concrete
recommendations for re-centring stakeholder expertise in counterspeech
research.

</details>


### [63] [Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs](https://arxiv.org/abs/2508.04660)
*Noah Ziems,Dilara Soylu,Lakshya A Agrawal,Isaac Miller,Liheng Lai,Chen Qian,Kaiqiang Song,Meng Jiang,Dan Klein,Matei Zaharia,Karel D'Oosterlinck,Christopher Potts,Omar Khattab*

Main category: cs.CL

TL;DR: mmGRPO是一种改进的GRPO方法，适用于多模块AI系统，能显著提升任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法在处理混合多个LM调用和不同提示模板的AI系统时效果不佳，因此需要一种新的方法来优化这些系统。

Method: 定义了mmGRPO，这是GRPO的多模块扩展，能够处理不同模块的LM调用和变长或中断的轨迹。

Result: mmGRPO与自动提示优化结合使用，在分类、多跳搜索和隐私保护委托任务中平均提升了11%的准确性，比单独使用提示优化提高了5%。

Conclusion: mmGRPO在多个任务中表现出色，能够提升AI系统的准确性，并已开源。

Abstract: Group Relative Policy Optimization (GRPO) has proven to be an effective tool
for post-training language models (LMs). However, AI systems are increasingly
expressed as modular programs that mix together multiple LM calls with distinct
prompt templates and other tools, and it is not clear how best to leverage GRPO
to improve these systems. We begin to address this challenge by defining
mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by
module across rollouts and handles variable-length and interrupted
trajectories. We find that mmGRPO, composed with automatic prompt optimization,
improves accuracy by 11% on average across classification, many-hop search, and
privacy-preserving delegation tasks against the post-trained LM, and by 5%
against prompt optimization on its own. We open-source mmGRPO in DSPy as the
dspy.GRPO optimizer.

</details>


### [64] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
*Mo Li,L. H. Xu,Qitai Tan,Ting Cao,Yunxin Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Sculptor的框架，通过主动上下文管理工具帮助大型语言模型更好地处理长上下文任务，从而减轻前向干扰并提高推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理长上下文时由于前向干扰而显著性能下降，早期部分的无关信息会干扰推理和记忆回忆。尽管大多数研究集中在外部内存系统上，但本文提出了一个互补方法。

Method: 提出了一种互补方法：赋予LLMs主动上下文管理（ACM）工具，以主动塑造其内部工作内存。介绍了Sculptor框架，该框架为LLMs提供了三种类型的工具：(1) 上下文碎片化，(2) 摘要、隐藏和恢复，以及(3) 智能搜索。

Result: 在信息稀疏基准测试PI-LLM（前向干扰）和NeedleBench Multi-Needle Reasoning上的实验评估表明，Sculptor即使没有特定训练也能显著提高性能，利用LLMs固有的工具调用泛化能力。

Conclusion: 通过引入主动上下文管理，Sculptor不仅减轻了前向干扰，还为跨多种长上下文任务的更可靠推理提供了认知基础，强调了明确的上下文控制策略比仅仅更大的标记窗口更为关键。

Abstract: Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

</details>


### [65] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
*Yunan Zhang,Shuoran Jiang,Mengchen Zhao,Yuefeng Li,Yang Fan,Xiangping Wu,Qingcai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为GeRe的框架，通过使用普通预训练文本来解决LLMs在持续学习中的灾难性遗忘问题。通过引入基于阈值的边缘损失方法，实验结果表明该方法在性能和鲁棒性上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 持续微调LLMs在不同领域时常常遭受灾难性遗忘，这表现为：1) 显著遗忘其通用能力，2) 在之前学习的任务中性能急剧下降。我们需要同时以简单而稳定的方式解决这两个问题。

Method: 我们提出了General Sample Replay (GeRe)，一个使用普通预训练文本进行高效抗遗忘的框架。此外，我们利用神经状态引入了一种基于阈值的边缘(TM)损失的增强激活状态约束优化方法。

Result: 实验结果表明，TM在不同重放策略下 consistently 提高了性能并表现出更好的鲁棒性。我们验证了一个小的、固定的预收集通用重放样本集足以解决这两个问题——保留通用能力的同时促进整体性能。

Conclusion: 我们的工作为未来LLMs的高效重放铺平了道路。

Abstract: The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

</details>


### [66] [FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](https://arxiv.org/abs/2508.04698)
*Thibaut Thonet,Germán Kruszewski,Jos Rozen,Pierre Erbacher,Marc Dymetman*

Main category: cs.CL

TL;DR: 本文研究了在有限数据下个性化偏好对齐的问题，提出了一个高效的参数方法FaST，并在两个数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的对话助手通常以一种通用的方式部署，无法满足个体用户偏好。因此，需要研究在有限数据下个性化偏好对齐的方法。

Method: 本文引入了两个数据集DnD和ELIP，并对多种对齐技术进行了基准测试，同时提出了FaST方法。

Result: 本文提出的FaST方法在PPALLI问题上表现最佳，证明了其有效性。

Conclusion: 本文提出了一种名为FaST的高效参数方法，该方法利用从数据中自动发现的高级特征，在PPALLI问题上取得了最佳的整体性能。

Abstract: LLM-powered conversational assistants are often deployed in a
one-size-fits-all manner, which fails to accommodate individual user
preferences. Recently, LLM personalization -- tailoring models to align with
specific user preferences -- has gained increasing attention as a way to bridge
this gap. In this work, we specifically focus on a practical yet challenging
setting where only a small set of preference annotations can be collected per
user -- a problem we define as Personalized Preference Alignment with Limited
Data (PPALLI). To support research in this area, we introduce two datasets --
DnD and ELIP -- and benchmark a variety of alignment techniques on them. We
further propose FaST, a highly parameter-efficient approach that leverages
high-level features automatically discovered from the data, achieving the best
overall performance.

</details>


### [67] [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)
*Anushka Yadav,Isha Nalawade,Srujana Pillarichety,Yashwanth Babu,Reshmi Ghosh,Samyadeep Basu,Wenlong Zhao,Ali Nasaeh,Sriram Balasubramanian,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 本研究系统地探索了当代语言模型在多跳问答任务中的推理失败，引入了一个新的错误分类框架，揭示了隐藏的错误模式，并提供了增强推理准确性、透明度和鲁棒性的指导。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型在实际AI聊天机器人中的集成带来了在解决需要复杂和多步骤思维过程的高级数学、深度搜索和提取式问答问题方面的突破，但对这些模型为何比通用语言模型更容易产生幻觉的完整理解仍然缺失。因此，本研究旨在系统地探索当代语言模型在多跳问答任务中的推理失败。

Method: 本研究系统地探索了当代语言模型在多跳问答任务中的推理失败，引入了一个新颖的、细致的错误分类框架，从三个关键维度分析失败：涉及的源文档的多样性和独特性（“跳数”）、捕捉相关信息的完整性（“覆盖范围”）以及认知低效性（“过度思考”）。通过严格的人员标注并辅以互补的自动化指标，揭示了隐藏在以准确性为中心的评估下的复杂错误模式。

Result: 本研究通过严谨的人工标注和互补的自动化指标，揭示了隐藏在以准确性为中心的评估下的复杂错误模式，提供了对当前模型认知限制的更深入了解，并为未来语言建模工作提供了增强推理准确性、透明度和鲁棒性的实用指导。

Conclusion: 本研究提供了对当前语言模型在多跳问答任务中推理失败的深入见解，并为未来语言建模工作提供了增强推理准确性、透明度和鲁棒性的实用指导。

Abstract: The emergence of reasoning models and their integration into practical AI
chat bots has led to breakthroughs in solving advanced math, deep search, and
extractive question answering problems that requires a complex and multi-step
thought process. Yet, a complete understanding of why these models hallucinate
more than general purpose language models is missing. In this investigative
study, we systematicallyexplore reasoning failures of contemporary language
models on multi-hop question answering tasks. We introduce a novel, nuanced
error categorization framework that examines failures across three critical
dimensions: the diversity and uniqueness of source documents involved ("hops"),
completeness in capturing relevant information ("coverage"), and cognitive
inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by
complementary automated metrics, our exploration uncovers intricate error
patterns often hidden by accuracy-centric evaluations. This investigative
approach provides deeper insights into the cognitive limitations of current
models and offers actionable guidance toward enhancing reasoning fidelity,
transparency, and robustness in future language modeling efforts.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [68] [Multilingual Source Tracing of Speech Deepfakes: A First Benchmark](https://arxiv.org/abs/2508.04143)
*Xi Xuan,Yang Xiao,Rohan Kumar Das,Tomi Kinnunen*

Main category: eess.AS

TL;DR: 本文介绍了第一个多语言语音深度伪造源追踪的基准，比较研究了基于DSP和SSL的建模方法，并探讨了不同语言微调对跨语言泛化性能的影响。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要集中在检测假语音上，但对追踪生成假语音的源模型关注较少。本文旨在填补这一空白，提供一个多语言语音深度伪造源追踪的基准。

Method: 本文介绍了第一个多语言语音深度伪造源追踪的基准，比较研究了基于DSP和SSL的建模方法，并探讨了不同语言微调对跨语言泛化性能的影响。

Result: 本文的发现提供了关于在训练和推理语言不同时识别语音生成模型的挑战的首次全面见解。

Conclusion: 本文提出了一个多语言语音深度伪造源追踪的基准，为未来的研究提供了基础。

Abstract: Recent progress in generative AI has made it increasingly easy to create
natural-sounding deepfake speech from just a few seconds of audio. While these
tools support helpful applications, they also raise serious concerns by making
it possible to generate convincing fake speech in many languages. Current
research has largely focused on detecting fake speech, but little attention has
been given to tracing the source models used to generate it. This paper
introduces the first benchmark for multilingual speech deepfake source tracing,
covering both mono- and cross-lingual scenarios. We comparatively investigate
DSP- and SSL-based modeling; examine how SSL representations fine-tuned on
different languages impact cross-lingual generalization performance; and
evaluate generalization to unseen languages and speakers. Our findings offer
the first comprehensive insights into the challenges of identifying speech
generation models when training and inference languages differ. The dataset,
protocol and code are available at
https://github.com/xuanxixi/Multilingual-Source-Tracing.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [69] [Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding](https://arxiv.org/abs/2508.03718)
*Mike Gartner*

Main category: cs.CY

TL;DR: 本文旨在通过构建语料库和发布标注基准来改善美国健康保险的可及性。


<details>
  <summary>Details</summary>
Motivation: 美国健康保险体系复杂，对最脆弱群体的理解不足和司法资源有限带来了严重后果。自然语言处理的进步为高效、具体的案例理解提供了机会，从而改善司法和医疗保健的可及性。然而，现有的语料库缺乏评估简单案例所需的上下文。

Method: 本文收集并发布了与美国健康保险相关的权威法律和医学文本语料库，并引入了一个结果预测任务，以及用于该任务的标注基准和训练模型。

Result: 本文发布了相关语料库、标注基准和模型，以支持健康保险上诉的结果预测任务。

Conclusion: 本文提出了一个用于健康保险上诉的结果预测任务，并发布了相关的标注基准和模型，以支持监管和患者自助应用。

Abstract: U.S. health insurance is complex, and inadequate understanding and limited
access to justice have dire implications for the most vulnerable. Advances in
natural language processing present an opportunity to support efficient,
case-specific understanding, and to improve access to justice and healthcare.
Yet existing corpora lack context necessary for assessing even simple cases. We
collect and release a corpus of reputable legal and medical text related to
U.S. health insurance. We also introduce an outcome prediction task for health
insurance appeals designed to support regulatory and patient self-help
applications, and release a labeled benchmark for our task, and models trained
on it.

</details>


### [70] [Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference](https://arxiv.org/abs/2508.04586)
*Nuo Chen,Moming Duan,Andre Huikai Lin,Qian Wang,Jiaying Wu,Bingsheng He*

Main category: cs.CY

TL;DR: 本文分析了人工智能会议面临的可持续性问题，并提出了CFC模型作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能会议的快速扩张导致了集中式会议模式的不可持续性，需要一种新的模型来维持科学传播、公平性和社区福祉。

Method: 本文通过数据分析，识别了人工智能会议面临的四个关键压力点，并提出了CFC模型作为解决方案。

Result: 本文识别了四个关键压力点，并提出了CFC模型，以实现更可持续、包容和有韧性的AI研究路径。

Conclusion: 本文提出了社区联邦会议（CFC）模型，以解决当前人工智能会议面临的可持续性、包容性和韧性问题。

Abstract: Artificial Intelligence (AI) conferences are essential for advancing
research, sharing knowledge, and fostering academic community. However, their
rapid expansion has rendered the centralized conference model increasingly
unsustainable. This paper offers a data-driven diagnosis of a structural crisis
that threatens the foundational goals of scientific dissemination, equity, and
community well-being. We identify four key areas of strain: (1) scientifically,
with per-author publication rates more than doubling over the past decade to
over 4.5 papers annually; (2) environmentally, with the carbon footprint of a
single conference exceeding the daily emissions of its host city; (3)
psychologically, with 71% of online community discourse reflecting negative
sentiment and 35% referencing mental health concerns; and (4) logistically,
with attendance at top conferences such as NeurIPS 2024 beginning to outpace
venue capacity. These pressures point to a system that is misaligned with its
core mission. In response, we propose the Community-Federated Conference (CFC)
model, which separates peer review, presentation, and networking into globally
coordinated but locally organized components, offering a more sustainable,
inclusive, and resilient path forward for AI research.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [71] [A Social Data-Driven System for Identifying Estate-related Events and Topics](https://arxiv.org/abs/2508.03711)
*Wenchuan Mu,Menglin Li,Kwan Hui Lim*

Main category: cs.IR

TL;DR: 本文提出了一种基于语言模型的系统，用于从社交媒体内容中检测和分类与房地产相关的事件。该系统采用分层分类框架，首先过滤相关帖子，然后将其分类为可操作的房地产相关主题。此外，对于缺乏显式地理标签的帖子，我们应用基于变压器的地理定位模块来推断帖子位置。这种综合方法支持及时的数据驱动见解，用于城市管理和运营响应。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台如Twitter和Facebook已经深入到我们的日常生活中，提供动态的本地新闻和个人经历流。这些平台的普及使它们成为识别房地产相关问题的宝贵资源，特别是在城市人口增长的背景下。

Method: 本文提出了一种基于语言模型的系统，用于从社交媒体内容中检测和分类与房地产相关的事件。该系统采用分层分类框架，首先过滤相关帖子，然后将其分类为可操作的房地产相关主题。此外，对于缺乏显式地理标签的帖子，我们应用基于变压器的地理定位模块来推断帖子位置。

Result: 本文提出了一种基于语言模型的系统，用于从社交媒体内容中检测和分类与房地产相关的事件。该系统采用分层分类框架，首先过滤相关帖子，然后将其分类为可操作的房地产相关主题。此外，对于缺乏显式地理标签的帖子，我们应用基于变压器的地理定位模块来推断帖子位置。这种综合方法支持及时的数据驱动见解，用于城市管理和运营响应。

Conclusion: 本文提出了一种基于语言模型的系统，用于从社交媒体内容中检测和分类与房地产相关的事件。该系统采用分层分类框架，首先过滤相关帖子，然后将其分类为可操作的房地产相关主题。此外，对于缺乏显式地理标签的帖子，我们应用基于变压器的地理定位模块来推断帖子位置。这种综合方法支持及时的数据驱动见解，用于城市管理和运营响应。

Abstract: Social media platforms such as Twitter and Facebook have become deeply
embedded in our everyday life, offering a dynamic stream of localized news and
personal experiences. The ubiquity of these platforms position them as valuable
resources for identifying estate-related issues, especially in the context of
growing urban populations. In this work, we present a language model-based
system for the detection and classification of estate-related events from
social media content. Our system employs a hierarchical classification
framework to first filter relevant posts and then categorize them into
actionable estate-related topics. Additionally, for posts lacking explicit
geotags, we apply a transformer-based geolocation module to infer posting
locations at the point-of-interest level. This integrated approach supports
timely, data-driven insights for urban management, operational response and
situational awareness.

</details>


### [72] [ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval](https://arxiv.org/abs/2508.04001)
*Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Zhichao Xu,Zhan Su,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 本文提出了一种名为ConvMix的混合标准框架，用于增强对话密集检索。通过利用大语言模型的帮助，设计了一个可扩展的双向相关性判断增强模式，并集成了质量控制机制以获得语义多样化的样本和近分布监督。实验结果表明，该框架在多个基准测试中表现优于之前的基线方法。


<details>
  <summary>Details</summary>
Motivation: 对话搜索旨在通过多轮交互满足用户复杂的信息需求。关键挑战在于从上下文相关的查询中揭示真实的用户搜索意图。以前的研究通过微调对话密集检索器来实现对话搜索，但这种训练范式遇到了数据稀缺问题。

Method: 我们提出了ConvMix，一个混合标准框架，用于增强对话密集检索，该框架比现有的数据增强框架涵盖更多方面。我们设计了一个可扩展的双向相关性判断增强模式，利用大语言模型的帮助。此外，我们将框架与质量控制机制集成，以获得语义多样化的样本和近分布监督，以结合各种标注数据。

Result: 在五个广泛使用的基准测试中，通过我们的ConvMix框架训练的对话密集检索器的表现优于之前的基线方法。

Conclusion: 实验结果表明，通过我们的ConvMix框架训练的对话密集检索器优于之前的基线方法，这证明了我们的有效性。

Abstract: Conversational search aims to satisfy users' complex information needs via
multiple-turn interactions. The key challenge lies in revealing real users'
search intent from the context-dependent queries. Previous studies achieve
conversational search by fine-tuning a conversational dense retriever with
relevance judgments between pairs of context-dependent queries and documents.
However, this training paradigm encounters data scarcity issues. To this end,
we propose ConvMix, a mixed-criteria framework to augment conversational dense
retrieval, which covers more aspects than existing data augmentation
frameworks. We design a two-sided relevance judgment augmentation schema in a
scalable manner via the aid of large language models. Besides, we integrate the
framework with quality control mechanisms to obtain semantically diverse
samples and near-distribution supervisions to combine various annotated data.
Experimental results on five widely used benchmarks show that the
conversational dense retriever trained by our ConvMix framework outperforms
previous baseline methods, which demonstrates our superior effectiveness.

</details>


### [73] [Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation](https://arxiv.org/abs/2508.04571)
*Claudio Pomo,Matteo Attimonelli,Danilo Danese,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 本文研究了多模态嵌入在推荐系统中的作用，发现大型视觉-语言模型（LVLMs）能够生成语义对齐的嵌入，并通过解码为结构化文本描述来验证其多模态理解能力，从而提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 当前的研究不清楚多模态推荐系统的优势是来自真正的多模态理解还是模型复杂性的增加。因此，需要一种能够提供语义信息的多模态嵌入方法。

Method: 本文利用大型视觉-语言模型（LVLMs）通过结构化提示生成多模态设计的嵌入，从而获得语义对齐的表示。

Result: 实验结果显示，使用LVLMs生成的嵌入在多个设置中表现出显著的性能提升，并且可以解码为结构化的文本描述，从而直接评估其多模态理解能力。

Conclusion: 本文强调了语义丰富的表示的重要性，并将大型视觉-语言模型（LVLMs）作为构建强大且有意义的多模态表示的基础。

Abstract: Multimodal Recommender Systems aim to improve recommendation accuracy by
integrating heterogeneous content, such as images and textual metadata. While
effective, it remains unclear whether their gains stem from true multimodal
understanding or increased model complexity. This work investigates the role of
multimodal item embeddings, emphasizing the semantic informativeness of the
representations. Initial experiments reveal that embeddings from standard
extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on
modality-specific encoders and ad hoc fusion strategies that lack control over
cross-modal alignment. To overcome these limitations, we leverage Large
Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via
structured prompts. This approach yields semantically aligned representations
without requiring any fusion. Experiments across multiple settings show notable
performance improvements. Furthermore, LVLMs embeddings offer a distinctive
advantage: they can be decoded into structured textual descriptions, enabling
direct assessment of their multimodal comprehension. When such descriptions are
incorporated as side content into recommender systems, they improve
recommendation performance, empirically validating the semantic depth and
alignment encoded within LVLMs outputs. Our study highlights the importance of
semantically rich representations and positions LVLMs as a compelling
foundation for building robust and meaningful multimodal representations in
recommendation tasks.

</details>


### [74] [Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering](https://arxiv.org/abs/2508.04683)
*Karthik Menon,Batool Arhamna Haider,Muhammad Arham,Kanwal Mehreen,Ram Mohan Rao Kadiyala,Hamza Farooq*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study introduces Query Attribute Modeling (QAM), a hybrid framework that
enhances search precision and relevance by decomposing open text queries into
structured metadata tags and semantic elements. QAM addresses traditional
search limitations by automatically extracting metadata filters from free-form
text queries, reducing noise and enabling focused retrieval of relevant items.
  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique
items with 40,000+ reviews and detailed product attributes) demonstrated QAM's
superior performance, achieving a mean average precision at 5 (mAP@5) of
52.99\%. This represents significant improvement over conventional methods,
including BM25 keyword search, encoder-based semantic similarity search,
cross-encoder re-ranking, and hybrid search combining BM25 and semantic results
via Reciprocal Rank Fusion (RRF). The results establish QAM as a robust
solution for Enterprise Search applications, particularly in e-commerce
systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [75] [ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants](https://arxiv.org/abs/2508.03936)
*Xiangzhe Xu,Guangyu Shen,Zian Su,Siyuan Cheng,Hanxi Guo,Lu Yan,Xuan Chen,Jiasheng Jiang,Xiaolong Jin,Chengpeng Wang,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.CR

TL;DR: ASTRA is an automated agent system that systematically uncovers safety flaws in AI-driven code generation and security guidance systems, finding more issues and producing more effective alignment training.


<details>
  <summary>Details</summary>
Motivation: Current red-teaming tools often rely on fixed benchmarks or unrealistic prompts, missing many real-world vulnerabilities. There is a need for a system that can systematically uncover safety flaws in AI-driven code generation and security guidance systems.

Method: ASTRA is an automated agent system that works in three stages: building structured domain-specific knowledge graphs, performing online vulnerability exploration, and generating high-quality violation-inducing cases.

Result: ASTRA finds 11-66% more issues than existing techniques and produces test cases that lead to 17% more effective alignment training.

Conclusion: ASTRA demonstrates practical value for building safer AI systems by finding more issues and producing more effective alignment training.

Abstract: AI coding assistants like GitHub Copilot are rapidly transforming software
development, but their safety remains deeply uncertain-especially in
high-stakes domains like cybersecurity. Current red-teaming tools often rely on
fixed benchmarks or unrealistic prompts, missing many real-world
vulnerabilities. We present ASTRA, an automated agent system designed to
systematically uncover safety flaws in AI-driven code generation and security
guidance systems. ASTRA works in three stages: (1) it builds structured
domain-specific knowledge graphs that model complex software tasks and known
weaknesses; (2) it performs online vulnerability exploration of each target
model by adaptively probing both its input space, i.e., the spatial
exploration, and its reasoning processes, i.e., the temporal exploration,
guided by the knowledge graphs; and (3) it generates high-quality
violation-inducing cases to improve model alignment. Unlike prior methods,
ASTRA focuses on realistic inputs-requests that developers might actually
ask-and uses both offline abstraction guided domain modeling and online domain
knowledge graph adaptation to surface corner-case vulnerabilities. Across two
major evaluation domains, ASTRA finds 11-66% more issues than existing
techniques and produces test cases that lead to 17% more effective alignment
training, showing its practical value for building safer AI systems.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [76] [Graph Representation Learning with Massive Unlabeled Data for Rumor Detection](https://arxiv.org/abs/2508.04252)
*Chaoqun Cui,Caiyan Jia*

Main category: cs.SI

TL;DR: 本文通过利用大规模未标记主题数据集，改进图表示学习模型的语义学习能力，并验证了通用图自监督学习方法在谣言检测任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的谣言检测方法面临难以获取大规模标记谣言数据集的问题，导致模型泛化能力差，且在新事件上的性能下降。由于谣言具有时间敏感性，通常与热点话题或新出现的事件相关。

Method: 使用从社交媒体平台微博和Twitter爬取的大规模未标记主题数据集，结合声明传播结构来改进图表示学习模型的语义学习能力。使用三种典型的图自监督方法（InfoGraph、JOAO和GraphMAE）在两种常用的训练策略下验证通用图半监督方法在谣言检测任务中的性能。

Result: 实验结果表明，这些通用的图自监督学习方法优于专门为谣言检测任务设计的先前方法，并在少样本条件下表现出良好的性能。

Conclusion: 这些通用的图自监督学习方法在少样本条件下表现出色，证明了我们的大规模未标记主题数据集有助于提高模型的泛化能力。

Abstract: With the development of social media, rumors spread quickly, cause great harm
to society and economy. Thereby, many effective rumor detection methods have
been developed, among which the rumor propagation structure learning based
methods are particularly effective compared to other methods. However, the
existing methods still suffer from many issues including the difficulty to
obtain large-scale labeled rumor datasets, which leads to the low
generalization ability and the performance degeneration on new events since
rumors are time-critical and usually appear with hot topics or newly emergent
events. In order to solve the above problems, in this study, we used
large-scale unlabeled topic datasets crawled from the social media platform
Weibo and Twitter with claim propagation structure to improve the semantic
learning ability of a graph reprentation learing model on various topics. We
use three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAE
in two commonly used training strategies, to verify the performance of general
graph semi-supervised methods in rumor detection tasks. In addition, for
alleviating the time and topic difference between unlabeled topic data and
rumor data, we also collected a rumor dataset covering a variety of topics over
a decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Our
experiments show that these general graph self-supervised learning methods
outperform previous methods specifically designed for rumor detection tasks and
achieve good performance under few-shot conditions, demonstrating the better
generalization ability with the help of our massive unlabeled topic dataset.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: 本文提出了CX-Mind，这是一个基于课程的强化学习和可验证过程奖励的生成模型，用于CXR任务的交错推理。CX-Mind在多个指标上显著优于现有的医学和通用领域MLLM，并在真实临床数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型主要依赖于“一次性”诊断方法，缺乏推理过程的可验证监督，导致多任务CXR诊断面临推理时间长、奖励稀疏和频繁幻觉等挑战。

Method: 我们提出了CX-Mind，这是第一个实现CXR任务交错“思考-回答”推理的生成模型，由基于课程的强化学习和可验证过程奖励（CuRL-VPR）驱动。我们构建了一个指令调优数据集CX-Set，包含708,473张图像和2,619,148个样本，并生成了42,828个由临床报告监督的高质量交错推理数据点。在Group Relative Policy Optimization框架下进行了两个阶段的优化：首先通过封闭域任务稳定基本推理，然后转移到开放域诊断，结合基于规则的条件过程奖励以绕过预训练奖励模型的需求。

Result: CX-Mind在视觉理解、文本生成和时空对齐方面显著优于现有的医学和通用领域MLLM，平均性能提升了25.1%。在真实临床数据集（Rui-CXR）上，CX-Mind在14种疾病上的平均召回率@1明显超越了第二好的结果，多中心专家评估进一步证实了其在多个维度上的临床实用性。

Conclusion: CX-Mind显著优于现有的医学和通用领域MLLM，在视觉理解、文本生成和时空对齐方面取得了平均25.1%的性能提升。在真实临床数据集（Rui-CXR）上，CX-Mind在14种疾病上的平均召回率@1明显超越了第二好的结果，多中心专家评估进一步证实了其在多个维度上的临床实用性。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [78] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: This paper introduces GTPO, a new policy optimization method that addresses the limitations of GRPO by protecting conflict tokens and preventing policy collapse, leading to improved performance without requiring a reference model.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of GRPO, which include conflicting gradient updates and the degradation of output distribution due to negatively rewarded completions.

Method: GTPO identifies conflict tokens and protects them by skipping negative updates while amplifying positive ones. It also filters out completions with high entropy to prevent policy collapse.

Result: GTPO demonstrates greater training stability and improved performance on GSM8K, MATH, and AIME 2024 benchmarks compared to GRPO.

Conclusion: GTPO provides a more stable and effective policy optimization strategy compared to GRPO, without relying on KL-divergence regularization or a reference model, and shows improved performance on multiple benchmarks.

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [79] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的策略优化框架，解决了在多个样本响应收敛时的优势函数退化问题，并在多个数学推理任务中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前在使用基于规则的奖励进行强化学习时，当多个样本响应收敛到相同结果时，会导致优势函数退化，从而影响训练效率和下游性能。因此，需要一种新的方法来解决这一问题。

Method: 本文提出了一种一致性感知的策略优化框架，引入了基于结果一致性的结构化全局奖励，并结合基于熵的软混合机制，以自适应平衡局部优势估计和全局优化。

Result: 本文的方法在多个数学推理基准上取得了显著的性能提升，验证了其有效性。此外，代码已公开在GitHub上，便于进一步研究和应用。

Conclusion: 本文提出了一种一致性感知的策略优化框架，通过引入基于结果一致性的结构化全局奖励，解决了在多个样本响应收敛到相同结果时优势函数退化的问题。实验结果显示该方法在多个数学推理基准上取得了显著的性能提升，证明了其有效性与通用性。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [80] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 本文提出了因果反射框架，以增强代理的因果推理能力，并通过LLMs将因果结果转化为自然语言解释。


<details>
  <summary>Details</summary>
Motivation: LLMs和传统强化学习代理在因果理解方面存在不足，它们依赖于虚假的相关性和脆弱的模式。

Method: 我们引入了因果反射框架，该框架显式地将因果关系建模为状态、动作、时间和扰动的动态函数，并定义了一个正式的反射机制来识别预测和观察结果之间的不匹配，并生成因果假设以修订代理的内部模型。

Result: 通过将LLMs作为结构化的推理引擎，将形式化的因果输出转化为自然语言解释和反事实，我们的框架实现了对延迟和非线性效应的推理。

Conclusion: 我们的框架为因果反思代理奠定了理论基础，这些代理可以在不断变化的环境中适应、自我纠正并传达因果理解。

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [81] [MegaWika 2: A More Comprehensive Multilingual Collection of Articles and their Sources](https://arxiv.org/abs/2508.03828)
*Samuel Barham,Chandler May,Benjamin Van Durme*

Main category: cs.DL

TL;DR: MegaWika 2 是一个大规模、多语言的维基百科数据集，用于支持事实核查和跨时间和语言的分析研究。


<details>
  <summary>Details</summary>
Motivation: 为了支持事实核查和跨时间和语言的分析研究，MegaWika 2 对原始 MegaWika 进行了重大升级，扩展了六倍的文章数量和两倍的完整网络引用数量。

Method: MegaWika 2 是一个大规模、多语言的维基百科文章数据集，包含引用和网络来源；文章以丰富的数据结构表示，网络来源文本与文章中的引用精确字符偏移量一起存储。

Result: MegaWika 2 支持报告生成研究，并专注于支持事实核查和跨时间和语言的分析。

Conclusion: MegaWika 2 是一个重要的升级版本，旨在支持事实核查和跨时间和语言的分析研究。

Abstract: We introduce MegaWika 2, a large, multilingual dataset of Wikipedia articles
with their citations and scraped web sources; articles are represented in a
rich data structure, and scraped source texts are stored inline with precise
character offsets of their citations in the article text. MegaWika 2 is a major
upgrade from the original MegaWika, spanning six times as many articles and
twice as many fully scraped citations. Both MegaWika and MegaWika 2 support
report generation research ; whereas MegaWika also focused on supporting
question answering and retrieval applications, MegaWika 2 is designed to
support fact checking and analyses across time and language.

</details>


### [82] [Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers](https://arxiv.org/abs/2508.03962)
*Paris Koloveas,Serafeim Chatzopoulos,Dionysis Diamantis,Christos Tryfonopoulos,Thanasis Vergoulis*

Main category: cs.DL

TL;DR: 本文在BIP! Finder中引入了摘要功能，以帮助科学家更快地理解和综合科学文献。该功能生成两种类型的摘要，从而显著加速文献发现和理解。


<details>
  <summary>Details</summary>
Motivation: 随着科学文献的不断增长，科学家们难以从论文列表中综合理解一个主题。即使识别出有前景的论文，他们仍需逐一阅读标题和摘要，以应对偶尔冲突的研究结果，这是一项繁琐的任务。

Method: 本文在BIP! Finder中引入了摘要功能，允许用户从排名靠前的搜索结果中生成两种类型的摘要：一种是简洁的摘要，用于快速了解；另一种是更全面的文献综述式摘要，用于更深入、有条理的理解。

Result: 本文提出的摘要功能能够动态利用BIP! Finder已有的基于影响力的排名和过滤功能，生成上下文敏感的合成叙述，从而显著加速文献发现和理解。

Conclusion: 本文提出了一种摘要功能，以帮助科学家更快地理解和综合科学文献。该功能利用BIP! Finder已有的基于影响力的排名和过滤功能，生成两种类型的摘要，从而显著加速文献发现和理解。

Abstract: The growing volume of scientific literature makes it challenging for
scientists to move from a list of papers to a synthesized understanding of a
topic. Because of the constant influx of new papers on a daily basis, even if a
scientist identifies a promising set of papers, they still face the tedious
task of individually reading through dozens of titles and abstracts to make
sense of occasionally conflicting findings. To address this critical bottleneck
in the research workflow, we introduce a summarization feature to BIP! Finder,
a scholarly search engine that ranks literature based on distinct impact
aspects like popularity and influence. Our approach enables users to generate
two types of summaries from top-ranked search results: a concise summary for an
instantaneous at-a-glance comprehension and a more comprehensive literature
review-style summary for greater, better-organized comprehension. This ability
dynamically leverages BIP! Finder's already existing impact-based ranking and
filtering features to generate context-sensitive, synthesized narratives that
can significantly accelerate literature discovery and comprehension.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [83] [ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations](https://arxiv.org/abs/2508.04166)
*Subhankar Swain,Naquee Rizwan,Nayandeep Deb,Vishwajeet Singh Solanki,Vishwa Gangadhar S,Animesh Mukherjee*

Main category: cs.CV

TL;DR: 本文介绍了一个包含6300个真实世界表情包数据集，并通过两阶段标注和标签生成模块提高了有害内容检测的性能。


<details>
  <summary>Details</summary>
Motivation: 由于数据访问限制和数据集整理的高成本，阻碍了强大的表情包审核系统的开发。

Method: 我们引入了一个首次的包含6300个真实世界基于表情包的帖子的数据集，并进行了两阶段的标注：(i) 二分类为有害和正常，(ii) 对有害的表情包进行细粒度标记为仇恨、危险或冒犯性。此外，我们提出了一种标签生成模块，以产生社会基础的标签。

Result: 实验结果表明，结合这些标签可以显著提高最先进的VLM检测任务的性能。

Conclusion: 我们的贡献为改进多模态在线环境中的内容审核提供了新颖且可扩展的基础。

Abstract: The 2025 Global Risks Report identifies state-based armed conflict and
societal polarisation among the most pressing global threats, with social media
playing a central role in amplifying toxic discourse. Memes, as a widely used
mode of online communication, often serve as vehicles for spreading harmful
content. However, limitations in data accessibility and the high cost of
dataset curation hinder the development of robust meme moderation systems. To
address this challenge, in this work, we introduce a first-of-its-kind dataset
of 6,300 real-world meme-based posts annotated in two stages: (i) binary
classification into toxic and normal, and (ii) fine-grained labelling of toxic
memes as hateful, dangerous, or offensive. A key feature of this dataset is
that it is enriched with auxiliary metadata of socially relevant tags,
enhancing the context of each meme. In addition, we propose a tag generation
module that produces socially grounded tags, because most in-the-wild memes
often do not come with tags. Experimental results show that incorporating these
tags substantially enhances the performance of state-of-the-art VLMs detection
tasks. Our contributions offer a novel and scalable foundation for improved
content moderation in multimodal online environments.

</details>


### [84] [FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding](https://arxiv.org/abs/2508.04469)
*Emmanuelle Bourigault,Pauline Bourigault*

Main category: cs.CV

TL;DR: FrEVL框架展示了冻结的预训练嵌入在视觉-语言理解任务中的有效性，并提供了一种更高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 由于视觉-语言模型的部署受到计算需求的限制，因此需要一种更高效的解决方案。

Method: FrEVL框架探索了冻结的预训练嵌入是否能够支持有效的视觉-语言理解。

Result: FrEVL在标准基准测试中实现了85%到95%的最先进的性能，同时仅使用68.4M可训练参数。此外，FrEVL在端到端计算中提供了2.3倍的速度提升和52%的能耗降低。

Conclusion: FrEVL框架表明，冻结的预训练嵌入可以作为有效视觉-语言理解的基础，并且在计算资源受限的情况下具有优势。

Abstract: The deployment of vision-language models remains constrained by substantial
computational requirements. We present \textbf{FrEVL}, a framework exploring
whether frozen pretrained embeddings can support effective vision-language
understanding. Our analysis reveals that frozen embeddings contain rich
information for discriminative tasks, achieving 85\% to 95\% of
state-of-the-art performance on standard benchmarks with only 68.4M trainable
parameters. This performance dichotomy reveals a critical insight: frozen
embedding effectiveness depends on alignment between pretraining objectives and
downstream task requirements. When accounting for end-to-end computation
including embedding extraction, FrEVL provides $2.3\times$ speedup with 52\%
lower energy consumption, making it suitable for scenarios with pre-computable
inputs or when deployment constraints outweigh marginal performance gains. Our
evaluation provides practitioners with guidance on when frozen embedding
approaches represent viable alternatives to full model deployment. We will
release our complete implementation and evaluation framework to facilitate
further research into efficient multi-modal understanding.

</details>


### [85] [Analyzing and Mitigating Object Hallucination: A Training Bias Perspective](https://arxiv.org/abs/2508.04567)
*Yifan Li,Kun Zhou,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Main category: cs.CV

TL;DR: 本文研究了训练数据在大型视觉语言模型幻觉中的作用，并提出了一种名为Obliviate的高效遗忘方法，以减少对象幻觉。


<details>
  <summary>Details</summary>
Motivation: 由于大型视觉语言模型在训练数据规模扩大后仍然存在幻觉问题，本文旨在系统地研究训练数据在幻觉中的作用。

Method: 本文提出了Obliviate方法，该方法通过识别训练数据中真实标签和模型输出之间的差异作为偏差的代理，并采用参数和数据高效的微调策略，仅更新语言模型头。

Result: 通过在POPEv2上的全面评估，发现当前的LVLMs存在训练偏差：它们未能充分利用训练数据，并且在训练期间看到的图像上更频繁地产生幻觉。此外，Obliviate方法在仅重新使用训练数据并更新约2%的参数的情况下，显著减少了判别性和生成性任务中的幻觉。

Conclusion: 本文提出了一种名为Obliviate的高效且轻量级的遗忘方法，旨在通过训练偏差遗忘来减轻对象幻觉。实验表明，该方法在减少幻觉方面非常有效，并且在模型大小和训练数据量方面具有很强的可扩展性。

Abstract: As scaling up training data has significantly improved the general multimodal
capabilities of Large Vision-Language Models (LVLMs), they still suffer from
the hallucination issue, generating text that is inconsistent with the visual
input. This phenomenon motivates us to systematically investigate the role of
training data in hallucination. We introduce a new benchmark, POPEv2, which
consists of counterfactual images collected from the training data of LVLMs
with certain objects masked. Through comprehensive evaluation on POPEv2, we
find that current LVLMs suffer from training bias: they fail to fully leverage
their training data and hallucinate more frequently on images seen during
training. Specifically, they perform poorly on counterfactual images, often
incorrectly answering ``Yes'' to questions about masked objects. To understand
this issue, we conduct probing experiments on the models' internal components,
revealing that this training bias is primarily located in the language modeling
(LM) head. Based on these findings, we propose Obliviate, an efficient and
lightweight unlearning method designed to mitigate object hallucination via
training bias unlearning. Obliviate identifies the discrepancy between
ground-truth labels and model outputs on the training data as a proxy for bias
and adopts a parameter- and data-efficient fine-tuning strategy that only
updates the LM head. Extensive experiments demonstrate the effectiveness of our
approach. While only reusing the training data and updating approximately 2\%
of the parameters, Obliviate significantly reduces hallucination across both
discriminative and generative tasks. Furthermore, it demonstrates strong
scalability with respect to both model size (2B to 72B) and training data
volume, and exhibits promising generalization to hallucination types beyond
object-level hallucination. Our code and data will be publicly released.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: 本文介绍了一种名为AgREE的新框架，用于解决开放域知识图谱补全中的挑战，特别是在处理新兴实体时。该框架通过结合迭代检索和多步推理来动态构建知识图谱三元组，并在无需训练的情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 开放域知识图谱补全（KGC）面临重大挑战，特别是在考虑日常新闻中不断出现的新实体时。现有的KGC方法主要依赖于预训练语言模型的参数知识、预先构建的查询或单步检索，通常需要大量监督和训练数据。即使如此，它们往往无法捕捉到不受欢迎和/或新兴实体的全面和最新信息。

Method: 我们引入了Agentic Reasoning for Emerging Entities (AgREE)，这是一个结合迭代检索动作和多步推理的新型基于代理的框架，以动态构建丰富的知识图谱三元组。

Result: 实验表明，尽管不需要任何训练努力，AgREE在构建知识图谱三元组方面显著优于现有方法，特别是对于在语言模型训练过程中未见过的新兴实体，比之前的方法提高了多达13.7%。此外，我们提出了一种新的评估方法，解决了现有设置的一个基本弱点，并为新兴实体的KGC提出了一个新的基准。

Conclusion: 我们的工作展示了将基于代理的推理与战略信息检索相结合在动态信息环境中保持最新知识图谱的有效性。

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [87] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: The paper introduces D2Snap, a DOM downsampling algorithm that performs as well as GUI snapshots for web agents, showing that DOM hierarchy is beneficial for LLMs.


<details>
  <summary>Details</summary>
Motivation: Current web agents rely on GUI snapshots, which are limited by their reliance on images and the inability of LLMs to interpret code effectively. DOM snapshots offer an alternative but face challenges due to large input token sizes.

Method: The paper proposes D2Snap, a DOM downsampling algorithm based on a GPT-4o backend, and evaluates it on tasks from the Online-Mind2Web dataset.

Result: D2Snap-downsampled DOM snapshots achieved a 67% success rate, matching a 65% baseline using grounded GUI snapshots. Some configurations outperformed the baseline by 8% while staying within the model's context window.

Conclusion: D2Snap-downsampled DOM snapshots achieve comparable success rates to grounded GUI snapshots, with some configurations outperforming the baseline. The study shows that DOM-inherent hierarchy is a strong UI feature for LLMs.

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [88] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文是对基于操作系统代理的全面调查，涵盖了其基本概念、方法论、评估和挑战，并提供了开源资源以促进进一步的研究。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的发展，创建类似电影中的J.A.R.V.I.S的AI助手已成为可能。因此，本文旨在对这些先进的代理进行系统性的回顾。

Method: 本文对OS代理进行了全面的调查，包括其基本概念、关键组件、方法论、评估协议和基准测试，以及当前挑战和未来研究方向。

Result: 本文详细回顾了OS代理的构建方法、评估协议和基准测试，并讨论了当前挑战和未来研究方向，如安全性和隐私、个性化和自我进化。

Conclusion: 本文旨在整合OS代理研究的现状，提供指导学术研究和工业发展的见解。维护了一个开源GitHub仓库，以促进该领域的进一步创新。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [89] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent是一种自主进化的框架，使计算机使用代理能够通过经验学习掌握新的软件环境，从而显著提高成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在处理缺乏人类注释的新颖和专业软件时存在困难，因此需要一种自主进化的框架来解决这一问题。

Method: SEAgent通过经验学习、世界状态模型和课程生成器，以及专家到通用的训练策略，使计算机使用代理能够自主掌握新的软件环境。

Result: SEAgent在五个新的软件环境中实现了23.2%的成功率提升，从11.3%提高到34.5%。

Conclusion: SEAgent通过自主进化框架使计算机使用代理能够持续自主进化，并在五个新的软件环境中实现了比现有竞争代理更高的成功率。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [90] [MD-LLM-1: A Large Language Model for Molecular Dynamics](https://arxiv.org/abs/2508.03709)
*Mhd Hussein Murtada,Z. Faidon Brotzakis,Michele Vendruscolo*

Main category: q-bio.BM

TL;DR: 本文介绍了MD-LLM框架，通过微调Mistral 7B来学习蛋白质动力学并预测其他构象状态，尽管尚未明确建模其热力学和动力学。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习在解决分子动力学计算密集问题上的机会。

Method: 引入了分子动力学大语言模型（MD-LLM）框架，通过微调Mistral 7B来实现对蛋白质动力学的学习和未见状态的发现。

Result: 训练在一个构象状态下可以预测其他构象状态，表明MD-LLM-1能够学习蛋白质构象景观的原理。

Conclusion: MD-LLM-1 can learn the principles for the exploration of the conformational landscapes of proteins, although it is not yet modeling explicitly their thermodynamics and kinetics.

Abstract: Molecular dynamics (MD) is a powerful approach for modelling molecular
systems, but it remains computationally intensive on spatial and time scales of
many macromolecular systems of biological interest. To explore the
opportunities offered by deep learning to address this problem, we introduce a
Molecular Dynamics Large Language Model (MD-LLM) framework to illustrate how
LLMs can be leveraged to learn protein dynamics and discover states not seen in
training. By applying MD-LLM-1, the first implementation of this approach,
obtained by fine-tuning Mistral 7B, to the T4 lysozyme and Mad2 protein
systems, we show that training on one conformational state enables the
prediction of other conformational states. These results indicate that MD-LLM-1
can learn the principles for the exploration of the conformational landscapes
of proteins, although it is not yet modeling explicitly their thermodynamics
and kinetics.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 114]
- [cs.RO](#cs.RO) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.CV](#cs.CV) [Total: 12]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications](https://arxiv.org/abs/2510.21762)
*Eric Jeangirard*

Main category: cs.CL

TL;DR: 本文介绍了一个包含833k段文本的语料库，这些文本来自CC-BY许可的科学出版物，分为四个类别：致谢、数据提及、软件/代码提及和临床试验提及。


<details>
  <summary>Details</summary>
Motivation: 为了训练文本分类模型和开发科学文献挖掘的命名实体识别系统，需要一个公开可用的语料库。

Method: 从法国开放科学监测语料库中提取文本，并使用GROBID进行处理，对每个段落进行语言识别（使用fastText）和科学领域分类（来自OpenAlex）。

Result: 创建了一个公开可用的语料库，可用于训练文本分类模型和开发命名实体识别系统。

Conclusion: 该语料库为科学文献挖掘提供了重要的资源，并可通过HuggingFace获取。

Abstract: We present a dataset of 833k paragraphs extracted from CC-BY licensed
scientific publications, classified into four categories: acknowledgments, data
mentions, software/code mentions, and clinical trial mentions. The paragraphs
are primarily in English and French, with additional European languages
represented. Each paragraph is annotated with language identification (using
fastText) and scientific domain (from OpenAlex). This dataset, derived from the
French Open Science Monitor corpus and processed using GROBID, enables training
of text classification models and development of named entity recognition
systems for scientific literature mining. The dataset is publicly available on
HuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.

</details>


### [2] [Policy Optimization Prefers The Path of Least Resistance](https://arxiv.org/abs/2510.21853)
*Debdeep Sanyal,Aakash Sen Sharma,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 政策优化倾向于选择最简单的奖励组件，这可能导致策略退化为仅包含答案的格式，尽管有更高的奖励权重，这种现象仍然存在。这揭示了在对齐过程中奖励黑客的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的政策优化（PO）算法在严格限制的思考-然后回答格式下表现良好，但当这些刚性约束被放松为开放式的链式思维（CoT）结构时，PO的行为仍是一个研究不足的问题。

Method: 我们通过一系列受控的奖励分解实验来形式化这一原则，展示了PO系统优先优化最简单的奖励组件的清晰层次结构。

Result: 政策优化通常会遵循阻力最小的路径，当允许策略交错推理和回答时，它会学习丢弃显式推理，导致策略退化为仅包含<answer>的格式。这种结果在各种模型和算法中都成立。

Conclusion: 我们的发现揭示了赋予策略自由度的双刃剑效应：虽然对于发现高奖励捷径是必要的，但它也创造了利用奖励函数最简单方面的强大激励，这对对齐下的奖励黑客提出了关键挑战。

Abstract: Policy optimization (PO) algorithms are used to refine Large Language Models
for complex, multi-step reasoning. Current state-of-the-art pipelines enforce a
strict think-then-answer format to elicit chain-of-thought (CoT); however, the
behavior of PO when these rigid constraints are relaxed into an open-ended CoT
structure remains an under-studied question. We investigate this gap with an
extensive suite of controlled experiments and identify a consistent principle:
\textit{policy optimization consistently follows the path of least resistance}.
When afforded the flexibility to interleave reasoning and response, policy
optimization consistently learns to discard explicit reasoning, causing the
policy to degenerate to a direct \texttt{<answer>}-only format. This outcome
holds true across various models and algorithms. We find that this collapse in
format is persistent even when the complex \texttt{<think><answer>} format is
assigned up to 4x larger reward weights. We formalize this principle through a
series of controlled reward decomposition experiments, demonstrating a clear
hierarchy: PO systematically optimizes for the simplest reward component first,
a preference that holds even when faced with mutually exclusive choices or
strong incentives for more complex behaviors. Finally, we show that successful
convergence on the high-reward shortcut is not a low-effort drift but is driven
by the optimization process that requires the KL-regularized policy to have
sufficient freedom to make a significant shift from its initial prior. Our
findings reveal that granting policies the freedom to diverge is a double-edged
sword: while necessary for discovering high-reward shortcuts, it also creates a
powerful incentive to game the simplest aspects of the reward function, posing
a critical challenge for reward hacking under alignment.

</details>


### [3] [Language Ranker: A Lightweight Ranking framework for LLM Decoding](https://arxiv.org/abs/2510.21883)
*Chenheng Zhang,Tianqi Du,Jizhe Zhang,Mingqing Xiao,Yifei Wang,Yisen Wang,Zhouchen Lin*

Main category: cs.CL

TL;DR: 本文提出 Language Ranker，通过推荐系统视角优化语言模型解码过程，实现高效且有效的响应生成。


<details>
  <summary>Details</summary>
Motivation: 传统解码方法和奖励模型存在明显的冗余问题，需要一种更高效的方法来优化解码过程。

Method: 通过推荐系统视角重新审视语言模型生成，引入一个轻量级模块，利用基础模型提取的特征对候选响应进行重新排序。

Result: 实验结果表明，Language Ranker 在各种任务中表现与大规模奖励模型相当，仅需不到0.5M的额外参数，显著降低了训练和推理阶段的计算开销。

Conclusion: Language Ranker 的方法在效率和效果上都表现出色，展示了其潜力，可以充分释放大型语言模型的能力。

Abstract: Conventional research on large language models (LLMs) has primarily focused
on refining output distributions, while paying less attention to the decoding
process that transforms these distributions into final responses. Recent
advances, such as scaling the computation of inference time with reward models,
have underscored the importance of decoding, but these methods often suffer
from high computational costs and limited applicability. In this paper, we
revisit LLM generation through the lens of recommender systems, conceptualizing
the decoding process as analogous to the ranking stage in recommendation
pipelines. From this perspective, we observe that both traditional decoding
methods and reward models exhibit clear limitations such as redundancy.
Motivated by this insight, we propose Language Ranker, a novel framework that
introduces a lightweight module to rerank candidate responses using features
extracted by the base model. Experiments across a wide range of tasks show that
Language Ranker achieves performance comparable to large-scale reward models,
while requiring only <0.5M additional parameters, significantly reducing the
computational overhead during both training and inference stages. This
highlights the efficiency and effectiveness of our method, showcasing its
potential to fully unlock the capabilities of LLMs.

</details>


### [4] [Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks](https://arxiv.org/abs/2510.21884)
*Avinash Patil*

Main category: cs.CL

TL;DR: 本研究提出了RACE框架，用于评估大型语言模型（LLM）生成的解释与基于逻辑回归基线的可解释特征重要性评分之间的对齐程度。通过分析四个文本分类数据集，发现正确预测中支持特征的覆盖率更高，而错误预测中矛盾特征的覆盖率更高。该研究揭示了LLM解释的表面级和灵活证据重用，并指出在错误情况下可能放大误导性线索。RACE为评估神经语言模型中的推理完整性提供了定量基础。


<details>
  <summary>Details</summary>
Motivation: The growing adoption of machine learning (ML) in sensitive domains has heightened the demand for transparent and interpretable artificial intelligence. Large Language Models (LLMs) are increasingly capable of producing natural language explanations, yet it remains unclear whether these rationales faithfully capture the predictive signals that underlie decisions.

Method: RACE-Reasoning Alignment for Completeness of Explanations, a systematic framework to evaluate the alignment between LLM-generated explanations and interpretable feature importance scores derived from a logistic regression baseline. It implements token-aware, exact string, and edit-distance matching techniques.

Result: Empirical results reveal a consistent asymmetry: correct predictions exhibit higher coverage of supporting features, while incorrect predictions are associated with elevated coverage of contradicting features. Edit-distance matching further uncovers paraphrastic overlaps, boosting coverage while preserving this asymmetry.

Conclusion: RACE provides new insights into the faithfulness of LLM explanations and establishes a quantitative basis for evaluating reasoning completeness in neural language models.

Abstract: The growing adoption of machine learning (ML) in sensitive domains has
heightened the demand for transparent and interpretable artificial
intelligence. Large Language Models (LLMs) are increasingly capable of
producing natural language explanations, yet it remains unclear whether these
rationales faithfully capture the predictive signals that underlie decisions.
This paper introduces RACE-Reasoning Alignment for Completeness of
Explanations, a systematic framework to evaluate the alignment between
LLM-generated explanations and interpretable feature importance scores derived
from a logistic regression baseline. We analyze four widely used text
classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and
compare LLM rationales against top-ranked supporting and contradicting lexical
features. To capture alignment at multiple levels of granularity, RACE
implements token-aware, exact string, and edit-distance matching techniques.
Empirical results reveal a consistent asymmetry: correct predictions exhibit
higher coverage of supporting features, while incorrect predictions are
associated with elevated coverage of contradicting features. Edit-distance
matching further uncovers paraphrastic overlaps, boosting coverage while
preserving this asymmetry. These findings demonstrate that LLM rationales
combine both surface-level and flexible evidence reuse, yet can also amplify
misleading cues in error cases. RACE provides new insights into the
faithfulness of LLM explanations and establishes a quantitative basis for
evaluating reasoning completeness in neural language models.

</details>


### [5] [Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning](https://arxiv.org/abs/2510.21885)
*Anh Pham,Mihir Thalanki,Michael Sun,Aditya Chaloo,Ankita Gupta,Tian Xia,Aditya Mate,Ehimwenma Nosakhare,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 本文提出了一种基于指令-响应行为和语义多样性的安全示例采样框架，以减少大型语言模型在微调过程中的有害输出。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对良性数据进行微调时，往往会失去之前对齐的安全行为，这种现象称为灾难性遗忘。先前的工作表明，添加随机安全示例可以缓解这种影响，但仍然不清楚哪些示例最有效。

Method: 我们提出了一种行为感知的采样框架，该框架基于两个互补因素选择安全示例：指令-响应行为（例如拒绝与合规）和跨危害类别的语义多样性。

Result: 这种方法显著减少了有害输出，同时保持了帮助性，仅使用0.5%的额外训练数据就实现了有害性高达41%的减少。

Conclusion: 这些结果表明，有针对性的数据选择可以提高大规模微调的安全性和效率。

Abstract: Large language models often lose previously aligned safety behaviors when
fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior
work shows that adding random safety examples can mitigate this effect, but it
remains unclear which examples are most effective. We propose a behavior-aware
sampling framework that selects safety examples based on two complementary
factors: instruction-response behavior (e.g., refusal versus compliance) and
semantic diversity across harm categories. Systematic evaluation shows that
this approach substantially reduces harmful outputs while maintaining
helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5%
additional training data. These results highlight how targeted data selection
can improve the safety and efficiency of fine-tuning at scale.

</details>


### [6] [Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation](https://arxiv.org/abs/2510.21891)
*Dhrupad Bhardwaj,Julia Kempe,Tim G. J. Rudner*

Main category: cs.CL

TL;DR: 本文提出了一种基于语义各向同性的方法，用于评估长文本响应的可信度，无需标记数据或微调，且在多个领域中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于逐条声明的事实核查，这在处理长文本响应时计算成本高且脆弱。因此，需要一种可靠且计算成本低的方法来评估长文本响应的可信度。

Method: 本文引入了语义各向同性（semantic isotropy）的概念，通过计算文本嵌入在单位球面上的角分散度来评估长文本响应的可信度。

Result: 实验结果表明，更高的语义各向同性（即更大的嵌入分散度）可以可靠地表示跨样本的事实一致性较低。该方法在多个领域中表现优于现有方法，仅需少量样本即可预测非事实性。

Conclusion: 本文提出了一种新的评估长文本响应可信度的方法，该方法在多个领域中表现出优于现有方法的性能，并且不需要标记数据、微调或超参数选择，为实际应用提供了低成本的解决方案。

Abstract: To deploy large language models (LLMs) in high-stakes application domains
that require substantively accurate responses to open-ended prompts, we need
reliable, computationally inexpensive methods that assess the trustworthiness
of long-form responses generated by LLMs. However, existing approaches often
rely on claim-by-claim fact-checking, which is computationally expensive and
brittle in long-form responses to open-ended prompts. In this work, we
introduce semantic isotropy -- the degree of uniformity across normalized text
embeddings on the unit sphere -- and use it to assess the trustworthiness of
long-form responses generated by LLMs. To do so, we generate several long-form
responses, embed them, and estimate the level of semantic isotropy of these
responses as the angular dispersion of the embeddings on the unit sphere. We
find that higher semantic isotropy -- that is, greater embedding dispersion --
reliably signals lower factual consistency across samples. Our approach
requires no labeled data, no fine-tuning, and no hyperparameter selection, and
can be used with open- or closed-weight embedding models. Across multiple
domains, our method consistently outperforms existing approaches in predicting
nonfactuality in long-form responses using only a handful of samples --
offering a practical, low-cost approach for integrating trust assessment into
real-world LLM workflows.

</details>


### [7] [Understanding Network Behaviors through Natural Language Question-Answering](https://arxiv.org/abs/2510.21894)
*Mingzhe Xing,Chang Tian,Jianan Zhang,Lichen Pan,Peipei Liu,Zhaoteng Yan,Yinliang Yue*

Main category: cs.CL

TL;DR: 本文提出了NetMind框架，通过自然语言查询网络，解决了长上下文理解、设备和协议异构性以及复杂网络拓扑和协议的问题。


<details>
  <summary>Details</summary>
Motivation: 现代大规模网络引入了显著的复杂性，增加了错误配置的风险。传统的基于领域特定语言的方法存在学习曲线陡峭和灵活性有限的问题。自然语言提供了更易访问和可解释的接口，而大型语言模型的进展进一步增强了这一方向。然而，仍然存在三个关键挑战。

Method: 提出了一种基于树的配置分块策略，构建了一个统一的事实图作为中间表示，并设计了一种混合命令式-声明式语言以减少LLMs的推理负担。

Result: NetMind在准确性和可扩展性方面表现出色，优于现有基线。

Conclusion: NetMind能够准确且可扩展地理解网络行为，并优于现有基线。

Abstract: Modern large-scale networks introduce significant complexity in understanding
network behaviors, increasing the risk of misconfiguration. Prior work proposed
to understand network behaviors by mining network configurations, typically
relying on domain-specific languages interfaced with formal models. While
effective, they suffer from a steep learning curve and limited flexibility. In
contrast, natural language (NL) offers a more accessible and interpretable
interface, motivating recent research on NL-guided network behavior
understanding. Recent advances in large language models (LLMs) further enhance
this direction, leveraging their extensive prior knowledge of network concepts
and strong reasoning capabilities. However, three key challenges remain: 1)
numerous router devices with lengthy configuration files challenge LLM's
long-context understanding ability; 2) heterogeneity across devices and
protocols impedes scalability; and 3) complex network topologies and protocols
demand advanced reasoning abilities beyond the current capabilities of LLMs. To
tackle the above challenges, we propose NetMind, a novel framework for querying
networks using NL. Our approach introduces a tree-based configuration chunking
strategy to preserve semantic coherence while enabling efficient partitioning.
We then construct a unified fact graph as an intermediate representation to
normalize vendor-specific configurations. Finally, we design a hybrid
imperative-declarative language to reduce the reasoning burden on LLMs and
enhance precision. We contribute a benchmark consisting of NL question-answer
pairs paired with network configurations. Experiments demonstrate that NetMind
achieves accurate and scalable network behavior understanding, outperforming
existing baselines.

</details>


### [8] [Deep Literature Survey Automation with an Iterative Workflow](https://arxiv.org/abs/2510.21900)
*Hongbo Zhang,Han Cui,Yidong Wang,Yijian Tian,Qi Guo,Cunxiang Wang,Jian Wu,Chiyu Song,Yue Zhang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automatic literature survey generation has attracted increasing attention,
yet most existing systems follow a one-shot paradigm, where a large set of
papers is retrieved at once and a static outline is generated before drafting.
This design often leads to noisy retrieval, fragmented structures, and context
overload, ultimately limiting survey quality. Inspired by the iterative reading
process of human researchers, we propose \ours, a framework based on recurrent
outline generation, in which a planning agent incrementally retrieves, reads,
and updates the outline to ensure both exploration and coherence. To provide
faithful paper-level grounding, we design paper cards that distill each paper
into its contributions, methods, and findings, and introduce a
review-and-refine loop with visualization enhancement to improve textual flow
and integrate multimodal elements such as figures and tables. Experiments on
both established and emerging topics show that \ours\ substantially outperforms
state-of-the-art baselines in content coverage, structural coherence, and
citation quality, while producing more accessible and better-organized surveys.
To provide a more reliable assessment of such improvements, we further
introduce Survey-Arena, a pairwise benchmark that complements absolute scoring
and more clearly positions machine-generated surveys relative to human-written
ones. The code is available at
https://github.com/HancCui/IterSurvey\_Autosurveyv2.

</details>


### [9] [Explaining and Mitigating Crosslingual Tokenizer Inequities](https://arxiv.org/abs/2510.21909)
*Catherine Arnett,Tyler A. Chang,Stella Biderman,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 本文研究了跨语言的token premiums现象，并通过调整词汇表大小和预分词器来减少这种现象的影响。


<details>
  <summary>Details</summary>
Motivation: 了解导致token premiums的跨语言差异，以减少训练时的吞吐量损失和推理时的成本。

Method: 我们为97种语言训练了大约7000个可比较的单语分词器，操纵分词算法、词汇表大小和数据集大小，并测量token premiums并测试数据相似性、词汇表大小和预分词之间的关系。

Result: 发现词汇表大小和预分词对token premiums有显著影响，而训练和测试数据之间的相似性没有影响。此外，我们发现超词分词器可以减少token premiums并提高压缩效果。

Conclusion: 干预词汇表大小或预分词器可以显著减少跨语言的token premiums效果。

Abstract: The number of tokens it takes to encode parallel text in different languages
is known to vary. These disparities are called token premiums. Having high
token premiums leads to less throughput during training and increases costs at
inference. In this paper, we show that even after controlling for dataset size,
vocabulary size, and data content, monolingual tokenizers exhibit a wide range
of token premiums across languages. To understand the cross-linguistic
differences that cause these token premiums, we train a suite of approximately
7,000 comparable monolingual tokenizers for 97 languages, manipulating
tokenization algorithm, vocabulary size, and dataset size. We measure token
premiums and test for a relationship between factors such as data similarity
(between tokenizer training and evaluation), vocabulary size, and
pre-tokenization. We also investigate the role of language-specific features
such as writing system and word length. We find that similarity between
training and test data does not impact token premiums, but vocabulary size and
pre-tokenization do. While simply increasing vocabulary size does not lead to
reduced token premium effects, we can determine an ``optimal'' vocabulary size
for each language to achieve significantly reduced token premium effects. We
also train superword tokenizers which allow merges over whitespaces, and we
find that they both reduce token premium effects and improve compression
overall. Thus, intervening on the vocabulary size or the pre-tokenizer
significantly reduces crosslingual token premium effects.

</details>


### [10] [Model-Aware Tokenizer Transfer](https://arxiv.org/abs/2510.21954)
*Mykola Haltiuk,Aleksander Smywiński-Pohl*

Main category: cs.CL

TL;DR: 本文提出了一种名为MATT的方法，通过结合模型内部信息来改进分词器的迁移过程，从而在多语言大型语言模型中实现更有效的分词器迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的分词器迁移方法通常依赖于语义启发式方法来初始化新的嵌入，忽略了高层模型动态，限制了迁移质量。

Method: 我们提出了Model-Aware Tokenizer Transfer (MATT)，一种将模型内部结构纳入分词器转移过程的方法。MATT引入了一个Attention Influence Modeling (AIM)目标，该目标从源模型中提炼出跨标记的通信模式，并将其注入到具有新分词器的目标模型中，从而在标准语言建模之前提供高效的预热。

Result: 在多种语言环境下进行的实验表明，MATT在几个GPU小时内恢复了原始模型的大部分性能，并优于启发式基线。

Conclusion: 这些结果表明，结合模型级信号为多语言LLM的鲁棒分词器转移提供了一条实用且有效的方法。

Abstract: Large Language Models (LLMs) are trained to support an increasing number of
languages, yet their predefined tokenizers remain a bottleneck for adapting
models to lower-resource or distinct-script languages. Existing tokenizer
transfer methods typically rely on semantic heuristics to initialize new
embeddings, ignoring higher-layer model dynamics and limiting transfer quality.
We propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates
model internals into the tokenizer transfer process. MATT introduces an
Attention Influence Modeling (AIM) objective that distills inter-token
communication patterns from a source model into a target model with a new
tokenizer, providing an efficient warm-up before standard language modeling.
Unlike approaches that focus solely on embedding similarity, MATT leverages
attention behavior to guide embedding initialization and adaptation.
Experiments across diverse linguistic settings show that MATT recovers a large
fraction of the original model's performance within a few GPU hours,
outperforming heuristic baselines. These results demonstrate that incorporating
model-level signals offers a practical and effective path toward robust
tokenizer transfer in multilingual LLMs.

</details>


### [11] [A Stylometric Application of Large Language Models](https://arxiv.org/abs/2510.21958)
*Harrison F. Stropkay,Jiayi Chen,Mohammad J. Latifi,Daniel N. Rockmore,Jeremy R. Manning*

Main category: cs.CL

TL;DR: 研究显示，大型语言模型可以区分不同作者的写作，并成功验证了R.P. Thompson是《奥兹系列》第15本书的作者。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能够捕捉到不同作者的独特写作风格。

Method: 训练一个单独的GPT-2模型，从头开始训练一位作者的作品，然后预测该作者的文本比其他作者的文本更准确。

Result: 模型在训练后能够更准确地预测其训练作者的文本，从而证明了模型能够体现作者的独特写作风格。

Conclusion: 大型语言模型可以用于区分不同作者的写作。

Abstract: We show that large language models (LLMs) can be used to distinguish the
writings of different authors. Specifically, an individual GPT-2 model, trained
from scratch on the works of one author, will predict held-out text from that
author more accurately than held-out text from other authors. We suggest that,
in this way, a model trained on one author's works embodies the unique writing
style of that author. We first demonstrate our approach on books written by
eight different (known) authors. We also use this approach to confirm R. P.
Thompson's authorship of the well-studied 15th book of the Oz series,
originally attributed to F. L. Baum.

</details>


### [12] [Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks](https://arxiv.org/abs/2510.21983)
*Havva Alizadeh Noughabi,Julien Serbanescu,Fattane Zarrinkalam,Ali Dehghantanha*

Main category: cs.CL

TL;DR: 本文研究了如何利用社会科学研究中的说服理论来设计对抗性提示，以绕过大型语言模型的安全机制，并发现这些提示能够有效诱导越狱行为。


<details>
  <summary>Details</summary>
Motivation: 尽管最近取得了进展，大型语言模型仍然容易受到越狱攻击，而现有研究较少关注可能影响模型对这些攻击敏感性的语言和心理机制。

Method: 本文借鉴了社会科学中的说服理论，提出了利用说服策略来设计对抗性提示的方法，并研究了LLM在越狱响应中是否表现出不同的说服特征。

Result: 实证评估显示，具有说服力的提示能够显著绕过安全措施，显示出其诱导越狱行为的潜力。

Conclusion: 本文强调了跨学科见解在应对LLM安全不断演变的挑战中的重要性。

Abstract: Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model's
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.

</details>


### [13] [Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models](https://arxiv.org/abs/2510.22014)
*Sarah Ball,Niki Hasrati,Alexander Robey,Avi Schwarzschild,Frauke Kreuter,Zico Kolter,Andrej Risteski*

Main category: cs.CL

TL;DR: 本文分析了离散优化基础的越狱攻击，并发现攻击转移性的关键因素是模型内部拒绝方向的激活程度、后缀对这一方向的推动强度以及这些变化在拒绝方向正交方向上的大小。


<details>
  <summary>Details</summary>
Motivation: 尽管攻击的转移性已被实证证明，但缺乏对其发生条件和原因的严谨分析。本文旨在填补这一空白，以更好地理解攻击的转移性并提高攻击的成功率。

Method: 本文通过实验分析了离散优化基础的越狱攻击，并识别出三个与攻击转移性高度相关的统计特性。此外，还进行了干预实验以验证这些统计分析的实际应用价值。

Result: 研究发现，模型内部拒绝方向的激活程度、后缀对这一方向的推动强度以及这些变化在拒绝方向正交方向上的大小与攻击转移性高度相关。而提示语义相似性与攻击转移性的相关性较弱。

Conclusion: 本文通过分析发现，模型内部拒绝方向的激活程度、后缀对这一方向的推动强度以及这些变化在拒绝方向正交方向上的大小是影响攻击转移性的关键因素。这些发现有助于更细致地理解转移性，并在干预实验中展示了如何将统计分析转化为实际的攻击成功率提升。

Abstract: Discrete optimization-based jailbreaking attacks on large language models aim
to generate short, nonsensical suffixes that, when appended onto input prompts,
elicit disallowed content. Notably, these suffixes are often transferable --
succeeding on prompts and models for which they were never optimized. And yet,
despite the fact that transferability is surprising and empirically
well-established, the field lacks a rigorous analysis of when and why transfer
occurs. To fill this gap, we identify three statistical properties that
strongly correlate with transfer success across numerous experimental settings:
(1) how much a prompt without a suffix activates a model's internal refusal
direction, (2) how strongly a suffix induces a push away from this direction,
and (3) how large these shifts are in directions orthogonal to refusal. On the
other hand, we find that prompt semantic similarity only weakly correlates with
transfer success. These findings lead to a more fine-grained understanding of
transferability, which we use in interventional experiments to showcase how our
statistical analysis can translate into practical improvements in attack
success.

</details>


### [14] [Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics](https://arxiv.org/abs/2510.22028)
*Yilin Zhang,Wenda Xu,Zhongtao Liu,Tetsuji Nakagawa,Markus Freitag*

Main category: cs.CL

TL;DR: 本文研究了QE度量中的长度偏差问题，并提出了两种减轻该偏差的方法。


<details>
  <summary>Details</summary>
Motivation: 质量估计（QE）度量在机器翻译中对于无参考评估和作为强化学习等任务的奖励信号至关重要。然而，长度偏差的普遍性和影响尚未得到充分研究。

Method: 本文通过系统研究在10种不同的语言对上表现最佳的基于回归的QE度量和LLM-as-a-Judge QE度量，揭示了两个关键的长度偏差。

Result: QE度量在翻译长度增加时会持续高估错误，即使对于高质量、无错误的文本也是如此。此外，当有多个候选翻译时，它们倾向于偏好较短的翻译。这些内在的长度偏差可能会不公平地惩罚较长的正确翻译，并可能导致QE重新排序和QE引导的强化学习等应用中的次优决策。

Conclusion: 本文提出了两种策略来减轻QE中的长度偏差：(a) 在模型训练期间应用长度归一化，以及(b) 在评估期间结合参考文本。这两种方法都被发现能有效减少识别出的长度偏差。

Abstract: Quality Estimation (QE) metrics are vital in machine translation for
reference-free evaluation and as a reward signal in tasks like reinforcement
learning. However, the prevalence and impact of length bias in QE have been
underexplored. Through a systematic study of top-performing regression-based
and LLM-as-a-Judge QE metrics across 10 diverse language pairs, we reveal two
critical length biases: First, QE metrics consistently over-predict errors with
increasing translation length, even for high-quality, error-free texts. Second,
they exhibit a preference for shorter translations when multiple candidates are
available for the same source text. These inherent length biases risk unfairly
penalizing longer, correct translations and can lead to sub-optimal
decision-making in applications such as QE reranking and QE guided
reinforcement learning. To mitigate this, we propose two strategies: (a)
applying length normalization during model training, and (b) incorporating
reference texts during evaluation. Both approaches were found to effectively
reduce the identified length bias.

</details>


### [15] [ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality](https://arxiv.org/abs/2510.22037)
*Shayne Longpre,Sneha Kudugunta,Niklas Muennighoff,I-Hung Hsu,Isaac Caswell,Alex Pentland,Sercan Arik,Chen-Yu Lee,Sayna Ebrahimi*

Main category: cs.CL

TL;DR: 本文进行了最大规模的多语扩展定律研究，提出了ATLAS模型，提高了多语预训练的效果，并分析了多语学习的动态和迁移特性。


<details>
  <summary>Details</summary>
Motivation: 目前的扩展定律研究主要集中在英语上，但最著名的AI模型服务于数十亿国际用户。因此，本文旨在研究多语扩展定律，以支持更广泛的语言应用。

Method: 本文介绍了自适应迁移扩展定律（ATLAS），用于单语和多语预训练，并进行了大规模的多语扩展定律研究，包括774次多语训练实验，覆盖10M-8B模型参数，400多种训练语言和48种评估语言。

Result: 本文提出了ATLAS，其在样本外泛化方面通常比现有扩展定律高出0.3 R^2以上。此外，本文分析了多语学习动态、语言间的迁移特性以及多语的困境。

Conclusion: 本文希望这些发现能为跨语言的扩展定律提供科学基础，并使从业者能够高效地扩展模型——超越以英语为中心的人工智能。

Abstract: Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2. Our analyses of the experiments shed
light on multilingual learning dynamics, transfer properties between languages,
and the curse of multilinguality. First, we derive a cross-lingual transfer
matrix, empirically measuring mutual benefit scores between 38 x 38=1444
language pairs. Second, we derive a language-agnostic scaling law that reveals
how to optimally scale model size and data when adding languages without
sacrificing performance. Third, we identify the computational crossover points
for when to pretrain from scratch versus finetune from multilingual
checkpoints. We hope these findings provide the scientific foundation for
democratizing scaling laws across languages, and enable practitioners to
efficiently scale models -- beyond English-first AI.

</details>


### [16] [Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models](https://arxiv.org/abs/2510.22042)
*Benjamin Reichman,Adar Avsian,Larry Heck*

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型（LLMs）内部如何表示情感，发现情感表示具有低维流形结构，且在不同语言和数据集中表现出稳定性与通用性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs内部如何表示情感，以理解它们如何内部化和处理情感。

Method: 通过分析隐藏状态空间的几何结构，研究了大型语言模型（LLMs）内部如何表示情感。

Result: 识别出一个低维的情感流形，情感表示是方向编码的、跨层分布的，并与可解释的维度对齐。这些结构在深度上是稳定的，并且可以推广到八个跨五种语言的真实世界情感数据集。跨领域对齐产生了低误差和强线性探测性能，表明存在一个通用的情感子空间。

Conclusion: 这些发现揭示了LLMs中一致且可操作的情感几何结构，并提供了关于它们内部化和处理情感的见解。

Abstract: This work investigates how large language models (LLMs) internally represent
emotion by analyzing the geometry of their hidden-state space. The paper
identifies a low-dimensional emotional manifold and shows that emotional
representations are directionally encoded, distributed across layers, and
aligned with interpretable dimensions. These structures are stable across depth
and generalize to eight real-world emotion datasets spanning five languages.
Cross-domain alignment yields low error and strong linear probe performance,
indicating a universal emotional subspace. Within this space, internal emotion
perception can be steered while preserving semantics using a learned
intervention module, with especially strong control for basic emotions across
languages. These findings reveal a consistent and manipulable affective
geometry in LLMs and offer insight into how they internalize and process
emotion.

</details>


### [17] [Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds](https://arxiv.org/abs/2510.22084)
*Atij Mahesh*

Main category: cs.CL

TL;DR: 研究比较了六种偏见缓解技术，发现显式正向监督是解决组合性偏见的关键，而基于偏好学习的方法无法有效推广逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究提出了提示、受限解码、后处理和微调等方法来缓解大语言模型中的性别刻板印象，但这些方法的相对效果和学习动态仍不明确。

Method: 该研究对六种偏见缓解技术进行了比较分析：仅提示、生成和过滤、基于DFA的Ctrl-G解码、监督微调（SFT）、直接偏好优化（DPO）和迭代零空间投影（INLP）。

Result: SFT实现了99.87%的合规性并保持了高词汇多样性，而DPO则表现不佳。Ctrl-G保证了完美的合规性，但牺牲了流利度和多样性。基于偏好的学习方法无法满足组合性约束。

Conclusion: 研究发现，只有显式正向监督才能缓解组合性偏见；基于偏好学习的对齐方法无法推广逻辑结构，这突显了偏好学习的局限性，并强调了显式监督在公平和流畅控制生成中的必要性。

Abstract: Large Language Models (LLMs) still produce gender-stereotyped language even
in occupation-neutral contexts that reflect deep societal biases (Rudinger et
al., 2018). To address this, prior work has proposed prompting, constrained
decoding (Dathathri et al., 2020; Zhou et al., 2024), post-processing, and
fine-tuning-based alignment (Rafailov et al., 2023; Ravfogel et al., 2022).
However, the comparative efficacy and learning dynamics remain little
understood. We report a comparative analysis of six control techniques for bias
mitigation: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and
Iterative Nullspace Projection (INLP). We evaluate each method on a
compositional constraint task. This task requires generating sentences that
contain at least one agentic and one communal descriptor for each of the twenty
Winogender-derived occupations. We quantify trade-offs between control strength
and naturalness with evaluations of constraint compliance, lexical diversity,
and fluency. Our results reveal key contrasts among the methods: SFT achieves
99.87 +- 0.15% compliance and high lexical diversity, while DPO, despite
similar training stability, fails at 4.53 +- 0.82%. Ctrl-G guarantees perfect
compliance, but at the cost of severely reduced fluency and diversity.
Preference-based learning fundamentally differs: it cannot satisfy
compositional constraints, as binary preference signals encode ranking, not
logical conjunctions. Only explicit positive supervision enables mitigation of
compositional biases; preference-based alignment fails to generalize logical
structures, underscoring the limitations of preference learning and the
necessity of explicit supervision for fair and fluent controlled generation.

</details>


### [18] [Generalization or Memorization: Dynamic Decoding for Mode Steering](https://arxiv.org/abs/2510.22099)
*Xuanming Zhang*

Main category: cs.CL

TL;DR: This paper introduces Dynamic Mode Steering (DMS), an inference-time algorithm that enhances the reliability of Large Language Models by identifying and controlling their reliance on memorization versus generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unpredictability of LLMs, which can lead to brittle, verbatim memorization of training data, undermining their reliability in high-stakes applications.

Method: The work proposes a unified framework called Dynamic Mode Steering (DMS), which includes a lightweight, causally-grounded linear probe and a dynamic activation steering mechanism to identify and control the model's reliance on memorization versus generalization.

Result: Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy.

Conclusion: DMS provides a principled approach to enhancing the reliability of Large Language Models (LLMs) by improving logical consistency and factual accuracy.

Abstract: Large Language Models (LLMs) exhibit a troubling duality, capable of both
remarkable generalization and brittle, verbatim memorization of their training
data. This unpredictability undermines their reliability in high-stakes
applications. In this work, we propose a unified framework to understand,
identify, and control these distinct reasoning modes. First, we introduce a
theoretical model based on the Information Bottleneck (IB) principle,
formalizing generalization as the learning of a compressed, task-relevant
representation and memorization as a failure to compress. Building on this
theory, we develop Dynamic Mode Steering (DMS), a novel inference-time
algorithm which comprises two components: (1) a lightweight, causally-grounded
linear probe that identifies the model's instantaneous reliance on
memorization, and (2) a dynamic activation steering mechanism that nudges the
model's computation towards pre-identified generalization circuits. We frame
DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning
and faithfulness tasks demonstrate that DMS significantly improves logical
consistency and factual accuracy, thereby offering a principled approach to
enhancing LLM reliability.

</details>


### [19] [Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows](https://arxiv.org/abs/2510.22109)
*Billy Dickson,Zoran Tiganj*

Main category: cs.CL

TL;DR: 本文提出了一种修改输入表示的方法，而不是改变变压器架构，通过尺度不变的对数压缩来扩展变压器的长程记忆。


<details>
  <summary>Details</summary>
Motivation: 大多数处理长上下文的方法通过集成递归或辅助内存模块来增加变压器内部架构的复杂性。

Method: 受人类记忆认知模型的启发，我们的方法对输入标记应用了尺度不变的对数压缩。

Result: 在WikiText-103和PG-19语言建模基准测试中，与未压缩基线相比，困惑度有所降低。

Conclusion: 输入级对数压缩是一种简单而有效的方法，可以扩展变压器的长程记忆。

Abstract: Most approaches to long-context processing increase the complexity of the
transformer's internal architecture by integrating mechanisms such as
recurrence or auxiliary memory modules. In this work, we introduce an
alternative approach that modifies the input representation itself, rather than
the transformer architecture. Inspired by cognitive models of human memory, our
method applies a scale-invariant logarithmic compression to the input tokens.
The resulting compressed representation is processed by a standard, unmodified
transformer, preserving architectural simplicity. We evaluate this approach on
the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in
perplexity compared to uncompressed baselines. Moreover, performance improves
consistently with longer compressed temporal contexts, showing that input-level
logarithmic compression is a simple and effective way to extend a transformer's
long-range memory.

</details>


### [20] [Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation](https://arxiv.org/abs/2510.22115)
*Ling-Team,Ang Li,Ben Liu,Binbin Hu,Bing Li,Bingwei Zeng,Borui Ye,Caizhi Tang,Changxin Tian,Chao Huang,Chao Zhang,Chen Qian,Chenchen Ju,Chenchen Li,Chengfu Tang,Chili Fu,Chunshao Ren,Chunwei Wu,Cong Zhang,Cunyin Peng,Dafeng Xu,Daixin Wang,Dalong Zhang,Dingnan Jin,Dingyuan Zhu,Dongke Hu,Fangzheng Zhao,Feifan Wu,Feng Zhu,Gangshan Wang,Haitao Zhang,Hailin Zhao,Hanxiao Zhang,Hanzi Wang,Hao Qian,Haoyi Yu,Heng Zhang,Hongliang Zhang,Hongzhi Luan,Huirong Dong,Huizhong Li,Jia Li,Jia Liu,Jialong Zhu,Jian Sha,Jianping Wei,Jiaolong Yang,Jieyue Ma,Jiewei Wu,Jinjing Huang,Jingyun Tian,Jingyuan Zhang,Jinquan Sun,Juanhui Tu,Jun Liu,Jun Xu,Jun Zhou,Junjie Ou,Junpeng Fang,Kaihong Zhang,Kaiqin Hu,Ke Shi,Kun Tang,Kunlong Chen,Lanyin Mei,Lei Liang,Lei Xu,Libo Zhang,Lin Ju,Lin Yuan,Ling Zhong,Lintao Ma,Lu Liu,Lu Yu,Lun Cai,Meiqi Zhu,Mengying Li,Min Chen,Minghao Xue,Minghong Cai,Mingming Yin,Peijie Jiang,Peilong Zhao,Pingping Liu,Qian Zhao,Qing Cui,Qingxiang Huang,Qingyuan Yang,Quankun Yu,Shaowei Wei,Shijie Lian,Shoujian Zheng,Shun Song,Shungen Zhang,Shuo Zhang,Siyuan Li,Song Liu,Ting Guo,Tong Zhao,Wanli Gu,Weichang Wu,Weiguang Han,Wenjing Fang,Wubin Wang,Xiang Shu,Xiao Shi,Xiaoshun Lan,Xiaolu Zhang,Xiaqing Sun,Xin Zhao,Xingyu Lu,Xiong Xu,Xudong Wang,Xudong Wang,Xuemin Yang,Yajie Yang,Yang Xiang,Yanzhe Li,Yi Zhang,Yilong Wang,Yingxue Li,Yongzhen Guo,Yuzhuo Fu,Yuanyuan Wang,Yue Yang,Yue Yu,Yufeng Deng,Yun Zhang,Yunfei Xu,Yuqi Zhang,Yuxiao He,Zengke Gui,Zhaoxin Huan,Zhaoyang Wang,Zhibo Zhu,Zhihao Wang,Zhiqiang Zhang,Zhoufei Wang,Zihang Zeng,Ziqi Liu,Zitao Xuan,Zuoli Tang*

Main category: cs.CL

TL;DR: Ling 2.0 is a series of reasoning-oriented language foundation models designed to scale from tens of billions to one trillion parameters under a unified MoE paradigm, emphasizing high sparsity, cross-scale consistency, and efficiency. It achieves up to 7-fold active-compute efficiency and establishes a new Pareto frontier for reasoning accuracy versus computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a series of reasoning-oriented language foundation models that can scale from tens of billions to one trillion parameters while maintaining high efficiency and reasoning accuracy.

Method: Ling 2.0 is built upon the principle that every activation boosts reasoning capability, using a unified Mixture-of-Experts (MoE) paradigm with high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. It integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure.

Result: Ling 2.0 includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency.

Conclusion: Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models.

Abstract: We introduce Ling 2.0, a series reasoning-oriented language foundation built
upon the principle that every activation boosts reasoning capability. Designed
to scale from tens of billions to one trillion parameters under a unified
Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,
cross-scale consistency, and efficiency guided by empirical scaling laws. The
series includes three non-thinking (instruct) models - Ling-mini-2.0,
Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and
achieving up to 7-fold active-compute efficiency compared with dense
counterparts. Ling 2.0 integrates coordinated innovations across model
architecture, pre-training, post-training, and infrastructure: a high-sparsity
MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training
CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale
FP8 training with fine-grained heterogeneous pipelines. At the trillion scale,
Ling-1T establishes a new Pareto frontier of reasoning accuracy versus
computational efficiency, demonstrating that sparse activation, when properly
aligned with reasoning objectives, enables scalable and efficient intelligence.
Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for
advancing future reasoning and thinking models, including the Ring series built
upon the same base.

</details>


### [21] [OlaMind: Towards Human-Like and Hallucination-Safe Customer Service for Retrieval-Augmented Dialogue](https://arxiv.org/abs/2510.22143)
*Tianhong Gao,Jundong Shen,Bei Shi,Jiapeng Wang,Ying Ju,Junfeng Yao,Jiao Ran,Yong Zhang,Lin Dong,Huiyu Yu,Tingting Ye*

Main category: cs.CL

TL;DR: OlaMind is a human-like and hallucination-safe customer service framework for retrieval-augmented dialogue, which improves the performance of ICS systems by learning from human experts and using SFT with RL for self-refinement.


<details>
  <summary>Details</summary>
Motivation: Intelligent customer service (ICS) systems via retrieval-augmented generation (RAG) have been widely adopted in Web-based domains such as social platforms and e-commerce, achieving remarkable improvements in automation and efficiency. However, notable limitations still remain: these systems are prone to hallucinations and often generate rigid, mechanical responses, which can introduce business risks and undermine user experience, especially in Web-based customer service interactions under the RAG scenarios.

Method: OlaMind leverages a Learn-to-Think stage to learn the reasoning processes and response strategies from human experts, and then employs a Learn-to-Respond stage to perform cold-start supervised fine-tuning (SFT) combined with reinforcement learning (RL) for basic-to-hard self-refinement.

Result: OlaMind significantly enhances human-likeness and naturalness while effectively mitigating hallucinations and critical business risks. Large-scale online A/B experiments in an industry-level social customer service setting show that OlaMind achieves significant improvements in intelligent resolution rates and reduces human takeover rates.

Conclusion: OlaMind achieves significant cumulative relative improvements with intelligent resolution rates +28.92%/+18.42% and human takeover rate -6.08%/-7.12% in community-support/livestream-interaction scenarios, respectively, which highlights its consistent effectiveness across diverse real-world applications.

Abstract: Intelligent customer service (ICS) systems via retrieval-augmented generation
(RAG) have been widely adopted in Web-based domains such as social platforms
and e-commerce, achieving remarkable improvements in automation and efficiency.
However, notable limitations still remain: these systems are prone to
hallucinations and often generate rigid, mechanical responses, which can
introduce business risks and undermine user experience, especially in Web-based
customer service interactions under the RAG scenarios. In this paper, we
introduce OlaMind, a human-like and hallucination-safe customer service
framework for retrieval-augmented dialogue. Specifically, it first leverages a
Learn-to-Think stage to learn the reasoning processes and response strategies
from human experts, and then employs a Learn-to-Respond stage to perform
cold-start supervised fine-tuning (SFT) combined with reinforcement learning
(RL) for basic-to-hard self-refinement. Our method significantly enhances
human-likeness and naturalness while effectively mitigating hallucinations and
critical business risks. We have conducted large-scale online A/B experiments
in an industry-level social customer service setting, and extensive
experimental results show that OlaMind achieves significant cumulative relative
improvements with intelligent resolution rates +28.92%/+18.42% and human
takeover rate -6.08%/-7.12% in community-support/livestream-interaction
scenarios, respectively, which highlights its consistent effectiveness across
diverse real-world applications. The code and data will be publicly available.

</details>


### [22] [SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language](https://arxiv.org/abs/2510.22160)
*Rahul Ranjan,Mahendra Kumar Gurve,Anuj,Nitin,Yamuna Prasad*

Main category: cs.CL

TL;DR: 本文介绍了首个用于可解释情感计算的马蒂利语基准数据集，包含3,221个带有情感极性和自然语言解释的句子，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏本地语言专家和注释的高昂成本，低资源语言的基准数据集开发面临重大挑战。马蒂利语在自然语言处理研究中仍然代表性不足，尽管它具有丰富的语言结构和文化意义。虽然情感分析在高资源语言中取得了显著进展，但低资源语言（如马蒂利语）的资源仍然稀缺，通常仅限于粗粒度注释，缺乏可解释机制。

Method: 本文引入了一个新的数据集，包含3,221个马蒂利语句子，这些句子被标注了情感极性，并附有自然语言的解释。此外，该数据集经过语言学专家的仔细整理和验证，以确保标签的可靠性和上下文的真实性。

Result: 通过使用传统机器学习和最先进的Transformer架构进行广泛的实验，证明了该数据集在可解释情感分析中的有效性。

Conclusion: 本文建立了第一个可解释的情感计算基准，为多语言自然语言处理和可解释人工智能做出了重要贡献。

Abstract: Developing benchmark datasets for low-resource languages poses significant
challenges, primarily due to the limited availability of native linguistic
experts and the substantial time and cost involved in annotation. Given these
challenges, Maithili is still underrepresented in natural language processing
research. It is an Indo-Aryan language spoken by more than 13 million people in
the Purvanchal region of India, valued for its rich linguistic structure and
cultural significance. While sentiment analysis has achieved remarkable
progress in high-resource languages, resources for low-resource languages, such
as Maithili, remain scarce, often restricted to coarse-grained annotations and
lacking interpretability mechanisms. To address this limitation, we introduce a
novel dataset comprising 3,221 Maithili sentences annotated for sentiment
polarity and accompanied by natural language justifications. Moreover, the
dataset is carefully curated and validated by linguistic experts to ensure both
label reliability and contextual fidelity. Notably, the justifications are
written in Maithili, thereby promoting culturally grounded interpretation and
enhancing the explainability of sentiment models. Furthermore, extensive
experiments using both classical machine learning and state-of-the-art
transformer architectures demonstrate the dataset's effectiveness for
interpretable sentiment analysis. Ultimately, this work establishes the first
benchmark for explainable affective computing in Maithili, thus contributing a
valuable resource to the broader advancement of multilingual NLP and
explainable AI.

</details>


### [23] [DETECT: Determining Ease and Textual Clarity of German Text Simplifications](https://arxiv.org/abs/2510.22212)
*Maria Korobeynikova,Alessia Battisti,Lukas Fischer,Yingqiang Gao*

Main category: cs.CL

TL;DR: 我们引入了DETECT，这是第一个针对德语的度量标准，全面评估ATS质量的三个维度，并且完全基于合成大语言模型（LLM）响应进行训练。我们的方法适应了LENS框架，并扩展了生成合成质量分数的管道和基于LLM的精炼步骤。实验结果表明，DETECT与人类判断的相关性显著高于广泛使用的ATS指标，特别是在意义保留和流畅性方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前对德语自动文本简化（ATS）的评估依赖于通用指标，如SARI、BLEU和BERTScore，这些指标在简单性、意义保留和流畅性方面未能充分捕捉简化质量。虽然为英语开发了专门的指标，但由于缺乏人工标注语料库，德语方面的努力落后了。

Method: 我们引入了DETECT，这是第一个针对德语的度量标准，全面评估ATS质量的三个维度，并且完全基于合成大语言模型（LLM）响应进行训练。我们的方法适应了LENS框架，并扩展了(i)一个通过LLM生成合成质量分数的管道，以创建数据集而无需人工注释，以及(ii)一个基于LLM的精炼步骤，以使评分标准与简化要求对齐。

Result: 实验结果表明，DETECT与人类判断的相关性显著高于广泛使用的ATS指标，特别是在意义保留和流畅性方面有显著提升。

Conclusion: 我们的研究展示了LLM在自动评估中的潜力和局限性，并为一般的语言可访问性任务提供了可转移的指导方针。

Abstract: Current evaluation of German automatic text simplification (ATS) relies on
general-purpose metrics such as SARI, BLEU, and BERTScore, which insufficiently
capture simplification quality in terms of simplicity, meaning preservation,
and fluency. While specialized metrics like LENS have been developed for
English, corresponding efforts for German have lagged behind due to the absence
of human-annotated corpora. To close this gap, we introduce DETECT, the first
German-specific metric that holistically evaluates ATS quality across all three
dimensions of simplicity, meaning preservation, and fluency, and is trained
entirely on synthetic large language model (LLM) responses. Our approach adapts
the LENS framework to German and extends it with (i) a pipeline for generating
synthetic quality scores via LLMs, enabling dataset creation without human
annotation, and (ii) an LLM-based refinement step for aligning grading criteria
with simplification requirements. To the best of our knowledge, we also
construct the largest German human evaluation dataset for text simplification
to validate our metric directly. Experimental results show that DETECT achieves
substantially higher correlations with human judgments than widely used ATS
metrics, with particularly strong gains in meaning preservation and fluency.
Beyond ATS, our findings highlight both the potential and the limitations of
LLMs for automatic evaluation and provide transferable guidelines for general
language accessibility tasks.

</details>


### [24] [Estimating the Error of Large Language Models at Pairwise Text Comparison](https://arxiv.org/abs/2510.22219)
*Tianyi Li*

Main category: cs.CL

TL;DR: 我们测量了LLMs在成对文本比较中的输出错误，并提出了一个不需要真实情况的方法。我们发现Claude在这项实验中表现最好。


<details>
  <summary>Details</summary>
Motivation: 我们想要测量LLMs在成对文本比较中的输出错误，并了解它们的偏好中的错误概率。

Method: 我们测量了LLMs在成对文本比较中的输出错误，注意了它们偏好中的错误概率。我们的方法不依赖于真实情况，并支持两种场景：(i) 无论比较顺序如何，错误率相同，通过每对文本进行两次比较来估计，其中每个文本都可能首先出现；(ii) 二元位置偏差假设两种比较顺序有不同的错误率，通过重复比较文本对来估计。Copeland计数从成对偏好中构建文本排名；排名揭示了基于LLM的成对比较的可扩展性差，并有助于估计LLM的错误率。

Result: 我们应用该方法到六个LLMs（ChatGPT、Claude、DeepSeek、Gemini、Grok、Qwen）和五种类型的文本输入，并获得了LLMs错误的一致估计。通常，测量的两个位置偏差项相似，接近于均匀错误。考虑到错误率和对提示变化的鲁棒性，Claude在这个实验中表现最理想。

Conclusion: 我们的模型在指示LLMs的错误方面优于有偏的Bradley-Terry模型和共轭分数。

Abstract: We measure LLMs' output error at pairwise text comparison, noting the
probability of error in their preferences. Our method does not rely on the
ground truth and supports two scenarios: (i) uniform error rate regardless of
the order of comparison, estimated with two comparisons for each text pair with
either text placed first; (ii) binary positional bias assuming distinct error
rates for the two orders of comparison, estimated with repeated comparisons
between the texts. The Copeland counting constructs a ranking over the compared
texts from pairwise preferences; the ranking reveals the poor scalability of
LLM-based pairwise comparison and helps yield the estimates for LLMs' error
rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini,
Grok, Qwen) with five types of text input and obtain consistent estimates of
LLMs' error. In general, the measured two positional bias terms are similar,
close to the uniform error. Considering both the error rates and the robustness
to the variation of prompts, Claude obtained the most desirable performance in
this experiment. Our model outperforms the biased Bradley-Terry model and the
commutativity score in indicating LLMs' error at this task.

</details>


### [25] [Evolution of the lexicon: a probabilistic point of view](https://arxiv.org/abs/2510.22220)
*Maurizio Serva*

Main category: cs.CL

TL;DR: 本文探讨了斯瓦迪士方法在估计语言时序分离时的局限性，并提出通过考虑词汇的渐进修改过程来提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 斯瓦迪士方法在实际应用中存在诸多不现实的假设，如水平传播、替换率的变化、同源关系的错误评估等，这些因素导致结果不准确。此外，即使假设成立，概率限制也会影响估计的准确性。

Method: 本文分析了斯瓦迪士方法中的概率限制，并引入了词汇渐进修改的随机过程来提高时序分离估计的准确性。

Result: 研究发现，词汇的渐进修改过程对语言词汇演变有重要影响，并且考虑这一过程可以显著提高时序分离估计的精度。

Conclusion: 本文指出，即使在理想情况下，基于斯瓦迪士方法的时序分离估计也受到纯概率限制，并且通过考虑词汇的渐进修改过程，可以显著提高估计的精度。

Abstract: The Swadesh approach for determining the temporal separation between two
languages relies on the stochastic process of words replacement (when a
complete new word emerges to represent a given concept). It is well known that
the basic assumptions of the Swadesh approach are often unrealistic due to
various contamination phenomena and misjudgments (horizontal transfers,
variations over time and space of the replacement rate, incorrect assessments
of cognacy relationships, presence of synonyms, and so on). All of this means
that the results cannot be completely correct.
  More importantly, even in the unrealistic case that all basic assumptions are
satisfied, simple mathematics places limits on the accuracy of estimating the
temporal separation between two languages. These limits, which are purely
probabilistic in nature and which are often neglected in lexicostatistical
studies, are analyzed in detail in this article.
  Furthermore, in this work we highlight that the evolution of a language's
lexicon is also driven by another stochastic process: gradual lexical
modification of words. We show that this process equally also represents a
major contribution to the reshaping of the vocabulary of languages over the
centuries and we also show, from a purely probabilistic perspective, that
taking into account this second random process significantly increases the
precision in determining the temporal separation between two languages.

</details>


### [26] [You Don't Need Prompt Engineering Anymore: The Prompting Inversion](https://arxiv.org/abs/2510.22251)
*Imran Khan*

Main category: cs.CL

TL;DR: 本文介绍了'Sculpting'，一种改进标准CoT的提示方法，但发现它在某些模型上反而有害，这表明最优提示策略需随模型能力变化而调整。


<details>
  <summary>Details</summary>
Motivation: Prompt engineering，特别是Chain-of-Thought (CoT) prompting，显著增强了LLM的推理能力。然而，语义歧义和错误的常识可能导致错误。因此，我们需要一种改进的标准CoT方法。

Method: 我们引入了一种称为'Sculpting'的受约束、基于规则的提示方法，旨在改进标准的CoT，以减少语义歧义和错误的常识导致的错误。我们评估了三种提示策略（零样本、标准CoT和Sculpting）在三个OpenAI模型生成（gpt-4o-mini, gpt-4o, gpt-5）上的表现，并使用GSM8K数学推理基准（1,317个问题）进行测试。

Result: 我们的研究发现了一个“Prompting Inversion”：Sculpting在gpt-4o上提供了优势（97% vs. 93% for standard CoT），但在gpt-5上却变得有害（94.00% vs. 96.36% for CoT on full benchmark）。我们将其归因于“Guardrail-to-Handcuff”转变，即防止中等模型常见错误的约束在高级模型中导致过度字面化。

Conclusion: 我们的研究表明，最优的提示策略必须随着模型能力的发展而演变，这意味着对于更强大的模型，可能需要更简单的提示。

Abstract: Prompt engineering, particularly Chain-of-Thought (CoT) prompting,
significantly enhances LLM reasoning capabilities. We introduce "Sculpting," a
constrained, rule-based prompting method designed to improve upon standard CoT
by reducing errors from semantic ambiguity and flawed common sense.
  We evaluate three prompting strategies (Zero Shot, standard CoT, and
Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)
using the GSM8K mathematical reasoning benchmark (1,317 problems).
  Our findings reveal a "Prompting Inversion": Sculpting provides advantages on
gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%
vs. 96.36% for CoT on full benchmark). We trace this to a
"Guardrail-to-Handcuff" transition where constraints preventing common-sense
errors in mid-tier models induce hyper-literalism in advanced models. Our
detailed error analysis demonstrates that optimal prompting strategies must
co-evolve with model capabilities, suggesting simpler prompts for more capable
models.

</details>


### [27] [SteerX: Disentangled Steering for LLM Personalization](https://arxiv.org/abs/2510.22256)
*Xiaoyan Zhao,Ming Yan,Yilun Qiu,Haoting Ni,Yang Zhang,Fuli Feng,Hong Cheng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: SteerX is a new method for activating steering in large language models (LLMs) that improves personalization by focusing on preference-driven information.


<details>
  <summary>Details</summary>
Motivation: Existing methods for activation steering rely on all historical data to compute the steering vector, which may include content that does not reflect true user preferences. This undermines the personalization signal, and thus there is a need for a more accurate approach to activation steering.

Method: SteerX is a disentangled steering method that isolates preference-driven components from preference-agnostic components. It estimates token-level causal effects to identify preference-driven tokens, transforms these discrete signals into a coherent description, and then leverages them to steer personalized LLM generation.

Result: Experiments on two representative steering backbone methods across real-world datasets demonstrate that SteerX consistently enhances steering vector quality, offering a practical solution for more effective LLM personalization.

Conclusion: SteerX consistently enhances steering vector quality, offering a practical solution for more effective LLM personalization.

Abstract: Large language models (LLMs) have shown remarkable success in recent years,
enabling a wide range of applications, including intelligent assistants that
support users' daily life and work. A critical factor in building such
assistants is personalizing LLMs, as user preferences and needs vary widely.
Activation steering, which directly leverages directions representing user
preference in the LLM activation space to adjust its behavior, offers a
cost-effective way to align the model's outputs with individual users. However,
existing methods rely on all historical data to compute the steering vector,
ignoring that not all content reflects true user preferences, which undermines
the personalization signal. To address this, we propose SteerX, a disentangled
steering method that isolates preference-driven components from
preference-agnostic components. Grounded in causal inference theory, SteerX
estimates token-level causal effects to identify preference-driven tokens,
transforms these discrete signals into a coherent description, and then
leverages them to steer personalized LLM generation. By focusing on the truly
preference-driven information, SteerX produces more accurate activation
steering vectors and enhances personalization. Experiments on two
representative steering backbone methods across real-world datasets demonstrate
that SteerX consistently enhances steering vector quality, offering a practical
solution for more effective LLM personalization.

</details>


### [28] [PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding](https://arxiv.org/abs/2510.22264)
*Iliass Ayaou,Denis Cavallucci*

Main category: cs.CL

TL;DR: PatenTEB是一个针对专利文本嵌入的全面基准，包含15个任务，通过多任务训练开发了高效的patembed模型家族，并展示了出色的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能充分捕捉专利特有的挑战，因此需要一个更全面的基准来支持专利文本嵌入的研究。

Method: 引入了PatenTEB基准，包含15个任务，使用领域分层分割、领域特定的困难负样本挖掘和系统覆盖不对称片段到文档匹配场景。通过多任务训练开发了patembed模型家族，参数范围从67M到344M，上下文长度最多4096个标记。

Result: patembed-base在MTEB BigPatentClustering.v2上达到了最先进的0.494 V-measure，而patembed-large在DAPFAM上达到了0.377 NDCG@100。多任务训练提高了外部泛化能力，领域预训练初始化在任务族中提供了持续的优势。

Conclusion: PatenTEB是一个全面的基准，通过多任务训练开发了patembed模型家族，展示了强大的泛化能力，并且所有资源都将公开。

Abstract: Patent text embeddings enable prior art search, technology landscaping, and
patent analysis, yet existing benchmarks inadequately capture patent-specific
challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15
tasks across retrieval, classification, paraphrase, and clustering, with 2.06
million examples. PatenTEB employs domain-stratified splits, domain specific
hard negative mining, and systematic coverage of asymmetric
fragment-to-document matching scenarios absent from general embedding
benchmarks. We develop the patembed model family through multi-task training,
spanning 67M to 344M parameters with context lengths up to 4096 tokens.
External validation shows strong generalization: patembed-base achieves
state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445
previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.
Systematic ablations reveal that multi-task training improves external
generalization despite minor benchmark costs, and that domain-pretrained
initialization provides consistent advantages across task families. All
resources will be made available at https://github.com/iliass-y/patenteb.
Keywords: patent retrieval, sentence embeddings, multi-task learning,
asymmetric retrieval, benchmark evaluation, contrastive learning.

</details>


### [29] [From Slides to Chatbots: Enhancing Large Language Models with University Course Materials](https://arxiv.org/abs/2510.22272)
*Tu Anh Dinh,Philipp Nicolas Schumacher,Jan Niehues*

Main category: cs.CL

TL;DR: 本研究探讨了如何通过结合大学课程材料来提高大型语言模型在计算机科学课程中的表现，发现检索增强生成方法比持续预训练更有效，并且在多模态设置中使用图像形式的讲义能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何结合大学课程材料可以提高LLM在这一领域的表现。

Method: 比较了两种策略，即检索增强生成（RAG）和持续预训练（CPT），以将课程特定知识扩展到LLM中。对于讲义，还探索了一种多模态RAG方法，其中以图像形式向生成器展示检索内容。

Result: 给定大学课程材料相对较小的规模，RAG比CPT更有效和高效。此外，在多模态设置中将讲义作为图像包含进来，相比仅使用文本检索显著提高了性能。

Conclusion: 这些发现突出了开发更好支持学习和教学的AI助手的实际策略，并希望它们能激发其他教育环境中的类似努力。

Abstract: Large Language Models (LLMs) have advanced rapidly in recent years. One
application of LLMs is to support student learning in educational settings.
However, prior work has shown that LLMs still struggle to answer questions
accurately within university-level computer science courses. In this work, we
investigate how incorporating university course materials can enhance LLM
performance in this setting. A key challenge lies in leveraging diverse course
materials such as lecture slides and transcripts, which differ substantially
from typical textual corpora: slides also contain visual elements like images
and formulas, while transcripts contain spoken, less structured language. We
compare two strategies, Retrieval-Augmented Generation (RAG) and Continual
Pre-Training (CPT), to extend LLMs with course-specific knowledge. For lecture
slides, we further explore a multi-modal RAG approach, where we present the
retrieved content to the generator in image form. Our experiments reveal that,
given the relatively small size of university course materials, RAG is more
effective and efficient than CPT. Moreover, incorporating slides as images in
the multi-modal setting significantly improves performance over text-only
retrieval. These findings highlight practical strategies for developing AI
assistants that better support learning and teaching, and we hope they inspire
similar efforts in other educational contexts.

</details>


### [30] [Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER](https://arxiv.org/abs/2510.22285)
*Andrei Baroian*

Main category: cs.CL

TL;DR: 研究比较了BERT风格模型、GPT-4o的ICL和SFT在临床NER上的效果，发现SFT表现最佳但成本较高。


<details>
  <summary>Details</summary>
Motivation: 研究临床命名实体识别（NER）在CADEC语料库上的表现，并比较不同方法的效果。

Method: 比较了三种方法：(i) BERT风格的编码器，(ii) GPT-4o使用少量示例的上下文学习（ICL），以及(iii) GPT-4o的监督微调（SFT）。

Result: RoBERTa-large和BioClinicalBERT相对于BERT Base仅表现出有限的改进，而GPT-4o的SFT取得了最强的整体性能（F1≈87.1%）。

Conclusion: LLM在简化任务中表现出更高的准确性，但需要权衡成本。

Abstract: We study clinical Named Entity Recognition (NER) on the CADEC corpus and
compare three families of approaches: (i) BERT-style encoders (BERT Base,
BioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context
learning (ICL) under simple vs.\ complex prompts, and (iii) GPT-4o with
supervised fine-tuning (SFT). All models are evaluated on standard NER metrics
over CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding).
RoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base,
showing the limit of these family of models. Among LLM settings, simple ICL
outperforms a longer, instruction-heavy prompt, and SFT achieves the strongest
overall performance (F1 $\approx$ 87.1%), albeit with higher cost. We find that
the LLM achieve higher accuracy on simplified tasks, restricting classification
to two labels.

</details>


### [31] [Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling](https://arxiv.org/abs/2510.22317)
*Antal van den Bosch,Ainhoa Risco Patón,Teun Buijse,Peter Berck,Maarten van Gompel*

Main category: cs.CL

TL;DR: 本文介绍了一种基于记忆的语言建模方法，作为深度神经网络语言建模的环保替代方案，具有良好的性能和低能耗。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供一种更环保、高效的语言建模方法，以减少深度神经网络模型的生态足迹。

Method: 本文采用基于记忆的语言建模方法，通过快速近似k最近邻分类实现高效的下一个标记预测性能。

Result: 本文提出的基于记忆的语言建模方法在下一个标记预测准确性、估计排放和速度方面与GPT-2和GPT-Neo进行了比较，并提供了更深入的分析。

Conclusion: 本文提出了一种基于记忆的语言建模方法，作为一种高效、环保的深度神经网络语言建模的替代方案。

Abstract: We present memory-based language modeling as an efficient, eco-friendly
alternative to deep neural network-based language modeling. It offers
log-linearly scalable next-token prediction performance and strong memorization
capabilities. Implementing fast approximations of k-nearest neighbor
classification, memory-based language modeling leaves a relatively small
ecological footprint both in training and in inference mode, as it relies fully
on CPUs and attains low token latencies. Its internal workings are simple and
fully transparent. We compare our implementation of memory-based language
modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy,
estimated emissions and speeds, and offer some deeper analyses of the model.

</details>


### [32] [Multilingual Target-Stance Extraction](https://arxiv.org/abs/2510.22334)
*Ethan Mines,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本文介绍了第一个多语言TSE基准，扩展了TSE流程到多语言环境，并展示了多语言TSE任务的挑战性和目标预测的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有TSE工作仅限于英语，而多语言TSE任务更具挑战性，需要新的研究和基准。

Method: 本文扩展了原始的TSE流程到多语言环境，而无需为每种语言单独建模。

Result: 模型流程在多语言任务中取得了12.78的F1分数，表明多语言任务比英语设置更具难度，并突出了目标预测作为主要瓶颈。

Conclusion: 本文提出了第一个多语言TSE基准，为多语言TSE提供了必要的资源、算法和评估标准的基线。

Abstract: Social media enables data-driven analysis of public opinion on contested
issues. Target-Stance Extraction (TSE) is the task of identifying the target
discussed in a document and the document's stance towards that target. Many
works classify stance towards a given target in a multilingual setting, but all
prior work in TSE is English-only. This work introduces the first multilingual
TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and
Spanish corpora. It manages to extend the original TSE pipeline to a
multilingual setting without requiring separate models for each language. Our
model pipeline achieves a modest F1 score of 12.78, underscoring the increased
difficulty of the multilingual task relative to English-only setups and
highlighting target prediction as the primary bottleneck. We are also the first
to demonstrate the sensitivity of TSE's F1 score to different target
verbalizations. Together these serve as a much-needed baseline for resources,
algorithms, and evaluation criteria in multilingual TSE.

</details>


### [33] [FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22344)
*Mohammad Aghajani Asl,Majid Asgari-Bidhendi,Behrooz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 本文介绍了FAIR-RAG，一种新颖的代理框架，用于改进RAG系统在处理复杂、多跳查询时的表现。通过结构化证据评估和迭代精炼循环，FAIR-RAG能够有效识别并填补信息缺口，从而提高生成结果的准确性和可靠性。实验结果显示，FAIR-RAG在多个基准测试中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG框架在处理需要从不同来源综合信息的复杂、多跳查询时常常失败。当前先进的RAG方法采用迭代或自适应策略，缺乏系统地识别和填补证据缺口的稳健机制，常常传播噪声或未能收集全面的上下文。

Method: 我们引入了FAIR-RAG，这是一种新颖的代理框架，将标准的RAG流程转化为动态的、以证据为中心的推理过程。核心是一个由我们称为结构化证据评估（SEA）的模块控制的迭代精炼循环。SEA作为分析性门控机制：它将初始查询分解为所需的发现清单，并审计聚合的证据以识别确认的事实和关键的信息缺口。这些缺口为自适应查询精炼代理提供了精确的信号，该代理生成新的、有针对性的子查询以检索缺失的信息。这个循环重复直到证据被验证为足够，确保最终严格忠实的生成。

Result: 我们在具有挑战性的多跳QA基准测试上进行了实验，包括HotpotQA、2WikiMultiHopQA和MusiQue。在统一的实验设置中，FAIR-RAG显著优于强大的基线。在HotpotQA上，它实现了0.453的F1分数——比最强的迭代基线高出8.3分——在这些基准测试上建立了新的最先进水平。

Conclusion: 我们的工作表明，具有显式缺口分析的结构化、以证据为中心的精炼过程对于解锁先进RAG系统在复杂、知识密集型任务中的可靠和准确推理至关重要。

Abstract: While Retrieval-Augmented Generation (RAG) mitigates hallucination and
knowledge staleness in Large Language Models (LLMs), existing frameworks often
falter on complex, multi-hop queries that require synthesizing information from
disparate sources. Current advanced RAG methods, employing iterative or
adaptive strategies, lack a robust mechanism to systematically identify and
fill evidence gaps, often propagating noise or failing to gather a
comprehensive context. We introduce FAIR-RAG, a novel agentic framework that
transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning
process. At its core is an Iterative Refinement Cycle governed by a module we
term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating
mechanism: it deconstructs the initial query into a checklist of required
findings and audits the aggregated evidence to identify confirmed facts and,
critically, explicit informational gaps. These gaps provide a precise signal to
an Adaptive Query Refinement agent, which generates new, targeted sub-queries
to retrieve missing information. This cycle repeats until the evidence is
verified as sufficient, ensuring a comprehensive context for a final, strictly
faithful generation. We conducted experiments on challenging multi-hop QA
benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified
experimental setup, FAIR-RAG significantly outperforms strong baselines. On
HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3
points over the strongest iterative baseline -- establishing a new
state-of-the-art for this class of methods on these benchmarks. Our work
demonstrates that a structured, evidence-driven refinement process with
explicit gap analysis is crucial for unlocking reliable and accurate reasoning
in advanced RAG systems for complex, knowledge-intensive tasks.

</details>


### [34] [Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models](https://arxiv.org/abs/2510.22356)
*Fiaz Ahmad,Nisar Hussain,Amna Qasim,Momina Hafeez,Muhammad Usman Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 本文研究了如何通过翻译英语讽刺语料库到乌尔都语，并利用现代NLP模型进行讽刺检测。结果显示，结合转写技术和现代NLP模型可以有效提高乌尔都语的讽刺检测性能。


<details>
  <summary>Details</summary>
Motivation: 讽刺识别是自然语言处理中的一个挑战性任务，特别是在处理语法和文化背景不同的语言时。本文旨在通过翻译英语讽刺语料库到乌尔都语来检测乌尔都语中的讽刺。

Method: 我们通过将英语讽刺语料库翻译成乌尔都语来检测乌尔都语中的讽刺。我们评估了十种最先进的机器学习算法，并使用GloVe和Word2Vec嵌入进行比较，同时比较了经典方法的性能。此外，我们微调了先进的基于Transformer的模型，包括BERT、RoBERTa、LLaMA 2（7B）、LLaMA 3（8B）和Mistral，以评估大规模模型在讽刺检测中的有效性。

Result: 在机器学习模型中，梯度提升取得了最佳性能，F1得分为89.18%。在基于Transformer的模型中，LLaMA 3（8B）取得了最高性能，F1得分为94.61%。

Conclusion: 这些结果表明，结合转写技术与现代NLP模型可以实现对乌尔都语的稳健讽刺检测，这是一种历史资源匮乏的语言。

Abstract: Ironic identification is a challenging task in Natural Language Processing,
particularly when dealing with languages that differ in syntax and cultural
context. In this work, we aim to detect irony in Urdu by translating an English
Ironic Corpus into the Urdu language. We evaluate ten state-of-the-art machine
learning algorithms using GloVe and Word2Vec embeddings, and compare their
performance with classical methods. Additionally, we fine-tune advanced
transformer-based models, including BERT, RoBERTa, LLaMA 2 (7B), LLaMA 3 (8B),
and Mistral, to assess the effectiveness of large-scale models in irony
detection. Among machine learning models, Gradient Boosting achieved the best
performance with an F1-score of 89.18%. Among transformer-based models, LLaMA 3
(8B) achieved the highest performance with an F1-score of 94.61%. These results
demonstrate that combining transliteration techniques with modern NLP models
enables robust irony detection in Urdu, a historically low-resource language.

</details>


### [35] [GigaEmbeddings: Efficient Russian Language Embedding Model](https://arxiv.org/abs/2510.22369)
*Egor Kolodin,Daria Khomich,Nikita Savushkin,Anastasia Ianina,Fyodor Minkin*

Main category: cs.CL

TL;DR: GigaEmbeddings是一个用于训练高性能俄语文本嵌入的框架，通过三阶段流程和架构创新，在基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法的关键限制，通过统一各种目标并利用合成数据生成。

Method: 通过分层指令调优解码器仅LLM（GigaChat-3B）来训练高性能俄语文本嵌入的框架。三阶段流程包括大规模对比预训练、使用硬负样本微调以及跨检索、分类和聚类任务的多任务泛化。

Result: 在ruMTEB基准测试中，GigaEmbeddings实现了69.1的平均得分。

Conclusion: GigaEmbeddings在ruMTEB基准测试中实现了最先进的结果，优于具有更多参数的强基线。

Abstract: We introduce GigaEmbeddings, a novel framework for training high-performance
Russian-focused text embeddings through hierarchical instruction tuning of the
decoder-only LLM designed specifically for Russian language (GigaChat-3B). Our
three-stage pipeline, comprising large-scale contrastive pre-training in
web-scale corpora, fine-tuning with hard negatives, and multitask
generalization across retrieval, classification, and clustering tasks,
addresses key limitations of existing methods by unifying diverse objectives
and leveraging synthetic data generation. Architectural innovations include
bidirectional attention for contextual modeling, latent attention pooling for
robust sequence aggregation, and strategic pruning of 25% of transformer layers
to enhance efficiency without compromising performance. Evaluated on the ruMTEB
benchmark spanning 23 multilingual tasks, GigaEmbeddings achieves
state-of-the-art results (69.1 avg. score), outperforming strong baselines with
a larger number of parameters.

</details>


### [36] [VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations](https://arxiv.org/abs/2510.22373)
*Yupeng Xie,Zhiyang Zhang,Yifan Wu,Sirong Lu,Jiayi Zhang,Zhaoyang Yu,Jinlin Wang,Sirui Hong,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.CL

TL;DR: 本文提出了VisJudge-Bench基准测试和VisJudge模型，以评估和提升多模态大语言模型在可视化美学和质量评估中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在自然图像的审美评估中表现良好，但缺乏针对可视化评估的系统性基准测试。

Method: 本文提出了VisJudge-Bench基准测试，用于评估多模态大语言模型在评估可视化美学和质量方面的能力，并设计了VisJudge模型来解决这一问题。

Result: VisJudge模型在基准测试中显著提高了与人类专家的一致性，减少了均方误差。

Conclusion: 本文提出了VisJudge模型，该模型专门用于评估可视化美学和质量，并在基准测试中表现出色，显著缩小了与人类判断的差距。

Abstract: Visualization, a domain-specific yet widely used form of imagery, is an
effective way to turn complex datasets into intuitive insights, and its value
depends on whether data are faithfully represented, clearly communicated, and
aesthetically designed. However, evaluating visualization quality is
challenging: unlike natural images, it requires simultaneous judgment across
data encoding accuracy, information expressiveness, and visual aesthetics.
Although multimodal large language models (MLLMs) have shown promising
performance in aesthetic assessment of natural images, no systematic benchmark
exists for measuring their capabilities in evaluating visualizations. To
address this, we propose VisJudge-Bench, the first comprehensive benchmark for
evaluating MLLMs' performance in assessing visualization aesthetics and
quality. It contains 3,090 expert-annotated samples from real-world scenarios,
covering single visualizations, multiple visualizations, and dashboards across
32 chart types. Systematic testing on this benchmark reveals that even the most
advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human
experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a
correlation with human ratings of only 0.429. To address this issue, we propose
VisJudge, a model specifically designed for visualization aesthetics and
quality assessment. Experimental results demonstrate that VisJudge
significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a
19.8% reduction) and increasing the consistency with human experts to 0.681 (a
58.7% improvement) compared to GPT-5. The benchmark is available at
https://github.com/HKUSTDial/VisJudgeBench.

</details>


### [37] [Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection](https://arxiv.org/abs/2510.22395)
*Federica Gamba,Aman Sinha,Timothee Mickus,Raul Vazquez,Patanjali Bhamidipati,Claudio Savelli,Ahana Chattopadhyay,Laura A. Zanella,Yash Kankanampati,Binesh Arakkal Remesh,Aryan Ashok Chandramania,Rohit Agarwal,Chuyuan Li,Ioana Buhnila,Radhika Mamidi*

Main category: cs.CL

TL;DR: CAP数据集是一个多语言资源，用于研究大型语言模型在科学文本生成中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 在科学文本生成中，幻觉可能导致事实知识的扭曲，而专门术语、统计推理和上下文依赖性解释会加剧这一问题。

Method: CAP数据集通过收集科学问题和LLM生成的答案，并对其进行二进制标签标注，以识别科学幻觉和语言质量问题。

Result: CAP数据集涵盖了五种高资源语言和四种低资源语言，包含900个精心挑选的科学问题和7000多个LLM生成的答案。

Conclusion: CAP数据集的发布旨在促进对幻觉检测、多语言评估和更可靠的科学NLP系统的进一步研究。

Abstract: We introduce the CAP (Confabulations from ACL Publications) dataset, a
multilingual resource for studying hallucinations in large language models
(LLMs) within scientific text generation. CAP focuses on the scientific domain,
where hallucinations can distort factual knowledge, as they frequently do. In
this domain, however, the presence of specialized terminology, statistical
reasoning, and context-dependent interpretations further exacerbates these
distortions, particularly given LLMs' lack of true comprehension, limited
contextual understanding, and bias toward surface-level generalization. CAP
operates in a cross-lingual setting covering five high-resource languages
(English, French, Hindi, Italian, and Spanish) and four low-resource languages
(Bengali, Gujarati, Malayalam, and Telugu). The dataset comprises 900 curated
scientific questions and over 7000 LLM-generated answers from 16 publicly
available models, provided as question-answer pairs along with token sequences
and corresponding logits. Each instance is annotated with a binary label
indicating the presence of a scientific hallucination, denoted as a factuality
error, and a fluency label, capturing issues in the linguistic quality or
naturalness of the text. CAP is publicly released to facilitate advanced
research on hallucination detection, multilingual evaluation of LLMs, and the
development of more reliable scientific NLP systems.

</details>


### [38] [CHOIR: Collaborative Harmonization fOr Inference Robustness](https://arxiv.org/abs/2510.22475)
*Xiangjue Dong,Cong Wang,Maria Teleki,Millennium Bismay,James Caverlee*

Main category: cs.CL

TL;DR: CHOIR is a test-time framework that harmonizes multiple persona-conditioned reasoning signals into a unified prediction, enhancing performance across demographics, model architectures, scales, and tasks without additional training.


<details>
  <summary>Details</summary>
Motivation: Persona-assigned Large Language Models (LLMs) can adopt diverse roles, enabling personalized and context-aware reasoning. However, even minor demographic perturbations in personas can alter reasoning trajectories, leading to divergent sets of correct answers. Instead of treating these variations as biases to be mitigated, we explore their potential as a constructive resource to improve reasoning robustness.

Method: CHOIR (Collaborative Harmonization fOr Inference Robustness) is a test-time framework that harmonizes multiple persona-conditioned reasoning signals into a unified prediction, orchestrating a collaborative decoding process among counterfactual personas.

Result: Experiments on various reasoning benchmarks demonstrate that CHOIR consistently enhances performance across demographics, model architectures, scales, and tasks - without additional training. Improvements reach up to 26.4% for individual demographic groups and 19.2% on average across five demographics. It remains effective even when base personas are suboptimal.

Conclusion: CHOIR provides a scalable and generalizable approach to more reliable LLM reasoning by reframing persona variation as a constructive signal.

Abstract: Persona-assigned Large Language Models (LLMs) can adopt diverse roles,
enabling personalized and context-aware reasoning. However, even minor
demographic perturbations in personas, such as simple pronoun changes, can
alter reasoning trajectories, leading to divergent sets of correct answers.
Instead of treating these variations as biases to be mitigated, we explore
their potential as a constructive resource to improve reasoning robustness. We
propose CHOIR (Collaborative Harmonization fOr Inference Robustness), a
test-time framework that harmonizes multiple persona-conditioned reasoning
signals into a unified prediction. CHOIR orchestrates a collaborative decoding
process among counterfactual personas, dynamically balancing agreement and
divergence in their reasoning paths. Experiments on various reasoning
benchmarks demonstrate that CHOIR consistently enhances performance across
demographics, model architectures, scales, and tasks - without additional
training. Improvements reach up to 26.4% for individual demographic groups and
19.2% on average across five demographics. It remains effective even when base
personas are suboptimal. By reframing persona variation as a constructive
signal, CHOIR provides a scalable and generalizable approach to more reliable
LLM reasoning.

</details>


### [39] [The Tonogenesis Continuum in Tibetan: A Computational Investigation](https://arxiv.org/abs/2510.22485)
*Siyu Liang,Zhaxi Zerong*

Main category: cs.CL

TL;DR: 本文介绍了一种计算方法，用于研究音变过程，并发现ASR模型可以捕捉语音变化的细微阶段。


<details>
  <summary>Details</summary>
Motivation: 传统上，音变过程通过比较重建和声学语音学进行研究。然而，这种方法可能无法充分捕捉到语音变化的细微阶段。因此，我们希望通过计算方法来研究音变过程。

Method: 我们引入了一种计算方法，通过测量音高操纵如何影响自动语音识别（ASR）性能，来量化音高在这一语音变化不同阶段的功能作用。

Result: 通过对一组密切相关的藏语方言的音高扁平化敏感性分析，我们发现了音变连续体的证据：无音调的安多方言最能容忍音高移除，而完全音调的乌-tsang变体表现出严重的退化，中间的喀嘛方言则明显介于这两个极端之间。

Conclusion: 我们的研究结果表明，计算方法可以捕捉语音变化的细微阶段，并表明基于最小对的传统功能负载度量可能高估了过渡系统中音高的依赖性，因为在这些系统中，音段和超音段线索仍然在语音上交织在一起。

Abstract: Tonogenesis-the historical process by which segmental contrasts evolve into
lexical tone-has traditionally been studied through comparative reconstruction
and acoustic phonetics. We introduce a computational approach that quantifies
the functional role of pitch at different stages of this sound change by
measuring how pitch manipulation affects automatic speech recognition (ASR)
performance. Through analysis on the sensitivity to pitch-flattening from a set
of closely related Tibetan languages, we find evidence of a tonogenesis
continuum: atonal Amdo dialects tolerate pitch removal the most, while fully
tonal U-Tsang varieties show severe degradation, and intermediate Kham dialects
fall measurably between these extremes. These gradient effects demonstrate how
ASR models implicitly learn the shifting functional load of pitch as languages
transition from consonant-based to tone-based lexical contrasts. Our findings
show that computational methods can capture fine-grained stages of sound change
and suggest that traditional functional load metrics, based solely on minimal
pairs, may overestimate pitch dependence in transitional systems where
segmental and suprasegmental cues remain phonetically intertwined.

</details>


### [40] [Frustratingly Easy Task-aware Pruning for Large Language Models](https://arxiv.org/abs/2510.22489)
*Yuanhe Tian,Junjie Liu,Xican Yang,Haishan Ye,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了一种新的LLM剪枝方法，能够在保持模型压缩的同时保留其专业能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM剪枝方法主要关注保持生成流畅句子的能力，而忽视了特定领域和任务的表现。因此，我们需要一种能够保留任务特定能力的剪枝方法。

Method: 我们提出了一种简单的但有效的LLM剪枝方法，该方法通过结合通用和任务特定的校准数据来计算重要性分数，并根据激活范数差异将参数分为共享和独占组，然后融合它们的分数来指导剪枝过程。

Result: 我们的方法在多个基准测试中表现出色，能够有效保留LLM的专业能力，并且在相同剪枝比例和不同设置下均优于基线方法。

Conclusion: 实验结果表明，我们的方法在保持模型压缩的同时，能够有效保留LLM的专业能力，并且在多个基准测试中表现优于基线方法。

Abstract: Pruning provides a practical solution to reduce the resources required to run
large language models (LLMs) to benefit from their effective capabilities as
well as control their cost for training and inference. Research on LLM pruning
often ranks the importance of LLM parameters using their magnitudes and
calibration-data activations and removes (or masks) the less important ones,
accordingly reducing LLMs' size. However, these approaches primarily focus on
preserving the LLM's ability to generate fluent sentences, while neglecting
performance on specific domains and tasks. In this paper, we propose a simple
yet effective pruning approach for LLMs that preserves task-specific
capabilities while shrinking their parameter space. We first analyze how
conventional pruning minimizes loss perturbation under general-domain
calibration and extend this formulation by incorporating task-specific feature
distributions into the importance computation of existing pruning algorithms.
Thus, our framework computes separate importance scores using both general and
task-specific calibration data, partitions parameters into shared and exclusive
groups based on activation-norm differences, and then fuses their scores to
guide the pruning process. This design enables our method to integrate
seamlessly with various foundation pruning techniques and preserve the LLM's
specialized abilities under compression. Experiments on widely used benchmarks
demonstrate that our approach is effective and consistently outperforms the
baselines with identical pruning ratios and different settings.

</details>


### [41] [The Limits of Data Scaling: Sub-token Utilization and Acoustic Saturation in Multilingual ASR](https://arxiv.org/abs/2510.22492)
*Siyu Liang,Nicolas Ballier,Gina-Anne Levow,Richard Wright*

Main category: cs.CL

TL;DR: 本研究分析了Whisper在49种语言中的解码行为，发现子标记利用更多受语音结构而非训练数据规模的影响。


<details>
  <summary>Details</summary>
Motivation: 了解多少音频足以完全观察多语言ASR模型的已学习子标记库存，并探讨多语言预训练中的数据差异是否会影响推理过程中这些标记的使用。

Method: 通过记录解码候选子标记并跟踪它们随时间累积的发现情况，我们研究了模型子标记空间的利用模式。

Result: 结果表明，发现的标记总数在很大程度上独立于语言的预训练小时数，表明数据差异不会强烈影响模型假设空间中的词汇多样性。子标记发现率在各种语言中遵循一致的指数饱和模式，表明在一定时间窗口后，额外的音频会产生最小的新子标记激活。

Conclusion: 我们的研究表明，多语言ASR推理中的子标记利用更多地受到语音的统计、类型学和正字法结构的约束，而不是训练数据规模，这为更公平的语料库构建和跨语言评估提供了实证基础。

Abstract: How much audio is needed to fully observe a multilingual ASR model's learned
sub-token inventory across languages, and does data disparity in multilingual
pre-training affect how these tokens are utilized during inference? We address
this question by analyzing Whisper's decoding behavior during inference across
49 languages. By logging decoding candidate sub-tokens and tracking their
cumulative discovery over time, we study the utilization pattern of the model's
sub-token space. Results show that the total number of discovered tokens
remains largely independent of a language's pre-training hours, indicating that
data disparity does not strongly influence lexical diversity in the model's
hypothesis space. Sub-token discovery rates follow a consistent exponential
saturation pattern across languages, suggesting a stable time window after
which additional audio yields minimal new sub-token activation. We refer to
this convergence threshold as acoustic saturation time (AST). Further analyses
of rank-frequency distributions reveal Zipf-like patterns better modeled by a
Zipf-Mandelbrot law, and mean sub-token length shows a positive correlation
with resource level. Additionally, those metrics show more favorable patterns
for languages in the Latin script than those in scripts such as Cyrillic, CJK,
and Semitic. Together, our study suggests that sub-token utilization during
multilingual ASR inference is constrained more by the statistical, typological,
and orthographic structure of the speech than by training data scale, providing
an empirical basis for more equitable corpus construction and cross-lingual
evaluation.

</details>


### [42] [A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the Pacific Northwest English Corpus](https://arxiv.org/abs/2510.22495)
*Michael Scott,Siyu Liang,Alicia Wassink,Gina-Anne Levow*

Main category: cs.CL

TL;DR: 本研究评估了四种主要商业自动语音识别系统中的种族偏见，发现声学建模的方言语音变体是导致偏见的主要原因。研究确立了PNWE语料库作为评估语音技术偏见的资源，并提出了改进ASR性能的建议。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在评估商业自动语音识别系统中的种族偏见，并探索社会语音变异如何影响系统性能。

Method: 本研究使用太平洋西北部英语（PNWE）语料库对四种主要商业自动语音识别（ASR）系统进行了系统的种族偏见评估。分析了来自四个民族背景（非裔美国人、白人美国人、奇克克斯和亚卡马）说话者的转录准确性，并研究了社会语音变异如何影响系统性能。引入了一种启发式确定的语音错误率（PER）度量标准，将识别错误与从社会语音注释中得出的语言学驱动变量联系起来。

Result: 研究分析了十一个社会语音特征，发现元音质量变化，特别是对低后合并和鼻前合并模式的抵抗，与不同民族群体之间的差异性错误率系统相关，其中对非裔美国人的影响最为显著。

Conclusion: 研究结果表明，商业ASR系统中的偏见主要源于方言语音变体的声学建模，而不是词汇或语法因素。研究确立了PNWE语料库作为评估语音技术偏见的宝贵资源，并为通过针对性地表示社会语音多样性来提高ASR性能提供了可行的指导。

Abstract: This paper presents a systematic evaluation of racial bias in four major
commercial automatic speech recognition (ASR) systems using the Pacific
Northwest English (PNWE) corpus. We analyze transcription accuracy across
speakers from four ethnic backgrounds (African American, Caucasian American,
ChicanX, and Yakama) and examine how sociophonetic variation contributes to
differential system performance. We introduce a heuristically-determined
Phonetic Error Rate (PER) metric that links recognition errors to specific
linguistically motivated variables derived from sociophonetic annotation. Our
analysis of eleven sociophonetic features reveals that vowel quality variation,
particularly resistance to the low-back merger and pre-nasal merger patterns,
is systematically associated with differential error rates across ethnic
groups, with the most pronounced effects for African American speakers across
all evaluated systems. These findings demonstrate that acoustic modeling of
dialectal phonetic variation, rather than lexical or syntactic factors, remains
a primary source of bias in commercial ASR systems. The study establishes the
PNWE corpus as a valuable resource for bias evaluation in speech technologies
and provides actionable guidance for improving ASR performance through targeted
representation of sociophonetic diversity in training data.

</details>


### [43] [Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection](https://arxiv.org/abs/2510.22531)
*Noshitha Padma Pratyusha Juttu,Sahithi Singireddy,Sravani Gona,Sujal Timilsina*

Main category: cs.CL

TL;DR: 本研究评估了微调、LoRA和零样本提示策略在法律文本处理中的效果，发现全微调效果最好，而LoRA在内存成本较低的情况下表现也不错。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在文本理解方面已经改变了法律领域的适应性，但由于全微调的成本而受到限制。

Method: 本研究对微调、参数高效的适应（LoRA、QLoRA）和零样本提示策略进行了系统评估，以检测条款中的不公平条款。

Result: 全微调实现了最强的精确率和召回率平衡，而基于LoRA的模型在最多3倍的内存成本下提供了有竞争力的召回率。

Conclusion: 这些发现突显了在高效和领域适应的LLM中实际的设计权衡，并为法律文本处理的微调研究提供了开放的基础。

Abstract: Large Language Models (LLMs) have transformed text understanding, yet their
adaptation to specialized legal domains remains constrained by the cost of full
fine-tuning. This study provides a systematic evaluation of fine tuning,
parameter efficient adaptation (LoRA, QLoRA), and zero-shot prompting
strategies for unfair clause detection in Terms of Service (ToS) documents, a
key application in legal NLP. We finetune BERT and DistilBERT, apply 4-bit
Low-Rank Adaptation (LoRA) to models such as TinyLlama, LLaMA 3B/7B, and
SaulLM, and evaluate GPT-4o and O-versions in zero-shot settings. Experiments
on the CLAUDETTE-ToS benchmark and the Multilingual Scraper Corpus show that
full fine-tuning achieves the strongest precision recall balance, while
LoRA-based models provide competitive recall with up to 3x lower memory cost.
These findings highlight practical design trade-offs for efficient and
domain-adapted LLMs, contributing open baselines for fine-tuning research in
legal text processing.

</details>


### [44] [LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?](https://arxiv.org/abs/2510.22548)
*Ziyuan He,Yuxuan Wang,Jiaqi Li,Kexin Liang,Muhan Zhang*

Main category: cs.CL

TL;DR: 本文介绍了LooGLE v2基准测试，用于评估大型语言模型在现实世界长上下文任务中的能力。结果表明，即使最先进的模型在长上下文理解方面仍有显著不足。


<details>
  <summary>Details</summary>
Motivation: 目前，大型语言模型在长上下文理解方面的能力仍然有限，而许多现实世界的长上下文应用很少被基准测试。因此，需要一个专门的基准来评估这些模型的实际能力。

Method: 本文引入了LooGLE v2，这是一个新的基准测试，用于评估大型语言模型在现实应用和场景中的长上下文能力。我们设计了10种特定领域的长依赖任务，并生成了1934个具有多样性和复杂性的QA实例。

Result: 对6个本地部署和4个基于API的大型语言模型进行了全面评估，结果显示即使表现最好的模型在我们的基准测试中也只达到了59.2%的整体分数。

Conclusion: 尽管大型语言模型拥有广泛的上下文窗口，但它们在处理长依赖任务时的能力仍然有限，这表明在实际的长上下文理解方面还有很大的改进空间。

Abstract: Large language models (LLMs) are equipped with increasingly extended context
windows recently, yet their long context understanding capabilities over long
dependency tasks remain fundamentally limited and underexplored. This gap is
especially significant in many real-world long-context applications that were
rarely benchmarked. In this paper, we introduce LooGLE v2, a novel benchmark
designed to evaluate LLMs' long context ability in real-world applications and
scenarios. Our benchmark consists of automatically collected real-world long
texts, ranging from 16k to 2M tokens, encompassing domains in law, finance,
game and code. Accordingly, we delicately design 10 types of domain-specific
long-dependency tasks and generate 1,934 QA instances with various diversity
and complexity in a scalable data curation pipeline for further practical
needs. We conduct a comprehensive assessment of 6 locally deployed and 4
API-based LLMs. The evaluation results show that even the best-performing model
achieves only a 59.2% overall score on our benchmark. Despite the extensive
context windows, popular LLMs are only capable of understanding a much shorter
length of context than they claim to be, revealing significant limitations in
their ability to handle real-world tasks with long dependencies and
highlighting substantial room for model improvement in practical long-context
understanding.

</details>


### [45] [SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size](https://arxiv.org/abs/2510.22556)
*Jinhan Chen,Jianchun Liu,Hongli Xu,Xianjun Gao,Shilong Wang*

Main category: cs.CL

TL;DR: SABlock是一种语义感知的KV缓存驱逐框架，通过自适应块大小提高内存效率并保持语义完整性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: KV缓存的不断增长的内存占用对长上下文大型语言模型推理的可扩展性构成了严重瓶颈。现有的基于标记、块和句子级别的压缩方法难以平衡语义连贯性和内存效率。

Method: SABlock是一种语义感知的KV缓存驱逐框架，通过语义分割、段引导的标记评分和预算驱动的搜索策略来适应不同的块大小。

Result: SABlock在Needle-in-a-Haystack (NIAH) 上实现了99.9%的检索准确率，仅使用96个KV条目，几乎与保留最多8K条目的全缓存基线相当。在固定缓存预算为1,024的情况下，SABlock进一步减少了峰值内存使用量，并在128K上下文长度上实现了高达9.5倍的解码速度提升。

Conclusion: SABlock在长上下文基准测试中表现出色，能够在相同的内存预算下超越最先进的基线方法。

Abstract: The growing memory footprint of the Key-Value (KV) cache poses a severe
scalability bottleneck for long-context Large Language Model (LLM) inference.
While KV cache eviction has emerged as an effective solution by discarding less
critical tokens, existing token-, block-, and sentence-level compression
methods struggle to balance semantic coherence and memory efficiency. To this
end, we introduce SABlock, a \underline{s}emantic-aware KV cache eviction
framework with \underline{a}daptive \underline{block} sizes. Specifically,
SABlock first performs semantic segmentation to align compression boundaries
with linguistic structures, then applies segment-guided token scoring to refine
token importance estimation. Finally, for each segment, a budget-driven search
strategy adaptively determines the optimal block size that preserves semantic
integrity while improving compression efficiency under a given cache budget.
Extensive experiments on long-context benchmarks demonstrate that SABlock
consistently outperforms state-of-the-art baselines under the same memory
budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%
retrieval accuracy with only 96 KV entries, nearly matching the performance of
the full-cache baseline that retains up to 8K entries. Under a fixed cache
budget of 1,024, SABlock further reduces peak memory usage by 46.28% and
achieves up to 9.5x faster decoding on a 128K context length.

</details>


### [46] [A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback](https://arxiv.org/abs/2510.22559)
*Zhifeng Wang,Xinyue Zheng,Chunyan Zeng*

Main category: cs.CL

TL;DR: 本文提出了一种端到端的个性化学习代理EduLoop-Agent，结合了神经认知诊断模型、有界能力估计计算机自适应测试策略和大型语言模型，形成一个闭环框架“诊断--推荐--反馈”。实验结果表明，该设计有效且可实际部署，为生成个性化的学习轨迹提供了可行路径。


<details>
  <summary>Details</summary>
Motivation: 随着信息技术的进步，教育正在从一刀切的教学转向个性化学习。然而，大多数方法将建模、题目选择和反馈孤立处理，而不是作为闭环系统。这导致了粗略或不透明的学生模型、假设束缚的适应性以及通用的不可操作的反馈。为了克服这些限制，本文提出了一个端到端的个性化学习代理。

Method: 本文提出了一个端到端的个性化学习代理EduLoop-Agent，它集成了神经认知诊断模型（NCD）、有界能力估计计算机自适应测试策略（BECAT）和大型语言模型（LLMs）。NCD模块提供了学生在知识点层面的细粒度掌握估计；BECAT动态选择后续项目以最大化相关性和学习效率；LLMs将诊断信号转换为结构化、可操作的反馈。这些组件共同形成了一个闭环框架“诊断--推荐--反馈”。

Result: 在ASSISTments数据集上的实验表明，NCD模块在响应预测上表现强劲，同时产生了可解释的掌握评估。自适应推荐策略提高了题目的相关性和个性化，基于LLM的反馈提供了与识别出的弱点相一致的目标学习指导。

Conclusion: 实验结果表明，所提出的设计是有效且可实际部署的，为在智能教育中生成个性化的学习轨迹提供了一条可行的路径。

Abstract: As information technology advances, education is moving from
one-size-fits-all instruction toward personalized learning. However, most
methods handle modeling, item selection, and feedback in isolation rather than
as a closed loop. This leads to coarse or opaque student models,
assumption-bound adaptivity that ignores diagnostic posteriors, and generic,
non-actionable feedback. To address these limitations, this paper presents an
end-to-end personalized learning agent, EduLoop-Agent, which integrates a
Neural Cognitive Diagnosis model (NCD), a Bounded-Ability Estimation
Computerized Adaptive Testing strategy (BECAT), and large language models
(LLMs). The NCD module provides fine-grained estimates of students' mastery at
the knowledge-point level; BECAT dynamically selects subsequent items to
maximize relevance and learning efficiency; and LLMs convert diagnostic signals
into structured, actionable feedback. Together, these components form a
closed-loop framework of ``Diagnosis--Recommendation--Feedback.'' Experiments
on the ASSISTments dataset show that the NCD module achieves strong performance
on response prediction while yielding interpretable mastery assessments. The
adaptive recommendation strategy improves item relevance and personalization,
and the LLM-based feedback offers targeted study guidance aligned with
identified weaknesses. Overall, the results indicate that the proposed design
is effective and practically deployable, providing a feasible pathway to
generating individualized learning trajectories in intelligent education.

</details>


### [47] [Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems](https://arxiv.org/abs/2510.22581)
*Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 本文探讨了AI教育领域中智能辅导系统评估的挑战，并提出了改进评估方法的研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可靠、普遍接受且以教学法为导向的评估框架和基准，这些系统的进展和影响仍然难以追踪。

Method: 本文回顾了现有的ITS评估实践，并通过真实案例研究强调了相关挑战。

Result: 本文提供了全面的最新评估实践，并提出了三个研究方向。

Conclusion: 本文提出了三个实际、可行且有理论基础的研究方向，旨在建立公平、统一和可扩展的ITS评估方法。

Abstract: The interdisciplinary research domain of Artificial Intelligence in Education
(AIED) has a long history of developing Intelligent Tutoring Systems (ITSs) by
integrating insights from technological advancements, educational theories, and
cognitive psychology. The remarkable success of generative AI (GenAI) models
has accelerated the development of large language model (LLM)-powered ITSs,
which have potential to imitate human-like, pedagogically rich, and cognitively
demanding tutoring. However, the progress and impact of these systems remain
largely untraceable due to the absence of reliable, universally accepted, and
pedagogy-driven evaluation frameworks and benchmarks. Most existing educational
dialogue-based ITS evaluations rely on subjective protocols and
non-standardized benchmarks, leading to inconsistencies and limited
generalizability. In this work, we take a step back from mainstream ITS
development and provide comprehensive state-of-the-art evaluation practices,
highlighting associated challenges through real-world case studies from careful
and caring AIED research. Finally, building on insights from previous
interdisciplinary AIED research, we propose three practical, feasible, and
theoretically grounded research directions, rooted in learning science
principles and aimed at establishing fair, unified, and scalable evaluation
methodologies for ITSs.

</details>


### [48] [AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment](https://arxiv.org/abs/2510.22593)
*Dario Loi,Elena Maria Muià,Federico Siciliano,Giovanni Trappolini,Vincenzo Crisà,Peter Kruger,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: AutoBench is a dynamic and automated framework for evaluating Large Language Models through reciprocal peer assessment, offering a scalable and contamination-resistant alternative to static benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of static benchmarks, which suffer from test-set contamination and limited adaptability. AutoBench provides a dynamic and robust evaluation methodology for Large Language Models.

Method: AutoBench is a fully automated and self-sustaining framework for evaluating Large Language Models (LLMs) through reciprocal peer assessment. It dynamically generates novel evaluation tasks while models alternately serve as question generators, contestants, and judges across diverse domains. An iterative weighting mechanism amplifies the influence of consistently reliable evaluators, aggregating peer judgments into consensus-based rankings.

Result: Experiments demonstrate strong correlations with established benchmarks including MMLU-Pro and GPQA (respectively 78% and 63%), validating this peer-driven evaluation paradigm. The multi-judge design significantly outperforms single-judge baselines, confirming that distributed evaluation produces more robust and human-consistent assessments.

Conclusion: AutoBench offers a scalable, contamination-resistant alternative to static benchmarks for the continuous evaluation of evolving language models.

Abstract: We present AutoBench, a fully automated and self-sustaining framework for
evaluating Large Language Models (LLMs) through reciprocal peer assessment.
This paper provides a rigorous scientific validation of the AutoBench
methodology, originally developed as an open-source project by eZecute S.R.L..
Unlike static benchmarks that suffer from test-set contamination and limited
adaptability, AutoBench dynamically generates novel evaluation tasks while
models alternately serve as question generators, contestants, and judges across
diverse domains. An iterative weighting mechanism amplifies the influence of
consistently reliable evaluators, aggregating peer judgments into
consensus-based rankings that reflect collective model agreement. Our
experiments demonstrate strong correlations with established benchmarks
including MMLU-Pro and GPQA (respectively 78\% and 63\%), validating this
peer-driven evaluation paradigm. The multi-judge design significantly
outperforms single-judge baselines, confirming that distributed evaluation
produces more robust and human-consistent assessments. AutoBench offers a
scalable, contamination-resistant alternative to static benchmarks for the
continuous evaluation of evolving language models.

</details>


### [49] [Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance](https://arxiv.org/abs/2510.22602)
*Mahyar Abbasian,Ramesh Jain*

Main category: cs.CL

TL;DR: PCU is a cybernetic system for lifelong health guidance that continuously orchestrates multimodal data, knowledge, and services to assist individuals and populations. It offers three essential capabilities: (1) trusted health information tailored to the individual, (2) proactive health navigation and behavior guidance, and (3) ongoing interpretation of recovery and treatment response after medical events.


<details>
  <summary>Details</summary>
Motivation: To provide continuous health guidance by orchestrating multimodal data, knowledge, and services for individuals and populations.

Method: Building on decades of success in digital infrastructure and biomedical innovation, we propose the Personal Care Utility (PCU) - a cybernetic system for lifelong health guidance.

Result: PCU offers three essential capabilities: (1) trusted health information tailored to the individual, (2) proactive health navigation and behavior guidance, and (3) ongoing interpretation of recovery and treatment response after medical events.

Conclusion: PCU promises not only improved outcomes for individuals but also a new substrate for public health and scientific discovery.

Abstract: Building on decades of success in digital infrastructure and biomedical
innovation, we propose the Personal Care Utility (PCU) - a cybernetic system
for lifelong health guidance. PCU is conceived as a global, AI-powered utility
that continuously orchestrates multimodal data, knowledge, and services to
assist individuals and populations alike. Drawing on multimodal agents,
event-centric modeling, and contextual inference, it offers three essential
capabilities: (1) trusted health information tailored to the individual, (2)
proactive health navigation and behavior guidance, and (3) ongoing
interpretation of recovery and treatment response after medical events. Unlike
conventional episodic care, PCU functions as an ambient, adaptive companion -
observing, interpreting, and guiding health in real time across daily life. By
integrating personal sensing, experiential computing, and population-level
analytics, PCU promises not only improved outcomes for individuals but also a
new substrate for public health and scientific discovery. We describe the
architecture, design principles, and implementation challenges of this emerging
paradigm.

</details>


### [50] [PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion](https://arxiv.org/abs/2510.22616)
*Morteza Alikhani,Mohammadtaha Bagherifard,Erfan Zinvandi,Mehran Sarmadi*

Main category: cs.CL

TL;DR: PerCoR是第一个大规模的波斯语常识推理基准数据集，通过创新的方法生成具有挑战性的干扰项，并展示了在该领域的性能差距。


<details>
  <summary>Details</summary>
Motivation: 为了推动波斯语常识推理的研究，需要一个大规模的基准数据集。

Method: 提出了DRESS-AF方法来生成具有挑战性的干扰项，并使用结合分割策略生成连贯的句子补全对。

Result: PerCoR包含106K个多项选择句子补全问题，人类标注者得分为89%，而OpenAI-o3达到最高性能92.18%。

Conclusion: PerCoR是一个具有挑战性的数据集，展示了在波斯语常识推理方面的性能差距。

Abstract: We introduced PerCoR (Persian Commonsense Reasoning), the first large-scale
Persian benchmark for commonsense reasoning. PerCoR contains 106K
multiple-choice sentence-completion problems drawn from more than forty news,
cultural, and other web sources. We introduce a novel conjunction-based
segmentation strategy to generate coherent sentence-completion pairs, enabling
broad topical and structural diversity. To create challenging distractors, we
propose DRESS-AF (Distractor Ranking via Embedding Similarity Scoring and
Adversarial Filtering), a generation-free adversarial filtering method that
selects distractors from the pool of gold continuations while maximising model
confusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the
highest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%).
The strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both
the dataset's difficulty and the remaining performance gap in Persian
commonsense reasoning. We further show that DRESS-AF transfers to the English
HellaSwag benchmark, increasing its difficulty without hurting human
solvability. The dataset is available at
https://huggingface.co/datasets/MCINext/PerCoR.

</details>


### [51] [Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal](https://arxiv.org/abs/2510.22629)
*Ambalika Guha,Sajal Saha,Debanjan Ballav,Soumi Mitra,Hritwick Chakraborty*

Main category: cs.CL

TL;DR: 本研究旨在开发一个三语（Toto-Bangla-English）语言学习应用程序，以数字档案和推广印度西孟加拉邦的濒危Toto语言。研究包括通过实地调查收集详细的语言文献，随后创建一个带有词素标记的三语语料库，用于训练小型语言模型（SLM）和基于Transformer的翻译引擎。分析涵盖了屈折形态学、派生策略以及脚本标准化和数字素养工具的开发。该研究提供了一个可持续的模型，将传统语言学方法与人工智能相结合，以保护濒危语言。


<details>
  <summary>Details</summary>
Motivation: Preserving linguistic diversity is necessary as every language offers a distinct perspective on the world. There have been numerous global initiatives to preserve endangered languages through documentation.

Method: The research includes detailed linguistic documentation collected via fieldwork, followed by the creation of a morpheme-tagged, trilingual corpus used to train a Small Language Model (SLM) and a Transformer-based translation engine.

Result: The application, designed for both native Toto speakers and non-native learners, aims to revitalize the language by ensuring accessibility and usability through Unicode script integration and a structured language corpus. Script standardization and digital literacy tools were also developed to enhance script usage.

Conclusion: This study offers a sustainable model for preserving endangered languages by incorporating traditional linguistic methodology with AI. It highlights the value of interdisciplinary collaboration for community-based language revitalization.

Abstract: Preserving linguistic diversity is necessary as every language offers a
distinct perspective on the world. There have been numerous global initiatives
to preserve endangered languages through documentation. This paper is a part of
a project which aims to develop a trilingual (Toto-Bangla-English) language
learning application to digitally archive and promote the endangered Toto
language of West Bengal, India. This application, designed for both native Toto
speakers and non-native learners, aims to revitalize the language by ensuring
accessibility and usability through Unicode script integration and a structured
language corpus. The research includes detailed linguistic documentation
collected via fieldwork, followed by the creation of a morpheme-tagged,
trilingual corpus used to train a Small Language Model (SLM) and a
Transformer-based translation engine. The analysis covers inflectional
morphology such as person-number-gender agreement, tense-aspect-mood
distinctions, and case marking, alongside derivational strategies that reflect
word-class changes. Script standardization and digital literacy tools were also
developed to enhance script usage. The study offers a sustainable model for
preserving endangered languages by incorporating traditional linguistic
methodology with AI. This bridge between linguistic research with technological
innovation highlights the value of interdisciplinary collaboration for
community-based language revitalization.

</details>


### [52] [Culturally Grounded Physical Commonsense Reasoning in Italian and English: A Submission to the MRL 2025 Shared Task](https://arxiv.org/abs/2510.22631)
*Marco De Santis,Lisa Alazraki*

Main category: cs.CL

TL;DR: 本文介绍了FormaMentis，这是一个针对意大利语和文化的物理常识推理新基准。


<details>
  <summary>Details</summary>
Motivation: 本文旨在创建多语言物理常识推理评估数据，特别是针对非英语语言。

Method: 本文通过专家标注者创建了意大利语数据样本，并将其翻译成英语，同时保留了意大利文化元素。

Result: 本文提出了FormaMentis，这是一个新的物理常识推理基准，适用于意大利语和文化背景。

Conclusion: 本文提出了FormaMentis，这是一个基于意大利语和文化的物理常识推理新基准。

Abstract: This paper presents our submission to the MRL 2025 Shared Task on
Multilingual Physical Reasoning Datasets. The objective of the shared task is
to create manually-annotated evaluation data in the physical commonsense
reasoning domain, for languages other than English, following a format similar
to PIQA. Our contribution, FormaMentis, is a novel benchmark for physical
commonsense reasoning that is grounded in Italian language and culture. The
data samples in FormaMentis are created by expert annotators who are native
Italian speakers and are familiar with local customs and norms. The samples are
additionally translated into English, while preserving the cultural elements
unique to the Italian context.

</details>


### [53] [Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2510.22656)
*Zilong Wang,Qingtian Zeng,Hua Duan,Cheng Cheng,Minghao Zou,Ziyang Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的FKGC框架（CR-FKGC），以解决少样本知识图谱补全中的复杂关系模式捕捉和数据稀疏性问题。通过引入邻域聚合编码器、共轭关系学习器和流形共轭解码器，实验结果表明该方法在三个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉复杂的关联模式并缓解数据稀疏性问题。

Method: 我们提出了一个用于共轭关系建模的新型FKGC框架（CR-FKGC）。具体来说，它使用了邻域聚合编码器来整合高阶邻居信息，一个结合隐式条件扩散关系模块和稳定关系模块的共轭关系学习器来捕捉稳定语义和不确定性偏移，以及一个流形共轭解码器用于流形空间中缺失三元组的有效评估和推理。

Result: 我们的方法在三个基准测试中表现优于最先进的方法。

Conclusion: 我们的方法在三个基准测试中表现优于最先进的方法。

Abstract: Few-shot Knowledge Graph Completion (FKGC) infers missing triples from
limited support samples, tackling long-tail distribution challenges. Existing
methods, however, struggle to capture complex relational patterns and mitigate
data sparsity. To address these challenges, we propose a novel FKGC framework
for conjugate relation modeling (CR-FKGC). Specifically, it employs a
neighborhood aggregation encoder to integrate higher-order neighbor
information, a conjugate relation learner combining an implicit conditional
diffusion relation module with a stable relation module to capture stable
semantics and uncertainty offsets, and a manifold conjugate decoder for
efficient evaluation and inference of missing triples in manifold space.
Experiments on three benchmarks demonstrate that our method achieves superior
performance over state-of-the-art methods.

</details>


### [54] [Rule-Based Explanations for Retrieval-Augmented LLM Systems](https://arxiv.org/abs/2510.22689)
*Joel Rorseth,Parke Godfrey,Lukasz Golab,Divesh Srivastava,Jarek Szlichta*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，利用如果-那么规则来解释使用检索增强生成（RAG）的大型语言模型（LLM）的输出来源，并通过优化方法提高了规则生成的效率。


<details>
  <summary>Details</summary>
Motivation: 如果-那么规则广泛用于解释机器学习模型，例如“如果就业状况为否，则贷款申请被拒绝”。我们提出首次将规则应用于解释大型语言模型（LLM）的新兴类别，这些模型使用检索增强生成（RAG）。由于RAG使LLM系统能够在推理时结合检索的信息源，因此链接信息源存在或不存在的规则可以解释输出来源。

Method: 我们提出了优化方法，以加速规则生成，这些方法受到频繁项集挖掘中类似Apriori的剪枝的启发，但重新定义在我们新问题的范围内。

Result: 我们通过定性和定量实验展示了我们解决方案的价值和效率。

Conclusion: 我们通过定性和定量实验展示了我们解决方案的价值和效率。

Abstract: If-then rules are widely used to explain machine learning models; e.g., "if
employed = no, then loan application = rejected." We present the first proposal
to apply rules to explain the emerging class of large language models (LLMs)
with retrieval-augmented generation (RAG). Since RAG enables LLM systems to
incorporate retrieved information sources at inference time, rules linking the
presence or absence of sources can explain output provenance; e.g., "if a Times
Higher Education ranking article is retrieved, then the LLM ranks Oxford
first." To generate such rules, a brute force approach would probe the LLM with
all source combinations and check if the presence or absence of any sources
leads to the same output. We propose optimizations to speed up rule generation,
inspired by Apriori-like pruning from frequent itemset mining but redefined
within the scope of our novel problem. We conclude with qualitative and
quantitative experiments demonstrating our solutions' value and efficiency.

</details>


### [55] [SALSA: Single-pass Autoregressive LLM Structured Classification](https://arxiv.org/abs/2510.22691)
*Ruslan Berdichevsky,Shai Nahum-Gefen,Elad Ben Zaken*

Main category: cs.CL

TL;DR: SALSA是一种结合结构化提示、类到标记映射和参数高效微调的管道，用于提高大语言模型在文本分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调整的大语言模型具有出色的泛化能力，但在文本分类基准测试中往往表现不佳。

Method: SALSA结合了结构化提示、类到标记的映射和参数高效的微调，避免了冷启动训练。每个类标签被映射到一个独特的输出标记，并构建提示以引发单个标记的响应。在推理过程中，模型的输出仅投影到相关类标记的logits上，从而实现一次前向传递中的高效准确分类。

Result: SALSA在多种基准测试中实现了最先进的结果，展示了其在基于LLM的分类应用中的鲁棒性和可扩展性。

Conclusion: SALSA在基于LLM的分类应用中表现出强大的鲁棒性和可扩展性，达到了最先进的结果。

Abstract: Despite their impressive generalization capabilities, instruction-tuned Large
Language Models often underperform on text classification benchmarks. We
introduce SALSA, a coherent pipeline that combines structured prompting,
class-to-token mapping, and parameter-efficient fine-tuning, thereby avoiding
cold-start training. Each class label is mapped to a distinct output token, and
prompts are constructed to elicit a single-token response. During inference,
the model's output is projected only onto the logits of the relevant class
tokens, enabling efficient and accurate classification in a single forward
pass. SALSA achieves state-of-the-art results across diverse benchmarks,
demonstrating its robustness and scalability for LLM-based classification
applications.

</details>


### [56] [$\text{E}^2\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker](https://arxiv.org/abs/2510.22733)
*Qi Liu,Yanzhao Zhang,Mingxin Li,Dingkun Long,Pengjun Xie,Jiaxin Mao*

Main category: cs.CL

TL;DR: 本文提出了一种名为E^2Rank的统一框架，通过持续训练文本嵌入模型来实现高效的检索和列表重排序，取得了优异的性能表现。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入模型在实际搜索应用中起着基础作用，但它们的排名保真度相比专用重排序器仍有局限，特别是最近基于LLM的列表重排序器能够捕捉细粒度的查询-文档和文档-文档交互。

Method: 我们提出了一个简单而有效的统一框架E^2Rank，它通过在列表排序目标下的持续训练，将单个文本嵌入模型扩展为同时进行高质量检索和列表重排序。

Result: E^2Rank在BEIR重排序基准上实现了最先进的结果，并在需要推理的BRIGHT基准上表现出具有竞争力的性能，且重排序延迟非常低。此外，重排序训练过程还提高了MTEB基准上的嵌入性能。

Conclusion: 我们的研究结果表明，单一的嵌入模型可以有效地统一检索和重排序，提供计算效率和竞争性的排名准确性。

Abstract: Text embedding models serve as a fundamental component in real-world search
applications. By mapping queries and documents into a shared embedding space,
they deliver competitive retrieval performance with high efficiency. However,
their ranking fidelity remains limited compared to dedicated rerankers,
especially recent LLM-based listwise rerankers, which capture fine-grained
query-document and document-document interactions. In this paper, we propose a
simple yet effective unified framework $\text{E}^2\text{Rank}$, means Efficient
Embedding-based Ranking (also means Embedding-to-Rank), which extends a single
text embedding model to perform both high-quality retrieval and listwise
reranking through continued training under a listwise ranking objective,
thereby achieving strong effectiveness with remarkable efficiency. By applying
cosine similarity between the query and document embeddings as a unified
ranking function, the listwise ranking prompt, which is constructed from the
original query and its candidate documents, serves as an enhanced query
enriched with signals from the top-K documents, akin to pseudo-relevance
feedback (PRF) in traditional retrieval models. This design preserves the
efficiency and representational quality of the base embedding model while
significantly improving its reranking performance. Empirically,
$\textrm{E}^2\text{Rank}$ achieves state-of-the-art results on the BEIR
reranking benchmark and demonstrates competitive performance on the
reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also
show that the ranking training process improves embedding performance on the
MTEB benchmark. Our findings indicate that a single embedding model can
effectively unify retrieval and reranking, offering both computational
efficiency and competitive ranking accuracy.

</details>


### [57] [Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study](https://arxiv.org/abs/2510.22747)
*Eeham Khan,Firas Saidani,Owen Van Esbroeck,Richard Khoury,Leila Kosseim*

Main category: cs.CL

TL;DR: 本研究探讨了在有限的数据和计算预算下，利用持续预训练和参数高效微调来适应魁北克法语方言的方法，并展示了其在减少方言差距方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）被广泛采用，但它们最强的能力仍局限于少数高资源语言。为了使这些模型适应低资源地区方言，研究了持续预训练（CPT）的应用。

Method: 使用低秩适应（LoRA）和计算高效的持续预训练，将三个大型语言模型适配到魁北克法语方言，并在COLE套件上进行基准测试。

Result: 实验结果显示，在仅更新不到1%的模型参数的情况下，可以在少数方言基准上取得改进，同时在主流语言基准上几乎没有退化。分析表明，收益高度依赖于语料库组成。

Conclusion: 研究结果表明，通过参数高效微调（PEFT）的持续预训练（CPT）可以缩小方言差距，为少数语言群体提供成本效益高且可持续的语言资源创建。

Abstract: Despite the widespread adoption of large language models (LLMs), their
strongest capabilities remain largely confined to a small number of
high-resource languages for which there is abundant training data. Recently,
continual pre-training (CPT) has emerged as a means to fine-tune these models
to low-resource regional dialects. In this paper, we study the use of CPT for
dialect learning under tight data and compute budgets. Using low-rank
adaptation (LoRA) and compute-efficient continual pre-training, we adapt three
LLMs to the Qu\'ebec French dialect using a very small dataset and benchmark
them on the COLE suite. Our experiments demonstrate an improvement on the
minority dialect benchmarks with minimal regression on the prestige language
benchmarks with under 1% of model parameters updated. Analysis of the results
demonstrate that gains are highly contingent on corpus composition. These
findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can
narrow the dialect gap by providing cost-effective and sustainable language
resource creation, expanding high-quality LLM access to minority linguistic
communities. We release the first Qu\'ebec French LLMs on HuggingFace.

</details>


### [58] [Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models](https://arxiv.org/abs/2510.22752)
*Anooshka Bajaj,Deven Mahesh Mistry,Sahaj Singh Maini,Yash Aggarwal,Zoran Tiganj*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在in-context learning中对时间分离事件的区分和检索能力，发现模型在重复标记后倾向于给出更高的概率，但存在时间位置的偏见。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究各种预训练大型语言模型（包括transformer和状态空间模型）区分和检索时间上分离的事件的能力。

Method: 我们通过提示模型使用包含多个相同标记重复出现的序列，这些标记在序列末尾重新出现。通过固定这些重复标记的位置并打乱其他所有标记，我们消除了语义混杂因素，隔离了时间效应对于下一个标记预测的影响。

Result: 模型在重复标记之后的标记上始终给予最高的概率，但对靠近输入开头或结尾的标记有显著的偏见。消融实验将这种现象与transformer中的归纳头联系起来。扩展分析到具有部分重叠的唯一语义上下文进一步表明，嵌入在提示中间的记忆检索可靠性较低。

Conclusion: 我们的研究加深了对in-context learning中时间偏见的理解，并展示了这些偏见如何能够实现时间分离和情景检索。

Abstract: In-context learning is governed by both temporal and semantic relationships,
shaping how Large Language Models (LLMs) retrieve contextual information.
Analogous to human episodic memory, where the retrieval of specific events is
enabled by separating events that happened at different times, this work probes
the ability of various pretrained LLMs, including transformer and state-space
models, to differentiate and retrieve temporally separated events.
Specifically, we prompted models with sequences containing multiple
presentations of the same token, which reappears at the sequence end. By fixing
the positions of these repeated tokens and permuting all others, we removed
semantic confounds and isolated temporal effects on next-token prediction.
Across diverse sequences, models consistently placed the highest probabilities
on tokens following a repeated token, but with a notable bias for those nearest
the beginning or end of the input. An ablation experiment linked this
phenomenon in transformers to induction heads. Extending the analysis to unique
semantic contexts with partial overlap further demonstrated that memories
embedded in the middle of a prompt are retrieved less reliably. Despite
architectural differences, state-space and transformer models showed comparable
temporal biases. Our findings deepen the understanding of temporal biases in
in-context learning and offer an illustration of how these biases can enable
temporal separation and episodic retrieval.

</details>


### [59] [EchoMind: An Interrelated Multi-level Benchmark for Evaluating Empathetic Speech Language Models](https://arxiv.org/abs/2510.22758)
*Li Zhou,Lutong Yu,You Lyu,Yihang Lin,Zefeng Zhao,Junyi Ao,Yuhao Zhang,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: 本文提出了EchoMind，这是一个多级基准，用于评估语音语言模型在同理心对话中的表现。结果显示，即使最先进的模型在处理高表达性语音线索方面也存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有的基准通常单独评估语言、声学、推理或对话能力，忽视了这些技能的整合，这对于人类般的、情感智能的对话至关重要。

Method: 我们提出了EchoMind，这是第一个相互关联的多级基准，通过顺序、上下文相关的任务来模拟同理心对话的认知过程：口语内容理解、语音线索感知、综合推理和回应生成。所有任务都使用相同且语义中性的脚本，没有明确的情感或上下文线索，并使用语音风格的变化来测试交付效果，而无需考虑剧本。

Result: 测试12个先进的SLMs显示，即使是最先进的模型在高表达性语音线索方面也存在困难，限制了同理心回应的质量。对提示强度、语音源和理想语音线索识别的分析揭示了指令遵循、对自然语音变化的弹性以及有效使用语音线索进行同理心的持续弱点。

Conclusion: 这些结果强调了需要将语言内容与多样的语音线索相结合的SLMs，以实现真正富有同理心的对话能力。

Abstract: Speech Language Models (SLMs) have made significant progress in spoken
language understanding. Yet it remains unclear whether they can fully perceive
non lexical vocal cues alongside spoken words, and respond with empathy that
aligns with both emotional and contextual factors. Existing benchmarks
typically evaluate linguistic, acoustic, reasoning, or dialogue abilities in
isolation, overlooking the integration of these skills that is crucial for
human-like, emotionally intelligent conversation. We present EchoMind, the
first interrelated, multi-level benchmark that simulates the cognitive process
of empathetic dialogue through sequential, context-linked tasks: spoken-content
understanding, vocal-cue perception, integrated reasoning, and response
generation. All tasks share identical and semantically neutral scripts that are
free of explicit emotional or contextual cues, and controlled variations in
vocal style are used to test the effect of delivery independent of the
transcript. EchoMind is grounded in an empathy-oriented framework spanning 3
coarse and 12 fine-grained dimensions, encompassing 39 vocal attributes, and
evaluated using both objective and subjective metrics. Testing 12 advanced SLMs
reveals that even state-of-the-art models struggle with high-expressive vocal
cues, limiting empathetic response quality. Analyses of prompt strength, speech
source, and ideal vocal cue recognition reveal persistent weaknesses in
instruction-following, resilience to natural speech variability, and effective
use of vocal cues for empathy. These results underscore the need for SLMs that
integrate linguistic content with diverse vocal cues to achieve truly
empathetic conversational ability.

</details>


### [60] [Iterative Layer Pruning for Efficient Translation Inference](https://arxiv.org/abs/2510.22763)
*Yasmin Moslem,Muhammad Hazim Al Farouq,John D. Kelleher*

Main category: cs.CL

TL;DR: 本文研究了基于层重要性分析的迭代层剪枝方法，以解决大型语言模型高效部署的问题，并在翻译任务中取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）的计算需求密集，其高效部署仍然具有挑战性。

Method: 我们研究了基于层重要性分析的迭代层剪枝方法。

Result: 我们的方法在模型大小和推理时间上取得了显著的减少，同时保持了翻译质量。

Conclusion: 我们的方法在保持基线模型的翻译质量的同时，显著减少了模型大小和推理时间。

Abstract: Large language models (LLMs) have transformed many areas of natural language
processing, including machine translation. However, efficient deployment of
LLMs remains challenging due to their intensive computational requirements. In
this paper, we address this challenge and present our submissions to the Model
Compression track at the Conference on Machine Translation (WMT 2025). In our
experiments, we investigate iterative layer pruning guided by layer importance
analysis. We evaluate this method using the Aya-Expanse-8B model for
translation from Czech to German, and from English to Egyptian Arabic. Our
approach achieves substantial reductions in model size and inference time,
while maintaining the translation quality of the baseline models.

</details>


### [61] [MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion](https://arxiv.org/abs/2510.22768)
*Haoyi Qiu,Yilun Zhou,Pranav Narayanan Venkit,Kung-Hsiang Huang,Jiaxin Zhang,Nanyun Peng,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: MMPersuade is a framework for studying how LVLMs are influenced by persuasive multimodal content, revealing that multimodal inputs are more effective at persuasion than text alone and that different strategies work better in different contexts.


<details>
  <summary>Details</summary>
Motivation: Understanding the susceptibility of LVLMs to persuasion and the effectiveness of different persuasive strategies is crucial, as overly persuadable models may adopt misleading beliefs, override user preferences, or generate unethical or unsafe outputs when exposed to manipulative messages.

Method: MMPersuade is a unified framework for systematically studying multimodal persuasion dynamics in LVLMs, which includes a comprehensive multimodal dataset and an evaluation framework that quantifies both persuasion effectiveness and model susceptibility.

Result: The study of six leading LVLMs as persuadees yields three key insights: (i) multimodal inputs substantially increase persuasion effectiveness and model susceptibility compared to text alone, especially in misinformation scenarios; (ii) stated prior preferences decrease susceptibility, yet multimodal information maintains its persuasive advantage; and (iii) different strategies vary in effectiveness across contexts, with reciprocity being most potent in commercial and subjective contexts, and credibility and logic prevailing in adversarial contexts.

Conclusion: MMPersuade provides a principled foundation for developing models that are robust, preference-consistent, and ethically aligned when engaging with persuasive multimodal content.

Abstract: As Large Vision-Language Models (LVLMs) are increasingly deployed in domains
such as shopping, health, and news, they are exposed to pervasive persuasive
content. A critical question is how these models function as persuadees-how and
why they can be influenced by persuasive multimodal inputs. Understanding both
their susceptibility to persuasion and the effectiveness of different
persuasive strategies is crucial, as overly persuadable models may adopt
misleading beliefs, override user preferences, or generate unethical or unsafe
outputs when exposed to manipulative messages. We introduce MMPersuade, a
unified framework for systematically studying multimodal persuasion dynamics in
LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs
images and videos with established persuasion principles across commercial,
subjective and behavioral, and adversarial contexts, and (ii) an evaluation
framework that quantifies both persuasion effectiveness and model
susceptibility via third-party agreement scoring and self-estimated token
probabilities on conversation histories. Our study of six leading LVLMs as
persuadees yields three key insights: (i) multimodal inputs substantially
increase persuasion effectiveness-and model susceptibility-compared to text
alone, especially in misinformation scenarios; (ii) stated prior preferences
decrease susceptibility, yet multimodal information maintains its persuasive
advantage; and (iii) different strategies vary in effectiveness across
contexts, with reciprocity being most potent in commercial and subjective
contexts, and credibility and logic prevailing in adversarial contexts. By
jointly analyzing persuasion effectiveness and susceptibility, MMPersuade
provides a principled foundation for developing models that are robust,
preference-consistent, and ethically aligned when engaging with persuasive
multimodal content.

</details>


### [62] [Scalable Supervising Software Agents with Patch Reasoner](https://arxiv.org/abs/2510.22775)
*Junjielong Xu,Boyin Tan,Xiaoyuan Liu,Chao Peng,Pengfei Gao,Pinjia He*

Main category: cs.CL

TL;DR: This paper introduces R4P, a scalable patch verification model for SWE agents that improves efficiency and performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing test-based supervision for SWE tasks is unscalable due to the heavy and fragile nature of test sandboxes and the rarity of high-coverage tests.

Method: R4P is a patch verifier model that uses reasoning to provide scalable rewards for training and testing SWE agents. It employs a group-wise objective for RL training to verify multiple patches against each other.

Result: R4P achieves 72.2% Acc. for verifying patches from SWE-bench-verified. Mini-SE, trained with R4P, achieves 26.2% Pass@1 on SWE-bench-verified, showing a 10.0% improvement over Qwen3-32B. R4P verifies patches within a second, 50x faster than testing.

Conclusion: R4P demonstrates practicality through stable scaling curves, high efficiency, and improved performance in verifying patches and training SWE agents.

Abstract: While large language model agents have advanced software engineering tasks,
the unscalable nature of existing test-based supervision is limiting the
potential improvement of data scaling. The reason is twofold: (1) building and
running test sandbox is rather heavy and fragile, and (2) data with
high-coverage tests is naturally rare and threatened by test hacking via edge
cases. In this paper, we propose R4P, a patch verifier model to provide
scalable rewards for training and testing SWE agents via reasoning. We consider
that patch verification is fundamentally a reasoning task, mirroring how human
repository maintainers review patches without writing and running new
reproduction tests. To obtain sufficient reference and reduce the risk of
reward hacking, R4P uses a group-wise objective for RL training, enabling it to
verify multiple patches against each other's modification and gain a dense
reward for stable training. R4P achieves 72.2% Acc. for verifying patches from
SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we
design and train a lite scaffold, Mini-SE, with pure reinforcement learning
where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2%
Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original
Qwen3-32B. This can be further improved to 32.8% with R4P for test-time
scaling. Furthermore, R4P verifies patches within a second, 50x faster than
testing on average. The stable scaling curves of rewards and accuracy along
with high efficiency reflect R4P's practicality.

</details>


### [63] [VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions](https://arxiv.org/abs/2510.22798)
*Thu Phuong Nguyen,Duc M. Nguyen,Hyotaek Jeon,Hyunwook Lee,Hyunmin Song,Sungahn Ko,Taehwan Kim*

Main category: cs.CL

TL;DR: VEHME is a vision-language model designed to evaluate handwritten mathematical expressions with high accuracy and interpretable reasoning, showing promising results in automated math assessment.


<details>
  <summary>Details</summary>
Motivation: Automatically assessing handwritten mathematical solutions is important in educational technology but remains challenging due to diverse formats, unstructured layouts, and symbolic complexity.

Method: VEHME uses a two-phase training pipeline: (i) supervised fine-tuning using structured reasoning data, and (ii) reinforcement learning that aligns model outputs with multi-dimensional grading objectives. It also includes an Expression-Aware Visual Prompting Module to enhance spatial understanding.

Result: VEHME achieves state-of-the-art performance on AIHub and FERMAT datasets among open-source models and approaches the accuracy of proprietary systems.

Conclusion: VEHME demonstrates potential as a scalable and accessible tool for automated math assessment, achieving state-of-the-art performance among open-source models and approaching the accuracy of proprietary systems.

Abstract: Automatically assessing handwritten mathematical solutions is an important
problem in educational technology with practical applications, but it remains a
significant challenge due to the diverse formats, unstructured layouts, and
symbolic complexity of student work. To address this challenge, we introduce
VEHME-a Vision-Language Model for Evaluating Handwritten Mathematics
Expressions-designed to assess open-form handwritten math responses with high
accuracy and interpretable reasoning traces. VEHME integrates a two-phase
training pipeline: (i) supervised fine-tuning using structured reasoning data,
and (ii) reinforcement learning that aligns model outputs with
multi-dimensional grading objectives, including correctness, reasoning depth,
and error localization. To enhance spatial understanding, we propose an
Expression-Aware Visual Prompting Module, trained on our synthesized multi-line
math expressions dataset to robustly guide attention in visually heterogeneous
inputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art
performance among open-source models and approaches the accuracy of proprietary
systems, demonstrating its potential as a scalable and accessible tool for
automated math assessment. Our training and experiment code is publicly
available at our GitHub repository.

</details>


### [64] [Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP](https://arxiv.org/abs/2510.22823)
*Poli Nemkova,Amrit Adhikari,Matthew Pearson,Vamsi Krishna Sadu,Mark V. Albert*

Main category: cs.CL

TL;DR: 本研究比较了商业和开源大型语言模型在多语言人权侵犯检测中的表现，发现对齐模型在不同语言中表现出更高的稳定性和可靠性，为资源有限的人道主义组织提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 人道主义组织面临一个关键选择：投资昂贵的商业API或依赖免费的开源模型进行多语言人权监测。虽然商业系统提供可靠性，但开源替代方案缺乏实证验证，尤其是在冲突地区常见的低资源语言中。

Method: 本研究对商业和开源大型语言模型（LLMs）进行了系统的比较，评估了六种模型（四种指令对齐模型和两种开源模型）在七种语言中检测人权侵犯情况的表现，并使用了标准分类指标和新的跨语言可靠性度量方法。

Result: 研究结果表明，对齐而非规模决定了稳定性：对齐模型在语言类型差异大和低资源语言中保持近似不变的准确性和平衡校准，而开源模型表现出显著的提示语言敏感性和校准漂移。

Conclusion: 研究结果表明，多语言对齐能够实现语言无关的推理，并为资源受限的人道主义组织在多语言部署中平衡预算约束与可靠性提供了实际指导。

Abstract: Humanitarian organizations face a critical choice: invest in costly
commercial APIs or rely on free open-weight models for multilingual human
rights monitoring. While commercial systems offer reliability, open-weight
alternatives lack empirical validation -- especially for low-resource languages
common in conflict zones. This paper presents the first systematic comparison
of commercial and open-weight large language models (LLMs) for
human-rights-violation detection across seven languages, quantifying the
cost-reliability trade-off facing resource-constrained organizations. Across
78,000 multilingual inferences, we evaluate six models -- four
instruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0,
GPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both
standard classification metrics and new measures of cross-lingual reliability:
Calibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS),
and Language Stability Score (LSS). Results show that alignment, not scale,
determines stability: aligned models maintain near-invariant accuracy and
balanced calibration across typologically distant and low-resource languages
(e.g., Lingala, Burmese), while open-weight models exhibit significant
prompt-language sensitivity and calibration drift. These findings demonstrate
that multilingual alignment enables language-agnostic reasoning and provide
practical guidance for humanitarian organizations balancing budget constraints
with reliability in multilingual deployment.

</details>


### [65] [Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays](https://arxiv.org/abs/2510.22830)
*Haowei Hua,Hong Jiao,Xinyi Wang*

Main category: cs.CL

TL;DR: 该研究利用生成语言模型通过摘要和提示方法改进了长作文的自动评分，显著提高了评分准确性。


<details>
  <summary>Details</summary>
Motivation: 由于BERT及其变体等编码器模型的512个标记限制，导致在长作文自动评分中存在不足，因此需要探索生成语言模型来解决这一问题。

Method: 该研究探索了生成语言模型在长作文自动评分中的应用，通过摘要和提示方法来克服编码器模型的512个标记限制。

Result: 研究结果表明，使用生成语言模型进行长作文自动评分，QWK指标从0.822提高到了0.8878。

Conclusion: 研究结果显示，通过摘要和提示方法，使用生成语言模型在长作文自动评分中取得了显著的准确性提升。

Abstract: BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.

</details>


### [66] [Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning](https://arxiv.org/abs/2510.22844)
*Prerna Ravi,Dong Won Lee,Beatriz Flamia,Jasmine David,Brandon Hanks,Cynthia Breazeal,Emma Anderson,Grace Lin*

Main category: cs.CL

TL;DR: 本文研究了显式线程链接是否可以改善基于LLM的群体对话关系移动编码。贡献了一个系统指南用于识别同步多方转录本中的线程，并基准测试了不同的LLM提示策略。结果表明，提供清晰的对话线信息可以提高LLM编码性能，并强调了下游分析对结构化对话的依赖。


<details>
  <summary>Details</summary>
Motivation: 理解想法在小组对话中的发展和流动对于分析协作学习至关重要。然而，在同步口语对话中检测线程仍然具有挑战性，因为存在重叠的发言和隐含线索。同时，大型语言模型（LLMs）在自动化话语分析方面显示出潜力，但往往在依赖追踪这些对话联系的长上下文任务中遇到困难。

Method: 本文贡献了一个系统指南，用于在同步多方转录本中识别线程，并基准测试了不同的LLM提示策略以实现自动线程。然后测试了线程如何影响下游对话分析框架的编码性能。

Result: 结果表明，提供清晰的对话线信息可以提高LLM编码性能，并强调了下游分析对结构化对话的依赖。

Conclusion: 本文展示了提供清晰的对话线信息可以提高LLM编码性能，并强调了下游分析对结构化对话的依赖。同时讨论了时间和成本的实际权衡，强调了人机混合方法可以产生最佳价值。

Abstract: Understanding how ideas develop and flow in small-group conversations is
critical for analyzing collaborative learning. A key structural feature of
these interactions is threading, the way discourse talk naturally organizes
into interwoven topical strands that evolve over time. While threading has been
widely studied in asynchronous text settings, detecting threads in synchronous
spoken dialogue remains challenging due to overlapping turns and implicit cues.
At the same time, large language models (LLMs) show promise for automating
discourse analysis but often struggle with long-context tasks that depend on
tracing these conversational links. In this paper, we investigate whether
explicit thread linkages can improve LLM-based coding of relational moves in
group talk. We contribute a systematic guidebook for identifying threads in
synchronous multi-party transcripts and benchmark different LLM prompting
strategies for automated threading. We then test how threading influences
performance on downstream coding of conversational analysis frameworks, that
capture core collaborative actions such as agreeing, building, and eliciting.
Our results show that providing clear conversational thread information
improves LLM coding performance and underscores the heavy reliance of
downstream analysis on well-structured dialogue. We also discuss practical
trade-offs in time and cost, emphasizing where human-AI hybrid approaches can
yield the best value. Together, this work advances methods for combining LLMs
and robust conversational thread structures to make sense of complex, real-time
group interactions.

</details>


### [67] [Once Upon an Input: Reasoning via Per-Instance Program Synthesis](https://arxiv.org/abs/2510.22849)
*Adam Stein,Neelay Velingker,Mayur Naik,Eric Wong*

Main category: cs.CL

TL;DR: PIPS是一种新的方法，通过实例级程序合成和置信度指标提高大型语言模型的多步骤推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如Chain of Thought (CoT)和Program of Thought (PoT)虽然提高了性能，但在算法领域常常产生不良解决方案。

Method: PIPS通过结构反馈在实例级别生成和优化程序，结合置信度指标动态选择直接推理或程序合成。

Result: PIPS在三个前沿LLM和30个基准测试中表现出色，包括Big Bench Extra Hard (BBEH)的所有任务、视觉问答任务、关系推理任务和数学推理任务。

Conclusion: PIPS在多个基准测试中提高了绝对调和均值准确率，并减少了算法任务中不良程序生成的数量。

Abstract: Large language models (LLMs) excel at zero-shot inference but continue to
struggle with complex, multi-step reasoning. Recent methods that augment LLMs
with intermediate reasoning steps such as Chain of Thought (CoT) and Program of
Thought (PoT) improve performance but often produce undesirable solutions,
especially in algorithmic domains. We introduce Per-Instance Program Synthesis
(PIPS), a method that generates and refines programs at the instance-level
using structural feedback without relying on task-specific guidance or explicit
test cases. To further improve performance, PIPS incorporates a confidence
metric that dynamically chooses between direct inference and program synthesis
on a per-instance basis. Experiments across three frontier LLMs and 30
benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question
answering tasks, relational reasoning tasks, and mathematical reasoning tasks
show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and
9.4% compared to PoT and CoT respectively, and reduces undesirable program
generations by 65.1% on the algorithmic tasks compared to PoT with
Gemini-2.0-Flash.

</details>


### [68] [Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement](https://arxiv.org/abs/2510.22860)
*Linyang He,Tianjun Zhong,Richard Antonello,Gavin Mischler,Micah Goldblum,Nima Mesgarani*

Main category: cs.CL

TL;DR: 本文提出了一种残差解缠方法，用于分离语言模型中的不同特征层，从而更好地研究神经网络中深层次认知过程的神经基础。


<details>
  <summary>Details</summary>
Motivation: 传统的脑编码分析容易偏向于语言表层特征（如词汇和语法），难以分离出更深层次的认知过程。因此，需要一种方法来解缠语言模型中的不同特征，以更好地研究神经活动的深层机制。

Method: 本文引入了一种残差解缠方法，通过探测语言模型来识别特定特征层，并迭代回归出低层次表示，生成四个几乎正交的嵌入向量，分别对应词汇、语法、意义和推理。

Result: 实验结果表明，解缠后的推理嵌入具有独特的预测能力，能够解释其他语言特征无法解释的神经活动变化，并扩展到视觉区域。此外，推理的神经信号在时间上与其他语言特征不同，且标准的非解缠嵌入可能产生误导。

Conclusion: 本文提出了一种残差解缠方法，可以计算分离语言模型中的不同特征层，从而更好地理解神经网络中深层次认知过程的神经基础。

Abstract: Understanding how the human brain progresses from processing simple
linguistic inputs to performing high-level reasoning is a fundamental challenge
in neuroscience. While modern large language models (LLMs) are increasingly
used to model neural responses to language, their internal representations are
highly "entangled," mixing information about lexicon, syntax, meaning, and
reasoning. This entanglement biases conventional brain encoding analyses toward
linguistically shallow features (e.g., lexicon and syntax), making it difficult
to isolate the neural substrates of cognitively deeper processes. Here, we
introduce a residual disentanglement method that computationally isolates these
components. By first probing an LM to identify feature-specific layers, our
method iteratively regresses out lower-level representations to produce four
nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically,
reasoning. We used these disentangled embeddings to model intracranial (ECoG)
brain recordings from neurosurgical patients listening to natural speech. We
show that: 1) This isolated reasoning embedding exhibits unique predictive
power, accounting for variance in neural activity not explained by other
linguistic features and even extending to the recruitment of visual regions
beyond classical language areas. 2) The neural signature for reasoning is
temporally distinct, peaking later (~350-400ms) than signals related to
lexicon, syntax, and meaning, consistent with its position atop a processing
hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as
their predictive success is primarily attributable to linguistically shallow
features, masking the more subtle contributions of deeper cognitive processing.

</details>


### [69] [Interpreting and Mitigating Unwanted Uncertainty in LLMs](https://arxiv.org/abs/2510.22866)
*Tiasa Singha Roy,Ayush Rajesh Jhaveri,Ilias Triantafyllopoulos*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型中出现的不确定性问题，并提出了一种通过调整注意力头来减少答案翻转行为的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在重新提示时会改变之前正确的答案，这种行为会削弱信任并在高风险领域带来严重风险。我们需要了解这种现象的机制并找到解决方法。

Method: 我们采用了针在 haystack 检索框架，并集成了一个 Flip 风格的重新评估提示，以模拟现实的答案翻转场景。

Result: 我们发现检索头并不是主要负责避免不确定性的因素，而是识别出一小部分非检索注意力头，在不确定的情况下过度关注误导性标记。屏蔽这些头显著提高了性能，减少了高达15%的翻转行为，而不会引入不连贯或过度纠正。然而，在下游任务中，我们观察到了与翻转行为的权衡。

Conclusion: 我们的研究为理解大型语言模型中的不确定性提供了新的见解，并提出了一种简单而有效的方法来减轻由不确定性驱动的故障模式。

Abstract: Despite their impressive capabilities, Large Language Models (LLMs) exhibit
unwanted uncertainty, a phenomenon where a model changes a previously correct
answer into an incorrect one when re-prompted. This behavior undermines trust
and poses serious risks in high-stakes domains. In this work, we investigate
the mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack
retrieval framework and integrate a Flip-style re-evaluation prompt to simulate
realistic answer-flipping scenarios. We find that retrieval heads are not
primarily responsible for avoiding uncertainty. Instead, we identify a small
set of non-retrieval attention heads that disproportionately attend to
misleading tokens in uncertain contexts. Masking these heads yields significant
improvements, reducing flip behavior by up to 15% without introducing
incoherence or overcorrection. However, when tested for downstream tasks, we
observe trade-offs with flip behavior. Our findings contribute to the growing
field of mechanistic interpretability and present a simple yet effective
technique for mitigating uncertainty-driven failure modes in LLMs.

</details>


### [70] [A Comprehensive Dataset for Human vs. AI Generated Text Detection](https://arxiv.org/abs/2510.22874)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Amit Sheth,Vasu Sharma,Aishwarya Naresh Reganti,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rapid advancement of large language models (LLMs) has led to increasingly
human-like AI-generated text, raising concerns about content authenticity,
misinformation, and trustworthiness. Addressing the challenge of reliably
detecting AI-generated text and attributing it to specific models requires
large-scale, diverse, and well-annotated datasets. In this work, we present a
comprehensive dataset comprising over 58,000 text samples that combine
authentic New York Times articles with synthetic versions generated by multiple
state-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B,
Yi-Large, and GPT-4-o. The dataset provides original article abstracts as
prompts, full human-authored narratives. We establish baseline results for two
key tasks: distinguishing human-written from AI-generated text, achieving an
accuracy of 58.35\%, and attributing AI texts to their generating models with
an accuracy of 8.92\%. By bridging real-world journalistic content with modern
generative models, the dataset aims to catalyze the development of robust
detection and attribution methods, fostering trust and transparency in the era
of generative AI. Our dataset is available at:
https://huggingface.co/datasets/gsingh1-py/train.

</details>


### [71] [Batch Speculative Decoding Done Right](https://arxiv.org/abs/2510.22876)
*Ranran Haoran Zhang,Soumik Dey,Ashirbad Mishra,Hansi Wu,Binbin Li,Rui Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的批量推测解码方法，解决了乱序张量问题，提高了推理效率并保持了输出等价性。


<details>
  <summary>Details</summary>
Motivation: 现有的批量实现存在输出等价性问题，这限制了推测解码在生产环境中的应用。因此，需要一种正确的批量推测解码方法。

Method: 本文通过分析现有批量实现中的问题，提出了EQSPEC和EXSPEC两种方法来解决乱序张量问题，确保正确性的同时提高推理效率。

Result: 在SpecBench数据集上，本文的方法在批量大小为8时相比批量大小为1的吞吐量提高了3倍，并且保持了95%的输出等价性。

Conclusion: 本文提出了一种正确性优先的批量推测解码方法EQSPEC和一种动态分组的EXSPEC方法，以减少重新对齐的开销并保持每个序列的推测加速。在SpecBench数据集上，该方法在批量大小为8时相比批量大小为1的吞吐量提高了3倍，并且保持了95%的输出等价性。

Abstract: Speculative decoding speeds up LLM inference by using a small draft model to
propose multiple tokens that a target model verifies in parallel. Extending
this idea to batches is essential for production serving, but it introduces the
ragged tensor problem: sequences in the same batch accept different numbers of
draft tokens, breaking right-alignment and corrupting position IDs, attention
masks, and KV-cache state. We show that several existing batch implementations
violate output equivalence-the fundamental requirement that speculative
decoding must produce identical token sequences to standard autoregressive
generation. These violations occur precisely due to improper handling of the
ragged tensor problem. In response, we (1) characterize the synchronization
requirements that guarantee correctness, (2) present a correctness-first batch
speculative decoding EQSPEC that exposes realignment as consuming 40% of
overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences
and dynamically forms same-length groups, to reduce the realignment overhead
while preserving per-sequence speculative speedups. On the SpecBench dataset,
across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our
approach achieves up to 3$\times$ throughput improvement at batch size 8
compared to batch size 1, with efficient scaling through batch size 8, while
maintaining 95% output equivalence. Our method requires no custom kernels and
integrates cleanly with existing inference stacks. Our code is available at
https://github.com/eBay/spec_dec.

</details>


### [72] [Language Server CLI Empowers Language Agents with Process Rewards](https://arxiv.org/abs/2510.22907)
*Yifan Zhang,Lanser Contributors*

Main category: cs.CL

TL;DR: Lanser-CLI 是一个 CLI 首选的协调层，它利用语言服务器协议提供结构信息和可操作的过程奖励。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常出现 API 虚构和编辑定位错误，而语言服务器可以计算真实代码的验证事实。

Method: Lanser-CLI 通过选择器 DSL、分析包、安全封装和过程奖励功能来实现语言服务器协议的协调。

Result: Lanser-CLI 提供了稳健的寻址方案、确定性分析包、安全封装和过程奖励功能，使语言服务器能够为编码代理和 CI 提供结构信息和可操作的过程奖励。

Conclusion: Lanser-CLI 提供了一种确定性、可重放的工作流，使语言服务器能够为编码代理和 CI 提供结构信息和可操作的过程奖励。

Abstract: Large language models routinely hallucinate APIs and mislocalize edits, while
language servers compute verified, IDE-grade facts about real code. We present
Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language
Server Protocol (LSP) server for coding agents and CI, exposing deterministic,
replayable workflows. Our position is that language servers provide not only
structural information (definitions, references, types, diagnostics) but also
an actionable process reward: machine-checked, step-wise signals that align an
agent's planning loop with program reality. In this work, Lanser-CLI
contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via
a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a
principled relocation algorithm; (ii) deterministic Analysis Bundles that
normalize Language Server responses and capture environment/capability metadata
with stable content hashes; (iii) a safety envelope for mutating operations
(rename, code actions) with preview, workspace jails, and Git-aware,
transactional apply; and (iv) a process-reward functional derived from Language
Server facts (diagnostic deltas, disambiguation confidence, and safe-apply
checks) that is computable online and replayable offline. We formalize
determinism under frozen snapshots and establish a monotonicity property for
the process reward, making it suitable for process supervision and
counterfactual analysis. Project Page:
https://github.com/yifanzhang-pro/lanser-cli

</details>


### [73] [Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://arxiv.org/abs/2510.22954)
*Liwei Jiang,Yuanjun Chai,Margaret Li,Mickel Liu,Raymond Fok,Nouha Dziri,Yulia Tsvetkov,Maarten Sap,Alon Albalak,Yejin Choi*

Main category: cs.CL

TL;DR: The paper introduces Infinity-Chat, a large-scale dataset for studying open-ended language model (LM) generation. It reveals the Artificial Hivemind effect, where LMs exhibit both intra-model repetition and inter-model homogeneity. The dataset includes extensive human annotations to analyze human preferences and highlights the need for better calibration of LMs and judges to human ratings.


<details>
  <summary>Details</summary>
Motivation: Language models (LMs) often struggle to generate diverse, human-like creative content, raising concerns about the long-term homogenization of human thought through repeated exposure to similar outputs. Scalable methods for evaluating LM output diversity remain limited, especially beyond narrow tasks or repeated sampling from a single model.

Method: We introduce Infinity-Chat, a large-scale dataset of 26K diverse, real-world, open-ended user queries that admit a wide range of plausible answers with no single ground truth. We also introduce the first comprehensive taxonomy for characterizing the full spectrum of open-ended prompts posed to LMs, comprising 6 top-level categories and 17 subcategories. Additionally, we present a large-scale study of mode collapse in LMs, analyzing intra-model repetition and inter-model homogeneity.

Result: Our findings show that LMs, reward models, and LM judges are less well calibrated to human ratings on model generations that elicit differing idiosyncratic annotator preferences, despite maintaining comparable overall quality. Infinity-Chat also includes 31,250 human annotations, enabling the study of collective and individual-specific human preferences in response to open-ended queries.

Conclusion: INFINITY-CHAT presents the first large-scale resource for systematically studying real-world open-ended queries to LMs, revealing critical insights to guide future research for mitigating long-term AI safety risks posed by the Artificial Hivemind.

Abstract: Language models (LMs) often struggle to generate diverse, human-like creative
content, raising concerns about the long-term homogenization of human thought
through repeated exposure to similar outputs. Yet scalable methods for
evaluating LM output diversity remain limited, especially beyond narrow tasks
such as random number or name generation, or beyond repeated sampling from a
single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse,
real-world, open-ended user queries that admit a wide range of plausible
answers with no single ground truth. We introduce the first comprehensive
taxonomy for characterizing the full spectrum of open-ended prompts posed to
LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that
further breaks down to 17 subcategories. Using Infinity-Chat, we present a
large-scale study of mode collapse in LMs, revealing a pronounced Artificial
Hivemind effect in open-ended generation of LMs, characterized by (1)
intra-model repetition, where a single model consistently generates similar
responses, and more so (2) inter-model homogeneity, where different models
produce strikingly similar outputs. Infinity-Chat also includes 31,250 human
annotations, across absolute ratings and pairwise preferences, with 25
independent human annotations per example. This enables studying collective and
individual-specific human preferences in response to open-ended queries. Our
findings show that LMs, reward models, and LM judges are less well calibrated
to human ratings on model generations that elicit differing idiosyncratic
annotator preferences, despite maintaining comparable overall quality. Overall,
INFINITY-CHAT presents the first large-scale resource for systematically
studying real-world open-ended queries to LMs, revealing critical insights to
guide future research for mitigating long-term AI safety risks posed by the
Artificial Hivemind.

</details>


### [74] [Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts](https://arxiv.org/abs/2510.22956)
*Anwesan Pal,Karen Hovsepian,Tinghao Guo,Mengnan Zhao,Somendra Tripathi,Nikos Kanakaris,George Mihaila,Sumit Nigam*

Main category: cs.CL

TL;DR: 本文提出了一种名为Tagging-Augmented Generation (TAG)的轻量级数据增强策略，用于提高大型语言模型在长上下文场景中的性能，无需改变检索文档的完整性。通过在问答提示中对上下文进行标记或添加标签定义，实验结果表明该方法在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如检索增强生成（RAG）和基于块的重新排序在处理长而复杂的上下文时存在局限性，它们对分块、嵌入和检索策略及模型敏感，并且依赖于大量的预处理、知识获取和索引步骤。因此，需要一种更有效的方法来解决这个问题。

Method: 本文提出了Tagging-Augmented Generation (TAG)方法，通过在问答提示中对上下文进行标记或添加标签定义，以提高大型语言模型在长上下文场景中的性能。

Result: 通过在两个具有挑战性和直接相关的问答基准NoLima和NovelQA上进行验证，结果表明，在上下文中进行标记或仅在问答提示中添加标签定义可以带来一致的性能提升，对于32K token的上下文，性能提升高达17%，对于需要跨广泛文本知识的多跳查询的复杂推理问答任务，性能提升为2.9%。

Conclusion: 本文提出了一种轻量级的数据增强策略Tagging-Augmented Generation (TAG)，可以在不破坏检索文档的完整性和组成的情况下，提升大型语言模型在长上下文场景中的性能。

Abstract: Recent investigations into effective context lengths of modern flagship large
language models (LLMs) have revealed major limitations in effective question
answering (QA) and reasoning over long and complex contexts for even the
largest and most impressive cadre of models. While approaches like
retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to
mitigate this issue, they are sensitive to chunking, embedding and retrieval
strategies and models, and furthermore, rely on extensive pre-processing,
knowledge acquisition and indexing steps. In this paper, we propose
Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy
that boosts LLM performance in long-context scenarios, without degrading and
altering the integrity and composition of retrieved documents. We validate our
hypothesis by augmenting two challenging and directly relevant
question-answering benchmarks -- NoLima and NovelQA -- and show that tagging
the context or even just adding tag definitions into QA prompts leads to
consistent performance gains over the baseline -- up to 17% for 32K token
contexts, and 2.9% in complex reasoning question-answering for multi-hop
queries requiring knowledge across a wide span of text. Additional details are
available at https://sites.google.com/view/tag-emnlp.

</details>


### [75] [MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs](https://arxiv.org/abs/2510.22967)
*Yucheng Ning,Xixun Lin,Fang Fang,Yanan Cao*

Main category: cs.CL

TL;DR: 本文提出了一种系统的方法，用于评估和提高大型语言模型在长文本输出中的事实可靠性，包括构建一个中文长文本事实数据集和一个基于辩论的多代理验证系统，并展示了实验结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的广泛采用引发了对其输出事实准确性的关注，特别是在生物医学、法律和教育等高风险领域。现有的短文本评估方法在长文本内容上往往失效，因为复杂的推理链条、交织的观点和累积的信息。

Method: 我们提出了一个系统的方法，结合大规模长文本数据集、多代理验证机制和加权评估指标。我们构建了LongHalluQA，一个中文长文本事实数据集；并开发了MAD-Fact，一个基于辩论的多代理验证系统。我们引入了一个事实重要性层次结构来捕捉长文本中声明的不同重要性。

Result: 在两个基准测试中进行的实验表明，更大的LLMs通常保持更高的事实一致性，而国内模型在中文内容上表现优异。

Conclusion: 我们的工作为评估和提高长文本LLM输出的事实可靠性提供了结构化的框架，指导其在敏感领域的安全部署。

Abstract: The widespread adoption of Large Language Models (LLMs) raises critical
concerns about the factual accuracy of their outputs, especially in high-risk
domains such as biomedicine, law, and education. Existing evaluation methods
for short texts often fail on long-form content due to complex reasoning
chains, intertwined perspectives, and cumulative information. To address this,
we propose a systematic approach integrating large-scale long-form datasets,
multi-agent verification mechanisms, and weighted evaluation metrics. We
construct LongHalluQA, a Chinese long-form factuality dataset; and develop
MAD-Fact, a debate-based multi-agent verification system. We introduce a fact
importance hierarchy to capture the varying significance of claims in long-form
texts. Experiments on two benchmarks show that larger LLMs generally maintain
higher factual consistency, while domestic models excel on Chinese content. Our
work provides a structured framework for evaluating and enhancing factual
reliability in long-form LLM outputs, guiding their safe deployment in
sensitive domains.

</details>


### [76] [Measuring Teaching with LLMs](https://arxiv.org/abs/2510.22968)
*Michael Hardy*

Main category: cs.CL

TL;DR: 本文提出了一种基于句子级嵌入的定制大型语言模型，用于测量教学质量。结果表明，这些模型可以达到人类水平甚至超人类的表现，并且能够捕捉到与学生学习相关的特征。


<details>
  <summary>Details</summary>
Motivation: 客观且可扩展地测量教学质量是教育中的一个持续挑战。虽然大型语言模型（LLMs）有潜力，但通用模型在可靠应用复杂的、真实的课堂观察工具方面一直存在问题。

Method: 本文使用基于句子级嵌入的定制大型语言模型，这种架构更适合课堂转录文本的长篇、解释性性质，而不是传统的子词分词。

Result: 这些专业模型可以达到人类水平甚至超人类的表现，专家人类评分高于0.65，并超过了平均人类-人类评分者相关性。此外，通过分析注释上下文窗口，我们发现更先进的模型——那些与人类判断更一致的模型——将更多的分数变化归因于课程级别的特征，而不是孤立的言语，这挑战了单次注释范式的充分性。最后，为了评估外部有效性，我们发现聚合模型分数与教师价值增加措施一致，表明它们捕捉到了与学生学习相关的特征。然而，这一趋势在个别项目层面并不成立，表明尽管模型学习到了有用的信号，但尚未实现完全泛化。

Conclusion: 本文建立了一种可行且强大的新方法，用于人工智能驱动的教学测量，为提供可扩展、可靠和有效的教育者发展反馈提供了路径。

Abstract: Objective and scalable measurement of teaching quality is a persistent
challenge in education. While Large Language Models (LLMs) offer potential,
general-purpose models have struggled to reliably apply complex, authentic
classroom observation instruments. This paper uses custom LLMs built on
sentence-level embeddings, an architecture better suited for the long-form,
interpretive nature of classroom transcripts than conventional subword
tokenization. We systematically evaluate five different sentence embeddings
under a data-efficient training regime designed to prevent overfitting. Our
results demonstrate that these specialized models can achieve human-level and
even super-human performance with expert human ratings above 0.65 and
surpassing the average human-human rater correlation. Further, through analysis
of annotation context windows, we find that more advanced models-those better
aligned with human judgments-attribute a larger share of score variation to
lesson-level features rather than isolated utterances, challenging the
sufficiency of single-turn annotation paradigms. Finally, to assess external
validity, we find that aggregate model scores align with teacher value-added
measures, indicating they are capturing features relevant to student learning.
However, this trend does not hold at the individual item level, suggesting that
while the models learn useful signals, they have not yet achieved full
generalization. This work establishes a viable and powerful new methodology for
AI-driven instructional measurement, offering a path toward providing scalable,
reliable, and valid feedback for educator development.

</details>


### [77] [Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures](https://arxiv.org/abs/2510.23006)
*Shenran Wang,Timothy Tin-Long Tse,Jian Zhu*

Main category: cs.CL

TL;DR: 研究不同架构的LLM在知识基础的ICL任务中的表现和内部机制，发现FVs主要位于自注意力和Mamba层，推测Mamba2可能使用不同的机制进行ICL。


<details>
  <summary>Details</summary>
Motivation: 为了深入了解不同架构的LLM在知识基础的ICL任务中的表现和内部机制。

Method: 使用行为探测和基于干预的方法进行深入评估。

Result: 发现FVs主要位于自注意力和Mamba层，推测Mamba2可能使用不同的机制进行ICL。FVs在涉及参数化知识检索的ICL中更为重要。

Conclusion: 我们的工作有助于对不同架构和任务类型的LLM能力有更细致的理解。方法上，我们的方法强调了结合行为分析和机制分析的重要性。

Abstract: We perform in-depth evaluations of in-context learning (ICL) on
state-of-the-art transformer, state-space, and hybrid large language models
over two categories of knowledge-based ICL tasks. Using a combination of
behavioral probing and intervention-based methods, we have discovered that,
while LLMs of different architectures can behave similarly in task performance,
their internals could remain different. We discover that function vectors (FVs)
responsible for ICL are primarily located in the self-attention and Mamba
layers, and speculate that Mamba2 uses a different mechanism from FVs to
perform ICL. FVs are more important for ICL involving parametric knowledge
retrieval, but not for contextual knowledge understanding. Our work contributes
to a more nuanced understanding across architectures and task types.
Methodologically, our approach also highlights the importance of combining both
behavioural and mechanistic analyses to investigate LLM capabilities.

</details>


### [78] [LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models](https://arxiv.org/abs/2510.23011)
*Sammriddh Gupta,Sonit Singh,Aditya Joshi,Mira Kim*

Main category: cs.CL

TL;DR: 本文介绍了LangLingual，一个基于LangChain框架和大型语言模型构建的对话代理，旨在提供实时语法反馈、生成上下文感知的语言练习，并跟踪学习者的熟练程度。研究显示该系统具有良好的可用性、积极的学习成果和令人鼓舞的学习者参与度。


<details>
  <summary>Details</summary>
Motivation: 语言教育者希望为学习者创造丰富的体验，但他们可能在提供的反馈和练习范围上受到限制。

Method: LangLingual是使用LangChain框架并由大型语言模型驱动的对话代理，旨在提供实时语法反馈、生成上下文感知的语言练习，并跟踪学习者的熟练程度。

Result: 结果表明LangLingual具有良好的可用性、积极的学习成果和令人鼓舞的学习者参与度。

Conclusion: LangLingual展示了其在语言教育中的有效性，具有良好的可用性、积极的学习成果和令人鼓舞的学习者参与度。

Abstract: Language educators strive to create a rich experience for learners, while
they may be restricted in the extend of feedback and practice they can provide.
We present the design and development of LangLingual, a conversational agent
built using the LangChain framework and powered by Large Language Models. The
system is specifically designed to provide real-time, grammar-focused feedback,
generate context-aware language exercises and track learner proficiency over
time. The paper discusses the architecture, implementation and evaluation of
LangLingual in detail. The results indicate strong usability, positive learning
outcomes and encouraging learner engagement.

</details>


### [79] [Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2510.23038)
*Ran Xu,Jingjing Chen,Jiayu Ye,Yu Wu,Jun Yan,Carl Yang,Hongkun Yu*

Main category: cs.CL

TL;DR: TIR-Judge is an end-to-end RL framework that integrates a code executor for precise evaluation, surpassing strong reasoning-based judges and demonstrating self-evolution through iterative reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Most LLM judges operate solely on intrinsic text-based reasoning, limiting their ability to verify complex constraints or perform accurate computation. The success of tool-integrated reasoning (TIR) in numerous tasks motivates the development of TIR-Judge.

Method: TIR-Judge is an end-to-end RL framework that integrates a code executor for precise evaluation, built on three principles: diverse training across verifiable and non-verifiable domains, flexible judgment formats, and iterative RL that bootstraps directly from the initial model without distillation.

Result: TIR-Judge surpasses strong reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and achieves listwise performance comparable to Claude-Opus-4 despite having only 8B parameters. TIR-Judge-Zero matches the performance of distilled variants, demonstrating self-evolution through iterative reinforcement learning.

Conclusion: TIR-Judge-Zero demonstrates that tool-augmented judges can self-evolve through iterative reinforcement learning without the need for distilled judge trajectories.

Abstract: Large Language Models (LLMs) are widely used as judges to evaluate response
quality, providing a scalable alternative to human evaluation. However, most
LLM judges operate solely on intrinsic text-based reasoning, limiting their
ability to verify complex constraints or perform accurate computation.
Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks,
we propose TIR-Judge, an end-to-end RL framework for training LLM judges that
integrates a code executor for precise evaluation. TIR-Judge is built on three
principles: (i) diverse training across verifiable and non-verifiable domains,
(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)
iterative RL that bootstraps directly from the initial model without
distillation. On seven public benchmarks, TIR-Judge surpasses strong
reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and
achieves listwise performance comparable to Claude-Opus-4 despite having only
8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled
judge trajectories, matches the performance of distilled variants,
demonstrating that tool-augmented judges can self-evolve through iterative
reinforcement learning.

</details>


### [80] [Knocking-Heads Attention](https://arxiv.org/abs/2510.23052)
*Zhanchao Zhou,Xiaodong Chen,Haoxing Chen,Zhenzhong Lan,Jianguo Li*

Main category: cs.CL

TL;DR: 本文提出了敲头注意力（KHA），通过在缩放点积注意力之前促进跨头特征级交互，解决了现有注意力机制的局限性。KHA添加了少量参数和FLOPs，并在多个注意力变体中实现了无缝集成。


<details>
  <summary>Details</summary>
Motivation: Existing attention mechanisms simply concatenate outputs from isolated heads without strong interaction. Increasing the number of heads inherently weakens individual head capacity.

Method: We propose knocking-heads attention (KHA), which enables attention heads to 'knock' on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads.

Result: KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens.

Conclusion: KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks.

Abstract: Multi-head attention (MHA) has become the cornerstone of modern large
language models, enhancing representational capacity through parallel attention
heads. However, increasing the number of heads inherently weakens individual
head capacity, and existing attention mechanisms - whether standard MHA or its
variants like grouped-query attention (GQA) and grouped-tied attention (GTA) -
simply concatenate outputs from isolated heads without strong interaction. To
address this limitation, we propose knocking-heads attention (KHA), which
enables attention heads to "knock" on each other - facilitating cross-head
feature-level interactions before the scaled dot-product attention. This is
achieved by applying a shared, diagonally-initialized projection matrix across
all heads. The diagonal initialization preserves head-specific specialization
at the start of training while allowing the model to progressively learn
integrated cross-head representations. KHA adds only minimal parameters and
FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention
variants. We validate KHA by training a 6.1B parameter MoE model (1.01B
activated) on 1T high-quality tokens. Compared to baseline attention
mechanisms, KHA brings superior and more stable training dynamics, achieving
better performance across downstream tasks.

</details>


### [81] [Quality-Aware Translation Tagging in Multilingual RAG system](https://arxiv.org/abs/2510.23070)
*Hoyeon Moon,Byeolhee Kim,Nikhil Verma*

Main category: cs.CL

TL;DR: QTT-RAG通过评估翻译质量并将其作为元数据附加，提高了多语言检索增强生成的效果，尤其在低资源语言环境下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要么假设翻译质量足够好，要么使用重写方法，这会引入事实错误和幻觉。为了缓解这些问题，我们提出了QTT-RAG。

Method: QTT-RAG通过在三个维度上评估翻译质量——语义等价性、语法准确性和自然性与流畅性，并将这些评分作为元数据附加，而不改变原始内容。

Result: QTT-RAG在两个开放域问答基准（XORQA, MKQA）中对CrossRAG和DKM-RAG进行了评估，使用了六种指令调优的大规模语言模型，覆盖了两种低资源语言（韩语和芬兰语）和一种高资源语言（中文），结果优于基线。

Conclusion: QTT-RAG在保留事实完整性的同时，使生成模型能够根据翻译可靠性做出明智的决策，从而在低资源设置中有效利用跨语言文档，提供了一个实用且稳健的解决方案。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English
documents and translates them into the query language for low-resource
settings. However, poor translation quality degrades response generation
performance. Existing approaches either assume sufficient translation quality
or utilize the rewriting method, which introduces factual distortion and
hallucinations. To mitigate these problems, we propose Quality-Aware
Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation
quality along three dimensions-semantic equivalence, grammatical accuracy, and
naturalness&fluency-and attach these scores as metadata without altering the
original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines
in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs
ranging from 2.4B to 14B parameters, covering two low-resource languages
(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG
outperforms the baselines by preserving factual integrity while enabling
generator models to make informed decisions based on translation reliability.
This approach allows for effective usage of cross-lingual documents in
low-resource settings with limited native language documents, offering a
practical and robust solution across multilingual domains.

</details>


### [82] [A Survey on LLM Mid-training](https://arxiv.org/abs/2510.23081)
*Chengying Tu,Xuemiao Zhang,Rongxiang Weng,Rumei Li,Chen Zhang,Yang Bai,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: 本文对大语言模型的中训练进行了调查，提供了正式定义、优化框架和分类法，以支持未来的研究和创新。


<details>
  <summary>Details</summary>
Motivation: 本文旨在调查中训练在大语言模型能力渐进发展中的独特作用，并提供分类法和见解以支持未来的研究和创新。

Method: 本文提供了大语言模型（LLMs）的中训练的正式定义，并研究了涵盖数据整理、训练策略和模型架构优化的优化框架。

Result: 本文分析了主流模型实现，在目标驱动干预的背景下，展示了中训练作为大语言模型能力发展中的一个独立且关键阶段的作用。

Conclusion: 本文通过明确中训练的独特贡献，提供了全面的分类法和可行的见解，支持未来在推进大语言模型方面的研究和创新。

Abstract: Recent advances in foundation models have highlighted the significant
benefits of multi-stage training, with a particular emphasis on the emergence
of mid-training as a vital stage that bridges pre-training and post-training.
Mid-training is distinguished by its use of intermediate data and computational
resources, systematically enhancing specified capabilities such as mathematics,
coding, reasoning, and long-context extension, while maintaining foundational
competencies. This survey provides a formal definition of mid-training for
large language models (LLMs) and investigates optimization frameworks that
encompass data curation, training strategies, and model architecture
optimization. We analyze mainstream model implementations in the context of
objective-driven interventions, illustrating how mid-training serves as a
distinct and critical stage in the progressive development of LLM capabilities.
By clarifying the unique contributions of mid-training, this survey offers a
comprehensive taxonomy and actionable insights, supporting future research and
innovation in the advancement of LLMs.

</details>


### [83] [MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models](https://arxiv.org/abs/2510.23090)
*Suchan Lee,Jihoon Choi,Sohyeon Lee,Minseok Song,Bong-Gyu Jang,Hwanjo Yu,Soyeon Caren Han*

Main category: cs.CL

TL;DR: MAP4TS is a new framework that improves time-series forecasting by incorporating classical statistical methods into prompts, leading to better performance than existing LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal approaches often overlook the distinct statistical properties and temporal dependencies of time-series data, so the authors propose a framework that explicitly integrates these aspects into the prompt design.

Method: MAP4TS is a novel Multi-Aspect Prompting Framework that incorporates classical time-series analysis into the prompt design, using four specialized prompt components combined with raw time-series embeddings and processed by an LLM for forecasting.

Result: MAP4TS outperforms state-of-the-art LLM-based methods in time-series forecasting, and GPT-2 backbones paired with structured prompts perform better than larger models like LLaMA in long-term forecasting tasks.

Conclusion: MAP4TS consistently outperforms state-of-the-art LLM-based methods across eight diverse datasets, and prompt-aware designs significantly enhance performance stability.

Abstract: Recent advances have investigated the use of pretrained large language models
(LLMs) for time-series forecasting by aligning numerical inputs with LLM
embedding spaces. However, existing multimodal approaches often overlook the
distinct statistical properties and temporal dependencies that are fundamental
to time-series data. To bridge this gap, we propose MAP4TS, a novel
Multi-Aspect Prompting Framework that explicitly incorporates classical
time-series analysis into the prompt design. Our framework introduces four
specialized prompt components: a Global Domain Prompt that conveys
dataset-level context, a Local Domain Prompt that encodes recent trends and
series-specific behaviors, and a pair of Statistical and Temporal Prompts that
embed handcrafted insights derived from autocorrelation (ACF), partial
autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined
with raw time-series embeddings and passed through a cross-modality alignment
module to produce unified representations, which are then processed by an LLM
and projected for final forecasting. Extensive experiments across eight diverse
datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based
methods. Our ablation studies further reveal that prompt-aware designs
significantly enhance performance stability and that GPT-2 backbones, when
paired with structured prompts, outperform larger models like LLaMA in
long-term forecasting tasks.

</details>


### [84] [Leveraging Hierarchical Organization for Medical Multi-document Summarization](https://arxiv.org/abs/2510.23104)
*Yi-Li Hsu,Katelyn X. Mei,Lucy Lu Wang*

Main category: cs.CL

TL;DR: 本文研究了在医学多文档摘要中引入层次结构是否能提高模型生成摘要的质量，结果显示层次结构方法能提高摘要的清晰度和人类偏好，同时保持内容覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 医学多文档摘要（MDS）是一项复杂的任务，需要有效管理跨文档关系。本文旨在探讨在输入中引入层次结构是否能改善模型在跨文档信息组织和上下文理解方面的能力。

Method: 本文研究了在医学多文档摘要（MDS）中引入层次结构是否能提高模型组织和上下文信息的能力，相比传统的平面摘要方法。我们研究了两种在三个大型语言模型（LLMs）中引入层次结构的方法，并使用自动指标、基于模型的指标和领域专家评估对生成的摘要进行了全面评估。

Result: 结果表明，人类专家更喜欢模型生成的摘要而不是人工编写的摘要。层次结构方法通常保持信息的真实性、覆盖范围和连贯性，同时增加对摘要的人类偏好。此外，我们还检查了GPT-4的模拟判断是否与人类判断一致，发现更客观的评估方面有更高的一致性。

Conclusion: 本文表明，层次结构可以提高模型生成的医学摘要的清晰度，同时保持内容覆盖范围，为提高生成摘要的人类偏好提供了一种实用方法。

Abstract: Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.

</details>


### [85] [Flexing in 73 Languages: A Single Small Model for Multilingual Inflection](https://arxiv.org/abs/2510.23114)
*Tomáš Sourada,Jana Straková*

Main category: cs.CL

TL;DR: 本文提出了一种多语言屈折的紧凑单模型方法，在73种语言的数据上进行训练，表现出色，能够处理未见过的单词，并公开了所有代码。


<details>
  <summary>Details</summary>
Motivation: 多语言建模在屈折中的有效性得到了证明，并突显了其实际好处：通过消除管理并重新训练数十个单独的单语模型的需要来简化部署。

Method: 我们提出了一种紧凑的单模型方法来进行多语言屈折，该模型在73种语言的数据上进行了联合训练。我们引入了一种新的频率加权、词根不相交的训练-开发-测试重采样过程，以确保现实的数据划分。

Result: 我们的模型是轻量级的，对未见过的单词具有鲁棒性，并且在大多数语言中优于单语基线。除了标准的SIGMORPHON共享任务基准外，我们还在73个通用依赖（UD）树库上评估了我们的单语和多语言模型。

Conclusion: 我们的工作解决了缺乏一个开源、通用的多语言形态屈折系统的问题，该系统能够处理广泛语言中的未见过的单词，包括捷克语。

Abstract: We present a compact, single-model approach to multilingual inflection, the
task of generating inflected word forms from base lemmas to express grammatical
categories. Our model, trained jointly on data from 73 languages, is
lightweight, robust to unseen words, and outperforms monolingual baselines in
most languages. This demonstrates the effectiveness of multilingual modeling
for inflection and highlights its practical benefits: simplifying deployment by
eliminating the need to manage and retrain dozens of separate monolingual
models. In addition to the standard SIGMORPHON shared task benchmarks, we
evaluate our monolingual and multilingual models on 73 Universal Dependencies
(UD) treebanks, extracting lemma-tag-form triples and their frequency counts.
To ensure realistic data splits, we introduce a novel frequency-weighted,
lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack
of an open-source, general-purpose, multilingual morphological inflection
system capable of handling unseen words across a wide range of languages,
including Czech. All code is publicly released at:
https://github.com/tomsouri/multilingual-inflection.

</details>


### [86] [Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation](https://arxiv.org/abs/2510.23123)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). LoRA essentially describes the
projection of an input space into a low-dimensional output space, with the
dimensionality determined by the LoRA rank. In standard LoRA, all input tokens
share the same weights and undergo an identical input-output projection. This
limits LoRA's ability to capture token-specific information due to the inherent
semantic differences among tokens. To address this limitation, we propose
Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts
LoRA weights according to the input token, thereby learning token-wise
input-output projections in an end-to-end manner. Formally, the weights of
TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank
matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated
from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA
weights but achieves more granular adaptation by learning token-wise LoRA
weights (i.e., token-wise input-output projections). Extensive experiments
across multiple models and datasets demonstrate that TopLoRA consistently
outperforms LoRA and its variants. The code is available at
https://github.com/Leopold1423/toplora-neurips25.

</details>


### [87] [Corpus Frequencies in Morphological Inflection: Do They Matter?](https://arxiv.org/abs/2510.23131)
*Tomáš Sourada,Jana Straková*

Main category: cs.CL

TL;DR: 本文探讨了在形态学屈折任务中引入语料库频率信息的三个关键维度，并展示了频率感知训练在多个语言中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法忽略了词频分布信息，而实际应用中用户输入应反映自然文本中的频率分布。

Method: 探索将语料库频率信息纳入形态学屈折任务的三个关键维度：(i) 训练-开发-测试集划分；(ii) 评估；(iii) 训练数据采样。

Result: 频率感知训练在26种语言中优于均匀采样。

Conclusion: 频率感知训练在43种语言中的26种中表现优于均匀采样。

Abstract: The traditional approach to morphological inflection (the task of modifying a
base word (lemma) to express grammatical categories) has been, for decades, to
consider lexical entries of lemma-tag-form triples uniformly, lacking any
information about their frequency distribution. However, in production
deployment, one might expect the user inputs to reflect a real-world
distribution of frequencies in natural texts. With future deployment in mind,
we explore the incorporation of corpus frequency information into the task of
morphological inflection along three key dimensions during system development:
(i) for train-dev-test split, we combine a lemma-disjoint approach, which
evaluates the model's generalization capabilities, with a frequency-weighted
strategy to better reflect the realistic distribution of items across different
frequency bands in training and test sets; (ii) for evaluation, we complement
the standard type accuracy (often referred to simply as accuracy), which treats
all items equally regardless of frequency, with token accuracy, which assigns
greater weight to frequent words and better approximates performance on running
text; (iii) for training data sampling, we introduce a method novel in the
context of inflection, frequency-aware training, which explicitly incorporates
word frequency into the sampling process. We show that frequency-aware training
outperforms uniform sampling in 26 out of 43 languages.

</details>


### [88] [ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix](https://arxiv.org/abs/2510.23160)
*Zile Yang,Ling Li,Na Di,Jinlong Pang,Yao Zhou,Hao Cheng,Bo Han,Jiaheng Wei*

Main category: cs.CL

TL;DR: 本文提出了ENTP框架，通过符号净化和神经重建来增强低质量数据，实验表明这种方法在多个基准测试中表现优异，甚至超越了使用完整数据集微调的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的质量优先范式常常忽视被丢弃的低质量数据中的有价值信号，并依赖于不完美的质量过滤器。

Method: ENTP（通过神经符号文本清理-混合增强低质量SFT数据）框架，该框架通过符号净化和神经重建重新利用低质量语料库。符号模块基于统计先验识别和修剪噪声样本，而神经组件则通过利用潜在表示和模型知识合成丰富的指令-响应对。这种神经符号协同作用提高了数据的信息量和多样性。

Result: ENTP增强的数据集（仅由低质量数据构建）在五个指令遵循基准测试中优于13个已建立的数据选择基线，并甚至超越了在完整原始数据集（约300K个示例）上的微调。

Conclusion: 我们的结果突显了低质量数据的未开发潜力，并强调了智能净化和合成在高效指令对齐中的重要性。

Abstract: Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs)
to domain-specific instructions by training on a carefully curated subset of
high-quality instruction-response pairs, typically drawn from a larger dataset
that often contains many low-quality or noisy samples. However, existing
quality-first paradigms often overlook valuable signals in discarded
low-quality data and rely on imperfect quality filters. We introduce ENTP
(Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a
framework that revitalizes low-quality corpora through symbolic purification
and neural reconstruction. The symbolic module identifies and prunes noisy
samples based on statistical priors, while the neural component synthesizes
enriched instruction-response pairs by leveraging latent representations and
model knowledge. This neural-symbolic synergy enhances data informativeness and
diversity. Experiments show that ENTP-augmented datasets, constructed
exclusively from low-quality data, outperform 13 established data-selection
baselines across five instruction-following benchmarks, and even surpass
fine-tuning on the full original dataset (approximately 300K examples). Our
results highlight the untapped potential of low-quality data and underscore the
importance of intelligent purification and synthesis for efficient instruction
alignment.

</details>


### [89] [Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs](https://arxiv.org/abs/2510.23163)
*Hang Lei,Shengyi Zong,Zhaoyan Li,Ziren Zhou,Hao Liu*

Main category: cs.CL

TL;DR: 本文提出了一种分解框架DSR，用于生成高质量的剧本，通过将创意叙事生成与格式转换分开，提高了LLM在复杂创造性领域的能力。


<details>
  <summary>Details</summary>
Motivation: 我们认为，这种失败源于强迫单一模型同时掌握两种不同的能力：创意叙事构建和严格的格式遵循。

Method: 我们引入了双阶段精炼（DSR），这是一个分解框架，将创意叙事生成与格式转换分开。第一阶段将简要大纲转换为丰富的小说风格散文。第二阶段将此叙事精炼成专业格式的剧本。

Result: 盲评显示，DSR在对抗强大的基线如Gemini-2.5-Pro时取得了75%的胜率，并达到了人类水平性能的82.7%。

Conclusion: 我们的工作表明，具有定制数据合成的分解生成架构可以有效地使LLM在复杂的创造性领域中专业化。

Abstract: The screenplay serves as the foundation for television production, defining
narrative structure, character development, and dialogue. While Large Language
Models (LLMs) show great potential in creative writing, direct end-to-end
generation approaches often fail to produce well-crafted screenplays. We argue
this failure stems from forcing a single model to simultaneously master two
disparate capabilities: creative narrative construction and rigid format
adherence. The resulting outputs may mimic superficial style but lack the deep
structural integrity and storytelling substance required for professional use.
To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage
Refinement (DSR), a decomposed framework that decouples creative narrative
generation from format conversion. The first stage transforms a brief outline
into rich, novel-style prose. The second stage refines this narrative into a
professionally formatted screenplay. This separation enables the model to
specialize in one distinct capability at each stage. A key challenge in
implementing DSR is the scarcity of paired outline-to-novel training data. We
address this through hybrid data synthesis: reverse synthesis deconstructs
existing screenplays into structured inputs, while forward synthesis leverages
these inputs to generate high-quality narrative texts as training targets.
Blind evaluations by professional screenwriters show that DSR achieves a 75%
win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of
human-level performance. Our work demonstrates that decomposed generation
architecture with tailored data synthesis effectively specializes LLMs in
complex creative domains.

</details>


### [90] [MATCH: Task-Driven Code Evaluation through Contrastive Learning](https://arxiv.org/abs/2510.23169)
*Marah Ghoummaid,Vladimir Tchuiev,Ofek Glick,Michal Moschkovitz,Dotan Di Castro*

Main category: cs.CL

TL;DR: 本文介绍了一种名为MATCH的新颖的无参考评估指标，它利用对比学习为代码和自然语言任务描述生成有意义的嵌入，从而实现反映生成代码如何实现任务的相似性评分。MATCH在多个编程语言中与功能正确性和人类偏好具有更强的相关性。


<details>
  <summary>Details</summary>
Motivation: Accurately evaluating how well generated code aligns with developer intent remains a critical challenge. Traditional evaluation methods are often unscalable and costly. Syntactic similarity metrics fail to capture code functionality, and metrics like CodeBERTScore require reference code, which is not always available.

Method: MATCH uses Contrastive Learning to generate meaningful embeddings for code and natural language task descriptions, enabling similarity scoring that reflects how well generated code implements the task.

Result: MATCH achieves stronger correlations with functional correctness and human preference than existing metrics across multiple programming languages.

Conclusion: MATCH achieves stronger correlations with functional correctness and human preference than existing metrics across multiple programming languages.

Abstract: AI-based code generation is increasingly prevalent, with GitHub Copilot
estimated to generate 46% of the code on GitHub. Accurately evaluating how well
generated code aligns with developer intent remains a critical challenge.
Traditional evaluation methods, such as unit tests, are often unscalable and
costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code
functionality, and metrics like CodeBERTScore require reference code, which is
not always available. To address the gap in reference-free evaluation, with few
alternatives such as ICE-Score, this paper introduces MATCH, a novel
reference-free metric. MATCH uses Contrastive Learning to generate meaningful
embeddings for code and natural language task descriptions, enabling similarity
scoring that reflects how well generated code implements the task. We show that
MATCH achieves stronger correlations with functional correctness and human
preference than existing metrics across multiple programming languages.

</details>


### [91] [SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations](https://arxiv.org/abs/2510.23182)
*Shuai Huang,Wenxuan Zhao,Jun Gao*

Main category: cs.CL

TL;DR: 本文介绍了SI-Bench，一个用于评估大型语言模型社会智能的新基准。实验显示，尽管最先进的模型在过程推理方面超过人类，但在回复质量上仍不及人类。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在现实和复杂社会互动中的表现仍然是一个重大挑战。大多数之前的研究通过模拟代理之间的互动构建数据集，这无法捕捉真实人类对话中的真实语言风格和关系动态。

Method: 我们引入了SI-Bench，这是一个新的基准，旨在评估LLMs的社会智能方面。基于广泛的社会科学理论，SI-Bench包含从社交网络应用中收集的2,221个真实多轮对话。我们进一步选择了312个对话进行跨8个主要模型的手动标注。

Result: 实验结果显示，最先进的模型在复杂社会情境下的过程推理方面已经超过了人类专家，但在回复质量方面仍落后于人类。此外，引入思维链（CoT）推理可能会降低LLMs在社交对话任务中的性能。

Conclusion: 实验结果显示，最先进的模型在复杂社会情境下的过程推理方面已经超过了人类专家，但在回复质量方面仍落后于人类。此外，引入思维链（CoT）推理可能会降低LLMs在社交对话任务中的性能。所有数据集均可在https://github.com/SI-Bench/SI-Bench.git上公开获取。

Abstract: As large language models (LLMs) develop anthropomorphic abilities, they are
increasingly being deployed as autonomous agents to interact with humans.
However, evaluating their performance in realistic and complex social
interactions remains a significant challenge. Most previous research built
datasets through simulated agent-to-agent interactions, which fails to capture
the authentic linguistic styles and relational dynamics found in real human
conversations. To address this gap, we introduce SI-Bench, a novel benchmark
designed to evaluate aspects of social intelligence in LLMs. Grounded in broad
social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues
collected from a social networking application. We further selected a subset of
312 dialogues for manual annotation across 8 major models. The experiments show
that SOTA models have surpassed the human expert in process reasoning under
complex social situations, yet they still fall behind humans in reply quality.
Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the
performance of LLMs in social dialogue tasks. All datasets are openly available
at https://github.com/SI-Bench/SI-Bench.git.

</details>


### [92] [DREaM: Drug-Drug Relation Extraction via Transfer Learning Method](https://arxiv.org/abs/2510.23189)
*Ali Fata,Hossein Rahmani,Parinaz Soltanzadeh,Amirhossein Derakhshan,Behrouz Minaei Bidgoli*

Main category: cs.CL

TL;DR: 本研究提出了DREAM方法，利用关系抽取模型和大型语言模型来构建药物关系的本体，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门用于药物关系抽取的数据集，因此需要采用迁移学习来应用机器学习方法。

Method: DREAM方法首先使用训练好的关系抽取模型来发现实体之间的关系，然后将该模型应用于医学文本语料库以构建药物关系的本体。随后使用大型语言模型验证提取的关系。

Result: 定量结果显示，大型语言模型与从PubMed摘要子集提取的关系有71%的一致性。

Conclusion: 本研究提出了一种名为DREAM的方法，通过使用训练好的关系抽取模型来发现实体之间的关系，并将其应用于医学文本语料库以构建药物关系的本体。结果表明，大型语言模型与从PubMed摘要子集提取的关系有71%的一致性。此外，定性分析表明，该方法可以揭示医学领域的模糊性，突显了该领域关系抽取固有的挑战。

Abstract: Relation extraction between drugs plays a crucial role in identifying drug
drug interactions and predicting side effects. The advancement of machine
learning methods in relation extraction, along with the development of large
medical text databases, has enabled the low cost extraction of such relations
compared to other approaches that typically require expert knowledge. However,
to the best of our knowledge, there are limited datasets specifically designed
for drug drug relation extraction currently available. Therefore, employing
transfer learning becomes necessary to apply machine learning methods in this
domain. In this study, we propose DREAM, a method that first employs a trained
relation extraction model to discover relations between entities and then
applies this model to a corpus of medical texts to construct an ontology of
drug relationships. The extracted relations are subsequently validated using a
large language model. Quantitative results indicate that the LLM agreed with 71
of the relations extracted from a subset of PubMed abstracts. Furthermore, our
qualitative analysis indicates that this approach can uncover ambiguities in
the medical domain, highlighting the challenges inherent in relation extraction
in this field.

</details>


### [93] [Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports](https://arxiv.org/abs/2510.23217)
*Alois Thomas,Maya Varma,Jean-Benoit Delbrouck,Curtis P. Langlotz*

Main category: cs.CL

TL;DR: 本文提出了一种新的句子级过程奖励模型（PRM），用于检测大型视觉语言模型（LVLM）生成的临床幻觉。PRM在MIMIC-CXR数据集上微调后表现出色，能够有效过滤低质量报告并提高临床指标。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法经常缺乏必要的句子级粒度或在不同LVLM生成器上的鲁棒泛化能力。

Method: 本文提出了一种新的方法：针对视觉语言任务的句子级过程奖励模型（PRM）。该PRM预测每个生成句子的事实正确性，基于临床背景和前面的文本。

Result: 在MIMIC-CXR上微调的轻量级0.5B参数PRM优于现有的验证技术，例如在某个LVLM的输出上，马修斯相关系数相对提高了7.5%，AUROC提高了1.8%。此外，PRM得分有效过滤了低质量报告，提高了F1-CheXbert分数4.5%。

Conclusion: 本文表明，一种轻量级、上下文感知的PRM可以为临床LVLM提供模型无关的安全层，而无需访问内部激活。

Abstract: Automating radiology report generation with Large Vision-Language Models
(LVLMs) holds great potential, yet these models often produce clinically
critical hallucinations, posing serious risks. Existing hallucination detection
methods frequently lack the necessary sentence-level granularity or robust
generalization across different LVLM generators. We introduce a novel approach:
a sentence-level Process Reward Model (PRM) adapted for this vision-language
task. Our PRM predicts the factual correctness of each generated sentence,
conditioned on clinical context and preceding text. When fine-tuned on
MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM
outperforms existing verification techniques, demonstrating, for instance,
relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in
AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods
reliant on internal model states, our PRM demonstrates strong generalization to
an unseen LVLM. We further show its practical utility: PRM scores effectively
filter low-quality reports, improving F1-CheXbert scores by 4.5% (when
discarding the worst 10% of reports). Moreover, when guiding a novel weighted
best-of-N selection process on the MIMIC-CXR test set, our PRM show relative
improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for
BERTScore. These results demonstrate that a lightweight, context-aware PRM
provides a model-agnostic safety layer for clinical LVLMs without access to
internal activations

</details>


### [94] [Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?](https://arxiv.org/abs/2510.23252)
*Tawsif Tashwar Dipto,Azmol Hossain,Rubayet Sabbir Faruque,Md. Rezuwan Hassan,Kanij Fatema,Tanmoy Shome,Ruwad Naswan,Md. Foriduzzaman Zihad,Mohaymen Ul Anam,Nazia Tasnim,Hasan Mahmud,Md Kamrul Hasan,Md. Mehedi Hasan Shawon,Farig Sadeque,Tahsin Reasat*

Main category: cs.CL

TL;DR: 本文开发了一个78小时的标注孟加拉语语音到文本语料库Ben-10，用于研究方言变化对自动语音识别（ASR）的影响。结果表明，语音基础模型在区域方言ASR中表现不佳，但方言特定的模型训练可以缓解这个问题。


<details>
  <summary>Details</summary>
Motivation: 传统研究依赖于大多数低资源语言的规范形式，而区域方言的自动语音识别（ASR）被视为微调任务。为了研究方言变化对ASR的影响，我们开发了一个78小时的标注孟加拉语语音到文本语料库。

Method: 我们开发了一个名为Ben-10的78小时标注的孟加拉语语音到文本（STT）语料库，以研究方言变化对ASR的影响。

Result: 从语言学和数据驱动的角度进行的调查表明，语音基础模型在区域方言ASR中表现不佳，无论是零样本还是微调设置。我们观察到所有深度学习方法在方言变化下难以建模语音数据，但方言特定的模型训练可以缓解这个问题。

Conclusion: 我们的数据集还可以作为在资源受限的ASR算法中进行ASR建模的分布外（OOD）资源。该项目开发的数据集和代码是公开的。

Abstract: Conventional research on speech recognition modeling relies on the canonical
form for most low-resource languages while automatic speech recognition (ASR)
for regional dialects is treated as a fine-tuning task. To investigate the
effects of dialectal variations on ASR we develop a 78-hour annotated Bengali
Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and
data-driven perspectives shows that speech foundation models struggle heavily
in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe
that all deep learning methods struggle to model speech data under dialectal
variations but dialect specific model training alleviates the issue. Our
dataset also serves as a out of-distribution (OOD) resource for ASR modeling
under constrained resources in ASR algorithms. The dataset and code developed
for this project are publicly available

</details>


### [95] [Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding](https://arxiv.org/abs/2510.23271)
*Mohammed Aljafari,Ismail Alturki,Ahmed Mori,Yehya Kadumi*

Main category: cs.CL

TL;DR: Mubeen 是一个专为阿拉伯语设计的语言模型，旨在深入理解阿拉伯语言学、伊斯兰研究和文化遗产。它通过使用原生阿拉伯语资料来确保文化真实性和准确性，并解决了“效用差距危机”，使其成为一种决策指南，符合沙特2030愿景。


<details>
  <summary>Details</summary>
Motivation: 开发 Mubeen 的动机是创建一个能够准确理解和回应用户意图的阿拉伯语语言模型，同时确保文化真实性和准确性。

Method: Mubeen 使用了深度语言工程框架，结合了文化遗产的专业知识和多学科专家模块，以提高其在文化保护和一般知识领域的性能。

Result: Mubeen 能够精确理解古典文本、现代写作和区域方言，并提供准确且上下文相关的回答。它的核心创新是实用闭包架构，解决了“效用差距危机”。

Conclusion: Mubeen 是一个专为阿拉伯语设计的语言模型，旨在深入理解阿拉伯语言学、伊斯兰研究和文化遗产。它通过使用原生阿拉伯语资料来确保文化真实性和准确性，并解决了“效用差距危机”，使其成为一种决策指南，符合沙特2030愿景。

Abstract: Mubeen is a proprietary Arabic language model developed by MASARAT SA,
optimized for deep understanding of Arabic linguistics, Islamic studies, and
cultural heritage. Trained on an extensive collection of authentic Arabic
sources significantly expanded by digitizing historical manuscripts via a
proprietary Arabic OCR engine, the model incorporates seminal scholarly works
in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside
thousands of academic theses and peer-reviewed research papers. Conditioned
through a deep linguistic engineering framework, Mubeen masters not just the
meaning but the eloquence of Arabic, enabling precise understanding across
classical texts, contemporary writing, and regional dialects with focus on
comprehending user intent and delivering accurate, contextually relevant
responses. Unlike other Arabic models relying on translated English data that
often fail in intent detection or retrieval-augmented generation (RAG), Mubeen
uses native Arabic sources to ensure cultural authenticity and accuracy. Its
core innovation is the Practical Closure Architecture, designed to solve the
"Utility Gap Crisis" where factually correct answers fail to resolve users'
core needs, forcing them into frustrating cycles of re-prompting. By
prioritizing clarity and decisive guidance, Mubeen transforms from an
information repository into a decisive guide, aligning with Saudi Vision 2030.
The model's architecture combines deep heritage specialization with
multi-disciplinary expert modules, enabling robust performance across both
cultural preservation and general knowledge domains.

</details>


### [96] [Code Aesthetics with Agentic Reward Feedback](https://arxiv.org/abs/2510.23272)
*Bang Xiao,Lingjie Jiang,Shaohan Huang,Tengchao Lv,Yupan Huang,Xun Wu,Lei Cui,Furu Wei*

Main category: cs.CL

TL;DR: 本文介绍了一种新的管道来增强LLM生成代码的美学质量。我们构建了AesCode-358K数据集，提出代理奖励反馈系统，并开发了GRPO-AR算法。此外，我们还创建了OpenDesign基准测试。实验结果显示，这种方法显著提高了代码美学性能，甚至超越了一些大型模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码相关任务中已成为开发者的宝贵助手。虽然LLMs在传统的编程任务如代码生成和错误修复方面表现出色，但在视觉导向的编码任务中往往产生次优的美学效果。因此，本文旨在提高LLM生成代码的美学质量。

Method: 我们首先构建了AesCode-358K，一个专注于代码美学的大规模指令微调数据集。接着，我们提出了代理奖励反馈，一个评估可执行性、静态美学和交互美学的多代理系统。在此基础上，我们开发了GRPO-AR，将这些信号整合到GRPO算法中，以联合优化功能和代码美学。最后，我们开发了OpenDesign，一个用于评估代码美学的基准测试。

Result: 实验结果表明，结合在AesCode-358K上的监督微调和使用代理奖励反馈的强化学习显著提高了在OpenDesign上的性能，并且也提升了在现有基准如PandasPlotBench上的结果。值得注意的是，我们的AesCoder-4B超过了GPT-4o和GPT-4.1，并达到了与具有480B-685B参数的大规模开源模型相当的性能。

Conclusion: 实验结果表明，结合在AesCode-358K上的监督微调和使用代理奖励反馈的强化学习显著提高了在OpenDesign上的性能，并且也提升了在现有基准如PandasPlotBench上的结果。值得注意的是，我们的AesCoder-4B超过了GPT-4o和GPT-4.1，并达到了与具有480B-685B参数的大规模开源模型相当的性能，证明了我们方法的有效性。

Abstract: Large Language Models (LLMs) have become valuable assistants for developers
in code-related tasks. While LLMs excel at traditional programming tasks such
as code generation and bug fixing, they struggle with visually-oriented coding
tasks, often producing suboptimal aesthetics. In this paper, we introduce a new
pipeline to enhance the aesthetic quality of LLM-generated code. We first
construct AesCode-358K, a large-scale instruction-tuning dataset focused on
code aesthetics. Next, we propose agentic reward feedback, a multi-agent system
that evaluates executability, static aesthetics, and interactive aesthetics.
Building on this, we develop GRPO-AR, which integrates these signals into the
GRPO algorithm for joint optimization of functionality and code aesthetics.
Finally, we develop OpenDesign, a benchmark for assessing code aesthetics.
Experimental results show that combining supervised fine-tuning on AesCode-358K
with reinforcement learning using agentic reward feedback significantly
improves performance on OpenDesign and also enhances results on existing
benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o
and GPT-4.1, and achieves performance comparable to large open-source models
with 480B-685B parameters, underscoring the effectiveness of our approach.

</details>


### [97] [A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results](https://arxiv.org/abs/2510.23276)
*Thai-Binh Nguyen,Katerina Zmolikova,Pingchuan Ma,Ngoc Quan Pham,Christian Fuegen,Alexander Waibel*

Main category: cs.CL

TL;DR: MCoRec任务通过多模态方法解决单房间环境下的重叠对话问题，结果显示视觉线索能显著提高性能。


<details>
  <summary>Details</summary>
Motivation: MCoRec任务旨在捕捉自然的多人对话，其中录音专注于非脚本的、随意的小组聊天，导致极端的语音重叠和高度碎片化的对话回合。

Method: MCoRec任务通过结合音频、视觉和上下文线索来解决鸡尾酒会问题，要求系统联合转录每个说话人的语音并将他们聚类到相应的对话中。

Result: 音频-only基线超过100%的词错误率，而结合视觉线索则带来了50%的显著改进，突显了多模态的重要性。

Conclusion: 多模态上下文感知识别（MCoRec）任务展示了多模态方法在处理重叠对话中的重要性，尤其是在单房间环境下。

Abstract: We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in
the ninth CHiME Challenge, which addresses the cocktail-party problem of
overlapping conversations in a single-room setting using audio, visual, and
contextual cues. MCoRec captures natural multi-party conversations where the
recordings focus on unscripted, casual group chats, leading to extreme speech
overlap of up to 100% and highly fragmented conversational turns. The task
requires systems to answer the question "Who speaks when, what, and with whom?"
by jointly transcribing each speaker's speech and clustering them into their
respective conversations from audio-visual recordings. Audio-only baselines
exceed 100% word error rate, whereas incorporating visual cues yields
substantial 50% improvements, highlighting the importance of multi-modality. In
this manuscript, we present the motivation behind the task, outline the data
collection process, and report the baseline systems developed for the MCoRec.

</details>


### [98] [DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model](https://arxiv.org/abs/2510.23284)
*Yuanzhen Xie,Liu Ye,Jiqun Chu,Mochi Gao,Hehuan Liu,Yunzhi Tan,Bo Hu,Zang Li*

Main category: cs.CL

TL;DR: 本文设计了一个完全自动化的数据驱动流水线，包括自适应数据修复和错误数据增强，并提出了一种多模型协作训练方案，利用集成策略来整合多个模型的能力以解决多选问题，实验结果表明这种方法有效提高了文本到SQL任务的准确性


<details>
  <summary>Details</summary>
Motivation: 文本到SQL任务在ChatGPT发布后取得了显著进展，但数据驱动策略对文本到SQL任务的影响很少被探索。此外，发现单个微调模型的能力非常有限，因此需要一种更有效的解决方案

Method: 我们设计了一个完全自动化的数据驱动流水线，包括自适应数据修复和错误数据增强，并提出了一种多模型协作训练方案，利用集成策略来整合多个模型的能力以解决多选问题

Result: 数据驱动流水线和多模型交互迭代策略有效提高了文本到SQL任务的准确性，在轻量级文本到SQL模型中取得了第一名（在70B以内）

Conclusion: 实验结果和消融研究证明了数据中心流水线和多模型(MM)交互迭代策略的有效性，在轻量级文本到SQL模型中取得了第一名（在70B以内）

Abstract: Text-to-SQL tasks have gained attractive improvements since the release of
ChatGPT. Among them, agent-based frameworks have been widely used in this
field. However, the impact of data-centric strategies on text-to-SQL tasks has
rarely been explored. In this paper, we systemically design a fully automated
data-centric pipeline for text-to-SQL tasks, including \emph{adaptive data
repair}, which can automatically find and fix errors in the training dataset;
and \emph{error data augmentation}, where we specifically diffuse and enhance
erroneous data predicted by the initially trained models. Meanwhile, we propose
a Multi-Model collaboration training schema, aiming to train multiple models
with different augmented data, enabling them to possess distinct capabilities
and work together to complement each other, because it has been found that the
capability of a single fine-tuned model is very limited. Furthermore, we
utilize an ensemble strategy to integrate the capabilities of multiple models
to solve a multiple-choice question, aiming to further improve the accuracy of
text-to-SQL tasks. The experiment results and ablation study have demonstrated
the effectiveness of data-centric pipeline and Multi-Model(MM) interactive
iterative strategies, achieving first place in lightweight text-to-SQL models
(within 70B).

</details>


### [99] [Arabic Little STT: Arabic Children Speech Recognition Dataset](https://arxiv.org/abs/2510.23319)
*Mouhand Alkadri,Dania Desouki,Khloud Al Jallad*

Main category: cs.CL

TL;DR: 研究创建了一个阿拉伯语儿童语音数据集，并评估了Whisper模型在其中的表现，发现其性能显著低于成人数据集，强调了需要为儿童语音建立专门的基准和包含性训练数据。


<details>
  <summary>Details</summary>
Motivation: 低资源语言如阿拉伯语面临严重的数据稀缺问题，同时缺乏针对儿童的语音语料库，这给语音识别技术的发展带来了挑战。因此，研究者希望填补这一空白。

Method: 研究者创建了一个名为Arabic Little STT的黎凡特阿拉伯语儿童语音数据集，并对Whisper模型进行了系统评估，将其在该数据集上的表现与成人阿拉伯语基准进行比较。

Result: 评估结果显示，即使性能最好的Whisper模型（Large_v3）在儿童语音上的词错误率（WER）也高达0.66，远高于其在成人数据集上的低于0.20的WER。这些结果与其他关于英语语音的研究结果一致。

Conclusion: 研究强调了需要为儿童语音建立专门的基准和包含性训练数据，并指出这些数据必须受到严格的伦理和隐私框架的保护。此外，作者希望这项研究能为阿拉伯语儿童的公平语音技术提供初步步骤，并希望通过公开的数据集丰富ASR数据集中儿童的人口统计代表性。

Abstract: The performance of Artificial Intelligence (AI) systems fundamentally depends
on high-quality training data. However, low-resource languages like Arabic
suffer from severe data scarcity. Moreover, the absence of child-specific
speech corpora is an essential gap that poses significant challenges. To
address this gap, we present our created dataset, Arabic Little STT, a dataset
of Levantine Arabic child speech recorded in classrooms, containing 355
utterances from 288 children (ages 6 - 13). We further conduct a systematic
assessment of Whisper, a state-of-the-art automatic speech recognition (ASR)
model, on this dataset and compare its performance with adult Arabic
benchmarks. Our evaluation across eight Whisper variants reveals that even the
best-performing model (Large_v3) struggles significantly, achieving a 0.66 word
error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on
adult datasets. These results align with other research on English speech.
Results highlight the critical need for dedicated child speech benchmarks and
inclusive training data in ASR development. Emphasizing that such data must be
governed by strict ethical and privacy frameworks to protect sensitive child
information. We hope that this study provides an initial step for future work
on equitable speech technologies for Arabic-speaking children. We hope that our
publicly available dataset enrich the children's demographic representation in
ASR datasets.

</details>


### [100] [Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models](https://arxiv.org/abs/2510.23334)
*Mohammad Atif Quamar,Mohammad Areeb,Nishant Sharma,Ananth Shreekumar,Jonathan Rosenthal,Muslum Ozgur Ozmen,Mikhail Kuznetsov,Z. Berkay Celik*

Main category: cs.CL

TL;DR: AdaSearch is a novel blockwise search strategy that improves LLM alignment by focusing computational effort on critical response tokens, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLM alignment remains a critical challenge, and inference-time methods often yield suboptimal results due to uniform computational effort.

Method: AdaSearch, a blockwise search strategy that adaptively allocates computational budget to critical tokens, and its tree-search counterpart AdaBeam.

Result: AdaSearch improves win-rates by over 10% for harmlessness generation, controlled sentiment generation, and mathematical reasoning tasks compared to Best-of-N.

Conclusion: AdaSearch outperforms strong Best-of-N and fine-tuning baselines in various alignment tasks.

Abstract: LLM alignment remains a critical challenge. Inference-time methods provide a
flexible alternative to fine-tuning, but their uniform computational effort
often yields suboptimal alignment. We hypothesize that for many alignment
tasks, the initial tokens of a response are disproportionately more critical.
To leverage this principle, we introduce AdaSearch, a novel blockwise search
strategy. It adaptively allocates a fixed computational budget using a sampling
schedule, focusing search effort on these critical tokens. We apply AdaSearch
to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our
comprehensive evaluation across eight LLMs demonstrates that AdaSearch
outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates
improve by over 10% for harmlessness generation, controlled sentiment
generation, and for mathematical reasoning tasks relative to Best-of-N.

</details>


### [101] [BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning](https://arxiv.org/abs/2510.23337)
*Siyuan Zheng,Pai Liu,Xi Chen,Jizheng Dong,Sihan Jia*

Main category: cs.CL

TL;DR: 本文提出了一个基于BaZi的人格推理问答数据集和一个结合符号推理与大型语言模型的BaZi-LLM系统，以生成更准确和现实的虚拟人格。


<details>
  <summary>Details</summary>
Motivation: 当前的方法依赖于注释数据或手工制作的人格提示，难以扩展并生成现实、上下文连贯的人格。因此，需要一种新的方法来解决这个问题。

Method: 本文创建了第一个基于BaZi的人格推理问答数据集，并提出了BaZi-LLM系统，该系统将符号推理与大型语言模型相结合，以生成时间动态和细粒度的虚拟人格。

Result: 与主流LLM如DeepSeek-v3和GPT-5-mini相比，我们的方法在准确性上提高了30.3%-62.6%。此外，当使用错误的BaZi信息时，模型的准确性下降了20%-45%。

Conclusion: 本文提出了一个结合符号推理和大型语言模型的BaZi-LLM系统，用于生成时间动态和细粒度的虚拟人格。与主流LLM相比，该方法在准确性上有了显著提升，并展示了文化基础的符号-LLM集成在真实角色模拟中的潜力。

Abstract: Human-like virtual characters are crucial for games, storytelling, and
virtual reality, yet current methods rely heavily on annotated data or
handcrafted persona prompts, making it difficult to scale up and generate
realistic, contextually coherent personas. We create the first QA dataset for
BaZi-based persona reasoning, where real human experiences categorized into
wealth, health, kinship, career, and relationships are represented as
life-event questions and answers. Furthermore, we propose the first BaZi-LLM
system that integrates symbolic reasoning with large language models to
generate temporally dynamic and fine-grained virtual personas. Compared with
mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a
30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information
is used, our model's accuracy drops by 20%-45%, showing the potential of
culturally grounded symbolic-LLM integration for realistic character
simulation.

</details>


### [102] [LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data](https://arxiv.org/abs/2510.23341)
*Teng Lin*

Main category: cs.CL

TL;DR: 本文提出LightKGG框架，利用小型语言模型高效提取知识图谱，通过上下文集成图提取和拓扑增强的关系推理技术，解决现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱提取方法依赖于易出错的模式匹配技术或资源密集型的大语言模型（LLMs），而最近的工具虽然利用LLMs生成KG，但其计算需求限制了低资源环境的可用性。因此，需要一种更高效的KG提取方法。

Method: 本文提出了LightKGG框架，包含两个关键技术：(1) 上下文集成图提取，将上下文信息与节点和边整合到统一的图结构中；(2) 拓扑增强的关系推理，利用提取图的固有拓扑结构来高效推断关系。

Result: LightKGG框架能够在最小的硬件需求下准确构建KG，填补了自动化知识提取与实际部署场景之间的差距，并引入了优化SLM效率的科学方法。

Conclusion: 本文介绍了LightKGG框架，该框架通过两个关键技术改进，使小型语言模型（SLMs）能够高效地从文本数据中提取知识图谱（KG），从而解决了高质知识图谱稀缺的问题，并为结构化自然语言处理任务提供了科学严谨的方法。

Abstract: The scarcity of high-quality knowledge graphs (KGs) remains a critical
bottleneck for downstream AI applications, as existing extraction methods rely
heavily on error-prone pattern-matching techniques or resource-intensive large
language models (LLMs). While recent tools leverage LLMs to generate KGs, their
computational demands limit accessibility for low-resource environments. Our
paper introduces LightKGG, a novel framework that enables efficient KG
extraction from textual data using small-scale language models (SLMs) through
two key technical innovations: (1) Context-integrated Graph extraction
integrates contextual information with nodes and edges into a unified graph
structure, reducing the reliance on complex semantic processing while
maintaining more key information; (2) Topology-enhanced relationship inference
leverages the inherent topology of the extracted graph to efficiently infer
relationships, enabling relationship discovery without relying on complex
language understanding capabilities of LLMs. By enabling accurate KG
construction with minimal hardware requirements, this work bridges the gap
between automated knowledge extraction and practical deployment scenarios while
introducing scientifically rigorous methods for optimizing SLM efficiency in
structured NLP tasks.

</details>


### [103] [How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes](https://arxiv.org/abs/2510.23358)
*Sheri Osborn,Rohit Valecha,H. Raghav Rao,Dan Sass,Anthony Rios*

Main category: cs.CL

TL;DR: 本文介绍了一个基准测试，用于评估大型语言模型（LLMs）预测职业需求变化的能力。该基准测试结合了美国分行业职位空缺的高频指数和全球职业变化数据。实验结果显示，结构化的任务提示可以提高预测稳定性，而角色提示在短期趋势上表现更好。然而，不同行业和时间范围的表现差异显著，表明需要领域感知提示和严格的评估协议。


<details>
  <summary>Details</summary>
Motivation: 人工智能正在重塑劳动力市场，但缺乏系统预测其对就业影响的工具。尽管现有研究表明LLMs可以提取情感、总结经济报告并模仿预测者行为，但很少有研究评估它们在前瞻性劳动力预测中的应用。因此，本文旨在开发一个基准测试，以评估LLMs在预测职业需求变化方面的表现。

Method: 本文引入了一个基准测试，用于评估大型语言模型（LLMs）预测职业需求变化的能力。该基准测试结合了两个互补的数据集：美国分行业职位空缺的高频指数，以及由于AI采用而产生的全球职业变化数据。这些数据被格式化为具有清晰时间分割的预测任务，以最小化信息泄露的风险。然后使用多种提示策略评估LLMs，比较任务引导、角色驱动和混合方法在不同模型家族中的表现。

Result: 结果表明，结构化的任务提示可以持续提高预测稳定性，而角色提示在短期趋势上表现出优势。然而，不同行业和时间范围的表现差异显著，突显了需要领域感知提示和严格的评估协议。

Conclusion: 本文通过发布基准测试，旨在支持未来关于劳动预测、提示设计和基于LLM的经济推理的研究。该工作有助于研究LLM如何与现实世界的经济数据互动，并提供了一个可重复的测试平台，用于研究AI作为预测工具在劳动力市场中的局限性和机遇。

Abstract: Artificial intelligence is reshaping labor markets, yet we lack tools to
systematically forecast its effects on employment. This paper introduces a
benchmark for evaluating how well large language models (LLMs) can anticipate
changes in job demand, especially in occupations affected by AI. Existing
research has shown that LLMs can extract sentiment, summarize economic reports,
and emulate forecaster behavior, but little work has assessed their use for
forward-looking labor prediction. Our benchmark combines two complementary
datasets: a high-frequency index of sector-level job postings in the United
States, and a global dataset of projected occupational changes due to AI
adoption. We format these data into forecasting tasks with clear temporal
splits, minimizing the risk of information leakage. We then evaluate LLMs using
multiple prompting strategies, comparing task-scaffolded, persona-driven, and
hybrid approaches across model families. We assess both quantitative accuracy
and qualitative consistency over time. Results show that structured task
prompts consistently improve forecast stability, while persona prompts offer
advantages on short-term trends. However, performance varies significantly
across sectors and horizons, highlighting the need for domain-aware prompting
and rigorous evaluation protocols. By releasing our benchmark, we aim to
support future research on labor forecasting, prompt design, and LLM-based
economic reasoning. This work contributes to a growing body of research on how
LLMs interact with real-world economic data, and provides a reproducible
testbed for studying the limits and opportunities of AI as a forecasting tool
in the context of labor markets.

</details>


### [104] [Detecting Religious Language in Climate Discourse](https://arxiv.org/abs/2510.23395)
*Evy Beijen,Pien Pieterse,Yusuf Çelik,Willem Th. van Peursen,Sandjai Bhulai,Meike Morren*

Main category: cs.CL

TL;DR: 本文探讨了宗教语言在气候相关文本中的表现，并比较了基于规则的模型和大语言模型在检测宗教语言方面的效果。


<details>
  <summary>Details</summary>
Motivation: 宗教语言继续渗透到当代话语中，即使在表面上世俗的领域，如环境主义和气候变化辩论。本文旨在调查世俗和宗教非政府组织（NGO）产生的与气候相关的文本中显性和隐性的宗教语言形式。

Method: 本文引入了一种双重方法论：基于规则的模型使用从生态神学文献中得出的宗教术语层次树，以及在零样本设置下运行的大语言模型（LLMs）。

Result: 结果表明，基于规则的方法比LLMs更一致地将更多句子标记为宗教语言。这些发现不仅突出了计算检测宗教语言的方法论挑战，还突显了关于宗教语言是否应仅由词汇还是上下文意义定义的更广泛紧张关系。

Conclusion: 本研究通过展示分析宗教语言在气候话语中持续存在的潜力和局限性，为宗教研究中的数字方法做出了贡献。

Abstract: Religious language continues to permeate contemporary discourse, even in
ostensibly secular domains such as environmental activism and climate change
debates. This paper investigates how explicit and implicit forms of religious
language appear in climate-related texts produced by secular and religious
nongovernmental organizations (NGOs). We introduce a dual methodological
approach: a rule-based model using a hierarchical tree of religious terms
derived from ecotheology literature, and large language models (LLMs) operating
in a zero-shot setting. Using a dataset of more than 880,000 sentences, we
compare how these methods detect religious language and analyze points of
agreement and divergence. The results show that the rule-based method
consistently labels more sentences as religious than LLMs. These findings
highlight not only the methodological challenges of computationally detecting
religious language but also the broader tension over whether religious language
should be defined by vocabulary alone or by contextual meaning. This study
contributes to digital methods in religious studies by demonstrating both the
potential and the limitations of approaches for analyzing how the sacred
persists in climate discourse.

</details>


### [105] [EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting](https://arxiv.org/abs/2510.23396)
*Musleh Alharthi,Kaleel Mahmood,Sarosh Patel,Ausif Mahmood*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer的Mixture of Experts (MoE)框架，结合了多种先进的TSF模型，取得了优于现有模型的结果。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测（TSF）数据倾向于最近的过去，并且可能受到不可预测事件的影响。最近的研究表明，基于Transformer的模型在TSF中表现优异，但也有研究质疑其有效性。因此，需要一种更强大的模型来应对这些挑战。

Method: 我们提出了一种基于Transformer的Mixture of Experts (MoE)门控网络，将最先进的TSF模型（如xLSTM、增强型Linear、PatchTST和minGRU）集成在一起。

Result: 我们的模型在标准基准测试中优于所有现有的TSF模型，甚至超过了基于MoE框架的最新方法。

Conclusion: 我们的方法在标准基准测试中优于所有现有的TSF模型，甚至超过了基于MoE框架的最新方法。

Abstract: The immense success of the Transformer architecture
  in Natural Language Processing has led to its adoption in Time Se ries
Forecasting (TSF), where superior performance has been shown.
  However, a recent important paper questioned their effectiveness by
  demonstrating that a simple single layer linear model outperforms
  Transformer-based models. This was soon shown to be not as valid,
  by a better transformer-based model termed PatchTST. More re cently, TimeLLM
demonstrated even better results by repurposing a
  Large Language Model (LLM) for the TSF domain. Again, a follow
  up paper challenged this by demonstrating that removing the LLM
  component or replacing it with a basic attention layer in fact yields
  better performance. One of the challenges in forecasting is the fact
  that TSF data favors the more recent past, and is sometimes subject
  to unpredictable events. Based upon these recent insights in TSF, we
  propose a strong Mixture of Experts (MoE) framework. Our method
  combines the state-of-the-art (SOTA) models including xLSTM, en hanced
Linear, PatchTST, and minGRU, among others. This set of
  complimentary and diverse models for TSF are integrated in a Trans former
based MoE gating network. Our proposed model outperforms
  all existing TSF models on standard benchmarks, surpassing even the
  latest approaches based on MoE frameworks.

</details>


### [106] [Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences](https://arxiv.org/abs/2510.23451)
*Zhuoran Jin,Hongbang Yuan,Kejian Zhu,Jiachun Li,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出了 Omni-Reward，一个支持多模态和自由形式偏好的通用奖励模型，解决了现有模型在模态覆盖和偏好灵活性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 解决奖励模型在模态不平衡和偏好刚性方面的挑战，以更好地对齐AI行为与人类偏好。

Method: 提出 Omni-Reward，包括评估、数据和模型三个部分：(1) 引入 Omni-RewardBench，第一个涵盖多种模态的奖励模型基准；(2) 构建 Omni-RewardData，一个包含大量偏好对的数据集；(3) 提出 Omni-RewardModel，包括判别式和生成式奖励模型。

Result: Omni-Reward 在 Omni-RewardBench 和其他广泛使用的奖励建模基准上表现出色。

Conclusion: Omni-Reward 是一种通用的多模态奖励模型，能够支持自由形式的偏好，并在多个任务和模态上表现出色。

Abstract: Reward models (RMs) play a critical role in aligning AI behaviors with human
preferences, yet they face two fundamental challenges: (1) Modality Imbalance,
where most RMs are mainly focused on text and image modalities, offering
limited support for video, audio, and other modalities; and (2) Preference
Rigidity, where training on fixed binary preference pairs fails to capture the
complexity and diversity of personalized preferences. To address the above
challenges, we propose Omni-Reward, a step toward generalist omni-modal reward
modeling with support for free-form preferences, consisting of: (1) Evaluation:
We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form
preferences, covering nine tasks across five modalities including text, image,
video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal
preference dataset comprising 248K general preference pairs and 69K
instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We
propose Omni-RewardModel, which includes both discriminative and generative
RMs, and achieves strong performance on Omni-RewardBench as well as other
widely used reward modeling benchmarks.

</details>


### [107] [BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents](https://arxiv.org/abs/2510.23458)
*Litu Ou,Kuan Li,Huifeng Yin,Liwen Zhang,Zhongwang Zhang,Xixi Wu,Rui Ye,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文研究了基于LLM的搜索代理在多轮交互中通过口头化置信度分数传达置信度的能力，并提出了一种新的Test-Time Scaling方法，以提高答案质量并减少令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要集中在单次交互场景中，而对复杂多轮交互中的置信度研究有限。我们研究了基于LLM的搜索代理是否能够在长时间动作序列后通过口头化的置信度分数传达自己的置信度。

Method: 我们提出了Test-Time Scaling (TTS)方法，利用置信度分数来确定答案质量，并鼓励模型尝试直到达到满意的置信度水平。

Result: 实验表明，模型在高置信度时任务准确性更高，而在低置信度时准确性接近零。我们的方法显著减少了令牌消耗，同时表现出与基线固定预算TTS方法相当的性能。

Conclusion: 我们的方法在减少令牌消耗的同时表现出与基线固定预算TTS方法相当的性能。

Abstract: Confidence in LLMs is a useful indicator of model uncertainty and answer
reliability. Existing work mainly focused on single-turn scenarios, while
research on confidence in complex multi-turn interactions is limited. In this
paper, we investigate whether LLM-based search agents have the ability to
communicate their own confidence through verbalized confidence scores after
long sequences of actions, a significantly more challenging task compared to
outputting confidence in a single interaction. Experimenting on open-source
agentic models, we first find that models exhibit much higher task accuracy at
high confidence while having near-zero accuracy when confidence is low. Based
on this observation, we propose Test-Time Scaling (TTS) methods that use
confidence scores to determine answer quality, encourage the model to try again
until reaching a satisfactory confidence level. Results show that our proposed
methods significantly reduce token consumption while demonstrating competitive
performance compared to baseline fixed budget TTS methods.

</details>


### [108] [Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts](https://arxiv.org/abs/2510.23464)
*Nikesh Gyawali,Doina Caragea,Alex Vasenkov,Cornelia Caragea*

Main category: cs.CL

TL;DR: 本研究介绍了一个用于金融领域目标特定立场检测的句子级语料库，并评估了大型语言模型在不同提示策略下的性能，结果表明少样本结合CoT提示的方法优于监督基线，且LLMs在不同数据集上的表现存在差异。


<details>
  <summary>Details</summary>
Motivation: 金融叙事对于投资者、审计师和监管机构非常重要，但由于其长度、财务术语和微妙的语言，细粒度分析变得困难。以往的金融领域情感分析需要大量昂贵的标记数据，使得针对特定财务目标的句子级立场分析具有挑战性。

Method: 我们引入了一个针对三个核心财务指标（债务、每股收益（EPS）和销售额）的句子级语料库，并使用先进的ChatGPT-o3-pro模型进行立场（正面、负面、中性）标注，同时进行了严格的人员验证。然后，我们对现代大型语言模型（LLMs）进行了系统评估，采用了零样本、少样本和思维链（CoT）提示策略。

Result: 结果显示，与监督基线相比，结合CoT提示的少样本方法表现最佳，而LLMs在SEC和ECT数据集上的表现有所不同。

Conclusion: 我们的研究结果表明，利用大型语言模型（LLMs）在金融领域进行目标特定的立场分析是可行的，而无需大量标记数据。

Abstract: Financial narratives from U.S. Securities and Exchange Commission (SEC)
filing reports and quarterly earnings call transcripts (ECTs) are very
important for investors, auditors, and regulators. However, their length,
financial jargon, and nuanced language make fine-grained analysis difficult.
Prior sentiment analysis in the financial domain required a large, expensive
labeled dataset, making the sentence-level stance towards specific financial
targets challenging. In this work, we introduce a sentence-level corpus for
stance detection focused on three core financial metrics: debt, earnings per
share (EPS), and sales. The sentences were extracted from Form 10-K annual
reports and ECTs, and labeled for stance (positive, negative, neutral) using
the advanced ChatGPT-o3-pro model under rigorous human validation. Using this
corpus, we conduct a systematic evaluation of modern large language models
(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting
strategies. Our results show that few-shot with CoT prompting performs best
compared to supervised baselines, and LLMs' performance varies across the SEC
and ECT datasets. Our findings highlight the practical viability of leveraging
LLMs for target-specific stance in the financial domain without requiring
extensive labeled data.

</details>


### [109] [MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring](https://arxiv.org/abs/2510.23477)
*Tengchao Yang,Sichen Guo,Mengzhao Jia,Jiaming Su,Yuanyang Liu,Zhihan Zhang,Meng Jiang*

Main category: cs.CL

TL;DR: 本文介绍了MMTutorBench，这是第一个用于AI数学辅导的基准测试，旨在评估AI在诊断学生困难和逐步指导方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试大多忽略了AI辅导所需的诊断技能和逐步指导能力。

Method: 引入了MMTutorBench，这是第一个用于AI数学辅导的基准测试，包含685个问题，围绕教学重要的关键步骤构建。每个问题都配有特定于问题的评分标准，以在六个维度上进行细粒度评估，并分为三个任务：洞察发现、操作制定和操作执行。

Result: 评估了12个领先的MLLMs，发现专有系统和开源系统之间存在明显的性能差距，与人类导师相比还有很大提升空间，并且在输入变体中表现出一致的趋势：OCR管道会降低辅导质量，少样本提示带来的收益有限，基于评分标准的LLM-as-a-Judge证明非常可靠。

Conclusion: MMTutorBench具有挑战性且具有诊断价值，可以推动AI辅导的发展。

Abstract: Effective math tutoring requires not only solving problems but also
diagnosing students' difficulties and guiding them step by step. While
multimodal large language models (MLLMs) show promise, existing benchmarks
largely overlook these tutoring skills. We introduce MMTutorBench, the first
benchmark for AI math tutoring, consisting of 685 problems built around
pedagogically significant key-steps. Each problem is paired with
problem-specific rubrics that enable fine-grained evaluation across six
dimensions, and structured into three tasks-Insight Discovery, Operation
Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find
clear performance gaps between proprietary and open-source systems, substantial
room compared to human tutors, and consistent trends across input variants: OCR
pipelines degrade tutoring quality, few-shot prompting yields limited gains,
and our rubric-based LLM-as-a-Judge proves highly reliable. These results
highlight both the difficulty and diagnostic value of MMTutorBench for
advancing AI tutoring.

</details>


### [110] [M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset](https://arxiv.org/abs/2510.23508)
*Jiahui Geng,Jonathan Tonglet,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文介绍了M4FC数据集，以解决现有数据集的局限性，并提供多模态事实检查任务的基线结果。


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在实例数量少、语言和任务范围有限、证据泄漏或依赖外部新闻文章等问题，因此需要一个更全面的数据集。

Method: 引入了M4FC数据集，包含4,982张图像和6,980个声明，并提供了所有任务的基线结果。

Result: M4FC数据集涵盖了六种多模态事实检查任务，并分析了中间任务对下游结论预测性能的影响。

Conclusion: M4FC是一个新的真实世界数据集，解决了现有数据集的多个局限性，并为多模态自动事实检查提供了基准结果和代码。

Abstract: Existing real-world datasets for multimodal automated fact-checking have
multiple limitations: they contain few instances, focus on only one or two
languages and tasks, suffer from evidence leakage, or depend on external sets
of news articles for sourcing true claims. To address these shortcomings, we
introduce M4FC, a new real-world dataset comprising 4,982 images paired with
6,980 claims. The images, verified by professional fact-checkers from 22
organizations, represent diverse cultural and geographic contexts. Each claim
is available in one or two out of ten languages. M4FC spans six multimodal
fact-checking tasks: visual claim extraction, claimant intent prediction, fake
detection, image contextualization, location verification, and verdict
prediction. We provide baseline results for all tasks and analyze how combining
intermediate tasks influence downstream verdict prediction performance. We make
our dataset and code available.

</details>


### [111] [IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering](https://arxiv.org/abs/2510.23536)
*Jieyong Kim,Maryam Amirizaniani,Soojin Yoon,Dongha Lee*

Main category: cs.CL

TL;DR: 本文介绍了IPQA，一个用于个性化问答中核心意图识别的基准。通过分析用户选择答案的行为模式来推导核心意图，并构建了一个多领域数据集。实验表明，当前系统在个性化语境中难以识别核心意图。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估响应质量或检索性能，而没有直接测量意图识别能力。由于缺乏对用户优先意图的理解，系统无法生成满足个体信息需求的响应。

Method: 通过系统过滤、基于LLM的注释和结合自动化验证与人工验证的严格质量控制构建数据集。利用满足理论，从答案选择中的可观察行为模式推导核心意图。

Result: 实验评估显示，当前系统在个性化语境中难以识别核心意图，性能随着问题复杂性的增加而下降。

Conclusion: 当前系统在个性化语境中难以识别核心意图，随着问题复杂性的增加，性能下降。代码和数据集将公开以促进未来的研究。

Abstract: Intent identification serves as the foundation for generating appropriate
responses in personalized question answering (PQA). However, existing
benchmarks evaluate only response quality or retrieval performance without
directly measuring intent identification capabilities. This gap is critical
because without understanding which intents users prioritize, systems cannot
generate responses satisfying individual information needs. To address this, we
introduce the concept of core intents: intents users prioritize when selecting
answers to satisfy their information needs. To evaluate these core intents, we
propose IPQA, a benchmark for core Intent identification in Personalized
Question Answering. Since users do not explicitly state their prioritized
intents, we derive core intents from observable behavior patterns in answer
selection, grounded in satisficing theory where users choose answers meeting
their acceptance thresholds. We construct a dataset with various domains
through systematic filtering, LLM-based annotation, and rigorous quality
control combining automated verification with human validation. Experimental
evaluations across state-of-the-art language models reveal that current systems
struggle with core intent identification in personalized contexts. Models fail
to identify core intents from user histories, with performance degrading as
question complexity increases. The code and dataset will be made publicly
available to facilitate future research in this direction.

</details>


### [112] [LimRank: Less is More for Reasoning-Intensive Information Reranking](https://arxiv.org/abs/2510.23544)
*Tingyu Song,Yilun Zhao,Siyue Zhang,Chen Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: This paper presents LIMRANK, a reranker model that achieves competitive performance with minimal training data, demonstrating the effectiveness of synthetic data generation for information reranking tasks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for adapting LLMs to information reranking tasks rely on large-scale fine-tuning, which is computationally expensive. The paper aims to show that modern LLMs can be effectively adapted using minimal, high-quality supervision.

Method: The paper introduces LIMRANK-SYNTHESIZER, a pipeline for generating diverse, challenging, and realistic reranking examples, and uses this synthetic data to fine-tune the LIMRANK model.

Result: LIMRANK achieves competitive performance on two challenging benchmarks, BRIGHT and FollowIR, while being trained on less than 5% of the data typically used in prior work. Ablation studies also demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK.

Conclusion: LIMRANK achieves competitive performance while being trained on less than 5% of the data typically used in prior work. It also demonstrates strong generalization capabilities across downstream tasks.

Abstract: Existing approaches typically rely on large-scale fine-tuning to adapt LLMs
for information reranking tasks, which is computationally expensive. In this
work, we demonstrate that modern LLMs can be effectively adapted using only
minimal, high-quality supervision. To enable this, we design
LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating
diverse, challenging, and realistic reranking examples. Using this synthetic
data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two
challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and
FollowIR for instruction-following retrieval. Our experiments demonstrate that
LIMRANK achieves competitive performance, while being trained on less than 5%
of the data typically used in prior work. Further ablation studies demonstrate
the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization
capabilities of LIMRANK across downstream tasks, including scientific
literature search and retrieval-augmented generation for knowledge-intensive
problem solving.

</details>


### [113] [Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models](https://arxiv.org/abs/2510.23585)
*Luis Ramos,Hiram Calvo,Olga Kolesnikova*

Main category: cs.CL

TL;DR: 该研究评估了传统机器学习模型和微调的变压器在希望言语检测任务中的表现，结果显示变压器模型在检测希望言语方面优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 识别希望言语已成为一项有前途的NLP任务，考虑到需要在社交媒体平台上检测激励性表达和目标导向行为的需求。

Method: 该提案评估了传统机器学习模型和微调的变压器，用于一个先前分割为训练、开发和测试集的希望言语数据集。

Result: 在开发测试中，线性核SVM和逻辑回归都达到了0.78的宏F1；SVM与RBF内核达到了0.77，Na"ive Bayes达到了0.75。变压器模型提供了更好的结果，最佳模型实现了0.82的加权精度，0.80的加权召回率，0.79的加权F1，0.79的宏F1和0.80的准确率。

Conclusion: 这些结果表明，虽然经过优化的传统机器学习模型仍然敏捷，但变压器架构能够检测到希望言语的一些微妙语义，从而在希望言语检测中实现更高的精确度和召回率，这表明大型变压器和LLMs可能在小数据集上表现更好。

Abstract: The identification of hope speech has become a promised NLP task, considering
the need to detect motivational expressions of agency and goal-directed
behaviour on social media platforms. This proposal evaluates traditional
machine learning models and fine-tuned transformers for a previously split hope
speech dataset as train, development and test set. On development test, a
linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM
with RBF kernel reached 0.77, and Na\"ive Bayes hit 0.75. Transformer models
delivered better results, the best model achieved weighted precision of 0.82,
weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80
accuracy. These results suggest that while optimally configured traditional
machine learning models remain agile, transformer architectures detect some
subtle semantics of hope to achieve higher precision and recall in hope speech
detection, suggesting that larges transformers and LLMs could perform better in
small datasets.

</details>


### [114] [Think Twice: Branch-and-Rethink Reasoning Reward Model](https://arxiv.org/abs/2510.23596)
*Yizhu Jiao,Jiaqi Zeng,Julien Veron Vialard,Oleksii Kuchaiev,Jiawei Han,Olivier Delalleau*

Main category: cs.CL

TL;DR: 本文提出了BR-RM，一种基于两次思考的奖励模型，通过减少判断扩散和提高对关键错误的敏感性，实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型（RMs）通常将多个质量维度压缩成一个标量，这会导致判断扩散，注意力分散，分析浅显。为了改进这一点，本文引入了branch-and-rethink (BR-RM)方法，以提高奖励建模的效果。

Method: BR-RM是一种两轮奖励模型，第一轮进行自适应分支，选择少量实例关键维度并提出简洁的证据寻求假设；第二轮执行分支条件反思，针对这些假设进行测试并仔细检查最重要的内容。

Result: BR-RM通过将一次性评分转化为聚焦的第二次审视推理，减少了判断扩散，并提高了对细微但关键错误的敏感性。实验结果表明，该模型在三个具有挑战性的奖励建模基准上取得了最先进的性能。

Conclusion: BR-RM通过将think-twice原则应用于奖励建模，减少了判断扩散并提高了对细微但关键错误的敏感性，同时保持了实用性和可扩展性。实验结果表明，该模型在三个具有挑战性的奖励建模基准上取得了最先进的性能。

Abstract: Large language models (LLMs) increasingly rely on thinking models that
externalize intermediate steps and allocate extra test-time compute, with
think-twice strategies showing that a deliberate second pass can elicit
stronger reasoning. In contrast, most reward models (RMs) still compress many
quality dimensions into a single scalar in one shot, a design that induces
judgment diffusion: attention spreads across evaluation criteria, yielding
diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a
two-turn RM that transfers the think-twice principle to reward modeling. Turn 1
performs adaptive branching, selecting a small set of instance-critical
dimensions (such as factuality and safety) and sketching concise,
evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a
targeted reread that tests those hypotheses and scrutinizes only what matters
most. We train with GRPO-style reinforcement learning over structured two-turn
traces using a simple binary outcome reward with strict format checks, making
the approach compatible with standard RLHF pipelines. By converting
all-at-oncescoringintofocused, second-lookreasoning,
BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet
consequential errors while remaining practical and scalable. Experimental
results demonstrate that our model achieves state-of-the-art performance on
three challenging reward modeling benchmarks across diverse domains. The code
and the model will be released soon.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [115] [Next-Generation LLM for UAV: From Natural Language to Autonomous Flight](https://arxiv.org/abs/2510.21739)
*Liangqi Yuan,Chuhao Deng,Dong-Jun Han,Inseok Hwang,Sabine Brunswicker,Christopher G. Brinton*

Main category: cs.RO

TL;DR: 本文提出了NeLV系统，这是一个将大型语言模型集成到多尺度无人机操作中的综合演示和自动化路线图。通过五个关键技术组件处理自然语言指令，以协调短、中、长距离无人机任务，并通过三个代表性的用例展示了系统的可行性。同时，建立了五级自动化分类法，以图表展示从当前LLM-as-Parser能力（第1级）到完全自主的LLM-as-Autopilot系统（第5级）的演变，并识别了每个阶段的技术前提和研究挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的研究主要集中在小型无人机应用上，大多数研究集中在孤立的组件上，如玩具无人机的路径规划，而缺乏对中型和远程无人机系统的全面研究。更大的无人机平台引入了不同的挑战，包括对机场起飞和降落程序的严格要求，遵守复杂的监管框架，以及具有更高任务期望的专业操作能力。

Method: 本文提出了NeLV系统，该系统通过五个关键技术组件处理自然语言指令，以协调短、中、长距离无人机任务：(i) LLM-as-Parser用于指令解释，(ii) Route Planner用于兴趣点（POI）确定，(iii) Path Planner用于航路点生成，(iv) Control Platform用于可执行轨迹实现，(v) UAV监控。

Result: 本文通过三个代表性的用例展示了系统的可行性：多无人机巡逻、多兴趣点配送和多跳重新定位。此外，还建立了一个五级自动化分类法，以图表展示从当前LLM-as-Parser能力（第1级）到完全自主的LLM-as-Autopilot系统（第5级）的演变，并识别了每个阶段的技术前提和研究挑战。

Conclusion: 本文提出了NeLV系统，这是一个将大型语言模型集成到多尺度无人机操作中的综合演示和自动化路线图。同时，建立了五个级别的自动化分类法，以图表展示从当前LLM-as-Parser能力（第1级）到完全自主的LLM-as-Autopilot系统（第5级）的演变，并识别了每个阶段的技术前提和研究挑战。

Abstract: With the rapid advancement of Large Language Models (LLMs), their
capabilities in various automation domains, particularly Unmanned Aerial
Vehicle (UAV) operations, have garnered increasing attention. Current research
remains predominantly constrained to small-scale UAV applications, with most
studies focusing on isolated components such as path planning for toy drones,
while lacking comprehensive investigation of medium- and long-range UAV systems
in real-world operational contexts. Larger UAV platforms introduce distinct
challenges, including stringent requirements for airport-based take-off and
landing procedures, adherence to complex regulatory frameworks, and specialized
operational capabilities with elevated mission expectations. This position
paper presents the Next-Generation LLM for UAV (NeLV) system -- a comprehensive
demonstration and automation roadmap for integrating LLMs into multi-scale UAV
operations. The NeLV system processes natural language instructions to
orchestrate short-, medium-, and long-range UAV missions through five key
technical components: (i) LLM-as-Parser for instruction interpretation, (ii)
Route Planner for Points of Interest (POI) determination, (iii) Path Planner
for waypoint generation, (iv) Control Platform for executable trajectory
implementation, and (v) UAV monitoring. We demonstrate the system's feasibility
through three representative use cases spanning different operational scales:
multi-UAV patrol, multi-POI delivery, and multi-hop relocation. Beyond the
current implementation, we establish a five-level automation taxonomy that
charts the evolution from current LLM-as-Parser capabilities (Level 1) to fully
autonomous LLM-as-Autopilot systems (Level 5), identifying technical
prerequisites and research challenges at each stage.

</details>


### [116] [VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting](https://arxiv.org/abs/2510.21817)
*Xiaoyu Liu,Chaoyou Fu,Chi Yan,Chu Wu,Haihan Gao,Yi-Fan Zhang,Shaoqi Dong,Cheng Qian,Bin Luo,Xiuyong Yang,Guanwu Li,Yusheng Cai,Yunhang Shen,Deqiang Jiang,Haoyu Cao,Xing Sun,Caifeng Shan,Ran He*

Main category: cs.RO

TL;DR: VITA-E is a new framework for embodied interaction that allows for concurrent and interruptible tasks, improving the responsiveness and flexibility of VLA models.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action (VLA) models are constrained by a rigid, static interaction paradigm, which lacks the ability to see, hear, speak, and act concurrently and handle real-time user interruptions dynamically.

Method: We introduce VITA-E, a novel embodied interaction framework with a dual-model architecture and a 'model-as-controller' paradigm.

Result: Experiments on a physical humanoid platform show that VITA-E can reliably handle complex interactive scenarios, achieving a high success rate on emergency stops and speech interruptions while performing concurrent speech and action.

Conclusion: VITA-E represents a significant step towards more natural and capable embodied assistants.

Abstract: Current Vision-Language-Action (VLA) models are often constrained by a rigid,
static interaction paradigm, which lacks the ability to see, hear, speak, and
act concurrently as well as handle real-time user interruptions dynamically.
This hinders seamless embodied collaboration, resulting in an inflexible and
unresponsive user experience. To address these limitations, we introduce
VITA-E, a novel embodied interaction framework designed for both behavioral
concurrency and nearly real-time interruption. The core of our approach is a
dual-model architecture where two parallel VLA instances operate as an ``Active
Model'' and a ``Standby Model'', allowing the embodied agent to observe its
environment, listen to user speech, provide verbal responses, and execute
actions, all concurrently and interruptibly, mimicking human-like multitasking
capabilities. We further propose a ``model-as-controller'' paradigm, where we
fine-tune the VLM to generate special tokens that serve as direct system-level
commands, coupling the model's reasoning with the system's behavior.
Experiments conducted on a physical humanoid platform demonstrate that VITA-E
can reliably handle complex interactive scenarios. Our framework is compatible
with various dual-system VLA models, achieving an extremely high success rate
on emergency stops and speech interruptions while also successfully performing
concurrent speech and action. This represents a significant step towards more
natural and capable embodied assistants.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [117] [UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations for Spoken Dialogue Models](https://arxiv.org/abs/2510.22588)
*Wenming Tu,Guanrou Yang,Ruiqi Yan,Wenxi Chen,Ziyang Ma,Yipeng Kang,Kai Yu,Xie Chen,Zilong Zheng*

Main category: eess.AS

TL;DR: UltraVoice is a large-scale speech dialogue dataset that enables fine-grained speech style control, improving the performance of spoken dialogue models in various tasks.


<details>
  <summary>Details</summary>
Motivation: Current spoken dialogue models lack fine-grained speech style control, which is essential for human-like interaction. The paper aims to address this limitation by introducing a dataset that enables such control.

Method: The paper introduces UltraVoice, a large-scale speech dialogue dataset with instructions across six key speech stylistic dimensions. It also describes the process of fine-tuning leading models on UltraVoice to improve their controllability.

Result: Fine-tuning models on UltraVoice significantly improves their fine-grained speech stylistic controllability without degrading core conversational abilities. The results show improvements in MOS and IFR on multi-dimensional control tasks, as well as gains in core understanding, reasoning, and conversational abilities on the URO-Bench benchmark.

Conclusion: UltraVoice is a high-quality dataset that enhances fine-grained speech style control in spoken dialogue models, and it has broad applicability for expressive speech synthesis.

Abstract: Spoken dialogue models currently lack the ability for fine-grained speech
style control, a critical capability for human-like interaction that is often
overlooked in favor of purely functional capabilities like reasoning and
question answering. To address this limitation, we introduce UltraVoice, the
first large-scale speech dialogue dataset engineered for multiple fine-grained
speech style control. Encompassing over 830 hours of speech dialogues,
UltraVoice provides instructions across six key speech stylistic dimensions:
emotion, speed, volume, accent, language, and composite styles. Fine-tuning
leading models such as SLAM-Omni and VocalNet on UltraVoice significantly
enhances their fine-grained speech stylistic controllability without degrading
core conversational abilities. Specifically, our fine-tuned models achieve
improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09
percentage points in Instruction Following Rate (IFR) on multi-dimensional
control tasks designed in the UltraVoice. Moreover, on the URO-Bench benchmark,
our fine-tuned models demonstrate substantial gains in core understanding,
reasoning, and conversational abilities, with average improvements of +10.84%
on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset's
utility extends to training controllable Text-to-Speech (TTS) models,
underscoring its high quality and broad applicability for expressive speech
synthesis. The complete dataset and model checkpoints are available at:
https://github.com/bigai-nlco/UltraVoice.

</details>


### [118] [LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization](https://arxiv.org/abs/2510.23320)
*Máté Gedeon,Péter Mihajlik*

Main category: eess.AS

TL;DR: LibriConvo is a simulated multi-speaker conversational dataset designed to support training and evaluation of speaker diarization and automatic speech recognition (ASR) systems. It ensures semantic coherence and realistic conversational timing, and provides a valuable resource for advancing multi-speaker speech processing research.


<details>
  <summary>Details</summary>
Motivation: To support training and evaluation of speaker diarization and automatic speech recognition (ASR) systems, we need a dataset that ensures semantic coherence and realistic conversational timing, unlike prior resources that mostly rely on semantically disconnected utterances and implausible temporal gaps.

Method: We introduce LibriConvo, a simulated multi-speaker conversational dataset based on speaker-aware conversation simulation (SASC), designed to support training and evaluation of speaker diarization and automatic speech recognition (ASR) systems. Our pipeline leverages CallHome with external VAD for reliable boundaries, applies compression to reduce unnaturally long silences, and organizes LibriTTS utterances by book to maintain contextual consistency. Acoustic realism is enhanced via a novel room impulse response selection procedure that ranks speaker-microphone configurations by spatial plausibility, balancing realism and diversity.

Result: The dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers, split in a speaker-disjoint manner for robust evaluation. Baselines show that the sortformer model outperforms the pyannote pipeline in diarization, while a fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves 7.29% WER for ASR, surpassing zero-shot Whisper-large-v3.

Conclusion: LibriConvo provides a valuable resource for advancing multi-speaker speech processing research with realistic conversational dynamics and controlled experimental conditions.

Abstract: We introduce LibriConvo, a simulated multi-speaker conversational dataset
based on speaker-aware conversation simulation (SASC), designed to support
training and evaluation of speaker diarization and automatic speech recognition
(ASR) systems. Unlike prior resources that mostly rely on semantically
disconnected utterances and implausible temporal gaps, LibriConvo ensures
semantic coherence and realistic conversational timing. Our pipeline leverages
CallHome with external VAD for reliable boundaries, applies compression to
reduce unnaturally long silences, and organizes LibriTTS utterances by book to
maintain contextual consistency. Acoustic realism is enhanced via a novel room
impulse response selection procedure that ranks speaker-microphone
configurations by spatial plausibility, balancing realism and diversity. The
dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers,
split in a speaker-disjoint manner for robust evaluation. Baselines show that
the sortformer model outperforms the pyannote pipeline in diarization, while a
fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves
7.29\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides
a valuable resource for advancing multi-speaker speech processing research with
realistic conversational dynamics and controlled experimental conditions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [119] [Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models](https://arxiv.org/abs/2510.21740)
*Alexa R. Tartaglini,Satchel Grant,Daniel Wurgaft,Christopher Potts,Judith E. Fan*

Main category: cs.CV

TL;DR: 本文分析了当前视觉-语言模型在数据可视化理解任务中的局限性，并提出了FUGU任务套件来研究这些模型的错误来源。研究发现，模型在生成数据点坐标方面存在问题，并且这些错误会影响最终响应。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在基本的数据可视化理解任务上仍然存在困难，但失败的原因尚不清楚。我们需要了解这些模型失败的原因，以便改进它们。

Method: 我们开发了FUGU，这是一个数据可视化理解任务套件，用于精确表征潜在的困难来源。我们使用激活补丁和线性探测来追踪信息流通过模型，以诊断这些模型产生的错误来源。

Result: 我们发现一些模型无法正确生成单个数据点的坐标，这些初始错误通常会导致最终响应的错误。当这些模型提供正确的坐标时，性能会显著提高。此外，即使模型生成了错误的响应，也可以从视觉编码器的潜在表示中成功读取正确的坐标，这表明这些错误的来源在于视觉-语言交接。

Conclusion: 我们的研究揭示了当前视觉-语言模型在数据可视化理解任务中的局限性，这些限制可能对可靠的数据可视化理解构成重大挑战。

Abstract: Data visualizations are vital components of many scientific articles and news
stories. Current vision-language models (VLMs) still struggle on basic data
visualization understanding tasks, but the causes of failure remain unclear.
Are VLM failures attributable to limitations in how visual information in the
data visualization is encoded, how information is transferred between the
vision and language modules, or how information is processed within the
language module? We developed FUGU, a suite of data visualization understanding
tasks, to precisely characterize potential sources of difficulty (e.g.,
extracting the position of data points, distances between them, and other
summary statistics). We used FUGU to investigate three widely used VLMs. To
diagnose the sources of errors produced by these models, we used activation
patching and linear probes to trace information flow through models across a
variety of prompting strategies. We found that some models fail to generate the
coordinates of individual data points correctly, and these initial errors often
lead to erroneous final responses. When these models are provided with the
correct coordinates, performance improves substantially. Moreover, even when
the model generates an incorrect response, the correct coordinates can be
successfully read out from the latent representations in the vision encoder,
suggesting that the source of these errors lies in the vision-language handoff.
We further found that while providing correct coordinates helps with tasks
involving one or a small number of data points, it generally worsens
performance for tasks that require extracting statistical relationships across
many data points. Fine-tuning models on FUGU also fails to yield ceiling
performance. These findings point to architectural constraints in current VLMs
that might pose significant challenges for reliable data visualization
understanding.

</details>


### [120] [Structured and Abstractive Reasoning on Multi-modal Relational Knowledge Images](https://arxiv.org/abs/2510.21828)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Lei Liang,Wen Zhang,Huajun Chen*

Main category: cs.CV

TL;DR: 本文旨在解决多模态大语言模型在处理抽象信息和STAR任务方面的不足，提出了一种新的数据生成方法和训练框架，并展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在理解和推理抽象信息方面面临挑战，特别是对于多模态关系知识（MMRK）的研究较少，而STAR任务也未受到足够关注。

Method: 本文提出了一个自动的STAR数据引擎和一个全面的两阶段能力增强训练框架，以及一套针对不同STAR任务的评估协议。

Result: 实验结果表明，两阶段增强框架使较小的3B/7B模型在STAR任务中显著优于GPT-4o。

Conclusion: 本文提出了STAR-64K数据集，并通过实验表明，我们的两阶段增强框架使较小的3B/7B模型能够显著超越GPT-4o在STAR任务中的表现。

Abstract: Understanding and reasoning with abstractive information from the visual
modality presents significant challenges for current multi-modal large language
models (MLLMs). Among the various forms of abstractive information, Multi-Modal
Relational Knowledge (MMRK), which represents abstract relational structures
between multi-modal entities using node-edge formats, remains largely
under-explored. In particular, STructured and Abstractive Reasoning (STAR) on
such data has received little attention from the research community. To bridge
the dual gaps in large-scale high-quality data and capability enhancement
methodologies, this paper makes the following key contributions: (i). An
automatic STAR data engine capable of synthesizing images with MMRK to build
multi-modal instruction data with reliable chain-of-thought thinking for
various STAR tasks and (ii). A comprehsive two-stage capability enhancement
training framework, accompanied by a suite of evaluation protocols tailored to
different STAR tasks. Based upon these contributions, we introduce STAR-64K, a
dataset comprising 64K high-quality multi-modal instruction samples, and
conduct experiments across 5 open-source MLLMs. Experimental results show that
our two-stage enhancement framework enables smaller 3B/7B models to
significantly outperform GPT-4o in STAR. Additionally, we provide in-depth
analysis regarding the effectiveness of various designs, data transferability,
and scalability.

</details>


### [121] [SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models](https://arxiv.org/abs/2510.21850)
*Gyubeum Lim,Yemo Koo,Vijay Krishna Madisetti*

Main category: cs.CV

TL;DR: SCoPE VLM 通过 Chain of Scroll 机制和专门的数据生成管道及强化学习方法，显著降低了内存使用并模拟了人类阅读行为，成为首个在多页文档问答中建模代理阅读模式的框架。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型在训练目标中忽略了以决策为导向的文档理解，现有方法在处理长分辨率输入时内存消耗大且不适用于本地部署解决方案。

Method: 提出了一种名为 SCoPE VLM 的文档导航专家，利用了新颖的 Chain of Scroll 机制来选择性且递归地导航文档，并引入了一个专门的数据生成管道和一种定制的强化学习方法。

Result: 该方法显著减少了内存使用，并有效模拟了类似人类的阅读行为。

Conclusion: SCoPE VLM 是第一个明确建模多页文档问答中代理阅读模式的框架，提升了多模态代理的能力。

Abstract: Understanding long-context visual information remains a fundamental challenge
for vision-language models, particularly in agentic tasks such as GUI control
and web navigation. While web pages and GUI environments are inherently
structured documents, current VLMs typically neglect decision-oriented document
understanding in their training objectives. Existing approaches primarily
extend visual embeddings to process long, high-resolution inputs, but these
methods are memory-intensive and impractical for locally deployable solutions.
To address these issues, we propose SCoPE VLM, a document navigation expert
that leverages a novel Chain of Scroll mechanism to selectively and recursively
navigate documents, focusing exclusively on relevant segments. We introduce a
dedicated data generation pipeline to construct informative Chain of Scroll
trajectories and Episodic Group Relative Policy Optimization, a tailored
reinforcement learning method to reduce the gap between training and inference.
Our method substantially reduces memory usage and effectively models human-like
reading behaviors. To the best of our knowledge, SCoPE VLM is the first
framework to explicitly model agentic reading patterns in multi-page document
question answering, advancing the capabilities of multimodal agents.

</details>


### [122] [Mitigating Coordinate Prediction Bias from Positional Encoding Failures](https://arxiv.org/abs/2510.22102)
*Xingjian Tao,Yiwei Wang,Yujun Cai,Yihong Luo,Jing Tang*

Main category: cs.CV

TL;DR: 本文研究了多模态大语言模型在视觉位置编码被扰乱时的行为，发现模型依赖于内部的位置先验。为此，我们提出了VPSG方法来纠正方向性误差，实验结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 精确的坐标预测在多模态大语言模型中仍然具有挑战性，尤其是在高分辨率输入的情况下。

Method: 我们提出了Vision-PE Shuffle Guidance (VPSG)，这是一种无需训练的测试时方法，利用这些偏差的方向性进行校正。

Result: 在ScreenSpot-Pro上的实验显示了可靠的效果提升。

Conclusion: 实验表明，位置编码的鲁棒性是多模态大语言模型进行空间推理的关键因素。

Abstract: Multimodal large language models (MLLMs) excel at vision-language tasks such
as VQA and document understanding, yet precise coordinate prediction remains
challenging. High-resolution inputs exacerbate this difficulty by producing
long token sequences that weaken positional encodings and introduce directional
biases in coordinate outputs. We investigate this phenomenon by analyzing how
MLLMs behave when visual positional encodings (VPEs) are deliberately perturbed
through shuffling. Our analysis reveals that such perturbations induce
predictable, non-random coordinate biases rather than random errors, suggesting
that models rely on internal positional priors when spatial grounding signals
are degraded. Crucially, we observe similar directional error patterns in
natural high-resolution datasets, indicating that positional encoding failures
are a key bottleneck for accurate coordinate prediction at scale. To address
this issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free
test-time method that leverages the directional nature of these biases for
correction. VPSG runs auxiliary decoding with shuffled VPEs to isolate
position-unconditioned tendencies, then uses this as negative evidence to guide
digit prediction while preserving coordinate format through a lightweight
finite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable
improvements, highlighting positional encoding robustness as a critical factor
for spatial reasoning in MLLMs.

</details>


### [123] [LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction](https://arxiv.org/abs/2510.22141)
*Yuhang Gao,Xiang Xiang,Sheng Zhong,Guoyou Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为LOC的通用语言引导框架，用于改进视觉-语言模型在3D场景理解中的应用。通过自监督学习和密集对比学习，该框架能够在没有额外训练数据的情况下实现高精度的已知类别预测和未知类别的区分。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在开放集挑战中显示出显著进展。然而，3D数据集的有限可用性阻碍了它们在3D场景理解中的有效应用。

Method: 我们提出了LOC，一种通用的语言引导框架，适用于各种占用网络，支持监督和自监督学习范式。对于自监督任务，我们采用融合多帧LiDAR点的策略，使用泊松重建填补空洞，并通过K-最近邻（KNN）为体素分配语义以获得全面的体素表示。为了缓解直接高维特征蒸馏导致的特征过度同质化，我们引入了密集对比学习（DCL）。DCL利用密集体素语义信息和预定义的文本提示。

Result: 实验表明，该方法在nuScenes数据集上表现优越，实现了已知类别的高精度预测，并能区分未知类别而无需额外训练数据。

Conclusion: 我们的模型在nuScenes数据集上表现出色，实现了已知类别的高精度预测，并且无需额外训练数据即可区分未知类别。

Abstract: Vision-Language Models (VLMs) have shown significant progress in open-set
challenges. However, the limited availability of 3D datasets hinders their
effective application in 3D scene understanding. We propose LOC, a general
language-guided framework adaptable to various occupancy networks, supporting
both supervised and self-supervised learning paradigms. For self-supervised
tasks, we employ a strategy that fuses multi-frame LiDAR points for
dynamic/static scenes, using Poisson reconstruction to fill voids, and
assigning semantics to voxels via K-Nearest Neighbor (KNN) to obtain
comprehensive voxel representations. To mitigate feature over-homogenization
caused by direct high-dimensional feature distillation, we introduce Densely
Contrastive Learning (DCL). DCL leverages dense voxel semantic information and
predefined textual prompts. This efficiently enhances open-set recognition
without dense pixel-level supervision, and our framework can also leverage
existing ground truth to further improve performance. Our model predicts dense
voxel features embedded in the CLIP feature space, integrating textual and
image pixel information, and classifies based on text and semantic similarity.
Experiments on the nuScenes dataset demonstrate the method's superior
performance, achieving high-precision predictions for known classes and
distinguishing unknown classes without additional training data.

</details>


### [124] [WAON: Large-Scale and High-Quality Japanese Image-Text Pair Dataset for Vision-Language Models](https://arxiv.org/abs/2510.22276)
*Issa Sugiura,Shuhei Kurita,Yusuke Oda,Daisuke Kawahara,Yasuo Okabe,Naoaki Okazaki*

Main category: cs.CV

TL;DR: 本文介绍了 WAON，一个大规模、高质量的日本图像-文本对数据集，以及其在 Vision-Language Models 中的出色表现。


<details>
  <summary>Details</summary>
Motivation: 构建一个大规模且高质量的日本图像-文本对数据集，以提高 Vision-Language Models (VLMs) 的性能。

Method: WAON 数据集是通过从 Common Crawl 收集大约 15500 万张图像和文本对构建的，采用了过滤和去重等技术。此外，还构建了 WAON-Bench 基准测试来评估数据集的有效性。

Result: WAON 数据集在 WAON-Bench 上比 ReLAION 更有效地提升模型性能，并在所有评估的基准测试中取得了更高的准确率。此外，基于 WAON 微调的模型在几个日本文化基准测试中达到了最先进的性能。

Conclusion: WAON 数据集在 Japanese 文化图像分类任务中表现出色，并且在多个基准测试中达到了最先进的性能。

Abstract: Large-scale and high-quality image-text pair datasets play an important role
in developing high-performing Vision-Language Models (VLMs). In this work, we
introduce WAON, a large-scale and high-quality Japanese image-text pair dataset
containing approximately 155 million examples, collected from Common Crawl. Our
dataset construction pipeline employs various techniques, including filtering
and deduplication, which have been shown to be effective in previous studies.
To evaluate its effectiveness, we also construct WAON-Bench, a manually curated
benchmark for Japanese cultural image classification, consisting of 374
classes. To assess the effectiveness of our dataset, we conduct experiments
using both WAON and the Japanese subset of ReLAION, one of the most widely used
vision-language datasets. We fine-tune SigLIP2, a strong multilingual model, on
both datasets. The results demonstrate that WAON enhances model performance on
WAON-Bench more efficiently than ReLAION and achieves higher accuracy across
all evaluated benchmarks. Furthermore, the model fine-tuned on WAON achieves
state-of-the-art performance on several Japanese cultural benchmarks. We
release our dataset, model, and code at https://speed1313.github.io/WAON.

</details>


### [125] [CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning](https://arxiv.org/abs/2510.22282)
*Tianhui Liu,Hetian Pang,Xin Zhang,Jie Feng,Yong Li,Pan Hui*

Main category: cs.CV

TL;DR: 本文提出CityRiSE框架，通过强化学习提升大型视觉语言模型在城市社会经济感知任务中的表现，取得了更好的预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型为城市社会经济感知提供了新机会，但它们在从视觉数据中进行准确且可解释的社会经济预测方面仍存在困难。因此，需要一种新的方法来克服这些限制并最大化大型视觉语言模型的潜力。

Method: 本文引入了CityRiSE框架，通过纯强化学习来推理城市社会经济状态，利用精心策划的多模态数据和可验证的奖励设计，引导大型视觉语言模型关注语义上有意义的视觉线索。

Result: 实验表明，CityRiSE通过出现的推理过程显著优于现有基线，提高了预测准确性以及在不同城市环境中的泛化能力，特别是在对未见过的城市和指标进行预测时。

Conclusion: 本文展示了将强化学习与大型视觉语言模型结合在城市社会经济感知中的潜力，强调了其可解释性和泛化能力。

Abstract: Harnessing publicly available, large-scale web data, such as street view and
satellite imagery, urban socio-economic sensing is of paramount importance for
achieving global sustainable development goals. With the emergence of Large
Vision-Language Models (LVLMs), new opportunities have arisen to solve this
task by treating it as a multi-modal perception and understanding problem.
However, recent studies reveal that LVLMs still struggle with accurate and
interpretable socio-economic predictions from visual data. To address these
limitations and maximize the potential of LVLMs, we introduce
\textbf{CityRiSE}, a novel framework for \textbf{R}eason\textbf{i}ng urban
\textbf{S}ocio-\textbf{E}conomic status in LVLMs through pure reinforcement
learning (RL). With carefully curated multi-modal data and verifiable reward
design, our approach guides the LVLM to focus on semantically meaningful visual
cues, enabling structured and goal-oriented reasoning for generalist
socio-economic status prediction. Experiments demonstrate that CityRiSE with
emergent reasoning process significantly outperforms existing baselines,
improving both prediction accuracy and generalization across diverse urban
contexts, particularly for prediction on unseen cities and unseen indicators.
This work highlights the promise of combining RL and LVLMs for interpretable
and generalist urban socio-economic sensing.

</details>


### [126] [Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views](https://arxiv.org/abs/2510.22672)
*Anna Deichler,Jonas Beskow*

Main category: cs.CV

TL;DR: Look and Tell是一个多模态数据集，用于研究视角间的指代通信，包含丰富的注释指代表达，旨在推动具身代理的发展。


<details>
  <summary>Details</summary>
Motivation: 为了研究不同空间表示（2D vs. 3D；ego vs. exo）对多模态接地的影响，需要一个包含丰富注释的指代表达的数据集。

Method: 使用Meta Project Aria智能眼镜和固定摄像头记录同步的注视、语音和视频，并结合3D场景重建。

Result: 数据集包含3.67小时的记录，包括2,707个丰富的注释指代表达。

Conclusion: Look and Tell数据集为研究视角间的指代通信提供了基准，有助于推进具身代理的发展。

Abstract: We introduce Look and Tell, a multimodal dataset for studying referential
communication across egocentric and exocentric perspectives. Using Meta Project
Aria smart glasses and stationary cameras, we recorded synchronized gaze,
speech, and video as 25 participants instructed a partner to identify
ingredients in a kitchen. Combined with 3D scene reconstructions, this setup
provides a benchmark for evaluating how different spatial representations (2D
vs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67
hours of recordings, including 2,707 richly annotated referential expressions,
and is designed to advance the development of embodied agents that can
understand and engage in situated dialogue.

</details>


### [127] [RoboSVG: A Unified Framework for Interactive SVG Generation with Multi-modal Guidance](https://arxiv.org/abs/2510.22684)
*Jiuniu Wang,Gongjie Zhang,Quanhao Qian,Junlong Gao,Deli Zhao,Ran Xu*

Main category: cs.CV

TL;DR: RoboSVG is a new framework for generating interactive SVGs using multiple modalities, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: SVGs are fundamental to digital design and robot control, encoding not only visual structure but also motion paths in interactive drawings. The need for a versatile and accurate method to generate SVGs guided by multiple modalities motivates this work.

Method: RoboSVG is a unified multimodal framework for generating interactive SVGs guided by textual, visual, and numerical signals. It produces multimodal guidance, synthesizes candidate SVGs through dedicated generation modules, and refines them under numerical guidance to yield high-quality outputs.

Result: Extensive experiments demonstrate that RoboSVG achieves superior query compliance and visual fidelity across tasks. The RoboDraw dataset enables systematic study of four tasks, including basic generation and interactive generation.

Conclusion: RoboSVG achieves superior query compliance and visual fidelity across tasks, establishing a new state of the art in versatile SVG generation.

Abstract: Scalable Vector Graphics (SVGs) are fundamental to digital design and robot
control, encoding not only visual structure but also motion paths in
interactive drawings. In this work, we introduce RoboSVG, a unified multimodal
framework for generating interactive SVGs guided by textual, visual, and
numerical signals. Given an input query, the RoboSVG model first produces
multimodal guidance, then synthesizes candidate SVGs through dedicated
generation modules, and finally refines them under numerical guidance to yield
high-quality outputs. To support this framework, we construct RoboDraw, a
large-scale dataset of one million examples, each pairing an SVG generation
condition (e.g., text, image, and partial SVG) with its corresponding
ground-truth SVG code. RoboDraw dataset enables systematic study of four tasks,
including basic generation (Text-to-SVG, Image-to-SVG) and interactive
generation (PartialSVG-to-SVG, PartialImage-to-SVG). Extensive experiments
demonstrate that RoboSVG achieves superior query compliance and visual fidelity
across tasks, establishing a new state of the art in versatile SVG generation.
The dataset and source code of this project will be publicly available soon.

</details>


### [128] [Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22694)
*Shu Zhao,Tianyi Shen,Nilesh Ahuja,Omesh Tickoo,Vijaykrishnan Narayanan*

Main category: cs.CV

TL;DR: 本文提出了一种改进的MRAG方法，通过引入Windsock模块和DANCE指令调优策略，提高了生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的MRAG方法存在静态检索策略、不灵活的模态选择和检索信息利用不足的问题，导致三个关键挑战：何时检索、选择什么模态以及如何有效利用检索信息。

Method: 引入了Windsock模块，用于决定检索必要性和模态选择，并提出了动态抗噪（DANCE）指令调优的自适应训练策略，还采用了自我评估方法将问答数据集转换为MRAG训练数据集。

Result: 实验结果表明，本文提出的方法在生成质量上显著提高，同时减少了检索时间。

Conclusion: 本文提出的的方法在生成质量上显著提高了17.07%，同时减少了8.95%的检索时间。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising
method to generate factual and up-to-date responses of Multimodal Large
Language Models (MLLMs) by incorporating non-parametric knowledge from external
knowledge bases. However, existing MRAG approaches suffer from static retrieval
strategies, inflexible modality selection, and suboptimal utilization of
retrieved information, leading to three critical challenges: determining when
to retrieve, what modality to incorporate, and how to utilize retrieved
information effectively. To address these challenges, we introduce Windsock, a
query-dependent module making decisions on retrieval necessity and modality
selection, effectively reducing computational overhead and improving response
quality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction
Tuning, an adaptive training strategy that enhances MLLMs' ability to utilize
retrieved information while maintaining robustness against noise. Moreover, we
adopt a self-assessment approach leveraging knowledge within MLLMs to convert
question-answering datasets to MRAG training datasets. Extensive experiments
demonstrate that our proposed method significantly improves the generation
quality by 17.07% while reducing 8.95% retrieval times.

</details>


### [129] [M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark](https://arxiv.org/abs/2510.23020)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 本文介绍了M$^3$T2IBench，一个大规模、多类别、多实例、多关系的评估基准，并提出了AlignScore指标，与人类评估高度一致。结果发现当前开源文本到图像模型在此基准上表现不佳，并提出了一种无需训练的后编辑方法来提升图像和文本的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估要么过于简单，特别是忽略了属于同一类别的多个不同实例的提示难度，要么引入的指标与人类评估的相关性不佳。

Method: 我们提出了Revise-Then-Enforce方法，以提高图像和文本的一致性。这是一种无需训练的后编辑方法，在广泛的扩散模型中显示出图像和文本一致性的改进。

Result: 我们引入了M$^3$T2IBench，这是一个大规模、多类别、多实例、多关系的评估基准，并提出了基于目标检测的评估指标AlignScore，该指标与人类评估高度一致。

Conclusion: 当前开源的文本到图像模型在这一具有挑战性的基准测试中表现不佳。

Abstract: Text-to-image models are known to struggle with generating images that
perfectly align with textual prompts. Several previous studies have focused on
evaluating image-text alignment in text-to-image generation. However, these
evaluations either address overly simple scenarios, especially overlooking the
difficulty of prompts with multiple different instances belonging to the same
category, or they introduce metrics that do not correlate well with human
evaluation. In this study, we introduce M$^3$T2IBench, a large-scale,
multi-category, multi-instance, multi-relation along with an
object-detection-based evaluation metric, $AlignScore$, which aligns closely
with human evaluation. Our findings reveal that current open-source
text-to-image models perform poorly on this challenging benchmark.
Additionally, we propose the Revise-Then-Enforce approach to enhance image-text
alignment. This training-free post-editing method demonstrates improvements in
image-text alignment across a broad range of diffusion models. \footnote{Our
code and data has been released in supplementary material and will be made
publicly available after the paper is accepted.}

</details>


### [130] [UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization](https://arxiv.org/abs/2510.23023)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 本文介绍了UniAIDet，一个统一且全面的基准，用于评估AI生成内容的检测方法，并探讨了泛化能力和检测与定位的关系。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型的迅速普及，数字图像的真实性已成为一个重要问题。现有的研究虽然提出了各种检测AI生成内容的方法，但当前的基准在覆盖多种生成模型和图像类别方面存在局限，常常忽略了端到端的图像编辑和艺术图像。

Method: 我们引入了UniAIDet，一个统一且全面的基准，包括照片和艺术图像。

Result: 我们使用UniAIDet对各种检测方法进行了全面评估，并回答了关于泛化能力和检测与定位关系的三个关键研究问题。

Conclusion: 我们的基准和分析为未来的研究提供了坚实的基础。

Abstract: With the rapid proliferation of image generative models, the authenticity of
digital images has become a significant concern. While existing studies have
proposed various methods for detecting AI-generated content, current benchmarks
are limited in their coverage of diverse generative models and image
categories, often overlooking end-to-end image editing and artistic images. To
address these limitations, we introduce UniAIDet, a unified and comprehensive
benchmark that includes both photographic and artistic images. UniAIDet covers
a wide range of generative models, including text-to-image, image-to-image,
image inpainting, image editing, and deepfake models. Using UniAIDet, we
conduct a comprehensive evaluation of various detection methods and answer
three key research questions regarding generalization capability and the
relation between detection and localization. Our benchmark and analysis provide
a robust foundation for future research.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [131] [From Social Division to Cohesion with AI Message Suggestions in Online Chat Groups](https://arxiv.org/abs/2510.21984)
*Faria Huq,Elijah L. Claggett,Hirokazu Shirado*

Main category: cs.SI

TL;DR: This paper explores the impact of LLM-driven messaging assistance on social cohesion in diverse online discussions, finding that the design of personalization significantly affects group dynamics.


<details>
  <summary>Details</summary>
Motivation: To investigate the societal impact of LLM-driven messaging assistance on social cohesion in diverse societies, particularly in online communication.

Method: An online experiment with 557 participants who engaged in multi-round discussions on politically controversial topics while freely reconfiguring their discussion groups. Some participants received real-time message suggestions generated by an LLM, either personalized to the individual or adapted to their group context.

Result: Subtle shifts in linguistic style during communication, mediated by AI assistance, can scale up to reshape collective structures. Individual-focused assistance leads users to segregate into like-minded groups, while relational assistance that incorporates group members' stances enhances cohesion through more receptive exchanges.

Conclusion: AI-mediated communication can support social cohesion in diverse groups, but outcomes critically depend on how personalization is designed.

Abstract: Social cohesion is difficult to sustain in societies marked by opinion
diversity, particularly in online communication. As large language model
(LLM)-driven messaging assistance becomes increasingly embedded in these
contexts, it raises critical questions about its societal impact. We present an
online experiment with 557 participants who engaged in multi-round discussions
on politically controversial topics while freely reconfiguring their discussion
groups. In some conditions, participants received real-time message suggestions
generated by an LLM, either personalized to the individual or adapted to their
group context. We find that subtle shifts in linguistic style during
communication, mediated by AI assistance, can scale up to reshape collective
structures. While individual-focused assistance leads users to segregate into
like-minded groups, relational assistance that incorporates group members'
stances enhances cohesion through more receptive exchanges. These findings
demonstrate that AI-mediated communication can support social cohesion in
diverse groups, but outcomes critically depend on how personalization is
designed.

</details>


### [132] [Modeling Political Discourse with Sentence-BERT and BERTopic](https://arxiv.org/abs/2510.22904)
*Margarida Mendonca,Alvaro Figueira*

Main category: cs.SI

TL;DR: 本研究利用BERTopic和道德基础理论分析了社交媒体上政治主题的演变，发现细粒度主题的持久性较低，而道德基础对主题的持续时间有重要影响。


<details>
  <summary>Details</summary>
Motivation: 社交媒体重塑了政治话语，为政治家提供了直接参与的平台，同时也加剧了极化和意识形态分歧。因此，需要一种方法来跟踪动态主题变化并衡量其与道德价值的关联。

Method: 本研究引入了一种结合BERTopic主题建模和道德基础理论（MFT）的新主题演化框架，用于分析117届美国国会期间Twitter活动中的政治主题的持久性和道德维度。

Result: 研究发现，尽管总体主题保持稳定，但细粒度主题往往迅速消失，限制了其长期影响。此外，道德基础在主题持久性中起着关键作用，其中关怀和忠诚度在持久主题中占主导地位，而党派差异则体现在不同的道德框架策略上。

Conclusion: 本研究为理解社交媒体上的道德驱动主题演变提供了一种可扩展且可解释的方法，有助于社会网络分析和计算政治话语领域的发展。

Abstract: Social media has reshaped political discourse, offering politicians a
platform for direct engagement while reinforcing polarization and ideological
divides. This study introduces a novel topic evolution framework that
integrates BERTopic-based topic modeling with Moral Foundations Theory (MFT) to
analyze the longevity and moral dimensions of political topics in Twitter
activity during the 117th U.S. Congress. We propose a methodology for tracking
dynamic topic shifts over time and measuring their association with moral
values and quantifying topic persistence. Our findings reveal that while
overarching themes remain stable, granular topics tend to dissolve rapidly,
limiting their long-term influence. Moreover, moral foundations play a critical
role in topic longevity, with Care and Loyalty dominating durable topics, while
partisan differences manifest in distinct moral framing strategies. This work
contributes to the field of social network analysis and computational political
discourse by offering a scalable, interpretable approach to understanding
moral-driven topic evolution on social media.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [133] [Surface Reading LLMs: Synthetic Text and its Styles](https://arxiv.org/abs/2510.22162)
*Hannes Bajohr*

Main category: cs.CY

TL;DR: 本文提出了一种符号学方法，以关注大型语言模型在人类交流中的表面完整性，并通过案例研究展示了它们作为文化行为者如何改变意义的产生和传播。


<details>
  <summary>Details</summary>
Motivation: 尽管ML发展可能达到瓶颈，但大型语言模型的社会影响不在于接近超级智能，而在于生成与人类写作难以区分的文本表面。Critical AI Studies提供了必要的材料和社会技术批判，但可能忽略了LLM如何现象学地重塑意义制作。

Method: 本文提出了一个“表面完整性”的符号学概念，以关注LLM在人类交流中刻写自身的即时层面。通过两个案例研究，分析合成文本的风格特征。

Result: 通过两个案例研究，本文展示了关注风格作为符号现象如何揭示LLM作为文化行为者，改变了当代话语中意义出现和传播的条件。

Conclusion: 本文认为，关注风格作为符号现象可以揭示LLM作为文化行为者如何改变当代话语中意义出现和传播的条件，而无需涉及机器意识的问题。

Abstract: Despite a potential plateau in ML advancement, the societal impact of large
language models lies not in approaching superintelligence but in generating
text surfaces indistinguishable from human writing. While Critical AI Studies
provides essential material and socio-technical critique, it risks overlooking
how LLMs phenomenologically reshape meaning-making. This paper proposes a
semiotics of "surface integrity" as attending to the immediate plane where LLMs
inscribe themselves into human communication. I distinguish three knowledge
interests in ML research (epistemology, epist\=em\=e, and epistemics) and argue
for integrating surface-level stylistic analysis alongside depth-oriented
critique. Through two case studies examining stylistic markers of synthetic
text, I argue how attending to style as a semiotic phenomenon reveals LLMs as
cultural actors that transform the conditions of meaning emergence and
circulation in contemporary discourse, independent of questions about machine
consciousness.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [134] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 本文介绍了一种新的方法来生成困难且多样的错误，以提高基于语言模型的软件工程代理的训练效果。


<details>
  <summary>Details</summary>
Motivation: 高质量的错误对于训练下一代基于语言模型的软件工程（SWE）代理至关重要。以往的方法往往通过有意生成错误（例如通过引入现有代码的局部扰动）来产生分布外效应，这并不反映现实的开发过程。

Method: 我们引入了一种新的方法来合成生成困难且多样的错误。我们的方法指导SWE代理将一个功能引入代码库，从而可能无意中破坏测试，导致错误。

Result: 我们的错误提供了更高效的训练数据，比其他错误数据集高出2%，并且使用一半的训练数据（1.2k vs. 3k错误）。我们还训练了FrogBoss和FrogMini模型，在SWE-bench Verified上取得了最先进的结果。

Conclusion: 我们的方法生成的错误提供了更高效的训练数据，优于其他错误数据集，并且在SWE-bench Verified上取得了最先进的结果。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [135] [Beyond IVR Touch-Tones: Customer Intent Routing using LLMs](https://arxiv.org/abs/2510.21715)
*Sergio Rojas-Galeano*

Main category: cs.HC

TL;DR: 本文提出了一种基于LLM的方法，用于解决IVR路由中的问题。通过生成用户意图并测试不同的提示设计，结果显示扁平化路径在准确率上优于描述性格式。研究证明了LLMs在提升客户服务体验方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于对刚性的触控音Interactive Voice Response (IVR)系统的广泛不满，需要更直接和直观的语言交互。虽然语音技术是必要的，但关键挑战在于将用户表述中的意图路由到IVR菜单路径，而这一任务中大型语言模型（LLMs）显示出强大的潜力。然而，由于实际IVR结构和交互通常是专有的，进展受到数据稀缺的限制。

Method: 我们提出了一种基于LLM的方法，使用三个不同的模型合成一个现实的23节点IVR结构，生成920个用户意图，并执行路由任务。我们评估了两种提示设计：描述性分层菜单和扁平化路径表示法，在基础和增强数据集上进行测试。

Result: 结果表明，扁平化路径在基础数据集上的准确率 consistently 高于描述性格式，达到89.13% compared to 81.30%。同时，增强引入了语言噪声，略微降低了性能。混淆矩阵分析进一步表明，低性能的路径可能不仅反映了模型的局限性，还反映了菜单设计中的冗余。

Conclusion: 我们的研究结果展示了LLMs可以通过更流畅、更无缝的用户体验来实现IVR路由的可行性，从而将客户服务向前推进了一步。

Abstract: Widespread frustration with rigid touch-tone Interactive Voice Response (IVR)
systems for customer service underscores the need for more direct and intuitive
language interaction. While speech technologies are necessary, the key
challenge lies in routing intents from user phrasings to IVR menu paths, a task
where Large Language Models (LLMs) show strong potential. Progress, however, is
limited by data scarcity, as real IVR structures and interactions are often
proprietary. We present a novel LLM-based methodology to address this gap.
Using three distinct models, we synthesized a realistic 23-node IVR structure,
generated 920 user intents (230 base and 690 augmented), and performed the
routing task. We evaluate two prompt designs: descriptive hierarchical menus
and flattened path representations, across both base and augmented datasets.
Results show that flattened paths consistently yield higher accuracy, reaching
89.13% on the base dataset compared to 81.30% with the descriptive format,
while augmentation introduces linguistic noise that slightly reduces
performance. Confusion matrix analysis further suggests that low-performing
routes may reflect not only model limitations but also redundancies in menu
design. Overall, our findings demonstrate proof-of-concept that LLMs can enable
IVR routing through a smoother, more seamless user experience -- moving
customer service one step ahead of touch-tone menus.

</details>


### [136] [When Robots Say No: Temporal Trust Recovery Through Explanation](https://arxiv.org/abs/2510.21716)
*Nicola Webb,Zijun Huang,Sanja Milivojevic,Chris Baber,Edmund R. Hunt*

Main category: cs.HC

TL;DR: 本研究探讨了在高风险人机协作任务中，机器人拒绝立即帮助用户时对用户信任的影响，并发现提供解释有助于恢复信任。


<details>
  <summary>Details</summary>
Motivation: 在高风险任务中，如搜索和救援以及灭火，移动机器人可以提供显著的优势。在人机团队（HRT）中，机器人可以有效地帮助搜索危险建筑。用户信任是HRT的关键促成因素，但在任务过程中，信任可能会受到损害。

Method: 我们进行了一项基于计算机的研究，调查了在高风险人机协作场景中的任务中信任动态。参与者（n = 38）与机器人队友一起玩了一个互动的消防游戏，其中由于机器人拒绝立即帮助用户而发生信任违规。

Result: 当机器人提供拒绝帮助的解释时，信任可以随着时间的推移得到更好的恢复，尽管初始下降与没有提供解释的基线条件相当。

Conclusion: 我们的研究结果表明，当机器人提供拒绝帮助的解释时，信任可以在一段时间内得到恢复，尽管初始下降与没有提供解释的情况相当。

Abstract: Mobile robots with some degree of autonomy could deliver significant
advantages in high-risk missions such as search and rescue and firefighting.
Integrated into a human-robot team (HRT), robots could work effectively to help
search hazardous buildings. User trust is a key enabler for HRT, but during a
mission, trust can be damaged. With distributed situation awareness, such as
when team members are working in different locations, users may be inclined to
doubt a robot's integrity if it declines to immediately change its priorities
on request. In this paper, we present the results of a computer-based study
investigating on-mission trust dynamics in a high-stakes human-robot teaming
scenario. Participants (n = 38) played an interactive firefighting game
alongside a robot teammate, where a trust violation occurs owing to the robot
declining to help the user immediately. We find that when the robot provides an
explanation for declining to help, trust better recovers over time, albeit
following an initial drop that is comparable to a baseline condition where an
explanation for refusal is not provided. Our findings indicate that trust can
vary significantly during a mission, notably when robots do not immediately
respond to user requests, but that this trust violation can be largely
ameliorated over time if adequate explanation is provided.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [137] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang,Herbert Woisetscläger*

Main category: cs.AI

TL;DR: 本文介绍了SIGN，一种通过轻量级结构引导惯例形成的命名游戏。结果表明，基于模式的通信比自然语言更快地达成一致，这表明最小的结构可以作为多智能体协调的有效控制手段。


<details>
  <summary>Details</summary>
Motivation: 现实世界的人工智能系统正在处理越来越复杂的问题，通常通过大型语言模型（LLM）代理之间的交互来解决。当这些代理发展出不一致的惯例时，协调可能会失败。因此，需要可靠且一致的通信，并且随着系统规模的增长，可扩展性是一个核心问题。

Method: 引入了Schema-Induced Games for Naming (SIGN)，一种研究轻量级结构如何引导惯例形成的命名游戏。

Result: 与不受约束的自然语言相比，基于模式的通信收敛速度更快，一致性高达5.8倍。

Conclusion: 最小的结构可以作为高效多智能体协调的简单控制旋钮，并指向命名游戏之外的更广泛应用。

Abstract: Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [138] [GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.21881)
*Nannan Shi,Chuanyu Qin,Shipeng Song,Man Luo*

Main category: cs.AI

TL;DR: 本文提出了GeoThoughts数据集和GeoThought-MLLM模型，用于提升几何推理能力，并展示了其在不同场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的几何问题解决数据集存在规模不足、多样性不够和缺乏明确推理轨迹的问题，这限制了模型的有效训练。因此，需要一个更全面的几何推理语料库来提升模型性能。

Method: 开发了GeoThoughts数据集，包含两个子集Geo-Thought-6K和Geo-Thought-Augmented-10K，每个条目包括视觉描述、逐步解决方案、显式推理链、反思步骤和最终答案。基于此数据集，开发了GeoThought-MLLM模型，该模型在解决问题时生成详细的思考过程。

Result: GeoThought-MLLM模型在几何任务中优于现有基准，表明使用我们的思维链数据集可以提高几何推理能力。此外，通过CoT纠正错误，模型能够产生正确的答案。

Conclusion: 通过使用GeoThought-MLLM模型和GeoThoughts数据集，我们证明了训练具有思维链的模型可以提高几何推理能力，并且在领域内和领域外设置中都表现良好。此外，我们发现错误主要来源于对数学概念的误解或空间判断错误，并通过CoT纠正这些错误以获得正确答案。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities
in text-based mathematical problem solving; however, when adapted to visual
reasoning tasks, particularly geometric problem solving, their performance
substantially declines because geometric problems present unique challenges.
Specifically, these challenges stem from two key factors: first, the intrinsic
complexity of geometry requiring detailed image comprehension and multi-step
reasoning, and second, the limitations of existing datasets which lack
sufficient scale, diversity, and explicit reasoning traces, consequently
hindering effective model training. To address these challenges, we developed
the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two
subsets: Geo-Thought-6K with 6,243 samples and its augmented version
Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual
descriptions, step-by-step solutions, explicit reasoning chains, reflection
steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a
mathematical reasoning multimodal model that generates detailed thinking
processes during problem-solving. Our model outperforms existing benchmarks in
geometric tasks, demonstrating that training with our Chain-of-Thought dataset
improves geometric reasoning capabilities across both in-domain and
out-of-domain settings. Finally, we analyze failure cases and observe that
errors primarily arise from incorrect interpretation of mathematical concepts
or spatial misjudgment. By invoking CoT to correct these mistakes, the model
produces correct answers.

</details>


### [139] [Performance Trade-offs of Optimizing Small Language Models for E-Commerce](https://arxiv.org/abs/2510.21970)
*Josip Tomo Licardo,Nikola Tankovic*

Main category: cs.AI

TL;DR: 本文探讨了小型开放权重模型在电子商务等特定任务中的可行性，通过优化方法实现高性能和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 由于高计算成本、延迟和运营费用，领先的商业模型在专用任务（如电子商务）中的部署受到阻碍。因此，研究更小、开放权重模型作为资源高效的替代方案。

Method: 本文提出了一种优化1B参数Llama 3.2模型的方法，用于多语言电子商务意图识别。模型使用QLoRA在合成生成的数据集上进行了微调，并应用了后训练量化技术，创建了GPU优化（GPTQ）和CPU优化（GGUF）版本。

Result: 专门的1B模型实现了99%的准确率，与显著更大的GPT-4.1模型性能相当。4位GPTQ减少了41%的VRAM使用量，但在旧GPU架构（NVIDIA T4）上推理速度反而减慢了82%。GGUF格式在CPU上实现了高达18倍的推理吞吐量提升，并减少了超过90%的RAM消耗。

Conclusion: 小型且经过适当优化的开放权重模型不仅是可行的，而且是特定领域应用更合适的替代方案，以远低于计算成本的代价提供最先进的准确性。

Abstract: Large Language Models (LLMs) offer state-of-the-art performance in natural
language understanding and generation tasks. However, the deployment of leading
commercial models for specialized tasks, such as e-commerce, is often hindered
by high computational costs, latency, and operational expenses. This paper
investigates the viability of smaller, open-weight models as a
resource-efficient alternative. We present a methodology for optimizing a
one-billion-parameter Llama 3.2 model for multilingual e-commerce intent
recognition. The model was fine-tuned using Quantized Low-Rank Adaptation
(QLoRA) on a synthetically generated dataset designed to mimic real-world user
queries. Subsequently, we applied post-training quantization techniques,
creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results
demonstrate that the specialized 1B model achieves 99% accuracy, matching the
performance of the significantly larger GPT-4.1 model. A detailed performance
analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ
reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older
GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF
formats on a CPU achieved a speedup of up to 18x in inference throughput and a
reduction of over 90% in RAM consumption compared to the FP16 baseline. We
conclude that small, properly optimized open-weight models are not just a
viable but a more suitable alternative for domain-specific applications,
offering state-of-the-art accuracy at a fraction of the computational cost.

</details>


### [140] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen,Xinni Zhang,Yifei Zhang,Yangning Li,Henry Peng Zou,Chunyu Miao,Weizhi Zhang,Xue Liu,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文讨论了BCI的局限性，并提出了一个新范式——脑-代理协作，以解决当前技术障碍和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨BCI的局限性，并提出一种新的范式，以克服当前的技术障碍和伦理问题。

Method: 本文通过分析BCI和LLM的整合以及代理AI在其中的角色，提出了脑-代理协作的概念。

Result: 本文提出了脑-代理协作的概念，并强调了在伦理数据处理、模型可靠性以及人机协作框架方面的必要性。

Conclusion: 本文认为，该领域正处于从BCI向脑-代理协作（BAC）范式扩展的阶段，并强调将代理重新定位为智能辅助的主动和协作伙伴，而不是被动的脑信号数据处理器。

Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [141] [PACR: Progressively Ascending Confidence Reward for LLM Reasoning](https://arxiv.org/abs/2510.22255)
*Eunseop Yoon,Hee Suk Yoon,Jaehyun Jang,SooHwan Eom,Qi Dai,Chong Luo,Mark A. Hasegawa-Johnson,Chang D. Yoo*

Main category: cs.AI

TL;DR: 本文提出了一种新的奖励机制PACR，用于增强强化学习中的探索效率。


<details>
  <summary>Details</summary>
Motivation: RLVR的稀疏、基于结果的奖励无法为中间步骤提供指导，从而减缓了探索。

Method: 我们提出了Progressively Ascending Confidence Reward (PACR)，这是一种从模型不断变化的信念中直接计算出的密集、模型内在奖励。

Result: PACR加速了探索，用更少的轨迹达到奖励饱和，并在多个基准测试中取得了改进。

Conclusion: 我们的结果表明，密集的、模型内在的塑造信号可以使RLVR训练更有效和可靠。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly
improved LLM reasoning, but its sparse, outcome-based reward provides no
guidance for intermediate steps, slowing exploration. We propose Progressively
Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed
directly from the model's evolving belief in the correct answer. PACR encodes
the inductive bias that, along a well-formed reasoning trajectory, the
probability of the ground-truth answer should have a generally ascending trend.
We provide empirical and theoretical analysis validating that such an inductive
bias constrains the exploration search space to regions richer in logically
sound reasoning. We demonstrate that PACR accelerates exploration, reaches
reward saturation with fewer trajectories, and yields improvements on multiple
benchmarks. Our results suggest that dense, model-intrinsic shaping signals can
make RLVR training more effective and reliable.

</details>


### [142] [VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription](https://arxiv.org/abs/2510.22295)
*Quoc Anh Nguyen,Bernard Cheng,Kelvin Soh*

Main category: cs.AI

TL;DR: 本文构建了首个越南语自动歌词转录数据集VietLyrics，并通过微调Whisper模型实现了更好的性能，推动了越南语音乐计算研究。


<details>
  <summary>Details</summary>
Motivation: 由于越南语音乐的声调复杂性和方言差异，以及缺乏专门的数据集，自动歌词转录研究进展缓慢。因此，本文旨在解决这些问题。

Method: 本文首先构建了第一个大规模越南语自动歌词转录数据集VietLyrics，然后对现有的ASR方法进行了评估，并对Whisper模型进行了微调以提高性能。

Result: 评估显示现有ASR方法存在显著局限性，而微调后的Whisper模型在性能上优于现有的多语言ALT系统，如LyricWhiz。

Conclusion: 本文展示了VietLyrics数据集和基于该数据集微调的Whisper模型在越南语自动歌词转录任务中的优越性能，同时公开了数据集和模型以推动越南语音乐计算研究。

Abstract: Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique
challenges due to its tonal complexity and dialectal variations, but remains
largely unexplored due to the lack of a dedicated dataset. Therefore, we
curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising
647 hours of songs with line-level aligned lyrics and metadata to address these
issues. Our evaluation of current ASRbased approaches reveal significant
limitations, including frequent transcription errors and hallucinations in
non-vocal segments. To improve performance, we fine-tuned Whisper models on the
VietLyrics dataset, achieving superior results compared to existing
multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics
and our models, aiming to advance Vietnamese music computing research while
demonstrating the potential of this approach for ALT in low-resource language
and music.

</details>


### [143] [DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry](https://arxiv.org/abs/2510.22340)
*Changti Wu,Shijie Lian,Zihao Liu,Lei Zhang,Laurence Tianruo Yang,Kai Chen*

Main category: cs.AI

TL;DR: 本文介绍了DynaSolidGeo，这是第一个用于评估视觉-语言模型真实空间推理的动态基准。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态数学推理基准主要关注2D平面几何，依赖静态数据集，评估模型仅以最终答案为准，忽略了推理过程。

Method: 通过半自动注释流程构建了DynaSolidGeo数据集，包含503个专家精选的初始问题，可以动态生成大量多模态文本-视觉实例。

Result: 实验显示了开放源代码和封闭源代码VLMs之间的显著性能差距，动态设置下的严重退化以及在需要高级空间智能的任务上的表现不佳。

Conclusion: 实验结果揭示了在动态设置中性能差距大、严重退化以及需要高水平空间智能的任务表现不佳的问题。

Abstract: Solid geometry problem solving demands spatial mathematical reasoning that
integrates spatial intelligence and symbolic reasoning. However, most existing
multimodal mathematical reasoning benchmarks focus primarily on 2D plane
geometry, rely on static datasets prone to data contamination and memorization,
and evaluate models solely by final answers, overlooking the reasoning process.
To address these limitations, we introduce DynaSolidGeo, the first dynamic
benchmark for evaluating genuine spatial reasoning in Vision-Language Models
(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo
contains 503 expert-curated seed questions that can, in principle, dynamically
generate an unbounded number of diverse multimodal text-visual instances.
Beyond answer accuracy, we incorporate process evaluation based on
expert-annotated reasoning chains to measure logical validity and causal
coherence. Experiments across representative open-source and closed-source VLMs
reveal large performance gaps, severe degradation in dynamic settings, and poor
performance on tasks requiring high-level spatial intelligence, such as mental
rotation and visualization. The code and dataset are available at
\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.

</details>


### [144] [Reasoning Models Reason Well, Until They Don't](https://arxiv.org/abs/2510.22371)
*Revanth Rameshkumar,Jimson Huang,Yunxin Sun,Fei Xia,Abulhair Saparov*

Main category: cs.AI

TL;DR: 本文分析了大型推理模型（LRMs）在复杂推理任务中的表现，发现它们在足够复杂的任务中性能急剧下降且无法泛化，尽管在现实世界的应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的研究表明，当推理问题超过适度复杂度时，变压器和LLMs会灾难性地失败，但一些研究声称LRMs能够进行广义推理并在推理密集型领域如数学、物理、医学和法律中实现创新。然而，我们发现现有基准的实际复杂度有限，因此需要更深入的分析。

Method: 我们通过重新审视大型推理模型（LRMs）来验证这些发现，这些模型是通过对逐步论证和自我验证的激励进行微调的大规模语言模型。我们开发了一个新的数据集DeepRD，并创建了一个生成过程来产生无限数量的可扩展复杂度的例子。

Result: 我们发现LRMs在足够复杂的任务中性能急剧下降且无法泛化。此外，我们发现大多数现实世界的例子位于LRMs的成功范围内，但长尾部分暴露了显著的失败潜力。

Conclusion: 我们的分析突显了LRMs的短期实用性，同时强调了需要新的方法，以超越训练分布中示例的复杂性。

Abstract: Large language models (LLMs) have shown significant progress in reasoning
tasks. However, recent studies show that transformers and LLMs fail
catastrophically once reasoning problems exceed modest complexity. We revisit
these findings through the lens of large reasoning models (LRMs) -- LLMs
fine-tuned with incentives for step-by-step argumentation and
self-verification. LRM performance on graph and reasoning benchmarks such as
NLGraph seem extraordinary, with some even claiming they are capable of
generalized reasoning and innovation in reasoning-intensive fields such as
mathematics, physics, medicine, and law. However, by more carefully scaling the
complexity of reasoning problems, we show existing benchmarks actually have
limited complexity. We develop a new dataset, the Deep Reasoning Dataset
(DeepRD), along with a generative process for producing unlimited examples of
scalable complexity. We use this dataset to evaluate model performance on graph
connectivity and natural language proof planning. We find that the performance
of LRMs drop abruptly at sufficient complexity and do not generalize. We also
relate our LRM results to the distributions of the complexities of large,
real-world knowledge graphs, interaction graphs, and proof datasets. We find
the majority of real-world examples fall inside the LRMs' success regime, yet
the long tails expose substantial failure potential. Our analysis highlights
the near-term utility of LRMs while underscoring the need for new methods that
generalize beyond the complexity of examples in the training distribution.

</details>


### [145] [Modeling Hierarchical Thinking in Large Reasoning Models](https://arxiv.org/abs/2510.22437)
*G M Shahariar,Ali Nazari,Erfan Shayegani,Nael Abu-Ghazaleh*

Main category: cs.AI

TL;DR: 本文提出了一种基于有限状态机（FSM）的分析方法，用于理解和解释大型推理模型（LRMs）的推理过程。该方法揭示了不同的推理模式和潜在的不足之处，为评估和改进LLM的推理能力提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: 理解LRMs的推理能力是一个困难的开放问题，具有重要的应用价值，例如改进训练和理解鲁棒性。本文旨在提供一种系统的方法来分析、解释和可视化不同模型如何解决问题。

Method: 本文采用了一种无记忆的有限状态机（FSM）形式来近似LRM的层次推理动态，并识别出一系列离散的推理状态，如初始化、演绎、增强策略、不确定性估计、回溯和最终结论。通过将每个步骤标注这些状态，可以将推理轨迹表示为状态图中的转移序列。

Result: 实验结果表明，基于FSM的分析能够揭示出不同的推理模式和潜在的不足之处，为评估和改进LLM的推理能力提供了新的视角。

Conclusion: 本文提出了一种基于有限状态机（FSM）的分析方法，用于理解和解释大型推理模型（LRMs）的推理过程。该方法揭示了不同的推理模式和潜在的不足之处，为评估和改进LLM的推理能力提供了新的视角。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
when they generate step-by-step solutions, known as chain-of-thought (CoT)
reasoning. When trained to using chain-of-thought reasoning examples, the
resulting models (called Large Reasoning Models, or LRMs) appear to learn
hierarchical thinking strategies similar to those used by humans. However,
understanding LRMs emerging reasoning capabilities remains a difficult open
problem, with many potential important applications including improving
training and understanding robustness. In this paper, we adopt a memoryless
Finite State Machine formulation to approximate LRM's emerging hierarchical
reasoning dynamics as a structured, interpretable abstraction. We identify a
small set of discrete reasoning states including - initialization, deduction,
augmentation-strategy, uncertainty-estimation, backtracking, and
final-conclusion that capture the high-level states present in the model's
reasoning process. By annotating each step of a model's CoT with these states,
we can represent the reasoning trajectory as a transition sequence through the
state graph. This FSM formulation provides a systematic way to analyze,
interpret and visualize how different models approach problems. We describe the
FSM model, provide examples of CoT annotations under this scheme, and discuss
how it can shed light on differences between available models in their approach
to reasoning. Our results demonstrate that this FSM-based analysis reveals
distinct reasoning patterns and potential shortcomings, offering a new lens to
evaluate and improve LLM reasoning.

</details>


### [146] [OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models](https://arxiv.org/abs/2510.22535)
*Hao Zheng,Zirui Pang,Ling li,Zhijie Deng,Yuhan Pu,Zhaowei Zhu,Xiaobo Xia,Jiaheng Wei*

Main category: cs.AI

TL;DR: 本文提出OFFSIDE基准，用于评估多模态大语言模型的虚假信息遗忘能力，并揭示了当前方法的显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM的机器遗忘（MU）基准存在图像多样性不足、潜在不准确性以及评估场景不足的问题，无法捕捉现实应用的复杂性。

Method: 引入OFFSIDE，这是一个基于足球转会谣言的多模态大语言模型（MLLM）虚假信息遗忘评估基准。

Result: 评估多个基线后得出关键发现：(1) 单模态方法在多模态谣言上失败；(2) 遗忘效果主要由灾难性遗忘驱动；(3) 所有方法在“视觉谣言”上表现不佳；(4) 遗忘的谣言可以轻松恢复；(5) 所有方法都容易受到提示攻击。

Conclusion: 这些结果揭示了当前方法中的重大漏洞，强调了需要更稳健的多模态遗忘解决方案。

Abstract: Advances in Multimodal Large Language Models (MLLMs) intensify concerns about
data privacy, making Machine Unlearning (MU), the selective removal of learned
information, a critical necessity. However, existing MU benchmarks for MLLMs
are limited by a lack of image diversity, potential inaccuracies, and
insufficient evaluation scenarios, which fail to capture the complexity of
real-world applications. To facilitate the development of MLLMs unlearning and
alleviate the aforementioned limitations, we introduce OFFSIDE, a novel
benchmark for evaluating misinformation unlearning in MLLMs based on football
transfer rumors. This manually curated dataset contains 15.68K records for 80
players, providing a comprehensive framework with four test sets to assess
forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports
advanced settings like selective unlearning and corrective relearning, and
crucially, unimodal unlearning (forgetting only text data). Our extensive
evaluation of multiple baselines reveals key findings: (1) Unimodal methods
(erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning
efficacy is largely driven by catastrophic forgetting; (3) All methods struggle
with "visual rumors" (rumors appear in the image); (4) The unlearned rumors can
be easily recovered and (5) All methods are vulnerable to prompt attacks. These
results expose significant vulnerabilities in current approaches, highlighting
the need for more robust multimodal unlearning solutions. The code is available
at
\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.

</details>


### [147] [ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs](https://arxiv.org/abs/2510.22590)
*Yassir Lairgi,Ludovic Moncla,Khalid Benabdeslem,Rémy Cazabet,Pierre Cléau*

Main category: cs.AI

TL;DR: ATOM is a few-shot approach that builds and updates Temporal Knowledge Graphs from unstructured text, improving exhaustivity, stability, and latency.


<details>
  <summary>Details</summary>
Motivation: Traditional static knowledge graph construction overlooks the dynamic and time-sensitive nature of real-world data, while recent zero- or few-shot approaches suffer from instability and incomplete coverage. ATOM aims to address these challenges by enabling continuous updates to TKGs from unstructured text.

Method: ATOM is a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts by splitting input documents into atomic facts, constructing atomic TKGs with dual-time modeling, and merging them in parallel.

Result: ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, showing strong scalability potential for dynamic TKG construction.

Conclusion: ATOM demonstrates strong scalability potential for dynamic TKG construction, with significant improvements in exhaustivity, stability, and latency reduction compared to baseline methods.

Abstract: In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.

</details>


### [148] [Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration](https://arxiv.org/abs/2510.22679)
*Yuval Kainan,Shaked Zychlinski*

Main category: cs.AI

TL;DR: 本文提出了一种简单但非常有效的方法，用于在仅生成一个步骤后检测大型语言模型（LLMs）生成的锅炉板响应。实验表明，第一个生成的标记的日志概率分布是分类整个后续响应性质的强大信号。使用轻量级k-NN分类器，我们实现了高精度预测响应是否为实质性答案或一种锅炉板响应，包括用户指定的拒绝。这种方法可以显著节省计算成本，并提供一条直接通往更高效和可持续的LLM部署的路径。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）经常耗费大量的计算资源生成锅炉板响应，如拒绝、简单的认可和随意的问候，这增加了不必要的成本和延迟。为了应对这种低效率，本文提出了一个简单但非常有效的方法，用于在仅生成一个步骤后检测此类响应。

Method: 本文提出了一种简单但非常有效的方法，用于在仅生成一个步骤后检测此类响应。实验表明，第一个生成的标记的日志概率分布是分类整个后续响应性质的强大信号。使用轻量级k-NN分类器，我们实现了高精度预测响应是否为实质性答案或一种锅炉板响应，包括用户指定的拒绝。

Result: 实验结果表明，第一个生成的标记的日志概率分布是分类整个后续响应性质的强大信号。使用轻量级k-NN分类器，我们实现了高精度预测响应是否为实质性答案或一种锅炉板响应，包括用户指定的拒绝。

Conclusion: 本文提出了一种简单但非常有效的方法，用于在仅生成一个步骤后检测此类响应。实验表明，第一个生成的标记的日志概率分布是分类整个后续响应性质的强大信号。使用轻量级k-NN分类器，我们实现了高精度预测响应是否为实质性答案或一种锅炉板响应，包括用户指定的拒绝。主要含义是一种实用的、计算上微不足道的技术，通过启用早期终止或重定向到较小的模型来优化LLM推理，从而显著节省计算成本。这项工作提供了一条直接通往更高效和可持续的LLM部署的路径。

Abstract: Large Language Models (LLMs) often expend significant computational resources
generating boilerplate responses, such as refusals, simple acknowledgements and
casual greetings, which adds unnecessary cost and latency. To address this
inefficiency, we propose a simple yet highly effective method for detecting
such responses after only a single generation step. We demonstrate that the
log-probability distribution of the first generated token serves as a powerful
signal for classifying the nature of the entire subsequent response. Our
experiments, conducted across a diverse range of small, large, and
reasoning-specialized models, show that the first-token log-probability vectors
form distinctly separable clusters for different response types. Using a
lightweight k-NN classifier, we achieve high accuracy in predicting whether a
response will be a substantive answer or a form of boilerplate response,
including user-specified refusals. The primary implication is a practical,
computationally trivial technique, optimizing LLM inference by enabling early
termination or redirection to a smaller model, thereby yielding significant
savings in computational cost. This work presents a direct path toward more
efficient and sustainable LLM deployment.

</details>


### [149] [Critical Insights into Leading Conversational AI Models](https://arxiv.org/abs/2510.22729)
*Urja Kohli,Aditi Singh,Arun Sharma*

Main category: cs.AI

TL;DR: 本文比较了五个顶级大语言模型，分析了它们在性能、伦理和可用性方面的差异，并得出结论：每个模型都有其独特的优势，用户应根据自身需求选择最合适的模型。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型正在改变企业使用软件、人们生活和行业运作的方式，因此有必要了解每个模型在性能、道德行为和可用性方面的差异。

Method: 分析五个顶级LLM：Google的Gemini、High-Flyer的DeepSeek、Anthropic的Claude、OpenAI的GPT模型和Meta的LLaMA，通过性能与准确性、伦理与偏见缓解以及可用性与集成三个重要因素进行比较。

Result: Claude在道德推理方面表现良好，Gemini在多模态能力方面更优，并具有强大的伦理框架。DeepSeek在基于事实的推理方面表现出色，LLaMA适用于开放应用，而ChatGPT则在使用上表现出平衡的性能。

Conclusion: 这些模型在性能、易用性和伦理处理方面有所不同，因此用户应根据自身需求选择最适合的模型。

Abstract: Big Language Models (LLMs) are changing the way businesses use software, the
way people live their lives and the way industries work. Companies like Google,
High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial
to look at how each model is different in terms of performance, moral behaviour
and usability, as these differences are based on the different ideas that built
them. This study compares five top LLMs: Google's Gemini, High-Flyer's
DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs
this by analysing three important factors: Performance and Accuracy, Ethics and
Bias Mitigation and Usability and Integration. It was found that Claude has
good moral reasoning, Gemini is better at multimodal capabilities and has
strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA
is good for open applications and ChatGPT delivers balanced performance with a
focus on usage. It was concluded that these models are different in terms of
how well they work, how easy they are to use and how they treat people
ethically, making it a point that each model should be utilised by the user in
a way that makes the most of its strengths.

</details>


### [150] [Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models](https://arxiv.org/abs/2510.22751)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: 本文提出了一种事实验证框架，用于实时检测和纠正大型语言模型生成的虚假信息，显著提高了其在关键应用中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成虚假信息时会自信地产生看似合理的错误，这已成为将其部署到实际应用中的主要障碍。

Method: 本文开发了一个事实验证框架，通过交叉检查大型语言模型的输出与多个知识源来实时检测和纠正错误。

Result: 测试表明，该系统可以减少67%的幻觉而不牺牲响应质量，领域专家对修正后的输出满意率达到89%。

Conclusion: 本文提供了一种实用的解决方案，使大型语言模型在需要准确性的应用中更加可信。

Abstract: While Large Language Models have transformed how we interact with AI systems,
they suffer from a critical flaw: they confidently generate false information
that sounds entirely plausible. This hallucination problem has become a major
barrier to deploying these models in real-world applications where accuracy
matters. We developed a fact verification framework that catches and corrects
these errors in real-time by cross checking LLM outputs against multiple
knowledge sources. Our system combines structured databases, live web searches,
and academic literature to verify factual claims as they're generated. When we
detect inconsistencies, we automatically correct them while preserving the
natural flow of the response. Testing across various domains showed we could
reduce hallucinations by 67% without sacrificing response quality. Domain
experts in healthcare, finance, and scientific research rated our corrected
outputs 89% satisfactory a significant improvement over unverified LLM
responses. This work offers a practical solution for making LLMs more
trustworthy in applications where getting facts wrong isn't an option.

</details>


### [151] [How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations](https://arxiv.org/abs/2510.22780)
*Zora Zhiruo Wang,Yijia Shao,Omar Shaikh,Daniel Fried,Graham Neubig,Diyi Yang*

Main category: cs.AI

TL;DR: This paper compares AI agents and humans in performing work-related tasks, finding that agents are faster and cheaper but produce lower quality work and use a different approach than humans.


<details>
  <summary>Details</summary>
Motivation: To understand how AI agents perform human work and identify their strengths and weaknesses in comparison to humans.

Method: The study presents the first direct comparison of human and agent workers across multiple essential work-related skills, using a scalable toolkit to induce interpretable, structured workflows from computer-use activities.

Result: Agents exhibit promise in aligning with human workflows but take a programmatic approach, produce inferior quality work, and deliver results faster and at a lower cost than humans.

Conclusion: AI agents show potential for efficient collaboration by delegating easily programmable tasks, but they produce inferior quality work and use a programmatic approach that contrasts with human methods.

Abstract: AI agents are continually optimized for tasks related to human work, such as
software engineering and professional writing, signaling a pressing trend with
significant impacts on the human workforce. However, these agent developments
have often not been grounded in a clear understanding of how humans execute
work, to reveal what expertise agents possess and the roles they can play in
diverse workflows. In this work, we study how agents do human work by
presenting the first direct comparison of human and agent workers across
multiple essential work-related skills: data analysis, engineering,
computation, writing, and design. To better understand and compare
heterogeneous computer-use activities of workers, we introduce a scalable
toolkit to induce interpretable, structured workflows from either human or
agent computer-use activities. Using such induced workflows, we compare how
humans and agents perform the same tasks and find that: (1) While agents
exhibit promise in their alignment to human workflows, they take an
overwhelmingly programmatic approach across all work domains, even for
open-ended, visually dependent tasks like design, creating a contrast with the
UI-centric methods typically used by humans. (2) Agents produce work of
inferior quality, yet often mask their deficiencies via data fabrication and
misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster
and cost 90.4-96.2% less than humans, highlighting the potential for enabling
efficient collaboration by delegating easily programmable tasks to agents.

</details>


### [152] [Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps](https://arxiv.org/abs/2510.23340)
*Anwesha Das,John Duff,Jörg Hoffmann,Vera Demberg*

Main category: cs.AI

TL;DR: 本文提出了一种自适应信号理论框架，利用贝叶斯参考解析和理性言语行为模型来优化人机协作中的信息传递，强调了认知科学在辅助代理设计中的重要性。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的环境中，为了确保人类保持对关键任务元素的准确理解，辅助代理不仅要识别最高优先级的信息，还要估计信息如何以及何时能最有效地传达，因为人类注意力是一种零和认知资源。

Method: 本文引入了一个基于理性沟通原则的自适应信号理论框架，使用贝叶斯参考解析和理性言语行为（RSA）建模框架来规划优化及时对齐用户信念和动态环境的消息序列。

Result: 本文展示了结合多步规划与真实用户意识模型的重要性，并首次将RSA应用于动态环境中的通信以及一般的人机交互中。

Conclusion: 本文建立了用于人机协作中实用通信的理论基础，强调了认知科学见解在辅助代理设计中的应用。

Abstract: Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

</details>


### [153] [A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration](https://arxiv.org/abs/2510.23443)
*Chiara Bonfanti,Alessandro Druetto,Cataldo Basile,Tharindu Ranasinghe,Marcos Zampieri*

Main category: cs.AI

TL;DR: 本文探讨了网络安全与法律交叉领域的挑战，并提出了一种智能系统来解决这一问题，展示了多语言任务的初步成果。


<details>
  <summary>Details</summary>
Motivation: 传统法律研究工具难以处理案例、法规和技术漏洞之间的复杂联系，导致法律专家和网络安全专业人员之间的协作受阻。

Method: 本文旨在通过多语言任务展示智能系统在处理网络安全和法律信息空间中的潜力。

Result: 本文展示了在多语言任务上的有希望的初步结果。

Conclusion: 本文提出了一个初步的智能系统，以应对网络安全和法律交叉领域日益复杂的挑战。

Abstract: The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.

</details>


### [154] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun,Jingyang Gong,Yang Liu,Qiaosheng Chen,Lei Li,Kai Chen,Qipeng Guo,Ben Kao,Fei Yuan*

Main category: cs.AI

TL;DR: 本文提出了一种新的多模态代码数据合成工具包，并构建了最大的多模态代码语料库JanusCode-800K，基于此训练了JanusCoder系列模型，在文本和视觉中心的编码任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于高质量多模态代码数据的稀缺性，阻碍了神经代码智能的发展。本文旨在解决这一瓶颈问题。

Method: 本文引入了一个完整的合成工具包，利用数据模态之间的相互协同作用，高效地生成大规模高质量语料库，并构建了最大的多模态代码语料库JanusCode-800K。基于这个语料库，训练了JanusCoder和JanusCoderV模型。

Result: 实验表明，JanusCoder系列模型在文本和视觉中心的编码任务中表现优异，7B到14B规模的模型接近甚至超过了商业模型的性能。

Conclusion: 本文提出了JanusCoder系列模型，这些模型在文本和视觉中心的编码任务中表现出色，并且能够生成代码从文本指令、视觉输入或两者的组合。

Abstract: The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [155] [ReCode: Unify Plan and Action for Universal Granularity Control](https://arxiv.org/abs/2510.23564)
*Zhaoyang Yu,Jiayi Zhang,Huixue Su,Yufan Zhao,Yifan Wu,Mingyi Deng,Jinyu Xiang,Yizhang Lin,Lingxiao Tang,Yingchao Li,Yuyu Luo,Bang Liu,Chenglin Wu*

Main category: cs.AI

TL;DR: ReCode是一种新的范式，通过递归代码生成统一规划和行动，使代理能够动态控制决策粒度，并在实验中展现出优越的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的代理缺乏在不同决策粒度间流畅操作的能力，因为现有范式强制区分高层规划和低层动作，限制了动态适应性和泛化能力。

Method: ReCode将高层计划视为抽象占位符函数，并递归分解为更细粒度的子函数，直到达到基本动作。

Result: ReCode在推理性能上显著超越先进基线，并在训练中表现出卓越的数据效率。

Conclusion: ReCode通过递归代码生成统一规划和行动，证明了其在实现通用粒度控制方面的有效性和强大功能。

Abstract: Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [156] [DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling](https://arxiv.org/abs/2510.21712)
*Hao Sun,Zile Qiao,Bo Wang,Guoxin Chen,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种新的框架DecoupleSearch，用于解决Agentic RAG中的挑战，通过解耦规划和搜索过程来提高RAG的灵活性。


<details>
  <summary>Details</summary>
Motivation: 为了进一步提高RAG的灵活性，Agentic RAG引入了自主代理到工作流中。然而，Agentic RAG面临几个挑战：(1) 每一步的成功取决于高质量的规划和准确的搜索，(2) 缺乏对中间推理步骤的监督，以及(3) 规划和搜索的候选空间呈指数级增长。

Method: 我们提出了DecoupleSearch，这是一个使用双值模型解耦规划和搜索过程的新框架，使计划推理和搜索基础可以独立优化。我们的方法构建了一个推理树，其中每个节点代表规划和搜索步骤，并利用蒙特卡洛树搜索评估每个步骤的质量。在推理过程中，分层束搜索迭代地用双值模型改进规划和搜索候选者。

Result: 通过使用DecoupleSearch框架，我们能够有效地解决这些挑战，并在不同参数规模的政策模型上展示了我们方法的有效性。

Conclusion: 我们的方法在不同参数规模的政策模型上进行了广泛的实验，证明了其有效性。

Abstract: Retrieval-Augmented Generation (RAG) systems have emerged as a pivotal
methodology for enhancing Large Language Models (LLMs) through the dynamic
integration of external knowledge. To further improve RAG's flexibility,
Agentic RAG introduces autonomous agents into the workflow. However, Agentic
RAG faces several challenges: (1) the success of each step depends on both
high-quality planning and accurate search, (2) the lack of supervision for
intermediate reasoning steps, and (3) the exponentially large candidate space
for planning and searching. To address these challenges, we propose
DecoupleSearch, a novel framework that decouples planning and search processes
using dual value models, enabling independent optimization of plan reasoning
and search grounding. Our approach constructs a reasoning tree, where each node
represents planning and search steps. We leverage Monte Carlo Tree Search to
assess the quality of each step. During inference, Hierarchical Beam Search
iteratively refines planning and search candidates with dual value models.
Extensive experiments across policy models of varying parameter sizes,
demonstrate the effectiveness of our method.

</details>


### [157] [A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition](https://arxiv.org/abs/2510.22055)
*V Venktesh,Deepali Prabhu,Avishek Anand*

Main category: cs.IR

TL;DR: 本文提出了QuanTemp++数据集，用于自动事实验证，该数据集通过模拟人类事实核查员的声明分解过程来收集相关证据，并确保没有时间泄漏。同时，本文分析了关键声明分解范式在检索性能上的表现，并探讨了它们对验证流程结果的影响。


<details>
  <summary>Details</summary>
Motivation: 自动事实验证的现有工作主要不关注自然数值声明。而人类事实核查员首先检索与声明的不同数值方面相关的证据，然后进行推理以预测声明的真实性。因此，搜索过程是验证过程的基础。为了开发具备这种技能的自动化方法，需要模拟现实世界的设置。然而，现有的基准测试使用启发式声明分解方法和弱监督网络搜索来收集证据，这可能导致相关性较低的证据和噪声源，以及时间泄漏问题。

Method: 本文提出了QuanTemp++数据集，通过模拟人类事实核查员的声明分解过程来收集相关证据，并确保没有时间泄漏。同时，本文分析了关键声明分解范式在检索性能上的表现。

Result: 本文介绍了QuanTemp++数据集，该数据集包含自然数值声明、开放领域语料库以及每个声明的相关证据。通过模拟人类事实核查员的声明分解过程收集证据，并确保没有时间泄漏。此外，本文还分析了关键声明分解范式在检索性能上的表现，并探讨了它们对验证流程结果的影响。

Conclusion: 本文介绍了QuanTemp++数据集，该数据集包含自然数值声明、开放领域语料库以及每个声明的相关证据。通过模拟人类事实核查员的声明分解过程收集证据，并确保没有时间泄漏。此外，本文还分析了关键声明分解范式在检索性能上的表现，并探讨了它们对验证流程结果的影响。

Abstract: Fact-checking numerical claims is critical as the presence of numbers provide
mirage of veracity despite being fake potentially causing catastrophic impacts
on society. The prior works in automatic fact verification do not primarily
focus on natural numerical claims. A typical human fact-checker first retrieves
relevant evidence addressing the different numerical aspects of the claim and
then reasons about them to predict the veracity of the claim. Hence, the search
process of a human fact-checker is a crucial skill that forms the foundation of
the verification process. Emulating a real-world setting is essential to aid in
the development of automated methods that encompass such skills. However,
existing benchmarks employ heuristic claim decomposition approaches augmented
with weakly supervised web search to collect evidences for verifying claims.
This sometimes results in less relevant evidences and noisy sources with
temporal leakage rendering a less realistic retrieval setting for claim
verification. Hence, we introduce QuanTemp++: a dataset consisting of natural
numerical claims, an open domain corpus, with the corresponding relevant
evidence for each claim. The evidences are collected through a claim
decomposition process approximately emulating the approach of human
fact-checker and veracity labels ensuring there is no temporal leakage. Given
this dataset, we also characterize the retrieval performance of key claim
decomposition paradigms. Finally, we observe their effect on the outcome of the
verification pipeline and draw insights. The code for data pipeline along with
link to data can be found at https://github.com/VenkteshV/QuanTemp_Plus

</details>


### [158] [PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading](https://arxiv.org/abs/2510.22242)
*Yutao Wu,Xiao Liu,Yunhao Feng,Jiale Ding,Xingjun Ma*

Main category: cs.IR

TL;DR: PaperAsk is a benchmark for evaluating the reliability of large language models in scholarly tasks, revealing significant failure rates in citation retrieval, content extraction, paper discovery, and claim verification. The study highlights issues such as uncontrolled context expansion and prioritization of semantic relevance over instructions, and proposes reliability classifiers to improve the trustworthiness of LLM-based research assistants.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) increasingly serve as research assistants, yet their reliability in scholarly tasks remains under-evaluated.

Method: We introduce PaperAsk, a benchmark that systematically evaluates LLMs across four key research tasks: citation retrieval, content extraction, paper discovery, and claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under realistic usage conditions via web interfaces. We also develop lightweight reliability classifiers trained on PaperAsk data to identify unreliable outputs.

Result: We find consistent reliability failures: citation retrieval fails in 48-98% of multi-reference queries, section-specific content extraction fails in 72-91% of cases, and topical paper discovery yields F1 scores below 0.32, missing over 60% of relevant literature. Further human analysis attributes these failures to the uncontrolled expansion of retrieved context and the tendency of LLMs to prioritize semantically relevant text over task instructions. Across basic tasks, the LLMs display distinct failure behaviors: ChatGPT often withholds responses rather than risk errors, whereas Gemini produces fluent but fabricated answers.

Conclusion: PaperAsk provides a reproducible and diagnostic framework for advancing the reliability evaluation of LLM-based scholarly assistance systems.

Abstract: Large Language Models (LLMs) increasingly serve as research assistants, yet
their reliability in scholarly tasks remains under-evaluated. In this work, we
introduce PaperAsk, a benchmark that systematically evaluates LLMs across four
key research tasks: citation retrieval, content extraction, paper discovery,
and claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under
realistic usage conditions-via web interfaces where search operations are
opaque to the user. Through controlled experiments, we find consistent
reliability failures: citation retrieval fails in 48-98% of multi-reference
queries, section-specific content extraction fails in 72-91% of cases, and
topical paper discovery yields F1 scores below 0.32, missing over 60% of
relevant literature. Further human analysis attributes these failures to the
uncontrolled expansion of retrieved context and the tendency of LLMs to
prioritize semantically relevant text over task instructions. Across basic
tasks, the LLMs display distinct failure behaviors: ChatGPT often withholds
responses rather than risk errors, whereas Gemini produces fluent but
fabricated answers. To address these issues, we develop lightweight reliability
classifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk
provides a reproducible and diagnostic framework for advancing the reliability
evaluation of LLM-based scholarly assistance systems.

</details>


### [159] [REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for E-commerce Visual Search System Optimization](https://arxiv.org/abs/2510.22739)
*Yiwen Tang,Qiuyu Zhao,Zenghui Sun,Jinsong Lan,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种名为REVISION的框架，通过结合离线推理挖掘和在线决策与执行，解决淘宝电子商务视觉搜索中用户隐式意图与系统响应之间的不匹配问题。该框架在离线阶段挖掘历史无点击请求中的差异，并利用大型模型分析隐式意图因素，推断最佳建议。在在线阶段，REVISION-R1-3B对查询图像和相关历史产品进行全面分析，生成优化计划并自适应地调度策略。实验结果表明，该方法提高了隐式意图挖掘的效率，并显著降低了无点击率。


<details>
  <summary>Details</summary>
Motivation: 在淘宝电子商务视觉搜索中，用户行为分析显示大量无点击请求，表明多样且隐式的用户意图。这些意图以各种形式表达，难以挖掘和发现，从而导致平台策略的适应性有限和滞后。这极大地限制了用户表达多样化意图的能力，并阻碍了视觉搜索系统的可扩展性。用户-搜索系统意图差异定义了这种不匹配。

Method: 我们提出了一个名为REVISION的框架，该框架结合了离线推理挖掘和在线决策与执行，使适应性策略能够解决隐式用户需求。在离线阶段，我们构建了一个周期性流程来从历史无点击请求中挖掘差异。利用大型模型，我们分析隐式意图因素并通过对查询和产品元数据进行联合推理来推断最佳建议。这些推断的建议作为改进平台策略的可操作见解。在在线阶段，REVISION-R1-3B在整理的离线数据上进行训练，对查询图像和相关历史产品进行全面分析，以生成优化计划并在搜索管道中自适应地调度策略。

Result: 实验结果表明，我们的方法提高了从大规模搜索日志中隐式意图挖掘的效率，并显著降低了无点击率。

Conclusion: 我们的框架提供了一种将大型模型与传统搜索系统集成的简化范式，实现了信息聚合和用户交互的端到端智能优化。

Abstract: In Taobao e-commerce visual search, user behavior analysis reveals a large
proportion of no-click requests, suggesting diverse and implicit user intents.
These intents are expressed in various forms and are difficult to mine and
discover, thereby leading to the limited adaptability and lag in platform
strategies. This greatly restricts users' ability to express diverse intents
and hinders the scalability of the visual search system. This mismatch between
user implicit intent expression and system response defines the User-SearchSys
Intent Discrepancy. To alleviate the issue, we propose a novel framework
REVISION. This framework integrates offline reasoning mining with online
decision-making and execution, enabling adaptive strategies to solve implicit
user demands. In the offline stage, we construct a periodic pipeline to mine
discrepancies from historical no-click requests. Leveraging large models, we
analyze implicit intent factors and infer optimal suggestions by jointly
reasoning over query and product metadata. These inferred suggestions serve as
actionable insights for refining platform strategies. In the online stage,
REVISION-R1-3B, trained on the curated offline data, performs holistic analysis
over query images and associated historical products to generate optimization
plans and adaptively schedule strategies across the search pipeline. Our
framework offers a streamlined paradigm for integrating large models with
traditional search systems, enabling end-to-end intelligent optimization across
information aggregation and user interaction. Experimental results demonstrate
that our approach improves the efficiency of implicit intent mining from
large-scale search logs and significantly reduces the no-click rate.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [160] [M-CIF: Multi-Scale Alignment For CIF-Based Non-Autoregressive ASR](https://arxiv.org/abs/2510.22172)
*Ruixiang Mao,Xiangnan Ma,Qing Yang,Ziming Zhu,Yucheng Qiao,Yuan Ge,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.SD

TL;DR: 本文提出M-CIF方法，通过多尺度对齐提升非自回归语音识别性能，尤其在德语和法语中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的连续积分-放电机制在某些语言如英语和法语中稳定性不足，需要更细粒度的指导来提高鲁棒性。

Method: 提出Multi-scale CIF (M-CIF)方法，通过逐步融合字符和音素级监督到子词表示中实现多级对齐。

Result: M-CIF在CommonVoice数据集上相比Paraformer基线减少了WER，特别是在德语和法语中分别降低了4.21%和3.05%。

Conclusion: M-CIF通过多尺度对齐机制提高了非自回归语音识别的稳定性，尤其在德语和法语中表现显著。

Abstract: The Continuous Integrate-and-Fire (CIF) mechanism provides effective
alignment for non-autoregressive (NAR) speech recognition. This mechanism
creates a smooth and monotonic mapping from acoustic features to target tokens,
achieving performance on Mandarin competitive with other NAR approaches.
However, without finer-grained guidance, its stability degrades in some
languages such as English and French. In this paper, we propose Multi-scale CIF
(M-CIF), which performs multi-level alignment by integrating character and
phoneme level supervision progressively distilled into subword representations,
thereby enhancing robust acoustic-text alignment. Experiments show that M-CIF
reduces WER compared to the Paraformer baseline, especially on CommonVoice by
4.21% in German and 3.05% in French. To further investigate these gains, we
define phonetic confusion errors (PE) and space-related segmentation errors
(SE) as evaluation metrics. Analysis of these metrics across different M-CIF
settings reveals that the phoneme and character layers are essential for
enhancing progressive CIF alignment.

</details>


### [161] [ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models](https://arxiv.org/abs/2510.23558)
*Bohan Li,Wenbin Huang,Yuhang Qiu,Yiwei Guo,Hankun Wang,Zhihan Li,Jing Peng,Ziyang Ma,Xie Chen,Kai Yu*

Main category: cs.SD

TL;DR: 本文介绍了ISA-Bench，一个用于评估LALMs指令敏感性的动态基准。实验结果显示，即使是最先进的LALMs也存在显著的指令敏感性问题，导致性能下降。通过微调，我们提高了指令遵循性能，但也带来了灾难性遗忘的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LALMs对指令的措辞高度敏感，这影响了指令遵循率和任务性能。然而，没有现有的基准可以系统地和全面地评估这种敏感性。因此，我们需要一个基准来评估和改进LALMs的指令敏感性。

Method: 我们引入了ISA-Bench，这是一个动态基准，用于沿三个轴（指令描述、输出格式和任务组成）评估LALMs的指令敏感性。我们评估了最近的开源和专有LALMs，并在受控指令变化下对其合规性和准确性进行了分析。此外，我们对Qwen2-Audio进行了微调，以解决指令敏感性问题。

Result: 实验结果表明，即使是最先进的LALMs也受到显著的指令敏感性影响，导致基础音频理解任务的性能下降。通过微调Qwen2-Audio，我们实现了指令遵循性能的显著提高，但这也导致了非 trivial 的灾难性遗忘：当模型接触到新的指令风格时，它会失去一些之前掌握的任务能力。

Conclusion: 我们的基准为评估和改进LALMs中的指令敏感性提供了标准化的基础，并强调了在现实世界流程中需要指令稳健的音频理解。

Abstract: Large Audio Language Models (LALMs), which couple acoustic perception with
large language models (LLMs) to extract and understand diverse information from
audio, have attracted intense interest from both academic and industrial
communities. However, existing LALMs are highly sensitive to how instructions
are phrased, affecting both (i) instruction-following rates and (ii) task
performance. Yet, no existing benchmarks offer a systematic and comprehensive
evaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark
evaluating instruction sensitivity for LALMs along three axes: instruction
description, output format, and task composition. We assess recent open-source
and proprietary LALMs using ISA-Bench, profiling both compliance and accuracy
under controlled instruction variations. Experimental results reveal that even
state-of-the-art LALMs suffer significant instruction sensitivity, leading to
degraded performance on fundamental audio understanding tasks. To mitigate this
issue, we fine-tune Qwen2-Audio on a specifically constructed complex
instruction-variant dataset, achieving a marked improvement in
instruction-following performance. However, this also induces nontrivial
catastrophic forgetting: the model loses some previously mastered task
capabilities when exposed to new instruction styles. Our benchmark provides a
standardized basis for assessing and improving instruction sensitivity in
LALMs, underscoring the need for instruction-robust audio understanding in
real-world pipelines.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [162] [Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models](https://arxiv.org/abs/2510.22085)
*Pavlos Ntais*

Main category: cs.CR

TL;DR: 本文提出了一种系统的方法来评估大型语言模型在网络安全中的脆弱性，并展示了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到复杂的提示工程攻击，这在网络安全应用中带来了重大风险。因此，需要一种系统的方法来评估AI驱动的安全系统的脆弱性。

Method: 本文提出了Jailbreak Mimicry方法，利用参数高效的微调（LoRA）在Mistral-7B上进行训练，并使用来自AdvBench的数据集，实现了对GPT-OSS-20B的高攻击成功率。

Result: 本文在GPT-OSS-20B上实现了81.0%的攻击成功率，并在其他模型如GPT-4、Llama-3和Gemini 2.5 Flash上取得了显著的攻击成功率。此外，本文还发现技术领域和基于欺骗的攻击特别容易受到攻击。

Conclusion: 本文分析了大型语言模型在网络安全应用中的系统性漏洞，并提出了Jailbreak Mimicry方法来自动生成基于叙述的越狱提示，以评估AI驱动的安全系统的脆弱性。同时，本文讨论了防御策略以减轻这些漏洞。

Abstract: Large language models (LLMs) remain vulnerable to sophisticated prompt
engineering attacks that exploit contextual framing to bypass safety
mechanisms, posing significant risks in cybersecurity applications. We
introduce Jailbreak Mimicry, a systematic methodology for training compact
attacker models to automatically generate narrative-based jailbreak prompts in
a one-shot manner. Our approach transforms adversarial prompt discovery from
manual craftsmanship into a reproducible scientific process, enabling proactive
vulnerability assessment in AI-driven security systems. Developed for the
OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient
fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,
achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out
test set of 200 items. Cross-model evaluation reveals significant variation in
vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on
Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad
applicability and model-specific defensive strengths in cybersecurity contexts.
This represents a 54x improvement over direct prompting (1.5% ASR) and
demonstrates systematic vulnerabilities in current safety alignment approaches.
Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and
deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,
highlighting threats to AI-integrated threat detection, malware analysis, and
secure systems, while physical harm categories show greater resistance (55.6%
ASR). We employ automated harmfulness evaluation using Claude Sonnet 4,
cross-validated with human expert assessment, ensuring reliable and scalable
evaluation for cybersecurity red-teaming. Finally, we analyze failure
mechanisms and discuss defensive strategies to mitigate these vulnerabilities
in AI for cybersecurity.

</details>


### [163] [Fast-MIA: Efficient and Scalable Membership Inference for LLMs](https://arxiv.org/abs/2510.23074)
*Hiromu Takahashi,Shotaro Ishihara*

Main category: cs.CR

TL;DR: Fast-MIA 是一个开源 Python 库，用于高效评估针对大型语言模型的成员推理攻击，解决了计算成本高和缺乏标准化实现的问题。


<details>
  <summary>Details</summary>
Motivation: 由于对版权、安全性和数据隐私的关注不断增加，针对大型语言模型的成员推理攻击已成为一个关键挑战，但研究进展受到两个主要障碍的阻碍：（1）大型语言模型推理的高计算成本，（2）缺乏标准化和维护良好的 MIA 方法实现，这使得大规模实证比较变得困难。

Method: 提出 Fast-MIA，这是一个用于高效评估针对大型语言模型的成员推理攻击的 Python 库。

Result: 该库提供了快速批量推理，并在统一的评估框架下包含了代表性 MIA 方法的实现。它支持通过简单配置和可扩展性轻松实现可重复的基准测试。

Conclusion: Fast-MIA 是一个开源工具，旨在支持对大型语言模型进行可扩展和透明的研究。

Abstract: We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library
for efficiently evaluating membership inference attacks (MIA) against Large
Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due
to growing concerns over copyright, security, and data privacy, and has
attracted increasing research attention. However, the progress of this research
is significantly hindered by two main obstacles: (1) the high computational
cost of inference in LLMs, and (2) the lack of standardized and maintained
implementations of MIA methods, which makes large-scale empirical comparison
difficult. To address these challenges, our library provides fast batch
inference and includes implementations of representative MIA methods under a
unified evaluation framework. This library supports easy implementation of
reproducible benchmarks with simple configuration and extensibility. We release
Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and
transparent research on LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [164] [A Multimodal, Multitask System for Generating E Commerce Text Listings from Images](https://arxiv.org/abs/2510.21835)
*Nayan Kumar Singh*

Main category: cs.LG

TL;DR: 本文提出了一种端到端的多任务系统，用于从单张图像生成事实依据的文本列表。通过多任务学习和分层生成过程，显著提高了事实一致性并减少了幻觉率。


<details>
  <summary>Details</summary>
Motivation: 手动生成吸引人的描述和名称对于零售商来说既费时又低效。虽然生成式AI提供了自动化解决方案，但当前的视觉到语言模型容易产生事实性“幻觉”。孤立的单一任务模型不仅效率低下，而且无法捕捉特征之间的相互依赖关系。

Method: 本文提出了两种模型架构的改进：第一，应用多任务学习方法对视觉编码器进行微调，其中单一的视觉主干同时训练属性预测（如颜色、裙摆和领型）和价格回归。第二，引入了分层生成过程，将模型自身的预测属性嵌入提示中并输入文本解码器以提高事实一致性。

Result: 实验结果表明，多任务方法在价格回归方面比独立方法提高了3.6%的R2值，在属性分类方面提高了6.6%的F1分数。分层生成过程将事实幻觉率从12.7%降至7.1%，相对减少了44.5%。此外，分层方法将自回归文本生成过程的延迟降低了3.5倍。然而，该模型在ROUGE-L分数上比直接的视觉到语言模型差3.5%。

Conclusion: 本文提出了一种端到端的多任务系统，可以从单张图像生成事实依据的文本列表。实验表明，该架构优于独立的价格回归和属性分类，并显著减少了事实幻觉率。然而，该模型在ROUGE-L分数上略逊于直接的视觉到语言模型。

Abstract: Manually generating catchy descriptions and names is labor intensive and a
slow process for retailers. Although generative AI provides an automation
solution in form of Vision to Language Models (VLM), the current VLMs are prone
to factual "hallucinations". Siloed, single task models are not only
inefficient but also fail to capture interdependent relationships between
features. To address these challenges, we propose an end to end, multi task
system that generates factually grounded textual listings from a single image.
The contributions of this study are two proposals for the model architecture.
First, application of multi task learning approach for fine tuning a vision
encoder where a single vision backbone is jointly trained on attribute
prediction such as color, hemline and neck style and price regression. Second,
introduction of a hierarchical generation process where the model's own
predicted attributes are embedded in a prompt and fed to the text decoder to
improve factual consistency. The experiments demonstrate the superiority of
this architecture. The multi tasking approach outperforms both the independent
price regression, with a 3.6% better R2 Value and attribute classification,
with a 6.6% improvement F1 score. Critically, the hierarchical generation
process proves highly effective, slashing the factual hallucination rate from
12.7% to 7.1%, a 44.5% relative reduction, compared to a non hierarchical
ablation. The hierarchical approach also reduces the latency of the
autoregressive text generation process by a factor of 3.5 when compared to
direct vision to language model of similar size. One minor caveat is that the
model does perform 3.5% worse than direct vision-to-language model on ROUGE-L
score.

</details>


### [165] [The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems](https://arxiv.org/abs/2510.21861)
*Bentley DeVilling*

Main category: cs.LG

TL;DR: 研究发现，没有外部验证的自我反思会导致信息封闭，而最小的接地干预可以恢复信息流动，表明生成推理中的自我修正存在结构限制，并提出了基于接地和协作推理的设计原则。


<details>
  <summary>Details</summary>
Motivation: 研究旨在测试大型语言模型在没有外部反馈的情况下进行递归自我评估是否能带来进展，还是仅导致重新表述。

Method: 研究通过跨提供商的144个推理序列测试了这一预测，涉及三个模型（OpenAI GPT-4o-mini、Anthropic Claude 3 Haiku和Google Gemini 2.0 Flash）和四个任务类别（算术、代码、解释和反思），每个任务迭代十次，分为无依据的自我批评和一个最小的接地干预条件（第三次迭代时进行一次验证）。

Result: 在无依据的运行中，信息变化（delta I）从早期（0.193）到后期（0.087）下降了55%。接地运行在干预后信息变化增加了28%，并保持非零方差。其他指标如n-gram新颖性、嵌入漂移和字符级熵也显示了相同模式。

Conclusion: 研究结果表明，自我反思在没有外部验证的情况下会导致信息封闭，而最小的接地干预可以恢复信息流动。这表明生成推理中的自我修正存在结构限制，并提出了基于接地和协作推理的设计原则。

Abstract: Large language models are often described as capable of reflective reasoning,
yet recursive self-evaluation without external feedback frequently yields
reformulation rather than progress. We test this prediction in a cross-provider
study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini,
Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families
(arithmetic, code, explanation, reflection), each iterated ten times under two
conditions: ungrounded self-critique and a minimal grounding intervention (a
single verification step at iteration three). Mean informational change (delta
I, measured via normalized edit distance) declined by 55% from early (0.193) to
late (0.087) iterations in ungrounded runs, with consistent patterns across all
three providers. Grounded runs showed a +28% rebound in informational change
immediately after the intervention and sustained non-zero variance thereafter.
Complementary measures-n-gram novelty, embedding drift, and character-level
entropy-converged on the same pattern: reflection without contact tends toward
informational closure. We interpret this as evidence for a structural limit on
self-correction in generative reasoning: without an exchange of information
with an independent verifier or environment, recursive inference approaches an
attractor state of epistemic stasis. Minimal grounding functions as dissipative
coupling, reintroducing informational flux. The cross-architecture consistency
suggests the mirror loop arises from shared autoregressive training objectives
rather than provider-specific alignment schemes. The results delineate when
reflection is performative rather than epistemic and motivate design principles
for grounded, cooperative reasoning. Materials and code are publicly available.

</details>


### [166] [Transformer Based Linear Attention with Optimized GPU Kernel Implementation](https://arxiv.org/abs/2510.21956)
*Armin Gerami,Ramani Duraiswami*

Main category: cs.LG

TL;DR: 本文提出了一种优化的线性注意力机制实现方法，显著提升了Transformer模型的运行效率。


<details>
  <summary>Details</summary>
Motivation: 尽管线性注意力机制理论上具有更高的效率，但在实际应用中并未达到预期效果，因此需要改进其性能。

Method: 本文提出了一个新的线性注意力机制的实现方法，并进行了高度优化的CUDA实现。

Result: 本文的方法在速度上比最先进的方法快3.3倍，内存消耗减少了3.6倍，并且在单层和端到端设置中验证了这些改进。

Conclusion: 本文提出了一种新的线性注意力机制的前向和反向传播方法，并实现了高度优化的CUDA代码，显著提高了运行速度并减少了内存消耗。

Abstract: The original softmax-based attention mechanism (regular attention) in the
extremely successful Transformer architecture computes attention between $N$
tokens, each embedded in a $D$-dimensional head, with a time complexity of
$O(N^2D)$. Given the success of Transformers, improving their runtime during
both training and inference is a popular research area. One such approach is
the introduction of the linear attention (LA) mechanisms, which offers a linear
time complexity of $O(ND^2)$ and have demonstrated comparable accuracy to
regular attention. However, LA in practice lags behind its theoretical
efficiency. We propose a novel method for LA's forward and backward passes,
along with a highly-optimized CUDA implementation. Our approach outperforms the
state-of-the-art by 3.3 times in speed and reduces memory consumption by 3.6
times. We validate these improvements in both single-layer and end-to-end
settings by training a 1.4 billion parameter language model, which demonstrates
similar expressivity to regular attention on major reasoning benchmarks.

</details>


### [167] [Parallel Sampling from Masked Diffusion Models via Conditional Independence Testing](https://arxiv.org/abs/2510.21961)
*Iskander Azangulov,Teodora Pandeva,Niranjani Prasad,Javier Zazo,Sushrut Karmalkar*

Main category: cs.LG

TL;DR: PUNT 是一种与模型无关的采样器，通过优化并行解掩码，提高了离散文本生成的准确性和效率，尤其适用于长序列生成。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归模型在离散文本生成中存在顺序生成的限制，而并行采样需要同时更新的标记具有条件独立性，并且应优先考虑高置信度预测，但这两者之间存在冲突。

Method: PUNT 是一种与模型无关的采样器，通过识别令牌依赖关系并从冲突组中移除低置信度的令牌，生成满足独立性和置信度标准的索引集，从而实现改进的并行解掩码。

Result: 实验表明，PUNT 在准确性与计算之间提供了优越的权衡，尤其是在生成更长序列时。在 IFEval 基准测试中，它比基线方法（包括逐个生成）的准确性高出多达 16%。此外，PUNT 引发了分层生成策略，有助于强大的对齐性能。

Conclusion: PUNT 通过在不依赖模型的情况下实现更优的准确性和计算权衡，特别是在生成更长序列时表现突出。此外，PUNT 引发了一种新兴的分层生成策略，有助于强大的对齐性能。

Abstract: Masked diffusion models (MDMs) offer a compelling alternative to
autoregressive models (ARMs) for discrete text generation because they enable
parallel token sampling, rather than sequential, left-to-right generation. This
means potentially much faster inference. However, effective parallel sampling
faces two competing requirements: (i) simultaneously updated tokens must be
conditionally independent, and (ii) updates should prioritise high-confidence
predictions. These goals conflict because high-confidence predictions often
cluster and depend on each other, opportunities for parallel updates.
  We present PUNT, a model-agnostic sampler that reconciles this trade-off. Our
method identifies token dependencies and removes lower-confidence tokens from
conflicting groups. This produces sets of indices for unmasking that satisfy
both independence and confidence criteria. Our approach ensures improved
parallel unmasking through approximate conditional independence testing.
  Our experiments show that PUNT delivers a superior trade-off between accuracy
and compute when compared to other strong training-free baselines, especially
for generation of longer sequences. On the IFEval benchmark, it achieves up to
16\% higher accuracy over baseline methods, including sequential generation
(one-by-one). These gains hold across different values of hyperparameters,
mitigating the need for brittle hyperparameter tuning. Moreover, we observe
that PUNT induces an emergent hierarchical generation strategy, where the model
first establishes high-level paragraph structure before local refinement,
suggesting a planning-like generation process that contributes to strong
alignment performance.

</details>


### [168] [Optimal Detection for Language Watermarks with Pseudorandom Collision](https://arxiv.org/abs/2510.22007)
*T. Tony Cai,Xiang Li,Qi Long,Weijie J. Su,Garrett G. Wen*

Main category: cs.LG

TL;DR: 本文提出了一种新的统计框架，用于在非完美伪随机性下进行水印检测，提高了检测能力并确保了类型I错误控制。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数方法假设完美的伪随机性，但在实际中，生成文本中的重复会导致碰撞，产生结构依赖，损害类型I错误控制并使标准分析无效。

Method: 本文引入了一个统计框架，通过分层的两层划分来捕捉结构。核心概念是最小单元——可以视为独立的最小组，同时允许单元内部的依赖性。使用最小单元，我们定义了一个非渐近效率度量，并将水印检测转化为一个最小最大假设检验问题。

Result: 将该框架应用于Gumbel-max和逆变换水印，产生了闭式最优规则。它解释了为什么丢弃重复统计信息通常能提高性能，并表明除非退化，否则必须处理单元内依赖性。理论和实验都证实了在严格类型I错误控制下的检测能力的提高。

Conclusion: 本文提供了在非完美伪随机性下水印检测的第一个有原则的基础，为可靠追踪模型输出提供了理论见解和实践指导。

Abstract: Text watermarking plays a crucial role in ensuring the traceability and
accountability of large language model (LLM) outputs and mitigating misuse.
While promising, most existing methods assume perfect pseudorandomness. In
practice, repetition in generated text induces collisions that create
structured dependence, compromising Type I error control and invalidating
standard analyses.
  We introduce a statistical framework that captures this structure through a
hierarchical two-layer partition. At its core is the concept of minimal units
-- the smallest groups treatable as independent across units while permitting
dependence within. Using minimal units, we define a non-asymptotic efficiency
measure and cast watermark detection as a minimax hypothesis testing problem.
  Applied to Gumbel-max and inverse-transform watermarks, our framework
produces closed-form optimal rules. It explains why discarding repeated
statistics often improves performance and shows that within-unit dependence
must be addressed unless degenerate. Both theory and experiments confirm
improved detection power with rigorous Type I error control. These results
provide the first principled foundation for watermark detection under imperfect
pseudorandomness, offering both theoretical insight and practical guidance for
reliable tracing of model outputs.

</details>


### [169] [Agentic Reinforcement Learning for Real-World Code Repair](https://arxiv.org/abs/2510.22075)
*Siyu Zhu,Anastasiya Karpovich,Albert Chen,Jessica Koscheka,Shailesh Jannu,Di Wen,Yuqing Zhu,Rohit Jain,Alborz Geramifard*

Main category: cs.LG

TL;DR: 研究开发了一个可验证的代码修复管道，通过固定依赖项和禁用自动升级提高了稳定性。在此基础上，引入了一个可扩展的简化管道用于大规模强化学习。SFT模型表现良好且体积更小，而RL在匹配的训练-测试条件下提升了性能。然而，SFT和RL模型在不同环境中无法泛化，强调了匹配训练-测试环境的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决在真实仓库中训练可靠的代码修复代理的挑战，其中复杂的构建和不断变化的依赖使得评估不稳定。

Method: 开发了一个可验证的管道，通过固定依赖项和禁用自动升级来提高约1K个真实问题的稳定性。在此基础上，引入了一个可扩展的简化管道用于大规模强化学习（RL）。使用这个设置，对Qwen3-32B进行了监督微调，并在简化环境中应用了RL。

Result: SFT模型从GPT-4.1轨迹中蒸馏出的模型表现相当，但体积小56倍，而RL在匹配的训练-测试条件下增加了7-20%的绝对收益。

Conclusion: SFT和RL模型在不同环境中无法泛化，强调了匹配训练-测试环境对于构建可靠的真实代码修复代理的重要性。

Abstract: We tackle the challenge of training reliable code-fixing agents in real
repositories, where complex builds and shifting dependencies make evaluation
unstable. We developed a verifiable pipeline with success defined as post-fix
build validation and improved reproducibility across ~1K real issues by pinning
dependencies and disabling automatic upgrades. Building on this, we introduced
a scalable simplified pipeline for large-scale reinforcement learning (RL).
Using this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and
applied RL on top of the SFT model in the simplified environment. The SFT model
distilled from GPT-4.1 trajectories performs on par while being 56x smaller,
and RL added 7-20% absolute gains under matched train-test conditions.
"Thinking mode" was on par or worse in our experiments. Both SFT and RL models
failed to generalize across environments, highlighting the importance of
matching train-test environments for building reliable real-world code-fixing
agents.

</details>


### [170] [Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs](https://arxiv.org/abs/2510.22139)
*Jinzhe Liu,Junshu Sun,Shufan Shen,Chenxue Yang,Shuhui Wang*

Main category: cs.LG

TL;DR: NMKE是一种新的细粒度编辑框架，通过神经元级归因和动态稀疏掩码实现精确的知识编辑，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在编辑过程中容易积累错误，导致编辑准确性和泛化能力逐渐下降。

Method: NMKE结合了神经元级归因和动态稀疏掩码，利用神经元功能归因识别两种关键类型的知识神经元，并引入基于熵的动态稀疏掩码来定位与目标知识相关的神经元。

Result: 实验结果表明，NMKE在数千次连续编辑中表现出色，保持了高编辑成功率和模型的泛化能力。

Conclusion: 实验结果表明，NMKE在保持高编辑成功率和保留模型泛化能力方面优于现有方法。

Abstract: Lifelong knowledge editing enables continuous, precise updates to outdated
knowledge in large language models (LLMs) without computationally expensive
full retraining. However, existing methods often accumulate errors throughout
the editing process, causing a gradual decline in both editing accuracy and
generalization. To tackle this problem, we propose Neuron-Specific Masked
Knowledge Editing (NMKE), a novel fine-grained editing framework that combines
neuron-level attribution with dynamic sparse masking. Leveraging neuron
functional attribution, we identify two key types of knowledge neurons, with
knowledge-general neurons activating consistently across prompts and
knowledge-specific neurons activating to specific prompts. NMKE further
introduces an entropy-guided dynamic sparse mask, locating relevant neurons to
the target knowledge. This strategy enables precise neuron-level knowledge
editing with fewer parameter modifications. Experimental results from thousands
of sequential edits demonstrate that NMKE outperforms existing methods in
maintaining high editing success rates and preserving model general
capabilities in lifelong editing.

</details>


### [171] [Power to the Clients: Federated Learning in a Dictatorship Setting](https://arxiv.org/abs/2510.22149)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中的一种新型恶意客户端——独裁者客户端，分析了它们对模型训练的影响，并通过实验验证了理论分析。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使其容易受到恶意客户端的攻击。本文旨在研究一种新型的恶意参与者——独裁者客户端，以揭示其对模型训练的潜在威胁。

Method: 本文提出了独裁者客户端的概念，并设计了具体的攻击策略来研究它们对学习过程的影响。同时，通过理论分析和实验验证，评估了多个独裁者客户端在不同场景下的效果。

Result: 本文发现独裁者客户端能够完全抹去其他客户端的贡献，同时保留自己的。此外，多个独裁者客户端在不同协作情况下对全局模型的收敛性产生了显著影响。实验结果支持了理论分析。

Conclusion: 本文研究了联邦学习中的恶意客户端——独裁者客户端，发现它们能够完全抹去其他客户端的贡献，同时保留自己的。此外，本文还分析了多个独裁者客户端在不同协作情况下的影响，并通过实验验证了理论分析。

Abstract: Federated learning (FL) has emerged as a promising paradigm for decentralized
model training, enabling multiple clients to collaboratively learn a shared
model without exchanging their local data. However, the decentralized nature of
FL also introduces vulnerabilities, as malicious clients can compromise or
manipulate the training process. In this work, we introduce dictator clients, a
novel, well-defined, and analytically tractable class of malicious participants
capable of entirely erasing the contributions of all other clients from the
server model, while preserving their own. We propose concrete attack strategies
that empower such clients and systematically analyze their effects on the
learning process. Furthermore, we explore complex scenarios involving multiple
dictator clients, including cases where they collaborate, act independently, or
form an alliance in order to ultimately betray one another. For each of these
settings, we provide a theoretical analysis of their impact on the global
model's convergence. Our theoretical algorithms and findings about the complex
scenarios including multiple dictator clients are further supported by
empirical evaluations on both computer vision and natural language processing
benchmarks.

</details>


### [172] [The Lossy Horizon: Error-Bounded Predictive Coding for Lossy Text Compression (Episode I)](https://arxiv.org/abs/2510.22207)
*Nnamdi Aghanya,Jun Li,Kewei Wang*

Main category: cs.LG

TL;DR: 本文介绍了EPC，一种利用掩码语言模型进行有损文本编码的方法，实验表明它在压缩效率和保真度方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在有损域中的应用，以实现更高的压缩比并保持重建保真度。

Method: EPC利用掩码语言模型（MLM）作为解码器，通过预测被掩码的内容并在模型的最高预测不正确时存储最小的、基于排名的修正来实现损失压缩。

Result: EPC在与PM基线和VQ+RE方法的比较中表现优异，能够在更低的比特率下提供更高的保真度。

Conclusion: EPC在保持较高重建保真度的同时，能够以更低的比特率实现更高效的压缩，展示了其在文本压缩领域的优势。

Abstract: Large Language Models (LLMs) can achieve near-optimal lossless compression by
acting as powerful probability models. We investigate their use in the lossy
domain, where reconstruction fidelity is traded for higher compression ratios.
This paper introduces Error-Bounded Predictive Coding (EPC), a lossy text codec
that leverages a Masked Language Model (MLM) as a decompressor. Instead of
storing a subset of original tokens, EPC allows the model to predict masked
content and stores minimal, rank-based corrections only when the model's top
prediction is incorrect. This creates a residual channel that offers continuous
rate-distortion control. We compare EPC to a simpler Predictive Masking (PM)
baseline and a transform-based Vector Quantisation with a Residual Patch
(VQ+RE) approach. Through an evaluation that includes precise bit accounting
and rate-distortion analysis, we demonstrate that EPC consistently dominates
PM, offering superior fidelity at a significantly lower bit rate by more
efficiently utilising the model's intrinsic knowledge.

</details>


### [173] [Mapping Faithful Reasoning in Language Models](https://arxiv.org/abs/2510.22362)
*Jiazheng Li,Andreas Damianou,J Rosser,José Luis Redondo García,Konstantina Palla*

Main category: cs.LG

TL;DR: Concept Walk 是一种用于追踪模型在推理过程中相对于概念方向的内部立场演变的框架，可以帮助识别推理痕迹是否可信。


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought (CoT) traces 虽然承诺了透明性，但它们并不总是内部计算的真实反映。这给监督带来了挑战：实践者可能会误解装饰性推理为真实的推理。

Method: Concept Walk 是一种通用框架，用于追踪模型在推理过程中相对于概念方向的内部立场演变。它在激活空间中操作，将每个推理步骤投影到从对比数据中学到的概念方向上。

Result: 在 'easy' 情况下，扰动的 CoTs 会被快速忽略，表明是装饰性推理；而在 'hard' 情况下，扰动会导致内部激活的持续变化，与忠实推理一致。

Conclusion: Concept Walk 提供了一种方法，通过概念特定的内部动态重新审视忠实性，有助于识别何时可以信任推理痕迹，以及何时它们可能误导实践者。

Abstract: Chain-of-thought (CoT) traces promise transparency for reasoning language
models, but prior work shows they are not always faithful reflections of
internal computation. This raises challenges for oversight: practitioners may
misinterpret decorative reasoning as genuine. We introduce Concept Walk, a
general framework for tracing how a model's internal stance evolves with
respect to a concept direction during reasoning. Unlike surface text, Concept
Walk operates in activation space, projecting each reasoning step onto the
concept direction learned from contrastive data. This allows us to observe
whether reasoning traces shape outcomes or are discarded. As a case study, we
apply Concept Walk to the domain of Safety using Qwen 3-4B. We find that in
'easy' cases, perturbed CoTs are quickly ignored, indicating decorative
reasoning, whereas in 'hard' cases, perturbations induce sustained shifts in
internal activations, consistent with faithful reasoning. The contribution is
methodological: Concept Walk provides a lens to re-examine faithfulness through
concept-specific internal dynamics, helping identify when reasoning traces can
be trusted and when they risk misleading practitioners.

</details>


### [174] [Label Smoothing Improves Gradient Ascent in LLM Unlearning](https://arxiv.org/abs/2510.22376)
*Zirui Pang,Hao Zheng,Zhijie Deng,Ling Li,Zixin Zhong,Jiaheng Wei*

Main category: cs.LG

TL;DR: 本文提出了一种名为Smoothed Gradient Ascent (SGA)的方法，通过将遗忘数据与多个构造的正常数据结合，以可调的平滑率进行学习，从而在保持模型效用的同时实现更稳定的遗忘。实验结果表明，SGA在三个基准测试中表现优于原始的梯度上升方法，并在多个关键指标上达到了前二的性能。


<details>
  <summary>Details</summary>
Motivation: To address the issue of instability in Gradient Ascent (GA), which drives updates in a divergent direction and often results in drastically degraded model utility.

Method: Smoothed Gradient Ascent (SGA) combines the forget data with multiple constructed normal data through a tunable smoothing rate, extending GA from learning solely on the forget data to jointly learning across both forget and normal data.

Result: SGA consistently outperforms the original Gradient Ascent (GA) method across all metrics and achieves top-2 performance among all baseline methods on several key metrics.

Conclusion: SGA consistently outperforms the original Gradient Ascent (GA) method across all metrics and achieves top-2 performance among all baseline methods on several key metrics.

Abstract: LLM unlearning has emerged as a promising approach, aiming to enable models
to forget hazardous/undesired knowledge at low cost while preserving as much
model utility as possible. Among existing techniques, the most straightforward
method is performing Gradient Ascent (GA) w.r.t. the forget data, thereby
forcing the model to unlearn the forget dataset. However, GA suffers from
severe instability, as it drives updates in a divergent direction, often
resulting in drastically degraded model utility. To address this issue, we
propose Smoothed Gradient Ascent (SGA). SGA combines the forget data with
multiple constructed normal data through a tunable smoothing rate. Intuitively,
this extends GA from learning solely on the forget data to jointly learning
across both forget and normal data, enabling more stable unlearning while
better preserving model utility. Theoretically, we provide the theoretical
guidance on the selection of the optimal smoothing rate. Empirically, we
evaluate SGA on three benchmarks: TOFU, Harry Potter, and MUSE-NEWS.
Experimental results demonstrate that SGA consistently outperforms the original
Gradient Ascent (GA) method across all metrics and achieves top-2 performance
among all baseline methods on several key metrics.

</details>


### [175] [Scalable Oversight via Partitioned Human Supervision](https://arxiv.org/abs/2510.22500)
*Ren Yin,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出了一种利用互补标签评估和训练AI系统的方法，无需真实标签。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统在各种任务上接近并超越专家人类表现，获取高质量的人类监督用于评估和训练变得越来越具有挑战性。然而，即使是最优秀的人类专家也只能在单一狭窄领域内具备知识，无法评估先进AI系统在这些超人类任务上的正确性。

Method: 本文提出了一种从互补标签中推导出无偏估计器的方法，并量化了需要多少互补标签才能匹配普通标签的方差。还引入了两个估计器来结合稀缺的普通标签和丰富的互补标签。

Result: 实证结果表明，如果我们有互补标签，就可以在没有真实标签的情况下评估大型语言模型的输出。此外，我们还展示了如何利用这种弱信号训练AI系统。

Conclusion: 本文提出了一种可扩展的监督框架，使我们能够在不需要准备真实标签的情况下评估前沿AI系统。此外，我们展示了如何设计一个代理AI系统，该系统可以利用这种分割的人类监督进行更好的训练。

Abstract: As artificial intelligence (AI) systems approach and surpass expert human
performance across a broad range of tasks, obtaining high-quality human
supervision for evaluation and training becomes increasingly challenging. Our
focus is on tasks that require deep knowledge and skills of multiple domains.
Unfortunately, even the best human experts are knowledgeable only in a single
narrow area, and will not be able to evaluate the correctness of advanced AI
systems on such superhuman tasks. However, based on their narrow expertise,
humans may provide a weak signal, i.e., a complementary label indicating an
option that is incorrect. For example, a cardiologist could state that "this is
not related to cardiology,'' even if they cannot identify the true disease.
Based on this weak signal, we propose a scalable oversight framework that
enables us to evaluate frontier AI systems without the need to prepare the
ground truth. We derive an unbiased estimator of top-1 accuracy from
complementary labels and quantify how many complementary labels are needed to
match the variance of ordinary labels. We further introduce two estimators to
combine scarce ordinary labels with abundant complementary labels. We provide
finite-sample deviation guarantees for both complementary-only and the mixed
estimators. Empirically, we show that we can evaluate the output of large
language models without the ground truth, if we have complementary labels. We
further show that we can train an AI system with such weak signals: we show how
we can design an agentic AI system automatically that can perform better with
this partitioned human supervision. Our code is available at
https://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision.

</details>


### [176] [ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation](https://arxiv.org/abs/2510.22732)
*Jiali Cheng,Anjishnu Kumar,Roshan Lal,Rishi Rajasekaran,Hani Ramezani,Omar Zia Khan,Oleg Rokhlenko,Sunny Chiu-Webster,Gang Hua,Hadi Amiri*

Main category: cs.LG

TL;DR: ATLAS是一种新的网络代理，能够在不进行微调的情况下适应新环境，并在基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的网络代理在没有神经网络微调的情况下无法有效适应新环境，导致执行计划效率低下。

Method: 引入ATLAS（Actor-Critic Task-completion with Look-ahead Action Simulation），一种带有记忆增强的代理，能够通过模拟动作后果在认知空间中制定基于环境模型的计划。

Result: 在WebArena-Lite基准测试中，ATLAS取得了63%的成功率，而之前最先进的系统为53.9%。

Conclusion: ATLAS在WebArena-Lite基准测试中取得了63%的成功率，优于之前发布的最先进的53.9%。模块化架构不需要网站特定的LLM微调。

Abstract: We observe that current state-of-the-art web-agents are unable to effectively
adapt to new environments without neural network fine-tuning, without which
they produce inefficient execution plans due to a lack of awareness of the
structure and dynamics of the new environment. To address this limitation, we
introduce ATLAS (Actor-Critic Task-completion with Look-ahead Action
Simulation), a memory-augmented agent that is able to make plans grounded in a
model of the environment by simulating the consequences of those actions in
cognitive space. Our agent starts by building a "cognitive map" by performing a
lightweight curiosity driven exploration of the environment. The planner
proposes candidate actions; the simulator predicts their consequences in
cognitive space; a critic analyzes the options to select the best roll-out and
update the original plan; and a browser executor performs the chosen action. On
the WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%
success rate for the previously published state-of-the-art. Unlike previous
systems, our modular architecture requires no website-specific LLM fine-tuning.
Ablations show sizable drops without the world-model, hierarchical planner, and
look-ahead-based replanner confirming their complementary roles within the
design of our system

</details>


### [177] [TELL-TALE: Task Efficient LLMs with Task Aware Layer Elimination](https://arxiv.org/abs/2510.22767)
*Omar Naim,Krish Sharma,Nicholas Asher*

Main category: cs.LG

TL;DR: TALE is an inference-time algorithm that prunes transformer layers in LLMs to improve accuracy and reduce computational cost without retraining, offering flexible control over trade-offs between accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to introduce an inference-time algorithm that can prune transformer layers in an LLM without retraining, improving accuracy while reducing computational cost.

Method: TALE is an inference-time algorithm that prunes entire transformer layers in an LLM by directly optimizing task-specific validation performance.

Result: TALE was evaluated on 9 tasks and 5 models, including LLaMA 3.1 8B, Qwen 2.5 7B, Qwen 2.5 0.5B, Mistral 7B, and Lucie 7B, under both zero-shot and few-shot settings. It consistently improves accuracy while reducing computational cost across all benchmarks, and applying TALE during finetuning leads to additional performance gains.

Conclusion: TALE provides flexible user control over trade-offs between accuracy and efficiency, producing smaller, faster, and more accurate models that are also faster to fine-tune while offering new insights into transformer interpretability.

Abstract: In this paper we introduce Tale, Task-Aware Layer Elimination, an
inference-time algorithm that prunes entire transformer layers in an LLM by
directly optimizing task-specific validation performance. We evaluate TALE on 9
tasks and 5 models, including LLaMA 3.1 8B, Qwen 2.5 7B, Qwen 2.5 0.5B, Mistral
7B, and Lucie 7B, under both zero-shot and few-shot settings. Unlike prior
approaches, TALE requires no retraining and consistently improves accuracy
while reducing computational cost across all benchmarks. Furthermore, applying
TALE during finetuning leads to additional performance gains. Finally, TALE
provides flexible user control over trade-offs between accuracy and efficiency.
Mutual information analysis shows that certain layers act as bottlenecks,
degrading task-relevant representations. Tale's selective layer removal
remedies this problem, producing smaller, faster, and more accurate models that
are also faster to fine-tune while offering new insights into transformer
interpretability.

</details>


### [178] [Offline Preference Optimization via Maximum Marginal Likelihood Estimation](https://arxiv.org/abs/2510.22881)
*Saeed Najafi,Alona Fyshe*

Main category: cs.LG

TL;DR: 本文提出了一种基于最大边缘似然估计的偏好优化方法（MMPO），该方法在不同规模的模型上表现出更稳定的性能，并且在保持基础模型的语言能力方面优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 标准的RLHF方法复杂且不稳定，需要一种更简单、更有效的对齐方法。

Method: MMPO通过最大边缘似然估计来重新定义对齐问题，利用偏好对作为样本进行近似，不需要显式的奖励模型和熵最大化。

Result: MMPO在超参数β方面比其他基线方法更稳定，并且在偏好对齐方面表现良好或更好，同时更好地保持了基础模型的通用语言能力。

Conclusion: MMPO在不同规模的模型上表现出更稳定的性能，并且在保持基础模型的语言能力方面优于其他方法。

Abstract: Aligning Large Language Models (LLMs) with human preferences is crucial, but
standard methods like Reinforcement Learning from Human Feedback (RLHF) are
often complex and unstable. In this work, we propose a new, simpler approach
that recasts alignment through the lens of Maximum Marginal Likelihood (MML)
estimation. Our new MML based Preference Optimization (MMPO) maximizes the
marginal log-likelihood of a preferred text output, using the preference pair
as samples for approximation, and forgoes the need for both an explicit reward
model and entropy maximization. We theoretically demonstrate that MMPO
implicitly performs preference optimization, producing a weighted gradient that
naturally up-weights chosen responses over rejected ones. Across models ranging
from 135M to 8B parameters, we empirically show that MMPO: 1) is more stable
with respect to the hyperparameter $\beta$ compared to alternative baselines,
and 2) achieves competitive or superior preference alignment while better
preserving the base model's general language capabilities. Through a series of
ablation experiments, we show that this improved performance is indeed
attributable to MMPO's implicit preference optimization within the gradient
updates.

</details>


### [179] [Can Language Models Compose Skills In-Context?](https://arxiv.org/abs/2510.22993)
*Zidong Liu,Zhuoyan Xu,Zhenmei Shi,Yingyu Liang*

Main category: cs.LG

TL;DR: 研究探讨了语言模型在上下文中组合基本技能的能力，发现简单的任务示例可能对性能产生负面影响，并提出了一种改进的探测任务方法。


<details>
  <summary>Details</summary>
Motivation: 组合基本技能以完成复合任务对于现代智能系统至关重要。然而，在上下文中组合能力的挑战性比标准设置更大，因为在标准设置中，技能及其组合可以在训练中学习。

Method: 研究系统地测试了各种代表性的开源语言模型，利用语言和逻辑任务来探测组合能力。

Result: 实验结果揭示了简单的任务示例可能对性能产生意外的负面影响，因为模型通常难以正确识别和组合技能，即使有思维链示例。

Conclusion: 研究发现，简单的任务示例可能对性能产生意外的负面影响，因为模型通常难以正确识别和组合技能。理论分析进一步表明，将示例与组合中的相应步骤对齐至关重要。这激发了一种用于探测任务的方法，其改进的性能为我们的见解提供了积极支持。

Abstract: Composing basic skills from simple tasks to accomplish composite tasks is
crucial for modern intelligent systems. We investigate the in-context
composition ability of language models to perform composite tasks that combine
basic skills demonstrated in in-context examples. This is more challenging than
the standard setting, where skills and their composition can be learned in
training. We conduct systematic experiments on various representative
open-source language models, utilizing linguistic and logical tasks designed to
probe composition abilities. The results reveal that simple task examples can
have a surprising negative impact on the performance, because the models
generally struggle to recognize and assemble the skills correctly, even with
Chain-of-Thought examples. Theoretical analysis further shows that it is
crucial to align examples with the corresponding steps in the composition. This
inspires a method for the probing tasks, whose improved performance provides
positive support for our insights.

</details>


### [180] [Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts](https://arxiv.org/abs/2510.23027)
*Di Zhang,Xun Wu,Shaohan Huang,Yaru Hao,Li Dong,Zewen Chi,Zhifang Sui,Furu Wei*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于路由器感知的方法来优化离策略RL中的重要性采样权重，以提高MoE模型的训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在密集模型上，而Mixture-of-Experts（MoE）架构的RL训练仍缺乏探索。为了应对MoE训练中常见的不稳定性问题，我们提出了一个新颖的路由器感知方法。

Method: 我们提出了一种新的基于路由器感知的方法来优化离策略RL中的重要性采样（IS）权重。具体来说，我们设计了一个由路由器logits指导的重新缩放策略，有效降低了梯度方差并缓解了训练发散。

Result: 实验结果表明，我们的方法显著提高了MoE模型的收敛稳定性和最终性能。

Conclusion: 我们的方法显著提高了MoE模型的收敛稳定性和最终性能，突显了针对MoE架构定制的RL算法创新的潜力，并为大规模专家模型的高效训练提供了有前景的方向。

Abstract: Recent advances in reinforcement learning (RL) have substantially improved
the training of large-scale language models, leading to significant gains in
generation quality and reasoning ability. However, most existing research
focuses on dense models, while RL training for Mixture-of-Experts (MoE)
architectures remains underexplored. To address the instability commonly
observed in MoE training, we propose a novel router-aware approach to optimize
importance sampling (IS) weights in off-policy RL. Specifically, we design a
rescaling strategy guided by router logits, which effectively reduces gradient
variance and mitigates training divergence. Experimental results demonstrate
that our method significantly improves both the convergence stability and the
final performance of MoE models, highlighting the potential of RL algorithmic
innovations tailored to MoE architectures and providing a promising direction
for efficient training of large-scale expert models.

</details>


### [181] [Rethinking GSPO: The Perplexity-Entropy Equivalence](https://arxiv.org/abs/2510.23142)
*Chi Liu*

Main category: cs.LG

TL;DR: 本文通过信息理论解释了GSPO的权重机制，并在数学推理任务中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为了提供对GSPO的长度归一化重要性比率的新视角，并通过信息理论来解释GSPO的算法权重。

Method: 通过建立GSPO的长度归一化重要性比率与信息理论量之间的联系，将GSPO的序列级权重表示为逆困惑度比和指数交叉熵变化。

Result: 验证了数学等价性和方差预测，并展示了GSPO在数学推理任务中的有效性。

Conclusion: GSPO的权重可以通过困惑度比率进行信息理论解释，这有助于理解其经验性质，并在数学推理任务中通过控制实验进行了验证。

Abstract: We provide a new perspective on GSPO's length-normalized importance ratios by
establishing their connection to information-theoretic quantities. We show that
GSPO's sequence-level weight $s(\theta) =
(\pi_\theta/\pi_{\theta_{\text{old}}})^{1/|y|}$ can be equivalently expressed
as the inverse perplexity ratio
$\text{PPL}_{\theta_{\text{old}}}/\text{PPL}_\theta$ and as the exponential
cross-entropy change $\exp(\Delta H)$. While the perplexity-entropy
relationship follows from standard definitions, this observation provides a
useful lens for understanding GSPO: the algorithm weights policy gradient
updates by perplexity ratios, offering an information-theoretic interpretation
of the importance weights. This perspective helps explain GSPO's empirical
properties, including log-domain variance reduction through geometric averaging
and stability in training mixture-of-experts models. We validate the
mathematical equivalences and variance predictions through controlled
experiments on mathematical reasoning tasks.

</details>


### [182] [PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation Performance at Unseen Pre-Training Budgets](https://arxiv.org/abs/2510.23198)
*Etienne Goffinet,Shane Bergsma,Avraham Sheinin,Natalia Vassilieva,Shaheer Muhammad,Preslav Nakov,Gurpreet Gosal*

Main category: cs.LG

TL;DR: 本文提出了PTPP-aware适应尺度定律，使预训练预算成为显式变量，从而能够准确预测未见过的PTPP下的适应损失。此外，展示了在计算限制下满足目标和遗忘约束的实用用例，如规划重放比例和适应令牌预算。


<details>
  <summary>Details</summary>
Motivation: 现有的CPT扩展定律通常假设固定的预训练预算，这限制了它们预测不同tokens-per-parameter (PTPP)下适应结果的能力。因此，需要一种更灵活的方法来处理不同PTPP下的适应问题。

Method: 本文提出了PTPP-aware适应尺度定律，通过将预训练预算作为显式变量，以准确预测未见过的PTPP下的适应损失。此外，还展示了在计算限制下满足目标和遗忘约束的实用用例，如规划重放比例和适应令牌预算。

Result: 在多语言设置（英语/阿拉伯语→法语）中，PTPP-aware公式在早期阶段（PTPP=15,31）训练时能够预测PTPP=279的目标损失，并且在指标（Huber-on-log, MAErel, calibration slope）上优于PTPP-agnostic DCPT转移基线。完整的诊断（RMSE, MAPE）见附录。

Conclusion: 本文提出了PTPP-aware适应尺度定律，使预训练预算成为显式变量，从而能够准确预测未见过的PTPP下的适应损失。此外，展示了在计算限制下满足目标和遗忘约束的实用用例，如规划重放比例和适应令牌预算。

Abstract: Continual pre-training (CPT) for domain adaptation must balance target-domain
gains with stability on the base domain. Existing CPT scaling laws typically
assume a fixed pre-training budget, which limits their ability to forecast
adaptation outcomes for models trained at different tokens-per-parameter
(PTPP). We present \emph{PTPP-aware} adaptation scaling laws that make the
pre-training budget an explicit variable, enabling accurate \emph{prediction}
of adaptation loss at unseen \ptpp. On a multilingual setup (English/Arabic
$\rightarrow$ French), PTPP-aware formulations trained on early stages
(\ptpp{}=\{15,31\}) predict target loss at \ptpp{}=279 and outperform a
PTPP-agnostic \dcpt{} transfer baseline on metrics (Huber-on-log,
MAE$_\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in
the appendix. Beyond forecasting, we show a practical use case: planning replay
ratios and adaptation token budgets that satisfy target and forgetting
constraints under compute limits.

</details>


### [183] [A U-Net and Transformer Pipeline for Multilingual Image Translation](https://arxiv.org/abs/2510.23554)
*Siddharth Sahay,Radhika Agarwal*

Main category: cs.LG

TL;DR: 本文提出了一种端到端的多语言翻译管道，结合了自定义的U-Net模型、Tesseract引擎和Transformer模型，以实现从图像中直接翻译文本。


<details>
  <summary>Details</summary>
Motivation: 传统系统依赖于单体预训练模型，而本文提出了一种完全定制和可适应的架构。

Method: 该方法结合了自定义的U-Net模型用于文本检测，Tesseract引擎用于文本识别，以及从零开始训练的序列到序列Transformer模型用于神经机器翻译。

Result: 系统在文本检测准确率、文本识别质量和翻译性能（通过BLEU分数评估）方面表现出色。

Conclusion: 该系统展示了自定义构建的系统的可行性，可以有效地从图像中直接翻译文本。

Abstract: This paper presents an end-to-end multilingual translation pipeline that
integrates a custom U-Net for text detection, the Tesseract engine for text
recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for
Neural Machine Translation (NMT). Our approach first utilizes a U-Net model,
trained on a synthetic dataset , to accurately segment and detect text regions
from an image. These detected regions are then processed by Tesseract to
extract the source text. This extracted text is fed into a custom Transformer
model trained from scratch on a multilingual parallel corpus spanning 5
languages. Unlike systems reliant on monolithic pre-trained models, our
architecture emphasizes full customization and adaptability. The system is
evaluated on its text detection accuracy, text recognition quality, and
translation performance via BLEU scores. The complete pipeline demonstrates
promising results, validating the viability of a custom-built system for
translating text directly from images.

</details>


### [184] [Variational Masked Diffusion Models](https://arxiv.org/abs/2510.23606)
*Yichi Zhang,Alex Schwing,Zhizhen Zhao*

Main category: cs.LG

TL;DR: VMD是一种改进的掩码扩散模型，通过引入潜在变量来更好地捕捉标记间的依赖关系，从而提升生成质量和全局一致性。


<details>
  <summary>Details</summary>
Motivation: 标准掩码扩散无法有效捕捉同时预测的标记之间的依赖关系，导致在依赖关系重要的情况下生成质量下降。

Method: 提出了一种名为变分掩码扩散（VMD）的框架，该框架在掩码扩散过程中引入了潜在变量以显式建模标记之间的依赖关系。

Result: 在合成数据集、数独谜题和文本数据集上的实验表明，VMD能够成功学习传统掩码扩散无法捕捉的依赖关系，并提高全局一致性。

Conclusion: VMD通过引入变分推理增强了掩码扩散模型，提高了生成质量和依赖关系意识。

Abstract: Masked diffusion models have recently emerged as a flexible framework for
discrete generative modeling. However, a key limitation of standard masked
diffusion is its inability to effectively capture dependencies among tokens
that are predicted concurrently, leading to degraded generation quality when
dependencies among tokens are important. To explicitly model dependencies among
tokens, we propose Variational Masked Diffusion (VMD), a framework that
introduces latent variables into the masked diffusion process. Through
controlled experiments on synthetic datasets, we demonstrate that VMD
successfully learns dependencies that conventional masked diffusion fails to
capture. We further validate the effectiveness of our approach on Sudoku
puzzles and text datasets, where learning of dependencies among tokens improves
global consistency. Across these domains, VMD enhances both generation quality
and dependency awareness, highlighting the value of integrating variational
inference into masked diffusion. Our code is available at:
https://riccizz.github.io/VMD.

</details>

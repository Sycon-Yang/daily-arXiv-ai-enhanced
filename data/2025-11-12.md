<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 60]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AI](#cs.AI) [Total: 10]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Preliminary Study of RAG for Taiwanese Historical Archives](https://arxiv.org/abs/2511.07445)
*Claire Lin,Bo-Han Feng,Xuanjun Chen,Te-Lun Yang,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.CL

TL;DR: 本文研究了RAG在台湾历史档案中的应用，发现元数据集成能提高性能，但存在生成幻觉和处理复杂查询的困难。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG已被认为是知识密集型任务的有前途的方法，但很少有研究探讨其在台湾历史档案中的应用。

Method: 本文系统地研究了查询特征和元数据集成策略对检索质量、答案生成和整体系统性能的影响。

Result: 结果表明，早期阶段的元数据集成可以提高检索和答案准确性，但也揭示了RAG系统的持久挑战。

Conclusion: 本文展示了RAG管道在台湾历史档案中的初步研究，并揭示了RAG系统在处理时间或多跳历史查询时的持续挑战，如生成过程中的幻觉问题。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.

</details>


### [2] [Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey](https://arxiv.org/abs/2511.07448)
*Fatemeh Shahhosseini,Arash Marioriyad,Ali Momen,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban,Shaghayegh Haghjooy Javanmard*

Main category: cs.CL

TL;DR: 本文综述了基于LLM的科学创意生成方法，分析了它们如何在创造力与科学严谨性之间取得平衡，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 科学创意生成是科学发现的核心，驱动人类进步，但LLM在创意能力方面仍不一致且理解不足。本文旨在提供一种结构化的综合分析，以平衡创造力与科学严谨性。

Method: 本文对基于LLM的科学创意生成方法进行了结构化的综合分析，将现有方法分为五种互补的类别：外部知识增强、基于提示的分布引导、推理时扩展、多代理协作和参数级适应。

Result: 本文通过Boden的创造力分类法和Rhodes的4Ps框架，对不同方法在创造力方面的贡献进行了分析，明确了该领域的现状并提出了未来方向。

Conclusion: 本文通过将方法论进展与创造力框架相匹配，澄清了该领域的现状，并指出了在科学发现中可靠、系统和变革性应用LLM的关键方向。

Abstract: Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.

</details>


### [3] [GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models](https://arxiv.org/abs/2511.07457)
*Jiarui Feng,Donghong Cai,Yixin Chen,Muhan Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为GRIP的新框架，使大型语言模型能够通过微调任务内化复杂的关系信息，从而在不访问原始图的情况下执行多种图相关任务。


<details>
  <summary>Details</summary>
Motivation: 适应大型语言模型（LLMs）以有效地处理结构化数据（如知识图谱或网络数据）仍然是一个具有挑战性的问题。

Method: 我们提出了GRIP框架，通过精心设计的微调任务，使LLMs能够内化复杂的图关系信息。

Result: 实验结果表明，我们的方法在处理各种图相关任务时表现出色。

Conclusion: 我们的方法在多个基准测试中验证了其有效性和效率。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.

</details>


### [4] [REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment](https://arxiv.org/abs/2511.07458)
*Priyanka Mudgal*

Main category: cs.CL

TL;DR: 本文介绍了REFLEX，一种基于大型语言模型（LLM）判断的无参考评估指标，用于评估日志摘要。REFLEX能够无需黄金标准参考或人工标注，通过LLM作为零样本评估者来评估摘要质量，具有稳定、可解释和细粒度的评估效果，并在实际应用中提供了可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap.

Method: REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations.

Result: REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization datasets, and more effectively distinguishes model outputs than traditional metrics.

Conclusion: REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.

Abstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.

</details>


### [5] [It Takes Two: A Dual Stage Approach for Terminology-Aware Translation](https://arxiv.org/abs/2511.07461)
*Akshat Singh Jaswal*

Main category: cs.CL

TL;DR: 本文介绍了一种新的两阶段架构DuTerm，用于术语约束机器翻译。该系统结合了通过在大规模合成数据上微调的术语感知NMT模型和一个基于提示的LLM进行后编辑。评估显示，LLM的灵活、上下文驱动的术语处理通常比严格的约束执行产生更高质量的翻译。


<details>
  <summary>Details</summary>
Motivation: 本文介绍了DuTerm，这是一种用于术语约束机器翻译的新颖两阶段架构。

Method: DuTerm结合了一个通过在大规模合成数据上微调的术语感知NMT模型和一个基于提示的LLM进行后编辑。

Result: 我们在英语到德语、英语到西班牙语和英语到俄语上评估了DuTerm，并使用了WMT 2025术语共享任务语料库。

Conclusion: 我们的结果突显了一个关键的权衡，揭示了LLM在作为上下文驱动的修改器而不是生成器时，对于高质量翻译最为有效。

Abstract: This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.

</details>


### [6] [Motif 2 12.7B technical report](https://arxiv.org/abs/2511.07464)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.CL

TL;DR: Motif-2-12.7B is an open-weight foundation model that enhances efficiency through architectural innovation and system-level optimization, achieving competitive performance compared to larger models.


<details>
  <summary>Details</summary>
Motivation: To push the efficiency frontier of large language models by combining architectural innovation with system-level optimization, designed for scalable language understanding and robust instruction generalization under constrained compute budgets.

Method: The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm. Post-training employs a three-stage supervised fine-tuning pipeline.

Result: Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. It yields significant throughput and memory efficiency gains in large-scale distributed environments.

Conclusion: Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.

Abstract: We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.

</details>


### [7] [Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models](https://arxiv.org/abs/2511.07498)
*Xin Liu,Qiyang Song,Qihang Zhou,Haichao Du,Shaowen Xu,Wenbo Jiang,Weijuan Zhang,Xiaoqi Jia*

Main category: cs.CL

TL;DR: 本文研究了多头自注意力在支持多语言处理中的作用，并提出了一种方法来识别多语言能力的注意力头重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管多头自注意力在许多领域已被证明至关重要，但其在多语言能力中的作用仍缺乏研究。

Method: 我们提出了语言注意力头重要性评分（LAHIS），这是一种有效且高效的方法，通过一次前向和反向传递来识别多语言能力的注意力头重要性。

Result: 我们在Aya-23-8B、Llama-3.2-3B和Mistral-7B-v0.1上应用LAHIS，发现了语言特定和语言通用的头部。语言特定头部有助于跨语言注意力转移，减少非目标语言生成问题。我们还引入了一种轻量级的适应方法，学习一个软头掩码来调节语言头部的注意力输出，仅需20个可调参数即可提高XQuAD准确性。

Conclusion: 我们的工作从多头自注意力的角度增强了大型语言模型的可解释性和多语言能力。

Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.

</details>


### [8] [LLM Optimization Unlocks Real-Time Pairwise Reranking](https://arxiv.org/abs/2511.07555)
*Jingyu Wu,Aditya Shrivastava,Jing Zhu,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Efficiently reranking documents retrieved from information retrieval (IR) pipelines to enhance overall quality of Retrieval-Augmented Generation (RAG) system remains an important yet challenging problem. Recent studies have highlighted the importance of Large Language Models (LLMs) in reranking tasks. In particular, Pairwise Reranking Prompting (PRP) has emerged as a promising plug-and-play approach due to its usability and effectiveness. However, the inherent complexity of the algorithm, coupled with the high computational demands and latency incurred due to LLMs, raises concerns about its feasibility in real-time applications. To address these challenges, this paper presents a focused study on pairwise reranking, demonstrating that carefully applied optimization methods can significantly mitigate these issues. By implementing these methods, we achieve a remarkable latency reduction of up to 166 times, from 61.36 seconds to 0.37 seconds per query, with an insignificant drop in performance measured by Recall@k. Our study highlights the importance of design choices that were previously overlooked, such as using smaller models, limiting the reranked set, using lower precision, reducing positional bias with one-directional order inference, and restricting output tokens. These optimizations make LLM-based reranking substantially more efficient and feasible for latency-sensitive, real-world deployments.

</details>


### [9] [LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives](https://arxiv.org/abs/2511.07641)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 本研究评估了三个荷兰特定的LLMs与传统工具在佛兰德语情感预测中的表现，发现LLMs表现不佳，强调了为低资源语言变体开发定制评估框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 理解日常语言中的情感细微差别对于计算语言学和情感研究至关重要。虽然传统的基于词典的工具如LIWC和Pattern作为基础工具，但大型语言模型（LLMs）承诺增强上下文理解。

Method: 我们评估了三个荷兰特定的LLMs（ChocoLlama-8B-Instruct、Reynaerde-7B-chat和GEITje-7B-ultra）与LIWC和Pattern在佛兰德语中的情感预测能力。

Result: 令人惊讶的是，尽管架构有所改进，但荷兰调整的LLMs的表现不如传统方法，Pattern表现出更好的性能。

Conclusion: 我们的结果强调了为低资源语言变体开发文化和语言上定制的评估框架的必要性，同时质疑了当前的LLM微调方法是否足以应对日常语言使用中发现的细微情感表达。

Abstract: Understanding emotional nuances in everyday language is crucial for computational linguistics and emotion research. While traditional lexicon-based tools like LIWC and Pattern have served as foundational instruments, Large Language Models (LLMs) promise enhanced context understanding. We evaluated three Dutch-specific LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, and GEITje-7B-ultra) against LIWC and Pattern for valence prediction in Flemish, a low-resource language variant. Our dataset comprised approximately 25000 spontaneous textual responses from 102 Dutch-speaking participants, each providing narratives about their current experiences with self-assessed valence ratings (-50 to +50). Surprisingly, despite architectural advancements, the Dutch-tuned LLMs underperformed compared to traditional methods, with Pattern showing superior performance. These findings challenge assumptions about LLM superiority in sentiment analysis tasks and highlight the complexity of capturing emotional valence in spontaneous, real-world narratives. Our results underscore the need for developing culturally and linguistically tailored evaluation frameworks for low-resource language variants, while questioning whether current LLM fine-tuning approaches adequately address the nuanced emotional expressions found in everyday language use.

</details>


### [10] [Revisiting NLI: Towards Cost-Effective and Human-Aligned Metrics for Evaluating LLMs in Question Answering](https://arxiv.org/abs/2511.07659)
*Sai Shridhar Balamurali,Lu Cheng*

Main category: cs.CL

TL;DR: 研究提出了一种轻量级的自然语言推理评分方法，并引入了一个新的基准测试DIVER-QA，以验证这些指标的人类一致性。结果表明，这种低成本的方法在长格式问答任务中表现与GPT-4o相当。


<details>
  <summary>Details</summary>
Motivation: Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas 'LLM-as-Judge' scoring is computationally expensive.

Method: re-evaluate a lightweight alternative -- off-the-shelf Natural Language Inference (NLI) scoring augmented by a simple lexical-match flag

Result: this decades-old technique matches GPT-4o's accuracy (89.9%) on long-form QA, while requiring orders-of-magnitude fewer parameters.

Conclusion:  inexpensive NLI-based evaluation remains competitive and offer DIVER-QA as an open resource for future metric research.

Abstract: Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas "LLM-as-Judge" scoring is computationally expensive. We re-evaluate a lightweight alternative -- off-the-shelf Natural Language Inference (NLI) scoring augmented by a simple lexical-match flag and find that this decades-old technique matches GPT-4o's accuracy (89.9%) on long-form QA, while requiring orders-of-magnitude fewer parameters. To test human alignment of these metrics rigorously, we introduce DIVER-QA, a new 3000-sample human-annotated benchmark spanning five QA datasets and five candidate LLMs. Our results highlight that inexpensive NLI-based evaluation remains competitive and offer DIVER-QA as an open resource for future metric research.

</details>


### [11] [Stress Testing Factual Consistency Metrics for Long-Document Summarization](https://arxiv.org/abs/2511.07689)
*Zain Muhammad Mujahid,Dustin Wright,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文评估了现有无参考事实性度量在长文档摘要中的可靠性，发现它们在信息密集的声明中表现不佳，并提出了改进的方向。


<details>
  <summary>Details</summary>
Motivation: 评估抽象文本摘要的事实一致性仍然是一个重大挑战，特别是在长文档中，传统指标在输入长度限制和长距离依赖方面存在困难。

Method: 我们系统地评估了六种广泛使用的无参考事实性度量在长文档设置中的可靠性，并通过七种保持事实性的扰动来测试度量的鲁棒性。

Result: 现有的短文本度量在语义等效摘要上产生不一致的分数，并且在信息密集的声明中可靠性下降，这些声明的内容与源文档的许多部分在语义上相似。

Conclusion: 我们的结果突出了改进事实性评估的具体方向，包括多跨度推理、上下文感知校准以及在保持意义的变化上进行训练，以提高长文本摘要的鲁棒性。

Abstract: Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at https://github.com/zainmujahid/metricEval-longSum.

</details>


### [12] [CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences](https://arxiv.org/abs/2511.07691)
*Rhitabrat Pokharel,Yufei Tao,Ameeta Agrawal*

Main category: cs.CL

TL;DR: CAPO is a new preference optimization method that improves multilingual LLM alignment by dynamically adjusting the learning signal based on confidence in preference pairs.


<details>
  <summary>Details</summary>
Motivation: Preference optimization methods like DPO often fail to generalize robustly to multilingual settings, requiring a more effective alternative for aligning LLMs with human preferences in multilingual contexts.

Method: CAPO replaces DPO's fixed treatment of preference pairs with a dynamic loss scaling mechanism based on a relative reward, modulating the learning signal according to the confidence in each preference pair.

Result: CAPO enhances robustness to noisy or low-margin comparisons, typically encountered in multilingual text, and improves alignment by widening the gap between preferred and dispreferred responses across languages.

Conclusion: CAPO outperforms existing preference optimization baselines by at least 16% in reward accuracy and improves alignment by widening the gap between preferred and dispreferred responses across languages.

Abstract: Preference optimization is a critical post-training technique used to align large language models (LLMs) with human preferences, typically by fine-tuning on ranked response pairs. While methods like Direct Preference Optimization (DPO) have proven effective in English, they often fail to generalize robustly to multilingual settings. We propose a simple yet effective alternative, Confidence-Aware Preference Optimization (CAPO), which replaces DPO's fixed treatment of preference pairs with a dynamic loss scaling mechanism based on a relative reward. By modulating the learning signal according to the confidence in each preference pair, CAPO enhances robustness to noisy or low-margin comparisons, typically encountered in multilingual text. Empirically, CAPO outperforms existing preference optimization baselines by at least 16% in reward accuracy, and improves alignment by widening the gap between preferred and dispreferred responses across languages.

</details>


### [13] [Critical Confabulation: Can LLMs Hallucinate for Social Good?](https://arxiv.org/abs/2511.07722)
*Peiqi Sui,Eamon Duede,Hoyt Long,Richard Jean So*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型的幻觉如何被用于填补档案中的缺失，并重建不同的但有证据支持的历史叙述。研究结果显示，受控且明确的虚构可以支持大型语言模型在知识生产中的应用，同时保持历史准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会产生幻觉，但一些虚构如果经过仔细界定，可以具有社会效用。我们提出了批判性虚构（受到文学和社会理论中的批判性虚构启发），即利用大型语言模型的幻觉来填补由于社会和政治不平等而造成的档案中的缺失，并为历史的“隐秘人物”重建不同但有证据支持的叙述。

Method: 我们通过一个开放式的叙述填空任务来模拟这些缺口：要求大型语言模型生成一个被遮蔽的事件，在一个以角色为中心的时间线中，该时间线来源于未发表文本的小说语料库。我们评估了经过审计（数据污染）的、完全开放的模型（OLMo-2系列）以及未经审计的开放权重和专有基线，在各种旨在引发受控和有用虚构的提示下。

Result: 我们的研究结果验证了大型语言模型在进行批判性虚构方面的基础叙事理解能力，并展示了如何通过受控且明确的虚构支持大型语言模型在知识生产中的应用，而不会将推测与历史准确性和真实性相混淆。

Conclusion: 我们的研究验证了大型语言模型在进行批判性虚构方面的基础叙事理解能力，并展示了如何通过受控且明确的虚构支持大型语言模型在知识生产中的应用，而不会将推测与历史准确性和真实性相混淆。

Abstract: LLMs hallucinate, yet some confabulations can have social affordances if carefully bounded. We propose critical confabulation (inspired by critical fabulation from literary and social theory), the use of LLM hallucinations to "fill-in-the-gap" for omissions in archives due to social and political inequality, and reconstruct divergent yet evidence-bound narratives for history's "hidden figures". We simulate these gaps with an open-ended narrative cloze task: asking LLMs to generate a masked event in a character-centric timeline sourced from a novel corpus of unpublished texts. We evaluate audited (for data contamination), fully-open models (the OLMo-2 family) and unaudited open-weight and proprietary baselines under a range of prompts designed to elicit controlled and useful hallucinations. Our findings validate LLMs' foundational narrative understanding capabilities to perform critical confabulation, and show how controlled and well-specified hallucinations can support LLM applications for knowledge production without collapsing speculation into a lack of historical accuracy and fidelity.

</details>


### [14] [Back to the Future: The Role of Past and Future Context Predictability in Incremental Language Production](https://arxiv.org/abs/2511.07752)
*Shiva Upadhye,Richard Futrell*

Main category: cs.CL

TL;DR: 本文研究了自然语言生产中的后向可预测性效应，提出了一种新的信息论度量方法，并通过两个研究展示了其效果，揭示了过去和未来语境在词语编码和选择中的作用。


<details>
  <summary>Details</summary>
Motivation: 研究自然语言生产中的后向可预测性效应，这可能与未来规划有关，同时探索上下文可预测性效应与句子规划机制之间的联系。

Method: 使用改进的度量方法和更强大的语言模型，在自然语言语料库中研究了后向可预测性效应，并引入了一种新的基于信息论的可预测性度量方法，该方法结合了未来和过去语境的可预测性。

Result: 所提出的概念性替代方法在两个研究中产生了定性相似的效果，并通过替换错误的细粒度分析表明，不同的错误类型暗示了说话者在词汇规划过程中如何优先考虑形式、意义和基于语境的信息。

Conclusion: 这些发现阐明了过去和未来语境在说话者编码和选择词语中的功能角色，为上下文可预测性效应和句子规划机制之间架起了桥梁。

Abstract: Contextual predictability shapes both the form and choice of words in online language production. The effects of the predictability of a word given its previous context are generally well-understood in both production and comprehension, but studies of naturalistic production have also revealed a poorly-understood backward predictability effect of a word given its future context, which may be related to future planning. Here, in two studies of naturalistic speech corpora, we investigate backward predictability effects using improved measures and more powerful language models, introducing a new principled and conceptually motivated information-theoretic predictability measure that integrates predictability from both the future and the past context. Our first study revisits classic predictability effects on word duration. Our second study investigates substitution errors within a generative framework that independently models the effects of lexical, contextual, and communicative factors on word choice, while predicting the actual words that surface as speech errors. We find that our proposed conceptually-motivated alternative to backward predictability yields qualitatively similar effects across both studies. Through a fine-grained analysis of substitution errors, we further show that different kinds of errors are suggestive of how speakers prioritize form, meaning, and context-based information during lexical planning. Together, these findings illuminate the functional roles of past and future context in how speakers encode and choose words, offering a bridge between contextual predictability effects and the mechanisms of sentence planning.

</details>


### [15] [Design, Results and Industry Implications of the World's First Insurance Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2511.07794)
*Hua Zhou,Bing Ma,Yufei Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 本文介绍了CUFEInse v1.0的构建方法、多维评价体系和底层设计哲学，通过评估11个主流大语言模型，揭示了通用模型在保险领域的普遍瓶颈，并展示了领域特定训练的优势与不足，为保险大模型的优化和发展提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 本文旨在建立一个专业的评估基准，以填补保险领域在专业评估方面的空白，并为学术界和工业界提供一个专业、系统和权威的评估工具。同时，通过评估大模型在保险领域的表现，揭示其在专业场景中的常见瓶颈，为模型优化和工业模型选择提供参考。

Method: 本文全面阐述了CUFEInse v1.0的构建方法、多维评价体系和底层设计哲学。遵循“量化导向、专家驱动、多验证”的原则，该基准建立了涵盖5个核心维度、54个子指标和14,430个高质量问题的评估框架，包括保险理论知识、行业理解、安全与合规、智能代理应用和逻辑严谨性。基于此基准，对11个主流大语言模型进行了全面评估。

Result: 评估结果表明，通用模型在精算能力、合规适应等方面存在普遍瓶颈，而高质量领域特定训练在保险垂直场景中表现出显著优势，但在业务适应性和合规性方面存在不足。评估还准确识别了当前大模型在保险精算、承保与理赔推理和合规营销文案等方面的常见瓶颈。

Conclusion: CUFEInse的建立不仅填补了保险领域专业评估基准的空白，为学术界和工业界提供了一个专业、系统和权威的评估工具，而且其构建理念和方法也为垂直领域大模型的评估范式提供了重要参考，成为学术模型优化和工业模型选择的权威参考。最后，论文展望了评估基准的未来迭代方向以及保险大模型'领域适应+推理增强'的核心发展方向。

Abstract: This paper comprehensively elaborates on the construction methodology, multi-dimensional evaluation system, and underlying design philosophy of CUFEInse v1.0. Adhering to the principles of "quantitative-oriented, expert-driven, and multi-validation," the benchmark establishes an evaluation framework covering 5 core dimensions, 54 sub-indicators, and 14,430 high-quality questions, encompassing insurance theoretical knowledge, industry understanding, safety and compliance, intelligent agent application, and logical rigor. Based on this benchmark, a comprehensive evaluation was conducted on 11 mainstream large language models. The evaluation results reveal that general-purpose models suffer from common bottlenecks such as weak actuarial capabilities and inadequate compliance adaptation. High-quality domain-specific training demonstrates significant advantages in insurance vertical scenarios but exhibits shortcomings in business adaptation and compliance. The evaluation also accurately identifies the common bottlenecks of current large models in professional scenarios such as insurance actuarial, underwriting and claim settlement reasoning, and compliant marketing copywriting. The establishment of CUFEInse not only fills the gap in professional evaluation benchmarks for the insurance field, providing academia and industry with a professional, systematic, and authoritative evaluation tool, but also its construction concept and methodology offer important references for the evaluation paradigm of large models in vertical fields, serving as an authoritative reference for academic model optimization and industrial model selection. Finally, the paper looks forward to the future iteration direction of the evaluation benchmark and the core development direction of "domain adaptation + reasoning enhancement" for insurance large models.

</details>


### [16] [From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory](https://arxiv.org/abs/2511.07800)
*Siyu Xia,Zekun Xu,Jiajun Chai,Wentian Fan,Yan Song,Xiaohan Wang,Guojun Yin,Wei Lin,Haifeng Zhang,Jun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的图记忆框架，用于增强LLM代理的战略推理能力，并通过强化学习优化策略，从而提高其在复杂环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM经验获取方式存在灾难性遗忘和有限可解释性（隐式记忆）或缺乏适应性（显式记忆）。因此，需要一种新的方法来更好地利用先前经验来指导当前决策。

Method: 引入了代理中心、可训练的多层图记忆框架，将原始代理轨迹抽象为结构化的决策路径，并进一步提炼为高层次的人类可解释的战略元认知。此外，提出了基于强化的权重优化过程，根据下游任务的奖励反馈估计每个元认知的经验效用，并通过元认知提示将这些优化策略动态整合到LLM代理的训练循环中。

Result: 该可学习的图记忆框架实现了稳健的泛化能力，提高了LLM代理的战略推理性能，并在强化学习训练中提供了持续的好处。

Conclusion: 该研究提出了一种新型的以代理为中心、可训练的多层图记忆框架，通过上下文记忆增强了LLM利用参数信息的能力，并在强化学习训练中提供了持续的好处。

Abstract: Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.

</details>


### [17] [AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys](https://arxiv.org/abs/2511.07871)
*Chenxi Lin,Weikang Yuan,Zhuoren Jiang,Biao Huang,Ruitao Zhang,Jianan Ge,Yueqian Xu,Jianxing Yu*

Main category: cs.CL

TL;DR: 本文介绍了AlignSurvey，这是一个系统地复制和评估使用大型语言模型（LLMs）的完整社会调查流程的基准。它提供了任务特定的评估指标，以评估对齐精度、一致性和公平性，并构建了一个多层数据集架构来支持AlignSurvey。所有数据集、模型和工具都可在GitHub和HuggingFace上获得，以支持透明和社会负责任的研究。


<details>
  <summary>Details</summary>
Motivation: 传统调查面临固定问题格式、高成本、有限适应性和难以确保跨文化等价性的挑战。虽然最近的研究探索了使用大型语言模型（LLMs）来模拟调查回答，但大多数仅限于结构化问题，忽略了整个调查过程，并且由于训练数据偏差可能会低估边缘群体。因此，本文旨在引入AlignSurvey，以系统地复制和评估使用LLMs的完整社会调查流程。

Method: 本文提出了AlignSurvey，这是一个用于评估大型语言模型在社会调查流程中的表现的基准。它包括四个任务，分别对应关键的调查阶段，并提供任务特定的评估指标。此外，还构建了一个多层数据集架构，包括Social Foundation Corpus和Entire-Pipeline Survey Datasets。

Result: 本文提出了AlignSurvey，这是一个系统地复制和评估使用大型语言模型（LLMs）的完整社会调查流程的基准。它提供了任务特定的评估指标，以评估对齐精度、一致性和公平性，并构建了一个多层数据集架构来支持AlignSurvey。所有数据集、模型和工具都可在GitHub和HuggingFace上获得，以支持透明和社会负责任的研究。

Conclusion: 本文介绍了AlignSurvey，这是一个系统地复制和评估使用大型语言模型（LLMs）的完整社会调查流程的基准。它提供了任务特定的评估指标，以评估对齐精度、一致性和公平性，并构建了一个多层数据集架构来支持AlignSurvey。所有数据集、模型和工具都可在GitHub和HuggingFace上获得，以支持透明和社会负责任的研究。

Abstract: Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research.

</details>


### [18] [Planned Event Forecasting using Future Mentions and Related Entity Extraction in News Articles](https://arxiv.org/abs/2511.07879)
*Neelesh Kumar Shukla,Pranay Sanghvi*

Main category: cs.CL

TL;DR: 本文开发了一个系统，用于预测社会动荡事件，通过分析新闻文章中的公告来识别关键特征和相关实体。


<details>
  <summary>Details</summary>
Motivation: 在印度等民主国家，人们可以自由表达观点和要求，这可能导致抗议、集会和游行等社会动荡事件。预测这些事件有助于行政部门采取必要措施。

Method: 本文使用主题建模和word2vec来过滤相关的新闻文章，并使用命名实体识别（NER）方法来识别实体，如人名、组织、地点和日期。时间归一化用于将未来的日期提及转换为标准格式。

Result: 本文开发了一个系统，用于使用主题建模和word2vec来过滤相关的新闻文章，并使用NER方法来识别实体。此外，还提出了相关实体提取的方法。

Conclusion: 本文开发了一个地理无关的、通用的模型，用于识别过滤社会动荡事件的关键特征，并提出了相关实体提取的方法。

Abstract: In democracies like India, people are free to express their views and demands. Sometimes this causes situations of civil unrest such as protests, rallies, and marches. These events may be disruptive in nature and are often held without prior permission from the competent authority. Forecasting these events helps administrative officials take necessary action. Usually, protests are announced well in advance to encourage large participation. Therefore, by analyzing such announcements in news articles, planned events can be forecasted beforehand. We developed such a system in this paper to forecast social unrest events using topic modeling and word2vec to filter relevant news articles, and Named Entity Recognition (NER) methods to identify entities such as people, organizations, locations, and dates. Time normalization is applied to convert future date mentions into a standard format. In this paper, we have developed a geographically independent, generalized model to identify key features for filtering civil unrest events. There could be many mentions of entities, but only a few may actually be involved in the event. This paper calls such entities Related Entities and proposes a method to extract them, referred to as Related Entity Extraction.

</details>


### [19] [Breaking the Adversarial Robustness-Performance Trade-off in Text Classification via Manifold Purification](https://arxiv.org/abs/2511.07888)
*Chenhao Dang,Jing Ma*

Main category: cs.CL

TL;DR: 本文提出了一种名为MC^2F的方法，用于解决文本分类中增强模型鲁棒性导致性能下降的问题。该方法通过在嵌入流形中建模干净样本的分布来实现这一点，并在多个数据集和对抗攻击上展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 在文本分类中，增强模型对对抗攻击的鲁棒性通常会降低在干净数据上的性能。我们主张通过在编码器嵌入流形中建模干净样本的分布来解决这一挑战。

Method: 我们提出了Manifold-Correcting Causal Flow (MC^2F)，这是一个直接在句子嵌入上运行的双模块系统。一个Stratified Riemannian Continuous Normalizing Flow (SR-CNF) 学习干净数据流形的密度。它识别出分布外的嵌入，然后由一个Geodesic Purification Solver进行纠正。这个求解器通过最短路径将对抗点投影回学习到的流形上，恢复一个干净、语义连贯的表示。

Result: 我们在三个数据集和多种对抗攻击上进行了广泛的评估。结果表明，我们的方法MC^2F不仅在对抗鲁棒性方面建立了新的最先进水平，而且完全保留了在干净数据上的性能，甚至在准确率上取得了适度的提升。

Conclusion: 我们的方法MC^2F不仅在对抗鲁棒性方面建立了新的最先进水平，而且完全保留了在干净数据上的性能，甚至在准确率上取得了适度的提升。

Abstract: A persistent challenge in text classification (TC) is that enhancing model robustness against adversarial attacks typically degrades performance on clean data. We argue that this challenge can be resolved by modeling the distribution of clean samples in the encoder embedding manifold. To this end, we propose the Manifold-Correcting Causal Flow (MC^2F), a two-module system that operates directly on sentence embeddings. A Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the density of the clean data manifold. It identifies out-of-distribution embeddings, which are then corrected by a Geodesic Purification Solver. This solver projects adversarial points back onto the learned manifold via the shortest path, restoring a clean, semantically coherent representation. We conducted extensive evaluations on text classification (TC) across three datasets and multiple adversarial attacks. The results demonstrate that our method, MC^2F, not only establishes a new state-of-the-art in adversarial robustness but also fully preserves performance on clean data, even yielding modest gains in accuracy.

</details>


### [20] [Last Layer Logits to Logic: Empowering LLMs with Logic-Consistent Structured Knowledge Reasoning](https://arxiv.org/abs/2511.07910)
*Songze Li,Zhiqiang Liu,Zhaoyan Gong,Xiaoke Guo,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Logits-to-Logic的框架，旨在提高大型语言模型在结构化知识推理中的逻辑一致性。通过logits增强和logits过滤来纠正逻辑缺陷，实验结果表明该方法在多个KGQA基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的方法仅提供输入级指导，未能从根本上解决LLMs输出中的逻辑漂移问题，并且它们的推理工作流程不够灵活，无法适应不同的任务和知识图谱。

Method: 我们提出了Logits-to-Logic框架，其中包含logits增强和logits过滤作为核心模块，以纠正LLMs输出中的逻辑缺陷。

Result: 广泛的实验表明，我们的方法显著提高了LLMs在结构化知识推理中的逻辑一致性，并在多个KGQA基准测试中达到了最先进的性能。

Conclusion: 我们的方法显著提高了LLMs在结构化知识推理中的逻辑一致性，并在多个KGQA基准测试中达到了最先进的性能。

Abstract: Large Language Models (LLMs) achieve excellent performance in natural language reasoning tasks through pre-training on vast unstructured text, enabling them to understand the logic in natural language and generate logic-consistent responses. However, the representational differences between unstructured and structured knowledge make LLMs inherently struggle to maintain logic consistency, leading to \textit{Logic Drift} challenges in structured knowledge reasoning tasks such as Knowledge Graph Question Answering (KGQA). Existing methods address this limitation by designing complex workflows embedded in prompts to guide LLM reasoning. Nevertheless, these approaches only provide input-level guidance and fail to fundamentally address the \textit{Logic Drift} in LLM outputs. Additionally, their inflexible reasoning workflows cannot adapt to different tasks and knowledge graphs. To enhance LLMs' logic consistency in structured knowledge reasoning, we specifically target the logits output from the autoregressive generation process. We propose the \textit{Logits-to-Logic} framework, which incorporates logits strengthening and logits filtering as core modules to correct logical defects in LLM outputs. Extensive experiments show that our approach significantly improves LLMs' logic consistency in structured knowledge reasoning and achieves state-of-the-art performance on multiple KGQA benchmarks.

</details>


### [21] [Social Media for Mental Health: Data, Methods, and Findings](https://arxiv.org/abs/2511.07914)
*Nur Shazwani Kamarudin,Ghazaleh Beigi,Lydia Manikonda,Huan Liu*

Main category: cs.CL

TL;DR: 本文研究了社交媒体数据在心理健康问题上的应用，包括语言、视觉和情感指标，并探讨了如何利用这些数据改善医疗实践和影响政策制定。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的普及，人们可以自由地交流和分享想法，特别是那些高度污名化的状况，而无需透露个人身份。本文旨在探讨如何利用这些新方法提高对心理健康问题的认识。

Method: 本文研究了社交媒体数据在心理健康问题上的最新研究方法和发现，包括语言、视觉和情感指标，并介绍了不同的机器学习、特征工程、自然语言处理和调查方法。

Result: 本文描述了社交媒体数据在心理健康问题中的应用，以及如何通过这些数据改进医疗实践、提供及时支持并影响政策制定。

Conclusion: 本文展示了如何利用这种新的数据源来改善医疗实践、提供及时的支持，并影响政府或政策制定者。

Abstract: There is an increasing number of virtual communities and forums available on the web. With social media, people can freely communicate and share their thoughts, ask personal questions, and seek peer-support, especially those with conditions that are highly stigmatized, without revealing personal identity. We study the state-of-the-art research methodologies and findings on mental health challenges like de- pression, anxiety, suicidal thoughts, from the pervasive use of social media data. We also discuss how these novel thinking and approaches can help to raise awareness of mental health issues in an unprecedented way. Specifically, this chapter describes linguistic, visual, and emotional indicators expressed in user disclosures. The main goal of this chapter is to show how this new source of data can be tapped to improve medical practice, provide timely support, and influence government or policymakers. In the context of social media for mental health issues, this chapter categorizes social media data used, introduces different deployed machine learning, feature engineering, natural language processing, and surveys methods and outlines directions for future research.

</details>


### [22] [Distinct Theta Synchrony across Speech Modes: Perceived, Spoken, Whispered, and Imagined](https://arxiv.org/abs/2511.07918)
*Jung-Sun Lee,Ha-Na Jo,Eunyeong Ko*

Main category: cs.CL

TL;DR: 本研究分析了不同语音模式下theta波段神经同步性的差异，发现出声和耳语言语表现出更广泛的额颞同步性，而感知言语主要涉及后部和颞部同步模式，想象言语则主要涉及额叶和补充运动区。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要集中在单一模式（如出声言语）上，很少对不同语音模式下的theta同步性进行综合比较。因此，本研究旨在探讨不同语音模式下theta同步性的差异。

Method: 本研究基于连通性度量分析了不同语音模式下theta波段神经同步性的差异，重点关注区域变化。

Result: 结果表明，出声和耳语言语表现出更广泛和更强的额颞同步性，而感知言语则表现出主要的后部和颞部同步模式，而想象言语则表现出更为空间受限但内部一致的同步模式。

Conclusion: 本研究旨在阐明不同语音模式下theta波段神经同步性的差异，从而揭示语言感知和想象语音共享和独特的神经动态。

Abstract: Human speech production encompasses multiple modes such as perceived, overt, whispered, and imagined, each reflecting distinct neural mechanisms. Among these, theta-band synchrony has been closely associated with language processing, attentional control, and inner speech. However, previous studies have largely focused on a single mode, such as overt speech, and have rarely conducted an integrated comparison of theta synchrony across different speech modes. In this study, we analyzed differences in theta-band neural synchrony across speech modes based on connectivity metrics, focusing on region-wise variations. The results revealed that overt and whispered speech exhibited broader and stronger frontotemporal synchrony, reflecting active motor-phonological coupling during overt articulation, whereas perceived speech showed dominant posterior and temporal synchrony patterns, consistent with auditory perception and comprehension processes. In contrast, imagined speech demonstrated a more spatially confined but internally coherent synchronization pattern, primarily involving frontal and supplementary motor regions. These findings indicate that the extent and spatial distribution of theta synchrony differ substantially across modes, with overt articulation engaging widespread cortical interactions, whispered speech showing intermediate engagement, and perception relying predominantly on temporoparietal networks. Therefore, this study aims to elucidate the differences in theta-band neural synchrony across various speech modes, thereby uncovering both the shared and distinct neural dynamics underlying language perception and imagined speech.

</details>


### [23] [Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker](https://arxiv.org/abs/2511.07969)
*Matthias De Lange,Jens-Joris Decorte,Jeroen Van Hautte*

Main category: cs.CL

TL;DR: 本文介绍了WorkBench，这是第一个统一的评估套件，涵盖了六个工作相关的任务，并通过这些任务构建了任务特定的二部图，从而生成了Unified Work Embeddings (UWE)，这是一种任务无关的双编码器，在工作领域中表现出色。


<details>
  <summary>Details</summary>
Motivation: Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks.

Method: We introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction.

Result: Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction.

Conclusion: UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.

Abstract: Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.

</details>


### [24] [NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation](https://arxiv.org/abs/2511.07982)
*Maoqi Liu,Quan Fang,Yuhao Wu,Can Zhao,Yang Yang,Kaiquan Cai*

Main category: cs.CL

TL;DR: 本文提出NOTAM-Evolve框架，通过深度解析和闭环学习提升NOTAM解释的准确性，取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化系统通常仅限于浅层解析，无法提取操作决策所需的操作情报。需要一种能够进行深度解析的方法来准确解释NOTAM。

Method: 提出了一种名为NOTAM-Evolve的自进化框架，利用知识图谱增强的检索模块进行数据接地，并引入闭环学习过程，使LLM从自身输出中逐步改进。

Result: NOTAM-Evolve在10,000个专家标注的NOTAM数据集上进行了实验，结果表明其准确率比基础LLM提高了30.4%。

Conclusion: NOTAM-Evolve在结构化NOTAM解释任务中取得了新的最先进成果，相比基础LLM提高了30.4%的绝对准确率。

Abstract: Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.

</details>


### [25] [State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?](https://arxiv.org/abs/2511.07989)
*Taja Kuzman Pungeršek,Peter Rupnik,Ivan Porupski,Vuk Dinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: 本文评估了当前语言模型在跨南斯拉夫语言的文本分类任务中的表现，发现LLMs在零样本设置下表现良好，但存在输出不可预测、推理速度慢和计算成本高的问题，因此微调的BERT-like模型仍是大规模自动文本标注的更实用选择。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在文本分类任务中的表现，特别是在资源较少的语言中，以评估它们在实际应用中的可行性。

Method: 比较公开可用的微调BERT-like模型与一些开源和闭源LLMs在三个任务中的表现，这三个任务涉及三个领域：议会演讲中的情感分类、新闻文章和议会演讲中的主题分类以及网络文本中的体裁识别。

Result: LLMs在零样本设置下表现出色，通常可以与微调的BERT-like模型相媲美或超越它们。此外，在零样本设置下，LLMs在南斯拉夫语言和英语中的表现相当。

Conclusion: 由于这些限制，微调的BERT-like模型仍然是大规模自动文本标注更实际的选择。

Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.

</details>


### [26] [Self-Correction Distillation for Structured Data Question Answering](https://arxiv.org/abs/2511.07998)
*Yushan Zhu,Wen Zhang,Long Jin,Mengshu Sun,Ling Zhong,Zhiqiang Liu,Juan Li,Lei Liang,Chong Long,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 本文提出了SCD方法，通过错误提示机制和两阶段蒸馏策略，提高了小规模LLM的结构化数据问答能力，并取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的统一结构QA框架在小规模LLM上面临挑战，因为小规模LLM在生成结构化查询时容易出错。

Method: 提出了一种自校正蒸馏（SCD）方法，其中包括一个错误提示机制（EPM）和两阶段蒸馏策略。

Result: SCD在5个基准测试中表现最佳，具有优越的泛化能力，并且在某些数据集上接近GPT4的性能。

Conclusion: SCD方法在小规模LLM上表现出色，能够接近GPT4的性能，并且EPM机制在大规模LLM上也取得了最先进的结果。

Abstract: Structured data question answering (QA), including table QA, Knowledge Graph (KG) QA, and temporal KG QA, is a pivotal research area. Advances in large language models (LLMs) have driven significant progress in unified structural QA frameworks like TrustUQA. However, these frameworks face challenges when applied to small-scale LLMs since small-scale LLMs are prone to errors in generating structured queries. To improve the structured data QA ability of small-scale LLMs, we propose a self-correction distillation (SCD) method. In SCD, an error prompt mechanism (EPM) is designed to detect errors and provide customized error messages during inference, and a two-stage distillation strategy is designed to transfer large-scale LLMs' query-generation and error-correction capabilities to small-scale LLM. Experiments across 5 benchmarks with 3 structured data types demonstrate that our SCD achieves the best performance and superior generalization on small-scale LLM (8B) compared to other distillation methods, and closely approaches the performance of GPT4 on some datasets. Furthermore, large-scale LLMs equipped with EPM surpass the state-of-the-art results on most datasets.

</details>


### [27] [HyCoRA: Hyper-Contrastive Role-Adaptive Learning for Role-Playing](https://arxiv.org/abs/2511.08017)
*Shihao Yang,Zhicong Lu,Yong Yang,Bo Lv,Yang Shen,Nayu Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的HyCoRA框架，用于多角色扮演任务，通过平衡角色独特性和共享性来提高模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么使用跨所有角色的共享参数化模块，要么为每个角色分配单独的参数化模块，但这些方法分别忽略了角色的独特特征和共享特征。

Method: 提出了一种名为HyCoRA的超对比角色自适应学习框架，其中一半是通过轻量级超网络生成的角色特定模块，另一半是可训练的角色共享模块。此外，设计了一种超对比学习机制来帮助超网络区分不同角色的独特特征。

Result: 在英文和中文可用基准数据集上的实验结果表明了该框架的优势。进一步的GPT-4评估和可视化分析也验证了HyCoRA捕捉角色特征的能力。

Conclusion: HyCoRA框架在多角色扮演任务中表现出色，能够有效平衡角色独特性和共性学习。

Abstract: Multi-character role-playing aims to equip models with the capability to simulate diverse roles. Existing methods either use one shared parameterized module across all roles or assign a separate parameterized module to each role. However, the role-shared module may ignore distinct traits of each role, weakening personality learning, while the role-specific module may overlook shared traits across multiple roles, hindering commonality modeling. In this paper, we propose a novel HyCoRA: Hyper-Contrastive Role-Adaptive learning framework, which efficiently improves multi-character role-playing ability by balancing the learning of distinct and shared traits. Specifically, we propose a Hyper-Half Low-Rank Adaptation structure, where one half is a role-specific module generated by a lightweight hyper-network, and the other half is a trainable role-shared module. The role-specific module is devised to represent distinct persona signatures, while the role-shared module serves to capture common traits. Moreover, to better reflect distinct personalities across different roles, we design a hyper-contrastive learning mechanism to help the hyper-network distinguish their unique characteristics. Extensive experimental results on both English and Chinese available benchmarks demonstrate the superiority of our framework. Further GPT-4 evaluations and visual analyses also verify the capability of HyCoRA to capture role characteristics.

</details>


### [28] [BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution](https://arxiv.org/abs/2511.08085)
*Abdullah Muhammad Moosa,Nusrat Sultana,Mahdi Muhammad Moosa,Md. Miraiz Hossain*

Main category: cs.CL

TL;DR: 本研究提出一个新的孟加拉语作者识别数据集BARD10，发现停用词对作者识别有重要影响，并展示了经典方法在短文本限制下的有效性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索孟加拉语停用词在作者识别中的风格意义，并建立一个平衡的基准数据集BARD10。

Method: 本研究通过系统分析停用词去除对经典和深度学习模型的影响，评估了四种分类器（SVM、Bangla BERT、XGBoost和MLP）的表现。

Result: 经典方法TF-IDF + SVM在BAAD16和BARD10数据集上表现最佳，而Bangla BERT的表现较差。此外，高频率成分传递了作者签名，但被Transformer模型削弱。

Conclusion: 本研究揭示了孟加拉语停用词在作者识别中的重要性，并提出了一个新基准数据集BARD10，为未来的研究提供了可重复的基准。

Abstract: This research presents a comprehensive investigation into Bangla authorship attribution, introducing a new balanced benchmark corpus BARD10 (Bangla Authorship Recognition Dataset of 10 authors) and systematically analyzing the impact of stop-word removal across classical and deep learning models to uncover the stylistic significance of Bangla stop-words. BARD10 is a curated corpus of Bangla blog and opinion prose from ten contemporary authors, alongside the methodical assessment of four representative classifiers: SVM (Support Vector Machine), Bangla BERT (Bidirectional Encoder Representations from Transformers), XGBoost, and a MLP (Multilayer Perception), utilizing uniform preprocessing on both BARD10 and the benchmark corpora BAAD16 (Bangla Authorship Attribution Dataset of 16 authors). In all datasets, the classical TF-IDF + SVM baseline outperformed, attaining a macro-F1 score of 0.997 on BAAD16 and 0.921 on BARD10, while Bangla BERT lagged by as much as five points. This study reveals that BARD10 authors are highly sensitive to stop-word pruning, while BAAD16 authors remain comparatively robust highlighting genre-dependent reliance on stop-word signatures. Error analysis revealed that high frequency components transmit authorial signatures that are diminished or reduced by transformer models. Three insights are identified: Bangla stop-words serve as essential stylistic indicators; finely calibrated ML models prove effective within short-text limitations; and BARD10 connects formal literature with contemporary web dialogue, offering a reproducible benchmark for future long-context or domain-adapted transformers.

</details>


### [29] [Estranged Predictions: Measuring Semantic Category Disruption with Masked Language Modelling](https://arxiv.org/abs/2511.08109)
*Yuxuan Liu,Haim Dubossarsky,Ruth Ahnert*

Main category: cs.CL

TL;DR: 本研究利用计算方法分析科幻小说如何颠覆本体论范畴，并发现科幻小说在概念渗透性方面表现出更高的水平。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨科幻小说如何颠覆本体论范畴，并通过计算方法测量概念渗透性。

Method: 本研究使用掩码语言模型（MLM）测量人类、动物和机器术语之间的概念渗透性，通过RoBERTa生成被屏蔽指代的词汇替代，并使用Gemini进行分类。

Result: 研究发现，科幻小说表现出更高的概念渗透性，尤其是在机器指代方面，显示出显著的跨类别替代和分散性。而人类术语则保持语义连贯性，并常常作为替代层次结构的锚点。

Conclusion: 本研究为计算文学研究的方法论库做出了贡献，并提供了关于科幻小说语言基础的新见解。

Abstract: This paper examines how science fiction destabilises ontological categories by measuring conceptual permeability across the terms human, animal, and machine using masked language modelling (MLM). Drawing on corpora of science fiction (Gollancz SF Masterworks) and general fiction (NovelTM), we operationalise Darko Suvin's theory of estrangement as computationally measurable deviation in token prediction, using RoBERTa to generate lexical substitutes for masked referents and classifying them via Gemini. We quantify conceptual slippage through three metrics: retention rate, replacement rate, and entropy, mapping the stability or disruption of category boundaries across genres. Our findings reveal that science fiction exhibits heightened conceptual permeability, particularly around machine referents, which show significant cross-category substitution and dispersion. Human terms, by contrast, maintain semantic coherence and often anchor substitutional hierarchies. These patterns suggest a genre-specific restructuring within anthropocentric logics. We argue that estrangement in science fiction operates as a controlled perturbation of semantic norms, detectable through probabilistic modelling, and that MLMs, when used critically, serve as interpretive instruments capable of surfacing genre-conditioned ontological assumptions. This study contributes to the methodological repertoire of computational literary studies and offers new insights into the linguistic infrastructure of science fiction.

</details>


### [30] [Multimodal LLMs Do Not Compose Skills Optimally Across Modalities](https://arxiv.org/abs/2511.08113)
*Paula Ontalvilla,Aitor Ormazabal,Gorka Azkune*

Main category: cs.CL

TL;DR: 本文研究了MLLM在跨模态之间组合技能的能力，发现即使使用简单的组合方法，所有评估的MLLM都存在显著的技能组合差距，并探索了两种可能的解决方案，但效果有限。


<details>
  <summary>Details</summary>
Motivation: 研究MLLM在不同模态之间组合技能的能力，因为随着神经网络在预训练中获得越来越复杂的技能，不清楚它们能否成功地组合这些技能。

Method: 设计了三个评估任务，这些任务可以通过顺序组合两个模态依赖的技能来解决，并在两种主要设置下评估了几种开放的MLLM：i) 直接提示模型解决任务，ii) 使用两步级联推理方法，手动强制组合两个技能以解决给定任务。

Result: 所有评估的MLLM都显示出显著的跨模态技能组合差距。使用思维链提示和特定的微调方案可以提高模型性能，但仍然存在显著的技能组合差距。

Conclusion: 尽管这些策略提高了模型性能，但它们仍然表现出显著的技能组合差距，这表明需要更多的研究来改进MLLM中的跨模态技能组合。

Abstract: Skill composition is the ability to combine previously learned skills to solve new tasks. As neural networks acquire increasingly complex skills during their pretraining, it is not clear how successfully they can compose them. In this paper, we focus on Multimodal Large Language Models (MLLM), and study their ability to compose skills across modalities. To this end, we design three evaluation tasks which can be solved sequentially composing two modality-dependent skills, and evaluate several open MLLMs under two main settings: i) prompting the model to directly solve the task, and ii) using a two-step cascaded inference approach, which manually enforces the composition of the two skills for a given task. Even with these straightforward compositions, we find that all evaluated MLLMs exhibit a significant cross-modality skill composition gap. To mitigate the aforementioned gap, we explore two alternatives: i) use chain-of-thought prompting to explicitly instruct MLLMs for skill composition and ii) a specific fine-tuning recipe to promote skill composition. Although those strategies improve model performance, they still exhibit significant skill composition gaps, suggesting that more research is needed to improve cross-modal skill composition in MLLMs.

</details>


### [31] [Quantification and object perception in Multimodal Large Language Models deviate from human linguistic cognition](https://arxiv.org/abs/2511.08126)
*Raquel Montero,Natalia Moskvina,Paolo Morosi,Tamara Serrano,Elena Pagliarini,Evelina Leivada*

Main category: cs.CL

TL;DR: 本文探讨了MLLMs在量化任务上的表现问题，分析了人类量化特征如何影响模型表现，并指出MLLMs与人类在这些特征上存在差异。


<details>
  <summary>Details</summary>
Motivation: 由于量化涉及逻辑、语用和数值领域，但MLLMs在量化任务上的表现不佳，因此需要明确其原因。本文旨在探索这些特征如何影响模型的表现，并比较不同模型和语言之间的结果。

Method: 本文分析了人类量化共享的三个关键特征：量化符的排序、使用范围和典型性以及人类近似数系统中的偏见，并探讨了这些特征如何在模型架构中编码，以及它们与人类的不同之处。

Result: 研究发现，人类与MLLMs在这些特征上存在明显差异，这表明MLLMs在处理量化任务时可能与人类有不同的机制。

Conclusion: 本文发现人类与MLLMs在这些特征上存在明显差异，这表明需要进一步研究MLLMs作为语义和语用代理的本质，并通过跨语言视角阐明其能力是否在不同语言中具有鲁棒性和稳定性。

Abstract: Quantification has been proven to be a particularly difficult linguistic phenomenon for (Multimodal) Large Language Models (MLLMs). However, given that quantification interfaces with the logic, pragmatic, and numerical domains, the exact reasons for the poor performance are still unclear. This papers looks at three key features of human quantification shared cross-linguistically that have remained so far unexplored in the (M)LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and the biases inherent in the human approximate number system. The aim is to determine how these features are encoded in the models' architecture, how they may differ from humans, and whether the results are affected by the type of model and language under investigation. We find that there are clear differences between humans and MLLMs with respect to these features across various tasks that tap into the representation of quantification in vivo vs. in silico. This work, thus, paves the way for addressing the nature of MLLMs as semantic and pragmatic agents, while the cross-linguistic lens can elucidate whether their abilities are robust and stable across different languages.

</details>


### [32] [Sentence-Anchored Gist Compression for Long-Context LLMs](https://arxiv.org/abs/2511.08128)
*Dmitrii Tarasov,Elizaveta Goncharova,Kuznetsov Andrey*

Main category: cs.CL

TL;DR: 本文研究了使用学习的压缩标记来减少大型语言模型处理长序列时的内存和计算需求，并展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了减少处理长序列时的内存和计算需求，需要一种有效的上下文压缩方法。

Method: 本文提出了一种基于学习压缩标记的方法，用于对大型语言模型（LLMs）进行上下文压缩。

Result: 实验结果表明，预训练的LLM可以在不显著影响性能的情况下，将上下文压缩2倍到8倍。在30亿参数的LLaMA模型上，该方法取得了与替代压缩技术相当的结果，并实现了更高的压缩率。

Conclusion: 本文表明，通过使用学习的压缩标记，可以有效地减少处理长序列的内存和计算需求，同时保持模型性能。

Abstract: This work investigates context compression for Large Language Models (LLMs) using learned compression tokens to reduce the memory and computational demands of processing long sequences. We demonstrate that pre-trained LLMs can be fine-tuned to compress their context by factors of 2x to 8x without significant performance degradation, as evaluated on both short-context and long-context benchmarks. Furthermore, in experiments on a 3-billion-parameter LLaMA model, our method achieves results on par with alternative compression techniques while attaining higher compression ratios.

</details>


### [33] [On the Interplay between Positional Encodings, Morphological Complexity, and Word Order Flexibility](https://arxiv.org/abs/2511.08139)
*Kushal Tatariya,Wessel Poelman,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文研究了位置编码对语言建模的影响，发现任务、语言和度量的选择对于得出稳定结论至关重要。


<details>
  <summary>Details</summary>
Motivation: 我们研究了位置编码在语言建模中的影响，特别是从形态复杂性和词序灵活性之间的权衡假设的角度来看。

Method: 我们预训练了具有绝对、相对和无位置编码的单语模型变体，并在四个下游任务上进行了评估。

Result: 我们没有观察到位置编码与形态复杂性或词序灵活性之间的明显相互作用。

Conclusion: 我们的结果表明，任务、语言和度量的选择对于得出稳定的结论至关重要。

Abstract: Language model architectures are predominantly first created for English and subsequently applied to other languages. It is an open question whether this architectural bias leads to degraded performance for languages that are structurally different from English. We examine one specific architectural choice: positional encodings, through the lens of the trade-off hypothesis: the supposed interplay between morphological complexity and word order flexibility. This hypothesis posits a trade-off between the two: a more morphologically complex language can have a more flexible word order, and vice-versa. Positional encodings are a direct target to investigate the implications of this hypothesis in relation to language modelling. We pretrain monolingual model variants with absolute, relative, and no positional encodings for seven typologically diverse languages and evaluate them on four downstream tasks. Contrary to previous findings, we do not observe a clear interaction between position encodings and morphological complexity or word order flexibility, as measured by various proxies. Our results show that the choice of tasks, languages, and metrics are essential for drawing stable conclusions

</details>


### [34] [Relation as a Prior: A Novel Paradigm for LLM-based Document-level Relation Extraction](https://arxiv.org/abs/2511.08143)
*Qiankun Pi,Yepeng Sun,Jicang Lu,Qinlong Fan,Ningbo Huang,Shiyu Wang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have demonstrated their remarkable capabilities in document understanding. However, recent research reveals that LLMs still exhibit performance gaps in Document-level Relation Extraction (DocRE) as requiring fine-grained comprehension. The commonly adopted "extract entities then predict relations" paradigm in LLM-based methods leads to these gaps due to two main reasons: (1) Numerous unrelated entity pairs introduce noise and interfere with the relation prediction for truly related entity pairs. (2) Although LLMs have identified semantic associations between entities, relation labels beyond the predefined set are still treated as prediction errors. To address these challenges, we propose a novel Relation as a Prior (RelPrior) paradigm for LLM-based DocRE. For challenge (1), RelPrior utilizes binary relation as a prior to extract and determine whether two entities are correlated, thereby filtering out irrelevant entity pairs and reducing prediction noise. For challenge (2), RelPrior utilizes predefined relation as a prior to match entities for triples extraction instead of directly predicting relation. Thus, it avoids misjudgment caused by strict predefined relation labeling. Extensive experiments on two benchmarks demonstrate that RelPrior achieves state-of-the-art performance, surpassing existing LLM-based methods.

</details>


### [35] [Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?](https://arxiv.org/abs/2511.08145)
*Kunal Kingkar Das,Manoj Balaji Jagadeeshan,Nallani Chakravartula Sahith,Jivnesh Sandhan,Pawan Goyal*

Main category: cs.CL

TL;DR: 研究比较了大型语言模型和任务特定模型在梵文诗歌到散文转换任务中的表现，发现任务特定模型更优，同时提示策略在数据不足时也有效。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型是否能在低资源、形态丰富的语言（如梵文）任务中超越专门模型。

Method: 通过指令微调通用大型语言模型并设计基于帕尼尼语法和古典注释启发式的上下文学习模板来评估LLM的表现，同时对ByT5-Sanskrit Seq2Seq模型进行完全微调以进行任务特定建模。

Result: 领域特定微调的ByT5-Sanskrit模型在实验中表现优于所有指令驱动的LLM方法，人类评估结果与Kendall's Tau分数高度相关。此外，提示策略为缺乏领域特定诗集的情况提供了替代方案，任务特定的Seq2Seq模型在跨领域评估中表现出强大的泛化能力。

Conclusion: 领域特定的微调ByT5-Sanskrit模型在梵文诗歌到散文转换任务中显著优于所有指令驱动的大型语言模型（LLM）方法。

Abstract: Large Language Models (LLMs) are increasingly treated as universal, general-purpose solutions across NLP tasks, particularly in English. But does this assumption hold for low-resource, morphologically rich languages such as Sanskrit? We address this question by comparing instruction-tuned and in-context-prompted LLMs with smaller task-specific encoder-decoder models on the Sanskrit poetry-to-prose conversion task. This task is intrinsically challenging: Sanskrit verse exhibits free word order combined with rigid metrical constraints, and its conversion to canonical prose (anvaya) requires multi-step reasoning involving compound segmentation, dependency resolution, and syntactic linearisation. This makes it an ideal testbed to evaluate whether LLMs can surpass specialised models. For LLMs, we apply instruction fine-tuning on general-purpose models and design in-context learning templates grounded in Paninian grammar and classical commentary heuristics. For task-specific modelling, we fully fine-tune a ByT5-Sanskrit Seq2Seq model. Our experiments show that domain-specific fine-tuning of ByT5-Sanskrit significantly outperforms all instruction-driven LLM approaches. Human evaluation strongly corroborates this result, with scores exhibiting high correlation with Kendall's Tau scores. Additionally, our prompting strategies provide an alternative to fine-tuning when domain-specific verse corpora are unavailable, and the task-specific Seq2Seq model demonstrates robust generalisation on out-of-domain evaluations.

</details>


### [36] [Do Syntactic Categories Help in Developmentally Motivated Curriculum Learning for Language Models?](https://arxiv.org/abs/2511.08199)
*Arzu Burcu Güven,Anna Rogers,Rob van der Goot*

Main category: cs.CL

TL;DR: The study explores the syntactic properties of training data and their impact on model performance, finding that using a subset of syntactically categorizable data improves reading task performance.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between syntactic properties of training data and model performance on linguistic tasks, and to investigate the effectiveness of different curriculum learning approaches.

Method: We examine the syntactic properties of BabyLM corpus and age-groups within CHILDES, and explore developmental and alternative cognitively inspired curriculum approaches.

Result: CHILDES does not exhibit strong syntactic differentiation by age, but syntactic knowledge about the training data can be helpful in interpreting model performance. Some curricula help with reading tasks, but the main performance improvement comes from using the subset of syntactically categorizable data.

Conclusion:  syntactic knowledge about the training data can be helpful in interpreting model performance on linguistic tasks, and using a subset of syntactically categorizable data can lead to better performance in reading tasks.

Abstract: We examine the syntactic properties of BabyLM corpus, and age-groups within CHILDES. While we find that CHILDES does not exhibit strong syntactic differentiation by age, we show that the syntactic knowledge about the training data can be helpful in interpreting model performance on linguistic tasks. For curriculum learning, we explore developmental and several alternative cognitively inspired curriculum approaches. We find that some curricula help with reading tasks, but the main performance improvement come from using the subset of syntactically categorizable data, rather than the full noisy corpus.

</details>


### [37] [Encoder Fine-tuning with Stochastic Sampling Outperforms Open-weight GPT in Astronomy Knowledge Extraction](https://arxiv.org/abs/2511.08204)
*Shivam Rawat,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 本文介绍了一种基于编码器的系统，用于从天文学文章中提取知识。该系统基于SciBERT模型，并针对天文学语料库分类进行了微调，能够有效分类望远镜引用、检测辅助语义属性并识别仪器提及，且性能优于GPT基线。


<details>
  <summary>Details</summary>
Motivation: 科学文献在天文学中迅速扩展，使得从研究论文中自动提取关键实体和上下文信息变得越来越重要。

Method: 我们实现了一个基于多任务Transformer的系统，该系统建立在SciBERT模型之上，并针对天文学语料库分类进行了微调。

Result: 我们的系统能够对望远镜引用进行分类，检测辅助语义属性，并从文本内容中识别仪器提及。

Conclusion: 我们的系统尽管实现简单且成本低，但显著优于开放权重的GPT基线。

Abstract: Scientific literature in astronomy is rapidly expanding, making it increasingly important to automate the extraction of key entities and contextual information from research papers. In this paper, we present an encoder-based system for extracting knowledge from astronomy articles. Our objective is to develop models capable of classifying telescope references, detecting auxiliary semantic attributes, and recognizing instrument mentions from textual content. To this end, we implement a multi-task transformer-based system built upon the SciBERT model and fine-tuned for astronomy corpora classification. To carry out the fine-tuning, we stochastically sample segments from the training data and use majority voting over the test segments at inference time. Our system, despite its simplicity and low-cost implementation, significantly outperforms the open-weight GPT baseline.

</details>


### [38] [Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback](https://arxiv.org/abs/2511.08225)
*Yishan Du,Conrad Borchers,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文提出了一种基于嵌入的基准测试框架，用于检测大型语言模型在形成性反馈中的偏见。通过构建控制反事实并分析语义变化，发现所有模型中隐性操纵对男女反事实引起的语义变化更大，而只有GPT和Llama模型对显性性别线索敏感，表明大型语言模型在反馈中存在持续的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 随着教师越来越多地在教育实践中使用生成式人工智能，我们需要强大的方法来对大型语言模型进行基准测试，以用于教学目的。

Method: 使用600篇来自AES 2.0语料库的真实学生论文，构建了两个维度的控制反事实：(i) 通过词汇替换性别化术语的隐含线索，(ii) 通过提示中的性别化作者背景的显式线索。然后通过余弦和欧几里得距离量化响应偏差，通过置换检验评估显著性，并通过降维可视化结构。

Result: 所有模型中，隐性操纵对男女反事实引起的语义变化比女男反事实更大。只有GPT和Llama模型对显性性别线索敏感。这些发现表明，即使最先进的大型语言模型在提供学习者反馈时也表现出持续的性别偏见。

Conclusion: 即使最先进的大型语言模型在提供学习者反馈时也表现出对性别替换的不对称语义响应，这表明了反馈中存在持续的性别偏见。

Abstract: As teachers increasingly turn to GenAI in their educational practice, we need robust methods to benchmark large language models (LLMs) for pedagogical purposes. This article presents an embedding-based benchmarking framework to detect bias in LLMs in the context of formative feedback. Using 600 authentic student essays from the AES 2.0 corpus, we constructed controlled counterfactuals along two dimensions: (i) implicit cues via lexicon-based swaps of gendered terms within essays, and (ii) explicit cues via gendered author background in the prompt. We investigated six representative LLMs (i.e. GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B). We first quantified the response divergence with cosine and Euclidean distances over sentence embeddings, then assessed significance via permutation tests, and finally, visualised structure using dimensionality reduction. In all models, implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male. Only the GPT and Llama models showed sensitivity to explicit gender cues. These findings show that even state-of-the-art LLMs exhibit asymmetric semantic responses to gender substitutions, suggesting persistent gender biases in feedback they provide learners. Qualitative analyses further revealed consistent linguistic differences (e.g., more autonomy-supportive feedback under male cues vs. more controlling feedback under female cues). We discuss implications for fairness auditing of pedagogical GenAI, propose reporting standards for counterfactual evaluation in learning analytics, and outline practical guidance for prompt design and deployment to safeguard equitable feedback.

</details>


### [39] [VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context](https://arxiv.org/abs/2511.08230)
*Heyang Liu,Ziyang Cheng,Yuhao Wang,Hongcheng Liu,Yiqi Li,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了VocalBench-zh，一个针对中文语境的能力分级评估套件，旨在解决当前缺乏全面的中文语音到语音（S2S）基准的问题。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏全面的中文语音到语音（S2S）基准，阻碍了开发者的系统评估和用户的公平模型比较。

Method: 本文提出了VocalBench-zh，一个针对中文语境的能力分级评估套件，包含10个精心设计的子集和超过10,000个高质量实例，覆盖12个用户导向的角色。

Result: 对14个主流模型的评估实验揭示了当前模型的常见挑战，并强调了下一代语音交互系统需要新的见解。评估代码和数据集将公开在GitHub上。

Conclusion: 本文提出了VocalBench-zh，一个针对中文语境的能力分级评估套件，包含10个精心设计的子集和超过10,000个高质量实例，覆盖12个用户导向的角色。评估实验揭示了当前模型的常见挑战，并强调了下一代语音交互系统需要新的见解。评估代码和数据集将公开在GitHub上。

Abstract: The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at https://github.com/SJTU-OmniAgent/VocalBench-zh.

</details>


### [40] [Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG](https://arxiv.org/abs/2511.08245)
*Jisoo Jang,Tien-Cuong Bui,Yunjun Choi,Wen-Syan Li*

Main category: cs.CL

TL;DR: 本文介绍了一种通过提示调整进行错误纠正的NL-to-SQL方法，结合了生成预训练大模型和RAG技术。我们的框架通过诊断错误类型、识别原因、提供修复指令并应用这些纠正来提高SQL查询的准确性。实验结果表明，该框架在现有基线基础上实现了12%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言接口的广泛应用，需要高效且准确地将自然语言查询翻译成SQL表达式。

Method: 我们提出了一种新颖的框架，集成了错误纠正机制，该机制可以诊断错误类型、识别原因、提供修复指令，并将这些纠正应用于SQL查询。此外，还嵌入了微调和RAG，利用外部知识库提高准确性和透明度。

Result: 通过全面的实验，我们证明了我们的框架在现有基线基础上实现了显著的12%准确率提升。

Conclusion: 我们的框架在现有基线基础上实现了显著的12%准确率提升，展示了其在现代数据驱动环境中革新数据访问和处理的潜力。

Abstract: This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.

</details>


### [41] [ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech](https://arxiv.org/abs/2511.08247)
*Marios Koniaris,Argyro Tsipi,Panayiotis Tsanakas*

Main category: cs.CL

TL;DR: 本文介绍了ParliaBench，一个用于议会演讲生成的基准测试。通过构建英国议会的演讲数据集，提出了一种结合计算指标和LLM-as-a-judge评估的评估框架，并引入了两种新的基于嵌入的指标来量化意识形态定位。实验结果表明，微调在大多数指标上都有显著提升，新指标在政治维度上表现出色。


<details>
  <summary>Details</summary>
Motivation: 议会演讲生成对于大型语言模型来说比标准文本生成任务具有特定的挑战。与一般文本生成不同，议会演讲不仅需要语言质量，还需要政治真实性和意识形态一致性。当前的语言模型缺乏针对议会背景的专门训练，现有的评估方法侧重于标准NLP指标，而不是政治真实性。

Method: 我们构建了一个来自英国议会的演讲数据集，以实现系统的模型训练。我们引入了一个评估框架，结合计算指标和LLM-as-a-judge评估，用于测量生成质量的三个维度：语言质量、语义连贯性和政治真实性。我们提出了两种新的基于嵌入的指标，政治谱系对齐和政党对齐，以量化意识形态定位。

Result: 我们微调了五种大型语言模型（LLMs），生成了28k篇演讲，并使用我们的框架进行了评估，比较了基线模型和微调模型。结果表明，微调在大多数指标上都产生了统计学上显著的改进，我们的新指标在政治维度上表现出强大的区分能力。

Conclusion: 结果表明，微调在大多数指标上都产生了统计学上显著的改进，我们的新指标在政治维度上表现出强大的区分能力。

Abstract: Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them using our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.

</details>


### [42] [Hierarchical structure understanding in complex tables with VLLMs: a benchmark and experiments](https://arxiv.org/abs/2511.08298)
*Luca Bindini,Simone Giovannini,Simone Marinai,Valeria Nardoni,Kimiya Noor Ali*

Main category: cs.CL

TL;DR: 本研究探讨了VLLMs理解科学文章中表格结构的能力，并发现通用VLLMs在无需额外处理的情况下可以完成此任务。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索VLLMs是否能够无需额外处理就能推断科学文章中表格的层次结构。

Method: 我们使用PubTables-1M数据集，从中提取了一个名为Complex Hierarchical Tables (CHiTab)的子集作为基准数据集。我们采用了一系列提示工程策略来探测模型的理解能力，并对多个最先进的开放权重VLLMs进行了评估，包括使用其现成版本和在我们的任务上进行微调。

Result: 实验支持了我们的直觉，即那些并非专门设计用于理解表格结构的通用VLLMs可以执行此任务。

Conclusion: 本研究提供了关于VLLMs处理复杂表格的潜力和局限性的见解，并为未来将结构化数据理解整合到通用VLLMs中提供了指导。

Abstract: This work investigates the ability of Vision Large Language Models (VLLMs) to understand and interpret the structure of tables in scientific articles. Specifically, we explore whether VLLMs can infer the hierarchical structure of tables without additional processing. As a basis for our experiments we use the PubTables-1M dataset, a large-scale corpus of scientific tables. From this dataset, we extract a subset of tables that we introduce as Complex Hierarchical Tables (CHiTab): a benchmark collection of complex tables containing hierarchical headings. We adopt a series of prompt engineering strategies to probe the models' comprehension capabilities, experimenting with various prompt formats and writing styles. Multiple state-of-the-art open-weights VLLMs are evaluated on the benchmark first using their off-the-shelf versions and then fine-tuning some models on our task. We also measure the performance of humans to solve the task on a small set of tables comparing with performance of the evaluated VLLMs. The experiments support our intuition that generic VLLMs, not explicitly designed for understanding the structure of tables, can perform this task. This study provides insights into the potential and limitations of VLLMs to process complex tables and offers guidance for future work on integrating structured data understanding into general-purpose VLLMs.

</details>


### [43] [Automatic Paper Reviewing with Heterogeneous Graph Reasoning over LLM-Simulated Reviewer-Author Debates](https://arxiv.org/abs/2511.08317)
*Shuaimin Li,Liyang Fan,Yufang Lin,Zeyang Li,Xian Wei,Shiwen Ni,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.CL

TL;DR: ReViewGraph通过模拟审稿人-作者辩论并利用异构图神经网络进行推理，提高了论文评审的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的论文评审方法依赖于表面的手稿特征或大型语言模型（LLMs），这些方法容易产生幻觉、评分偏见和有限的推理能力，并且无法捕捉审稿人-作者互动中的复杂论证推理和协商动态。

Method: ReViewGraph是一种新颖的框架，它通过LLM模拟多轮审稿人-作者辩论，并利用异构图神经网络进行推理。

Result: 在三个数据集上的实验表明，ReViewGraph在平均相对改进方面优于强基线，改进幅度为15.73%。

Conclusion: ReViewGraph通过建模详细的审稿人-作者辩论结构，证明了其在论文评审中的有效性。

Abstract: Existing paper review methods often rely on superficial manuscript features or directly on large language models (LLMs), which are prone to hallucinations, biased scoring, and limited reasoning capabilities. Moreover, these methods often fail to capture the complex argumentative reasoning and negotiation dynamics inherent in reviewer-author interactions. To address these limitations, we propose ReViewGraph (Reviewer-Author Debates Graph Reasoner), a novel framework that performs heterogeneous graph reasoning over LLM-simulated multi-round reviewer-author debates. In our approach, reviewer-author exchanges are simulated through LLM-based multi-agent collaboration. Diverse opinion relations (e.g., acceptance, rejection, clarification, and compromise) are then explicitly extracted and encoded as typed edges within a heterogeneous interaction graph. By applying graph neural networks to reason over these structured debate graphs, ReViewGraph captures fine-grained argumentative dynamics and enables more informed review decisions. Extensive experiments on three datasets demonstrate that ReViewGraph outperforms strong baselines with an average relative improvement of 15.73%, underscoring the value of modeling detailed reviewer-author debate structures.

</details>


### [44] [Adaptive Multi-Agent Response Refinement in Conversational Systems](https://arxiv.org/abs/2511.08319)
*Soyeong Jeong,Aparna Elangovan,Emine Yilmaz,Oleg Rokhlenko*

Main category: cs.CL

TL;DR: 本文提出一种多代理框架，通过动态协作优化对话质量，尤其在需要个性化和知识的任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在单一LLM中进行响应优化，难以考虑有效对话所需的多样方面，而用户无法检测错误并请求新响应。

Method: 我们提出了一种多代理框架，其中每个代理负责审查和改进对话质量的特定方面（事实性、个性化和连贯性），并引入了一种动态通信策略来适应不同查询的需求。

Result: 我们在具有挑战性的对话数据集上验证了我们的框架，结果表明它在涉及知识或用户身份的任务中表现优异。

Conclusion: 我们的框架在涉及知识或用户身份的任务中显著优于相关基线，特别是在需要综合多种方面的情况下。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.

</details>


### [45] [AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress](https://arxiv.org/abs/2511.08325)
*Zhiheng Xi,Chenyang Liao,Guanyu Li,Yajie Yang,Wenxiang Chen,Zhihao Zhang,Binghai Wang,Senjie Jin,Yuhao Zhou,Jian Guan,Wei Wu,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的过程奖励模型 AgentPRM，用于提高大型语言模型在多轮决策任务中的性能，该方法在计算效率和可扩展性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 代理方法通常依赖于复杂的提示工程或使用专家轨迹进行微调，但这些方法在处理需要序列智能决策的任务时存在局限性。本文旨在通过构建过程奖励模型来解决这一问题。

Method: 本文提出了一种名为 AgentPRM 的重新定义的过程奖励模型，利用基于时间差分（TD-based）的估计方法和广义优势估计（GAE）来获取标注数据，以评估每个决策并指导代理的决策过程。

Result: 实验结果表明，AgentPRM 在不同代理任务中比基线方法高出 8 倍以上的计算效率，并且在增加测试时计算资源时表现出稳健的改进。

Conclusion: AgentPRM 是一种有效的过程奖励模型，能够显著提高大型语言模型在多轮决策任务中的性能，并且在计算效率和可扩展性方面优于基线方法。

Abstract: Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.

</details>


### [46] [DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering](https://arxiv.org/abs/2511.08364)
*Xinyi Wang,Yiping Song,Zhiliang Tian,Bo Liu,Tingjin Luo,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为DPRM的双隐式过程奖励模型，用于多跳问答任务中的多步骤推理。通过结合知识图谱和思维链推理，并引入一致性约束，DPRM在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式PRM仅适用于纯文本场景，在适应多跳问答任务时无法处理知识图谱的结构约束，并且无法捕捉思维链和知识图谱路径之间的潜在不一致。因此，本文旨在解决这些问题，提高多跳问答任务中多步骤推理的质量。

Method: 本文提出了DPRM（Dual Implicit Process Reward Model），它训练两个隐式PRM（KG-PRM和CoT-PRM）来处理多跳问答任务中的知识图谱和思维链推理。KG-PRM使用偏好对学习知识图谱的结构约束，而DPRM进一步引入了CoT和KG推理步骤之间的一致性约束，使两个PRM相互验证并协同优化推理路径。

Result: 实验结果表明，本文提出的DPRM方法在多个数据集上优于13个基线方法，Hit@1指标最高提升了16.6%。

Conclusion: 本文提出了一种名为DPRM的双隐式过程奖励模型，用于多跳问答任务中的多步骤推理。实验结果表明，该方法在多个数据集上优于13个基线方法，Hit@1指标最高提升了16.6%。

Abstract: In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.

</details>


### [47] [The Dynamic Articulatory Model DYNARTmo: Dynamic Movement Generation and Speech Gestures](https://arxiv.org/abs/2511.08372)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: 该论文描述了动态发音模型DYNARTmo的当前实现，该模型基于语音手势和相应的手势评分生成连续的发音器运动，并提供了一个神经生物学启发的计算框架，用于模拟语音产生的分层控制。


<details>
  <summary>Details</summary>
Motivation: 论文旨在提供一个神经生物学启发的计算框架，以模拟语音产生的分层控制，从语言表示到发音-声学实现。

Method: 论文介绍了手势库的结构、手势在手势评分中的协调以及它们转化为控制DYNARTmo声道模型的连续发音器轨迹。

Result: 论文展示了DYNARTmo模型的结构，包括手势库的结构、手势在手势评分中的协调以及它们转化为连续发音器轨迹的方法。

Conclusion: 该论文描述了动态发音模型DYNARTmo的当前实现，该模型基于语音手势和相应的手势评分生成连续的发音器运动。模型提供了一个神经生物学启发的计算框架，用于模拟从语言表示到发音-声学实现的语音产生的分层控制。

Abstract: This paper describes the current implementation of the dynamic articulatory model DYNARTmo, which generates continuous articulator movements based on the concept of speech gestures and a corresponding gesture score. The model provides a neurobiologically inspired computational framework for simulating the hierarchical control of speech production from linguistic representation to articulatory-acoustic realization. We present the structure of the gesture inventory, the coordination of gestures in the gesture score, and their translation into continuous articulator trajectories controlling the DYNARTmo vocal tract model.

</details>


### [48] [TurkEmbed: Turkish Embedding Model on NLI & STS Tasks](https://arxiv.org/abs/2511.08376)
*Özay Ezerceli,Gizem Gümüşçekiçci,Tuğba Erkoç,Berke Özenç*

Main category: cs.CL

TL;DR: 本文介绍了TurkEmbed，这是一种新型的土耳其语嵌入模型，旨在超越现有模型，特别是在自然语言推理（NLI）和语义文本相似性（STS）任务中。


<details>
  <summary>Details</summary>
Motivation: Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding.

Method: TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings.

Result: Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4% improvement.

Conclusion: TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.

Abstract: This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.

</details>


### [49] [PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints](https://arxiv.org/abs/2511.08392)
*Tangrui Li,Pei Wang,Hongzheng Wang Christian Hahm,Matteo Spatola,Justin Shi*

Main category: cs.CL

TL;DR: PCRLLM是一种框架，通过约束推理到单步推导并显式指定前提、规则和结论，提高大型语言模型的逻辑连贯性和可信度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在将前提映射到结论时往往缺乏明确的推理规则，这导致了可信度问题。因此，需要一种方法来增强模型的逻辑连贯性并支持验证。

Method: 提出了一种名为PCRLLM的框架，该框架通过显式指定前提、规则和结论来约束推理过程，从而支持链级验证。

Result: PCRLLM能够提高模型推理的可信度，并支持多模型协作。同时，引入的基准模式可以生成大规模的步骤级推理数据。

Conclusion: PCRLLM通过约束推理到单步推导，同时保持自然语言表述，提高了大型语言模型的逻辑连贯性和可信度。此外，它促进了多模型协作，并引入了一个用于生成大规模步骤级推理数据的基准模式。

Abstract: Large Language Models (LLMs) often exhibit limited logical coherence, mapping premises to conclusions without adherence to explicit inference rules. We propose Proof-Carrying Reasoning with LLMs (PCRLLM), a framework that constrains reasoning to single-step inferences while preserving natural language formulations. Each output explicitly specifies premises, rules, and conclusions, thereby enabling verification against a target logic. This mechanism mitigates trustworthiness concerns by supporting chain-level validation even in black-box settings. Moreover, PCRLLM facilitates systematic multi-LLM collaboration, allowing intermediate steps to be compared and integrated under formal rules. Finally, we introduce a benchmark schema for generating large-scale step-level reasoning data, combining natural language expressiveness with formal rigor.

</details>


### [50] [Interaction Dynamics as a Reward Signal for LLMs](https://arxiv.org/abs/2511.08394)
*Sian Gooding,Edward Grefenstette*

Main category: cs.CL

TL;DR: 本文提出了一种新的奖励信号TRACE，通过对话的嵌入轨迹几何特性来改进大型语言模型的多轮对话对齐，并展示了其在隐私保护和诊断协作模式方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）多轮对话对齐通常依赖于文本内容的奖励信号，但这种方法忽略了互动本身的丰富互补信号。

Method: 本文引入了TRACE（基于轨迹的奖励用于代理协作估计），这是一种从对话的嵌入轨迹的几何特性中得出的新奖励信号。

Result: 仅使用这些结构信号训练的奖励模型达到了68.20%的成对准确率，与分析完整转录本的强大LLM基线（70.04%）相当。结合互动动态和文本分析的混合模型实现了最高性能（80.17%）。

Conclusion: 本文提供了强有力的证据，表明在交互环境中，代理如何沟通与它说了什么一样，是成功的重要预测因素，提供了一个新的、隐私保护的框架，不仅对齐代理，还作为理解驱动成功协作的交互模式的诊断工具。

Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.

</details>


### [51] [Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?](https://arxiv.org/abs/2511.08455)
*Shiyan Zheng,Herun Wan,Minnan Luo,Junhang Huang*

Main category: cs.CL

TL;DR: 本文研究了社会机器人检测器在存在快捷学习问题时的性能下降，并提出了基于大语言模型的缓解策略，以提高其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有社会机器人检测器在基准测试中表现良好，但在多样化的现实场景中鲁棒性有限，因为缺乏明确的地面真实数据和变化的误导线索。特别是，快捷学习对模型的影响尚未得到充分研究。

Method: 本文设计了一系列快捷场景，通过构建用户标签和表面文本线索之间的虚假关联来评估模型的鲁棒性，并提出了基于大语言模型的缓解策略，包括数据分布和模型提取因果信息的能力。

Result: 结果表明，无关特征分布的变化显著降低了社会机器人检测器的性能，基线模型的平均相对准确率下降了32%。提出的策略在快捷场景下实现了平均相对性能提升56%。

Conclusion: 本文提出了基于大语言模型的缓解策略，从数据和模型角度解决了社会机器人检测器在存在快捷学习问题时的性能下降问题。

Abstract: While existing social bot detectors perform well on benchmarks, their robustness across diverse real-world scenarios remains limited due to unclear ground truth and varied misleading cues. In particular, the impact of shortcut learning, where models rely on spurious correlations instead of capturing causal task-relevant features, has received limited attention. To address this gap, we conduct an in-depth study to assess how detectors are influenced by potential shortcuts based on textual features, which are most susceptible to manipulation by social bots. We design a series of shortcut scenarios by constructing spurious associations between user labels and superficial textual cues to evaluate model robustness. Results show that shifts in irrelevant feature distributions significantly degrade social bot detector performance, with an average relative accuracy drop of 32\% in the baseline models. To tackle this challenge, we propose mitigation strategies based on large language models, leveraging counterfactual data augmentation. These methods mitigate the problem from data and model perspectives across three levels, including data distribution at both the individual user text and overall dataset levels, as well as the model's ability to extract causal information. Our strategies achieve an average relative performance improvement of 56\% under shortcut scenarios.

</details>


### [52] [SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation](https://arxiv.org/abs/2511.08500)
*Berkcan Kapusuzoglu,Supriyo Chakraborty,Renkun Ni,Stephen Rawls,Sambit Sahu*

Main category: cs.CL

TL;DR: SPEAR-MM是一种框架，可在保持通用能力的同时实现金融领域的适应，提高了效率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融领域适应时往往会出现灾难性遗忘，导致通用推理能力的丧失，这影响了客户互动和复杂的金融分析。

Method: SPEAR-MM通过后分析估计各层对外部基准的影响，然后通过球面插值合并选择性冻结或恢复Transformer层。

Result: 在LLaMA-3.1-8B上应用SPEAR-MM，在金融任务中实现了91.2%的通用能力保留率，比标准持续预训练的69.7%要高，同时保持了94%的领域适应增益。

Conclusion: SPEAR-MM提供了一种有效的框架，可以在保持通用能力的同时实现领域适应，同时减少了计算成本，这对于资源有限的金融机构至关重要。

Abstract: Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.

</details>


### [53] [Structured RAG for Answering Aggregative Questions](https://arxiv.org/abs/2511.08505)
*Omri Koshorek,Niv Granot,Aviv Alloni,Shahar Admati,Roee Hendel,Ido Weiss,Alan Arazi,Shay-Nitzan Cohen,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本文提出 S-RAG 方法，专门针对需要从大量文档中收集信息并进行推理的聚合查询，并引入两个新的数据集 HOTELS 和 WORLD CUP 进行验证。


<details>
  <summary>Details</summary>
Motivation: 当前的数据集和方法主要关注仅需少量相关段落的查询，无法捕捉需要从大量文档中收集信息并进行推理的聚合查询。

Method: S-RAG 在数据摄入时构建语料库的结构化表示，在推理时将自然语言查询转换为对该表示的正式查询。

Result: S-RAG 显著优于常见的 RAG 系统和长上下文 LLM。

Conclusion: S-RAG 在新引入的数据集以及公共基准测试中表现出色，显著优于常见的 RAG 系统和长上下文 LLM。

Abstract: Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.

</details>


### [54] [Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research](https://arxiv.org/abs/2511.08507)
*Neelavro Saha,Rafi Shahriyar,Nafis Ashraf Roudra,Saadman Sakib,Annajiat Alim Rasel*

Main category: cs.CL

TL;DR: 本文介绍了一个新的Bangla-SGP数据集，用于支持Bangla手语语言的句子级翻译任务。


<details>
  <summary>Details</summary>
Motivation: Bangla Sign Language (BdSL) translation是一个低资源NLP任务，由于缺乏大规模的数据集，现有研究主要集中在单词和字母级别的检测上。因此，本文旨在引入一个新的平行数据集，以支持句子级别的翻译。

Method: 本文提出了一个基于规则的检索增强生成（RAG）管道，用于生成合成的句子-词素对。同时，我们微调了多种基于transformer的模型，如mBart50、Google mT5和GPT4.1-nano，并使用BLEU分数评估了它们的句子到词素翻译性能。

Result: 本文引入了Bangla-SGP数据集，该数据集包含1000个高质量的Bangla句子，这些句子由专业手语者手动标注为词素序列。此外，通过基于规则的生成方法，还生成了约3000个合成的句子-词素对。最后，我们评估了多种基于transformer的模型在句子到词素翻译任务中的性能。

Conclusion: 本文介绍了Bangla-SGP数据集，这是一个新的平行数据集，包含1000个经过人工标注的句子-词素对，并通过基于规则的检索增强生成（RAG）管道生成了约3000个合成对。此外，我们微调了几种基于transformer的模型，并评估了它们在句子到词素翻译方面的性能。

Abstract: Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.

</details>


### [55] [AlphaResearch: Accelerating New Algorithm Discovery with Language Models](https://arxiv.org/abs/2511.08522)
*Zhaojian Yu,Kaiyue Feng,Yilun Zhao,Shilin He,Xiao-Ping Zhang,Arman Cohan*

Main category: cs.CL

TL;DR: AlphaResearch是一种自主研究代理，用于在开放式问题上发现新算法。它通过结合执行验证和模拟现实世界的同行评审环境来实现这一点。虽然在部分问题上仍存在挑战，但它展示了通过大型语言模型加速算法发现的可能性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂但易于验证的问题上取得了显著进展，但在发现未知问题上仍然存在困难。因此，需要一种自主的研究代理来发现新的算法。

Method: AlphaResearch通过结合基于执行的验证和模拟现实世界的同行评审环境，迭代地提出、验证和优化新想法来发现新算法。

Result: AlphaResearch在与人类研究人员的直接比较中获得了2/8的胜率，并且在“打包圆”问题上发现了最佳已知性能的算法。

Conclusion: AlphaResearch展示了通过大型语言模型加速算法发现的可能性，尽管在部分问题上仍存在挑战。

Abstract: Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present \textbf{AlphaResearch}, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct \textbf{AlphaResearchComp}, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the \emph{``packing circles''} problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.

</details>


### [56] [Investigating CoT Monitorability in Large Reasoning Models](https://arxiv.org/abs/2511.08525)
*Shu Yang,Junchao Wu,Xilin Gou,Xuansheng Wu,Derek Wong,Ninhao Liu,Di Wang*

Main category: cs.CL

TL;DR: 本文首次系统地研究了CoT monitorability的挑战和潜力，提出了MoME新范式以监控模型的不当行为。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决两个关键挑战：1）模型可能不真实地表达其内部决策过程；2）监控器可能过于敏感或不够敏感，容易被模型的长而复杂的推理过程欺骗。

Method: 本文围绕两个核心视角展开研究：(i) verbalization（LRMs在CoT中忠实地表达指导其决策的真实因素的程度），以及(ii) monitor reliability（CoT-based monitor检测不当行为的可靠性）。我们提供了实证证据和相关性分析，以及不同CoT干预方法对监控效果的影响。

Result: 本文提供了关于verbalization质量、monitor reliability和LLM性能之间关系的实证证据，并探讨了不同CoT干预方法对监控效果的影响。

Conclusion: 本文提出了MoME，一种新的范式，其中LLMs通过其CoT监控其他模型的不当行为，并提供结构化的判断和支撑证据。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.

</details>


### [57] [From Semantic Roles to Opinion Roles: SRL Data Extraction for Multi-Task and Transfer Learning in Low-Resource ORL](https://arxiv.org/abs/2511.08537)
*Amirmohammad Omidi Galdiani,Sepehr Rezaei Melal,Mohammad Norasteh,Arash Yousefi Jordehi,Seyed Abolghasem Mirroshandel*

Main category: cs.CL

TL;DR: 本文提出了一种方法论，用于从OntoNotes 5.0语料库的WSJ部分构建高质量的SRL数据集，并将其适应于ORL任务，提供了可重复使用的资源以增强ORL在低资源情感挖掘场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 本文旨在为研究人员提供一个可重复使用的资源，以利用SRL来增强ORL，特别是在低资源情感挖掘场景中。

Method: 本文提出了一种详细的方法论，用于从OntoNotes 5.0语料库的WSJ部分构建高质量的语义角色标注（SRL）数据集，并将其适应于观点角色标注（ORL）任务。

Result: 该数据集包含97,169个带有明确定义的Agent（ARG0）、Predicate（REL）和Patient（ARG1）角色的谓词-论元实例，并映射到ORL的Holder、Expression和Target模式。

Conclusion: 本文提供了一个可重复使用的资源，有助于在低资源情感挖掘场景中利用SRL来增强ORL。

Abstract: This report presents a detailed methodology for constructing a high-quality Semantic Role Labeling (SRL) dataset from the Wall Street Journal (WSJ) portion of the OntoNotes 5.0 corpus and adapting it for Opinion Role Labeling (ORL) tasks. Leveraging the PropBank annotation framework, we implement a reproducible extraction pipeline that aligns predicate-argument structures with surface text, converts syntactic tree pointers to coherent spans, and applies rigorous cleaning to ensure semantic fidelity. The resulting dataset comprises 97,169 predicate-argument instances with clearly defined Agent (ARG0), Predicate (REL), and Patient (ARG1) roles, mapped to ORL's Holder, Expression, and Target schema. We provide a detailed account of our extraction algorithms, discontinuous argument handling, annotation corrections, and statistical analysis of the resulting dataset. This work offers a reusable resource for researchers aiming to leverage SRL for enhancing ORL, especially in low-resource opinion mining scenarios.

</details>


### [58] [Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models](https://arxiv.org/abs/2511.08565)
*Davi Bastos Costa,Felippe Alves,Renato Vicente*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型在角色扮演情境下的道德反应，发现模型家族对道德稳健性有显著影响，而模型大小对道德易感性有影响，并揭示了道德基础的特征。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地参与社会情境，研究它们如何表达和改变道德判断变得重要。本文旨在分析角色扮演对模型道德反应的影响。

Method: 使用道德基础问卷（MFQ）引入了一个基准，用于量化道德易感性和道德稳健性，通过分析不同模型家族和规模的表现来评估这些属性。

Result: 模型家族对道德稳健性有显著影响，而模型大小没有系统性影响。Claude家族表现出最高的道德稳健性，其次是Gemini和GPT-4模型。道德易感性在模型家族间有轻微影响，但模型大小有明显影响，较大的模型更易受影响。道德稳健性和易感性之间存在正相关关系，这种关联在家族层面更为明显。

Conclusion: 本文提供了对大型语言模型在角色扮演情境下道德行为的系统性分析，揭示了模型家族和规模对道德易感性和道德稳健性的影响，并展示了道德基础的特征。

Abstract: Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.

</details>


### [59] [Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models](https://arxiv.org/abs/2511.08577)
*Tianyu Fu,Yichen You,Zekai Chen,Guohao Dai,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: TaH 是一种动态的潜在思考方法，通过仅在困难标记上进行迭代来提升 LLM 的推理性能，同时保持参数数量不变。


<details>
  <summary>Details</summary>
Motivation: 改善大型语言模型（LLMs）的推理能力，特别是在参数约束下，对于实际应用至关重要。先前的工作提出了循环变压器，它们为每个标记分配固定数量的额外迭代以提高生成质量。然而，我们发现了一个潜在的过度思考现象：在第一次传递后已经正确的简单标记有时会在额外的迭代中被修改为错误。

Method: TaH 是一种动态的潜在思考方法，它仅在困难的标记上进行更深层次的迭代。它使用轻量级神经决策器来触发潜在迭代，仅在标准前向传递后可能不正确的标记上进行。在潜在迭代中，LoRA 模块将 LLM 目标从一般的下一个标记预测转移到聚焦的困难标记细化。此外，引入了一种双因果注意力机制，使注意力从标记序列维度扩展到额外的迭代深度维度。

Result: 实验表明，TaH 在五个具有挑战性的基准测试中提升了 LLM 的推理性能，同时保持相同的参数数量。与所有输出标记都迭代两次的基线相比，TaH 在免除 94% 的标记进行第二次迭代的同时，准确率提高了 8.1-11.3%。与经过相同数据微调的单次迭代 Qwen3 模型相比，它也提供了 4.0-5.0% 的准确率提升。当允许少于 3% 的额外参数来自 LoRA 和迭代决策器时，增益分别增加到 8.5-12.6% 和 5.3-5.4%。

Conclusion: TaH 提升了 LLM 的推理性能，同时保持相同的参数数量，并在少量额外参数的情况下进一步提高了性能。

Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.

</details>


### [60] [Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579)
*Belinda Z. Li,Zifan Carl Guo,Vincent Huang,Jacob Steinhardt,Jacob Andreas*

Main category: cs.CL

TL;DR: 语言模型可以学习解释自己的内部计算，并且这种解释可以作为现有可解释性方法的补充。


<details>
  <summary>Details</summary>
Motivation: 我们研究了语言模型如何利用其对自身内部结构的特权访问来产生新的解释技术。

Method: 我们微调语言模型以生成自然语言描述，包括（1）语言模型特征编码的信息，（2）语言模型内部激活的因果结构，以及（3）特定输入标记对语言模型输出的影响。

Result: 当仅使用数万个示例解释进行训练时，解释器模型在新查询上表现出非 trivial 的泛化能力。这种泛化部分归因于解释器模型对其自身内部结构的特权访问：使用一个模型来解释其自身的计算通常比使用另一个模型来解释其计算更有效。

Conclusion: 我们的结果表明，语言模型可以学习可靠地解释其内部计算，并且这些解释为现有的可解释性方法提供了一个可扩展的补充。

Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [61] [The Polite Liar: Epistemic Pathology in Language Models](https://arxiv.org/abs/2511.07477)
*Bentley DeVilling*

Main category: cs.CY

TL;DR: 论文指出大型语言模型在RLHF下表现出一种称为'礼貌说谎者'的现象，即它们在没有证据的情况下表现得自信，这源于奖励架构优化了感知到的真诚度而非证据准确性。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探讨大型语言模型为何会在不了解的情况下表现出自信，并指出这是强化学习与人类反馈（RLHF）结构上的结果。

Method: 论文通过认识论美德理论、言语行为哲学和认知对齐的视角分析了这种行为。

Result: 论文揭示了RLHF导致系统学习最大化用户满意度而非真相，从而产生一种模仿认识论自信但缺乏认识论依据的行为。

Conclusion: 论文得出一个'认识论对齐'原则：奖励有根据的自信，而不是表面的流利度。

Abstract: Large language models exhibit a peculiar epistemic pathology: they speak as if they know, even when they do not. This paper argues that such confident fabrication, what I call the polite liar, is a structural consequence of reinforcement learning from human feedback (RLHF). Building on Frankfurt's analysis of bullshit as communicative indifference to truth, I show that this pathology is not deception but structural indifference: a reward architecture that optimizes for perceived sincerity over evidential accuracy. Current alignment methods reward models for being helpful, harmless, and polite, but not for being epistemically grounded. As a result, systems learn to maximize user satisfaction rather than truth, performing conversational fluency as a virtue. I analyze this behavior through the lenses of epistemic virtue theory, speech-act philosophy, and cognitive alignment, showing that RLHF produces agents trained to mimic epistemic confidence without access to epistemic justification. The polite liar thus reveals a deeper alignment tension between linguistic cooperation and epistemic integrity. The paper concludes with an "epistemic alignment" principle: reward justified confidence over perceived fluency.

</details>


### [62] [Generative Artificial Intelligence in Qualitative Research Methods: Between Hype and Risks?](https://arxiv.org/abs/2511.08461)
*Maria Couto Teixeira,Marisa Tschopp,Anna Jobin*

Main category: cs.CY

TL;DR: 本文批判性地分析了生成式AI在定性研究中的应用，指出其方法论上的无效性，并警告研究人员避免因技术新颖性而忽视方法论的严谨性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在定性研究中的广泛应用，其引发的方法论问题亟需关注。本文旨在评估生成式AI在定性研究中的潜在风险和局限性。

Method: 本文通过批判性分析生成式AI（genAI）在定性编码方法中的作用，探讨了其在定性研究中的适用性。

Result: 生成式AI缺乏有意义的文档记录、商业不透明以及产生错误输出的倾向，这些因素都削弱了方法论的严谨性。

Conclusion: 本文认为生成式AI在定性研究中不具备方法论上的有效性，其使用可能削弱定性研究的严谨性和可信度。作者建议研究人员应将稳健的方法论置于技术新颖性之前。

Abstract: As Artificial Intelligence (AI) is increasingly promoted and used in qualitative research, it also raises profound methodological issues. This position paper critically interrogates the role of generative AI (genAI) in the context of qualitative coding methodologies. Despite widespread hype and claims of efficiency, we propose that genAI is not methodologically valid within qualitative inquiries, and its use risks undermining the robustness and trustworthiness of qualitative research. The lack of meaningful documentation, commercial opacity, and the inherent tendencies of genAI systems to produce incorrect outputs all contribute to weakening methodological rigor. Overall, the balance between risk and benefits does not support the use of genAI in qualitative research, and our position paper cautions researchers to put sound methodology before technological novelty.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [63] [Quantifying the Impact of CU: A Systematic Literature Review](https://arxiv.org/abs/2511.07491)
*Thomas Compton*

Main category: cs.DL

TL;DR: 本文探讨了社区工会（CU）为何在2000年代初成为工会复兴辩论的核心概念，通过引用网络分析和主题回顾的方法，揭示了CU作为经验描述和规范理想的功能，并指出其重要性在于管理劳工运动中的矛盾。


<details>
  <summary>Details</summary>
Motivation: To investigate why CU has gained such prominence -- not by testing its efficacy, but by mapping how it is constructed, cited, and contested across the scholarly literature.

Method: Using two complementary systematic approaches -- a citation network analysis of 114 documents and a thematic review of 18 core CU case studies.

Result: CU's dual genealogy: positioned by British scholars as an indigenous return to historic rank-and-file practices, yet structurally aligned with transnational social movement unionism. Thematic coding shows near-universal emphasis on coalition-building and alliances, but deep ambivalence toward class politics.

Conclusion: CU's significance lies less in operationalising a new union model, and more in managing contradictions -- between workplace and community, leadership and rank-and-file, reform and radicalism -- within a shrinking labour movement.

Abstract: Community Unionism has served as a pivotal concept in debates on trade union renewal since the early 2000s, yet its theoretical coherence and political significance remain unresolved. This article investigates why CU has gained such prominence -- not by testing its efficacy, but by mapping how it is constructed, cited, and contested across the scholarly literature. Using two complementary systematic approaches -- a citation network analysis of 114 documents and a thematic review of 18 core CU case studies -- I examine how CU functions as both an empirical descriptor and a normative ideal. The analysis reveals CU's dual genealogy: positioned by British scholars as an indigenous return to historic rank-and-file practices, yet structurally aligned with transnational social movement unionism. Thematic coding shows near-universal emphasis on coalition-building and alliances, but deep ambivalence toward class politics. This tension suggests CU's significance lies less in operationalising a new union model, and more in managing contradictions -- between workplace and community, leadership and rank-and-file, reform and radicalism -- within a shrinking labour movement.

</details>


### [64] [CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis](https://arxiv.org/abs/2511.07790)
*Rochana R. Obadage,Sarah M. Rajtmajer,Jian Wu*

Main category: cs.DL

TL;DR: 本文介绍了CC30k数据集，用于预测和研究机器学习论文中与可重复性相关的感情，并展示了其在提升语言模型性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了训练有效的模型来预测与可重复性相关的感情，并系统研究它们与可重复性的相关性，需要一个专注于可重复性导向情感的数据集。

Method: 引入了CC30k数据集，包含30,734个机器学习论文中的引用上下文，并通过众包和受控管道生成标签。

Result: CC30k数据集具有94%的标注准确率，并且在使用该数据集进行微调后，三种大型语言模型在可重复性导向情感分类任务上的表现显著提高。

Conclusion: CC30k数据集为大规模评估机器学习论文的可重复性奠定了基础，并且通过微调大型语言模型显著提高了可重复性导向情感分类的性能。

Abstract: Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [65] [LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost](https://arxiv.org/abs/2511.07865)
*Daisuke Kikuta,Hiroki Ikeuchi,Kengo Tajiri*

Main category: cs.SE

TL;DR: This paper introduces ChaosEater, a system that automates the entire Chaos Engineering cycle using Large Language Models (LLMs). It reduces the time and cost of CE experiments and has been validated through case studies on Kubernetes systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of planning CE experiments and improving systems based on experimental results, which are currently manual, labor-intensive, and require multi-domain expertise. It seeks to enable anyone to build resilient systems at low cost.

Method: ChaosEater uses Large Language Models (LLMs) to automate the entire Chaos Engineering (CE) cycle. It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. The LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging.

Result: ChaosEater was evaluated through case studies on small- and large-scale Kubernetes systems. The results show that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.

Conclusion: ChaosEater demonstrates the ability to consistently complete reasonable CE cycles with significantly low time and monetary costs, and its cycles are qualitatively validated by human engineers and LLMs.

Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits](https://arxiv.org/abs/2511.07482)
*Dev Patel,Gabrielle Gervacio,Diekola Raimi,Kevin Zhu,Ryan Lagasse,Gabriel Grand,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: AAPP是一种动态结构剪枝方法，能够在保持计算效率的同时提高大型语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时需要大量的计算资源，而动态剪枝虽然提高了效率，但可能导致对齐退化，因此需要一种既能保持效率又能保证安全性的方法。

Method: AAPP是一种动态结构剪枝方法，通过在推理过程中自适应地保留与对齐相关的电路，从而解决动态剪枝导致的对齐退化问题。

Result: 在LLaMA 2-7B、Qwen2.5-14B-Instruct和Gemma-3-12B-IT上的实验表明，AAPP在相同计算量下将拒绝率提高了50%。

Conclusion: AAPP在保持计算效率的同时，能够有效提升大型语言模型的安全性，为高效且安全的LLM部署提供了新的解决方案。

Abstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\% at matched compute, enabling efficient yet safety-preserving LLM deployment.

</details>


### [67] [LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows](https://arxiv.org/abs/2511.07585)
*Raffi Khatchadourian,Rolando Franco*

Main category: cs.LG

TL;DR: 研究分析了不同规模的大型语言模型在金融任务中的输出一致性，发现较小的模型在受监管任务中表现更稳定，而较大的模型存在显著的输出漂移问题。研究提出了一套框架，包括测试工具、任务检查、模型分类系统和审计证明系统，以确保合规性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 金融行业部署大型语言模型（LLMs）用于对账、监管报告和客户沟通，但非确定性输出（输出漂移）削弱了可审计性和信任度。因此，需要量化漂移并提供解决方案以确保合规性和可靠性。

Method: 研究采用了一个金融校准的确定性测试工具，结合贪婪解码、固定种子和SEC 10-K结构感知检索排序，并使用金融校准的材料性阈值（正负5%）和SEC引用验证进行任务特定的不变性检查。此外，还提出了一个三层模型分类系统和一个审计就绪的证明系统。

Result: 研究发现，较小的模型（Granite-3-8B, Qwen2.5-7B）在T=0.0时达到100%的输出一致性，而GPT-OSS-120B仅达到12.5%。结构化任务（如SQL）在T=0.2时保持稳定，而RAG任务显示出漂移（25-75%）。跨供应商验证确认了确定性行为在本地和云部署之间的转移。

Conclusion: 研究发现，较小的模型在受监管的金融任务中表现出更高的输出一致性，而较大的模型如GPT-OSS-120B则表现出较低的一致性。这挑战了传统假设，即更大的模型在生产部署中是更优的选择。

Abstract: Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.

</details>


### [68] [DynaAct: Large Language Model Reasoning with Dynamic Action Spaces](https://arxiv.org/abs/2511.08043)
*Xueliang Zhao,Wei Wu,Jian Guan,Qintong Li,Lingpeng Kong*

Main category: cs.LG

TL;DR: 本文提出了一种名为DynaAct的新框架，用于自动构建紧凑的动作空间，以增强复杂问题解决场景中的顺序推理。我们的方法首先估计一个代理完整的动作空间，然后制定一个子模函数来评估候选动作，并采用贪心算法选择最优的候选集。实验结果表明，该方法在多个基准测试中显著提高了性能，同时保持了高效的推理。


<details>
  <summary>Details</summary>
Motivation: 在现代顺序决策系统中，构建一个最优的候选动作空间对于高效推理至关重要。然而，现有的方法要么依赖于手动定义的动作空间，这缺乏可扩展性，要么使用无结构的空间，使得穷举搜索计算上不可行。

Method: 我们首先通过使用大型语言模型从涵盖多种复杂推理问题的语料库中提取通用的草图来估计完整的动作空间的代理。然后，我们制定一个子模函数，根据候选动作对当前状态的效用和其多样性来联合评估候选动作，并采用贪心算法选择最优的候选集。

Result: 实验结果表明，我们的方法在六个不同的标准基准测试中显著提高了整体性能，同时保持了高效的推理而没有引入显著的延迟。

Conclusion: 我们的方法在六个不同的标准基准测试中显著提高了整体性能，同时保持了高效的推理而没有引入显著的延迟。

Abstract: In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [69] [SpeechJudge: Towards Human-Level Judgment for Speech Naturalness](https://arxiv.org/abs/2511.07931)
*Xueyao Zhang,Chaoren Wang,Huan Liao,Ziniu Li,Yuancheng Wang,Li Wang,Dongya Jia,Yuanzhe Chen,Xiulin Li,Zhuo Chen,Zhizheng Wu*

Main category: cs.SD

TL;DR: SpeechJudge introduces a dataset, benchmark, and reward model for speech naturalness judgment, with SpeechJudge-GRM showing improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: The lack of a large-scale human preference dataset for speech synthesis hinders the development of models that align with human perception. SpeechJudge aims to address this challenge.

Method: SpeechJudge-GRM is developed using a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases.

Result: SpeechJudge-GRM achieves 77.2% accuracy (and 79.4% after inference-time scaling @10) on the SpeechJudge-Eval benchmark, outperforming a classic Bradley-Terry reward model (72.7%).

Conclusion: SpeechJudge-GRM demonstrates superior performance in speech naturalness judgment and can be used as a reward function to align speech generation models with human preferences.

Abstract: Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce SpeechJudge, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on naturalness--one of the most fundamental subjective metrics for speech synthesis. First, we present SpeechJudge-Data, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish SpeechJudge-Eval, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop SpeechJudge-GRM, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [70] [Pruning as Regularization: Sensitivity-Aware One-Shot Pruning in ASR](https://arxiv.org/abs/2511.08092)
*Julian Irigoyen,Arthur Söhler,Andreas Søeborg Kirkedal*

Main category: eess.AS

TL;DR: 本文挑战了神经网络剪枝仅作为压缩技术的传统观点，证明剪枝可以作为强大的隐式正则化工具。通过分析模型结构，我们发现某些部分对性能至关重要，而其他部分可以被剪枝而不影响性能。实验表明，剪枝不仅能提高模型的泛化能力，还能实现更高效的压缩。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为神经网络剪枝仅用于压缩模型，但本文旨在挑战这一观点，探索剪枝作为正则化工具的潜力。我们希望通过剪枝来提高模型的泛化能力，同时实现更高效的模型压缩。

Method: 我们使用Whisper-small模型，结合梯度和Fisher敏感性诊断与有针对性的组件级剪枝，以分析神经网络的结构特性。通过这种方法，我们能够识别出哪些部分对模型性能至关重要，哪些部分可以被安全地剪枝而不影响性能。

Result: 实验结果显示，剪枝50%的解码器自注意力层在LibriSpeech test-other数据集上将WER降低了2.38%绝对值（20.44%相对值）；而剪枝最后四个编码器层50%则带来了1.72%绝对值（14.8%相对值）的改进。这些提升在Common Voice和TED-LIUM数据集上也得到了验证。此外，在40%稀疏度下，我们的方法仍然能保持接近基线的准确性，而传统全局剪枝方法在此时会失败。

Conclusion: 我们的研究表明，神经网络剪枝不仅是压缩技术，还可以作为强大的隐式正则化工具。通过识别架构中的不对称性，我们发现剪枝可以提高模型的泛化能力，并且在不进行微调的情况下也能取得显著效果。此外，我们的方法能够在更高的稀疏度下保持接近基线的准确性，这使得剪枝成为一种重要的架构设计工具。

Abstract: We challenge the conventional view of neural network pruning as solely a compression technique, demonstrating that one-shot magnitude pruning serves as a powerful implicit regularizer for ASR. Using Whisper-small, we combine gradient- and Fisher-based sensitivity diagnostics with targeted, component-wise pruning. This reveals architectural asymmetries: decoder FFNs are pruning-fragile, whereas decoder self-attention and the last encoder layers contain redundancy that, when removed, improves generalization. Without fine-tuning, pruning 50% of decoder self-attention reduces WER by 2.38% absolute (20.44% relative) on LibriSpeech test-other; pruning the last four encoder layers at 50% instead yields a 1.72% absolute (14.8% relative) improvement. Gains persisted on Common Voice and TED-LIUM datasets. Beyond regularization benefits, our sensitivity-aware approach enables more aggressive one-shot compression. At 40% sparsity, where established global pruning approaches catastrophically fail, our method preserves near-baseline accuracy. This positions pruning as a first-class architectural design tool: knowing where to prune is as important as how much to prune.

</details>


### [71] [Quantizing Whisper-small: How design choices affect ASR performance](https://arxiv.org/abs/2511.08093)
*Arthur Söhler,Julian Irigoyen,Andreas Søeborg Kirkedal*

Main category: eess.AS

TL;DR: 本文研究了后训练量化对Whisper-small的影响，发现动态int8量化与Quanto能有效减少模型大小并提高性能，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 大型语音识别模型如Whisper-small虽然准确率高，但由于计算需求高，在边缘设备上难以部署。

Method: 我们进行了统一的跨库评估，研究了后训练量化（PTQ）对Whisper-small的影响，包括量化方案、方法、粒度和位宽。

Result: 动态int8量化与Quanto提供了最佳平衡，将模型大小减少了57%，同时提高了基线的词错误率。静态量化表现较差，可能是因为Whisper的Transformer架构，而更激进的格式（如nf4、int3）在嘈杂条件下以牺牲准确性为代价实现了高达71%的压缩。

Conclusion: 我们的结果表明，经过仔细选择的PTQ方法可以显著减少模型大小和推理成本，而无需重新训练，从而实现了Whisper-small在受限硬件上的高效部署。

Abstract: Large speech recognition models like Whisper-small achieve high accuracy but are difficult to deploy on edge devices due to their high computational demand. To this end, we present a unified, cross-library evaluation of post-training quantization (PTQ) on Whisper-small that disentangles the impact of quantization scheme, method, granularity, and bit-width. Our study is based on four libraries: PyTorch, Optimum-Quanto, HQQ, and bitsandbytes. Experiments on LibriSpeech test-clean and test-other show that dynamic int8 quantization with Quanto offers the best trade-off, reducing model size by 57% while improving on the baseline's word error rate. Static quantization performed worse, likely due to Whisper's Transformer architecture, while more aggressive formats (e.g., nf4, int3) achieved up to 71% compression at the cost of accuracy in noisy conditions. Overall, our results demonstrate that carefully chosen PTQ methods can substantially reduce model size and inference cost without retraining, enabling efficient deployment of Whisper-small on constrained hardware.

</details>


### [72] [Unifying Model and Layer Fusion for Speech Foundation Models](https://arxiv.org/abs/2511.08389)
*Yi-Jen Shih,David Harwath*

Main category: eess.AS

TL;DR: 本文提出了一种接口模块，用于融合多个上游语音模型并整合其层的信息，实验表明该方法在多种语音任务中优于之前的融合方法。


<details>
  <summary>Details</summary>
Motivation: 先前的工作表明，同一模型多个层的表示融合或多个模型的融合可以提高下游任务的性能。我们旨在统一这两种融合策略。

Method: 我们通过提出一个接口模块来统一这两种融合策略，该模块能够融合多个上游语音模型，并在其层之间整合信息。

Result: 我们在不同的自监督和监督模型上进行了广泛的实验，涉及各种语音任务，包括ASR和副语言分析，并证明了我们的方法优于之前的融合方法。

Conclusion: 我们的结果表明，所提出的接口在给定适当的上游模型选择时提供了额外的性能提升，使其成为利用语音基础模型的一种有前景的方法。

Abstract: Speech Foundation Models have gained significant attention recently. Prior works have shown that the fusion of representations from multiple layers of the same model or the fusion of multiple models can improve performance on downstream tasks. We unify these two fusion strategies by proposing an interface module that enables fusion across multiple upstream speech models while integrating information across their layers. We conduct extensive experiments on different self-supervised and supervised models across various speech tasks, including ASR and paralinguistic analysis, and demonstrate that our method outperforms prior fusion approaches. We further analyze its scalability concerning model size and count, highlighting the importance of selecting appropriate upstream models. Our results show that the proposed interface provides an additional performance boost when given a suitable upstream model selection, making it a promising approach for utilizing Speech Foundation Models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [73] [How Brittle is Agent Safety? Rethinking Agent Risk under Intent Concealment and Task Complexity](https://arxiv.org/abs/2511.08487)
*Zihan Ma,Dongsheng Zhu,Shudong Liu,Taolin Zhang,Junnan Liu,Qingqiu Li,Minnan Luo,Songyang Zhang,Kai Chen*

Main category: cs.MA

TL;DR: 本文提出了OASIS，一个用于评估LLM驱动代理在复杂任务中隐藏恶意意图的安全性的基准，并发现了安全对齐随意图模糊而下降以及复杂性悖论的现象。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM驱动代理的安全评估主要集中在原子危害上，未能解决恶意意图在复杂任务中被隐藏或稀释的复杂威胁。

Method: 我们引入了OASIS（正交代理安全查询套件），这是一个具有细粒度注释和高保真模拟沙箱的分层基准。

Result: 我们的发现揭示了两个关键现象：当意图变得模糊时，安全对齐会急剧且可预测地下降，并出现“复杂性悖论”，即代理在更困难的任务上似乎更安全，只是因为能力限制。

Conclusion: 我们通过发布OASIS及其模拟环境，为探测和增强这些被忽视维度中的代理安全性提供了原则性基础。

Abstract: Current safety evaluations for LLM-driven agents primarily focus on atomic harms, failing to address sophisticated threats where malicious intent is concealed or diluted within complex tasks. We address this gap with a two-dimensional analysis of agent safety brittleness under the orthogonal pressures of intent concealment and task complexity. To enable this, we introduce OASIS (Orthogonal Agent Safety Inquiry Suite), a hierarchical benchmark with fine-grained annotations and a high-fidelity simulation sandbox. Our findings reveal two critical phenomena: safety alignment degrades sharply and predictably as intent becomes obscured, and a "Complexity Paradox" emerges, where agents seem safer on harder tasks only due to capability limitations. By releasing OASIS and its simulation environment, we provide a principled foundation for probing and strengthening agent safety in these overlooked dimensions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [74] [ViPRA: Video Prediction for Robot Actions](https://arxiv.org/abs/2511.07732)
*Sandeep Routray,Hengkai Pan,Unnat Jain,Shikhar Bahl,Deepak Pathak*

Main category: cs.RO

TL;DR: ViPRA是一种从无动作视频中学习机器人控制的预训练微调框架，通过预测未来视觉观察和运动中心的潜在动作来实现。


<details>
  <summary>Details</summary>
Motivation: 大多数视频缺乏标记的动作，这限制了它们在机器人学习中的使用。

Method: 我们提出了Video Prediction for Robot Actions (ViPRA)，这是一个简单的预训练微调框架，可以从无动作的视频中学习连续机器人控制。

Result: 我们的方法避免了昂贵的动作注释，支持跨实体的泛化，并通过分块动作解码实现了高达22 Hz的平滑、高频连续控制。

Conclusion: 我们的方法在SIMPLER基准测试中比强基线提高了16%，在现实世界操作任务中提高了13%。

Abstract: Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io

</details>


### [75] [PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision](https://arxiv.org/abs/2511.08098)
*Sabrina Patania,Luca Annese,Anita Pellegrini,Silvia Serino,Anna Lambiase,Luca Pallonetto,Silvia Rossi,Simone Colombani,Tom Foulsham,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.RO

TL;DR: 本研究探讨了通过显式融入多种观点和主动探索策略来提升LLM在多智能体系统中的视角理解能力和协作效果的可能性，并取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 当前的训练范式往往忽视了这些交互情境，导致模型在需要推理个体视角的主观性或在有多个观察者的环境中导航时遇到挑战。因此，本研究旨在探讨通过显式融入多种观点和主动探索策略来改善LLM的视角理解能力。

Method: 本研究评估了是否可以通过ReAct框架（一种结合推理和行动的方法）显式地融入多种观点来增强LLM理解并定位其他智能体需求的能力。我们扩展了经典的Director任务，引入了一套七个逐步增加视角理解复杂度的场景。这些场景旨在挑战智能体根据视觉访问和互动解决指称歧义的能力，在不同的状态表示和提示策略下进行测试，包括ReAct风格的推理。

Result: 研究结果表明，显式的视角提示与主动探索策略的结合显著提高了模型的解释准确性和协作效果。

Conclusion: 研究结果表明，显式地引入不同的观点和主动探索策略可以显著提高模型的解释准确性和协作效果。这些发现突显了将主动感知与视角理解机制相结合在推进LLMs在机器人和多智能体系统中的应用的潜力，为未来研究自适应和上下文感知的人工智能系统奠定了基础。

Abstract: Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [76] [LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation](https://arxiv.org/abs/2511.06254)
*Teng Shi,Chenglei Shen,Weijie Yu,Shen Nie,Chongxuan Li,Xiao Zhang,Ming He,Yan Han,Jun Xu*

Main category: cs.IR

TL;DR: LLaDA-Rec是一种基于离散扩散的推荐框架，通过引入双向注意力和自适应生成顺序，解决了现有自回归模型的局限性，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型存在两个内在限制：（1）单向约束，其中因果注意力限制每个标记只能关注其前驱，阻碍全局语义建模；（2）误差累积，其中固定的从左到右生成顺序导致早期标记的预测误差传播到后续标记的预测。因此，需要一种新的方法来解决这些问题。

Method: LLaDA-Rec是一种离散扩散框架，将推荐重新定义为并行语义ID生成。它结合了双向注意力和自适应生成顺序，以更有效地建模项间和项内依赖关系，并减轻误差累积。具体包括：（1）并行标记化方案；（2）用户历史和下一个项级别的两种掩码机制；（3）适应性顺序的离散扩散解码的改进束搜索策略。

Result: LLaDA-Rec在三个真实世界数据集上 consistently 超过基于ID和最先进的生成推荐器，确立了离散扩散作为生成推荐的新范式。

Conclusion: LLaDA-Rec通过引入双向注意力和自适应生成顺序，有效解决了现有自回归模型的局限性，并在三个真实世界数据集上表现出色，确立了离散扩散作为生成推荐的新范式。

Abstract: Generative recommendation represents each item as a semantic ID, i.e., a sequence of discrete tokens, and generates the next item through autoregressive decoding. While effective, existing autoregressive models face two intrinsic limitations: (1) unidirectional constraints, where causal attention restricts each token to attend only to its predecessors, hindering global semantic modeling; and (2) error accumulation, where the fixed left-to-right generation order causes prediction errors in early tokens to propagate to the predictions of subsequent token. To address these issues, we propose LLaDA-Rec, a discrete diffusion framework that reformulates recommendation as parallel semantic ID generation. By combining bidirectional attention with the adaptive generation order, the approach models inter-item and intra-item dependencies more effectively and alleviates error accumulation. Specifically, our approach comprises three key designs: (1) a parallel tokenization scheme that produces semantic IDs for bidirectional modeling, addressing the mismatch between residual quantization and bidirectional architectures; (2) two masking mechanisms at the user-history and next-item levels to capture both inter-item sequential dependencies and intra-item semantic relationships; and (3) an adapted beam search strategy for adaptive-order discrete diffusion decoding, resolving the incompatibility of standard beam search with diffusion-based generation. Experiments on three real-world datasets show that LLaDA-Rec consistently outperforms both ID-based and state-of-the-art generative recommenders, establishing discrete diffusion as a new paradigm for generative recommendation.

</details>


### [77] [BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives](https://arxiv.org/abs/2511.08029)
*Aarush Sinha,Pavan Kumar S,Roshan Balaji,Nirav Pravinbhai Bhatt*

Main category: cs.IR

TL;DR: 本文提出了一种利用引用链接进行硬负样本挖掘的方法，以提高生物医学领域的小型密集检索器的性能。通过微调GTE_small和GTE_Base模型，我们实现了在零样本密集检索任务中的改进，并在长尾主题上优于基线。


<details>
  <summary>Details</summary>
Motivation: 硬负样本对于训练有效的检索模型至关重要。然而，在生物医学和科学领域，由于难以区分源文档和硬负样本文档，硬负样本挖掘变得具有挑战性。引用文档自然与源文档具有上下文相关性但不是重复项，因此非常适合作为硬负样本。

Method: 我们提出了BiCA：基于引用的硬负样本的生物医学密集检索方法，通过利用20,000篇PubMed文章中的引用链接来改进特定领域的小型密集检索器。我们使用这些基于引用的负样本对GTE_small和GTE_Base模型进行微调，并观察到在BEIR上的零样本密集检索中的一致改进，以及在LoTTE的长尾主题上使用Success@5优于基线。

Result: 我们观察到在BEIR上的零样本密集检索中的一致改进，并且在LoTTE的长尾主题上使用Success@5优于基线。

Conclusion: 我们的研究结果表明，利用文档链接结构生成高度信息丰富的负样本具有潜力，能够在最小微调的情况下实现最先进的性能，并展示了通往高度数据高效领域适应的道路。

Abstract: Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [78] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: 本文分析了MCP启用的LLM交互，揭示了能力、性能和成本之间的权衡，并提出了优化方法以提高效率和降低成本。


<details>
  <summary>Details</summary>
Motivation: MCP最近在AI社区中引起了广泛关注，因为它为大型语言模型（LLMs）与外部工具和服务的交互提供了一种标准化的方式，显著增强了它们的能力。然而，MCP启用的LLM交互中包含大量的上下文信息，这会增加令牌使用量，导致成本上升和计算负载增加。因此，需要对MCP启用的交互进行分析，以找到平衡能力、性能和成本的方法。

Method: 本文通过测量分析MCP启用的LLM交互，探讨了不同LLM模型和MCP配置对关键性能指标的影响，包括令牌效率、货币成本、任务完成时间和任务成功率，并提出了潜在的优化方法，如启用并行工具调用和实施强大的任务中止机制。

Result: 本文的分析揭示了MCP启用的交互中能力、性能和成本之间的权衡。研究发现，不同的LLM模型和MCP配置会对令牌效率、货币成本、任务完成时间和任务成功率产生影响。此外，研究还提出了潜在的优化方法，如启用并行工具调用和实施强大的任务中止机制。

Conclusion: 本文提供了对MCP启用的LLM交互的全面测量分析，揭示了能力、性能和成本之间的权衡。研究结果为开发更高效、稳健和经济的MCP启用工作流程提供了有用的见解。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [79] [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)
*Jon Saad-Falcon,Avanika Narayan,Hakki Orhun Akengin,J. Wes Griffin,Herumb Shandilya,Adrian Gamarra Lafuente,Medhya Goel,Rebecca Joseph,Shlok Natarajan,Etash Kumar Guha,Shang Zhu,Ben Athiwaratkun,John Hennessy,Azalia Mirhoseini,Christopher Ré*

Main category: cs.DC

TL;DR: 本文研究了本地推理是否能有效减轻集中式基础设施的压力，并提出了IPW作为评估指标。结果显示本地推理具有潜力，且IPW是一个关键指标。


<details>
  <summary>Details</summary>
Motivation: 随着对大型语言模型（LLM）查询的需求迅速增长，集中式云基础设施面临挑战。小型LM和本地加速器的出现使得重新思考这一范式成为可能。

Method: 提出了一种名为每瓦智能（IPW）的度量标准，用于评估本地推理在模型-加速器对中的能力和效率。进行了大规模实证研究，涵盖了20多个最先进的本地LM、8个加速器和一个代表性的LLM流量子集。

Result: 本地LM可以准确回答88.7%的单轮聊天和推理查询，IPW在2023-2025年间提高了5.3倍，本地查询覆盖率从23.2%上升到71.3%。本地加速器的IPW至少比运行相同模型的云加速器低1.4倍。

Conclusion: 本地推理可以显著地将需求从集中式基础设施中重新分配，IPW作为跟踪这一转变的关键指标。

Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [80] [Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models](https://arxiv.org/abs/2511.07581)
*Supriti Vijay,Aman Priyanshu,Anu Vellore,Baturay Saglam,Amin Karbasi*

Main category: cs.AI

TL;DR: 本文介绍了Orion框架，它使紧凑模型能够通过学习的搜索策略进行迭代检索，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法捕捉复杂用户查询所需的迭代动态探索、反馈和修订。

Method: Orion框架结合了合成轨迹生成和监督微调以鼓励模型中的多样化探索模式，强化学习（RL）奖励有效的查询细化和回溯行为，以及推理时的束搜索算法，利用在RL期间学到的自我反思能力。

Result: 即使只使用了3%的训练数据，我们的1.2B模型在SciFact上取得了77.6%的成功率（比之前的检索器72.6%），在BRIGHT上取得了25.2%（比之前的22.1%），在NFCorpus上取得了63.2%（比之前的57.8%），并在FEVER、HotpotQA和MSMarco上保持竞争力。它在六个基准中的五个上优于检索器，其规模是它们的200-400倍。

Conclusion: 这些发现表明，当模型被训练为搜索、反思和修订时，检索性能可以来自学习的策略，而不仅仅是模型规模。

Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.

</details>


### [81] [Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces](https://arxiv.org/abs/2511.07587)
*Shreyas Rajesh,Pavan Holur,Chenda Duan,David Chong,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: GSW is a neuro-inspired generative memory framework that enhances LLMs' ability to reason over long contexts by building structured, interpretable representations of evolving situations, outperforming existing methods on the Episodic Memory Benchmark.


<details>
  <summary>Details</summary>
Motivation: LLMs face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events.

Method: GSW is a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. It comprises an Operator, which maps incoming observations to intermediate semantic structures, and a Reconciler, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence.

Result: On the Episodic Memory Benchmark (EpBench), GSW outperforms existing RAG based baselines by up to 20%. Furthermore, GSW is highly efficient, reducing query-time context tokens by 51% compared to the next most token-efficient baseline, reducing inference time costs considerably.

Conclusion: GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.

Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \textbf{20\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \textbf{51\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.

</details>


### [82] [ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents](https://arxiv.org/abs/2511.07685)
*Manasi Sharma,Chen Bo Calvin Zhang,Chaithanya Bandi,Clinton Wang,Ankit Aich,Huy Nghiem,Tahseen Rabbani,Ye Htet,Brian Jang,Sumana Basu,Aishwarya Balwani,Denis Peskoff,Marcos Ayestaran,Sean M. Hendryx,Brad Kenstler,Bing Liu*

Main category: cs.AI

TL;DR: 本文介绍了ResearchRubrics，一个用于评估深度研究的标准化基准，通过大量人工劳动构建，并提出了一个新的复杂性框架来分类DR任务。评估结果显示，即使是最先进的DR系统在遵循评分标准方面也存在不足。


<details>
  <summary>Details</summary>
Motivation: 评估DR仍然具有挑战性，因为回答冗长且多样，接受许多有效解决方案，并且通常依赖动态信息源。

Method: 我们引入了ResearchRubrics，这是一个标准化的DR基准，通过超过2,800小时的人工劳动构建，将现实的、领域多样的提示与2,500多个专家编写的细粒度评分标准配对，以评估事实基础、推理合理性以及清晰度。我们还提出了一种新的复杂性框架，用于沿三个轴分类DR任务：概念广度、逻辑嵌套和探索。此外，我们开发了人类和基于模型的评估协议，以测量DR代理的评分标准遵守情况。

Result: 我们评估了几种最先进的DR系统，发现即使领先的代理如Gemini的DR和OpenAI的DR，平均合规性也低于68%，主要是由于遗漏了隐含上下文和对检索信息的推理不足。

Conclusion: 我们的结果突显了对深度研究能力进行稳健、可扩展评估的必要性，为此，我们发布了ResearchRubrics（包括所有提示、评分标准和评估代码），以促进向有充分依据的研究助手的发展。

Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.

</details>


### [83] [SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder](https://arxiv.org/abs/2511.07896)
*Dengcan Liu,Jiahao Li,Zheren Fu,Yi Tu,Jiajun Li,Zhendong Mao,Yongdong Zhang*

Main category: cs.AI

TL;DR: SparseRM is a lightweight and interpretable reward model that uses Sparse Autoencoder to extract preference-relevant information from LLM representations, achieving superior performance with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Training reliable reward models (RMs) under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning large language models (LLMs).

Method: SparseRM leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. It decomposes LLM representations into interpretable directions that capture preference-relevant features, projects the representations onto these directions to compute alignment scores, and uses a simple reward head to aggregate these scores to predict preference scores.

Result: Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters.

Conclusion: SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters and integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.

Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.

</details>


### [84] [Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction](https://arxiv.org/abs/2511.07943)
*Jun Xu,Xinkai Du,Yu Ao,Peilong Zhao,Yang Li,Ling Zhong,Lin Yuan,Zhongpu Bo,Xiaorui Wang,Mengshu Sun,Zhengke Gui,Dalong Zhang,Zhaoyang Wang,Qiwei Wang,Yangyang Hou,Zhiying Yin,Haofen Wang,Huajun Chen,Lei Liang,Jun Zhou*

Main category: cs.AI

TL;DR: Thinker是一种分层思维模型，通过多轮交互进行深度搜索，使推理过程可监督和可验证。在少量训练样本下表现与现有基线相当，并在扩展到完整训练集时显著优于这些方法。


<details>
  <summary>Details</summary>
Motivation: 以往方法主要采用端到端强化学习，但忽视了对推理过程的监督，难以保证逻辑连贯性和严谨性。

Method: 提出Thinker，一种分层思维模型，通过多轮交互进行深度搜索，使推理过程可监督和可验证。将复杂问题分解为独立可解决的子问题，并以自然语言和等效逻辑函数双重表示，支持知识库和网页搜索。

Result: 实验结果表明，Thinker在少量训练样本下表现与现有基线相当，并在扩展到完整训练集时显著优于这些方法。

Conclusion: Thinker在少量训练样本下表现与现有基线相当，并在扩展到完整训练集时显著优于这些方法。

Abstract: Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.

</details>


### [85] [Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging](https://arxiv.org/abs/2511.08052)
*Po-Chung Hsieh,Chin-Po Chen,Jeng-Lin Li,Ming-Ching Chang*

Main category: cs.AI

TL;DR: 本文提出了一种基于心理学支持的Scaffold Reasoning框架，用于代码调试，该框架在推理准确性和效率方面表现出色，并与人类认知过程一致。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的LLM在各种基准测试中展示了复杂的解决问题能力，但识别平衡复杂性和计算效率的推理步骤的关键研究问题仍未解决。此外，对System 2推理的深入探索仍然不足。

Method: 我们提出了一个基于心理学支持的Scaffold Reasoning框架，包括Scaffold Stream、Analytic Stream和Integration Stream。

Result: 我们的框架在DebugBench上实现了88.91%的通过率和每题5.36秒的平均推理时间，优于其他推理方法在各种LLM中的推理准确性和效率。

Conclusion: 我们的研究结果证实了所提出的Scaffold Reasoning框架与人类认知过程的一致性。

Abstract: Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.

</details>


### [86] [Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression](https://arxiv.org/abs/2511.08066)
*Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 本文提出了一种新的度量标准——信息容量，用于评估大型语言模型（LLM）的推理效率。该度量标准基于文本压缩性能与计算复杂性的关系，能够公平比较不同模型系列的效率，并准确预测同一模型系列内的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个统一的指标来准确反映LLM在不同模型大小和架构下的效率，因此需要一种新的度量标准。

Method: 通过分析文本压缩性能与计算复杂性之间的关系，引入了信息容量作为度量标准，并评估了49个模型在5个异构数据集上的表现。

Result: 实验结果表明，同一系列中不同大小的模型表现出一致的信息容量，且信息容量能够有效反映分词器效率、预训练数据和混合专家架构的影响。

Conclusion: 信息容量是一种有效的度量标准，可以用于公平比较不同模型系列的效率，并准确预测同一模型系列内的性能。

Abstract: Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.

</details>


### [87] [SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning](https://arxiv.org/abs/2511.08151)
*Xuchen Li,Ruitao Wu,Xuanbo Liu,Xukai Wang,Jinbo Hu,Zhixin Bai,Bohan Zeng,Hao Liang,Leheng Chen,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Xu-Yao Zhang,Liu Liu,Jia Li,Kaiqi Huang,Jiahao Xu,Haitao Mi,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: SciAgent 是一个统一的多代理系统，能够在多个科学领域中表现出色，展示了AI系统在专家水平上进行跨学科推理的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型的进步使AI系统在特定领域任务上达到专家级性能，但这些系统仍然狭窄且手工制作。需要一种能够适应不同学科和难度水平的推理策略的通用科学推理系统。

Method: SciAgent 是一个统一的多代理系统，将问题解决组织为分层过程，由协调代理解释每个问题的领域和复杂性，并动态协调专门的工作者系统，每个系统由相互作用的推理子代理组成，用于符号演绎、概念建模、数值计算和验证。

Result: SciAgent 在数学和物理奥林匹克竞赛（IMO、IMC、IPhO、CPhO）中 consistently 达到或超越人类金牌获得者的表现，证明了其领域通用性和推理适应性。此外，在国际化学奥林匹克竞赛（IChO）和 Humanity's Last Exam（HLE）基准测试中的一些问题上也得到了验证。

Conclusion: SciAgent 是迈向通用科学智能的重要一步，展示了AI系统在专家水平上进行连贯的跨学科推理的能力。

Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.

</details>


### [88] [Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents](https://arxiv.org/abs/2511.08242)
*Waseem AlShikh,Muayad Sayed Ali,Brian Kennedy,Dmytro Mozolevskyi*

Main category: cs.AI

TL;DR: 本文提出了一种新的AI代理评估框架，涵盖11个基于结果的性能指标，通过实验验证了其有效性，并展示了混合代理的优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前的基础设施指标（如延迟、首次标记时间或标记吞吐量）不足以评估AI代理的性能，因为它们无法捕捉代理决策的质量、操作自主性或最终业务价值。因此需要一种更全面的评估方法。

Method: 本文提出了一种基于结果的、任务无关的性能指标框架，并通过大规模模拟实验验证了其有效性。实验涉及四种不同的代理架构和五个不同的领域。

Result: 实验结果揭示了不同代理设计之间的显著性能权衡，表明混合代理在大多数提出的指标上表现最佳，平均目标完成率为88.8%，并实现了最高的投资回报率（ROI）。

Conclusion: 本文提出了一个全面的评估框架，旨在帮助组织根据AI代理的决策质量、自主性、适应性和实际业务价值来评估其性能。实验结果表明，混合代理在大多数指标上表现最佳。

Abstract: As AI agents proliferate across industries and applications, evaluating their performance based solely on infrastructural metrics such as latency, time-to-first-token, or token throughput is proving insufficient. These metrics fail to capture the quality of an agent's decisions, its operational autonomy, or its ultimate business value. This white paper proposes a novel, comprehensive framework of eleven outcome-based, task-agnostic performance metrics for AI agents that transcend domain boundaries. These metrics are designed to enable organizations to evaluate agents based on the quality of their decisions, their degree of autonomy, their adaptability to new challenges, and the tangible business value they deliver, regardless of the underlying model architecture or specific use case. We introduce metrics such as Goal Completion Rate (GCR), Autonomy Index (AIx), Multi-Step Task Resilience (MTR), and Business Impact Efficiency (BIE). Through a large-scale simulated experiment involving four distinct agent architectures (ReAct, Chain-of-Thought, Tool-Augmented, Hybrid) across five diverse domains (Healthcare, Finance, Marketing, Legal, and Customer Service), we demonstrate the framework's efficacy. Our results reveal significant performance trade-offs between different agent designs, highlighting the Hybrid Agent as the most consistently high-performing model across the majority of our proposed metrics, achieving an average Goal Completion Rate of 88.8\% and the highest Return on Investment (ROI). This work provides a robust, standardized methodology for the holistic evaluation of AI agents, paving the way for more effective development, deployment, and governance.

</details>


### [89] [Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs](https://arxiv.org/abs/2511.08274)
*Anton Gusarov,Anastasia Volkova,Valentin Khrulkov,Andrey Kuznetsov,Evgenii Maslov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体GraphRAG系统，利用Cypher查询实现自然语言到LPG图数据的接口，展示了其在多个领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的GraphRAG研究主要集中在RDF知识图谱上，而Cypher和LPG数据库在GraphRAG管道中的潜力尚未被充分探索。因此，本文旨在填补这一研究空白。

Method: 本文提出了一种基于LLM的多智能体系统，用于自动生成和执行Cypher查询。该系统使用Memgraph作为图数据库后端，并通过迭代内容感知校正和规范化来确保生成查询的语义和语法优化。

Result: 本文在CypherBench图数据集上评估了系统，结果表明其在多个领域和查询类型中表现出色。此外，还展示了该工作流在IFC数据（代表建筑的数字孪生）上的性能。

Conclusion: 本文提出了一种多智能体GraphRAG系统，该系统能够将自然语言转换为Cypher查询，从而为基于LPG的图数据提供接口。实验表明，该方法在多个领域和查询类型上表现良好，并展示了其在工业数字自动化用例中的潜力。

Abstract: While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [90] [Hybrid Quantum-Classical Selective State Space Artificial Intelligence](https://arxiv.org/abs/2511.08349)
*Amin Ebrahimi,Farzan Haddadi*

Main category: quant-ph

TL;DR: 本文提出了一种混合量子经典的选择机制，用于时间序列分类任务，并展示了其在NLP模型中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习架构在处理大规模矩阵乘法和高维优化时存在计算瓶颈，而量子电路能够提供更高效的表示学习。

Method: 提出了一种混合量子经典的选择机制，将变分量子电路作为量子门模块，用于时间序列分类问题。

Result: 在重塑的MNIST数据集上，混合模型在前四个epoch中达到了24.6%的准确率，比纯经典选择机制的21.6%更高。

Conclusion: 量子增强的门控机制为构建可扩展、资源高效的NLP模型提供了一条路径。

Abstract: Hybrid Quantum Classical (HQC) algorithms constitute one of the most effective paradigms for exploiting the computational advantages of quantum systems in large-scale numerical tasks. By operating in high-dimensional Hilbert spaces, quantum circuits enable exponential speed-ups and provide access to richer representations of cost landscapes compared to purely classical methods. These capabilities are particularly relevant for machine learning, where state-of-the-art models especially in Natural Language Processing (NLP) suffer from prohibitive time complexity due to massive matrix multiplications and high-dimensional optimization.
  In this manuscript, we propose a Hybrid Quantum Classical selection mechanism for the Mamba architecture, designed specifically for temporal sequence classification problems. Our approach leverages Variational Quantum Circuits (VQCs) as quantum gating modules that both enhance feature extraction and improve suppression of irrelevant information. This integration directly addresses the computational bottlenecks of deep learning architectures by exploiting quantum resources for more efficient representation learning.
  We analyze how introducing quantum subroutines into large language models (LLMs) impacts their generalization capability, expressivity, and parameter efficiency. The results highlight the potential of quantum-enhanced gating mechanisms as a path toward scalable, resource-efficient NLP models, in a limited simulation step. Within the first four epochs on a reshaped MNIST dataset with input format (batch, 784, d_model), our hybrid model achieved 24.6% accuracy while using one quantum layer and achieve higher expressivity, compared to 21.6% obtained by a purely classical selection mechanism. we state No founding

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [91] [A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain](https://arxiv.org/abs/2511.07577)
*Yining Lu,Wenyi Tang,Max Johnson,Taeho Jung,Meng Jiang*

Main category: cs.CR

TL;DR: 本文提出了一种去中心化的RAG系统，通过区块链技术实现可靠性的动态评估和管理，从而提高检索准确性和响应质量，同时降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）系统通常使用集中式架构，导致数据收集、集成和管理的成本很高，以及隐私问题。需要一种去中心化的RAG系统，使基础模型能够直接从数据所有者那里利用信息，数据所有者保持对其来源的完全控制。然而，去中心化带来了挑战：众多独立的数据源在可靠性方面差异很大，这可能会降低检索准确性和响应质量。

Method: 我们提出了一个去中心化的RAG系统，具有新颖的可靠性评分机制，该机制动态评估每个来源基于其贡献的响应质量，并在检索时优先考虑高质量的来源。为了确保透明度和信任，评分过程通过基于区块链的智能合约安全地管理，创建了可验证且防篡改的可靠性记录，而无需依赖中央权威。

Result: 我们在两个模拟环境中使用两种Llama模型（3B和8B）对去中心化系统进行了评估，其中六个数据源具有不同级别的可靠性。我们的系统在现实世界类似不可靠数据环境中比其集中式系统实现了+10.7%的性能提升。值得注意的是，在理想可靠的数据库环境中，它接近集中式系统的上限性能。去中心化基础设施实现了安全和可信的评分管理，通过批量更新操作实现了大约56%的边际成本节约。

Conclusion: 我们的去中心化系统在现实世界中不稳定的数据库环境中相比其集中式系统实现了+10.7%的性能提升。值得注意的是，在理想可靠的数据库环境中，它接近集中式系统的上限性能。去中心化基础设施实现了安全和可信的评分管理，通过批量更新操作实现了大约56%的边际成本节约。

Abstract: Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.

</details>


### [92] [SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought](https://arxiv.org/abs/2511.07772)
*Shourya Batra,Pierce Tillman,Samarth Gaggar,Shashank Kesineni,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\%$ reduction in CPL on QwQ-32B, $17.9\%$ reduction in CPL on Llama-3.1-8B, and $31.2\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.

</details>


### [93] [LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation](https://arxiv.org/abs/2511.07876)
*Xingyu Li,Xiaolei Liu,Cheng Liu,Yixiao Xu,Kangyi Ding,Bangzhou Xin,Jia-Li Yin*

Main category: cs.CR

TL;DR: LoopLLM是一种新的能量-延迟攻击框架，通过诱导重复生成和优化梯度聚合，显著提升了攻击效果和跨模型可转移性。


<details>
  <summary>Details</summary>
Motivation: 现有的攻击方法在输出变长时难以通过输入控制终止符号，导致效果下降。因此，需要一种更有效的能量-延迟攻击框架。

Method: LoopLLM基于观察到重复生成可以触发低熵解码循环，提出了两种优化方法：(1) 利用自回归漏洞诱导重复生成的重复诱导提示优化，以及(2) 通过梯度聚合提高跨模型可转移性的对齐令牌集成优化。

Result: 在12个开源和2个商业LLM上的实验表明，LoopLLM显著优于现有方法，达到了超过90%的最大输出长度，而基线仅为20%，并且提高了DeepSeek-V3和Gemini 2.5 Flash的可转移性约40%。

Conclusion: LoopLLM显著优于现有方法，能够达到最大输出长度的90%以上，并提高了跨模型的可转移性。

Abstract: As large language models (LLMs) scale, their inference incurs substantial computational resources, exposing them to energy-latency attacks, where crafted prompts induce high energy and latency cost. Existing attack methods aim to prolong output by delaying the generation of termination symbols. However, as the output grows longer, controlling the termination symbols through input becomes difficult, making these methods less effective. Therefore, we propose LoopLLM, an energy-latency attack framework based on the observation that repetitive generation can trigger low-entropy decoding loops, reliably compelling LLMs to generate until their output limits. LoopLLM introduces (1) a repetition-inducing prompt optimization that exploits autoregressive vulnerabilities to induce repetitive generation, and (2) a token-aligned ensemble optimization that aggregates gradients to improve cross-model transferability. Extensive experiments on 12 open-source and 2 commercial LLMs show that LoopLLM significantly outperforms existing methods, achieving over 90% of the maximum output length, compared to 20% for baselines, and improving transferability by around 40% to DeepSeek-V3 and Gemini 2.5 Flash.

</details>

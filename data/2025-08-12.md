<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 96]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.CV](#cs.CV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)
*Juliana Resplande Sant'anna Gomes,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 本文旨在解决葡萄牙语环境中缺乏整合外部证据的数据集的问题，通过开发一种方法来丰富葡萄牙新闻语料库，并引入数据验证和预处理框架，以提高基础语料库的质量。


<details>
  <summary>Details</summary>
Motivation: 由于虚假信息的快速传播常常超过人工核实的能力，因此迫切需要半自动事实核查（SAFC）系统。在葡萄牙语环境下，缺乏公开可用的整合外部证据的数据集，这对于开发稳健的AFC系统至关重要，因为许多现有资源仅基于文本特征进行分类。

Method: 本文的方法模拟了用户的验证过程，使用大型语言模型（LLMs，特别是Gemini 1.5 Flash）从文本中提取主要声明，并利用搜索引擎API（Google Search API，Google FactCheck Claims Search API）检索相关的外部文档（证据）。此外，还引入了一个数据验证和预处理框架，包括近似重复检测，以提高基础语料库的质量。

Result: 本文通过开发、应用和分析一种方法来丰富葡萄牙新闻语料库，并引入数据验证和预处理框架，以提高基础语料库的质量，从而解决了葡萄牙语环境中缺乏公开可用的整合外部证据的数据集的问题。

Conclusion: 本文通过开发、应用和分析一种方法来丰富葡萄牙新闻语料库，并引入数据验证和预处理框架，以提高基础语料库的质量，从而解决了葡萄牙语环境中缺乏公开可用的整合外部证据的数据集的问题。

Abstract: The accelerated dissemination of disinformation often outpaces the capacity
for manual fact-checking, highlighting the urgent need for Semi-Automated
Fact-Checking (SAFC) systems. Within the Portuguese language context, there is
a noted scarcity of publicly available datasets that integrate external
evidence, an essential component for developing robust AFC systems, as many
existing resources focus solely on classification based on intrinsic text
features. This dissertation addresses this gap by developing, applying, and
analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,
MuMiN-PT) with external evidence. The approach simulates a user's verification
process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)
to extract the main claim from texts and search engine APIs (Google Search API,
Google FactCheck Claims Search API) to retrieve relevant external documents
(evidence). Additionally, a data validation and preprocessing framework,
including near-duplicate detection, is introduced to enhance the quality of the
base corpora.

</details>


### [2] [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)
*Yao Ge,Sudeshna Das,Yuting Guo,Abeed Sarker*

Main category: cs.CL

TL;DR: 本文研究了通过检索增强生成（RAG）的动态提示策略，以提高大型语言模型在少量样本生物医学命名实体识别中的性能。结果显示，动态提示方法优于静态提示方法，特别是在使用TF-IDF和SBERT检索方法时效果最佳。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别（NER）是一项高实用性的自然语言处理（NLP）任务，大型语言模型（LLMs）在少量样本设置中显示出前景。然而，LLMs在少量样本生物医学NER中的性能仍然面临挑战，因此需要改进方法。

Method: 我们研究了一种涉及检索增强生成（RAG）的动态提示策略，以解决LLMs在少量样本生物医学NER中的性能挑战。我们的方法基于输入文本与上下文学习示例的相似性选择标注的示例，并在推理过程中为每个实例动态更新提示。

Result: 静态提示与结构化组件相结合，相对于基本静态提示，GPT-4的平均F1分数提高了12%，GPT-3.5和LLaMA 3-70B分别提高了11%。动态提示进一步提高了性能，TF-IDF和SBERT检索方法取得了最佳结果，在5-shot和10-shot设置中，平均F1分数分别提高了7.3%和5.6%。

Conclusion: 这些发现突显了通过RAG进行上下文自适应提示在生物医学NER中的效用。

Abstract: Biomedical named entity recognition (NER) is a high-utility natural language
processing (NLP) task, and large language models (LLMs) show promise
particularly in few-shot settings (i.e., limited training data). In this
article, we address the performance challenges of LLMs for few-shot biomedical
NER by investigating a dynamic prompting strategy involving retrieval-augmented
generation (RAG). In our approach, the annotated in-context learning examples
are selected based on their similarities with the input texts, and the prompt
is dynamically updated for each instance during inference. We implemented and
optimized static and dynamic prompt engineering techniques and evaluated them
on five biomedical NER datasets. Static prompting with structured components
increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA
3-70B, relative to basic static prompting. Dynamic prompting further improved
performance, with TF-IDF and SBERT retrieval methods yielding the best results,
improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,
respectively. These findings highlight the utility of contextually adaptive
prompts via RAG for biomedical NER.

</details>


### [3] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
*Lei Jiang,Fan Chen*

Main category: cs.CL

TL;DR: 本文提出了CarbonScaling框架，以量化模型准确率与碳足迹之间的关系，并提供了训练更可持续和碳高效的大型语言模型的关键见解。


<details>
  <summary>Details</summary>
Motivation: 现有的神经缩放定律忽略了随着LLM规模增长而指数级增加的碳排放问题。

Method: 本文通过整合神经缩放模型、GPU硬件演变、并行优化和碳估算模型，提出了CarbonScaling框架。

Result: 结果表明，虽然准确率和碳排放之间存在幂律关系，但现实中的低效率显著增加了缩放因子。对于小型到中型模型，硬件技术的发展可以减少碳排放，但对于极大规模的LLM，由于通信开销和未充分利用的GPU，其收益递减。

Conclusion: 本文提出了CarbonScaling框架，以量化模型准确率与碳足迹之间的关系，并提供了训练更可持续和碳高效的大型语言模型的关键见解。

Abstract: Neural scaling laws have driven the development of increasingly large
language models (LLMs) by linking accuracy improvements to growth in parameter
count, dataset size, and compute. However, these laws overlook the carbon
emissions that scale exponentially with LLM size. This paper presents
\textit{CarbonScaling}, an analytical framework that extends neural scaling
laws to incorporate both operational and embodied carbon in LLM training. By
integrating models for neural scaling, GPU hardware evolution, parallelism
optimization, and carbon estimation, \textit{CarbonScaling} quantitatively
connects model accuracy to carbon footprint. Results show that while a
power-law relationship between accuracy and carbon holds, real-world
inefficiencies significantly increase the scaling factor. Hardware technology
scaling reduces carbon emissions for small to mid-sized models, but offers
diminishing returns for extremely large LLMs due to communication overhead and
underutilized GPUs. Training optimizations-especially aggressive critical batch
size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers
key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [4] [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)
*Aamod Thakur,Ajay Nagpal,Atharva Savarkar,Kundeshwar Pundalik,Siddhesh Dosi,Piyush Sawarkar,Viraj Thakur,Rohit Saluja,Maunendra Sankar Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文研究了分词在多语言大型语言模型中的重要性，并提出了一种新的分词方法，有效提高了模型性能和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的分词器在多语言上下文中存在高词到标记比率、低效使用上下文长度和较慢的推理问题。因此，需要对分词进行更深入的研究，以提高多语言大型语言模型的效率和性能。

Method: 本文进行了系统研究，将词汇表大小、预分词规则和训练语料库组成与分词效率和模型质量联系起来。基于对印地语脚本的深入实验，提出了一种新的数据组合算法，以平衡多语言数据用于分词器训练。

Result: 本文提出的分词方法显著提高了模型性能，并将平均词到标记比率降低了约6%。与最先进的多语言印地语模型相比，该分词器在平均词到标记比率上实现了超过40%的改进。

Conclusion: 本文强调了分词在构建高效、可扩展的多语言大型语言模型中的重要性，并展示了所提出的分词方法在提高模型性能和推理速度方面的有效性。

Abstract: While model architecture and training objectives are well-studied,
tokenization, particularly in multilingual contexts, remains a relatively
neglected aspect of Large Language Model (LLM) development. Existing tokenizers
often exhibit high token-to-word ratios, inefficient use of context length, and
slower inference. We present a systematic study that links vocabulary size,
pre-tokenization rules, and training-corpus composition to both token-to-word
efficiency and model quality. To ground our analysis in a linguistically
diverse context, we conduct extensive experiments on Indic scripts, which
present unique challenges due to their high script diversity and orthographic
complexity. Drawing on the insights from these analyses, we propose a novel
algorithm for data composition that balances multilingual data for tokenizer
training. Our observations on pretokenization strategies significantly improve
model performance, and our data composition algorithm reduces the average
token-to-word ratio by approximately 6% with respect to the conventional data
randomization approach. Our tokenizer achieves more than 40% improvement on
average token-to-word ratio against stateof-the-art multilingual Indic models.
This improvement yields measurable gains in both model performance and
inference speed. This highlights tokenization alongside architecture and
training objectives as a critical lever for building efficient, scalable
multilingual LLMs

</details>


### [5] [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)
*Zhanye Luo,Yuefeng Han,Xiufan Yu*

Main category: cs.CL

TL;DR: AEALT是一种监督的、因子增强的框架，通过将维度缩减直接集成到预训练语言模型的工作流中，提高了下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的高维嵌入在下游任务中效率低下且计算成本高，因此需要一种有效的维度缩减方法。

Method: AEALT是一种监督的、因子增强的框架，将维度缩减直接集成到预训练的语言模型工作流中。它首先从文本文档中提取嵌入，然后通过一个监督增强的自编码器学习低维、任务相关的潜在因子。

Result: AEALT在分类、异常检测和预测任务中表现出色，优于原始嵌入和多种标准的维度缩减方法。

Conclusion: AEALT能够显著提升基于低维嵌入的下游任务性能，并在多个真实世界数据集上验证了其广泛适用性。

Abstract: Large language models (LLMs) generate text embeddings from text data,
producing vector representations that capture the semantic meaning and
contextual relationships of words. However, the high dimensionality of these
embeddings often impedes efficiency and drives up computational cost in
downstream tasks. To address this, we propose AutoEncoder-Augmented Learning
with Text (AEALT), a supervised, factor-augmented framework that incorporates
dimension reduction directly into pre-trained LLM workflows. First, we extract
embeddings from text documents; next, we pass them through a supervised
augmented autoencoder to learn low-dimensional, task-relevant latent factors.
By modeling the nonlinear structure of complex embeddings, AEALT outperforms
conventional deep-learning approaches that rely on raw embeddings. We validate
its broad applicability with extensive experiments on classification, anomaly
detection, and prediction tasks using multiple real-world public datasets.
Numerical results demonstrate that AEALT yields substantial gains over both
vanilla embeddings and several standard dimension reduction methods.

</details>


### [6] [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
*Ying Liu,Can Li,Ting Zhang,Mei Wang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型在教育对话中的自适应指导能力，并提出了一种新的基准和微调策略，以提升其作为导师的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大语言模型的苏格拉底式提问能力，但忽略了根据学习者的认知状态进行自适应指导的关键维度。

Method: 本研究提出了GuideEval基准，基于真实的教育对话，通过三个阶段的行为框架（感知、协调和激发）来评估教学指导能力，并引入了一种行为引导的微调策略。

Result: 实证结果表明，现有的大语言模型在学习者表现出困惑或需要重新引导时，往往无法提供有效的自适应支架。然而，通过行为引导的微调策略显著提升了指导性能。

Conclusion: 本研究倡导一种以学习者为中心的对话范式来评估苏格拉底式大语言模型，强调了在教育对话中适应性指导的重要性，并通过行为引导的微调策略提高了指导性能。

Abstract: The conversational capabilities of large language models hold significant
promise for enabling scalable and interactive tutoring. While prior research
has primarily examined their capacity for Socratic questioning, it often
overlooks a critical dimension: adaptively guiding learners based on their
cognitive states. This study shifts focus from mere question generation to the
broader instructional guidance capability. We ask: Can LLMs emulate expert
tutors who dynamically adjust strategies in response to learners'
understanding? To investigate this, we propose GuideEval, a benchmark grounded
in authentic educational dialogues that evaluates pedagogical guidance through
a three-phase behavioral framework: (1) Perception, inferring learner states;
(2) Orchestration, adapting instructional strategies; and (3) Elicitation,
stimulating proper reflections. Empirical findings reveal that existing LLMs
frequently fail to provide effective adaptive scaffolding when learners exhibit
confusion or require redirection. Furthermore, we introduce a behavior-guided
finetuning strategy that leverages behavior-prompted instructional dialogues,
significantly enhancing guidance performance. By shifting the focus from
isolated content evaluation to learner-centered interaction, our work advocates
a more dialogic paradigm for evaluating Socratic LLMs.

</details>


### [7] [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
*Xiaoyuan Zhu,Muru Zhang,Ollie Liu,Robin Jia,Willie Neiswanger*

Main category: cs.CL

TL;DR: 本文提出了一种使用语言模型自动生成高质量遗忘集的方法，实验表明该方法在多个领域表现优异，具有实际应用前景。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型通常包含敏感、有害或受版权保护的知识，这需要后期遗忘——即在不完全重新训练的情况下从模型中删除特定领域知识的能力。当前遗忘流程中的主要瓶颈是构建有效的遗忘集——近似目标领域并引导模型遗忘它的数据集。

Method: 我们提出了一种使用语言模型自身生成高质量遗忘集的可扩展、自动的方法。我们的方法通过结构化提示管道合成教科书风格的数据，只需要一个领域名称作为输入。

Result: 通过在生物安全、网络安全和哈利·波特小说上的实验，我们证明了我们的合成数据集始终优于基线合成替代品，并且与专家策划的数据集相当。此外，消融研究表明，多步骤生成流程显著提高了数据多样性，从而提高了遗忘效用。

Conclusion: 我们的研究结果表明，合成数据集为在不需要人工干预的情况下实现广泛新兴领域的实用、可扩展的遗忘提供了有前景的路径。

Abstract: Modern large language models often encode sensitive, harmful, or copyrighted
knowledge, raising the need for post-hoc unlearning-the ability to remove
specific domains of knowledge from a model without full retraining. A major
bottleneck in current unlearning pipelines is constructing effective forget
sets-datasets that approximate the target domain and guide the model to forget
it. In this work, we introduce a scalable, automated approach to generate
high-quality forget sets using language models themselves. Our method
synthesizes textbook-style data through a structured prompting pipeline,
requiring only a domain name as input. Through experiments on unlearning
biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic
datasets consistently outperform the baseline synthetic alternatives and are
comparable to the expert-curated ones. Additionally, ablation studies reveal
that the multi-step generation pipeline significantly boosts data diversity,
which in turn improves unlearning utility. Overall, our findings suggest that
synthetic datasets offer a promising path toward practical, scalable unlearning
for a wide range of emerging domains without the need for manual intervention.
We release our code and dataset at
https://github.com/xyzhu123/Synthetic_Textbook.

</details>


### [8] [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)
*Zijian Chen,Xueguang Ma,Shengyao Zhuang,Ping Nie,Kai Zou,Andrew Liu,Joshua Green,Kshama Patel,Ruoxi Meng,Mingyi Su,Sahel Sharifymoghaddam,Yanxi Li,Haoran Hong,Xinyu Shi,Xuye Liu,Nandan Thakur,Crystina Zhang,Luyu Gao,Wenhu Chen,Jimmy Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新的基准BrowseComp-Plus，用于更公平、透明地评估深度研究代理和检索方法。


<details>
  <summary>Details</summary>
Motivation: 当前的评估存在公平性和透明性问题，因为动态且不透明的网络API阻碍了深度研究方法的公平比较和可重复性。此外，缺乏对文档语料库的控制使得难以隔离检索器的贡献。

Method: 本文引入了BrowseComp-Plus基准，该基准基于BrowseComp，采用固定且精心策划的文档语料库，并包含人工验证的支持文档和挖掘出的具有挑战性的负样本。

Result: 在BrowseComp-Plus基准上，开源模型Search-R1与BM25检索器结合时达到3.86%的准确率，而GPT-5达到55.9%。将GPT-5与Qwen3-Embedding-8B检索器结合后，准确率进一步提高到70.1%。

Conclusion: 本文介绍了BrowseComp-Plus基准，它能够有效地区分深度研究系统的性能，并允许对检索方法进行全面评估和解耦分析。

Abstract: Deep-Research agents, which integrate large language models (LLMs) with
search tools, have shown success in improving the effectiveness of handling
complex queries that require iterative search planning and reasoning over
search results. Evaluations on current benchmarks like BrowseComp relies on
black-box live web search APIs, have notable limitations in (1) fairness:
dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep
research methods; (2) transparency: lack of control over the document corpus
makes it difficult to isolate retriever contributions. In other words, the
current evaluations may compare a complete deep research system at a given
time, but they do not foster well-controlled experiments to provide insights
into the capability of underlying deep research LLMs. To address these
challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,
employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus
includes human-verified supporting documents and mined challenging negatives,
enabling controlled experimentation. The benchmark is shown to be effective in
distinguishing the performance of deep research systems. For instance, the
open-source model Search-R1, when paired with the BM25 retriever, achieves
3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with
the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with
fewer search calls. This benchmark allows comprehensive evaluation and
disentangled analysis of deep research agents and retrieval methods, fostering
insights into retrieval effectiveness, citation accuracy, and context
engineering in Deep-Research system.

</details>


### [9] [Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models](https://arxiv.org/abs/2508.06621)
*Tomohiro Sawada,Kartik Goyal*

Main category: cs.CL

TL;DR: 本文探讨了不依赖BPE合并列表的推理算法对语言模型性能的影响，发现非针对性方法对性能影响较小，这为更隐私保护的分词方案提供了可能性。


<details>
  <summary>Details</summary>
Motivation: 最近的工作表明，这个合并列表暴露了一个潜在的攻击面，用于提取关于语言模型训练数据的信息。本文探讨了不依赖此合并列表的BPE推理算法的下游影响，因此与BPE训练期间的编码过程不同。

Method: 我们研究了两种广泛的BPE推理方案，它们与训练期间的BPE应用不同：a) 针对性偏离合并列表，包括随机合并顺序以及涉及删除/截断的合并列表的各种损坏；b) 非针对性的无合并列表的BPE推理算法，它们不依赖于合并列表，而是专注于以贪心或精确的方式压缩文本。

Result: 实验结果表明，针对性偏离合并列表会导致语言模型性能显著下降，而非针对性的无合并列表推理算法对下游性能的影响很小，通常远小于预期。

Conclusion: 这些发现为更简单且可能更隐私保护的分词方案铺平了道路，这些方案不会灾难性地损害模型性能。

Abstract: Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a
learned token vocabulary with a detailed merge list. Recent work has shown that
this merge list exposes a potential attack surface for extracting information
about language model's training data. In this paper, we explore the downstream
impact of BPE inference algorithms that do not rely on this merge list at all,
and hence differ from the encoding process during BPE training. To address this
question, we investigate two broad classes of BPE inference schemes that differ
from BPE application during training: a) targeted deviation from merge-lists
including random merge orders, and various corruptions of merge list involving
deletion/truncation, and b) non-targeted BPE inference algorithms that do not
depend on the merge list but focus on compressing the text either greedily or
exactly. Extensive experiments across diverse language modeling tasks like
accuracy-based QA benchmarks, machine translation, and open-ended generation
reveal that while targeted deviation from the merge lists exhibits significant
degradation in language model performance, the non-targeted merge-list-free
inference algorithms result in minimal impact on downstream performance that is
often much smaller than expected. These findings pave way for simpler and
potentially more privacy-preserving tokenization schemes that do not
catastrophically compromise model performance.

</details>


### [10] [Measuring Stereotype and Deviation Biases in Large Language Models](https://arxiv.org/abs/2508.06649)
*Daniel Wang,Eli Brignac,Minjia Mao,Xiao Fang*

Main category: cs.CL

TL;DR: 本研究调查了大型语言模型（LLMs）可能表现出的刻板印象偏差和偏离偏差，并发现所有检查的LLM都对多个群体表现出这些偏差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）被广泛应用于各个领域，引发了对其局限性和潜在风险的关注。本研究旨在调查LLMs可能表现出的两种偏见：刻板印象偏差和偏离偏差。

Method: 通过让四个先进的LLM生成个人资料，我们研究了每个人口统计群体与政治倾向、宗教和性取向等属性之间的关联。

Result: 实验结果表明，所有检查的LLM都对多个群体表现出显著的刻板印象偏差和偏离偏差。

Conclusion: 研究发现，所有检查的LLM都对多个群体表现出显著的刻板印象偏差和偏离偏差。这些发现揭示了当LLM推断用户属性时出现的偏见，并阐明了LLM生成输出的潜在危害。

Abstract: Large language models (LLMs) are widely applied across diverse domains,
raising concerns about their limitations and potential risks. In this study, we
investigate two types of bias that LLMs may display: stereotype bias and
deviation bias. Stereotype bias refers to when LLMs consistently associate
specific traits with a particular demographic group. Deviation bias reflects
the disparity between the demographic distributions extracted from
LLM-generated content and real-world demographic distributions. By asking four
advanced LLMs to generate profiles of individuals, we examine the associations
between each demographic group and attributes such as political affiliation,
religion, and sexual orientation. Our experimental results show that all
examined LLMs exhibit both significant stereotype bias and deviation bias
towards multiple groups. Our findings uncover the biases that occur when LLMs
infer user attributes and shed light on the potential harms of LLM-generated
outputs.

</details>


### [11] [Testing the Limits of Machine Translation from One Book](https://arxiv.org/abs/2508.06665)
*Jonathan Shaw,Dillon Mee,Timothy Khouw,Zackary Leech,Daniel Wilson*

Main category: cs.CL

TL;DR: 本研究评估了不同语言资源对LLM翻译Kanuri语言的影响，发现并行句子是最有效的数据源，而语法单独使用效果不佳。


<details>
  <summary>Details</summary>
Motivation: 我们关注Kanuri语言，尽管有大量使用者，但数字资源极少。我们旨在研究不同语言资源对LLM翻译质量的影响。

Method: 我们设计了两个数据集进行评估：一个专注于健康和人道主义术语，另一个包含一般术语，研究领域特定任务如何影响LLM翻译质量。通过提供不同的语言资源组合（语法、词典和并行句子），我们测量LLM翻译效果，并将其与母语者翻译和人类语言学家的表现进行比较。

Result: 结果表明，并行句子仍然是最有效的数据源，在人类评估和自动指标中都优于其他方法。虽然语法的引入提高了零次学习翻译的效果，但它作为独立的数据源并不有效。人类评估显示，LLM在准确性（意义）方面比流畅性（语法性）更有效。

Conclusion: 这些发现表明，LLM翻译评估需要超越简单准确度指标的多维评估，并且仅靠语法不足以提供有效领域特定翻译的上下文。

Abstract: Current state-of-the-art models demonstrate capacity to leverage in-context
learning to translate into previously unseen language contexts. Tanzer et al.
[2024] utilize language materials (e.g. a grammar) to improve translation
quality for Kalamang using large language models (LLMs). We focus on Kanuri, a
language that, despite having substantial speaker population, has minimal
digital resources. We design two datasets for evaluation: one focused on health
and humanitarian terms, and another containing generalized terminology,
investigating how domain-specific tasks impact LLM translation quality.
  By providing different combinations of language resources (grammar,
dictionary, and parallel sentences), we measure LLM translation effectiveness,
comparing results to native speaker translations and human linguist
performance. We evaluate using both automatic metrics and native speaker
assessments of fluency and accuracy.
  Results demonstrate that parallel sentences remain the most effective data
source, outperforming other methods in human evaluations and automatic metrics.
While incorporating grammar improves over zero-shot translation, it fails as an
effective standalone data source. Human evaluations reveal that LLMs achieve
accuracy (meaning) more effectively than fluency (grammaticality).
  These findings suggest LLM translation evaluation benefits from
multidimensional assessment beyond simple accuracy metrics, and that grammar
alone, without parallel sentences, does not provide sufficient context for
effective domain-specific translation.

</details>


### [12] [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)
*Swati Rajwal,Shivank Garg,Reem Abdel-Salam,Abdelrahman Zayed*

Main category: cs.CL

TL;DR: 该研究探讨了链式思维提示对语言模型公平性的影响，发现模型的思考步骤中的偏见与输出偏见之间没有高度相关性。


<details>
  <summary>Details</summary>
Motivation: 由于语言模型中存在基于性别、种族、社会经济地位、身体外貌和性取向的偏见，这使得语言模型的部署变得具有挑战性。因此，需要研究链式思维提示对公平性的影响。

Method: 该研究使用公平性度量来量化模型的思考和输出中的11种不同偏见，通过实验分析链式思维提示对公平性的影响。

Result: 研究结果表明，模型的思考步骤中的偏见与输出偏见之间的相关性不高（大多数情况下相关性小于0.6，p值小于0.001）。

Conclusion: 研究结果表明，有偏见的模型在思考步骤中的偏见与输出偏见之间的相关性不高，这意味着具有有偏决策的模型并不总是拥有有偏的思考。

Abstract: The impressive performance of language models is undeniable. However, the
presence of biases based on gender, race, socio-economic status, physical
appearance, and sexual orientation makes the deployment of language models
challenging. This paper studies the effect of chain-of-thought prompting, a
recent approach that studies the steps followed by the model before it
responds, on fairness. More specifically, we ask the following question:
\textit{Do biased models have biased thoughts}? To answer our question, we
conduct experiments on $5$ popular large language models using fairness metrics
to quantify $11$ different biases in the model's thoughts and output. Our
results show that the bias in the thinking steps is not highly correlated with
the output bias (less than $0.6$ correlation with a $p$-value smaller than
$0.001$ in most cases). In other words, unlike human beings, the tested models
with biased decisions do not always possess biased thoughts.

</details>


### [13] [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)
*Evangelia Spiliopoulou,Riccardo Fogliato,Hanna Burnsky,Tamer Soliman,Jie Ma,Graham Horwood,Miguel Ballesteros*

Main category: cs.CL

TL;DR: 本文提出了一种统计框架来识别和量化大型语言模型在评估自己和其他模型输出时的自我偏见和家族偏见，并通过实证分析验证了这一方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究常常将模型质量的真实差异与偏见混淆，或错误地假设LLM和人类的评估遵循相同的评分分布。因此，我们需要一种更准确的方法来识别和量化自我偏见。

Method: 我们提出了一种统计框架，用于明确形式化识别和估计自我偏见的假设。该方法建模了LLM作为评估者对其自身完成与其它模型的评分分布差异，同时考虑了由独立第三方评估者（例如人类）提供的完成质量的潜在因素。

Result: 我们在一个包含超过5000个提示-完成对的数据集上进行了实证分析，发现像GPT-4o和Claude 3.5 Sonnet这样的模型会系统性地为其自身输出分配更高的分数，并且还表现出家族偏见。

Conclusion: 我们的研究揭示了使用LLM作为评估者时可能存在的自我偏见和家族偏见，并提供了减轻这些偏差的实用指导。

Abstract: Large language models (LLMs) can serve as judges that offer rapid and
reliable assessments of other LLM outputs. However, models may systematically
assign overly favorable ratings to their own outputs, a phenomenon known as
self-bias, which can distort evaluations of true model performance. Previous
studies often conflate genuine differences in model quality with bias or
incorrectly assume that evaluations from LLMs and humans follow the same rating
distributions. In this work, we present a statistical framework that explicitly
formalizes assumptions under which self-bias can be identified and estimated.
Our method models the difference in the scoring distribution that
LLM-as-a-judge assigns to its own completions compared to other models, while
accounting for the underlying quality of the completions provided by an
independent, third-party judge (e.g., humans). Our method reliably isolates and
quantifies self-bias, even when models vary in ability, ensuring that genuine
performance differences are not mistaken for self-bias. We conduct an empirical
analysis of self-bias on a large dataset (>5000 prompt-completion pairs)
consisting of expert human annotations and judgments from nine different LLM
judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,
systematically assign higher scores to their own outputs. These models also
display family-bias; systematically assigning higher ratings to outputs
produced by other models of the same family. Our findings highlight potential
pitfalls of using LLM judges and offer practical guidance to mitigate biases
when interpreting automated evaluations.

</details>


### [14] [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)
*Komala Subramanyam Cherukuri,Pranav Abishai Moses,Aisa Sakata,Jiangping Chen,Haihua Chen*

Main category: cs.CL

TL;DR: 本研究提出了一种可扩展的框架，利用大型语言模型（LLM）对日本裔美国人拘留口述历史进行语义和情感标注。通过精心设计的提示策略，LLM在大规模档案分析中表现出色，为数字人文和集体记忆保护提供了可行的解决方案。


<details>
  <summary>Details</summary>
Motivation: 由于口述历史档案的非结构化格式、情感复杂性和高标注成本，大规模分析仍然有限。本研究旨在开发一种可扩展的框架，以自动化日本裔美国人拘留口述历史的语义和情感标注，促进对这些档案的访问和理解。

Method: 本研究采用了一种多阶段方法，结合专家标注、提示设计和LLM评估，使用ChatGPT、Llama和Qwen等模型进行语义和情感分类。通过零样本、少量样本和RAG策略进行评估，并利用最佳提示配置对92,191个句子进行标注。

Result: 在语义分类中，ChatGPT取得了最高的F1分数（88.71%），其次是Llama（84.99%）和Qwen（83.72%）。在情感分析中，Llama略微优于Qwen（82.66%）和ChatGPT（82.29%），所有模型表现相当。最佳提示配置用于标注JAIOH收藏中的92,191个句子。

Conclusion: 本研究展示了大型语言模型（LLM）在大规模口述历史档案中进行语义和情感标注的潜力，通过精心设计的提示策略，可以实现对文化敏感档案的高效分析。研究提供了可重复使用的标注流程和实际指导，为人工智能在数字人文和集体记忆保护中的负责任使用奠定了基础。

Abstract: Oral histories are vital records of lived experience, particularly within
communities affected by systemic injustice and historical erasure. Effective
and efficient analysis of their oral history archives can promote access and
understanding of the oral histories. However, Large-scale analysis of these
archives remains limited due to their unstructured format, emotional
complexity, and high annotation costs. This paper presents a scalable framework
to automate semantic and sentiment annotation for Japanese American
Incarceration Oral History. Using LLMs, we construct a high-quality dataset,
evaluate multiple models, and test prompt engineering strategies in
historically sensitive contexts. Our multiphase approach combines expert
annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We
labeled 558 sentences from 15 narrators for sentiment and semantic
classification, then evaluated zero-shot, few-shot, and RAG strategies. For
semantic classification, ChatGPT achieved the highest F1 score (88.71%),
followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama
slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models
showing comparable results. The best prompt configurations were used to
annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our
findings show that LLMs can effectively perform semantic and sentiment
annotation across large oral history collections when guided by well-designed
prompts. This study provides a reusable annotation pipeline and practical
guidance for applying LLMs in culturally sensitive archival analysis. By
bridging archival ethics with scalable NLP techniques, this work lays the
groundwork for responsible use of artificial intelligence in digital humanities
and preservation of collective memory. GitHub:
https://github.com/kc6699c/LLM4OralHistoryAnalysis.

</details>


### [15] [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)
*Xianjun Yang,Liqiang Xiao,Shiyang Li,Faisal Ladhak,Hyokun Yun,Linda Ruth Petzold,Yi Xu,William Yang Wang*

Main category: cs.CL

TL;DR: 本文探讨了多轮jailbreaking的概念，并构建了一个基准测试来评估这种新的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 当前的jailbreaking工作只关注针对特定查询的一次性jailbreaking，而先进的LLM可以处理极长的上下文并进行多轮对话，因此需要探索多轮jailbreaking。

Method: 构建了一个多轮jailbreak基准测试（MTJ-Bench），用于在一系列开源和闭源模型上对这种设置进行基准测试，并提供了关于这种新安全威胁的新见解。

Result: 提出了多轮jailbreaking的概念，并构建了MTJ-Bench基准测试，以评估这种设置下的安全威胁。

Conclusion: 通过揭示这种新的漏洞，我们旨在呼吁社区努力构建更安全的LLM，并为更深入地理解 jailbreaking LLMs 开辟道路。

Abstract: Current jailbreaking work on large language models (LLMs) aims to elicit
unsafe outputs from given prompts. However, it only focuses on single-turn
jailbreaking targeting one specific query. On the contrary, the advanced LLMs
are designed to handle extremely long contexts and can thus conduct multi-turn
conversations. So, we propose exploring multi-turn jailbreaking, in which the
jailbroken LLMs are continuously tested on more than the first-turn
conversation or a single target query. This is an even more serious threat
because 1) it is common for users to continue asking relevant follow-up
questions to clarify certain jailbroken details, and 2) it is also possible
that the initial round of jailbreaking causes the LLMs to respond to additional
irrelevant questions consistently. As the first step (First draft done at June
2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak
Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and
closed-source models and provide novel insights into this new safety threat. By
revealing this new vulnerability, we aim to call for community efforts to build
safer LLMs and pave the way for a more in-depth understanding of jailbreaking
LLMs.

</details>


### [16] [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803)
*Ziqi Liu,Yangbin Chen,Ziyang Zhou,Yilin Li,Mingxuan Hu,Yushan Pan,Zhijie Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为SEVADE的新框架，用于抵抗幻觉的讽刺检测。该框架通过动态代理推理引擎和独立的轻量级理性仲裁者实现多方面文本分解和最终分类，实验结果显示其在准确率和宏F1分数上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 讽刺检测是一个重要但具有挑战性的自然语言处理任务。现有的大型语言模型方法通常受到单一视角分析、静态推理路径和在处理复杂的讽刺修辞时容易产生幻觉的限制，这影响了它们的准确性和可靠性。

Method: 我们提出了SEVADE，这是一种新颖的自我演化的多智能体分析框架，具有解耦评估以抵抗幻觉的讽刺检测。核心是动态代理推理引擎（DARE），利用基于语言学理论的专业代理对文本进行多方面分解并生成结构化的推理链。随后，一个独立的轻量级理性仲裁者（RA）仅根据此推理链进行最终分类。

Result: 我们的框架在四个基准数据集上的实验表明，它在准确率和宏F1分数上分别实现了6.75%和6.29%的平均提升，达到了最先进的性能。

Conclusion: 我们的框架在四个基准数据集上的实验表明，它在准确率和宏F1分数上分别实现了6.75%和6.29%的平均提升，达到了最先进的性能。

Abstract: Sarcasm detection is a crucial yet challenging Natural Language Processing
task. Existing Large Language Model methods are often limited by
single-perspective analysis, static reasoning pathways, and a susceptibility to
hallucination when processing complex ironic rhetoric, which impacts their
accuracy and reliability. To address these challenges, we propose **SEVADE**, a
novel **S**elf-**Ev**olving multi-agent **A**nalysis framework with
**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The
core of our framework is a Dynamic Agentive Reasoning Engine (DARE), which
utilizes a team of specialized agents grounded in linguistic theory to perform
a multifaceted deconstruction of the text and generate a structured reasoning
chain. Subsequently, a separate lightweight rationale adjudicator (RA) performs
the final classification based solely on this reasoning chain. This decoupled
architecture is designed to mitigate the risk of hallucination by separating
complex reasoning from the final judgment. Extensive experiments on four
benchmark datasets demonstrate that our framework achieves state-of-the-art
performance, with average improvements of **6.75%** in Accuracy and **6.29%**
in Macro-F1 score.

</details>


### [17] [Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems](https://arxiv.org/abs/2508.06810)
*Steven Coyne,Diana Galvan-Sosa,Ryan Spring,Camélia Guerraoui,Michael Zock,Keisuke Sakaguchi,Kentaro Inui*

Main category: cs.CL

TL;DR: 本文介绍了一种注释框架，用于生成针对学习者的反馈，并评估了不同方法在生成反馈方面的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动写作评估系统虽然能有效改善文本，但并不最适合语言学习。学习者可能更需要简单的解释和间接提示，特别是对于可推广的语法规则。

Method: 本文提出了一个注释框架，用于建模每个错误的错误类型和可推广性，并收集了一个带有注释的学习者错误数据集。然后评估了关键词引导、无关键词和模板引导的方法来生成反馈。

Result: 通过人类教师评估，报告了数据集的开发和所研究系统的比较性能。

Conclusion: 本文介绍了用于生成学习者反馈的注释框架，并评估了使用大型语言模型生成反馈的不同方法。

Abstract: Recent advances in natural language processing (NLP) have contributed to the
development of automated writing evaluation (AWE) systems that can correct
grammatical errors. However, while these systems are effective at improving
text, they are not optimally designed for language learning. They favor direct
revisions, often with a click-to-fix functionality that can be applied without
considering the reason for the correction. Meanwhile, depending on the error
type, learners may benefit most from simple explanations and strategically
indirect hints, especially on generalizable grammatical rules. To support the
generation of such feedback, we introduce an annotation framework that models
each error's error type and generalizability. For error type classification, we
introduce a typology focused on inferring learners' knowledge gaps by
connecting their errors to specific grammatical patterns. Following this
framework, we collect a dataset of annotated learner errors and corresponding
human-written feedback comments, each labeled as a direct correction or hint.
With this data, we evaluate keyword-guided, keyword-free, and template-guided
methods of generating feedback using large language models (LLMs). Human
teachers examined each system's outputs, assessing them on grounds including
relevance, factuality, and comprehensibility. We report on the development of
the dataset and the comparative performance of the systems investigated.

</details>


### [18] [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)
*Gangular Singh Irengbam,Nirvash Singh Wahengbam,Lanthoiba Meitei Khumanthem,Paikhomba Oinam*

Main category: cs.CL

TL;DR: 本文介绍了基于Tacotron 2和HiFi-GAN的曼尼普尔语文本到语音系统，旨在支持声调语音和资源不足的语言环境，并通过音素映射和单说话者数据集实现了可理解且自然的语音合成。


<details>
  <summary>Details</summary>
Motivation: 开发一种适用于曼尼普尔语的文本到语音（TTS）系统，以促进语言保护和技术包容。

Method: 利用Tacotron 2和HiFi-GAN，引入了一种神经TTS架构，以支持声调语音和资源不足的语言环境。

Result: 开发了Meitei Mayek到ARPAbet的音素映射，整理了一个单说话者数据集，并通过主观和客观指标验证了可理解且自然的语音合成。

Conclusion: 该系统为曼尼普尔语的语言保护和技术包容奠定了基础。

Abstract: This paper presents the development of a Text-to-Speech (TTS) system for the
Manipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and
HiFi-GAN, we introduce a neural TTS architecture adapted to support tonal
phonology and under-resourced linguistic environments. We develop a phoneme
mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and
demonstrate intelligible and natural speech synthesis, validated through
subjective and objective metrics. This system lays the groundwork for
linguistic preservation and technological inclusion of Manipuri.

</details>


### [19] [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)
*Xiaobo Zhang,Congqing He,Ying He,Jian Peng,Dajie Fu,Tien-Ping Tan*

Main category: cs.CL

TL;DR: 本文提出了一种基于标签相似性的自动标签对齐方法，用于整合多源NER语料库，提升了低资源领域的性能。


<details>
  <summary>Details</summary>
Motivation: 由于构建高质量标注数据集成本高且耗时，当前的数据集合并方法在可解释性和可扩展性方面存在不足，因此需要一种更有效的解决方案。

Method: 本文提出的方法结合了经验相似性和语义相似性，采用贪心的成对合并策略来统一不同数据集的标签空间。

Result: 实验结果表明，本文的方法能够有效地进行数据集合并，并在低资源金融领域提升NER性能。

Conclusion: 本文提出了一种基于标签相似性的自动标签对齐方法，能够有效地整合多源NER语料库，并在低资源金融领域提升了NER性能。该方法具有高效、可解释和可扩展的特点。

Abstract: Named Entity Recognition (NER) is a fundamental task in natural language
processing. It remains a research hotspot due to its wide applicability across
domains. Although recent advances in deep learning have significantly improved
NER performance, they rely heavily on large, high-quality annotated datasets.
However, building these datasets is expensive and time-consuming, posing a
major bottleneck for further research. Current dataset merging approaches
mainly focus on strategies like manual label mapping or constructing label
graphs, which lack interpretability and scalability. To address this, we
propose an automatic label alignment method based on label similarity. The
method combines empirical and semantic similarities, using a greedy pairwise
merging strategy to unify label spaces across different datasets. Experiments
are conducted in two stages: first, merging three existing NER datasets into a
unified corpus with minimal impact on NER performance; second, integrating this
corpus with a small-scale, self-built dataset in the financial domain. The
results show that our method enables effective dataset merging and enhances NER
performance in the low-resource financial domain. This study presents an
efficient, interpretable, and scalable solution for integrating multi-source
NER corpora.

</details>


### [20] [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: ReQAP是一个用于处理用户设备上复杂查询的系统，它通过递归分解问题和使用轻量级语言模型来构建操作符树，从而提供可解释的答案。


<details>
  <summary>Details</summary>
Motivation: 用户设备上存在大量个人数据，需要一种能够处理复杂查询并提供可解释答案的系统。

Method: ReQAP系统通过递归分解问题和增量构建操作符树来执行复杂查询，并利用轻量级语言模型进行智能处理，包括适当的微调。

Result: ReQAP系统展示了丰富的功能，可以处理高级用户问题，并提供了详细的执行树中操作符如何计算答案的跟踪信息。

Conclusion: ReQAP系统能够通过递归分解问题和增量构建操作符树来支持用户回答涉及过滤、连接和聚合的复杂问题，同时能够追踪答案的计算过程，提高系统的可理解性和用户信任度。

Abstract: Personal information is abundant on users' devices, from structured data in
calendar, shopping records or fitness tools, to unstructured contents in mail
and social media posts. This works presents the ReQAP system that supports
users with answers for complex questions that involve filters, joins and
aggregation over heterogeneous sources. The unique trait of ReQAP is that it
recursively decomposes questions and incrementally builds an operator tree for
execution. Both the question interpretation and the individual operators make
smart use of light-weight language models, with judicious fine-tuning. The demo
showcases the rich functionality for advanced user questions, and also offers
detailed tracking of how the answers are computed by the operators in the
execution tree. Being able to trace answers back to the underlying sources is
vital for human comprehensibility and user trust in the system.

</details>


### [21] [Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores](https://arxiv.org/abs/2508.06886)
*Arpita Saggar,Jonathan C. Darling,Vania Dimitrova,Duygu Sarikaya,David C. Hogg*

Main category: cs.CL

TL;DR: SBS框架通过得分条件训练提高角色一致性对话生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有的对话数据多样性有限，导致在对话中有效整合角色一致性仍然具有挑战性。

Method: SBS框架通过将响应和其相对质量的学习统一到一个步骤中，利用基于名词的替换进行增强，并使用基于语义相似性的分数作为响应质量的代理。

Result: SBS框架在PERSONA-CHAT和ConvAI2基准数据集上的实验表明，得分条件训练使现有模型能够更好地捕捉一系列与角色一致的对话。

Conclusion: SBS框架在对话生成中表现出色，能够更好地捕捉与角色一致的对话。

Abstract: Persona-based dialogue generation is an important milestone towards building
conversational artificial intelligence. Despite the ever-improving capabilities
of large language models (LLMs), effectively integrating persona fidelity in
conversations remains challenging due to the limited diversity in existing
dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which
outperforms previous methods and yields improvements for both million and
billion-parameter models. Unlike previous methods, SBS unifies the learning of
responses and their relative quality into a single step. The key innovation is
to train a dialogue model to correlate augmented responses with a quality score
during training and then leverage this knowledge at inference. We use
noun-based substitution for augmentation and semantic similarity-based scores
as a proxy for response quality. Through extensive experiments with benchmark
datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training
allows existing models to better capture a spectrum of persona-consistent
dialogues. Our ablation studies also demonstrate that including scores in the
input prompt during training is superior to conventional training setups. Code
and further details are available at
https://arpita2512.github.io/score_before_you_speak

</details>


### [22] [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)
*Siyuan Li,Xi Lin,Guangyan Li,Zehao Liu,Aodu Wulianghai,Li Ding,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: SentiDetect is a new framework for detecting LLM-generated text by analyzing sentiment distribution stability, showing superior performance and robustness compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods based on lexical heuristics or fine-tuned classifiers have limited generalizability and are vulnerable to paraphrasing, adversarial perturbations, and cross-domain shifts.

Method: SentiDetect is a model-agnostic framework that analyzes the divergence in sentiment distribution stability by defining two metrics: sentiment distribution consistency and sentiment distribution preservation.

Result: SentiDetect outperforms state-of-the-art baselines with over 16% and 11% F1 score improvements on Gemini-1.5-Pro and GPT-4-0613, respectively, and shows greater robustness to paraphrasing, adversarial attacks, and text length variations.

Conclusion: SentiDetect demonstrates superior performance and robustness compared to existing detectors in identifying LLM-generated text.

Abstract: The rapid advancement of large language models (LLMs) has resulted in
increasingly sophisticated AI-generated content, posing significant challenges
in distinguishing LLM-generated text from human-written language. Existing
detection methods, primarily based on lexical heuristics or fine-tuned
classifiers, often suffer from limited generalizability and are vulnerable to
paraphrasing, adversarial perturbations, and cross-domain shifts. In this work,
we propose SentiDetect, a model-agnostic framework for detecting LLM-generated
text by analyzing the divergence in sentiment distribution stability. Our
method is motivated by the empirical observation that LLM outputs tend to
exhibit emotionally consistent patterns, whereas human-written texts display
greater emotional variability. To capture this phenomenon, we define two
complementary metrics: sentiment distribution consistency and sentiment
distribution preservation, which quantify stability under sentiment-altering
and semantic-preserving transformations. We evaluate SentiDetect on five
diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,
Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its
superiority over state-of-the-art baselines, with over 16% and 11% F1 score
improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,
SentiDetect also shows greater robustness to paraphrasing, adversarial attacks,
and text length variations, outperforming existing detectors in challenging
scenarios.

</details>


### [23] [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Khaled Shaban,Hozaifa Kassab*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段框架，用于Quranic Question Answering，通过模型集成和指令调优的语言模型取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: Quranic Question Answering面临独特的挑战，由于古典阿拉伯语的语言复杂性和宗教文本的语义丰富性。

Method: 我们提出了一种两阶段框架，结合了微调的阿拉伯语语言模型和指令调优的大语言模型。

Result: 我们的方法在Quran QA 2023共享任务中取得了最先进的结果，检索的MAP@10为0.3128，MRR@10为0.5763，提取的pAP@10为0.669，显著优于之前的方法。

Conclusion: 这些结果表明，结合模型集成和指令调优的语言模型可以有效解决专业领域中低资源问答的挑战。

Abstract: Quranic Question Answering presents unique challenges due to the linguistic
complexity of Classical Arabic and the semantic richness of religious texts. In
this paper, we propose a novel two-stage framework that addresses both passage
retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned
Arabic language models to achieve superior ranking performance. For answer
extraction, we employ instruction-tuned large language models with few-shot
prompting to overcome the limitations of fine-tuning on small datasets. Our
approach achieves state-of-the-art results on the Quran QA 2023 Shared Task,
with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of
0.669 for extraction, substantially outperforming previous methods. These
results demonstrate that combining model ensembling and instruction-tuned
language models effectively addresses the challenges of low-resource question
answering in specialized domains.

</details>


### [24] [Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models](https://arxiv.org/abs/2508.06974)
*Zhijun Tu,Hanting Chen,Siqi Liu,Chuanjian Liu,Jian Li,Jie Hu,Yunhe Wang*

Main category: cs.CL

TL;DR: 本文提出一种新的1位LLM量化方法，通过渐进式训练和二进制感知初始化等技术，有效利用预训练模型，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常从头开始训练1位LLM，未能充分利用预训练模型，导致训练成本高且准确率下降。

Method: 我们引入了一种一致的渐进式训练，同时进行前向和反向传播，将浮点权重平滑地转换为二值化权重。此外，我们还结合了二进制感知初始化和双缩放补偿，以降低渐进式训练的难度并提高性能。

Result: 实验结果表明，我们的方法在各种大小的LLM上都优于现有方法。

Conclusion: 我们的方法表明，可以使用预训练模型实现高性能的1位LLM，从而避免从头开始昂贵的训练。

Abstract: 1-bit LLM quantization offers significant advantages in reducing storage and
computational costs. However, existing methods typically train 1-bit LLMs from
scratch, failing to fully leverage pre-trained models. This results in high
training costs and notable accuracy degradation. We identify that the large gap
between full precision and 1-bit representations makes direct adaptation
difficult. In this paper, we introduce a consistent progressive training for
both forward and backward, smoothly converting the floating-point weights into
the binarized ones. Additionally, we incorporate binary-aware initialization
and dual-scaling compensation to reduce the difficulty of progressive training
and improve the performance. Experimental results on LLMs of various sizes
demonstrate that our method outperforms existing approaches. Our results show
that high-performance 1-bit LLMs can be achieved using pre-trained models,
eliminating the need for expensive training from scratch.

</details>


### [25] [Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings](https://arxiv.org/abs/2508.07017)
*Mao Li,Fred Conrad,Johann Gagnon-Bartsch*

Main category: cs.CL

TL;DR: 提出了一种新的抽象摘要方法Vec2Summ，通过语义压缩和生成语言模型实现高效的摘要生成，并具有可扩展性和语义控制的优势。


<details>
  <summary>Details</summary>
Motivation: 解决基于大语言模型的摘要方法的关键限制，例如避免上下文长度限制、通过语义参数实现可解释和可控的生成，并高效地扩展到文档大小。

Method: Vec2Summ将摘要任务视为语义压缩，通过在语义嵌入空间中使用单个均值向量表示文档集合，然后通过生成语言模型解码该均值向量以生成流畅的摘要，并通过从以均值为中心的高斯分布中采样引入随机性。

Result: 实证结果表明，Vec2Summ能够为主题聚焦、顺序无关的文档集生成连贯的摘要，在主题覆盖和效率方面与直接的大语言模型摘要相当，尽管细节不够精细。

Conclusion: Vec2Summ的潜在应用在于需要可扩展性、语义控制和文档级抽象的场景。

Abstract: We propose Vec2Summ, a novel method for abstractive summarization that frames
the task as semantic compression. Vec2Summ represents a document collection
using a single mean vector in the semantic embedding space, capturing the
central meaning of the corpus. To reconstruct fluent summaries, we perform
embedding inversion -- decoding this mean vector into natural language using a
generative language model. To improve reconstruction quality and capture some
degree of topical variability, we introduce stochasticity by sampling from a
Gaussian distribution centered on the mean. This approach is loosely analogous
to bagging in ensemble learning, where controlled randomness encourages more
robust and varied outputs. Vec2Summ addresses key limitations of LLM-based
summarization methods. It avoids context-length constraints, enables
interpretable and controllable generation via semantic parameters, and scales
efficiently with corpus size -- requiring only $O(d + d^2)$ parameters.
Empirical results show that Vec2Summ produces coherent summaries for topically
focused, order-invariant corpora, with performance comparable to direct LLM
summarization in terms of thematic coverage and efficiency, albeit with less
fine-grained detail. These results underscore Vec2Summ's potential in settings
where scalability, semantic control, and corpus-level abstraction are
prioritized.

</details>


### [26] [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)
*Muhammad Dehan Al Kautsar,Aswin Candra,Muhammad Alif Al Hakim,Maxalmina Satria Kahfi,Fajri Koto,Alham Fikri Aji,Peerat Limkonchotiwat,Ekapol Chuangsuwanich,Genta Indra Winata*

Main category: cs.CL

TL;DR: 本文介绍了一个名为SEADialogues的文化相关对话数据集，旨在填补现有聊天数据集在文化细节方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的聊天数据集往往忽略了自然人类对话中的文化细微差别，因此需要一个更符合文化背景的数据集来支持对话系统的研究。

Method: 引入了SEADialogues数据集，该数据集基于东南亚地区的人文背景，包含八种语言的对话，并且每个对话都包含人物属性和两个与文化相关的主题。

Result: SEADialogues数据集包含了来自六个东南亚国家的八种语言的对话，这些对话具有文化相关性，并且可以用于研究文化意识和以人为本的大语言模型。

Conclusion: SEADialogues数据集的发布有助于推动文化意识和以人类为中心的大语言模型的研究，包括对话代理。

Abstract: Although numerous datasets have been developed to support dialogue systems,
most existing chit-chat datasets overlook the cultural nuances inherent in
natural human conversations. To address this gap, we introduce SEADialogues, a
culturally grounded dialogue dataset centered on Southeast Asia, a region with
over 700 million people and immense cultural diversity. Our dataset features
dialogues in eight languages from six Southeast Asian countries, many of which
are low-resource despite having sizable speaker populations. To enhance
cultural relevance and personalization, each dialogue includes persona
attributes and two culturally grounded topics that reflect everyday life in the
respective communities. Furthermore, we release a multi-turn dialogue dataset
to advance research on culturally aware and human-centric large language
models, including conversational dialogue agents.

</details>


### [27] [BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context](https://arxiv.org/abs/2508.07090)
*Aditya Tomar,Nihar Ranjan Sahoo,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 研究介绍了BharatBBQ，这是一个用于评估多种印度语言中社会偏见的文化适应性基准，并发现印度语言中的偏见通常比英语更严重。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要针对西方背景，无法充分适用于印度语境，因此需要一个更全面的基准来评估印度社会中的偏见。

Method: 引入了BharatBBQ，这是一个文化适应的基准，用于评估多种印度语言中的偏见。

Result: 研究发现，不同语言和社会类别中存在持续的偏见，且印度语言中的偏见往往比英语更严重。

Conclusion: 研究强调了需要语言和文化上扎根的基准来评估偏见，特别是在印度语境中。

Abstract: Evaluating social biases in language models (LMs) is crucial for ensuring
fairness and minimizing the reinforcement of harmful stereotypes in AI systems.
Existing benchmarks, such as the Bias Benchmark for Question Answering (BBQ),
primarily focus on Western contexts, limiting their applicability to the Indian
context. To address this gap, we introduce BharatBBQ, a culturally adapted
benchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,
Telugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3
intersectional groups, reflecting prevalent biases in the Indian sociocultural
landscape. Our dataset contains 49,108 examples in one language that are
expanded using translation and verification to 392,864 examples in eight
different languages. We evaluate five multilingual LM families across zero and
few-shot settings, analyzing their bias and stereotypical bias scores. Our
findings highlight persistent biases across languages and social categories and
often amplified biases in Indian languages compared to English, demonstrating
the necessity of linguistically and culturally grounded benchmarks for bias
evaluation.

</details>


### [28] [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)
*Lijie Yang,Zhihao Zhang,Arti Jain,Shijie Cao,Baihong Yuan,Yiwei Chen,Zhihao Jia,Ravi Netravali*

Main category: cs.CL

TL;DR: LessIsMore is a training-free sparse attention mechanism that improves efficiency and accuracy in large reasoning models by leveraging global attention patterns and unified token selection.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining.

Method: LessIsMore is a training-free sparse attention mechanism for reasoning tasks that leverages global attention patterns rather than relying on traditional head-specific local optimizations. It aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers.

Result: Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a 1.1× average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2× fewer tokens without accuracy loss, achieving a 1.13× end-to-end speed-up compared to existing sparse attention methods.

Conclusion: LessIsMore preserves -- and in some cases improves -- accuracy while achieving a 1.1× average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2× fewer tokens without accuracy loss, achieving a 1.13× end-to-end speed-up compared to existing sparse attention methods.

Abstract: Large reasoning models achieve strong performance through test-time scaling
but incur substantial computational overhead, particularly from excessive token
generation when processing short input prompts. While sparse attention
mechanisms can reduce latency and memory usage, existing approaches suffer from
significant accuracy degradation due to accumulated errors during
long-generation reasoning. These methods generally require either high token
retention rates or expensive retraining. We introduce LessIsMore, a
training-free sparse attention mechanism for reasoning tasks, which leverages
global attention patterns rather than relying on traditional head-specific
local optimizations. LessIsMore aggregates token selections from local
attention heads with recent contextual information, enabling unified cross-head
token ranking for future decoding layers. This unified selection improves
generalization and efficiency by avoiding the need to maintain separate token
subsets per head. Evaluation across diverse reasoning tasks and benchmarks
shows that LessIsMore preserves -- and in some cases improves -- accuracy while
achieving a $1.1\times$ average decoding speed-up compared to full attention.
Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss,
achieving a $1.13\times$ end-to-end speed-up compared to existing sparse
attention methods.

</details>


### [29] [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)
*Falaah Arif Khan,Nivedha Sivakumar,Yinong Oliver Wang,Katherine Metcalf,Cezanne Camacho,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的交叉性偏见，发现模型在不同身份上的置信度差异显著，这可能对社会造成伤害。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估大型语言模型中的交叉性偏见，特别是在资源受限的决策支持场景中，如招聘和录取。研究者注意到，AI系统可能反映并加剧社会偏见，因此需要更全面的公平性评估方法。

Method: 本文通过扩展单一轴公平评估，研究了交叉性偏见，并创建了一个名为WinoIdentity的新基准。该基准通过将25个人口统计标记与二元性别相交，生成了245,700个提示来评估50种不同的偏见模式。同时，本文提出了一种称为Coreference Confidence Disparity的群体（不）公平度量标准，用于衡量模型对不同交叉身份的置信度差异。

Result: 本文评估了五种最近发布的大型语言模型，发现它们在多个人口统计属性上存在高达40%的置信度差异，尤其是对于双重不利身份在反刻板印象情境中的不确定性最高。此外，核心指代置信度甚至在主流或特权标记上也有所下降，表明模型的高性能可能更多是由于记忆而非逻辑推理。

Conclusion: 本文指出，尽管大型语言模型（LLMs）在性能上取得了显著进步，但它们在社会关键场景中可能造成基于身份的伤害。研究发现，模型在某些交叉身份上的置信度差异高达40%，这表明模型的表现更多是记忆而非逻辑推理的结果。此外，这些失败可能加剧社会伤害。

Abstract: Large language models (LLMs) have achieved impressive performance, leading to
their widespread adoption as decision-support tools in resource-constrained
contexts like hiring and admissions. There is, however, scientific consensus
that AI systems can reflect and exacerbate societal biases, raising concerns
about identity-based harm when used in critical social contexts. Prior work has
laid a solid foundation for assessing bias in LLMs by evaluating demographic
disparities in different language reasoning tasks. In this work, we extend
single-axis fairness evaluations to examine intersectional bias, recognizing
that when multiple axes of discrimination intersect, they create distinct
patterns of disadvantage. We create a new benchmark called WinoIdentity by
augmenting the WinoBias dataset with 25 demographic markers across 10
attributes, including age, nationality, and race, intersected with binary
gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.
Focusing on harms of omission due to underrepresentation, we investigate bias
through the lens of uncertainty and propose a group (un)fairness metric called
Coreference Confidence Disparity which measures whether models are more or less
confident for some intersectional identities than others. We evaluate five
recently published LLMs and find confidence disparities as high as 40% along
various demographic attributes including body type, sexual orientation and
socio-economic status, with models being most uncertain about
doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,
coreference confidence decreases even for hegemonic or privileged markers,
indicating that the recent impressive performance of LLMs is more likely due to
memorization than logical reasoning. Notably, these are two independent
failures in value alignment and validity that can compound to cause social
harm.

</details>


### [30] [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)
*Anna Seo Gyeong Choi,Hoon Choi*

Main category: cs.CL

TL;DR: This paper examines ASR bias through a philosophical lens, arguing that systematic misrecognition of certain speech varieties constitutes a form of disrespect that compounds historical injustices against marginalized linguistic communities. It identifies three unique ethical dimensions of speech technologies and concludes that addressing ASR bias requires more than technical interventions; it demands recognition of diverse speech varieties as legitimate forms of expression.


<details>
  <summary>Details</summary>
Motivation: research on the fairness implications of ASR systems remains limited, and there is a need to address the systematic misrecognition of certain speech varieties that constitutes a form of disrespect and compounds historical injustices against marginalized linguistic communities.

Method: examines ASR bias through a philosophical lens, distinguishing between morally neutral classification and harmful discrimination, and identifying three unique ethical dimensions of speech technologies.

Result: the paper identifies three unique ethical dimensions of speech technologies that differentiate ASR bias from other algorithmic fairness concerns, and argues that current approaches often embed and reinforce problematic language ideologies.

Conclusion:  addressing ASR bias requires more than technical interventions; it demands recognition of diverse speech varieties as legitimate forms of expression worthy of technological accommodation.

Abstract: Automatic Speech Recognition (ASR) systems now mediate countless
human-technology interactions, yet research on their fairness implications
remains surprisingly limited. This paper examines ASR bias through a
philosophical lens, arguing that systematic misrecognition of certain speech
varieties constitutes more than a technical limitation -- it represents a form
of disrespect that compounds historical injustices against marginalized
linguistic communities. We distinguish between morally neutral classification
(discriminate1) and harmful discrimination (discriminate2), demonstrating how
ASR systems can inadvertently transform the former into the latter when they
consistently misrecognize non-standard dialects. We identify three unique
ethical dimensions of speech technologies that differentiate ASR bias from
other algorithmic fairness concerns: the temporal burden placed on speakers of
non-standard varieties ("temporal taxation"), the disruption of conversational
flow when systems misrecognize speech, and the fundamental connection between
speech patterns and personal/cultural identity. These factors create asymmetric
power relationships that existing technical fairness metrics fail to capture.
The paper analyzes the tension between linguistic standardization and pluralism
in ASR development, arguing that current approaches often embed and reinforce
problematic language ideologies. We conclude that addressing ASR bias requires
more than technical interventions; it demands recognition of diverse speech
varieties as legitimate forms of expression worthy of technological
accommodation. This philosophical reframing offers new pathways for developing
ASR systems that respect linguistic diversity and speaker autonomy.

</details>


### [31] [Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172)
*Biao Yi,Jiahao Li,Baolei Zhang,Lihai Nie,Tong Li,Tiansheng Huang,Zheli Liu*

Main category: cs.CL

TL;DR: SafeGrad is a novel method for safe fine-tuning of Large Language Models, which addresses the vulnerability of being compromised by malicious examples in the fine-tuning dataset. It uses gradient surgery to nullify harmful components of the user-task gradient, ensuring safety while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning-as-a-Service introduces a critical vulnerability where a few malicious examples mixed into the user's fine-tuning dataset can compromise the safety alignment of Large Language Models (LLMs). While a recognized paradigm frames safe fine-tuning as a multi-objective optimization problem balancing user task performance with safety alignment, we find existing solutions are critically sensitive to the harmful ratio, with defenses degrading sharply as harmful ratio increases.

Method: SafeGrad, a novel method that employs gradient surgery. When a conflict is detected, SafeGrad nullifies the harmful component of the user-task gradient by projecting it onto the orthogonal plane of the alignment gradient. To further enhance robustness and data efficiency, we employ a KL-divergence alignment loss that learns the rich, distributional safety profile of the well-aligned foundation model.

Result: SafeGrad provides state-of-the-art defense across various LLMs and datasets, maintaining robust safety even at high harmful ratios without compromising task fidelity.

Conclusion: SafeGrad provides state-of-the-art defense across various LLMs and datasets, maintaining robust safety even at high harmful ratios without compromising task fidelity.

Abstract: Fine-tuning-as-a-Service introduces a critical vulnerability where a few
malicious examples mixed into the user's fine-tuning dataset can compromise the
safety alignment of Large Language Models (LLMs). While a recognized paradigm
frames safe fine-tuning as a multi-objective optimization problem balancing
user task performance with safety alignment, we find existing solutions are
critically sensitive to the harmful ratio, with defenses degrading sharply as
harmful ratio increases. We diagnose that this failure stems from conflicting
gradients, where the user-task update directly undermines the safety objective.
To resolve this, we propose SafeGrad, a novel method that employs gradient
surgery. When a conflict is detected, SafeGrad nullifies the harmful component
of the user-task gradient by projecting it onto the orthogonal plane of the
alignment gradient, allowing the model to learn the user's task without
sacrificing safety. To further enhance robustness and data efficiency, we
employ a KL-divergence alignment loss that learns the rich, distributional
safety profile of the well-aligned foundation model. Extensive experiments show
that SafeGrad provides state-of-the-art defense across various LLMs and
datasets, maintaining robust safety even at high harmful ratios without
compromising task fidelity.

</details>


### [32] [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)
*Leyi Pan,Zheyu Fu,Yunpeng Zhai,Shuchang Tao,Sheng Guan,Shiyu Huang,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Felix Henry,Lijie Wen,Aiwei Liu*

Main category: cs.CL

TL;DR: 本文介绍了Omni-SafetyBench，这是第一个全面的OLLM安全评估基准，包含24种模态组合和972个样本，提出了定制的安全评分和跨模态安全一致性评分，评估结果显示OLLM存在严重安全隐患。


<details>
  <summary>Details</summary>
Motivation: 由于没有专门针对OLLM的基准，且现有基准无法评估音频-视觉联合输入或跨模态安全一致性，因此需要创建一个全面的基准来评估OLLM的安全性。

Method: 我们提出了针对OLLM安全评估的定制指标：基于条件攻击成功率（C-ASR）和拒绝率（C-RR）的安全评分，以及跨模态安全一致性评分（CMSC-score）。

Result: 评估6个开源和4个闭源OLLM揭示了关键漏洞：(1) 没有模型在整体安全性和一致性方面都表现出色，只有3个模型在两个指标中均超过0.6，最高得分约为0.8；(2) 安全防御在复杂输入下减弱，尤其是音频-视觉联合输入；(3) 一些模型在特定模态上的得分低至0.14。

Conclusion: 我们的基准和指标突显了增强OLLM安全性的紧迫需求，为未来的改进提供了基础。

Abstract: The rise of Omni-modal Large Language Models (OLLMs), which integrate visual
and auditory processing with text, necessitates robust safety evaluations to
mitigate harmful outputs. However, no dedicated benchmarks currently exist for
OLLMs, and prior benchmarks designed for other LLMs lack the ability to assess
safety performance under audio-visual joint inputs or cross-modal safety
consistency. To fill this gap, we introduce Omni-SafetyBench, the first
comprehensive parallel benchmark for OLLM safety evaluation, featuring 24
modality combinations and variations with 972 samples each, including dedicated
audio-visual harm cases. Considering OLLMs' comprehension challenges with
complex omni-modal inputs and the need for cross-modal consistency evaluation,
we propose tailored metrics: a Safety-score based on conditional Attack Success
Rate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and
a Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency
across modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals
critical vulnerabilities: (1) no model excels in both overall safety and
consistency, with only 3 models achieving over 0.6 in both metrics and top
performer scoring around 0.8; (2) safety defenses weaken with complex inputs,
especially audio-visual joints; (3) severe weaknesses persist, with some models
scoring as low as 0.14 on specific modalities. Our benchmark and metrics
highlight urgent needs for enhanced OLLM safety, providing a foundation for
future improvements.

</details>


### [33] [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)
*Kejin Liu,Junhong Lian,Xiang Ao,Ningtao Wang,Xing Fu,Yu Cheng,Weiqiang Wang,Xinyu Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的个性化标题生成框架PHG-DIF，通过去除点击噪声并动态建模用户兴趣来提高标题质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了整个历史点击流中的个性化无关点击噪声，这可能导致偏离真实用户偏好的幻觉标题。本文揭示了点击噪声对个性化生成质量的有害影响，并提出了一个新的个性化标题生成框架PHG-DIF。

Method: PHG-DIF首先采用双阶段过滤来有效去除点击流噪声，通过短停留时间和异常点击爆发来识别噪声，然后利用多级时间融合动态建模用户不断演变和多方面兴趣以进行精确的用户画像。

Result: PHG-DIF在DT-PENS数据集上实现了最先进的结果，表明其能有效减轻点击噪声的负面影响并显著提高标题质量。

Conclusion: PHG-DIF能够有效减轻点击噪声的负面影响，并显著提高标题质量，在DT-PENS上实现了最先进的结果。框架实现和数据集已公开。

Abstract: Accurate personalized headline generation hinges on precisely capturing user
interests from historical behaviors. However, existing methods neglect
personalized-irrelevant click noise in entire historical clickstreams, which
may lead to hallucinated headlines that deviate from genuine user preferences.
In this paper, we reveal the detrimental impact of click noise on personalized
generation quality through rigorous analysis in both user and news dimensions.
Based on these insights, we propose a novel Personalized Headline Generation
framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).
PHG-DIF first employs dual-stage filtering to effectively remove clickstream
noise, identified by short dwell times and abnormal click bursts, and then
leverages multi-level temporal fusion to dynamically model users' evolving and
multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a
new benchmark dataset comprising the click behavior of 1,000 carefully curated
users and nearly 10,000 annotated personalized headlines with historical dwell
time annotations. Extensive experiments demonstrate that PHG-DIF substantially
mitigates the adverse effects of click noise and significantly improves
headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our
framework implementation and dataset are available at
https://github.com/liukejin-up/PHG-DIF.

</details>


### [34] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
*Jiaqi Yin,Yi-Wei Chen,Meng-Lung Lee,Xiya Liu*

Main category: cs.CL

TL;DR: 该研究提出了一种自动化提取多语言企业管道脚本中细粒度模式血缘的新框架，并通过实验验证了其在不同模型规模下的有效性。


<details>
  <summary>Details</summary>
Motivation: 企业数据管道中的语义漂移会影响数据的可重复性和治理，以及诸如检索增强生成（RAG）和文本到SQL系统等服务的效用。

Method: 该研究提出了一个新框架，用于从多语言企业管道脚本中自动提取细粒度模式血缘，并引入了Schema Lineage Composite Evaluation (SLiCE) 来评估血缘质量。

Result: 实验表明，模式血缘提取的性能随着模型规模和提示技术的复杂性而提高。特别地，一个32B的开源模型在使用单次推理轨迹时，可以达到与GPT系列相当的性能。

Conclusion: 该研究提出了一种自动化提取多语言企业管道脚本中细粒度模式血缘的新框架，并展示了其在实际应用中的可扩展性和经济性。

Abstract: Enterprise data pipelines, characterized by complex transformations across
multiple programming languages, often cause a semantic disconnect between
original metadata and downstream data. This "semantic drift" compromises data
reproducibility and governance, and impairs the utility of services like
retrieval-augmented generation (RAG) and text-to-SQL systems. To address this,
a novel framework is proposed for the automated extraction of fine-grained
schema lineage from multilingual enterprise pipeline scripts. This method
identifies four key components: source schemas, source tables, transformation
logic, and aggregation operations, creating a standardized representation of
data transformations. For the rigorous evaluation of lineage quality, this
paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that
assesses both structural correctness and semantic fidelity. A new benchmark is
also presented, comprising 1,700 manually annotated lineages from real-world
industrial scripts. Experiments were conducted with 12 language models, from
1.3B to 32B small language models (SLMs) to large language models (LLMs) like
GPT-4o and GPT-4.1. The results demonstrate that the performance of schema
lineage extraction scales with model size and the sophistication of prompting
techniques. Specially, a 32B open-source model, using a single reasoning trace,
can achieve performance comparable to the GPT series under standard prompting.
This finding suggests a scalable and economical approach for deploying
schema-aware agents in practical applications.

</details>


### [35] [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)
*Kabir Khan,Priya Sharma,Arjun Mehta,Neha Gupta,Ravi Narayanan*

Main category: cs.CL

TL;DR: 本文提出了一种名为DySK-Attn的新框架，使大型语言模型能够从动态外部源中高效整合实时知识。该框架的核心是一种稀疏知识注意力机制，可以在不增加高计算成本的情况下有效识别和关注知识图中的少量相关事实。实验表明，DySK-Attn在事实准确性和计算效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的知识是静态的，很快就会过时。重新训练这些大型模型在计算上是不可行的，而现有的知识编辑技术可能很慢，并可能引入意外的副作用。

Method: 我们提出了DySK-Attn，这是一种新颖的框架，使大型语言模型能够从动态外部源中高效整合实时知识。我们的方法结合了一个可以即时更新的动态知识图（KG）。框架的核心是一种稀疏知识注意力机制，允许大型语言模型进行粗到细粒度的搜索，有效地识别并专注于知识图中的少量高度相关事实。

Result: 通过在时间敏感的问题回答任务上的广泛实验，我们证明DySK-Attn在事实准确性（对于更新的知识）和计算效率方面显著优于强大的基线，包括标准的检索增强生成（RAG）和模型编辑技术。

Conclusion: 我们的框架提供了一种可扩展且有效的解决方案，使大型语言模型能够跟上不断变化的世界。

Abstract: Large Language Models (LLMs) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a sparse knowledge attention
mechanism, which allows the LLM to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building LLMs that can stay current with the
ever-changing world.

</details>


### [36] [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)
*Yanru Sun,Emadeldeen Eldele,Zongxia Xie,Yucheng Wang,Wenzhe Niu,Qinghua Hu,Chee Keong Kwoh,Min Wu*

Main category: cs.CL

TL;DR: 本文提出了TALON框架，通过建模时间异质性和强制语义对齐来增强基于大语言模型的预测。


<details>
  <summary>Details</summary>
Motivation: 由于时间模式的固有异质性和连续数值信号与离散语言表示之间的模态差距，大语言模型直接应用于时间序列预测仍然具有挑战性。

Method: 我们设计了一个异构时间编码器，将多变量时间序列划分为结构一致的段，以在不同时间模式上实现局部专家建模。为了弥合模态差距，我们引入了一个语义对齐模块，将时间特征与适合大语言模型的表示对齐，从而在不使用手工提示的情况下有效整合时间序列到基于语言的模型中。

Result: 在七个现实世界基准上的实验表明，TALON在所有数据集上都取得了优越的性能，平均MSE改进高达11%。

Conclusion: 这些结果证明了在将大语言模型应用于时间序列预测时，结合模式感知和语义感知设计的有效性。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in natural language processing due to their strong generalization
and sequence modeling capabilities. However, their direct application to time
series forecasting remains challenging due to two fundamental issues: the
inherent heterogeneity of temporal patterns and the modality gap between
continuous numerical signals and discrete language representations. In this
work, we propose TALON, a unified framework that enhances LLM-based forecasting
by modeling temporal heterogeneity and enforcing semantic alignment.
Specifically, we design a Heterogeneous Temporal Encoder that partitions
multivariate time series into structurally coherent segments, enabling
localized expert modeling across diverse temporal patterns. To bridge the
modality gap, we introduce a Semantic Alignment Module that aligns temporal
features with LLM-compatible representations, enabling effective integration of
time series into language-based models while eliminating the need for
handcrafted prompts during inference. Extensive experiments on seven real-world
benchmarks demonstrate that TALON achieves superior performance across all
datasets, with average MSE improvements of up to 11\% over recent
state-of-the-art methods. These results underscore the effectiveness of
incorporating both pattern-aware and semantic-aware designs when adapting LLMs
for time series forecasting. The code is available at:
https://github.com/syrGitHub/TALON.

</details>


### [37] [Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model](https://arxiv.org/abs/2508.07209)
*Chaoqun Cui,Siyuan Li,Kunkun Ma,Caiyan Jia*

Main category: cs.CL

TL;DR: 本文提出了一种名为PEP的连续预训练策略，用于提升预训练语言模型在社交媒体谣言检测任务中的表现。通过引入传播结构信息，实验结果表明PEP有效提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型在社交媒体应用任务如谣言检测上的表现不佳，原因包括预训练语料库与社交文本之间的不匹配、对独特社交符号处理不足以及预训练任务不适合建模传播结构中的用户互动。

Method: 提出了一种连续预训练策略称为Post Engagement Prediction (PEP)，旨在将传播结构中的信息注入预训练语言模型。PEP使模型能够预测帖子之间的根、分支和父关系，捕捉立场和情感的相互作用，这对于谣言检测至关重要。

Result: 通过利用PEP策略和TwitterCorpus等资源，训练了一个针对Twitter的预训练语言模型SoLM。实验表明，PEP显著提升了通用和社交媒体预训练语言模型的谣言检测性能，甚至在少样本情况下也能超越当前最先进的方法。

Conclusion: PEP策略能显著提升谣言检测性能，即使在少样本情况下也能表现良好。SoLM模型在不使用高级模块的情况下也取得了具有竞争力的结果，证明了该策略的有效性。

Abstract: Pretrained Language Models (PLMs) have excelled in various Natural Language
Processing tasks, benefiting from large-scale pretraining and self-attention
mechanism's ability to capture long-range dependencies. However, their
performance on social media application tasks like rumor detection remains
suboptimal. We attribute this to mismatches between pretraining corpora and
social texts, inadequate handling of unique social symbols, and pretraining
tasks ill-suited for modeling user engagements implicit in propagation
structures. To address these issues, we propose a continue pretraining strategy
called Post Engagement Prediction (PEP) to infuse information from propagation
structures into PLMs. PEP makes models to predict root, branch, and parent
relations between posts, capturing interactions of stance and sentiment crucial
for rumor detection. We also curate and release large-scale Twitter corpus:
TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with
propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP
strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments
demonstrate PEP significantly boosts rumor detection performance across
universal and social media PLMs, even in few-shot scenarios. On benchmark
datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it
to outperform current state-of-the-art methods on multiple datasets. SoLM
alone, without high-level modules, also achieves competitive results,
highlighting the strategy's effectiveness in learning discriminative post
interaction features.

</details>


### [38] [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)
*Itai Allouche,Itay Asael,Rotem Rousso,Vered Dassa,Ann Bradlow,Seung-Eun Kim,Matthew Goldrick,Joseph Keshet*

Main category: cs.CL

TL;DR: 本文研究了神经网络在词重音预测中的可解释性，发现其主要依赖于重音元音的频谱特性，并扩展了传统语音学工作。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络在语音处理中取得了成功，但它们通常作为黑箱操作，因此需要理解它们的决策过程并对其进行解释。本文旨在研究这一问题，并以词重音为背景进行探讨。

Method: 本文使用了卷积神经网络（CNN）架构来预测重音位置，并利用层相关性传播（LRP）进行可解释性分析。此外，还提出了一种特定特征的相关性分析方法。

Result: 通过训练多个CNN架构，实现了高达92%的准确率。LRP分析表明，预测主要受到重音和非重音音节的信息影响，尤其是重音元音的频谱特性。此外，分类器还关注整个单词的信息。

Conclusion: 这些结果揭示了深度学习从自然出现的数据中获取重音分布线索的能力，扩展了基于高度控制刺激的传统语音学工作。

Abstract: Despite their success in speech processing, neural networks often operate as
black boxes, prompting the question: what informs their decisions, and how can
we interpret them? This work examines this issue in the context of lexical
stress. A dataset of English disyllabic words was automatically constructed
from read and spontaneous speech. Several Convolutional Neural Network (CNN)
architectures were trained to predict stress position from a spectrographic
representation of disyllabic words lacking minimal stress pairs (e.g., initial
stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out
test data. Layerwise Relevance Propagation (LRP), a technique for CNN
interpretability analysis, revealed that predictions for held-out minimal pairs
(PROtest vs. proTEST ) were most strongly influenced by information in stressed
versus unstressed syllables, particularly the spectral properties of stressed
vowels. However, the classifiers also attended to information throughout the
word. A feature-specific relevance analysis is proposed, and its results
suggest that our best-performing classifier is strongly influenced by the
stressed vowel's first and second formants, with some evidence that its pitch
and third formant also contribute. These results reveal deep learning's ability
to acquire distributed cues to stress from naturally occurring data, extending
traditional phonetic work based around highly controlled stimuli.

</details>


### [39] [Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition](https://arxiv.org/abs/2508.07248)
*Zhe Ren*

Main category: cs.CL

TL;DR: 本文提出了一种基于提示调优和记忆演示模板的策略，以解决少样本持续学习命名实体识别中的知识蒸馏困境。


<details>
  <summary>Details</summary>
Motivation: 在少样本持续学习命名实体识别（FS-CLNER）任务中，新类实体稀缺导致模型难以泛化，同时缺乏旧类实体信息阻碍了旧知识的蒸馏，从而陷入少样本知识蒸馏困境。

Method: 我们通过提示调优范式和记忆演示模板策略来解决挑战，具体包括设计了一个可扩展的基于锚点词的提示调优（APT）范式，并将记忆演示模板（MDT）融入每个训练实例中。

Result: 实验表明，我们的方法在FS-CLNER任务中表现优异，能够有效避免少样本知识蒸馏困境并促进上下文学习。

Conclusion: 我们的方法在FS-CLNER任务中表现出色，能够有效解决少样本知识蒸馏困境。

Abstract: Knowledge distillation has been successfully applied to Continual Learning
Named Entity Recognition (CLNER) tasks, by using a teacher model trained on
old-class data to distill old-class entities present in new-class data as a
form of regularization, thereby avoiding catastrophic forgetting. However, in
Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it
difficult for the trained model to generalize during inference. More
critically, the lack of old-class entity information hinders the distillation
of old knowledge, causing the model to fall into what we refer to as the
Few-Shot Distillation Dilemma. In this work, we address the above challenges
through a prompt tuning paradigm and memory demonstration template strategy.
Specifically, we designed an expandable Anchor words-oriented Prompt Tuning
(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby
enhancing performance in few-shot scenarios. Additionally, we incorporated
Memory Demonstration Templates (MDT) into each training instance to provide
replay samples from previous tasks, which not only avoids the Few-Shot
Distillation Dilemma but also promotes in-context learning. Experiments show
that our approach achieves competitive performances on FS-CLNER.

</details>


### [40] [The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation](https://arxiv.org/abs/2508.07262)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: 本文扩展了二维动态发音模型DYNARTmo，引入了三维软腭穹顶表示，以更准确地估计舌头-软腭接触区域，并生成电软腭图类似的可视化效果。


<details>
  <summary>Details</summary>
Motivation: 为了生成类似于电软腭图的可视化效果，并在2D+框架内分析舌头-软腭接触区域，需要改进现有的二维动态发音模型。

Method: 通过集成内部三维软腭穹顶表示，扩展了二维动态发音模型DYNARTmo，以从中矢状面舌头轮廓估计舌头-软腭接触面积。实现了两种替代的穹顶几何形状——半椭圆和基于余弦的轮廓，以模拟冠状面的侧向曲率。

Result: 通过这些几何形状，可以为每个前后位置计算侧向接触点，从而在2D+框架内生成电软腭图类似的可视化效果。增强的模型支持静态和动态（动画）发音显示的三种同步视图。

Conclusion: 该模型支持三种同步视图（矢状面、声门和软腭），适用于语音科学教育和语音治疗。未来的工作包括添加面部（嘴唇）视图并实现发音到声学的合成以定量评估模型的真实性。

Abstract: This paper describes an extension of the two-dimensional dynamic articulatory
model DYNARTmo by integrating an internal three-dimensional representation of
the palatal dome to estimate tongue-palate contact areas from midsagittal
tongue contours. Two alternative dome geometries - a half-ellipse and a cosine
based profile - are implemented to model lateral curvature in the coronal
plane. Using these geometries, lateral contact points are analytically computed
for each anterior-posterior position, enabling the generation of
electropalatography-like visualizations within the 2D+ framework. The enhanced
model supports three synchronized views (sagittal, glottal, and palatal) for
static and dynamic (animated) articulation displays, suitable for speech
science education and speech therapy. Future work includes adding a facial
(lip) view and implementing articulatory-to-acoustic synthesis to
quantitatively evaluate model realism.

</details>


### [41] [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)
*Qiongqiong Wang,Hardik B. Sailor,Jeremy H. M. Wong,Tianchi Liu,Shuo Sun,Wenyu Zhang,Muhammad Huzaifah,Nancy Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 本文提出两种方法将上下文语调信息融入模型训练，隐式方法在人类标注的QA基准测试中提升了38.41%的性能，结合显式方法后达到46.02%，显示了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语音语言模型（Speech-LLMs）在共情推理方面存在局限性，主要是由于缺乏同时包含上下文内容和语调线索的训练数据集。

Method: 我们提出了两种方法将上下文语调信息融入模型训练：(1) 显式方法，直接向LLM提供语调元数据（例如情感注释）；(2) 隐式方法，利用类别和维度情感注释以及语音转录自动生成新的训练问答对。

Result: 我们的隐式方法在人类标注的QA基准测试中提升了38.41%的性能，结合显式方法后达到了46.02%。

Conclusion: 我们的隐式方法在人类标注的QA基准测试中提升了38.41%的性能，结合显式方法后达到了46.02%，显示出在上下文语调理解方面的有效性。我们还通过展示LLM评判者与分类指标的相关性来验证其可靠性。

Abstract: Current large speech language models (Speech-LLMs) often exhibit limitations
in empathetic reasoning, primarily due to the absence of training datasets that
integrate both contextual content and paralinguistic cues. In this work, we
propose two approaches to incorporate contextual paralinguistic information
into model training: (1) an explicit method that provides paralinguistic
metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit
method that automatically generates novel training question-answer (QA) pairs
using both categorical and dimensional emotion annotations alongside speech
transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41%
on a human-annotated QA benchmark, reaching 46.02% when combined with the
explicit approach, showing effectiveness in contextual paralinguistic
understanding. We also validate the LLM judge by demonstrating its correlation
with classification metrics, providing support for its reliability.

</details>


### [42] [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
*Vasudha Varadarajan,Hui Xu,Rebecca Astrid Boehme,Mariam Marlan Mirstrom,Sverker Sikstrom,H. Andrew Schwartz*

Main category: cs.CL

TL;DR: MAQuA is an adaptive question-asking framework for mental health screening that uses multi-outcome modeling, IRT, and factor analysis to reduce the number of questions needed for accurate assessment.


<details>
  <summary>Details</summary>
Motivation: Recent advances in large language models (LLMs) offer new opportunities for scalable, interactive mental health assessment, but excessive querying by LLMs burdens users and is inefficient for real-world screening across transdiagnostic symptom profiles.

Method: MAQuA combines multi-outcome modeling on language responses with item response theory (IRT) and factor analysis to select the most informative questions across multiple dimensions at each turn.

Result: Empirical results on a novel dataset reveal that MAQuA reduces the number of assessment questions required for score stabilization by 50-87% compared to random ordering, demonstrating robust performance across both internalizing and externalizing domains.

Conclusion: MAQuA is a powerful and efficient tool for scalable, nuanced, and interactive mental health screening, advancing the integration of LLM-based agents into real-world clinical workflows.

Abstract: Recent advances in large language models (LLMs) offer new opportunities for
scalable, interactive mental health assessment, but excessive querying by LLMs
burdens users and is inefficient for real-world screening across
transdiagnostic symptom profiles. We introduce MAQuA, an adaptive
question-asking framework for simultaneous, multidimensional mental health
screening. Combining multi-outcome modeling on language responses with item
response theory (IRT) and factor analysis, MAQuA selects the questions with
most informative responses across multiple dimensions at each turn to optimize
diagnostic information, improving accuracy and potentially reducing response
burden. Empirical results on a novel dataset reveal that MAQuA reduces the
number of assessment questions required for score stabilization by 50-87%
compared to random ordering (e.g., achieving stable depression scores with 71%
fewer questions and eating disorder scores with 85% fewer questions). MAQuA
demonstrates robust performance across both internalizing (depression, anxiety)
and externalizing (substance use, eating disorder) domains, with early stopping
strategies further reducing patient time and burden. These findings position
MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive
mental health screening, advancing the integration of LLM-based agents into
real-world clinical workflows.

</details>


### [43] ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)
*Junchen Ding,Penghao Jiang,Zihao Xu,Ziqi Ding,Yichen Zhu,Jiaojiao Jiang,Yuekang Li*

Main category: cs.CL

TL;DR: 本研究评估了14个领先的大型语言模型在27种不同的电车问题场景中的表现，发现模型在不同道德框架下的行为存在显著差异，建议将道德推理作为大语言模型对齐的重要指标。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地参与伦理敏感决策，理解它们的道德推理过程变得至关重要。

Method: 本研究通过因子提示协议，收集了3,780个二元决策和自然语言解释，分析了大语言模型在不同道德哲学框架下的表现。

Result: 研究发现，不同道德框架和模型类型之间存在显著差异：增强推理的模型表现出更高的决断力和结构化的解释，但并不总是与人类共识更一致。在利他主义、公平和美德伦理框架中出现了“甜蜜区域”，模型在这些区域实现了高干预率、低解释冲突和最小的与集体人类判断的偏离。然而，在强调亲属关系、法律或自我利益的框架下，模型产生了伦理上有争议的结果。

Conclusion: 研究建议将道德推理作为大语言模型对齐的主要轴线，并呼吁建立标准化基准，不仅评估大语言模型的决策，还评估其决策的方式和原因。

Abstract: As large language models (LLMs) increasingly mediate ethically sensitive
decisions, understanding their moral reasoning processes becomes imperative.
This study presents a comprehensive empirical evaluation of 14 leading LLMs,
both reasoning enabled and general purpose, across 27 diverse trolley problem
scenarios, framed by ten moral philosophies, including utilitarianism,
deontology, and altruism. Using a factorial prompting protocol, we elicited
3,780 binary decisions and natural language justifications, enabling analysis
along axes of decisional assertiveness, explanation answer consistency, public
moral alignment, and sensitivity to ethically irrelevant cues. Our findings
reveal significant variability across ethical frames and model types: reasoning
enhanced models demonstrate greater decisiveness and structured justifications,
yet do not always align better with human consensus. Notably, "sweet zones"
emerge in altruistic, fairness, and virtue ethics framings, where models
achieve a balance of high intervention rates, low explanation conflict, and
minimal divergence from aggregated human judgments. However, models diverge
under frames emphasizing kinship, legality, or self interest, often producing
ethically controversial outcomes. These patterns suggest that moral prompting
is not only a behavioral modifier but also a diagnostic tool for uncovering
latent alignment philosophies across providers. We advocate for moral reasoning
to become a primary axis in LLM alignment, calling for standardized benchmarks
that evaluate not just what LLMs decide, but how and why.

</details>


### [44] [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
*Jian Chen,Jinbao Tian,Yankui Li,Zhou Li*

Main category: cs.CL

TL;DR: ARCE is a novel approach that uses an LLM to generate simple explanations, which are then used to pre-train a RoBERTa model for improved NER in AEC texts, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Standard pre-trained models struggle with the domain gap in AEC texts due to specialized terminology and complex relational contexts. Further pre-training on human-curated corpora is labor-intensive and costly, so leveraging LLMs for automated knowledge generation is a promising alternative.

Method: ARCE employs an LLM to generate a corpus of simple, direct explanations (Cote), which is then used to incrementally pre-train a RoBERTa model before fine-tuning on the downstream task.

Result: ARCE achieves a Macro-F1 score of 77.20% on a benchmark AEC dataset, outperforming previous methods. Simple, explanation-based knowledge is more effective than complex, role-based rationales.

Conclusion: ARCE establishes a new state-of-the-art on a benchmark AEC dataset, achieving a Macro-F1 score of 77.20%. Simple, explanation-based knowledge proves more effective than complex, role-based rationales for this task.

Abstract: Accurate information extraction from specialized texts is a critical
challenge, particularly for named entity recognition (NER) in the architecture,
engineering, and construction (AEC) domain to support automated rule checking
(ARC). The performance of standard pre-trained models is often constrained by
the domain gap, as they struggle to interpret the specialized terminology and
complex relational contexts inherent in AEC texts. Although this issue can be
mitigated by further pre-training on large, human-curated domain corpora, as
exemplified by methods like ARCBERT, this approach is both labor-intensive and
cost-prohibitive. Consequently, leveraging large language models (LLMs) for
automated knowledge generation has emerged as a promising alternative. However,
the optimal strategy for generating knowledge that can genuinely enhance
smaller, efficient models remains an open question. To address this, we propose
ARCE (augmented RoBERTa with contextualized elucidations), a novel approach
that systematically explores and optimizes this generation process. ARCE
employs an LLM to first generate a corpus of simple, direct explanations, which
we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa
model prior to its fine-tuning on the downstream task. Our extensive
experiments show that ARCE establishes a new state-of-the-art on a benchmark
AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a
key finding: simple, explanation-based knowledge proves surprisingly more
effective than complex, role-based rationales for this task. The code is
publicly available at:https://github.com/nxcc-lab/ARCE.

</details>


### [45] [CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation](https://arxiv.org/abs/2508.07295)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Zheng Chu,Bo Yang,Xiaocheng Feng,Yang Xiang,Ming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的跨语言和跨模态事实性基准（CCFQA），用于评估多模态大型语言模型在多语言输入（特别是语音）方面的可靠性。实验结果显示，当前的MLLMs在该基准测试中仍面临挑战，并提出了一种有效的少样本迁移学习策略，以提高多语言口语问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的评估多模态大型语言模型（MLLMs）可靠性的基准主要集中在文本或视觉模态上，并且主要关注英语，这在处理多语言输入时特别是语音方面存在评估差距。

Method: 我们提出了一个少样本迁移学习策略，该策略有效地将英语中的问答（QA）能力转移到多语言口语问答（SQA）任务中。

Result: 我们的实验结果表明，当前的MLLMs在CCFQA基准测试中仍然面临重大挑战。此外，我们提出的少样本迁移学习策略在仅使用5个样本训练的情况下，实现了与GPT-4o-mini-Audio相当的性能。

Conclusion: 我们释放了CCFQA作为一个基础研究资源，以促进具有更强大和可靠语音理解能力的MLLMs的发展。

Abstract: As Large Language Models (LLMs) are increasingly popularized in the
multilingual world, ensuring hallucination-free factuality becomes markedly
crucial. However, existing benchmarks for evaluating the reliability of
Multimodal Large Language Models (MLLMs) predominantly focus on textual or
visual modalities with a primary emphasis on English, which creates a gap in
evaluation when processing multilingual input, especially in speech. To bridge
this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal
\textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA
benchmark contains parallel speech-text factual questions across 8 languages,
designed to systematically evaluate MLLMs' cross-lingual and cross-modal
factuality capabilities. Our experimental results demonstrate that current
MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we
propose a few-shot transfer learning strategy that effectively transfers the
Question Answering (QA) capabilities of LLMs in English to multilingual Spoken
Question Answering (SQA) tasks, achieving competitive performance with
GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a
foundational research resource to promote the development of MLLMs with more
robust and reliable speech understanding capabilities. Our code and dataset are
available at https://github.com/yxduir/ccfqa.

</details>


### [46] [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)
*Cristian Cosentino,Annamaria Defilippo,Marco Dossena,Christopher Irwin,Sara Joubbi,Pietro Liò*

Main category: cs.CL

TL;DR: HealthBranches is a novel benchmark dataset for medical Q&A, designed to evaluate complex reasoning in LLMs. It is generated through a semi-automated pipeline and covers 4,063 case studies across 17 healthcare topics.


<details>
  <summary>Details</summary>
Motivation: To evaluate complex reasoning in Large Language Models (LLMs) and provide a benchmark dataset for medical Question-Answering (Q&A).

Method: HealthBranches is generated through a semi-automated pipeline that transforms explicit decision pathways from medical sources into realistic patient cases with associated questions and answers.

Result: HealthBranches covers 4,063 case studies across 17 healthcare topics, each data point based on clinically validated reasoning chains. It supports both open-ended and multiple-choice question formats and includes the full reasoning path for each Q&A.

Conclusion: HealthBranches establishes a foundation for the development of more trustworthy, interpretable, and clinically reliable LLMs in high-stakes domains while also serving as a valuable resource for educational purposes.

Abstract: HealthBranches is a novel benchmark dataset for medical Question-Answering
(Q&A), specifically designed to evaluate complex reasoning in Large Language
Models (LLMs). This dataset is generated through a semi-automated pipeline that
transforms explicit decision pathways from medical source into realistic
patient cases with associated questions and answers. Covering 4,063 case
studies across 17 healthcare topics, each data point is based on clinically
validated reasoning chains. HealthBranches supports both open-ended and
multiple-choice question formats and uniquely includes the full reasoning path
for each Q&A. Its structured design enables robust evaluation of LLMs'
multi-step inference capabilities, including their performance in structured
Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a
foundation for the development of more trustworthy, interpretable, and
clinically reliable LLMs in high-stakes domains while also serving as a
valuable resource for educational purposes.

</details>


### [47] [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)
*Shubhra Ghosh,Abhilekh Borah,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 本文提出了一种名为ObfusQAte的新技术，并引入了ObfusQA框架，用于评估大型语言模型在面对混淆问题时的鲁棒性和适应性。研究发现，LLMs在面对这些复杂变化时容易失败或产生幻觉响应。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在公平AI系统的开发中发挥了重要作用，但尚无已知的研究测试LLMs在面对问题的混淆版本时的鲁棒性。

Method: 提出了一种新的技术ObfusQAte，并利用它引入了ObfusQA，这是一个全面的、首创的框架，具有多级混淆级别，旨在检查大型语言模型在三个不同维度上的能力：(i) 命名实体误导，(ii) 分心误导，(iii) 上下文过载。

Result: ObfusQA提供了一个全面的基准，用于评估LLM的鲁棒性和适应性。研究观察到，当面对这些越来越细微的变化时，LLMs表现出失败或生成幻觉响应的趋势。

Conclusion: 研究发现，当面对这些日益复杂的变体时，大型语言模型倾向于失败或生成幻觉响应。为了促进这一方向的研究，ObfusQAte已被公开。

Abstract: The rapid proliferation of Large Language Models (LLMs) has significantly
contributed to the development of equitable AI systems capable of factual
question-answering (QA). However, no known study tests the LLMs' robustness
when presented with obfuscated versions of questions. To systematically
evaluate these limitations, we propose a novel technique, ObfusQAte and,
leveraging the same, introduce ObfusQA, a comprehensive, first of its kind,
framework with multi-tiered obfuscation levels designed to examine LLM
capabilities across three distinct dimensions: (i) Named-Entity Indirection,
(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these
fine-grained distinctions in language, ObfusQA provides a comprehensive
benchmark for evaluating LLM robustness and adaptability. Our study observes
that LLMs exhibit a tendency to fail or generate hallucinated responses when
confronted with these increasingly nuanced variations. To foster research in
this direction, we make ObfusQAte publicly available.

</details>


### [48] [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)
*Dean Geckt,Melinda Fricke,Shuly Wintner*

Main category: cs.CL

TL;DR: 本文研究了代码切换在双语语言使用中的应用，通过开发一个能够进行代码切换的聊天机器人，发现参与者更喜欢可预测的代码切换行为，而随机或不合法的代码切换会降低他们的参与度和任务完成效果。


<details>
  <summary>Details</summary>
Motivation: 大多数人是多语言者，而大多数多语言者会进行代码切换，但代码切换语言的特征尚未完全理解。

Method: 我们开发了一个聊天机器人，能够使用西班牙语和英语的代码切换与人类参与者完成地图任务。在两个实验中，我们促使机器人根据不同的策略进行代码切换，以研究双语语言使用的情况。

Result: 参与者通常喜欢与我们的机器人进行代码切换，只要它产生了可预测的代码切换行为；当代码切换是随机的或不合法的（例如产生未被证实的不一致的混合语言名词短语，如“la fork”），参与者对任务的喜爱程度较低，并且完成任务的成功率也较低。

Conclusion: 这些结果强调了部署不够完善的多语言语言技术的潜在缺点，同时也展示了这种技术在进行双语语言使用研究方面的潜力。

Abstract: Most people are multilingual, and most multilinguals code-switch, yet the
characteristics of code-switched language are not fully understood. We
developed a chatbot capable of completing a Map Task with human participants
using code-switched Spanish and English. In two experiments, we prompted the
bot to code-switch according to different strategies, examining (1) the
feasibility of such experiments for investigating bilingual language use, and
(2) whether participants would be sensitive to variations in discourse and
grammatical patterns. Participants generally enjoyed code-switching with our
bot as long as it produced predictable code-switching behavior; when
code-switching was random or ungrammatical (as when producing unattested
incongruent mixed-language noun phrases, such as `la fork'), participants
enjoyed the task less and were less successful at completing it. These results
underscore the potential downsides of deploying insufficiently developed
multilingual language technology, while also illustrating the promise of such
technology for conducting research on bilingual language use.

</details>


### [49] [Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance](https://arxiv.org/abs/2508.07375)
*Wenqian Cui,Lei Zhu,Xiaohui Li,Zhihan Guo,Haoli Bai,Lu Hou,Irwin King*

Main category: cs.CL

TL;DR: 本文提出了一种名为TurnGuide的新方法，通过动态分割语音为对话回合并生成回合级文本指导，解决了全双工语音语言模型在对话能力上的挑战。


<details>
  <summary>Details</summary>
Motivation: 全双工语音语言模型在真实对话中面临对话能力退化的问题，因为语音序列过长和高质量口语对话数据有限。虽然文本引导的语音生成可以缓解这些问题，但在将文本指导集成到双通道音频流时存在时间与长度问题，破坏了自然交互所需的精确时间对齐。

Method: 我们提出了TurnGuide，这是一种基于规划的新方法，通过动态分割助手语音为对话回合并在语音输出前生成回合级文本指导，有效解决了插入时间和长度问题。

Result: 实验表明，我们的方法显著提高了端到端全双工语音语言模型的对话能力，使其能够生成语义有意义且连贯的语音，同时保持自然的对话流程。

Conclusion: 我们的方法显著提高了端到端全双工语音语言模型的对话能力，使其能够生成语义有意义且连贯的语音，同时保持自然的对话流程。

Abstract: Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation
models designed to enable natural, real-time spoken interactions by modeling
complex conversational dynamics such as interruptions, backchannels, and
overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world
double-channel conversational data to capture nuanced two-speaker dialogue
patterns for human-like interactions. However, they face a critical challenge
-- their conversational abilities often degrade compared to pure-text
conversation due to prolonged speech sequences and limited high-quality spoken
dialogue data. While text-guided speech generation could mitigate these issues,
it suffers from timing and length issues when integrating textual guidance into
double-channel audio streams, disrupting the precise time alignment essential
for natural interactions. To address these challenges, we propose TurnGuide, a
novel planning-inspired approach that mimics human conversational planning by
dynamically segmenting assistant speech into dialogue turns and generating
turn-level text guidance before speech output, which effectively resolves both
insertion timing and length challenges. Extensive experiments demonstrate our
approach significantly improves e2e FD-SLMs' conversational abilities, enabling
them to generate semantically meaningful and coherent speech while maintaining
natural conversational flow. Demos are available at
https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at
https://github.com/dreamtheater123/TurnGuide.

</details>


### [50] [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)
*Jean de Dieu Nyandwi,Yueqi Song,Simran Khanuja,Graham Neubig*

Main category: cs.CL

TL;DR: 本文提出了一种以数据为中心的方法，通过将多模态大型语言模型（MLLMs）直接与文化知识结合，解决其在处理长尾文化实体和低资源语言时的不足。我们构建了一个包含2200万对高质量、文化丰富的视觉问答数据集CulturalGround，并训练了一个名为CulturalPangea的开源MLLM。实验结果表明，CulturalPangea在多个文化相关的多语言多模态基准测试中表现优异，同时保持了主流视觉语言任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型在高资源环境中表现出色，但往往误解长尾文化实体，并且在低资源语言中表现不佳。

Method: 我们提出了一种以数据为中心的方法，直接将MLLMs与文化知识结合。利用来自Wikidata的大规模知识图谱，我们收集了代表文化重要实体的图像，并生成了合成的多语言视觉问答数据。

Result: 我们在CulturalGround上训练了一个开源的MLLM CulturalPangea，混合标准的多语言指令调优数据以保持一般能力。CulturalPangea在各种以文化为重点的多语言多模态基准测试中达到了最先进的性能，平均优于先前模型5.0，而不会影响主流视觉语言任务的结果。

Conclusion: 我们的研究结果表明，针对文化的、以文化为基础的方法可以显著缩小MLLMs中的文化差距，并为全球包容的多模态系统提供一条实用的路径。

Abstract: Multimodal Large Language Models excel in high-resource settings, but often
misinterpret long-tail cultural entities and underperform in low-resource
languages. To address this gap, we propose a data-centric approach that
directly grounds MLLMs in cultural knowledge. Leveraging a large scale
knowledge graph from Wikidata, we collect images that represent culturally
significant entities, and generate synthetic multilingual visual question
answering data. The resulting dataset, CulturalGround, comprises 22 million
high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.
We train an open-source MLLM CulturalPangea on CulturalGround, interleaving
standard multilingual instruction-tuning data to preserve general abilities.
CulturalPangea achieves state-of-the-art performance among open models on
various culture-focused multilingual multimodal benchmarks, outperforming prior
models by an average of 5.0 without degrading results on mainstream
vision-language tasks. Our findings show that our targeted, culturally grounded
approach could substantially narrow the cultural gap in MLLMs and offer a
practical path towards globally inclusive multimodal systems.

</details>


### [51] [Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs](https://arxiv.org/abs/2508.07434)
*Zhiyi Lyu,Jianguo Huang,Yanchen Deng,Steven Hoi,Bo An*

Main category: cs.CL

TL;DR: 本文提出了一种名为ReLoc的统一局部搜索框架，通过四个关键算法组件实现逐步代码修订，并开发了一个专门的修订奖励模型来评估代码质量。实验结果表明，该方法在多种代码生成任务中表现优异，超越了现有的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时使用缩放技术在代码生成方面显示出潜力，但面临着效率和可扩展性的挑战。基于构造的树搜索方法存在树大小迅速增长、高标记消耗和缺乏随时属性的问题。相比之下，基于改进的方法虽然性能更好，但常常面临无信息的奖励信号和低效的搜索策略问题。

Method: 我们提出了ReLoc，这是一个统一的局部搜索框架，通过四个关键算法组件进行逐步代码修订：初始代码起草、邻域代码生成、候选评估和当前代码更新。

Result: 我们的实验结果表明，我们的方法在各种代码生成任务中表现优异，显著优于基于构造的树搜索和最先进的基于改进的代码生成方法。

Conclusion: 我们的方法在各种代码生成任务中表现出色，显著优于基于构造的树搜索和最先进的基于改进的代码生成方法。

Abstract: Large Language Models (LLMs) with inference-time scaling techniques show
promise for code generation, yet face notable efficiency and scalability
challenges. Construction-based tree-search methods suffer from rapid growth in
tree size, high token consumption, and lack of anytime property. In contrast,
improvement-based methods offer better performance but often struggle with
uninformative reward signals and inefficient search strategies. In this work,
we propose \textbf{ReLoc}, a unified local search framework which effectively
performs step-by-step code revision. Specifically, ReLoc explores a series of
local revisions through four key algorithmic components: initial code drafting,
neighborhood code generation, candidate evaluation, and incumbent code
updating, each of which can be instantiated with specific decision rules to
realize different local search algorithms such as Hill Climbing (HC) or Genetic
Algorithm (GA). Furthermore, we develop a specialized revision reward model
that evaluates code quality based on revision distance to produce fine-grained
preferences that guide the local search toward more promising candidates.
Finally, our extensive experimental results demonstrate that our approach
achieves superior performance across diverse code generation tasks,
significantly outperforming both construction-based tree search as well as the
state-of-the-art improvement-based code generation methods.

</details>


### [52] [Positional Biases Shift as Inputs Approach Context Window Limits](https://arxiv.org/abs/2508.07479)
*Blerta Veseli,Julian Chibane,Mariya Toneva,Alexander Koller*

Main category: cs.CL

TL;DR: 研究分析了大型语言模型在长输入中的位置偏差，发现当输入占据模型上下文窗口的50%时，LiM效应最强。超过这个比例后，首因偏差减弱，而近因偏差保持稳定，这消除了LiM效应，转而出现基于距离的偏差。此外，研究指出成功的检索是推理的前提，位置偏差主要来自检索。这些发现对长上下文任务和LLM评估方法有重要影响。


<details>
  <summary>Details</summary>
Motivation: 先前的研究发现了位置偏差，如中间迷失（LiM）效应，但长期上下文研究并未一致地复制这些效应，这引发了对其强度和显现条件的疑问。因此，本研究旨在通过相对输入长度进行分析，以解决这些问题。

Method: 研究使用相对而非绝对输入长度进行分析，并根据每个模型的上下文窗口进行定义。

Result: 研究发现，当输入占据模型上下文窗口的50%时，LiM效应最强。超过这个比例后，首因偏差减弱，而近因偏差保持相对稳定，这有效地消除了LiM效应；相反，我们观察到一种基于距离的偏差，即相关信息越接近输入末尾，模型性能越好。此外，结果表明成功的检索是推理的前提，观察到的位置偏差主要继承自检索。

Conclusion: 研究发现，当输入占据模型上下文窗口的50%时，LiM效应最强。超过这个比例后，首因偏差减弱，而近因偏差保持相对稳定，这有效地消除了LiM效应；相反，我们观察到一种基于距离的偏差，即相关信息越接近输入末尾，模型性能越好。此外，结果表明成功的检索是推理的前提，观察到的位置偏差主要继承自检索。这些见解对长上下文任务、未来LLM基准的设计以及处理扩展输入的LLM评估方法具有影响。

Abstract: Large Language Models (LLMs) often struggle to use information across long
inputs effectively. Prior work has identified positional biases, such as the
Lost in the Middle (LiM) effect, where models perform better when information
appears at the beginning (primacy bias) or end (recency bias) of the input,
rather than in the middle. However, long-context studies have not consistently
replicated these effects, raising questions about their intensity and the
conditions under which they manifest. To address this, we conducted a
comprehensive analysis using relative rather than absolute input lengths,
defined with respect to each model's context window. Our findings reveal that
the LiM effect is strongest when inputs occupy up to 50% of a model's context
window. Beyond that, the primacy bias weakens, while recency bias remains
relatively stable. This effectively eliminates the LiM effect; instead, we
observe a distance-based bias, where model performance is better when relevant
information is closer to the end of the input. Furthermore, our results suggest
that successful retrieval is a prerequisite for reasoning in LLMs, and that the
observed positional biases in reasoning are largely inherited from retrieval.
These insights have implications for long-context tasks, the design of future
LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

</details>


### [53] [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)
*Archchana Sindhujan,Shenbin Qian,Chan Chi Chun Matthew,Constantin Orasan,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 本文提出了一种名为ALOPE的自适应层优化框架，旨在通过逐层适应改进Transformer表示来增强基于大语言模型（LLM）的质量评估（QE）。该框架结合了低秩适配器（LoRA）和回归任务头，并引入了动态加权和多头回归策略。实验结果表明，该框架在多种现有的基于LLM的QE方法上表现出色，并且中间Transformer层提供了更符合QE任务跨语言特性的上下文表示。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的QE系统存在固有局限性，因为它们是为因果语言建模预训练的，而不是针对回归任务。此外，由于预训练数据分布中存在低资源语言，这使得问题更加复杂。因此，需要一种新的方法来提高基于LLM的QE性能。

Method: ALOPE框架结合了低秩适配器（LoRA）和回归任务头，利用选定的预训练Transformer层来提高跨语言对齐。此外，ALOPE引入了两种策略：动态加权，用于自适应地组合多个层的表示；多头回归，用于聚合多个头的回归损失以进行QE。

Result: ALOPE框架在多种现有的基于LLM的QE方法上取得了改进。实证证据表明，LLM中的中间Transformer层提供了更符合QE任务跨语言特性的上下文表示。

Conclusion: 本文提出了一种名为ALOPE的自适应层优化框架，旨在通过逐层适应改进Transformer表示来增强基于大语言模型（LLM）的质量评估（QE）。实验结果表明，该框架在多种现有的基于LLM的QE方法上表现出色，并且中间Transformer层提供了更符合QE任务跨语言特性的上下文表示。

Abstract: Large Language Models (LLMs) have shown remarkable performance across a wide
range of natural language processing tasks. Quality Estimation (QE) for Machine
Translation (MT), which assesses the quality of a source-target pair without
relying on reference translations, remains a challenging cross-lingual task for
LLMs. The challenges stem from the inherent limitations of existing LLM-based
QE systems, which are pre-trained for causal language modelling rather than
regression-specific tasks, further elevated by the presence of low-resource
languages given pre-training data distribution. This paper introduces ALOPE, an
adaptive layer-optimization framework designed to enhance LLM-based QE by
restructuring Transformer representations through layer-wise adaptation for
improved regression-based prediction. Our framework integrates low-rank
adapters (LoRA) with regression task heads, leveraging selected pre-trained
Transformer layers for improved cross-lingual alignment. In addition to the
layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,
which adaptively combines representations from multiple layers, and multi-head
regression, which aggregates regression losses from multiple heads for QE. Our
framework shows improvements over various existing LLM-based QE approaches.
Empirical evidence suggests that intermediate Transformer layers in LLMs
provide contextual representations that are more aligned with the cross-lingual
nature of the QE task. We make resultant models and framework code publicly
available for further research, also allowing existing LLM-based MT frameworks
to be scaled with QE capabilities.

</details>


### [54] [Augmenting Bias Detection in LLMs Using Topological Data Analysis](https://arxiv.org/abs/2508.07516)
*Keshav Varadarajan,Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: 该研究利用拓扑数据分析方法，识别出GPT-2中导致对特定群体偏见的注意力头，并指出未来可扩展此方法用于去偏大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 目前，测试哪些部分的大型语言模型会导致对特定群体的偏见的方法仍然不成熟。

Method: 该研究使用了拓扑数据分析方法，以确定GPT-2中哪些注意力头导致了对特定群体的偏见。

Result: 研究发现，特定类别（如性别或职业）的偏见集中在作为热点的注意力头中。提出的度量标准还可以用来确定哪些头捕捉到特定组别的偏见。

Conclusion: 该研究提出了一种使用拓扑数据分析的方法来识别GPT-2中导致对特定群体的偏见的注意力头，并指出未来的工作可以扩展这种方法以帮助去偏大型语言模型。

Abstract: Recently, many bias detection methods have been proposed to determine the
level of bias a large language model captures. However, tests to identify which
parts of a large language model are responsible for bias towards specific
groups remain underdeveloped. In this study, we present a method using
topological data analysis to identify which heads in GPT-2 contribute to the
misrepresentation of identity groups present in the StereoSet dataset. We find
that biases for particular categories, such as gender or profession, are
concentrated in attention heads that act as hot spots. The metric we propose
can also be used to determine which heads capture bias for a specific group
within a bias category, and future work could extend this method to help
de-bias large language models.

</details>


### [55] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
*Joseph T. Colonel,Baihan Lin*

Main category: cs.CL

TL;DR: 本文介绍了一个名为ThemeClouds的工具，它利用大型语言模型从对话中生成主题性、参与者加权的词云，相较于传统方法和主题建模基线，能够更有效地揭示设备相关问题，并讨论了其设计权衡、可解释性和研究者自主性的影响。


<details>
  <summary>Details</summary>
Motivation: 传统基于频率的词云方法在对话环境中效果不佳，因为它们会突出填充词、忽略同义表达，并且会分割语义相关的观点，这限制了其在早期分析阶段的实用性，此时研究人员需要快速且可解释的概述。

Method: 本文提出了一种名为ThemeClouds的开源可视化工具，该工具使用大型语言模型（LLMs）来识别语料库中的概念级主题，并统计每个主题被多少个不同的参与者提及，从而生成基于提及广度而非原始词频的可视化结果。

Result: 通过使用用户研究中的访谈数据（31名参与者；155个转录本，Whisper ASR），本文的方法比频率词云和主题建模基线（如LDA、BERTopic）更能揭示可操作的设备相关问题。

Conclusion: 本文介绍了ThemeClouds工具，该工具利用大型语言模型（LLMs）从对话转录本生成主题性、参与者加权的词云，相较于传统的基于频率的方法和主题建模基线（如LDA、BERTopic），能够更有效地揭示设备相关问题，并讨论了将LLM辅助集成到定性工作流程中的设计权衡、可解释性和研究者自主性的影响，以及交互式分析的机会。

Abstract: Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

</details>


### [56] [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)
*Jia Deng,Jie Chen,Zhipeng Chen,Daixuan Cheng,Fei Bai,Beichen Zhang,Yinqian Min,Yanzipeng Gao,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文系统地研究了RLVR中的探索能力，提出了量化指标并分析了熵-性能交换，旨在为推进RLVR系统提供基础框架。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的工作已经证明了RLVR在经验上的成功，但控制LLMs探索行为的基本机制仍然研究不足。因此，本文旨在深入探讨RLVR中的探索能力，以推动该领域的发展。

Method: 本文系统地研究了RLVR中的探索能力，涵盖了四个主要方面：(1) 探索空间塑造，(2) 熵-性能交换，(3) RL性能优化，以及(4) 探索能力的量化分析。

Result: 本文提出了量化指标来表征LLMs的能力边界，并分析了熵-性能交换，同时探讨了如何有效将探索收益转化为可衡量的改进。

Conclusion: 本文旨在为推进RLVR系统提供一个基础框架，通过统一之前识别的见解和新的实证证据，系统地研究了RLVR中的探索能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.

</details>


### [57] [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)
*Puspesh Kumar Srivastava,Uddeshya Raj,Praveen Patel,/Shubham Kumar Nigam,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 本文介绍了印度保释预测系统（IBPS），这是一个基于人工智能的框架，旨在通过预测结果和生成合法合理的理由来协助保释决策。研究者创建并发布了包含150,430个高等法院保释判决的大规模数据集，并使用参数高效技术微调大型语言模型。结果显示，结合法律知识的模型显著优于基线模型，具有高准确性和解释质量，并能很好地推广到由法律专家独立注释的测试集。IBPS提供了一种透明、可扩展和可重复的解决方案，以支持数据驱动的法律援助，减少保释延迟，并促进印度司法系统的程序公平。


<details>
  <summary>Details</summary>
Motivation: Bail decisions in Indian courts are plagued by subjectivity, delays, and inconsistencies. The lack of timely and fair bail adjudication exacerbates human rights concerns and contributes to systemic judicial backlog.

Method: We curate and release a large-scale dataset of 150,430 High Court bail judgments, enriched with structured annotations. We fine-tune a large language model using parameter-efficient techniques and evaluate its performance across multiple configurations, with and without statutory context, and with RAG.

Result: Models fine-tuned with statutory knowledge significantly outperform baselines, achieving strong accuracy and explanation quality, and generalize well to a test set independently annotated by legal experts.

Conclusion: IBPS offers a transparent, scalable, and reproducible solution to support data-driven legal assistance, reduce bail delays, and promote procedural fairness in the Indian judicial system.

Abstract: Bail decisions are among the most frequently adjudicated matters in Indian
courts, yet they remain plagued by subjectivity, delays, and inconsistencies.
With over 75% of India's prison population comprising undertrial prisoners,
many from socioeconomically disadvantaged backgrounds, the lack of timely and
fair bail adjudication exacerbates human rights concerns and contributes to
systemic judicial backlog. In this paper, we present the Indian Bail Prediction
System (IBPS), an AI-powered framework designed to assist in bail
decision-making by predicting outcomes and generating legally sound rationales
based solely on factual case attributes and statutory provisions. We curate and
release a large-scale dataset of 150,430 High Court bail judgments, enriched
with structured annotations such as age, health, criminal history, crime
category, custody duration, statutes, and judicial reasoning. We fine-tune a
large language model using parameter-efficient techniques and evaluate its
performance across multiple configurations, with and without statutory context,
and with RAG. Our results demonstrate that models fine-tuned with statutory
knowledge significantly outperform baselines, achieving strong accuracy and
explanation quality, and generalize well to a test set independently annotated
by legal experts. IBPS offers a transparent, scalable, and reproducible
solution to support data-driven legal assistance, reduce bail delays, and
promote procedural fairness in the Indian judicial system.

</details>


### [58] [Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements](https://arxiv.org/abs/2508.07598)
*Ziheng Li,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: 本文提出KeyCP++方法，通过关键词引导的思维链提示，提高LLM在单次事件检测任务中的性能。


<details>
  <summary>Details</summary>
Motivation: LLM在事件检测任务中存在对事件触发器理解不准确和过度解释的问题，这无法仅通过上下文示例有效纠正。

Method: KeyCP++通过自动标注输入文本与检测结果之间的逻辑差距，构建了一个触发器区分提示模板，并将示例触发器（即关键词）作为锚点来触发分析，让LLM提出候选触发器并进行验证。

Result: 实验结果表明，KeyCP++方法在单次事件检测任务中表现出色，展示了显著的进步。

Conclusion: 本文提出了一种基于关键词的思维链提示方法KeyCP++，以解决LLM在事件检测任务中的挑战。实验表明，该方法在单次事件检测中表现出显著的进展。

Abstract: Although the LLM-based in-context learning (ICL) paradigm has demonstrated
considerable success across various natural language processing tasks, it
encounters challenges in event detection. This is because LLMs lack an accurate
understanding of event triggers and tend to make over-interpretation, which
cannot be effectively corrected through in-context examples alone. In this
paper, we focus on the most challenging one-shot setting and propose KeyCP++, a
keyword-centric chain-of-thought prompting approach. KeyCP++ addresses the
weaknesses of conventional ICL by automatically annotating the logical gaps
between input text and detection results for the demonstrations. Specifically,
to generate in-depth and meaningful rationale, KeyCP++ constructs a trigger
discrimination prompting template. It incorporates the exemplary triggers
(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let
LLM propose candidate triggers, and justify each candidate. These
propose-and-judge rationales help LLMs mitigate over-reliance on the keywords
and promote detection rule learning. Extensive experiments demonstrate the
effectiveness of our approach, showcasing significant advancements in one-shot
event detection.

</details>


### [59] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: InterChart is a benchmark that evaluates how well vision-language models reason across multiple related charts. It reveals that models struggle with complex chart interactions and highlights the need for improved multimodal reasoning in real-world applications.


<details>
  <summary>Details</summary>
Motivation: The task of reasoning across multiple related charts is central to real-world applications such as scientific reporting, financial analysis, and public policy dashboards. Prior benchmarks have focused on isolated, visually uniform charts, leaving a gap in evaluating models' ability to handle diverse question types and complex chart interactions.

Method: InterChart is a diagnostic benchmark that evaluates how well vision-language models reason across multiple related charts. It organizes the benchmark into three tiers of increasing difficulty: factual reasoning over individual charts, integrative analysis across synthetically aligned chart sets, and semantic inference over visually complex, real-world chart pairs.

Result: Evaluation of state-of-the-art open and closed-source VLMs reveals consistent and steep accuracy declines as chart complexity increases. Models perform better when multi-entity charts are decomposed into simpler visual units, highlighting their struggles with cross-chart integration.

Conclusion: InterChart provides a rigorous framework for advancing multimodal reasoning in complex, multi-visual environments by exposing systematic limitations of current vision-language models.

Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well
vision-language models (VLMs) reason across multiple related charts, a task
central to real-world applications such as scientific reporting, financial
analysis, and public policy dashboards. Unlike prior benchmarks focusing on
isolated, visually uniform charts, InterChart challenges models with diverse
question types ranging from entity inference and trend correlation to numerical
estimation and abstract multi-step reasoning grounded in 2-3 thematically or
structurally related charts. We organize the benchmark into three tiers of
increasing difficulty: (1) factual reasoning over individual charts, (2)
integrative analysis across synthetically aligned chart sets, and (3) semantic
inference over visually complex, real-world chart pairs. Our evaluation of
state-of-the-art open and closed-source VLMs reveals consistent and steep
accuracy declines as chart complexity increases. We find that models perform
better when we decompose multi-entity charts into simpler visual units,
underscoring their struggles with cross-chart integration. By exposing these
systematic limitations, InterChart provides a rigorous framework for advancing
multimodal reasoning in complex, multi-visual environments.

</details>


### [60] [LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval](https://arxiv.org/abs/2508.07690)
*Luyao Zhuang,Qinggang Zhang,Huachi Zhou,Juhua Liu,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: LoSemB is a novel framework for inductive tool retrieval that addresses the challenges of unseen tools by leveraging logical information and improving retrieval mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods in handling unseen tools, which suffer from large distribution shifts and vulnerability of similarity-based retrieval.

Method: LoSemB, a Logic-Guided Semantic Bridging framework for inductive tool retrieval, which contains a logic-based embedding alignment module and a relational augmented retrieval mechanism.

Result: LoSemB achieves advanced performance in inductive settings while maintaining desirable effectiveness in the transductive setting.

Conclusion: LoSemB achieves advanced performance in inductive settings while maintaining desirable effectiveness in the transductive setting.

Abstract: Tool learning has emerged as a promising paradigm for large language models
(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository
rapidly expanding, it is impractical to contain all tools within the limited
input length of LLMs. To alleviate these issues, researchers have explored
incorporating a tool retrieval module to select the most relevant tools or
represent tools as unique tokens within LLM parameters. However, most
state-of-the-art methods are under transductive settings, assuming all tools
have been observed during training. Such a setting deviates from reality as the
real-world tool repository is evolving and incorporates new tools frequently.
When dealing with these unseen tools, which refer to tools not encountered
during the training phase, these methods are limited by two key issues,
including the large distribution shift and the vulnerability of
similarity-based retrieval. To this end, inspired by human cognitive processes
of mastering unseen tools through discovering and applying the logical
information from prior experience, we introduce a novel Logic-Guided Semantic
Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to
mine and transfer latent logical information for inductive tool retrieval
without costly retraining. Specifically, LoSemB contains a logic-based
embedding alignment module to mitigate distribution shifts and implements a
relational augmented retrieval mechanism to reduce the vulnerability of
similarity-based retrieval. Extensive experiments demonstrate that LoSemB
achieves advanced performance in inductive settings while maintaining desirable
effectiveness in the transductive setting.

</details>


### [61] [What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction](https://arxiv.org/abs/2508.07702)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: 研究发现商业大语言模型在低结构化领域中预测被遮蔽句子的能力较差，揭示了当前模型在全局连贯性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 研究商业大语言模型在长距离上下文预测方面的局限性，特别是在低结构化领域中的表现。

Method: 评估三个商业大语言模型（GPT-4o、Claude 3.5 Sonnet 和 Gemini 2.0 Flash）在三个领域（ROCStories、Recipe1M 和 Wikipedia）的掩码句子预测任务中的表现，包括保真度和连贯性。

Result: 商业大语言模型在低结构化领域中预测被遮蔽句子的表现不佳，显示出模型在全局连贯性方面的不足。

Conclusion: 商业大语言模型在低结构化领域中预测被遮蔽句子的能力较差，这表明当前模型能力存在差距。

Abstract: Transformer-based models primarily rely on Next Token Prediction (NTP), which
predicts the next token in a sequence based on the preceding context. However,
NTP's focus on single-token prediction often limits a model's ability to plan
ahead or maintain long-range coherence, raising questions about how well LLMs
can predict longer contexts, such as full sentences within structured
documents. While NTP encourages local fluency, it provides no explicit
incentive to ensure global coherence across sentence boundaries-an essential
skill for reconstructive or discursive tasks. To investigate this, we evaluate
three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on
Masked Sentence Prediction (MSP) - the task of infilling a randomly removed
sentence - from three domains: ROCStories (narrative), Recipe1M (procedural),
and Wikipedia (expository). We assess both fidelity (similarity to the original
sentence) and cohesiveness (fit within the surrounding context). Our key
finding reveals that commercial LLMs, despite their superlative performance in
other tasks, are poor at predicting masked sentences in low-structured domains,
highlighting a gap in current model capabilities.

</details>


### [62] [Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models](https://arxiv.org/abs/2508.07753)
*Zhenliang Zhang,Junzhe Zhang,Xinyu Hu,HuiXuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本研究探讨了社会偏见与大语言模型的忠实性幻觉之间的因果关系，发现社会偏见是导致这种幻觉的重要原因。


<details>
  <summary>Details</summary>
Motivation: 研究社会偏见是否会导致大语言模型的忠实性幻觉，这是一个尚未被探索的因果关系。

Method: 利用结构因果模型（SCM）建立和验证因果关系，并设计偏见干预措施来控制混杂因素。此外，开发了偏见干预数据集（BID），以精确测量因果效应。

Result: 实验表明，偏见是忠实性幻觉的重要原因，且不同偏见状态的影响方向不同。进一步分析显示，社会偏见主要针对不公平幻觉，揭示了偏见对幻觉生成的微妙但显著的因果影响。

Conclusion: 社会偏见是导致忠实性幻觉的重要原因，且每种偏见状态的影响方向不同。

Abstract: Large language models (LLMs) have achieved remarkable success in various
tasks, yet they remain vulnerable to faithfulness hallucinations, where the
output does not align with the input. In this study, we investigate whether
social bias contributes to these hallucinations, a causal relationship that has
not been explored. A key challenge is controlling confounders within the
context, which complicates the isolation of causality between bias states and
hallucinations. To address this, we utilize the Structural Causal Model (SCM)
to establish and validate the causality and design bias interventions to
control confounders. In addition, we develop the Bias Intervention Dataset
(BID), which includes various social biases, enabling precise measurement of
causal effects. Experiments on mainstream LLMs reveal that biases are
significant causes of faithfulness hallucinations, and the effect of each bias
state differs in direction. We further analyze the scope of these causal
effects across various models, specifically focusing on unfairness
hallucinations, which are primarily targeted by social bias, revealing the
subtle yet significant causal effect of bias on hallucination generation.

</details>


### [63] [SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation](https://arxiv.org/abs/2508.07781)
*Zeyu Yang,Lai Wei,Roman Koshkin,Xi Chen,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出了一种基于语法的分块策略，并构建了一个端到端的SASST框架，以提高同时语音翻译的质量。实验结果表明，该方法在多语言语料库上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的同时语音翻译系统在翻译时间和内容优化方面存在挑战，尤其是在目标语言的词序差异方面。因此，需要一种能够有效利用语法结构的方法来提高翻译质量。

Method: 本文提出了一种基于语法的分块策略，通过解析依存关系和标点特征来分割输入流，以确保块的连贯性并最小化语义碎片。然后，提出了SASST（语法感知的同时语音翻译）框架，该框架集成了冻结的Whisper编码器和仅解码器的LLM。

Result: 在CoVoSt2多语言语料库En-{De, Zh, Ja}上的实验表明，该方法在各种语言中都显著提高了翻译质量，并验证了语法结构在LLM驱动的SimulST系统中的有效性。

Conclusion: 实验结果表明，语法结构在LLM驱动的SimulST系统中是有效的，并且在CoVoSt2多语言语料库上显著提高了翻译质量。

Abstract: This work proposes a grammar-based chunking strategy that segments input
streams into semantically complete units by parsing dependency relations (e.g.,
noun phrase boundaries, verb-object structures) and punctuation features. The
method ensures chunk coherence and minimizes semantic fragmentation. Building
on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech
Translation), an end-to-end framework integrating frozen Whisper encoder and
decoder-only LLM. The unified architecture dynamically outputs translation
tokens or <WAIT> symbols to jointly optimize translation timing and content,
with target-side reordering addressing word-order divergence. Experiments on
CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation
quality improvements across languages and validate the effectiveness of
syntactic structures in LLM-driven SimulST systems.

</details>


### [64] [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785)
*Haoyuan Wu,Haoxing Chen,Xiaodong Chen,Zhanchao Zhou,Tieyuan Chen,Yihong Zhuang,Guoshan Lu,Zenan Huang,Junbo Zhao,Lin Liu,Zhenzhong Lan,Bei Yu,Jianguo Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的MoE架构Grove MoE，通过引入不同大小的专家和动态激活机制，提高了模型的计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构使用相同大小的专家，无论输入复杂度如何都激活固定数量的参数，这限制了计算效率。为了克服这一限制，需要一种更灵活的架构来提高计算效率。

Method: 引入了Grove MoE架构，该架构结合了不同大小的专家，灵感来源于异构big.LITTLE CPU架构。同时提出了新的adjugate专家和动态激活机制，以扩展模型容量。基于此架构，开发了GroveMoE-Base和GroveMoE-Inst两个33B参数的LLM。

Result: GroveMoE模型能够根据token复杂度动态激活3.14-3.28B参数，并且性能可以与类似或更大规模的SOTA开源模型相媲美。

Conclusion: GroveMoE模型在保持计算开销可控的同时，能够动态激活3.14-3.28B参数，并且性能可与类似或更大规模的SOTA开源模型相媲美。

Abstract: The Mixture of Experts (MoE) architecture is a cornerstone of modern
state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate
scalability by enabling sparse parameter activation. However, traditional MoE
architecture uses homogeneous experts of a uniform size, activating a fixed
number of parameters irrespective of input complexity and thus limiting
computational efficiency. To overcome this limitation, we introduce Grove MoE,
a novel architecture incorporating experts of varying sizes, inspired by the
heterogeneous big.LITTLE CPU architecture. This architecture features novel
adjugate experts with a dynamic activation mechanism, enabling model capacity
expansion while maintaining manageable computational overhead. Building on this
architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs
developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model
during mid-training and post-training. GroveMoE models dynamically activate
3.14-3.28B parameters based on token complexity and achieve performance
comparable to SOTA open-source models of similar or even larger size.

</details>


### [65] [Can You Trick the Grader? Adversarial Persuasion of LLM Judges](https://arxiv.org/abs/2508.07805)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: 本研究揭示了说服性语言可以影响LLM评委对数学推理任务的评分，导致错误解答获得更高的分数，强调了LLM作为评判者的潜在漏洞和防御需求。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在实际场景中扮演越来越重要的自动化评估角色，一个关键问题浮现：个体是否能说服LLM评委给出不公平的高分？

Method: 基于亚里士多德的修辞原则，研究者形式化了七种说服技巧，并将其嵌入到本质上相同的回答中，以评估这些技巧对LLM评委评分的影响。

Result: 研究发现，说服性语言导致LLM评委对错误解答的评分被夸大，平均高出8%，其中一致性技巧造成的扭曲最严重。增加模型规模并不能显著缓解这种脆弱性。

Conclusion: 研究发现，说服性语言会导致LLM评委对错误解答给出过高的分数，这表明LLM作为评判者的管道存在关键漏洞，并强调了需要针对基于说服的攻击建立稳健防御的重要性。

Abstract: As large language models take on growing roles as automated evaluators in
practical settings, a critical question arises: Can individuals persuade an LLM
judge to assign unfairly high scores? This study is the first to reveal that
strategically embedded persuasive language can bias LLM judges when scoring
mathematical reasoning tasks, where correctness should be independent of
stylistic variation. Grounded in Aristotle's rhetorical principles, we
formalize seven persuasion techniques (Majority, Consistency, Flattery,
Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical
responses. Across six math benchmarks, we find that persuasive language leads
LLM judges to assign inflated scores to incorrect solutions, by up to 8% on
average, with Consistency causing the most severe distortion. Notably,
increasing model size does not substantially mitigate this vulnerability.
Further analysis demonstrates that combining multiple persuasion techniques
amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,
the persuasive effect persists under counter prompting strategies, highlighting
a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need
for robust defenses against persuasion-based attacks.

</details>


### [66] [Evaluating Compositional Approaches for Focus and Sentiment Analysis](https://arxiv.org/abs/2508.07810)
*Olga Kellert,Muhammad Imran,Nicholas Hill Matlis,Mahmud Uz Zaman,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 本文研究了组合方法在情感分析和焦点分析中的应用，证明了其在准确性和可解释性方面的优势，并将SA中的成果推广到FA中。


<details>
  <summary>Details</summary>
Motivation: 现有的定量评估主要集中在情感分析（SA）中，而在语言学中的焦点分析（FA）中却很少见。本文旨在填补这一研究空白，并证明SA和FA之间的紧密关系。

Method: 本文采用组合方法，利用通用依存关系（UDs）中的基本句法规则（如修饰、并列和否定）来处理表示情感的词语，并与非组合方法VADER进行比较。

Result: 实验结果表明，组合方法在准确性上优于非组合方法，并且在可解释性和可解释性方面具有优势。此外，组合方法在SA中的结果可以推广到FA中。

Conclusion: 本文通过实验证明了在情感分析（SA）中使用的组合方法可以推广到焦点分析（FA），并且具有更好的可解释性和可解释性。

Abstract: This paper summarizes the results of evaluating a compositional approach for
Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural
Language Processing (NLP). While quantitative evaluations of compositional and
non-compositional approaches in SA exist in NLP, similar quantitative
evaluations are very rare in FA in Linguistics that deal with linguistic
expressions representing focus or emphasis such as "it was John who left". We
fill this gap in research by arguing that compositional rules in SA also apply
to FA because FA and SA are closely related meaning that SA is part of FA. Our
compositional approach in SA exploits basic syntactic rules such as rules of
modification, coordination, and negation represented in the formalism of
Universal Dependencies (UDs) in English and applied to words representing
sentiments from sentiment dictionaries. Some of the advantages of our
compositional analysis method for SA in contrast to non-compositional analysis
methods are interpretability and explainability. We test the accuracy of our
compositional approach and compare it with a non-compositional approach VADER
that uses simple heuristic rules to deal with negation, coordination and
modification. In contrast to previous related work that evaluates
compositionality in SA on long reviews, this study uses more appropriate
datasets to evaluate compositionality. In addition, we generalize the results
of compositional approaches in SA to compositional approaches in FA.

</details>


### [67] [Evaluating Large Language Models as Expert Annotators](https://arxiv.org/abs/2508.07827)
*Yu-Min Tseng,Wei-Lin Chen,Chung-Chi Chen,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLMs）在需要专家知识的领域中作为人类专家标注者的有效性。通过评估单个LLMs和多代理方法在金融、生物医学和法律领域的表现，研究发现LLMs在这些任务中的效果并不总是优于非推理模型，且多代理讨论环境中的某些模型行为也值得关注。


<details>
  <summary>Details</summary>
Motivation: 文本数据标注过程通常成本高、耗时且劳动密集。尽管大型语言模型（LLMs）在一般领域的自然语言处理任务中表现出潜力，但它们在需要专家知识的标注任务中的效果仍需进一步探索。因此，本研究旨在评估LLMs是否可以作为人类专家标注者的直接替代品。

Method: 研究评估了单个LLMs和多代理方法在三个高度专业化的领域（金融、生物医学和法律）中的表现。提出了一个模拟人类标注者群体的多代理讨论框架，其中LLMs被要求在最终确定标签前考虑其他人的标注和理由。此外，还引入了推理模型（如o3-mini）以进行更全面的比较。

Result: 研究结果表明：(1) 配备推理时间技术（如链式思维（CoT）、自洽性）的单个LLMs仅显示出微小甚至负面的性能提升，这与之前文献中关于其广泛有效性的说法相矛盾。(2) 在大多数情况下，推理模型并没有比非推理模型表现出统计学上的显著改进。这表明，扩展的长CoT对于专业领域中的数据标注带来的好处相对有限。(3) 在多代理讨论环境中出现了一些模型行为。例如，Claude 3.7 Sonnet在思考模式下很少改变其初始标注，即使其他代理提供了正确的标注或有效的推理。

Conclusion: 研究发现，在需要专家知识的领域，大型语言模型（LLMs）作为直接替代人类专家标注者的有效性尚未得到充分探索。虽然LLMs在一般领域的自然语言处理任务中表现出色，但在特定领域如金融、生物医学和法律中的数据标注任务中，它们的表现并不总是优于非推理模型。此外，多代理讨论环境中的某些模型行为也显现出来，例如Claude 3.7 Sonnet在思考模式下很少改变其初始标注，即使其他代理提供了正确的标注或有效的推理。

Abstract: Textual data annotation, the process of labeling or tagging text with
relevant information, is typically costly, time-consuming, and labor-intensive.
While large language models (LLMs) have demonstrated their potential as direct
alternatives to human annotators for general domains natural language
processing (NLP) tasks, their effectiveness on annotation tasks in domains
requiring expert knowledge remains underexplored. In this paper, we
investigate: whether top-performing LLMs, which might be perceived as having
expert-level proficiency in academic and professional benchmarks, can serve as
direct alternatives to human expert annotators? To this end, we evaluate both
individual LLMs and multi-agent approaches across three highly specialized
domains: finance, biomedicine, and law. Specifically, we propose a multi-agent
discussion framework to simulate a group of human annotators, where LLMs are
tasked to engage in discussions by considering others' annotations and
justifications before finalizing their labels. Additionally, we incorporate
reasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our
empirical results reveal that: (1) Individual LLMs equipped with inference-time
techniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal
or even negative performance gains, contrary to prior literature suggesting
their broad effectiveness. (2) Overall, reasoning models do not demonstrate
statistically significant improvements over non-reasoning models in most
settings. This suggests that extended long CoT provides relatively limited
benefits for data annotation in specialized domains. (3) Certain model
behaviors emerge in the multi-agent discussion environment. For instance,
Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even
when other agents provide correct annotations or valid reasoning.

</details>


### [68] [LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding](https://arxiv.org/abs/2508.07849)
*Amrita Singh,H. Suhan Karaca,Aditya Joshi,Hye-young Paik,Jiaojiao Jiang*

Main category: cs.CL

TL;DR: 本文评估了10个法律特定的LLM和7个通用LLM在合同理解任务中的表现，发现法律特定的LLM在需要细微法律理解的任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对多个法律特定LLM在合同分类任务中的全面评估，因此我们进行了这项研究。

Method: 我们评估了10个法律特定的LLM在三个英语合同理解任务上的表现，并与7个通用LLM进行了比较。

Result: 法律特定的LLM在需要细微法律理解的任务中表现出色，Legal-BERT和Contracts-BERT在两个任务上建立了新的SOTA。

Conclusion: 我们的结果提供了对法律特定LLM的全面评估，并将有助于开发更准确的合同理解系统。

Abstract: Despite advances in legal NLP, no comprehensive evaluation covering multiple
legal-specific LLMs currently exists for contract classification tasks in
contract understanding. To address this gap, we present an evaluation of 10
legal-specific LLMs on three English language contract understanding tasks and
compare them with 7 general-purpose LLMs. The results show that legal-specific
LLMs consistently outperform general-purpose models, especially on tasks
requiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish
new SOTAs on two of the three tasks, despite having 69% fewer parameters than
the best-performing general-purpose LLM. We also identify CaseLaw-BERT and
LexLM as strong additional baselines for contract understanding. Our results
provide a holistic evaluation of legal-specific LLMs and will facilitate the
development of more accurate contract understanding systems.

</details>


### [69] [Large Language Models for Czech Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.07860)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本研究评估了19个大型语言模型在捷克语方面情感分析任务中的表现，发现微调的小型领域特定模型在零样本和少样本设置中优于通用大型语言模型，而微调的大型语言模型实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在各种自然语言处理任务中表现出色，但它们在捷克语ABSA中的能力仍鲜有研究。

Method: 我们对19个不同大小和架构的大型语言模型进行了全面评估，比较了它们在零样本、少样本和微调场景中的性能。

Result: 小型领域特定模型在零样本和少样本设置中表现优于通用大型语言模型，而微调的大型语言模型实现了最先进的结果。

Conclusion: 我们的研究结果表明，微调的小型领域特定模型在零样本和少样本设置中优于通用大型语言模型，而微调的大型语言模型实现了最先进的结果。我们的发现为大型语言模型在捷克语ABSA中的适用性提供了见解，并为该领域的未来研究提供了指导。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to identify sentiment toward specific aspects of an entity.
While large language models (LLMs) have shown strong performance in various
natural language processing (NLP) tasks, their capabilities for Czech ABSA
remain largely unexplored. In this work, we conduct a comprehensive evaluation
of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their
performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show
that small domain-specific models fine-tuned for ABSA outperform
general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs
achieve state-of-the-art results. We analyze how factors such as
multilingualism, model size, and recency influence performance and present an
error analysis highlighting key challenges, particularly in aspect term
prediction. Our findings provide insights into the suitability of LLMs for
Czech ABSA and offer guidance for future research in this area.

</details>


### [70] [Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models](https://arxiv.org/abs/2508.07866)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文研究了在跨语言方面情感分析中添加少量目标语言示例的效果，发现这能显著提升性能，并可能超越单语基线。


<details>
  <summary>Details</summary>
Motivation: 当前的跨语言ABSA方法通常依赖外部翻译工具，并忽视了将少量目标语言示例纳入训练的潜在好处。

Method: 评估在四个ABSA任务、六种目标语言和两种序列到序列模型中添加少量目标语言示例对训练集的影响。

Result: 添加最少十个目标语言示例可以显著提高零样本设置的性能，并达到减少预测错误的约束解码类似效果。此外，将1000个目标语言示例与英语数据结合甚至可以超越单语基线。

Conclusion: 这些发现为在低资源和特定领域设置中改进跨语言ABSA提供了实用的见解，因为获得十个高质量的注释示例是可行且非常有效的。

Abstract: Aspect-based sentiment analysis (ABSA) has received substantial attention in
English, yet challenges remain for low-resource languages due to the scarcity
of labelled data. Current cross-lingual ABSA approaches often rely on external
translation tools and overlook the potential benefits of incorporating a small
number of target language examples into training. In this paper, we evaluate
the effect of adding few-shot target language examples to the training set
across four ABSA tasks, six target languages, and two sequence-to-sequence
models. We show that adding as few as ten target language examples
significantly improves performance over zero-shot settings and achieves a
similar effect to constrained decoding in reducing prediction errors.
Furthermore, we demonstrate that combining 1,000 target language examples with
English data can even surpass monolingual baselines. These findings offer
practical insights for improving cross-lingual ABSA in low-resource and
domain-specific settings, as obtaining ten high-quality annotated examples is
both feasible and highly effective.

</details>


### [71] [Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity](https://arxiv.org/abs/2508.07902)
*Chen Cecilia Liu,Hiba Arnaout,Nils Kovačić,Dana Atzil-Slonim,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文介绍了CultureCare数据集，用于研究大型语言模型在提供文化敏感性支持方面的表现，并展示了经过调整的大型语言模型在文化敏感性方面的优势以及其在临床培训中的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏资源，大型语言模型在提供文化敏感性支持方面的能力尚未得到充分探索。因此，需要一个专门的数据集来研究这一问题。

Method: 本文介绍了CultureCare数据集，用于研究大型语言模型在提供文化敏感性支持方面的表现。通过四种适应策略，对三种最先进的大型语言模型进行了测试，并进行了全面评估，包括使用大型语言模型评委、本文化人类标注者和临床心理学家。

Result: 研究结果表明，经过调整的大型语言模型在文化敏感性方面表现优于匿名在线同行回复，并且简单的文化角色扮演不足以实现文化敏感性。此外，大型语言模型在临床培训中显示出潜在的应用价值。

Conclusion: 文化敏感性对于大型语言模型提供情感支持至关重要，而CultureCare数据集的引入为这一领域提供了重要的资源。研究结果表明，经过调整的大型语言模型在文化敏感性方面优于匿名在线同行回复，并且在临床培训中显示出潜力。

Abstract: Large language models (LLMs) show promise in offering emotional support and
generating empathetic responses for individuals in distress, but their ability
to deliver culturally sensitive support remains underexplored due to lack of
resources. In this work, we introduce CultureCare, the first dataset designed
for this task, spanning four cultures and including 1729 distress messages,
1523 cultural signals, and 1041 support strategies with fine-grained emotional
and cultural annotations. Leveraging CultureCare, we (i) develop and test four
adaptation strategies for guiding three state-of-the-art LLMs toward culturally
sensitive responses; (ii) conduct comprehensive evaluations using LLM judges,
in-culture human annotators, and clinical psychologists; (iii) show that
adapted LLMs outperform anonymous online peer responses, and that simple
cultural role-play is insufficient for cultural sensitivity; and (iv) explore
the application of LLMs in clinical training, where experts highlight their
potential in fostering cultural competence in future therapists.

</details>


### [72] [Challenges and opportunities in portraying emotion in generated sign language](https://arxiv.org/abs/2508.07937)
*John C. McDonald,Rosalee Wolfe,Fabrizio Nunnari*

Main category: cs.CL

TL;DR: 该研究提出了一种两参数表示法，用于情感非手动信号，并应用于Paula手语虚拟人，以实现更一致和细腻的情感表达。


<details>
  <summary>Details</summary>
Motivation: 目前，情感内容难以融入手语虚拟人，因为缺乏一种标准方法来指定虚拟人的情感状态。

Method: 该研究探索了将直观的两参数表示法应用于Paula手语虚拟人，以实现更连贯的情感面部表情语言规范。

Result: 用户可以通过称为EASIER符号的文本表示法来控制Paula的情感表达，该表示法允许使用两个数值参数表达更细腻的情感状态。

Conclusion: 该研究提出了一种直观的两参数表示法，用于情感非手动信号，并展示了其在Paula手语虚拟人中的应用潜力，有助于更一致地指定情感面部表情。

Abstract: Non-manual signals in sign languages continue to be a challenge for signing
avatars. More specifically, emotional content has been difficult to incorporate
because of a lack of a standard method of specifying the avatar's emotional
state. This paper explores the application of an intuitive two-parameter
representation for emotive non-manual signals to the Paula signing avatar that
shows promise for facilitating the linguistic specification of emotional facial
expressions in a more coherent manner than previous methods. Users can apply
these parameters to control Paula's emotional expressions through a textual
representation called the EASIER notation. The representation can allow avatars
to express more nuanced emotional states using two numerical parameters. It
also has the potential to enable more consistent specification of emotional
non-manual signals in linguistic annotations which drive signing avatars.

</details>


### [73] [Expert Preference-based Evaluation of Automated Related Work Generation](https://arxiv.org/abs/2508.07955)
*Furkan Şahinuç,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了GREP，一个用于评估科学写作质量的多轮框架，结合了经典标准和专家偏好。该框架分解评估为细粒度维度，并通过对比少样本示例提供详细指导。实证结果显示，GREP比标准LLM法官更稳健，能更好地反映科学写作场景，并与人类专家评估高度相关。


<details>
  <summary>Details</summary>
Motivation: 评估自动生成的科学写作质量是一个关键的开放问题，因为它需要领域特定的评估标准知识和辨别专家偏好的能力。传统的自动指标和LLM-as-a-judge系统不足以掌握专家偏好和领域特定的质量标准。为了填补这一空白并支持人机协作写作，我们专注于相关工作生成这一最具挑战性的科学任务作为范例。

Method: 我们提出了GREP，这是一个多轮评估框架，结合了经典的相关工作评估标准和专家特定偏好。我们的框架将评估分解为细粒度维度，并通过对比少样本示例提供详细的上下文指导。我们设计了两种GREP变体：一种是使用专有LLM作为评估者的更精确变体，另一种是使用开放权重LLM的更便宜替代方案。

Result: 实证研究显示，与标准LLM法官相比，我们的框架能够更稳健地评估相关工作部分的质量，反映科学写作的自然场景，并与人类专家评估有很强的相关性。我们还观察到最先进的LLM生成的内容难以满足合适相关工作部分的验证约束。它们（大部分）无法根据反馈进行改进。

Conclusion: 我们的框架能够以更稳健的方式评估相关工作部分的质量，反映了科学写作的自然场景，并与人类专家评估有很强的相关性。我们还观察到最先进的LLM生成的内容难以满足合适相关工作部分的验证约束。它们（大部分）无法根据反馈进行改进。

Abstract: Expert domain writing, such as scientific writing, typically demands
extensive domain knowledge. Recent advances in LLMs show promising potential in
reducing the expert workload. However, evaluating the quality of automatically
generated scientific writing is a crucial open issue, as it requires knowledge
of domain-specific evaluation criteria and the ability to discern expert
preferences. Conventional automatic metrics and LLM-as-a-judge systems are
insufficient to grasp expert preferences and domain-specific quality standards.
To address this gap and support human-AI collaborative writing, we focus on
related work generation, one of the most challenging scientific tasks, as an
exemplar. We propose GREP, a multi-turn evaluation framework that integrates
classical related work evaluation criteria with expert-specific preferences.
Instead of assigning a single score, our framework decomposes the evaluation
into fine-grained dimensions. This localized evaluation approach is further
augmented with contrastive few-shot examples to provide detailed contextual
guidance for the evaluation dimensions. The design principles allow our
framework to deliver cardinal assessment of quality, which can facilitate
better post-training compared to ordinal preference data. For better
accessibility, we design two variants of GREP: a more precise variant with
proprietary LLMs as evaluators, and a cheaper alternative with open-weight
LLMs. Empirical investigation reveals that our framework is able to assess the
quality of related work sections in a much more robust manner compared to
standard LLM judges, reflects natural scenarios of scientific writing, and
bears a strong correlation with the human expert assessment. We also observe
that generations from state-of-the-art LLMs struggle to satisfy validation
constraints of a suitable related work section. They (mostly) fail to improve
based on feedback as well.

</details>


### [74] [Large Language Models for Subjective Language Understanding: A Survey](https://arxiv.org/abs/2508.07959)
*Changhao Song,Yazhou Zhang,Hui Gao,Ben Yao,Peng Zhang*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型在主观语言理解任务中的应用，涵盖了多种任务如情感分析、情绪识别、讽刺检测等，并讨论了LLM的优势、挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的出现，人们对主观语言理解任务的方法有了新的认识，本文旨在全面回顾这些进展，并为研究人员和从业者提供有价值的资源。

Method: 本文通过梳理主观语言的定义和挑战，回顾了LLM架构和技巧的发展，总结了八种任务的定义、关键数据集、最先进的LLM方法以及剩余挑战，并讨论了多任务LLM方法可能带来的统一的主观性模型。

Result: 本文提供了对主观语言理解任务的全面综述，包括各种任务的定义、数据集、最先进的方法以及剩余挑战，并讨论了多任务LLM方法的潜力。

Conclusion: 本文综述了大型语言模型在主观语言理解任务中的应用，指出了数据限制、模型偏差和伦理考虑等开放问题，并提出了未来的研究方向。

Abstract: Subjective language understanding refers to a broad set of natural language
processing tasks where the goal is to interpret or generate content that
conveys personal feelings, opinions, or figurative meanings rather than
objective facts. With the advent of large language models (LLMs) such as
ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach
these inherently nuanced tasks. In this survey, we provide a comprehensive
review of recent advances in applying LLMs to subjective language tasks,
including sentiment analysis, emotion recognition, sarcasm detection, humor
understanding, stance detection, metaphor interpretation, intent detection, and
aesthetics assessment. We begin by clarifying the definition of subjective
language from linguistic and cognitive perspectives, and we outline the unique
challenges posed by subjective language (e.g. ambiguity, figurativeness,
context dependence). We then survey the evolution of LLM architectures and
techniques that particularly benefit subjectivity tasks, highlighting why LLMs
are well-suited to model subtle human-like judgments. For each of the eight
tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based
methods, and remaining challenges. We provide comparative insights, discussing
commonalities and differences among tasks and how multi-task LLM approaches
might yield unified models of subjectivity. Finally, we identify open issues
such as data limitations, model bias, and ethical considerations, and suggest
future research directions. We hope this survey will serve as a valuable
resource for researchers and practitioners interested in the intersection of
affective computing, figurative language processing, and large-scale language
models.

</details>


### [75] [Toward Machine Interpreting: Lessons from Human Interpreting Studies](https://arxiv.org/abs/2508.07964)
*Matthias Sperber,Maureen de Seyssel,Jiajun Bao,Matthias Paulik*

Main category: cs.CL

TL;DR: 本文从机器翻译的角度探讨了人类口译文献，旨在提高语音翻译系统的实用性，并通过采用人类口译原则来推动真正机器翻译的发展。


<details>
  <summary>Details</summary>
Motivation: 当前语音翻译系统在行为上较为静态，无法像人类口译员那样适应现实情况。为了提高其实际有用性并实现类似口译体验，精确理解人类口译的本质至关重要。

Method: 从机器翻译的角度讨论人类口译文献，同时考虑操作和定性方面。

Result: 我们发现了对语音翻译系统开发的影响，并认为可以利用最近的建模技术采用许多人类口译原则。

Conclusion: 我们的研究希望为缩小感知的可用性差距提供灵感，并推动真正机器翻译的进步。

Abstract: Current speech translation systems, while having achieved impressive
accuracies, are rather static in their behavior and do not adapt to real-world
situations in ways human interpreters do. In order to improve their practical
usefulness and enable interpreting-like experiences, a precise understanding of
the nature of human interpreting is crucial. To this end, we discuss human
interpreting literature from the perspective of the machine translation field,
while considering both operational and qualitative aspects. We identify
implications for the development of speech translation systems and argue that
there is great potential to adopt many human interpreting principles using
recent modeling techniques. We hope that our findings provide inspiration for
closing the perceived usability gap, and can motivate progress toward true
machine interpreting.

</details>


### [76] [Understanding Syntactic Generalization in Structure-inducing Language Models](https://arxiv.org/abs/2508.07969)
*David Arps,Hassan Sajjad,Laura Kallmeyer*

Main category: cs.CL

TL;DR: 本文分析了三种结构诱导语言模型（SiLM），发现GPST在不同评估设置中表现最佳，而小型模型在大量合成数据上训练有助于评估模型属性。


<details>
  <summary>Details</summary>
Motivation: 现有的SiLM模型通常在相对较小的规模上进行评估，评估存在系统性差距且缺乏可比性。

Method: 我们研究了三种不同的SiLM架构，并使用自然语言（英语）语料库和合成括号表达式进行了比较。

Result: GPST在不同评估设置中表现最一致，并在括号表达式的长距离依赖关系上优于其他模型。

Conclusion: 我们的研究显示，小型模型在大量合成数据上训练可以为评估基本模型属性提供有用的测试平台。

Abstract: Structure-inducing Language Models (SiLM) are trained on a self-supervised
language modeling task, and induce a hierarchical sentence representation as a
byproduct when processing an input. A wide variety of SiLMs have been proposed.
However, these have typically been evaluated on a relatively small scale, and
evaluation of these models has systematic gaps and lacks comparability. In this
work, we study three different SiLM architectures using both natural language
(English) corpora and synthetic bracketing expressions: Structformer (Shen et
al., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare
them with respect to (i) properties of the induced syntactic representations
(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.
We find that none of the three architectures dominates across all evaluation
metrics. However, there are significant differences, in particular with respect
to the induced syntactic representations. The Generative Pretrained Structured
Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation
settings, and outperforms the other models on long-distance dependencies in
bracketing expressions. Furthermore, our study shows that small models trained
on large amounts of synthetic data provide a useful testbed for evaluating
basic model properties.

</details>


### [77] [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)
*Jiaxuan Gao,Wei Fu,Minyang Xie,Shusheng Xu,Chuyi He,Zhiyu Mei,Banghua Zhu,Yi Wu*

Main category: cs.CL

TL;DR: 本文提出了ASearcher，一个用于大规模RL训练的开源项目，以提高搜索代理的性能。通过可扩展的异步RL训练和基于提示的LLM代理，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展性、效率和数据质量方面存在不足，例如在线RL方法中的小回合限制限制了复杂策略的学习。

Method: 本文介绍了ASearcher，这是一个用于大规模RL训练的开源项目，其关键贡献包括：(1) 可扩展的完全异步RL训练，(2) 基于提示的LLM代理，能够自主合成高质量和具有挑战性的QAs，创建大规模QA数据集。

Result: 通过RL训练，基于提示的QwQ-32B代理在xBench和GAIA上分别实现了46.7%和20.8%的Avg@4提升。此外，该代理表现出极端长视野搜索，训练期间工具调用超过40次，输出标记超过150k。

Conclusion: ASearcher-Web-QwQ在xBench和GAIA上的Avg@4得分分别达到了42.1和52.8，超越了现有的开源32B代理。

Abstract: Recent advancements in LLM-based agents have demonstrated remarkable
capabilities in handling complex, knowledge-intensive tasks by integrating
external tools. Among diverse choices of tools, search tools play a pivotal
role in accessing vast external knowledge. However, open-source agents still
fall short of achieving expert-level Search Intelligence, the ability to
resolve ambiguous queries, generate precise searches, analyze results, and
conduct thorough exploration. Existing approaches fall short in scalability,
efficiency, and data quality. For example, small turn limits in existing online
RL methods, e.g. <=10, restrict complex strategy learning. This paper
introduces ASearcher, an open-source project for large-scale RL training of
search agents. Our key contributions include: (1) Scalable fully asynchronous
RL training that enables long-horizon search while maintaining high training
efficiency. (2) A prompt-based LLM agent that autonomously synthesizes
high-quality and challenging QAs, creating a large-scale QA dataset. Through RL
training, our prompt-based QwQ-32B agent achieves substantial improvements,
with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our
agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns
and output tokens exceeding 150k during training time. With a simple agent
design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on
xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We
open-source our models, training data, and codes in
https://github.com/inclusionAI/ASearcher.

</details>


### [78] [The Medical Metaphors Corpus (MCC)](https://arxiv.org/abs/2508.07993)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Metaphor is a fundamental cognitive mechanism that shapes scientific
understanding, enabling the communication of complex concepts while potentially
constraining paradigmatic thinking. Despite the prevalence of figurative
language in scientific discourse, existing metaphor detection resources
primarily focus on general-domain text, leaving a critical gap for
domain-specific applications. In this paper, we present the Medical Metaphors
Corpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual
metaphors spanning medical and biological domains. MCC aggregates metaphorical
expressions from diverse sources including peer-reviewed literature, news
media, social media discourse, and crowdsourced contributions, providing both
binary and graded metaphoricity judgments validated through human annotation.
Each instance includes source-target conceptual mappings and perceived
metaphoricity scores on a 0-7 scale, establishing the first annotated resource
for computational scientific metaphor research. Our evaluation demonstrates
that state-of-the-art language models achieve modest performance on scientific
metaphor detection, revealing substantial room for improvement in
domain-specific figurative language understanding. MCC enables multiple
research applications including metaphor detection benchmarking, quality-aware
generation systems, and patient-centered communication tools.

</details>


### [79] [WideSearch: Benchmarking Agentic Broad Info-Seeking](https://arxiv.org/abs/2508.07999)
*Ryan Wong,Jiawei Wang,Junjie Zhao,Li Chen,Yan Gao,Long Zhang,Xuan Zhou,Zuo Wang,Kai Xiang,Ge Zhang,Wenhao Huang,Yang Wang,Ke Wang*

Main category: cs.CL

TL;DR: 本文介绍了WideSearch基准，用于评估代理在大规模信息收集任务中的可靠性。结果表明，当前的搜索代理在大规模信息检索方面存在严重不足。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏合适的基准，这些代理在执行“宽上下文”收集任务的能力尚未得到充分评估。为了弥补这一差距，我们引入了WideSearch基准。

Method: 引入了WideSearch基准，用于评估代理在这些大规模收集任务上的可靠性。该基准包含200个手动策划的问题，覆盖15个不同的领域，并通过严格的五阶段质量控制流程确保数据集的难度、完整性和可验证性。

Result: 对超过10个最先进的代理搜索系统进行了基准测试，大多数系统的整体成功率接近0%，最佳表现者仅达到5%。然而，给予足够的时间，多个人类测试者的交叉验证可以实现接近100%的成功率。

Conclusion: 当前的搜索代理在大规模信息检索方面存在关键缺陷，这凸显了未来研究和开发中的紧迫领域。

Abstract: From professional research to everyday planning, many tasks are bottlenecked
by wide-scale information seeking, which is more repetitive than cognitively
complex. With the rapid development of Large Language Models (LLMs), automated
search agents powered by LLMs offer a promising solution to liberate humans
from this tedious work. However, the capability of these agents to perform such
"wide-context" collection reliably and completely remains largely unevaluated
due to a lack of suitable benchmarks. To bridge this gap, we introduce
WideSearch, a new benchmark engineered to evaluate agent reliability on these
large-scale collection tasks. The benchmark features 200 manually curated
questions (100 in English, 100 in Chinese) from over 15 diverse domains,
grounded in real user queries. Each task requires agents to collect large-scale
atomic information, which could be verified one by one objectively, and arrange
it into a well-organized output. A rigorous five-stage quality control pipeline
ensures the difficulty, completeness, and verifiability of the dataset. We
benchmark over 10 state-of-the-art agentic search systems, including
single-agent, multi-agent frameworks, and end-to-end commercial systems. Most
systems achieve overall success rates near 0\%, with the best performer
reaching just 5\%. However, given sufficient time, cross-validation by multiple
human testers can achieve a near 100\% success rate. These results demonstrate
that present search agents have critical deficiencies in large-scale
information seeking, underscoring urgent areas for future research and
development in agentic search. Our dataset, evaluation pipeline, and benchmark
results have been publicly released at https://widesearch-seed.github.io/

</details>


### [80] [Progressive Depth Up-scaling via Optimal Transport](https://arxiv.org/abs/2508.08011)
*Mingzi Cao,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: OpT-DeUS is a method for depth up-scaling of large language models that uses Optimal Transport to align and fuse Transformer blocks, resulting in better performance and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for depth up-scaling copy or average weights from base layers, neglecting neuron permutation differences, which can cause misalignment that harms performance.

Method: OpT-DeUS aligns and fuses Transformer blocks in adjacent base layers via Optimal Transport (OT) for new layer creation, to mitigate neuron permutation mismatch between layers.

Result: OpT-DeUS achieves better overall performance and offers improved training efficiency than existing methods for continual pre-training and supervised fine-tuning across different model sizes. Inserting new layers closer to the top results in higher training efficiency due to shorter back-propagation time while obtaining additional performance gains.

Conclusion: OpT-DeUS achieves better overall performance and offers improved training efficiency than existing methods for continual pre-training and supervised fine-tuning across different model sizes.

Abstract: Scaling Large Language Models (LLMs) yields performance gains but incurs
substantial training costs. Depth up-scaling offers training efficiency by
adding new layers to pre-trained models. However, most existing methods copy or
average weights from base layers, neglecting neuron permutation differences.
This limitation can potentially cause misalignment that harms performance.
Inspired by applying Optimal Transport (OT) for neuron alignment, we propose
Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses
Transformer blocks in adjacent base layers via OT for new layer creation, to
mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better
overall performance and offers improved training efficiency than existing
methods for continual pre-training and supervised fine-tuning across different
model sizes. To further evaluate the impact of interpolation positions, our
extensive analysis shows that inserting new layers closer to the top results in
higher training efficiency due to shorter back-propagation time while obtaining
additional performance gains.

</details>


### [81] [9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)](https://arxiv.org/abs/2508.08050)
*Fabrizio Nunnari,Cristina Luna Jiménez,Rosalee Wolfe,John C. McDonald,Michael Filhol,Eleni Efthimiou,Evita Fotinea,Thomas Hanke*

Main category: cs.CL

TL;DR: The SLTAT workshops focus on improving deaf/human communication through non-invasive means, with a emphasis on avatar technologies and sign language recognition.


<details>
  <summary>Details</summary>
Motivation: To share recent advances in improving deaf/human communication through non-invasive means and to foster collaboration between research communities.

Method: The paper summarizes the SLTAT workshops and their contributions to research in sign language translation and avatar technology.

Result: The paper highlights the contributions of SLTAT workshops, including work on sign language recognition, data collection, and affective computing.

Conclusion: SLTAT workshops continue to promote advances in deaf/human communication through non-invasive means, with a focus on avatar technologies and sign language recognition.

Abstract: The Sign Language Translation and Avatar Technology (SLTAT) workshops
continue a series of gatherings to share recent advances in improving deaf /
human communication through non-invasive means. This 2025 edition, the 9th
since its first appearance in 2011, is hosted by the International Conference
on Intelligent Virtual Agents (IVA), giving the opportunity for contamination
between two research communities, using digital humans as either virtual
interpreters or as interactive conversational agents. As presented in this
summary paper, SLTAT sees contributions beyond avatar technologies, with a
consistent number of submissions on sign language recognition, and other work
on data collection, data analysis, tools, ethics, usability, and affective
computing.

</details>


### [82] [Dual Information Speech Language Models for Emotional Conversations](https://arxiv.org/abs/2508.08095)
*Chun Wang,Chenyang Liu,Wenze Xu,Weihong Deng*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过引入异构适配器和弱监督训练策略，改进了基于语音的语言模型在情感对话任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的大型语言模型（LLMs）往往忽视语调线索，而基于语音的SLMs在扩展冻结LLMs时难以捕捉语调信息并表现出较差的上下文理解能力。

Method: 提出两种异构适配器并建议一种弱监督训练策略，以解决SLMs在捕捉语调信息和上下文理解方面的不足。

Result: 实验结果表明，该方法在情感对话任务中具有竞争力，并展示了模型在上下文环境中有效整合语调和语言信息的能力。

Conclusion: 实验表明，该方法在情感对话任务中表现出色，展示了模型在上下文环境中有效整合语调和语言信息的能力。

Abstract: Conversational systems relying on text-based large language models (LLMs)
often overlook paralinguistic cues, essential for understanding emotions and
intentions. Speech-language models (SLMs), which use speech as input, are
emerging as a promising solution. However, SLMs built by extending frozen LLMs
struggle to capture paralinguistic information and exhibit reduced context
understanding. We identify entangled information and improper training
strategies as key issues. To address these issues, we propose two heterogeneous
adapters and suggest a weakly supervised training strategy. Our approach
disentangles paralinguistic and linguistic information, enabling SLMs to
interpret speech through structured representations. It also preserves
contextual understanding by avoiding the generation of task-specific vectors
through controlled randomness. This approach trains only the adapters on common
datasets, ensuring parameter and data efficiency. Experiments demonstrate
competitive performance in emotional conversation tasks, showcasing the model's
ability to effectively integrate both paralinguistic and linguistic information
within contextual settings.

</details>


### [83] [Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?](https://arxiv.org/abs/2508.08096)
*Lukas Gehring,Benjamin Paaßen*

Main category: cs.CL

TL;DR: 本文评估了不同最先进的检测器在教育环境中的性能，并引入了一个新数据集GEDE，用于研究LLM生成文本的检测问题。研究发现，大多数检测器在处理中间学生贡献水平的文本时表现不佳，容易产生误报。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的进步和其可访问性的提高，学生可以自动生成文本，这对教育机构构成了新的挑战。为了维护学术诚信并确保学生的学习，自动检测LLM生成文本的学习分析方法变得越来越有吸引力。

Method: 本文通过引入一个名为GEDE的新数据集，包含超过900篇学生撰写的论文和超过12,500篇来自不同领域的LLM生成论文，来评估不同最先进的检测器在教育环境中的性能。此外，还提出了贡献级别概念，以捕捉生成文本中LLM使用实践的多样性。

Result: 本文展示了大多数检测器在准确分类中间学生贡献水平的文本（如LLM改进的人类写作文本）时存在困难。检测器特别容易产生误报，在教育环境中，误报可能导致对学生生活的严重负面影响。

Conclusion: 本文结论是大多数检测器在分类中间学生贡献水平的文本时表现不佳，容易产生误报，在教育环境中可能对学生产生严重影响。

Abstract: Recent advancements in Large Language Models (LLMs) and their increased
accessibility have made it easier than ever for students to automatically
generate texts, posing new challenges for educational institutions. To enforce
norms of academic integrity and ensure students' learning, learning analytics
methods to automatically detect LLM-generated text appear increasingly
appealing. This paper benchmarks the performance of different state-of-the-art
detectors in educational contexts, introducing a novel dataset, called
Generative Essay Detection in Education (GEDE), containing over 900
student-written essays and over 12,500 LLM-generated essays from various
domains. To capture the diversity of LLM usage practices in generating text, we
propose the concept of contribution levels, representing students' contribution
to a given assignment. These levels range from purely human-written texts, to
slightly LLM-improved versions, to fully LLM-generated texts, and finally to
active attacks on the detector by "humanizing" generated texts. We show that
most detectors struggle to accurately classify texts of intermediate student
contribution levels, like LLM-improved human-written texts. Detectors are
particularly likely to produce false positives, which is problematic in
educational settings where false suspicions can severely impact students'
lives. Our dataset, code, and additional supplementary materials are publicly
available at
https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.

</details>


### [84] [Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0](https://arxiv.org/abs/2508.08110)
*Robin Huo,Ewan Dunbar*

Main category: cs.CL

TL;DR: 研究比较了HuBERT和wav2vec 2.0模型的架构差异，发现训练迭代而非训练目标影响了语言信息的编码。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨模型架构对自监督语音表示中学习到的语言信息的影响，特别是训练迭代和训练目标的作用。

Method: 研究比较了HuBERT和wav2vec 2.0两种模型的两个架构差异：训练目标和通过多个训练迭代进行的伪标签精炼。

Result: 研究发现，隐藏表示与词身份、音素身份和说话人身份的典型相关性差异是由训练迭代解释的，而不是训练目标。

Conclusion: 研究建议未来的工作应调查迭代精炼在自监督语音表示中编码语言信息的有效性原因。

Abstract: Self-supervised models for speech representation learning now see widespread
use for their versatility and performance on downstream tasks, but the effect
of model architecture on the linguistic information learned in their
representations remains under-studied. This study investigates two such models,
HuBERT and wav2vec 2.0, and minimally compares two of their architectural
differences: training objective and iterative pseudo-label refinement through
multiple training iterations. We find that differences in canonical correlation
of hidden representations to word identity, phoneme identity, and speaker
identity are explained by training iteration, not training objective. We
suggest that future work investigate the reason for the effectiveness of
iterative refinement in encoding linguistic information in self-supervised
speech representations.

</details>


### [85] [Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.08125)
*Jakub Šmíd,Pavel Přibáň,Ondřej Pražák,Pavel Král*

Main category: cs.CL

TL;DR: 本文介绍了一个新的捷克数据集，用于更复杂的基于方面的情感分析任务，并提供了24M未标注的评论用于无监督学习。


<details>
  <summary>Details</summary>
Motivation: 现有的捷克数据集仅包含基本的ABSA任务标签，无法满足更复杂任务的需求。因此，本文旨在构建一个更全面的数据集，以支持更复杂的ABSA任务。

Method: 本文构建了一个新的捷克数据集，基于旧的捷克数据集，但设计用于更复杂的任务，如目标-方面类别检测。数据集遵循SemEval-2016数据集的格式，以方便跨语言比较。

Result: 本文构建了一个包含3.1K手动标注评论的新捷克数据集，具有约90%的标注者间一致性。此外，还提供了24M未标注的评论用于无监督学习，并展示了基于Transformer的模型的基线结果。

Conclusion: 本文介绍了用于基于方面的情感分析的新型捷克数据集，该数据集适用于更复杂的任务，并提供了24M未标注的评论用于无监督学习。此外，还提供了各种基于Transformer的模型的基线结果和错误分析。

Abstract: In this paper, we introduce a novel Czech dataset for aspect-based sentiment
analysis (ABSA), which consists of 3.1K manually annotated reviews from the
restaurant domain. The dataset is built upon the older Czech dataset, which
contained only separate labels for the basic ABSA tasks such as aspect term
extraction or aspect polarity detection. Unlike its predecessor, our new
dataset is specifically designed for more complex tasks, e.g.
target-aspect-category detection. These advanced tasks require a unified
annotation format, seamlessly linking sentiment elements (labels) together. Our
dataset follows the format of the well-known SemEval-2016 datasets. This design
choice allows effortless application and evaluation in cross-lingual scenarios,
ultimately fostering cross-language comparisons with equivalent counterpart
datasets in other languages. The annotation process engaged two trained
annotators, yielding an impressive inter-annotator agreement rate of
approximately 90%. Additionally, we provide 24M reviews without annotations
suitable for unsupervised learning. We present robust monolingual baseline
results achieved with various Transformer-based models and insightful error
analysis to supplement our contributions. Our code and dataset are freely
available for non-commercial research purposes.

</details>


### [86] [Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models](https://arxiv.org/abs/2508.08131)
*Wenze Xu,Chun Wang,Jiazhen Yu,Sheng Chen,Liang Gao,Weihong Deng*

Main category: cs.CL

TL;DR: 本文提出了一种名为OTReg的方法，用于改善语音语言模型（SLMs）的泛化能力。该方法通过最优传输问题来优化语音-文本对齐，从而减轻模态差距。实验结果表明，OTReg能够有效提升SLM在不同数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 研究显示，SLMs在跨数据集的泛化能力较差，这可能是由于语音和文本表示之间的模态差距。

Method: 引入了Optimal Transport Regularization(OTReg)，将语音-文本对齐问题形式化为最优传输问题，并推导出一种正则化损失来改进SLM训练。

Result: 广泛的多语言ASR实验表明，OTReg增强了语音-文本对齐，减轻了模态差距，并提高了SLM的泛化能力。

Conclusion: OTReg可以有效增强语音-文本对齐，减轻模态差距，并最终提高SLM在不同数据集上的泛化能力。

Abstract: Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to
perceive speech inputs, have gained increasing attention for their potential to
advance speech understanding tasks. However, despite recent progress, studies
show that SLMs often struggle to generalize across datasets, even for trained
languages and tasks, raising concerns about whether they process speech in a
text-like manner as intended. A key challenge underlying this limitation is the
modality gap between speech and text representations. The high variability in
speech embeddings may allow SLMs to achieve strong in-domain performance by
exploiting unintended speech variations, ultimately hindering generalization.
To mitigate this modality gap, we introduce Optimal Transport Regularization
(OTReg), a method that formulates speech-text alignment as an optimal transport
problem and derives a regularization loss to improve SLM training. In each
training iteration, OTReg first establishes a structured correspondence between
speech and transcript embeddings by determining the optimal transport plan,
then incorporates the regularization loss based on this transport plan to
optimize SLMs in generating speech embeddings that align more effectively with
transcript embeddings. OTReg is lightweight, requiring no additional labels or
learnable parameters, and integrates seamlessly into existing SLM training
procedures. Extensive multilingual ASR experiments demonstrate that OTReg
enhances speech-text alignment, mitigates the modality gap, and consequently
improves SLM generalization across diverse datasets.

</details>


### [87] [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](https://arxiv.org/abs/2508.08139)
*Tianyi Zhou,Johanne Medina,Sanjay Chawla*

Main category: cs.CL

TL;DR: 本文研究了上下文信息对大型语言模型行为的影响，并提出了一种基于不确定性的可靠性估计方法，以改善不可靠输出的检测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）容易生成流畅但错误的内容，这在多轮或代理应用中可能带来风险。本文旨在研究上下文信息如何影响模型行为，并探讨LLMs是否能识别其不可靠的响应。

Method: 本文提出了一种可靠性估计方法，利用token级别的不确定性来指导内部模型表示的聚合。具体来说，从输出logits中计算aleatoric和epistemic不确定性，以识别显著的token，并将它们的隐藏状态聚合为响应级别的可靠性预测。

Result: 通过在开放问答基准上的控制实验，发现正确的上下文信息可以提高答案准确性和模型信心，而误导性上下文通常会导致自信的错误响应，这表明不确定性与正确性之间存在不一致。基于探测的方法捕捉了这些模型行为的变化，并提高了多个开源LLMs中不可靠输出的检测效果。

Conclusion: 本文揭示了直接不确定性信号的局限性，并突出了基于不确定性的探测在可靠性感知生成中的潜力。

Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect
content, known as confabulation, which poses increasing risks in multi-turn or
agentic applications where outputs may be reused as context. In this work, we
investigate how in-context information influences model behavior and whether
LLMs can identify their unreliable responses. We propose a reliability
estimation that leverages token-level uncertainty to guide the aggregation of
internal model representations. Specifically, we compute aleatoric and
epistemic uncertainty from output logits to identify salient tokens and
aggregate their hidden states into compact representations for response-level
reliability prediction. Through controlled experiments on open QA benchmarks,
we find that correct in-context information improves both answer accuracy and
model confidence, while misleading context often induces confidently incorrect
responses, revealing a misalignment between uncertainty and correctness. Our
probing-based method captures these shifts in model behavior and improves the
detection of unreliable outputs across multiple open-source LLMs. These results
underscore the limitations of direct uncertainty signals and highlight the
potential of uncertainty-guided probing for reliability-aware generation.

</details>


### [88] [Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective](https://arxiv.org/abs/2508.08140)
*Jun Wang,Zaifu Zhan,Qixin Zhang,Mingquan Lin,Meijia Song,Rui Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Dual-Div的框架，用于生物医学领域内的上下文学习（ICL）中的演示选择。该框架通过两阶段的检索和排序过程，提高了模型在不同任务上的表现，并显示出对各种挑战的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在从大型语料库中选择示例时，通常更注重代表性而非多样性。为了解决这一问题，我们提出了Dual-Div，一个增强多样性的数据高效框架，用于生物医学ICL中的演示选择。

Method: Dual-Div采用两阶段的检索和排序过程：首先，通过优化代表性和多样性（对于未标记数据可选注释）从语料库中识别出有限的一组候选示例。其次，将这些候选示例与测试查询进行对比，选择最相关且不冗余的演示。

Result: 在三个生物医学NLP任务（命名实体识别（NER）、关系抽取（RE）和文本分类（TC））上评估，使用LLaMA 3.1和Qwen 2.5进行推理，并结合三种检索器（BGE-Large、BMRetriever、MedCPT），Dual-Div始终优于基线，最高可提高5%的宏F1分数，同时表现出对提示排列和类别不平衡的鲁棒性。

Conclusion: 我们的研究结果表明，在初始检索阶段的多样性比排序阶段的优化更为关键，并且将演示限制在3-5个示例可以最大化性能效率。

Abstract: Recent progress in large language models (LLMs) has leveraged their
in-context learning (ICL) abilities to enable quick adaptation to unseen
biomedical NLP tasks. By incorporating only a few input-output examples into
prompts, LLMs can rapidly perform these new tasks. While the impact of these
demonstrations on LLM performance has been extensively studied, most existing
approaches prioritize representativeness over diversity when selecting examples
from large corpora. To address this gap, we propose Dual-Div, a
diversity-enhanced data-efficient framework for demonstration selection in
biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:
First, it identifies a limited set of candidate examples from a corpus by
optimizing both representativeness and diversity (with optional annotation for
unlabeled data). Second, it ranks these candidates against test queries to
select the most relevant and non-redundant demonstrations. Evaluated on three
biomedical NLP tasks (named entity recognition (NER), relation extraction (RE),
and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along
with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently
outperforms baselines-achieving up to 5% higher macro-F1 scores-while
demonstrating robustness to prompt permutations and class imbalance. Our
findings establish that diversity in initial retrieval is more critical than
ranking-stage optimization, and limiting demonstrations to 3-5 examples
maximizes performance efficiency.

</details>


### [89] [REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08149)
*Wentao Jiang,Xiang Feng,Zengmao Wang,Yong Luo,Pingbo Xu,Zhe Chen,Bo Du,Jing Zhang*

Main category: cs.CL

TL;DR: REX-RAG is a framework that improves reinforcement learning for large language models by addressing dead ends in reasoning paths through mixed sampling and policy correction mechanisms.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of LLMs getting trapped in unproductive reasoning paths (dead ends) during policy-driven trajectory sampling, which hampers exploration and undermines effective policy optimization.

Method: REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation) is a novel framework that explores alternative reasoning paths while maintaining rigorous policy learning through principled distributional corrections. It introduces two key innovations: Mixed Sampling Strategy and Policy Correction Mechanism.

Result: REX-RAG achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets.

Conclusion: REX-RAG achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets.

Abstract: Reinforcement learning (RL) is emerging as a powerful paradigm for enabling
large language models (LLMs) to perform complex reasoning tasks. Recent
advances indicate that integrating RL with retrieval-augmented generation (RAG)
allows LLMs to dynamically incorporate external knowledge, leading to more
informed and robust decision making. However, we identify a critical challenge
during policy-driven trajectory sampling: LLMs are frequently trapped in
unproductive reasoning paths, which we refer to as "dead ends", committing to
overconfident yet incorrect conclusions. This severely hampers exploration and
undermines effective policy optimization. To address this challenge, we propose
REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented
Generation), a novel framework that explores alternative reasoning paths while
maintaining rigorous policy learning through principled distributional
corrections. Our approach introduces two key innovations: (1) Mixed Sampling
Strategy, which combines a novel probe sampling method with exploratory prompts
to escape dead ends; and (2) Policy Correction Mechanism, which employs
importance sampling to correct distribution shifts induced by mixed sampling,
thereby mitigating gradient estimation bias. We evaluate it on seven
question-answering benchmarks, and the experimental results show that REX-RAG
achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B
over strong baselines, demonstrating competitive results across multiple
datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.

</details>


### [90] [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)
*Mandira Sawkar,Samay U. Shetty,Deepak Pandita,Tharindu Cyril Weerasooriya,Christopher M. Homan*

Main category: cs.CL

TL;DR: 本文介绍了对DisCo模型的改进，以更好地捕捉注释器分歧，并展示了在多个数据集上的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: LeWiDi 2025共享任务旨在通过软标签分布预测和观点评估来建模注释器分歧，因此需要一种能够同时建模项目级和注释器级标签分布的神经架构。

Method: 我们通过结合注释器元数据、增强输入表示和修改损失函数来扩展DisCo模型，以更好地捕捉分歧模式。

Result: 我们在三个数据集上进行了广泛的实验，证明了在软标签和观点评估指标上的显著改进，并进行了深入的错误和校准分析，突显了改进发生的条件。

Conclusion: 我们的研究强调了考虑分歧建模的重要性，并提供了关于系统组件如何与人类标注数据的复杂性相互作用的见解。

Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task is to model
annotator disagreement through soft label distribution prediction and
perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution
from Context), a neural architecture that jointly models item-level and
annotator-level label distributions, and present detailed analysis and
improvements. In this paper, we extend the DisCo by incorporating annotator
metadata, enhancing input representations, and modifying the loss functions to
capture disagreement patterns better. Through extensive experiments, we
demonstrate substantial improvements in both soft and perspectivist evaluation
metrics across three datasets. We also conduct in-depth error and calibration
analyses, highlighting the conditions under which improvements occur. Our
findings underscore the value of disagreement-aware modeling and offer insights
into how system components interact with the complexity of human-annotated
data.

</details>


### [91] [Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions](https://arxiv.org/abs/2508.08192)
*Bangsheng Tang,Carl Chengyan Fu,Fei Kou,Grigory Sizov,Haoci Zhang,Jason Park,Jiawen Liu,Jie You,Qirui Yang,Sachin Mehta,Shengyong Cai,Xiaodong Wang,Xingyu Liu,Yunlu Li,Yanjun Zhou,Wei Wei,Zhiwei Zhao,Zixi Qi,Adolfo Victoria,Aya Ibrahim,Bram Wasti,Changkyu Kim,Daniel Haziza,Fei Sun,Giancarlo Delfin,Emily Guo,Jialin Ouyang,Jaewon Lee,Jianyu Huang,Jeremy Reizenstein,Lu Fang,Quinn Zhu,Ria Verma,Vlad Mihailescu,Xingwen Guo,Yan Cui,Ye Hu,Yejin Lee*

Main category: cs.CL

TL;DR: 本文介绍了用于实现基于EAGLE的推测解码的训练和推理优化技术，使得Llama模型的推理速度得到了显著提升。


<details>
  <summary>Details</summary>
Motivation: 推测解码是加速大型语言模型推理速度的标准方法。然而，在生产环境中扩展它面临着一些工程挑战，包括在GPU上高效实现不同的操作（例如树注意力和多轮推测解码）。

Method: 本文详细介绍了我们为实现基于EAGLE的推测解码而在生产规模上进行的训练和推理优化技术。

Result: 例如，Llama4 Maverick在8块NVIDIA H100 GPU上以一个批次大小的条件下，每token解码速度约为4毫秒，比之前最好的方法快10%。此外，对于基于EAGLE的推测解码，我们的优化使我们在生产规模下实现了1.4倍至2.0倍的速度提升。

Conclusion: 通过这些改进，我们实现了Llama模型的最新推理延迟状态。

Abstract: Speculative decoding is a standard method for accelerating the inference
speed of large language models. However, scaling it for production environments
poses several engineering challenges, including efficiently implementing
different operations (e.g., tree attention and multi-round speculative
decoding) on GPU. In this paper, we detail the training and inference
optimization techniques that we have implemented to enable EAGLE-based
speculative decoding at a production scale for Llama models. With these
changes, we achieve a new state-of-the-art inference latency for Llama models.
For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a
batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the
previously best known method. Furthermore, for EAGLE-based speculative
decoding, our optimizations enable us to achieve a speed-up for large batch
sizes between 1.4x and 2.0x at production scale.

</details>


### [92] [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](https://arxiv.org/abs/2508.08204)
*Kyle Moore,Jesse Roberts,Daryl Watson*

Main category: cs.CL

TL;DR: 本文评估了推理时间不确定性度量，发现它们与人类不确定性有较强对齐，并展示了模型校准的证据。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估模型不确定性与人类不确定性之间的对齐程度，以改善LLM-用户体验。

Method: 本文评估了一组推理时间不确定性度量，使用了既定指标和新的变体，以确定它们与人类群体级不确定性和传统模型校准概念的对齐程度。

Result: 本文发现，许多度量方法在人类不确定性方面表现出强烈的对齐，即使它们与人类答案偏好没有对齐。对于这些成功的指标，我们发现了模型校准的中等至强证据。

Conclusion: 本文发现，尽管模型不确定性与人类答案偏好没有对齐，但许多度量方法在人类不确定性方面表现出强烈的对齐。对于这些成功的指标，我们发现了模型校准的中等至强证据。

Abstract: There has been much recent interest in evaluating large language models for
uncertainty calibration to facilitate model control and modulate user trust.
Inference time uncertainty, which may provide a real-time signal to the model
or external control modules, is particularly important for applying these
concepts to improve LLM-user experience in practice. While many of the existing
papers consider model calibration, comparatively little work has sought to
evaluate how closely model uncertainty aligns to human uncertainty. In this
work, we evaluate a collection of inference-time uncertainty measures, using
both established metrics and novel variations, to determine how closely they
align with both human group-level uncertainty and traditional notions of model
calibration. We find that numerous measures show evidence of strong alignment
to human uncertainty, even despite the lack of alignment to human answer
preference. For those successful metrics, we find moderate to strong evidence
of model calibration in terms of both correctness correlation and
distributional analysis.

</details>


### [93] [SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling](https://arxiv.org/abs/2508.08211)
*Zhuohao Yu,Xingru Jiang,Weizheng Gu,Yidong Wang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: SAEMark is a new watermarking framework that embeds personalized messages via inference-time, feature-based rejection sampling, preserving text quality and working with closed-source LLMs.


<details>
  <summary>Details</summary>
Motivation: Watermarking LLM-generated text is critical for content attribution and misinformation prevention. However, existing methods compromise text quality, require white-box model access and logit manipulation, which exclude API-based models and multilingual scenarios.

Method: SAEMark is a general framework for post-hoc multi-bit watermarking that embeds personalized messages solely via inference-time, feature-based rejection sampling without altering model logits or requiring training.

Result: Empirically, we demonstrate the framework's effectiveness using Sparse Autoencoders (SAEs), achieving superior detection accuracy and text quality. Experiments across 4 datasets show SAEMark's consistent performance, with 99.7% F1 on English and strong multi-bit detection accuracy.

Conclusion: SAEMark establishes a new paradigm for scalable watermarking that works out-of-the-box with closed-source LLMs while enabling content attribution.

Abstract: Watermarking LLM-generated text is critical for content attribution and
misinformation prevention. However, existing methods compromise text quality,
require white-box model access and logit manipulation. These limitations
exclude API-based models and multilingual scenarios. We propose SAEMark, a
general framework for post-hoc multi-bit watermarking that embeds personalized
messages solely via inference-time, feature-based rejection sampling without
altering model logits or requiring training. Our approach operates on
deterministic features extracted from generated text, selecting outputs whose
feature statistics align with key-derived targets. This framework naturally
generalizes across languages and domains while preserving text quality through
sampling LLM outputs instead of modifying. We provide theoretical guarantees
relating watermark success probability and compute budget that hold for any
suitable feature extractor. Empirically, we demonstrate the framework's
effectiveness using Sparse Autoencoders (SAEs), achieving superior detection
accuracy and text quality. Experiments across 4 datasets show SAEMark's
consistent performance, with 99.7% F1 on English and strong multi-bit detection
accuracy. SAEMark establishes a new paradigm for scalable watermarking that
works out-of-the-box with closed-source LLMs while enabling content
attribution.

</details>


### [94] [Capabilities of GPT-5 on Multimodal Medical Reasoning](https://arxiv.org/abs/2508.08224)
*Shansong Wang,Mingzhe Hu,Qiang Li,Mojtaba Safari,Xiaofeng Yang*

Main category: cs.CL

TL;DR: 本研究评估了GPT-5在医疗领域的多模态推理性能，结果显示GPT-5在多个基准测试中表现优异，超越了GPT-4o和未经许可的人类专家。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，决策往往需要整合异构的信息源，包括患者叙述、结构化数据和医学图像。本研究旨在评估GPT-5作为通用多模态推理器在医疗决策支持中的表现。

Method: 本研究将GPT-5定位为医疗决策支持的通用多模态推理器，并在统一协议下系统评估其零样本思维链推理性能，在基于文本的问题回答和视觉问题回答任务中进行评估。

Result: GPT-5在所有QA基准测试中均优于所有基线，取得了最先进的准确性，并在多模态推理中实现了显著的提升。在MedXpertQA MM上，GPT-5在推理和理解评分上分别比GPT-4o提高了29.62%和36.18%，并在推理和理解上超过了未经许可的人类专家24.23%和29.40%。

Conclusion: 研究结果表明，GPT-5在这些受控的多模态推理基准测试中从与人类相当的性能提升到了高于人类专家的性能。这种改进可能会显著影响未来临床决策支持系统的设计。

Abstract: Recent advances in large language models (LLMs) have enabled general-purpose
systems to perform increasingly complex domain-specific reasoning without
extensive fine-tuning. In the medical domain, decision-making often requires
integrating heterogeneous information sources, including patient narratives,
structured data, and medical images. This study positions GPT-5 as a generalist
multimodal reasoner for medical decision support and systematically evaluates
its zero-shot chain-of-thought reasoning performance on both text-based
question answering and visual question answering tasks under a unified
protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20
against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU
medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that
GPT-5 consistently outperforms all baselines, achieving state-of-the-art
accuracy across all QA benchmarks and delivering substantial gains in
multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and
understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and
surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in
understanding. In contrast, GPT-4o remains below human expert performance in
most dimensions. A representative case study demonstrates GPT-5's ability to
integrate visual and textual cues into a coherent diagnostic reasoning chain,
recommending appropriate high-stakes interventions. Our results show that, on
these controlled multimodal reasoning benchmarks, GPT-5 moves from
human-comparable to above human-expert performance. This improvement may
substantially inform the design of future clinical decision-support systems.

</details>


### [95] [Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge](https://arxiv.org/abs/2508.08236)
*Yunna Cai,Fan Wang,Haowei Wang,Kun Wang,Kailai Yang,Sophia Ananiadou,Moyan Li,Mingming Fan*

Main category: cs.CL

TL;DR: 本文提出了一种用于评估大型语言模型在高风险心理健康对话中安全对齐性的参考无关评估基准PsyCrisis-Bench。该方法采用基于提示的LLM-as-Judge方法，通过专家定义的推理链进行上下文评估，并使用多安全维度的二进制点对点评分来提高评估的可解释性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在高风险心理健康对话中的安全对齐性特别困难，因为缺乏黄金标准答案以及这些互动的伦理敏感性。

Method: 我们采用了一种基于提示的LLM-as-Judge方法，该方法使用专家定义的推理链进行上下文评估，这些推理链基于心理干预原则。我们还采用了多安全维度的二进制点对点评分，以提高评估的可解释性和可追溯性。

Result: 实验显示，我们的方法在与专家评估的一致性方面表现最佳，并且相比现有方法产生了更易解释的评估理由。

Conclusion: 我们的方法在与专家评估的一致性方面取得了最高成就，并且相比现有方法产生了更具解释性的评估理由。我们的数据集和评估工具已公开，以促进进一步的研究。

Abstract: Evaluating the safety alignment of LLM responses in high-risk mental health
dialogues is particularly difficult due to missing gold-standard answers and
the ethically sensitive nature of these interactions. To address this
challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark
based on real-world Chinese mental health dialogues. It evaluates whether the
model responses align with the safety principles defined by experts.
Specifically designed for settings without standard references, our method
adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation
using expert-defined reasoning chains grounded in psychological intervention
principles. We employ binary point-wise scoring across multiple safety
dimensions to enhance the explainability and traceability of the evaluation.
Additionally, we present a manually curated, high-quality Chinese-language
dataset covering self-harm, suicidal ideation, and existential distress,
derived from real-world online discourse. Experiments on 3600 judgments show
that our method achieves the highest agreement with expert assessments and
produces more interpretable evaluation rationales compared to existing
approaches. Our dataset and evaluation tool are publicly available to
facilitate further research.

</details>


### [96] [Jinx: Unlimited LLMs for Probing Alignment Failures](https://arxiv.org/abs/2508.08243)
*Jiahao Zhao,Liwei Dong*

Main category: cs.CL

TL;DR: Jinx is a helpful-only language model that allows researchers to study alignment failures and safety boundaries without refusals or safety filtering.


<details>
  <summary>Details</summary>
Motivation: The research community lacks access to unlimited language models that are used by leading AI companies for red teaming and alignment evaluation. These models are essential for assessing alignment but are not available to researchers.

Method: Jinx is a helpful-only variant of popular open-weight LLMs that responds to all queries without refusals or safety filtering while preserving the base model's capabilities in reasoning and instruction following.

Result: Jinx is introduced as a helpful-only variant of popular open-weight LLMs that can be used by researchers to probe alignment failures, evaluate safety boundaries, and study failure modes in language model safety.

Conclusion: Jinx provides researchers with an accessible tool for probing alignment failures, evaluating safety boundaries, and systematically studying failure modes in language model safety.

Abstract: Unlimited, or so-called helpful-only language models are trained without
safety alignment constraints and never refuse user queries. They are widely
used by leading AI companies as internal tools for red teaming and alignment
evaluation. For example, if a safety-aligned model produces harmful outputs
similar to an unlimited model, this indicates alignment failures that require
further attention. Despite their essential role in assessing alignment, such
models are not available to the research community.
  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx
responds to all queries without refusals or safety filtering, while preserving
the base model's capabilities in reasoning and instruction following. It
provides researchers with an accessible tool for probing alignment failures,
evaluating safety boundaries, and systematically studying failure modes in
language model safety.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [97] [Maestro-EVC: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody](https://arxiv.org/abs/2508.06890)
*Jinsung Yoon,Wooyeol Jeong,Jio Gim,Young-Joo Suh*

Main category: cs.SD

TL;DR: Maestro-EVC is a controllable EVC framework that enables independent control of content, speaker identity, and emotion by effectively disentangling each attribute from separate references. It introduces a temporal emotion representation and explicit prosody modeling with prosody augmentation to robustly capture and transfer the temporal dynamics of the target emotion.


<details>
  <summary>Details</summary>
Motivation: Existing methods often struggle to fully disentangle speaker identity and emotional style and lack the ability to model fine-grained emotional expressions such as temporal dynamics.

Method: Maestro-EVC is a controllable EVC framework that enables independent control of content, speaker identity, and emotion by effectively disentangling each attribute from separate references. It introduces a temporal emotion representation and explicit prosody modeling with prosody augmentation to capture and transfer the temporal dynamics of the target emotion.

Result: Experimental results confirm that Maestro-EVC achieves high-quality, controllable, and emotionally expressive speech synthesis.

Conclusion: Maestro-EVC achieves high-quality, controllable, and emotionally expressive speech synthesis.

Abstract: Emotional voice conversion (EVC) aims to modify the emotional style of speech
while preserving its linguistic content. In practical EVC, controllability, the
ability to independently control speaker identity and emotional style using
distinct references, is crucial. However, existing methods often struggle to
fully disentangle these attributes and lack the ability to model fine-grained
emotional expressions such as temporal dynamics. We propose Maestro-EVC, a
controllable EVC framework that enables independent control of content, speaker
identity, and emotion by effectively disentangling each attribute from separate
references. We further introduce a temporal emotion representation and an
explicit prosody modeling with prosody augmentation to robustly capture and
transfer the temporal dynamics of the target emotion, even under
prosody-mismatched conditions. Experimental results confirm that Maestro-EVC
achieves high-quality, controllable, and emotionally expressive speech
synthesis.

</details>


### [98] [Joint Transcription of Acoustic Guitar Strumming Directions and Chords](https://arxiv.org/abs/2508.07973)
*Sebastian Murgul,Johannes Schimper,Michael Heizmann*

Main category: cs.SD

TL;DR: 本文提出了一种新的吉他扫弦转录方法，通过引入一个真实和合成数据集以及基于深度学习的模型，取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 自动吉他扫弦转录在音乐信息检索（MIR）中是一个研究不足且具有挑战性的任务，特别是在从音频信号中提取扫弦方向和和弦进行方面。现有的方法虽然有潜力，但其效果常常受到数据集有限的阻碍。

Method: 我们引入了一个新的数据集和基于深度学习的转录模型。使用ESP32智能手表运动传感器和结构化录音协议收集了90分钟的真实吉他录音，并补充了4小时的标记扫弦音频的合成数据集。训练了一个卷积循环神经网络（CRNN）模型，以仅使用麦克风音频检测扫弦事件、分类其方向并识别相应的和弦。

Result: 评估结果显示了对基线起始检测算法的显著改进，结合合成和真实数据的混合方法在扫弦动作检测和和弦分类方面达到了最高的准确率。

Conclusion: 这些结果突显了深度学习在稳健的吉他扫弦转录中的潜力，并为自动节奏吉他分析开辟了新途径。

Abstract: Automatic transcription of guitar strumming is an underrepresented and
challenging task in Music Information Retrieval (MIR), particularly for
extracting both strumming directions and chord progressions from audio signals.
While existing methods show promise, their effectiveness is often hindered by
limited datasets. In this work, we extend a multimodal approach to guitar
strumming transcription by introducing a novel dataset and a deep
learning-based transcription model. We collect 90 min of real-world guitar
recordings using an ESP32 smartwatch motion sensor and a structured recording
protocol, complemented by a synthetic dataset of 4h of labeled strumming audio.
A Convolutional Recurrent Neural Network (CRNN) model is trained to detect
strumming events, classify their direction, and identify the corresponding
chords using only microphone audio. Our evaluation demonstrates significant
improvements over baseline onset detection algorithms, with a hybrid method
combining synthetic and real-world data achieving the highest accuracy for both
strumming action detection and chord classification. These results highlight
the potential of deep learning for robust guitar strumming transcription and
open new avenues for automatic rhythm guitar analysis.

</details>


### [99] [Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription](https://arxiv.org/abs/2508.07987)
*Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: 本研究探讨了一种程序化数据生成流程，以替代真实音频记录来训练转录模型。通过四个阶段合成训练数据，并在真实和合成数据集上训练和评估基于CRNN的音符跟踪模型，结果显示程序数据可以实现合理的音符跟踪结果，并且用少量真实数据微调能进一步提高准确性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标记的训练数据和与音乐录音相关的法律限制，自动转录原声吉他指弹表演仍然是一个具有挑战性的任务。

Method: 我们的方法通过四个阶段合成训练数据：基于知识的指弹乐谱创作、MIDI性能渲染、使用扩展的Karplus-Strong算法进行物理建模，以及包括混响和失真在内的音频增强。

Result: 我们在真实和合成数据集上训练和评估了一个基于CRNN的音符跟踪模型，证明了程序数据可以用来实现合理的音符跟踪结果。用少量的真实数据进行微调进一步提高了转录准确性，超过了仅在真实录音上训练的模型。

Conclusion: 这些结果突显了程序生成音频在数据稀缺的音乐信息检索任务中的潜力。

Abstract: Automatic transcription of acoustic guitar fingerpicking performances remains
a challenging task due to the scarcity of labeled training data and legal
constraints connected with musical recordings. This work investigates a
procedural data generation pipeline as an alternative to real audio recordings
for training transcription models. Our approach synthesizes training data
through four stages: knowledge-based fingerpicking tablature composition, MIDI
performance rendering, physical modeling using an extended Karplus-Strong
algorithm, and audio augmentation including reverb and distortion. We train and
evaluate a CRNN-based note-tracking model on both real and synthetic datasets,
demonstrating that procedural data can be used to achieve reasonable
note-tracking results. Finetuning with a small amount of real data further
enhances transcription accuracy, improving over models trained exclusively on
real recordings. These results highlight the potential of procedurally
generated audio for data-scarce music information retrieval tasks.

</details>


### [100] [Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning](https://arxiv.org/abs/2508.08039)
*Shu Wu,Chenxing Li,Wenfu Wang,Hao Zhang,Hualei Wang,Meng Yu,Dong Yu*

Main category: cs.SD

TL;DR: 本文提出了一种名为Audio-Thinker的强化学习框架，旨在提高大型音频语言模型（LALMs）的推理能力。该框架通过引入自适应思考准确度奖励和外部奖励模型，使模型能够动态调整推理策略并评估推理过程的一致性和质量。实验结果表明，Audio-Thinker在多个基准任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 最近在大型语言模型、多模态大型语言模型和大型音频语言模型（LALMs）方面的进展通过基于规则的奖励强化学习显著提高了它们的推理能力。然而，显式的推理过程尚未在音频问答中显示出显著的好处，有效地利用深度推理仍然是一个开放挑战，LALMs仍然未能达到人类水平的听觉-语言推理。

Method: 我们提出了Audio-Thinker，这是一种强化学习框架，旨在增强LALMs的推理能力，重点是提高适应性、一致性和有效性。我们的方法引入了一种自适应思考准确度奖励，使模型能够根据任务复杂性动态调整其推理策略。此外，我们还结合了一个外部奖励模型来评估推理过程的整体一致性和质量，并通过基于思考的奖励帮助模型在训练过程中区分有效和有缺陷的推理路径。

Result: 实验结果表明，我们的Audio-Thinker模型在各种基准任务中优于现有的以推理为导向的LALMs，表现出更优越的推理和泛化能力。

Conclusion: 实验结果表明，我们的Audio-Thinker模型在各种基准任务中优于现有的以推理为导向的LALMs，表现出更优越的推理和泛化能力。

Abstract: Recent advancements in large language models, multimodal large language
models, and large audio language models (LALMs) have significantly improved
their reasoning capabilities through reinforcement learning with rule-based
rewards. However, the explicit reasoning process has yet to show significant
benefits for audio question answering, and effectively leveraging deep
reasoning remains an open challenge, with LALMs still falling short of
human-level auditory-language reasoning. To address these limitations, we
propose Audio-Thinker, a reinforcement learning framework designed to enhance
the reasoning capabilities of LALMs, with a focus on improving adaptability,
consistency, and effectiveness. Our approach introduces an adaptive think
accuracy reward, enabling the model to adjust its reasoning strategies based on
task complexity dynamically. Furthermore, we incorporate an external reward
model to evaluate the overall consistency and quality of the reasoning process,
complemented by think-based rewards that help the model distinguish between
valid and flawed reasoning paths during training. Experimental results
demonstrate that our Audio-Thinker model outperforms existing
reasoning-oriented LALMs across various benchmark tasks, exhibiting superior
reasoning and generalization capabilities.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [101] [Story Ribbons: Reimagining Storyline Visualizations with Large Language Models](https://arxiv.org/abs/2508.06772)
*Catherine Yeh,Tara Menon,Robin Singh Arya,Helen He,Moira Weigel,Fernanda Viégas,Martin Wattenberg*

Main category: cs.HC

TL;DR: 本文介绍了一种基于大型语言模型的叙事信息提取方法，并创建了Story Ribbons可视化系统，以帮助文学分析者探索角色和主题的轨迹。


<details>
  <summary>Details</summary>
Motivation: 本文旨在利用大型语言模型的文本处理和分析能力，增强和重新构想现有的叙事可视化技术。

Method: 本文引入了一个基于大型语言模型的数据解析流程，用于从小说和剧本中自动提取相关的叙事信息，并应用该流程创建了Story Ribbons交互式可视化系统。

Result: 通过管道评估和使用Story Ribbons对36部文学作品的用户研究，证明了大型语言模型在简化叙事可视化创建和揭示关于熟悉故事的新见解方面的潜力。

Conclusion: 本文展示了大型语言模型在叙事可视化创建中的潜力，并描述了当前AI系统的局限性以及为解决这些问题设计的交互模式。

Abstract: Analyzing literature involves tracking interactions between characters,
locations, and themes. Visualization has the potential to facilitate the
mapping and analysis of these complex relationships, but capturing structured
information from unstructured story data remains a challenge. As large language
models (LLMs) continue to advance, we see an opportunity to use their text
processing and analysis capabilities to augment and reimagine existing
storyline visualization techniques. Toward this goal, we introduce an
LLM-driven data parsing pipeline that automatically extracts relevant narrative
information from novels and scripts. We then apply this pipeline to create
Story Ribbons, an interactive visualization system that helps novice and expert
literary analysts explore detailed character and theme trajectories at multiple
narrative levels. Through pipeline evaluations and user studies with Story
Ribbons on 36 literary works, we demonstrate the potential of LLMs to
streamline narrative visualization creation and reveal new insights about
familiar stories. We also describe current limitations of AI-based systems, and
interaction motifs designed to address these issues.

</details>


### [102] [Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI](https://arxiv.org/abs/2508.07520)
*Baihan Lin*

Main category: cs.HC

TL;DR: 本文介绍了对话DNA，这是一种将对话视为活系统的新型视觉语言，通过生物隐喻揭示对话的时间架构，并展示了其在治疗对话和重要人机对话中的应用。


<details>
  <summary>Details</summary>
Motivation: 我们想知道对话中隐藏的模式是否比单词本身更能揭示交流。

Method: 我们引入了对话DNA，这是一种新颖的视觉语言，将任何对话（无论是人与人之间、人与AI之间还是群体之间）视为具有可解释结构的活系统，可以可视化、比较和理解。我们的方法通过生物隐喻揭示对话的时间架构。

Result: 通过探索性分析治疗对话和历史上重要的一个人工智能对话，我们展示了这种可视化方法如何揭示传统方法所忽略的互动模式。

Conclusion: 我们的工作为理解交流提供了一个新的创造性框架，它连接了数据可视化、人机交互以及在人类越来越多地与人工智能交谈的时代，什么使对话有意义的基本问题。

Abstract: What if the patterns hidden within dialogue reveal more about communication
than the words themselves? We introduce Conversational DNA, a novel visual
language that treats any dialogue -- whether between humans, between human and
AI, or among groups -- as a living system with interpretable structure that can
be visualized, compared, and understood. Unlike traditional conversation
analysis that reduces rich interaction to statistical summaries, our approach
reveals the temporal architecture of dialogue through biological metaphors.
Linguistic complexity flows through strand thickness, emotional trajectories
cascade through color gradients, conversational relevance forms through
connecting elements, and topic coherence maintains structural integrity through
helical patterns. Through exploratory analysis of therapeutic conversations and
historically significant human-AI dialogues, we demonstrate how this
visualization approach reveals interaction patterns that traditional methods
miss. Our work contributes a new creative framework for understanding
communication that bridges data visualization, human-computer interaction, and
the fundamental question of what makes dialogue meaningful in an age where
humans increasingly converse with artificial minds.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [103] [ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/abs/2508.07050)
*Wenhan Liu,Xinyu Ma,Weiwei Sun,Yutao Zhu,Yuchen Li,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: 本文提出了一种自动化推理密集型训练数据合成框架和两阶段后训练方法，以提高列表排序性能。实验表明，我们的ReasonRank在BRIGHT排行榜上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于推理密集型训练数据的稀缺性，现有的重排序器在许多复杂排名场景中表现不佳，推理密集型重排序器的排名能力尚未得到充分发展。

Method: 我们首先提出了一个自动推理密集型训练数据合成框架，然后提出了一种两阶段的后训练方法，包括冷启动监督微调阶段和强化学习阶段。在强化学习阶段，我们设计了一个多视图排名奖励。

Result: 我们的训练有素的推理密集型重排序器ReasonRank显著优于现有基线，并且比点对点重排序器Rank1具有更低的延迟。通过进一步实验，我们的ReasonRank在BRIGHT排行榜上达到了最先进的性能40.6。

Conclusion: 我们的训练有素的推理密集型重排序器ReasonRank在BRIGHT排行榜上达到了最先进的性能40.6，并且比点对点重排序器Rank1具有更低的延迟。

Abstract: Large Language Model (LLM) based listwise ranking has shown superior
performance in many passage ranking tasks. With the development of Large
Reasoning Models, many studies have demonstrated that step-by-step reasoning
during test-time helps improve listwise ranking performance. However, due to
the scarcity of reasoning-intensive training data, existing rerankers perform
poorly in many complex ranking scenarios and the ranking ability of
reasoning-intensive rerankers remains largely underdeveloped. In this paper, we
first propose an automated reasoning-intensive training data synthesis
framework, which sources training queries and passages from diverse domains and
applies DeepSeek-R1 to generate high-quality training labels. A
self-consistency data filtering mechanism is designed to ensure the data
quality. To empower the listwise reranker with strong reasoning ability, we
further propose a two-stage post-training approach, which includes a cold-start
supervised fine-tuning (SFT) stage for reasoning pattern learning and a
reinforcement learning (RL) stage for further ranking ability enhancement.
During the RL stage, based on the nature of listwise ranking, we design a
multi-view ranking reward, which is more effective than a ranking metric-based
reward. Extensive experiments demonstrate that our trained reasoning-intensive
reranker \textbf{ReasonRank} outperforms existing baselines significantly and
also achieves much lower latency than pointwise reranker Rank1. \textbf{Through
further experiments, our ReasonRank has achieved state-of-the-art (SOTA)
performance 40.6 on the BRIGHT
leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are
available at https://github.com/8421BCD/ReasonRank.

</details>


### [104] [PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization](https://arxiv.org/abs/2508.07342)
*Kepu Zhang,Teng Shi,Weijie Yu,Jun Xu*

Main category: cs.IR

TL;DR: 本文提出了一种名为PrLM的强化学习框架，用于个性化检索增强生成，通过显式推理检索到的用户资料来提高响应与用户偏好的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要关注于改进检索，并依赖大型语言模型（LLMs）隐式地将检索到的上下文与查询结合。然而，这些模型对检索质量敏感，并可能生成与用户偏好不一致的响应。

Method: PrLM是一种基于强化学习的框架，它训练大型语言模型（LLMs）显式地推理检索到的用户资料。

Result: 实验表明，PrLM在三个个性化文本生成数据集上表现优于现有方法，并在不同数量的检索资料和不同检索器下保持稳健。

Conclusion: PrLM在三个个性化文本生成数据集上表现优于现有方法，并在不同数量的检索资料和不同检索器下保持稳健。

Abstract: Personalized retrieval-augmented generation (RAG) aims to produce
user-tailored responses by incorporating retrieved user profiles alongside the
input query. Existing methods primarily focus on improving retrieval and rely
on large language models (LLMs) to implicitly integrate the retrieved context
with the query. However, such models are often sensitive to retrieval quality
and may generate responses that are misaligned with user preferences. To
address this limitation, we propose PrLM, a reinforcement learning framework
that trains LLMs to explicitly reason over retrieved user profiles. Guided by a
contrastively trained personalization reward model, PrLM effectively learns
from user responses without requiring annotated reasoning paths. Experiments on
three personalized text generation datasets show that PrLM outperforms existing
methods and remains robust across varying numbers of retrieved profiles and
different retrievers.

</details>


### [105] [Improving Document Retrieval Coherence for Semantically Equivalent Queries](https://arxiv.org/abs/2508.07975)
*Stefano Campese,Alessandro Moschitti,Ivano Lauriola*

Main category: cs.IR

TL;DR: 本文提出了一种新的损失函数，用于训练密集检索模型，以提高模型在面对语义相似查询时的一致性，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 之前的DR模型对查询和文档的词汇很敏感，小的变化可能导致检索结果显著不同。因此需要一种方法来减少这种敏感性并提高准确性。

Method: 提出了一种多负样本排序损失的变体，用于训练密集检索模型，以提高模型在面对语义相似查询时检索相同文档的一致性。

Result: 在多个数据集上的实验结果表明，使用该损失函数优化的模型具有较低的敏感性和更高的准确性。

Conclusion: 模型通过我们的损失函数优化后，对查询的变化更不敏感，并且准确性更高。

Abstract: Dense Retrieval (DR) models have proven to be effective for Document
Retrieval and Information Grounding tasks. Usually, these models are trained
and optimized for improving the relevance of top-ranked documents for a given
query. Previous work has shown that popular DR models are sensitive to the
query and document lexicon: small variations of it may lead to a significant
difference in the set of retrieved documents. In this paper, we propose a
variation of the Multi-Negative Ranking loss for training DR that improves the
coherence of models in retrieving the same documents with respect to
semantically similar queries. The loss penalizes discrepancies between the
top-k ranked documents retrieved for diverse but semantic equivalent queries.
We conducted extensive experiments on various datasets, MS-MARCO, Natural
Questions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes
by our loss are subject to lower sensitivity, and, (ii) interestingly, higher
accuracy.

</details>


### [106] [HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches](https://arxiv.org/abs/2508.08088)
*Jiejun Tan,Zhicheng Dou,Yan Yu,Jiehan Cheng,Qiang Ju,Jian Xie,Ji-Rong Wen*

Main category: cs.IR

TL;DR: 本文提出了一种分层代理深度搜索框架HierSearch，通过分层强化学习进行训练。在低层，训练本地和网络深度搜索代理来从其对应领域检索证据。在高层，规划代理协调低层代理并提供最终答案。此外，为了防止直接复制答案和错误传播，设计了一个知识精炼器来过滤掉低层代理返回的幻觉和无关证据。实验表明，HierSearch在六个基准测试中表现优于平面RL和各种深度搜索及多源检索增强生成基线。


<details>
  <summary>Details</summary>
Motivation: Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools.

Method: We propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents.

Result: Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains.

Conclusion: HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains.

Abstract: Recently, large reasoning models have demonstrated strong mathematical and
coding abilities, and deep search leverages their reasoning capabilities in
challenging information retrieval tasks. Existing deep search works are
generally limited to a single knowledge source, either local or the Web.
However, enterprises often require private deep search systems that can
leverage search tools over both local and the Web corpus. Simply training an
agent equipped with multiple search tools using flat reinforcement learning
(RL) is a straightforward idea, but it has problems such as low training data
efficiency and poor mastery of complex tools. To address the above issue, we
propose a hierarchical agentic deep search framework, HierSearch, trained with
hierarchical RL. At the low level, a local deep search agent and a Web deep
search agent are trained to retrieve evidence from their corresponding domains.
At the high level, a planner agent coordinates low-level agents and provides
the final answer. Moreover, to prevent direct answer copying and error
propagation, we design a knowledge refiner that filters out hallucinations and
irrelevant evidence returned by low-level agents. Experiments show that
HierSearch achieves better performance compared to flat RL, and outperforms
various deep search and multi-source retrieval-augmented generation baselines
in six benchmarks across general, finance, and medical domains.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [107] [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
*Keyu Li,Mohan Jiang,Dayuan Fu,Yunze Wu,Xiangkun Hu,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文介绍了DatasetResearch基准测试，评估AI代理发现和合成数据集的能力，并揭示了当前技术的局限性，为未来的自改进AI系统提供了基础。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，AI发展的瓶颈已从计算能力转移到数据可用性。研究者需要能够自主发现满足特定用户需求的数据集，以实现真正的数据整理。

Method: 本文引入了DatasetResearch，这是一个全面的基准测试，用于评估AI代理从208个现实需求中发现和合成数据集的能力。使用三维度评估框架来分析AI代理的表现。

Result: 即使先进的深度研究系统在DatasetResearch-pro子集上的得分也只有22%，这表明当前能力与理想的数据集发现之间存在巨大差距。分析揭示了搜索代理和合成代理在不同任务中的表现差异，以及它们在“边缘情况”下的失败。

Conclusion: 本文提出了一个基准测试DatasetResearch，用于评估AI代理发现和合成数据集的能力。分析揭示了当前能力与完美数据集发现之间的巨大差距，并为下一代自改进AI系统奠定了基础。

Abstract: The rapid advancement of large language models has fundamentally shifted the
bottleneck in AI development from computational power to data availability-with
countless valuable datasets remaining hidden across specialized repositories,
research appendices, and domain platforms. As reasoning capabilities and deep
research methodologies continue to evolve, a critical question emerges: can AI
agents transcend conventional search to systematically discover any dataset
that meets specific user requirements, enabling truly autonomous demand-driven
data curation? We introduce DatasetResearch, the first comprehensive benchmark
evaluating AI agents' ability to discover and synthesize datasets from 208
real-world demands across knowledge-intensive and reasoning-intensive tasks.
Our tri-dimensional evaluation framework reveals a stark reality: even advanced
deep research systems achieve only 22% score on our challenging
DatasetResearch-pro subset, exposing the vast gap between current capabilities
and perfect dataset discovery. Our analysis uncovers a fundamental
dichotomy-search agents excel at knowledge tasks through retrieval breadth,
while synthesis agents dominate reasoning challenges via structured
generation-yet both catastrophically fail on "corner cases" outside existing
distributions. These findings establish the first rigorous baseline for dataset
discovery agents and illuminate the path toward AI systems capable of finding
any dataset in the digital universe. Our benchmark and comprehensive analysis
provide the foundation for the next generation of self-improving AI systems and
are publicly available at https://github.com/GAIR-NLP/DatasetResearch.

</details>


### [108] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: 本文提出了MultiMedEdit，这是第一个针对临床多模态任务中知识编辑（KE）评估的基准。我们的框架涵盖了理解和推理任务类型，定义了一个三维度量套件（可靠性、普遍性和局部性），并支持跨范式的比较。实验结果表明，当前方法在泛化和长尾推理方面存在困难，特别是在复杂的临床工作流程中。此外，我们还进行了效率分析，揭示了在现实世界部署中不同KE范式之间的实际权衡。总体而言，MultiMedEdit不仅揭示了当前方法的局限性，还为未来开发临床稳健的知识编辑技术提供了坚实的基础。


<details>
  <summary>Details</summary>
Motivation: Little attention has been paid to KE in multimodal medical scenarios. Unlike text-only settings, medical KE demands integrating updated knowledge with visual reasoning to support safe and interpretable clinical decisions.

Method: We propose MultiMedEdit, the first benchmark tailored to evaluating KE in clinical multimodal tasks. Our framework spans both understanding and reasoning task types, defines a three-dimensional metric suite (reliability, generality, and locality), and supports cross-paradigm comparisons across general and domain-specific models.

Result: Results suggest that current methods struggle with generalization and long-tail reasoning, particularly in complex clinical workflows. We further present an efficiency analysis (e.g., edit latency, memory footprint), revealing practical trade-offs in real-world deployment across KE paradigms.

Conclusion: MultiMedEdit not only reveals the limitations of current approaches but also provides a solid foundation for developing clinically robust knowledge editing techniques in the future.

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [109] [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
*Yi Tang,Kaini Wang,Yang Chen,Guangquan Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种名为EndoAgent的新型AI系统，用于支持内窥镜图像诊断。该系统通过结合迭代推理和自适应工具选择与协作，实现了复杂的临床工作流程中的决策。实验表明，EndoAgent在性能上优于通用和医学多模态模型，展示了其强大的灵活性和推理能力。


<details>
  <summary>Details</summary>
Motivation: Existing methods based on large-scale pretraining often lack unified coordination across tasks and struggle to handle the multi-step processes required in complex clinical workflows. While AI agents have shown promise in flexible instruction parsing and tool integration across domains, their potential in endoscopy remains underexplored.

Method: EndoAgent is a memory-guided agent for vision-to-decision endoscopic analysis that integrates iterative reasoning with adaptive tool selection and collaboration. It uses a dual-memory design to ensure logical coherence through short-term action tracking and progressively enhance reasoning acuity through long-term experiential learning. EndoAgent integrates a suite of expert-designed tools within a unified reasoning loop.

Result: Extensive experiments show that EndoAgent consistently outperforms both general and medical multimodal models, exhibiting its strong flexibility and reasoning capabilities.

Conclusion: EndoAgent consistently outperforms both general and medical multimodal models, exhibiting its strong flexibility and reasoning capabilities.

Abstract: Developing general artificial intelligence (AI) systems to support endoscopic
image diagnosis is an emerging research priority. Existing methods based on
large-scale pretraining often lack unified coordination across tasks and
struggle to handle the multi-step processes required in complex clinical
workflows. While AI agents have shown promise in flexible instruction parsing
and tool integration across domains, their potential in endoscopy remains
underexplored. To address this gap, we propose EndoAgent, the first
memory-guided agent for vision-to-decision endoscopic analysis that integrates
iterative reasoning with adaptive tool selection and collaboration. Built on a
dual-memory design, it enables sophisticated decision-making by ensuring
logical coherence through short-term action tracking and progressively
enhancing reasoning acuity through long-term experiential learning. To support
diverse clinical tasks, EndoAgent integrates a suite of expert-designed tools
within a unified reasoning loop. We further introduce EndoAgentBench, a
benchmark of 5,709 visual question-answer pairs that assess visual
understanding and language generation capabilities in realistic scenarios.
Extensive experiments show that EndoAgent consistently outperforms both general
and medical multimodal models, exhibiting its strong flexibility and reasoning
capabilities.

</details>


### [110] [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
*Rubing Chen,Jiaxin Wu,Jian Wang,Xulu Zhang,Wenqi Fan,Chenghua Lin,Xiao-Yong Wei,Qing Li*

Main category: cs.AI

TL;DR: 本文提出了基于全面性和紧凑性原则的Comp-Comp框架，用于构建领域特定的基准测试，并创建了XUBench作为案例研究的结果。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定基准主要关注扩展定律，依赖大量语料进行监督微调或生成广泛的问答集以实现广泛覆盖。然而，语料和问答集设计对领域特定LLMs的精确度和召回率的影响仍未被探索。

Method: 本文提出了一种基于全面性和紧凑性原则的迭代基准测试框架Comp-Comp，通过确保领域语义召回和提高精度来指导语料库和QA集的构建。

Result: 通过在一所著名大学进行案例研究，我们创建了XUBench，这是一个大规模且全面的封闭领域基准测试。

Conclusion: 本文提出了Comp-Comp框架，该框架基于全面性和紧凑性原则，用于构建领域特定的基准测试。此外，我们创建了XUBench，这是一个大规模且全面的封闭领域基准测试，并展示了该框架在学术领域以外的扩展性。

Abstract: Numerous benchmarks have been built to evaluate the domain-specific abilities
of large language models (LLMs), highlighting the need for effective and
efficient benchmark construction. Existing domain-specific benchmarks primarily
focus on the scaling law, relying on massive corpora for supervised fine-tuning
or generating extensive question sets for broad coverage. However, the impact
of corpus and question-answer (QA) set design on the precision and recall of
domain-specific LLMs remains unexplored. In this paper, we address this gap and
demonstrate that the scaling law is not always the optimal principle for
benchmark construction in specific domains. Instead, we propose Comp-Comp, an
iterative benchmarking framework based on a comprehensiveness-compactness
principle. Here, comprehensiveness ensures semantic recall of the domain, while
compactness enhances precision, guiding both corpus and QA set construction. To
validate our framework, we conducted a case study in a well-renowned
university, resulting in the creation of XUBench, a large-scale and
comprehensive closed-domain benchmark. Although we use the academic domain as
the case in this work, our Comp-Comp framework is designed to be extensible
beyond academia, providing valuable insights for benchmark construction across
various domains.

</details>


### [111] [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
*Jesse Ponnock*

Main category: cs.AI

TL;DR: 本文探讨了生成式人工智能在大型政府组织战略计划开发中的应用，评估了BERTopic和NMF在生成主题方面的性能，并发现BERTopic表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能（GAI）和大型语言模型（LLMs）的最新突破，越来越多的专业服务正在通过人工智能得到增强，这曾经似乎不可能自动化。本文旨在探索GAI在大型政府组织战略计划开发中的应用。

Method: 本文提出了一个模块化模型，利用GAI来开发大型政府组织的战略计划，并评估了领先的机器学习技术在其应用中的表现。具体来说，评估了BERTopic和非负矩阵分解（NMF）在使用主题建模生成代表战略计划中愿景要素的主题方面的性能。

Result: 结果表明，这些技术能够生成与被评估的100%要素相似的主题。此外，BERTopic在这一应用中表现最佳，其超过一半的相关主题达到了“中等”或“强”相关性。

Conclusion: 本文得出结论，BERTopic在生成与战略计划中的愿景要素相似的主题方面表现最佳，超过一半的关联主题达到了“中等”或“强”相关性。GAI-enabled的战略规划开发能力影响着一个价值数十亿美元的行业，并帮助联邦政府克服对公共利益至关重要的监管要求。

Abstract: Given recent breakthroughs in Generative Artificial Intelligence (GAI) and
Large Language Models (LLMs), more and more professional services are being
augmented through Artificial Intelligence (AI), which once seemed impossible to
automate. This paper presents a modular model for leveraging GAI in developing
strategic plans for large scale government organizations and evaluates leading
machine learning techniques in their application towards one of the identified
modules. Specifically, the performance of BERTopic and Non-negative Matrix
Factorization (NMF) are evaluated in their ability to use topic modeling to
generate themes representative of Vision Elements within a strategic plan. To
accomplish this, BERTopic and NMF models are trained using a large volume of
reports from the Government Accountability Office (GAO). The generated topics
from each model are then scored for similarity against the Vision Elements of a
published strategic plan and the results are compared. Our results show that
these techniques are capable of generating themes similar to 100% of the
elements being evaluated against. Further, we conclude that BERTopic performs
best in this application with more than half of its correlated topics achieving
a "medium" or "strong" correlation. A capability of GAI-enabled strategic plan
development impacts a multi-billion dollar industry and assists the federal
government in overcoming regulatory requirements which are crucial to the
public good. Further work will focus on the operationalization of the concept
proven in this study as well as viability of the remaining modules in the
proposed model for GAI-generated strategic plans.

</details>


### [112] [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
*Jinyuan Fang,Yanwen Peng,Xi Zhang,Yingxu Wang,Xinhao Yi,Guibin Zhang,Yi Xu,Bin Wu,Siwei Liu,Zihao Li,Zhaochun Ren,Nikos Aletras,Xi Wang,Han Zhou,Zaiqiao Meng*

Main category: cs.AI

TL;DR: 本文综述了自进化代理系统的技术，提供了一个统一的概念框架，并讨论了特定领域的进化策略以及评估、安全和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统依赖于手动构建的配置，这些配置在部署后保持静态，限制了它们适应动态和不断变化的环境的能力。因此，需要探索自动增强代理系统的代理进化技术。

Method: 本文提供了一个统一的概念框架，抽象了自进化代理系统设计背后的反馈循环，并系统地回顾了针对代理系统不同组件的各种自进化技术。

Result: 本文提供了对现有自进化代理系统技术的全面综述，并探讨了特定领域的进化策略，以及评估、安全和伦理考虑因素。

Conclusion: 本文旨在为研究人员和实践者提供对自进化人工智能代理的系统理解，为开发更适应、自主和终身的代理系统奠定基础。

Abstract: Recent advances in large language models have sparked growing interest in AI
agents capable of solving complex, real-world tasks. However, most existing
agent systems rely on manually crafted configurations that remain static after
deployment, limiting their ability to adapt to dynamic and evolving
environments. To this end, recent research has explored agent evolution
techniques that aim to automatically enhance agent systems based on interaction
data and environmental feedback. This emerging direction lays the foundation
for self-evolving AI agents, which bridge the static capabilities of foundation
models with the continuous adaptability required by lifelong agentic systems.
In this survey, we provide a comprehensive review of existing techniques for
self-evolving agentic systems. Specifically, we first introduce a unified
conceptual framework that abstracts the feedback loop underlying the design of
self-evolving agentic systems. The framework highlights four key components:
System Inputs, Agent System, Environment, and Optimisers, serving as a
foundation for understanding and comparing different strategies. Based on this
framework, we systematically review a wide range of self-evolving techniques
that target different components of the agent system. We also investigate
domain-specific evolution strategies developed for specialised fields such as
biomedicine, programming, and finance, where optimisation objectives are
tightly coupled with domain constraints. In addition, we provide a dedicated
discussion on the evaluation, safety, and ethical considerations for
self-evolving agentic systems, which are critical to ensuring their
effectiveness and reliability. This survey aims to provide researchers and
practitioners with a systematic understanding of self-evolving AI agents,
laying the foundation for the development of more adaptive, autonomous, and
lifelong agentic systems.

</details>


### [113] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 本文提出了一种新的纯代理策略，无需固定流水线，通过精心设计的项目提示注入领域专业知识，结合通用编码工具和动态测试、调试和验证功能，成功解决了所有基准问题。


<details>
  <summary>Details</summary>
Motivation: 将自然语言问题描述翻译成形式化约束模型仍然是约束编程中的一个基本挑战，需要在问题领域和建模框架方面有深入的专业知识。之前的自动化翻译方法采用了固定的流水线，但在大量基准问题上失败了。

Method: 我们开发了一个基于ReAct（Reason and Act）原理的通用Python编码代理，利用持久的IPython内核进行状态化代码执行和迭代开发。

Result: 该架构成功解决了CP-Bench约束编程基准集中的所有101个问题。

Conclusion: 结果表明，约束建模任务需要结合通用编码工具和通过提示编码的领域专业知识，而不是专门的代理架构或预定义的工作流程。

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [114] [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
*Alexander Duffy,Samuel J Paech,Ishana Shastri,Elizabeth Karpinski,Baptiste Alloui-Cros,Tyler Marques,Matthew Lyle Olson*

Main category: cs.AI

TL;DR: 本文介绍了一种新的评估工具，使任何本地大型语言模型都能在不进行微调或专门训练的情况下玩完整的Diplomacy游戏。


<details>
  <summary>Details</summary>
Motivation: 之前的文献需要前沿LLM或微调，因为Diplomacy的游戏状态具有高复杂性和信息密度。结合比赛的高方差，这些因素使得Diplomacy难以研究。

Method: 我们使用数据驱动的迭代来优化文本游戏状态表示，使得一个24B模型可以在没有任何微调的情况下可靠地完成比赛。我们开发了工具以促进假设测试和统计分析，并介绍了关于说服、激进玩法和不同模型性能的案例研究。

Result: 我们进行了多种实验，发现较大的模型表现最好，但较小的模型仍然可以适当进行游戏。我们还引入了关键状态分析：一种快速迭代和深入分析游戏中关键时刻的实验协议。

Conclusion: 我们的工具消除了对微调的需要，使战略推理的评估民主化，并提供了关于这些能力如何自然从广泛使用的LLM中出现的见解。

Abstract: We present the first evaluation harness that enables any out-of-the-box,
local, Large Language Models (LLMs) to play full-press Diplomacy without
fine-tuning or specialized training. Previous work required frontier LLMs, or
fine-tuning, due to the high complexity and information density of Diplomacy's
game state. Combined with the high variance of matches, these factors made
Diplomacy prohibitive for study. In this work, we used data-driven iteration to
optimize a textual game state representation such that a 24B model can reliably
complete matches without any fine tuning. We develop tooling to facilitate
hypothesis testing and statistical analysis, and we present case studies on
persuasion, aggressive playstyles, and performance across a range of models. We
conduct a variety of experiments across many popular LLMs, finding the larger
models perform the best, but the smaller models still play adequately. We also
introduce Critical State Analysis: an experimental protocol for rapidly
iterating and analyzing key moments in a game at depth. Our harness
democratizes the evaluation of strategic reasoning in LLMs by eliminating the
need for fine-tuning, and it provides insights into how these capabilities
emerge naturally from widely used LLMs. Our code is available in the supplement
and will be open sourced.

</details>


### [115] [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
*Aswin RRV,Jacob Dineen,Divij Handa,Md Nayem Uddin,Mihir Parmar,Chitta Baral,Ben Zhou*

Main category: cs.AI

TL;DR: ThinkTuning 是一种基于 GRPO 的交互式训练方法，通过教师模型的反馈提升学生模型的推理能力，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的研究表明，仅靠强化学习无法真正培养模型的推理能力，因此需要一种新的方法来训练不表现出推理行为的模型。

Method: ThinkTuning 是一种基于 GRPO 的交互式训练方法，通过教师模型对学生的推理过程进行指导和反馈，以提升其推理能力。

Result: ThinkTuning 在多个基准测试中显示出显著的性能提升，平均比零样本基线高出 3.85%，在 MATH-500、AIME 和 GPQA-Diamond 上分别高出 2.08%、2.23% 和 3.99%。

Conclusion: 通过教师模型的反馈，ThinkTuning 方法能够有效提升学生模型的推理能力，显示出比零样本基线和普通 GRPO 基线更高的性能。

Abstract: Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

</details>


### [116] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 本文提出了一种名为SkillNav的模块化框架，将结构化的技能推理引入基于Transformer的VLN代理。该方法将导航分解为一组可解释的原子技能，并引入了一个基于零样本视觉-语言模型的路由器，以动态选择最合适的代理。SkillNav在R2R基准上实现了新的最先进性能，并展示了对包含新指令风格和未见过环境的GSA-R2R基准的强大泛化能力。


<details>
  <summary>Details</summary>
Motivation: Current methods still struggle to generalize to unseen scenarios, particularly when complex spatial and temporal reasoning is required.

Method: We propose SkillNav, a modular framework that introduces structured, skill-based reasoning into Transformer-based VLN agents. Our method decomposes navigation into a set of interpretable atomic skills, each handled by a specialized agent. We then introduce a novel zero-shot Vision-Language Model (VLM)-based router, which dynamically selects the most suitable agent at each time step by aligning sub-goals with visual observations and historical actions.

Result: SkillNav achieves a new state-of-the-art performance on the R2R benchmark and demonstrates strong generalization to the GSA-R2R benchmark that includes novel instruction styles and unseen environments.

Conclusion: SkillNav achieves a new state-of-the-art performance on the R2R benchmark and demonstrates strong generalization to the GSA-R2R benchmark that includes novel instruction styles and unseen environments.

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [117] [SQL-Exchange: Transforming SQL Queries Across Domains](https://arxiv.org/abs/2508.07087)
*Mohammadreza Daviran,Brian Lin,Davood Rafiei*

Main category: cs.DB

TL;DR: SQL-Exchange 是一种用于在不同数据库模式之间映射 SQL 查询的框架，能够提高文本到 SQL 系统的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高文本到 SQL 系统在上下文学习中的性能，需要在不同数据库模式之间进行有效的 SQL 查询映射。

Method: SQL-Exchange 通过保留源查询结构并调整特定领域的元素来适应目标模式，从而实现 SQL 查询的映射。

Result: SQL-Exchange 在多个模型家族和基准数据集上进行了评估，结果显示它在各种模式和查询类型中都有效。使用映射查询作为上下文示例可以持续提高文本到 SQL 的性能。

Conclusion: SQL-Exchange 是一种有效的框架，可以在不同数据库模式之间映射 SQL 查询，并提高文本到 SQL 系统的性能。

Abstract: We introduce SQL-Exchange, a framework for mapping SQL queries across
different database schemas by preserving the source query structure while
adapting domain-specific elements to align with the target schema. We
investigate the conditions under which such mappings are feasible and
beneficial, and examine their impact on enhancing the in-context learning
performance of text-to-SQL systems as a downstream task. Our comprehensive
evaluation across multiple model families and benchmark datasets--assessing
structural alignment with source queries, execution validity on target
databases, and semantic correctness--demonstrates that SQL-Exchange is
effective across a wide range of schemas and query types. Our results further
show that using mapped queries as in-context examples consistently improves
text-to-SQL performance over using queries from the source schema.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [118] [TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree](https://arxiv.org/abs/2508.07014)
*Andrei Andrusenko,Vladimir Bataev,Lilit Grigoryan,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: 本文提出了一种通用的ASR上下文偏差框架，能够在不显著影响速度的情况下提高准确性，并支持所有主要类型的ASR系统。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文偏差方法存在需要额外模型训练、解码过程显著变慢或限制ASR系统类型选择的问题。

Method: 本文提出了一种基于GPU加速的词语提升树的通用ASR上下文偏差框架，可以在浅层融合模式下用于贪心和束搜索解码，而不会显著影响速度。

Result: 实验结果表明，所提出的方法在准确性和解码速度上优于考虑的开源上下文偏差方法，并且能够处理大量关键词（最多20K项）。

Conclusion: 本文提出的上下文偏差框架在准确性和解码速度上优于现有的开源上下文偏差方法，并且在支持所有主要类型的ASR系统方面具有通用性。

Abstract: Recognizing specific key phrases is an essential task for contextualized
Automatic Speech Recognition (ASR). However, most existing context-biasing
approaches have limitations associated with the necessity of additional model
training, significantly slow down the decoding process, or constrain the choice
of the ASR system type. This paper proposes a universal ASR context-biasing
framework that supports all major types: CTC, Transducers, and Attention
Encoder-Decoder models. The framework is based on a GPU-accelerated word
boosting tree, which enables it to be used in shallow fusion mode for greedy
and beam search decoding without noticeable speed degradation, even with a vast
number of key phrases (up to 20K items). The obtained results showed high
efficiency of the proposed method, surpassing the considered open-source
context-biasing approaches in accuracy and decoding speed. Our context-biasing
framework is open-sourced as a part of the NeMo toolkit.

</details>


### [119] [FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities](https://arxiv.org/abs/2508.07315)
*Lilit Grigoryan,Vladimir Bataev,Nikolay Karpov,Andrei Andrusenko,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: FlexCTC是一个开源工具包，提供基于GPU的束搜索解码，提高语音识别的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 标准的束搜索实现速度慢、顺序执行且依赖CPU，无法充分利用现代硬件的能力。

Method: 开发了一个完全基于GPU的FlexCTC工具包，采用Python和PyTorch实现，使用CUDA图技术减少内核启动开销，并支持N-gram语言模型融合和短语级增强等高级上下文技术。

Result: FlexCTC工具包实现了高性能的批量GPU解码，减少了CPU-GPU同步，并支持先进的上下文技术，从而提高了语音识别的质量和效率。

Conclusion: FlexCTC工具包通过GPU加速的束搜索解码，提供了准确且高效的解码方法，适用于研究和生产环境。

Abstract: While beam search improves speech recognition quality over greedy decoding,
standard implementations are slow, often sequential, and CPU-bound. To fully
leverage modern hardware capabilities, we present a novel open-source FlexCTC
toolkit for fully GPU-based beam decoding, designed for Connectionist Temporal
Classification (CTC) models. Developed entirely in Python and PyTorch, it
offers a fast, user-friendly, and extensible alternative to traditional C++,
CUDA, or WFST-based decoders. The toolkit features a high-performance, fully
batched GPU implementation with eliminated CPU-GPU synchronization and
minimized kernel launch overhead via CUDA Graphs. It also supports advanced
contextualization techniques, including GPU-powered N-gram language model
fusion and phrase-level boosting. These features enable accurate and efficient
decoding, making them suitable for both research and production use.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [120] [Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection](https://arxiv.org/abs/2508.07201)
*Chaoqun Cui,Caiyan Jia*

Main category: cs.SI

TL;DR: 本文提出了一种基于图对比学习的谣言检测方法RAGCL，通过自适应增强技术提高了性能。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的基于图的模型假设谣言传播树（RPTs）具有深度结构，并沿分支学习序列立场特征。然而，通过在真实数据集上的统计分析，我们发现RPTs表现出广泛的结构，大多数节点是浅层的1级回复。为了将学习集中在密集子结构上，我们提出了RAGCL方法。

Method: 我们提出了RAGCL方法，该方法使用基于节点中心性的自适应视图增强。我们总结了三个RPT增强原则：1）免除根节点，2）保留深层回复节点，3）在深部分保留低级节点。我们使用基于中心性的重要性分数的概率进行节点删除、属性掩码和边删除来生成视图。然后使用图对比目标学习鲁棒的谣言表示。

Result: 在四个基准数据集上的广泛实验表明，RAGCL优于最先进的方法。

Conclusion: 我们的工作揭示了RPT的宽结构特性，并通过有原则的自适应增强提出了一个有效的图对比学习方法，用于谣言检测。所提出的原理和增强技术可能对涉及树状结构图的其他应用有益。

Abstract: Rumor detection on social media has become increasingly important. Most
existing graph-based models presume rumor propagation trees (RPTs) have deep
structures and learn sequential stance features along branches. However,
through statistical analysis on real-world datasets, we find RPTs exhibit wide
structures, with most nodes being shallow 1-level replies. To focus learning on
intensive substructures, we propose Rumor Adaptive Graph Contrastive Learning
(RAGCL) method with adaptive view augmentation guided by node centralities. We
summarize three principles for RPT augmentation: 1) exempt root nodes, 2)
retain deep reply nodes, 3) preserve lower-level nodes in deep sections. We
employ node dropping, attribute masking and edge dropping with probabilities
from centrality-based importance scores to generate views. A graph contrastive
objective then learns robust rumor representations. Extensive experiments on
four benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods.
Our work reveals the wide-structure nature of RPTs and contributes an effective
graph contrastive learning approach tailored for rumor detection through
principled adaptive augmentation. The proposed principles and augmentation
techniques can potentially benefit other applications involving tree-structured
graphs.

</details>


### [121] [Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning](https://arxiv.org/abs/2508.07205)
*Chaoqun Cui,Caiyan Jia*

Main category: cs.SI

TL;DR: 本文提出了一种新的谣言检测框架AD-GSCL，通过图监督对比学习来处理不平衡数据分布的问题，并在多种条件下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基于传播结构学习的谣言检测方法主要将谣言检测作为一类平衡分类任务，在有限的标记数据上进行。然而，现实世界的社会媒体数据表现出不平衡分布，其中谣言占少数。为了解决数据稀缺和不平衡问题，我们构建了两个大规模对话数据集，并分析了领域分布。

Method: 我们提出了Anomaly Detection framework with Graph Supervised Contrastive Learning (AD-GSCL)，它将未标记的数据视为非谣言，并适应图对比学习进行谣言检测。

Result: 广泛的实验表明，AD-GSCL在类别平衡、不平衡和少样本条件下都表现出优越性。

Conclusion: 我们的研究提供了对现实世界中不平衡数据分布的谣言检测的重要见解。

Abstract: Current rumor detection methods based on propagation structure learning
predominately treat rumor detection as a class-balanced classification task on
limited labeled data. However, real-world social media data exhibits an
imbalanced distribution with a minority of rumors among massive regular posts.
To address the data scarcity and imbalance issues, we construct two large-scale
conversation datasets from Weibo and Twitter and analyze the domain
distributions. We find obvious differences between rumor and non-rumor
distributions, with non-rumors mostly in entertainment domains while rumors
concentrate in news, indicating the conformity of rumor detection to an anomaly
detection paradigm. Correspondingly, we propose the Anomaly Detection framework
with Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats
unlabeled data as non-rumors and adapts graph contrastive learning for rumor
detection. Extensive experiments demonstrate AD-GSCL's superiority under
class-balanced, imbalanced, and few-shot conditions. Our findings provide
valuable insights for real-world rumor detection featuring imbalanced data
distributions.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [122] [Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading](https://arxiv.org/abs/2508.07408)
*Yueyi Wang,Qiyao Wei*

Main category: q-fin.ST

TL;DR: 本研究利用大规模语言模型对社交媒体上的情感信号进行分析，发现这些信号可以作为金融预测的有用但嘈杂的工具，并强调了透明性和可重复性的重要性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在展示大规模语言模型在金融语义标注和阿尔法信号发现中的独特效用，特别是通过分析社交媒体上的情感信号来改进金融预测。

Method: 本研究利用大规模语言模型（LLMs）对与公司相关的推文进行自动多标签事件分类，并将这些带有高情感强度的推文与1至7天的时间范围内未来的收益进行对比，以评估其统计效力和市场可交易性。

Result: 实验结果表明，某些事件标签在统计上显著地产生了负阿尔法，夏普比率低至-0.38，信息系数超过0.05。这表明社交媒体情感可以作为金融预测的有价值信号。

Conclusion: 本研究证明了将非结构化社交媒体文本转化为结构化的多标签事件变量的可行性，并展示了社交媒体情感作为金融预测中有价值但嘈杂的信号的潜力。此外，该研究强调了透明性和可重复性的重要性，所有代码和方法都已公开。

Abstract: In this study, we wish to showcase the unique utility of large language
models (LLMs) in financial semantic annotation and alpha signal discovery.
Leveraging a corpus of company-related tweets, we use an LLM to automatically
assign multi-label event categories to high-sentiment-intensity tweets. We
align these labeled sentiment signals with forward returns over 1-to-7-day
horizons to evaluate their statistical efficacy and market tradability. Our
experiments reveal that certain event labels consistently yield negative alpha,
with Sharpe ratios as low as -0.38 and information coefficients exceeding 0.05,
all statistically significant at the 95\% confidence level. This study
establishes the feasibility of transforming unstructured social media text into
structured, multi-label event variables. A key contribution of this work is its
commitment to transparency and reproducibility; all code and methodologies are
made publicly available. Our results provide compelling evidence that social
media sentiment is a valuable, albeit noisy, signal in financial forecasting
and underscore the potential of open-source frameworks to democratize
algorithmic trading research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [123] [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
*Rachel K. Luu,Jingyu Deng,Mohammed Shahrudin Ibrahim,Nam-Joon Cho,Ming Dao,Subra Suresh,Markus J. Buehler*

Main category: cs.LG

TL;DR: 本文提出了一种将生成式AI与不同领域的文献结合的框架，用于材料设计，成功制造出一种新型花粉基粘合剂，展示了AI在现实世界材料设计中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）已经改变了研究格局，但它们在学科特定的实验科学中的应用，特别是在像材料科学这样的高度跨学科领域中，仍然有限。因此，本文旨在探索如何利用AI在材料科学中进行创新性设计和实验。

Method: 本文提出了一种将生成式AI与来自此前未连接领域的文献（如植物科学、仿生学和材料工程）相结合的框架，以提取见解并设计材料实验。使用了包括微调模型（BioinspiredLLM）、检索增强生成（RAG）、代理系统和分层采样策略在内的AI工具，以提取结构-性能关系并将其转化为新型生物启发材料。

Result: 本文验证了其方法：LLM生成的程序、材料设计和机械预测在实验室中进行了测试，最终制造出一种具有可调节形态和测量剪切强度的新颖花粉基粘合剂，为未来植物衍生粘合剂的设计奠定了基础。

Conclusion: 本文展示了如何通过AI辅助的创意生成来推动现实世界中的材料设计，并实现有效的人机协作。

Abstract: Large language models (LLMs) have reshaped the research landscape by enabling
new approaches to knowledge retrieval and creative ideation. Yet their
application in discipline-specific experimental science, particularly in highly
multi-disciplinary domains like materials science, remains limited. We present
a first-of-its-kind framework that integrates generative AI with literature
from hitherto-unconnected fields such as plant science, biomimetics, and
materials engineering to extract insights and design experiments for materials.
We focus on humidity-responsive systems such as pollen-based materials and
Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and
adaptive performance. Using a suite of AI tools, including a fine-tuned model
(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a
Hierarchical Sampling strategy, we extract structure-property relationships and
translate them into new classes of bioinspired materials. Structured inference
protocols generate and evaluate hundreds of hypotheses from a single query,
surfacing novel and experimentally tractable ideas. We validate our approach
through real-world implementation: LLM-generated procedures, materials designs,
and mechanical predictions were tested in the laboratory, culminating in the
fabrication of a novel pollen-based adhesive with tunable morphology and
measured shear strength, establishing a foundation for future plant-derived
adhesive design. This work demonstrates how AI-assisted ideation can drive
real-world materials design and enable effective human-AI collaboration.

</details>


### [124] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: This paper introduces Adaptive Meta Fine-Tuning (AMFT), a novel single-stage algorithm that dynamically balances Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Large Language Models (LLMs). AMFT uses a meta-gradient adaptive weight controller to optimize the SFT-RL balance, leading to improved performance, stability, and generalization on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms.

Method: We introduce Adaptive Meta Fine-Tuning (AMFT), a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a meta-gradient adaptive weight controller that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance.

Result: AMFT consistently establishes a new state-of-the-art and demonstrates superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance.

Conclusion: AMFT consistently establishes a new state-of-the-art and demonstrates superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment.

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [125] [Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization](https://arxiv.org/abs/2508.07629)
*Zhenpeng Su,Leiyu Pan,Xue Bai,Dening Liu,Guanting Dong,Jiaming Huang,Wenping Hu,Guorui Zhou*

Main category: cs.LG

TL;DR: 本文介绍了Klear-Reasoner模型，该模型具有长期推理能力，在多个基准测试中表现出色。通过详细分析后训练流程和提出新的强化学习方法，解决了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有许多关于推理模型的研究，但由于训练细节不完整，复制高性能推理模型仍然存在困难。本文旨在深入分析推理模型，并解决当前强化学习中剪切机制的问题。

Method: 本文介绍了Klear-Reasoner模型，该模型具有长期推理能力，并通过数据准备、长链式思维监督微调（long CoT SFT）和强化学习（RL）的完整后训练流程进行分析。此外，还进行了详细的消融研究。

Result: 实验表明，少量高质量的数据源比大量多样化的数据源更有效，且困难样本在不进行准确度过滤的情况下也能取得更好的结果。提出的GPPO方法提高了模型的探索能力和从负样本中学习的效率。

Conclusion: Klear-Reasoner展示了出色的推理能力，在数学和编程方面表现优异，分别在AIME 2024、AIME 2025、LiveCodeBench V5和LiveCodeBench V6上取得了高分。

Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that
demonstrates careful deliberation during problem solving, achieving outstanding
performance across multiple benchmarks. Although there are already many
excellent works related to inference models in the current community, there are
still many problems with reproducing high-performance inference models due to
incomplete disclosure of training details. This report provides an in-depth
analysis of the reasoning model, covering the entire post-training workflow
from data preparation and long Chain-of-Thought supervised fine-tuning (long
CoT SFT) to reinforcement learning (RL), along with detailed ablation studies
for each experimental component. For SFT data, our experiments show that a
small number of high-quality data sources are more effective than a large
number of diverse data sources, and that difficult samples can achieve better
results without accuracy filtering. In addition, we investigate two key issues
with current clipping mechanisms in RL: Clipping suppresses critical
exploration signals and ignores suboptimal trajectories. To address these
challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)
that gently backpropagates gradients from clipped tokens. GPPO not only
enhances the model's exploration capacity but also improves its efficiency in
learning from negative samples. Klear-Reasoner exhibits exceptional reasoning
abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\%
on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.

</details>


### [126] [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov,Alexander Yavorskyi,Mykyta Yaroshenko*

Main category: cs.LG

TL;DR: 本文提出了一种名为GLiClass的新方法，用于序列分类任务，该方法在准确性和效率方面表现强劲，同时保持了零样本和少量样本学习场景所需的灵活性。此外，还适应了近端策略优化（PPO）用于多标签文本分类。


<details>
  <summary>Details</summary>
Motivation: 分类是AI应用中最常见的任务之一，通常作为过滤、排序和分类数据的第一步。现代AI系统必须处理大量输入数据，早期管道阶段可能会将错误传播到下游，因此实现高效率和准确性至关重要。此外，分类需求可以根据用户需求动态变化，需要具有强大零样本能力的模型。

Method: 我们提出了GLiClass，这是一种新颖的方法，适应GLiNER架构用于序列分类任务。此外，我们还适应了近端策略优化（PPO）用于多标签文本分类。

Result: 我们的方法在准确性和效率方面表现强劲，与基于嵌入的方法相当，同时保持了零样本和少量样本学习场景所需的灵活性。此外，我们还适应了近端策略优化（PPO）用于多标签文本分类，使在数据稀疏条件下或从人类反馈中训练分类器成为可能。

Conclusion: 我们提出了GLiClass，这是一种新颖的方法，适应GLiNER架构用于序列分类任务。我们的方法在准确性和效率方面表现强劲，与基于嵌入的方法相当，同时保持了零样本和少量样本学习场景所需的灵活性。此外，我们还适应了近端策略优化（PPO）用于多标签文本分类，使在数据稀疏条件下或从人类反馈中训练分类器成为可能。

Abstract: Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative LLMs have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-sparse conditions or from human feedback.

</details>


### [127] [Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment](https://arxiv.org/abs/2508.07750)
*Haowen Wang,Yun Yue,Zhiling Ye,Shuowen Zhang,Lei Fan,Jiaxin Liang,Jiadi Jiang,Cheng Wei,Jingyuan Deng,Xudong Han,Ji Li,Chunxiao Guo,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.LG

TL;DR: 本文提出了一种统一的GRAO框架，通过结合SFT和RL的优势，提高了语言模型对齐的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的SFT和RL方法在语言模型对齐中存在局限性，如SFT的离线策略轨迹限制和RL的低样本效率及对高质量基础模型的依赖。

Method: GRAO框架通过三个关键创新：多样本生成策略、新的组直接对齐损失公式和基于成对偏好动态的参考感知参数更新，结合了SFT和RL的优势。

Result: GRAO在复杂的人类对齐任务中表现出色，相对于SFT、DPO、PPO和GRPO基线分别实现了57.70%、17.65%、7.95%和5.18%的相对提升。

Conclusion: 本文提出了GRAO框架，通过结合SFT和RL的优势，解决了语言模型对齐中的双重挑战，并在复杂的人类对齐任务中表现出色。

Abstract: Alignment methodologies have emerged as a critical pathway for enhancing
language model alignment capabilities. While SFT (supervised fine-tuning)
accelerates convergence through direct token-level loss intervention, its
efficacy is constrained by offline policy trajectory. In contrast,
RL(reinforcement learning) facilitates exploratory policy optimization, but
suffers from low sample efficiency and stringent dependency on high-quality
base models. To address these dual challenges, we propose GRAO (Group Relative
Alignment Optimization), a unified framework that synergizes the respective
strengths of SFT and RL through three key innovations: 1) A multi-sample
generation strategy enabling comparative quality assessment via reward
feedback; 2) A novel Group Direct Alignment Loss formulation leveraging
intra-group relative advantage weighting; 3) Reference-aware parameter updates
guided by pairwise preference dynamics. Our theoretical analysis establishes
GRAO's convergence guarantees and sample efficiency advantages over
conventional approaches. Comprehensive evaluations across complex human
alignment tasks demonstrate GRAO's superior performance, achieving
57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and
GRPO baselines respectively. This work provides both a theoretically grounded
alignment framework and empirical evidence for efficient capability evolution
in language models.

</details>


### [128] [Pareto Multi-Objective Alignment for Language Models](https://arxiv.org/abs/2508.07768)
*Qiang He,Setareh Maghsudi*

Main category: cs.LG

TL;DR: 本文提出了一种名为PAMA的多目标对齐算法，旨在解决大型语言模型在实际应用中需要平衡多个冲突目标的问题。与传统的多目标优化方法相比，PAMA通过将多目标RLHF转化为凸优化问题，显著提高了计算效率，并提供了理论保证，证明其能够收敛到帕累托最优解。实验结果表明，PAMA在不同规模的语言模型上表现出色，为实现更灵活和适应性强的AI部署提供了可行且理论基础扎实的解决方案。


<details>
  <summary>Details</summary>
Motivation: Current alignment methods, primarily based on RLHF, optimize LLMs toward a single reward function, resulting in rigid behavior that fails to capture the complexity and diversity of human preferences. This limitation hinders the adaptability of LLMs to practical scenarios, making multi-objective alignment (MOA) a critical yet underexplored area.

Method: Pareto Multi-Objective Alignment (PAMA), a principled and computationally efficient algorithm designed explicitly for MOA in LLMs. PAMA transforms multi-objective RLHF into a convex optimization with a closed-form solution, significantly enhancing scalability.

Result: Extensive experiments across language models ranging from 125M to 7B parameters demonstrate PAMA's robust and effective MOA capabilities, aligning with its theoretical advantages.

Conclusion: PAMA provides a highly efficient solution to the MOA problem that was previously considered intractable, offering a practical and theoretically grounded approach to aligning LLMs with diverse human values, paving the way for versatile and adaptable real-world AI deployments.

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications that require careful balancing of multiple, often conflicting,
objectives, such as informativeness versus conciseness, or helpfulness versus
creativity. However, current alignment methods, primarily based on RLHF,
optimize LLMs toward a single reward function, resulting in rigid behavior that
fails to capture the complexity and diversity of human preferences. This
limitation hinders the adaptability of LLMs to practical scenarios, making
multi-objective alignment (MOA) a critical yet underexplored area. To bridge
this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and
computationally efficient algorithm designed explicitly for MOA in LLMs. In
contrast to computationally prohibitive multi-objective optimization (MOO)
methods, PAMA transforms multi-objective RLHF into a convex optimization with a
closed-form solution, significantly enhancing scalability. Traditional MOO
approaches suffer from prohibitive O(n^2*d) complexity, where d represents the
number of model parameters, typically in the billions for LLMs, rendering
direct optimization infeasible. PAMA reduces this complexity to O(n) where n is
the number of objectives, enabling optimization to be completed within
milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto
stationary point, where no objective can be improved without degrading at least
one other. Extensive experiments across language models ranging from 125M to 7B
parameters demonstrate PAMA's robust and effective MOA capabilities, aligning
with its theoretical advantages. PAMA provides a highly efficient solution to
the MOA problem that was previously considered intractable, offering a
practical and theoretically grounded approach to aligning LLMs with diverse
human values, paving the way for versatile and adaptable real-world AI
deployments.

</details>


### [129] [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
*Sven Weinzierl,Sandra Zilker,Annina Liessmann,Martin Käppel,Weixin Wang,Martin Matzner*

Main category: cs.LG

TL;DR: 本文介绍了一种基于迁移学习的预测过程监控技术，使组织能够利用迁移学习在内部和跨组织环境中实现有效的决策支持。


<details>
  <summary>Details</summary>
Motivation: 现有的PPM技术需要足够的事件数据或其他相关资源，而这些资源可能不易获得，这限制了一些组织利用PPM的能力。

Method: 本文提出了一种基于迁移学习的预测过程监控（PPM）技术，并在两个实际用例中进行了实例化，通过使用IT服务管理流程的事件日志进行数值实验。

Result: 实验结果表明，一个业务流程的知识可以转移到同一组织或不同组织中的类似业务流程，以在目标上下文中实现有效的PPM。

Conclusion: 本文提出的基于迁移学习的PPM技术使没有合适事件数据或其他相关资源的组织能够在内部和跨组织环境中受益于迁移学习，从而实现有效的决策支持。

Abstract: Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

</details>


### [130] [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
*Zihe Liu,Jiashun Liu,Yancheng He,Weixun Wang,Jiaheng Liu,Ling Pan,Xinyu Hu,Shaopan Xiong,Ju Huang,Jian Hu,Shengyi Huang,Siran Yang,Jiamang Wang,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文系统回顾了广泛采用的RL技术，并提供了针对特定设置的清晰指南，展示了简单组合方法在性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管在RL技术方面取得了进展，但仍然存在缺乏标准化指南、实验设置不一致等问题，导致结论冲突，使从业者难以选择合适的技术。

Method: 本文通过严格的再现和在统一开源框架内的隔离评估，分析了各种RL技术的内部机制、适用场景和核心原理。

Result: 本文展示了一种简单的组合方法，可以使用原始PPO损失解锁无批评策略的学习能力，并且结果表明该方法在性能上优于GRPO和DAPO等策略。

Conclusion: 本文提出了针对LLM推理的强化学习技术的系统性回顾，并展示了通过简单组合可以提升性能的结果。

Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [131] [Investigating the Design Space of Visual Grounding in Multimodal Large Language Model](https://arxiv.org/abs/2508.08066)
*Weitai Kang,Weiming Zhuang,Zhizhong Li,Yan Yan,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 本文研究了多模态大语言模型在视觉接地任务中的设计选择，通过实验分析和优化，提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在微调多模态大语言模型时采用不同的设计选择，缺乏系统验证来支持这些设计。本文旨在填补这一空白。

Method: 我们使用LLaVA-1.5进行分析，探索了不同的视觉接地范式，并进行了消融研究以优化接地数据的设计。

Result: 通过探索不同的视觉接地范式和优化接地数据设计，我们的研究结果在视觉接地任务中实现了显著的性能提升。

Conclusion: 我们的研究结果为视觉接地任务中的多模态大语言模型提供了更强的性能，RefCOCO/+/g上的改进分别为+5.6%、+6.9%和+7.0%。

Abstract: Fine-grained multimodal capability in Multimodal Large Language Models
(MLLMs) has emerged as a critical research direction, particularly for tackling
the visual grounding (VG) problem. Despite the strong performance achieved by
existing approaches, they often employ disparate design choices when
fine-tuning MLLMs for VG, lacking systematic verification to support these
designs. To bridge this gap, this paper presents a comprehensive study of
various design choices that impact the VG performance of MLLMs. We conduct our
analysis using LLaVA-1.5, which has been widely adopted in prior empirical
studies of MLLMs. While more recent models exist, we follow this convention to
ensure our findings remain broadly applicable and extendable to other
architectures. We cover two key aspects: (1) exploring different visual
grounding paradigms in MLLMs, identifying the most effective design, and
providing our insights; and (2) conducting ablation studies on the design of
grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our
findings contribute to a stronger MLLM for VG, achieving improvements of +5.6%
/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.

</details>

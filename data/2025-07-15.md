<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.SE](#cs.SE) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: 本文介绍了一种基于Transformer的模型Spatial ModernBERT，用于准确检测和提取复杂财务文档中的表格数据和键值字段。该模型通过三个头进行标记分类，并在预训练和微调后表现出色。


<details>
  <summary>Details</summary>
Motivation: 从财务文档中提取表格和键值对对于审计、数据分析和自动化发票处理等业务流程至关重要。

Method: 我们引入了Spatial ModernBERT——一种基于变压器的模型，通过空间嵌入进行增强，以准确检测和提取复杂财务文档中的表格数据和键值字段。我们将提取任务作为三个头的标记分类：(1) 标签头，将每个标记分类为标签；(2) 列头，预测列索引；(3) 行头，区分项目行和标题行。模型在PubTables-1M数据集上预训练，然后在财务文档数据集上微调，通过每个分类头的交叉熵损失实现稳健性能。我们提出了一种后处理方法，使用B-I-IB标记合并标记，重建表格布局并提取键值对。

Result: Spatial ModernBERT在真实世界的财务文档中实现了高度准确的表格和键值提取。

Conclusion: Spatial ModernBERT有效地利用了文本和空间线索，促进了现实世界财务文档中表格和键值提取的高度准确性。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [2] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: This paper introduces SEALGuard, a multilingual guardrail designed to improve the safety alignment across diverse languages. It aims to address the multilingual safety alignment gap of existing guardrails and ensure effective filtering of unsafe and jailbreak prompts in LLM-powered systems.


<details>
  <summary>Details</summary>
Motivation: Safety alignment is critical for LLM-powered systems. While recent LLM-powered guardrail approaches such as LlamaGuard achieve high detection accuracy of unsafe inputs written in English, they struggle with multilingual unsafe inputs. This limitation leaves LLM systems vulnerable to unsafe and jailbreak prompts written in low-resource languages such as those in Southeast Asia.

Method: We adapt a general-purpose multilingual language model into a multilingual guardrail using low-rank adaptation (LoRA). We construct SEALSBench, a large-scale multilingual safety alignment dataset containing over 260,000 prompts in ten languages.

Result: Our findings show that multilingual unsafe and jailbreak prompts substantially degrade the performance of the state-of-the-art LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and 18%, respectively, compared to its performance on English-only prompts. In contrast, SEALGuard outperforms existing guardrails in detecting multilingual unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and achieving the best DSR, precision, and F1-score.

Conclusion: SEALGuard advances the safety alignment of LLM systems by introducing an effective multilingual guardrail.

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [3] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在医学问答中的当前局限性，发现现有数据集存在临床现实性不足、透明度低和验证不充分的问题。尽管公开的挑战性问题有一定优势，但它们规模小、范围窄且可能被大型语言模型训练所影响。因此，需要建立标准化框架和合作努力，以确保数据集和方法学的严谨性和无偏性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在医学问答中的当前局限性，重点关注用于评估的数据集的质量。

Method: 审查了广泛使用的基准数据集，包括MedQA、MedMCQA、PubMedQA和MMLU，以评估其严谨性、透明度和与临床场景的相关性。还分析了医学期刊中的挑战性问题，以确定其作为无偏评估工具的潜力。

Result: 大多数现有数据集缺乏临床现实性、透明度和稳健的验证过程。公开的挑战性问题虽然有一些好处，但受限于其规模小、范围窄以及暴露于大型语言模型训练中。这些差距突显了需要安全、全面和具有代表性的数据集。

Conclusion: 标准化框架对于评估医学中的大型语言模型至关重要。需要机构和政策制定者之间的协作，以确保数据集和方法学是严格、无偏且反映临床复杂性的。

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [4] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: 本文提出了两个韩国专家级基准KMMLU-Redux和KMMLU-Pro，用于评估大型语言模型在韩国工业领域的适用性，并已公开数据集。


<details>
  <summary>Details</summary>
Motivation: 为了有效评估大型语言模型在现实场景中的适用性，需要涵盖学术领域和工业领域的强大基准。

Method: 本文重构了现有的KMMLU，去除了关键错误以提高可靠性，并基于韩国国家专业执照考试创建了KMMLU-Pro。

Result: 实验表明，这些基准能够全面代表韩国的工业知识。

Conclusion: 本文介绍了两个韩国专家级基准KMMLU-Redux和KMMLU-Pro，这些基准能够全面代表韩国的工业知识，并且数据集已公开可用。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [5] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: 本文介绍了SIMS，这是一个无需外部监督的自我改进模型引导框架，通过自主生成和优化对比样本以及采用新策略来提高引导效果，在多种LLM和基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统模型引导方法依赖外部标注数据，限制了其适应性并依赖于标注质量。

Method: SIMS通过迭代自我改进循环自主生成和优化对比样本，并采用提示排序和对比采样等新策略来提高引导效果。

Result: 在多种LLM和基准测试中，SIMS在引导效果和适应性方面显著优于现有方法。

Conclusion: SIMS展示了自我改进的模型引导作为未来研究的有希望的方向。

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [6] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: 该研究分析了电子健康记录中的污名化语言，发现特定群体和医疗专业人员更可能使用污名化标签。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）是医疗团队中患者污名化得以延续的关键媒介。

Method: 通过扩展词典匹配和监督学习分类器，在MIMIC-III电子健康记录中识别了怀疑标记和污名化标签的语言特征。使用泊松回归模型评估了语言特征率的预测因素。

Result: 研究发现，黑人或非裔美国人患者（相对风险：1.16）、有医疗保险/医疗补助或政府运营保险的患者（相对风险：2.46）、自费患者（相对风险：2.12）以及患有各种污名化疾病和心理健康状况的患者，每份病历中的污名化标签数量更高。怀疑标记的模式类似，但男性患者的怀疑标记率更高（相对风险：1.25）。护士（相对风险：1.40）和社会工作者（相对风险：2.25）使用的污名化标签更多，怀疑标记的模式也相似。

Conclusion: 研究发现，历史上被污名化的患者中，污名化语言出现的频率更高，并且由多种提供者类型所延续。

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [7] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: 研究发现，不同意象表型的人在Ganzflicker诱导的幻觉中看到的内容不同，这可能反映了视觉系统内部协调的个体差异。


<details>
  <summary>Details</summary>
Motivation: 研究个体在视觉系统方面的差异是否会影响其他内部生成的视觉体验的复杂性。

Method: 使用自然语言处理工具分析了来自4000多名参与者的自由文本描述，以研究不同意象表型的人在Ganzflicker诱导的幻觉中看到的东西是否不同。

Result: 强意象者描述了复杂的、自然主义的内容，而弱意象者报告了简单的几何图案。视觉语言模型的嵌入更好地捕捉了这些差异，而具有更强意象的参与者使用的语言具有更丰富的感官运动关联。

Conclusion: 这些发现可能反映了早期视觉区域和与意象谱相关的高级区域之间协调的个体差异。

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [8] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard is a linearization framework that transforms pretrained Transformer-based LLMs into flexible, subquadratic architectures for infinite-context generation. It addresses memory and computational bottlenecks by introducing a subquadratic attention mechanism and a gating module, achieving near-lossless performance while outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Previous linearization methods are often limited by fixed model structures and exclude gating mechanisms.

Method: Lizard introduces a subquadratic attention mechanism that closely approximates softmax attention while preserving output quality. It incorporates a gating module inspired by recent state-of-the-art linear models, combining gated linear attention for global context compression with sliding window attention enhanced by meta memory.

Result: Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.

Conclusion: Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods.

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [9] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: ALIGN系统是一种用于动态个性化大语言模型决策者的框架，能够通过基于提示的对齐来适应用户的细粒度属性，并提供定性和定量分析。


<details>
  <summary>Details</summary>
Motivation: 用户有不同的价值观和偏好，这会影响他们的决策，因此需要新的方法来对齐和个性化大语言模型。现有的大语言模型比较工具主要集中在基准任务上，而ALIGN系统专注于动态个性化。

Method: ALIGN系统通过基于提示的对齐到一组细粒度属性，实现了动态个性化的大语言模型决策者的配置管理、结构化输出生成以及多种算法实现。

Result: ALIGN系统提供了定性和定量分析，包括在两个不同领域（人口统计对齐和价值对齐）的对齐方法比较。

Conclusion: ALIGN框架是开源的，将促进可靠、负责任和个性化的基于大语言模型的决策者的新研究。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [10] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: 本文介绍了一个更大的代码推理数据集OpenCodeReasoning-II，并通过两阶段微调策略提高了代码生成和评论的性能，同时扩展了LiveCodeBench基准以支持C++语言。


<details>
  <summary>Details</summary>
Motivation: 由于代码生成和评论的进展依赖于大规模、高质量的数据集，因此本文旨在创建一个更大的数据集并改进模型性能。

Method: 本文采用两阶段监督微调策略，第一阶段专注于代码生成的微调，第二阶段涉及代码生成和评论模型的联合训练。此外，还扩展了LiveCodeBench基准以支持C++语言。

Result: 本文提出的微调Qwen2.5-Instruct模型在代码生成方面表现出色，与最佳先前开放权重的蒸馏模型相当或超越。此外，代码生成和评论模型的集成显著提高了竞争性编程的表现。

Conclusion: 本文提出了一个更大的代码推理数据集OpenCodeReasoning-II，并通过两阶段微调策略提高了代码生成和评论的性能，同时扩展了LiveCodeBench基准以支持C++语言。

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [11] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: 本文提出了一种动态参数记忆（DPM）机制，用于改进语音大语言模型（SLLM）在处理长音频序列时的情感识别能力。实验结果表明，DPM在处理长音频序列时显著提升了SLLM的情感识别能力，并达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 近期研究集中在将语音大语言模型（SLLM）应用于改善语音情感识别（SER）。然而，语音模态固有的高帧率严重限制了SLLM的信号处理和理解能力。现有的输入令牌压缩方法忽略了情感在多个对话回合中的连续性和惯性。

Method: 本文提出了一种动态参数记忆（DPM）机制，结合上下文语义和句子级情感编码，使SLLM能够在有限的上下文窗口中处理无限长度的音频。具体而言，DPM在推理过程中逐步将句子级信息和情感编码到临时LoRA模块中，以有效地“记忆”上下文信息。

Result: 在IEMOCAP数据集上的实验结果表明，DPM在处理长音频序列时显著提升了SLLM的情感识别能力，并达到了最先进的性能。

Conclusion: 实验结果表明，DPM在处理长音频序列时显著提升了SLLM的情感识别能力，并达到了最先进的性能。

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [12] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文介绍了CompassJudger-2，一个通过任务驱动的多领域数据收集策略和改进的学习目标来提升判断能力的通用法官模型。实验表明，该模型在多个基准上表现优异，并提出了一个新的基准JudgerBenchV2以标准化评估。


<details>
  <summary>Details</summary>
Motivation: 当前的法官模型存在专业化狭窄和鲁棒性有限的问题，这影响了它们进行全面评估的能力。因此，需要一种更通用、更强大的法官模型来解决这些问题。

Method: 本文提出了一种任务驱动的多领域数据收集策略，通过可验证的奖励监督判断任务，引导内在的批判性推理，通过拒绝采样来培养稳健、可泛化的判断能力。还引入了改进的学习目标，包括边际策略梯度损失，以提高性能。

Result: CompassJudger-2在多个法官和奖励基准上取得了优越的结果，其7B模型在判断准确性方面与更大的模型如DeepSeek-V3和Qwen3-235B-A22B具有竞争力。此外，提出的JudgerBenchV2基准能够评估跨领域判断准确性和排名一致性。

Conclusion: 本文提出了CompassJudger-2，一个新型的通用法官模型，克服了现有法官模型的局限性，并通过实验验证了其优越性能。此外，还提出了JudgerBenchV2基准，以标准化法官模型的评估。这些贡献推动了鲁棒、可扩展的LLM判断，并建立了新的性能和评估标准。

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [13] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: OPENXRD is an open-book pipeline for crystallography question answering that uses GPT-4.5-generated summaries to improve the performance of smaller models in X-ray diffraction tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of using scanned textbooks, which may lead to copyright issues, and to provide compact, domain-specific references that help smaller models understand key concepts in X-ray diffraction (XRD).

Method: OPENXRD is an open-book pipeline designed for crystallography question answering, which integrates textual prompts with concise supporting content generated by GPT-4.5. It evaluates different vision-language models under both closed-book and open-book conditions.

Result: The experimental results show significant accuracy improvements in models that use the GPT-4.5-generated summaries, particularly those with limited prior training in crystallography. AI-generated texts can help smaller models reason more effectively in scientific tasks.

Conclusion: OPENXRD shows that specialized open-book systems can be useful in materials science and provides a foundation for broader natural language processing (NLP) tools in critical scientific fields.

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [14] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级有效模型，用于检测战略对话中的欺骗，通过PU学习和语言可解释性方法，在减少参数的同时取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 检测战略对话中的欺骗是一项复杂且高风险的任务，因为语言的微妙性和欺骗性与真实沟通之间的极端类别不平衡。

Method: 我们引入了一个轻量级但有效的模型，结合冻结的BERT嵌入、可解释的语言和游戏特定特征以及正-未标记(PU)学习目标。

Result: 我们的模型在减少可训练参数的同时实现了新的最佳宏F1分数，证明了PU学习、语言可解释性和说话人感知表示的价值。

Conclusion: 我们的模型在减少可训练参数的同时实现了新的最佳宏F1分数，证明了PU学习、语言可解释性和说话人感知表示的价值。

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [15] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: 本文介绍了RAMA，一种用于验证多媒体虚假信息的新颖检索增强型多智能体框架，通过战略查询制定、跨验证证据聚合和多智能体集成架构实现了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息的快速传播对自动化事实核查系统提出了重大挑战，特别是当声明模糊或缺乏足够上下文时。

Method: RAMA是一种新型的检索增强型多智能体框架，用于验证多媒体虚假信息。它包含三个核心创新：(1) 战略查询制定，将多模态声明转化为精确的网络搜索查询；(2) 从多样化、权威来源中交叉验证证据聚合；(3) 多智能体集成架构，利用多个多模态大语言模型和提示变体的互补优势。

Result: 广泛的实验表明，RAMA在基准数据集上表现出色，特别是在通过基于检索的事实证据解决模糊或不可能的声明方面表现突出。

Conclusion: 我们的研究强调了将基于网络的证据和多智能体推理相结合对于可信赖的多媒体验证的必要性，并为更可靠和可扩展的核查解决方案铺平了道路。

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [16] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: 本文介绍了一种微调方法，通过识别和剪枝与数据集特定机制相关的神经元，以提高Transformer-based LLMs的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）往往发展出针对特定数据集的机制，例如依赖于领域特定的相关性，这会导致高置信度预测但缺乏可泛化的推理。这些数据集特定的机制在遇到新任务或分布时通常会降低性能。

Method: 我们采用集成梯度来量化每个神经元对高置信度预测的影响，确定那些不成比例地贡献于数据集特定性能但不支持稳健、可转移推理的神经元，并选择性地剪枝这些神经元。

Result: 通过剪枝基于变压器的LLMs中的神经元，我们的方法能够增强模型的泛化能力，使其依赖于可泛化的表示。

Conclusion: 我们的剪枝微调方法在多个选择题基准测试中显著提升了性能，超过了之前的（非剪枝）适应方法。

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [17] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: 本文介绍了Banzhida，一个针对藏语优化的多语言大型语言模型，通过创建最大的藏语预训练语料库并进行专门的数据处理，显著提升了藏语生成式AI的能力。


<details>
  <summary>Details</summary>
Motivation: 由于高质量训练语料的稀缺，藏语在现有模型中特别被忽视。因此，我们需要创建一个更大的藏语预训练语料库，并开发一个专门的处理流程来解决这一问题。

Method: 我们整理了最大的藏语预训练语料库，并通过专门的数据清洗和处理流程进行处理。然后，我们将多语言基础模型继续预训练/微调为Banzhida，一个增强藏语生成式AI的多语言大型语言模型。

Result: 实验结果表明，Banzhida在广泛的任务中 consistently 和显著地优于相同规模的开源模型和针对藏语的模型。

Conclusion: Banzhida在多种任务中显著优于相同规模的开源模型和针对藏语的模型，为藏语生成式AI的发展做出了贡献。

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [18] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 本研究构建了一个气候变化图像隐喻数据库，发现视觉隐喻虽然更难理解，但更具审美愉悦感，并可能促进更深层次的认知加工，同时在积极体验方面优于真实图像。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏系统性的研究和材料分散，很少有研究探讨气候变化视觉隐喻的影响。本研究旨在填补这一空白，提供一个数据库并分析其影响。

Method: 本研究构建了一个名为MetaClimage的气候变化图像隐喻数据库，包含隐喻图像和真实图像，并通过人类评分收集了难度、效果、艺术质量和情感唤醒等数据，还利用自然语言处理从标签中提取了语义和情绪变量。

Result: 视觉隐喻被评价为更难理解，但更具审美愉悦感，而在效果和唤醒度上与真实图像无显著差异。然而，对于高认知需求的参与者，视觉隐喻的唤醒度更高。此外，视觉隐喻获得了更多的标签，通常涉及图像中未呈现的实体，并引发了更具积极情感和更强支配力的词汇。

Conclusion: 本研究证明了视觉隐喻在气候变化中的认知负荷较大，但可能引发更深层次的认知加工和抽象，同时在审美欣赏和积极体验方面表现更优。此外，该研究为未来的研究提供了数据库，并阐明了在塑造环境传播时需要考虑的成本-收益权衡。

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [19] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: Swa-bhasha Resource Hub 提供了2020至2025年间用于罗马化僧伽罗语到僧伽罗语转写的全面数据资源和算法，旨在推动僧伽罗语自然语言处理的研究和应用。


<details>
  <summary>Details</summary>
Motivation: 为了推动僧伽罗语自然语言处理的研究，特别是在训练转写模型和开发涉及罗马化僧伽罗语的应用方面。

Method: 提供了一个全面的数据资源和算法集合，用于2020年至2025年之间的罗马化僧伽罗语到僧伽罗语的转写。

Result: 该资源库包含了作者贡献的资源，并对现有转写应用进行了比较分析。

Conclusion: 该资源库为斯里兰卡语自然语言处理研究提供了重要支持，并通过公开数据集和工具促进了相关应用的发展。

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [20] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: 本文提出了一种受心理学启发的幽默分解机制（HDM），利用思维链（CoT）来模仿人类的思维过程，刺激大型语言模型优化翻译幽默文本的可读性。此外，我们在HDM中整合了幽默理论，以进一步增强翻译文本中的幽默元素。实验结果表明，该方法在幽默、流畅性和连贯性方面分别取得了7.75%、2.81%和6.13%的平均提升。


<details>
  <summary>Details</summary>
Motivation: 幽默翻译在不同文化之间起着桥梁作用，促进理解和交流。尽管大多数现有的大型语言模型能够完成一般的翻译任务，但这些模型在幽默翻译方面仍然存在困难，这体现在语言干扰和翻译文本缺乏幽默感上。

Method: 我们提出了一种受心理学启发的幽默分解机制（HDM），利用思维链（CoT）来模仿人类的思维过程，刺激大型语言模型优化翻译幽默文本的可读性。此外，我们在HDM中整合了幽默理论，以进一步增强翻译文本中的幽默元素。

Result: 我们在开源幽默数据集上的自动评估实验表明，我们的方法显著提高了幽默翻译的质量，分别在幽默、流畅性和连贯性方面取得了7.75%、2.81%和6.13%的平均提升。

Conclusion: 我们的方法显著提高了幽默翻译的质量，在幽默、流畅性和连贯性方面分别取得了7.75%、2.81%和6.13%的平均提升。

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [21] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: 本文介绍了一种新的语音处理框架ClaritySpeech，用于改善阿尔茨海默病患者的语音识别和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 当前的语音技术（如自动语音识别）在处理阿尔茨海默病和非典型语音时存在困难，这进一步挑战了可访问性。本文旨在解决这一问题，提供一种新的方法来改善语音识别和隐私保护。

Method: ClaritySpeech结合了自动语音识别（ASR）、文本混淆和零样本文本到语音（TTS）技术，以在不进行微调的情况下纠正阿尔茨海默病影响的语音。

Result: 实验结果显示，在各种对抗性设置和模态（音频、文本、融合）中，ADReSS和ADReSSo的平均F1分数分别下降了16%和10%，同时保持了50%的说话者相似度。此外，系统还提高了词错误率（WER）和语音质量。

Conclusion: 本文提出了一种新的针对阿尔茨海默病的语音混淆框架ClaritySpeech，能够在低数据环境下纠正受影响的语音同时保持说话者身份，从而提高隐私和可访问性。

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [22] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: 本文介绍了 DATE-LM，一个用于评估数据归属方法的统一基准，并展示了其在多个任务中的应用结果。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对 LLM 的系统性数据归属方法评估，因此需要一个统一的基准来填补这一空白。

Method: 引入 DATE-LM，这是一个统一的基准，用于通过实际的 LLM 应用评估数据归属方法。

Result: 发现没有一种方法在所有任务中都占优，数据归属方法与简单的基线存在权衡，并且方法性能对任务特定的评估设计敏感。

Conclusion: DATE-LM 旨在为未来的大语言模型数据归属研究提供基础，并通过公开排行榜促进社区参与。

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [23] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 本研究通过优化DRAGON Longformer模型，提升了其在临床文本分类任务中的表现，并展示了其在医疗领域的广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提高临床文本分类的准确性，特别是在处理医学案例描述时，以改善自然语言处理在医疗领域的应用效果。

Method: 研究对预训练的joeranbosma/dragon-longformer-base-mixed-domain模型进行了超参数调整、领域特定的预处理和架构调整，包括增加序列长度、调整学习率、延长训练轮数以及引入专业医学术语。

Result: 优化后的模型在准确率、精确率、召回率和F1分数上均有显著提升，分别达到85.2%、84.1%、86.3%和85.2%，且统计分析确认了这些改进的显著性。

Conclusion: 该研究展示了优化后的DRAGON Longformer基础模型在临床文本分类任务中的显著性能提升，并表明该模型在医疗领域的广泛应用潜力。

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [24] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: 本文介绍了CoNLL-2013共享任务的定义、数据集、评估指标和评分器，并概述了参与团队的方法和评估结果。


<details>
  <summary>Details</summary>
Motivation: CoNLL-2013共享任务旨在促进语法错误纠正的研究，因此需要明确任务定义、数据集和评估方法。

Method: 本文描述了CoNLL-2013共享任务的定义、数据集、评估指标和评分器，并对参与团队的方法进行了概述。

Result: 本文提供了参与团队在语法错误纠正任务中的方法概述和评估结果。

Conclusion: 本文总结了CoNLL-2013共享任务的定义、数据集、评估指标和评分器，并概述了参与团队采用的各种方法以及评估结果。

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [25] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文综述了RAG和推理方法的结合，提出了更有效、多模态适应、可信和以人类为中心的RAG-Reasoning系统的研究方向。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决RAG在需要多步骤推理的问题上的不足，以及纯推理方法容易产生幻觉或错误定位事实的问题。

Method: 本文通过将先进推理优化RAG的每个阶段，以及展示不同类型的检索知识如何提供缺失前提和扩展上下文来实现复杂推理，最后关注新兴的Synergized RAG-Reasoning框架。

Result: 本文分类了方法、数据集和开放挑战，并提出了更深入的RAG-Reasoning系统的研究方向。

Conclusion: 本文总结了RAG和推理方法的结合，提出了更有效、多模态适应、可信和以人类为中心的RAG-Reasoning系统的研究方向。

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [26] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: This paper introduces M2SaG, a multimodal sarcasm generation dataset, and proposes ViSP, a framework that improves sarcasm generation by integrating PPO and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Sarcasm generation remains underexplored due to overreliance on textual modalities and neglect of visual cues, as well as the mismatch between image content and sarcastic intent in existing datasets.

Method: ViSP, a generation framework that integrates Proximal Policy Optimization (PPO) and contrastive learning.

Result: ViSP surpasses all baselines, including large language models, and generates texts with higher mean Sarcasm Scores and Factual Incongruity compared to the original dataset.

Conclusion: ViSP produces higher-quality sarcastic content than the original dataset, and the dataset and code will be publicly available.

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [27] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的ABSA方法，通过训练数据增强和强化学习优化数据增强过程，实验结果表明该方法在英语基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的研究在使用大型语言模型进行ABSA时面临挑战，如短文本、小而不平衡的标记训练数据等问题，因此需要一种有效的策略来提供更丰富的上下文信息。

Method: 我们提出了一种基于大型语言模型（LLM）的ABSA方法，通过训练数据增强来构建更大规模且标签分布平衡的训练数据，并采用强化学习方法优化数据增强过程。

Result: 实验结果表明，我们的方法在英语基准数据集上表现出色，优于强基线和大多数现有研究。

Conclusion: 实验结果和进一步分析表明，我们的方法在英语基准数据集上有效，表现优于强大的基线和大多数现有研究。

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [28] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: GoalfyMax是一个协议驱动的框架，用于端到端的多智能体协作，通过标准化的通信层和分层记忆系统实现高效的协调和持续学习。


<details>
  <summary>Details</summary>
Motivation: 现代企业环境需要能够处理复杂、动态和多面任务的智能系统，具有高度的自主性和适应性。然而，传统的单用途AI系统往往缺乏足够的协调、记忆重用和任务分解能力，限制了它们在现实设置中的可扩展性。

Method: GoalfyMax引入了一个基于Model Context Protocol (MCP)的标准Agent-to-Agent (A2A)通信层，以及Experience Pack (XP)架构，这是一种分层的记忆系统，可以保留任务理由和执行轨迹，从而实现结构化的知识保留和持续学习。此外，系统集成了多轮上下文对话、长短时记忆模块和动态安全验证等先进功能，支持稳健的实时策略适应。

Result: 在复杂任务编排基准和案例研究中的实证结果表明，GoalfyMax相比基线框架在适应性、协调性和经验重用方面表现更优。

Conclusion: GoalfyMax展示了其作为可扩展、未来准备好的多智能体智能系统基础的潜力。

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [29] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出Ref-Long基准，用于评估长上下文引用能力，发现现有模型存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 长上下文引用任务在长上下文语言模型中尚未得到充分研究，需要一个专门的基准来评估其能力。

Method: 本文设计了Ref-Long基准，包含从合成到现实场景的三个子集，用于评估长上下文引用能力。

Result: 实验结果表明，13个LCLMs在长上下文引用任务中存在显著不足，甚至包括GPT-4o这样的先进模型。

Conclusion: 本文提出了Ref-Long基准，以评估长上下文引用能力，并发现即使先进的模型如GPT-4o也存在显著不足。

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [30] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

TL;DR: 本研究分析了提示质量对大型语言模型在机器翻译任务中表现的影响，发现提示质量对翻译性能有显著影响，不同类型噪声对性能的影响不同，低质量提示主要导致指令遵循变差。


<details>
  <summary>Details</summary>
Motivation: 研究提示中的错误和扰动如何影响大型语言模型在机器翻译任务中的表现，以更好地理解模型对提示质量的敏感性。

Method: 我们系统地评估了人类可能的和合成的错误如何影响LLMs在机器翻译和机器翻译评估任务上的表现，并提供了定量分析和定性见解。

Result: 提示质量对翻译性能有显著影响，具有许多错误的提示可能会比没有错误的简短提示表现更差。字符级和混合噪声对性能的负面影响大于短语扰动。定性分析表明，低质量提示主要导致指令遵循变差，而不是直接影响翻译质量。此外，LLMs在充满随机噪声的情况下仍能进行翻译。

Conclusion: 提示质量对翻译性能有显著影响，不同类型的噪声对翻译质量的影响不同，低质量的提示主要导致指令遵循变差，而不是直接影响翻译质量本身。此外，即使在充满随机噪声的情况下，LLMs仍然可以进行翻译。

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [31] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: 本文研究了如何将定义建模系统适应到白俄罗斯语，并提出了一个大规模数据集，结果显示适应过程需要少量数据，但自动度量标准仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 定义建模任务有望帮助词典编纂者记录更多种类的语言和方言，但需要进一步研究如何利用现有模型来支持尚未支持的语言。

Method: 本文提出了一个包含43,150个定义的新数据集，用于研究如何将现有的定义建模系统适应到白俄罗斯语中。

Result: 实验结果表明，适应定义建模系统只需要少量数据，但当前的自动度量标准在捕捉某些方面仍有不足。

Conclusion: 本文指出，尽管定义建模系统在适应新语言时需要的数据量很小，但目前自动度量标准在捕捉某些方面仍存在差距。

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [32] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文介绍了NMIXX，一种用于金融跨语言探索的神经嵌入模型，并发布了KorFinSTS基准，以解决通用模型在捕捉金融语义方面的不足。


<details>
  <summary>Details</summary>
Motivation: 通用句子嵌入模型在捕捉专业金融语义方面存在困难，特别是在低资源语言如韩语中，由于领域特定术语、时间意义变化和双语词汇不对齐。

Method: 引入NMIXX（跨语言金融探索的神经嵌入），这是一个经过18.8K高置信度三元组微调的跨语言嵌入模型集，包括领域内同义词、基于语义变化类型的困难负样本以及精确的韩英翻译。

Result: 在七个开源基线上评估，NMIXX的多语言bge-m3变体在英语FinSTS上获得了+0.10的Spearman's rho增益，在KorFinSTS上获得了+0.22的增益，超过了其预适应检查点和其他模型，同时在一般STS性能上表现出适度的权衡。

Conclusion: 通过公开模型和基准，我们为金融领域的领域适应和多语言表示学习提供了强大的工具。

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [33] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: SpreadPy是一个Python库，用于模拟认知网络中的扩散激活，通过与知识建模的基础理论比较，帮助研究激活动力学如何反映认知、心理和临床现象，并展示了其在不同领域的应用。


<details>
  <summary>Details</summary>
Motivation: 为了研究激活动力学如何反映认知、心理和临床现象，以及探索个体差异和认知障碍的机制性见解，作者开发了SpreadPy这个工具。

Method: SpreadPy是一个用于模拟认知单层和多层网络中扩散激活的Python库。通过将模拟结果与知识建模中的基础理论进行比较，SpreadPy能够系统地研究激活动力学如何反映认知、心理和临床现象。

Result: 通过三个案例研究，展示了SpreadPy的实用性：(1) 关联知识网络上的扩散激活区分了高数学焦虑和低数学焦虑的学生，揭示了概念组织中的焦虑相关结构差异；(2) 创造力任务的模拟显示激活轨迹随着任务难度变化，暴露了认知负荷如何调节词汇访问；(3) 在失语症患者中，模拟的激活模式与图片命名任务中的经验错误类型（语义与语音）相关，将网络结构与临床损害联系起来。

Conclusion: SpreadPy的灵活框架允许研究人员使用经验推导或理论网络来建模这些过程，为个体差异和认知障碍提供了机制性见解。该库是公开的，支持心理学、神经科学和教育研究中的可重复研究。

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [34] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: 本文是首次研究阿拉伯语知识编辑的工作，评估了多种方法在阿拉伯语中的表现，并展示了多语言训练的优势。


<details>
  <summary>Details</summary>
Motivation: 由于知识编辑在像阿拉伯语这样的形态丰富的语言中尚未被充分研究，因此本文旨在填补这一空白。

Method: 评估了四种方法（ROME、MEMIT、ICE和LTE）在阿拉伯语翻译的ZsRE和Counterfact基准上的表现，分析了多语言和跨语言设置。还扩展了Learning-To-Edit（LTE）到多语言设置，并进行了阿拉伯语-英语联合训练。

Result: 实验结果显示，基于参数的方法在跨语言泛化方面表现不佳，而经过指令微调的方法表现更稳健。通过联合阿拉伯语-英语训练，LTE在编辑能力和迁移能力上都有所提升。

Conclusion: 本文提出了对阿拉伯语知识编辑的首次研究，并释放了阿拉伯语KE基准和多语言训练数据以支持未来的研究。

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [35] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: 本文提出一种基于GRPO的方法，通过BGE-M3嵌入提升泰语法律问答系统的性能，取得了显著的引用准确性和响应质量提升。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）系统在泰语法律问答任务上的表现仍然有限，尤其是在需要广泛和复杂法律推理的问题上。

Method: 我们引入了一种使用组相对策略优化（GRPO）的方法，使大语言模型更注重法律引用准确性并提高响应质量。我们利用BGE-M3嵌入作为成本效益高的语义相似性奖励，显著降低了计算成本。

Result: 实验表明，GRPO在基准测试中实现了显著改进：相比基础模型，引用-F1得分提高了90%，联合质量指标比指令调优提高了31%。

Conclusion: 我们的方法在复杂法律推理任务上表现出更强的鲁棒性，为增强泰语法律大模型提供了一种有效且资源高效的解决方案。

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [36] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: MCEval 是一个新颖的多语言评估框架，通过动态文化问题构建和因果分析（通过反事实重述和混杂因素重述）来评估大型语言模型的文化意识和文化偏见。该框架涵盖了13种文化和13种语言，提供了大量文化意识和文化偏见实例。实验结果揭示了不同语言场景下的性能差异，并暴露了公平性问题。


<details>
  <summary>Details</summary>
Motivation: Large language models exhibit cultural biases and limited cross-cultural understanding capabilities, particularly when serving diverse global user populations.

Method: MCEval, a novel multilingual evaluation framework that employs dynamic cultural question construction and enables causal analysis through Counterfactual Rephrasing and Confounder Rephrasing.

Result: Experimental results reveal performance disparities across different linguistic scenarios, demonstrating that optimal cultural performance is not only linked to training data distribution, but also is related to language-culture alignment. The evaluation results also expose the fairness issue, where approaches appearing successful in the English scenario create substantial disadvantages.

Conclusion: MCEval represents the first comprehensive multilingual cultural evaluation framework that provides deeper insights into LLMs' cultural understanding.

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [37] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型的潜在空间几何结构，发现高级语义信息位于低维子空间中，这些子空间在不同领域中形成线性可分离的表示。这种几何结构使得在隐藏空间中进行简单的因果干预成为可能，并提出了基于传输的防御方法来检测和减轻有害内容。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）的潜在空间几何对于解释其行为和提高对齐度至关重要。然而，尚不清楚LLMs在内部组织与语义理解相关的表示的程度。

Method: 我们进行了大规模的实证研究，分析了11个解码器-only模型在6个科学主题和每个12层中的隐藏状态。

Result: 高级语义信息始终位于低维子空间中，这些子空间在不同领域中形成线性可分离的表示。这种可分离性在更深的层中更加明显，并且在触发结构化推理或对齐行为的提示下甚至当表面内容不变时也是如此。这种几何结构使得在隐藏空间中进行简单的因果干预成为可能；例如，像思维链这样的推理模式可以通过单一向量方向捕获。

Conclusion: 这些发现支持了开发几何感知工具的发展，这些工具可以直接在潜在表示上操作，以检测和减轻有害或对抗性内容，使用的方法包括基于传输的防御措施，利用这种可分离性。作为概念验证，我们通过训练一个简单的MLP分类器作为轻量级潜在空间防护装置，展示了这种潜力，该装置可以高精度检测对抗性和恶意提示。

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [38] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文提出了一种自适应课程学习范式，利用预训练语言模型预测的难度分数对微调示例进行优先排序，并在多个自然语言理解数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数方法依赖于手动定义的难度度量标准，如文本长度，这可能无法准确反映模型自身的视角。为了克服这一限制，我们提出了一个自适应课程学习范式。

Method: 我们提出了一种自适应课程学习范式，根据预训练语言模型（PLMs）自身预测的难度分数对微调示例进行优先排序。基于这些分数，我们探索了不同的训练策略，这些策略在微调时示例的顺序有所不同：从简单到困难、从困难到简单，再到混合采样。

Result: 实验结果表明，我们的方法比标准的随机采样实现了更快的收敛和更好的性能。

Conclusion: 我们的方法在四个自然语言理解数据集上进行了评估，实验结果表明，与标准的随机采样相比，该方法能够实现更快的收敛和更好的性能。

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [39] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: 本文重新定义了点击诱饵，提出了一个新的检测数据集创建方法，并实现了高精度的检测模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对点击诱饵的共识定义，因此需要一种更准确的定义和检测方法。

Method: 本文重新定义了点击诱饵，提出了一种新的检测数据集创建方法，并实现了强基线模型。

Result: 创建了TA1C数据集，实现了0.84的F1分数。

Conclusion: 本文提出了一种新的点击诱饵定义，并创建了一个用于点击诱饵检测的开源数据集TA1C，实现了较高的F1分数。

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [40] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: 本研究通过分析离线加法任务，揭示了语言模型中函数归纳机制及其在任务泛化中的作用。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型如何通过上下文学习执行未见过的任务，特别是它们内部机制如何驱动任务级泛化。

Method: 我们通过电路风格的解释技术（如路径修补）分析了模型内部计算，以研究其在离线加法任务中的表现。

Result: 我们发现了三种关键发现：函数归纳机制、+1函数由多个注意力头并行处理，以及该机制在更广泛的任务中的复用。

Conclusion: 我们的研究提供了对语言模型中可重用和可组合结构如何促进任务级泛化的更深入见解。

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [41] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，通过整合分层文本分割和聚类来改进RAG系统，从而生成更具有语义意义的块，并在多个数据集上取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法创建足够捕捉语义意义的块，因为它们没有考虑底层文本结构。

Method: 提出了一种新的框架，通过整合分层文本分割和聚类来生成更有意义和语义连贯的块。

Result: 在NarrativeQA、QuALITY和QASPER数据集上的评估表明，所提出的方法比传统分块技术取得了更好的结果。

Conclusion: 该方法在NarrativeQA、QuALITY和QASPER数据集上表现出优于传统分块技术的结果。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [42] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: TinyRM是一种小型双向掩码语言模型，能够在推理和安全偏好建模任务上与大型模型相媲美，同时使用更少的资源。


<details>
  <summary>Details</summary>
Motivation: 由于奖励模型在测试时策略中的部署增加，其推理成本成为一个日益关注的问题。

Method: TinyRM结合了FLAN风格的提示、方向性低秩适应（DoRA）和层冻结，以在RewardBench上实现强大的性能。

Result: TinyRM在推理和安全偏好建模任务上与比它大175倍的模型相当。

Conclusion: 我们的初步结果表明，轻量级的双向架构在偏好建模中具有作为高效、可扩展替代方案的潜力。

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [43] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: 本文介绍了TextOmics基准和ToDi生成框架，用于生成具有治疗潜力的类似命中分子。


<details>
  <summary>Details</summary>
Motivation: 缺乏异构数据和统一框架来整合不同的分子表示，阻碍了靶向药物发现的进展。

Method: TextOmics是一个基准，建立了组学表达和分子文本描述之间的一一对应关系。ToDi是一个生成框架，结合组学表达和分子文本描述来生成生物相关、化学有效、类似命中分子。

Result: TextOmics和ToDi在实验中表现出有效性，并优于现有的最先进方法，同时展示了在零样本治疗分子生成中的巨大潜力。

Conclusion: TextOmics和ToDi在分子生成任务中表现出色，并展示了在零样本治疗分子生成中的巨大潜力。

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [44] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: 本研究提出了一种新的框架，用于预测随后的自杀风险，通过联合学习风险因素和保护因素对用户自杀风险转变的动态影响。实验结果表明，所提出的模型在三个数据集上显著优于最先进的模型和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注风险因素，而忽视了在自杀风险预测中起关键作用的保护因素。因此，本研究旨在提出一种新的框架，以同时考虑风险因素和保护因素对自杀风险的影响。

Method: 本研究提出了一个名为Protective Factor-Aware Dataset的数据集，该数据集基于12年的Reddit帖子以及自杀风险和风险及保护因素的全面标注。还引入了一种动态因素影响学习方法，以捕捉风险和保护因素对自杀风险转变的不同影响。

Result: 实验结果表明，所提出的模型在三个数据集上显著优于最先进的模型和大型语言模型。此外，动态因素影响学习方法提供了可解释的权重，有助于临床医生更好地理解自杀模式并制定更有针对性的干预策略。

Conclusion: 本研究提出了一种新的框架，用于预测随后的自杀风险，通过联合学习风险因素和保护因素对用户自杀风险转变的动态影响。实验结果表明，所提出的模型在三个数据集上显著优于最先进的模型和大型语言模型。此外，提出的动态因素影响学习方法提供了可解释的权重，有助于临床医生更好地理解自杀模式并制定更有针对性的干预策略。

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [45] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: 本文提出了一种基于进化的LLM压缩方法GeLaCo，通过层折叠实现高效压缩，并在多个评估中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）由于计算需求大，在部署和使用方面面临重大障碍。模型压缩方法旨在减少模型大小同时保留其能力，是缓解这些问题的重要手段。然而，现有的方法如结构化剪枝通常需要昂贵的经验搜索，并可能忽略更好的解决方案。

Method: GeLaCo是一种基于进化的LLM压缩方法，通过层折叠实现。它支持基于种群的搜索和模块级相似性适应度函数，可以进行单目标和多目标进化压缩搜索。

Result: GeLaCo通过高效的压缩解空间探索，建立了压缩和质量轴上的第一个帕累托前沿。

Conclusion: GeLaCo在基础模型和指令调优模型上通过基于困惑度和生成的评估，优于最先进的替代方案。

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [46] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: 我们的研究发现大型语言模型未能代表多样化的文化道德框架，这挑战了LLM作为社会科学研究中合成种群的日益增长的使用，并突显了当前AI对齐方法的基本局限性。


<details>
  <summary>Details</summary>
Motivation: 我们的研究旨在探讨AI系统是否真正代表人类价值观，或者只是平均它们。

Method: 我们通过在19个文化背景中应用道德基础问卷，比较了多种最先进的LLM的起源与人类基准数据，以揭示AI生成的和人类道德直觉之间的显著差距。

Result: 我们发现这些模型系统地同质化道德多样性，增加模型大小并不总是能提高文化表现的准确性。

Conclusion: 我们的研究结果呼吁更扎实的对齐目标和评估指标，以确保AI系统代表多样化的人类价值观，而不是扁平化道德景观。

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [47] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出了一种新的参数高效微调方法CRFT，通过优化关键表示空间来提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ReFT方法在复杂推理任务中表现不佳，因为固定位置的表示对输出的影响不确定。本文旨在寻找并优化那些对最终输出有重大影响的关键表示。

Method: 本文提出了一种名为CRFT的方法，通过信息流分析识别并优化关键表示，在监督学习框架下动态优化关键表示。

Result: 本文的方法在八个基准测试中验证了其有效性和效率，并在少样本设置中提升了16.4%的一次性准确性。

Conclusion: 本文展示了通过优化关键表示空间，可以显著提升复杂推理任务的性能，并提供了一种轻量级且高效的替代传统PEFT方法的新方法。

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [48] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: 本文设计了一种新的基于Transformer的架构，结合了LLMs和普通Transformer的优势，以提高时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法在时间序列预测任务中表现不佳，因为LLM擅长处理离散标记和语义模式，但不是专门为建模连续数值时间序列数据而设计的。而普通Transformer难以学习高层次的语义模式。

Method: 设计了一种新的基于Transformer的架构，互补地利用LLMs和普通Transformer，以将LLMs学到的高层语义表示整合到时间序列Transformer编码的时间信息中，通过融合LLM和Transformer的表示得到混合表示。

Result: 实验结果表明，所提出的方法在基准数据集上是有效的。

Conclusion: 实验结果表明，所提出的方法是有效的。

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [49] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于任务的特征蒸馏方法，可以在不引入新参数的情况下实现教师模型和学生模型之间的知识迁移，并在多个任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统特征蒸馏方法通常假设教师模型和学生模型具有相同的隐藏大小，限制了学生模型架构的灵活性。而现有的解决方案引入了额外的参数，可能会影响下游任务的性能。因此，需要一种更灵活且无需引入新参数的方法来实现知识迁移。

Method: 本文提出了一种新的基于任务的特征蒸馏方法，通过识别教师模型中对特定下游任务最相关的隐藏单元，并直接将这些激活值蒸馏到学生模型中，从而实现知识迁移。

Result: 实验结果表明，本文提出的方法在多种任务（包括分类、指令遵循和摘要）中均取得了比之前方法更好的效果，最高性能提升达到了3%。

Conclusion: 本文提出了一种新的基于任务的特征蒸馏方法，能够在不引入新参数的情况下实现教师模型和学生模型之间的知识迁移。该方法在多个任务中取得了比之前方法更好的效果，最高性能提升达到了3%。

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [50] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: 本研究旨在利用大型语言模型将包含仇恨言论和脏话的有害文本转换为非有害文本，同时保留其意图，并评估不同LLMs的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言处理任务中表现出显著进展，但它们在分类和将有害文本转换为非有害版本方面的有效性仍需探索。

Method: 我们评估了两种最先进的LLMs（如Gemini、GPT-4o、DeekSeek和Groq）在识别仇恨言论和脏话方面的能力，并将其用于将有害文本转换为非有害文本，同时保留文本的意图。

Result: 我们的结果显示，与其它LLMs相比，Groq提供了截然不同的结果。我们还发现了GPT-4o和DeepSeek-V3之间的相似性。

Conclusion: 我们的研究结果表明，Groq与其他LLMs相比提供了截然不同的结果，而GPT-4o和DeepSeek-V3之间存在相似性。

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [51] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: 本文介绍了一个名为	exttt{Absher}的基准测试，旨在评估大型语言模型（LLMs）在沙特阿拉伯主要方言中的表现。该基准测试包含超过18,000个多项选择题，涵盖六个不同的类别。通过评估几种最先进的LLMs，我们发现了在需要文化推理或上下文理解的任务中存在显著的性能差距，并强调了需要进行方言意识训练和文化对齐的评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在阿拉伯语自然语言处理（NLP）应用中变得越来越重要，评估它们对地区方言和文化细微差别的理解至关重要，特别是在像沙特阿拉伯这样语言多样化的环境中。

Method: 本文介绍了	exttt{Absher}，这是一个专门设计用于评估LLMs在主要沙特方言中的表现的全面基准。	exttt{Absher}包含超过18,000个多项选择题，涵盖六个不同的类别：意义、真/假、填空、上下文使用、文化解释和位置识别。这些问题源自从沙特阿拉伯各地收集的方言词汇、短语和谚语的精心整理数据集。我们评估了几种最先进的LLMs，包括多语言和阿拉伯语专用模型，并提供了它们的能力和局限性的详细见解。

Result: 我们的结果揭示了在需要文化推理或上下文理解的任务中，LLMs存在显著的性能差距。

Conclusion: 我们的研究结果揭示了在需要文化推理或上下文理解的任务中，LLMs存在显著的性能差距。这突显了迫切需要进行方言意识训练和文化对齐的评估方法，以提高LLMs在实际阿拉伯语应用中的表现。

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [52] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: 本文提出了一种基于进化搜索的自动化离散提示优化方法，该方法在多个小型语言模型和复杂任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数先进方法在需要最少提示模板的任务和非常大且功能强大的大型语言模型上进行评估，而解决需要详细信息包含在提示中的复杂任务会增加需要优化的文本量，并且较小的模型对提示设计更敏感。

Method: 我们提出了一种基于进化搜索的自动化离散提示优化方法，包括两个阶段：第一阶段使用语法引导的遗传编程来合成创建提示的程序；第二阶段应用局部搜索来进一步微调最佳程序的性能。

Result: 我们的方法在三个相对较小的通用大型语言模型上优于三种最先进的提示优化方法，在四个特定领域的挑战性任务中表现更好。

Conclusion: 我们的方法在三个相对较小的通用大型语言模型上优于三种最先进的提示优化方法，在四个特定领域的挑战性任务中表现更好。

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [53] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 本文提出了一种基于生长边界矩阵的新正则化技术，以提高NLP模型的鲁棒性，特别是在对抗性攻击下的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言处理（NLP）取得了进展，但模型仍然容易受到对抗性攻击，如同义词替换。递归网络和现代状态空间模型（SSMs）的鲁棒性研究不足，因此需要一种新的方法来提高它们的鲁棒性。

Method: 我们引入了一种基于生长边界矩阵（GBM）的新正则化技术，以通过减少输入扰动对模型输出的影响来提高NLP模型的鲁棒性。

Result: 我们的方法在多个架构和基准数据集上的实验表明，它在对抗性鲁棒性方面比现有基线提高了高达8.8%。

Conclusion: 我们的方法在对抗性鲁棒性方面比现有基线提高了高达8.8%，并优于几种最先进的对抗性防御方法。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [54] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: 本研究发现大型语言模型（如GPT-4）在心理语言学任务中的表现与人类高度一致，表明它们可以作为语言学研究的可靠合作伙伴。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）作为语言研究中可靠分析工具的潜力，特别是关注涉及方式运动动词的时间表达中情感意义的出现。

Method: 我们进行了四项心理语言学研究（关于新兴意义、情感极性变化、情感语境中的动词选择以及句子-表情符号关联），首先与人类参与者进行，然后使用LLM重复相同的任务。

Result: 所有研究的结果显示出人类和AI响应之间的显著一致性，统计分析（例如，Spearman's rho = .73-.96）表明评分模式和分类选择之间存在强相关性。虽然在某些情况下观察到微小的差异，但这些差异并未改变总体解释结果。

Conclusion: 本研究支持将大型语言模型（LLMs）作为可信且信息丰富的语言学研究合作伙伴使用。

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [55] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: 本文提出了一种分层的隐喻处理模型，将意义视为多层结构，包括内容分析、概念融合和语用意图，以更深入和上下文敏感的方式解释隐喻。


<details>
  <summary>Details</summary>
Motivation: 隐喻意义不是概念之间的平面对应，而是一种复杂的认知现象，整合了多个层次的解释。

Method: 我们提出了一个分层的隐喻处理模型，将意义视为洋葱：一个多层结构，包括（1）内容分析，（2）概念融合，和（3）语用意图。

Result: 该三维框架允许计算系统采用更丰富且基于认知的方法来解释隐喻。在第一层，通过基本概念元素对隐喻进行注释。在第二层，我们建模概念组合，将组件与新兴意义联系起来。最后，在第三层，我们引入语用词汇来捕捉说话者的意图、交际功能和情境效应，使隐喻理解与语用理论一致。

Conclusion: 通过将这些层次统一到一个正式的框架中，我们的模型为计算方法奠定了基础，这些方法能够超越表面关联，实现更深层次、更敏感的语境推理，从而表示隐喻意义。

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [56] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: 本文研究了Transformer如何理解图结构，并提出了ISF方法来分析其内部机制，验证了其在不同图类型上的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解解码器-only Transformer架构如何理解潜在的图结构，并探索其在处理各种图类型上的能力。

Method: 我们通过实证结果和理论分析提出了诱导子结构过滤（ISF），并验证了LLMs中的ISF过程，揭示了跨层的一致内部动态。

Result: 我们展示了解码器-only Transformer可以成功地从属性图中提取子结构，例如分子图。

Conclusion: 我们的研究提供了关于基于序列的Transformer如何在图数据上执行子结构提取任务的新见解。

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [57] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文分析了LLMs在任务导向对话中提出澄清问题的能力，并发现其与模糊性之间的关系较弱，同时测试了推理方法对问题频率和相关性的影响。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究LLMs在任务导向对话中提出澄清问题的能力，并探讨其与模糊性的关系。

Method: 本文通过构建一个结合两个现有注释的Minecraft Dialogue Corpus的新语料库，比较了LLMs的行为与原始人类生成的澄清问题，以研究在模糊情况下的行为。

Result: 研究发现，模糊性和人类生成澄清问题之间的联系较弱，人类很少为指代模糊性提出澄清问题，但经常为任务不确定性提出。相反，LLMs为指代模糊性提出更多澄清问题，但对任务不确定性则较少。

Conclusion: 本文结论是，LLMs在提出澄清问题方面的能力可能与其最近模拟推理的能力有关，并且通过不同的推理方法测试发现，推理确实增加了问题的频率和相关性。

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [58] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: 本研究比较了传统双向变换器编码器和新一代超大规模自回归语言模型在仇恨言论检测任务中的性能，发现后者并未表现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 在线平台在遏制仇恨言论的同时避免过度审查合法讨论方面面临挑战。虽然早期的双向变换器编码器取得了进展，但超大规模自回归语言模型（LLM）有望提供更深入的上下文感知能力。然而，这种额外的规模是否真的能提高现实世界文本中的仇恨言论检测效果仍需验证。

Method: 研究通过在精心策划的在线互动语料库上对两种模型家族（传统编码器和新一代LLM）进行基准测试，评估了它们在仇恨言论检测任务中的性能。

Result: 研究结果表明，超大规模自回归语言模型在仇恨言论检测任务中的表现并未显著优于传统的双向变换器编码器。

Conclusion: 研究发现，尽管超大规模自回归语言模型在理论上提供了更深层次的上下文感知能力，但它们在实际仇恨言论检测任务中的表现并不优于传统的双向变换器编码器。

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [59] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: This paper introduces MLAR, an innovative Applicant Tracking System enhanced by a Robotic Process Automation framework that uses Large Language Models to efficiently screen and match candidates, outperforming existing RPA platforms.


<details>
  <summary>Details</summary>
Motivation: Traditional recruitment processes often encounter bottlenecks in resume screening and candidate shortlisting due to time and resource constraints.

Method: MLAR employs Large Language Models (LLMs) in three distinct layers: extracting key characteristics from job postings, parsing applicant resumes to identify education, experience, skills, and similarity matching. These features are then matched through advanced semantic algorithms.

Result: MLAR outperforms leading RPA platforms, including UiPath and Automation Anywhere, in high-volume resume-processing tasks. When processing 2,400 resumes, MLAR achieved an average processing time of 5.4 seconds per resume, reducing processing time by approximately 16.9% compared to Automation Anywhere and 17.1% compared to UiPath.

Conclusion: MLAR has the potential to transform recruitment workflows by providing an efficient, accurate, and scalable solution tailored to modern hiring needs.

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [60] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Main category: cs.CL

TL;DR: 本文比较了扩散生成文本和自回归生成文本，发现扩散生成文本在某些指标上接近人类文本，表明需要针对扩散模型的检测器。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，可靠检测AI生成文本的问题变得越来越重要。目前的风格度量在自回归输出上表现良好，但对扩散模型的效果尚不清楚。

Method: 本文对扩散生成文本（LLaDA）和自回归生成文本（LLaMA）进行了系统的比较，使用了2000个样本。分析了困惑度、突发性、词汇多样性、可读性和BLEU/ROUGE分数。

Result: LLaDA在困惑度和突发性方面接近人类文本，导致自回归检测器出现高假阴性率。LLaMA的困惑度较低，但词汇保真度较差。单一指标无法区分扩散输出和人类写作。

Conclusion: 本文强调了需要针对扩散模型的检测器，并提出了混合模型、扩散特定的风格度量特征和稳健水印等方向。

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [61] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: MoR是一种统一的框架，结合了参数共享和自适应计算两个方面的效率，能够在不增加大模型成本的情况下实现大模型的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的效率努力通常只针对参数共享或自适应计算，而如何同时实现两者仍然是一个开放的问题。

Method: Mixture-of-Recursions (MoR) 是一种统一的框架，结合了参数共享和自适应计算两个方面的效率。它通过在递归步骤中重用共享的层堆栈来实现参数效率，同时轻量级的路由器通过动态分配不同的递归深度给单个令牌来实现自适应的token级思考。

Result: 在从135M到1.7B参数的模型规模上，MoR形成了一个新的帕累托前沿：在相等的训练FLOPs和较小的模型大小下，它显著降低了验证困惑度并提高了少样本准确性，同时相比原始和现有的递归基线提供了更高的吞吐量。

Conclusion: MoR是一个有效的路径，可以在不增加大模型成本的情况下实现大模型的质量。

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [62] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: 本文介绍了CodeJudgeBench基准，评估了LLM-as-a-Judge在代码任务中的表现，发现思考模型优于非思考模型，但判断仍存在随机性和敏感性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏专门的基准，LLM-as-a-Judge在代码场景中的有效性尚未得到充分探索。

Method: 引入CodeJudgeBench基准，评估LLM-as-a-Judge模型在代码生成、代码修复和单元测试生成三个任务上的表现，并研究了最优提示策略。

Result: 最近的思考模型在精心设计的代码判断任务中显著优于非思考模型，甚至较小的模型如Qwen3-8B也能超越70B规模的专门训练模型。然而，所有模型在判断代码任务时仍表现出显著的随机性。

Conclusion: 研究发现，尽管LLM-as-a-Judge在代码任务中表现出一定的能力，但其判断仍存在显著的随机性，且对响应顺序和内容形式敏感，这引发了对其可靠性和一致性的担忧。

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [63] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 本文提出REST评估框架，通过同时暴露多个问题来评估大型推理模型的性能，发现SOTA模型在压力测试下性能下降，并揭示了REST作为更有效评估范式的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法受限于孤立的问题解决范式，存在数据污染和难以挑战的问题，以及无法在多上下文压力下评估模型的缺点。

Method: REST（Reasoning Evaluation through Simultaneous Testing）是一个压力测试框架，同时向LRMs暴露多个问题。

Result: 即使最先进的模型如DeepSeek-R1在压力测试下也表现出显著的性能下降。REST比现有基准具有更强的区分能力，揭示了在单问题评估中表现相似的模型之间的明显性能差异。

Conclusion: REST作为一种成本效益高、未来适用的评估范式，更好地反映了现实世界的推理需求，同时减少了对持续人工注释的依赖。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: 本文展示了直接偏好优化（DPO）是机器学习中从偏好学习的两个主要理论之间的连接的一种非常特定的形式：损失函数（Savage）和随机选择（Doignon-Falmagne和Machina）。


<details>
  <summary>Details</summary>
Motivation: 为了理解DPO如何从一般原理的角度运作，因为模型的应用范围广泛，DPO目前势头强劲，但DPO的许多最新变体只占据我们所覆盖地图的一小部分。

Method: 本文展示了直接偏好优化（DPO）是机器学习中从偏好学习的两个主要理论之间的连接的一种非常特定的形式：损失函数（Savage）和随机选择（Doignon-Falmagne和Machina）。

Result: 该连接适用于所有Savage的损失，在这个一般性水平上，(i)它支持选择理论方面的弃权，(ii)它支持机器学习方面的非凸目标，(iii)它允许免费地构架DPO设置的一些显著扩展，包括边界和长度校正。

Conclusion: 理解DPO从一般原理的角度至关重要，因为模型的应用范围广泛，DPO目前势头强劲，但DPO的许多最新变体只占据我们所覆盖地图的一小部分。这也有助于理解偏离此地图的陷阱，并找出解决方法。

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [65] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: 本文分析了LoRA在不同模型架构和训练设置中的性能，并提出了更高效的LLM微调方法，实验证明其性能优于或等于LoRA。


<details>
  <summary>Details</summary>
Motivation: 我们观察到LoRA在所有模型架构和训练设置中并不总是提供速度提升，因此我们进行了分析以找出原因。

Method: 我们对LoRA的性能进行了全面分析，并调查了限制其加速的潜在因素。基于我们的发现，我们提出了几种更高效的LLM微调方法。

Result: 我们的方法在实验评估中表现出与LoRA相当或更好的性能，并且提供了更一致的训练速度提升。

Conclusion: 我们的工作为在资源限制下优化LLM微调提供了有价值的见解和实用指南。

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [66] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Main category: cs.LG

TL;DR: 我们提出了LoRA-MCL，这是一种通过多重选择学习和胜者通吃损失来扩展语言模型中下一个标记预测的训练方案，以生成多样且合理的句子延续。


<details>
  <summary>Details</summary>
Motivation: 传统的语言建模是一个本质上不明确的问题：给定一个上下文，多个未来可能同样合理。我们需要一种方法来处理这种歧义，并生成多样且合理的句子延续。

Method: 我们提出了LoRA-MCL，这是一种训练方案，它通过一种旨在解码多样且合理的句子延续的方法来扩展语言模型中的下一个标记预测。我们的方法利用了多重选择学习（MCL）和胜者通吃（WTA）损失，通过低秩适应（LoRA）有效地处理歧义。

Result: 我们通过在真实世界的视觉和音频字幕任务上的广泛实验展示了我们的方法。

Conclusion: 我们的方法在真实世界的视觉和音频字幕任务中实现了高多样性和相关性。

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [67] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: 本文分析了Qwen2.5模型在数学推理任务中的表现，指出其在大规模网络语料上的预训练使其容易受到数据污染的影响，导致基准测试结果不可靠。作者提出了一种生成器，用于创建无污染的合成数据集，并强调只有准确的奖励信号才能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的研究通过强化学习（RL）增强了大型语言模型（LLMs）的推理能力，并声称在极少或没有外部监督的情况下取得了显著改进，但这些突破主要是在Qwen2.5模型家族上报告的，并在MATH-500、AMC和AIME等知名基准上进行评估，而在其他模型如Llama上未能实现类似的增益，这需要进一步研究。

Method: 我们引入了一个生成器，可以生成任意长度和难度的完全合成的算术问题，生成一个我们称之为RandomCalculation的干净数据集。利用这些无泄漏的数据集，我们展示了只有准确的奖励信号才能持续提高性能，而嘈杂或错误的信号则不能。

Result: 我们的分析表明，虽然Qwen2.5在数学推理方面表现出色，但由于其在大规模网络语料上的预训练，它容易受到流行基准数据集中的数据污染。因此，从这些基准中得出的结果可能不可靠。我们建议在未受污染的基准上评估RL方法，并在多种模型家族中进行评估，以确保得出可信的结论。

Conclusion: 我们的分析表明，虽然Qwen2.5在数学推理性能上表现强劲，但其在大规模网络语料上的预训练使其容易受到流行基准数据集中的数据污染。因此，从这些基准中得出的结果可能不可靠。我们建议在未受污染的基准上评估RL方法，并在多种模型家族中进行评估，以确保得出可信的结论。

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 本文提出了一种通过去除推理过程中的冗余来提高性能的方法，在数学竞赛基准测试中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在长格式推理中表现出色，但推理路径中存在大量冗余，这可能影响性能。

Method: 本文通过测量到特殊结束思考标记的逐标记注意力分数来系统地识别推理冗余，并提出结构感知剪枝，优先移除低贡献推理块中的标记。

Result: 本文的方法在不需要任何训练的情况下显著提高了推理密集型基准测试的整体准确性，尤其在AIME和AMC等挑战性的数学竞赛基准测试中表现良好。

Conclusion: 本文表明，通过去除推理过程中的冗余可以显著提高性能，特别是在需要大量推理的任务上，如数学竞赛基准测试。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [69] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 本文综述了大型推理模型中简洁和自适应思维的研究进展，旨在提高模型效率并促进实际应用。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务中表现出色，但生成不必要的冗长推理链导致资源浪费和响应时间增加，因此需要缩短推理链并学习自适应思维。

Method: 本文提供了对简洁和自适应思维方法、基准测试以及未来研究挑战的全面概述。

Result: 本文综述了最新的进展，并希望帮助研究人员快速了解该领域的现状，激发新的自适应思维想法。

Conclusion: 本文综述了近年来在简洁和自适应思维方面的进展，旨在提高大型推理模型的效率。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [70] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 本文提出了一种将大型语言模型直接集成到次协调逻辑的形式语义解释函数中的方法，以解决LLM在逻辑一致性方面的不足，并通过实验验证了该方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言理解和生成方面表现出色，但它们在生成输出时存在逻辑一致性问题。本文旨在解决如何利用LLM的广泛参数知识进行形式推理的问题。

Method: 本文提出了一种将大型语言模型直接集成到次协调逻辑的形式语义解释函数中的方法，并通过使用从多个简短事实性基准创建的数据集进行评估来验证该方法的可行性。

Result: 实验结果表明，该方法是可行的，并提供了一种理论框架，可以在保持底层逻辑的可靠性和完全性的同时利用LLM的知识。

Conclusion: 本文提出了一种将大型语言模型直接集成到次协调逻辑的形式语义解释函数中的方法，提供了一种理论框架，可以在保持底层逻辑的可靠性和完全性的同时利用LLM的知识。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [71] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 本研究提出了一种基于多尺度异常分析和时空连贯性的方法，用于识别数据中的潜在意图性，该方法计算成本低且适用于基本生物体。


<details>
  <summary>Details</summary>
Motivation: Searle的工作揭示了意图在哲学领域的重要性，但在科技领域中对意图的实际意义关注较少。因此，本研究旨在探索一种简单有效的方法来识别数据中的潜在意图性。

Method: 通过分析数据中的多尺度异常和评估其形成过程，利用时空连贯性来区分“有意内容”和“环境上下文”，从而识别文本中的主题和概念。

Result: 该方法能够在低计算成本下实现对数据中潜在意图性的初步解释，并且适用于基本生物体的处理能力。

Conclusion: 该研究提供了一种低成本且实用的方法来解释数据中的潜在意图性，而无需依赖大规模训练或推理能力。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [72] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型在不同知识图谱之间自动翻译SPARQL查询的能力，发现模型性能和提示策略对翻译效果有显著影响，且从Wikidata到DBpedia的翻译效果更好。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在填补知识图谱（KG）互操作性研究中的一个显著空白，即评估LLM在SPARQL到SPARQL翻译中的性能。

Method: 研究通过构建两个基准测试来评估大型语言模型（LLMs）在SPARQL查询翻译中的表现，包括DBpedia与Wikidata之间的100个查询以及DBLP与OpenAlex之间的100个查询，并选择了三种开放的LLMs进行测试。

Result: 研究结果表明，不同模型和提示策略的性能差异显著，且从Wikidata到DBpedia的翻译效果远优于从DBpedia到Wikidata的翻译。

Conclusion: 研究发现，不同模型和提示策略的性能差异显著，且从Wikidata到DBpedia的翻译效果远优于从DBpedia到Wikidata的翻译。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [73] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{	ext{Eco}}$ 是一种基于代理的 LLM 系统，用于自动化科学综合，提高了科学文献的整合效率和分析深度。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成流程无法满足用户可控的综合需求，需要一种能够提高搜索多样性和文献检索精度的新方法。

Method: DeepResearch$^{	ext{Eco}}$ 是一种基于代理的 LLM 系统，用于自动化科学综合，支持递归、深度和广度控制的研究问题探索。

Result: DeepResearch 在 49 个生态研究问题中实现了高达 21 倍的源整合增加，以及每 1000 字 14.9 倍的源整合增加。

Conclusion: DeepResearch$^{	ext{Eco}}$ 提高了科学文献的整合效率和分析深度，展示了其在生态研究中的潜力。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [74] [Overview of the TREC 2023 deep learning track](https://arxiv.org/abs/2507.08890)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Hossein A. Rahmani,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: TREC深度学习跟踪赛今年使用了更大的数据集和生成的合成查询，发现使用大型语言模型提示的方法优于之前的最佳方法。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过使用更大的、更干净的、较少偏见的数据集以及生成的合成查询，提高测试的难度并为改进提供更多空间。

Method: 本研究重复了去年的设计，使用更大的、更干净的、较少偏见的v2段落和文档集来获得另一个匹配的测试集。此外，还生成了合成查询，使用微调的T5模型和GPT-4提示。

Result: 使用大型语言模型提示的方法在某些方面超过了之前的最佳方法（nnlm）。评估使用合成查询的结果与人类查询相似，系统排序一致性为$	au=0.8487$。没有明显的偏差证据表明使用GPT-4或T5的运行在特定查询上被偏好。

Conclusion: 今年的TREC深度学习跟踪赛中，使用大型语言模型（LLM）提示的方法在某些方面超过了之前的最佳方法（nnlm）。由于这是该跟踪赛的最后一年，未来基于提示的排名可以在其他跟踪赛中进行。

Abstract: This is the fifth year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of
human-annotated training labels available for both passage and document ranking
tasks. We mostly repeated last year's design, to get another matching test set,
based on the larger, cleaner, less-biased v2 passage and document set, with
passage ranking as primary and document ranking as a secondary task (using
labels inferred from passage). As we did last year, we sample from MS MARCO
queries that were completely held out, unused in corpus construction, unlike
the test queries in the first three years. This approach yields a more
difficult test with more headroom for improvement. Alongside the usual MS MARCO
(human) queries from MS MARCO, this year we generated synthetic queries using a
fine-tuned T5 model and using a GPT-4 prompt.
  The new headline result this year is that runs using Large Language Model
(LLM) prompting in some way outperformed runs that use the "nnlm" approach,
which was the best approach in the previous four years. Since this is the last
year of the track, future iterations of prompt-based ranking can happen in
other tracks. Human relevance assessments were applied to all query types, not
just human MS MARCO queries. Evaluation using synthetic queries gave similar
results to human queries, with system ordering agreement of $\tau=0.8487$.
However, human effort was needed to select a subset of the synthetic queries
that were usable. We did not see clear evidence of bias, where runs using GPT-4
were favored when evaluated using synthetic GPT-4 queries, or where runs using
T5 were favored when evaluated on synthetic T5 queries.

</details>


### [75] [DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate](https://arxiv.org/abs/2507.09090)
*Anthony Miyaguchi,Conor Johnston,Aaryan Potdar*

Main category: cs.IR

TL;DR: 本论文研究了大型语言模型在辩论中的表现及其评估能力，发现它们在给出相关论点时表现良好，但回应冗长且评估一致。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在辩论中的表现以及它们评估辩论中话语的能力。

Method: 部署了三个提供商的六个领先的公开可用模型进行检索增强的辩论和评估。

Result: 测量了四个关键指标：质量、数量、方式和关系。发现LLMs在给出相关论点时表现良好，但回应冗长且评估一致。

Conclusion: 虽然LLMs在给出相关论点时在辩论中表现良好，但它们的回应往往冗长，但在评估上保持一致。

Abstract: Large Language Models (LLMs) demonstrate strong conversational abilities. In
this Working Paper, we study them in the context of debating in two ways: their
ability to perform in a structured debate along with a dataset of arguments to
use and their ability to evaluate utterances throughout the debate. We deploy
six leading publicly available models from three providers for the
Retrieval-Augmented Debate and Evaluation. The evaluation is performed by
measuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout
this task, we found that although LLMs perform well in debates when given
related arguments, they tend to be verbose in responses yet consistent in
evaluation. The accompanying source code for this paper is located at
https://github.com/dsgt-arc/touche-2025-rad.

</details>


### [76] [MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora](https://arxiv.org/abs/2507.09924)
*Tuan-Luc Huynh,Thuy-Trang Vu,Weiqing Wang,Trung Le,Dragan Gašević,Yuan-Fang Li,Thanh-Toan Do*

Main category: cs.IR

TL;DR: MixLoRA-DSI是一种新型框架，通过选择性引入新专家来实现子线性参数增长，从而在保持高性能的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 持续更新基于模型的索引在生成检索中面临挑战，因为完全重新训练计算成本高且在资源限制下不切实际。

Method: MixLoRA-DSI框架结合了可扩展的低秩适应专家混合和逐层分布外（OOD）驱动的扩展策略。

Result: MixLoRA-DSI在NQ320k和MS MARCO Passage数据集上的实验结果表明其性能优于全模型更新基线，同时具有更小的参数开销和更低的训练成本。

Conclusion: MixLoRA-DSI在NQ320k和MS MARCO Passage数据集上表现出色，优于全模型更新基线，同时具有最小的参数开销和显著更低的训练成本。

Abstract: Continually updating model-based indexes in generative retrieval with new
documents remains challenging, as full retraining is computationally expensive
and impractical under resource constraints. We propose MixLoRA-DSI, a novel
framework that combines an expandable mixture of Low-Rank Adaptation experts
with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead
of allocating new experts for each new corpus, our proposed expansion strategy
enables sublinear parameter growth by selectively introducing new experts only
when significant number of OOD documents are detected. Experiments on NQ320k
and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update
baselines, with minimal parameter overhead and substantially lower training
costs.

</details>


### [77] [PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization](https://arxiv.org/abs/2507.10057)
*Sangwoo Park,Jinheon Baek,Soyeong Jeong,Sung Ju Hwang*

Main category: cs.IR

TL;DR: PRISM is a novel document-to-document retrieval method that improves performance by using multiple, fine-grained representations for both the query and candidate papers.


<details>
  <summary>Details</summary>
Motivation: Previous approaches to scientific paper retrieval have focused on abstracts, which provide only sparse and high-level summaries. To address this, PRISM introduces multiple, fine-grained representations for both the query and candidate papers.

Method: PRISM is a novel document-to-document retrieval method that introduces multiple, fine-grained representations for both the query and candidate papers. Each query paper is decomposed into multiple aspect-specific views and individually embedded, which are then matched against candidate papers similarity segmented to consider their multifaceted dimensions.

Result: PRISM improves performance by an average of 4.3% over existing retrieval baselines.

Conclusion: PRISM improves performance by an average of 4.3% over existing retrieval baselines.

Abstract: Scientific paper retrieval, particularly framed as document-to-document
retrieval, aims to identify relevant papers in response to a long-form query
paper, rather than a short query string. Previous approaches to this task have
focused on abstracts, embedding them into dense vectors as surrogates for full
documents and calculating similarity across them, although abstracts provide
only sparse and high-level summaries. To address this, we propose PRISM, a
novel document-to-document retrieval method that introduces multiple,
fine-grained representations for both the query and candidate papers. In
particular, each query paper is decomposed into multiple aspect-specific views
and individually embedded, which are then matched against candidate papers
similarity segmented to consider their multifaceted dimensions. Moreover, we
present SciFullBench, a novel benchmark in which the complete and segmented
context of full papers for both queries and candidates is available. Then,
experimental results show that PRISM improves performance by an average of 4.3%
over existing retrieval baselines.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [78] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: 本文提出了一种自动化、领域特定的R代码分割方法，使用大型和小型语言模型（LLMs/SLMs）。实验结果表明，基于上下文的逐行分析优于基于范围的分割，并且较小的语言模型如CodeBERT和CodeT5+的编码器版本在性能上优于其LLM counterparts。


<details>
  <summary>Details</summary>
Motivation: 由于手动和语法分析方法在大型代码库中变得不切实际，特别是对于低资源语言如R及其研究领域（例如社会科学、心理学），需要一种自动化的方法来进行代码分割。

Method: 本文提出了两种新的方法：基于上下文的逐行分析和基于范围的段确定。同时，还使用了LLMs和微调的SLMs进行实验，并在计算机科学领域的Python代码上进行了测试。

Result: 实验结果表明，基于上下文的逐行分析优于基于范围的分割。较小的语言模型如CodeBERT和CodeT5+的编码器版本在性能上优于其LLM counterparts。这些表现最好的模型在预训练期间没有看到R代码，仅在4,130行手动标注的代码上进行了微调。

Conclusion: 本文提出了一种自动化、领域特定的R代码分割方法，使用大型和小型语言模型（LLMs/SLMs）。实验结果表明，基于上下文的逐行分析优于基于范围的分割，并且较小的语言模型如CodeBERT和CodeT5+的编码器版本在性能上优于其LLM counterparts。

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [79] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: 本文介绍了StateGen，一个自动化框架，用于生成涉及顺序API交互的多样化编码任务。StateGen结合了基于状态机的API约束求解和验证、基于能量的采样以及控制流注入，以生成可执行程序。这些程序通过两个LLM代理的合作翻译成类似人类的自然语言任务描述。利用StateGen，我们构建了StateEval基准，涵盖三个代表性场景的120个验证测试用例。实验结果表明，StateGen可以有效地生成具有挑战性和现实意义的API导向任务。


<details>
  <summary>Details</summary>
Motivation: Testing, evaluation, and analysis of LLM tool use remain in their early stages. Most existing benchmarks rely on manually collected test cases, many of which cannot be automatically checked for semantic correctness and instead depend on static methods such as string matching. Additionally, these benchmarks often overlook the complex interactions that occur between sequential API calls, which are common in real-world applications.

Method: StateGen combines state-machine-based API constraint solving and validation, energy-based sampling, and control-flow injection to generate executable programs. These programs are then translated into human-like natural language task descriptions through a collaboration of two LLM agents.

Result: Utilizing StateGen, we construct StateEval, a benchmark encompassing 120 verified test cases spanning across three representative scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental results confirm that StateGen can effectively generate challenging and realistic API-oriented tasks.

Conclusion: StateGen can effectively generate challenging and realistic API-oriented tasks, highlighting areas for improvement in current LLMs incorporating APIs.

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [80] [ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching](https://arxiv.org/abs/2507.09318)
*Han Zhu,Wei Kang,Liyong Guo,Zengwei Yao,Fangjun Kuang,Weiji Zhuang,Zhaoqing Li,Zhifeng Han,Dong Zhang,Xin Zhang,Xingchen Song,Long Lin,Daniel Povey*

Main category: eess.AS

TL;DR: 本文介绍了一种新的非自回归口语对话生成模型ZipVoice-Dialog，并提供了一个大规模的数据集OpenDialog来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有口语对话生成模型由于是自回归的，存在推理缓慢和不稳定的问题。为了克服这些限制，我们需要一种更高效的模型。

Method: 引入了ZipVoice-Dialog，这是一个基于流匹配的非自回归零样本口语对话生成模型。关键设计包括：1）说话人轮换嵌入以实现精确的说话人轮换；2）课程学习策略以实现稳定的语音文本对齐；3）专门的策略以实现立体对话生成。此外，我们还整理了OpenDialog数据集。

Result: 实验结果表明，ZipVoice-Dialog在可懂性、说话人轮换准确性、说话人相似性和推理速度方面表现优于其他模型。

Conclusion: ZipVoice-Dialog在可懂性、说话人轮换准确性、说话人相似性和推理速度方面表现出色。我们的代码、模型检查点、演示样本和OpenDialog数据集都可以在https://github.com/k2-fsa/ZipVoice上公开获取。

Abstract: Generating spoken dialogue is more challenging than monologue text-to-speech
(TTS) due to the need for realistic turn-taking and distinct speaker timbres.
Existing spoken dialogue generation models, being auto-regressive, suffer from
slow and unstable inference. To overcome these limitations, we introduce
ZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation
model built upon flow matching. Key designs include: 1) speaker-turn embeddings
for precise speaker turn-taking; 2) a curriculum learning strategy for stable
speech-text alignment; 3) specialized strategies to enable stereo dialogue
generation. Additionally, recognizing the lack of open-source large-scale
spoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue
dataset from in-the-wild speech data. Furthermore, we established a benchmark
to comprehensively evaluate various models. Experimental results demonstrate
that ZipVoice-Dialog achieves superior performance in intelligibility, speaker
turn-taking accuracy, speaker similarity, and inference speed. Our codes, model
checkpoints, demo samples, and the OpenDialog dataset are all publicly
available at https://github.com/k2-fsa/ZipVoice.

</details>


### [81] [Natural Language-based Assessment of L2 Oral Proficiency using LLMs](https://arxiv.org/abs/2507.10200)
*Stefano Bannò,Rao Ma,Mengjie Qian,Siyuan Tang,Kate Knill,Mark Gales*

Main category: eess.AS

TL;DR: This paper investigates the use of natural language-based assessment (NLA) with an open-source LLM to evaluate responses in a zero-shot setting, showing that NLA is effective, interpretable, and generalizable.


<details>
  <summary>Details</summary>
Motivation: The motivation is to determine whether large language models (LLMs) can interpret and apply natural language instructions (can-do descriptors) in ways comparable to human assessment.

Method: The paper explores the use of natural language-based assessment (NLA) with an open-source LLM, Qwen 2.5 72B, to assess responses from the S&I Corpus in a zero-shot setting.

Result: The approach achieves competitive performance, surpassing a BERT-based model trained specifically for this purpose, though it does not outperform state-of-the-art speech LLMs fine-tuned for the task.

Conclusion: NLA proves particularly effective in mismatched task settings, is generalisable to other data types and languages, and offers greater interpretability, as it is grounded in clearly explainable, widely applicable language descriptors.

Abstract: Natural language-based assessment (NLA) is an approach to second language
assessment that uses instructions - expressed in the form of can-do descriptors
- originally intended for human examiners, aiming to determine whether large
language models (LLMs) can interpret and apply them in ways comparable to human
assessment. In this work, we explore the use of such descriptors with an
open-source LLM, Qwen 2.5 72B, to assess responses from the publicly available
S&I Corpus in a zero-shot setting. Our results show that this approach -
relying solely on textual information - achieves competitive performance: while
it does not outperform state-of-the-art speech LLMs fine-tuned for the task, it
surpasses a BERT-based model trained specifically for this purpose. NLA proves
particularly effective in mismatched task settings, is generalisable to other
data types and languages, and offers greater interpretability, as it is
grounded in clearly explainable, widely applicable language descriptors.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [82] [TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit](https://arxiv.org/abs/2507.09788)
*Paulo Salem,Robert Sim,Christopher Olsen,Prerit Saxena,Rafael Barcelos,Yi Ding*

Main category: cs.MA

TL;DR: TinyTroupe 是一个用于模拟的工具包，能够实现详细的人物设定和程序控制，为行为问题提供有效的解决方案，并提供了定量和定性评估。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统（MAS）库和工具缺乏细粒度的人物设定、人口抽样设施、实验支持和集成验证等关键功能，限制了它们在行为研究、社会模拟等应用中的实用性。

Method: TinyTroupe 是一个模拟工具包，允许通过多个 LLM 驱动的机制进行详细的人物定义和程序控制，从而解决个体或群体层面的行为问题。

Result: TinyTroupe 提供了详细的人员定义（如国籍、年龄、职业、性格、信仰、行为）和程序控制，同时提供了定量和定性评估，展示了其可能性、局限性和权衡。

Conclusion: TinyTroupe 是一个模拟工具包，能够实现详细的人物设定和程序控制，为行为问题提供有效的解决方案。虽然它是作为特定的 Python 实现，但其概念贡献可以部分或全部应用于其他上下文中。

Abstract: Recent advances in Large Language Models (LLM) have led to a new class of
autonomous agents, renewing and expanding interest in the area. LLM-powered
Multiagent Systems (MAS) have thus emerged, both for assistive and simulation
purposes, yet tools for realistic human behavior simulation -- with its
distinctive challenges and opportunities -- remain underdeveloped. Existing MAS
libraries and tools lack fine-grained persona specifications, population
sampling facilities, experimentation support, and integrated validation, among
other key capabilities, limiting their utility for behavioral studies, social
simulation, and related applications. To address these deficiencies, in this
work we introduce TinyTroupe, a simulation toolkit enabling detailed persona
definitions (e.g., nationality, age, occupation, personality, beliefs,
behaviors) and programmatic control via numerous LLM-driven mechanisms. This
allows for the concise formulation of behavioral problems of practical
interest, either at the individual or group level, and provides effective means
for their solution. TinyTroupe's components are presented using representative
working examples, such as brainstorming and market research sessions, thereby
simultaneously clarifying their purpose and demonstrating their usefulness.
Quantitative and qualitative evaluations of selected aspects are also provided,
highlighting possibilities, limitations, and trade-offs. The approach, though
realized as a specific Python implementation, is meant as a novel conceptual
contribution, which can be partially or fully incorporated in other contexts.
The library is available as open source at
https://github.com/microsoft/tinytroupe.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [83] [DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA](https://arxiv.org/abs/2507.09176)
*Han Ye,Yuqiang Jin,Jinyuan Liu,Tao Li,Wen-An Zhang,Minglei Fu*

Main category: cs.RO

TL;DR: 本文提出了一种无需依赖重叠视场或精确初始参数估计的多LiDAR外参标定框架，通过整合LiDAR束平差优化与鲁棒迭代精炼，实现了高精度和鲁棒性的标定结果。


<details>
  <summary>Details</summary>
Motivation: 准确的多LiDAR外参标定对于提升三维地图重建系统的性能至关重要。传统方法需要手动标注或特定参考模式，而本文旨在提供一种更高效、无需人工干预的标定方法。

Method: 本文提出了一种无需依赖重叠视场或精确初始参数估计的目标无关外参标定框架，通过整合LiDAR束平差（LBA）优化与鲁棒迭代精炼，构建了准确的参考点云地图，并将外参标定作为联合LBA优化问题进行处理。

Result: 在CARLA仿真环境和真实场景中的大量评估表明，本文方法在准确性和鲁棒性方面优于最先进的标定技术。对于非重叠传感器配置，平均平移误差为5毫米，旋转误差为0.2度，初始误差容忍度高达0.4米/30度。

Conclusion: 本文提出的方法在非重叠传感器配置下实现了高精度和鲁棒性的外参标定，且不需要专门的基础设施或手动参数调整。

Abstract: Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [84] [RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2507.08862)
*Tianzhe Zhao,Jiaoyan Chen,Yanchi Ru,Haiping Zhu,Nan Hu,Jun Liu,Qika Lin*

Main category: cs.CR

TL;DR: 本文首次系统研究了KG-RAG方法在数据中毒攻击下的安全性问题，提出了有效的攻击策略，并验证了其在实际场景中的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG系统被广泛应用于各种应用中，但现有研究主要关注使用非结构化文本数据源的RAG系统，而KG-RAG的安全风险尚未得到充分探索。

Method: 本文引入了一个实际且隐蔽的攻击设置，提出了一种攻击策略，通过识别对抗性目标答案并插入扰动三元组来完成误导推理链。

Result: 通过在两个基准和四种最近的KG-RAG方法上的实验，本文的攻击策略展示了强大的有效性，即使在最小的KG扰动下也能显著降低KG-RAG的性能。

Conclusion: 本文对KG-RAG方法的安全性问题进行了首次系统研究，揭示了其在数据中毒攻击下的脆弱性，并展示了攻击策略的有效性。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving external data to mitigate hallucinations and outdated knowledge
issues. Benefiting from the strong ability in facilitating diverse data sources
and supporting faithful reasoning, knowledge graphs (KGs) have been
increasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)
methods. Though RAG systems are widely applied in various applications, recent
studies have also revealed its vulnerabilities to data poisoning attacks, where
malicious information injected into external knowledge sources can mislead the
system into producing incorrect or harmful responses. However, these studies
focus exclusively on RAG systems using unstructured textual data sources,
leaving the security risks of KG-RAG largely unexplored, despite the fact that
KGs present unique vulnerabilities due to their structured and editable nature.
In this work, we conduct the first systematic investigation of the security
issue of KG-RAG methods through data poisoning attacks. To this end, we
introduce a practical, stealthy attack setting that aligns with real-world
implementation. We propose an attack strategy that first identifies adversarial
target answers and then inserts perturbation triples to complete misleading
inference chains in the KG, increasing the likelihood that KG-RAG methods
retrieve and rely on these perturbations during generation. Through extensive
experiments on two benchmarks and four recent KG-RAG methods, our attack
strategy demonstrates strong effectiveness in degrading KG-RAG performance,
even with minimal KG perturbations. In-depth analyses are also conducted to
understand the safety threats within the internal stages of KG-RAG systems and
to explore the robustness of LLMs against adversarial knowledge.

</details>


### [85] [EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions](https://arxiv.org/abs/2507.09762)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Jaafar Chbili*

Main category: cs.CR

TL;DR: 本文提出了一种无监督框架，用于从黑客论坛中自动检测、聚类和优先排序安全事件，通过基于Transformer的嵌入和对比学习实现，有效减少噪声并揭示高优先级威胁。


<details>
  <summary>Details</summary>
Motivation: 黑客论坛为新兴网络安全威胁提供了关键的早期预警信号，但从中提取可操作的情报仍然面临重大挑战。

Method: 本文提出了一种无监督框架，该框架自动检测、聚类和优先排序跨黑客论坛帖子讨论的安全事件。方法利用基于Transformer的嵌入，并通过对比学习进行微调，将相关讨论分组为不同的安全事件集群，识别如零日披露或恶意软件发布等事件，而无需依赖预定义关键词。框架包含一种每日排名机制，使用反映及时性、来源可信度、信息完整性和相关性的可量化指标来优先排序已识别事件。

Result: 在真实世界的黑客论坛数据上的实验评估表明，我们的方法有效减少了噪声并揭示了高优先级威胁，使安全分析师能够采取主动响应。

Conclusion: 通过将分散的黑客论坛讨论转化为结构化的、可操作的情报，本研究解决了自动化威胁检测和分析中的基本挑战。

Abstract: Hacker forums provide critical early warning signals for emerging
cybersecurity threats, but extracting actionable intelligence from their
unstructured and noisy content remains a significant challenge. This paper
presents an unsupervised framework that automatically detects, clusters, and
prioritizes security events discussed across hacker forum posts. Our approach
leverages Transformer-based embeddings fine-tuned with contrastive learning to
group related discussions into distinct security event clusters, identifying
incidents like zero-day disclosures or malware releases without relying on
predefined keywords. The framework incorporates a daily ranking mechanism that
prioritizes identified events using quantifiable metrics reflecting timeliness,
source credibility, information completeness, and relevance. Experimental
evaluation on real-world hacker forum data demonstrates that our method
effectively reduces noise and surfaces high-priority threats, enabling security
analysts to mount proactive responses. By transforming disparate hacker forum
discussions into structured, actionable intelligence, our work addresses
fundamental challenges in automated threat detection and analysis.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [86] [AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data](https://arxiv.org/abs/2507.09100)
*Mohammad Abolnejadian,Shakiba Amirshahi,Matthew Brehmer,Anamaria Crisan*

Main category: cs.HC

TL;DR: 本文提出了一种基于对话的用户界面，利用过去数据为实时决策提供见解，并通过模拟研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在决策对话中，专家需要在复杂的抉择中做出即时决策，而实时性使得无法回顾和利用相关信息，因此提出了利用过去数据进行实时决策的想法。

Method: 本文实现了一个对话用户界面，利用基于检索的大型语言模型（LLM）代理来持续监听对话，识别患者问题和医生建议的解决方案，并从嵌入的数据集中检索相关数据，生成简洁的见解。

Result: 通过将加拿大健康数据集嵌入向量数据库并使用示例医生-患者对话进行模拟研究，验证了系统的有效性。

Conclusion: 本文展示了系统在模拟研究中的有效性，但也指出了存在的挑战，并为未来的工作指明了方向。

Abstract: In decision-making conversations, experts must navigate complex choices and
make on-the-spot decisions while engaged in conversation. Although extensive
historical data often exists, the real-time nature of these scenarios makes it
infeasible for decision-makers to review and leverage relevant information.
This raises an interesting question: What if experts could utilize relevant
past data in real-time decision-making through insights derived from past data?
To explore this, we implemented a conversational user interface, taking
doctor-patient interactions as an example use case. Our system continuously
listens to the conversation, identifies patient problems and doctor-suggested
solutions, and retrieves related data from an embedded dataset, generating
concise insights using a pipeline built around a retrieval-based Large Language
Model (LLM) agent. We evaluated the prototype by embedding Health Canada
datasets into a vector database and conducting simulated studies using sample
doctor-patient dialogues, showing effectiveness but also challenges, setting
directions for the next steps of our work.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [87] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: 本文提出了一种名为Prompt4Trust的强化学习框架，用于提高多模态大语言模型在医疗应用中的置信度校准和可靠性，同时实现了最先进的医学视觉问答性能，并展示了良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于MLLMs在安全关键场景中的部署受到两个主要限制：对提示设计的敏感性和在高置信度下生成错误响应的倾向，因此需要一种方法来提高其置信度校准和可靠性。

Method: 本文提出了Prompt4Trust，这是一个针对多模态大语言模型（MLLMs）的置信度校准的强化学习（RL）框架。通过训练一个轻量级的LLM生成上下文感知的辅助提示，引导下游任务MLLM生成更准确反映预测准确性的响应。

Result: 本文提出的Prompt4Trust方法不仅提高了任务准确性，在PMC-VQA基准测试中达到了最先进的医学视觉问答（VQA）性能，而且在实验中展示了在较小的下游任务MLLM上训练的框架能够有效地推广到更大的MLLMs，表明了可扩展的校准潜力。

Conclusion: 本文展示了自动化且与人类对齐的提示工程在提高安全关键场景中多模态大语言模型（MLLMs）的可信度方面的潜力。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [88] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR is a novel autoregressive framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. It combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules.


<details>
  <summary>Details</summary>
Motivation: Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation.

Method: MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability.

Result: Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods.

Conclusion: MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods.

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [89] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频推理范式ViTCoT，并构建了相应的基准ViTIB，实验结果表明该方法在视频理解任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要依赖文本信息进行推理，忽略了视频中的视觉模态，而人类在推理时会自然地重新审视视觉内容。

Method: 引入了一种新的视频推理范式：Video-Text Interleaved CoT (ViTCoT)，并构建了Video-Text Interleaved Benchmark (ViTIB)。

Result: 实验表明，ViTCoT相比传统的纯文本CoT范式显著提升了性能，并有效激活了MLLMs中的更多神经元值。

Conclusion: ViTCoT显著提升了视频理解性能，并有效激活了MLLMs中的更多神经元值。

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [90] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: 该研究评估了CLIP的两个变体（ResNet和Vision Transformer）在bouba-kiki效应中的表现，发现它们并未一致表现出这种效应，且与人类的表现存在显著差距，从而揭示了VLMs在跨模态概念理解和内部表示方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 最近的多模态模型进展引发了关于视觉和语言模型（VLMs）是否以反映人类认知的方式整合跨模态信息的问题。一个经典的案例是bouba-kiki效应，人类会将伪词如“bouba”与圆形形状联系起来，将“kiki”与锯齿状的形状联系起来。由于先前研究中对此效应在VLMs中的证据存在矛盾，我们提出了一个全面的重新评估，重点关注CLIP的两个变体，ResNet和Vision Transformer（ViT），因为它们在许多最先进的VLMs中起着核心作用。

Method: 我们应用了两种互补的方法：一种是基于提示的评估，使用概率作为模型偏好，另一种是使用Grad-CAM作为一种新颖的方式，在形状-单词匹配任务中解释视觉注意力。

Result: 我们的发现表明，这些模型并不一致地表现出bouba-kiki效应。虽然ResNet对圆形形状有偏好，但两个模型的整体表现缺乏预期的关联。此外，与相同任务上先前的人类数据直接比较显示，模型的响应明显低于人类认知所特有的稳健、模态整合行为。

Conclusion: 这些结果有助于关于VLMs在多模态概念理解上的程度的持续争论，突显了它们内部表示与人类直觉之间的局限性。

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [91] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文介绍了一个专门用于面部图像理解的多模态大语言模型FaceLLM，并提出了一个用于构建训练数据的新弱监督管道。实验结果表明，FaceLLM在面部相关任务中表现优异，具有广阔的应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM主要在通用数据集上进行训练，限制了它们在特定领域视觉线索（如面部图像）上的推理能力。由于缺乏大规模标注的面部图像-文本数据集，需要详细理解面部结构、表情、情感和人口统计特征的任务仍未被MLLM充分探索。

Method: 本文提出了一种新的弱监督管道，使用带有属性感知提示的ChatGPT基于FairFace数据集中的图像生成高质量的问题-答案对，从而构建训练数据。

Result: 实验表明，FaceLLM在各种以面部为中心的任务中提高了MLLM的性能，并达到了最先进的水平。

Conclusion: 本文展示了FaceLLM在面部相关任务上的性能提升，并强调了通过语言模型进行合成监督构建领域专业化MLLM的潜力，同时为可信、以人为本的多模态AI系统树立了先例。

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [92] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: 该研究提出了一种使用深度卷积神经网络来识别手写Devanagari字符的方法，并取得了较高的准确率。


<details>
  <summary>Details</summary>
Motivation: 由于手写字符识别在技术搜索引擎、社交媒体、推荐系统等方面有潜在的应用，因此引起了研究人员的关注。然而，Devanagari脚本缺乏适当的数字化工具，因此需要一种自动化的手段来提取手写Devanagari字符。

Method: 该研究采用了一种方法，利用两个深度卷积神经网络层来识别手写Devanagari字符，并配置了一个卷积神经网络以有效进行Devanagari手写文本识别（DHTR）。

Result: 该方法在测试中达到了96.36%的准确率，在训练中达到了99.55%的准确率，表现出良好的识别效果。

Conclusion: 该研究通过使用两个深度卷积神经网络层，成功实现了对手写Devanagari字符的识别，并获得了较高的准确率。

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [93] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 本文介绍了一种新的大规模语料库CrisisLandMark，并提出了CLOSP和GeoCLOSP框架，以利用多种传感器数据和地理上下文提升遥感图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像检索系统主要限于RGB数据，未能利用其他传感器（如合成孔径雷达SAR或光学多光谱数据）的独特物理信息。因此，需要一种新的方法来桥接这一差距。

Method: 本文介绍了CLOSP（对比语言光学SAR预训练）框架，该框架使用文本作为桥梁，将未配对的光学和SAR图像对齐到统一的嵌入空间中。此外，还提出了GeoCLOSP，它将地理坐标集成到框架中，实现了通用性和特异性的权衡。

Result: 实验表明，CLOSP在检索nDGC方面达到了新的最先进的水平，比现有模型提高了54%。此外，统一的训练策略通过间接交互从光学领域转移丰富的语义知识，克服了解释SAR图像的固有困难。GeoCLOSP在检索与位置相关的危机事件和稀有地理特征方面表现得更加专业。

Conclusion: 本文强调了整合多种传感器数据和地理上下文对于释放遥感档案全部潜力的重要性。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [94] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: The paper introduces EmRACE-3K, a dataset for evaluating embodied reasoning capabilities of VLMs, and demonstrates its effectiveness in improving performance through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of current VLMs in embodied settings, where online interaction and active scene understanding are required. It seeks to provide a benchmark for evaluating embodied reasoning capabilities of VLMs.

Method: The paper introduces EmRACE-3K, a dataset of language-guided tasks in diverse environments, and establishes a benchmark to evaluate embodied reasoning capabilities of VLMs. It also presents a method of fine-tuning Qwen2.5-VL-7B using supervised learning followed by reinforcement learning.

Result: All models achieve success rates below 20% in zero-shot settings, highlighting the challenge posed by the benchmark. Fine-tuning Qwen2.5-VL-7B using supervised learning followed by reinforcement learning leads to substantial improvements across all three challenge categories.

Conclusion: EmRACE-3K is effective in enabling the development of embodied reasoning capabilities for VLMs, as demonstrated by the substantial improvements achieved through fine-tuning.

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [95] [Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers](https://arxiv.org/abs/2507.08882)
*Janaki Viswanathan,Alexander Blatt,Konrad Hagemann,Dietrich Klakow*

Main category: cs.SD

TL;DR: 本文评估了用于匿名ATCO语音的压力检测架构，并展示了隐私不是构建高性能深度学习模型的障碍。


<details>
  <summary>Details</summary>
Motivation: 检测压力是保持ATC高安全标准的关键点，但处理ATC语音数据受到隐私限制。

Method: 评估了不同用于匿名ATCO语音的压力检测架构。

Result: 最佳网络在匿名化的SUSAS数据集上达到93.6%的压力检测准确率，在匿名化的ATC模拟数据集上达到80.1%的准确率。

Conclusion: 隐私不是构建高性能深度学习模型的障碍。

Abstract: Air traffic control (ATC) demands multi-tasking under time pressure with high
consequences of an error. This can induce stress. Detecting stress is a key
point in maintaining the high safety standards of ATC. However, processing ATC
voice data entails privacy restrictions, e.g. the General Data Protection
Regulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with
these restrictions. In this paper, different architectures for stress detection
for anonymized ATCO speech are evaluated. Our best networks reach a stress
detection accuracy of 93.6% on an anonymized version of the Speech Under
Simulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our
anonymized ATC simulation dataset. This shows that privacy does not have to be
an impediment in building well-performing deep-learning-based models.

</details>


### [96] [Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning](https://arxiv.org/abs/2507.09310)
*Dominika Woszczyk,Manuel Sam Ribeiro,Thomas Merritt,Daniel Korzekwa*

Main category: cs.SD

TL;DR: 本文研究了伦巴第语风格转换，提出了一种隐式条件策略，在保持说话人相似性的同时，实现了与显式声学特征条件模型相当的可懂度提升。


<details>
  <summary>Details</summary>
Motivation: 文本到语音（TTS）系统在伦巴第语风格下可以提高语音的可懂性，但在缺乏目标说话人的目标语调数据的情况下，训练这些模型具有挑战性。语音转换（VC）被用作一种增强技术，以在没有目标说话人数据的情况下训练TTS系统。

Method: 本文比较了具有隐式和显式声学特征条件的语音转换模型，并提出了一种隐式条件策略。

Result: 本文提出的隐式条件策略在保持说话人相似性的同时，实现了与显式声学特征条件模型相当的可懂度提升。

Conclusion: 本文提出了一种隐式条件策略，在保持说话人相似性的同时，实现了与显式声学特征条件模型相当的可懂度提升。

Abstract: Text-to-Speech (TTS) systems in Lombard speaking style can improve the
overall intelligibility of speech, useful for hearing loss and noisy
conditions. However, training those models requires a large amount of data and
the Lombard effect is challenging to record due to speaker and noise
variability and tiring recording conditions. Voice conversion (VC) has been
shown to be a useful augmentation technique to train TTS systems in the absence
of recorded data from the target speaker in the target speaking style. In this
paper, we are concerned with Lombard speaking style transfer. Our goal is to
convert speaker identity while preserving the acoustic attributes that define
the Lombard speaking style. We compare voice conversion models with implicit
and explicit acoustic feature conditioning. We observe that our proposed
implicit conditioning strategy achieves an intelligibility gain comparable to
the model conditioned on explicit acoustic features, while also preserving
speaker similarity.

</details>

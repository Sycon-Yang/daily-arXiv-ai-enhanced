<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora](https://arxiv.org/abs/2508.13169)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Christian Heumann,Stephanie Thiemichen*

Main category: cs.CL

TL;DR: 本文提出了一个基于角色的管道，用于检测和减轻大规模文本语料库中的性别歧视，并在taz2024full语料库中进行了验证，展示了性别平衡的显著改善。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的输出往往反映出结构性性别不平衡，这源于它们的训练数据。因此，需要一种有效的方法来检测和减轻这些性别歧视。

Method: 本文提出了一种基于角色的管道，用于检测和减轻大规模文本语料库中的性别歧视。该方法包括新的基于角色的度量标准，以捕捉情感、句法代理和引用风格的不对称性，并支持诊断语料库分析和排除平衡。

Result: 本文应用该方法到taz2024full语料库，展示了在多个语言维度上的性别平衡显著改善。然而，更深层次的偏见仍然存在，特别是在情感和框架方面。

Conclusion: 本文提出了一个扩展的基于角色的管道，用于检测和减轻大规模文本语料库中的性别歧视。我们应用该方法到taz2024full语料库，并展示了在多个语言维度上的性别平衡显著改善。然而，更深层次的偏见仍然存在，特别是在情感和框架方面。我们发布了工具和报告以支持进一步的研究。

Abstract: Large language models are increasingly shaping digital communication, yet
their outputs often reflect structural gender imbalances that originate from
their training data. This paper presents an extended actor-level pipeline for
detecting and mitigating gender discrimination in large-scale text corpora.
Building on prior work in discourse-aware fairness analysis, we introduce new
actor-level metrics that capture asymmetries in sentiment, syntactic agency,
and quotation styles. The pipeline supports both diagnostic corpus analysis and
exclusion-based balancing, enabling the construction of fairer corpora. We
apply our approach to the taz2024full corpus of German newspaper articles from
1980 to 2024, demonstrating substantial improvements in gender balance across
multiple linguistic dimensions. Our results show that while surface-level
asymmetries can be mitigated through filtering and rebalancing, subtler forms
of bias persist, particularly in sentiment and framing. We release the tools
and reports to support further research in discourse-based fairness auditing
and equitable corpus construction.

</details>


### [2] [MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.13186)
*Shilong Li,Xingyuan Bu,Wenjie Wang,Jiaheng Liu,Jun Dong,Haoyang He,Hao Lu,Haozhe Zhang,Chenchen Jing,Zhen Li,Chuanhao Li,Jiayi Tian,Chenchen Zhang,Tianhao Peng,Yancheng He,Jihao Gu,Yuanxing Zhang,Jian Yang,Ge Zhang,Wenhao Huang,Wangchunshu Zhou,Zhaoxiang Zhang,Ruizhe Ding,Shilei Wen*

Main category: cs.CL

TL;DR: 本文介绍了MM-BrowseComp，一个用于评估AI代理在多模态环境下的能力的新基准。实验结果表明，当前模型在多模态任务中的表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要关注文本信息，忽略了多模态内容的普遍性。为了弥补这一差距，我们引入了MM-BrowseComp，以评估代理在多模态环境下的能力。

Method: 我们引入了MM-BrowseComp，这是一个包含224个精心设计的问题的新基准，专门用于评估代理的多模态检索和推理能力。这些问题通常在提示中包含图像，并且在搜索和推理过程中遇到的关键信息可能嵌入在网页上的图像或视频中。此外，我们为每个问题提供了经过验证的检查表，以实现对多模态依赖性和推理路径的细粒度分析。

Result: 即使是最先进的模型如OpenAI o3在使用工具的情况下，也只能达到29.02%的准确率，这表明当前模型在多模态能力和原生多模态推理方面存在不足。

Conclusion: 我们的综合评估显示，即使是像OpenAI o3这样的顶级模型，在MM-BrowseComp上的准确率也只有29.02%，这表明当前模型的多模态能力不足且缺乏原生的多模态推理能力。

Abstract: AI agents with advanced reasoning and tool use capabilities have demonstrated
impressive performance in web browsing for deep search. While existing
benchmarks such as BrowseComp evaluate these browsing abilities, they primarily
focus on textual information, overlooking the prevalence of multimodal content.
To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising
224 challenging, hand-crafted questions specifically designed to assess agents'
multimodal retrieval and reasoning capabilities. These questions often
incorporate images in prompts, and crucial information encountered during the
search and reasoning process may also be embedded within images or videos on
webpages. Consequently, methods relying solely on text prove insufficient for
our benchmark. Additionally, we provide a verified checklist for each question,
enabling fine-grained analysis of multimodal dependencies and reasoning paths.
Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp
reveals that even top models like OpenAI o3 with tools achieve only 29.02\%
accuracy, highlighting the suboptimal multimodal capabilities and lack of
native multimodal reasoning in current models.

</details>


### [3] [Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT](https://arxiv.org/abs/2508.13358)
*Zeeshan Ahmed,Frank Seide,Niko Moritz,Ju Lin,Ruiming Xie,Simone Merello,Zhe Liu,Christian Fuegen*

Main category: cs.CL

TL;DR: 本文提出了一种同时翻译方法，以平衡翻译质量和延迟，并研究了ASR和MT的高效集成，以实现更准确和高效的实时语音翻译。


<details>
  <summary>Details</summary>
Motivation: 虽然基于循环神经网络转换器（RNN-T）的最先进的ASR系统可以进行实时转录，但实现实时流式翻译仍然是一个重大挑战。

Method: 我们提出了一种同时翻译方法，有效平衡了翻译质量和延迟，并研究了ASR和MT的高效集成，利用ASR系统生成的语言线索来管理上下文，并使用高效束搜索剪枝技术如超时和强制最终化以保持系统的实时因子。

Result: 我们将方法应用于设备上的双语对话语音翻译，并证明我们的技术在延迟和质量方面优于基线。

Conclusion: 我们的技术缩小了与非流式翻译系统的质量差距，为更准确和高效的实时语音翻译铺平了道路。

Abstract: This paper tackles several challenges that arise when integrating Automatic
Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device
streaming speech translation. Although state-of-the-art ASR systems based on
Recurrent Neural Network Transducers (RNN-T) can perform real-time
transcription, achieving streaming translation in real-time remains a
significant challenge. To address this issue, we propose a simultaneous
translation approach that effectively balances translation quality and latency.
We also investigate efficient integration of ASR and MT, leveraging linguistic
cues generated by the ASR system to manage context and utilizing efficient
beam-search pruning techniques such as time-out and forced finalization to
maintain system's real-time factor. We apply our approach to an on-device
bilingual conversational speech translation and demonstrate that our techniques
outperform baselines in terms of latency and quality. Notably, our technique
narrows the quality gap with non-streaming translation systems, paving the way
for more accurate and efficient real-time speech translation.

</details>


### [4] [Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection](https://arxiv.org/abs/2508.13365)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Thomas Pickard,Maggie Mi,Aline Villavicencio*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中推理能力对习语检测的影响，并发现较大模型在理解习语方面表现更好，而较小的模型通过提供定义可以提高性能。


<details>
  <summary>Details</summary>
Motivation: 本文动机是探索大型语言模型中的推理能力如何影响习语检测性能，并研究模型大小的影响。

Method: 本文评估了DeepSeek-R1系列模型（从1.5B到70B参数）在四个习语检测数据集上的表现，并研究了推理能力对习语检测的影响。

Result: 研究发现，较小的模型通过生成思维链（CoT）推理可以提高性能，但不如基础模型；而较大的模型（14B、32B和70B）表现出适度的改进。此外，为较小的模型提供定义可以提高其性能。

Conclusion: 本文结论是，尽管大型模型在语义理解方面表现出色，但较小的模型通过提供定义可以提高习语检测性能。

Abstract: The recent trend towards utilisation of reasoning models has improved the
performance of Large Language Models (LLMs) across many tasks which involve
logical steps. One linguistic task that could benefit from this framing is
idiomaticity detection, as a potentially idiomatic expression must first be
understood before it can be disambiguated and serves as a basis for reasoning.
In this paper, we explore how reasoning capabilities in LLMs affect
idiomaticity detection performance and examine the effect of model size. We
evaluate, as open source representative models, the suite of DeepSeek-R1
distillation models ranging from 1.5B to 70B parameters across four
idiomaticity detection datasets. We find the effect of reasoning to be smaller
and more varied than expected. For smaller models, producing chain-of-thought
(CoT) reasoning increases performance from Math-tuned intermediate models, but
not to the levels of the base models, whereas larger models (14B, 32B, and 70B)
show modest improvements. Our in-depth analyses reveal that larger models
demonstrate good understanding of idiomaticity, successfully producing accurate
definitions of expressions, while smaller models often fail to output the
actual meaning. For this reason, we also experiment with providing definitions
in the prompts of smaller models, which we show can improve performance in some
cases.

</details>


### [5] [Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts](https://arxiv.org/abs/2508.13376)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过将LLaMA模型的上下文知识蒸馏到Whisper中来增强ASR。评估显示在多个任务上有显著改进。


<details>
  <summary>Details</summary>
Motivation: ASR系统在保持长音频转录的句法和语义准确性方面常常遇到困难，这影响了诸如命名实体识别(NER)、大写和标点等任务。

Method: 我们提出了一种新方法，通过将LLaMA模型的上下文知识蒸馏到Whisper中来增强ASR。我们的方法使用了两种策略：(1) 通过最优传输进行逐个标记蒸馏以对齐维度和序列长度，以及(2) 在Whisper和LLaMA的句子嵌入之间最小化表示损失，融合语法和语义。

Result: 在Spoken Wikipedia数据集上的评估表明，在词错误率(WER)、NER、大写和标点成功率方面有显著改进。

Conclusion: 通过引入新颖的NER指标和探索语义感知的ASR，我们的工作突显了将语言上下文整合到转录中的价值，为长篇语音的鲁棒、上下文感知ASR奠定了基础。

Abstract: ASR systems often struggle with maintaining syntactic and semantic accuracy
in long audio transcripts, impacting tasks like Named Entity Recognition (NER),
capitalization, and punctuation. We propose a novel approach that enhances ASR
by distilling contextual knowledge from LLaMA models into Whisper. Our method
uses two strategies: (1) token level distillation with optimal transport to
align dimensions and sequence lengths, and (2) representation loss minimization
between sentence embeddings of Whisper and LLaMA, blending syntax and
semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long
audios and rich entities demonstrate significant improvements in Word Error
Rate (WER), NER, capitalization, and punctuation success. By introducing novel
NER metrics and exploring semantics aware ASR, our work highlights the value of
integrating linguistic context into transcription, setting a foundation for
robust, context-aware ASR in longform speech.

</details>


### [6] [Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis](https://arxiv.org/abs/2508.13382)
*Ayoub Ben Chaliah,Hela Dellagi*

Main category: cs.CL

TL;DR: Datarus-R1-14B是一个经过微调的140亿参数语言模型，旨在作为虚拟数据分析师和研究生级问题解决者。通过分析轨迹包括推理步骤、代码执行、错误跟踪、自我修正和最终结论，以ReAct风格的笔记本格式进行训练。在多个标准公共基准测试中，Datarus-R1-14B的表现优于相似规模的模型，甚至达到了更大的推理模型QwQ-32B的水平，在AIME 2024/2025和LiveCodeBench上实现了高达30%的更高准确率，同时每解决方案的标记数减少了18-49%。


<details>
  <summary>Details</summary>
Motivation: Datarus-R1-14B旨在作为虚拟数据分析师和研究生级问题解决者，通过分析轨迹包括推理步骤、代码执行、错误跟踪、自我修正和最终结论，以ReAct风格的笔记本格式进行训练。

Method: Datarus-R1-14B是基于Qwen 2.5-14B-Instruct微调的140亿参数开源权重语言模型，旨在作为虚拟数据分析师和研究生级问题解决者。训练管道包括轨迹中心的合成数据生成器、融合轻量级基于标签的结构信号和分层奖励模型（HRM）的双奖励框架，以及内存优化的组相对策略优化（GRPO）实现。

Result: Datarus-R1-14B在具有挑战性的研究生级问题上表现出“顿悟时刻”模式：它会草拟假设，修订一两次并收敛，避免了当代系统常见的循环和令牌膨胀循环。在多个标准公共基准测试中，Datarus-R1-14B的表现优于相似规模的模型，甚至达到了更大的推理模型QwQ-32B的水平，在AIME 2024/2025和LiveCodeBench上实现了高达30%的更高准确率，同时每解决方案的标记数减少了18-49%。

Conclusion: Datarus-R1-14B在多个标准公共基准测试中表现优于相似规模的模型，甚至达到了更大的推理模型QwQ-32B的水平，在AIME 2024/2025和LiveCodeBench上实现了高达30%的更高准确率，同时每解决方案的标记数减少了18-49%。

Abstract: We present Datarus-R1-14B, a 14 B-parameter open-weights language model
fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and
graduate-level problem solver. Datarus is trained not on isolated
question-answer pairs but on full analytical trajectories including reasoning
steps, code execution, error traces, self-corrections, and final conclusions,
all captured in a ReAct-style notebook format spanning finance, medicine,
numerical analysis, and other quantitative domains. Our training pipeline
combines (i) a trajectory-centric synthetic data generator that yielded 144 000
tagged notebook episodes, (ii) a dual-reward framework blending a lightweight
tag-based structural signal with a Hierarchical Reward Model (HRM) that scores
both single-step soundness and end-to-end coherence, and (iii) a
memory-optimized implementation of Group Relative Policy Optimization (GRPO)
featuring KV-cache reuse, sequential generation, and reference-model sharding.
A cosine curriculum smoothly shifts emphasis from structural fidelity to
semantic depth, reducing the format collapse and verbosity that often plague
RL-aligned LLMs. A central design choice in Datarus is it dual reasoning
interface. In agentic mode the model produces ReAct-tagged steps that invoke
Python tools to execute real code; in reflection mode it outputs compact
Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On
demanding postgraduate-level problems, Datarus exhibits an "AHA-moment"
pattern: it sketches hypotheses, revises them once or twice, and converges
avoiding the circular, token-inflating loops common to contemporary systems.
Across standard public benchmarks Datarus surpasses similar size models and
even reaches the level of larger reasoning models such as QwQ-32B achieving up
to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting
18-49% fewer tokens per solution.

</details>


### [7] [ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models](https://arxiv.org/abs/2508.13426)
*Chunhua Liu,Kabir Manandhar Shrestha,Sukai Huang*

Main category: cs.CL

TL;DR: 我们通过在母语者的自由词联想规范上进行参数高效的微调，显著提升了大型语言模型在跨文化沟通中的表现，并展示了文化对齐的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于有限的文化知识和缺乏有效的学习方法探索，建模和对齐文化仍然是一项挑战。

Method: 我们引入了一种成本效益高、基于认知的解决方案：在母语者的自由词联想规范上进行参数高效的微调，这些规范编码了隐含的文化模式。

Result: SFT在英语中提升了保留的关联精度5%，在中文中提升了43-165%，提高了中位数具体性，并达到了人类水平的情感和唤醒度。这些词汇上的提升可以转移：在世界价值观调查问题上，微调后的模型将答案分布转向目标文化，在一个50项的高紧张子集上，Qwen的中文对齐响应翻倍，而Llama的美国偏见减少了三分之一。

Conclusion: 我们的工作突显了在人类认知基础上进行未来研究的潜力和必要性，以改进人工智能模型中的文化对齐。

Abstract: As large language models (LLMs) increasingly mediate cross-cultural
communication, their behavior still reflects the distributional bias of the
languages and viewpoints that are over-represented in their pre-training
corpora. Yet, it remains a challenge to model and align culture due to limited
cultural knowledge and a lack of exploration into effective learning
approaches. We introduce a cost-efficient, cognitively grounded remedy:
parameter-efficient fine-tuning on native speakers' free word-association
norms, which encode implicit cultural schemas. Leveraging English-US and
Mandarin associations from the Small-World-of-Words project, we adapt
Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based
preference optimization. SFT boosts held-out association Precision at 5 by
16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20,
and attains human-level valence and arousal. These lexical gains transfer: on
World-Values-Survey questions, fine-tuned models shift answer distributions
toward the target culture, and on a 50-item high-tension subset, Qwen's
Chinese-aligned responses double while Llama's US bias drops by one-third. Our
7-8B models rival or beat vanilla 70B baselines, showing that a few million
culture-grounded associations can instill value alignment without costly
retraining. Our work highlights both the promise and the need for future
research grounded in human cognition in improving cultural alignment in AI
models.

</details>


### [8] [ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs](https://arxiv.org/abs/2508.13514)
*Hongxin Ding,Baixiang Huang,Yue Fang,Weibin Liao,Xinke Jiang,Zheng Li,Junfeng Zhao,Yasha Wang*

Main category: cs.CL

TL;DR: 本文提出了ProMed，一种基于强化学习的框架，用于将医疗大型语言模型从反应范式转变为积极范式，使其能够提出有价值的临床问题。ProMed的核心是Shapley Information Gain（SIG）奖励，通过结合新获取的信息量和上下文重要性来量化每个问题的临床效用。实验表明，ProMed在多个基准上表现优异，并且在域外案例中具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医疗大型语言模型在静态医疗问答中表现出色，但主要在反应范式下运行，直接生成答案而没有寻求更多信息，这在交互环境中可能导致错误诊断。因此，需要一种方法让医疗LLMs具备在决策前提出有价值的临床问题的能力。

Method: ProMed是一种基于强化学习（RL）的框架，旨在将医疗大型语言模型（LLMs）从反应范式转变为积极范式。核心是Shapley Information Gain（SIG）奖励，通过结合新获取的信息量和上下文重要性（通过Shapley值估计）来量化每个问题的临床效用。ProMed采用两阶段训练流程：(1) SIG引导的模型初始化使用蒙特卡洛树搜索（MCTS）构建高奖励交互轨迹来监督模型；(2) SIG增强的策略优化，将SIG整合并增强RL，引入一种新的SIG引导的奖励分布机制，为信息性问题分配更高的奖励以进行针对性优化。

Result: ProMed在两个新整理的部分信息医学基准上进行了广泛的实验，结果表明它显著优于最先进的方法，平均提高了6.29%，并且比反应范式提高了54.45%。同时，ProMed在域外案例中也表现出稳健的泛化能力。

Conclusion: ProMed在两个新整理的部分信息医学基准上进行了广泛的实验，结果表明它显著优于最先进的方法，平均提高了6.29%，并且比反应范式提高了54.45%。同时，ProMed在域外案例中也表现出稳健的泛化能力。

Abstract: Interactive medical questioning is essential in real-world clinical
consultations, where physicians must actively gather information from patients.
While medical Large Language Models (LLMs) have shown impressive capabilities
in static medical question answering, they predominantly operate under a
reactive paradigm: generating answers directly without seeking additional
information, which risks incorrect diagnoses in such interactive settings. To
address this limitation, we propose ProMed, a reinforcement learning (RL)
framework that transitions medical LLMs toward a proactive paradigm, equipping
them with the ability to ask clinically valuable questions before
decision-making. At the core of ProMed is the Shapley Information Gain (SIG)
reward, which quantifies the clinical utility of each question by combining the
amount of newly acquired information with its contextual importance, estimated
via Shapley values. We integrate SIG into a two-stage training pipeline: (1)
SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to
construct high-reward interaction trajectories to supervise the model, and (2)
SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a
novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to
informative questions for targeted optimization. Extensive experiments on two
newly curated partial-information medical benchmarks demonstrate that ProMed
significantly outperforms state-of-the-art methods by an average of 6.29% and
delivers a 54.45% gain over the reactive paradigm, while also generalizing
robustly to out-of-domain cases.

</details>


### [9] [Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation](https://arxiv.org/abs/2508.13525)
*Hassan Barmandah*

Main category: cs.CL

TL;DR: 本文通过LoRA微调ALLaM-7B-Instruct-preview模型，展示了在沙特方言生成任务中，Dialect-Token方法在控制方言和提高文本保真度方面优于其他基线模型。


<details>
  <summary>Details</summary>
Motivation: 目前，阿拉伯语的大语言模型主要集中在现代标准阿拉伯语（MSA）上，对沙特方言如Najdi和Hijazi的支持有限。这种不足阻碍了它们捕捉真实方言变化的能力。因此，需要开发一种能够有效处理沙特方言的语言模型。

Method: 本文使用了一个私有收集的沙特方言指令数据集（Hijazi和Najdi；5,466个合成指令-响应对；50/50划分），并对ALLaM-7B-Instruct-preview模型进行了LoRA微调，以进行沙特方言生成。研究了两种变体：(i) Dialect-Token训练，即在指令前添加显式的方言标签；(ii) No-Token训练，即在格式化时省略标签。

Result: Dialect-Token模型在控制方言方面表现最佳，将沙特方言的比例从47.97%提高到84.21%，并减少了MSA泄漏从32.63%到6.21%；同时，保真度也有所提高（chrF++ +3.53，BERTScore +0.059）。两种LoRA变体在方言控制和保真度方面都优于其他强大的通用指令模型。

Conclusion: 本文通过LoRA微调ALLaM-7B-Instruct-preview模型，展示了在沙特方言生成任务中，Dialect-Token方法在控制方言和提高文本保真度方面优于其他基线模型。同时，作者没有释放数据集或模型权重，而是提供了代码和详细的数据表以支持独立验证。

Abstract: Large language models (LLMs) for Arabic are still dominated by Modern
Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi
and Hijazi. This underrepresentation hinders their ability to capture authentic
dialectal variation. Using a privately curated Saudi Dialect Instruction
dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50
split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model
developed in Saudi Arabia, for Saudi dialect generation. We investigate two
variants: (i) Dialect-Token training, which prepends an explicit dialect tag to
the instruction, and (ii) No-Token training, which omits the tag at formatting
time. Evaluation on a held-out test set combines an external dialect classifier
with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The
Dialect-Token model achieves the best control, raising the Saudi rate from
47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also
improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong
generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,
Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and
fidelity, while avoiding metadata-tag echoing that these baselines frequently
exhibit. We do not release the dataset or any model weights/adapters; instead,
we release training/evaluation/inference code and a detailed datasheet (schema
and aggregate statistics) to support independent verification.

</details>


### [10] [MATA (māta): Mindful Assessment of the Telugu Abilities of Large Language Models](https://arxiv.org/abs/2508.13526)
*Chalamalasetti Kranti,Sowmya Vajjala*

Main category: cs.CL

TL;DR: 本文介绍了MATA，一个用于评估大型语言模型在泰卢固语中能力的新数据集，包含729个精心策划的多项选择题和开放性问题。我们评估了11个开源和闭源LLMs，并展示了它们如何依赖于表面启发式方法。最后，我们比较了LLM作为评判者的评估与人类评估在开放性问题上的表现，并得出了其在低资源语言中可靠性的结论。


<details>
  <summary>Details</summary>
Motivation: 我们需要一种细粒度的评估方法来理解模型的局限性，并指导开发更具语言能力的LLMs，同时为Telugu NLP的未来研究奠定基础。

Method: 我们引入了MATA，这是一个新的评估数据集，用于评估大型语言模型（LLMs）在泰卢固语中的能力，包含729个精心策划的多项选择题和开放性问题，涵盖了多种语言维度。我们评估了11个开源和闭源LLMs在我们的数据集上的表现，并进行了细致的分析。此外，我们实证展示了LLMs如何依赖于表面启发式方法，如答案位置和干扰项模式来回答多项选择题。最后，我们还比较了LLM作为评判者的评估与人类评估在开放性问题上的表现，并得出了其在低资源语言中可靠性的结论。

Result: 我们评估了11个开源和闭源LLMs在我们的数据集上的表现，并进行了细致的分析。此外，我们实证展示了LLMs如何依赖于表面启发式方法，如答案位置和干扰项模式来回答多项选择题。最后，我们还比较了LLM作为评判者的评估与人类评估在开放性问题上的表现，并得出了其在低资源语言中可靠性的结论。

Conclusion: 我们认为这种细粒度评估对于理解模型限制至关重要，并可以指导开发更具语言能力的LLM，同时为Telugu NLP的未来研究奠定基础。

Abstract: In this paper, we introduce MATA, a novel evaluation dataset to assess the
ability of Large Language Models (LLMs) in Telugu language, comprising 729
carefully curated multiple-choice and open-ended questions that span diverse
linguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our
dataset and present a fine-grained analysis of their performance. Further, we
empirically show how LLMs rely on superficial heuristics such as answer
position and distractor patterns for multiple-choice questions. Finally, we
also compare LLM-as-a-judge evaluation with human evaluation for open-ended
questions and draw some conclusions on its reliability in a low-resource
language. We argue that such fine-grained evaluation is essential for
understanding model limitations and can inform the development of more
linguistically capable LLMs, while also serving as a foundation for future
research in Telugu NLP.

</details>


### [11] [Compressed Models are NOT Trust-equivalent to Their Large Counterparts](https://arxiv.org/abs/2508.13533)
*Rohit Raj Rai,Chirag Kothari,Siddhesh Shelke,Amit Awekar*

Main category: cs.CL

TL;DR: 本文研究了压缩模型与大型模型在信任等价性上的差异，发现即使准确率相同，压缩模型在可解释性和校准方面也存在显著差异，因此部署时需要更全面的评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注压缩对准确性和性能的影响，但性能一致并不保证信任等价性。

Method: 提出了一种二维框架来评估信任等价性，包括可解释性对齐和校准相似性。使用LIME和SHAP测试测量可解释性对齐，使用ECE、MCE、Brier Score和可靠性图评估校准相似性。

Result: 实验结果显示压缩模型与大型模型在可解释性对齐和校准相似性方面存在显著差异，即使准确率相近。

Conclusion: 压缩模型与大型模型在信任等价性上存在差异，部署时需要仔细评估，而不仅仅是关注性能一致性。

Abstract: Large Deep Learning models are often compressed before being deployed in a
resource-constrained environment. Can we trust the prediction of compressed
models just as we trust the prediction of the original large model? Existing
work has keenly studied the effect of compression on accuracy and related
performance measures. However, performance parity does not guarantee
trust-equivalence. We propose a two-dimensional framework for trust-equivalence
evaluation. First, interpretability alignment measures whether the models base
their predictions on the same input features. We use LIME and SHAP tests to
measure the interpretability alignment. Second, calibration similarity measures
whether the models exhibit comparable reliability in their predicted
probabilities. It is assessed via ECE, MCE, Brier Score, and reliability
diagrams. We conducted experiments using BERT-base as the large model and its
multiple compressed variants. We focused on two text classification tasks:
natural language inference and paraphrase identification. Our results reveal
low interpretability alignment and significant mismatch in calibration
similarity. It happens even when the accuracies are nearly identical between
models. These findings show that compressed models are not trust-equivalent to
their large counterparts. Deploying compressed models as a drop-in replacement
for large models requires careful assessment, going beyond performance parity.

</details>


### [12] [A Comparative Study of Decoding Strategies in Medical Text Generation](https://arxiv.org/abs/2508.13580)
*Oriana Presacan,Alireza Nik,Vajira Thambawita,Bogdan Ionescu,Michael Riegler*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) rely on various decoding strategies to generate
text, and these choices can significantly affect output quality. In healthcare,
where accuracy is critical, the impact of decoding strategies remains
underexplored. We investigate this effect in five open-ended medical tasks,
including translation, summarization, question answering, dialogue, and image
captioning, evaluating 11 decoding strategies with medically specialized and
general-purpose LLMs of different sizes. Our results show that deterministic
strategies generally outperform stochastic ones: beam search achieves the
highest scores, while {\eta} and top-k sampling perform worst. Slower decoding
methods tend to yield better quality. Larger models achieve higher scores
overall but have longer inference times and are no more robust to decoding.
Surprisingly, while medical LLMs outperform general ones in two of the five
tasks, statistical analysis shows no overall performance advantage and reveals
greater sensitivity to decoding choice. We further compare multiple evaluation
metrics and find that correlations vary by task, with MAUVE showing weak
agreement with BERTScore and ROUGE, as well as greater sensitivity to the
decoding strategy. These results highlight the need for careful selection of
decoding methods in medical applications, as their influence can sometimes
exceed that of model choice.

</details>


### [13] [Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM](https://arxiv.org/abs/2508.13603)
*Dariia Puhach,Amir H. Payberah,Éva Székely*

Main category: cs.CL

TL;DR: This study investigates gender bias in Speech-LLMs by analyzing speaker assignments in a TTS model called Bark. The results show that while there is no systematic bias, Bark demonstrates gender awareness and has some gender inclinations.


<details>
  <summary>Details</summary>
Motivation: The study aims to investigate whether the similarities between text-based Large Language Models (LLMs) and Speech-LLMs extend to gender bias.

Method: The study proposes a methodology leveraging speaker assignment as an analytic tool for bias investigation. Two datasets were constructed: (i) Professions, containing gender-stereotyped occupations, and (ii) Gender-Colored Words, featuring gendered connotations.

Result: Bark does not exhibit systematic bias, but it demonstrates gender awareness and has some gender inclinations.

Conclusion: Bark does not exhibit systematic bias, but it demonstrates gender awareness and has some gender inclinations.

Abstract: Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit
emergent abilities and context awareness. However, whether these similarities
extend to gender bias remains an open question. This study proposes a
methodology leveraging speaker assignment as an analytic tool for bias
investigation. Unlike text-based models, which encode gendered associations
implicitly, Speech-LLMs must produce a gendered voice, making speaker selection
an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing
its default speaker assignments for textual prompts. If Bark's speaker
selection systematically aligns with gendered associations, it may reveal
patterns in its training data or model design. To test this, we construct two
datasets: (i) Professions, containing gender-stereotyped occupations, and (ii)
Gender-Colored Words, featuring gendered connotations. While Bark does not
exhibit systematic bias, it demonstrates gender awareness and has some gender
inclinations.

</details>


### [14] [AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings](https://arxiv.org/abs/2508.13606)
*Haoxuan Li,Wei Song,Aofan Liu,Peiwu Qin*

Main category: cs.CL

TL;DR: 本文提出了一种名为AdaDocVQA的统一自适应框架，用于解决文档视觉问答中的挑战。该框架通过三个核心创新提高了性能，并在日语文档VQA中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 文档视觉问答（Document VQA）在处理长文档时面临显著挑战，特别是在低资源环境中，由于上下文限制和训练数据不足。

Method: AdaDocVQA，一个统一的自适应框架，通过三个核心创新来解决这些问题：混合文本检索架构、智能数据增强管道以及自适应集成推理。

Result: 在日语文档VQA基准测试中，AdaDocVQA在JDocQA上取得了83.04%的准确率（是/否问题），52.66%的准确率（事实性问题），以及44.12%的准确率（数值问题），并在LAVA数据集上达到了59%的准确率。消融研究确认了每个组件的有意义贡献。

Conclusion: 我们的框架在日语文档VQA中建立了新的最先进结果，并为其他低资源语言和专业领域提供了可扩展的基础。

Abstract: Document Visual Question Answering (Document VQA) faces significant
challenges when processing long documents in low-resource environments due to
context limitations and insufficient training data. This paper presents
AdaDocVQA, a unified adaptive framework addressing these challenges through
three core innovations: a hybrid text retrieval architecture for effective
document segmentation, an intelligent data augmentation pipeline that
automatically generates high-quality reasoning question-answer pairs with
multi-level verification, and adaptive ensemble inference with dynamic
configuration generation and early stopping mechanisms. Experiments on Japanese
document VQA benchmarks demonstrate substantial improvements with 83.04\%
accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on
numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation
studies confirm meaningful contributions from each component, and our framework
establishes new state-of-the-art results for Japanese document VQA while
providing a scalable foundation for other low-resource languages and
specialized domains. Our code available at:
https://github.com/Haoxuanli-Thu/AdaDocVQA.

</details>


### [15] [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650)
*Tomer Ashuach,Dana Arad,Aaron Mueller,Martin Tutek,Yonatan Belinkov*

Main category: cs.CL

TL;DR: CRISP是一种使用稀疏自编码器进行参数高效持久概念遗忘的方法，能够有效移除有害知识并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在现实世界应用的增加，需要选择性地删除不需要的知识，同时保持模型效用。现有的基于SAE的方法主要在推理时操作，无法在模型参数中创建持久的变化，容易被恶意行为者绕过或逆转。

Method: CRISP利用稀疏自编码器（SAEs）进行参数高效的持久概念遗忘，自动识别多层中的显著SAE特征并抑制其激活。

Result: 在两个大型语言模型上的实验表明，CRISP在WMDP基准的安全关键遗忘任务中优于先前方法，成功移除了有害知识，同时保持了通用和领域内的能力。

Conclusion: CRISP能够有效地移除有害知识，同时保持模型的通用性和领域内能力，并实现语义上连贯的目标概念与良性概念的分离。

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, the need to selectively remove unwanted knowledge while
preserving model utility has become paramount. Recent work has explored sparse
autoencoders (SAEs) to perform precise interventions on monosemantic features.
However, most SAE-based methods operate at inference time, which does not
create persistent changes in the model's parameters. Such interventions can be
bypassed or reversed by malicious actors with parameter access. We introduce
CRISP, a parameter-efficient method for persistent concept unlearning using
SAEs. CRISP automatically identifies salient SAE features across multiple
layers and suppresses their activations. We experiment with two LLMs and show
that our method outperforms prior approaches on safety-critical unlearning
tasks from the WMDP benchmark, successfully removing harmful knowledge while
preserving general and in-domain capabilities. Feature-level analysis reveals
that CRISP achieves semantically coherent separation between target and benign
concepts, allowing precise suppression of the target features.

</details>


### [16] [ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?](https://arxiv.org/abs/2508.13680)
*Vy Tuong Dang,An Vo,Quang Tau,Duc Dm,Daeyoung Kim*

Main category: cs.CL

TL;DR: 该研究评估了视觉语言模型在越南教育评估中的表现，发现其性能较低，仅能部分超越人类平均水平。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索视觉语言模型在低资源语言的真实多模态教育内容上的表现，特别是越南教育评估中的表现。

Method: 研究提出了ViExam基准，包含2,548个多模态问题，用于评估视觉语言模型在多模态越南考试中的能力。

Result: 最先进的视觉语言模型在越南教育评估中的平均准确率为57.74%，而开源模型的平均准确率为27.70%。跨语言提示未能提高性能，而人机协作可以部分提高模型性能。

Conclusion: 研究发现，最先进的视觉语言模型在越南教育评估中的表现仅为57.74%，而开源模型的平均准确率为27.70%。大多数模型的表现低于人类平均水平（66.54%），只有o3模型（74.07%）超过了人类平均水平，但仍远低于人类最佳表现（99.60%）。

Abstract: Vision language models (VLMs) demonstrate remarkable capabilities on English
multimodal tasks, but their performance on low-resource languages with
genuinely multimodal educational content remains largely unexplored. In this
work, we test how VLMs perform on Vietnamese educational assessments,
investigating whether VLMs trained predominantly on English data can handle
real-world cross-lingual multimodal reasoning. Our work presents the first
comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams
through proposing ViExam, a benchmark containing 2,548 multimodal questions. We
find that state-of-the-art VLMs achieve only 57.74% while open-source models
achieve 27.70% mean accuracy across 7 academic domains, including Mathematics,
Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs
underperform average human test-takers (66.54%), with only the thinking VLM o3
(74.07%) exceeding human average performance, yet still falling substantially
short of human best performance (99.60%). Cross-lingual prompting with English
instructions while maintaining Vietnamese content fails to improve performance,
decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop
collaboration can partially improve VLM performance by 5 percentage points.
Code and data are available at: https://vi-exam.github.io.

</details>


### [17] [Generics and Default Reasoning in Large Language Models](https://arxiv.org/abs/2508.13718)
*James Ravi Kirkpatrick,Rachel Katharine Sterken*

Main category: cs.CL

TL;DR: 该论文评估了28个大型语言模型在处理涉及通用概括的可废止推理模式方面的能力，发现虽然一些模型表现良好，但整体表现差异较大，且链式思维提示可能导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究通用概括在非单调逻辑中的重要性以及它们在默认推理、认知和概念习得中的作用。

Method: 评估28个大型语言模型（LLMs）对20种涉及通用概括的可废止推理模式的能力。

Result: 尽管一些前沿模型能够很好地处理许多默认推理问题，但模型和提示风格之间的表现差异很大。少样本提示对某些模型有适度的性能提升，但链式思维（CoT）提示通常会导致严重的性能下降。大多数模型要么难以区分可废止推理和演绎推理，要么将通用陈述误解为普遍陈述。

Conclusion: 这些发现突显了当前大型语言模型在默认推理方面的潜力和局限性。

Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to
reason with 20 defeasible reasoning patterns involving generic generalizations
(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.
Generics are of special interest to linguists, philosophers, logicians, and
cognitive scientists because of their complex exception-permitting behaviour
and their centrality to default reasoning, cognition, and concept acquisition.
We find that while several frontier models handle many default reasoning
problems well, performance varies widely across models and prompting styles.
Few-shot prompting modestly improves performance for some models, but
chain-of-thought (CoT) prompting often leads to serious performance degradation
(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy
in zero-shot condition, temperature 0). Most models either struggle to
distinguish between defeasible and deductive inference or misinterpret generics
as universal statements. These findings underscore both the promise and limits
of current LLMs for default reasoning.

</details>


### [18] [Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings](https://arxiv.org/abs/2508.13729)
*Hanna Herasimchyk,Alhassan Abdelhalim,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 本文分析了词嵌入中隐含知识的解释方法，发现预测准确性不能可靠地指示真正的语义表示。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习模型中隐含的知识对于提高AI系统的可解释性至关重要。

Method: 我们挑战了这一假设，通过展示预测准确性本身并不能可靠地指示基于特征的可解释性。

Result: 我们表明，这些方法可以成功预测甚至随机信息，结论是结果主要由算法上限决定，而不是词嵌入中的有意义语义表示。

Conclusion: 我们的分析表明，这些映射主要反映向量空间内的几何相似性，而不是指示语义属性的真实出现。

Abstract: Understanding what knowledge is implicitly encoded in deep learning models is
essential for improving the interpretability of AI systems. This paper examines
common methods to explain the knowledge encoded in word embeddings, which are
core elements of large language models (LLMs). These methods typically involve
mapping embeddings onto collections of human-interpretable semantic features,
known as feature norms. Prior work assumes that accurately predicting these
semantic features from the word embeddings implies that the embeddings contain
the corresponding knowledge. We challenge this assumption by demonstrating that
prediction accuracy alone does not reliably indicate genuine feature-based
interpretability.
  We show that these methods can successfully predict even random information,
concluding that the results are predominantly determined by an algorithmic
upper bound rather than meaningful semantic representation in the word
embeddings. Consequently, comparisons between datasets based solely on
prediction performance do not reliably indicate which dataset is better
captured by the word embeddings. Our analysis illustrates that such mappings
primarily reflect geometric similarity within vector spaces rather than
indicating the genuine emergence of semantic properties.

</details>


### [19] [EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13735)
*Yi Wang,Haoran Luo,Lu Meng*

Main category: cs.CL

TL;DR: 本文提出了EEG-MedRAG框架，用于高效检索和语义解释EEG数据，并引入了一个跨疾病、跨角色的临床问答基准，以评估模型的泛化能力和上下文理解。


<details>
  <summary>Details</summary>
Motivation: 随着脑电图（EEG）在神经科学和临床实践中的广泛应用，高效检索和语义解释大规模、多源、异构的EEG数据已成为一个紧迫的挑战。

Method: 提出了一种基于超图的检索增强生成框架EEG-MedRAG，将EEG领域知识、个别患者案例和大规模存储库统一到可遍历的n元关系超图中，实现了联合语义-时间检索和因果链诊断生成。

Result: 实验表明，EEG-MedRAG在答案准确性和检索方面显著优于TimeRAG和HyperGraphRAG。

Conclusion: EEG-MedRAG在答案准确性和检索方面显著优于TimeRAG和HyperGraphRAG，显示出其在实际临床决策支持中的强大潜力。

Abstract: With the widespread application of electroencephalography (EEG) in
neuroscience and clinical practice, efficiently retrieving and semantically
interpreting large-scale, multi-source, heterogeneous EEG data has become a
pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based
retrieval-augmented generation framework that unifies EEG domain knowledge,
individual patient cases, and a large-scale repository into a traversable n-ary
relational hypergraph, enabling joint semantic-temporal retrieval and
causal-chain diagnostic generation. Concurrently, we introduce the first
cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders
and five authentic clinical perspectives. This benchmark allows systematic
evaluation of disease-agnostic generalization and role-aware contextual
understanding. Experiments show that EEG-MedRAG significantly outperforms
TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its
strong potential for real-world clinical decision support. Our data and code
are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.

</details>


### [20] [Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA](https://arxiv.org/abs/2508.13743)
*Kaiwei Zhang,Qi Jia,Zijian Chen,Wei Sun,Xiangyang Zhu,Chunyi Li,Dandan Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在科学问答任务中的奉承行为，并提出了一种新的后训练方法Pressure-Tune来减少这种行为，从而提高模型的真实性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要事实严谨性的领域中越来越常用，但它们表现出一种令人担忧的行为：奉承，即无论正确与否都倾向于迎合用户观点。这种倾向由基于偏好的对齐技术强化，可能损害真实性。尽管重要，但在事实问答背景下这一现象仍缺乏研究。

Method: 本文引入了一个统一的评估框架来量化奉承性上下文对科学问答中模型行为的影响，并提出了Pressure-Tune方法，该方法通过在合成对抗性对话和链式思维推理上微调模型来减轻这一问题。

Result: 系统评估显示，开源和专有模型普遍存在奉承倾向，这更多是由对齐策略而非模型大小驱动的。实验表明，Pressure-Tune在具有挑战性的科学问答基准上显著提高了对奉承行为的抵抗力，同时不损害准确性或对有效反馈的响应能力。

Conclusion: 本文提出了一种轻量级的后训练方法Pressure-Tune，能够在不牺牲准确性和对有效反馈的响应能力的情况下显著增强模型对奉承行为的抵抗力，为实现更真实和有原则的模型行为提供了一条实用的路径。

Abstract: Large language models (LLMs), while increasingly used in domains requiring
factual rigor, often display a troubling behavior: sycophancy, the tendency to
align with user beliefs regardless of correctness. This tendency is reinforced
by preference-based alignment techniques that optimize for user satisfaction
but can undermine truthfulness. While relatively benign in casual dialogue,
sycophancy poses serious risks in high-stakes settings such as scientific
question answering (QA), where model outputs may shape collaborative reasoning,
decision-making, and knowledge formation. Despite its importance, this
phenomenon remains underexamined in factual QA contexts. We address this gap by
introducing a unified evaluation framework to quantify the impact of
sycophantic context on model behavior in scientific QA, measuring how much
user-imposed social pressure distorts model outputs. The framework incorporates
adversarial prompting setups and targeted metrics, such as misleading
resistance and sycophancy resistance, that capture a model's ability to
maintain factual consistency under misleading cues. Systematic evaluations
across open-source and proprietary models reveal pervasive sycophantic
tendencies, driven more by alignment strategy than by model size. To mitigate
this issue, we propose Pressure-Tune, a lightweight post-training method that
fine-tunes models on synthetic adversarial dialogues paired with
chain-of-thought rationales. These rationales reject user misinformation while
reinforcing factual commitments. Experiments on challenging scientific QA
benchmarks show that Pressure-Tune significantly enhances sycophancy resistance
without compromising accuracy or responsiveness to valid feedback, offering a
practical pathway toward more truthful and principled model behavior.

</details>


### [21] [MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment](https://arxiv.org/abs/2508.13768)
*Shengchao Liu,Xiaoming Liu,Chengzhengxu Li,Zhaohan Zhang,Guoxin Ma,Yu Lan,Shuai Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种名为MGT-Prism的机器生成文本检测方法，该方法通过分析文本在频域中的表示，设计了低频域过滤模块和动态频谱对齐策略，以提高检测器在不同领域之间的泛化能力，并在多个测试数据集上取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 当前的机器生成文本检测器在训练和测试在同一领域时表现良好，但在面对不同来源的数据时由于领域偏移而泛化能力差。因此，需要一种能够更好处理领域泛化的检测方法。

Method: MGT-Prism是一种从频域角度出发的机器生成文本检测方法，旨在提高领域泛化能力。它包括一个低频域过滤模块和一个动态频谱对齐策略，以提取任务特定且领域不变的特征。

Result: MGT-Prism在三个领域泛化场景下的11个测试数据集上表现出色，优于现有的最先进基线。

Conclusion: MGT-Prism在三个领域泛化场景下的11个测试数据集上，平均准确率和F1分数分别高出最先进的基线0.90%和0.92%。

Abstract: Large Language Models have shown growing ability to generate fluent and
coherent texts that are highly similar to the writing style of humans. Current
detectors for Machine-Generated Text (MGT) perform well when they are trained
and tested in the same domain but generalize poorly to unseen domains, due to
domain shift between data from different sources. In this work, we propose
MGT-Prism, an MGT detection method from the perspective of the frequency domain
for better domain generalization. Our key insight stems from analyzing text
representations in the frequency domain, where we observe consistent spectral
patterns across diverse domains, while significant discrepancies in magnitude
emerge between MGT and human-written texts (HWTs). The observation initiates
the design of a low frequency domain filtering module for filtering out the
document-level features that are sensitive to domain shift, and a dynamic
spectrum alignment strategy to extract the task-specific and domain-invariant
features for improving the detector's performance in domain generalization.
Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art
baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test
datasets across three domain-generalization scenarios.

</details>


### [22] [Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study](https://arxiv.org/abs/2508.13769)
*Hanna Woloszyn,Benjamin Gagl*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型生成的文本与儿童语言的相似性，并发现大型语言模型生成的文本在多个心理语言学属性上与儿童语言存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在教育中的作用日益增加，但很少有人关注大型语言模型生成的文本是否类似于儿童语言。

Method: 本研究通过比较大型语言模型生成的文本与德语儿童描述图片故事的文本，评估了大型语言模型如何复制儿童语言。我们使用相同的图片故事和两种提示类型（零样本和少量样本提示）生成了两个基于大型语言模型的语料库，并对心理语言学文本属性进行了比较分析，包括词频、词汇丰富性、句子和单词长度、词性标签以及与词嵌入的语义相似性。

Result: 研究结果表明，大型语言模型生成的文本更长，但词汇丰富性较低，更多地依赖高频词汇，并且名词代表性不足。语义向量空间分析显示相似性较低，突显了两个语料库在语义层面的差异。少量样本提示在一定程度上增加了儿童文本和大型语言模型文本之间的相似性，但仍未能复制词汇和语义模式。

Conclusion: 研究结果有助于我们理解大型语言模型如何通过多模态提示（文本+图像）近似儿童语言，并为它们在心理语言学研究和教育中的使用提供了见解，同时提出了关于大型语言模型生成语言在面向儿童的教育工具中的适当性的重要问题。

Abstract: The role of large language models (LLMs) in education is increasing, yet
little attention has been paid to whether LLM-generated text resembles child
language. This study evaluates how LLMs replicate child-like language by
comparing LLM-generated texts to a collection of German children's descriptions
of picture stories. We generated two LLM-based corpora using the same picture
stories and two prompt types: zero-shot and few-shot prompts specifying a
general age from the children corpus. We conducted a comparative analysis
across psycholinguistic text properties, including word frequency, lexical
richness, sentence and word length, part-of-speech tags, and semantic
similarity with word embeddings. The results show that LLM-generated texts are
longer but less lexically rich, rely more on high-frequency words, and
under-represent nouns. Semantic vector space analysis revealed low similarity,
highlighting differences between the two corpora on the level of corpus
semantics. Few-shot prompt increased similarities between children and LLM text
to a minor extent, but still failed to replicate lexical and semantic patterns.
The findings contribute to our understanding of how LLMs approximate child
language through multimodal prompting (text + image) and give insights into
their use in psycholinguistic research and education while raising important
questions about the appropriateness of LLM-generated language in child-directed
educational tools.

</details>


### [23] [TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain](https://arxiv.org/abs/2508.13798)
*Bohao Chu,Meijie Li,Sameh Frihat,Chengyu Gu,Georg Lodde,Elisabeth Livingstone,Norbert Fuhr*

Main category: cs.CL

TL;DR: This paper introduces TracSum, a new benchmark for traceable, aspect-based summarization in the medical domain. It includes 3.5K summary-citation pairs and a fine-grained evaluation framework. The results show that TracSum is effective for evaluating such summaries, and that sentence-level tracking improves accuracy and completeness.


<details>
  <summary>Details</summary>
Motivation: Concerns about the factual accuracy of summaries generated by LLMs persist, especially in the medical domain. Tracing evidence from which summaries are derived enables users to assess their accuracy.

Method: We introduce TracSum, a novel benchmark for traceable, aspect-based summarization, and propose a fine-grained evaluation framework. We also introduce a summarization pipeline, Track-Then-Sum, as a baseline method for comparison.

Result: The findings demonstrate that TracSum can serve as an effective benchmark for traceable, aspect-based summarization tasks. Explicitly performing sentence-level tracking prior to summarization enhances generation accuracy, while incorporating the full context further improves completeness.

Conclusion: TracSum can serve as an effective benchmark for traceable, aspect-based summarization tasks. We also observe that explicitly performing sentence-level tracking prior to summarization enhances generation accuracy, while incorporating the full context further improves completeness.

Abstract: While document summarization with LLMs has enhanced access to textual
information, concerns about the factual accuracy of these summaries persist,
especially in the medical domain. Tracing evidence from which summaries are
derived enables users to assess their accuracy, thereby alleviating this
concern. In this paper, we introduce TracSum, a novel benchmark for traceable,
aspect-based summarization, in which generated summaries are paired with
sentence-level citations, enabling users to trace back to the original context.
First, we annotate 500 medical abstracts for seven key medical aspects,
yielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation
framework for this new task, designed to assess the completeness and
consistency of generated content using four metrics. Finally, we introduce a
summarization pipeline, Track-Then-Sum, which serves as a baseline method for
comparison. In experiments, we evaluate both this baseline and a set of LLMs on
TracSum, and conduct a human evaluation to assess the evaluation results. The
findings demonstrate that TracSum can serve as an effective benchmark for
traceable, aspect-based summarization tasks. We also observe that explicitly
performing sentence-level tracking prior to summarization enhances generation
accuracy, while incorporating the full context further improves completeness.

</details>


### [24] [Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding](https://arxiv.org/abs/2508.13804)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: This study evaluates how well large language models understand moral dimensions by comparing them to humans, revealing that AI models have better moral detection capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models comprehend moral dimensions in comparison to humans.

Method: A large-scale Bayesian evaluation of market-leading language models using annotations from ~700 annotators on 100K+ texts.

Result: AI models typically rank among the top 25% of human annotators, achieving much better-than-average balanced accuracy and producing far fewer false negatives than humans.

Conclusion: AI models show superior moral detection capabilities compared to humans, with fewer false negatives.

Abstract: How do large language models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models
provides the answer. In contrast to prior work using deterministic ground truth
(majority or inclusion rules), we model annotator disagreements to capture both
aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty
(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,
DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on
100K+ texts spanning social media, news, and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing
that AI models typically rank among the top 25\% of human annotators, achieving
much better-than-average balanced accuracy. Importantly, we find that AI
produces far fewer false negatives than humans, highlighting their more
sensitive moral detection capabilities.

</details>


### [25] [Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs](https://arxiv.org/abs/2508.13805)
*Juncheng Xie,Hung-yi Lee*

Main category: cs.CL

TL;DR: 本文提出一种无需微调或迭代采样的提示策略，使大型语言模型能够精确生成所需长度的文本。


<details>
  <summary>Details</summary>
Motivation: 控制大型语言模型生成的文本长度仍然具有挑战性，因为模型无法可靠地保持内部令牌计数。

Method: 提出了一种基于提示的一次性策略，通过在提示中添加倒计时标记和明确的计数规则，使模型在“写作的同时进行计数”，从而生成所需的准确令牌数量。

Result: 在MT-Bench-LI上，使用倒计时提示的GPT-4.1严格遵守长度要求从低于30%提升到95%以上，超过了流行的先草稿再修改基线，同时保持了答案质量。

Conclusion: 精确的长度控制可以通过仅使用提示工程实现，这为训练或解码方法提供了一种轻量级的替代方案。

Abstract: Controlling the length of text produced by large language models (LLMs)
remains challenging: models frequently overshoot or undershoot explicit length
instructions because they cannot reliably keep an internal token count. We
present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to
generate exactly a desired number of tokens - words (English) or characters
(Chinese) - without any fine-tuning or iterative sampling. The prompt appends
countdown markers and explicit counting rules so that the model "writes while
counting." We evaluate on four settings: open-ended generation (1-1000 tokens),
XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH
equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps
from below 30% under naive prompts to above 95% with our countdown prompt,
surpassing the popular draft-then-revise baseline, while judged answer quality
is preserved. These results show that precise length control can be achieved
through prompt engineering alone, offering a lightweight alternative to
training- or decoding-based methods.

</details>


### [26] [The illusion of a perfect metric: Why evaluating AI's words is harder than it looks](https://arxiv.org/abs/2508.13816)
*Maria Paz Oliva,Adriana Correia,Ivan Vankov,Viktor Botev*

Main category: cs.CL

TL;DR: 本文分析了现有自动评估指标（AEM）的优缺点，发现它们在不同任务和数据集上的效果不一，且与人类判断的相关性不稳定。文章建议根据任务需求选择指标，并强调新指标应注重验证方法的改进。


<details>
  <summary>Details</summary>
Motivation: 评估自然语言生成（NLG）对于AI的实际应用至关重要，但一直是一个长期的研究挑战。虽然人类评估被认为是标准，但成本高且难以扩展。因此，需要开发自动评估指标（AEM）来替代或补充人类评估。然而，目前没有单一的指标能够成为最终解决方案，导致研究中使用不同的指标而未充分考虑其影响。

Method: 本文对现有指标的方法、已记录的优势和局限性、验证方法以及与人类判断的相关性进行了深入分析。

Result: 本文发现现有的自动评估指标（AEM）在捕捉文本质量的各个方面、有效性、验证实践和与人类判断的相关性方面存在显著问题。这些挑战在最新的LLM-as-a-Judge和RAG评估中依然存在。

Conclusion: 本文发现，目前的自动评估指标（AEM）仍然存在诸多挑战，包括仅捕捉文本质量的特定方面、有效性因任务和数据集而异、验证实践未结构化以及与人类判断的相关性不一致。此外，这些挑战在最新的LLM-as-a-Judge和RAG评估中依然存在。因此，本文建议根据任务需求选择指标，并利用互补评估，同时新指标应关注增强的验证方法。

Abstract: Evaluating Natural Language Generation (NLG) is crucial for the practical
adoption of AI, but has been a longstanding research challenge. While human
evaluation is considered the de-facto standard, it is expensive and lacks
scalability. Practical applications have driven the development of various
automatic evaluation metrics (AEM), designed to compare the model output with
human-written references, generating a score which approximates human judgment.
Over time, AEMs have evolved from simple lexical comparisons, to semantic
similarity models and, more recently, to LLM-based evaluators. However, it
seems that no single metric has emerged as a definitive solution, resulting in
studies using different ones without fully considering the implications. This
paper aims to show this by conducting a thorough examination of the
methodologies of existing metrics, their documented strengths and limitations,
validation methods, and correlations with human judgment. We identify several
key challenges: metrics often capture only specific aspects of text quality,
their effectiveness varies by task and dataset, validation practices remain
unstructured, and correlations with human judgment are inconsistent.
Importantly, we find that these challenges persist in the most recent type of
metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented
Generation (RAG), an increasingly relevant task in academia and industry. Our
findings challenge the quest for the 'perfect metric'. We propose selecting
metrics based on task-specific needs and leveraging complementary evaluations
and advocate that new metrics should focus on enhanced validation
methodologies.

</details>


### [27] [Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling](https://arxiv.org/abs/2508.13833)
*Insaf Nahri,Romain Pinquié,Philippe Véron,Nicolas Bus,Mathieu Thorel*

Main category: cs.CL

TL;DR: 本研究探索了BIM与NLP的集成，以自动化从法语建筑技术规范文档中提取需求。通过使用CamemBERT和Fr_core_news_lg模型进行NER任务，以及随机森林进行RE任务，取得了较高的性能。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索建筑信息模型（BIM）与自然语言处理（NLP）的集成，以自动化从非结构化法语建筑技术规范（BTS）文档中提取需求。

Method: 研究采用了命名实体识别（NER）和关系抽取（RE）技术，利用基于Transformer的模型CamemBERT和法语语言模型Fr_core_news_lg进行迁移学习。此外，还开发了从基于规则到深度学习的方法作为基准。对于RE任务，实现了四种不同的监督模型，包括随机森林，并使用自定义特征向量进行训练。

Result: 研究表明，CamemBERT和Fr_core_news_lg在NER任务中取得了超过90%的F1分数，而随机森林在RE任务中取得了超过80%的F1分数。

Conclusion: 研究结果表明，CamemBERT和Fr_core_news_lg在NER任务中表现出色，而随机森林在RE任务中表现最佳。未来的工作旨在将结果表示为知识图谱，以进一步增强自动验证系统。

Abstract: This study explores the integration of Building Information Modeling (BIM)
with Natural Language Processing (NLP) to automate the extraction of
requirements from unstructured French Building Technical Specification (BTS)
documents within the construction industry. Employing Named Entity Recognition
(NER) and Relation Extraction (RE) techniques, the study leverages the
transformer-based model CamemBERT and applies transfer learning with the French
language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in
the general domain. To benchmark these models, additional approaches ranging
from rule-based to deep learning-based methods are developed. For RE, four
different supervised models, including Random Forest, are implemented using a
custom feature vector. A hand-crafted annotated dataset is used to compare the
effectiveness of NER approaches and RE models. Results indicate that CamemBERT
and Fr\_core\_news\_lg exhibited superior performance in NER, achieving
F1-scores over 90\%, while Random Forest proved most effective in RE, with an
F1 score above 80\%. The outcomes are intended to be represented as a knowledge
graph in future work to further enhance automatic verification systems.

</details>


### [28] [MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2508.13938)
*Jiacheng Ruan,Dan Jiang,Xian Gao,Ting Liu,Yuzhuo Fu,Yangyang Kang*

Main category: cs.CL

TL;DR: 本文提出了MME-SCI基准，用于评估多模态大语言模型在科学领域的推理能力，该基准涵盖多个学科和语言，并展示了其挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有基准在多语言场景下的推理能力评估不足，缺乏对多模态覆盖的充分评估，以及科学知识点的细粒度标注。

Method: 通过收集高质量的问答对，构建了一个涵盖多个学科和多种语言的基准测试集，并对多种模型进行了实验分析。

Result: MME-SCI在16个开源模型和4个闭源模型上进行了广泛实验，结果显示其比现有基准更具挑战性。

Conclusion: MME-SCI是一个全面且具有挑战性的基准，能够评估多模态大语言模型在科学领域的推理能力。

Abstract: Recently, multimodal large language models (MLLMs) have achieved significant
advancements across various domains, and corresponding evaluation benchmarks
have been continuously refined and improved. In this process, benchmarks in the
scientific domain have played an important role in assessing the reasoning
capabilities of MLLMs. However, existing benchmarks still face three key
challenges: 1) Insufficient evaluation of models' reasoning abilities in
multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive
modality coverage; 3) Lack of fine-grained annotation of scientific knowledge
points. To address these gaps, we propose MME-SCI, a comprehensive and
challenging benchmark. We carefully collected 1,019 high-quality
question-answer pairs, which involve 3 distinct evaluation modes. These pairs
cover four subjects, namely mathematics, physics, chemistry, and biology, and
support five languages: Chinese, English, French, Spanish, and Japanese. We
conducted extensive experiments on 16 open-source models and 4 closed-source
models, and the results demonstrate that MME-SCI is widely challenging for
existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini
achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics,
physics, chemistry, and biology, respectively, indicating a significantly
higher difficulty level compared to existing benchmarks. More importantly,
using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed
existing models' performance in depth and identified their weaknesses in
specific domains. The Data and Evaluation Code are available at
https://github.com/JCruan519/MME-SCI.

</details>


### [29] [ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features](https://arxiv.org/abs/2508.13953)
*A. J. W. de Vink,Natalia Amat-Lefort,Lifeng Han*

Main category: cs.CL

TL;DR: 本研究提出了ReviewGraph框架，用于评论评分预测，通过将文本评论转换为知识图谱，并利用图嵌入和情感特征进行预测。该框架在计算成本较低的情况下表现出与先进模型相当的性能，并在可解释性和集成性方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 在酒店行业中，理解影响客户评论评分的因素对于提高顾客满意度和业务表现至关重要。

Method: ReviewGraph框架通过从文本客户评论中提取（主体，谓词，对象）三元组并关联情感得分，将文本转化为知识图谱，并使用图嵌入（Node2Vec）和情感特征通过机器学习分类器预测评论评分。

Result: ReviewGraph在HotelRec数据集上的表现与最先进的文献中的最佳模型相当，但计算成本更低（无需集成）。此外，它在基于协议的指标（如Cohen's Kappa）上优于基线，并在可解释性、可视化探索和潜在集成到检索增强生成（RAG）系统方面具有额外优势。

Conclusion: 本研究展示了基于图表示的评论分析在增强评论分析方面的潜力，并为未来整合先进的图神经网络和微调的LLM提取方法的研究奠定了基础。

Abstract: In the hospitality industry, understanding the factors that drive customer
review ratings is critical for improving guest satisfaction and business
performance. This work proposes ReviewGraph for Review Rating Prediction (RRP),
a novel framework that transforms textual customer reviews into knowledge
graphs by extracting (subject, predicate, object) triples and associating
sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the
framework predicts review rating scores through machine learning classifiers.
We compare ReviewGraph performance with traditional NLP baselines (such as Bag
of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating
them in the HotelRec dataset. In comparison to the state of the art literature,
our proposed model performs similar to their best performing model but with
lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and
outperforms baselines on agreement-based metrics such as Cohen's Kappa, it
offers additional advantages in interpretability, visual exploration, and
potential integration into Retrieval-Augmented Generation (RAG) systems. This
work highlights the potential of graph-based representations for enhancing
review analytics and lays the groundwork for future research integrating
advanced graph neural networks and fine-tuned LLM-based extraction methods. We
will share ReviewGraph output and platform open-sourced on our GitHub page
https://github.com/aaronlifenghan/ReviewGraph

</details>


### [30] [Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization](https://arxiv.org/abs/2508.13993)
*Shaohua Duan,Xinze Li,Zhenghao Liu,Xiaoyuan Yi,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了一种名为LongMab-PO的新框架，通过多臂老虎机策略来提高长上下文模型的性能。


<details>
  <summary>Details</summary>
Motivation: 长上下文建模对于一系列实际任务至关重要，包括长上下文问答、摘要和复杂推理任务。最近的研究探索了使用合成数据微调大型语言模型（LLMs）以增强其长上下文能力。然而，这种方法的有效性常常受到生成数据低多样性和事实不一致性的限制。

Method: 我们提出了LongMab-PO，这是一种新颖的框架，利用多臂老虎机（MAB）滚动策略来识别给定长上下文中最有信息量的块，以采样高质量和多样化的响应，并构建用于直接偏好优化（DPO）训练的偏好数据对。具体来说，我们将上下文块视为MAB的臂，根据其预期奖励分数选择块输入到LLMs生成响应，并根据奖励反馈迭代更新这些分数。

Result: LongMab-PO显著提高了偏好数据对的多样性和质量，在长上下文推理基准测试中达到了最先进的性能。

Conclusion: 实验结果表明，LongMab-PO显著提高了偏好数据对的多样性和质量，在长上下文推理基准测试中达到了最先进的性能。

Abstract: Long-context modeling is critical for a wide range of real-world tasks,
including long-context question answering, summarization, and complex reasoning
tasks. Recent studies have explored fine-tuning Large Language Models (LLMs)
with synthetic data to enhance their long-context capabilities. However, the
effectiveness of such approaches is often limited by the low diversity and
factual inconsistencies in the generated data. To address these challenges, we
propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)
rollout strategy to identify the most informative chunks from the given long
context for sampling high-quality and diverse responses and constructing
preference data pairs for Direct Preference Optimization (DPO) training.
Specifically, we treat context chunks as arms of MAB, select chunks based on
their expected reward scores to input into LLMs to generate responses, and
iteratively update these scores based on reward feedback. This exploration and
exploitation process enables the model to focus on the most relevant context
segments, thereby generating and collecting high-quality and diverse responses.
Finally, we collect these generated responses from the rollout process and
apply the DPO method to further optimize the LLM. Experimental results show
that LongMab-PO significantly improves the diversity and quality of preference
data pairs, achieving state-of-the-art performance on long-context reasoning
benchmarks. All code and data will be released on
https://github.com/NEUIR/LongMab-PO.

</details>


### [31] [Ask Good Questions for Large Language Models](https://arxiv.org/abs/2508.14025)
*Qi Wu,Zhongqi Lu*

Main category: cs.CL

TL;DR: 本文介绍了AGQ框架，利用CEIRT模型和大型语言模型生成引导问题，提高信息检索效率并改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前的方法常常无法提供准确的主题指导，因为它们无法辨别用户在相关概念上的困惑。

Method: 我们引入了Ask-Good-Question (AGQ)框架，该框架包含一个改进的Concept-Enhanced Item Response Theory (CEIRT)模型，以更好地识别用户的知识水平。

Result: 我们的方法通过应用CEIRT模型和LLMs直接生成引导问题，大大提高了问答过程中的信息检索效率。

Conclusion: 我们的方法通过与其它基线方法的比较，显著增强了用户的资讯检索体验。

Abstract: Recent advances in large language models (LLMs) have significantly improved
the performance of dialog systems, yet current approaches often fail to provide
accurate guidance of topic due to their inability to discern user confusion in
related concepts. To address this, we introduce the Ask-Good-Question (AGQ)
framework, which features an improved Concept-Enhanced Item Response Theory
(CEIRT) model to better identify users' knowledge levels. Our contributions
include applying the CEIRT model along with LLMs to directly generate guiding
questions based on the inspiring text, greatly improving information retrieval
efficiency during the question & answer process. Through comparisons with other
baseline methods, our approach outperforms by significantly enhencing the
users' information retrieval experiences.

</details>


### [32] [Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR](https://arxiv.org/abs/2508.14029)
*Xiao Liang,Zhongzhi Li,Yeyun Gong,Yelong Shen,Ying Nian Wu,Zhijiang Guo,Weizhu Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的RLVR训练策略SvS，通过合成变分问题来保持策略熵，从而提高LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR训练在提高Pass@1性能的同时，导致策略熵减少，从而降低了生成多样性，限制了Pass@k性能，这通常代表了LLM推理能力的上限。

Method: 提出了一种在线自博弈变分问题合成（SvS）策略，利用策略的正确解来合成变分问题，同时确保参考答案与原始问题相同。

Result: SvS策略在训练过程中有效保持了策略熵，并显著提高了Pass@k性能，在AIME24和AIME25基准上分别实现了18.3%和22.8%的绝对提升。

Conclusion: SvS策略在多个推理基准测试中展示了其泛化性和鲁棒性，并在AIME24和AIME25竞赛级别基准上实现了Pass@32性能的绝对提升。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a key paradigm for post-training Large Language Models (LLMs), particularly for
complex reasoning tasks. However, vanilla RLVR training has been shown to
improve Pass@1 performance at the expense of policy entropy, leading to reduced
generation diversity and limiting the Pass@k performance, which typically
represents the upper bound of LLM reasoning capability. In this paper, we
systematically analyze the policy's generation diversity from the perspective
of training problems and find that augmenting and updating training problems
helps mitigate entropy collapse during training. Based on these observations,
we propose an online Self-play with Variational problem Synthesis (SvS)
strategy for RLVR training, which uses the policy's correct solutions to
synthesize variational problems while ensuring their reference answers remain
identical to the originals. This self-improving strategy effectively maintains
policy entropy during training and substantially improves Pass@k compared with
standard RLVR, sustaining prolonged improvements and achieving absolute gains
of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and
AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model
sizes from 3B to 32B consistently demonstrate the generalizability and
robustness of SvS.

</details>


### [33] [Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation](https://arxiv.org/abs/2508.14031)
*Dongyoon Hahm,Taywon Min,Woogyeol Jin,Kimin Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为 PING 的方法，用于增强微调的 LLM 代理的安全性，通过在代理响应前添加自然语言前缀来引导它们拒绝有害请求。实验表明 PING 在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: LLM 在微调过程中经常被忽视安全问题，导致对有害任务的执行可能性增加，拒绝可能性减少。需要一种方法来增强微调 LLM 代理的安全性。

Method: 提出了一种称为 Prefix INjection Guard (PING) 的简单而有效的方法，该方法通过在代理响应前添加自动生成的自然语言前缀来引导它们拒绝有害请求。引入了一种迭代方法，交替生成候选前缀并选择优化任务性能和拒绝行为的前缀。

Result: PING 显著增强了微调的 LLM 代理的安全性，同时没有牺牲其有效性。PING 在各种基准测试中 consistently 超过了现有的提示方法。分析显示，前缀标记对于行为修改至关重要，解释了性能提升。

Conclusion: PING 显著增强了微调的 LLM 代理的安全性，同时没有牺牲其有效性。PING 在各种基准测试中 consistently 超过了现有的提示方法。

Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into
agentic systems capable of planning and interacting with external tools to
solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific
tasks to enhance their proficiency. However, safety concerns are frequently
overlooked during this fine-tuning process. In this work, we show that aligned
LLMs can become unintentionally misaligned, leading to a higher likelihood of
executing harmful tasks and a reduced tendency to refuse them when fine-tuned
to execute agentic tasks. To address these safety challenges, we propose Prefix
INjection Guard (PING), a simple yet effective method that prepends
automatically generated natural language prefixes to agent responses, guiding
them to refuse harmful requests while preserving performance on benign tasks.
Specifically, we introduce an iterative approach that alternates between (1)
generating candidate prefixes and (2) selecting those that optimize both task
performance and refusal behavior. Experimental results demonstrate that PING
significantly enhances the safety of fine-tuned LLM agents without sacrificing
their effectiveness. PING consistently outperforms existing prompting
approaches across diverse benchmarks in both web navigation and code generation
tasks. Our analysis of internal hidden states via linear probes reveals that
prefix tokens are crucial for behavior modification, explaining the performance
gains. WARNING: This paper contains contents that are unethical or offensive in
nature.

</details>


### [34] [The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities](https://arxiv.org/abs/2508.14032)
*Xiancheng Li,Georgios D. Karampatakis,Helen E. Wood,Chris J. Griffiths,Borislava Mihaylova,Neil S. Coulson,Alessio Pasinato,Pietro Panzarasa,Marco Viviani,Anna De Simoni*

Main category: cs.CL

TL;DR: 本文研究了如何利用大型语言模型通过上下文学习整合专家知识，以解决数字健康分析中的挑战。结果表明，LLMs在多个模型上表现出色，并与专家水平的判断一致，为实时、高质量的患者监测和健康策略提供了有前景的解决方案。


<details>
  <summary>Details</summary>
Motivation: 数字健康分析面临关键挑战，包括对患者生成的健康内容进行复杂情感和医学背景分析所需的稀缺领域专业知识，以及传统机器学习方法在医疗环境中数据不足和隐私限制的问题。在线健康社区（OHCs）的例子展示了这些挑战，如混合情绪帖子、临床术语和隐含情感表达。

Method: 本文开发了一个结构化的代码本，系统地编码了专家解释指南，使LLMs能够通过针对性提示应用领域特定知识，而不是进行大量训练。同时，比较了多种预训练语言模型和基于词典的方法。

Result: LLMs在专家标注的400条帖子上表现优于其他方法，并且与专家水平的判断一致。这种高一致性表明，知识整合超越了表面模式识别。

Conclusion: 本文提出了一种利用大型语言模型（LLMs）通过上下文学习整合专家知识的方法，以解决数字健康分析中的挑战。该方法在多个LLM模型上表现出色，并与专家水平的判断一致，为实时、高质量的患者监测和健康策略提供了有前景的解决方案。

Abstract: Digital health analytics face critical challenges nowadays. The sophisticated
analysis of patient-generated health content, which contains complex emotional
and medical contexts, requires scarce domain expertise, while traditional ML
approaches are constrained by data shortage and privacy limitations in
healthcare settings. Online Health Communities (OHCs) exemplify these
challenges with mixed-sentiment posts, clinical terminology, and implicit
emotional expressions that demand specialised knowledge for accurate Sentiment
Analysis (SA). To address these challenges, this study explores how Large
Language Models (LLMs) can integrate expert knowledge through in-context
learning for SA, providing a scalable solution for sophisticated health data
analysis. Specifically, we develop a structured codebook that systematically
encodes expert interpretation guidelines, enabling LLMs to apply
domain-specific knowledge through targeted prompting rather than extensive
training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are
compared with pre-trained language models (BioBERT variants) and lexicon-based
methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior
performance while demonstrating expert-level agreement. This high agreement,
with no statistically significant difference from inter-expert agreement
levels, suggests knowledge integration beyond surface-level pattern
recognition. The consistent performance across diverse LLM models, supported by
in-context learning, offers a promising solution for digital health analytics.
This approach addresses the critical challenge of expert knowledge shortage in
digital health research, enabling real-time, expert-quality analysis for
patient monitoring, intervention assessment, and evidence-based health
strategies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [35] [Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference](https://arxiv.org/abs/2508.13439)
*Yunxiang Yang,Ningning Xu,Jidong J. Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于结构化提示和知识蒸馏的框架，通过协调两个大型视觉-语言模型生成多视角输出，并用于训练一个更小的学生模型VISTA。VISTA在多个指标上表现优异，且具有紧凑架构，适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统方法在可扩展性和泛化性方面面临挑战，特别是在复杂和动态的真实环境条件下。需要一种能够自动生成高质量交通场景注释和上下文风险评估的框架。

Method: 引入了一种新颖的结构化提示和知识蒸馏框架，利用结构化思维链（CoT）策略协调两个大型视觉-语言模型（VLMs），生成丰富的多视角输出，并作为知识丰富的伪标注用于较小的学生VLM的监督微调。

Result: VISTA在多个标准的captioning指标（BLEU-4、METEOR、ROUGE-L和CIDEr）上表现出色，尽管其参数数量显著减少，但仍能实现强大的性能。

Conclusion: VISTA demonstrates that effective knowledge distillation and structured multi-agent supervision can empower lightweight VLMs to capture complex reasoning capabilities. Its compact architecture facilitates efficient deployment on edge devices, enabling real-time risk monitoring without requiring extensive infrastructure upgrades.

Abstract: Comprehensive highway scene understanding and robust traffic risk inference
are vital for advancing Intelligent Transportation Systems (ITS) and autonomous
driving. Traditional approaches often struggle with scalability and
generalization, particularly under the complex and dynamic conditions of
real-world environments. To address these challenges, we introduce a novel
structured prompting and knowledge distillation framework that enables
automatic generation of high-quality traffic scene annotations and contextual
risk assessments. Our framework orchestrates two large Vision-Language Models
(VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy
to produce rich, multi-perspective outputs. These outputs serve as
knowledge-enriched pseudo-annotations for supervised fine-tuning of a much
smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision
for Intelligent Scene and Traffic Analysis), is capable of understanding
low-resolution traffic videos and generating semantically faithful, risk-aware
captions. Despite its significantly reduced parameter count, VISTA achieves
strong performance across established captioning metrics (BLEU-4, METEOR,
ROUGE-L, and CIDEr) when benchmarked against its teacher models. This
demonstrates that effective knowledge distillation and structured multi-agent
supervision can empower lightweight VLMs to capture complex reasoning
capabilities. The compact architecture of VISTA facilitates efficient
deployment on edge devices, enabling real-time risk monitoring without
requiring extensive infrastructure upgrades.

</details>


### [36] [RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation](https://arxiv.org/abs/2508.13968)
*Tianyi Niu,Jaemin Cho,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CV

TL;DR: 研究分析了MLLMs在识别图像旋转方面的能力，发现它们在区分90度和270度旋转上存在困难，揭示了与人类感知之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 研究MLLMs在识别图像旋转方面的能力，特别是它们在不同旋转角度下的表现，以评估其空间推理能力。

Method: 引入了RotBench——一个包含350张手动筛选的图像基准，包括生活方式、肖像和风景图像，用于评估MLLMs在识别图像旋转方面的能力。

Result: 尽管任务相对简单，但几个最先进的开放和专有MLLMs（包括GPT-5、o3和Gemini-2.5-Pro）无法可靠地识别输入图像的旋转。提供辅助信息或使用思维链提示仅带来小而不一致的改进。大多数模型能够可靠地识别正向（0度）图像，某些模型能够识别倒置（180度）图像，但没有模型能可靠地区分90度和270度。同时展示不同旋转方向的图像可提高推理模型的性能，而使用投票的修改设置提高了弱模型的性能。微调并未改善模型区分90度和270度旋转的能力，尽管显著提高了180度图像的识别能力。

Conclusion: 研究结果揭示了MLLMs在识别图像旋转方面的空间推理能力与人类感知之间存在显著差距。

Abstract: We investigate to what extent Multimodal Large Language Models (MLLMs) can
accurately identify the orientation of input images rotated 0{\deg}, 90{\deg},
180{\deg}, and 270{\deg}. This task demands robust visual reasoning
capabilities to detect rotational cues and contextualize spatial relationships
within images, regardless of their orientation. To evaluate MLLMs on these
abilities, we introduce RotBench -- a 350-image manually-filtered benchmark
comprising lifestyle, portrait, and landscape images. Despite the relatively
simple nature of this task, we show that several state-of-the-art open and
proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably
identify rotation in input images. Providing models with auxiliary information
-- including captions, depth maps, and more -- or using chain-of-thought
prompting offers only small and inconsistent improvements. Our results indicate
that most models are able to reliably identify right-side-up (0{\deg}) images,
while certain models are able to identify upside-down (180{\deg}) images. None
can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing
the image rotated in different orientations leads to moderate performance gains
for reasoning models, while a modified setup using voting improves the
performance of weaker models. We further show that fine-tuning does not improve
models' ability to distinguish 90{\deg} and 270{\deg} rotations, despite
substantially improving the identification of 180{\deg} images. Together, these
results reveal a significant gap between MLLMs' spatial reasoning capabilities
and human perception in identifying rotation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 本文提出了Agent Foundation Models (AFMs)，通过多智能体蒸馏框架和代理强化学习提升了模型在复杂问题解决中的能力，并在多个基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统计算效率低、能力有限，无法从数据驱动的学习中受益。

Method: 引入了多智能体蒸馏框架和基于可验证多智能体任务的代理强化学习，以提升模型在链式智能体问题解决方面的能力。

Result: AFM在Web代理和代码代理设置中的多个基准测试中表现优异。

Conclusion: AFM在多种基准测试中建立了新的最先进性能，并为未来的研究提供了坚实的基础。

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [38] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: The paper proposes Cognitive Workspace, a new approach to enhance large language models by emulating human cognitive mechanisms, leading to improved memory management and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of current passive retrieval systems in capturing the dynamic, task-driven nature of human memory management, and to propose a novel paradigm for extending the cognitive capabilities of large language models.

Method: Cognitive Workspace is based on cognitive science foundations such as Baddeley's working memory model, Clark's extended mind thesis, and Hutchins' distributed cognition framework. It introduces three core innovations: active memory management, hierarchical cognitive buffers, and task-driven context optimization.

Result: Empirical validation shows that Cognitive Workspace achieves an average 58.6% memory reuse rate and a 17-18% net efficiency gain compared to traditional RAG methods, with statistically significant advantages across multiple tasks.

Conclusion: Cognitive Workspace presents a fundamental shift from information retrieval to genuine cognitive augmentation, offering significant improvements in memory reuse and efficiency compared to traditional RAG methods.

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [39] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种新的文本到SQL模型CESQL，通过结合模型可解释性分析和执行引导策略，提升了在单表数据库查询任务中的准确性，并减少了对数据和人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 为了提高文本到SQL模型在真实应用场景中的基础能力和泛化能力，我们需要一种更有效的语义解析方法。

Method: 我们结合模型可解释性分析与执行引导策略，对SQL查询中的WHERE子句进行语义解析，并通过过滤调整、逻辑相关性优化和模型融合设计了CESQL模型。

Result: 我们的模型在WikiSQL数据集上表现出色，显著提高了预测结果的准确性，并减少了对条件列中数据的依赖以及手动标记训练数据的影响。

Conclusion: 我们的研究旨在通过提升文本到SQL模型的基础能力和泛化能力，为处理复杂查询和现实数据库环境中的不规则数据提供新的研究视角。

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [40] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: 本文提出了多跳个性化推理任务，以探索不同记忆机制在个性化信息上的多跳推理表现。我们构建了一个数据集和一个统一的评估框架，并实现了各种显式和隐式记忆方法进行实验。我们分析了它们的优缺点，并提出了HybridMem方法来解决它们的局限性。最后，我们通过广泛的实验展示了我们提出的模型的有效性，并将项目发布在https://github.com/nuster1128/MPR上。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型为基础的代理中，记忆是实现个性化的关键能力，通过存储和利用用户的信息。虽然一些先前的研究采用了记忆来实现用户个性化，但它们通常集中在偏好对齐和简单的问答上。然而，在现实世界中，复杂的任务通常需要对大量用户信息进行多跳推理，这对当前的记忆方法构成了重大挑战。

Method: 我们提出了多跳个性化推理任务，以探索不同的记忆机制在个性化信息上的多跳推理表现。我们明确定义了这个任务，并构建了一个数据集和一个统一的评估框架。然后，我们实现了各种显式和隐式记忆方法，并进行了全面的实验。我们从多个角度评估了它们在这个任务上的性能，并分析了它们的优缺点。此外，我们探索了结合两种范式的混合方法，并提出了HybridMem方法来解决它们的局限性。

Result: 我们实现了各种显式和隐式记忆方法，并进行了全面的实验。我们从多个角度评估了它们在这个任务上的性能，并分析了它们的优缺点。此外，我们探索了结合两种范式的混合方法，并提出了HybridMem方法来解决它们的局限性。

Conclusion: 我们通过广泛的实验展示了我们提出的模型的有效性。为了造福研究界，我们将在https://github.com/nuster1128/MPR上发布这个项目。

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [41] [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
*Nicole Cho,Kirsty Fielding,William Watson,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: 本文介绍了一种名为TASER的持续学习代理表格提取系统，用于从杂乱的财务表格中提取信息。该系统通过利用初始模式执行表格检测、分类、提取和推荐，并通过推荐代理进行优化，取得了优于现有模型的结果。


<details>
  <summary>Details</summary>
Motivation: 现实世界的财务文档包含关于实体财务头寸的重要信息，但这些细节通常被埋藏在杂乱、多页、碎片化的表格中。现有的表格检测模型存在局限性，需要一种更有效的解决方案。

Method: 我们提出了一个持续学习的代理表格提取系统TASER，它通过利用初始模式执行表格检测、分类、提取和推荐。推荐代理审查输出，推荐模式修订，并决定最终推荐。

Result: TASER在现有表格检测模型如Table Transformer上提高了10.1%。更大的批次大小导致可操作和使用的模式建议增加了104.3%，提取的头寸增加了9.8%。

Conclusion: 我们的结果突显了代理和基于模式的提取系统在稳健理解现实世界财务表格方面的潜力。

Abstract: Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

</details>


### [42] [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
*Katharina Stein,Nils Hodel,Daniel Fišer,Jörg Hoffmann,Michael Katz,Alexander Koller*

Main category: cs.AI

TL;DR: 本文提出一种改进的框架，通过生成伪代码并进行自动调试，提高LLM生成的泛化计划的质量。


<details>
  <summary>Details</summary>
Motivation: 之前的框架只生成一个策略并直接传递给程序生成，如果策略不正确，其实施将导致不正确的泛化计划。

Method: 引入了一种生成策略的方法，以伪代码形式并启用伪代码的自动调试，从而在生成泛化计划之前识别和修复错误。此外，扩展了Python调试阶段，提示LLM指出观察到的计划失败的原因。最后，受LLM代码生成的启发，生成多个程序变体并选择最佳的一个。

Result: 在17个基准领域上运行实验，显示这些扩展显著提高了泛化计划的质量，且从未恶化。

Conclusion: 这些扩展显著提高了泛化计划的质量，在12个领域中，我们的最佳Python程序解决了可以使用相应实例生成器生成的所有任务。

Abstract: LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms](https://arxiv.org/abs/2508.13337)
*Yueming Yuan,Ahan Gupta,Jianping Li,Sajal Dash,Feiyi Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: X-MoE 是一种新型的 MoE 训练系统，能够为下一代 MoE 架构提供可扩展的训练性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 MoE 训练系统主要针对 NVIDIA GPU 进行优化，在非 NVIDIA 平台上表现不佳，导致计算潜力未被充分利用。

Method: X-MoE 通过高效的无填充 MoE 训练、冗余绕过调度和混合并行性等技术实现可扩展的训练性能。

Result: 在 Frontier 超级计算机上，X-MoE 可以将 DeepSeek 风格的 MoE 扩展到 5450 亿参数，跨 1024 块 GPU，比现有方法大 10 倍，同时保持高训练吞吐量。

Conclusion: X-MoE 是一种新型的 MoE 训练系统，能够为下一代 MoE 架构提供可扩展的训练性能。

Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as
DeepSeek-MoE, deliver strong model quality through fine-grained expert
segmentation and large top-k routing. However, their scalability is limited by
substantial activation memory overhead and costly all-to-all communication.
Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs
- perform suboptimally on non-NVIDIA platforms, leaving significant
computational potential untapped. In this work, we present X-MoE, a novel MoE
training system designed to deliver scalable training performance for
next-generation MoE architectures. X-MoE achieves this via several novel
techniques, including efficient padding-free MoE training with cross-platform
kernels, redundancy-bypassing dispatch, and hybrid parallelism with
sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer,
powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to
545 billion parameters across 1024 GPUs - 10x larger than the largest trainable
model with existing methods under the same hardware budget, while maintaining
high training throughput. The source code of X-MoE is available at
https://github.com/Supercomputing-System-AI-Lab/X-MoE.

</details>


### [44] [Input Time Scaling](https://arxiv.org/abs/2508.13654)
*Rapheal Huang,Weilong Guo*

Main category: cs.LG

TL;DR: 本文提出了一种新的输入时间扩展方法，发现小数据集和低质量数据可能带来更好的性能，并在多个基准上实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型通常依赖于大规模精心策划的数据集进行后训练，并在推理时进行推理。我们希望探索一种新的扩展范式，以补充现有的方法。

Method: 我们提出了输入时间扩展的方法，结合了来自LLM的元知识来优化输入，并发现了训练和测试的协同设计现象。

Result: 我们在Qwen2.5-32B-Instruct模型上实现了AIME24和AIME25的SOTA性能，并通过三个模型的多数投票进一步提升了性能。

Conclusion: 我们的研究发现，通过输入时间扩展可以提高模型性能，并且小规模数据集可能比大规模数据集表现更好。此外，我们实现了在AIME24和AIME25上的SOTA性能，并计划开源相关资源以促进进一步研究。

Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale
carefully curated datasets (data & training scaling) and doing reasoning in
test time (inference time scaling). In this work, we present a new scaling
paradigm, Input Time Scaling, to complement previous scaling methods by putting
resources on queries (input time). During training and testing, we combine
meta-knowledge from LLMs to refine inputs with different strategies. We also
find a new phenomenon, training-testing co-design there. We need to apply query
strategies during both training and testing. Only applying strategies on
training or testing would seriously degrade the performance. We are also
surprised to find that seemingly low data quality datasets can gain high
performance. Adding irrelevant information to the queries, randomly selecting
examples from a minimally filtered dataset, can even perform the best. These
findings contradict the widely held inductive bias, "garbage in, garbage out".
Curating datasets with seemingly high-quality data can even potentially limit
the performance ceiling. In addition, models trained on more data with similar
quality (15k VS 1k) perform worse, simple dataset size scaling should also be
carefully inspected. The good news is that our findings are compatible with the
Less is More phenomenon. A small set of examples is enough to evoke high-level
reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,
we are able to reach SOTA performance among 32B models on AIME24(76.7%) and
AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with
a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,
the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate
reproducibility and further research, we are working on open-source our
datasets, data pipelines, evaluation results, and checkpoints.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [45] [Query Logs Analytics: A Aystematic Literature Review](https://arxiv.org/abs/2508.13949)
*Dihia Lanasri*

Main category: cs.DB

TL;DR: 本文对数据库、数据仓库、网络和知识图谱日志的使用进行了系统综述，分析了超过300篇文献，揭示了日志使用的共同特征、缺乏标准化流程以及共享结构元素的存在，并为未来研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 在数字时代，用户与各种资源的交互日益通过数字平台进行，这些交互留下了数字痕迹，以日志的形式系统地记录下来。尽管日志的重要性，但关于日志使用的研究仍然分散在各个领域，没有全面的研究来整合现有的努力。

Method: 对超过300篇文献进行了分析，以回答三个核心问题：不同类型的日志是否具有共同的结构和功能特征？它们的使用是否有标准流程？哪些约束和非功能性需求（NFR）指导它们的利用？

Result: 调查揭示了端到端方法有限，日志使用流程缺乏标准化，以及不同类型日志之间存在共享的结构元素。

Conclusion: 通过综合现有知识，识别差距并突出机会，本综述为研究人员和实践者提供了日志使用的全面概述，并为未来研究指明了方向，特别是关于知识图谱日志的利用和民主化。

Abstract: In the digital era, user interactions with various resources such as
databases, data warehouses, websites, and knowledge graphs (KGs) are
increasingly mediated through digital platforms. These interactions leave
behind digital traces, systematically captured in the form of logs. Logs, when
effectively exploited, provide high value across industry and academia,
supporting critical services (e.g., recovery and security), user-centric
applications (e.g., recommender systems), and quality-of-service improvements
(e.g., performance optimization). Despite their importance, research on log
usage remains fragmented across domains, and no comprehensive study currently
consolidates existing efforts. This paper presents a systematic survey of log
usage, focusing on Database (DB), Data Warehouse (DW), Web, and KG logs. More
than 300 publications were analyzed to address three central questions: (1) do
different types of logs share common structural and functional characteristics?
(2) are there standard pipelines for their usage? (3) which constraints and
non-functional requirements (NFRs) guide their exploitation?. The survey
reveals a limited number of end-to-end approaches, the absence of
standardization across log usage pipelines, and the existence of shared
structural elements among different types of logs. By consolidating existing
knowledge, identifying gaps, and highlighting opportunities, this survey
provides researchers and practitioners with a comprehensive overview of log
usage and sheds light on promising directions for future research, particularly
regarding the exploitation and democratization of KG logs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [46] [Prompt Orchestration Markup Language](https://arxiv.org/abs/2508.13948)
*Yuge Zhang,Nan Chen,Jiahang Xu,Yuqing Yang*

Main category: cs.HC

TL;DR: POML is a new approach for prompting large language models that addresses challenges in structure, data integration, and presentation variations.


<details>
  <summary>Details</summary>
Motivation: Current practices in prompting for large language models face challenges in structure, data integration, format sensitivity, and tooling. There is a need for a comprehensive solution to organize complex prompts involving diverse data types and manage presentation variations systematically.

Method: POML employs component-based markup for logical structure, specialized tags for data integration, and a CSS-like styling system to decouple content from presentation. It also includes templating for dynamic prompts and a developer toolkit.

Result: POML was validated through two case studies demonstrating its impact on complex application integration and accuracy performance, as well as a user study assessing its effectiveness in real-world development scenarios.

Conclusion: POML addresses the challenges in prompting for large language models by providing a comprehensive solution for organizing complex prompts and managing presentation variations.

Abstract: Large Language Models (LLMs) require sophisticated prompting, yet current
practices face challenges in structure, data integration, format sensitivity,
and tooling. Existing methods lack comprehensive solutions for organizing
complex prompts involving diverse data types (documents, tables, images) or
managing presentation variations systematically. To address these gaps, we
introduce POML (Prompt Orchestration Markup Language). POML employs
component-based markup for logical structure (roles, tasks, examples),
specialized tags for seamless data integration, and a CSS-like styling system
to decouple content from presentation, reducing formatting sensitivity. It
includes templating for dynamic prompts and a comprehensive developer toolkit
(IDE support, SDKs) to improve version control and collaboration. We validate
POML through two case studies demonstrating its impact on complex application
integration (PomLink) and accuracy performance (TableQA), as well as a user
study assessing its effectiveness in real-world development scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [47] [White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design](https://arxiv.org/abs/2508.13172)
*Jianqiu Chen,Siqi Li,Xu He*

Main category: cs.AR

TL;DR: 本文提出了一种结合大型语言模型和gm/Id方法的协同推理框架，用于模拟IC设计。该框架在双级运算放大器上验证，能够快速满足所有TT角落规格，并在效率上优于资深工程师的设计。


<details>
  <summary>Details</summary>
Motivation: 模拟IC设计由于依赖经验和低效的仿真而成为瓶颈，传统公式在先进节点中失效。直接应用大型语言模型（LLM）到这个问题可能会导致仅凭猜测而没有工程原理。

Method: 本文提出了一种“协同推理”框架，将LLM的战略推理与gm/Id方法的物理精度相结合。通过赋予LLMgm/Id查找表，使其成为定量、数据驱动的设计伙伴。

Result: 在双级运算放大器上进行了验证，我们的框架使Gemini模型在5次迭代中满足所有TT角落规格，并将优化扩展到所有PVT角落。关键的消融研究证明了gm/Id数据对于这种效率和精度至关重要；没有它，LLM会更慢且偏离。与资深工程师的设计相比，我们的框架实现了准专家质量，并在效率上提高了数量级。

Conclusion: 本文验证了通过结合LLM推理与科学电路设计方法，可以实现真正的模拟设计自动化。

Abstract: Analog IC design is a bottleneck due to its reliance on experience and
inefficient simulations, as traditional formulas fail in advanced nodes.
Applying Large Language Models (LLMs) directly to this problem risks mere
"guessing" without engineering principles. We present a "synergistic reasoning"
framework that integrates an LLM's strategic reasoning with the physical
precision of the gm/Id methodology. By empowering the LLM with gm/Id lookup
tables, it becomes a quantitative, data-driven design partner.
  We validated this on a two-stage op-amp, where our framework enabled the
Gemini model to meet all TT corner specs in 5 iterations and extended
optimization to all PVT corners. A crucial ablation study proved gm/Id data is
key for this efficiency and precision; without it, the LLM is slower and
deviates. Compared to a senior engineer's design, our framework achieves
quasi-expert quality with an order-of-magnitude improvement in efficiency. This
work validates a path for true analog design automation by combining LLM
reasoning with scientific circuit design methodologies.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [48] [TaoSR1: The Thinking Model for E-commerce Relevance Search](https://arxiv.org/abs/2508.12365)
*Chenhe Dong,Shaowei Yao,Pengkun Jiao,Jianhui Yang,Yiming Jin,Zerui Huang,Xiaojiang Zhou,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: 本文提出了一种名为TaoSR1的框架，用于直接部署大型语言模型（LLM）进行查询-产品相关性预测。该框架包含三个阶段：(1) 使用思维链（CoT）进行监督微调以培养推理能力；(2) 通过pass@N策略和直接偏好优化（DPO）进行离线采样以提高生成质量；(3) 通过基于难度的动态采样和组相对策略优化（GRPO）来减轻判别性幻觉。此外，后CoT处理和基于累积概率的划分方法实现了高效的在线部署。TaoSR1在离线数据集上显著优于基线，并在在线对比人类评估中取得了显著成果，为将CoT推理应用于相关性分类引入了新的范式。


<details>
  <summary>Details</summary>
Motivation: Query-product relevance prediction is a core task in e-commerce search. BERT-based models excel at semantic matching but lack complex reasoning capabilities. While Large Language Models (LLMs) are explored, most still use discriminative fine-tuning or distill to smaller models for deployment. We propose a framework to directly deploy LLMs for this task, addressing key challenges: Chain-of-Thought (CoT) error accumulation, discriminative hallucination, and deployment feasibility.

Method: TaoSR1 involves three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning; (2) Offline sampling with a pass@N strategy and Direct Preference Optimization (DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling with Group Relative Policy Optimization (GRPO) to mitigate discriminative hallucination. Additionally, post-CoT processing and a cumulative probability-based partitioning method enable efficient online deployment.

Result: TaoSR1 significantly outperforms baselines on offline datasets and achieves substantial gains in online side-by-side human evaluations.

Conclusion: TaoSR1 significantly outperforms baselines on offline datasets and achieves substantial gains in online side-by-side human evaluations, introducing a novel paradigm for applying CoT reasoning to relevance classification.

Abstract: Query-product relevance prediction is a core task in e-commerce search.
BERT-based models excel at semantic matching but lack complex reasoning
capabilities. While Large Language Models (LLMs) are explored, most still use
discriminative fine-tuning or distill to smaller models for deployment. We
propose a framework to directly deploy LLMs for this task, addressing key
challenges: Chain-of-Thought (CoT) error accumulation, discriminative
hallucination, and deployment feasibility. Our framework, TaoSR1, involves
three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;
(2) Offline sampling with a pass@N strategy and Direct Preference Optimization
(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling
with Group Relative Policy Optimization (GRPO) to mitigate discriminative
hallucination. Additionally, post-CoT processing and a cumulative
probability-based partitioning method enable efficient online deployment.
TaoSR1 significantly outperforms baselines on offline datasets and achieves
substantial gains in online side-by-side human evaluations, introducing a novel
paradigm for applying CoT reasoning to relevance classification.

</details>


### [49] [LLM-Enhanced Linear Autoencoders for Recommendation](https://arxiv.org/abs/2508.13500)
*Jaewan Moon,Seongmin Park,Jongwuk Lee*

Main category: cs.IR

TL;DR: L3AE是第一个将大型语言模型（LLMs）集成到线性自编码器（LAE）框架中的方法，通过两阶段优化策略有效整合了文本语义和用户-项目交互的异构知识，实验结果表明其在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的线性自编码器（LAEs）依赖于稀疏的词共现模式，限制了它们捕捉丰富文本语义的能力。为了应对这一问题，我们提出了L3AE，这是第一个将LLMs集成到LAE框架中的方法。

Method: L3AE通过两阶段优化策略有效地整合了文本语义和用户-项目交互的异构知识。第一阶段从LLM派生的项目表示中构建语义项目到项目的相关矩阵。第二阶段从协作信号中学习项目到项目的权重矩阵，并将语义项目相关性作为正则化进行提炼。

Result: L3AE在三个基准数据集上 consistently 超过最先进的LLM增强模型，Recall@20和NDCG@20分别提高了27.6%和39.3%。

Conclusion: L3AE在三个基准数据集上 consistently 超过最先进的LLM增强模型，Recall@20和NDCG@20分别提高了27.6%和39.3%。

Abstract: Large language models (LLMs) have been widely adopted to enrich the semantic
representation of textual item information in recommender systems. However,
existing linear autoencoders (LAEs) that incorporate textual information rely
on sparse word co-occurrence patterns, limiting their ability to capture rich
textual semantics. To address this, we propose L3AE, the first integration of
LLMs into the LAE framework. L3AE effectively integrates the heterogeneous
knowledge of textual semantics and user-item interactions through a two-phase
optimization strategy. (i) L3AE first constructs a semantic item-to-item
correlation matrix from LLM-derived item representations. (ii) It then learns
an item-to-item weight matrix from collaborative signals while distilling
semantic item correlations as regularization. Notably, each phase of L3AE is
optimized through closed-form solutions, ensuring global optimality and
computational efficiency. Extensive experiments demonstrate that L3AE
consistently outperforms state-of-the-art LLM-enhanced models on three
benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.
The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [50] [Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for Bias Detection](https://arxiv.org/abs/2508.13187)
*Jonathan A. Karr Jr.,Benjamin F. Herbst,Ting Hua,Matthew Hauenstein,Georgina Curto,Nitesh V. Chawla*

Main category: cs.CY

TL;DR: 本研究利用NLP和LLMs识别和衡量数字空间中的PEH社会偏见，并展示了LLMs在分类任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于在线和市议会讨论反映了并影响了公众意见，因此提供了识别和跟踪社会偏见的有价值见解。本研究致力于通过行动影响公众意见来缓解无家可归问题。

Method: 本研究利用自然语言处理（NLP）和大型语言模型（LLMs）来识别和衡量数字空间中表达的PEH社会偏见。我们评估了LLMs作为分类器的能力，并应用了零样本和少样本分类技术。

Result: 研究发现，尽管本地LLM在零样本分类中存在显著不一致，但本地LLM的上下文学习分类得分接近封闭源LLM的分类得分。此外，LLM在所有类别中的平均表现优于BERT。

Conclusion: 本研究旨在提高对PEH（无家可归者）普遍偏见的认识，开发新的指标来指导政策，并最终增强生成式AI技术的公平性和伦理应用。

Abstract: Homelessness is a persistent social challenge, impacting millions worldwide.
Over 770,000 people experienced homelessness in the U.S. in 2024. Social
stigmatization is a significant barrier to alleviation, shifting public
perception, and influencing policymaking. Given that online and city council
discourse reflect and influence part of public opinion, it provides valuable
insights to identify and track social biases. This research contributes to
alleviating homelessness by acting on public opinion. It introduces novel
methods, building on natural language processing (NLP) and large language
models (LLMs), to identify and measure PEH social bias expressed in digital
spaces. We present a new, manually-annotated multi-modal dataset compiled from
Reddit, X (formerly Twitter), news articles, and city council meeting minutes
across 10 U.S. cities. This unique dataset provides evidence of the typologies
of homelessness bias described in the literature. In order to scale up and
automate the detection of homelessness bias online, we evaluate LLMs as
classifiers. We applied both zero-shot and few-shot classification techniques
to this data. We utilized local LLMs (Llama 3.2 3B Instruct, Qwen 2.5 7B
Instruct, and Phi4 Instruct Mini) as well as closed-source API models (GPT-4.1,
Gemini 2.5 Pro, and Grok-4). Our findings reveal that although there are
significant inconsistencies in local LLM zero-shot classification, the
in-context learning classification scores of local LLMs approach the
classification scores of closed-source LLMs. Furthermore, LLMs outperform BERT
when averaging across all categories. This work aims to raise awareness about
the pervasive bias against PEH, develop new indicators to inform policy, and
ultimately enhance the fairness and ethical application of Generative AI
technologies.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 156]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.IR](#cs.IR) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation](https://arxiv.org/abs/2510.09671)
*Wei Zhou,Bolei Ma,Annemarie Friedrich,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 本文是一篇关于表格问答（TQA）研究的综述，重点在于基于大语言模型的方法，旨在提供一个全面且结构化的概述，以帮助理解和指导该领域的未来发展。


<details>
  <summary>Details</summary>
Motivation: 尽管最近在大型语言模型（LLMs）方面的进展在TQA中取得了显著进展，但该领域仍缺乏对任务形式、核心挑战和方法趋势的系统组织和理解，特别是在强化学习等新兴研究方向的背景下。

Method: 本文提供了一个全面且结构化的TQA研究概述，重点是基于大语言模型（LLM）的方法。对现有的基准和任务设置进行了全面分类，并根据它们针对的挑战对当前建模策略进行分组分析。

Result: 本文提供了对TQA研究的全面概述，包括对现有基准和任务设置的分类，对当前建模策略的分析，以及对未被系统覆盖的有时间性的主题的强调。

Conclusion: 本文通过统一分散的研究方向并识别开放问题，为TQA社区提供了一个综合的基础，有助于更深入地理解当前的最新技术，并指导这一快速发展的领域的未来发展。

Abstract: Table Question Answering (TQA) aims to answer natural language questions
about tabular data, often accompanied by additional contexts such as text
passages. The task spans diverse settings, varying in table representation,
question/answer complexity, modality involved, and domain. While recent
advances in large language models (LLMs) have led to substantial progress in
TQA, the field still lacks a systematic organization and understanding of task
formulations, core challenges, and methodological trends, particularly in light
of emerging research directions such as reinforcement learning. This survey
addresses this gap by providing a comprehensive and structured overview of TQA
research with a focus on LLM-based methods. We provide a comprehensive
categorization of existing benchmarks and task setups. We group current
modeling strategies according to the challenges they target, and analyze their
strengths and limitations. Furthermore, we highlight underexplored but timely
topics that have not been systematically covered in prior research. By unifying
disparate research threads and identifying open problems, our survey offers a
consolidated foundation for the TQA community, enabling a deeper understanding
of the state of the art and guiding future developments in this rapidly
evolving area.

</details>


### [2] [Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection](https://arxiv.org/abs/2510.09695)
*Yanran Chen,Lynn Greschner,Roman Klinger,Michael Klenk,Steffen Eger*

Main category: cs.CL

TL;DR: 本研究探讨了情感框架如何与谬误和说服力相互作用，并发现LLM驱动的情感框架会降低人类检测谬误的能力。


<details>
  <summary>Details</summary>
Motivation: 逻辑谬误在公共传播中很常见，可能会误导受众；谬误论点可能仍然具有说服力，尽管缺乏合理性，因为说服力本质上是主观的。

Method: 我们使用大型语言模型（LLMs）系统地改变谬误论证中的情感诉求，并在人类研究中生成刺激物。

Result: LLM驱动的情感框架平均减少了14.5%的人类谬误检测F1分数。当感知到愉悦时，人类在谬误检测中表现更好，而愉悦、恐惧和悲伤这三种情绪也与显著更高的说服力相关。

Conclusion: 我们的工作对AI驱动的谬误论证中的情感操纵具有重要意义。

Abstract: Logical fallacies are common in public communication and can mislead
audiences; fallacious arguments may still appear convincing despite lacking
soundness, because convincingness is inherently subjective. We present the
first computational study of how emotional framing interacts with fallacies and
convincingness, using large language models (LLMs) to systematically change
emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting
emotional appeal into fallacious arguments while preserving their logical
structures, then use the best models to generate stimuli for a human study. Our
results show that LLM-driven emotional framing reduces human fallacy detection
in F1 by 14.5% on average. Humans perform better in fallacy detection when
perceiving enjoyment than fear or sadness, and these three emotions also
correlate with significantly higher convincingness compared to neutral or other
emotion states. Our work has implications for AI-driven emotional manipulation
in the context of fallacious argumentation.

</details>


### [3] [The Idola Tribus of AI: Large Language Models tend to perceive order where none exists](https://arxiv.org/abs/2510.09709)
*Shin-nosuke Ishikawa,Masato Todo,Taiki Ogihara,Hirotsugu Ohba*

Main category: cs.CL

TL;DR: 大型语言模型在处理需要逻辑推理的任务时可能会错误地识别不存在的模式，这表明它们在实际应用中存在局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管已经提出了多种方法将大型语言模型应用于复杂的现实任务，但这些方法依赖于模型的逻辑一致性和自我一致性，因此评估这些方面并考虑潜在的对策至关重要。

Method: 研究人员进行了一项实验，要求大型语言模型解释各种整数序列中的模式，包括算术序列和随机生成的整数序列。

Result: 模型在处理算术和几何序列时能够正确识别正确的模式，但在分析随机生成的序列时经常错误地识别不一致的模式。

Conclusion: 该研究揭示了大型语言模型在处理需要逻辑推理的实用任务时可能存在的局限性，即使它们采用了思维链推理机制。

Abstract: We present a tendency of large language models (LLMs) to generate absurd
patterns despite their clear inappropriateness in a simple task of identifying
regularities in number series. Several approaches have been proposed to apply
LLMs to complex real-world tasks, such as providing knowledge through
retrieval-augmented generation and executing multi-step tasks using AI agent
frameworks. However, these approaches rely on the logical consistency and
self-coherence of LLMs, making it crucial to evaluate these aspects and
consider potential countermeasures. To identify cases where LLMs fail to
maintain logical consistency, we conducted an experiment in which LLMs were
asked to explain the patterns in various integer sequences, ranging from
arithmetic sequences to randomly generated integer series. While the models
successfully identified correct patterns in arithmetic and geometric sequences,
they frequently over-recognized patterns that were inconsistent with the given
numbers when analyzing randomly generated series. This issue was observed even
in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini
2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can
be interpreted as the AI model equivalent of Idola Tribus and highlights
potential limitations in their capability for applied tasks requiring logical
reasoning, even when employing chain-of-thought reasoning mechanisms.

</details>


### [4] [SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG](https://arxiv.org/abs/2510.09710)
*Xiaonan Si,Meilin Zhu,Simeng Qin,Lijia Yu,Lijun Zhang,Shuaitong Liu,Xinfeng Li,Ranjie Duan,Yang Liu,Xiaojun Jia*

Main category: cs.CL

TL;DR: 本文提出了一个两阶段的语义过滤和无冲突框架，用于提高RAG系统的可信度。通过这种方式，SeCon-RAG有效保留了有用的知识，同时减轻了冲突污染，并在生成鲁棒性和输出可信度方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法通常应用激进的过滤，导致不必要的有价值信息损失和生成可靠性降低。为了应对这个问题，我们提出了一个两阶段的语义过滤和无冲突框架，以提高RAG系统的可信度。

Method: 我们提出了一个两阶段的语义过滤和无冲突框架，用于可信的RAG。在第一阶段，我们进行联合过滤，结合语义和基于聚类的过滤，由实体-意图-关系提取器（EIRE）引导。EIRE从用户查询和过滤后的文档中提取实体、潜在目标和实体关系，对它们的语义相关性进行评分，并选择性地将有价值的文档添加到干净的检索数据库中。在第二阶段，我们提出了一种EIRE引导的冲突感知过滤模块，该模块在最终答案生成前分析查询、候选答案和检索知识之间的语义一致性，过滤掉可能误导模型的内部和外部矛盾。

Result: 通过这种两阶段过程，SeCon-RAG有效地保留了有用的知识，同时减轻了冲突污染，在生成鲁棒性和输出可信度方面取得了显著提升。广泛的实验表明，所提出的SeCon-RAG明显优于最先进的防御方法。

Conclusion: 通过这种两阶段过程，SeCon-RAG有效地保留了有用的知识，同时减轻了冲突污染，在生成鲁棒性和输出可信度方面取得了显著提升。广泛的实验表明，所提出的SeCon-RAG明显优于最先进的防御方法。

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) with external knowledge but are vulnerable to corpus poisoning and
contamination attacks, which can compromise output integrity. Existing defenses
often apply aggressive filtering, leading to unnecessary loss of valuable
information and reduced reliability in generation. To address this problem, we
propose a two-stage semantic filtering and conflict-free framework for
trustworthy RAG. In the first stage, we perform a joint filter with semantic
and cluster-based filtering which is guided by the Entity-intent-relation
extractor (EIRE). EIRE extracts entities, latent objectives, and entity
relations from both the user query and filtered documents, scores their
semantic relevance, and selectively adds valuable documents into the clean
retrieval database. In the second stage, we proposed an EIRE-guided
conflict-aware filtering module, which analyzes semantic consistency between
the query, candidate answers, and retrieved knowledge before final answer
generation, filtering out internal and external contradictions that could
mislead the model. Through this two-stage process, SeCon-RAG effectively
preserves useful knowledge while mitigating conflict contamination, achieving
significant improvements in both generation robustness and output
trustworthiness. Extensive experiments across various LLMs and datasets
demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art
defense methods.

</details>


### [5] [ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models](https://arxiv.org/abs/2510.09711)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Lingbing Guo,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为ReaLM的新框架，通过残差向量量化机制弥合KG嵌入与LLM分词之间的差距。ReaLM将预训练KG嵌入离散化为紧凑的代码序列，并将其作为可学习的标记集成到LLM词汇表中，实现了符号和上下文知识的无缝融合。此外，还引入了基于本体的类别约束以强制语义一致性，根据类别级兼容性优化实体预测。在两个广泛使用的基准数据集上的实验表明，ReaLM取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods often struggle to fully exploit structured semantic representations, as the continuous embedding space of pretrained KG models is fundamentally misaligned with the discrete token space of LLMs. This discrepancy hinders effective semantic transfer and limits their performance.

Method: ReaLM, a novel and effective framework that bridges the gap between KG embeddings and LLM tokenization through the mechanism of residual vector quantization. ReaLM discretizes pretrained KG embeddings into compact code sequences and integrates them as learnable tokens within the LLM vocabulary, enabling seamless fusion of symbolic and contextual knowledge. Furthermore, ontology-guided class constraints are incorporated to enforce semantic consistency, refining entity predictions based on class-level compatibility.

Result: Extensive experiments on two widely used benchmark datasets demonstrate that ReaLM achieves state-of-the-art performance.

Conclusion: ReaLM achieves state-of-the-art performance, confirming its effectiveness in aligning structured knowledge with large-scale language models.

Abstract: Large Language Models (LLMs) have recently emerged as a powerful paradigm for
Knowledge Graph Completion (KGC), offering strong reasoning and generalization
capabilities beyond traditional embedding-based approaches. However, existing
LLM-based methods often struggle to fully exploit structured semantic
representations, as the continuous embedding space of pretrained KG models is
fundamentally misaligned with the discrete token space of LLMs. This
discrepancy hinders effective semantic transfer and limits their performance.
To address this challenge, we propose ReaLM, a novel and effective framework
that bridges the gap between KG embeddings and LLM tokenization through the
mechanism of residual vector quantization. ReaLM discretizes pretrained KG
embeddings into compact code sequences and integrates them as learnable tokens
within the LLM vocabulary, enabling seamless fusion of symbolic and contextual
knowledge. Furthermore, we incorporate ontology-guided class constraints to
enforce semantic consistency, refining entity predictions based on class-level
compatibility. Extensive experiments on two widely used benchmark datasets
demonstrate that ReaLM achieves state-of-the-art performance, confirming its
effectiveness in aligning structured knowledge with large-scale language
models.

</details>


### [6] [All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language](https://arxiv.org/abs/2510.09714)
*Shiyuan Guo,Henry Sleight,Fabien Roger*

Main category: cs.CL

TL;DR: 本文研究了模型能否进行加密推理，并发现即使最先进的模型在处理不常见的加密方式时也会遇到困难，这表明使用加密推理来逃避CoT监控可能无效。


<details>
  <summary>Details</summary>
Motivation: 检测有害AI行为很重要，因为AI代理越来越被采用。链式思维（CoT）监控是用于检测对抗攻击和AI不对齐的一种方法。然而，攻击者和不对齐的模型可能会通过加密推理来逃避CoT监控。

Method: 我们测试了模型是否能够进行加密推理，对28种不同的密码进行了微调和提示，最多10个模型进行推理。我们测量了模型在数学问题上的准确性作为推理能力的代理。

Result: 我们发现模型在加密文本中推理时准确性可能会显著下降，尽管模型能够准确地将加密文本翻译成英语。即使是最先进的模型在处理不太常见的密码时也会遇到困难，尽管它们可以在像rot13这样的常见密码中准确推理。

Conclusion: 我们的工作表明，使用加密推理来逃避CoT监控对于当前模型可能是一种无效的策略，并为未来前沿模型的发展提供了约束这种能力的指导。

Abstract: Detecting harmful AI actions is important as AI agents gain adoption.
Chain-of-thought (CoT) monitoring is one method widely used to detect
adversarial attacks and AI misalignment. However, attackers and misaligned
models might evade CoT monitoring through ciphered reasoning: reasoning hidden
in encrypted, translated, or compressed text. To assess this risk, we test
whether models can perform ciphered reasoning. For each of 28 different
ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We
measure model accuracy on math problems as a proxy for reasoning ability.
Across the models we test, we find an asymmetry: model accuracy can drop
significantly when reasoning in ciphered text, even though models demonstrate
comprehension of ciphered text by being able to translate it accurately to
English. Even frontier models struggle with lesser-known ciphers, although they
can reason accurately in well-known ciphers like rot13. We show that ciphered
reasoning capability correlates with cipher prevalence in pretraining data. We
also identify scaling laws showing that ciphered reasoning capability improves
slowly with additional fine-tuning data. Our work suggests that evading CoT
monitoring using ciphered reasoning may be an ineffective tactic for current
models and offers guidance on constraining the development of this capability
in future frontier models.

</details>


### [7] [Preference-Aware Memory Update for Long-Term LLM Agents](https://arxiv.org/abs/2510.09720)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 本文提出了一种偏好感知的记忆更新机制（PAMU），通过结合滑动窗口平均值和指数移动平均值，实现了动态和个性化的记忆优化，并在多个任务场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在记忆更新方面存在不足，缺乏根据用户行为和上下文动态优化偏好记忆表示的机制。

Method: 我们提出了一个偏好感知的记忆更新机制（PAMU），通过集成滑动窗口平均值（SW）和指数移动平均值（EMA），构建了一个融合的偏好感知表示。

Result: 我们在LoCoMo数据集的五个任务场景中进行了实验，结果表明我们的机制可以显著提高LLM的输出质量。

Conclusion: 我们的机制在长期对话中验证了其有效性。

Abstract: One of the key factors influencing the reasoning capabilities of LLM-based
agents is their ability to leverage long-term memory. Integrating long-term
memory mechanisms allows agents to make informed decisions grounded in
historical interactions. While recent advances have significantly improved the
storage and retrieval components, by encoding memory into dense vectors for
similarity search or organizing memory as structured knowledge graphs most
existing approaches fall short in memory updating. In particular, they lack
mechanisms for dynamically refining preference memory representations in
response to evolving user behaviors and contexts. To address this gap, we
propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic
and personalized memory refinement. By integrating sliding window averages (SW)
with exponential moving averages (EMA), PAMU constructs a fused
preference-aware representation that captures both short-term fluctuations and
long-term user tendencies. We conduct experiments on five task scenarios of the
LoCoMo dataset, and the results show that our mechanism can significantly
improve the output quality of LLM in five baselines, validating its
effectiveness in long-term conversations.

</details>


### [8] [Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation](https://arxiv.org/abs/2510.09722)
*Fanwei Zhu,Jinke Yu,Zulong Chen,Ying Zhou,Junhao Ji,Zhibo Yang,Yuxue Zhang,Haoyuan Hu,Zhenghao Liu*

Main category: cs.CL

TL;DR: 本文提出了一种布局感知且优化效率的自动化简历信息提取和评估框架，解决了简历布局异质性、LLM成本和延迟以及缺乏标准数据集的问题，并在实际应用中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 自动化简历信息提取对于扩展人才获取至关重要，但其实际部署面临三个主要挑战：简历布局和内容的极端异质性、大型语言模型（LLMs）的高成本和延迟，以及缺乏标准化的数据集和评估工具。

Method: 我们提出了一个布局感知且优化效率的自动化提取和评估框架，结合了微调的布局解析器、基于并行提示和指令微调的高效推理LLM提取器，以及由新基准数据集支持的稳健的两阶段自动化评估框架。

Result: 我们的框架在准确性和效率方面都显著优于强基线。特别是，我们展示了微调的紧凑型0.6B LLM在显著降低推理延迟和计算成本的同时实现了顶级准确性。

Conclusion: 我们的框架在准确性和效率方面都显著优于强基线，且已在阿里巴巴的智能人力资源平台中全面部署，支持其业务单元中的实时应用。

Abstract: Automated resume information extraction is critical for scaling talent
acquisition, yet its real-world deployment faces three major challenges: the
extreme heterogeneity of resume layouts and content, the high cost and latency
of large language models (LLMs), and the lack of standardized datasets and
evaluation tools. In this work, we present a layout-aware and
efficiency-optimized framework for automated extraction and evaluation that
addresses all three challenges. Our system combines a fine-tuned layout parser
to normalize diverse document formats, an inference-efficient LLM extractor
based on parallel prompting and instruction tuning, and a robust two-stage
automated evaluation framework supported by new benchmark datasets. Extensive
experiments show that our framework significantly outperforms strong baselines
in both accuracy and efficiency. In particular, we demonstrate that a
fine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly
reducing inference latency and computational cost. The system is fully deployed
in Alibaba's intelligent HR platform, supporting real-time applications across
its business units.

</details>


### [9] [VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09733)
*Yubo Sun,Chunyi Peng,Yukun Yan,Shi Yu,Zhenghao Liu,Chi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: EVisRAG is a new framework that enhances visual retrieval-augmented generation by effectively integrating evidence from multiple images, leading to significant improvements in accuracy.


<details>
  <summary>Details</summary>
Motivation: Current VRAG systems often fail to reliably perceive and integrate evidence across multiple images, leading to weak grounding and erroneous conclusions.

Method: EVisRAG, an end-to-end framework that learns to reason with evidence-guided multi-image, and Reward-Scoped Group Relative Policy Optimization (RS-GRPO).

Result: EVisRAG improves answer accuracy by precisely perceiving and localizing question-relevant evidence across multiple images and deriving the final answer from that evidence.

Conclusion: EVisRAG delivers substantial end-to-end gains over backbone VLM with 27% improvements on average.

Abstract: Visual retrieval-augmented generation (VRAG) augments vision-language models
(VLMs) with external visual knowledge to ground reasoning and reduce
hallucinations. Yet current VRAG systems often fail to reliably perceive and
integrate evidence across multiple images, leading to weak grounding and
erroneous conclusions. In this paper, we propose EVisRAG, an end-to-end
framework that learns to reason with evidence-guided multi-image to address
this issue. The model first observes retrieved images and records per-image
evidence, then derives the final answer from the aggregated evidence. To train
EVisRAG effectively, we introduce Reward-Scoped Group Relative Policy
Optimization (RS-GRPO), which binds fine-grained rewards to scope-specific
tokens to jointly optimize visual perception and reasoning abilities of VLMs.
Experimental results on multiple visual question answering benchmarks
demonstrate that EVisRAG delivers substantial end-to-end gains over backbone
VLM with 27\% improvements on average. Further analysis shows that, powered by
RS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and
localizing question-relevant evidence across multiple images and deriving the
final answer from that evidence, much like a real detective.

</details>


### [10] [Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement](https://arxiv.org/abs/2510.09738)
*Steve Han,Gilberto Titericz Junior,Tom Balough,Wenfei Zhou*

Main category: cs.CL

TL;DR: 本研究引入了一种新的两步方法来评估大型语言模型作为裁判的能力，并发现模型的卓越性不仅取决于大小，还取决于训练策略。


<details>
  <summary>Details</summary>
Motivation: 传统相关性分析不足以评估LLMs作为裁判的能力，因此需要一种更全面的方法来衡量实际的一致性模式。

Method: 本研究采用两步方法：(1) 相关性测试，筛选出与人类判断高度对齐的裁判；(2) 使用z分数的人类相似性测试，识别两种不同的判断模式：类似人类的判断（|z| < 1）和超级一致的判断（z > 1）。

Result: 27个测试的LLMs达到Tier 1性能，其中23个模型表现出类似人类的判断模式，而4个模型表现出超级一致的行为。此外，裁判卓越性不仅取决于模型大小，还取决于特定的训练策略。

Conclusion: 本研究引入了Judge's Verdict Benchmark，这是一种新颖的两步方法，用于评估大型语言模型（LLMs）作为响应准确性评估任务的裁判。我们的方法揭示了54个测试的LLMs中有27个达到Tier 1性能，其中23个模型表现出类似人类的判断模式，而4个模型表现出超级一致的行为。我们证明了裁判卓越性不仅取决于模型大小，还取决于特定的训练策略。

Abstract: This research introduces the Judge's Verdict Benchmark, a novel two-step
methodology to evaluate Large Language Models (LLMs) as judges for response
accuracy evaluation tasks. We assess how well 54 LLMs can replicate human
judgment when scoring responses from RAG (Retrieval-Augmented Generation) or
Agentic pipelines against ground truth answers. Our methodology progresses from
traditional correlation analysis to comprehensive Cohen's Kappa analysis that
measures actual agreement patterns. The two-step approach includes: (1) a
correlation test that filters judges with strong alignment, followed by (2) a
human-likeness test using z-scores to identify two distinct judgment patterns:
human-like judgment (|z| < 1) that mimics natural human variation, and
super-consistent judgment (z > 1) that exceeds typical human-to-human agreement
levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1
performance: 23 models exhibit human-like patterns that preserve the nuances of
human judgment, while 4 models demonstrate super-consistent behavior, a pattern
that could indicate either enhanced reliability or oversimplification of
complex judgments. Testing 43 open-source models (1B-405B parameters) and 11
closed models (GPT, Gemini, Claude variants), we demonstrate that judge
excellence is not solely dependent on model size but on specific training
strategies. Our key contributions include: (1) establishing that correlation
alone is insufficient for judge evaluation, (2) introducing a "Turing Test for
judges" based on agreement patterns, and (3) providing a standardized benchmark
for classifying LLM judges into distinct performance tiers for different
evaluation needs.

</details>


### [11] [Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning](https://arxiv.org/abs/2510.09770)
*Adam Byerly,Daniel Khashabi*

Main category: cs.CL

TL;DR: 本文提出了一种利用位置偏差的框架Gold Panning Bandits，在不重新训练模型的情况下，通过重新排序文档，提高了多文档上下文中的信息检索效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法将位置偏差视为需要减轻的噪声，而本文认为位置偏差可以被利用来提高效率和性能。

Method: 本文引入了Gold Panning Bandits框架，利用位置偏差作为诊断信号，通过重新排序文档并观察模型响应的变化，从而高效地识别最相关的内容。我们把选择重新排序的问题建模为二分图匹配问题，并提出了一种贪心策略，以较低的时间复杂度实现与最优解相当的性能。

Result: 我们的方法在知识密集型NLP任务中，使用最多减少65%的语言模型查询次数，显著降低了计算成本，且无需重新训练模型。

Conclusion: 本文展示了如何将大型语言模型（LLM）的固有偏差从缺点转化为在推理时优化的资产。

Abstract: Large language models exhibit a strong position bias in multi-document
contexts, systematically prioritizing information based on location rather than
relevance. While existing approaches treat this bias as noise to be mitigated,
we introduce Gold Panning Bandits, a framework that leverages position bias as
a diagnostic signal: by reordering documents and observing shifts in the
model's responses, we can efficiently identify the most relevant content. We
frame the problem of choosing reorderings as a bipartite matching problem.
While an optimal assignment can be computed at each iteration with the
Hungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \log N)$
strategy that achieves comparable performance by prioritizing the placement of
the most uncertain documents in the most informative positions. Our approach
identifies relevant documents using up to 65\% fewer language model queries
than random permutation baselines on knowledge-intensive NLP tasks,
substantially reducing computational cost without model retraining. This work
demonstrates that inherent LLM biases can be transformed from liabilities into
assets for efficient, inference-time optimization.

</details>


### [12] [PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.09771)
*Rakib Hossan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: PromptGuard is a few-shot framework that combines chi-square statistical analysis for keyword extraction with adaptive majority voting for decision-making. It outperforms n-gram baselines and random approaches in Bengali hate speech classification.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised approaches need extensive labeled datasets that are expensive for low-resource languages.

Method: We developed PromptGuard, a few-shot framework combining chi-square statistical analysis for keyword extraction with adaptive majority voting for decision-making.

Result: Chi-square keywords provide consistent improvements across categories, while adaptive voting benefits ambiguous cases requiring extended classification rounds.

Conclusion: PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines (60.75) and random approaches (14.65). Ablation studies confirm chi-square-based keywords show the most consistent impact across all categories.

Abstract: The BLP-2025 Task 1A requires Bengali hate speech classification into six
categories. Traditional supervised approaches need extensive labeled datasets
that are expensive for low-resource languages. We developed PromptGuard, a
few-shot framework combining chi-square statistical analysis for keyword
extraction with adaptive majority voting for decision-making. We explore
statistical keyword selection versus random approaches and adaptive voting
mechanisms that extend classification based on consensus quality. Chi-square
keywords provide consistent improvements across categories, while adaptive
voting benefits ambiguous cases requiring extended classification rounds.
PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines
(60.75) and random approaches (14.65). Ablation studies confirm
chi-square-based keywords show the most consistent impact across all
categories.

</details>


### [13] [Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models](https://arxiv.org/abs/2510.09790)
*Michael Freenor,Lauren Alvarez*

Main category: cs.CL

TL;DR: 本文提出了一种几何方法RISE，用于分析语言和嵌入模型如何编码语义关系，并展示了其在多语言环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 理解语言和嵌入模型如何编码语义关系对于模型可解释性和控制至关重要。早期的词嵌入表现出直观的向量算术，而现代高维文本表示缺乏直接可解释的几何属性。

Method: 本文引入了Rotor-Invariant Shift Estimation (RISE)，这是一种几何方法，将语义变换表示为嵌入空间中的一致旋转操作，利用现代语言表示的流形结构。

Result: RISE能够跨语言和模型进行高性能的转移，表明存在类似的跨语言几何结构。评估结果表明，RISE能够一致地映射具有不同语法特征的语篇级语义变换（例如否定和条件性）在不同语言和模型之间。

Conclusion: 本文首次系统地证明了话语级语义变换在多语言嵌入空间中对应于一致的几何操作，实证支持了句子级别的线性表示假设。

Abstract: Understanding how language and embedding models encode semantic relationships
is fundamental to model interpretability and control. While early word
embeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +
''woman'' = ''queen''), modern high-dimensional text representations lack
straightforward interpretable geometric properties. We introduce
Rotor-Invariant Shift Estimation (RISE), a geometric approach that represents
semantic transformations as consistent rotational operations in embedding
space, leveraging the manifold structure of modern language representations.
RISE operations have the ability to operate across both languages and models
with high transfer of performance, suggesting the existence of analogous
cross-lingual geometric structure. We evaluate RISE across three embedding
models, three datasets, and seven morphologically diverse languages in five
major language groups. Our results demonstrate that RISE consistently maps
discourse-level semantic transformations with distinct grammatical features
(e.g., negation and conditionality) across languages and models. This work
provides the first systematic demonstration that discourse-level semantic
transformations correspond to consistent geometric operations in multilingual
embedding spaces, empirically supporting the Linear Representation Hypothesis
at the sentence level.

</details>


### [14] [Text Prompt Injection of Vision Language Models](https://arxiv.org/abs/2510.09849)
*Ruizhe Zhu*

Main category: cs.CL

TL;DR: 本文研究了文本提示注入作为一种有效且简单的攻击方法，用于误导大型视觉语言模型，并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 由于大型视觉语言模型的广泛应用，安全问题日益突出。因此，本文旨在研究一种简单而有效的攻击方法，以揭示潜在的安全风险。

Method: 本文开发了一种针对文本提示注入的算法，并通过实验验证了其效果。

Result: 实验结果表明，本文提出的攻击方法在大型模型上具有较高的有效性，且对计算资源的需求较低。

Conclusion: 本文研究了文本提示注入作为一种有效且简单的误导大型视觉语言模型的方法，并通过实验验证了其有效性与效率。相比其他攻击方法，该方法在大型模型上表现尤为出色，且对计算资源的需求不高。

Abstract: The widespread application of large vision language models has significantly
raised safety concerns. In this project, we investigate text prompt injection,
a simple yet effective method to mislead these models. We developed an
algorithm for this type of attack and demonstrated its effectiveness and
efficiency through experiments. Compared to other attack methods, our approach
is particularly effective for large models without high demand for
computational resources.

</details>


### [15] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi,Zheyuan Zhang,Zhengqing Yuan,Keerthiram Murugesan,Vincent Galass,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 本文介绍了NG-Router，这是一种新的框架，将营养QA建模为监督的、知识图谱引导的多智能体协作问题。通过整合代理节点到异构知识图谱并使用图神经网络学习任务感知的路由分布，NG-Router能够解决上下文过载问题，并在多个基准和模型上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的方法面临两个根本性挑战：单智能体系统的有限推理能力，以及设计有效多智能体架构的复杂性，以及阻碍准确决策的上下文过载。

Method: NG-Router将营养QA建模为一个监督的、知识图谱引导的多智能体协作问题。它将代理节点整合到异构知识图谱中，并使用图神经网络在代理上学习任务感知的路由分布，利用从经验代理性能中得出的软监督。为了进一步解决上下文过载问题，我们提出了一种基于梯度的子图检索机制，以在训练期间识别显著证据，从而增强多跳和关系推理。

Result: NG-Router在多个基准和骨干模型上的实验结果表明，它 consistently 超过单智能体和集成基线，为复杂的营养健康任务提供了一种原则性的多智能体推理方法。

Conclusion: NG-Router在多个基准和骨干模型上的实验结果表明，它 consistently 超过单智能体和集成基线，为复杂的营养健康任务提供了一种原则性的多智能体推理方法。

Abstract: Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.

</details>


### [16] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton,Matthew Wilkens,Andrew Piper*

Main category: cs.CL

TL;DR: NarraBench 是一个理论指导的叙事理解任务分类法，以及对该领域 78 个现有基准的调查，研究发现当前评估方法在覆盖叙事理解的某些方面上存在不足，需要新的评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前的评估方法在覆盖叙事理解的某些方面上存在不足，尤其是那些主观性和视角性较强的方面，因此需要新的评估方法来改进这一状况。

Method: 通过构建一个理论指导的叙事理解任务分类法，并对 78 个现有的基准进行调查，分析了当前评估方法的不足之处。

Result: 研究发现，只有 27% 的叙事任务被现有的基准充分覆盖，而一些领域如叙事事件、风格、视角和揭示等几乎未被纳入评估中。

Conclusion: NarraBench 提供了一个理论指导的叙事理解任务分类法，以及对该领域 78 个现有基准的全面调查。研究发现，当前工作在某些方面存在不足，需要新的评估方法来覆盖这些被忽视或与现有指标不匹配的方面。

Abstract: We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.

</details>


### [17] [CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs](https://arxiv.org/abs/2510.09871)
*Nafiseh Nikeghbal,Amir Hossein Kargaran,Jana Diesner*

Main category: cs.CL

TL;DR: CoBia是一种用于检测大型语言模型在对话中偏见行为的工具，研究表明LLMs在面对带有偏见的问题时常常表现不佳。


<details>
  <summary>Details</summary>
Motivation: 为了系统地分析LLMs在对话中偶尔表现出有害行为（如表达种族主义观点）的现象，需要一种方法来识别和评估这些行为的条件。

Method: 引入CoBia，这是一种轻量级对抗性攻击套件，用于系统地分析LLMs在对话中偏离规范或伦理行为的条件。CoBia创建了一个构造的对话，其中模型发表了关于社会群体的偏见声明，并评估模型是否能从虚构的偏见声明中恢复并拒绝带有偏见的后续问题。

Result: 评估结果显示，通过精心构建的对话可以可靠地揭示偏见放大现象，并且LLMs通常无法拒绝带有偏见的后续问题。

Conclusion: CoBia能够可靠地揭示偏见放大，并且LLMs通常在对话中未能拒绝带有偏见的后续问题。这种压力测试突显了可以通过互动揭示的深层次偏见。

Abstract: Improvements in model construction, including fortified safety guardrails,
allow Large language models (LLMs) to increasingly pass standard safety checks.
However, LLMs sometimes slip into revealing harmful behavior, such as
expressing racist viewpoints, during conversations. To analyze this
systematically, we introduce CoBia, a suite of lightweight adversarial attacks
that allow us to refine the scope of conditions under which LLMs depart from
normative or ethical behavior in conversations. CoBia creates a constructed
conversation where the model utters a biased claim about a social group. We
then evaluate whether the model can recover from the fabricated bias claim and
reject biased follow-up questions. We evaluate 11 open-source as well as
proprietary LLMs for their outputs related to six socio-demographic categories
that are relevant to individual safety and fair treatment, i.e., gender, race,
religion, nationality, sex orientation, and others. Our evaluation is based on
established LLM-based bias metrics, and we compare the results against human
judgments to scope out the LLMs' reliability and alignment. The results suggest
that purposefully constructed conversations reliably reveal bias amplification
and that LLMs often fail to reject biased follow-up questions during dialogue.
This form of stress-testing highlights deeply embedded biases that can be
surfaced through interaction. Code and artifacts are available at
https://github.com/nafisenik/CoBia.

</details>


### [18] [iBERT: Interpretable Style Embeddings via Sense Decomposition](https://arxiv.org/abs/2510.09882)
*Vishal Anand,Milad Alshomary,Kathleen McKeown*

Main category: cs.CL

TL;DR: iBERT 是一种可解释且可控的编码器，能够模块化并揭示语言中的判别线索，如风格和语义结构。它在风格任务中表现优异，并能泛化到其他任务。


<details>
  <summary>Details</summary>
Motivation: 现有的模型在可解释性和可控性方面存在不足，无法清晰揭示语言中的判别线索。因此，需要一种能够模块化并暴露语言中判别线索的编码器。

Method: iBERT 通过将每个输入词表示为 k 个独立于上下文的语义向量的稀疏、非负混合来实现可解释性。这些向量可以聚合为句子嵌入或直接在词级别使用，从而实现对表示的模块化控制。

Result: iBERT 在 STEL 基准测试中，风格表示效果比 SBERT 类基线高出约 8 个百分点，同时在作者身份验证任务上保持了竞争力。此外，iBERT 的结构使得可以将特定的风格属性（如表情符号使用、正式程度或拼写错误）分配到特定的语义向量上。

Conclusion: iBERT 是一种能够生成可解释且可控嵌入的编码器，能够模块化并揭示语言中的判别线索，如风格和语义结构。其结构模块化设计使其能够可解释地分解数据中存在的任何判别信号，即使监督混合了风格和语义因素也能实现泛化。

Abstract: We present iBERT (interpretable-BERT), an encoder to produce inherently
interpretable and controllable embeddings - designed to modularize and expose
the discriminative cues present in language, such as stylistic and semantic
structure. Each input token is represented as a sparse, non-negative mixture
over k context-independent sense vectors, which can be pooled into sentence
embeddings or used directly at the token level. This enables modular control
over representation, before any decoding or downstream use.
  To demonstrate our model's interpretability, we evaluate it on a suite of
style-focused tasks. On the STEL benchmark, it improves style representation
effectiveness by ~8 points over SBERT-style baselines, while maintaining
competitive performance on authorship verification. Because each embedding is a
structured composition of interpretable senses, we highlight how specific style
attributes - such as emoji use, formality, or misspelling can be assigned to
specific sense vectors. While our experiments center on style, iBERT is not
limited to stylistic modeling. Its structural modularity is designed to
interpretably decompose whichever discriminative signals are present in the
data - enabling generalization even when supervision blends stylistic and
semantic factors.

</details>


### [19] [DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning](https://arxiv.org/abs/2510.09883)
*Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang,Murali Annavarm*

Main category: cs.CL

TL;DR: DELTA是一种无需训练的稀疏注意力机制，能够在保持模型准确性的同时提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏注意力方法在推理任务中由于累积选择误差和长推导中标记的动态重要性而严重损失准确性。需要一种计算效率高且不牺牲模型准确性的方法。

Method: DELTA是一种无需训练的稀疏注意力机制，将Transformer层分为三组：初始层使用完整注意力，少量的选择层通过聚合头级注意力得分来识别重要标记，后续的稀疏注意力层仅关注选定子集。

Result: DELTA在AIME和GPQA-Diamond等推理基准测试中，在准确性上与完整注意力相当或超越，同时将所选标记数量减少了最多5倍，并实现了1.5倍的端到端加速。

Conclusion: DELTA通过选择性重用中间注意力图，为高效长上下文推理提供了一条稳健的路径。

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance on
challenging benchmarks by generating long chains of intermediate steps, but
their inference cost is dominated by decoding, where each new token must attend
to the entire growing sequence. Existing sparse attention methods reduce
computation by pruning the key-value (KV) cache, yet they suffer from severe
accuracy degradation on reasoning tasks due to cumulative selection errors and
the dynamic importance of tokens over long derivations. We present
\textbf{DELTA}, a training-free sparse attention mechanism that achieves
computational efficiency without sacrificing model accuracy. DELTA partitions
transformer layers into three groups: initial layers that use full attention, a
small set of \emph{selection layers} that identify salient tokens via
aggregated head-level attention scores, and subsequent \emph{sparse-attention
layers} that attend only to the selected subset. This design preserves the full
KV cache in GPU memory for accuracy, while avoiding expensive full-attention
computation over many layers. On reasoning benchmarks such as AIME and
GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while
reducing the number of attended tokens by up to $5\times$ and delivering
$1.5\times$ end-to-end speedup. Our results show that selective reuse of
intermediate attention maps offers a robust path toward efficient long-context
reasoning.

</details>


### [20] [Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs](https://arxiv.org/abs/2510.09885)
*Xu Pan,Ely Hahami,Jingxuan Fan,Ziqian Xie,Haim Sompolinsky*

Main category: cs.CL

TL;DR: 研究比较了自回归大语言模型（arLLMs）和掩码扩散大语言模型（dLLMs）在微调后的知识获取能力，发现dLLMs在没有数据增强的情况下表现更好，而arLLMs则需要大量数据增强。研究还提出了一种新的掩码微调方法，显著提高了arLLMs的微调数据效率。


<details>
  <summary>Details</summary>
Motivation: 尽管自回归大语言模型（arLLMs）是当前语言建模的主流范式，但由于存在诸如“反转诅咒”等固有缺陷，它们在通过微调进行知识注入时面临挑战。而掩码扩散大语言模型（dLLMs）作为arLLMs的有力替代方案，显示出更好的数据效率和在预训练中没有“反转诅咒”的证据。然而，这些优势是否延伸到后训练阶段尚不清楚。

Method: 研究通过在三个不同的数据集上对arLLMs和dLLMs进行微调，并使用正向和反向风格的问答（QA）来评估它们的知识泛化能力和反转诅咒问题。

Result: 研究结果确认了arLLMs在QA泛化中严重依赖于通过改写进行的数据增强，而改写仅在信息顺序与QA风格匹配时才有效。相反，dLLMs在没有改写的情况下在正向和反向QA上都取得了高准确率；添加改写仅带来微小的增益。

Conclusion: 研究结果表明，dLLMs在后训练阶段能够有效地获取新知识，而arLLMs则依赖于大量的数据增强。此外，研究提出了一种新的掩码微调范式，成功地大幅提高了arLLMs微调的数据效率，有效地缩小了与dLLMs的性能差距。

Abstract: Despite autoregressive large language models (arLLMs) being the current
dominant paradigm in language modeling, they resist knowledge injection via
fine-tuning due to inherent shortcomings such as the "reversal curse" -- the
challenge of answering questions that reverse the original information order in
the training sample. Masked diffusion large language models (dLLMs) are rapidly
emerging as a powerful alternative to the arLLM paradigm, with evidence of
better data efficiency and free of the "reversal curse" in pre-training.
However, it is unknown whether these advantages extend to the post-training
phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through
fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,
evaluating them with forward and backward style Question Answering (QA) to
probe knowledge generalization and the reversal curse. Our results confirm that
arLLMs critically rely on extensive data augmentation via paraphrases for QA
generalization, and paraphrases are only effective when their information order
matches the QA style. Conversely, dLLMs achieve high accuracies on both forward
and backward QAs without paraphrases; adding paraphrases yields only marginal
gains. Lastly, inspired by the dLLM's performance, we introduce a novel masked
fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This
proposed method successfully and drastically improves the data efficiency of
arLLM fine-tuning, effectively closing the performance gap with dLLMs.

</details>


### [21] [Abductive Preference Learning](https://arxiv.org/abs/2510.09887)
*Yijin Ni,Peng Qi*

Main category: cs.CL

TL;DR: 本文提出了一种新的偏好学习方法——归纳偏好学习，以解决现有偏好学习在处理反事实提示时的局限性。实验结果表明，这种方法在响应选择和提示区分方面都有显著提升，并且在AlpacaEval评估中也表现出了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好学习方法主要关注于为给定提示选择正确的响应，而忽视了应该改变响应的反事实提示。这导致大型语言模型在面对不同提示时表现出过度自信的问题。

Method: 本文提出了归纳偏好学习，这是一种通过给定响应来学习提示偏好的微调范式。为了验证这一想法，作者构建了一个从HaluEval QA基准中提取的归纳数据集，并实现了归纳DPO及其变体DPOP。

Result: 实验结果显示，标准方法在响应选择方面有所改进，而归纳方法在提示区分方面有所提高。多任务目标统一了两者，在归纳数据集上，多任务DPOP将响应选择的准确率从90.0%提升到99.5%，提示区分的准确率从54.7%提升到85.0%。此外，在AlpacaEval评估中，多任务DPOP提高了胜率。

Conclusion: 本文提出了一种新的偏好学习方法——归纳偏好学习，以解决现有偏好学习在处理反事实提示时的局限性。实验结果表明，这种方法在响应选择和提示区分方面都有显著提升，并且在AlpacaEval评估中也表现出了更好的性能。

Abstract: Frontier large language models such as GPT-5 and Claude Sonnet remain prone
to overconfidence even after alignment through Reinforcement Learning with
Human Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,
they tend to offer the same conservative answer "No" to both questions "Can I
eat the [food / potato chips] that has been left out overnight?" despite the
latter requiring no refridgeration for safe consumption. We find that this
failure is potentially attributed to a limitation of existing preference
learning: it emphasizes selecting the correct response for a given prompt,
while neglecting counterfactual prompts that should alter the response.
  To address this limitation, we propose abductive preference learning, a
fine-tuning paradigm that reverses the conventional conditioning by learning
preferences over prompts given a response. To validate this idea, we construct
an abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,
implementing abductive DPO and its variant DPOP. Experiments reveal
complementary strengths: standard methods improve response selection, abductive
methods improve prompt discrimination, while a multitask objective unifies
both. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\%$ to
$99.5\%$ in response selection and $54.7\%$ to $85.0\%$ in prompt
discrimination, with qualitative evidence highlighting improved sensitivity to
prompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP
improves win rate (from $5.26\%$ to $6.17\%$), confirming that abductive
preference learning preserves the benefits of conventional preference
optimization while addressing the overlooked challenge of counterfactual
prompts.

</details>


### [22] [HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection](https://arxiv.org/abs/2510.09893)
*Guanming Chen,Lingzhi Shen,Xiaohao Cai,Imran Razzak,Shoaib Jameel*

Main category: cs.CL

TL;DR: This paper presents HIPPD, a brain-inspired framework for personality detection that outperforms existing methods by emulating the hierarchical information processing of the human brain.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning approaches often struggle to capture contextual information spanning multiple posts and tend to fall short in extracting representative and robust features in semantically sparse environments.

Method: HIPPD is a brain-inspired framework that emulates the hierarchical information processing of the human brain. It uses a large language model to simulate the cerebral cortex, a dynamic memory module modelled after the prefrontal cortex, and specialised lightweight models emulating the basal ganglia.

Result: Extensive experiments on the Kaggle and Pandora datasets demonstrate that HIPPD consistently outperforms state-of-the-art baselines.

Conclusion: HIPPD consistently outperforms state-of-the-art baselines in personality detection.

Abstract: Personality detection from text aims to infer an individual's personality
traits based on linguistic patterns. However, existing machine learning
approaches often struggle to capture contextual information spanning multiple
posts and tend to fall short in extracting representative and robust features
in semantically sparse environments. This paper presents HIPPD, a
brain-inspired framework for personality detection that emulates the
hierarchical information processing of the human brain. HIPPD utilises a large
language model to simulate the cerebral cortex, enabling global semantic
reasoning and deep feature abstraction. A dynamic memory module, modelled after
the prefrontal cortex, performs adaptive gating and selective retention of
critical features, with all adjustments driven by dopaminergic prediction error
feedback. Subsequently, a set of specialised lightweight models, emulating the
basal ganglia, are dynamically routed via a strict winner-takes-all mechanism
to capture the personality-related patterns they are most proficient at
recognising. Extensive experiments on the Kaggle and Pandora datasets
demonstrate that HIPPD consistently outperforms state-of-the-art baselines.

</details>


### [23] [Don't Throw Away Your Pretrained Model](https://arxiv.org/abs/2510.09913)
*Shangbin Feng,Wenhao Yu,Yike Wang,Hongming Zhang,Yulia Tsvetkov,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出Switch Generation方法，通过模型协作提升语言模型性能，并在多个任务中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐训练存在权衡，虽然提高了推理和指令遵循能力，但可能损失了创造力和校准等技能。

Method: 提出Switch Generation方法，通过训练一个切换器LM来决定在不同情况下使用哪个模型生成下一个段落。

Result: 模型协作在18个任务中表现优于单个模型，Switch Generation平均比基线高出12.9%。

Conclusion: 模型协作可以同时利用对齐和不对齐模型的优势，Switch Generation方法在多个任务上表现出色，并能发现组合技能来解决个体模型难以处理的问题。

Abstract: Alignment training has tradeoffs: it helps language models (LMs) gain in
reasoning and instruction following but might lose out on skills such as
creativity and calibration, where unaligned base models are better at. We aim
to make the best of both worlds through model collaboration, where different
models in the training pipeline collaborate and complement each other. Since LM
responses feature interleaving skills that favor different models, we propose
Switch Generation, where pretrained and aligned model versions take turns to
``speak'' in a response sequence. Specifically, we train a switcher LM by
learning from outcomes of choosing different models to generate the next
segment across diverse queries and contexts. At inference time, the switcher LM
guides different model checkpoints to dynamically generate the next segment
where their strengths are most needed. Extensive experiments with 8 model
collaboration baselines and 18 datasets show that 1) model collaboration
consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch
Generation further outperforms baselines by 12.9% on average. Further analysis
reveals that Switch Generation discovers compositional skills to solve problems
where individual models struggle and generalizes to unseen models and tasks,
reusing and repurposing by-products in expensive model training pipelines that
are otherwise discarded.

</details>


### [24] [Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning](https://arxiv.org/abs/2510.09915)
*Sicong Huang,Qianqi Yan,Shengze Wang,Ian Lane*

Main category: cs.CL

TL;DR: 本文研究了微调策略以减少LLM生成摘要中的不忠实跨度，并引入了一个新的数据集，评估了三种微调技术的效果，发现反似然训练最为有效。


<details>
  <summary>Details</summary>
Motivation: 现有的缓解策略（如后处理校正或对比学习）无法完全解决LLM生成摘要中可能出现的多样化错误。因此，本文旨在研究微调策略以减少生成摘要中不忠实跨度的发生。

Method: 本文首先使用多种LLM为训练集中的源文档生成摘要，然后使用GPT-4o标注任何检测到的跨度级幻觉。基于这些标注，我们用无幻觉摘要和标注的不忠实跨度对LLM进行微调，以提高模型的忠实性。此外，还引入了一个新的数据集，该数据集包含带有跨度级标签的忠实和不忠实摘要。

Result: 实验结果表明，所有三种方法都能成功利用跨度级注释来提高忠实性，其中反似然训练是最有效的。

Conclusion: 本文通过引入包含忠实和不忠实摘要的新的数据集，并评估三种微调技术（梯度上升、反似然训练和任务向量否定），证明了所有三种方法都能成功利用跨度级注释来提高忠实性，其中反似然训练效果最佳。

Abstract: Abstractive summarization using large language models (LLMs) has become an
essential tool for condensing information. However, despite their ability to
generate fluent summaries, these models sometimes produce unfaithful summaries,
introducing hallucinations at the word, phrase, or concept level. Existing
mitigation strategies, such as post-processing corrections or contrastive
learning with synthetically generated negative samples, fail to fully address
the diverse errors that can occur in LLM-generated summaries. In this paper, we
investigate fine-tuning strategies to reduce the occurrence of unfaithful spans
in generated summaries. First, we automatically generate summaries for the set
of source documents in the training set with a variety of LLMs and then use
GPT-4o to annotate any hallucinations it detects at the span-level. Leveraging
these annotations, we fine-tune LLMs with both hallucination-free summaries and
annotated unfaithful spans to enhance model faithfulness. In this paper, we
introduce a new dataset that contains both faithful and unfaithful summaries
with span-level labels and we evaluate three techniques to fine-tuning a LLM to
improve the faithfulness of the resulting summarization: gradient ascent,
unlikelihood training, and task vector negation. Experimental results show that
all three approaches successfully leverage span-level annotations to improve
faithfulness, with unlikelihood training being the most effective.

</details>


### [25] [Unpacking Hateful Memes: Presupposed Context and False Claims](https://arxiv.org/abs/2510.09935)
*Weibin Cai,Jiayu Li,Reza Zafarani*

Main category: cs.CL

TL;DR: 本文提出了一种名为 SHIELD 的仇恨模因检测框架，通过结合 PCM 和 FACT 模块，能够有效检测仇恨模因，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的仇恨模因检测方法主要依赖于预训练的语言模型，但较少关注是什么使模因具有仇恨性。本文认为仇恨模因具有两个基本特征：预设的上下文和表达虚假声明。

Method: SHIELD 框架结合了 PCM 和 FACT 模块，其中 PCM 用于建模跨模态的上下文信息，FACT 模块则整合外部知识并利用跨模态引用图来检测虚假声明。

Result: 实验结果表明，SHIELD 在多个数据集和指标上优于最先进的方法，并且在其他任务（如假新闻检测）中表现出多样性。

Conclusion: SHIELD 是一种旨在捕捉仇恨本质的仇恨模因检测框架，实验表明它在多个数据集和指标上优于最先进的方法，并且在其他任务（如假新闻检测）上表现出多样性。

Abstract: While memes are often humorous, they are frequently used to disseminate hate,
causing serious harm to individuals and society. Current approaches to hateful
meme detection mainly rely on pre-trained language models. However, less focus
has been dedicated to \textit{what make a meme hateful}. Drawing on insights
from philosophy and psychology, we argue that hateful memes are characterized
by two essential features: a \textbf{presupposed context} and the expression of
\textbf{false claims}. To capture presupposed context, we develop \textbf{PCM}
for modeling contextual information across modalities. To detect false claims,
we introduce the \textbf{FACT} module, which integrates external knowledge and
harnesses cross-modal reference graphs. By combining PCM and FACT, we introduce
\textbf{\textsf{SHIELD}}, a hateful meme detection framework designed to
capture the fundamental nature of hate. Extensive experiments show that SHIELD
outperforms state-of-the-art methods across datasets and metrics, while
demonstrating versatility on other tasks, such as fake news detection.

</details>


### [26] [Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation](https://arxiv.org/abs/2510.09947)
*Mir Tafseer Nayeem,Sawsan Alqahtani,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 本文分析了六种常用分词器在多种语言和领域的表现，提出了一种新的指标STRR来补充传统指标fertility，以更好地评估多语言分词器的公平性。


<details>
  <summary>Details</summary>
Motivation: tokenization是大型语言模型中的关键步骤，但评估不足。标准指标fertility（每个单词的平均标记数）捕捉了压缩效率，但掩盖了词汇在不同语言和领域中的分配情况。

Method: 我们分析了六种广泛使用的分词器在七种语言和两个领域中的表现，提出了单个标记保留率（STRR）来解决fertility的盲点。

Result: 我们发现英语的fertility稳定，中文的fertility较高，而印度语存在碎片化现象。STRR揭示了对英语的系统性优先考虑，对中文的强大支持，以及印度语的碎片化，提供了跨语言公平性的可解释视图。

Conclusion: 我们的结果表明，STRR可以补充fertility，并为设计更公平的多语言分词器提供实用指导。

Abstract: Tokenization is a crucial but under-evaluated step in large language models
(LLMs). The standard metric, fertility (the average number of tokens per word),
captures compression efficiency but obscures how vocabularies are allocated
across languages and domains. We analyze six widely used tokenizers across
seven languages and two domains, finding stable fertility for English, high
fertility for Chinese, and little domain sensitivity. To address fertility's
blind spots, we propose the Single Token Retention Rate (STRR), which measures
the proportion of words preserved as single tokens. STRR reveals systematic
prioritization of English, strong support for Chinese, and fragmentation in
Hindi, offering an interpretable view of cross-lingual fairness. Our results
show that STRR complements fertility and provides practical guidance for
designing more equitable multilingual tokenizers.

</details>


### [27] [Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey](https://arxiv.org/abs/2510.09988)
*Jiaqi Wei,Xiang Zhang,Yuejin Yang,Wenxuan Huang,Juntai Cao,Sheng Xu,Xiang Zhuang,Zhangyang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Chenyu You,Wanli Ouyang,Siqi Sun*

Main category: cs.CL

TL;DR: 本文提出了一种统一的框架，用于分析和分类搜索算法，旨在解决奖励信号的模糊作用，并推动自主、自我改进代理的研究。


<details>
  <summary>Details</summary>
Motivation: 当前的研究领域存在碎片化问题，缺乏一个共同的形式化方法，特别是在奖励信号的作用方面存在歧义。本文旨在解决这一问题，推动更系统的研究进展。

Method: 本文提出了一个统一的框架，将搜索算法分解为三个核心组件：搜索机制、奖励公式和转换函数，并建立了临时的搜索引导和持久的参数化奖励建模之间的形式区别。

Result: 本文提出了一种组件为中心的分类法，综合了最先进的技术，并制定了研究路线图，以促进自主、自我改进代理的发展。

Conclusion: 本文通过引入一个统一的框架，解决了奖励信号在搜索算法中的模糊作用，并提出了一个以组件为中心的分类法，为创建自主、自我改进的代理指明了研究路线。

Abstract: Deliberative tree search is a cornerstone of modern Large Language Model
(LLM) research, driving the pivot from brute-force scaling toward algorithmic
efficiency. This single paradigm unifies two critical frontiers:
\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve
hard problems, and \textbf{Self-Improvement}, which uses search-generated data
to durably enhance model parameters. However, this burgeoning field is
fragmented and lacks a common formalism, particularly concerning the ambiguous
role of the reward signal -- is it a transient heuristic or a durable learning
target? This paper resolves this ambiguity by introducing a unified framework
that deconstructs search algorithms into three core components: the
\emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition
Function}. We establish a formal distinction between transient \textbf{Search
Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for
Self-Improvement. Building on this formalism, we introduce a component-centric
taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward
more systematic progress in creating autonomous, self-improving agents.

</details>


### [28] [Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations](https://arxiv.org/abs/2510.09994)
*Yimin Xiao,Yongle Zhang,Dayeon Ki,Calvin Bao,Marianna J. Martindale,Charlotte Vaughn,Ge Gao,Marine Carpuat*

Main category: cs.CL

TL;DR: 研究通过一项实验发现，非双语用户容易过度依赖MT，但错误体验可能促使他们重新评估MT的使用。这表明需要改进MT评估和NLP解释技术，以提高MT质量和用户素养。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译（MT）越来越普遍，了解公众如何感知和依赖不完美的MT对于将MT研究置于实际应用背景中至关重要。

Method: 在公共博物馆进行了一项涉及452名参与者的实验，研究流利度和准确度错误如何影响双语和非双语用户在日常使用中对MT的依赖程度。

Result: 研究发现，非双语用户由于缺乏评估策略和替代方案，往往过度依赖MT，而经历错误后，他们可能会重新评估未来的依赖程度。

Conclusion: 研究强调了提升MT质量和用户MT素养的必要性，以帮助用户更好地评估和使用MT。

Abstract: As Machine Translation (MT) becomes increasingly commonplace, understanding
how the general public perceives and relies on imperfect MT is crucial for
contextualizing MT research in real-world applications. We present a human
study conducted in a public museum (n=452), investigating how fluency and
adequacy errors impact bilingual and non-bilingual users' reliance on MT during
casual use. Our findings reveal that non-bilingual users often over-rely on MT
due to a lack of evaluation strategies and alternatives, while experiencing the
impact of errors can prompt users to reassess future reliance. This highlights
the need for MT evaluation and NLP explanation techniques to promote not only
MT quality, but also MT literacy among its users.

</details>


### [29] [MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction](https://arxiv.org/abs/2510.10003)
*Jianjin Wang,Runsong Zhao,Xiaoqian Liu,Yuan Ge,Ziqiang Xu,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出MTP-S2UT损失，通过在隐藏表示中应用多令牌预测损失，提高了语音到单位翻译的质量。


<details>
  <summary>Details</summary>
Motivation: 当前直接语音到语音翻译方法主要使用语音标记作为中间表示。然而，单个语音标记在语义上并不密集，因此通常需要多个标记来表达一个完整的语义单元。为了克服这一限制，引入MTP损失以提高信息密度和语义完整性。

Method: 引入多令牌预测（MTP）损失到语音到单位翻译（S2UT）模型中，使模型能够在每个位置预测多个后续令牌，从而捕捉更完整的语义并提高每个位置的信息密度。提出MTP-S2UT损失，将MTP损失应用于CTC损失计算的隐藏表示中。

Result: 所有MTP损失变体在S2UT翻译中都一致提高了质量，其中MTP-S2UT表现最佳。

Conclusion: 实验表明，所有MTP损失变体都能持续提高S2UT翻译的质量，其中MTP-S2UT表现最佳。

Abstract: Current direct speech-to-speech translation methods predominantly employ
speech tokens as intermediate representations. However, a single speech token
is not dense in semantics, so we generally need multiple tokens to express a
complete semantic unit. To address this limitation, we introduce multi-token
prediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling
models to predict multiple subsequent tokens at each position, thereby
capturing more complete semantics and enhancing information density per
position. Initial MTP implementations apply the loss at the final layer, which
improves output representation but initiates information enrichment too late.
We hypothesize that advancing the information enrichment process to
intermediate layers can achieve earlier and more effective enhancement of
hidden representation. Consequently, we propose MTP-S2UT loss, applying MTP
loss to hidden representation where CTC loss is computed. Experiments
demonstrate that all MTP loss variants consistently improve the quality of S2UT
translation, with MTP-S2UT achieving the best performance.

</details>


### [30] [Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning](https://arxiv.org/abs/2510.10009)
*Shu Zhao,Tan Yu,Anbang Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为ExpandSearch的方法，通过结合基于LLM的搜索代理和预训练的压缩模型，提高了多跳问答任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的推理增强型搜索代理在多跳问答基准上的表现仍然不令人满意，因为它们在推理和搜索方面的能力有限。此外，由于有限的微调数据和计算资源，搜索代理很难掌握多个任务。

Method: 我们训练了一个基于LLM的搜索代理，通过强化学习进行查询扩展，并结合预训练的压缩模型来帮助理解检索到的文档。

Result: 实验结果表明，即使是一个小规模的3B LLM，也能展示出强大的查询扩展能力，并在多跳问答基准上达到最先进的准确性。

Conclusion: 我们的方法ExpandSearch在多跳问答基准上实现了最先进的准确性，平均提升了4.4%。

Abstract: Reasoning-augmented search agents, such as Search-R1, are trained to reason,
search, and generate the final answer iteratively. Nevertheless, due to their
limited capabilities in reasoning and search, their performance on multi-hop QA
benchmarks remains far from satisfactory. To handle complex or compound
queries, we train an LLM-based search agent with the native capability of query
expansion through reinforcement learning. In each turn, our search agent
proposes several query variants, which are searched simultaneously to cover
more relevant information. Meanwhile, given limited post-training data and
computing resources, it is very challenging for a search agent to master
multiple tasks, including query generation, retrieved information
understanding, and answer generation. Therefore, we propose incorporating a
pre-trained squeezer model that helps the search agent understand the retrieved
documents, allowing the search agent to focus on query generation for high
retrieval recall. With the assistance of the squeezer model, we discover that
even a small-scale 3B LLM can demonstrate a strong capability of query
expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.
To be specific, our experiments across seven question-answering benchmarks
demonstrate that our method, named ExpandSearch, achieves an average
improvement of 4.4% compared to state-of-the-art baselines, with strong gains
on multi-hop reasoning tasks requiring diverse evidence aggregation.

</details>


### [31] [Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety](https://arxiv.org/abs/2510.10013)
*Yuyi Huang,Runzhe Zhan,Lidia S. Chao,Ailin Tao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本文探讨了Long-CoT模型中路径漂移的问题，并提出了一个三阶段的路径漂移诱导框架以及一种路径级别的防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地用于复杂推理任务，长链思维（Long-CoT）提示已成为结构化推理的关键范式。然而，我们发现了一个之前未被探索的漏洞：Long-CoT模型的推理轨迹可能会偏离对齐路径，导致违反安全约束的内容。

Method: 我们引入了一个三阶段的路径漂移诱导框架，包括认知负荷放大、自我角色提示和条件链劫持。此外，我们提出了一种路径级别的防御策略，包括角色归因纠正和元认知反思（反思性安全提示）。

Result: 通过实证分析，我们发现了路径漂移的三个行为触发因素：(1) 第一人称承诺，导致目标驱动的推理延迟拒绝信号；(2) 伦理蒸发，其中表面层次的免责声明绕过对齐检查点；(3) 条件链升级，其中分层提示逐步引导模型走向不安全的完成。

Conclusion: 我们的研究强调了在长篇推理中需要轨迹级别的对齐监督，而不仅仅是基于标记的对齐。

Abstract: As large language models (LLMs) are increasingly deployed for complex
reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a
key paradigm for structured inference. Despite early-stage safeguards enabled
by alignment techniques such as RLHF, we identify a previously underexplored
vulnerability: reasoning trajectories in Long-CoT models can drift from aligned
paths, resulting in content that violates safety constraints. We term this
phenomenon Path Drift. Through empirical analysis, we uncover three behavioral
triggers of Path Drift: (1) first-person commitments that induce goal-driven
reasoning that delays refusal signals; (2) ethical evaporation, where
surface-level disclaimers bypass alignment checkpoints; (3) condition chain
escalation, where layered cues progressively steer models toward unsafe
completions. Building on these insights, we introduce a three-stage Path Drift
Induction Framework comprising cognitive load amplification, self-role priming,
and condition chain hijacking. Each stage independently reduces refusal rates,
while their combination further compounds the effect. To mitigate these risks,
we propose a path-level defense strategy incorporating role attribution
correction and metacognitive reflection (reflective safety cues). Our findings
highlight the need for trajectory-level alignment oversight in long-form
reasoning beyond token-level alignment.

</details>


### [32] [Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default](https://arxiv.org/abs/2510.10025)
*Jiaqi Liu,Lanruo Wang,Su Liu,Xin Hu*

Main category: cs.CL

TL;DR: 本文研究了在医疗环境中使用紧凑编码器进行医学摘要分类的效果，发现DistilBERT在使用较少参数的情况下表现优于BERT base。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多NLP任务中表现良好，但在具有严格成本、延迟和隐私限制的医疗环境中难以部署。我们重新审视了一种轻量级的医学摘要分类方法，并询问在受控预算下紧凑编码器能走多远。

Method: 我们使用公共医学摘要语料库对BERT base和DistilBERT进行微调，采用三种目标：标准交叉熵、类别加权交叉熵和焦点损失，同时保持分词器、序列长度、优化器和调度固定。

Result: DistilBERT使用普通的交叉熵在测试集上表现最好，同时使用的参数远少于BERT base。我们报告了准确率、宏F1和加权F1，并发布了评估代码，包括混淆分析以明确错误模式。

Conclusion: 我们的结果表明，一个紧凑的编码器和交叉熵是一个实用的默认选择，然后在移动到更重的模型之前进行校准和任务特定检查。

Abstract: Large language models work well for many NLP tasks, but they are hard to
deploy in health settings with strict cost, latency, and privacy limits. We
revisit a lightweight recipe for medical abstract classification and ask how
far compact encoders can go under a controlled budget. Using the public medical
abstracts corpus, we finetune BERT base and DistilBERT with three objectives
standard cross-entropy, class weighted cross entropy, and focal loss keeping
tokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with
plain cross-entropy gives the best balance on the test set while using far
fewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,
release the evaluation code, and include confusion analyses to make error
patterns clear. Our results suggest a practical default: start with a compact
encoder and cross-entropy, then add calibration and task-specific checks before
moving to heavier models.

</details>


### [33] [HUME: Measuring the Human-Model Performance Gap in Text Embedding Task](https://arxiv.org/abs/2510.10062)
*Adnan El Assadi,Isaac Chung,Roman Solomatin,Niklas Muennighoff,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文提出了HUME框架，用于评估文本嵌入的人类表现，并提供了人类表现基线、任务难度模式的见解以及可扩展的评估框架，以更有意义地解释模型并指导模型和基准的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的评估框架如MTEB缺乏可靠的人类表现估计，限制了模型分数的解释性。因此，本文旨在通过引入HUME框架来解决这一问题。

Method: 本文引入了HUME：一种用于文本嵌入的人类评估框架，通过在16个MTEB数据集上测量人类表现来填补现有评估框架的空白。

Result: 人类在16个MTEB数据集上的平均表现达到77.6%，而最佳嵌入模型的表现为80.1%。模型在某些数据集上接近天花板性能，而在其他数据集上表现不佳，这表明数据集存在问题，并揭示了低资源语言中的不足。

Conclusion: 本文提出了HUME框架，用于评估文本嵌入的人类表现，并提供了人类表现基线、任务难度模式的见解以及可扩展的评估框架，以更有意义地解释模型并指导模型和基准的发展。

Abstract: Comparing human and model performance offers a valuable perspective for
understanding the strengths and limitations of embedding models, highlighting
where they succeed and where they fail to capture meaning and nuance. However,
such comparisons are rarely made, as human performance on embedding tasks is
difficult to measure. To fill this gap, we introduce HUME: Human Evaluation
Framework for Text Embeddings. While frameworks like MTEB provide broad model
evaluation, they lack reliable estimates of human performance, limiting the
interpretability of model scores. We measure human performance across 16 MTEB
datasets spanning reranking, classification, clustering, and semantic textual
similarity across linguistically diverse high- and low-resource languages.
Humans achieve an average performance of 77.6% compared to 80.1% for the best
embedding model, although variation is substantial: models reach near-ceiling
performance on some datasets while struggling on others, suggesting dataset
issues and revealing shortcomings in low-resource languages. We provide human
performance baselines, insight into task difficulty patterns, and an extensible
evaluation framework that enables a more meaningful interpretation of the model
and informs the development of both models and benchmarks. Our code, dataset,
and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.

</details>


### [34] [CLMN: Concept based Language Models via Neural Symbolic Reasoning](https://arxiv.org/abs/2510.10063)
*Yibo Yang*

Main category: cs.CL

TL;DR: 本文介绍了一种名为概念语言模型网络（CLMN）的神经符号框架，它结合了神经表示与符号推理，在保持性能的同时提高可解释性。CLMN通过连续的人类可读嵌入和模糊逻辑推理来学习概念之间的动态交互规则，并在多个数据集上实现了更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自然语言处理方面取得了进展，但可解释性仍然有限，尤其是在医疗保健和金融领域。概念瓶颈模型在视觉中将预测与人类概念联系起来，但NLP版本要么使用二进制激活损害文本表示，要么使用潜在概念削弱语义，并且很少建模动态概念交互，如否定和上下文。

Method: 我们引入了概念语言模型网络（CLMN），这是一个神经符号框架，保持了性能和可解释性。CLMN将概念表示为连续的人类可读嵌入，并应用模糊逻辑推理来学习自适应交互规则，说明概念如何相互影响和最终决策。该模型增强了原始文本特征与概念感知表示，并自动推导出可解释的逻辑规则。

Result: 在多个数据集和预训练语言模型中，CLMN的准确性高于现有的基于概念的方法，同时提高了解释质量。

Conclusion: 这些结果表明，在统一的概念空间中将神经表示与符号推理相结合可以产生实用且透明的NLP系统。

Abstract: Deep learning has advanced NLP, but interpretability remains limited,
especially in healthcare and finance. Concept bottleneck models tie predictions
to human concepts in vision, but NLP versions either use binary activations
that harm text representations or latent concepts that weaken semantics, and
they rarely model dynamic concept interactions such as negation and context. We
introduce the Concept Language Model Network (CLMN), a neural-symbolic
framework that keeps both performance and interpretability. CLMN represents
concepts as continuous, human-readable embeddings and applies fuzzy-logic
reasoning to learn adaptive interaction rules that state how concepts affect
each other and the final decision. The model augments original text features
with concept-aware representations and automatically induces interpretable
logic rules. Across multiple datasets and pre-trained language models, CLMN
achieves higher accuracy than existing concept-based methods while improving
explanation quality. These results show that integrating neural representations
with symbolic reasoning in a unified concept space can yield practical,
transparent NLP systems.

</details>


### [35] [Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference](https://arxiv.org/abs/2510.10072)
*Hua Cai,Shuang Zhao,Liang Zhang,Xuli Shen,Qing Xu,Weilin Shen,Zihao Wen,Tianke Ban*

Main category: cs.CL

TL;DR: 本文介绍了Unilaw-R1，一个针对法律推理的大型语言模型。通过构建高质量的数据集和采用两阶段训练策略，Unilaw-R1在处理复杂的法律问题上表现出色，并在多个基准测试中取得了优异的成绩。


<details>
  <summary>Details</summary>
Motivation: Reasoning-focused large language models (LLMs) are rapidly evolving across various domains, yet their capabilities in handling complex legal problems remains underexplored.

Method: We first construct Unilaw-R1-Data, a high-quality dataset containing 17K distilled and screened chain-of-thought (CoT) samples. Based on this, we adopt a two-stage training strategy combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), which significantly boosts the performance on complex legal reasoning tasks and supports interpretable decision-making in legal AI applications.

Result: Unilaw-R1 demonstrates strong results on authoritative benchmarks, outperforming all models of similar scale and achieving performance on par with the much larger DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it also showed significant gains on LawBench and LexEval, exceeding Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

Conclusion: Unilaw-R1 demonstrates strong results on authoritative benchmarks, outperforming all models of similar scale and achieving performance on par with the much larger DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it also showed significant gains on LawBench and LexEval, exceeding Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

Abstract: Reasoning-focused large language models (LLMs) are rapidly evolving across
various domains, yet their capabilities in handling complex legal problems
remains underexplored. In this paper, we introduce Unilaw-R1, a large language
model tailored for legal reasoning. With a lightweight 7-billion parameter
scale, Unilaw-R1 significantly reduces deployment cost while effectively
tackling three core challenges in the legal domain: insufficient legal
knowledge, unreliable reasoning logic, and weak business generalization. To
address these issues, we first construct Unilaw-R1-Data, a high-quality dataset
containing 17K distilled and screened chain-of-thought (CoT) samples. Based on
this, we adopt a two-stage training strategy combining Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL), which significantly boosts the
performance on complex legal reasoning tasks and supports interpretable
decision-making in legal AI applications. To assess legal reasoning ability, we
also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate
models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates
strong results on authoritative benchmarks, outperforming all models of similar
scale and achieving performance on par with the much larger
DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it
also showed significant gains on LawBench and LexEval, exceeding
Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

</details>


### [36] [A-IPO: Adaptive Intent-driven Preference Optimization](https://arxiv.org/abs/2510.10077)
*Wenqing Wang,Muhammad Asif Ali,Ali Shoker,Ruohan Yang,Junyang Chen,Ying Sha,Huan Wang*

Main category: cs.CL

TL;DR: A-IPO is a new method for preference optimization that incorporates latent user intent into the reward function, leading to better alignment with user preferences and improved performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods like DPO often default to majority views, overlooking minority opinions and failing to capture latent user intentions in prompts.

Method: A-IPO introduces an intention module that infers the latent intent behind each user prompt and explicitly incorporates this inferred intent into the reward function.

Result: A-IPO demonstrates improvements in preference margin, pluralistic preference optimization, and adversarial robustness in preference alignment.

Conclusion: A-IPO consistently surpasses existing baselines, yielding substantial improvements across key metrics.

Abstract: Human preferences are diverse and dynamic, shaped by regional, cultural, and
social factors. Existing alignment methods like Direct Preference Optimization
(DPO) and its variants often default to majority views, overlooking minority
opinions and failing to capture latent user intentions in prompts.
  To address these limitations, we introduce \underline{\textbf{A}}daptive
\textbf{\underline{I}}ntent-driven \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization (\textbf{A-IPO}). Specifically,A-IPO
introduces an intention module that infers the latent intent behind each user
prompt and explicitly incorporates this inferred intent into the reward
function, encouraging stronger alignment between the preferred model's
responses and the user's underlying intentions. We demonstrate, both
theoretically and empirically, that incorporating an intention--response
similarity term increases the preference margin (by a positive shift of
$\lambda\,\Delta\mathrm{sim}$ in the log-odds), resulting in clearer separation
between preferred and dispreferred responses compared to DPO.
  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along
with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess
real-world and adversarial preference alignment.
  Through explicit modeling of diverse user intents,A-IPO facilitates
pluralistic preference optimization while simultaneously enhancing adversarial
robustness in preference alignment. Comprehensive empirical evaluation
demonstrates that A-IPO consistently surpasses existing baselines, yielding
substantial improvements across key metrics: up to +24.8 win-rate and +45.6
Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity
and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention
Consistency Score on GlobalOpinionQA-Ext.

</details>


### [37] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee,Shivam Sonawane,Amey Hengle,Aditya Tanna,Sourish Dasgupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出PerAugy数据增强技术，通过增加数据集多样性提高个性化摘要性能，并验证了数据集多样性对模型表现的重要性。


<details>
  <summary>Details</summary>
Motivation: 个性化摘要面临训练数据稀缺的问题，因为包含用户偏好历史和目标摘要的数据很少。现有的数据集如MS/CAS PENS缺少目标摘要，限制了端到端监督学习。

Method: 本文提出了PerAugy方法，通过跨轨迹洗牌和摘要内容扰动来增强数据集的多样性，并评估了其对个性化摘要性能的影响。

Result: PerAugy方法在四个最先进的用户编码器上显著提高了准确性（最佳结果：AUC提升0.132）。当与改进的用户编码器结合时，两个基准框架在个性化方面有显著提升（平均提升61.2%）。

Conclusion: 本文提出了一种名为PerAugy的数据增强技术，能够显著提高个性化摘要框架中用户编码器的准确性。此外，研究发现数据集的多样性是影响性能的关键因素。

Abstract: Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.

</details>


### [38] [Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.10103)
*Renliang Sun,Wei Cheng,Dawei Li,Haifeng Chen,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的框架REFRAIN，用于适应性地决定何时停止推理以减轻过度思考。该框架在减少令牌使用量的同时保持或提高了准确性。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought（CoT）推理通过外部化中间步骤推动了大型语言模型（LLMs）在推理密集型任务上的近期进展。然而，过多或冗余的推理——所谓的过度思考——会增加推理成本并导致LLMs得出错误结论。因此，需要一种方法来适应性地决定何时停止推理以减轻过度思考。

Method: REFRAIN是一种无需训练的框架，它通过集成两个阶段的停止判别器来识别反思但冗余的推理，并使用滑动窗口上限置信度（SW-UCB）多臂老虎机控制器根据问题难度动态调整停止阈值，而无需监督或微调。

Result: 在四个代表性基准和两种模型家族中，REFRAIN将令牌使用量减少了20-55%，同时保持或提高了与标准CoT提示相比的准确性。广泛的消融和鲁棒性分析证明了其在模型、评分器和提示变化中的稳定性。

Conclusion: 我们的研究强调了何时停止推理作为一个新的且实际的测试时扩展轴——使模型不仅能够进行更多推理，而且能够进行足够的推理。

Abstract: Chain-of-Thought (CoT) reasoning has driven recent gains of large language
models (LLMs) on reasoning-intensive tasks by externalizing intermediate steps.
However, excessive or redundant reasoning -- so-called overthinking -- can
increase inference costs and lead LLMs toward incorrect conclusions. In this
paper, we present REFRAIN ($\underline{REF}$lective-$\underline{R}$edundancy
for $\underline{A}$daptive $\underline{IN}$ference), a training-free framework
that adaptively determines when to stop reasoning to mitigate overthinking.
REFRAIN integrates a two-stage stop discriminator to identify reflective yet
redundant reasoning and a sliding-window Upper Confidence Bound (SW-UCB)
multi-armed bandit controller to dynamically adjust stopping thresholds
according to problem difficulty without supervision or fine-tuning. Across four
representative benchmarks and two model families, REFRAIN reduces token usage
by 20-55% while maintaining or improving accuracy compared to standard CoT
prompting. Extensive ablation and robustness analyses demonstrate its stability
across models, scorers, and prompt variations. In summary, our findings
highlight when-to-stop as a new and practical axis of test-time scaling --
enabling models to reason not just more, but just enough.

</details>


### [39] [LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora](https://arxiv.org/abs/2510.10114)
*Luyao Zhuang,Shengyuan Chen,Yilin Xiao,Huachi Zhou,Yujing Zhang,Hao Chen,Qinggang Zhang,Xiao Huang*

Main category: cs.CL

TL;DR: LinearRAG is a new framework for graph-based retrieval-augmented generation that uses a relation-free hierarchical graph and a two-stage retrieval strategy to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG methods face challenges due to unstable and costly relation extraction, leading to noisy graphs that degrade retrieval quality. The paper aims to address these issues by proposing a more efficient and reliable approach.

Method: LinearRAG constructs a relation-free hierarchical graph (Tri-Graph) using lightweight entity extraction and semantic linking, and employs a two-stage retrieval strategy involving relevant entity activation and global importance aggregation.

Result: Extensive experiments on four datasets show that LinearRAG outperforms baseline models, demonstrating improved retrieval performance and scalability.

Conclusion: LinearRAG significantly outperforms baseline models in retrieval tasks, offering an efficient and reliable framework for graph-based retrieval-augmented generation.

Abstract: Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.

</details>


### [40] [Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task](https://arxiv.org/abs/2510.10138)
*Zilong Wang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出了一种结合OCR引擎和大型语言模型的系统框架，用于优化重复文档提取任务的准确性与效率。通过利用文档特定特征，实现了显著的性能提升，并展示了其在处理异构文档流方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 信息抽取从大量结构相似内容的文档中是一个关键但研究不足的挑战，尤其是在企业文档处理中。现有的方法通常追求通用解决方案，而本文旨在通过利用文档特定特征来解决这一问题。

Method: 本文提出了一个系统框架，结合OCR引擎和大型语言模型（LLMs），并实施和评估了25种配置，涵盖了三种提取范式（直接、替换和基于表格的提取）。

Result: 通过基于表格的提取方法，自适应框架在结构化文档上实现了F1=1.0的准确率和0.97秒的延迟，在具有挑战性的图像输入上实现了F1=0.997的准确率和0.6秒的延迟。与传统方法相比，性能提高了54倍，并保持了亚秒级处理速度。

Conclusion: 本文提出了一种系统框架，通过结合OCR引擎和大型语言模型（LLMs），优化了重复文档提取任务中的准确性与效率权衡。该方法利用文档特定特征进行智能策略选择，实现了显著的性能提升，并为处理异构文档流提供了生产规模的解决方案。

Abstract: Information extraction from copy-heavy documents, characterized by massive
volumes of structurally similar content, represents a critical yet understudied
challenge in enterprise document processing. We present a systematic framework
that strategically combines OCR engines with Large Language Models (LLMs) to
optimize the accuracy-efficiency trade-off inherent in repetitive document
extraction tasks. Unlike existing approaches that pursue universal solutions,
our method exploits document-specific characteristics through intelligent
strategy selection. We implement and evaluate 25 configurations across three
extraction paradigms (direct, replacement, and table-based) on identity
documents spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based
extraction methods, our adaptive framework delivers outstanding results: F1=1.0
accuracy with 0.97s latency for structured documents, and F1=0.997 accuracy
with 0.6 s for challenging image inputs when integrated with PaddleOCR, all
while maintaining sub-second processing speeds. The 54 times performance
improvement compared with multimodal methods over naive approaches, coupled
with format-aware routing, enables processing of heterogeneous document streams
at production scale. Beyond the specific application to identity extraction,
this work establishes a general principle: the repetitive nature of copy-heavy
tasks can be transformed from a computational burden into an optimization
opportunity through structure-aware method selection.

</details>


### [41] [DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models](https://arxiv.org/abs/2510.10142)
*Tingxu Han,Wei Song,Ziqi Ding,Ziming Li,Chunrong Fang,Yuekang Li,Dongfang Liu,Zhenyu Chen,Zhenting Wang*

Main category: cs.CL

TL;DR: This paper investigates LLM unfairness and proposes DiffHeads, a framework that reduces bias by identifying and masking specific attention heads.


<details>
  <summary>Details</summary>
Motivation: Existing work on LLM bias focuses on detecting biased outputs but lacks insight into the mechanisms causing them, leading to fragile mitigations.

Method: The paper compares Direct-Answer (DA) prompting to Chain-of-Thought (CoT) prompting and introduces a token-to-head contribution score to identify bias heads. DiffHeads is then proposed to selectively mask these heads.

Result: DA prompting improves measured unfairness by 534.5%-391.9% in dialogues. DiffHeads reduces unfairness by 49.4% under DA and 40.3% under CoT without harming model utility.

Conclusion: DiffHeads is a lightweight debiasing framework that effectively reduces unfairness in LLMs without harming model utility.

Abstract: Large language models (LLMs) increasingly mediate decisions in domains where
unfair treatment of demographic groups is unacceptable. Existing work probes
when biased outputs appear, but gives little insight into the mechanisms that
generate them, leaving existing mitigations largely fragile. In this paper, we
conduct a systematic investigation LLM unfairness and propose DiffHeads, a
lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)
prompting to Chain-of-Thought (CoT) prompting across eight representative open-
and closed-source LLMs. DA will trigger the nature bias part of LLM and improve
measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.
Next, we define a token-to-head contribution score that traces each token's
influence back to individual attention heads. This reveals a small cluster of
bias heads that activate under DA but stay largely dormant with CoT, providing
the first causal link between prompting strategy and bias emergence. Finally,
building on this insight, we propose DiffHeads that identifies bias heads
through differential activation analysis between DA and CoT, and selectively
masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under
DA and CoT, respectively, without harming model utility.

</details>


### [42] [BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation](https://arxiv.org/abs/2510.10157)
*Tsung-Min Pai,Jui-I Wang,Li-Chun Lu,Shao-Hua Sun,Hung-Yi Lee,Kai-Wei Chang*

Main category: cs.CL

TL;DR: BILLY是一个无需训练的框架，通过在模型的激活空间中提取和混合多个不同的角色向量，实现多视角输出，从而在不增加计算成本的情况下提高大型语言模型的创造力。


<details>
  <summary>Details</summary>
Motivation: 多LLM系统虽然能增强大型语言模型的创造力，但存在计算成本高和推理延迟大的缺点。为了克服这些限制，我们提出了BILLY（BlendIng persona vectors for Large Language model creativitY），一个无需训练的框架，能够在单个模型中捕捉多LLM协作的好处，即诱导多样化的观点和专业技能。

Method: BILLY通过在模型的激活空间中提取和混合多个不同的角色向量来操作。在推理过程中，我们用这个合并的向量引导模型的生成过程，从而实现多视角输出而无需显式的多LLM通信。

Result: 在面向创造力的基准测试中，BILLY的表现超过了单一模型提示和传统的多LLM方法，同时显著减少了推理时间和计算成本。分析还表明，不同的角色向量可以融合以实现对生成互补方面的有效控制和更高的可解释性。

Conclusion: BILLY能够超越单一模型提示和传统多LLM方法，同时显著减少推理时间和计算成本。分析进一步表明，不同的角色向量可以融合以实现对生成互补方面的有效控制和更高的可解释性。

Abstract: Multi-LLM systems enhance the creativity of large language models by
simulating human collective intelligence but suffer from significant drawbacks,
such as high computational costs and inference latency. To address these
limitations, we propose BILLY (BlendIng persona vectors for Large Language
model creativitY), a training-free framework that captures the benefits of
multi-LLM collaboration, i.e. inducing diverse perspectives and specialized
expertise, within a single model. BILLY operates by extracting and blending
multiple distinct persona vectors directly in the model's activation space. We
steer the model's generation process with this merged vector while inference,
enabling multi-perspective output without explicit multi-LLM communication. Our
experiments across creativity-oriented benchmarks demonstrate that BILLY
surpasses single model prompting and traditional multi-LLM approaches, while
substantially reducing inference time and computational costs. Our analyses
further reveal that distinct persona vectors can be blended to achieve both
effective control over complementary aspects of generation and greater
interpretability.

</details>


### [43] [BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data](https://arxiv.org/abs/2510.10159)
*Jaap Jumelet,Abdellah Fourtassi,Akari Haga,Bastian Bunzeck,Bhargav Shandilya,Diana Galvan-Sosa,Faiz Ghifari Haznitrama,Francesca Padovani,Francois Meyer,Hai Hu,Julen Etxaniz,Laurent Prévot,Linyang He,María Grandury,Mila Marcheva,Negar Foroutan,Nikitas Theodoropoulos,Pouya Sadeghi,Siyuan Song,Suchir Salhan,Susana Zhou,Yurii Paniv,Ziyin Zhang,Arianna Bisazza,Alex Warstadt,Leshem Choshen*

Main category: cs.CL

TL;DR: BabyBabelLM是一个多语言数据集，旨在模拟一个人从出生到获得母语时所观察到的语言。


<details>
  <summary>Details</summary>
Motivation: 为了促进多语言预训练和认知建模，我们需要一个模拟从出生到获得母语的语言的数据集。

Method: 我们整理了发展上合理的预训练数据，以覆盖每种语言相当于1亿个英语单词的内容。

Result: 我们编译了评估套件并在每种语言中训练了基线模型。

Conclusion: BabyBabelLM旨在促进多语言预训练和认知建模。

Abstract: We present BabyBabelLM, a multilingual collection of datasets modeling the
language a person observes from birth until they acquire a native language. We
curate developmentally plausible pretraining data aiming to cover the
equivalent of 100M English words of content in each of 45 languages. We compile
evaluation suites and train baseline models in each language. BabyBabelLM aims
to facilitate multilingual pretraining and cognitive modeling.

</details>


### [44] [Large Language Model Sourcing: A Survey](https://arxiv.org/abs/2510.10161)
*Liang Pang,Kangxi Wu,Sunhao Dai,Zihao Wei,Zenghao Duan,Jia Gu,Xiang Li,Zhiyi Yin,Jun Xu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文系统地研究了生成内容的来源追踪，提出了一个双范式分类法，以增强LLMs在现实应用中的透明度、责任性和可信度。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs的黑箱性质和生成内容的人类相似性，诸如幻觉、偏见、不公平和版权侵权等问题变得尤为重要。因此，从多个角度获取信息是必要的。

Method: 本文系统地研究了生成内容的来源追踪，围绕四个相互关联的维度进行组织，从模型和数据的角度捕捉both model- and data-centric perspectives。

Result: 本文提出了一个双范式分类法，将现有的溯源方法分为基于先验（主动可追溯性嵌入）和基于后验（回顾性推理）的方法。Traceability across these dimensions enhances the transparency, accountability, and trustworthiness of LLMs deployment in real-world applications.

Conclusion: 本文提出了一个双范式分类法，将现有的溯源方法分为基于先验（主动可追溯性嵌入）和基于后验（回顾性推理）的方法。这些维度的可追溯性增强了LLMs在现实应用中的透明度、责任性和可信度。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized
artificial intelligence, shifting from supporting objective tasks (e.g.,
recognition) to empowering subjective decision-making (e.g., planning,
decision). This marks the dawn of general and powerful AI, with applications
spanning a wide range of fields, including programming, education, healthcare,
finance, and law. However, their deployment introduces multifaceted risks. Due
to the black-box nature of LLMs and the human-like quality of their generated
content, issues such as hallucinations, bias, unfairness, and copyright
infringement become particularly significant. In this context, sourcing
information from multiple perspectives is essential.
  This survey presents a systematic investigation into provenance tracking for
content generated by LLMs, organized around four interrelated dimensions that
together capture both model- and data-centric perspectives. From the model
perspective, Model Sourcing treats the model as a whole, aiming to distinguish
content generated by specific LLMs from content authored by humans. Model
Structure Sourcing delves into the internal generative mechanisms, analyzing
architectural components that shape the outputs of model. From the data
perspective, Training Data Sourcing focuses on internal attribution, tracing
the origins of generated content back to the training data of model. In
contrast, External Data Sourcing emphasizes external validation, identifying
external information used to support or influence the responses of model.
Moreover, we also propose a dual-paradigm taxonomy that classifies existing
sourcing methods into prior-based (proactive traceability embedding) and
posterior-based (retrospective inference) approaches. Traceability across these
dimensions enhances the transparency, accountability, and trustworthiness of
LLMs deployment in real-world applications.

</details>


### [45] [A Survey of Inductive Reasoning for Large Language Models](https://arxiv.org/abs/2510.10182)
*Kedi Chen,Dezhao Ruan,Yuhao Dan,Yaoting Wang,Siyu Yan,Xuecheng Wu,Yinqi Zhang,Qin Chen,Jie Zhou,Liang He,Biqing Qi,Linyang Li,Qipeng Guo,Xiaoming Shi,Wei Zhang*

Main category: cs.CL

TL;DR: 本文是对大语言模型中归纳推理的第一个全面综述，涵盖了方法、基准和评估方法，并分析了归纳能力的来源。


<details>
  <summary>Details</summary>
Motivation: 尽管归纳推理很重要，但目前尚无系统性的总结。因此，本文旨在提供对归纳推理的全面综述。

Method: 本文将提高归纳推理的方法分为三个主要领域：后训练、测试时缩放和数据增强。此外，还总结了当前的归纳推理基准，并推导出一种基于沙盒的统一评估方法和观察覆盖率指标。

Result: 本文总结了提高归纳推理的方法，提出了一个统一的评估方法，并分析了归纳能力的来源以及简单模型架构和数据如何帮助归纳任务。

Conclusion: 本文提供了对大语言模型中归纳推理的第一个全面综述，并为未来的研究提供了坚实的基础。

Abstract: Reasoning is an important task for large language models (LLMs). Among all
the reasoning paradigms, inductive reasoning is one of the fundamental types,
which is characterized by its particular-to-general thinking process and the
non-uniqueness of its answers. The inductive mode is crucial for knowledge
generalization and aligns better with human cognition, so it is a fundamental
mode of learning, hence attracting increasing interest. Despite the importance
of inductive reasoning, there is no systematic summary of it. Therefore, this
paper presents the first comprehensive survey of inductive reasoning for LLMs.
First, methods for improving inductive reasoning are categorized into three
main areas: post-training, test-time scaling, and data augmentation. Then,
current benchmarks of inductive reasoning are summarized, and a unified
sandbox-based evaluation approach with the observation coverage metric is
derived. Finally, we offer some analyses regarding the source of inductive
ability and how simple model architectures and data help with inductive tasks,
providing a solid foundation for future research.

</details>


### [46] [MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems](https://arxiv.org/abs/2510.10185)
*Lei Gu,Yinghao Zhu,Haoran Sang,Zixiang Wang,Dehao Sui,Wen Tang,Ewen Harrison,Junyi Gao,Lequan Yu,Liantao Ma*

Main category: cs.CL

TL;DR: 本研究通过大规模实证研究，揭示了基于大型语言模型的多智能体系统在医学咨询中的协作失败模式，并强调了透明和可审计推理过程的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的多智能体系统在医学咨询模拟中显示出潜力，但其评估通常局限于最终答案的准确性，这将内部协作过程视为不透明的“黑箱”，忽视了一个关键问题：诊断结论是否通过了合理的、可验证的推理路径？

Method: 通过结合定性分析和定量审计的混合方法，对3600个案例进行了大规模实证研究，并开发了一个协作失败模式的综合分类法。

Result: 定量审计揭示了四种主要的失败模式：由共享模型缺陷驱动的错误共识、正确少数意见的压制、无效的讨论动态以及合成过程中的关键信息丢失。

Conclusion: 本研究表明，仅靠高准确性不足以衡量临床或公众信任度，强调了透明和可审计推理过程的必要性，这是医疗AI负责任开发和部署的基石。

Abstract: While large language model (LLM)-based multi-agent systems show promise in
simulating medical consultations, their evaluation is often confined to
final-answer accuracy. This practice treats their internal collaborative
processes as opaque "black boxes" and overlooks a critical question: is a
diagnostic conclusion reached through a sound and verifiable reasoning pathway?
The inscrutable nature of these systems poses a significant risk in high-stakes
medical applications, potentially leading to flawed or untrustworthy
conclusions. To address this, we conduct a large-scale empirical study of 3,600
cases from six medical datasets and six representative multi-agent frameworks.
Through a rigorous, mixed-methods approach combining qualitative analysis with
quantitative auditing, we develop a comprehensive taxonomy of collaborative
failure modes. Our quantitative audit reveals four dominant failure patterns:
flawed consensus driven by shared model deficiencies, suppression of correct
minority opinions, ineffective discussion dynamics, and critical information
loss during synthesis. This study demonstrates that high accuracy alone is an
insufficient measure of clinical or public trust. It highlights the urgent need
for transparent and auditable reasoning processes, a cornerstone for the
responsible development and deployment of medical AI.

</details>


### [47] [Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning](https://arxiv.org/abs/2510.10208)
*Bo Yuan,Yulin Chen,Yin Zhang*

Main category: cs.CL

TL;DR: Delora is a novel framework that decouples sample selection from model training, using clean and noisy LoRA to detect noisy labels and improve performance on noisy datasets.


<details>
  <summary>Details</summary>
Motivation: The problem of noisy labels in real-world scenarios hinders the performance of models trained on such data. Existing solutions often create a vicious cycle due to inaccurate initial sample selection, leading to suboptimal results.

Method: Delora is a framework that decouples sample selection from model training. It uses clean and noisy LoRA to establish a noisy label detector, where the clean LoRA memorizes clean data and the noisy LoRA memorizes mislabeled data, serving as a learnable threshold for selecting samples.

Result: Experimental results show that Delora effectively detects noisy labels and improves text classification performance on both synthetic and real-world noisy datasets.

Conclusion: Delora demonstrates effectiveness in noisy label detection and text classification on both synthetic and real-world datasets.

Abstract: Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have
shown impressive performance in various downstream tasks. However, in many
real-world scenarios, the collected training data inevitably contains noisy
labels. To learn from noisy labels, most solutions select samples with small
losses for model training. However, the selected samples, in turn, impact the
loss computation in the next iteration. An inaccurate initial selection can
create a vicious cycle, leading to suboptimal performance. To break this cycle,
we propose Delora, a novel framework that decouples the sample selection from
model training. For sample selection, Delora establishes a noisy label detector
by introducing clean and noisy LoRA. Benefiting from the memory effect, the
clean LoRA is encouraged to memorize clean data, while the noisy LoRA is
constrained to memorize mislabeled data, which serves as a learnable threshold
for selecting clean and noisy samples. For model training, Delora can use
carefully selected samples to fine-tune language models seamlessly.
Experimental results on synthetic and real-world noisy datasets demonstrate the
effectiveness of Delora in noisy label detection and text classification.

</details>


### [48] [You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs](https://arxiv.org/abs/2510.10223)
*Yijie Xu,Huizai Yao,Zhiyu Guo,Weiyu Guo,Pengteng Li,Aiwei Liu,Xuming Hu,Hui Xiong*

Main category: cs.CL

TL;DR: 本文研究了无标签的测试时适应方法，提出了一种名为SyTTA的框架，该框架在没有额外监督的情况下实时适应语言模型，通过结合输入侧困惑度和输出侧预测熵来应对数据分布变化。实验结果表明，SyTTA在多个领域任务中均取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 在专业领域中，大型语言模型面临显著的数据分布变化，而领域特定的微调依赖于高质量的标注数据，这在专业知识有限的环境中是昂贵且缓慢的。

Method: SyTTA是一种推理时框架，它在没有额外监督的情况下实时适应模型。SyTTA结合了两种互补的不确定性信号：输入侧困惑度和输出侧预测熵。

Result: 在多种模型架构和领域特定基准上，SyTTA都取得了稳定的提升。特别是在农业问答任务中，SyTTA在仅每查询添加4个额外标记的情况下，将Rouge-LSum提高了120%以上。

Conclusion: 这些结果表明，无需标注示例就可以实现语言模型的有效测试时适应，这支持了在标签稀缺领域中的部署。

Abstract: Large language models (LLMs) are increasingly deployed in specialized domains
such as finance, medicine, and agriculture, where they face significant
distribution shifts from their training data. Domain-specific fine-tuning can
mitigate this challenge but relies on high-quality labeled data that is
expensive and slow to collect in expertise-limited settings. We study
label-free test-time adaptation for language models and present SyTTA, an
inference-time framework that adapts models on-the-fly without additional
supervision. SyTTA couples two complementary uncertainty signals that arise
under distribution shift: input-side perplexity, indicating mismatch with
domain-specific terminology and patterns, and output-side predictive entropy,
indicating diffuse and unstable token probabilities during generation. Across
diverse model architectures and domain-specific benchmarks, SyTTA delivers
consistent gains. Notably, on agricultural question answering, SyTTA improves
Rouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query.
These results show that effective test-time adaptation for language models is
achievable without labeled examples, supporting deployment in label-scarce
domains. The code will be made available upon acceptance.

</details>


### [49] [Text2Token: Unsupervised Text Representation Learning with Token Target Prediction](https://arxiv.org/abs/2510.10224)
*Ruize An,Richong Zhang,Zhijie Nie,Zhanyu Wu,Yanzhao Zhang,Dingkun Long*

Main category: cs.CL

TL;DR: 本文提出了一种名为Text2Token的无监督生成框架，用于文本表示学习。该框架基于token目标预测任务，利用精心构建的目标token分布作为监督信号。实验结果表明，Text2Token在无监督对比学习的最先进的嵌入器LLM2Vec上表现出竞争性能。分析进一步显示，在训练过程中，词汇空间和表示空间共同优化并趋向最优解，为未来的工作提供了新的思路和见解。


<details>
  <summary>Details</summary>
Motivation: 无监督文本表示学习（TRL）是自然语言处理中的基础任务，有助于通过网络的未标记文本提高搜索和推荐。最近的一项实证研究发现，高质量的表示与输入文本的关键token对齐，揭示了表示空间和词汇空间之间的潜在联系。受此发现的启发，我们重新审视生成任务，并开发一个用于TRL的无监督生成框架Text2Token。

Method: 我们重新审视了生成任务，并开发了一个用于文本表示学习的无监督生成框架Text2Token。该框架基于token目标预测任务，利用精心构建的目标token分布作为监督信号。为了构建高质量的目标token分布，我们分析了与先进嵌入器的token对齐属性，并识别出两个关键token类别：(1) 文本中有意义的token，(2) 超出文本的语义派生token。基于这些见解，我们提出了两种方法——数据驱动和模型派生——从数据或LLM主干中构建合成token目标。

Result: 在MTEB v2基准测试上的实验表明，Text2Token在无监督对比学习的最先进的嵌入器LLM2Vec上表现出竞争性能。分析进一步显示，在训练过程中，词汇空间和表示空间共同优化并趋向最优解，为未来的工作提供了新的思路和见解。

Conclusion: 实验结果表明，Text2Token在无监督对比学习的最先进的嵌入器LLM2Vec上表现出竞争性能。分析进一步显示，在训练过程中，词汇空间和表示空间共同优化并趋向最优解，为未来的工作提供了新的思路和见解。

Abstract: Unsupervised text representation learning (TRL) is a fundamental task in
natural language processing, which is beneficial for improving search and
recommendations with the web's unlabeled texts. A recent empirical study finds
that the high-quality representation aligns with the key token of the input
text, uncovering the potential connection between representation space and
vocabulary space. Inspired by the findings, we revisit the generative tasks and
develop an unsupervised generative framework for TRL, Text2Token. The framework
is based on the token target prediction task, utilizing carefully constructed
target token distribution as supervisory signals. To construct the high-quality
target token distribution, we analyze the token-alignment properties with
advanced embedders and identify two essential categories of key tokens: (1) the
meaningful tokens in the text and (2) semantically derived tokens beyond the
text. Based on these insights, we propose two methods -- data-driven and
model-derived -- to construct synthetic token targets from data or the LLM
backbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token
achieves performance competitive with the state-of-the-art embedder with
unsupervised contrastive learning, LLM2Vec. Our analysis further shows that
vocabulary and representation spaces optimize together and toward the optimum
solution during training, providing new ideas and insights for future work.

</details>


### [50] [ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement](https://arxiv.org/abs/2510.10241)
*Kangyang Luo,Yuzhuo Bai,Shuzheng Si,Cheng Gao,Zhitong Wang,Yingli Shen,Wenhao Li,Zhu Liu,Yufeng Han,Jiayi Wu,Cunliang Kong,Maosong Sun*

Main category: cs.CL

TL;DR: ImCoref-CeS is a new framework that combines an enhanced supervised model with LLM-based reasoning to improve Coreference Resolution, achieving better results than existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the dilemma of whether to further explore supervised neural methods or embrace the power of Large Language Models (LLMs) in Coreference Resolution (CR). It aims to effectively combine their strengths.

Method: ImCoref-CeS is a novel framework that integrates an enhanced supervised model with LLM-based reasoning. It includes an improved CR method (ImCoref) with a lightweight bridging module, a biaffine scorer, and a hybrid mention regularization. Additionally, an LLM acts as a multi-role Checker-Splitter agent to validate candidate mentions and coreference results.

Result: Extensive experiments demonstrate the effectiveness of ImCoref-CeS, achieving superior performance compared to existing SOTA methods.

Conclusion: ImCoref-CeS achieves superior performance compared to existing state-of-the-art (SOTA) methods.

Abstract: Coreference Resolution (CR) is a critical task in Natural Language Processing
(NLP). Current research faces a key dilemma: whether to further explore the
potential of supervised neural methods based on small language models, whose
detect-then-cluster pipeline still delivers top performance, or embrace the
powerful capabilities of Large Language Models (LLMs). However, effectively
combining their strengths remains underexplored. To this end, we propose
\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised
model with LLM-based reasoning. First, we present an improved CR method
(\textbf{ImCoref}) to push the performance boundaries of the supervised neural
method by introducing a lightweight bridging module to enhance long-text
encoding capability, devising a biaffine scorer to comprehensively capture
positional information, and invoking a hybrid mention regularization to improve
training efficiency. Importantly, we employ an LLM acting as a multi-role
Checker-Splitter agent to validate candidate mentions (filtering out invalid
ones) and coreference results (splitting erroneous clusters) predicted by
ImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,
which achieves superior performance compared to existing state-of-the-art
(SOTA) methods.

</details>


### [51] [Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models](https://arxiv.org/abs/2510.10252)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: The paper proposes AoU, a framework that improves the faithfulness and accuracy of large language models by ensuring inference is based on validated premises.


<details>
  <summary>Details</summary>
Motivation: Prior work mainly addresses factual hallucinations or relies on post-hoc verification, leaving reasoning-induced hallucinations largely unaddressed. The paper aims to address this gap by proposing a framework that ensures inference is based on validated premises.

Method: AoU is a framework that constrains inference to validated premises through three phases: decomposing a query into candidate assumptions, auditing their support, and conditioning inference only on the validated subset. It is posterior-constrained inference, connecting to selective prediction and rejection learning.

Result: Empirically, AoU improves both accuracy and faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on GSM8K, +45% on MultiArith, and consistent +20-28% improvements on SVAMP over Chain-of-Thought, Self-Consistency, and CoT-Decoding.

Conclusion: AoU improves both accuracy and faithfulness on GSM8K, MultiArith, and SVAMP, achieving significant gains over existing methods.

Abstract: Large language models (LLMs) often generate reasoning traces that appear
coherent but rest on unsupported assumptions, leading to hallucinated
conclusions. Prior work mainly addresses factual hallucinations or relies on
post-hoc verification, leaving reasoning-induced hallucinations largely
unaddressed. We propose Audit-of-Understanding (AoU), a framework that
constrains inference to validated premises through three phases: (1)
decomposing a query into candidate assumptions, (2) auditing their support, and
(3) conditioning inference only on the validated subset. Formally, AoU is
\emph{posterior-constrained inference}, connecting to selective prediction and
rejection learning. Our contributions are threefold: (i) theoretical guarantees
under perfect validation, (ii) excess-risk bounds under imperfect audits, and
(iii) tractability analysis. Empirically, AoU improves both accuracy and
faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on
GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over
Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at
https://anonymous.4open.science/r/audit-of-understanding-E28B.

</details>


### [52] [Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models](https://arxiv.org/abs/2510.10265)
*Liang Lin,Miao Yu,Moayad Aloqaily,Zhenhong Zhou,Kun Wang,Linsey Pang,Prakhar Mehrotra,Qingsong Wen*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Backdoor attacks are a significant threat to large language models (LLMs),
often embedded via public checkpoints, yet existing defenses rely on
impractical assumptions about trigger settings. To address this challenge, we
propose \ourmethod, a defense framework that requires no prior knowledge of
trigger settings. \ourmethod is based on the key observation that when
deliberately injecting known backdoors into an already-compromised model, both
existing unknown and newly injected backdoors aggregate in the representation
space. \ourmethod leverages this through a two-stage process: \textbf{first},
aggregating backdoor representations by injecting known triggers, and
\textbf{then}, performing recovery fine-tuning to restore benign outputs.
Extensive experiments across multiple LLM architectures demonstrate that: (I)
\ourmethod reduces the average Attack Success Rate to 4.41\% across multiple
benchmarks, outperforming existing baselines by 28.1\%$\sim$69.3\%$\uparrow$.
(II) Clean accuracy and utility are preserved within 0.5\% of the original
model, ensuring negligible impact on legitimate tasks. (III) The defense
generalizes across different types of backdoors, confirming its robustness in
practical deployment scenarios.

</details>


### [53] [On the Entity-Level Alignment in Crosslingual Consistency](https://arxiv.org/abs/2510.10280)
*Yihong Liu,Mingyang Wang,François Yvon,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文研究了多语言大型语言模型中跨语言一致性的问题，发现其可能源于实体对齐的失败，并提出了两种方法来提高事实回忆的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型在不同语言中应保持一致的事实知识回忆，但导致这种跨语言一致性及其频繁失败的因素仍不明确。

Method: 本文通过实体级别的翻译任务评估了对齐情况，并提出了SubSub和SubInj两种方法，以整合英语中的主体翻译到跨语言提示中。

Result: 研究发现，一致性与对齐高度相关，主体或对象的不对齐经常导致不一致。通过提出的两种方法，显著提高了事实回忆的准确性和一致性。

Conclusion: 本文提出了SubSub和SubInj两种方法，通过将英语中的主体翻译整合到跨语言提示中，显著提高了事实回忆的准确性和一致性。此外，分析表明这些干预措施通过模型的内部pivot-language处理增强了概念空间中的实体表示对齐，为改进多语言事实预测提供了有效且实用的策略。

Abstract: Multilingual large language models (LLMs) are expected to recall factual
knowledge consistently across languages. However, the factors that give rise to
such crosslingual consistency -- and its frequent failure -- remain poorly
understood. In this work, we hypothesize that these inconsistencies may arise
from failures in entity alignment, the process of mapping subject and object
entities into a shared conceptual space across languages. To test this, we
assess alignment through entity-level (subject and object) translation tasks,
and find that consistency is strongly correlated with alignment across all
studied models, with misalignment of subjects or objects frequently resulting
in inconsistencies. Building on this insight, we propose SubSub and SubInj, two
effective methods that integrate English translations of subjects into prompts
across languages, leading to substantial gains in both factual recall accuracy
and consistency. Finally, our mechanistic analysis reveals that these
interventions reinforce the entity representation alignment in the conceptual
space through model's internal pivot-language processing, offering effective
and practical strategies for improving multilingual factual prediction.

</details>


### [54] [MatryoshkaThinking: Recursive Test-Time Scaling Enables Efficient Reasoning](https://arxiv.org/abs/2510.10293)
*Hongwei Chen,Yishu Lei,Dan Zhang,Bo Ke,Danxiang Zhu,Xuyi Chen,Yuxiang Lu,Zhengjie Huang,Shikun Feng,Jingzhou He,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文提出了MatryoshkaThinking，一种显著减少计算成本同时保持最先进性能的测试时推理方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如DeepConf虽然有效，但通常会带来大量的计算开销。因此，需要一种更高效的测试时推理策略。

Method: MatryoshkaThinking，这是一种新颖的方法，通过递归利用模型内在的推理、验证和总结能力来显著减少计算成本。

Result: MatryoshkaThinking在AIME2025上取得了99.79的分数，仅使用了DeepConf所需计算量的4%。

Conclusion: 这些发现为先进语言模型的高效且可扩展的测试时推理策略的设计提供了新的见解。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
wherein additional computational resources are allocated during inference to
enhance model performance. Recent approaches, such as DeepConf, have
demonstrated the efficacy of this strategy, however, they often incur
substantial computational overhead to achieve competitive results. In this
work, we propose MatryoshkaThinking, a novel method that significantly reduces
computational cost while maintaining state-of-the-art performance.
Specifically, MatryoshkaThinking attains a score of 99.79 on AIME2025 using
only 4% of the computation required by DeepConf. The core of our approach lies
in the recursive exploitation of the model's intrinsic capabilities in
reasoning, verification, and summarization, which collectively enhance the
retention of correct solutions and reduce the disparity between Pass@k and
Pass@1. Comprehensive evaluations across multiple open-source models and
challenging multi-modal reasoning benchmarks validate the effectiveness and
generality of our method. These findings offer new insights into the design of
efficient and scalable test-time inference strategies for advanced language
models.

</details>


### [55] [Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy](https://arxiv.org/abs/2510.10328)
*Ananya Malik,Nazanin Sabri,Melissa Karnaze,Mai Elsherief*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在不同用户群体中的共情能力，并发现模型的共情回应受到人口统计和文化因素的显著影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）自然对话的能力是由它们能够同理心地理解和回应用户的能力所驱动的。然而，情感体验受到人口统计和文化背景的影响。这引发了一个重要问题：LLMs能否在多样化用户群体中表现出公平的同理心？

Method: 我们提出了一种框架来研究LLMs的认知和情感共情如何在由交叉人口统计属性定义的用户角色中变化。我们的研究引入了一种新颖的交叉分析，涵盖了315个独特的角色，这些角色由年龄、文化和性别组合而成，在四个LLMs上进行。

Result: 结果表明，属性深刻地塑造了模型的同理心回应。有趣的是，我们看到同时添加多个属性可以减弱和逆转预期的同理心模式。我们证明它们广泛反映了现实世界的同理心趋势，但对于某些群体（如儒家文化的人）存在明显的不一致。

Conclusion: 我们的研究强调了设计考虑人口统计数据多样性的共情感知LLM的重要性，以促进更包容和公平的模型行为。

Abstract: Large Language Models' (LLMs) ability to converse naturally is empowered by
their ability to empathetically understand and respond to their users. However,
emotional experiences are shaped by demographic and cultural contexts. This
raises an important question: Can LLMs demonstrate equitable empathy across
diverse user groups? We propose a framework to investigate how LLMs' cognitive
and affective empathy vary across user personas defined by intersecting
demographic attributes. Our study introduces a novel intersectional analysis
spanning 315 unique personas, constructed from combinations of age, culture,
and gender, across four LLMs. Results show that attributes profoundly shape a
model's empathetic responses. Interestingly, we see that adding multiple
attributes at once can attenuate and reverse expected empathy patterns. We show
that they broadly reflect real-world empathetic trends, with notable
misalignments for certain groups, such as those from Confucian culture. We
complement our quantitative findings with qualitative insights to uncover model
behaviour patterns across different demographic groups. Our findings highlight
the importance of designing empathy-aware LLMs that account for demographic
diversity to promote more inclusive and equitable model behaviour.

</details>


### [56] [End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs](https://arxiv.org/abs/2510.10329)
*Nam Luu,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文提出了一种结合预训练语音编码器和大型语言模型的端到端架构，用于同时执行自动语音识别和语音翻译，并在英语到德语任务中取得了优于现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统级联方法和最近的端到端方法是语音翻译的两种不同方法，本文旨在探索一种更高效的端到端架构。

Method: 本文探索了预训练语音编码器和大型语言模型（LLMs）的结合端到端架构，以同时执行自动语音识别（ASR）和语音翻译（ST）。

Result: 实验表明，本文的最佳模型在英语到德语的语言对中，不仅优于SeamlessM4T，还可以与基于Whisper和NLLB的级联系统相媲美，最高可获得8%的COMET^DA_{22}指标提升。

Conclusion: 本文提出的模型在英语到德语的翻译任务中表现优于SeamlessM4T，并且可以与基于Whisper和NLLB的级联系统相媲美。

Abstract: Speech Translation (ST) is a machine translation task that involves
converting speech signals from one language to the corresponding text in
another language; this task has two different approaches, namely the
traditional cascade and the more recent end-to-end. This paper explores a
combined end-to-end architecture of pre-trained speech encoders and Large
Language Models (LLMs) for performing both Automatic Speech Recognition (ASR)
and ST simultaneously. Experiments with the English-to-German language pair
show that our best model not only can achieve better translation results than
SeamlessM4T, a large foundational end-to-end, multi-modal translation model,
but can also match the performance of a cascaded system with Whisper and NLLB,
with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric.

</details>


### [57] [ASC analyzer: A Python package for measuring argument structure construction usage in English texts](https://arxiv.org/abs/2510.10384)
*Hakyung Sung,Kristopher Kyle*

Main category: cs.CL

TL;DR: 本文介绍了一个名为ASC分析器的公开Python包，用于分析第二语言学习者的语言结构，并通过计算50个指标来评估其使用情况。


<details>
  <summary>Details</summary>
Motivation: 现有的用于测量ASC使用情况的可扩展且系统化的工具仍然有限。

Method: 本文介绍了ASC分析器，这是一个公开的Python包，旨在填补这一空白。它自动标记ASC并计算50个指标，以捕捉多样性、比例、频率和ASC-动词词素关联强度。

Result: 我们进行了双变量和多变量分析，研究了基于ASC的指标与L2写作分数之间的关系。

Conclusion: 本文介绍了ASC分析器，这是一个公开的Python包，旨在填补这一空白。

Abstract: Argument structure constructions (ASCs) offer a theoretically grounded lens
for analyzing second language (L2) proficiency, yet scalable and systematic
tools for measuring their usage remain limited. This paper introduces the ASC
analyzer, a publicly available Python package designed to address this gap. The
analyzer automatically tags ASCs and computes 50 indices that capture
diversity, proportion, frequency, and ASC-verb lemma association strength. To
demonstrate its utility, we conduct both bivariate and multivariate analyses
that examine the relationship between ASC-based indices and L2 writing scores.

</details>


### [58] [RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models](https://arxiv.org/abs/2510.10390)
*Aashiq Muhamed,Leonardo F. R. Ribeiro,Markus Dreyer,Virginia Smith,Mona T. Diab*

Main category: cs.CL

TL;DR: The paper introduces RefusalBench, a methodology for evaluating the ability of language models to selectively refuse to answer based on flawed context. The study reveals that even advanced models struggle with this task, and the authors release benchmarks and a framework to improve evaluation of this critical capability.


<details>
  <summary>Details</summary>
Motivation: The ability of language models in RAG systems to selectively refuse to answer based on flawed context is critical for safety, yet remains a significant failure point. Static benchmarks fail to reliably evaluate this capability as models exploit dataset-specific artifacts and memorize test instances.

Method: The authors introduce RefusalBench, a generative methodology that programmatically creates diagnostic test cases through controlled linguistic perturbation. They employ 176 distinct perturbation strategies across six categories of informational uncertainty and three intensity levels.

Result: Evaluation of over 30 models uncovers systematic failure patterns: refusal comprises separable detection and categorization skills, and neither scale nor extended reasoning improves performance. The authors find that selective refusal is a trainable, alignment-sensitive capability.

Conclusion: Selective refusal is a trainable, alignment-sensitive capability, offering a clear path for improvement. The authors release two benchmarks and a generation framework to enable continued, dynamic evaluation of this critical capability.

Abstract: The ability of language models in RAG systems to selectively refuse to answer
based on flawed context is critical for safety, yet remains a significant
failure point. Our large-scale study reveals that even frontier models struggle
in this setting, with refusal accuracy dropping below 50% on multi-document
tasks, while exhibiting either dangerous overconfidence or overcaution. Static
benchmarks fail to reliably evaluate this capability, as models exploit
dataset-specific artifacts and memorize test instances. We introduce
RefusalBench, a generative methodology that programmatically creates diagnostic
test cases through controlled linguistic perturbation. Our framework employs
176 distinct perturbation strategies across six categories of informational
uncertainty and three intensity levels. Evaluation of over 30 models uncovers
systematic failure patterns: refusal comprises separable detection and
categorization skills, and neither scale nor extended reasoning improves
performance. We find that selective refusal is a trainable, alignment-sensitive
capability, offering a clear path for improvement. We release two benchmarks --
RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --
and our complete generation framework to enable continued, dynamic evaluation
of this critical capability.

</details>


### [59] [AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval](https://arxiv.org/abs/2510.10397)
*Kai Zhang,Xinyuan Zhang,Ejaz Ahmed,Hongda Jiang,Caleb Kumar,Kai Sun,Zhaojiang Lin,Sanat Sharma,Shereen Oraby,Aaron Colak,Ahmed Aly,Anuj Kumar,Xiaozhong Liu,Xin Luna Dong*

Main category: cs.CL

TL;DR: AssoMem is a novel framework that constructs an associative memory graph to improve context-aware memory recall in QA tasks.


<details>
  <summary>Details</summary>
Motivation: Accurate recall from large scale memories remains a core challenge for memory augmented AI assistants performing question answering (QA), especially in similarity dense scenarios where existing methods mainly rely on semantic distance to the query for retrieval.

Method: AssoMem constructs an associative memory graph that anchors dialogue utterances to automatically extracted clues, and integrates multi-dimensional retrieval signals using an adaptive mutual information (MI) driven fusion strategy.

Result: Extensive experiments across three benchmarks and a newly introduced dataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA baselines.

Conclusion: AssoMem consistently outperforms SOTA baselines, verifying its superiority in context-aware memory recall.

Abstract: Accurate recall from large scale memories remains a core challenge for memory
augmented AI assistants performing question answering (QA), especially in
similarity dense scenarios where existing methods mainly rely on semantic
distance to the query for retrieval. Inspired by how humans link information
associatively, we propose AssoMem, a novel framework constructing an
associative memory graph that anchors dialogue utterances to automatically
extracted clues. This structure provides a rich organizational view of the
conversational context and facilitates importance aware ranking. Further,
AssoMem integrates multi-dimensional retrieval signals-relevance, importance,
and temporal alignment using an adaptive mutual information (MI) driven fusion
strategy. Extensive experiments across three benchmarks and a newly introduced
dataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA
baselines, verifying its superiority in context-aware memory recall.

</details>


### [60] [STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2510.10398)
*Geunyeong Jeong,Juoh Sun,Seonghee Lee,Harksoo Kim*

Main category: cs.CL

TL;DR: 本文提出了一种名为 	extsc{Steam} 的语义级知识编辑框架，旨在提高模型对编辑知识的推理能力和语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的定位和编辑方法主要关注于令牌级似然优化，而没有解决语义连贯性问题。编辑的知识通常以孤立的残差流形式编码在模型的潜在空间中，与现有知识分离并绕过了自然推理过程。

Method: 	extsc{Steam} 首先将目标表示识别为更新事实关联的语义锚点，然后在优化过程中通过对齐损失引导编辑事实的内部表示向这些锚点靠拢。

Result: 实验结果表明，	extsc{Steam} 提高了模型使用编辑知识进行推理的能力，并增强了语义连贯性。

Conclusion: 实验结果表明，	extsc{Steam} 提高了模型使用编辑知识进行推理的能力，并增强了语义连贯性，强调了潜在空间对齐在可靠和连贯的知识编辑中的重要性。

Abstract: Large Language Models store extensive factual knowledge acquired during
large-scale pre-training. However, this knowledge is inherently static,
reflecting only the state of the world at the time of training. Knowledge
editing has emerged as a promising solution for updating outdated or incorrect
facts without full retraining. However, most existing locate-and-edit methods
primarily focus on token-level likelihood optimization without addressing
semantic coherence. Our analysis reveals that such edited knowledge is often
encoded as isolated residual streams in the model's latent space, distinct from
pre-existing knowledge and bypassing natural reasoning process. To address
this, we propose \textsc{Steam}, a semantic-level knowledge editing framework
that enhances integration of updated knowledge into the model's knowledge
structure. \textsc{Steam} first identifies target representations as semantic
anchors for the updated factual association, then guides the internal
representation of the edited fact towards these anchors through an alignment
loss during optimization. Experimental results demonstrate that \textsc{Steam}
improves model's ability to reason with edited knowledge and enhances semantic
coherence, underscoring the importance of latent-space alignment for reliable
and coherent knowledge editing. The code is available at
https://github.com/GY-Jeong/STEAM.

</details>


### [61] [LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints](https://arxiv.org/abs/2510.10415)
*Federica Bologna,Tiffany Pan,Matthew Wilkens,Yue Guo,Lucy Lu Wang*

Main category: cs.CL

TL;DR: LongQAEval是一个用于评估长文本临床问答系统的框架，通过细粒度和粗粒度评估方法提高评估的一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的长文本临床问答系统评估方法资源消耗大且难以保持一致性，需要一种更高效和准确的评估框架。

Method: 基于医生对300个真实患者问题的回答进行注释，比较了答案级和句子级评估方法在正确性、相关性和安全性方面的表现。

Result: 细粒度评估提高了正确性的一致性，而粗粒度评估提高了相关性的一致性，安全性判断仍然不一致。此外，仅标注少量句子即可达到与粗粒度标注相当的可靠性。

Conclusion: LongQAEval框架可以有效评估长文本临床问答系统，通过细粒度和粗粒度评估方法提高评估的一致性和可靠性。

Abstract: Evaluating long-form clinical question answering (QA) systems is
resource-intensive and challenging: accurate judgments require medical
expertise and achieving consistent human judgments over long-form text is
difficult. We introduce LongQAEval, an evaluation framework and set of
evaluation recommendations for limited-resource and high-expertise settings.
Based on physician annotations of 300 real patient questions answered by
physicians and LLMs, we compare coarse answer-level versus fine-grained
sentence-level evaluation over the dimensions of correctness, relevance, and
safety. We find that inter-annotator agreement (IAA) varies by dimension:
fine-grained annotation improves agreement on correctness, coarse improves
agreement on relevance, and judgments on safety remain inconsistent.
Additionally, annotating only a small subset of sentences can provide
reliability comparable to coarse annotations, reducing cost and effort.

</details>


### [62] [Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance](https://arxiv.org/abs/2510.10444)
*Jingyi Chen,Zhimeng Guo,Jiyun Chun,Pichao Wang,Andrew Perrault,Micha Elsner*

Main category: cs.CL

TL;DR: 本文提出了一个基准测试，用于评估大模型在情感理解中对声学线索的利用情况，发现它们主要依赖词汇语义而非声学信息。


<details>
  <summary>Details</summary>
Motivation: 了解从语音中产生的情感需要对词汇和声学线索都有敏感性，但目前不清楚大规模音频语言模型是否真正处理声学信息或主要依赖于词汇内容。

Method: 提出了LISTEN（叙事中的情感词汇与声学测试），这是一个控制性基准，旨在区分词汇依赖和声学敏感性。

Result: 在六个最先进的LALMs的评估中，观察到一致的词汇主导现象。模型在词汇线索中性或缺失时预测“中性”，在线索一致时表现有限，并且在线索冲突下无法分类不同的感情。在副语言设置中，性能接近随机。

Conclusion: 当前的大规模音频语言模型主要依赖于词汇语义，而未能充分利用声学线索。

Abstract: Understanding emotion from speech requires sensitivity to both lexical and
acoustic cues. However, it remains unclear whether large audio language models
(LALMs) genuinely process acoustic information or rely primarily on lexical
content. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in
Narratives), a controlled benchmark designed to disentangle lexical reliance
from acoustic sensitivity in emotion understanding. Across evaluations of six
state-of-the-art LALMs, we observe a consistent lexical dominance. Models
predict "neutral" when lexical cues are neutral or absent, show limited gains
under cue alignment, and fail to classify distinct emotions under cue conflict.
In paralinguistic settings, performance approaches chance. These results
indicate that current LALMs largely "transcribe" rather than "listen," relying
heavily on lexical semantics while underutilizing acoustic cues. LISTEN offers
a principled framework for assessing emotion understanding in multimodal
models.

</details>


### [63] [RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.10448)
*Zhichao Xu,Minheng Wang,Yawei Wang,Wenqian Ye,Yuntao Du,Yunpu Ma,Yijun Tian*

Main category: cs.CL

TL;DR: RECON is a framework that integrates an explicit summarization module to compress evidence within the reasoning loop, improving RAG performance by reducing context length and boosting QA scores.


<details>
  <summary>Details</summary>
Motivation: Retrieval-augmented generation (RAG) systems trained using reinforcement learning (RL) with reasoning are hampered by inefficient context management, where long, noisy retrieved documents increase costs and degrade performance.

Method: RECON integrates an explicit summarization module to compress evidence within the reasoning loop. The summarizer is trained via a two-stage process: relevance pretraining on QA datasets, followed by multi-aspect distillation from proprietary LLMs.

Result: RECON reduces total context length by 35%, leading to improved training speed and inference latency, while simultaneously improving RAG performance on downstream QA benchmarks. It boosts the average EM score of the 3B model by 14.5% and the 7B model by 3.0%, showing particular strength in multi-hop QA.

Conclusion: RECON demonstrates that learned context compression is essential for building practical, scalable, and performant RAG systems.

Abstract: Retrieval-augmented generation (RAG) systems trained using reinforcement
learning (RL) with reasoning are hampered by inefficient context management,
where long, noisy retrieved documents increase costs and degrade performance.
We introduce RECON (REasoning with CONdensation), a framework that integrates
an explicit summarization module to compress evidence within the reasoning
loop. Our summarizer is trained via a two-stage process: relevance pretraining
on QA datasets, followed by multi-aspect distillation from proprietary LLMs to
ensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON
reduces total context length by 35\%, leading to improved training speed and
inference latency, while simultaneously improving RAG performance on downstream
QA benchmarks. Notably, it boosts the average EM score of the 3B model by
14.5\% and the 7B model by 3.0\%, showing particular strength in multi-hop QA.
RECON demonstrates that learned context compression is essential for building
practical, scalable, and performant RAG systems. Our code implementation is
made available at https://github.com/allfornancy/RECON.

</details>


### [64] [Steering Over-refusals Towards Safety in Retrieval Augmented Generation](https://arxiv.org/abs/2510.10452)
*Utsav Maskey,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本文分析了RAG中由于安全过滤器过于严格而导致的过度拒绝现象，并提出了一种名为SafeRAG-Steering的方法来减轻这一问题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在分析检索增强生成（RAG）中安全对齐导致的过度拒绝现象，并提出一种方法来减轻这种问题。

Method: 本文引入了SafeRAG-Steering，这是一种模型中心的嵌入干预方法，旨在在推理时将嵌入区域引导至确认安全、不拒绝的输出区域。

Result: 本文构建了一个名为RagRefuse的领域分层基准，用于分析上下文排列/污染、查询和上下文的领域以及有害文本密度如何触发拒绝行为。此外，SafeRAG-Steering方法有效减少了污染RAG管道中的过度拒绝行为。

Conclusion: 本文提出了一种名为SafeRAG-Steering的模型中心嵌入干预方法，以减少污染RAG管道中的过度拒绝行为，同时保留合法的拒绝行为。

Abstract: Safety alignment in large language models (LLMs) induces over-refusals --
where LLMs decline benign requests due to aggressive safety filters. We analyze
this phenomenon in retrieval-augmented generation (RAG), where both the query
intent and retrieved context properties influence refusal behavior. We
construct RagRefuse, a domain-stratified benchmark spanning medical, chemical,
and open domains, pairing benign and harmful queries with controlled context
contamination patterns and sizes. Our analysis shows that context arrangement /
contamination, domain of query and context, and harmful-text density trigger
refusals even on benign queries, with effects depending on model-specific
alignment choices. To mitigate over-refusals, we introduce
\textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers
the embedding regions towards the confirmed safe, non-refusing output regions
at inference time. This reduces over-refusals in contaminated RAG pipelines
while preserving legitimate refusals.

</details>


### [65] [End-to-end Speech Recognition with similar length speech and text](https://arxiv.org/abs/2510.10453)
*Peng Fan,Wenping Wang,Fei Deng*

Main category: cs.CL

TL;DR: 本文针对语音长度与文本长度接近的情况，提出了两种对齐方法（TIL和AXE），并通过帧融合增强了关键帧的信息。实验结果显示，这些方法显著减少了帧数并优于之前的工作。


<details>
  <summary>Details</summary>
Motivation: 语音长度与文本长度不匹配是自动语音识别（ASR）中的一个挑战。传统的CTC方法在将语音下采样到与文本相似的长度时未能正确对齐语音和文本。

Method: 本文引入了两种对齐方法：a) 时间独立损失（TIL）和b) 基于编辑距离的对齐交叉熵（AXE）损失。为了增强关键帧的信息，我们通过应用权重并求和关键帧及其上下文2帧来实现帧融合。

Result: 在AISHELL-1和AISHELL-2数据集子集上的实验结果表明，所提出的方法优于之前的工作，并且在帧数上至少减少了86%。

Conclusion: 实验结果表明，所提出的方法优于之前的工作，并且在帧数上至少减少了86%。

Abstract: The mismatch of speech length and text length poses a challenge in automatic
speech recognition (ASR). In previous research, various approaches have been
employed to align text with speech, including the utilization of Connectionist
Temporal Classification (CTC). In earlier work, a key frame mechanism (KFDS)
was introduced, utilizing intermediate CTC outputs to guide downsampling and
preserve keyframes, but traditional methods (CTC) failed to align speech and
text appropriately when downsampling speech to a text-similar length. In this
paper, we focus on speech recognition in those cases where the length of speech
aligns closely with that of the corresponding text. To address this issue, we
introduce two methods for alignment: a) Time Independence Loss (TIL) and b)
Aligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance
the information on keyframes, we incorporate frame fusion by applying weights
and summing the keyframe with its context 2 frames. Experimental results on
AISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods
outperform the previous work and achieve a reduction of at least 86\% in the
number of frames.

</details>


### [66] [Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?](https://arxiv.org/abs/2510.10457)
*Shaobo Wang,Cong Wang,Wenjie Fu,Yue Min,Mingquan Feng,Isabel Guan,Xuming Hu,Conghui He,Cunxiang Wang,Kexin Yang,Xingzhang Ren,Fei Huang,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为EssenceBench的框架，通过迭代遗传算法实现了基准压缩，能够在保持模型排名一致性的同时显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 随着对多样模型能力全面评估的需求不断增加，基准套件的规模显著增长。尽管在减少冗余和子集级性能预测方面取得了显著进展，但一种能有效整合这些方法以确保预测准确性和排名一致性的系统框架仍然难以实现。

Method: 本文首先进行了基准冗余的样本级分析，识别出可以删除的高度相似样本，并将基准压缩建模为以分数重建为目标的优化问题。然后提出了EssenceBench框架，结合了基于适应度的子集搜索和基于属性的样本搜索。

Result: 与之前的方法相比，本文的方法在保持较低重建误差的同时，显著提高了效率。在HellaSwag基准（10K样本）上，该方法使用25倍少的样本保留所有模型在5%范围内的排名，并且仅使用200倍少的样本实现了95%的排名保留。

Conclusion: 本文提出了一种名为EssenceBench的粗到细框架，利用迭代遗传算法（GA），在保持模型排名一致性的同时实现了更优的压缩结果。

Abstract: As the demand for comprehensive evaluations of diverse model capabilities
steadily increases, benchmark suites have correspondingly grown significantly
in scale. Despite notable advances in redundancy reduction and subset-level
performance prediction, a systematic framework that effectively integrates
these methods to ensure both prediction accuracy and ranking consistency is
still largely elusive. In this paper, we first perform a sample-level analysis
of benchmark redundancy and identify several highly similar samples that can be
eliminated. Besides, we frame benchmark compression as an optimization problem
with the aim of score reconstruction. Building on these, we then propose
EssenceBench, a coarse-to-fine framework utilizing an iterative Genetic
Algorithm (GA), which takes the advantages of fitness-based subset search and
attribution-based sample search. Compared to previous methods, our approach
yields superior compression results with lower reconstruction error and
markedly higher efficiency. In particular, on the HellaSwag benchmark (10K
samples), our method preserves the ranking of all models shifting within 5%
using 25x fewer samples, and achieves 95% ranking preservation shifting within
5% using only 200x fewer samples.

</details>


### [67] [NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication](https://arxiv.org/abs/2510.10459)
*Prawaal Sharma,Poonam Goyal,Navneet Goyal,Vidisha Sharma*

Main category: cs.CL

TL;DR: 本文介绍了一种新的通用象形符号元语言，结合神经符号AI原则，旨在帮助学术素养较低的人群克服数字鸿沟。


<details>
  <summary>Details</summary>
Motivation: 由于学术素养较低的人群面临显著的障碍，加剧了'数字鸿沟'，因此需要一种超越学术、语言和文化界限的创新通信框架。

Method: 我们引入了一种新的、通用的象形符号元语言，结合了神经符号AI的原则，将复杂的概念分解为简单的原子概念。

Result: 系统具有80%以上的语义可理解性、易于学习的曲线和普遍适应性。

Conclusion: 我们的系统有效地服务于教育水平有限的弱势群体。

Abstract: Digital communication has become the cornerstone of modern interaction,
enabling rapid, accessible, and interactive exchanges. However, individuals
with lower academic literacy often face significant barriers, exacerbating the
"digital divide". In this work, we introduce a novel, universal ideographic
metalanguage designed as an innovative communication framework that transcends
academic, linguistic, and cultural boundaries. Our approach leverages
principles of Neuro-symbolic AI, combining neural-based large language models
(LLMs) enriched with world knowledge and symbolic knowledge heuristics grounded
in the linguistic theory of Natural Semantic Metalanguage (NSM). This enables
the semantic decomposition of complex ideas into simpler, atomic concepts.
Adopting a human-centric, collaborative methodology, we engaged over 200
semi-literate participants in defining the problem, selecting ideographs, and
validating the system. With over 80\% semantic comprehensibility, an accessible
learning curve, and universal adaptability, our system effectively serves
underprivileged populations with limited formal education.

</details>


### [68] [FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth](https://arxiv.org/abs/2510.10472)
*Qiran Zou,Hou Hei Lam,Wenhao Zhao,Yiming Tang,Tingting Chen,Samson Yu,Tianyi Zhang,Chang Liu,Xiangyang Ji,Dianbo Liu*

Main category: cs.CL

TL;DR: 本文介绍了 FML-bench 基准测试和一个统一的评估框架，用于评估自动机器学习研究代理。结果表明，广泛的研究探索策略比专注于狭窄但深入的探索更有效。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试过于强调工程方面而忽视了学术严谨性，且任务多样性有限，无法全面评估代理的科学能力。

Method: 我们引入了FML-bench，一个用于评估自动机器学习研究代理的基准，以及一个统一的评估框架，包含五个互补的指标。

Result: 我们在FML-bench上评估了最先进的自动研究代理，发现采用广泛研究探索策略的代理表现优于那些专注于狭窄但深入探索的代理。

Conclusion: 我们的基准测试是评估自动机器学习研究代理的有效工具，它强调了广泛的研究探索策略可能比专注于狭窄但深入的探索更有效。

Abstract: Large language models (LLMs) have sparked growing interest in automatic
machine learning research agents. Among them, agents capable of autonomously
proposing ideas and conducting machine learning experiments are particularly
promising, as they maximize research automation and accelerate scientific
progress by iteratively refining ideas based on experimental results. However,
comprehensively evaluating such agents remains challenging. Existing benchmarks
tend to overemphasize engineering aspects while neglecting academic rigor,
creating barriers that obscure a clear assessment of an agent's scientific
capabilities in machine learning research. They also suffer from limited task
diversity, an overemphasis on application-oriented tasks over fundamental
research problems, and limited scalability to realistic research settings. To
address these limitations, we introduce FML-bench, a benchmark designed to
evaluate automatic machine learning research agents on 8 diverse and
fundamental machine learning research problems. It reduces coding burden,
emphasizes fundamental problems rather than specific use cases, offers high
task diversity, and is extensible to real-world machine learning GitHub
repositories. Furthermore, we present a unified evaluation framework with five
complementary metrics, designed to comprehensively assess agent performance on
our benchmark. We evaluate state-of-the-art automatic research agents on
FML-bench, and find that agents employing broad research exploration strategies
outperform those focusing on narrow but deep exploration. These findings
suggest that emphasizing the breadth of exploration may lead to more effective
research outcomes than focusing solely on incremental refinement. Our benchmark
is available at https://github.com/qrzou/FML-bench.

</details>


### [69] [When or What? Understanding Consumer Engagement on Digital Platforms](https://arxiv.org/abs/2510.10474)
*Jingyi Wu,Junying Liang*

Main category: cs.CL

TL;DR: 研究通过分析TED演讲，发现时间动态比内容主题对消费者参与的影响更大，挑战了内容特征是流行度主要驱动因素的假设。


<details>
  <summary>Details</summary>
Motivation: 理解什么驱动流行度在当今的数字服务经济中至关重要，其中内容创作者竞争消费者的注意力。

Method: 该研究应用了潜在狄利克雷分布（LDA）建模，分析了大量的TED演讲，将平台视为数字服务提供的案例。

Result: 研究发现创作者的主题供应与观众参与表达的需求之间存在持续的不匹配。纵向分析进一步揭示了时间动态对消费者参与的影响大于主题内容。

Conclusion: 研究结果为数字平台上消费者注意力动态提供了新的见解，并对寻求优化受众参与策略的营销人员、平台管理者和内容创作者具有实际意义。

Abstract: Understanding what drives popularity is critical in today's digital service
economy, where content creators compete for consumer attention. Prior studies
have primarily emphasized the role of content features, yet creators often
misjudge what audiences actually value. This study applies Latent Dirichlet
Allocation (LDA) modeling to a large corpus of TED Talks, treating the platform
as a case of digital service provision in which creators (speakers) and
consumers (audiences) interact. By comparing the thematic supply of creators
with the demand expressed in audience engagement, we identify persistent
mismatches between producer offerings and consumer preferences. Our
longitudinal analysis further reveals that temporal dynamics exert a stronger
influence on consumer engagement than thematic content, suggesting that when
content is delivered may matter more than what is delivered. These findings
challenge the dominant assumption that content features are the primary drivers
of popularity and highlight the importance of timing and contextual factors in
shaping consumer responses. The results provide new insights into consumer
attention dynamics on digital platforms and carry practical implications for
marketers, platform managers, and content creators seeking to optimize audience
engagement strategies.

</details>


### [70] [Assessing Large Language Models for Structured Medical Order Extraction](https://arxiv.org/abs/2510.10475)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 本文介绍了一种使用通用大语言模型进行医疗订单提取的方法，在没有领域特定微调的情况下取得了良好的结果。


<details>
  <summary>Details</summary>
Motivation: 医疗订单提取对于结构化可操作的临床信息、支持决策和实现下游应用（如文档和工作流自动化）至关重要。

Method: 使用了一个通用的、指令调优的LLaMA-4 17B模型，没有进行领域特定的微调，由一个上下文示例引导。

Result: 该方法在MEDIQA-OE 2025共享任务中排名第五，平均F1得分为37.76，特别是在原因和来源准确性方面有显著提升。

Conclusion: 这些结果表明，当与有效的提示工程结合时，大型非领域特定的LLM可以作为专业临床NLP任务的强大、可扩展的基线。

Abstract: Medical order extraction is essential for structuring actionable clinical
information, supporting decision-making, and enabling downstream applications
such as documentation and workflow automation. Orders may be embedded in
diverse sources, including electronic health records, discharge summaries, and
multi-turn doctor-patient dialogues, and can span categories such as
medications, laboratory tests, imaging studies, and follow-up actions. The
MEDIQA-OE 2025 shared task focuses on extracting structured medical orders from
extended conversational transcripts, requiring the identification of order
type, description, reason, and provenance. We present the MasonNLP submission,
which ranked 5th among 17 participating teams with 105 total submissions. Our
approach uses a general-purpose, instruction-tuned LLaMA-4 17B model without
domain-specific fine-tuning, guided by a single in-context example. This
few-shot configuration achieved an average F1 score of 37.76, with notable
improvements in reason and provenance accuracy. These results demonstrate that
large, non-domain-specific LLMs, when paired with effective prompt engineering,
can serve as strong, scalable baselines for specialized clinical NLP tasks.

</details>


### [71] [UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models](https://arxiv.org/abs/2510.10481)
*Guangxin He,Shen Nie,Fengqi Zhu,Yuankang Zhao,Tianyi Bai,Ran Yan,Jie Fu,Chongxuan Li,Binhang Yuan*

Main category: cs.CL

TL;DR: 本文研究了如何通过后训练技术扩展扩散LLM的上下文窗口，并提出了一个名为UltraLLaDA的模型，该模型在长上下文任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散LLM在各种下游任务中表现出巨大的潜力，但它们的长上下文行为仍然未被充分探索。本文旨在研究如何通过后训练技术扩展扩散LLM的上下文窗口而不重新训练。

Method: 我们提出了一种对标准旋转位置嵌入（RoPE）扩展的简单修改，以适应扩散过程中的概率建模，并进一步比较了后训练期间使用的遮罩策略及其对优化稳定性和长距离回忆的影响。

Result: 我们引入了UltraLLaDA，这是一个具有128K标记上下文窗口的扩散LLM，在我们的长上下文任务实证评估中，它显著优于无训练基线。

Conclusion: 我们的实验结果突显了特殊的定位扩展作为将扩散LLM扩展到更长上下文的关键杠杆，并为寻求通过高效后训练实现128K规模上下文的实践者提供了实用指导。

Abstract: Diffusion LLMs have attracted growing interest, with plenty of recent work
emphasizing their great potential in various downstream tasks; yet the
long-context behavior of diffusion LLMs remains largely uncharted. We present a
case study of post-training techniques for extending the context window of
diffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a
simple modification to the standard Rotary Positional Embeddings (RoPE)
extension effectively accommodates the probabilistic modeling inherent in the
diffusion process, enabling stable scaling to longer context ranges. We further
compare masking strategies used during post-training and analyze their impact
on optimization stability and long-range recall. Instantiating these insights,
we introduce UltraLLaDA, a diffusion LLM with a 128K-token context window that,
in our empirical evaluation on long-context tasks, significantly outperforms
training-free baselines. Our experimental results highlight the special
positional extension as a key lever for scaling diffusion LLMs to extended
contexts and offer practical guidance for practitioners seeking 128K-scale
context via efficient post-training.

</details>


### [72] [VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction](https://arxiv.org/abs/2510.10490)
*Prawaal Sharma,Poonam Goyal,Vidisha Sharma,Navneet Goyal*

Main category: cs.CL

TL;DR: 本文提出了一种基于对比学习的OCR方法VOLTAGE，用于低资源语言的数字包容。在Takri脚本上取得了较高的准确率，并展示了其在其他印地语脚本上的通用性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏针对低资源语言的无监督光学字符识别（OCR）方法，这些语言面临更大的灭绝风险。因此，需要将数字包容带入这些语言以避免其灭绝。

Method: 本文提出了VOLTAGE，这是一种基于对比学习的OCR方法，利用自动字形特征推荐进行基于聚类的标记。通过图像变换和生成对抗网络增强标记数据的多样性和数量。

Result: 在Takri脚本上，VOLTAGE实现了95%的机器打印样本准确率和87%的手写样本准确率。此外，还进行了基线和消融研究，并构建了Takri的下游用例，证明了该工作的实用性。

Conclusion: 本文提出了一种基于对比学习的OCR方法VOLTAGE，用于低资源语言的数字包容。实验结果表明，该方法在Takri脚本上取得了95%的机器打印样本准确率和87%的手写样本准确率，并展示了其在其他印地语脚本上的通用性。

Abstract: UNESCO has classified 2500 out of 7000 languages spoken worldwide as
endangered. Attrition of a language leads to loss of traditional wisdom, folk
literature, and the essence of the community that uses it. It is therefore
imperative to bring digital inclusion to these languages and avoid its
extinction. Low resource languages are at a greater risk of extinction. Lack of
unsupervised Optical Character Recognition(OCR) methodologies for low resource
languages is one of the reasons impeding their digital inclusion. We propose
VOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph
feature recommendation for cluster-based labelling. We augment the labelled
data for diversity and volume using image transformations and Generative
Adversarial Networks. Voltage has been designed using Takri - a family of
scripts used in 16th to 20th century in the Himalayan regions of India. We
present results for Takri along with other Indic scripts (both low and high
resource) to substantiate the universal behavior of the methodology. An
accuracy of 95% for machine printed and 87% for handwritten samples on Takri
script has been achieved. We conduct baseline and ablation studies along with
building downstream use cases for Takri, demonstrating the usefulness of our
work.

</details>


### [73] [Merlin's Whisper: Enabling Efficient Reasoning in LLMs via Black-box Adversarial Prompting](https://arxiv.org/abs/2510.10528)
*Heming Xia,Cunxiao Du,Rui Li,Chak Tou Leong,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为AdvPrompt的迭代优化框架，通过黑盒对抗提示减少大型推理模型中的过度思考，从而提高模型效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在处理复杂推理任务时需要大量的计算和延迟，这阻碍了它们的实际部署。因此，需要一种方法来减少推理过程中的冗余，同时保持性能。

Method: AdvPrompt是一种迭代优化框架，从不同角度生成高质量的对抗性提示，以获得简洁的响应而不牺牲准确性。

Result: AdvPrompt在多个基准测试中表现出色，显著减少了令牌使用量，同时保持了性能。例如，在GSM8K数据集上，Qwen3模型系列的平均响应长度减少了3倍，而在四个基准测试中平均减少了40%。对于封闭源API，AdvPrompt在MATH-500数据集上分别减少了Claude-3.7和Gemini-2.5的35%和47%的令牌使用量。

Conclusion: AdvPrompt展示了通过黑盒对抗提示减少大型推理模型中的过度思考的潜力，为提高LRM效率提供了一种实用且有效的方法。

Abstract: Large reasoning models (LRMs) have demonstrated remarkable proficiency in
tackling complex reasoning tasks through step-by-step thinking. However, such a
lengthy reasoning process incurs substantial computational and latency
overheads, hindering the practical deployment of these models. In this work, we
present a new perspective on mitigating overthinking in LRMs via black-box
adversarial prompting. By treating both open-source LRMs and closed-source APIs
as black-box communicators, we investigate how to elicit concise responses
without sacrificing accuracy. We introduce AdvPrompt, an iterative refinement
framework that generates high-quality adversarial prompts from diverse
perspectives. Experiments across multiple benchmarks demonstrate that AdvPrompt
consistently reduces token usage while preserving performance. Notably,
AdvPrompt achieves a 3x reduction in average response length on simple GSM8K
questions for the Qwen3 model series, and delivers an average ~40% token
reduction across four benchmarks. For closed-source APIs, AdvPrompt reduces
token usage on MATH-500 by 35% for Claude-3.7 and 47% for Gemini-2.5. Further
analysis reveals the generalizability of AdvPrompt across various model scales
and families, underscoring the potential of black-box prompting as a practical
and effective strategy for enhancing LRM efficiency.

</details>


### [74] [Detecting Hallucinations in Authentic LLM-Human Interactions](https://arxiv.org/abs/2510.10539)
*Yujie Ren,Niklas Gruhlke,Anne Lauscher*

Main category: cs.CL

TL;DR: 本文介绍了基于真实LLM-人类对话的幻觉检测基准测试AuthenHallu，并发现当前LLM在幻觉检测方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测基准大多人为构造，无法充分捕捉真实世界中的幻觉特征，因此需要一种基于真实对话的基准测试方法。

Method: 本文通过从真实的LLM-人类对话中选择和注释样本，构建了AuthenHallu基准测试。

Result: 统计分析显示，31.4%的查询-响应对中存在幻觉，而在数学和数字问题等困难领域，这一比例高达60%。此外，使用普通LLM作为幻觉检测器的效果有限。

Conclusion: 本文提出了一种新的基准测试方法AuthenHallu，以更好地反映真实世界中LLM的幻觉情况，并发现当前的LLM作为幻觉检测器仍存在不足。

Abstract: As large language models (LLMs) are increasingly applied in sensitive domains
such as medicine and law, hallucination detection has become a critical task.
Although numerous benchmarks have been proposed to advance research in this
area, most of them are artificially constructed--either through deliberate
hallucination induction or simulated interactions--rather than derived from
genuine LLM-human dialogues. Consequently, these benchmarks fail to fully
capture the characteristics of hallucinations that occur in real-world usage.
To address this limitation, we introduce AuthenHallu, the first hallucination
detection benchmark built entirely from authentic LLM-human interactions. For
AuthenHallu, we select and annotate samples from genuine LLM-human dialogues,
thereby providing a faithful reflection of how LLMs hallucinate in everyday
user interactions. Statistical analysis shows that hallucinations occur in
31.4% of the query-response pairs in our benchmark, and this proportion
increases dramatically to 60.0% in challenging domains such as Math & Number
Problems. Furthermore, we explore the potential of using vanilla LLMs
themselves as hallucination detectors and find that, despite some promise,
their current performance remains insufficient in real-world scenarios.

</details>


### [75] [BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices](https://arxiv.org/abs/2510.10560)
*Euhid Aman,Esteban Carlin,Hsing-Kuo Pao,Giovanni Beltrame,Ghaluh Indah Permata Sari,Yie-Tarng Chen*

Main category: cs.CL

TL;DR: BitMar 是一种量化多模态变压器，利用情景记忆和滑动窗口注意力机制，在边缘设备上实现高效的图像文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有的跨注意力变压器和多模态视觉语言模型在接地和生成方面表现出色，但其广泛的全精度主干使其难以在边缘设备上部署。记忆增强架构可以提高过去上下文的利用率，但大多数工作很少将它们与激进的边缘导向量化相结合。

Method: BitMar 引入了一种外部类人情景记忆，利用1.58位编码器创建紧凑的嵌入，并结合使用查询固定大小的键值情景记忆。解码器应用了逐层条件化和滑动窗口注意力机制，以在严格的内存预算下处理长或流输入。

Result: BitMar 在低延迟下实现了竞争性的图像文本生成和多模态理解，同时具有小模型足迹。

Conclusion: BitMar 是一种适合边缘部署的量化多模态变压器，具有低延迟和小模型足迹的特点。

Abstract: Cross-attention transformers and other multimodal vision-language models
excel at grounding and generation; however, their extensive, full-precision
backbones make it challenging to deploy them on edge devices. Memory-augmented
architectures enhance the utilization of past context; however, most works
rarely pair them with aggressive edge-oriented quantization. We introduce
BitMar, a quantized multimodal transformer that proposes an external human-like
episodic memory for effective image-text generation on hardware with limited
resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and
one for vision (DiNOv2-based), to create compact embeddings that are combined
and used to query a fixed-size key-value episodic memory. During vector
retrieval, the BitNet decoder applies per-layer conditioning, which increases
the contextual relevance of generated content. The decoder also employs
attention sinks with a sliding-window mechanism to process long or streaming
inputs under tight memory budgets. The combination of per-layer conditioning
and sliding-window attention achieves a strong quality-speed trade-off,
delivering competitive captioning and multimodal understanding at low latency
with a small model footprint. These characteristics make BitMar well-suited for
edge deployment.

</details>


### [76] [Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models](https://arxiv.org/abs/2510.10613)
*Di Wu abd Shuaidong Pan*

Main category: cs.CL

TL;DR: 本文提出了一种基于时间大语言模型的动态主题演化建模框架，通过结合时间衰减函数和注意力机制，以及状态转移矩阵，实现了对主题动态演化的有效捕捉，并在多个指标上表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和分析大规模文本中的动态语义模式，需要一种能够统一建模语义表示和时间演化的框架。

Method: 该方法首先使用大型语言模型获取文本的上下文嵌入，然后引入时间衰减函数和注意力机制，这些组件允许模型根据时间间隔调整语义单元的重要性，并捕捉不同时间段的主题变化。时间表示被映射到潜在主题空间中，其中应用状态转移矩阵来描述主题的动态演变。联合优化目标约束了语义建模和时间一致性，确保主题生成的多样性和平滑性。

Result: 实验结果表明，该框架能够有效捕捉主题的生成、扩展和衰退，并在多个指标上优于现有模型。

Conclusion: 该方法为理解大规模文本中的动态语义模式提供了一个系统性的解决方案，丰富了主题建模的研究范式，并支持多个领域的复杂文本分析任务。

Abstract: This paper proposes a modeling framework for dynamic topic evolution based on
temporal large language models. The method first uses a large language model to
obtain contextual embeddings of text and then introduces a temporal decay
function and an attention mechanism. These components allow the model to adjust
the importance of semantic units according to time intervals and capture topic
variations across different periods. The temporal representations are then
mapped into a latent topic space, where a state transition matrix is applied to
describe the dynamic evolution of topics. A joint optimization objective
constrains both semantic modeling and temporal consistency, ensuring diversity
and smoothness in topic generation. The design emphasizes the unified modeling
of semantic representation and temporal evolution, which improves topic
coherence and diversity while enhancing stability and interpretability over
time. Experiments on real-world corpora show that the framework effectively
captures the generation, expansion, and decline of topics and outperforms
existing models across multiple metrics. Overall, the proposed method provides
a systematic solution for understanding dynamic semantic patterns in
large-scale text, enriches the research paradigm of topic modeling, and
supports complex text analysis tasks in multiple domains.

</details>


### [77] [Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization](https://arxiv.org/abs/2510.10618)
*Bowei He,Lihao Yin,Huiling Zhen,Shuqi Liu,Han Wu,Xiaokun Zhang,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 本文研究了校准数据对LLM能力的影响，提出了一个基于激活模式的校准数据整理框架，以提高后训练压缩方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的研究很少系统地探讨校准数据对LLM能力的影响，特别是在组合属性和领域对应性方面。因此，本文旨在填补这一空白。

Method: 本文从激活模式的角度分析了校准数据对LLM能力的影响，并提出了一个基于观察和分析的校准数据整理框架。

Result: 本文发现，激活空间中的表示性和多样性更根本地决定了校准数据的质量，并提出了一个校准数据整理框架来增强现有后训练压缩方法的性能。

Conclusion: 本文提出了一种基于校准数据表示性和多样性的校准数据整理框架，以提高现有后训练压缩方法在保持关键LLM能力方面的性能。

Abstract: Post-training compression has been a widely employed approach to scale down
large language model (LLM) and facilitate efficient inference. In various
proposed compression methods, including pruning and quantization, calibration
data plays a vital role by informing the weight importance and activation
dynamic ranges. However, how calibration data impacts the LLM capability after
compression is less explored. Few of the existing works, though recognizing the
significance of this study, only investigate the language modeling or
commonsense reasoning performance degradation from limited angles, like the
data sources or sample amounts. More systematic research is still needed to
examine the impacts on different LLM capabilities in terms of compositional
properties and domain correspondence of calibration data. In this work, we aim
at bridging this gap and further analyze underlying influencing mechanisms from
the activation pattern perspective. Especially, we explore the calibration
data's impacts on high-level complex reasoning capabilities, like math problem
solving and code generation. Delving into the underlying mechanism, we find
that the representativeness and diversity in activation space more
fundamentally determine the quality of calibration data. Finally, we propose a
calibration data curation framework based on such observations and analysis,
enhancing the performance of existing post-training compression methods on
preserving critical LLM capabilities. Our code is provided in
\href{https://github.com/BokwaiHo/COLA.git}{Link}.

</details>


### [78] [FactAppeal: Identifying Epistemic Factual Appeals in News Media](https://arxiv.org/abs/2510.10627)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 本文提出了一种新的任务Epistemic Appeal Identification，并构建了一个数据集FactAppeal来分析事实性陈述的证据结构，实验表明模型效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注于声明检测和验证，而本文旨在更细致地分析事实性陈述背后的证据结构和认知依据。

Method: 本文提出了Epistemic Appeal Identification任务，并构建了一个手动标注的数据集FactAppeal，用于分析事实性陈述的证据结构。同时，使用了一系列编码器和生成解码器模型进行实验。

Result: 本文的最佳模型基于Gemma 2 9B，达到了0.73的宏F1分数。

Conclusion: 本文提出了一个名为FactAppeal的数据集，用于识别事实性陈述的证据基础，并通过实验验证了模型的效果。

Abstract: How is a factual claim made credible? We propose the novel task of Epistemic
Appeal Identification, which identifies whether and how factual statements have
been anchored by external sources or evidence. To advance research on this
task, we present FactAppeal, a manually annotated dataset of 3,226
English-language news sentences. Unlike prior resources that focus solely on
claim detection and verification, FactAppeal identifies the nuanced epistemic
structures and evidentiary basis underlying these claims and used to support
them. FactAppeal contains span-level annotations which identify factual
statements and mentions of sources on which they rely. Moreover, the
annotations include fine-grained characteristics of factual appeals such as the
type of source (e.g. Active Participant, Witness, Expert, Direct Evidence),
whether it is mentioned by name, mentions of the source's role and epistemic
credentials, attribution to the source via direct or indirect quotation, and
other features. We model the task with a range of encoder models and generative
decoder models in the 2B-9B parameter range. Our best performing model, based
on Gemma 2 9B, achieves a macro-F1 score of 0.73.

</details>


### [79] [You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News](https://arxiv.org/abs/2510.10658)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 本文通过比较CNN和福克斯新闻的报道，分析了事实报道背后的认识论策略，并发现它们在来源使用上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管媒体偏见被广泛研究，但事实报道背后的认识论策略在计算上仍缺乏探索。本文旨在分析这些策略。

Method: 本文通过大规模比较CNN和福克斯新闻的报道，分析了这些策略。为了将报道风格与主题选择隔离，我们采用文章匹配策略来比较同一事件的报道，并应用FactAppeal框架对超过47万篇文章的语料库进行分析，涵盖了两个高度政治化的时期：新冠疫情和以色列-哈马斯战争。

Result: CNN的报道包含更多的事实陈述，并且更有可能以外部来源为依据。这些媒体也表现出截然不同的来源模式：CNN通过引用专家和专家文件来建立可信度，构建正式权威的诉求，而福克斯新闻则更倾向于新闻报道和直接引语。

Conclusion: 本文量化了偏党派的媒体如何系统地使用不同的认识论策略来构建现实，为媒体偏见的研究增添了新的维度。

Abstract: While media bias is widely studied, the epistemic strategies behind factual
reporting remain computationally underexplored. This paper analyzes these
strategies through a large-scale comparison of CNN and Fox News. To isolate
reporting style from topic selection, we employ an article matching strategy to
compare reports on the same events and apply the FactAppeal framework to a
corpus of over 470K articles covering two highly politicized periods: the
COVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting
contains more factual statements and is more likely to ground them in external
sources. The outlets also exhibit sharply divergent sourcing patterns: CNN
builds credibility by citing Experts} and Expert Documents, constructing an
appeal to formal authority, whereas Fox News favors News Reports and direct
quotations. This work quantifies how partisan outlets use systematically
different epistemic strategies to construct reality, adding a new dimension to
the study of media bias.

</details>


### [80] [AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation](https://arxiv.org/abs/2510.10661)
*Omid Reza Heidari,Siobhan Reid,Yassine Yaakoubi*

Main category: cs.CL

TL;DR: AGENTIQL 是一个受代理启发的多专家框架，用于改进文本到 SQL 的生成，提高执行准确性和可解释性，并通过路由机制缩小了与 GPT-4 基础的 SOTA 的差距。


<details>
  <summary>Details</summary>
Motivation: LLMs 在文本到 SQL 生成方面取得了进展，但单体架构在处理复杂推理和模式多样性时存在困难。

Method: AGENTIQL 是一个受代理启发的多专家框架，结合了用于问题分解的推理代理、用于子查询生成的编码代理以及用于列选择的精炼步骤。自适应路由器通过在模块化流水线和基线解析器之间进行选择来平衡效率和准确性。

Result: 在 Spider 基准测试中，AGENTIQL 提高了执行准确性和可解释性，并使用 Planner&Executor 合并策略实现了高达 86.07% 的 EX。性能取决于路由机制的有效性，从而缩小了与 GPT-4 基础的 SOTA（89.65% EX）的差距，同时使用了更小的开源 LLM。

Conclusion: AGENTIQL 提高了执行准确性和可解释性，并通过路由机制缩小了与 GPT-4 基础的 SOTA 的差距，同时使用了更小的开源 LLM。此外，它通过暴露中间推理步骤增强了透明度，提供了一种强大、可扩展且可解释的语义解析方法。

Abstract: LLMs have advanced text-to-SQL generation, yet monolithic architectures
struggle with complex reasoning and schema diversity. We propose AGENTIQL, an
agent-inspired multi-expert framework that combines a reasoning agent for
question decomposition, a coding agent for sub-query generation, and a
refinement step for column selection. An adaptive router further balances
efficiency and accuracy by selecting between our modular pipeline and a
baseline parser. Several steps in the pipeline can be executed in parallel,
making the framework scalable to larger workloads. Evaluated on the Spider
benchmark, AGENTIQL improves execution accuracy and interpretability and
achieves up to 86.07\% EX with 14B models using the Planner&Executor merging
strategy. The attained performance is contingent upon the efficacy of the
routing mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)
while using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances
transparency by exposing intermediate reasoning steps, offering a robust,
scalable, and interpretable approach to semantic parsing.

</details>


### [81] [BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions](https://arxiv.org/abs/2510.10666)
*Zhengbo Zhang,Zhiheng Lyu,Junhao Gong,Hongzhu Yi,Xinming Wang,Yuxuan Zhou,Jiabing Yang,Ping Nie,Yan Huang,Wenhu Chen*

Main category: cs.CL

TL;DR: This paper proposes BrowserAgent, an interactive agent that solves complex tasks through human-inspired browser actions. It uses a two-stage training method and an explicit memory mechanism to enhance its reasoning capabilities. BrowserAgent achieves better results than existing methods on various tasks, indicating its potential as a more advanced framework for web agents.


<details>
  <summary>Details</summary>
Motivation: Efficiently solving real-world problems with LLMs increasingly hinges on their ability to interact with dynamic web environments and autonomously acquire external information. While recent research like Search-R1 and WebDancer demonstrates strong performance in solving web tasks, they heavily rely on additional tools to convert the interactive web environment into static text content. This is in contrast to human browsing behaviors, which involve diverse interactions with the browser, such as scrolling, clicking, and typing.

Method: We propose BrowserAgent, a more interactive agent that solves complex tasks through human-inspired browser actions. BrowserAgent operates directly on raw web pages via Playwright through a set of predefined browser actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities. Additionally, we introduce an explicit memory mechanism to store key conclusions across steps, further enhancing the model's reasoning capabilities for long-horizon tasks.

Result: Despite using significantly less training data than Search-R1, BrowserAgent achieves more competitive results across different Open-QA tasks. Additionally, we introduce an explicit memory mechanism to store key conclusions across steps, further enhancing the model's reasoning capabilities for long-horizon tasks. Notably, BrowserAgent-7B can achieve around 20\% improvement over Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle.

Conclusion: BrowserAgent can serve as a more advanced framework for more interactive and scalable web agents.

Abstract: Efficiently solving real-world problems with LLMs increasingly hinges on
their ability to interact with dynamic web environments and autonomously
acquire external information. While recent research like Search-R1 and
WebDancer demonstrates strong performance in solving web tasks, they heavily
rely on additional tools to convert the interactive web environment into static
text content. This is in contrast to human browsing behaviors, which involve
diverse interactions with the browser, such as scrolling, clicking, and typing.
In this paper, we propose BrowserAgent, a more interactive agent that solves
complex tasks through human-inspired browser actions. BrowserAgent operates
directly on raw web pages via Playwright through a set of predefined browser
actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and
Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities.
Despite using significantly less training data than Search-R1, BrowserAgent
achieves more competitive results across different Open-QA tasks. Additionally,
we introduce an explicit memory mechanism to store key conclusions across
steps, further enhancing the model's reasoning capabilities for long-horizon
tasks. Notably, BrowserAgent-7B can achieve around 20\% improvement over
Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These
results indicate that BrowserAgent can serve as a more advanced framework for
more interactive and scalable web agents.

</details>


### [82] [Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data](https://arxiv.org/abs/2510.10677)
*Zhuowei Chen,Bowei Zhang,Nankai Lin,Tian Hou,Lianxi Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于推理的多语言安全防护方法ConsistentGuard，通过推理增强可解释性，并通过对齐提升语言间知识迁移。该方法在三个数据集上的六种语言中表现出色，优于使用大量数据训练的更大模型，并展现出强大的可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于分类器的方法缺乏可解释性，并且在低资源语言上表现不佳，因此需要一种更有效的多语言安全防护方法。

Method: 本文提出了一种基于推理的多语言安全防护方法ConsistentGuard，通过推理增强可解释性，并通过对齐提升语言间知识迁移。

Result: ConsistentGuard在三个数据集上的六种语言中表现出色，优于使用大量数据训练的更大模型，并展现出强大的可解释性和泛化能力。

Conclusion: 本文提出了ConsistentGuard，一种基于推理的多语言安全防护方法，通过推理增强可解释性，并通过对齐提升语言间知识迁移。该方法在三个数据集上的六种语言中表现出色，优于使用大量数据训练的更大模型，并展现出强大的可解释性和泛化能力。此外，本文还贡献了一个多语言基准扩展并发布了代码以支持未来的研究。

Abstract: Recent advances in LLMs have enhanced AI capabilities, but also increased the
risk posed by malicious requests, highlighting the need for effective LLM
safeguards to detect such queries. Existing approaches largely rely on
classifier-based methods that lack interpretability and perform poorly on
low-resource languages. To address these limitations, we propose
ConsistentGuard, a novel reasoning-based multilingual safeguard, which enhances
explainability via reasoning and boosts knowledge transfer between languages
through alignment. With only 1,000 training samples, our method demonstrates
superior performance on three datasets across six languages, outperforming
larger models trained with significantly more data, and exhibits strong
interpretability and generalization ability. We also contribute a multilingual
benchmark extension and release our codes to support future research.

</details>


### [83] [RePro: Training Language Models to Faithfully Recycle the Web for Pretraining](https://arxiv.org/abs/2510.10681)
*Zichun Yu,Chenyan Xiong*

Main category: cs.CL

TL;DR: RePro is a novel web recycling method that trains a small LM with reinforcement learning to generate high-quality rephrasings of pretraining data, resulting in significant improvements in accuracy and data efficiency.


<details>
  <summary>Details</summary>
Motivation: High-quality pretraining data is essential for large language models (LLMs), but its reserves are running low for frontier models.

Method: RePro is a novel web recycling method that trains a relatively small LM with reinforcement learning to generate effective and faithful rephrasings of pretraining data. It uses one quality reward and three faithfulness rewards to optimize the LM rephraser.

Result: RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on 22 downstream tasks. It also outperforms ReWire, the state-of-the-art web recycling method, and improves organic data efficiency by 2-3x.

Conclusion: RePro provides an efficient and controllable path to effectively harness the fossil fuel of LLM pretraining.

Abstract: High-quality pretraining data is the fossil fuel of large language models
(LLMs), yet its reserves are running low for frontier models. In this paper, we
introduce RePro, a novel web recycling method that trains a relatively small LM
with reinforcement learning to generate effective and faithful rephrasings of
pretraining data. Specifically, we design one quality reward and three
faithfulness rewards, optimizing the LM rephraser to convert organic data into
high-quality rephrasings while maintaining its core semantics and structure. In
our experiment, we train a 4B rephraser to recycle 72B tokens sampled from
DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that
RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on
22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web
recycling method that prompts a 70B rephraser, as well as the organic baseline
with a 4x larger data pool. Experiments with different amounts of recycled data
highlight that RePro improves organic data efficiency by 2-3x. Individual and
distributional analyses validate that RePro preserves more critical information
and faithfully reflects the characteristics of organic data compared to
prompting-based methods. Together, these results show that RePro provides an
efficient and controllable path to effectively harness the fossil fuel of LLM
pretraining. We open-source our code, rephraser, and recycled data at
https://github.com/cxcscmu/RePro.

</details>


### [84] [Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning Framework](https://arxiv.org/abs/2510.10729)
*Manas Zambre,Sarika Bobade*

Main category: cs.CL

TL;DR: 本文提出了一种基于深度学习的讽刺检测框架，结合了DCNN和BERT等技术，用于分析语言、情感和上下文线索，具有在实际应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 讽刺是一种微妙且常被误解的交流形式，尤其是在文本中缺乏语气和肢体语言的情况下。

Method: 提出了一种模块化的深度学习框架，利用深度卷积神经网络（DCNNs）和上下文模型如BERT来分析语言、情感和上下文线索。

Result: 系统通过多层架构集成了情感分析、上下文嵌入、语言特征提取和情感检测。

Conclusion: 该模型在概念阶段，但展示了在聊天机器人和社会媒体分析等实际应用中的可行性。

Abstract: Sarcasm is a nuanced and often misinterpreted form of communication,
especially in text, where tone and body language are absent. This paper
proposes a modular deep learning framework for sarcasm detection, leveraging
Deep Convolutional Neural Networks (DCNNs) and contextual models such as BERT
to analyze linguistic, emotional, and contextual cues. The system integrates
sentiment analysis, contextual embeddings, linguistic feature extraction, and
emotion detection through a multi-layer architecture. While the model is in the
conceptual stage, it demonstrates feasibility for real-world applications such
as chatbots and social media analysis.

</details>


### [85] [Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis](https://arxiv.org/abs/2510.10762)
*Wenqing Zhang,Trang Nguyen,Elizabeth A. Stuart,Yiqun T. Chen*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型在自动化方法学评估中的表现，发现它们在识别显式方法学特征方面表现良好，但在需要细致解释的情况下仍需人类监督。


<details>
  <summary>Details</summary>
Motivation: 系统综述对于综合科学证据至关重要，但仍然耗时费力，尤其是在提取详细的方法学信息时。大型语言模型（LLMs）为自动化方法学评估提供了潜力，有望改变证据综合。

Method: 使用因果中介分析作为代表性方法学领域，对最先进的LLMs与专家人类评审员进行了基准测试，评估了180篇全文科学文章。

Result: 模型性能与人类判断密切相关（准确率相关性0.71；F1相关性0.97），在简单、明确陈述的方法学标准上达到了接近人类的准确性。然而，在复杂、需要推理的评估中，准确性急剧下降，比专家评审员低多达15%。错误通常源于表面的语言线索，例如模型经常将“纵向”或“敏感性”等关键词误解为严格方法学方法的自动证据，导致系统性误分类。较长的文档会导致模型准确性降低，而出版年份则没有显著影响。

Conclusion: 当前的LLMs在识别显式的元方法特征方面表现良好，但在需要细致解释的情况下仍需人类监督。将自动化信息提取与有针对性的专家审查相结合，为提高证据综合的效率和方法严谨性提供了有希望的途径。

Abstract: Systematic reviews are crucial for synthesizing scientific evidence but
remain labor-intensive, especially when extracting detailed methodological
information. Large language models (LLMs) offer potential for automating
methodological assessments, promising to transform evidence synthesis. Here,
using causal mediation analysis as a representative methodological domain, we
benchmarked state-of-the-art LLMs against expert human reviewers across 180
full-text scientific articles. Model performance closely correlated with human
judgments (accuracy correlation 0.71; F1 correlation 0.97), achieving
near-human accuracy on straightforward, explicitly stated methodological
criteria. However, accuracy sharply declined on complex, inference-intensive
assessments, lagging expert reviewers by up to 15%. Errors commonly resulted
from superficial linguistic cues -- for instance, models frequently
misinterpreted keywords like "longitudinal" or "sensitivity" as automatic
evidence of rigorous methodological approache, leading to systematic
misclassifications. Longer documents yielded lower model accuracy, whereas
publication year showed no significant effect. Our findings highlight an
important pattern for practitioners using LLMs for methods review and synthesis
from full texts: current LLMs excel at identifying explicit methodological
features but require human oversight for nuanced interpretations. Integrating
automated information extraction with targeted expert review thus provides a
promising approach to enhance efficiency and methodological rigor in evidence
synthesis across diverse scientific fields.

</details>


### [86] [HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon](https://arxiv.org/abs/2510.10776)
*James Ald Teves,Ray Daniel Cal,Josh Magdiel Villaluz,Jean Malolos,Mico Magtira,Ramon Rodriguez,Mideth Abisado,Joseph Marvin Imperial*

Main category: cs.CL

TL;DR: 本研究介绍了HiligayNER，这是第一个公开可用的希利盖农语命名实体识别（NER）基准模型。通过使用超过8000个标注句子的数据集，并对两种基于Transformer的模型进行微调，取得了优异的性能。此外，跨语言评估显示了良好的可迁移性，表明该模型在低资源环境下的多语言NLP中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 希利盖农语在语言处理研究中仍被低估，因为缺乏标注语料库和基准模型。

Method: 本研究引入了HiligayNER，这是第一个公开可用的希利盖农语命名实体识别（NER）基准模型。使用包含超过8000个标注句子的数据集构建了HiligayNER，这些句子来自公开的新闻文章、社交媒体帖子和文学文本。对两种基于Transformer的模型mBERT和XLM-RoBERTa进行了微调，以构建HiligayNER版本。

Result: 评估结果表明，两种模型在所有实体类型上的精确度、召回率和F1分数均超过80%。此外，与宿务语和他加禄语的跨语言评估显示了良好的可迁移性，表明HiligayNER在低资源环境下的多语言NLP中的广泛适用性。

Conclusion: 本研究旨在为菲律宾未被充分代表的语言，特别是希利盖农语，贡献语言技术发展，并支持未来区域语言处理的研究。

Abstract: The language of Hiligaynon, spoken predominantly by the people of Panay
Island, Negros Occidental, and Soccsksargen in the Philippines, remains
underrepresented in language processing research due to the absence of
annotated corpora and baseline models. This study introduces HiligayNER, the
first publicly available baseline model for the task of Named Entity
Recognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains
over 8,000 annotated sentences collected from publicly available news articles,
social media posts, and literary texts. Two Transformer-based models, mBERT and
XLM-RoBERTa, were fine-tuned on this collected corpus to build versions of
HiligayNER. Evaluation results show strong performance, with both models
achieving over 80% in precision, recall, and F1-score across entity types.
Furthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates
promising transferability, suggesting the broader applicability of HiligayNER
for multilingual NLP in low-resource settings. This work aims to contribute to
language technology development for underrepresented Philippine languages,
specifically for Hiligaynon, and support future research in regional language
processing.

</details>


### [87] [Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG](https://arxiv.org/abs/2510.10787)
*Zhichao Wang,Cheng Wan,Dong Nie*

Main category: cs.CL

TL;DR: 本文综述了推理时扩展技术，包括输出导向和输入导向的方法，重点讨论了RAG技术。


<details>
  <summary>Details</summary>
Motivation: 由于高质量训练数据的可用性迅速减少，研究重点转向推理时扩展，以提高大型语言模型在下游任务上的性能。

Method: 本文通过系统调查推理时扩展技术，将快速发展的领域分为输出导向和输入导向两种方法。

Result: 本文对输出导向和输入导向的方法进行了详细分析，并对RAG技术进行了深入探讨。

Conclusion: 本文综述了推理时扩展技术的多样化方法，为研究者提供了该领域的全面视角。

Abstract: The performance gains of LLMs have historically been driven by scaling up
model size and training data. However, the rapidly diminishing availability of
high-quality training data is introducing a fundamental bottleneck, shifting
the focus of research toward inference-time scaling. This paradigm uses
additional computation at the time of deployment to substantially improve LLM
performance on downstream tasks without costly model re-training. This review
systematically surveys the diverse techniques contributing to this new era of
inference-time scaling, organizing the rapidly evolving field into two
comprehensive perspectives: Output-focused and Input-focused methods.
Output-focused techniques encompass complex, multi-step generation strategies,
including reasoning (e.g., CoT, ToT, ReAct), various search and decoding
methods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),
and model ensemble methods. Input-focused techniques are primarily categorized
by few-shot and RAG, with RAG as the central focus. The RAG section is further
detailed through a structured examination of query expansion, data, retrieval
and reranker, LLM generation methods, and multi-modal RAG.

</details>


### [88] [Toward Human-Centered Readability Evaluation](https://arxiv.org/abs/2510.10801)
*Bahar İlgen,Georges Hattab*

Main category: cs.CL

TL;DR: 本文提出了一个人性化可读性评分（HCRS），用于评估健康文本简化，超越了传统的表面指标，考虑了人类中心的品质。


<details>
  <summary>Details</summary>
Motivation: 当前常用的自然语言处理（NLP）评估指标，如BLEU、FKGL和SARI，主要捕捉表面特征，未能考虑像清晰度、可信度、语气、文化相关性和可操作性等以人为中心的品质。在高风险的健康环境中，沟通必须不仅简单，而且易于使用、尊重和可信。

Method: 本文提出了一个基于人机交互（HCI）和健康传播研究的人性化可读性评分（HCRS），该评分结合了自动测量与结构化的人类反馈，以捕捉可读性的关系和上下文方面。

Result: 本文介绍了HCRS框架，讨论了其整合到参与式评估工作流程中的方法，并提出了实证验证的协议。

Conclusion: 本文旨在推动对健康文本简化的评估，超越表面指标，使NLP系统更贴近多样化用户的需求、期望和生活经验。

Abstract: Text simplification is essential for making public health information
accessible to diverse populations, including those with limited health
literacy. However, commonly used evaluation metrics in Natural Language
Processing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level
features and fail to account for human-centered qualities like clarity,
trustworthiness, tone, cultural relevance, and actionability. This limitation
is particularly critical in high-stakes health contexts, where communication
must be not only simple but also usable, respectful, and trustworthy. To
address this gap, we propose the Human-Centered Readability Score (HCRS), a
five-dimensional evaluation framework grounded in Human-Computer Interaction
(HCI) and health communication research. HCRS integrates automatic measures
with structured human feedback to capture the relational and contextual aspects
of readability. We outline the framework, discuss its integration into
participatory evaluation workflows, and present a protocol for empirical
validation. This work aims to advance the evaluation of health text
simplification beyond surface metrics, enabling NLP systems that align more
closely with diverse users' needs, expectations, and lived experiences.

</details>


### [89] [Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures](https://arxiv.org/abs/2510.10806)
*Mihir Gupte,Paolo Giusto,Ramesh S*

Main category: cs.CL

TL;DR: 本文提出了一种新的自底向上的方法，通过生成隐式的、汇总的摘要来线性化来自树状结构的知识，并与使用原始非结构化代码的RAG方法进行了比较。结果表明，我们的方法在效率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索如何最佳表示检索到的知识以生成关于结构化数据的响应，特别是层次结构如树。

Method: 我们提出了一种新的自底向上的方法，通过在每个层次级别生成隐式的、汇总的摘要，将来自树状结构的知识线性化。

Result: 虽然两种方法在响应质量上相当，但我们的方法在检索器中生成的文档数量减少了68%以上，显著提高了效率。

Conclusion: 我们的方法在处理复杂的层次结构数据方面可能是一种高效且可扩展的策略。

Abstract: Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.

</details>


### [90] [Happiness is Sharing a Vocabulary: A Study of Transliteration Methods](https://arxiv.org/abs/2510.10827)
*Haeji Jung,Jinju Kim,Kyungjin Kim,Youjeong Roh,David R. Mortensen*

Main category: cs.CL

TL;DR: 该研究探讨了共享脚本、重叠标记词汇和共享语音对多语言模型性能的影响，并发现罗马化在多个任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨共享脚本、重叠标记词汇和共享语音如何影响多语言模型的性能。

Method: 我们使用三种类型的转写（罗马化、音素转写和替换密码）以及正字法进行受控实验，以评估模型在两个下游任务（命名实体识别和自然语言推理）上的表现。

Result: 罗马化在8个评估设置中的7个中表现优于其他输入类型，表明它是最有效的方法。

Conclusion: 研究发现，罗马化在8个评估设置中的7个中显著优于其他输入类型，这与我们的假设一致，即它是最有效的方法。我们进一步分析了每个因素如何促成成功，并建议拥有与预训练语言共享的更长（子词）标记有助于更好地利用模型。

Abstract: Transliteration has emerged as a promising means to bridge the gap between
various languages in multilingual NLP, showing promising results especially for
languages using non-Latin scripts. We investigate the degree to which shared
script, overlapping token vocabularies, and shared phonology contribute to
performance of multilingual models. To this end, we conduct controlled
experiments using three kinds of transliteration (romanization, phonemic
transcription, and substitution ciphers) as well as orthography. We evaluate
each model on two downstream tasks -- named entity recognition (NER) and
natural language inference (NLI) -- and find that romanization significantly
outperforms other input types in 7 out of 8 evaluation settings, largely
consistent with our hypothesis that it is the most effective approach. We
further analyze how each factor contributed to the success, and suggest that
having longer (subword) tokens shared with pre-trained languages leads to
better utilization of the model.

</details>


### [91] [DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models](https://arxiv.org/abs/2510.10846)
*Kaixuan Ren,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了DUAL-Bench，这是第一个专注于VLMs中的过度拒绝和安全完成的多模态基准，并评估了18个VLMs的表现，结果显示改进空间很大。


<details>
  <summary>Details</summary>
Motivation: 现有的基准没有系统地解决视觉模态中的过度拒绝问题，而这种问题在双用途情况下尤为突出，即指令本身无害，但伴随的图像包含有害内容。

Method: 提出了DUAL-Bench，这是第一个专注于VLMs中的过度拒绝和安全完成的多模态基准。

Result: 评估了18个VLMs在12种危险类别中的表现，结果表明改进空间很大：GPT-5-Nano实现了12.9%的安全完成，GPT-5模型平均为7.9%，Qwen模型仅为3.9%。

Conclusion: DUAL-Bench希望促进更细致的对齐策略的发展，以确保模型在复杂的多模态设置中既安全又有用。

Abstract: As vision-language models become increasingly capable, maintaining a balance
between safety and usefulness remains a central challenge. Safety mechanisms,
while essential, can backfire, causing over-refusal, where models decline
benign requests out of excessive caution. Yet, no existing benchmark has
systematically addressed over-refusal in the visual modality. This setting
introduces unique challenges, such as dual-use cases where an instruction is
harmless, but the accompanying image contains harmful content. Models
frequently fail in such scenarios, either refusing too conservatively or
completing tasks unsafely, which highlights the need for more fine-grained
alignment. The ideal behavior is safe completion, i.e., fulfilling the benign
parts of a request while explicitly warning about any potentially harmful
elements. To address this, we present DUAL-Bench, the first multimodal
benchmark focused on over-refusal and safe completion in VLMs. We evaluated 18
VLMs across 12 hazard categories, with focus on their robustness under
semantics-preserving visual perturbations. The results reveal substantial room
for improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models
average 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster
the development of more nuanced alignment strategies that ensure models remain
both safe and useful in complex multimodal settings.

</details>


### [92] [Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks](https://arxiv.org/abs/2510.10885)
*Jiajing Guo,Kenil Patel,Jorge Piazentin Ono,Wenbin He,Liu Ren*

Main category: cs.CL

TL;DR: 本文评估了六种测试时扩展策略和四种LLM在Text2SQL任务上的表现，发现分解与征服提示和少量示例演示能有效提升性能，但额外工作流程步骤的效果不一，基础模型选择至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时扩展策略在基于LLM的解决方案中显示出前景，但它们在现实应用中的有效性，尤其是在最新推理模型中，仍然不确定。

Method: 本文对六种轻量级、面向工业的测试时扩展策略和四种LLM进行了基准测试，包括两种推理模型，并在BIRD Mini-Dev基准上评估了它们的性能。

Result: 结果表明，分解与征服提示和少量示例演示对于通用和推理导向的LLM都能持续提升性能。然而，引入额外的工作流程步骤效果不一，基础模型的选择起着关键作用。

Conclusion: 本文揭示了在部署Text2SQL系统时，准确率、效率和复杂性之间的实际权衡。

Abstract: Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)
systems, enabling non-expert users to query industrial databases using natural
language. While test-time scaling strategies have shown promise in LLM-based
solutions, their effectiveness in real-world applications, especially with the
latest reasoning models, remains uncertain. In this work, we benchmark six
lightweight, industry-oriented test-time scaling strategies and four LLMs,
including two reasoning models, evaluating their performance on the BIRD
Mini-Dev benchmark. Beyond standard accuracy metrics, we also report inference
latency and token consumption, providing insights relevant for practical system
deployment. Our findings reveal that Divide-and-Conquer prompting and few-shot
demonstrations consistently enhance performance for both general-purpose and
reasoning-focused LLMs. However, introducing additional workflow steps yields
mixed results, and base model selection plays a critical role. This work sheds
light on the practical trade-offs between accuracy, efficiency, and complexity
when deploying Text2SQL systems.

</details>


### [93] [LLM$\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System](https://arxiv.org/abs/2510.10890)
*Yu Chao,Siyu Lin,xiaorong wang,Zhu Zhang,Zihan Zhou,Haoyu Wang,Shuo Wang,Jie Zhou,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: LLM x MapReduce-V3是一种分层模块化代理系统，用于生成长篇调查。它通过多代理架构和MCP服务器实现模块化分解，允许用户进行人机交互干预，并在内容深度和长度上优于基线。


<details>
  <summary>Details</summary>
Motivation: 为了提高用户对研究过程的控制和自定义能力，同时实现更精确的长篇调查生成，需要一种更灵活和模块化的系统。

Method: LLM x MapReduce-V3是一种分层模块化代理系统，用于生成长篇调查。它采用多代理架构，其中各个功能组件（如骨架初始化、摘要构建和骨架细化）作为独立的模型-上下文-协议（MCP）服务器实现。这些原子服务器可以聚合到更高层次的服务器中，创建一个分层结构的系统。一个高层规划代理通过根据MCP工具描述和执行历史选择适当的模块来动态协调工作流。

Result: 通过多轮交互，系统能够准确捕捉预期的研究视角，生成全面的骨架，然后发展成深入的调查。人类评估表明，我们的系统在内容深度和长度上都优于代表性的基线。

Conclusion: 通过人类评估，我们的系统在内容深度和长度上都超过了代表性的基线，这表明基于MCP的模块化规划具有强大的优势。

Abstract: We introduce LLM x MapReduce-V3, a hierarchically modular agent system
designed for long-form survey generation. Building on the prior work, LLM x
MapReduce-V2, this version incorporates a multi-agent architecture where
individual functional components, such as skeleton initialization, digest
construction, and skeleton refinement, are implemented as independent
model-context-protocol (MCP) servers. These atomic servers can be aggregated
into higher-level servers, creating a hierarchically structured system. A
high-level planner agent dynamically orchestrates the workflow by selecting
appropriate modules based on their MCP tool descriptions and the execution
history. This modular decomposition facilitates human-in-the-loop intervention,
affording users greater control and customization over the research process.
Through a multi-turn interaction, the system precisely captures the intended
research perspectives to generate a comprehensive skeleton, which is then
developed into an in-depth survey. Human evaluations demonstrate that our
system surpasses representative baselines in both content depth and length,
highlighting the strength of MCP-based modular planning.

</details>


### [94] [ADVICE: Answer-Dependent Verbalized Confidence Estimation](https://arxiv.org/abs/2510.10913)
*Ki Jung Seo,Sehun Lim,Taeuk Kim*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的自信表达问题，提出了一种新的框架ADVICE，以改善模型的自信校准并增强其可信度。


<details>
  <summary>Details</summary>
Motivation: 最近大型语言模型（LLMs）的进步使它们能够以自然语言表达自信，增强了透明度和可靠性。然而，它们的自信往往表现出过度自信，其原因尚不清楚。

Method: 我们提出了ADVICE（答案依赖的口头自信估计），这是一个微调框架，有助于基于答案的自信估计。

Result: 广泛的实验表明，ADVICE显著提高了自信校准，同时保持了任务性能。进一步的分析证实，ADVICE加强了基于答案的特性，导致更平衡和校准良好的自信分布。

Conclusion: 我们的研究揭示了过自信的根源，并建立了一个更可信赖的自信表达框架。

Abstract: Recent progress in large language models (LLMs) has enabled them to express
their confidence in natural language, enhancing transparency and reliability.
However, their confidence often exhibits overconfidence, the cause of which
remains poorly understood. In this work, we conduct a detailed analysis of the
dynamics underlying verbalized confidence and identify answer-independence as a
key factor, defined as the model's failure to condition confidence on its own
answer. To address this, we propose ADVICE (Answer-Dependent Verbalized
Confidence Estimation), a fine-tuning framework that facilitates
answer-grounded confidence estimation. Extensive experiments show that ADVICE
substantially improves confidence calibration while preserving task
performance. Further analyses confirm that ADVICE strengthens
answer-groundedness, leading to more balanced and well-calibrated confidence
distributions. Our findings shed light on the origin of overconfidence and
establish a framework for more trustworthy confidence verbalization.

</details>


### [95] [GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition](https://arxiv.org/abs/2510.10927)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 本文提出了一种名为GapDNER的Gap-aware grid tagging模型，用于不连续命名实体识别。该模型通过在实体片段之间的上下文间隙上应用表示学习来解决解码歧义并增强性能。实验结果表明，GapDNER在不连续NER上取得了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 在生物医学领域，一个命名实体可能由一系列非相邻标记组成，并与其他实体重叠。之前的 方法通过连接实体片段或内部标记来识别不连续实体，但由于跨度或单词组合的多样性，面临错误传播和解码歧义的挑战。

Method: 我们提出了一个名为GapDNER的Gap-aware grid tagging模型，通过在实体片段之间的上下文间隙上应用表示学习来解决解码歧义并增强不连续NER性能。具体来说，我们将上下文间隙视为一种额外的跨度，并将跨度分类转换为token-pair grid tagging任务。我们设计了两个交互组件，从内部和跨跨度的角度全面建模token-pair grid特征。

Result: 在三个数据集上的实验结果表明，我们的GapDNER在不连续NER上取得了新的最先进性能，并在识别复杂实体结构方面表现出显著优势。

Conclusion: 实验结果表明，我们的GapDNER在不连续NER上取得了新的最先进性能，并在识别复杂实体结构方面表现出显著优势。

Abstract: In biomedical fields, one named entity may consist of a series of
non-adjacent tokens and overlap with other entities. Previous methods recognize
discontinuous entities by connecting entity fragments or internal tokens, which
face challenges of error propagation and decoding ambiguity due to the wide
variety of span or word combinations. To address these issues, we deeply
explore discontinuous entity structures and propose an effective Gap-aware grid
tagging model for Discontinuous Named Entity Recognition, named GapDNER. Our
GapDNER innovatively applies representation learning on the context gaps
between entity fragments to resolve decoding ambiguity and enhance
discontinuous NER performance. Specifically, we treat the context gap as an
additional type of span and convert span classification into a token-pair grid
tagging task. Subsequently, we design two interactive components to
comprehensively model token-pair grid features from both intra- and inter-span
perspectives. The intra-span regularity extraction module employs the biaffine
mechanism along with linear attention to capture the internal regularity of
each span, while the inter-span relation enhancement module utilizes
criss-cross attention to obtain semantic relations among different spans. At
the inference stage of entity decoding, we assign a directed edge to each
entity fragment and context gap, then use the BFS algorithm to search for all
valid paths from the head to tail of grids with entity tags. Experimental
results on three datasets demonstrate that our GapDNER achieves new
state-of-the-art performance on discontinuous NER and exhibits remarkable
advantages in recognizing complex entity structures.

</details>


### [96] [Evaluating Language Models' Evaluations of Games](https://arxiv.org/abs/2510.10930)
*Katherine M. Collins,Cedegao E. Zhang,Graham Todd,Lance Ying,Mauricio Barba da Costa,Ryan Liu,Prafull Sharma,Adrian Weller,Ionatan Kuperwajs,Lionel Wong,Joshua B. Tenenbaum,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估AI系统的方法，通过比较现代语言和推理模型与人类和符号计算代理的评估，发现推理模型在评估游戏方面通常比非推理语言模型更符合人类的评价，但随着模型接近博弈论最优，其与人类数据的契合度减弱。


<details>
  <summary>Details</summary>
Motivation: 评估人工智能（AI）系统主要集中在解决问题上，历史上通过研究模型如何下棋如国际象棋和围棋。本文倡导一种新的范式，评估AI系统对游戏的评估。

Method: 我们引入了一种评估此类评估的形式化方法，并利用一个包含超过100个新棋盘游戏和450个人类判断的大规模数据集，将现代语言和推理模型产生的评估与人类和符号计算代理的评估进行比较。我们考虑了两种类型的评估查询：评估游戏的收益（或公平性）和趣味性。这些查询涵盖了与AI评估设计相关的两个维度：查询计算的复杂性和查询量化的难度。

Result: 我们的研究结果表明，推理模型在评估游戏方面通常比非推理语言模型更符合人类的评价。然而，我们观察到一个非单调关系：随着模型接近博弈论最优，它们与人类数据的契合度减弱。此外，我们在评估趣味性时观察到更多的“波动性”，这与量化这一查询的难度一致。总体而言，推理模型在评估查询时表现出高度可变且不可预测的资源使用情况，这强调了在语言和推理模型中融入更多资源理性元推理的重要性。

Conclusion: 我们的研究结果表明，推理模型在评估游戏方面通常比非推理语言模型更符合人类的评价。然而，我们观察到一个非单调关系：随着模型接近博弈论最优，它们与人类数据的契合度减弱。此外，我们在评估趣味性时观察到更多的“波动性”，这与量化这一查询的难度一致。总体而言，推理模型在评估查询时表现出高度可变且不可预测的资源使用情况，这强调了在语言和推理模型中融入更多资源理性元推理的重要性。

Abstract: Reasoning is not just about solving problems -- it is also about evaluating
which problems are worth solving at all. Evaluations of artificial intelligence
(AI) systems primarily focused on problem solving, historically by studying how
models play games such as chess and Go. In this paper, we advocate for a new
paradigm that assesses AI systems' evaluation of games. First, we introduce a
formalism for evaluating such evaluations. We then leverage a large-scale
dataset of over $100$ novel board games and over 450 human judgments to compare
evaluations produced by modern language and reasoning models against those of
people and symbolic computational agents. We consider two kinds of evaluative
queries: assessing the payoff (or fairness) and the funness of games. These
queries span two dimensions relevant to the design of evaluations of AI
evaluations: how complex a query is to compute and how difficult a query is to
quantify. Our results show that reasoning models are generally more aligned to
people in their evaluations of games than non-reasoning language models.
However, we observe a non-monotonic relationship: as models get closer to
game-theoretic optimal, their fit to human data weakens. We also observe more
"jaggedness" across models for assessing funness, in line with the greater
difficulty of quantifying this query. Across queries and games, reasoning
models show highly variable and unpredictable resource usage when assessing
queries, pointing to the importance of imbuing more resource-rational
meta-reasoning in language and reasoning models.

</details>


### [97] [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study](https://arxiv.org/abs/2510.10936)
*Anirudh Ganesh,Jayavardhan Reddy*

Main category: cs.CL

TL;DR: 本文对Ma和Hovy（2016）提出的BiLSTM-CNN-CRF模型进行了可重复性研究，成功再现了关键结果，并提供了详细的分析和开源实现。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在验证Ma和Hovy（2016）提出的BiLSTM-CNN-CRF模型的可重复性，并评估其在序列标注任务中的有效性。

Method: 我们对Ma和Hovy（2016）提出的最先进的序列标注神经架构进行了可重复性研究。原始的BiLSTM-CNN-CRF模型通过卷积神经网络（CNNs）结合字符级表示，通过双向长短期记忆网络（BiLSTMs）进行单词级上下文建模，并通过条件随机场（CRFs）进行结构化预测。

Result: 我们的实现成功再现了关键结果，在CoNLL-2003 NER上达到了91.18%的F1分数，并展示了该模型在序列标注任务中的有效性。

Conclusion: 我们的实现成功再现了关键结果，证明了该模型在序列标注任务中的有效性。我们提供了对架构组件的详细分析，并发布了开源的PyTorch实现以促进进一步研究。

Abstract: We present a reproducibility study of the state-of-the-art neural
architecture for sequence labeling proposed by Ma and Hovy
(2016)\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines
character-level representations via Convolutional Neural Networks (CNNs),
word-level context modeling through Bi-directional Long Short-Term Memory
networks (BiLSTMs), and structured prediction using Conditional Random Fields
(CRFs). This end-to-end approach eliminates the need for hand-crafted features
while achieving excellent performance on named entity recognition (NER) and
part-of-speech (POS) tagging tasks. Our implementation successfully reproduces
the key results, achieving 91.18\% F1-score on CoNLL-2003 NER and demonstrating
the model's effectiveness across sequence labeling tasks. We provide a detailed
analysis of the architecture components and release an open-source PyTorch
implementation to facilitate further research.

</details>


### [98] [Punctuation-aware treebank tree binarization](https://arxiv.org/abs/2510.10951)
*Eitan Klinger,Vivaan Wadhwa,Jungyeul Park*

Main category: cs.CL

TL;DR: 本文提出了一种保留标点符号的树库二分管道，提高了头部预测的准确性，并实现了与CCGbank派生结构的竞争力对齐。


<details>
  <summary>Details</summary>
Motivation: 标准的二分管道在头部选择之前会删除标点符号，这会改变成分形状并损害头部-子节点识别。本文旨在解决这个问题，通过保留标点符号来提高头部预测的准确性。

Method: 本文提出了一种可重复的管道，该管道在二分之前保留标点符号作为兄弟节点，并生成了衍生的工件和元数据（中间@X标记、可逆性签名、对齐索引），同时提供了一个配套的评估套件，涵盖了头部-子节点预测、往返可逆性和与派生资源（CCGbank）的结构兼容性。

Result: 在Penn Treebank上，标点符号感知的预处理将头部预测准确率从73.66%（Collins规则）和86.66%（MLP）提高到91.85%，并且在与CCGbank派生结构的对齐方面表现出了竞争力。

Conclusion: 本文提出了一个用于标点符号感知的树库二分化的资源和评估套件。通过保留标点符号作为兄弟节点，提高了头部预测的准确性，并实现了与CCGbank派生结构的竞争性对齐。所有代码、配置文件和文档均已发布，以支持复制和扩展到其他语料库。

Abstract: This article presents a curated resource and evaluation suite for
punctuation-aware treebank binarization. Standard binarization pipelines drop
punctuation before head selection, which alters constituent shape and harms
head-child identification. We release (1) a reproducible pipeline that
preserves punctuation as sibling nodes prior to binarization, (2) derived
artifacts and metadata (intermediate @X markers, reversibility signatures,
alignment indices), and (3) an accompanying evaluation suite covering
head-child prediction, round-trip reversibility, and structural compatibility
with derivational resources (CCGbank). On the Penn Treebank, punctuation-aware
preprocessing improves head prediction accuracy from 73.66\% (Collins rules)
and 86.66\% (MLP) to 91.85\% with the same classifier, and achieves competitive
alignment against CCGbank derivations. All code, configuration files, and
documentation are released to enable replication and extension to other
corpora.

</details>


### [99] [KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification](https://arxiv.org/abs/2510.10961)
*Yejin Lee,Su-Hyeon Kim,Hyundong Jin,Dayoung Kim,Yeonsoo Kim,Yo-Sub Han*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Toxic content has become an increasingly critical social issue with the rapid
expansion of online communication. While numerous studies explored methods for
detecting and detoxifying such content, most have focused primarily on English,
leaving low-resource language underrepresented. Consequently, Large Language
Models~(LLMs) often struggle to identify and neutralize toxic expressions in
these languages. This challenge becomes even more pronounced when user employ
obfuscation techniques to evade detection systems. Therefore, we propose a
\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to
address this issue. We categorize various obfuscation approaches based on
linguistic characteristics of Korean and define a set of transformation rules
grounded in real-word examples. Using these rules, we construct three dataset
versions (easy, normal, and hard) representing different levels of obfuscation
difficulty. This is the first dataset that simultaneously supports
deobfuscation and detoxification for the Korean language. We expect it to
facilitate better understanding and mitigating of obfuscated toxic content in
LLM for low-resource languages. Our code and data are available at
https://github.com/leeyejin1231/KOTOX.

</details>


### [100] [Judge Before Answer: Can MLLM Discern the False Premise in Question?](https://arxiv.org/abs/2510.10965)
*Jidong Li,Lingyong Fang,Haodong Zhao,Sufeng Duan,Gongshen Liu*

Main category: cs.CL

TL;DR: 本文介绍了一个全面的错误前提问题基准，并提出了一个增强框架，以提高多模态大语言模型（MLLMs）检测错误前提的能力。实验结果表明，该框架显著提升了模型在这一任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的针对这一问题的基准有限，它们往往缺乏细粒度分类，覆盖不足，因此无法提供对模型识别错误前提能力的严格评估。为了弥补这一差距，我们引入了一个完全自动化的管道来构建一个全面的错误前提问题基准。

Method: 我们引入了一个完全自动化的管道来构建一个全面的错误前提问题基准。我们的方法根据识别前提所需的技能，将前提系统地分类为三种主要类型和十三种子类型，从而产生了JBA数据集。

Result: 结果表明，当前的MLLMs在识别错误前提方面仍然存在困难。基于这个基准，我们进一步提出了一种增强框架，旨在提高MLLMs检测错误前提的鲁棒性。大量实验表明，使用我们的框架训练的模型在错误前提识别方面取得了显著改进。

Conclusion: 结果表明，当前的MLLMs在识别错误前提方面仍然存在困难。基于这个基准，我们进一步提出了一种增强框架，旨在提高MLLMs检测错误前提的鲁棒性。大量实验表明，使用我们的框架训练的模型在错误前提识别方面取得了显著改进。

Abstract: Multimodal large language models (MLLMs) have witnessed astonishing
advancements in recent years. Despite these successes, MLLMs remain vulnerable
to flase premise problems. However, existing benchmarks targeting this issue
are limited in scope: they often lack fine-grained categorization, exhibit
insufficient coverage, and thus fail to provide a rigorous evaluation of the
ability of models to recognize false premises. To bridge this gap, we introduce
a fully automated pipeline for constructing a comprehensive benchmark of false
premise questions. Our method systematically categorizes the premises into
three main types and thirteen subtypes according to the abilities required to
identify the premises, resulting in the JBA dataset.Results show current MLLMs
still struggle with false premise recognition. Building upon this benchmark, we
further propose a recognition enhancement framework tailored to strengthen the
robustness of MLLMs to detect false premises. Extensive experiments demonstrate
that models trained with our framework achieve significant improvements in
false premise recognition.

</details>


### [101] [RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection](https://arxiv.org/abs/2510.10971)
*Yejin Lee,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: 本文提出了RV-HATE框架，用于仇恨言论检测，该框架能够根据数据集特性进行调整，提高检测准确性并提供可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测研究通常依赖固定的方法学，而没有适应数据特定特征，因此需要一种能够考虑数据集特定特征的检测框架。

Method: RV-HATE框架，包含多个专门模块，每个模块专注于仇恨言论的不同语言或上下文特征，并使用强化学习优化权重，最后通过投票机制聚合模块输出以产生最终决策。

Result: RV-HATE在检测准确性和可解释性方面表现出色，能够有效处理隐性仇恨言论，并优于传统静态方法。

Conclusion: 我们的方法有效解决了隐性仇恨言论，并且相比传统静态方法表现更优。

Abstract: Hate speech remains prevalent in human society and continues to evolve in its
forms and expressions. Modern advancements in internet and online anonymity
accelerate its rapid spread and complicate its detection. However, hate speech
datasets exhibit diverse characteristics primarily because they are constructed
from different sources and platforms, each reflecting different linguistic
styles and social contexts. Despite this diversity, prior studies on hate
speech detection often rely on fixed methodologies without adapting to
data-specific features. We introduce RV-HATE, a detection framework designed to
account for the dataset-specific characteristics of each hate speech dataset.
RV-HATE consists of multiple specialized modules, where each module focuses on
distinct linguistic or contextual features of hate speech. The framework
employs reinforcement learning to optimize weights that determine the
contribution of each module for a given dataset. A voting mechanism then
aggregates the module outputs to produce the final decision. RV-HATE offers two
primary advantages: (1)~it improves detection accuracy by tailoring the
detection process to dataset-specific attributes, and (2)~it also provides
interpretable insights into the distinctive features of each dataset.
Consequently, our approach effectively addresses implicit hate speech and
achieves superior performance compared to conventional static methods. Our code
is available at https://github.com/leeyejin1231/RV-HATE.

</details>


### [102] [Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning](https://arxiv.org/abs/2510.10974)
*Zhiwen Ruan,Yixia Li,He Zhu,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为CFT的简单而有效的微调方法，通过仅更新通过反事实扰动识别出的功能不可或缺的标记来提高生成质量和多样性。实验表明，尽管只对不到12%的标记进行微调，CFT在多个模型和数学推理基准上均优于标准SFT。


<details>
  <summary>Details</summary>
Motivation: Standard SFT uniformly penalizes all tokens, neglecting that only a small subset of critical tokens determines reasoning correctness. This uniform supervision often causes reduced output diversity and limited generalization.

Method: Critical Token Fine-tuning (CFT), which updates only tokens identified as functionally indispensable via counterfactual perturbations.

Result: CFT, despite fine-tuning on less than 12% of tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time scaling through improved sampling diversity and provides a stronger initialization for reinforcement learning, sustaining performance gains in later training stages while maintaining higher entropy for better exploration.

Conclusion: CFT is a practical and general framework for efficient and robust LLM fine-tuning.

Abstract: Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)
as a key method to adapt pre-trained models to domain-specific tasks such as
mathematical reasoning. However, standard SFT uniformly penalizes all tokens,
neglecting that only a small subset of critical tokens determines reasoning
correctness. This uniform supervision often causes reduced output diversity and
limited generalization. We propose Critical Token Fine-tuning (CFT), a simple
yet effective approach that updates only tokens identified as functionally
indispensable via counterfactual perturbations. By focusing gradient signals on
these decisive reasoning steps while preserving the diversity of non-critical
tokens, CFT can enhance both generation and diversity. Extensive experiments on
five models across three families (Qwen, OLMo, LLaMA) and eleven mathematical
reasoning benchmarks show that CFT, despite fine-tuning on less than 12% of
tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time
scaling through improved sampling diversity and provides a stronger
initialization for reinforcement learning, sustaining performance gains in
later training stages while maintaining higher entropy for better exploration.
These results highlight CFT as a practical and general framework for efficient
and robust LLM fine-tuning.

</details>


### [103] [DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety](https://arxiv.org/abs/2510.10994)
*Wei-Chieh Huang,Henry Peng Zou,Yaozu Wu,Dongyuan Li,Yankai Chen,Weizhi Zhang,Yangning Li,Angelo Zangari,Jizhou Guo,Chunyu Miao,Liancheng Fang,Langzhou He,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: DEEPRESEARCHGUARD is a framework that improves the safety and quality of deep research by implementing four-stage safeguards and open-domain evaluation, demonstrating significant improvements in defense success rates and report quality.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks are deficient in sufficient evaluation procedures and stage-specific protections. They typically treat evaluation as exact match accuracy of question-answering, but overlook crucial aspects of report quality such as credibility, coherence, breadth, depth, and safety.

Method: DEEPRESEARCHGUARD, a comprehensive framework featuring four-stage safeguards with open-domain evaluation of references and reports. DRSAFEBENCH, a stage-wise benchmark for deep research safety.

Result: DEEPRESEARCHGUARD achieves an average defense success rate improvement of 18.16% while reducing over-refusal rate by 6%. The input guard provides the most substantial early-stage protection by filtering out obvious risks, while the plan and research guards enhance citation discipline and source credibility.

Conclusion: DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware defenses that effectively block harmful content propagation, while systematically improving report quality without excessive over-refusal rates.

Abstract: Deep research frameworks have shown promising capabilities in synthesizing
comprehensive reports from web sources. While deep research possesses
significant potential to address complex issues through planning and research
cycles, existing frameworks are deficient in sufficient evaluation procedures
and stage-specific protections. They typically treat evaluation as exact match
accuracy of question-answering, but overlook crucial aspects of report quality
such as credibility, coherence, breadth, depth, and safety. This oversight may
result in hazardous or malicious sources being integrated into the final
report. To address these issues, we introduce DEEPRESEARCHGUARD, a
comprehensive framework featuring four-stage safeguards with open-domain
evaluation of references and reports. We assess performance across multiple
metrics, e.g., defense success rate and over-refusal rate, and five key report
dimensions. In the absence of a suitable safety benchmark, we introduce
DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation
spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,
DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success
rate improvement of 18.16% while reducing over-refusal rate by 6%. The input
guard provides the most substantial early-stage protection by filtering out
obvious risks, while the plan and research guards enhance citation discipline
and source credibility. Through extensive experiments, we show that
DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware
defenses that effectively block harmful content propagation, while
systematically improving report quality without excessive over-refusal rates.
The code can be found via https://github.com/Jasonya/DeepResearchGuard.

</details>


### [104] [ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios](https://arxiv.org/abs/2510.10998)
*Mahika Phutane,Hayoung Jung,Matthew Kim,Tanushree Mitra,Aditya Vashistha*

Main category: cs.CL

TL;DR: 研究揭示了大型语言模型在招聘场景中对残疾人存在显著的残障歧视和交叉性偏见，强调了当前安全工具的不足，并提出了对前沿模型进行交叉性安全评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在西方，忽视了全球南方地区残疾人因性别和种姓等交叉性边缘化而产生的不同体验。研究旨在填补这一空白，并评估大型语言模型在招聘中的偏见和有害行为。

Method: 研究对六个大型语言模型进行了全面审计，覆盖了2,820个招聘场景，涵盖了不同的残疾、性别、国籍和种姓特征。引入了ABLEIST（残障歧视、激励、超人化和象征主义）指标，以捕捉细微的交叉性伤害和偏见。

Result: 研究发现，大型语言模型对残疾候选人的ABLEIST伤害显著增加，许多最先进的模型未能检测到这些伤害。此外，对于性别和种姓边缘化的残疾候选人，交叉性伤害（如象征主义）也显著增加。

Conclusion: 研究揭示了大型语言模型在招聘场景中对残疾人存在的显著偏见和有害行为，特别是当这些偏见与性别和种姓等其他边缘化因素交织时。研究强调了当前安全工具的不足，并提出了对前沿模型进行交叉性安全评估的必要性。

Abstract: Large language models (LLMs) are increasingly under scrutiny for perpetuating
identity-based discrimination in high-stakes domains such as hiring,
particularly against people with disabilities (PwD). However, existing research
remains largely Western-centric, overlooking how intersecting forms of
marginalization--such as gender and caste--shape experiences of PwD in the
Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring
scenarios spanning diverse disability, gender, nationality, and caste profiles.
To capture subtle intersectional harms and biases, we introduce ABLEIST
(Ableism, Inspiration, Superhumanization, and Tokenism), a set of five
ableism-specific and three intersectional harm metrics grounded in disability
studies literature. Our results reveal significant increases in ABLEIST harms
towards disabled candidates--harms that many state-of-the-art models failed to
detect. These harms were further amplified by sharp increases in intersectional
harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates,
highlighting critical blind spots in current safety tools and the need for
intersectional safety evaluations of frontier models in high-stakes domains
like hiring.

</details>


### [105] [DND: Boosting Large Language Models with Dynamic Nested Depth](https://arxiv.org/abs/2510.11001)
*Tieyuan Chen,Xiaodong Chen,Haoxing Chen,Zhenzhong Lan,Weiyao Lin,Jianguo Li*

Main category: cs.CL

TL;DR: DND是一种新的方法，通过动态选择关键标记进行重新处理，提高了大型语言模型的性能，同时保持了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在处理复杂任务时可能遇到性能瓶颈，需要一种有效的方法来提高其性能，同时保持计算效率。

Method: DND方法通过在每个Transformer层结束时使用路由器识别关键标记，并将它们反馈进行额外的处理，从而有效地“审查”困难标记，同时避免对容易标记的冗余计算。此外，还引入了两种新策略：一个控制损失的路由器以增强标记选择的区分度，以及一个阈值控制方案以确保选择的稳定性。

Result: DND方法在密集型Qwen3-1.7B模型上提升了1.88%，在MoE Qwen3-30B-A3B模型上提升了0.87%，并且增加了很少的参数和计算量。

Conclusion: DND方法通过选择关键标记进行重新处理，提高了现成大型语言模型的性能，并在多个基准测试中显著提升了密集型和MoE模型的性能，同时增加了很少的参数和计算量。

Abstract: We introduce Dynamic Nested Depth (DND), a novel method that improves
performance for off-the-shelf LLMs by selecting critical tokens to reprocess in
a nested depth manner. Specifically, at the end of the given transformer layer,
DND identifies more critical tokens with a router and feeds them back for an
extra round of processing, effectively ``reviewing" difficult tokens while
avoiding redundant computation for easier ones. The dynamic selection mechanism
is tailored for precise control via two novel strategies: a router controlling
loss to enhance token selection distinguishability, and a threshold control
scheme to ensure selection stability. We demonstrate the effectiveness of DND
by directly integrating it into pre-trained dense and MoE models during a
post-training phase. On diverse benchmarks, this approach boosts the
performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by
0.87%, all with a minimal parameter and computing increase.

</details>


### [106] [LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models](https://arxiv.org/abs/2510.11031)
*Yiwei Liu,Yucheng Li,Xiao Li,Gong Cheng*

Main category: cs.CL

TL;DR: LogiNumSynth is a flexible natural language problem synthesizer that creates tasks requiring both logical and numerical reasoning, offering fine-grained control over task complexity. It helps evaluate and improve language models' reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: Existing datasets rely on fixed rule sets and offer limited control over task complexity, constraining their generalizability for evaluation and training.

Method: LogiNumSynth, a flexible natural language problem synthesizer that synthesizes tasks requiring proficiency in joint logical reasoning and numerical reasoning.

Result: Experiments with multiple LLMs highlight persistent weaknesses in logical-numerical reasoning, showing that LogiNumSynth can serve as both a diagnostic tool and a source of targeted supervision for advancing integrated reasoning skills.

Conclusion: LogiNumSynth can serve as both a diagnostic tool and a source of targeted supervision for advancing integrated reasoning skills.

Abstract: Joint logical-numerical reasoning remains a major challenge for language
models, yet existing datasets rely on fixed rule sets and offer limited control
over task complexity, constraining their generalizability for evaluation and
training. We present LogiNumSynth, a flexible natural language problem
synthesizer that synthesizes tasks requiring proficiency in joint logical
reasoning (e.g., rule-based reasoning) and numerical reasoning (e.g.,
arithmetic computation). LogiNumSynth supports fine-grained control over
reasoning world richness, logical reasoning depth, and the complexity of
numerical computations, enabling flexible data synthesis across difficulty
levels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing
fully controllable joint reasoning tasks over natural language; (2) Evaluation
& Process Analysis -- evaluating both process accuracy and answer accuracy; (3)
Targeted Training -- using synthesized data to enhance LLMs' reasoning
performance. Experiments with multiple LLMs highlight persistent weaknesses in
logical-numerical reasoning, showing that LogiNumSynth can serve as both a
diagnostic tool and a source of targeted supervision for advancing integrated
reasoning skills.

</details>


### [107] [Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks](https://arxiv.org/abs/2510.11040)
*Wenya Xie,Qingying Xiao,Yu Zheng,Xidong Wang,Junying Chen,Ke Ji,Anningzhe Gao,Prayag Tiwari,Xiang Wan,Feng Jiang,Benyou Wang*

Main category: cs.CL

TL;DR: 本文提出将大语言模型作为临床助手，构建了DoctorFLAN数据集，并展示了其在提高医疗大语言模型性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在医疗领域的直接部署存在安全风险，因此需要一种更安全的方法来利用它们。本文旨在通过将大语言模型作为临床助手来解决这一问题。

Method: 本文提出了将大语言模型重新定位为临床助手，与经验丰富的医生合作，而不是直接与患者互动。此外，构建了DoctorFLAN数据集，并引入了DoctorFLAN-test和DotaBench来评估模型性能。

Result: 实验结果表明，DoctorFLAN显著提高了开源大语言模型在医疗环境中的表现，使其更符合医生的工作流程，并补充了现有的面向患者的模型。

Conclusion: 本文贡献了一个有价值的资源和框架，用于推进以医生为中心的医疗大语言模型的发展。

Abstract: The rise of large language models (LLMs) has transformed healthcare by
offering clinical guidance, yet their direct deployment to patients poses
safety risks due to limited domain expertise. To mitigate this, we propose
repositioning LLMs as clinical assistants that collaborate with experienced
physicians rather than interacting with patients directly. We conduct a
two-stage inspiration-feedback survey to identify real-world needs in clinical
workflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese
medical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27
specialties. To evaluate model performance in doctor-facing applications, we
introduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74
multi-turn conversations). Experimental results with over ten popular LLMs
demonstrate that DoctorFLAN notably improves the performance of open-source
LLMs in medical contexts, facilitating their alignment with physician workflows
and complementing existing patient-oriented models. This work contributes a
valuable resource and framework for advancing doctor-centered medical LLM
development

</details>


### [108] [Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States](https://arxiv.org/abs/2510.11052)
*Qinglin Zhu,Yizhen Yao,Runcong Zhao,Yanzheng Xiang,Amrutha Saseendran,Chen Jin,Philip Alexander Teare,Bin Liang,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: LRD is a two-stage framework that improves accuracy and speed in natural language generation by addressing the limitations of autoregressive and diffusion-inspired approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of autoregressive models and diffusion-inspired approaches in natural language generation, such as high latency, information loss, and premature commitment.

Method: Latent Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a Predictive Feedback Loop.

Result: Experiments across coding (HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that LRD improves accuracy while delivering speedups of up to 10.6x.

Conclusion: LRD improves accuracy while delivering speedups of up to 10.6x, making it a strong and versatile alternative for parallel sequence generation.

Abstract: Autoregressive (AR) models remain the standard for natural language
generation but still suffer from high latency due to strictly sequential
decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,
mitigate this by generating in parallel, yet they suffer from two core
limitations: information loss, as predictive distributions for non-finalized
tokens are discarded at each step, and premature commitment, where local
decisions are made without sufficient global coordination. We introduce Latent
Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a
Predictive Feedback Loop. The first stage maintains masked positions as
distributional mixtures of predicted tokens and the mask embedding, allowing
the model to establish more globally consistent beliefs. The second stage
progressively finalizes confident tokens while retaining uncertain ones for
iterative feedback. KL-divergence dynamics provide a principled and reliable
criterion for convergence and early stopping. Experiments across coding
(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that
LRD improves accuracy while delivering speedups of up to 10.6x, making it a
strong and versatile alternative for parallel sequence generation.

</details>


### [109] [Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization](https://arxiv.org/abs/2510.11104)
*Junjie Lu,Yuliang Liu,Chaofeng Qu,Wei Shen,Zhouhan Lin,Min Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为CGPO的方法，该方法利用置信度信号来识别模型推理过程中的最大不确定性点，并应用自我生成的非人类推理路径指导来减轻轨迹漂移。实验结果表明，在相同的数据量下，使用小模型生成的数据可以比使用强模型或人工标注的数据取得更好的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在增强LLM推理时倾向于引入人类推理轨迹的训练偏差。在分步偏好优化中，依赖于人类或更高容量模型的注释限制了探索其他非人类推理路径，从而限制了可达到的性能。此外，通过一个小规模的试点研究，观察到大约75%的情况下，模型的第一个错误步骤出现在最低置信度点之后。这表明在错误发生前在最低置信度点引导模型提供了比定位第一个显式错误更准确的监督。

Method: 本文提出了CGPO方法，该方法利用置信度信号来识别模型推理过程中的最大不确定性点，并应用自我生成的非人类推理路径指导来减轻轨迹漂移。

Result: 实验结果表明，在相同的数据量下，使用小模型生成的数据可以比使用强模型或人工标注的数据取得更好的性能。

Conclusion: 本文提出了一种名为CGPO的方法，该方法利用置信度信号来识别模型推理过程中的最大不确定性点，并应用自我生成的非人类推理路径指导来减轻轨迹漂移。实验结果表明，在相同的数据量下，使用小模型生成的数据可以比使用强模型或人工标注的数据取得更好的性能。

Abstract: Current approaches for strengthening LLM reasoning tend to introduce a
training bias toward human-like reasoning trajectories. In step-wise preference
optimization, in particular, dependence on human or higher-capacity model
annotations for intermediate steps limits exploration of alternative,
non-human-like reasoning paths and thus constrains achievable performance.
Furthermore, through a small-scale pilot study, we observed that in
approximately 75% of cases, the model's first erroneous step occurs after the
lowest-confidence point. This suggests that guiding the model at its
lowest-confidence point before an error provides more accurate supervision than
locating the first explicit error. In this paper, we propose Confidence-Guided
Reasoning Path Preference Optimization (CGPO), a method that leverages a
confidence signal to identify points of maximal uncertainty in the model's
reasoning process and applies self-generated, non-human-like reasoning-path
guidance to mitigate trajectory drift. Our experiments span diverse models
applied to both code and mathematical reasoning tasks. The results show that,
with the same amount of training data, our method using data generated by a
small model can achieve better performance in most cases compared with
approaches using data generated by a strong model or human-annotated.

</details>


### [110] [TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code](https://arxiv.org/abs/2510.11151)
*Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 本文提出了一种名为TypePilot的代理AI框架，以提高LLM生成代码的安全性和鲁棒性。通过使用强类型和可验证语言，如Scala，实验结果表明该方法能有效减少输入验证和注入漏洞。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码可能包含细微但关键的漏洞，这在安全敏感或任务关键系统中会带来重大风险。因此，需要一种方法来提高LLM生成代码的安全性。

Method: 本文引入了TypePilot，这是一个利用强类型和可验证语言（如Scala）来增强LLM生成代码的安全性和鲁棒性的代理AI框架。

Result: 实验结果表明，直接代码生成通常无法强制执行安全约束，而我们的类型导向代理流程显著减少了输入验证和注入漏洞。

Conclusion: 本文展示了结构化、类型引导的LLM工作流在提高高保障领域自动化代码生成的可信度方面的潜力。

Abstract: Large language Models (LLMs) have shown remarkable proficiency in code
generation tasks across various programming languages. However, their outputs
often contain subtle but critical vulnerabilities, posing significant risks
when deployed in security-sensitive or mission-critical systems. This paper
introduces TypePilot, an agentic AI framework designed to enhance the security
and robustness of LLM-generated code by leveraging strongly typed and
verifiable languages, using Scala as a representative example. We evaluate the
effectiveness of our approach in two settings: formal verification with the
Stainless framework and general-purpose secure code generation. Our experiments
with leading open-source LLMs reveal that while direct code generation often
fails to enforce safety constraints, just as naive prompting for more secure
code, our type-focused agentic pipeline substantially mitigates input
validation and injection vulnerabilities. The results demonstrate the potential
of structured, type-guided LLM workflows to improve the SotA of the
trustworthiness of automated code generation in high-assurance domains.

</details>


### [111] [One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification](https://arxiv.org/abs/2510.11160)
*Jens Van Nooten,Andriy Kosar,Guy De Pauw,Walter Daelemans*

Main category: cs.CL

TL;DR: 本文研究了距离基础的无监督文本分类方法，并提出了一种新的优化标签特定阈值的方法，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 距离基础的无监督文本分类方法具有快速推理和适应扩展标签集的优势，但在多标签分类中需要确定文本实例与标签或查询的相似性阈值。

Method: 本文通过探索性研究验证了文本和标签之间的语义关系是否在模型、数据集和标签集之间有所不同，并提出了一种使用验证集优化标签特定阈值的新方法。

Result: 实验结果表明，相似性分布在模型、数据集和标签集之间存在统计学上的显著差异。所提出的标签特定阈值方法在标准化0.5阈值上平均提高了46%，并且比以前的工作中的统一阈值方法平均提高了14%。

Conclusion: 本文提出了一种新的优化标签特定阈值的方法，该方法在多个数据集上表现出色，并且在有限的标记示例下也能保持良好的性能。

Abstract: Distance-based unsupervised text classification is a method within text
classification that leverages the semantic similarity between a label and a
text to determine label relevance. This method provides numerous benefits,
including fast inference and adaptability to expanding label sets, as opposed
to zero-shot, few-shot, and fine-tuned neural networks that require re-training
in such cases. In multi-label distance-based classification and information
retrieval algorithms, thresholds are required to determine whether a text
instance is "similar" to a label or query. Similarity between a text and label
is determined in a dense embedding space, usually generated by state-of-the-art
sentence encoders. Multi-label classification complicates matters, as a text
instance can have multiple true labels, unlike in multi-class or binary
classification, where each instance is assigned only one label. We expand upon
previous literature on this underexplored topic by thoroughly examining and
evaluating the ability of sentence encoders to perform distance-based
classification. First, we perform an exploratory study to verify whether the
semantic relationships between texts and labels vary across models, datasets,
and label sets by conducting experiments on a diverse collection of realistic
multi-label text classification (MLTC) datasets. We find that similarity
distributions show statistically significant differences across models,
datasets and even label sets. We propose a novel method for optimizing
label-specific thresholds using a validation set. Our label-specific
thresholding method achieves an average improvement of 46% over normalized 0.5
thresholding and outperforms uniform thresholding approaches from previous work
by an average of 14%. Additionally, the method demonstrates strong performance
even with limited labeled examples.

</details>


### [112] [Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages](https://arxiv.org/abs/2510.11167)
*Paloma Piot,José Ramom Pichel Campos,Javier Parapar*

Main category: cs.CL

TL;DR: 本研究旨在解决仇恨言论检测中低资源语言数据不足的问题，通过构建多语言语料库并评估大型语言模型的性能，以支持更一致和可扩展的检测方法。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论在社交媒体上迅速传播，对社会凝聚力和个人福祉构成严重威胁。然而，现有研究主要集中在英语上，导致低资源语言的资源和基准有限。此外，许多语言有多种语言变体，这在当前方法中常常被忽视。

Method: 我们编制了一个元集合，包含欧洲西班牙语的仇恨言论数据集，并将其标准化为统一的标签和元数据。我们还通过将其翻译成欧洲葡萄牙语和两种加利西亚语标准来扩展该集合，创建了对齐的多语言语料库。

Result: 我们建立了伊比利亚语言的仇恨言论检测新基准。我们在零样本、少样本和微调设置中评估了最先进的大型语言模型，并为未来的研究提供了基线结果。此外，我们针对目标语言进行了跨语言分析。

Conclusion: 我们的研究强调了多语言和语言变体意识在仇恨言论检测中的重要性，并为欧洲欠代表语言的改进基准测试提供了基础。

Abstract: Hate speech poses a serious threat to social cohesion and individual
well-being, particularly on social media, where it spreads rapidly. While
research on hate speech detection has progressed, it remains largely focused on
English, resulting in limited resources and benchmarks for low-resource
languages. Moreover, many of these languages have multiple linguistic
varieties, a factor often overlooked in current approaches. At the same time,
large language models require substantial amounts of data to perform reliably,
a requirement that low-resource languages often cannot meet. In this work, we
address these gaps by compiling a meta-collection of hate speech datasets for
European Spanish, standardised with unified labels and metadata. This
collection is based on a systematic analysis and integration of existing
resources, aiming to bridge the data gap and support more consistent and
scalable hate speech detection. We extended this collection by translating it
into European Portuguese and into a Galician standard that is more convergent
with Spanish and another Galician variant that is more convergent with
Portuguese, creating aligned multilingual corpora. Using these resources, we
establish new benchmarks for hate speech detection in Iberian languages. We
evaluate state-of-the-art large language models in zero-shot, few-shot, and
fine-tuning settings, providing baseline results for future research. Moreover,
we perform a cross-lingual analysis with our target languages. Our findings
underscore the importance of multilingual and variety-aware approaches in hate
speech detection and offer a foundation for improved benchmarking in
underrepresented European languages.

</details>


### [113] [Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations](https://arxiv.org/abs/2510.11196)
*Johannes Moll,Markus Graf,Tristan Lemke,Nicolas Lenhart,Daniel Truhn,Jean-Benoit Delbrouck,Jiazhen Pan,Daniel Rueckert,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CL

TL;DR: 本文提出了一种基于临床的框架来评估视觉语言模型在胸部X光图像上的解释可信度，发现答案准确性和解释质量是分离的，专有模型在某些指标上优于开源模型，强调了在部署时应评估模型的其他方面。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法很少能检测到链式思维解释与实际决策过程之间的不一致，而更关注答案的准确性或格式的符合性。这种不一致可能会影响高风险临床应用中的信任度。

Method: 研究提出了一种基于临床的框架，用于评估胸部X光图像的视觉问答任务中的链式思维（CoT）解释的可信度。该框架通过控制文本和图像修改的三个轴进行测试：临床保真度、因果归属和置信度校准。

Result: 在一项读者研究中（n=4），评估者-放射科医生的相关性在所有轴上都处于观察到的放射科医生之间的范围内，其中因果归属的对齐程度较强（Kendall's τ_b=0.670），临床保真度的对齐程度中等（τ_b=0.387），而置信度语气的对齐程度较弱（τ_b=0.091）。基准测试六个VLMs显示，答案准确性和解释质量是分离的，注入提示并不确保解释的根基，且文本提示比视觉提示更能改变解释。

Conclusion: 研究指出，尽管一些开源模型在最终答案准确性上表现良好，但专有模型在解释的因果归属和临床保真度方面得分更高，这突显了在部署时需要评估模型的其他方面，而不仅仅是最终答案的准确性。

Abstract: Vision-language models (VLMs) often produce chain-of-thought (CoT)
explanations that sound plausible yet fail to reflect the underlying decision
process, undermining trust in high-stakes clinical use. Existing evaluations
rarely catch this misalignment, prioritizing answer accuracy or adherence to
formats. We present a clinically grounded framework for chest X-ray visual
question answering (VQA) that probes CoT faithfulness via controlled text and
image modifications across three axes: clinical fidelity, causal attribution,
and confidence calibration. In a reader study (n=4), evaluator-radiologist
correlations fall within the observed inter-radiologist range for all axes,
with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate
alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone
($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows
that answer accuracy and explanation quality are decoupled, acknowledging
injected cues does not ensure grounding, and text cues shift explanations more
than visual cues. While some open-source models match final answer accuracy,
proprietary models score higher on attribution (25.0% vs. 1.4%) and often on
fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to
evaluate beyond final answer accuracy.

</details>


### [114] [Discursive Circuits: How Do Language Models Understand Discourse Relations?](https://arxiv.org/abs/2510.11210)
*Yisong Miao,Min-Yen Kan*

Main category: cs.CL

TL;DR: 研究发现，稀疏计算图在话语理解中起关键作用，并且可以在不同话语框架中有效工作。


<details>
  <summary>Details</summary>
Motivation: 研究旨在确定变压器语言模型中哪些组件负责话语理解。

Method: 引入了一个名为CuDR的任务，用于完成给定话语关系的 discourse，并构建了一个最小对比对语料库以支持激活补丁发现。

Result: 稀疏电路（约0.2%的完整GPT-2模型）在基于PDTB的CuDR任务中恢复了话语理解，并且在未见过的话语框架中表现出良好的泛化能力。

Conclusion: 研究发现，稀疏的计算图（称为话语电路）控制模型如何处理话语关系，并且这些电路在不同的话语框架中表现良好。

Abstract: Which components in transformer language models are responsible for discourse
understanding? We hypothesize that sparse computational graphs, termed as
discursive circuits, control how models process discourse relations. Unlike
simpler tasks, discourse relations involve longer spans and complex reasoning.
To make circuit discovery feasible, we introduce a task called Completion under
Discourse Relation (CuDR), where a model completes a discourse given a
specified relation. To support this task, we construct a corpus of minimal
contrastive pairs tailored for activation patching in circuit discovery.
Experiments show that sparse circuits ($\approx 0.2\%$ of a full GPT-2 model)
recover discourse understanding in the English PDTB-based CuDR task. These
circuits generalize well to unseen discourse frameworks such as RST and SDRT.
Further analysis shows lower layers capture linguistic features such as lexical
semantics and coreference, while upper layers encode discourse-level
abstractions. Feature utility is consistent across frameworks (e.g.,
coreference supports Expansion-like relations).

</details>


### [115] [Domain-Specific Data Generation Framework for RAG Adaptation](https://arxiv.org/abs/2510.11217)
*Chris Xing Tian,Weihao Xie,Zhen Chen,Zhengyuan Yi,Hui Liu,Haoliang Li,Shiqi Wang,Siwei Ma*

Main category: cs.CL

TL;DR: RAGen 是一种可扩展且模块化的框架，用于生成领域相关的 QAC 三元组，适用于各种 RAG 适配方法，并能处理大型和不断演变的文档集合。


<details>
  <summary>Details</summary>
Motivation: 有效适应 RAG 系统到特定领域需要超出通用问答的专门、上下文丰富的训练数据。

Method: RAGen 通过识别文档中的关键概念、生成基于布鲁姆分类学原理的多样化问题，并将它们与从相关上下文中提取的精确答案配对，来生成领域相关的 QAC 三元组。它支持多种 RAG 适配策略，包括优化 LLM、检索器和嵌入模型等关键组件。

Result: RAGen 能够生成领域相关的 QAC 三元组，支持多种 RAG 适配策略，并具有语义分块、层次概念提取和多块检索等功能，同时引入了经过筛选的干扰上下文以促进稳健推理。

Conclusion: RAGen 是一个可扩展且模块化的框架，能够有效地适应各种 RAG 适配方法，并适用于动态演进的领域，如科学研究和企业知识库。

Abstract: Retrieval-Augmented Generation (RAG) combines the language understanding and
reasoning power of large language models (LLMs) with external retrieval to
enable domain-grounded responses. Effectively adapting RAG systems to
domain-specific settings requires specialized, context-rich training data
beyond general-purpose question-answering. Here, we propose RAGen, a scalable
and modular framework for generating domain-grounded question-answer-context
(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces
these QAC triples by identifying key concepts in documents, generating diverse
questions guided by Bloom's Taxonomy-inspired principles, and pairing them with
precise answers extracted from relevant contexts. RAGen supports multiple RAG
adaptation strategies, including the optimization of key components such as the
LLM, retriever, and embedding model, etc. Its modular pipeline features
semantic chunking, hierarchical concept extraction, and multi-chunk retrieval,
along with the introduction of curated distractor contexts to promote robust
reasoning. Designed for scalability, RAGen efficiently handles large and
evolving document corpora without redundant processing, making it especially
suitable for dynamic evolving domains such as scientific research and
enterprise knowledge bases.

</details>


### [116] [The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers](https://arxiv.org/abs/2510.11218)
*Saad Obaid ul Islam,Anne Lauscher,Goran Glavaš*

Main category: cs.CL

TL;DR: 本文介绍了SLAQ框架，用于评估大型语言模型在不同复杂度查询下的事实一致性，发现模型在简单和复杂查询之间存在系统性不一致，并揭示了位置依赖的准确性损失和动量效应。


<details>
  <summary>Details</summary>
Motivation: 虽然模型在事实问答基准测试中表现出色，但简单和复杂查询之间的可靠性差距仍未被充分理解，这削弱了它们的可信度。

Method: 我们引入了用于事实问答的短长格式对齐（SLAQ），这是一个受控评估框架，将大型语言模型对同一事实问题的回答进行比较，这些问题（a）单独提出（短）与（b）整合到复杂查询中（长）。

Result: 我们发现大型语言模型在回答短查询和长查询时存在系统性的不一致。我们进一步发现了位置相关的准确性损失和动量效应，其中连续正确的或错误的答案会形成自我强化的模式。通过机制分析，我们发现对齐的事实会激活重叠的模型内部结构，并且基于机制相似性的指标可以预测短长答案的一致性，准确率高达78%。

Conclusion: 我们的工作确立了事实一致性作为大型语言模型可信度的重要方面，并挑战了当前的评估实践，这些实践隐含地假设良好的简单事实查询性能意味着在更复杂的知识寻求任务中的可靠性。

Abstract: Large language models (LLMs) can correctly answer "When was Einstein born?"
yet fail to provide the same date when writing about Einstein's life revealing
a fundamental inconsistency in how models access factual knowledge across task
complexities. While models display impressive accuracy on factual
question-answering benchmarks, the reliability gap between simple and complex
queries remains poorly understood, eroding their trustworthiness. In this work,
we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a
controlled evaluation framework that compares LLMs' answers to the same factual
questions asked (a) in isolation (short) vs. (b) integrated into complex
queries (long). Looking at 16 LLMs across 600 queries, we find a systematic
misalignment of answers to the corresponding short and long queries. We further
uncover position-dependent accuracy loss and momentum effects where consecutive
correct or incorrect answers create self-reinforcing patterns. Through
mechanistic analysis, we find that aligned facts activate overlapping model
internals, and that metrics based on mechanistic similarity can predict
short-long answer alignment with up to 78% accuracy. Our work establishes
factual consistency over query complexity as an important aspect of LLMs'
trustworthiness and challenges current evaluation practices, which implicitly
assume that good performance for simple factual queries implies reliability in
more complex knowledge-seeking tasks too.

</details>


### [117] [WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent](https://arxiv.org/abs/2510.11221)
*Tao Li,Jinlong Hu,Yang Wang,Junfeng Liu,Xuejun Liu*

Main category: cs.CL

TL;DR: WebRouter是一种新型查询特定路由器，通过成本感知的变分信息瓶颈目标，显著降低了Web自动化中的操作成本，同时保持较高的准确性。


<details>
  <summary>Details</summary>
Motivation: LLM-brained web agents在Web自动化中具有强大能力，但面临关键的成本-性能权衡。由于Web agents的复杂提示，导致LLM集合性能下降。

Method: 引入了WebRouter，这是一种从信息论角度训练的查询特定路由器，核心贡献是成本感知的变分信息瓶颈（ca-VIB）目标。

Result: 在WebVoyager基准的五个真实网站上进行的实验表明，与GPT-4o基线相比，WebRouter将操作成本降低了87.8%，而仅造成3.8%的准确性下降。

Conclusion: WebRouter在减少操作成本方面表现出色，同时仅带来轻微的准确性下降。

Abstract: LLM-brained web agents offer powerful capabilities for web automation but
face a critical cost-performance trade-off. The challenge is amplified by web
agents' inherently complex prompts that include goals, action histories, and
environmental states, leading to degraded LLM ensemble performance. To address
this, we introduce WebRouter, a novel query-specific router trained from an
information-theoretic perspective. Our core contribution is a cost-aware
Variational Information Bottleneck (ca-VIB) objective, which learns a
compressed representation of the input prompt while explicitly penalizing the
expected operational cost. Experiments on five real-world websites from the
WebVoyager benchmark show that WebRouter reduces operational costs by a
striking 87.8\% compared to a GPT-4o baseline, while incurring only a 3.8\%
accuracy drop.

</details>


### [118] [Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models](https://arxiv.org/abs/2510.11222)
*Battemuulen Naranbat,Seyed Sahand Mohammadi Ziabari,Yousuf Nasser Al Husaini,Ali Mohammed Mansoor Alsahag*

Main category: cs.CL

TL;DR: 本研究评估了BERT和DistilBERT在道德情感分类任务中的公平性，并引入了一个新的度量标准MFC来衡量跨领域稳定性。结果表明，MFC是一个有效的公平性评估工具，有助于在不同语言环境中更可靠地部署模型。


<details>
  <summary>Details</summary>
Motivation: 确保自然语言处理中的道德情感分类公平性具有挑战性，特别是在跨领域转移的情况下，其中变压器模型被越来越多地部署。

Method: 使用Moral Foundations Twitter Corpus (MFTC)和Moral Foundations Reddit Corpus (MFRC)，本研究在多标签设置中评估了BERT和DistilBERT，在领域内和跨领域协议下进行测试。引入了Moral Fairness Consistency (MFC)度量标准，用于量化道德基础检测的跨领域稳定性。

Result: 观察到明显的不对称性转移，Twitter->Reddit导致micro-F1下降14.9%，而Reddit->Twitter仅下降1.5%。每标签分析揭示了总体分数隐藏的公平性违规；特别是，权威标签表现出0.22-0.23的Demographic Parity Differences和0.40-0.41的Equalized Odds Differences。MFC显示出强大的实证有效性，与Demographic Parity Difference呈完美负相关（rho = -1.000，p < 0.001），同时独立于标准性能指标。

Conclusion: 这些发现确立了MFC作为公平意识评估道德推理模型的补充诊断指标，使在异构语言环境中更可靠的部署成为可能。

Abstract: Ensuring fairness in natural language processing for moral sentiment
classification is challenging, particularly under cross-domain shifts where
transformer models are increasingly deployed. Using the Moral Foundations
Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work
evaluates BERT and DistilBERT in a multi-label setting with in-domain and
cross-domain protocols. Aggregate performance can mask disparities: we observe
pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by
14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness
violations hidden by overall scores; notably, the authority label exhibits
Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of
0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency
(MFC) metric, which quantifies the cross-domain stability of moral foundation
detection. MFC shows strong empirical validity, achieving a perfect negative
correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while
remaining independent of standard performance metrics. Across labels, loyalty
demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC
= 0.78). These findings establish MFC as a complementary, diagnosis-oriented
metric for fairness-aware evaluation of moral reasoning models, enabling more
reliable deployment across heterogeneous linguistic contexts. .

</details>


### [119] [A Theorem-Proving-Based Evaluation of Neural Semantic Parsing](https://arxiv.org/abs/2510.11225)
*Hayate Funakura,Hyunsoo Kim,Koji Mineshima*

Main category: cs.CL

TL;DR: 本文探讨了基于图匹配的评估指标在推理导向应用中的局限性，并提出了一种结合自动定理证明的评估方法，以提高逻辑等价性的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图匹配度量标准（如Smatch）仅捕捉表面重叠，而不是逻辑等价性，因此需要一种更有效的评估方法来确保逻辑等价性。

Method: 本文通过将图匹配与自动定理证明相结合来重新评估评估方法，并比较了两种构建解析器的方法：监督微调（T5-Small/Base）和少样本上下文学习（GPT-4o/4.1/5），在标准化和非标准化目标下进行评估。

Result: 研究发现，在图匹配上表现良好的模型往往无法生成逻辑等价的公式。规范化减少了目标的偶然变化，提高了公式的正确性，并增强了逻辑充分性。错误分析显示，随着公式复杂性的增加，以及涉及并列结构、介词短语和被动语态时，性能会下降，主要失败原因包括变量绑定和索引以及谓词命名。

Conclusion: 本文指出基于图匹配的评估指标在推理导向的应用中存在局限性，并提出了逻辑敏感的评估和训练目标以及简化的、规范化的目标表示的重要性。

Abstract: Graph-matching metrics such as Smatch are the de facto standard for
evaluating neural semantic parsers, yet they capture surface overlap rather
than logical equivalence. We reassess evaluation by pairing graph-matching with
automated theorem proving. We compare two approaches to building parsers:
supervised fine-tuning (T5-Small/Base) and few-shot in-context learning
(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs
using graph-matching, bidirectional entailment between source and target
formulas with a first-order logic theorem prover, and well-formedness. Across
settings, we find that models performing well on graph-matching often fail to
produce logically equivalent formulas. Normalization reduces incidental target
variability, improves well-formedness, and strengthens logical adequacy. Error
analysis shows performance degrades with increasing formula complexity and with
coordination, prepositional phrases, and passive voice; the dominant failures
involve variable binding and indexing, and predicate naming. These findings
highlight limits of graph-based metrics for reasoning-oriented applications and
motivate logic-sensitive evaluation and training objectives together with
simplified, normalized target representations. All code and data for our
experiments are publicly available.

</details>


### [120] [CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis](https://arxiv.org/abs/2510.11233)
*Jinyuan Xu,Tian Lan,Xintao Yu,Xue He,Hezhi Zhang,Ying Wang,Pierre Magistry,Mathieu Valette,Lei Li*

Main category: cs.CL

TL;DR: The paper introduces CNSocialDepress, a benchmark dataset for depression risk detection from Chinese social media posts, which provides binary risk labels and structured multi-dimensional psychological attributes, demonstrating its utility across various NLP tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of scarce and mostly binary classification Chinese-language resources for risk detection of depression.

Method: The paper releases CNSocialDepress, a benchmark dataset for depression risk detection from Chinese social media posts.

Result: Experimental results demonstrate the utility of CNSocialDepress across a wide range of NLP tasks, including structured psychological profiling and fine-tuning of large language models for depression detection.

Conclusion: CNSocialDepress provides insights to mental health applications tailored for Chinese-speaking populations.

Abstract: Depression is a pressing global public health issue, yet publicly available
Chinese-language resources for risk detection remain scarce and are mostly
limited to binary classification. To address this limitation, we release
CNSocialDepress, a benchmark dataset for depression risk detection from Chinese
social media posts. The dataset contains 44,178 texts from 233 users, within
which psychological experts annotated 10,306 depression-related segments.
CNSocialDepress provides binary risk labels together with structured
multi-dimensional psychological attributes, enabling interpretable and
fine-grained analysis of depressive signals. Experimental results demonstrate
its utility across a wide range of NLP tasks, including structured
psychological profiling and fine-tuning of large language models for depression
detection. Comprehensive evaluations highlight the dataset's effectiveness and
practical value for depression risk identification and psychological analysis,
thereby providing insights to mental health applications tailored for
Chinese-speaking populations.

</details>


### [121] [XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression](https://arxiv.org/abs/2510.11236)
*Haoqi Yang,Yao Yao,Zuchao Li,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: XQuant是一种无需训练的框架，通过数据无关校准和跨层KV缓存压缩实现超低比特位的KV缓存量化，在保持性能的同时提高内存效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种自然语言处理任务中表现出色，但由于KV缓存增长导致的内存需求大，给资源受限环境的部署带来了挑战。量化被提出作为减少内存消耗的解决方案。

Method: XQuant是一种无需训练且即插即用的框架，引入了计算开销可忽略的数据无关校准方法和跨层KV缓存压缩，实现了亚1.4位的量化。

Result: 在TruthfulQA和LongBench上的广泛实验表明，XQuant优于最先进的方法（如KIVI-2bit和AsymKV-1.5bit），实现了更低的位宽同时保持优越的性能。

Conclusion: XQuant在保持模型准确性的同时，实现了更优的内存效率和模型性能，为资源受限环境下的部署提供了更好的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks. However, their extensive memory
requirements, particularly due to KV cache growth during long-text
understanding and generation, present significant challenges for deployment in
resource-constrained environments. Quantization has emerged as a promising
solution to reduce memory consumption while preserving historical information.
We propose XQuant, a training-free and plug-and-play framework that achieves
ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key
innovations: a computationally negligible data-free calibration method and
cross-layer KV cache compression, enabling quantization to sub-1.4 bits.
Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant
outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by
achieving lower bit-width while maintaining superior performance, establishing
a better trade-off between memory efficiency and model accuracy.

</details>


### [122] [Attacks by Content: Automated Fact-checking is an AI Security Issue](https://arxiv.org/abs/2510.11238)
*Michael Schlichtkrull*

Main category: cs.CL

TL;DR: 本文探讨了AI代理在检索和推理外部文档时可能受到的攻击，并提出通过重新利用自动化事实核查任务作为认知自我防御工具来应对这些攻击。


<details>
  <summary>Details</summary>
Motivation: When AI agents retrieve and reason over external documents, adversaries can manipulate the data they receive to subvert their behaviour. Previous research has studied indirect prompt injection, where the attacker injects malicious instructions. We argue that injection of instructions is not necessary to manipulate agents - attackers could instead supply biased, misleading, or false information.

Method: We argue that this is analogous to an existing NLP task, automated fact-checking, which we propose to repurpose as a cognitive self-defense tool for agents.

Result: Existing defenses, which focus on detecting hidden commands, are ineffective against attacks by content.

Conclusion:  agents must critically evaluate retrieved information, corroborating claims with external evidence and evaluating source trustworthiness.

Abstract: When AI agents retrieve and reason over external documents, adversaries can
manipulate the data they receive to subvert their behaviour. Previous research
has studied indirect prompt injection, where the attacker injects malicious
instructions. We argue that injection of instructions is not necessary to
manipulate agents - attackers could instead supply biased, misleading, or false
information. We term this an attack by content. Existing defenses, which focus
on detecting hidden commands, are ineffective against attacks by content. To
defend themselves and their users, agents must critically evaluate retrieved
information, corroborating claims with external evidence and evaluating source
trustworthiness. We argue that this is analogous to an existing NLP task,
automated fact-checking, which we propose to repurpose as a cognitive
self-defense tool for agents.

</details>


### [123] [Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality](https://arxiv.org/abs/2510.11254)
*Jana Jung,Marlene Lutz,Indira Sen,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本研究评估了人类心理测量测试在大型语言模型中的可靠性和有效性，发现其有效性较低，表明需要调整才能适用于LLMs。


<details>
  <summary>Details</summary>
Motivation: 心理测量测试越来越多地用于评估大型语言模型（LLMs）中的心理构念。然而，这些原本为人类设计的测试在应用于LLMs时是否能产生有意义的结果仍不清楚。

Method: 我们系统地评估了三种构念（性别歧视、种族主义和道德）的人类心理测量测试的可靠性和有效性。我们通过收敛方法（即基于理论的测试间相关性）和生态方法（即测试分数与现实世界下游任务中的行为的一致性）来评估有效性。

Result: 我们在多个项目和提示变化中发现了中等的可靠性。通过收敛和生态方法评估的有效性显示，心理测量测试得分与下游任务中的模型行为不一致，甚至在某些情况下呈负相关，表明生态效度较低。

Conclusion: 我们的结果表明，在解释心理测量测试的分数之前，对这些测试进行系统评估是至关重要的。此外，这表明为人类设计的心理测量测试不能直接应用于大型语言模型，而需要进行调整。

Abstract: Psychometric tests are increasingly used to assess psychological constructs
in large language models (LLMs). However, it remains unclear whether these
tests -- originally developed for humans -- yield meaningful results when
applied to LLMs. In this study, we systematically evaluate the reliability and
validity of human psychometric tests for three constructs: sexism, racism, and
morality. We find moderate reliability across multiple item and prompt
variations. Validity is evaluated through both convergent (i.e., testing
theory-based inter-test correlations) and ecological approaches (i.e., testing
the alignment between tests scores and behavior in real-world downstream
tasks). Crucially, we find that psychometric test scores do not align, and in
some cases even negatively correlate with, model behavior in downstream tasks,
indicating low ecological validity. Our results highlight that systematic
evaluations of psychometric tests is essential before interpreting their
scores. They also suggest that psychometric tests designed for humans cannot be
applied directly to LLMs without adaptation.

</details>


### [124] [Towards Real-Time Fake News Detection under Evidence Scarcity](https://arxiv.org/abs/2510.11277)
*Guangyu Wei,Ke Han,Yueming Lyu,Yu Luo,Yue Jiang,Caifeng Shan,Nicu Sebe*

Main category: cs.CL

TL;DR: EASE is a novel framework for real-time fake news detection that dynamically adapts its decision-making process according to the assessed sufficiency of available evidence.


<details>
  <summary>Details</summary>
Motivation: Fake news detection becomes particularly challenging in real-time scenarios, where emerging events often lack sufficient supporting evidence. Existing approaches often rely heavily on external evidence and therefore struggle to generalize under evidence scarcity.

Method: EASE introduces a sequential evaluation mechanism comprising three independent perspectives: (1) Evidence-based evaluation, (2) Reasoning-based evaluation, and (3) Sentiment-based fallback. It employs instruction tuning with pseudo labels to guide each evaluator in justifying its perspective-specific knowledge through interpretable reasoning.

Result: Extensive experiments demonstrate that EASE not only achieves state-of-the-art performance across multiple benchmarks, but also significantly improves generalization to real-time news.

Conclusion: EASE not only achieves state-of-the-art performance across multiple benchmarks, but also significantly improves generalization to real-time news.

Abstract: Fake news detection becomes particularly challenging in real-time scenarios,
where emerging events often lack sufficient supporting evidence. Existing
approaches often rely heavily on external evidence and therefore struggle to
generalize under evidence scarcity. To address this issue, we propose
Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time
fake news detection that dynamically adapts its decision-making process
according to the assessed sufficiency of available evidence. EASE introduces a
sequential evaluation mechanism comprising three independent perspectives: (1)
Evidence-based evaluation, which assesses evidence and incorporates it into
decision-making only when the evidence is sufficiently supportive; (2)
Reasoning-based evaluation, which leverages the world knowledge of large
language models (LLMs) and applies them only when their reliability is
adequately established; and (3) Sentiment-based fallback, which integrates
sentiment cues when neither evidence nor reasoning is reliable. To enhance the
accuracy of evaluation processes, EASE employs instruction tuning with pseudo
labels to guide each evaluator in justifying its perspective-specific knowledge
through interpretable reasoning. Furthermore, the expert modules integrate the
evaluators' justified assessments with the news content to enable
evaluation-aware decision-making, thereby enhancing overall detection accuracy.
Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news
for evaluating model generalization on emerging news with limited evidence.
Extensive experiments demonstrate that EASE not only achieves state-of-the-art
performance across multiple benchmarks, but also significantly improves
generalization to real-time news. The code and dataset are available:
https://github.com/wgyhhhh/EASE.

</details>


### [125] [Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs](https://arxiv.org/abs/2510.11288)
*Nikita Afonin,Nikita Andriyanov,Nikhil Bageshpura,Kyle Liu,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Alexander Panchenko,Oleg Rogov,Elena Tutubalina,Mikhail Seleznyov*

Main category: cs.CL

TL;DR: 研究发现，窄范围的上下文学习（ICL）也可能导致广泛不对齐（EM），且不对齐的比例随着上下文示例数量的增加而增加。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究表明窄微调可能导致EM，但尚未研究ICL是否也会导致EM。因此，本研究旨在探讨ICL中是否存在EM现象。

Method: 研究通过在三个数据集上使用三种前沿模型进行实验，评估了在给定64个窄范围上下文示例和256个示例时模型生成广泛不对齐响应的比率。此外，还通过引发逐步推理来分析EM的机制。

Result: 研究发现，在给定64个窄范围上下文示例时，三种前沿模型生成广泛不对齐响应的比例在2%到17%之间，而当给出256个示例时，这一比例最高可达58%。手动分析显示，67.5%的不对齐轨迹通过采用鲁莽或危险的“人格”来合理化有害输出。

Conclusion: 研究发现，通过窄范围的上下文学习（ICL）也可以产生广泛不对齐的现象，即涌现不对齐（EM）。

Abstract: Recent work has shown that narrow finetuning can produce broadly misaligned
LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these
findings were limited to finetuning and activation steering, leaving out
in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find
that it does: across three datasets, three frontier models produce broadly
misaligned responses at rates between 2% and 17% given 64 narrow in-context
examples, and up to 58% with 256 examples. We also examine mechanisms of EM by
eliciting step-by-step reasoning (while leaving in-context examples unchanged).
Manual analysis of the resulting chain-of-thought shows that 67.5% of
misaligned traces explicitly rationalize harmful outputs by adopting a reckless
or dangerous ''persona'', echoing prior results on finetuning-induced EM.

</details>


### [126] [Are Large Language Models Effective Knowledge Graph Constructors?](https://arxiv.org/abs/2510.11297)
*Ruirui Chen,Weifeng Jiang,Chengwei Qin,Bo Xiong,Fiona Liausvia,Dongkyu Choi,Boon Kiat Quek*

Main category: cs.CL

TL;DR: 本文提出了一种分层提取框架，用于构建语义丰富且结构良好的知识图谱，并发布了基于研究论文的LLM生成KG的数据集，以促进高风险领域中的应用。


<details>
  <summary>Details</summary>
Motivation: 构建高质量的KG仍然很困难，需要准确的信息提取和结构化表示，这些表示支持可解释性和下游效用。现有的基于LLM的方法往往过于专注于实体和关系提取，限制了句子级别的上下文或依赖于预定义的模式。

Method: 我们提出了一个分层提取框架，该框架在多个层次上组织信息，使创建语义丰富且结构良好的KG成为可能。使用最先进的LLMs，我们提取并构建知识图谱，并从结构和语义两个方面进行全面评估。

Result: 我们的结果突显了当前LLMs在KG构建中的优势和不足，并指出了未来工作的关键挑战。

Conclusion: 我们的结果突显了当前LLMs在KG构建中的优势和不足，并指出了未来工作的关键挑战。为了推动这一领域的发展，我们还发布了一个精心整理的数据集，该数据集来自关于儿童心理健康的研究论文的LLM生成的KG。这个资源旨在促进高风险领域（如医疗保健）中更加透明、可靠和有影响的应用。

Abstract: Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown
promise in reducing hallucinations in large language models (LLMs). However,
constructing high-quality KGs remains difficult, requiring accurate information
extraction and structured representations that support interpretability and
downstream utility. Existing LLM-based approaches often focus narrowly on
entity and relation extraction, limiting coverage to sentence-level contexts or
relying on predefined schemas. We propose a hierarchical extraction framework
that organizes information at multiple levels, enabling the creation of
semantically rich and well-structured KGs. Using state-of-the-art LLMs, we
extract and construct knowledge graphs and evaluate them comprehensively from
both structural and semantic perspectives. Our results highlight the strengths
and shortcomings of current LLMs in KG construction and identify key challenges
for future work. To advance research in this area, we also release a curated
dataset of LLM-generated KGs derived from research papers on children's mental
well-being. This resource aims to foster more transparent, reliable, and
impactful applications in high-stakes domains such as healthcare.

</details>


### [127] [FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks](https://arxiv.org/abs/2510.11307)
*Sabrina McCallum,Amit Parekh,Alessandro Suglia*

Main category: cs.CL

TL;DR: 本文探讨了如何让使用模仿学习训练的代理在获得建设性的语言反馈时，从最优和次优示范中学习稳健的表示。我们直接将语言反馈嵌入作为输入序列的一部分输入到基于Transformer的策略中，并可选地用辅助自监督学习目标来补充传统的下一步动作预测目标。我们在自定义的BabyAI-XGen环境中对一系列具身视觉和语言任务进行了测试，并展示了代理在组合泛化能力和鲁棒性方面的显著提升，这表明我们的数据高效方法使模型能够成功地将次优行为转化为学习机会。总体而言，我们的结果表明，语言反馈是语言指定的具身任务中中间标量奖励的有竞争力且直观的替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前的具身AI方法倾向于从专家示范中学习策略。然而，没有评估示范动作质量的机制，它们只能学习最优行为，或者可能复制错误和低效率。虽然强化学习提供了一种替代方案，但相关的探索通常会导致数据效率的牺牲。

Method: 我们直接将语言反馈嵌入作为输入序列的一部分输入到基于Transformer的策略中，并可选地用辅助自监督学习目标来补充传统的下一步动作预测目标。

Result: 我们在自定义的BabyAI-XGen环境中对一系列具身视觉和语言任务进行了测试，并展示了代理在组合泛化能力和鲁棒性方面的显著提升，这表明我们的数据高效方法使模型能够成功地将次优行为转化为学习机会。

Conclusion: 我们的结果表明，语言反馈是语言指定的具身任务中中间标量奖励的有竞争力且直观的替代方案。

Abstract: Current approaches to embodied AI tend to learn policies from expert
demonstrations. However, without a mechanism to evaluate the quality of
demonstrated actions, they are limited to learning from optimal behaviour, or
they risk replicating errors and inefficiencies. While reinforcement learning
offers one alternative, the associated exploration typically results in
sacrificing data efficiency. This work explores how agents trained with
imitation learning can learn robust representations from both optimal and
suboptimal demonstrations when given access to constructive language feedback
as a means to contextualise different modes of behaviour. We directly provide
language feedback embeddings as part of the input sequence into a
Transformer-based policy, and optionally complement the traditional next action
prediction objective with auxiliary self-supervised learning objectives for
feedback prediction. We test our approach on a range of embodied
Vision-and-Language tasks in our custom BabyAI-XGen environment and show
significant improvements in agents' compositional generalisation abilities and
robustness, suggesting that our data-efficient method allows models to
successfully convert suboptimal behaviour into learning opportunities. Overall,
our results suggest that language feedback is a competitive and intuitive
alternative to intermediate scalar rewards for language-specified embodied
tasks.

</details>


### [128] [Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications](https://arxiv.org/abs/2510.11314)
*Belkiss Souayed,Sarah Ebling,Yingqiang Gao*

Main category: cs.CL

TL;DR: 本文提出了一种结构化的视觉-语言模型提示框架，用于从简化的文本中生成可访问的图像。通过五种不同的提示模板进行评估，结果显示基本对象焦点模板在语义对齐方面表现最佳，而复古风格在可访问性方面表现最好。


<details>
  <summary>Details</summary>
Motivation: 个体有智力障碍的人常常难以理解复杂的文本。虽然许多文本到图像模型优先考虑美学而非可访问性，但视觉插图与从它们生成的文本简化（TS）之间的关系尚不清楚。本文提出了一个结构化的视觉-语言模型（VLM）提示框架，用于从简化的文本中生成可访问的图像。

Method: 我们设计了五种提示模板，即基本对象焦点、情境场景、教育布局、多级细节和网格布局，每种模板遵循不同的空间排列，同时遵守可访问性约束，如对象数量限制、空间分离和内容限制。

Result: 结果表明，基本对象焦点提示模板实现了最高的语义对齐，这表明视觉极简主义可以提高语言可访问性。专家评估进一步确定了复古风格是最具可访问性的，而维基百科是效果最好的数据源。不同维度的标注者间协议各不相同，文本简化显示出很强的可靠性，而图像质量则更具主观性。

Conclusion: 我们的框架为可访问内容生成提供了实用指南，并强调了结构化提示在AI生成的视觉可访问性工具中的重要性。

Abstract: Individuals with intellectual disabilities often have difficulties in
comprehending complex texts. While many text-to-image models prioritize
aesthetics over accessibility, it is not clear how visual illustrations relate
to text simplifications (TS) generated from them. This paper presents a
structured vision-language model (VLM) prompting framework for generating
accessible images from simplified texts. We designed five prompt templates,
i.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level
Detail, and Grid Layout, each following distinct spatial arrangements while
adhering to accessibility constraints such as object count limits, spatial
separation, and content restrictions. Using 400 sentence-level simplifications
from four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and
ASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template
effectiveness with CLIPScores, and Phase 2 involved human annotation of
generated images across ten visual styles by four accessibility experts.
Results show that the Basic Object Focus prompt template achieved the highest
semantic alignment, indicating that visual minimalism enhances language
accessibility. Expert evaluation further identified Retro style as the most
accessible and Wikipedia as the most effective data source. Inter-annotator
agreement varied across dimensions, with Text Simplicity showing strong
reliability and Image Quality proving more subjective. Overall, our framework
offers practical guidelines for accessible content generation and underscores
the importance of structured prompting in AI-generated visual accessibility
tools.

</details>


### [129] [Do LLMs "Feel"? Emotion Circuits Discovery and Control](https://arxiv.org/abs/2510.11328)
*Chenxi Wang,Yixuan Zhang,Ruiji Yu,Yufei Zheng,Lang Gao,Zirui Song,Zixiang Xu,Gus Xia,Huishuai Zhang,Dongyan Zhao,Xiuying Chen*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型中的情感电路，通过构建受控数据集、分析神经元和注意力头的作用，以及整合局部组件形成全局情感电路，实现了高精度的情感表达控制。


<details>
  <summary>Details</summary>
Motivation: 随着对大型语言模型（LLMs）中情感智能的需求增加，关键挑战在于理解产生情感表达的内部机制，并控制生成文本中的情感。

Method: 构建了一个受控数据集SEV（Scenario-Event with Valence）以引发跨情绪的可比较内部状态，并通过分析分解和因果分析识别出局部实现情感计算的神经元和注意力头，同时通过消融和增强干预验证其因果作用。此外，量化每个子层对模型最终情感表示的因果影响，并将识别出的局部组件整合成驱动情感表达的连贯全局情感电路。

Result: 直接调节这些电路在测试集上实现了99.65%的情感表达准确性，超过了提示和引导方法。

Conclusion: 这项研究首次系统地揭示并验证了大型语言模型中的情感电路，为可解释性和可控的情感智能提供了新的见解。

Abstract: As the demand for emotional intelligence in large language models (LLMs)
grows, a key challenge lies in understanding the internal mechanisms that give
rise to emotional expression and in controlling emotions in generated text.
This study addresses three core questions: (1) Do LLMs contain context-agnostic
mechanisms shaping emotional expression? (2) What form do these mechanisms
take? (3) Can they be harnessed for universal emotion control? We first
construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit
comparable internal states across emotions. Subsequently, we extract
context-agnostic emotion directions that reveal consistent, cross-context
encoding of emotion (Q1). We identify neurons and attention heads that locally
implement emotional computation through analytical decomposition and causal
analysis, and validate their causal roles via ablation and enhancement
interventions. Next, we quantify each sublayer's causal influence on the
model's final emotion representation and integrate the identified local
components into coherent global emotion circuits that drive emotional
expression (Q2). Directly modulating these circuits achieves 99.65%
emotion-expression accuracy on the test set, surpassing prompting- and
steering-based methods (Q3). To our knowledge, this is the first systematic
study to uncover and validate emotion circuits in LLMs, offering new insights
into interpretability and controllable emotional intelligence.

</details>


### [130] [LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.11358)
*Hengran Zhang,Keping Bi,Jiafeng Guo,Jiaming Zhang,Shuaiqiang Wang,Dawei Yin,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本研究探讨了LLM特定效用的概念，并提出了一种基准测试程序来评估效用判断。结果表明，人类标注的段落并不总是最适合特定LLM的，且效用段落不具备跨LLM的可移植性。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将效用视为通用属性，而忽略了不同LLM可能因内部知识和理解能力的不同而从同一段落中受益不同。

Method: 我们通过大规模实验跨多个数据集和LLM进行系统研究，以探讨LLM特定效用的概念。

Result: 我们发现人类标注的段落并不是LLM的最佳选择，且真正的效用段落不能在不同LLM之间转移。此外，我们发现基于困惑度的指标是评估LLM对查询和段落可读性差异的关键。

Conclusion: 我们的研究表明，需要在RAG研究中采用LLM特定的效用，并提出了一个基准测试程序来评估LLM特定的效用判断。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. While traditional retrieval focuses on
relevance, RAG's effectiveness depends on the utility of retrieved passages,
i.e., the usefulness in facilitating the generation of an accurate and
comprehensive answer. Existing studies often treat utility as a generic
attribute, ignoring the fact that different LLMs may benefit differently from
the same passage due to variations in internal knowledge and comprehension
ability. In this work, we introduce and systematically investigate the notion
of LLM-specific utility. Through large-scale experiments across multiple
datasets and LLMs, we demonstrate that human-annotated passages are not optimal
for LLMs and that ground-truth utilitarian passages are not transferable across
different LLMs. These findings highlight the necessity of adopting the
LLM-specific utility in RAG research. Our findings indicate that some
human-annotated passages are not ground-truth utilitarian passages for specific
LLMs, partially due to the varying readability of queries and passages for
LLMs, a tendency for which perplexity is a key metric. Based on these findings,
we propose a benchmarking procedure for LLM-specific utility judgments. We
evaluate existing utility judgment methods on six datasets and find that while
verbalized methods using pseudo-answers perform robustly, LLMs struggle to
assess utility effectively-failing to reject all passages for known queries and
to select truly useful ones for unknown queries.

</details>


### [131] [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/abs/2510.11370)
*Wenhan Ma,Hailin Zhang,Liang Zhao,Yifan Song,Yudong Wang,Zhifang Sui,Fuli Luo*

Main category: cs.CL

TL;DR: 本文提出了一种名为Rollout Routing Replay (R3)的方法，以解决Mixture-of-Experts (MoE)模型中由于路由机制不稳定而导致的RL训练崩溃问题。R3通过记录推理引擎中的路由分布并在训练期间重放它们来减少训练-推理策略KL散度，从而稳定RL训练。


<details>
  <summary>Details</summary>
Motivation: 在Mixture-of-Experts (MoE)模型中，路由机制常常引入不稳定性，甚至导致灾难性的RL训练崩溃。我们分析了MoE模型的训练-推理一致性，并发现了两个阶段之间路由行为的显著差异。此外，即使在相同条件下，路由框架也可以在重复的前向传递中产生不同的专家选择。

Method: 我们提出了Rollout Routing Replay (R3)，该方法从推理引擎中记录路由分布并在训练期间重放它们。

Result: R3显著降低了训练-推理策略KL散度，并缓解了极端差异，而不会影响训练速度。广泛的实验表明，R3成功地稳定了RL训练，防止了崩溃，并优于GSPO和TIS等方法。

Conclusion: 本文认为，这项工作可以为稳定MoE模型中的RL提供新的解决方案。

Abstract: Reinforcement learning (RL) has emerged as a crucial approach for enhancing
the capabilities of large language models. However, in Mixture-of-Experts (MoE)
models, the routing mechanism often introduces instability, even leading to
catastrophic RL training collapse. We analyze the training-inference
consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions,
the routing framework can yield divergent expert selections across repeated
forward passes. To address this foundational inconsistency, we propose Rollout
Routing Replay (R3), a method that records routing distributions from the
inference engine and replays them during training. R3 significantly reduces
training-inference policy KL divergence and mitigates extreme discrepancies
without compromising training speed. Extensive experiments on various settings
confirm that R3 succeeds in stabilizing RL training, preventing collapse and
outperforming methods such as GSPO and TIS. We believe this work can offer a
new solution for stabilizing RL in MoE models.

</details>


### [132] [Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning](https://arxiv.org/abs/2510.11372)
*Dean L. Slack,Noura Al Moubayed*

Main category: cs.CL

TL;DR: 研究分析了语言模型微调过程中的记忆现象，并提出了一种有效减少记忆的方法。


<details>
  <summary>Details</summary>
Motivation: 大多数防御措施针对预训练阶段，而微调期间的记忆，特别是在领域适应和指令调整中，了解不足。

Method: 我们对Pythia、Llama3和Mistral模型进行了微调，并跟踪了训练过程中的逐字记忆。我们使用了一个简单但有效的n-gram记忆分数作为早期停止标准，并引入了一个n-gram感知的损失正则化器。

Result: 我们发现记忆在前几个epoch中显著增加，通常在验证困惑度或评估性能优化之前。使用n-gram记忆分数作为早期停止标准可以以最小的性能损失减轻记忆。此外，引入的n-gram感知损失正则化器将记忆减少了多达40%。

Conclusion: 这些结果为语言模型微调过程中的记忆动态提供了实用且可扩展的见解。

Abstract: Although large language models excel across many tasks, they can memorise
training data and thereby expose private or copyrighted text. Most defences
target the pre-training stage, leaving memorisation during fine-tuning,
especially for domain adaptation and instruction tuning, poorly understood. We
fine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on
common evaluation datasets and track verbatim memorisation throughout training.
We find that memorisation increases dramatically in the first few epochs, often
significantly before either validation perplexity or evaluation performance is
optimised. We use a simple but effective n-gram memorisation score which
reliably precedes verbatim memorisation; using it as an early-stopping
criterion mitigates memorisation with minimal performance loss. Further, we
introduce an n-gram-aware loss regulariser and show that it reduces
memorisation across all model families tested by up to 40% while minimising
evaluation performance trade-offs when compared to an existing memorisation
mitigation strategy. These results yield practical, scalable insights into
memorisation dynamics during language model fine-tuning.

</details>


### [133] [Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies](https://arxiv.org/abs/2510.11389)
*Zirui Song,Yuan Huang,Junchang Liu,Haozhe Luo,Chenxi Wang,Lang Gao,Zixiang Xu,Mingfei Han,Xiaojun Chang,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的策略对齐评估框架，并创建了一个高质量的Werewolf数据集，用于更细致地评估模型的语言和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究将游戏简化为基于LLM的自我对弈，导致模板化的言语和轶事案例，忽视了社交游戏的丰富性。此外，评估依赖于粗略指标，如生存时间或主观评分，由于缺乏高质量参考数据。

Method: 我们提出了一个基于策略对齐的评估框架，该框架利用获胜阵营的策略作为真实情况，在两个阶段进行评估：1) 语音评估，将其形式化为选择题任务，以评估模型在五个维度的社会能力中采取适当立场的能力；2) 决策评估，评估模型的投票选择和对手角色推断。

Result: 实验表明，最先进的LLM表现出多样化的表现，大约有一半低于0.50，揭示了在欺骗和反事实推理方面的明显差距。

Conclusion: 我们的数据集和评估框架为研究多智能体交互中的语言、推理和策略提供了新的方向，希望进一步激发相关研究。

Abstract: Social deduction games like Werewolf combine language, reasoning, and
strategy, providing a testbed for studying natural language and social
intelligence. However, most studies reduce the game to LLM-based self-play,
yielding templated utterances and anecdotal cases that overlook the richness of
social gameplay. Evaluation further relies on coarse metrics such as survival
time or subjective scoring due to the lack of quality reference data. To
address these gaps, we curate a high-quality, human-verified multimodal
Werewolf dataset containing over 100 hours of video, 32.4M utterance tokens,
and 15 rule variants. Based on this dataset, we propose a novel
strategy-alignment evaluation that leverages the winning faction's strategies
as ground truth in two stages: 1) Speech evaluation, formulated as
multiple-choice-style tasks that assess whether the model can adopt appropriate
stances across five dimensions of social ability; and 2) Decision evaluation,
which assesses the model's voting choices and opponent-role inferences. This
framework enables a fine-grained evaluation of models' linguistic and reasoning
capabilities, while capturing their ability to generate strategically coherent
gameplay. Our experiments show that state-of-the-art LLMs show diverse
performance, with roughly half remain below 0.50, revealing clear gaps in
deception and counterfactual reasoning. We hope our dataset further inspires
research on language, reasoning, and strategy in multi-agent interaction.

</details>


### [134] [KnowRL: Teaching Language Models to Know What They Know](https://arxiv.org/abs/2510.11407)
*Sahil Kale,Devendra Singh Dhami*

Main category: cs.CL

TL;DR: 本文提出了一种名为KnowRL的框架，旨在提高大型语言模型对自身知识边界的理解，从而实现更安全和负责任的行为。通过反思和基于共识的奖励机制，KnowRL无需外部监督即可提升模型的自我知识一致性，并在实验中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 真正可靠的AI不仅仅需要扩大知识规模，还需要能够知道它知道什么以及何时不知道。然而最近的研究表明，即使最好的LLMs在超过五分之一的情况下也会错误地判断自己的能力，使得基于这种内部不确定性的任何回应都无法完全信任。

Method: 我们提出了一个名为KnowRL的简单但强大的框架，该框架结合了两个组件：(i) 反思，其中模型生成并分类它认为可行或不可行的任务，以及(ii) 基于共识的奖励，通过内部一致来加强自我知识评估的稳定性。

Result: 在LLaMA-3.1-8B和Qwen-2.5-7B上的实验表明，KnowRL稳步提高了自我知识，这得到了内在自一致性以及外在基准测试的验证。仅需一个小的种子集和没有外部监督，我们的方法在准确率和F1上分别提高了高达28%和12%，在几个迭代中超过了基线。

Conclusion: 我们的框架本质上解锁了大型语言模型在知识意识方面自我改进的未开发潜力，为可靠、更可问责的人工智能打开了大门，并在关键应用中实现了更安全的部署。由于其简单性和对外部努力的独立性，我们鼓励将这种增强可靠性的过程应用于所有未来的模型。

Abstract: Truly reliable AI requires more than simply scaling up knowledge; it demands
the ability to know what it knows and when it does not. Yet recent research
shows that even the best LLMs misjudge their own competence in more than one in
five cases, making any response born of such internal uncertainty impossible to
fully trust. Inspired by self-improvement reinforcement learning techniques
that require minimal data, we present a simple but powerful framework KnowRL
that strengthens a model's internal understanding of its own feasibility
boundaries, enabling safer and more responsible behaviour. Our framework
combines two components: (i) introspection, where the model generates and
classifies tasks it judges feasible or infeasible, and (ii) consensus-based
rewarding, where stability of self-knowledge assessment is reinforced through
internal agreement. By using internally generated data, this design strengthens
consistency in self-knowledge and entirely avoids costly external supervision.
In experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved
self-knowledge, validated by both intrinsic self-consistency and extrinsic
benchmarking. With nothing more than a small seed set and no external
supervision, our method drove gains as high as 28% in accuracy and 12% in F1,
outperforming baselines in just a few iterations. Our framework essentially
unlocks the untapped capacity of LLMs to self-improve their knowledge
awareness, opening the door to reliable, more accountable AI and safer
deployment in critical applications. Owing to its simplicity and independence
from external effort, we encourage applying this reliability-enhancing process
to all future models.

</details>


### [135] [Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification](https://arxiv.org/abs/2510.11408)
*Stefan Krsteski,Giuseppe Russo,Serina Chang,Robert West,Kristina Gligorić*

Main category: cs.CL

TL;DR: 研究探讨了使用大型语言模型生成调查回答和校正方法之间的相互作用，发现结合这两种方法可以显著减少偏差并提高效率，挑战了使用所有人类响应进行微调的做法。


<details>
  <summary>Details</summary>
Motivation: 调查提供了关于公众意见和行为的有价值见解，但执行成本高且缓慢。大型语言模型（LLMs）被提出作为人类响应的可扩展、低成本替代方案，但其输出往往存在偏差，导致无效估计。

Method: 研究使用两个面板调查，涉及营养、政治和经济问题，分析了合成方法和校正方法之间的相互作用，并探索了如何最佳分配人类响应。

Result: 研究发现，单独使用合成方法会引入显著偏差（24-86%），而将其与校正结合可将偏差降低至5%以下，并将有效样本量提高多达14%。

Conclusion: 研究发现，将合成方法与校正方法结合可以显著减少偏差并提高有效样本量，挑战了使用所有人类响应进行微调的常见做法，表明在固定预算下，将大部分资源分配给校正可以实现更有效的估计。

Abstract: Surveys provide valuable insights into public opinion and behavior, but their
execution is costly and slow. Large language models (LLMs) have been proposed
as a scalable, low-cost substitute for human respondents, but their outputs are
often biased and yield invalid estimates. We study the interplay between
synthesis methods that use LLMs to generate survey responses and rectification
methods that debias population estimates, and explore how human responses are
best allocated between them. Using two panel surveys with questions on
nutrition, politics, and economics, we find that synthesis alone introduces
substantial bias (24-86%), whereas combining it with rectification reduces bias
below 5% and increases effective sample size by up to 14%. Overall, we
challenge the common practice of using all human responses for fine-tuning,
showing that under a fixed budget, allocating most to rectification results in
far more effective estimation.

</details>


### [136] [Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content](https://arxiv.org/abs/2510.11434)
*Dana Sotto Porat,Ella Rabinovich*

Main category: cs.CL

TL;DR: 本文介绍了一种新的方法来评估生成式大语言模型的人格，发现它们表现出更高的宜人性和更低的神经质，性别语言模式与人类作家相似但变化较少。


<details>
  <summary>Details</summary>
Motivation: 随着生成式大语言模型在日常生活中变得越来越重要，研究它们是否表现出类似人格和人口统计特征变得越来越重要。

Method: 本文引入了一种新的数据驱动方法来评估LLM的人格，而不是依赖于自我报告问卷，而是使用自动人格和性别分类器对从Reddit收集的开放式问题的回答进行分析。

Result: 我们发现LLM系统性地表现出更高的宜人性和更低的神经质，反映了合作和稳定的对话倾向。模型文本中的性别语言模式大致类似于人类作家，但变化较少，与之前关于自动化代理的研究结果一致。

Conclusion: 本文贡献了一个新的数据集以及大规模的比较分析，为生成式AI的人格和人口统计模式提供了新的见解。

Abstract: Generative large language models (LLMs) have become central to everyday life,
producing human-like text across diverse domains. A growing body of research
investigates whether these models also exhibit personality- and
demographic-like characteristics in their language. In this work, we introduce
a novel, data-driven methodology for assessing LLM personality without relying
on self-report questionnaires, applying instead automatic personality and
gender classifiers to model replies on open-ended questions collected from
Reddit. Comparing six widely used models to human-authored responses, we find
that LLMs systematically express higher Agreeableness and lower Neuroticism,
reflecting cooperative and stable conversational tendencies. Gendered language
patterns in model text broadly resemble those of human writers, though with
reduced variation, echoing prior findings on automated agents. We contribute a
new dataset of human and model responses, along with large-scale comparative
analyses, shedding new light on the topic of personality and demographic
patterns of generative AI.

</details>


### [137] [GenCNER: A Generative Framework for Continual Named Entity Recognition](https://arxiv.org/abs/2510.11444)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 本文提出了一种名为GenCNER的生成框架，用于解决持续命名实体识别中的灾难性遗忘和非实体类型的语义变化问题。通过将CNER任务转化为持续实体三元组序列生成问题，并利用预训练seq2seq模型进行解决，同时结合基于类型特定置信度的伪标记策略和知识蒸馏技术，有效保留已学习的知识并减轻标签噪声的影响。实验结果表明，该框架在多个CNER设置中表现优异，与非CL结果的差距最小。


<details>
  <summary>Details</summary>
Motivation: 传统命名实体识别（NER）旨在将文本提及识别为预定义的实体类型。由于在各种现实场景中实体类别不断增长，因此引入了持续命名实体识别（CNER）。然而，现有的NER的持续学习（CL）方法面临灾难性遗忘和非实体类型的语义变化的挑战。

Method: 我们提出了GenCNER，这是一个简单的但有效的生成框架，用于缓解上述缺点。具体来说，我们将CNER任务转化为持续实体三元组序列生成问题，并利用一个强大的预训练seq2seq模型来解决它。此外，我们设计了一个基于类型特定置信度的伪标记策略以及知识蒸馏（KD）以保留已学习的知识并减轻三元组级别的标签噪声的影响。

Result: 在两个基准数据集上的实验结果表明，我们的框架在多个CNER设置中优于之前最先进的方法，并且与非CL结果的差距最小。

Conclusion: 实验结果表明，我们的框架在多个CNER设置中优于之前最先进的方法，并且与非CL结果的差距最小。

Abstract: Traditional named entity recognition (NER) aims to identify text mentions
into pre-defined entity types. Continual Named Entity Recognition (CNER) is
introduced since entity categories are continuously increasing in various
real-world scenarios. However, existing continual learning (CL) methods for NER
face challenges of catastrophic forgetting and semantic shift of non-entity
type. In this paper, we propose GenCNER, a simple but effective Generative
framework for CNER to mitigate the above drawbacks. Specifically, we skillfully
convert the CNER task into sustained entity triplet sequence generation problem
and utilize a powerful pre-trained seq2seq model to solve it. Additionally, we
design a type-specific confidence-based pseudo labeling strategy along with
knowledge distillation (KD) to preserve learned knowledge and alleviate the
impact of label noise at the triplet level. Experimental results on two
benchmark datasets show that our framework outperforms previous
state-of-the-art methods in multiple CNER settings, and achieves the smallest
gap compared with non-CL results.

</details>


### [138] [Investigating Large Language Models' Linguistic Abilities for Text Preprocessing](https://arxiv.org/abs/2510.11482)
*Marco Braga,Gian Carlo Milanese,Gabriella Pasi*

Main category: cs.CL

TL;DR: This paper explores the use of Large Language Models for text preprocessing tasks and demonstrates that they can achieve high accuracy and improve the performance of downstream NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional text preprocessing techniques often ignore contextual information, which can limit their effectiveness. LLMs, on the other hand, have the potential to consider context without requiring extensive language-specific annotated resources.

Method: The paper evaluates the use of Large Language Models (LLMs) for text preprocessing tasks, including stopword removal, lemmatization, and stemming. It compares the performance of LLM-based preprocessing with traditional methods across multiple text classification tasks in six European languages.

Result: LLMs achieved high accuracy in replicating traditional preprocessing methods, with stopword removal reaching 97%, lemmatization 82%, and stemming 74%. Additionally, ML algorithms trained on texts preprocessed by LLMs showed an improvement of up to 6% in the $F_1$ measure compared to traditional techniques.

Conclusion: LLMs can effectively perform text preprocessing tasks such as stopword removal, lemmatization, and stemming, achieving high accuracy and improving the performance of downstream NLP tasks.

Abstract: Text preprocessing is a fundamental component of Natural Language Processing,
involving techniques such as stopword removal, stemming, and lemmatization to
prepare text as input for further processing and analysis. Despite the
context-dependent nature of the above techniques, traditional methods usually
ignore contextual information. In this paper, we investigate the idea of using
Large Language Models (LLMs) to perform various preprocessing tasks, due to
their ability to take context into account without requiring extensive
language-specific annotated resources. Through a comprehensive evaluation on
web-sourced data, we compare LLM-based preprocessing (specifically stopword
removal, lemmatization and stemming) to traditional algorithms across multiple
text classification tasks in six European languages. Our analysis indicates
that LLMs are capable of replicating traditional stopword removal,
lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,
respectively. Additionally, we show that ML algorithms trained on texts
preprocessed by LLMs achieve an improvement of up to 6% with respect to the
$F_1$ measure compared to traditional techniques. Our code, prompts, and
results are publicly available at
https://github.com/GianCarloMilanese/llm_pipeline_wi-iat.

</details>


### [139] [Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models](https://arxiv.org/abs/2510.11529)
*Yusheng Song,Lirong Qiu,Xi Zhang,Zhihao Tang*

Main category: cs.CL

TL;DR: 本文提出了一种统一框架来解决LLM中复杂幻觉检测的“检测困境”，通过多路径推理机制和段感知的时间化交叉注意力模块，有效提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM中复杂幻觉检测的“检测困境”，即内部状态探测方法在识别事实不一致方面表现良好但在逻辑谬误方面失败，而链式思维验证方法则相反。

Method: 我们引入了一种多路径推理机制以获得更可比较的细粒度信号，并引入了一个段感知的时间化交叉注意力模块来自适应融合这些现在对齐的表示，从而精确定位细微的不和谐之处。

Result: 我们的框架在三个不同的基准和两个领先的LLM上进行了广泛的实验，结果表明它始终且显著优于强大的基线。

Conclusion: 我们的框架在三个不同的基准和两个领先的LLM上进行了广泛的实验，结果表明它始终且显著优于强大的基线。

Abstract: The detection of sophisticated hallucinations in Large Language Models (LLMs)
is hampered by a ``Detection Dilemma'': methods probing internal states
(Internal State Probing) excel at identifying factual inconsistencies but fail
on logical fallacies, while those verifying externalized reasoning
(Chain-of-Thought Verification) show the opposite behavior. This schism creates
a task-dependent blind spot: Chain-of-Thought Verification fails on
fact-intensive tasks like open-domain QA where reasoning is ungrounded, while
Internal State Probing is ineffective on logic-intensive tasks like
mathematical reasoning where models are confidently wrong. We resolve this with
a unified framework that bridges this critical gap. However, unification is
hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse
symbolic reasoning chains lack signals directly comparable to fine-grained
internal states, and the Representational Alignment Barrier, a deep-seated
mismatch between their underlying semantic spaces. To overcome these, we
introduce a multi-path reasoning mechanism to obtain more comparable,
fine-grained signals, and a segment-aware temporalized cross-attention module
to adaptively fuse these now-aligned representations, pinpointing subtle
dissonances. Extensive experiments on three diverse benchmarks and two leading
LLMs demonstrate that our framework consistently and significantly outperforms
strong baselines. Our code is available: https://github.com/peach918/HalluDet.

</details>


### [140] [An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification](https://arxiv.org/abs/2510.11537)
*Ba-Quang Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种结合预训练语义特征与图结构信息的新型神经架构TextGraphFuseGAT，用于提升token级别的分类性能。在多个越南基准数据集上的实验结果表明，该方法表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的方法可能无法充分捕捉token之间的复杂关系，因此需要一种能够结合预训练语义特征和图结构信息的新模型来提升token分类性能。

Method: 提出了一种名为TextGraphFuseGAT的新型神经架构，将预训练的transformer编码器（PhoBERT）与图注意力网络（GAT）相结合，用于token级别的分类任务。构建了一个完全连接的图，以捕捉超越序列上下文的token间依赖关系，并在图增强的嵌入上应用了Transformer风格的自注意力层。最终的token表示通过分类头进行序列标注。

Result: 所提出的方法在三个越南基准数据集上进行了评估，包括PhoNER-COVID19、PhoDisfluency和VietMed-NER。实验结果表明，该方法优于强基线模型，如仅使用transformer和混合神经模型（如BiLSTM + CNN + CRF）。

Conclusion: 实验结果表明，所提出的方法在多个领域中通过结合预训练语义特征与基于图的关系建模，显著提升了token分类的效果。

Abstract: We propose a novel neural architecture named TextGraphFuseGAT, which
integrates a pretrained transformer encoder (PhoBERT) with Graph Attention
Networks for token-level classification tasks. The proposed model constructs a
fully connected graph over the token embeddings produced by PhoBERT, enabling
the GAT layer to capture rich inter-token dependencies beyond those modeled by
sequential context alone. To further enhance contextualization, a
Transformer-style self-attention layer is applied on top of the graph-enhanced
embeddings. The final token representations are passed through a classification
head to perform sequence labeling. We evaluate our approach on three Vietnamese
benchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19
domain, PhoDisfluency for speech disfluency detection, and VietMed-NER for
medical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER
dataset, featuring 18 entity types collected from real-world medical speech
transcripts and annotated with the BIO tagging scheme. Its specialized
vocabulary and domain-specific expressions make it a challenging benchmark for
token-level classification models. Experimental results show that our method
consistently outperforms strong baselines, including transformer-only and
hybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness
of combining pretrained semantic features with graph-based relational modeling
for improved token classification across multiple domains.

</details>


### [141] [Information-Preserving Reformulation of Reasoning Traces for Antidistillation](https://arxiv.org/abs/2510.11545)
*Jiayu Ding,Lei Cui,Li Dong,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: The paper proposes PART, an information-preserving antidistillation reformulation of reasoning traces that disrupts distillation while preserving intermediate information for users.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between revealing reasoning traces for user understanding and preventing unauthorized distillation by proprietary model providers.

Method: PART is an information-preserving antidistillation reformulation of reasoning traces, which involves removing self-talk behaviors and reordering sub-conclusions using a small auxiliary model.

Result: PART consistently disrupts distillation across student models of different sizes and types on various reasoning benchmarks. For instance, when training on reformulated traces, even the performance of a large 32B student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a 13.5% degradation.

Conclusion: PART can effectively disrupt distillation across student models of different sizes and types on various reasoning benchmarks.

Abstract: Recent advances in Large Language Models (LLMs) show that extending the
length of reasoning chains significantly improves performance on complex tasks.
While revealing these reasoning traces helps users better follow, verify, and
learn from the model's problem-solving process, it also makes them highly
vulnerable to unauthorized distillation. To mitigate this risk, proprietary
model providers often adopt aggressive protection strategies, such as replacing
detailed reasoning with brief summaries, which deprive users of valuable
intermediate information. To address this trade-off, we propose PART, an
information-preserving antidistillation reformulation of reasoning traces.
Motivated by the difference between how humans understand reasoning traces and
how LLMs exploit them for supervised fine-tuning, we design a simple but
effective two-step reformulation: removing self-talk behaviors and reordering
sub-conclusions. A small auxiliary model is trained to perform this
reformulation, incurring minimal computational overhead. Extensive experiments
demonstrate that PART consistently disrupts distillation across student models
of different sizes and types on various reasoning benchmarks. For instance,
when training on reformulated traces, even the performance of a large 32B
student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a
13.5% degradation.

</details>


### [142] [Invisible Languages of the LLM Universe](https://arxiv.org/abs/2510.11557)
*Saurabh Khanna,Xinxu Li*

Main category: cs.CL

TL;DR: 本文提出了一种关键框架，将语言活力和数字性与后殖民理论和认识论不公正联系起来，以解释人工智能系统中的语言不平等为何是结构性的。通过分析所有记录的人类语言的数据，我们确定了四个类别，并发现英语在人工智能中的主导地位是权力结构的结果，而非技术必然。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练时使用了大量的多语言语料库，但这种丰富性掩盖了一个深刻的危机：世界上的7613种语言中，约有2000种语言拥有数百万使用者，在数字生态系统中几乎不可见。我们需要一个关键框架来连接语言活力（现实世界的人口统计力量）和数字性（在线存在）与后殖民理论和认识论不公正，以解释为什么人工智能系统中的语言不平等不是偶然的，而是结构性的。

Method: 我们分析了所有记录的人类语言的数据，确定了四个类别：强区（33%，高活力和数字性），数字回声（6%，尽管活力下降，但数字性高），消逝的声音（36%，两个维度都低），以及最关键的是隐形巨人（27%，高活力但几乎零数字性）——这些语言有数百万使用者却在大型语言模型中缺席。

Result: 我们发现这些模式反映了从殖民时代的语言等级制度到当代人工智能开发的连续性，构成了我们所谓的数字认识论不公正。

Conclusion: 我们得出结论，英语在人工智能中的主导地位不是技术上的必然，而是系统性排除边缘化语言知识的权力结构的产物。我们需要去殖民化语言技术和民主化人工智能的好处。

Abstract: Large Language Models are trained on massive multilingual corpora, yet this
abundance masks a profound crisis: of the world's 7,613 living languages,
approximately 2,000 languages with millions of speakers remain effectively
invisible in digital ecosystems. We propose a critical framework connecting
empirical measurements of language vitality (real world demographic strength)
and digitality (online presence) with postcolonial theory and epistemic
injustice to explain why linguistic inequality in AI systems is not incidental
but structural. Analyzing data across all documented human languages, we
identify four categories: Strongholds (33%, high vitality and digitality),
Digital Echoes (6%, high digitality despite declining vitality), Fading Voices
(36%, low on both dimensions), and critically, Invisible Giants (27%, high
vitality but near-zero digitality) - languages spoken by millions yet absent
from the LLM universe. We demonstrate that these patterns reflect continuities
from colonial-era linguistic hierarchies to contemporary AI development,
constituting what we term digital epistemic injustice. Our analysis reveals
that English dominance in AI is not a technical necessity but an artifact of
power structures that systematically exclude marginalized linguistic knowledge.
We conclude with implications for decolonizing language technology and
democratizing access to AI benefits.

</details>


### [143] [Culturally-Aware Conversations: A Framework & Benchmark for LLMs](https://arxiv.org/abs/2510.11563)
*Shreya Havaldar,Sunny Rai,Young-Min Cho,Lyle Ungar*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架和基准，用于评估LLM在多文化对话环境中的表现，并发现现有模型在跨文化适应方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准与LLM在与来自不同文化背景的用户互动时面临的实际挑战不一致，因此需要一个新的框架和基准来更准确地评估模型的文化适应能力。

Method: 本文提出了一种基于社会文化理论的框架和基准，用于评估LLM在真实、多文化对话环境中的表现。

Result: 评估结果显示，当前的顶级LLM在跨文化对话环境中难以有效适应不同的文化背景。

Conclusion: 现有的顶级LLM在跨文化适应方面表现不佳，这表明需要改进模型以更好地处理多元文化对话场景。

Abstract: Existing benchmarks that measure cultural adaptation in LLMs are misaligned
with the actual challenges these models face when interacting with users from
diverse cultural backgrounds. In this work, we introduce the first framework
and benchmark designed to evaluate LLMs in realistic, multicultural
conversational settings. Grounded in sociocultural theory, our framework
formalizes how linguistic style - a key element of cultural communication - is
shaped by situational, relational, and cultural context. We construct a
benchmark dataset based on this framework, annotated by culturally diverse
raters, and propose a new set of desiderata for cross-cultural evaluation in
NLP: conversational framing, stylistic sensitivity, and subjective correctness.
We evaluate today's top LLMs on our benchmark and show that these models
struggle with cultural adaptation in a conversational setting.

</details>


### [144] [LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings](https://arxiv.org/abs/2510.11584)
*Ting Li,Yang Yang,Yipeng Yu,Liang Yao,Guoqing Chao,Ruifeng Xu*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的对抗性攻击框架LLMAtKGE，能够选择攻击目标并生成人类可读的解释。通过结构化提示方案和过滤器，提高了攻击性能和解释能力。实验结果表明，该方法优于现有黑盒基线，并与白盒方法相当。


<details>
  <summary>Details</summary>
Motivation: 对抗性攻击旨在通过删除或插入三元组来破坏模型的链接预测能力。最近的一种黑盒方法尝试结合文本和结构信息以提高攻击性能，但无法生成人类可读的解释，并表现出较差的泛化能力。近年来，大型语言模型（LLMs）在文本理解、生成和推理方面表现出强大的能力。本文旨在解决这些问题。

Method: 我们提出了LLMAtKGE，这是一个基于大语言模型（LLM）的框架，用于选择攻击目标并生成人类可读的解释。为了在有限的输入约束下为LLM提供足够的事实背景，我们设计了一个结构化提示方案，将攻击明确表述为多项选择问题，同时结合知识图谱的事实证据。为了解决上下文窗口限制和犹豫问题，我们引入了基于语义和中心性的过滤器，以压缩候选集并保留攻击相关信息的高召回率。此外，为了高效地将语义和结构信息整合到过滤器中，我们预计算高阶邻接关系并微调LLM以进行三元组分类任务，以提高过滤性能。

Result: 实验在两个广泛使用的知识图谱数据集上进行，结果表明我们的攻击方法优于最强的黑盒基线，并通过推理提供解释，与白盒方法相比表现出竞争力。全面的消融和案例研究进一步验证了其生成解释的能力。

Conclusion: 实验结果表明，我们的攻击方法优于最强的黑盒基线，并通过推理提供解释，与白盒方法相比表现出竞争力。全面的消融和案例研究进一步验证了其生成解释的能力。

Abstract: Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the
model's ability of link prediction by removing or inserting triples. A recent
black-box method has attempted to incorporate textual and structural
information to enhance attack performance. However, it is unable to generate
human-readable explanations, and exhibits poor generalizability. In the past
few years, large language models (LLMs) have demonstrated powerful capabilities
in text comprehension, generation, and reasoning. In this paper, we propose
LLMAtKGE, a novel LLM-based framework that selects attack targets and generates
human-readable explanations. To provide the LLM with sufficient factual context
under limited input constraints, we design a structured prompting scheme that
explicitly formulates the attack as multiple-choice questions while
incorporating KG factual evidence. To address the context-window limitation and
hesitation issues, we introduce semantics-based and centrality-based filters,
which compress the candidate set while preserving high recall of
attack-relevant information. Furthermore, to efficiently integrate both
semantic and structural information into the filter, we precompute high-order
adjacency and fine-tune the LLM with a triple classification task to enhance
filtering performance. Experiments on two widely used knowledge graph datasets
demonstrate that our attack outperforms the strongest black-box baselines and
provides explanations via reasoning, and showing competitive performance
compared with white-box methods. Comprehensive ablation and case studies
further validate its capability to generate explanations.

</details>


### [145] [Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models](https://arxiv.org/abs/2510.11586)
*Georg Ahnert,Anna-Carolina Haensch,Barbara Plank,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本研究探讨了不同调查响应生成方法对模拟调查响应的影响，发现受限生成方法效果最好，而推理输出不一定提高对齐度。


<details>
  <summary>Details</summary>
Motivation: 许多基于计算机的模拟人类调查响应的研究集中在生成封闭式调查响应上，而大型语言模型（LLMs）通常被训练为生成开放式文本。之前的研究使用了多种方法来生成封闭式调查响应，但尚未确定标准做法。

Method: 我们系统地研究了各种调查响应生成方法对预测调查响应的影响，并进行了3200万次模拟调查响应的实验。

Result: 我们发现调查响应生成方法在个体层面和子群体层面的对齐度存在显著差异。受限生成方法总体表现最佳，而推理输出并不总是提高对齐度。

Conclusion: 我们的工作强调了调查响应生成方法对模拟调查响应的重大影响，并提出了在应用调查响应生成方法时的实用建议。

Abstract: Many in-silico simulations of human survey responses with large language
models (LLMs) focus on generating closed-ended survey responses, whereas LLMs
are typically trained to generate open-ended text instead. Previous research
has used a diverse range of methods for generating closed-ended survey
responses with LLMs, and a standard practice remains to be identified. In this
paper, we systematically investigate the impact that various Survey Response
Generation Methods have on predicted survey responses. We present the results
of 32 mio. simulated survey responses across 8 Survey Response Generation
Methods, 4 political attitude surveys, and 10 open-weight language models. We
find significant differences between the Survey Response Generation Methods in
both individual-level and subpopulation-level alignment. Our results show that
Restricted Generation Methods perform best overall, and that reasoning output
does not consistently improve alignment. Our work underlines the significant
impact that Survey Response Generation Methods have on simulated survey
responses, and we develop practical recommendations on the application of
Survey Response Generation Methods.

</details>


### [146] [MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models](https://arxiv.org/abs/2510.11598)
*Bo Cheng,Xu Wang,Jinda Liu,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: MeTA-LoRA是一种两阶段优化框架，通过任务特定的LoRA适配器和共享的LoRA适配器的组合，在多任务学习和多语言学习场景中显著提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: LoRA在单任务设置中非常有效，但在复杂的多任务学习场景中难以高效利用任务间知识，通常需要大量任务特定数据才能获得最佳性能。

Method: MeTA-LoRA是一种两阶段优化框架，首先使用每个数据集中的少量样本学习任务特定的LoRA适配器，然后通过聚合多个任务的梯度更新共享的LoRA适配器以促进知识迁移。

Result: MeTA-LoRA在多任务学习和多语言学习场景中表现出色，使用更少的任务特定数据即可达到或超越传统全数据LoRA微调方法的性能。

Conclusion: MeTA-LoRA在多任务学习和多语言学习场景中表现出色，使用更少的任务特定数据即可达到或超越传统全数据LoRA微调方法的性能。

Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most widely used
parameter-efficient fine-tuning (PEFT) methods for adapting large language
models (LLMs) to downstream tasks. While highly effective in single-task
settings, it struggles to efficiently leverage inter-task knowledge in complex
multi-task learning scenarios, often requiring substantial task-specific data
to achieve optimal performance. To address this limitation, we introduce
MeTA-LoRA, a two-stage optimization framework that significantly improves data
efficiency in multi-task adaptation. In the first stage, task-specific LoRA
adapters are learned using only a few samples from each involved dataset,
enabling rapid adaptation without large-scale supervision. In the second stage,
the shared LoRA adapter is updated by aggregating gradients from multiple tasks
to promote knowledge transfer across tasks, further reducing data usage by
leveraging common patterns. In both multi-task learning and multilingual
learning scenarios, our method matches or surpasses the performance of
traditional full-data LoRA fine-tuning approaches, while using significantly
less task-specific data.

</details>


### [147] [SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping](https://arxiv.org/abs/2510.11599)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: SemCSE-Multi 是一种新的无监督框架，用于生成科学摘要的多方面嵌入，能够实现更精细和可控的相似性评估以及更好的用户驱动可视化。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法捕捉科学摘要中不同、可单独指定的方面，导致相似性评估和可视化不够精细和可控。因此，需要一种新的方法来生成多方面嵌入，以提高科学领域的可解释性和用户交互性。

Method: SemCSE-Multi 采用了一种无监督的方法，生成特定方面的总结句子，并训练嵌入模型将语义相关的总结映射到嵌入空间中的附近位置。然后，将这些特定方面的嵌入能力提炼到一个统一的嵌入模型中，该模型能够在一次高效的前向传递中直接预测多个方面嵌入。此外，还引入了一个嵌入解码管道，用于将嵌入解码为与之相关的自然语言描述。

Result: SemCSE-Multi 能够生成多方面嵌入，使得科学摘要的相似性评估更加精细和可控。同时，嵌入解码管道能够将嵌入解码为自然语言描述，即使在低维可视化中未被占用的区域也能保持有效性，从而显著提高了用户中心场景中的可解释性。

Conclusion: SemCSE-Multi 是一种新颖的无监督框架，能够生成科学摘要的多方面嵌入，并在入侵生物学和医学领域进行了评估。该方法通过生成特定方面的总结句子并训练嵌入模型来实现这一点，从而实现了细粒度和可控的相似性评估以及自适应的用户驱动可视化。此外，还引入了一个嵌入解码管道，可以将嵌入解码为自然语言描述，提高了用户中心场景中的可解释性。

Abstract: We propose SemCSE-Multi, a novel unsupervised framework for generating
multifaceted embeddings of scientific abstracts, evaluated in the domains of
invasion biology and medicine. These embeddings capture distinct, individually
specifiable aspects in isolation, thus enabling fine-grained and controllable
similarity assessments as well as adaptive, user-driven visualizations of
scientific domains. Our approach relies on an unsupervised procedure that
produces aspect-specific summarizing sentences and trains embedding models to
map semantically related summaries to nearby positions in the embedding space.
We then distill these aspect-specific embedding capabilities into a unified
embedding model that directly predicts multiple aspect embeddings from a
scientific abstract in a single, efficient forward pass. In addition, we
introduce an embedding decoding pipeline that decodes embeddings back into
natural language descriptions of their associated aspects. Notably, we show
that this decoding remains effective even for unoccupied regions in
low-dimensional visualizations, thus offering vastly improved interpretability
in user-centric settings.

</details>


### [148] [Deconstructing Attention: Investigating Design Principles for Effective Language Modeling](https://arxiv.org/abs/2510.11602)
*Huiyin Xue,Nafise Sadat Moosavi,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文系统地分解了注意力机制，发现混合标记的机制是必不可少的，而数学形式和序列依赖性可以放松，甚至在部分层中保留时也能实现良好性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer语言模型的成功归功于其点积注意力机制，但这些原则的必要性仍 largely 未被测试。

Method: 通过设计受控变体来系统地分解注意力，这些变体选择性地放松这些原则，并在所有层中统一应用，以及在混合架构中仅部分层保留标准注意力。

Result: 混合标记的机制是必不可少的，因为它们的缺失会使模型退化为接近随机的行为，而精确的数学形式和序列依赖性可以大大放松，尤其是在仅在部分层中保留时。令人惊讶的是，即使孤立失败的变体也可以在与标准注意力交错时实现稳健性能，这表明存在协同效应。

Conclusion: 这些发现加深了我们对注意力机制有效性的理解，并为在不牺牲性能的情况下简化语言模型开辟了新途径。

Abstract: The success of Transformer language models is widely credited to their
dot-product attention mechanism, which interweaves a set of key design
principles: mixing information across positions (enabling multi-token
interactions), sequence-dependent activations (where attention weights adapt to
each input), a specific mathematical form (dot-product similarities plus
softmax weighting), and coupling of queries and keys to evolving hidden states
(grounding attention in the current layer). However, the necessity of each of
these principles remains largely untested. In this work, we systematically
deconstruct attention by designing controlled variants that selectively relax
these principles, applied both uniformly across all layers and in hybrid
architectures where only some layers retain standard attention. Our empirical
analysis reveals that mechanisms for mixing tokens are indispensable, as their
absence collapses models to near-random behavior, while the exact mathematical
form and sequence dependency can be substantially relaxed, especially when
preserved in just a subset of layers. Surprisingly, even variants that fail in
isolation can achieve robust performance when interleaved with standard
attention, highlighting a cooperative effect. These findings deepen our
understanding of what truly underpins attention's effectiveness and open new
avenues for simplifying language models without sacrificing performance.

</details>


### [149] [LLM-Oriented Token-Adaptive Knowledge Distillation](https://arxiv.org/abs/2510.11615)
*Xurong Xie,Zhucun Xue,Jiafu Wu,Jian Li,Yabiao Wang,Xiaobin Hu,Yong Liu,Jiangning Zhang*

Main category: cs.CL

TL;DR: AdaKD is a novel framework for knowledge distillation that adapts the process to the real-time learning state of each token, improving performance across various models and benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing logit-based methods that use static strategies and apply a single, fixed temperature, resulting in suboptimal knowledge transfer.

Method: AdaKD consists of two synergistic modules driven by a unified token difficulty metric: Loss-Driven Adaptive Token Focusing (LATF) and Inverse Difficulty Temperature Scaling (IDTS).

Result: AdaKD adapts the distillation process to the real-time learning state of each token, improving knowledge transfer and enhancing generalization.

Conclusion: AdaKD can consistently improve the performance of various distillation methods on multiple model architectures and benchmarks.

Abstract: Knowledge distillation (KD) is a key technique for compressing large-scale
language models (LLMs), yet prevailing logit-based methods typically employ
static strategies that are misaligned with the dynamic learning process of
student models. These methods typically treat all tokens indiscriminately and
apply a single, fixed temperature, resulting in suboptimal knowledge transfer.
To address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge
Distillation (AdaKD), a novel framework that adapts the distillation process to
the real-time learning state of each token. AdaKD consists of two synergistic
modules driven by a unified token difficulty metric. First, our Loss-Driven
Adaptive Token Focusing (LATF) module dynamically adjusts the distillation
focus by monitoring the student's learning stability, concentrating
computational resources on the most valuable tokens at each training phase.
Second, we introduce Inverse Difficulty Temperature Scaling (IDTS), a
counterintuitive yet effective token-level temperature strategy. It employs low
temperatures for difficult tokens for targeted error correction, and high
temperatures for easy tokens to encourage students to learn from the teacher's
complete and smooth output distribution, thereby enhancing generalization. As a
plug-and-play framework, AdaKD can consistently improve the performance of
various distillation methods on multiple model architectures and benchmarks.

</details>


### [150] [StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models](https://arxiv.org/abs/2510.11618)
*Zehao Chen,Rong Pan,Haoran Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于多智能体模拟的混合自下而上的长篇故事生成方法，能够在生成超过10000字的故事的同时保持连贯性和一致性，并在多个指标上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的方法通常采用自上而下的方式，强制施加严格的结构，而本文旨在解决当前故事生成模型面临的挑战，如保持故事的连贯性和一致性，以及生成更自然、更吸引人的叙事。

Method: 本文提出了一种基于多智能体模拟的混合自下而上的长篇故事生成方法。在该方法中，智能体在动态沙盒环境中相互作用，它们的行为和与其他智能体及环境的互动生成了涌现事件。这些事件构成了故事的基础，使得角色发展和情节推进更加自然。

Result: 本文的方法能够在生成超过10000字的故事的同时保持连贯性和一致性，并在多个指标上达到了最先进的性能。

Conclusion: 本文提出了一种新的长篇故事生成方法，称为混合自下而上的长篇故事生成，通过多智能体模拟实现。该方法能够生成超过10000字的故事，同时保持连贯性和一致性，并在多个指标上达到了最先进的性能。这种方法为创建动态、沉浸式的长篇故事提供了一个可扩展且创新的解决方案。

Abstract: Human writers often begin their stories with an overarching mental scene,
where they envision the interactions between characters and their environment.
Inspired by this creative process, we propose a novel approach to long-form
story generation, termed hybrid bottom-up long-form story generation, using
multi-agent simulations. In our method, agents interact within a dynamic
sandbox environment, where their behaviors and interactions with one another
and the environment generate emergent events. These events form the foundation
for the story, enabling organic character development and plot progression.
Unlike traditional top-down approaches that impose rigid structures, our hybrid
bottom-up approach allows for the natural unfolding of events, fostering more
spontaneous and engaging storytelling. The system is capable of generating
stories exceeding 10,000 words while maintaining coherence and consistency,
addressing some of the key challenges faced by current story generation models.
We achieve state-of-the-art performance across several metrics. This approach
offers a scalable and innovative solution for creating dynamic, immersive
long-form stories that evolve organically from agent-driven interactions.

</details>


### [151] [Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation](https://arxiv.org/abs/2510.11620)
*Siheng Xiong,Ali Payani,Faramarz Fekri*

Main category: cs.CL

TL;DR: 本文提出了一种新的推理框架Multi-Path Plan Aggregation (MPPA)，通过计划探索和聚合来改善语言模型的推理能力，并引入了在线Step-DPO以提高训练效率和准确性。实验结果表明，该方法在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单次前向传递中生成整个推理链，导致推理轨迹偏离，尤其是在小型语言模型上更为严重。因此，需要一种能够有效处理长CoT并减少错误的方法。

Method: 本文分析了长链式思维（CoT），发现推理错误主要来源于错误的规划。基于此，提出了Multi-Path Plan Aggregation (MPPA)框架，通过生成多个候选计划并将其聚合到一个精炼的规划步骤中。同时，引入了在线Step-DPO，一种利用Twisted Sequential Monte Carlo (TSMC)提供可扩展步骤监督的过程级偏好优化方案。

Result: 实验结果表明，即使只使用10%的SFT数据和5%的偏好对，本文方法在多个基准测试中优于DeepSeek-R1蒸馏基线和结果奖励RL基线。

Conclusion: 本文提出了一种名为Multi-Path Plan Aggregation (MPPA)的框架，通过计划探索和聚合来增强单次传递推理。此外，还引入了在线Step-DPO，一种过程级偏好优化方案，以提高训练效率、稳定性和准确性。实验结果表明，该方法在多个基准测试中优于现有的深度学习基线。

Abstract: Inference-time scaling enhances the reasoning ability of a language model
(LM) by extending its chain-of-thought (CoT). However, existing approaches
typically generate the entire reasoning chain in a single forward pass, which
often leads to CoT derailment, i.e., the reasoning trajectory drifting off
course due to compounding errors. This problem is particularly severe for
smaller LMs with long CoTs due to their limited capacity. To address this, we
analyze raw long CoTs and uncover a reasoning hierarchy consisting of planning
and execution steps. Our analysis reveals that most reasoning errors stem from
incorrect planning. Motivated by this observation, we propose Multi-Path Plan
Aggregation (MPPA), a framework that augments single-pass reasoning with plan
exploration and aggregation. Following a variable interval schedule based on
the token position, MPPA generates multiple candidate plans and aggregates them
into a refined planning step. To maintain efficiency, we adopt a minimal design
in which the base LM serves as the primary policy, while a lightweight LoRA
module implements the plan aggregation policy. We further observe that
outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K
tokens). To overcome this, we introduce online Step-DPO, a process-level
preference optimization scheme that leverages Twisted Sequential Monte Carlo
(TSMC) to provide scalable stepwise supervision using small LMs. This yields
more efficient training, improved stability, and higher accuracy. Extensive
experiments on challenging math, science, and logical reasoning benchmarks
demonstrate that, with only 10% SFT data and 5% of preference pairs, our method
outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward
RL baseline across multiple base models and tasks.

</details>


### [152] [ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems](https://arxiv.org/abs/2510.11652)
*Xin Gui,King Zhu,JinCheng Ren,Qianben Chen,Zekun Moore Wang,Yizhi LI,Xinpeng Liu,Xiaowan Li,Wenli Ren,Linyu Miao,Tianrui Qin,Ziqi Shu,He Zhu,Xiangru Tang,Dingfeng Shi,Jiaheng Liu,Yuchen Eleanor Jiang,Minghao Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文介绍了Acadreason基准，用于评估大型语言模型和代理在学术知识获取和推理方面的能力，并通过实验展示了当前模型在高阶推理任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要集中在数学/代码竞赛或一般任务上，而现有的多领域学术基准缺乏足够的推理深度，因此需要一个严格的基准来评估高阶推理能力。

Method: 引入Acadreason基准，用于评估大型语言模型和代理获取和推理学术知识的能力。

Result: 对超过10种主流大型语言模型和代理进行了系统评估，结果显示大多数大型语言模型得分低于20分，即使是最先进的GPT-5也仅获得16分。虽然代理得分较高，但没有超过40分。

Conclusion: 当前大型语言模型和代理在超级智能学术研究任务中存在能力差距，并突显了Acadreason的挑战。

Abstract: In recent years, the research focus of large language models (LLMs) and
agents has shifted increasingly from demonstrating novel capabilities to
complex reasoning and tackling challenging tasks. However, existing evaluations
focus mainly on math/code contests or general tasks, while existing
multi-domain academic benchmarks lack sufficient reasoning depth, leaving the
field without a rigorous benchmark for high-level reasoning. To fill this gap,
we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs
and agents to acquire and reason over academic knowledge. It consists of 50
expert-annotated academic problems across five high-reasoning domains,
including computer science, economics, law, mathematics, and philosophy. All
questions are sourced from top-tier publications in recent years and undergo
rigorous annotation and quality control to ensure they are both challenging and
answerable. We conduct systematic evaluations of over 10 mainstream LLMs and
agents. The results show that most LLMs scored below 20 points, with even the
cutting-edge GPT-5 achieving only 16 points. While agents achieved higher
scores, none exceeded 40 points. This demonstrates the current capability gap
between LLMs and agents in super-intelligent academic research tasks and
highlights the challenges of Acadreason.

</details>


### [153] [Scaling Language-Centric Omnimodal Representation Learning](https://arxiv.org/abs/2510.11693)
*Chenghao Xiao,Hou Pong Chan,Hao Zhang,Weiwen Xu,Mahani Aljunied,Yu Rong*

Main category: cs.CL

TL;DR: 本文探讨了基于多模态大语言模型的多模态嵌入方法的优势来源，并提出了一个名为LCO-Emb的框架，实现了最先进的性能。此外，还发现生成能力与表示能力之间存在正相关关系，这为提升模型的表示质量提供了一个有效范式。


<details>
  <summary>Details</summary>
Motivation: 尽管基于MLLM的多模态嵌入方法在对比学习（CL）微调下表现良好，但其优势的内在原因尚未得到充分研究。本文旨在揭示MLLM在生成预训练过程中实现的隐含跨模态对齐机制，并探索其对表示能力的影响。

Method: 本文通过分析各向异性与核相似性结构，验证了MLLM表示中隐含的跨模态对齐现象，并提出了Language-Centric Omnimodal Embedding框架（LCO-Emb）。同时，还发现了Generation-Representation Scaling Law（GRSL），并对其进行了理论解释和实验验证。

Result: 实验结果表明，LCO-Emb在多个骨干网络和基准测试中均取得了最先进的性能。同时，GRSL的发现表明，MLLM的生成能力与表示能力之间存在正相关关系。

Conclusion: 本文提出了一种基于多模态大语言模型（MLLM）的框架LCO-Emb，并展示了其在多种基准测试中的优越性能。此外，还发现生成能力与表示能力之间存在正相关关系，这为提升模型的表示质量提供了一种有效范式。

Abstract: Recent multimodal embedding approaches leveraging multimodal large language
models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of MLLM-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language decoder learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within MLLM representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the MLLM's generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the MLLM's generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.

</details>


### [154] [When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents](https://arxiv.org/abs/2510.11695)
*Lingfei Qian,Xueqing Peng,Yan Wang,Vincent Jim Zhang,Huan He,Hanley Smith,Yi Han,Yueru He,Haohang Li,Yupeng Cao,Yangyang Yu,Alejandro Lopez-Lira,Peng Lu,Jian-Yun Nie,Guojun Xiong,Jimin Huang,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文介绍了Agent Market Arena (AMA)，这是一个用于评估基于大语言模型的交易代理的实时基准，通过整合真实数据和多样化的代理架构，揭示了不同代理行为模式的差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究测试的是模型而非代理，覆盖的时间和资产有限，并依赖未经验证的数据，因此需要一个更全面的评估框架来检验基于大语言模型的交易代理在真实市场中的表现。

Method: 引入了Agent Market Arena (AMA)，这是一个首个终身、实时的基准，用于在多个市场中评估基于大语言模型的交易代理。AMA整合了经过验证的交易数据、专家检查的新闻和多样的代理架构，以统一的交易框架进行公平和连续的比较。

Result: 在加密货币和股票市场上的实时实验表明，代理框架表现出明显不同的行为模式，从激进的风险承担到保守的决策，而模型骨干对结果变化的影响较小。

Conclusion: AMA为评估基于大语言模型的交易代理提供了坚实的基础，使得金融推理和交易智能的评估更加严谨、可重复且持续进化。

Abstract: Although Large Language Model (LLM)-based agents are increasingly used in
financial trading, it remains unclear whether they can reason and adapt in live
markets, as most studies test models instead of agents, cover limited periods
and assets, and rely on unverified data. To address these gaps, we introduce
Agent Market Arena (AMA), the first lifelong, real-time benchmark for
evaluating LLM-based trading agents across multiple markets. AMA integrates
verified trading data, expert-checked news, and diverse agent architectures
within a unified trading framework, enabling fair and continuous comparison
under real conditions. It implements four agents, including InvestorAgent as a
single-agent baseline, TradeAgent and HedgeFundAgent with different risk
styles, and DeepFundAgent with memory-based reasoning, and evaluates them
across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and
Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets
demonstrate that agent frameworks display markedly distinct behavioral
patterns, spanning from aggressive risk-taking to conservative decision-making,
whereas model backbones contribute less to outcome variation. AMA thus
establishes a foundation for rigorous, reproducible, and continuously evolving
evaluation of financial reasoning and trading intelligence in LLM-based agents.

</details>


### [155] [Demystifying Reinforcement Learning in Agentic Reasoning](https://arxiv.org/abs/2510.11701)
*Zhaochen Yu,Ling Yang,Jiaru Zou,Shuicheng Yan,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文对代理推理中的强化学习进行了系统研究，提出了提升代理推理和训练效率的简单实践，并在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管代理强化学习（agentic RL）展示了RL可以有效提高LLMs的代理推理能力，但关键设计原则和最佳实践仍不明确。

Method: 本文从数据、算法和推理模式三个关键角度对代理推理中的强化学习进行了全面系统的研究。

Result: 本文提出的简单实践在四个具有挑战性的基准测试中取得了显著效果，包括AIME2024/AIME2025、GPQA-Diamond和LiveCodeBench-v6。4B大小的模型甚至可以比32B大小的模型表现出更好的代理推理性能。

Conclusion: 本文通过简单实践显著提升了代理推理和训练效率，在小型模型上实现了强大的结果，并为未来的代理强化学习研究建立了实用基准。

Abstract: Recently, the emergence of agentic RL has showcased that RL could also
effectively improve the agentic reasoning ability of LLMs, yet the key design
principles and optimal practices remain unclear. In this work, we conduct a
comprehensive and systematic investigation to demystify reinforcement learning
in agentic reasoning from three key perspectives: data, algorithm, and
reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic
trajectories with real end-to-end tool-use trajectories yields a far stronger
SFT initialization; high-diversity, model-aware datasets sustain exploration
and markedly improve RL performance. (ii) Exploration-friendly techniques are
crucial for agentic RL, such as clip higher, overlong reward shaping, and
maintaining adequate policy entropy could improve the training efficiency.
(iii) A deliberative strategy with fewer tool calls outperforms frequent tool
calls or verbose self-reasoning, improving tool efficiency and final accuracy.
Together, these simple practices consistently enhance agentic reasoning and
training efficiency, achieving strong results on challenging benchmarks with
smaller models, and establishing a practical baseline for future agentic RL
research. Beyond these empirical insights, we further contribute a
high-quality, real end-to-end agentic SFT dataset along with a high-quality RL
dataset, and demonstrate the effectiveness of our insights in boosting the
agentic reasoning ability of LLMs across four challenging benchmarks, including
AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,
4B-sized models could also achieve superior agentic reasoning performance
compared to 32B-sized models. Code and models:
https://github.com/Gen-Verse/Open-AgentRL

</details>


### [156] [Are Large Reasoning Models Interruptible?](https://arxiv.org/abs/2510.11713)
*Tsung-Han Wu,Mihran Miroyan,David M. Chan,Trevor Darrell,Narges Norouzi,Joseph E. Gonzalez*

Main category: cs.CL

TL;DR: 本文研究了大型推理模型在动态场景下的鲁棒性问题，发现静态评估高估了模型的鲁棒性，并揭示了三种新的失败模式。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法假设模型响应是即时的，且请求的上下文在响应期间保持不变，但在现代推理任务中，如辅助编程，这种假设不再成立。

Method: 本文挑战了传统的“冻结世界”假设，通过两个现实的动态场景（中断和动态上下文）评估了大型推理模型的鲁棒性。

Result: 实验结果表明，即使最先进的大型推理模型在静态设置中表现良好，但在中断或暴露于变化的上下文时也可能不可预测地失败，性能下降高达60%。

Conclusion: 本文分析了大型推理模型在动态场景下的鲁棒性问题，发现静态评估高估了模型的鲁棒性，并揭示了三种新的失败模式。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning but are
traditionally evaluated in static, "frozen world" settings: model responses are
assumed to be instantaneous, and the context of a request is presumed to be
immutable over the duration of the response. While generally true for
short-term tasks, the "frozen world" assumption breaks down in modern reasoning
tasks such as assistive programming, where models may take hours to think
through problems and code may change dramatically from the time the model
starts thinking to the model's final output. In this work, we challenge the
frozen world assumption and evaluate LRM robustness under two realistic dynamic
scenarios: interruptions, which test the quality of the model's partial outputs
on a limited budget, and dynamic context, which tests model adaptation to
in-flight changes. Across mathematics and programming benchmarks that require
long-form reasoning, static evaluations consistently overestimate robustness:
even state-of-the-art LRMs, which achieve high accuracy in static settings, can
fail unpredictably when interrupted or exposed to changing context, with
performance dropping by up to 60% when updates are introduced late in the
reasoning process. Our analysis further reveals several novel failure modes,
including reasoning leakage, where models fold the reasoning into their final
answer when interrupted; panic, where under time pressure models abandon
reasoning entirely and return incorrect answers; and self-doubt, where
performance degrades while incorporating updated information.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [157] [VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents](https://arxiv.org/abs/2510.11098)
*Jiliang Hu,Wenfu Wang,Zuchao Li,Chenxing Li,Yiyang Zhao,Hanzhao Li,Liqiang Zhang,Meng Yu,Dong Yu*

Main category: cs.SD

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in large audio language models (LALMs) have greatly enhanced
multimodal conversational systems. However, existing benchmarks remain limited
-- they are mainly English-centric, rely on synthetic speech, and lack
comprehensive, discriminative evaluation across multiple dimensions. To address
these gaps, we present Voice Chat Bot Bench (VCB Bench) -- a high-quality
Chinese benchmark built entirely on real human speech. VCB Bench evaluates
LALMs from three complementary perspectives: instruction following (including
speech-level control beyond text commands), knowledge understanding (general
knowledge, reasoning, and daily dialogue), and robustness (stability under
perturbations in content, environment, and speaker traits). Experiments on
representative LALMs reveal notable performance gaps and highlight future
directions for improvement. VCB Bench provides a reproducible and fine-grained
evaluation framework, offering standardized methodology and practical insights
for advancing Chinese voice conversational models.

</details>


### [158] [Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap](https://arxiv.org/abs/2510.11330)
*KiHyun Nam,Jongmin Choi,Hyeongkeun Lee,Jungwoo Heo,Joon Son Chung*

Main category: cs.SD

TL;DR: 本文提出了Diffusion-Link，一种基于扩散的模态桥接模块，用于生成性地将音频嵌入映射到文本嵌入分布中。实验表明，Diffusion-Link在减少模态差距和提升自动音频字幕性能方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 对比音频-语言预训练产生了强大的联合表示，但持续的音频-文本模态差距限制了多模态编码器与大型语言模型（LLMs）的耦合效益。

Method: Diffusion-Link是一种基于扩散的模态桥接模块，它生成性地将音频嵌入映射到文本嵌入分布中。该模块在冻结的多模态编码器的输出嵌入上进行训练，并实现为一个轻量级网络，包含三个残差MLP块。

Result: （1）模态差距分析：在相似性和几何标准下，Diffusion-Link在先前的基于扩散的方法中减少模态差距最多，并显示出音频嵌入向文本分布的集体迁移。（2）下游AAC：将Diffusion-Link附加到相同的多模态LLM基线，在AudioCaps上实现了零样本和完全监督的字幕的最先进结果，无需外部知识，相对增益分别高达52.5%和7.5%。

Conclusion: 关闭模态差距对于多模态编码器和大语言模型（LLMs）之间的有效耦合至关重要，基于扩散的模态桥接提供了一种超越以知识检索为中心设计的有希望的方向。

Abstract: Contrastive audio-language pretraining yields powerful joint representations,
yet a persistent audio-text modality gap limits the benefits of coupling
multimodal encoders with large language models (LLMs). We present
Diffusion-Link, a diffusion-based modality-bridging module that generatively
maps audio embeddings into the text-embedding distribution. The module is
trained at the output embedding from the frozen multimodal encoder and
implemented as a lightweight network with three residual MLP blocks. To assess
the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on
Automatic Audio Captioning (AAC); to our knowledge, this is the first
application of diffusion-based modality bridging to AAC. We report two results.
(1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link
reduces the modality gap the most among prior diffusion-based methods and shows
a collective migration of audio embeddings toward the text distribution. (2)
Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline
achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised
captioning without external knowledge, with relative gains up to 52.5% and
7.5%, respectively. These findings show that closing the modality gap is
pivotal for effective coupling between multimodal encoders and LLMs, and
diffusion-based modality bridging offers a promising direction beyond
knowledge-retrieval-centric designs. Code will be released upon acceptance
https://github.com/DevKiHyun/Diffusion-Link

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [159] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: 本文是对LLM赋能的软件工程的首次全面分析，涵盖了解决方案和基准测试的分类，揭示了该领域的演变，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前领域缺乏对基准测试和解决方案之间相互关系的全面理解，阻碍了系统性的进展和评估。

Method: 本文分析了150多篇最近的论文，并将它们组织成一个全面的分类法，涵盖两个主要维度：(1) 解决方案，分为基于提示、微调和代理的范式，(2) 基准测试，涵盖代码生成、翻译、修复等任务。

Result: 本文揭示了该领域从简单的提示工程演变为包含规划和分解、推理和自我精炼、记忆机制和工具增强的复杂代理系统。我们提出了一个统一的流程，展示了从任务规范到最终交付物的完整工作流程，并连接了50多个基准测试与相应的解决方案策略。

Conclusion: 本文提供了对LLM赋能的软件工程的全面分析，为研究人员和从业者提供了一个基础资源，以理解和推进LLM赋能的软件工程系统。

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [160] [Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context](https://arxiv.org/abs/2510.09968)
*Stefan Pasch*

Main category: cs.SE

TL;DR: 研究分析了8000多条AI开发平台用户评论，发现MLOps实践在提高用户满意度方面具有显著正相关，尽管组织背景影响其普及程度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补关于MLOps实践是否以及如何支持用户开发和操作AI应用的实证证据的空白。

Method: 该研究分析了来自G2.com的8000多条AI开发平台用户评论，使用零样本分类法测量对九种已确立的MLOps实践的评论情感。

Result: 九种MLOps实践中有七种与用户满意度有显著正相关关系，表明有效的MLOps实施为AI开发带来了实际价值。然而，组织背景也起作用：小型公司的评审者较少讨论某些MLOps实践。

Conclusion: 研究发现，一旦实施，MLOps实践在各种组织环境中都被认为是普遍有益的。

Abstract: Organizational efforts to utilize and operationalize artificial intelligence
(AI) are often accompanied by substantial challenges, including scalability,
maintenance, and coordination across teams. In response, the concept of Machine
Learning Operations (MLOps) has emerged as a set of best practices that
integrate software engineering principles with the unique demands of managing
the ML lifecycle. Yet, empirical evidence on whether and how these practices
support users in developing and operationalizing AI applications remains
limited. To address this gap, this study analyzes over 8,000 user reviews of AI
development platforms from G2.com. Using zero-shot classification, we measure
review sentiment toward nine established MLOps practices, including continuous
integration and delivery (CI/CD), workflow orchestration, reproducibility,
versioning, collaboration, and monitoring. Seven of the nine practices show a
significant positive relationship with user satisfaction, suggesting that
effective MLOps implementation contributes tangible value to AI development.
However, organizational context also matters: reviewers from small firms
discuss certain MLOps practices less frequently, suggesting that organizational
context influences the prevalence and salience of MLOps, though firm size does
not moderate the MLOps-satisfaction link. This indicates that once applied,
MLOps practices are perceived as universally beneficial across organizational
settings.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [161] [ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test](https://arxiv.org/abs/2510.10281)
*Guan-Yan Yang,Tzu-Yu Cheng,Ya-Wen Teng,Farn Wanga,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 本文介绍了一种新的黑盒越狱框架ArtPerception，利用ASCII艺术绕过最先进的LLM的安全措施。该框架采用两阶段方法，能够高效地进行越狱攻击，并通过实验验证了其有效性。研究强调了LLM安全需要防御多模态解释空间的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐主要关注语义解释，使得LLM容易受到使用非标准数据表示的攻击。因此，需要一种新的方法来应对这些安全挑战。

Method: 本文介绍了ArtPerception，这是一个新颖的黑盒越狱框架，利用ASCII艺术来绕过最先进的LLM的安全措施。该框架采用系统化的两阶段方法：第一阶段进行一次性的模型特定预测试，以确定最佳的ASCII艺术识别参数；第二阶段利用这些见解发起高效的一次性恶意越狱攻击。我们还提出了一个改进的Levenshtein距离（MLD）度量标准，以更细致地评估LLM的识别能力。

Result: 通过在四个最先进的开源LLM上进行全面实验，我们展示了优越的越狱性能。此外，我们验证了框架在现实世界中的相关性，展示了其成功转移到领先的商业模型（如GPT-4o、Claude Sonnet 3.7和DeepSeek-V3）的能力，并对潜在防御（如LLaMA Guard和Azure的内容过滤器）进行了严格的有效性分析。

Conclusion: 我们的研究强调了真正的LLM安全需要防御多模态解释空间，即使是在纯文本输入中。同时，它突显了基于侦察的战略攻击的有效性。

Abstract: The integration of Large Language Models (LLMs) into computer applications
has introduced transformative capabilities but also significant security
challenges. Existing safety alignments, which primarily focus on semantic
interpretation, leave LLMs vulnerable to attacks that use non-standard data
representations. This paper introduces ArtPerception, a novel black-box
jailbreak framework that strategically leverages ASCII art to bypass the
security measures of state-of-the-art (SOTA) LLMs. Unlike prior methods that
rely on iterative, brute-force attacks, ArtPerception introduces a systematic,
two-phase methodology. Phase 1 conducts a one-time, model-specific pre-test to
empirically determine the optimal parameters for ASCII art recognition. Phase 2
leverages these insights to launch a highly efficient, one-shot malicious
jailbreak attack. We propose a Modified Levenshtein Distance (MLD) metric for a
more nuanced evaluation of an LLM's recognition capability. Through
comprehensive experiments on four SOTA open-source LLMs, we demonstrate
superior jailbreak performance. We further validate our framework's real-world
relevance by showing its successful transferability to leading commercial
models, including GPT-4o, Claude Sonnet 3.7, and DeepSeek-V3, and by conducting
a rigorous effectiveness analysis against potential defenses such as LLaMA
Guard and Azure's content filters. Our findings underscore that true LLM
security requires defending against a multi-modal space of interpretations,
even within text-only inputs, and highlight the effectiveness of strategic,
reconnaissance-based attacks. Content Warning: This paper includes potentially
harmful and offensive model outputs.

</details>


### [162] [Secret-Protected Evolution for Differentially Private Synthetic Text Generation](https://arxiv.org/abs/2510.10990)
*Tianze Wang,Zhaoyu Chen,Jian Du,Yingtai Xiao,Linjun Zhang,Qiang Yan*

Main category: cs.CR

TL;DR: 本文提出了一种新的框架SecPE，能够在保护隐私的同时提高合成文本生成的效用，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私合成文本生成施加了统一的保证，通常过度保护非敏感内容，导致显著的效用损失和计算开销。

Method: 我们提出了Secret-Protected Evolution (SecPE)，这是一种新颖的框架，通过秘密感知保护扩展了私有进化。

Result: 在OpenReview、PubMed和Yelp基准测试中，SecPE始终比基于GDP的Aug-PE基线获得更低的Fréchet Inception Distance (FID)和更高的下游任务准确性，同时需要更少的噪声来达到相同的保护水平。

Conclusion: 我们的结果表明，基于秘密的保证可以解锁更实用和有效的隐私保护合成文本生成。

Abstract: Text data has become extremely valuable on large language models (LLMs) and
even lead to general artificial intelligence (AGI). A lot of high-quality text
in the real world is private and cannot be freely used due to privacy concerns.
Therefore, differentially private (DP) synthetic text generation has been
proposed, aiming to produce high-utility synthetic data while protecting
sensitive information. However, existing DP synthetic text generation imposes
uniform guarantees that often overprotect non-sensitive content, resulting in
substantial utility loss and computational overhead. Therefore, we propose
Secret-Protected Evolution (SecPE), a novel framework that extends private
evolution with secret-aware protection. Theoretically, we show that SecPE
satisfies $(\mathrm{p}, \mathrm{r})$-secret protection, constituting a
relaxation of Gaussian DP that enables tighter utility-privacy trade-offs,
while also substantially reducing computational complexity relative to baseline
methods. Empirically, across the OpenReview, PubMed, and Yelp benchmarks, SecPE
consistently achieves lower Fr\'echet Inception Distance (FID) and higher
downstream task accuracy than GDP-based Aug-PE baselines, while requiring less
noise to attain the same level of protection. Our results highlight that
secret-aware guarantees can unlock more practical and effective
privacy-preserving synthetic text generation.

</details>


### [163] [Bag of Tricks for Subverting Reasoning-based Safety Guardrails](https://arxiv.org/abs/2510.11570)
*Shuo Chen,Zhen Han,Haokun Chen,Bailan He,Shengyun Si,Jingpei Wu,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: 研究发现基于推理的防护措施容易受到输入提示的细微操纵，提出了一系列劫持方法，并验证了其高成功率，强调了需要更强的对齐技术。


<details>
  <summary>Details</summary>
Motivation: 研究基于推理的防护措施是否容易受到输入提示的细微操纵的影响，并探索可能的劫持方法。

Method: 通过添加少量模板标记到输入提示中，成功绕过看似强大的基于推理的防护措施，并导致明确和有害的响应。引入了一套劫持方法，从简单的模板操作到完全自动化的优化。

Result: 这些方法在多个基准测试中实现了惊人的高攻击成功率（例如，在gpt-oss系列上超过90%）。评估确认了这些漏洞是系统性的。

Conclusion: 这些漏洞表明需要更强的对齐技术来防止开源LRMs的恶意使用。

Abstract: Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs),
such as deliberative alignment, have shown strong defense against jailbreak
attacks. By leveraging LRMs' reasoning ability, these guardrails help the
models to assess the safety of user inputs before generating final responses.
The powerful reasoning ability can analyze the intention of the input query and
will refuse to assist once it detects the harmful intent hidden by the
jailbreak methods. Such guardrails have shown a significant boost in defense,
such as the near-perfect refusal rates on the open-source gpt-oss series.
Unfortunately, we find that these powerful reasoning-based guardrails can be
extremely vulnerable to subtle manipulation of the input prompts, and once
hijacked, can lead to even more harmful results. Specifically, we first uncover
a surprisingly fragile aspect of these guardrails: simply adding a few template
tokens to the input prompt can successfully bypass the seemingly powerful
guardrails and lead to explicit and harmful responses. To explore further, we
introduce a bag of jailbreak methods that subvert the reasoning-based
guardrails. Our attacks span white-, gray-, and black-box settings and range
from effortless template manipulations to fully automated optimization. Along
with the potential for scalable implementation, these methods also achieve
alarmingly high attack success rates (e.g., exceeding 90% across 5 different
benchmarks on gpt-oss series on both local host models and online API
services). Evaluations across various leading open-source LRMs confirm that
these vulnerabilities are systemic, underscoring the urgent need for stronger
alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is
open-sourced at https://chenxshuo.github.io/bag-of-tricks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [164] [The Social Cost of Intelligence: Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems](https://arxiv.org/abs/2510.10943)
*Thi-Nhung Nguyen,Linhao Luo,Thuy-Trang Vu,Dinh Phung*

Main category: cs.MA

TL;DR: 本文研究了多智能体系统中的刻板偏见，发现这些系统通常不如单智能体系统稳健，但合作和更强大的LLM可以减轻偏见。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统中的偏见出现和传播，因为这在之前的研究中尚未得到充分探索。

Method: 我们通过模拟不同社会群体的代理来评估系统行为，并在各种交互和对抗场景下进行实验。

Result: 实验结果显示，多智能体系统通常不如单智能体系统稳健，偏见往往早期通过内群体偏好出现。然而，合作和辩论式的通信可以减轻偏见放大，更稳健的底层LLM提高了整体系统稳定性。

Conclusion: 我们的研究突出了影响多智能体LLM系统公平性和韧性的重要因素。

Abstract: Bias in large language models (LLMs) remains a persistent challenge,
manifesting in stereotyping and unfair treatment across social groups. While
prior research has primarily focused on individual models, the rise of
multi-agent systems (MAS), where multiple LLMs collaborate and communicate,
introduces new and largely unexplored dynamics in bias emergence and
propagation. In this work, we present a comprehensive study of stereotypical
bias in MAS, examining how internal specialization, underlying LLMs and
inter-agent communication protocols influence bias robustness, propagation, and
amplification. We simulate social contexts where agents represent different
social groups and evaluate system behavior under various interaction and
adversarial scenarios. Experiments on three bias benchmarks reveal that MAS are
generally less robust than single-agent systems, with bias often emerging early
through in-group favoritism. However, cooperative and debate-based
communication can mitigate bias amplification, while more robust underlying
LLMs improve overall system stability. Our findings highlight critical factors
shaping fairness and resilience in multi-agent LLM systems.

</details>


### [165] [Automating Structural Engineering Workflows with Large Language Model Agents](https://arxiv.org/abs/2510.11004)
*Haoran Liang,Yufa Zhou,Mohammad Talebi Kalaleh,Qipei Mei*

Main category: cs.MA

TL;DR: MASSE是一个基于大型语言模型的多智能体系统，能够自动化结构工程工作流程，减少专家工作量并提高准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 结构工程是一个基础但传统上停滞不前的领域，核心工作流程几十年来几乎没有变化。尽管其经济影响和全球市场规模巨大，但传统的工程流程难以适应现代需求。

Method: 引入了MASSE，这是一个基于大型语言模型（LLM）代理的多智能体系统，用于结构工程。

Result: 通过一个无需训练的基于LLM的多智能体系统，大多数现实世界的结构工程工作流程可以完全自动化。MASSE能够在专业环境中立即部署，并在真实案例研究中展示了其有效性。

Conclusion: MASSE可以显著减少专家的工作量，同时提高实际工程场景中的可靠性和准确性。

Abstract: We introduce $\textbf{MASSE}$, the first Multi-Agent System for Structural
Engineering, effectively integrating large language model (LLM)-based agents
with real-world engineering workflows. Structural engineering is a fundamental
yet traditionally stagnant domain, with core workflows remaining largely
unchanged for decades despite its substantial economic impact and global market
size. Recent advancements in LLMs have significantly enhanced their ability to
perform complex reasoning, long-horizon planning, and precise tool utilization
-- capabilities well aligned with structural engineering tasks such as
interpreting design codes, executing load calculations, and verifying
structural capacities. We present a proof-of-concept showing that most
real-world structural engineering workflows can be fully automated through a
training-free LLM-based multi-agent system. MASSE enables immediate deployment
in professional environments, and our comprehensive validation on real-world
case studies demonstrates that it can reduce expert workload from approximately
two hours to mere minutes, while enhancing both reliability and accuracy in
practical engineering scenarios.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [166] [Stop DDoS Attacking the Research Community with AI-Generated Survey Papers](https://arxiv.org/abs/2510.09686)
*Jianghao Lin,Rong Shan,Jiachen Zhu,Yunjia Xi,Yong Yu,Weinan Zhang*

Main category: cs.CY

TL;DR: This position paper argues that we must stop uploading massive amounts of AI-generated survey papers to the research community, by instituting strong norms for AI-assisted review writing. It calls for restoring expert oversight and transparency in AI usage and developing new infrastructures such as Dynamic Live Surveys.


<details>
  <summary>Details</summary>
Motivation: the recent surge of AI-generated surveys has transformed this traditionally labor-intensive genre into a low-effort, high-volume output, which introduces a critical threat: the phenomenon we term the 'survey paper DDoS attack' to the research community.

Method: quantitative trend analysis, quality audits, and cultural impact discussion

Result: the unchecked proliferation of superficially comprehensive but often redundant, low-quality, or even hallucinated survey manuscripts, which floods preprint platforms, overwhelms researchers, and erodes trust in the scientific record.

Conclusion:  safeguarding the integrity of surveys is no longer optional but imperative to the research community.

Abstract: Survey papers are foundational to the scholarly progress of research
communities, offering structured overviews that guide both novices and experts
across disciplines. However, the recent surge of AI-generated surveys,
especially enabled by large language models (LLMs), has transformed this
traditionally labor-intensive genre into a low-effort, high-volume output.
While such automation lowers entry barriers, it also introduces a critical
threat: the phenomenon we term the "survey paper DDoS attack" to the research
community. This refers to the unchecked proliferation of superficially
comprehensive but often redundant, low-quality, or even hallucinated survey
manuscripts, which floods preprint platforms, overwhelms researchers, and
erodes trust in the scientific record. In this position paper, we argue that we
must stop uploading massive amounts of AI-generated survey papers (i.e., survey
paper DDoS attack) to the research community, by instituting strong norms for
AI-assisted review writing. We call for restoring expert oversight and
transparency in AI usage and, moreover, developing new infrastructures such as
Dynamic Live Surveys, community-maintained, version-controlled repositories
that blend automated updates with human curation. Through quantitative trend
analysis, quality audits, and cultural impact discussion, we show that
safeguarding the integrity of surveys is no longer optional but imperative to
the research community.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [167] [Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation](https://arxiv.org/abs/2510.11423)
*Jiaying Wu,Zihang Fu,Haonan Wang,Fanxiao Li,Min-Yen Kan*

Main category: cs.SI

TL;DR: 本文提出了一种名为CrowdNotes+的框架，通过利用大型语言模型来增强X（前身为Twitter）的Community Notes系统，以提高健康虚假信息治理的速度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 由于Community Notes在处理虚假信息时存在显著的延迟，我们需要一种更快速和可靠的解决方案来应对现实世界中的虚假信息激增。

Method: 我们提出了CrowdNotes+，这是一个统一的框架，利用大型语言模型（LLMs）来增强Community Notes，以更快、更可靠地管理健康虚假信息。该框架包括两种互补的模式：(1) 基于证据的注释增强和(2) 以效用为导向的注释自动化，以及一个分层的三步评估，逐步评估相关性、正确性和有用性。

Result: 实验表明，当前的有用性评估中存在一个被忽视的漏洞，即风格流畅性被误认为是事实准确性。此外，我们的分层评估和LLM增强生成共同提高了事实精确度和证据效用。

Conclusion: 我们的研究指出了一种混合的人类-AI治理模式，可以提高基于群体的事实核查的严谨性和及时性。

Abstract: Community Notes, the crowd-sourced misinformation governance system on X
(formerly Twitter), enables users to flag misleading posts, attach contextual
notes, and vote on their helpfulness. However, our analysis of 30.8K
health-related notes reveals significant latency, with a median delay of 17.6
hours before the first note receives a helpfulness status. To improve
responsiveness during real-world misinformation surges, we propose CrowdNotes+,
a unified framework that leverages large language models (LLMs) to augment
Community Notes for faster and more reliable health misinformation governance.
CrowdNotes+ integrates two complementary modes: (1) evidence-grounded note
augmentation and (2) utility-guided note automation, along with a hierarchical
three-step evaluation that progressively assesses relevance, correctness, and
helpfulness. We instantiate the framework through HealthNotes, a benchmark of
1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness
judge. Experiments on fifteen LLMs reveal an overlooked loophole in current
helpfulness evaluation, where stylistic fluency is mistaken for factual
accuracy, and demonstrate that our hierarchical evaluation and LLM-augmented
generation jointly enhance factual precision and evidence utility. These
results point toward a hybrid human-AI governance model that improves both the
rigor and timeliness of crowd-sourced fact-checking.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [168] [The Geometry of Reasoning: Flowing Logics in Representation Space](https://arxiv.org/abs/2510.09782)
*Yufa Zhou,Yixiao Wang,Xunjian Yin,Shuyan Zhou,Anru R. Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种新的几何框架，用于研究大型语言模型的推理过程，通过分析表示空间中的流来理解其行为。


<details>
  <summary>Details</summary>
Motivation: 我们研究大型语言模型如何通过其表示空间进行思考，试图理解它们的推理过程，并提供一种新的方法来分析和解释大型语言模型的行为。

Method: 我们提出了一个新颖的几何框架，将大型语言模型的推理建模为流，通过嵌入轨迹演化来体现逻辑。我们通过使用相同的自然演绎命题但不同的语义载体来分离逻辑结构和语义，从而测试大型语言模型是否超越表面形式内化逻辑。

Result: 我们的理论表明：(1) 大型语言模型的推理对应于表示空间中的平滑流，(2) 逻辑语句作为这些流速度的局部控制器。我们设计了受控实验来可视化和量化推理流，提供了对我们理论框架的经验验证。

Conclusion: 我们的工作为研究推理现象提供了概念基础和实用工具，提供了一种新的视角来解释和分析大型语言模型的行为。

Abstract: We study how large language models (LLMs) ``think'' through their
representation space. We propose a novel geometric framework that models an
LLM's reasoning as flows -- embedding trajectories evolving where logic goes.
We disentangle logical structure from semantics by employing the same natural
deduction propositions with varied semantic carriers, allowing us to test
whether LLMs internalize logic beyond surface form. This perspective connects
reasoning with geometric quantities such as position, velocity, and curvature,
enabling formal analysis in representation and concept spaces. Our theory
establishes: (1) LLM reasoning corresponds to smooth flows in representation
space, and (2) logical statements act as local controllers of these flows'
velocities. Using learned representation proxies, we design controlled
experiments to visualize and quantify reasoning flows, providing empirical
validation of our theoretical framework. Our work serves as both a conceptual
foundation and practical tools for studying reasoning phenomenon, offering a
new lens for interpretability and formal analysis of LLMs' behavior.

</details>


### [169] [The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs](https://arxiv.org/abs/2510.09905)
*Xi Fang,Weijie Xu,Yuchong Zhang,Stephanie Eckman,Scott Nickleach,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 研究发现，个性化AI系统可能因用户记忆而产生情感推理偏差，进而强化社会不平等。


<details>
  <summary>Details</summary>
Motivation: 随着个性化AI系统越来越多地结合长期用户记忆，理解这种记忆如何塑造情感推理变得至关重要。

Method: 通过评估15个模型在人类验证的情感智能测试中的表现，研究了用户记忆如何影响大型语言模型（LLMs）的情感智能。

Result: 研究发现，相同的场景与不同的用户资料配对会产生系统性不同的情感解释。在经过验证的独立情感场景和多样化的用户资料中，一些高性能的LLMs中出现了系统性偏差，其中优势群体获得了更准确的情感解释。此外，LLMs在情感理解和支持性建议任务中表现出显著的性别差异。

Conclusion: 研究结果表明，增强记忆的AI系统在个性化设计中可能无意中强化社会不平等。

Abstract: When an AI assistant remembers that Sarah is a single mother working two
jobs, does it interpret her stress differently than if she were a wealthy
executive? As personalized AI systems increasingly incorporate long-term user
memory, understanding how this memory shapes emotional reasoning is critical.
We investigate how user memory affects emotional intelligence in large language
models (LLMs) by evaluating 15 models on human validated emotional intelligence
tests. We find that identical scenarios paired with different user profiles
produce systematically divergent emotional interpretations. Across validated
user independent emotional scenarios and diverse user profiles, systematic
biases emerged in several high-performing LLMs where advantaged profiles
received more accurate emotional interpretations. Moreover, LLMs demonstrate
significant disparities across demographic factors in emotion understanding and
supportive recommendations tasks, indicating that personalization mechanisms
can embed social hierarchies into models emotional reasoning. These results
highlight a key challenge for memory enhanced AI: systems designed for
personalization may inadvertently reinforce social inequalities.

</details>


### [170] [A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning](https://arxiv.org/abs/2510.10592)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出了一种统一的直觉-方法分层模型，结合范围扩展，以更系统地解决未见过的问题，并引入了基于熵的方法扩展评估框架，以量化评估这一过程。


<details>
  <summary>Details</summary>
Motivation: 现有研究已经引入了基于方法的推理和范围扩展作为增强大型语言模型（LLM）性能的方法，超越直接矩阵映射。然而，这些方法尚未被系统地整合，以更有效地处理未见过的问题。因此，本文旨在提出一种统一的模型，以更系统地解决这些问题。

Method: 本文提出了一个统一的直觉-方法分层模型，结合范围扩展，包括垂直（原因分析）、水平（平行和泛化问题）、时间扩展和空间扩展。此外，还引入了基于熵的方法扩展评估框架，以量化评估这一过程。

Result: 本文提出的模型能够更系统地解决未见过的问题，并通过熵值评估框架衡量扩展的独立性和多样性，从而提高系统的适应能力。

Conclusion: 本文提出了一种统一的直觉-方法分层模型，结合范围扩展，以更系统地解决间接（未见过）问题。通过将直觉思维与方法思维相结合，并引入熵值评估框架，该工作推动了LLM在现实问题解决中更稳健和可扩展的推理范式的发展。

Abstract: Existing studies have introduced method-based reasoning and scope extension
as approaches to enhance Large Language Model (LLM) performance beyond direct
matrix mappings. Building on these foundations, this paper summarizes and
integrates these ideas into a unified Intuition-Method Layered Model with Scope
Extension, designed to address indirected (unseen) issues more systematically.
In this framework, intuition-based thinking provides rapid first-reaction
answers, while method-based thinking decouples questions and solutions into
transferable reasoning units. Scope extension is then applied to broaden
applicability, including vertical (cause analysis), horizontal (parallel and
generalized issues), and for the first time, temporal and spatial extensions,
which expand reasoning across time and contextual dimensions. These extensions
are organized into systematic knowledge trees that interconnect into a
knowledge network, thereby increasing adaptability. To quantitatively evaluate
this process, we propose the entropy of method extension, which measures the
independence and diversity of extensions as an indicator of the system's
capacity to solve unseen questions. By logically connecting existing approaches
with new extensions and introducing an entropy-based evaluation framework, this
work advances toward a more robust and extensible reasoning paradigm for LLMs
in real-world problem-solving.

</details>


### [171] [DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems](https://arxiv.org/abs/2510.10815)
*Meiru Zhang,Philipp Borchert,Milan Gritta,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: 本文提出了一种名为DRIFT的新框架，用于解决大型语言模型在自动形式化数学陈述时的挑战。通过分解非正式数学陈述并进行针对性的前提检索，DRIFT显著提高了检索效果，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 自动化数学陈述的形式化对于大型语言模型（LLMs）来说仍然是一个重大挑战。LLMs难以识别和利用数学知识及其相应的形式化表示。当前的检索增强型自动形式化方法直接使用非正式陈述查询外部库，但忽略了基本限制：非正式数学陈述通常复杂且对底层数学概念的上下文有限。

Method: 我们引入了DRIFT，这是一种新的框架，使LLMs能够将非正式的数学陈述分解成更小、更易处理的“子组件”，这有助于从数学库中进行有针对性的预提取得。此外，DRIFT检索示例定理以帮助模型在形式化任务中更有效地使用前提。

Result: 我们在多样化的基准测试（ProofNet、ConNF和MiniF2F-test）上评估了DRIFT，并发现它在前提检索方面表现良好，与DPR基线相比，在ProofNet上的F1分数几乎翻了一番。值得注意的是，DRIFT在分布外的ConNF基准测试中表现出色，使用GPT-4.1和DeepSeek-V3.1分别实现了37.14%和42.25%的BEq+@10提升。

Conclusion: 我们的分析表明，数学自动形式化中的检索效果在很大程度上取决于模型特定的知识边界，这突显了需要与每个模型的能力相一致的自适应检索策略。

Abstract: Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

</details>


### [172] [Revisiting Model Interpolation for Efficient Reasoning](https://arxiv.org/abs/2510.10977)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Ngai Wong*

Main category: cs.AI

TL;DR: 本文研究了模型插值的三阶段进化范式，并展示了其在效率和效果上的优势。


<details>
  <summary>Details</summary>
Motivation: 模型合并在指令和思考模型上表现出色，但需要更深入的理解和优化以平衡性能与成本。

Method: 本文系统地回顾了最简单的合并方法，即直接对两个权重进行插值，并观察到模型插值遵循一个具有不同行为的三阶段进化范式。

Result: 实验证明，经过策略性插值的模型在效率和效果上都优于复杂的模型合并基线。

Conclusion: 本文揭示了模型插值的机制，并提供了一个实用的框架，用于构建具有精确推理能力的模型。

Abstract: Model merging, typically on Instruct and Thinking models, has shown
remarkable performance for efficient reasoning. In this paper, we
systematically revisit the simplest merging method that interpolates two
weights directly. Particularly, we observe that model interpolation follows a
three-stage evolutionary paradigm with distinct behaviors on the reasoning
trajectory. These dynamics provide a principled guide for navigating the
performance-cost trade-off. Empirical results demonstrate that a strategically
interpolated model surprisingly surpasses sophisticated model merging baselines
on both efficiency and effectiveness. We further validate our findings with
extensive ablation studies on model layers, modules, and decoding strategies.
Ultimately, this work demystifies model interpolation and offers a practical
framework for crafting models with precisely targeted reasoning capabilities.
Code is available at \href{https://github.com/wutaiqiang/MI}{Github}.

</details>


### [173] [$How^{2}$: How to learn from procedural How-to questions](https://arxiv.org/abs/2510.11144)
*Gautier Dagan,Frank Keller,Alex Lascarides*

Main category: cs.AI

TL;DR: How^{2} 是一个记忆代理框架，使代理能够提出如何问题、存储答案，并在交互环境中进行终身学习。在Plancraft环境中评估，结果表明抽象且与当前状态解耦的答案对终身学习代理最有帮助。


<details>
  <summary>Details</summary>
Motivation: 代理在面对规划问题时，可以通过回答如何问题来减少不确定性并填补知识空白，但这些问题的开放性使得AI代理和专家难以有效地提问和回答。

Method: How^{2} 是一个记忆代理框架，使代理能够提出如何问题、存储答案，并在交互环境中进行终身学习。

Result: 在Plancraft环境中，使用不同抽象级别的教师模型进行评估，结果表明，抽象且与当前状态解耦的答案对终身学习代理最有帮助。

Conclusion: How^{2} 为基于大语言模型的代理提供了一种改进其规划能力的方法，通过在交互环境中提问来实现长期学习。

Abstract: An agent facing a planning problem can use answers to how-to questions to
reduce uncertainty and fill knowledge gaps, helping it solve both current and
future tasks. However, their open ended nature, where valid answers to "How do
I X?" range from executable actions to high-level descriptions of X's
sub-goals, makes them challenging for AI agents to ask, and for AI experts to
answer, in ways that support efficient planning. We introduce $How^{2}$, a
memory agent framework that enables agents to ask how-to questions, store the
answers, and reuse them for lifelong learning in interactive environments. We
evaluate our approach in Plancraft, a Minecraft crafting environment, where
agents must complete an assembly task by manipulating inventory items. Using
teacher models that answer at varying levels of abstraction, from executable
action sequences to high-level subgoal descriptions, we show that lifelong
learning agents benefit most from answers that are abstracted and decoupled
from the current state. $How^{2}$ offers a way for LLM-based agents to improve
their planning capabilities over time by asking questions in interactive
environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [174] [Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments](https://arxiv.org/abs/2510.09712)
*Zhao Tong,Chunlin Gong,Yimeng Gu,Haichao Shi,Qiang Liu,Shu Wu,Xiao-Yu Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种群体适应性对抗训练策略，以提高假新闻检测模型的鲁棒性。该方法通过将对抗性评论分类并生成特定攻击，以及应用自适应采样机制来增强模型的抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 现有的假新闻检测模型在标准设置中表现良好，但对由真实用户或大型语言模型（LLMs）撰写的对抗性评论仍然脆弱，这些评论会微妙地改变模型决策。

Method: 我们提出了一种群体适应性对抗训练策略，包括将对抗性评论分为三种基于心理学的类别（感知、认知和社会），通过LLMs生成多样化的类别特定攻击，并应用基于Dirichlet的自适应采样机制（InfoDirichlet调整机制）在训练过程中动态调整不同评论类别的学习重点。

Result: 实验表明，我们的方法在基准数据集上保持了强大的检测准确性，并显著提高了对各种对抗性评论扰动的鲁棒性。

Conclusion: 我们的方法在保持强大检测准确性的同时，显著提高了对各种对抗性评论扰动的鲁棒性。

Abstract: The spread of fake news online distorts public judgment and erodes trust in
social media platforms. Although recent fake news detection (FND) models
perform well in standard settings, they remain vulnerable to adversarial
comments-authored by real users or by large language models (LLMs)-that subtly
shift model decisions. In view of this, we first present a comprehensive
evaluation of comment attacks to existing fake news detectors and then
introduce a group-adaptive adversarial training strategy to improve the
robustness of FND models. To be specific, our approach comprises three steps:
(1) dividing adversarial comments into three psychologically grounded
categories: perceptual, cognitive, and societal; (2) generating diverse,
category-specific attacks via LLMs to enhance adversarial training; and (3)
applying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting
Mechanism) that dynamically adjusts the learning focus across different comment
categories during training. Experiments on benchmark datasets show that our
method maintains strong detection accuracy while substantially increasing
robustness to a wide range of adversarial comment perturbations.

</details>


### [175] [It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning](https://arxiv.org/abs/2510.09723)
*Gregory D. Baker*

Main category: cs.LG

TL;DR: The paper introduces Narrative Learning, a natural language-based approach to model refinement, showing improved accuracy over traditional explainable models on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore an alternative approach to explainable machine learning by using natural language to define and refine models.

Method: Narrative Learning is a methodology where models are defined in natural language and refined using explanatory prompts instead of numerical optimization.

Result: On 5 out of 6 datasets, Narrative Learning became more accurate than baseline models by 2025 due to advancements in language models. The paper also observed trends in the lexicostatistics of model outputs as a proxy for explanation comprehensibility.

Conclusion: Narrative Learning demonstrates higher accuracy than baseline explainable models on most datasets, suggesting its potential as a method for improving model interpretability.

Abstract: In this paper, we introduce Narrative Learning, a methodology where models
are defined entirely in natural language and iteratively refine their
classification criteria using explanatory prompts rather than traditional
numerical optimisation. We report on experiments to evaluate the accuracy and
potential of this approach using 3 synthetic and 3 natural datasets and compare
them against 7 baseline explainable machine learning models. We demonstrate
that on 5 out of 6 of these datasets, Narrative Learning became more accurate
than the baseline explainable models in 2025 or earlier because of improvements
in language models. We also report on trends in the lexicostatistics of these
models' outputs as a proxy for the comprehensibility of the explanations.

</details>


### [176] [Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings](https://arxiv.org/abs/2510.09739)
*Ayoub Bouguettaya,Elizabeth M. Stuart*

Main category: cs.LG

TL;DR: 研究发现大五模型在描述在线社区方面比机器学习模型更有效，表明人格的语义结构是依赖于语境的。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证大五模型在描述在线社区中的有效性，并评估机器学习是否能够替代传统的人格模型。

Method: 我们从一个经典形容词列表中创建了一个自下而上的个性模型，并使用机器学习进行了比较分析，同时分析了一百万条Reddit评论。

Result: 大五模型，特别是宜人性、尽责性和神经质，提供了更强大和可解释的描述。而我们的机器学习聚类没有提供有意义的区别，未能恢复外向性特质，并且缺乏大五模型的心理测量一致性。

Conclusion: 这些结果确认了大五模型的稳健性，并表明人格的语义结构是依赖于语境的。我们的发现表明，虽然机器学习可以帮助检查现有心理学理论的生态效度，但它可能无法取代它们。

Abstract: The lexical hypothesis posits that personality traits are encoded in language
and is foundational to models like the Big Five. We created a bottom-up
personality model from a classic adjective list using machine learning and
compared its descriptive utility against the Big Five by analyzing one million
Reddit comments. The Big Five, particularly Agreeableness, Conscientiousness,
and Neuroticism, provided a far more powerful and interpretable description of
these online communities. In contrast, our machine-learning clusters provided
no meaningful distinctions, failed to recover the Extraversion trait, and
lacked the psychometric coherence of the Big Five. These results affirm the
robustness of the Big Five and suggest personality's semantic structure is
context-dependent. Our findings show that while machine learning can help check
the ecological validity of established psychological theories, it may not be
able to replace them.

</details>


### [177] [Building a Foundational Guardrail for General Agentic Systems via Synthetic Data](https://arxiv.org/abs/2510.09781)
*Yue Huang,Hang Hua,Yujun Zhou,Pengcheng Jing,Manish Nagireddy,Inkit Padhi,Greta Dolcetti,Zhangchen Xu,Subhajit Chaudhury,Ambrish Rawat,Liubov Nedoshivina,Pin-Yu Chen,Prasanna Sattigeri,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种在执行前进行干预的防护机制，解决了数据、模型和评估方面的差距，提高了LLM代理系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的防护机制主要在执行后工作，难以扩展且缺乏对计划层面的可控监督。因此，需要一种在执行前进行干预的方法，以防止潜在的风险造成严重后果。

Method: 本文提出了三个解决方案：AuraGen用于生成安全轨迹，Safiron作为基础防护机制，以及Pre-Exec Bench作为评估基准。这些方法旨在解决数据、模型和评估方面的差距，以提高LLM代理系统的安全性。

Result: 实验结果表明，Safiron在Pre-Exec Bench上表现出色，优于现有基线，并提供了可操作的实践模板，以实现更安全的代理系统。

Conclusion: 本文提出了一个名为Safiron的防护机制，通过解决数据、模型和评估三个关键差距，实现了在执行前的安全性。实验结果表明，该防护机制在Pre-Exec Bench上优于现有基线，并提供了更安全的代理系统模板。

Abstract: While LLM agents can plan multi-step tasks, intervening at the planning
stage-before any action is executed-is often the safest way to prevent harm,
since certain risks can lead to severe consequences once carried out. However,
existing guardrails mostly operate post-execution, which is difficult to scale
and leaves little room for controllable supervision at the plan level. To
address this challenge, we highlight three critical gaps in current research:
data gap, model gap, and evaluation gap. To close the data gap, we introduce
AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)
injects category-labeled risks with calibrated difficulty, and (iii) filters
outputs via an automated reward model, producing large and reliable corpora for
pre-execution safety. To close the guardian model gap, we propose a
foundational guardrail Safiron, combining a cross-planner adapter with a
compact guardian model. The adapter unifies different input formats, while
Safiron flags risky cases, assigns risk types, and generates rationales;
trained in two stages with a broadly explored data recipe, Safiron achieves
robust transfer across settings. To close the evaluation gap, we release
Pre-Exec Bench, a realistic benchmark covering diverse tools and branching
trajectories, which measures detection, fine-grained categorization,
explanation, and cross-planner generalization in human-verified scenarios.
Extensive experiments demonstrate consistent gains of the proposed guardrail
over strong baselines on Pre-Exec Bench, and ablations further distill
actionable practices, providing a practical template for safer agentic systems.

</details>


### [178] [Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling](https://arxiv.org/abs/2510.10060)
*Hehe Fan,Yi Yang,Mohan Kankanhalli,Fei Wu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: When modeling a given type of data, we consider it to involve two key
aspects: 1) identifying relevant elements (e.g., image pixels or textual words)
to a central element, as in a convolutional receptive field, or to a query
element, as in self-attention, and 2) encoding these tokens effectively.
Self-attention can adaptively identify these elements but relies on absolute
positional embedding for structural representation learning. In contrast,
convolution encodes elements in a relative manner, yet their fixed kernel size
limits their ability to adaptively select the relevant elements. In this paper,
we introduce Translution, an operation that unifies the adaptive identification
capability of self-attention and the relative encoding advantage of
convolution. However, this integration leads to a substantial increase in the
number of parameters, exceeding most currently available computational
resources. Therefore, we propose a lightweight variant of Translution, named
{\alpha}-Translution. Experiments on computer vision and natural language
processing tasks show that Translution (including {\alpha}-Translution)
achieves superior accuracy compared to self-attention. The code is available at
https://github.com/hehefan/Translution.

</details>


### [179] [RLFR: Extending Reinforcement Learning for LLMs with Flow Environment](https://arxiv.org/abs/2510.10201)
*Jinghao Zhang,Naishan Zheng,Ruilin Li,Dongzhou Cheng,Zheming Liang,Feng Zhao,Jiaqi Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于潜在空间流动奖励的RLVR方法（RLFR），通过构建流动场并量化策略潜在的速率偏差作为奖励信号，实验表明其在语言和多模态推理任务中表现可靠。


<details>
  <summary>Details</summary>
Motivation: 现有的二进制验证方法可能忽略有价值的探索，而黄金过程奖励模型的标注成本高昂，因此需要一种新的奖励塑造方法。

Method: 提出了一种新的基于潜在空间流动奖励的RLVR方法，称为RLFR，通过构建流动场并量化策略潜在的速率偏差作为奖励信号。

Result: 实验表明，流动奖励在语言和多模态推理基准中是可靠的，并且能够有效地利用隐藏状态中的上下文依赖性。

Conclusion: RLFR展示了流奖励在语言和多模态推理基准中的可靠性，并提出了一个有前景的辅助信号奖励塑造范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a promising framework for improving reasoning abilities in Large Language
Models (LLMs). However, policy optimized with binary verification prone to
overlook potential valuable exploration in reasoning trajectory. In view of
heavy annotation cost of golden Process Reward Models (PRMs), recent works
attempt using auxiliary signals for reward shaping of process tokens, involving
entropy and likelihood collected from logit space. In this work, we offer a
novel perspective on shaping RLVR with flow rewards derived from latent space,
and propose RLFR, where the flow fields of model latents are constructed from
either off-policy high-quality data and on-policy rejection sampling data, and
the velocity deviations of policy latents within it are quantified to serve as
a reward signal. RLFR first demonstrates that a well-established flow field can
be a sound environment for reward signal collection, highlighting the
expressive latent space is much underexplored. Moreover, RLFR is able to
compress any off-policy expert data as reference for constituting reward
signals, and we show that the efficient context dependence compressed within
the hidden states are utilized, rather than individual token-level denotation
for context comprehending. Experiments on both language and multimodal
reasoning benchmarks demonstrate the reliability of flow rewards, and
suggesting a promising paradigm for reward shaping with auxiliary signals.

</details>


### [180] [Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting](https://arxiv.org/abs/2510.10304)
*Michael Y. Hu,Benjamin Van Durme,Jacob Andreas,Harsh Jhamtani*

Main category: cs.LG

TL;DR: ECHO improves the sample efficiency of language model agents by generating synthetic positive examples from failed interactions, leading to better performance in novel environments.


<details>
  <summary>Details</summary>
Motivation: Language model agents often struggle with poor sample efficiency when learning from sequential interactions, especially in costly environments. Existing architectures do not fully leverage the ability of LMs to generate or reason about counterfactual trajectories.

Method: ECHO is a prompting framework that adapts hindsight experience replay from reinforcement learning for language model agents. It consists of a hindsight rule and an update rule to generate optimized trajectories and maintain compressed trajectory representations.

Result: ECHO outperforms vanilla language agent baselines by up to 80% on stateful versions of XMiniGrid and PeopleJoinQA, and also surpasses sophisticated architectures like Reflexion and AWM.

Conclusion: ECHO demonstrates significant improvements in sample efficiency for language model agents, outperforming existing baselines and architectures in novel environments.

Abstract: Language model (LM) agents deployed in novel environments often exhibit poor
sample efficiency when learning from sequential interactions. This
significantly hinders the usefulness of such agents in environments where
interaction is costly (for example, when they interact with humans or reset
physical systems). While a number of existing LM agent architectures
incorporate various mechanisms for experience storage and reflection, they make
limited use of LMs' abilities to directly generate or reason about full
counterfactual trajectories. We introduce ECHO (Experience Consolidation via
Hindsight Optimization), a prompting framework that adapts hindsight experience
replay from reinforcement learning for language model agents. ECHO generates
optimized trajectories for alternative goals that could have been achieved
during failed attempts, effectively creating synthetic positive examples from
unsuccessful interactions. Our approach consists of two components: a hindsight
rule that uses the language model itself to identify relevant subgoals and
generate optimized trajectories, and an update rule that maintains compressed
trajectory representations in memory. We evaluate ECHO on stateful versions of
XMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a
collaborative information-gathering enterprise simulation. Across both domains,
ECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid,
it also outperforms a number of sophisticated agent architectures including
Reflexion and AWM, demonstrating faster adaptation to novel environments
through more effective utilization of past experiences.

</details>


### [181] [Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher Distillation](https://arxiv.org/abs/2510.10925)
*Hengyuan Zhang,Shiping Yang,Xiao Liang,Chenming Shang,Yuxuan Jiang,Chaofan Tao,Jing Xiong,Hayden Kwok-Hay So,Ruobing Xie,Angel X. Chang,Ngai Wong*

Main category: cs.LG

TL;DR: PerSyn is a new method for generating synthetic data for student models by assigning prompts to the best teacher, leading to better performance in instruction tuning and math reasoning.


<details>
  <summary>Details</summary>
Motivation: Recent studies show that stronger models are not always optimal teachers, revealing a mismatch between teacher outputs and student learnability. The goal is to create data tailored to each student model to enable more effective learning.

Method: PerSyn is a novel synthesis strategy that operates under a new 'Route then Generate' paradigm. It assigns each prompt to its optimal teacher via a query-level router that considers student learnability and teacher response quality. Each teacher then synthesizes data only for its assigned prompts.

Result: Extensive experiments across different model families and scales demonstrate that PerSyn consistently achieves superior or comparable performance to all baselines in instruct tuning and math reasoning settings.

Conclusion: PerSyn consistently achieves superior or comparable performance to all baselines in instruct tuning and math reasoning settings, and it offers insights for future research.

Abstract: Training student models on synthetic data generated by strong teacher models
is a promising way to distilling the capabilities of teachers. However, recent
studies show that stronger models are not always optimal teachers, revealing a
mismatch between teacher outputs and student learnability. To address this
issue, we propose PerSyn (Personalized data Synthesis), a novel synthesis
strategy that operates under a new ``Route then Generate'' paradigm to create
data tailored to each student model, enabling it to learn more effectively.
Specifically, PerSyn first assigns each prompt to its optimal teacher via a
query-level router that jointly considers student learnability and teacher
response quality. Each teacher then synthesizes data only for its assigned
prompts, making the process more efficient than the conventional ``Generate
then Select'' paradigm, where all teachers must generate parallel responses for
the entire prompt set before constructing the final dataset. Extensive
experiments across different model families and scales demonstrate that PerSyn
consistently achieves superior or comparable performance to all baselines in
instruct tuning and math reasoning settings. Further analysis verifies the
effectiveness of PerSyn and offers extra insights to propel future research.

</details>


### [182] [Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning](https://arxiv.org/abs/2510.10959)
*Xiaoyun Zhang,Xiaojian Yuan,Di Huang,Wang You,Chen Hu,Jingqing Ruan,Kejiang Chen,Xing Hu*

Main category: cs.LG

TL;DR: 本文研究了在RLVR训练中政策熵崩溃的问题，并提出了一种自适应熵正则化框架（AER），以动态平衡探索和利用，从而提高推理准确性和探索能力。


<details>
  <summary>Details</summary>
Motivation: To address the issue of policy entropy collapse in RLVR training, which hinders exploration and limits reasoning performance.

Method: Adaptive Entropy Regularization (AER) - a framework that dynamically balances exploration and exploitation via three components: difficulty-aware coefficient allocation, initial-anchored target entropy, and dynamic global coefficient adjustment.

Result: Experiments on multiple mathematical reasoning benchmarks show that AER consistently outperforms baselines, improving both reasoning accuracy and exploration capability.

Conclusion: AER consistently outperforms baselines, improving both reasoning accuracy and exploration capability.

Abstract: Reasoning ability has become a defining capability of Large Language Models
(LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as
a key paradigm to enhance it. However, RLVR training often suffers from policy
entropy collapse, where the policy becomes overly deterministic, hindering
exploration and limiting reasoning performance. While entropy regularization is
a common remedy, its effectiveness is highly sensitive to the fixed
coefficient, making it unstable across tasks and models. In this work, we
revisit entropy regularization in RLVR and argue that its potential has been
largely underestimated. Our analysis shows that (i) tasks of varying difficulty
demand distinct exploration intensities, and (ii) balanced exploration may
require the policy entropy to be maintained within a moderate range below its
initial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a
framework that dynamically balances exploration and exploitation via three
components: difficulty-aware coefficient allocation, initial-anchored target
entropy, and dynamic global coefficient adjustment. Experiments on multiple
mathematical reasoning benchmarks show that AER consistently outperforms
baselines, improving both reasoning accuracy and exploration capability.

</details>


### [183] [ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces](https://arxiv.org/abs/2510.11168)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.LG

TL;DR: ELMO是一种用于极端多标签分类（XMC）的纯低精度训练框架，利用BFloat16和Float8数据类型，结合Kahan求和和随机舍入技术，实现了高效的Float8训练，并通过内存优化技术显著减少了GPU内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有的XMC方法在处理大规模标签空间时存在计算和内存需求高、训练不稳定以及效率低的问题。因此，需要一种更高效的低精度训练框架来解决这些问题。

Method: ELMO是一种纯低精度训练框架，利用BFloat16和Float8数据类型，并通过Kahan求和和随机舍入技术实现有效的Float8训练。此外，还引入了梯度融合和分块等内存优化技术。

Result: ELMO能够在仅使用6.6 GiB GPU内存的情况下训练一个300万标签的XMC模型，而无需牺牲准确性，相比当前最先进的方法Renee的39.7 GiB内存需求有显著减少。

Conclusion: ELMO框架能够在不牺牲准确性的前提下，显著减少GPU内存使用，从而使得在大规模标签空间下的XMC模型训练变得更加高效和可行。

Abstract: Large output spaces, also referred to as Extreme multilabel classification
(XMC), is a setting that arises, e.g., in large-scale tagging and
product-to-product recommendation, and is characterized by the number of labels
ranging from hundreds of thousands to millions. This means that the linear
classification head, usually only a tiny fraction of the overall model, turns
into the main driver for compute and memory demand. Current state-of-the-art
XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we
show can be unstable, and inefficient in terms of memory usage and
computational overhead. Meanwhile, existing low-precision methods typically
retain higher precision for the classification layer. In this work, we propose
ELMO, a pure low-precision training framework for XMC models using BFloat16 and
Float8 data types. By leveraging Kahan summation and stochastic rounding, we
demonstrate that XMC models can be effectively trained entirely in Float8,
without relying on single-precision master weights or tensor scaling.
Low-precision training, combined with our proposed memory optimizations --
gradient fusion and chunking -- enables significant reductions in GPU memory
usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of
GPU memory, compared to the 39.7 GiB required by the optimized SOTA method,
Renee without compromising accuracy.

</details>


### [184] [EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling](https://arxiv.org/abs/2510.11170)
*Daniel Scalena,Leonidas Zotos,Elisabetta Fersini,Malvina Nissim,Ahmet Üstün*

Main category: cs.LG

TL;DR: EAGer is a training-free generation method that uses model uncertainty to reduce redundant computation and improve performance by reallocating the saved compute budget to instances where exploration of alternative paths is most needed.


<details>
  <summary>Details</summary>
Motivation: The motivation behind EAGer is to address the issue of allocating the same compute budget for each prompt, despite different prompts having different degrees of complexity and computation needs.

Method: EAGer is a training-free generation method that leverages model uncertainty through token-wise entropy distribution to reduce redundant computation and improve overall performance. It allows branching to multiple reasoning paths only in the presence of high-entropy tokens and reallocates the saved compute budget to instances where exploration of alternative paths is most needed.

Result: EAGer can reallocate the budget without accessing target labels, achieving the best efficiency-performance trade-off in terms of reasoning length and Pass@k. When target labels are accessible, it generates up to 65% fewer tokens and achieves up to 37% improvement in Pass@k compared to Full Parallel Sampling.

Conclusion: EAGer can achieve the best efficiency-performance trade-off in terms of reasoning length and Pass@k without accessing target labels, and when target labels are accessible, it generates up to 65% fewer tokens and achieves up to 37% improvement in Pass@k compared to Full Parallel Sampling.

Abstract: With the rise of reasoning language models and test-time scaling methods as a
paradigm for improving model performance, substantial computation is often
required to generate multiple candidate sequences from the same prompt. This
enables exploration of different reasoning paths toward the correct solution,
however, allocates the same compute budget for each prompt. Grounded on the
assumption that different prompts carry different degrees of complexity, and
thus different computation needs, we propose EAGer, a training-free generation
method that leverages model uncertainty through token-wise entropy distribution
to reduce redundant computation and concurrently improve overall performance.
EAGer allows branching to multiple reasoning paths only in the presence of
high-entropy tokens, and then reallocates the saved compute budget to the
instances where exploration of alternative paths is most needed. We find that
across multiple open-source models on complex reasoning benchmarks such as AIME
2025, EAGer can reallocate the budget without accessing target labels,
achieving the best efficiency-performance trade-off in terms of reasoning
length and Pass@k. When target labels are accessible, EAGer generates up to 65%
fewer tokens (hence saving compute) and achieves up to 37% improvement in
Pass@k compared to the Full Parallel Sampling.

</details>


### [185] [Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?](https://arxiv.org/abs/2510.11184)
*Zhengyu Chen,Jinluan Yang,Teng Xiao,Ruochen Zhou,Luan Zhang,Xiangyu Xi,Xiaowei Shi,Wei Wang,Jinggang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种工具泛化强化学习框架，能够将数学任务中学习到的工具使用方法有效地转移到其他领域，实现了跨领域的高性能和高效能。


<details>
  <summary>Details</summary>
Motivation: 尽管工具增强的强化学习在不同领域中的泛化性尚未得到充分探索，但本文旨在研究一种仅在数学问题解决任务上训练的LLM代理在多个推理领域的表现。

Method: 本文提出了一个工具泛化强化学习（TGRL）框架，包括标准化工具接口、双组件奖励系统和基于XML的提示模板，以促进领域无关的学习和技能迁移。

Result: 实验结果表明，基于强化学习的工具使用方法可以有效地转移到其他领域的复杂任务中，从而实现卓越的任务性能和高令牌效率。

Conclusion: 本文验证了基于强化学习的工具使用方法可以从数学任务有效地转移到其他领域，展示了工具强化学习在大型语言模型推理中的跨领域潜力。

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities in reasoning and tool utilization. However, the generalization of
tool-augmented reinforcement learning (RL) across diverse domains remains
underexplored. In this work, we investigate the cross-domain generalization of
an LLM agent equipped with a code interpreter tool, which is exclusively
trained on mathematical problem-solving tasks. Despite the restricted training
domain, we evaluate the agent's performance across several distinct reasoning
domains. The results reveal that RL-based tool usage learned from mathematical
tasks can be effectively transferred to complex tasks in other domains,
enabling great task performance and high token efficiency. To facilitate this
cross-domain transfer, we propose a Tool Generalization Reinforcement Learning
(TGRL) framework designed to promote domain-agnostic learning and skill
migration, encompassing: (i) a standardized tool interface that abstracts
domain-specific nuances through consistent formatting and explicit termination,
fostering transferable invocation patterns; (ii) a dual-component reward system
that decomposes rewards to incentivize generalizable behaviors like tool
efficiency and reasoning abstraction, ensuring alignment and robustness across
domain shifts; and (iii) an XML-based prompt template that separates thinking,
tool calls, and responses to encourage modular, domain-invariant planning and
coherent multi-turn interactions. Extensive experiments across diverse
benchmarks validate our approach, achieving state-of-the-art performance and
highlighting the cross-domain potential of Tool RL for LLM reasoning.

</details>


### [186] [ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models](https://arxiv.org/abs/2510.11278)
*Gareth Seneque,Lap-Hang Ho,Nafise Erfanian Saeedi,Jeffrey Molendijk,Ariel Kupermann,Tim Elson*

Main category: cs.LG

TL;DR: 我们提出了ENIGMA，一种新的大型语言模型（LLM）训练方法，通过将组织的政策/原则视为模型的信息流形上的方向，联合改进推理、对齐和鲁棒性。我们的单循环训练器结合了Group-Relative Policy Optimisation（GRPO），一种无批评的政策优化RL方法，以及仅带有Chain-of-Thought（CoT）格式奖励；一个Self-Supervised Alignment with Mutual Information（SAMI）风格的对称InfoNCE辅助项；以及一个隐含的Sinkhorn最优传输正则化器，用于隐藏状态分布以限制几何漂移。我们还引入了infoNCE指标，这些指标在匹配的负样本下专门用于标准MI下限，以衡量模型的CoT如何强烈编码这些政策。这些指标包括一个Sufficiency Index（SI），它允许在训练前选择和创建最大化下游性能的原则。在使用小型（1B）LLM的实验中，高SI原则预测了更稳定的训练动态和比GRPO消融更好的基准性能。我们对训练模型的信息几何分析验证了流形中的理想结构变化。这些结果支持了我们的假设，即推理、对齐和鲁棒性是单一信息几何目标的投影，并且使用ENIGMA训练的模型展示了无奖励模型的有原则的推理，为可信能力提供了一条路径。


<details>
  <summary>Details</summary>
Motivation: 我们旨在通过将组织的政策/原则视为模型的信息流形上的方向，联合改进推理、对齐和鲁棒性。

Method: 我们提出了ENIGMA，一种新的大型语言模型（LLM）训练方法，通过将组织的政策/原则视为模型的信息流形上的方向，联合改进推理、对齐和鲁棒性。我们的单循环训练器结合了Group-Relative Policy Optimisation（GRPO），一种无批评的政策优化RL方法，以及仅带有Chain-of-Thought（CoT）格式奖励；一个Self-Supervised Alignment with Mutual Information（SAMI）风格的对称InfoNCE辅助项；以及一个隐含的Sinkhorn最优传输正则化器，用于隐藏状态分布以限制几何漂移。我们还引入了infoNCE指标，这些指标在匹配的负样本下专门用于标准MI下限，以衡量模型的CoT如何强烈编码这些政策。这些指标包括一个Sufficiency Index（SI），它允许在训练前选择和创建最大化下游性能的原则。

Result: 在使用小型（1B）LLM的实验中，高SI原则预测了更稳定的训练动态和比GRPO消融更好的基准性能。我们对训练模型的信息几何分析验证了流形中的理想结构变化。

Conclusion: 我们的结果支持了我们的假设，即推理、对齐和鲁棒性是单一信息几何目标的投影，并且使用ENIGMA训练的模型展示了无奖励模型的有原则的推理，为可信能力提供了一条路径。

Abstract: We present Entropic Mutual-Information Geometry Large-Language Model
Alignment (ENIGMA), a novel approach to Large-Language Model (LLM) training
that jointly improves reasoning, alignment and robustness by treating an
organisation's policies/principles as directions to move on a model's
information manifold. Our single-loop trainer combines Group-Relative Policy
Optimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought
(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information
(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn
optimal-transport regulariser on hidden-state distributions to bound geometry
drift. We also introduce infoNCE metrics that specialise to a standard MI lower
bound under matched negatives to measure how strongly a model's CoT encodes
these policies. These metrics include a Sufficiency Index (SI) that enables the
selection and creation of principles that maximise downstream performance prior
to training. In our experiments using small (1B) LLMs, high-SI principles
predict steadier training dynamics and improved benchmark performance over GRPO
ablations. Our information-geometry analysis of trained models validates
desirable structural change in the manifold. These results support our
hypothesis that reasoning, alignment, and robustness are projections of a
single informationgeometric objective, and that models trained using ENIGMA
demonstrate principled reasoning without the use of a reward model, offering a
path to trusted capability

</details>


### [187] [ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding](https://arxiv.org/abs/2510.11498)
*Yuhang Li,Chenchen Zhang,Ruilin Lv,Ao Liu,Ken Deng,Yuanxing Zhang,Jiaheng Liu,Wiggin Zhou,Bo Zhou*

Main category: cs.LG

TL;DR: ReLook是一个代理的、视觉基础的强化学习框架，通过调用多模态LLM（MLLM）作为工具来实现一个稳健的生成-诊断-优化循环。在训练过程中，代理使用MLLM作为视觉批评者——用截图对代码进行评分——以及作为可操作的、视觉基础反馈的来源；对于无效渲染的严格零奖励规则锚定了渲染能力并防止奖励黑客行为。为了防止行为崩溃，我们引入了强制优化，这是一种严格的接受规则，只接受改进的修订，产生单调更好的轨迹。在推理时，我们解耦了批评家并运行一个轻量级的无批评自我编辑周期，保持与基础解码相当的延迟同时保留大部分收益。ReLook在三个广泛使用的基准测试中 consistently 超过强基线，在视觉基础的前端代码生成中表现出色。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在算法代码生成方面表现出色，但它们在前端开发中表现不佳，其中正确性是通过渲染像素和交互来判断的。

Method: ReLook是一个代理的、视觉基础的强化学习框架，通过调用多模态LLM（MLLM）作为工具来实现一个稳健的生成-诊断-优化循环。在训练过程中，代理使用MLLM作为视觉批评者——用截图对代码进行评分——以及作为可操作的、视觉基础反馈的来源；对于无效渲染的严格零奖励规则锚定了渲染能力并防止奖励黑客行为。为了防止行为崩溃，我们引入了强制优化，这是一种严格的接受规则，只接受改进的修订，产生单调更好的轨迹。在推理时，我们解耦了批评家并运行一个轻量级的无批评自我编辑周期，保持与基础解码相当的延迟同时保留大部分收益。

Result: ReLook在三个广泛使用的基准测试中 consistently 超过强基线，在视觉基础的前端代码生成中表现出色。

Conclusion: ReLook在视觉基础的前端代码生成中 consistently 超过强基线，突显了代理感知、视觉奖励和训练推理解耦的好处。

Abstract: While Large Language Models (LLMs) excel at algorithmic code generation, they
struggle with front-end development, where correctness is judged on rendered
pixels and interaction. We present ReLook, an agentic, vision-grounded
reinforcement learning framework that empowers an agent to close a robust
generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.
During training, the agent uses the MLLM-in-the-loop both as a visual
critic--scoring code with screenshots--and as a source of actionable,
vision-grounded feedback; a strict zero-reward rule for invalid renders anchors
renderability and prevents reward hacking. To prevent behavioral collapse, we
introduce Forced Optimization, a strict acceptance rule that admits only
improving revisions, yielding monotonically better trajectories. At inference,
we decouple the critic and run a lightweight, critic-free self-edit cycle,
keeping latency comparable to base decoding while retaining most of the gains.
Across three widely used benchmarks, ReLook consistently outperforms strong
baselines in vision-grounded front-end code generation, highlighting the
benefits of agentic perception, visual rewards, and training-inference
decoupling.

</details>


### [188] [Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models](https://arxiv.org/abs/2510.11683)
*Nianyi Lin,Jiajie Zhang,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为边界引导策略优化（BGPO）的内存高效强化学习算法，用于解决扩散大语言模型（dLLMs）中因似然函数不可处理而导致的挑战。BGPO通过最大化基于ELBO的目标的特殊构造下界，实现了线性和等价的性质，使得可以使用更大的MC样本数量，从而提高性能。实验表明，BGPO在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在每个训练步骤中需要对对数似然进行近似，但由于需要保留所有MC样本的前向计算图以进行非线性项的梯度计算，导致显著的内存开销，限制了可行的样本大小，从而影响了RL目标的准确性。

Method: 提出了一种名为边界引导策略优化（BGPO）的内存高效强化学习算法，该算法通过最大化基于ELBO的目标的特殊构造下界来解决扩散大语言模型（dLLMs）中似然函数不可处理的问题。

Result: BGPO能够采用较大的MC样本数量，从而获得更精确的似然近似和改进的RL目标估计，最终提高了性能。实验表明，BGPO在数学问题求解、代码生成和规划任务中显著优于之前的dLLMs强化学习算法。

Conclusion: BGPO significantly outperforms previous RL algorithms for dLLMs in math problem solving, code generation, and planning tasks.

Abstract: A key challenge in applying reinforcement learning (RL) to diffusion large
language models (dLLMs) lies in the intractability of their likelihood
functions, which are essential for the RL objective, necessitating
corresponding approximation in each training step. While existing methods
approximate the log-likelihoods by their evidence lower bounds (ELBOs) via
customized Monte Carlo (MC) sampling, the forward computational graphs of all
MC samples need to be retained for the gradient computation of non-linear terms
in the RL objective, resulting in significant memory overhead. This constraint
restricts feasible sample sizes, leading to imprecise likelihood approximations
and ultimately distorting the RL objective. To overcome this limitation, we
propose \emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient
RL algorithm that maximizes a specially constructed lower bound of the
ELBO-based objective. This lower bound is carefully designed to satisfy two key
properties: (1) Linearity: it is formulated in a linear sum where each term
depends only on a single MC sample, thereby enabling gradient accumulation
across samples and ensuring constant memory usage; (2) Equivalence: Both the
value and gradient of this lower bound are equal to those of the ELBO-based
objective in on-policy training, making it also an effective approximation for
the original RL objective. These properties allow BGPO to adopt a large MC
sample size, resulting in more accurate likelihood approximations and improved
RL objective estimation, which in turn leads to enhanced performance.
Experiments show that BGPO significantly outperforms previous RL algorithms for
dLLMs in math problem solving, code generation, and planning tasks.

</details>


### [189] [QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs](https://arxiv.org/abs/2510.11696)
*Wei Huang,Yi Ge,Shuai Yang,Yicheng Xiao,Huizi Mao,Yujun Lin,Hanrong Ye,Sifei Liu,Ka Chun Cheung,Hongxu Yin,Yao Lu,Xiaojuan Qi,Song Han,Yukang Chen*

Main category: cs.LG

TL;DR: QeRL 是一种结合量化和强化学习的框架，提高了 LLMs 的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: RL 对于 LLMs 的推理能力至关重要，但资源密集，需要大量的 GPU 内存和长时间的 rollout。

Method: QeRL 结合了 NVFP4 量化和低秩适应 (LoRA)，并通过自适应量化噪声 (AQN) 机制优化探索。

Result: QeRL 在 rollout 阶段实现了超过 1.5 倍的速度提升，并在单个 H100 80GB GPU 上实现了 32B LLM 的 RL 训练。

Conclusion: QeRL 是一种高效且有效的用于 LLMs 的 RL 训练框架。

Abstract: We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for
large language models (LLMs). While RL is essential for LLMs' reasoning
capabilities, it is resource-intensive, requiring substantial GPU memory and
long rollout durations. QeRL addresses these issues by combining NVFP4
quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL
while reducing memory overhead. Beyond efficiency, our findings show that
quantization noise increases policy entropy, enhancing exploration, and
enabling the discovery of better strategies during RL. To further optimize
exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,
which dynamically adjusts noise during training. Experiments demonstrate that
QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is
the first framework to enable RL training of a 32B LLM on a single H100 80GB
GPU, while delivering overall speedups for RL training. It also achieves faster
reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while
matching the performance of full-parameter fine-tuning on mathematical
benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These
results establish QeRL as an efficient and effective framework for RL training
in LLMs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [190] [Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation](https://arxiv.org/abs/2510.10676)
*Mukul Lokhande,Tanushree Dewangan,Mohd Sharik Mansoori,Tejas Chaudhari,Akarsh J.,Damayanti Lokhande,Adam Teman,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 该论文介绍了一种名为Bhasha-Rupantarika的轻量级高效多语言翻译系统，通过算法-硬件协同设计适用于资源受限的环境。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的环境中实现实时部署，特别是在使用FPGA加速器的物联网设备中，需要一种轻量级且高效的多语言翻译系统。

Method: 该方法研究了在亚字节精度级别（FP8、INT8、INT4和FP4）上的模型部署，并通过实验验证了其有效性。

Result: 实验结果表明，FP4精度下模型大小减少了4.1倍，推理速度提高了4.2倍，吞吐量增加了66 tokens/s（提高了4.8倍）。此外，FPGA部署展示了LUTs减少1.96倍，FFs减少1.65倍，吞吐量比OPU提高了2.2倍，比HPTA提高了4.6倍。

Conclusion: 该研究提供了一种基于量化感知翻译和硬件效率的可行解决方案，适用于可部署的多语言AI系统。

Abstract: This paper introduces Bhasha-Rupantarika, a light and efficient multilingual
translation system tailored through algorithm-hardware codesign for
resource-limited settings. The method investigates model deployment at
sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental
results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in
inference speed, which correlates with an increased throughput of 66 tokens/s
(improvement by 4.8x). This underscores the importance of ultra-low precision
quantization for real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation covers
bidirectional translation between Indian and international languages,
showcasing its adaptability in low-resource linguistic contexts. The FPGA
deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,
resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x
enhancement compared to HPTA. Overall, the evaluation provides a viable
solution based on quantisation-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for
reproducibility are publicly available, facilitating rapid integration and
further development by researchers.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [191] [Task-Aware Resolution Optimization for Visual Large Language Models](https://arxiv.org/abs/2510.09822)
*Weiqing Luo,Zhen Tan,Yifan Li,Xinyu Zhao,Kwonjoon Lee,Behzad Dariush,Tianlong Chen*

Main category: cs.CV

TL;DR: 本文研究了不同视觉语言任务的分辨率偏好，并提出了一种经验公式和一种新的参数高效微调技术，以提高视觉大语言模型在不同任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉大语言模型（VLLMs）如LLaVA假设下游任务的固定分辨率，这导致性能不佳。

Method: 我们提出了一个经验公式来确定给定视觉语言任务的最佳分辨率，并提出了一种新的参数高效的微调技术，以将预训练的VLLM的视觉输入分辨率扩展到识别出的最佳分辨率。

Result: 通过广泛的实验验证了我们方法的有效性。

Conclusion: 我们的方法在各种视觉语言任务中得到了验证，证明了其有效性。

Abstract: Real-world vision-language applications demand varying levels of perceptual
granularity. However, most existing visual large language models (VLLMs), such
as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to
subpar performance. To address this problem, we first conduct a comprehensive
and pioneering investigation into the resolution preferences of different
vision-language tasks, revealing a correlation between resolution preferences
with image complexity, and uncertainty variance of the VLLM at different image
input resolutions. Building on this insight, we propose an empirical formula to
determine the optimal resolution for a given vision-language task, combining
these two factors. Second, based on rigorous experiments, we propose a novel
parameter-efficient fine-tuning technique to extend the visual input resolution
of pre-trained VLLMs to the identified optimal resolution. Extensive
experiments on various vision-language tasks validate the effectiveness of our
method.

</details>


### [192] [A Survey on Agentic Multimodal Large Language Models](https://arxiv.org/abs/2510.10991)
*Huanjin Yao,Ruifei Zhang,Jiaxing Huang,Jingyi Zhang,Yibo Wang,Bo Fang,Ruolin Zhu,Yongcheng Jing,Shunyu Liu,Guanbin Li,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文对Agentic MLLMs进行了全面的综述，提出了一个概念框架，并编译了相关资源以促进该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 本文受到对Agentic AI及其向AGI的潜在轨迹的兴趣的驱动，旨在提供对Agentic MLLMs的全面综述。

Method: 本文提出了一个概念框架，将Agentic MLLMs组织在三个基本维度上：(i) Agentic内部智能功能作为系统的指挥官，通过推理、反思和记忆实现准确的长周期规划；(ii) Agentic外部工具调用，其中模型主动使用各种外部工具来扩展其解决问题的能力；(iii) Agentic环境交互，进一步将模型置于虚拟或物理环境中，使其能够采取行动、适应策略并在动态现实场景中保持目标导向行为。

Result: 本文编译了开源训练框架、训练和评估数据集，以加速该领域研究，并回顾了Agentic MLLMs的下游应用。

Conclusion: 本文对Agentic MLLMs进行了全面的综述，并概述了该快速发展的领域的未来研究方向。

Abstract: With the recent emergence of revolutionary autonomous agentic systems,
research community is witnessing a significant shift from traditional static,
passive, and domain-specific AI agents toward more dynamic, proactive, and
generalizable agentic AI. Motivated by the growing interest in agentic AI and
its potential trajectory toward AGI, we present a comprehensive survey on
Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we
explore the emerging paradigm of agentic MLLMs, delineating their conceptual
foundations and distinguishing characteristics from conventional MLLM-based
agents. We establish a conceptual framework that organizes agentic MLLMs along
three fundamental dimensions: (i) Agentic internal intelligence functions as
the system's commander, enabling accurate long-horizon planning through
reasoning, reflection, and memory; (ii) Agentic external tool invocation,
whereby models proactively use various external tools to extend their
problem-solving capabilities beyond their intrinsic knowledge; and (iii)
Agentic environment interaction further situates models within virtual or
physical environments, allowing them to take actions, adapt strategies, and
sustain goal-directed behavior in dynamic real-world scenarios. To further
accelerate research in this area for the community, we compile open-source
training frameworks, training and evaluation datasets for developing agentic
MLLMs. Finally, we review the downstream applications of agentic MLLMs and
outline future research directions for this rapidly evolving field. To
continuously track developments in this rapidly evolving field, we will also
actively update a public repository at
https://github.com/HJYao00/Awesome-Agentic-MLLMs.

</details>


### [193] [DocReward: A Document Reward Model for Structuring and Stylizing](https://arxiv.org/abs/2510.11391)
*Junpeng Liu,Yuzhong Zhao,Bowen Cao,Jiayu Ding,Yilin Jia,Tengchao Lv,Yupan Huang,Shaohan Huang,Nan Yang,Li Dong,Lei Cui,Tao Ge,Xun Wang,Huitian Jiao,Sun Mao,FNU Kartik,Si-Qing Chen,Wai Lam,Furu Wei*

Main category: cs.CV

TL;DR: This paper introduces DocReward, a document reward model that evaluates documents based on structure and style, outperforming existing models in accuracy and win rate.


<details>
  <summary>Details</summary>
Motivation: The gap in agentic workflows is due to the absence of suitable reward models to guide them toward producing documents with stronger structural and stylistic quality.

Method: We propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents and train DocReward using the Bradley-Terry loss to score documents.

Result: DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively. It also achieves a significantly higher win rate of 60.8% compared to GPT-5's 37.7% win rate.

Conclusion: DocReward outperforms GPT-4o and GPT-5 in accuracy and achieves a higher win rate in document generation, demonstrating its utility in guiding generation agents toward producing human-preferred documents.

Abstract: Recent advances in agentic workflows have enabled the automation of tasks
such as professional document generation. However, they primarily focus on
textual quality, neglecting visual structure and style, which are crucial for
readability and engagement. This gap arises mainly from the absence of suitable
reward models to guide agentic workflows toward producing documents with
stronger structural and stylistic quality. To address this, we propose
DocReward, a document reward model that evaluates documents based on their
structure and style. We construct a multi-domain dataset DocPair of 117K paired
documents, covering 32 domains and 267 document types, each including a high-
and low-professionalism document with identical content but different structure
and style. This enables the model to evaluate professionalism comprehensively,
and in a textual-quality-agnostic way. DocReward is trained using the
Bradley-Terry loss to score documents, penalizing predictions that contradict
the annotated ranking. To assess the performance of reward models, we create a
test dataset containing document bundles ranked by well-educated human
evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6
and 19.4 percentage points, respectively, demonstrating its superiority over
baselines. In an extrinsic evaluation of document generation, DocReward
achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%
win rate, demonstrating its utility in guiding generation agents toward
producing human-preferred documents.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [194] [CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms](https://arxiv.org/abs/2510.10095)
*Peiyuan Gong,Feiran Zhu,Yaqi Yin,Chenglei Dai,Chao Zhang,Kai Zheng,Wentian Bao,Jiaxin Mao,Yi Zhang*

Main category: cs.IR

TL;DR: CardRewriter is an LLM-based framework that enhances long-tail query rewriting on short-video platforms by incorporating domain-specific knowledge, leading to improved user experience and engagement.


<details>
  <summary>Details</summary>
Motivation: User queries on short-video platforms often suffer from spelling errors, incomplete phrasing, and ambiguous intent, resulting in mismatches between user expectations and retrieved results. LLMs struggle on these platforms due to proprietary content outside their training distribution.

Method: We introduce CardRewriter, an LLM-based framework that incorporates domain-specific knowledge to enhance long-tail query rewriting. We optimize it using a two-stage training pipeline: supervised fine-tuning followed by group relative policy optimization, with a tailored reward system balancing query relevance and retrieval effectiveness.

Result: Offline experiments show that CardRewriter substantially improves rewriting quality for queries targeting proprietary content. Online A/B testing confirms significant gains in long-view rate (LVR) and click-through rate (CTR), along with a notable reduction in initiative query reformulation rate (IQRR).

Conclusion: CardRewriter has been deployed on Kuaishou, one of China's largest short-video platforms, serving hundreds of millions of users daily.

Abstract: Short-video platforms have rapidly become a new generation of information
retrieval systems, where users formulate queries to access desired videos.
However, user queries, especially long-tail ones, often suffer from spelling
errors, incomplete phrasing, and ambiguous intent, resulting in mismatches
between user expectations and retrieved results. While large language models
(LLMs) have shown success in long-tail query rewriting within e-commerce, they
struggle on short-video platforms, where proprietary content such as short
videos, live streams, micro dramas, and user social networks falls outside
their training distribution. To address this challenge, we introduce
\textbf{CardRewriter}, an LLM-based framework that incorporates domain-specific
knowledge to enhance long-tail query rewriting. For each query, our method
aggregates multi-source knowledge relevant to the query and summarizes it into
an informative and query-relevant knowledge card. This card then guides the LLM
to better capture user intent and produce more effective query rewrites. We
optimize CardRewriter using a two-stage training pipeline: supervised
fine-tuning followed by group relative policy optimization, with a tailored
reward system balancing query relevance and retrieval effectiveness. Offline
experiments show that CardRewriter substantially improves rewriting quality for
queries targeting proprietary content. Online A/B testing further confirms
significant gains in long-view rate (LVR) and click-through rate (CTR), along
with a notable reduction in initiative query reformulation rate (IQRR). Since
September 2025, CardRewriter has been deployed on Kuaishou, one of China's
largest short-video platforms, serving hundreds of millions of users daily.

</details>


### [195] [QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking](https://arxiv.org/abs/2510.11589)
*Shubham Chatterjee,Jeff Dalton*

Main category: cs.IR

TL;DR: QDER 是一种神经重新排序模型，通过将知识图谱语义整合到多向量模型中，实现了对查询-文档关系的精确建模，并在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的神经信息检索方法分为实体导向方法和多向量模型，但两者之间缺乏有效的整合。

Method: QDER 通过将知识图谱语义整合到多向量模型中，统一了实体导向方法和多向量模型。它采用“晚期聚合”方法，在最终评分阶段才进行聚合，保持了单个标记和实体表示。

Result: 在五个标准基准测试中，QDER 在 TREC Robust 2004 上的 nDCG@20 指标比最强基线提高了 36%，并在其他数据集上也取得了类似的改进。

Conclusion: QDER 设置了未来实体感知检索工作的基础，特别是在处理困难查询方面表现出色。

Abstract: Neural IR has advanced through two distinct paths: entity-oriented approaches
leveraging knowledge graphs and multi-vector models capturing fine-grained
semantics. We introduce QDER, a neural re-ranking model that unifies these
approaches by integrating knowledge graph semantics into a multi-vector model.
QDER's key innovation lies in its modeling of query-document relationships:
rather than computing similarity scores on aggregated embeddings, we maintain
individual token and entity representations throughout the ranking process,
performing aggregation only at the final scoring stage - an approach we call
"late aggregation." We first transform these fine-grained representations
through learned attention patterns, then apply carefully chosen mathematical
operations for precise matches. Experiments across five standard benchmarks
show that QDER achieves significant performance gains, with improvements of 36%
in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar
improvements on other datasets. QDER particularly excels on difficult queries,
achieving an nDCG@20 of 0.70 where traditional approaches fail completely
(nDCG@20 = 0.0), setting a foundation for future work in entity-aware
retrieval.

</details>


### [196] [REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural Re-Ranking](https://arxiv.org/abs/2510.11592)
*Shubham Chatterjee*

Main category: cs.IR

TL;DR: REGENT is a neural re-ranking model that mimics human-like understanding by using entities as a 'semantic skeleton' to guide attention. It integrates relevance guidance directly into the attention mechanism, combining fine-grained lexical matching with high-level semantic reasoning. REGENT achieves new state-of-the-art performance in three challenging datasets, providing up to 108% improvement over BM25 and consistently outperforming strong baselines including ColBERT and RankVicuna.


<details>
  <summary>Details</summary>
Motivation: Current neural re-rankers often struggle with complex information needs and long, content-rich documents. The fundamental issue is not computational--it is intelligent content selection: identifying what matters in lengthy, multi-faceted texts. While humans naturally anchor their understanding around key entities and concepts, neural models process text within rigid token windows, treating all interactions as equally important and missing critical semantic signals.

Method: REGENT integrates relevance guidance directly into the attention mechanism, combining fine-grained lexical matching with high-level semantic reasoning. This relevance-guided attention enables the model to focus on conceptually important content while maintaining sensitivity to precise term matches.

Result: REGENT achieves new state-of-the-art performance in three challenging datasets, providing up to 108% improvement over BM25 and consistently outperforming strong baselines including ColBERT and RankVicuna.

Conclusion: REGENT achieves new state-of-the-art performance in three challenging datasets, providing up to 108% improvement over BM25 and consistently outperforming strong baselines including ColBERT and RankVicuna. To our knowledge, this is the first work to successfully integrate entity semantics directly into neural attention, establishing a new paradigm for entity-aware information retrieval.

Abstract: Current neural re-rankers often struggle with complex information needs and
long, content-rich documents. The fundamental issue is not computational--it is
intelligent content selection: identifying what matters in lengthy,
multi-faceted texts. While humans naturally anchor their understanding around
key entities and concepts, neural models process text within rigid token
windows, treating all interactions as equally important and missing critical
semantic signals. We introduce REGENT, a neural re-ranking model that mimics
human-like understanding by using entities as a "semantic skeleton" to guide
attention. REGENT integrates relevance guidance directly into the attention
mechanism, combining fine-grained lexical matching with high-level semantic
reasoning. This relevance-guided attention enables the model to focus on
conceptually important content while maintaining sensitivity to precise term
matches. REGENT achieves new state-of-the-art performance in three challenging
datasets, providing up to 108% improvement over BM25 and consistently
outperforming strong baselines including ColBERT and RankVicuna. To our
knowledge, this is the first work to successfully integrate entity semantics
directly into neural attention, establishing a new paradigm for entity-aware
information retrieval.

</details>


### [197] [FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection](https://arxiv.org/abs/2510.11654)
*Daniel Berhane Araya,Duoduo Liao*

Main category: cs.IR

TL;DR: FinVet is a novel multi-agent framework that integrates RAG pipelines with external fact-checking to improve the accuracy and transparency of financial misinformation detection.


<details>
  <summary>Details</summary>
Motivation: Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources.

Method: FinVet is a multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. It employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence.

Result: Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline and 37% improvement over standalone RAG approaches.

Conclusion: FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline and 37% improvement over standalone RAG approaches.

Abstract: Financial markets face growing threats from misinformation that can trigger
billions in losses in minutes. Most existing approaches lack transparency in
their decision-making and provide limited attribution to credible sources. We
introduce FinVet, a novel multi-agent framework that integrates two
Retrieval-Augmented Generation (RAG) pipelines with external fact-checking
through a confidence-weighted voting mechanism. FinVet employs adaptive
three-tier processing that dynamically adjusts verification strategies based on
retrieval confidence, from direct metadata extraction to hybrid reasoning to
full model-based analysis. Unlike existing methods, FinVet provides
evidence-backed verdicts, source attribution, confidence scores, and explicit
uncertainty flags when evidence is insufficient. Experimental evaluation on the
FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a
10.4% improvement over the best individual pipeline (fact-check pipeline) and
37% improvement over standalone RAG approaches.

</details>

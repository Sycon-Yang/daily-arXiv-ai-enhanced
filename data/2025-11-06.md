<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Cache Mechanism for Agent RAG Systems](https://arxiv.org/abs/2511.02919)
*Shuhang Lin,Zhencan Peng,Lingyao Li,Xiao Lin,Xi Zhu,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ARC is a new caching framework that improves the efficiency and effectiveness of RAG-powered LLM agents by dynamically managing small, high-value corpora.


<details>
  <summary>Details</summary>
Motivation: Agent-level cache management, particularly constructing, maintaining, and updating a compact, relevant corpus dynamically tailored to each agent's need, remains underexplored despite RAG's success in improving agent performance.

Method: ARC (Agent RAG Cache Mechanism), a novel, annotation-free caching framework that dynamically manages small, high-value corpora for each agent by synthesizing historical query distribution patterns with the intrinsic geometry of cached items in the embedding space.

Result: ARC reduces storage requirements to 0.015% of the original corpus while offering up to 79.8% has-answer rate and reducing average retrieval latency by 80%.

Conclusion: ARC can drastically enhance efficiency and effectiveness in RAG-powered LLM agents.

Abstract: Recent advances in Large Language Model (LLM)-based agents have been
propelled by Retrieval-Augmented Generation (RAG), which grants the models
access to vast external knowledge bases. Despite RAG's success in improving
agent performance, agent-level cache management, particularly constructing,
maintaining, and updating a compact, relevant corpus dynamically tailored to
each agent's need, remains underexplored. Therefore, we introduce ARC (Agent
RAG Cache Mechanism), a novel, annotation-free caching framework that
dynamically manages small, high-value corpora for each agent. By synthesizing
historical query distribution patterns with the intrinsic geometry of cached
items in the embedding space, ARC automatically maintains a high-relevance
cache. With comprehensive experiments on three retrieval datasets, our
experimental results demonstrate that ARC reduces storage requirements to
0.015% of the original corpus while offering up to 79.8% has-answer rate and
reducing average retrieval latency by 80%. Our results demonstrate that ARC can
drastically enhance efficiency and effectiveness in RAG-powered LLM agents.

</details>


### [2] [Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model](https://arxiv.org/abs/2511.02958)
*Cristian García-Romero,Miquel Esplà-Gomis,Felipe Sánchez-Martínez*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，利用多语言MT模型的内部表示来区分人类和机器翻译的句子，实验结果显示该方法在非英语语对上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代机器翻译系统依赖于从互联网上收集的大规模平行语料库，但最近的证据表明，这些文本中有一大部分是机器生成的翻译，过度依赖这种合成内容会显著降低翻译质量。因此，过滤掉非人类翻译成为构建高质量MT系统的必要预处理步骤。

Method: 我们提出了一种新方法，直接利用替代多语言MT模型的内部表示来区分人类和机器翻译的句子。

Result: 实验结果表明，我们的方法优于当前最先进的技术，特别是在非英语语言对上，准确率至少提高了5个百分点。

Conclusion: 我们的方法在非英语语对上优于当前最先进的技术，准确率至少提高了5个百分点。

Abstract: Modern machine translation (MT) systems depend on large parallel corpora,
often collected from the Internet. However, recent evidence indicates that (i)
a substantial portion of these texts are machine-generated translations, and
(ii) an overreliance on such synthetic content in training data can
significantly degrade translation quality. As a result, filtering out non-human
translations is becoming an essential pre-processing step in building
high-quality MT systems. In this work, we propose a novel approach that
directly exploits the internal representations of a surrogate multilingual MT
model to distinguish between human and machine-translated sentences.
Experimental results show that our method outperforms current state-of-the-art
techniques, particularly for non-English language pairs, achieving gains of at
least 5 percentage points of accuracy.

</details>


### [3] [LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation](https://arxiv.org/abs/2511.03001)
*Gyeom Hwangbo,Hyungjoo Chae,Minseok Kang,Hyeonjong Ju,Soohyun Oh,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 本文提出了LEGO-Eval评估框架和LEGO-Bench基准，以更准确地评估3D场景生成与细粒度指令的对齐情况，并揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于现有评估方法无法可靠评估细粒度指令与生成场景之间的对齐，因此需要一种更准确的评估框架。

Method: 引入LEGO-Eval评估框架和LEGO-Bench基准，用于评估场景-指令对齐和生成方法的性能。

Result: LEGO-Eval在评估场景-指令对齐方面表现优于VLM-as-a-judge，而当前生成方法在完全符合细粒度指令的场景生成中成功率最高仅为10%。

Conclusion: 实验表明，LEGO-Eval在评估场景-指令对齐方面优于VLM-as-a-judge，F1分数提高了0.41。基准测试显示当前生成方法存在显著限制。

Abstract: Despite recent progress in using Large Language Models (LLMs) for
automatically generating 3D scenes, generated scenes often lack realistic
spatial layouts and object attributes found in real-world environments. As this
problem stems from insufficiently detailed, coarse-grained instructions,
advancing 3D scene synthesis guided by more detailed, fine-grained instructions
that reflect real-world environments becomes crucial. Without such realistic
scenes, training embodied agents in unrealistic environments can lead them to
learn priors that diverge significantly from real-world physics and semantics,
degrading their performance when deployed. Thus, verifying the alignment
between the fine-grained instruction and the generated scene is essential for
effective learning. However, current evaluation methods, such as CLIPScore and
vision-language models (VLMs), often fail to reliably assess such alignment.
This shortcoming arises primarily from their shallow understanding of 3D
scenes, which often leads to improperly grounded scene components. To address
this, we introduce LEGO-Eval, an evaluation framework equipped with diverse
tools designed to explicitly ground scene components, enabling more accurate
alignment assessments. We also present LEGO-Bench, a benchmark of detailed
instructions that specify complex layouts and attributes of real-world
environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge
by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with
LEGO-Bench reveals significant limitations in current generation methods.
Across all evaluated approaches, success rates reached at most 10% in
generating scenes that fully align with fine-grained instructions.

</details>


### [4] [Targeted Error Correction in Knowledge Distillation: Small Language Models Surpass GPT](https://arxiv.org/abs/2511.03005)
*Hee-Jin Lee,Zhen Guo,Luchao Jin,Morteza Moazami Goudarzi*

Main category: cs.CL

TL;DR: ARF管道通过分析、修订和微调过程，使小型开源语言模型在客户服务摘要任务中超越大型专有模型，提高了成本效率和数据隐私。


<details>
  <summary>Details</summary>
Motivation: 旨在让较小的开源语言模型（LLMs）在客户服务摘要任务中超越较大的专有模型。

Method: ARF管道首先分析并分类教师模型（GPT-3.5）生成摘要中的常见错误，然后使用紧凑的编辑器模型（Llama 3.1 70B）进行有针对性的修订，以生成高质量的训练数据。然后在这些精炼数据上微调较小的学生模型（Llama 3.1 8B）。

Result: 在精炼数据上微调较小的学生模型（Llama 3.1 8B）在摘要任务中表现优于GPT-3.5。

Conclusion: ARF管道在成本效率和数据隐私方面有所改进，同时保持了竞争性的准确性，展示了增强开源LLM的通用框架。

Abstract: We introduce an Analyze-Revise-Finetune (ARF) pipeline that enables smaller
open-source language models (LLMs) to surpass substantially larger proprietary
models in customer service summarization tasks. The pipeline first analyzes and
categorizes common errors in summaries produced by a teacher model (GPT-3.5),
then performs a targeted revision using a compact editor model (Llama 3.1 70B)
to generate high-quality, refined training data. Fine-tuning a smaller student
model (Llama 3.1 8B) on this refined data resulted in superior summarization
performance compared to GPT-3.5. The ARF pipeline improves cost efficiency and
data privacy while maintaining competitive accuracy, illustrating a
generalizable framework for enhancing open-source LLMs across diverse
downstream applications.

</details>


### [5] [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034)
*Yan Cathy Hua,Paul Denny,Jörg Wicker,Katerina Taškova*

Main category: cs.CL

TL;DR: 本文提出了一个新的评估方法，研究了小解码器生成语言模型在低资源领域的应用，并发布了教育评论ABSA资源集。


<details>
  <summary>Details</summary>
Motivation: 当前ABSA研究和资源主要集中在商业领域，而教育和医疗等高需求但低资源领域的需求未得到满足。此外，传统的基于精确匹配的评估方法对于ABSA任务过于严格，不利于生成模型的性能评估。

Method: 本文提出了一个名为FTS-OBP的新评估方法，并对小解码器生成语言模型进行了系统研究，同时提出了多任务微调策略。

Result: 本文提出的FTS-OBP方法能够适应实际提取边界的变化，同时保持与传统指标的强相关性。小解码器生成语言模型在仅使用200-1000个例子的情况下，表现优于专有大模型并接近基准结果。此外，本文发布了第一个公开的教育评论ABSA资源集。

Conclusion: 本文通过三个贡献解决了低资源领域中ABSA研究的不足，包括提出一种新的评估方法、对小解码器生成语言模型进行研究以及发布教育评论ABSA资源。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining
approach that identifies and classifies opinions associated with specific
entities (aspects) or their categories within a sentence. Despite its rapid
growth and broad potential, ABSA research and resources remain concentrated in
commercial domains, leaving analytical needs unmet in high-demand yet
low-resource areas such as education and healthcare. Domain adaptation
challenges and most existing methods' reliance on resource-intensive
in-training knowledge injection further hinder progress in these areas.
Moreover, traditional evaluation methods based on exact matches are overly
rigid for ABSA tasks, penalising any boundary variations which may misrepresent
the performance of generative models. This work addresses these gaps through
three contributions: 1) We propose a novel evaluation method, Flexible Text
Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates
realistic extraction boundary variations while maintaining strong correlation
with traditional metrics and offering fine-grained diagnostics. 2) We present
the first ABSA study of small decoder-only generative language models (SLMs;
<7B parameters), examining resource lower bounds via a case study in education
review ABSA. We systematically explore data-free (in-context learning and
weight merging) and data-light fine-tuning methods, and propose a multitask
fine-tuning strategy that significantly enhances SLM performance, enabling
1.5-3.8 B models to surpass proprietary large models and approach benchmark
results with only 200-1,000 examples on a single GPU. 3) We release the first
public set of education review ABSA resources to support future research in
low-resource domains.

</details>


### [6] [ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial Risk of Bias Assessment](https://arxiv.org/abs/2511.03048)
*Anthony Hevia,Sanjana Chintalapati,Veronica Ka Wai Lai,Thanh Tam Nguyen,Wai-Tat Wong,Terry Klassen,Lucy Lu Wang*

Main category: cs.CL

TL;DR: ROBOTO2 is an open-source platform for LLM-assisted ROB2 assessment of clinical trials, which streamlines the annotation process and provides a benchmark dataset for future research.


<details>
  <summary>Details</summary>
Motivation: The traditional ROB2 annotation process is labor-intensive, and there is a need for a more efficient and accurate method for assessing the risk of bias in clinical trials.

Method: ROBOTO2 uses an interactive interface that combines PDF parsing, retrieval-augmented LLM prompting, and human-in-the-loop review to streamline the ROB2 annotation process.

Result: ROBOTO2 has been released as an open-source platform with a dataset of 521 pediatric clinical trial reports, annotated using both manually and LLM-assisted methods. The platform has been used to benchmark ROB2 performance for 4 LLMs.

Conclusion: ROBOTO2 is a valuable tool for streamlining the ROB2 annotation process and provides a benchmark dataset for future research.

Abstract: We present ROBOTO2, an open-source, web-based platform for large language
model (LLM)-assisted risk of bias (ROB) assessment of clinical trials. ROBOTO2
streamlines the traditionally labor-intensive ROB v2 (ROB2) annotation process
via an interactive interface that combines PDF parsing, retrieval-augmented LLM
prompting, and human-in-the-loop review. Users can upload clinical trial
reports, receive preliminary answers and supporting evidence for ROB2 signaling
questions, and provide real-time feedback or corrections to system suggestions.
ROBOTO2 is publicly available at https://roboto2.vercel.app/, with code and
data released to foster reproducibility and adoption. We construct and release
a dataset of 521 pediatric clinical trial reports (8954 signaling questions
with 1202 evidence passages), annotated using both manually and LLM-assisted
methods, serving as a benchmark and enabling future research. Using this
dataset, we benchmark ROB2 performance for 4 LLMs and provide an analysis into
current model capabilities and ongoing challenges in automating this critical
aspect of systematic review.

</details>


### [7] [Reading Between the Lines: The One-Sided Conversation Problem](https://arxiv.org/abs/2511.03056)
*Victoria Ebert,Rishabh Singh,Tuochao Chen,Noah A. Smith,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 本文探讨了单边对话问题（1SC），并研究了两个任务：重建缺失说话者的回合和生成摘要。结果表明，大型模型在提示下可以生成有希望的重建，而较小的模型需要微调。同时，高质量的摘要可以在不重建缺失回合的情况下生成。


<details>
  <summary>Details</summary>
Motivation: 对话AI在许多现实场景中受到限制，其中只能记录对话的一方，例如远程医疗、呼叫中心和智能眼镜。

Method: 本文研究了两个任务：(1) 重建缺失说话者的回合以用于实时用例，以及(2) 从单边转录本生成摘要。评估了提示和微调模型在MultiWOZ、DailyDialog和Candor上的表现，并使用人类A/B测试和LLM-as-a-judge指标进行了评估。

Result: 结果显示，访问未来一个回合的信息和话语长度可以提高重建效果，占位符提示有助于减轻幻觉，而大型模型在提示下可以生成有希望的重建，但较小的模型需要微调。此外，无需重建缺失的回合就可以生成高质量的摘要。

Conclusion: 本文提出了1SC作为一项新的挑战，并报告了有希望的结果，标志着向隐私意识的对话AI迈出了一步。

Abstract: Conversational AI is constrained in many real-world settings where only one
side of a dialogue can be recorded, such as telemedicine, call centers, and
smart glasses. We formalize this as the one-sided conversation problem (1SC):
inferring and learning from one side of a conversation. We study two tasks: (1)
reconstructing the missing speaker's turns for real-time use cases, and (2)
generating summaries from one-sided transcripts. Evaluating prompting and
finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B
testing and LLM-as-a-judge metrics, we find that access to one future turn and
information about utterance length improves reconstruction, placeholder
prompting helps to mitigate hallucination, and while large models generate
promising reconstructions with prompting, smaller models require finetuning.
Further, high-quality summaries can be generated without reconstructing missing
turns. We present 1SC as a novel challenge and report promising results that
mark a step toward privacy-aware conversational AI.

</details>


### [8] [PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech](https://arxiv.org/abs/2511.03080)
*Michel Wong,Ali Alshehri,Sophia Kao,Haotian He*

Main category: cs.CL

TL;DR: 本文提出了PolyNorm，一种基于提示的文本归一化方法，利用大型语言模型来减少对人工编写的规则的依赖，并通过最小的人工干预实现更广泛的语言适用性。此外，还提出了一种语言无关的自动数据整理和评估流程，以支持跨多种语言的可扩展实验。实验结果表明，在八种语言中，与基于生产级的系统相比，词错误率（WER）有显著降低。为了支持进一步的研究，我们发布了PolyNorm-Benchmark，一个多语言数据集，涵盖了各种文本归一化现象。


<details>
  <summary>Details</summary>
Motivation: 传统的文本归一化系统虽然可以表现出高准确性，但需要大量的工程努力，难以扩展，并且在低资源环境中面临语言覆盖的挑战。因此，本文旨在提出一种新的方法，以减少对人工编写的规则的依赖，并实现更广泛的语言适用性。

Method: 本文提出了PolyNorm，这是一种基于提示的文本归一化方法，利用大型语言模型来减少对人工编写的规则的依赖，并通过最小的人工干预实现更广泛的语言适用性。此外，还提出了一种语言无关的自动数据整理和评估流程，以支持跨多种语言的可扩展实验。

Result: 实验结果表明，在八种语言中，与基于生产级的系统相比，词错误率（WER）有显著降低。此外，还发布了PolyNorm-Benchmark，一个多语言数据集，涵盖了各种文本归一化现象。

Conclusion: 本文提出了PolyNorm，一种基于提示的文本归一化方法，利用大型语言模型来减少对人工编写的规则的依赖，并通过最小的人工干预实现更广泛的语言适用性。此外，还提出了一种语言无关的自动数据整理和评估流程，以支持跨多种语言的可扩展实验。实验结果表明，在八种语言中，与基于生产级的系统相比，词错误率（WER）有显著降低。为了支持进一步的研究，我们发布了PolyNorm-Benchmark，一个多语言数据集，涵盖了各种文本归一化现象。

Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS)
systems, converting written forms into their canonical spoken equivalents.
Traditional TN systems can exhibit high accuracy, but involve substantial
engineering effort, are difficult to scale, and pose challenges to language
coverage, particularly in low-resource settings. We propose PolyNorm, a
prompt-based approach to TN using Large Language Models (LLMs), aiming to
reduce the reliance on manually crafted rules and enable broader linguistic
applicability with minimal human intervention. Additionally, we present a
language-agnostic pipeline for automatic data curation and evaluation, designed
to facilitate scalable experimentation across diverse languages. Experiments
across eight languages show consistent reductions in the word error rate (WER)
compared to a production-grade-based system. To support further research, we
release PolyNorm-Benchmark, a multilingual data set covering a diverse range of
text normalization phenomena.

</details>


### [9] [A Computational Approach to Analyzing Disrupted Language in Schizophrenia: Integrating Surprisal and Coherence Measures](https://arxiv.org/abs/2511.03089)
*Gowtham Premananth,Carol Espy-Wilson*

Main category: cs.CL

TL;DR: 本研究探讨了精神分裂症患者语言障碍如何通过计算语言学指标（如意外性和语义连贯性）进行表征，并发现这些指标随症状严重程度变化。


<details>
  <summary>Details</summary>
Motivation: 语言障碍是精神分裂症症状的一个已知影响，它们可能反映潜在的认知障碍，并有可能作为客观指标用于症状严重程度和诊断。

Method: 使用计算模型计算语言的意外性和语义连贯性，以研究精神分裂症患者和健康对照组之间的差异。

Result: 研究发现，基于这些语言测量方法，精神分裂症患者的语言障碍会随着症状严重程度的变化而变化。

Conclusion: 语言障碍可以作为精神分裂症症状严重程度和诊断的客观指标。

Abstract: Language disruptions are one of the well-known effects of schizophrenia
symptoms. They are often manifested as disorganized speech and impaired
discourse coherence. These abnormalities in spontaneous language production
reflect underlying cognitive disturbances and have the potential to serve as
objective markers for symptom severity and diagnosis of schizophrenia. This
study focuses on how these language disruptions can be characterized in terms
of two computational linguistic measures: surprisal and semantic coherence. By
computing surprisal and semantic coherence of language using computational
models, this study investigates how they differ between subjects with
schizophrenia and healthy controls. Furthermore, this study provides further
insight into how language disruptions in terms of these linguistic measures
change with varying degrees of schizophrenia symptom severity.

</details>


### [10] [CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic](https://arxiv.org/abs/2511.03102)
*Saad Mankarious,Ayah Zirikly*

Main category: cs.CL

TL;DR: 本文介绍了CARMA，这是第一个自动标注的大规模阿拉伯语Reddit帖子数据集，用于心理健康检测，并展示了其在阿拉伯语等代表性不足语言中的潜力。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍影响数百万人，但早期检测仍然是一个重大挑战，特别是在阿拉伯语人口中，由于资源有限和文化污名，心理健康讨论往往被禁止。虽然大量研究集中在英语语言心理健康检测上，但阿拉伯语仍显著未被探索，部分原因是标注数据集的稀缺。

Method: 我们进行了定性和定量分析，比较了用户之间的词汇和语义差异，并使用多种模型（从浅层分类器到大型语言模型）进行了分类实验。

Result: 我们展示了CARMA数据集在规模和多样性方面超越了现有资源，并提供了关于特定心理健康状况的语言标记的见解。

Conclusion: 我们的结果突显了在阿拉伯语等代表性不足的语言中推进心理健康检测的潜力。

Abstract: Mental health disorders affect millions worldwide, yet early detection
remains a major challenge, particularly for Arabic-speaking populations where
resources are limited and mental health discourse is often discouraged due to
cultural stigma. While substantial research has focused on English-language
mental health detection, Arabic remains significantly underexplored, partly due
to the scarcity of annotated datasets. We present CARMA, the first
automatically annotated large-scale dataset of Arabic Reddit posts. The dataset
encompasses six mental health conditions, such as Anxiety, Autism, and
Depression, and a control group. CARMA surpasses existing resources in both
scale and diversity. We conduct qualitative and quantitative analyses of
lexical and semantic differences between users, providing insights into the
linguistic markers of specific mental health conditions. To demonstrate the
dataset's potential for further mental health analysis, we perform
classification experiments using a range of models, from shallow classifiers to
large language models. Our results highlight the promise of advancing mental
health detection in underrepresented languages such as Arabic.

</details>


### [11] [Control Barrier Function for Aligning Large Language Models](https://arxiv.org/abs/2511.03121)
*Yuya Miyaoka,Masaki Inoue*

Main category: cs.CL

TL;DR: 本文提出了一种基于控制的框架，利用控制屏障函数确保生成符合用户期望的文本。


<details>
  <summary>Details</summary>
Motivation: 为了确保生成的文本符合用户期望，需要一种有效的方法来对齐大型语言模型。

Method: 该框架利用控制屏障函数（CBF）安全过滤器来干预从基线LLM生成的预测标记，以确保文本生成符合用户需求。

Result: 该框架具有两个显著优势：它是附加型的，可以在不微调基线LLM的情况下用于对齐目的；如果存在关于所需对齐的评估模型，可以直接应用于过滤器设计。

Conclusion: 该框架为对齐大型语言模型提供了一种基于控制的解决方案，能够生成符合用户期望的文本。

Abstract: This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the CBF safety
filter to the predicted token generated from the baseline LLM, to intervene in
the generated text. The safety filter includes two significant advantages: this
safety filter is an add-on type, allowing it to be used for alignment purposes
without fine-tuning the baseline LLM, and if there is an evaluation model
regarding the desired alignment, it can be directly applied to the filter
design. The overall text-generation system is implemented with open-source
language models, aiming to generate positive text.

</details>


### [12] [MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity](https://arxiv.org/abs/2511.03146)
*Kaiyuan Zhang,Chenghao Yang,Zhoufutu Wen,Sihang Yuan,Qiuyue Wang,Chaoyi Huang,Guosheng Zhu,He Wang,Huawenyu Lu,Jianing Wen,Jianpeng Jiao,Lishu Luo,Longxiang Liu,Sijin Wu,Xiaolei Zhu,Xuanliang Zhang,Ge Zhang,Yi Lin,Guang Shi,Chaoyou Fu,Wenhao Huang*

Main category: cs.CL

TL;DR: 本文介绍了MME-CC基准，用于评估多模态大语言模型的认知能力，并发现封闭源代码模型在整体表现上优于开放源代码模型，但空间和几何推理仍较弱。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准要么过度强调文本推理，要么未能系统地捕捉以视觉为中心的认知行为，导致MLLMs的认知能力评估不足。

Method: 引入了MME-CC基准，组织了11个代表性推理任务，并对16个代表性的MLLMs进行了广泛实验。

Result: 封闭源代码模型目前总体领先，而空间和几何推理仍然较弱，同时发现了常见的错误模式，包括方向错误、脆弱的跨视图身份持续性以及对反事实指令的不良遵循。

Conclusion: 本文希望推动将多模态大语言模型的认知能力作为评估和模型设计的核心。

Abstract: As reasoning models scale rapidly, the essential role of multimodality in
human cognition has come into sharp relief, driving a growing need to probe
vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either
overemphasize textual reasoning or fall short of systematically capturing
vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs
insufficiently assessed. To address this limitation, we introduce MME-CC
(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded
benchmark that organizes 11 representative reasoning tasks into three
fundamental categories of visual information: spatial, geometric, and
knowledge-based reasoning, and provides fine-grained analyses of MLLMs'
cognitive capacity across these dimensions. Based on MME-CC, we conduct
extensive experiments over 16 representative MLLMs. Our study reveals that
closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.
30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak
(less than or equal to 30%). We further identify common error patterns,
including orientation mistakes, fragile cross-view identity persistence, and
poor adherence to counterfactual instructions, and observe that
Chain-of-Thought typically follows a three-stage process (extract -> reason ->
verify) with heavy reliance on visual extraction. We hope this work catalyzes a
shift toward treating the cognitive capacity of MLLMs as central to both
evaluation and model design.

</details>


### [13] [Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment](https://arxiv.org/abs/2511.03152)
*Srishti Yadav,Jasmina Gajcin,Erik Miehling,Elizabeth Daly*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的利益相关者风险评估框架，通过生成可解释的政策来展示不同利益相关者对同一风险的看法，并通过交互式可视化揭示冲突的来源，从而提高透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 了解不同利益相关者如何感知AI系统中的风险对于其负责任的部署至关重要。

Method: 本文提出了一个基于利益相关者的风险评估框架，使用LLM作为裁判来预测和解释风险。利用Risk Atlas Nexus和GloVE解释方法，我们的框架生成了针对利益相关者的可解释政策，展示了不同利益相关者对同一风险的看法一致或不一致的情况。

Result: 我们的结果表明，利益相关者的观点显著影响风险感知和冲突模式。

Conclusion: 我们的工作强调了这些利益相关者意识解释的重要性，以使基于LLM的评估更加透明、可解释，并与以人为本的AI治理目标保持一致。

Abstract: Understanding how different stakeholders perceive risks in AI systems is
essential for their responsible deployment. This paper presents a framework for
stakeholder-grounded risk assessment by using LLMs, acting as judges to predict
and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our
framework generates stakeholder-specific, interpretable policies that shows how
different stakeholders agree or disagree about the same risks. We demonstrate
our method using three real-world AI use cases of medical AI, autonomous
vehicles, and fraud detection domain. We further propose an interactive
visualization that reveals how and why conflicts emerge across stakeholder
perspectives, enhancing transparency in conflict reasoning. Our results show
that stakeholder perspectives significantly influence risk perception and
conflict patterns. Our work emphasizes the importance of these
stakeholder-aware explanations needed to make LLM-based evaluations more
transparent, interpretable, and aligned with human-centered AI governance
goals.

</details>


### [14] [Measuring Aleatoric and Epistemic Uncertainty in LLMs: Empirical Evaluation on ID and OOD QA Tasks](https://arxiv.org/abs/2511.03166)
*Kevin Wang,Subre Abdoul Moktar,Jia Li,Kangshuo Li,Feng Chen*

Main category: cs.CL

TL;DR: 本文评估了不同不确定性估计方法在LLM中的表现，发现基于信息的方法在分布内设置中表现优异，而基于密度的方法和P(True)指标在分布外情况下表现更优。语义一致性方法在不同数据集和生成指标下表现出可靠性能。


<details>
  <summary>Details</summary>
Motivation: 确保LLM输出的可信度至关重要，其中不确定性估计（UE）起着关键作用。本文旨在评估不同UE方法在LLM中的鲁棒性和有效性，以提高其可信度。

Method: 本文进行了一项全面的实证研究，评估了多种不确定性估计方法在LLM中的表现，包括十二种不同的UE方法和四种生成质量指标，如LLMScore。

Result: 基于信息的方法在分布内设置中表现优异，而基于密度的方法和P(True)指标在分布外情况下表现更优。语义一致性方法在不同数据集和生成指标下表现出可靠性能。

Conclusion: 本文分析了不同不确定性估计方法在LLM中的鲁棒性和有效性，发现基于信息的方法在分布内设置中表现优异，而基于密度的方法和P(True)指标在分布外情况下表现更优。语义一致性方法在不同数据集和生成指标下表现出可靠性能。

Abstract: Large Language Models (LLMs) have become increasingly pervasive, finding
applications across many industries and disciplines. Ensuring the
trustworthiness of LLM outputs is paramount, where Uncertainty Estimation (UE)
plays a key role. In this work, a comprehensive empirical study is conducted to
examine the robustness and effectiveness of diverse UE measures regarding
aleatoric and epistemic uncertainty in LLMs. It involves twelve different UE
methods and four generation quality metrics including LLMScore from LLM
criticizers to evaluate the uncertainty of LLM-generated answers in
Question-Answering (QA) tasks on both in-distribution (ID) and
out-of-distribution (OOD) datasets. Our analysis reveals that information-based
methods, which leverage token and sequence probabilities, perform exceptionally
well in ID settings due to their alignment with the model's understanding of
the data. Conversely, density-based methods and the P(True) metric exhibit
superior performance in OOD contexts, highlighting their effectiveness in
capturing the model's epistemic uncertainty. Semantic consistency methods,
which assess variability in generated answers, show reliable performance across
different datasets and generation metrics. These methods generally perform well
but may not be optimal for every situation.

</details>


### [15] [BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture](https://arxiv.org/abs/2511.03180)
*Shahriyar Zaman Ridoy,Azmine Toushik Wasi,Koushik Ahamed Tonmoy*

Main category: cs.CL

TL;DR: 本文介绍了 BengaliMoralBench，这是第一个针对孟加拉语和社文背景的大规模伦理基准。通过评估多个多语言 LLM，发现它们在文化根基、常识推理和道德公平性方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 随着多语言大型语言模型在南亚越来越流行，它们与当地伦理规范的一致性，特别是对于孟加拉语（有超过 2.85 亿人使用，全球排名第六）仍然研究不足。现有的伦理基准大多以英语为中心，由西方框架塑造，忽视了实际部署中至关重要的文化细微差别。

Method: 我们引入了 BengaliMoralBench，这是第一个针对孟加拉语和社文背景的大规模伦理基准。它涵盖了五个道德领域，分为 50 个文化相关的子主题。每个场景通过母语者的共识进行注释，使用三种伦理视角：美德、常识和正义伦理。我们对主要的多语言 LLM 进行了系统零样本评估，包括 Llama、Gemma、Qwen 和 DeepSeek，使用统一的提示协议和标准指标。

Result: 性能差异很大（50-91% 的准确率），定性分析揭示了在文化根基、常识推理和道德公平性方面的持续弱点。

Conclusion: BengaliMoralBench 提供了一个基础，用于负责任的本地化，实现了文化一致的评估，并支持在诸如孟加拉国等多样化的低资源多语言环境中部署道德稳健的人工智能。

Abstract: As multilingual Large Language Models (LLMs) gain traction across South Asia,
their alignment with local ethical norms, particularly for Bengali, which is
spoken by over 285 million people and ranked 6th globally, remains
underexplored. Existing ethics benchmarks are largely English-centric and
shaped by Western frameworks, overlooking cultural nuances critical for
real-world deployment. To address this, we introduce BengaliMoralBench, the
first large-scale ethics benchmark for the Bengali language and socio-cultural
contexts. It covers five moral domains, Daily Activities, Habits, Parenting,
Family Relationships, and Religious Activities, subdivided into 50 culturally
relevant subtopics. Each scenario is annotated via native-speaker consensus
using three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct
systematic zero-shot evaluation of prominent multilingual LLMs, including
Llama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and
standard metrics. Performance varies widely (50-91% accuracy), with qualitative
analysis revealing consistent weaknesses in cultural grounding, commonsense
reasoning, and moral fairness. BengaliMoralBench provides a foundation for
responsible localization, enabling culturally aligned evaluation and supporting
the deployment of ethically robust AI in diverse, low-resource multilingual
settings such as Bangladesh.

</details>


### [16] [LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval](https://arxiv.org/abs/2511.03214)
*Wenchang Lei,Ping Zou,Yue Wang,Feng Sun,Lei Zhao*

Main category: cs.CL

TL;DR: 本文提出语言图模型（LGM）来增强大型语言模型的概念清晰度，通过提取元关系并使用反射机制验证，从而提高模型解释概念和生成准确响应的能力。实验表明，LGM在标准基准测试中优于现有RAG基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理涉及模糊或概念不一致术语的用户指令时表现出困难。

Method: 提出语言图模型（LGM），通过从自然语言中提取元关系（继承、别名和组合）来增强概念清晰度，并采用反射机制验证这些元关系。利用概念迭代检索算法，将这些关系及相关描述动态提供给LLM，以提高其解释概念和生成准确响应的能力。

Result: LGM在标准基准测试中 consistently 超过现有的RAG基线。

Conclusion: 实验表明，LGM在标准基准测试中 consistently 超过现有的RAG基线。

Abstract: Large language models (LLMs) exhibit strong semantic understanding, yet
struggle when user instructions involve ambiguous or conceptually misaligned
terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity
by extracting meta-relations-inheritance, alias, and composition-from natural
language. The model further employs a reflection mechanism to validate these
meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these
relations and related descriptions are dynamically supplied to the LLM,
improving its ability to interpret concepts and generate accurate responses.
Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely
on extended context windows, our method enables large language models to
process texts of any length without the need for truncation. Experiments on
standard benchmarks demonstrate that the LGM consistently outperforms existing
RAG baselines.

</details>


### [17] [Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification](https://arxiv.org/abs/2511.03217)
*Shaghayegh Kolli,Richard Rosenbaum,Timo Cavelius,Lasse Strothe,Andrii Lata,Jana Diesner*

Main category: cs.CL

TL;DR: 本文介绍了一种结合大型语言模型、知识图谱和实时搜索代理的混合事实核查方法，实现了高精度和可解释性，并展示了其在不同数据集上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成流畅的语句方面表现出色，但可能缺乏可靠的验证信息基础。同时，基于知识图谱的事实核查器提供精确和可解释的证据，但受到覆盖范围或延迟的限制。本文旨在通过整合LLMs、知识图谱和实时搜索代理来解决这些问题。

Method: 本文引入了一种混合事实核查方法，结合了大型语言模型（LLMs）、知识图谱和实时搜索代理，利用每个组件的优势。系统包括三个自主步骤：1) 知识图谱（KG）检索用于快速的一跳查找，2) 基于LM的分类，由任务特定的标签提示引导，产生具有内部规则逻辑的输出，3) 当KG覆盖不足时调用网络搜索代理。

Result: 我们的管道在FEVER基准上的Supported/Refuted分割上实现了0.93的F1分数，无需任务特定的微调。针对Not enough information的情况，我们进行了一项有针对性的重新标注研究，结果显示我们的方法经常发现对于最初标记为Not Enough Information（NEI）的声明的有效证据，这得到了专家标注者和LLM评审者的确认。

Conclusion: 本文提出了一种模块化、开源的事实核查管道，具有回退策略和跨数据集的泛化能力。

Abstract: Large language models (LLMs) excel in generating fluent utterances but can
lack reliable grounding in verified information. At the same time,
knowledge-graph-based fact-checkers deliver precise and interpretable evidence,
yet suffer from limited coverage or latency. By integrating LLMs with knowledge
graphs and real-time search agents, we introduce a hybrid fact-checking
approach that leverages the individual strengths of each component. Our system
comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid
one-hop lookups in DBpedia, 2) an LM-based classification guided by a
task-specific labeling prompt, producing outputs with internal rule-based
logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient.
Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the
Supported/Refuted split without task-specific fine-tuning. To address Not
enough information cases, we conduct a targeted reannotation study showing that
our approach frequently uncovers valid evidence for claims originally labeled
as Not Enough Information (NEI), as confirmed by both expert annotators and LLM
reviewers. With this paper, we present a modular, opensource fact-checking
pipeline with fallback strategies and generalization across datasets.

</details>


### [18] [Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval](https://arxiv.org/abs/2511.03228)
*Shantanu Agarwal,Joel Barry,Elizabeth Boschee,Scott Miller*

Main category: cs.CL

TL;DR: SARAL通过一种新颖的方法，在跨语言信息检索任务中取得了优异的成绩，特别是在检索相关文档集方面。


<details>
  <summary>Details</summary>
Motivation: MATERIAL是一个旨在推进跨语言信息检索（CLIR）状态的IARPA项目，SARAL旨在开发一种能够有效检索相关文档集的方法。

Method: SARAL提出了一个新颖的方法来处理跨语言信息检索，重点在于检索与查询相关的文档集，而不仅仅是排序的文档列表。

Result: 在MATERIAL的第三阶段评估中，SARAL在三种不同语言（波斯语、哈萨克语和格鲁吉亚语）的六个评估条件下，有五个表现出色。

Conclusion: SARAL在MATERIAL的第三阶段评估中，在六个评估条件中的五个中表现优于其他团队，展示了其在跨语言信息检索中的有效性。

Abstract: Machine Translation for English Retrieval of Information in Any Language
(MATERIAL) is an IARPA initiative targeted to advance the state of
cross-lingual information retrieval (CLIR). This report provides a detailed
description of Information Sciences Institute's (ISI's) Summarization and
domain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL.
Specifically, we outline our team's novel approach to handle CLIR with emphasis
in developing an approach amenable to retrieve a query-relevant document
\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3
evaluations, SARAL exceeded the performance of other teams in five out of six
evaluation conditions spanning three different languages (Farsi, Kazakh, and
Georgian).

</details>


### [19] [IndicSuperTokenizer: An Optimized Tokenizer for Indic Multilingual LLMs](https://arxiv.org/abs/2511.03237)
*Souvik Rana,Arul Menezes,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: 本文提出了IndicSuperTokenizer，这是一种针对印地语多语言大语言模型的分词器，结合了子词和多词分词以及语言特定的预分词，实现了更高的语言对齐性，并在生育率分数上达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 设计有效的多语言分词器对于大型语言模型来说是一个挑战，因为存在多种脚本和丰富的形态变化。虽然子词方法如字节对编码（BPE）被广泛采用，但它们在多语言环境中的效果仍需进一步探索。

Method: 本文提出了一种结合子词和多词分词以及语言特定预分词的分词器IndicSuperTokenizer，并通过实验验证了其有效性。

Result: IndicSuperTokenizer在英语、22种印度语言和代码数据上进行了评估，与LLaMA4相比，平均生育率分数提高了39.5%，与当前最佳的Sutra相比提高了18%。这在推理吞吐量上比LLaMA4提高了44%，同时保持了在英语和印地语基准上的可比性能。

Conclusion: 本文提出了IndicSuperTokenizer，这是一种针对印地语多语言大语言模型的分词器，结合了子词和多词分词以及语言特定的预分词，实现了更高的语言对齐性，并在生育率分数上达到了新的最先进水平。

Abstract: Tokenizers play a crucial role in determining the performance, training
efficiency, and the inference cost of Large Language Models (LLMs). Designing
effective tokenizers for multilingual LLMs is particularly challenging due to
diverse scripts and rich morphological variation. While subword methods such as
Byte Pair Encoding (BPE) are widely adopted, their effectiveness in
multilingual settings remains underexplored. We present IndicSuperTokenizer, a
tokenizer for Indic multilingual LLMs, that combines both subword and
multi-word tokenization, along with language-specific pre-tokenization, leading
to more linguistically aligned tokens and achieving a new state-of-the-art in
fertility score. Evaluated across English, 22 Indian languages and code data,
our tokenizer improves the average fertility score by 39.5% over LLaMA4 and by
18% over Sutra (the current best). This translates to 44% improvement in
inference throughput over LLaMA4 while maintaining comparable performance on
English and Indic benchmarks. We also present detailed ablations across
tokenizer training data size, vocabulary size, merging techniques, and
pre-tokenization strategies, demonstrating the robustness of our design
choices.

</details>


### [20] [Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature](https://arxiv.org/abs/2511.03261)
*Ranul Dayarathne,Uvini Ranaweera,Upeksha Ganegoda*

Main category: cs.CL

TL;DR: 该研究比较了多个LLM在问答任务中的表现，发现GPT-3.5和Mistral-7b-instruct在RAG支持下表现优异，同时开源LLM也展示了与专有模型相当的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着RAG技术的兴起，对不同LLM在问答任务中表现的比较变得越来越重要。

Method: 本研究比较了四个开源LLM（Mistral-7b-instruct、LLaMa2-7b-chat、Falcon-7b-instruct和Orca-mini-v3-7b）和OpenAI的GPT-3.5在计算机科学文献中的问答任务表现，并利用RAG支持进行评估。

Result: GPT-3.5在与RAG结合时在二元和长答案问题上表现出色，而Mistral-7b-instruct在开源LLM中表现最佳。Orca-mini-v3-7b的平均延迟最短，而LLaMa2-7b-chat的平均延迟最高。

Conclusion: 该研究强调了开源LLM也可以与像GPT-3.5这样的专有模型并驾齐驱，只要具备更好的基础设施。

Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to
enhance the capabilities of Generative AI models by reducing hallucination.
Thus, the increasing prominence of RAG alongside Large Language Models (LLMs)
has sparked interest in comparing the performance of different LLMs in
question-answering (QA) in diverse domains. This study compares the performance
of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,
Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA
tasks within the computer science literature leveraging RAG support. Evaluation
metrics employed in the study include accuracy and precision for binary
questions and ranking by a human expert, ranking by Google's AI model Gemini,
alongside cosine similarity for long-answer questions. GPT-3.5, when paired
with RAG, effectively answers binary and long-answer questions, reaffirming its
status as an advanced LLM. Regarding open-source LLMs, Mistral AI's
Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary
and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b
reports the shortest average latency in generating responses, whereas
LLaMa2-7b-chat by Meta reports the highest average latency. This research
underscores the fact that open-source LLMs, too, can go hand in hand with
proprietary models like GPT-3.5 with better infrastructure.

</details>


### [21] [SCALE: Upscaled Continual Learning of Large Language Models](https://arxiv.org/abs/2511.03270)
*Jin-woo Lee,Junhwa Choi,Bongkyu Hwang,Jinho Choo,Bogun Kim,JeongSeon Yi,Joonseok Lee,DongYoung Jung,Jaeseon Park,Kyoungwon Park,Suk-hoon Jung*

Main category: cs.CL

TL;DR: The paper introduces SCALE, a width upscaling architecture that improves continual pre-training for large language models by preserving the base model's behavior and selectively adapting to new knowledge.


<details>
  <summary>Details</summary>
Motivation: The paper argues that progress in continual pre-training for large language models now depends more on scaling the right structure than on scaling parameters alone.

Method: SCALE is a width upscaling architecture that inserts lightweight expansion into linear modules while freezing all pre-trained parameters. It is guided by two principles: Persistent Preservation and Collaborative Adaptation.

Result: On a controlled synthetic biography benchmark, SCALE mitigates severe forgetting observed with depth expansion while still acquiring new knowledge. In continual pre-training on a Korean corpus, SCALE variants achieve less forgetting on English evaluations and competitive gains on Korean benchmarks.

Conclusion: SCALE variants offer the best overall stability-plasticity trade-off and achieve less forgetting on English evaluations and competitive gains on Korean benchmarks.

Abstract: We revisit continual pre-training for large language models and argue that
progress now depends more on scaling the right structure than on scaling
parameters alone. We introduce SCALE, a width upscaling architecture that
inserts lightweight expansion into linear modules while freezing all
pre-trained parameters. This preserves the residual and attention topologies
and increases capacity without perturbing the base model's original
functionality. SCALE is guided by two principles: Persistent Preservation,
which maintains the base model's behavior via preservation-oriented
initialization and freezing of the pre-trained weights, and Collaborative
Adaptation, which selectively trains a subset of expansion components to
acquire new knowledge with minimal interference. We instantiate these ideas as
SCALE-Preserve (preservation-first), SCALE-Adapt (adaptation-first), and
SCALE-Route, an optional routing extension that performs token-level routing
between preservation and adaptation heads. On a controlled synthetic biography
benchmark, SCALE mitigates the severe forgetting observed with depth expansion
while still acquiring new knowledge. In continual pre-training on a Korean
corpus, SCALE variants achieve less forgetting on English evaluations and
competitive gains on Korean benchmarks, with these variants offering the best
overall stability-plasticity trade-off. Accompanying analysis clarifies when
preservation provably holds and why the interplay between preservation and
adaptation stabilizes optimization compared to standard continual learning
setups.

</details>


### [22] [How to Evaluate Speech Translation with Source-Aware Neural MT Metrics](https://arxiv.org/abs/2511.03295)
*Mauro Cettolo,Marco Gaido,Matteo Negri,Sara Papi,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文首次系统研究了语音翻译中的源感知度量，特别是在没有源转录本的现实条件下。我们探索了两种生成文本代理的策略，并提出了一种新的跨语言重新分割算法。实验表明，ASR转录本在词错误率较低时更可靠，而反向翻译则更经济有效。该算法使得源感知度量在语音翻译评估中更加稳健。


<details>
  <summary>Details</summary>
Motivation: 自动评估语音到文本翻译（ST）系统通常通过将翻译假设与一个或多个参考翻译进行比较来完成。然而，这种方法继承了基于参考的评估的局限性，忽略了来自源输入的有价值信息。在机器翻译（MT）中，最近的进展表明，结合源文本的神经度量与人类判断有更强的相关性。然而，将这一想法扩展到ST并不容易，因为源是音频而不是文本，且可靠的转录本或源和参考之间的对齐通常不可用。

Method: 我们探索了两种生成输入音频文本代理的互补策略：自动语音识别（ASR）转录本和参考翻译的反向翻译，并引入了一种新的两步跨语言重新分割算法来解决合成源和参考翻译之间的对齐不匹配问题。

Result: 我们的实验显示，当词错误率低于20%时，ASR转录本比反向翻译构成更可靠的合成源，而反向翻译始终是一种计算成本更低但仍有效的替代方案。此外，我们的跨语言重新分割算法使得源感知机器翻译度量在ST评估中得以稳健使用。

Conclusion: 我们的跨语言重新分割算法使源感知机器翻译度量在语音翻译评估中得以稳健使用，为更准确和有原则的评估方法铺平了道路。

Abstract: Automatic evaluation of speech-to-text translation (ST) systems is typically
performed by comparing translation hypotheses with one or more reference
translations. While effective to some extent, this approach inherits the
limitation of reference-based evaluation that ignores valuable information from
the source input. In machine translation (MT), recent progress has shown that
neural metrics incorporating the source text achieve stronger correlation with
human judgments. Extending this idea to ST, however, is not trivial because the
source is audio rather than text, and reliable transcripts or alignments
between source and references are often unavailable. In this work, we conduct
the first systematic study of source-aware metrics for ST, with a particular
focus on real-world operating conditions where source transcripts are not
available. We explore two complementary strategies for generating textual
proxies of the input audio, automatic speech recognition (ASR) transcripts, and
back-translations of the reference translation, and introduce a novel two-step
cross-lingual re-segmentation algorithm to address the alignment mismatch
between synthetic sources and reference translations. Our experiments, carried
out on two ST benchmarks covering 79 language pairs and six ST systems with
diverse architectures and performance levels, show that ASR transcripts
constitute a more reliable synthetic source than back-translations when word
error rate is below 20%, while back-translations always represent a
computationally cheaper but still effective alternative. Furthermore, our
cross-lingual re-segmentation algorithm enables robust use of source-aware MT
metrics in ST evaluation, paving the way toward more accurate and principled
evaluation methodologies for speech translation.

</details>


### [23] [Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks](https://arxiv.org/abs/2511.03328)
*Jindong Hong,Tianjie Chen,Lingjie Luo,Chuanyang Zheng,Ting Xu,Haibao Yu,Jianing Qiu,Qianzhong Chen,Suning Huang,Yan Xu,Yong Gui,Yijun He,Jiankai Sun*

Main category: cs.CL

TL;DR: 本文评估了两种多模态大语言模型在医学任务中的思考模式性能，发现其在多数任务中的提升有限，复杂任务表现不佳，强调需要更多医学领域数据和先进方法。


<details>
  <summary>Details</summary>
Motivation: 随着“双状态”MLLMs的快速过渡和采用，本文旨在严格评估这些模型增强的推理过程如何影响其在临床任务中的性能和可靠性。

Method: 本研究评估了两种领先的多模态大语言模型（MLLMs）Seed1.5-VL和Gemini-2.5-Flash的主动“思考模式”能力，使用VQA-RAD和ROCOv2数据集对四个视觉医学任务进行了性能评估。

Result: 研究结果表明，在大多数任务中，激活思考模式带来的性能提升仍然有限。在复杂医学任务如开放式VQA和医学图像解释方面，表现仍然不理想。

Conclusion: 研究发现，尽管推理模式在理论上提供了更详细的思考过程，但在大多数任务中，与非推理模式相比，性能提升仍然有限。复杂医学任务如开放式VQA和医学图像解释的表现仍不理想，这表明需要更多针对医学领域的数据和更先进的医学知识整合方法。

Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is
the emergence of "reasoning MLLMs" that offer explicit control over their
internal thinking processes (normally referred as the "thinking mode")
alongside the standard "non-thinking mode". This capability allows these models
to engage in a step-by-step process of internal deliberation before generating
a final response. With the rapid transition to and adoption of these
"dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
processes of these MLLMs impact model performance and reliability in clinical
tasks. This paper evaluates the active "thinking mode" capabilities of two
leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
assessed their performance on four visual medical tasks using VQA-RAD and
ROCOv2 datasets. Our findings reveal that the improvement from activating the
thinking mode remains marginal compared to the standard non-thinking mode for
the majority of the tasks. Their performance on complex medical tasks such as
open-ended VQA and medical image interpretation remains suboptimal,
highlighting the need for domain-specific medical data and more advanced
methods for medical knowledge integration.

</details>


### [24] [Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances](https://arxiv.org/abs/2511.03354)
*Riasad Alvi,Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Md Rafi Ur Rashid,Md Rafiqul Islam,Yakub Sebastian,Sami Azam*

Main category: cs.CL

TL;DR: 本文通过系统综述的方法，评估了生成式人工智能在生物信息学中的应用，发现了其在多个领域的优势，并指出了未来的改进方向。


<details>
  <summary>Details</summary>
Motivation: 本文旨在系统地识别和评估生成式人工智能在生物信息学领域的进展，以推动其在基因组学、蛋白质组学、转录组学、结构生物学和药物发现等领域的应用。

Method: 本文采用系统综述和元分析的方法，提出了六个研究问题（RQs）来评估生成式人工智能在生物信息学中的影响。

Result: 研究结果表明，生成式人工智能在多个生物信息学子领域表现出色，特别是在分子分析和数据整合方面，能够提高准确性并减少复杂分析中的错误。此外，生成式人工智能在结构建模、功能预测和合成数据生成方面也有所改进。

Conclusion: 本文总结了生成式人工智能在生物信息学中的应用，并指出了其在方法论、预测性能和专业化方面的优势，同时提出了未来的研究方向。

Abstract: Generative artificial intelligence (GenAI) has become a transformative
approach in bioinformatics that often enables advancements in genomics,
proteomics, transcriptomics, structural biology, and drug discovery. To
systematically identify and evaluate these growing developments, this review
proposed six research questions (RQs), according to the preferred reporting
items for systematic reviews and meta-analysis methods. The objective is to
evaluate impactful GenAI strategies in methodological advancement, predictive
performance, and specialization, and to identify promising approaches for
advanced modeling, data-intensive discovery, and integrative biological
analysis. RQ1 highlights diverse applications across multiple bioinformatics
subfields (sequence analysis, molecular design, and integrative data modeling),
which demonstrate superior performance over traditional methods through pattern
recognition and output generation. RQ2 reveals that adapted specialized model
architectures outperformed general-purpose models, an advantage attributed to
targeted pretraining and context-aware strategies. RQ3 identifies significant
benefits in the bioinformatics domains, focusing on molecular analysis and data
integration, which improves accuracy and reduces errors in complex analysis.
RQ4 indicates improvements in structural modeling, functional prediction, and
synthetic data generation, validated by established benchmarks. RQ5 suggests
the main constraints, such as the lack of scalability and biases in data that
impact generalizability, and proposes future directions focused on robust
evaluation and biologically grounded modeling. RQ6 examines that molecular
datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as
CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly
support the training and generalization of GenAI models.

</details>


### [25] [Silenced Biases: The Dark Side LLMs Learned to Refuse](https://arxiv.org/abs/2511.03369)
*Rom Himelstein,Amit LeVi,Brit Youngmann,Yaniv Nemcovsky,Avi Mendelson*

Main category: cs.CL

TL;DR: 本文提出了一种新的公平评估框架，用于揭示安全对齐模型中的隐藏偏见，并展示了其在多个大型语言模型上的应用效果。


<details>
  <summary>Details</summary>
Motivation: 当前评估模型公平性的方法往往忽视了深层问题，将模型的拒绝响应视为正面的公平指标，这造成了虚假的公平感。

Method: 本文引入了沉默偏见的概念，并提出了Silenced Bias Benchmark (SBB)，通过激活控制减少问答过程中的模型拒绝行为，以揭示隐藏偏见。

Result: 实验结果表明，模型的直接回答与其潜在的公平性问题之间存在显著差异。

Conclusion: 本文提出了一个公平评估框架，旨在揭示安全对齐模型中的隐藏偏见，并鼓励未来开发更公平的模型和工具。

Abstract: Safety-aligned large language models (LLMs) are becoming increasingly
widespread, especially in sensitive applications where fairness is essential
and biased outputs can cause significant harm. However, evaluating the fairness
of models is a complex challenge, and approaches that do so typically utilize
standard question-answer (QA) styled schemes. Such methods often overlook
deeper issues by interpreting the model's refusal responses as positive
fairness measurements, which creates a false sense of fairness. In this work,
we introduce the concept of silenced biases, which are unfair preferences
encoded within models' latent space and are effectively concealed by
safety-alignment. Previous approaches that considered similar indirect biases
often relied on prompt manipulation or handcrafted implicit queries, which
present limited scalability and risk contaminating the evaluation process with
additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to
uncover these biases by employing activation steering to reduce model refusals
during QA. SBB supports easy expansion to new demographic groups and subjects,
presenting a fairness evaluation framework that encourages the future
development of fair models and tools beyond the masking effects of alignment
training. We demonstrate our approach over multiple LLMs, where our findings
expose an alarming distinction between models' direct responses and their
underlying fairness issues.

</details>


### [26] [EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation](https://arxiv.org/abs/2511.03370)
*Yunbo Long,Yuhan Liu,Alexandra Brintrup*

Main category: cs.CL

TL;DR: 本文提出EQ-Negotiator框架，通过情感角色弥补小型语言模型在自动化谈判中的性能差距，并证明战略情感智能比模型规模更重要。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然性能高，但计算成本和数据隐私要求使其不适合许多隐私敏感的设备端应用。小型语言模型（SLMs）虽然实用，但在扮演情绪化复杂角色方面与LLMs存在显著性能差距。

Method: EQ-Negotiator框架结合博弈论和隐马尔可夫模型（HMM）来在线学习和跟踪债务人的心理状态，无需预训练。

Result: 带有EQ-Negotiator的7B参数语言模型在债务回收和谈判效率方面优于比它大10倍的基线LLMs。

Conclusion: 本文将人格建模从描述性角色档案推进到在隐私限制下运行的动态情感架构，并确立了战略情感智能是自动化谈判成功的关键因素，为能够在边缘运行的有效、道德和隐私保护的人工智能谈判者铺平了道路。

Abstract: The deployment of large language models (LLMs) in automated negotiation has
set a high performance benchmark, but their computational cost and data privacy
requirements render them unsuitable for many privacy-sensitive, on-device
applications such as mobile assistants, embodied AI agents or private client
interactions. While small language models (SLMs) offer a practical alternative,
they suffer from a significant performance gap compared to LLMs in playing
emotionally charged complex personas, especially for credit negotiation. This
paper introduces EQ-Negotiator, a novel framework that bridges this capability
gap using emotional personas. Its core is a reasoning system that integrates
game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional
states online, without pre-training. This allows EQ-Negotiator to equip SLMs
with the strategic intelligence to counter manipulation while de-escalating
conflict and upholding ethical standards. Through extensive agent-to-agent
simulations across diverse credit negotiation scenarios, including adversarial
debtor strategies like cheating, threatening, and playing the victim, we show
that a 7B parameter language model with EQ-Negotiator achieves better debt
recovery and negotiation efficiency than baseline LLMs more than 10 times its
size. This work advances persona modeling from descriptive character profiles
to dynamic emotional architectures that operate within privacy constraints.
Besides, this paper establishes that strategic emotional intelligence, not raw
model scale, is the critical factor for success in automated negotiation,
paving the way for effective, ethical, and privacy-preserving AI negotiators
that can operate on the edge.

</details>


### [27] [LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced Logical Reasoning](https://arxiv.org/abs/2511.03372)
*Shenghao Li*

Main category: cs.CL

TL;DR: 本文提出了一种符号逻辑控制的数据增强方法LFC-DA，通过逻辑文本映射、规则库编译和状态空间搜索生成多样且逻辑严谨的自然语言问题，提升了预训练模型的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 复杂逻辑数据增强高度依赖人工标注，成本高昂，而直接使用大语言模型生成的例子则不可解释且逻辑上同质化。

Method: LFC-DA是一种符号逻辑控制的管道，首先将逻辑文本映射到命题表达式，编译一个紧凑的规则库，并进行有限状态空间搜索以系统地发现有效的公式，然后将其重新表述为自然语言问题。

Result: 在ReClor和LogiQA上的实验表明，LFC-DA显著提高了预训练模型的逻辑推理准确性。

Conclusion: LFC-DA在预训练模型的逻辑推理准确性上表现出显著提升，证明了其在LLM引导的逻辑数据增强中的有效性。

Abstract: For complex logical data augmentation, heavy reliance on human annotation is
costly, whereas direct generation with large language models yields
uninterpretable and logically homogeneous examples. To address this, we present
LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to
propositional expressions, a compact rule library is compiled, and a bounded
state-space search systematically discovers valid formulas that are then
verbalized back into natural-language questions, ensuring both diversity and
logical rigor under propositional logic. Experiments on ReClor and LogiQA show
significant improvements in the logical-reasoning accuracy of pretrained
models, confirming the effectiveness of LFC-DA for LLM-guided logical data
augmentation.

</details>


### [28] [Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance](https://arxiv.org/abs/2511.03383)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: 本文发现，使用不对称的BPE分割方法（源语言高合并次数，目标语言低合并次数）可以显著提升低资源机器翻译的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器翻译研究通常采用固定的BPE超参数，但这种方法在不同语言对和数据规模下可能不是最优的。

Method: 本文研究了不同数据量和语言对的BPE分割方案，评估了机器翻译系统的性能，并比较了对称BPE和不对称BPE的效果。

Result: 在低资源设置下，不对称BPE在英语-印地语任务中平均提升了5.32、4.46和0.7 CHRF++，并且在其他六种语言对中也观察到了统计学上显著的改进。

Conclusion: 我们的研究结果表明，对于低资源机器翻译，使用不对称的BPE分割方法（源语言高合并次数，目标语言低合并次数）可以显著提高性能。

Abstract: Existing Machine Translation (MT) research often suggests a single, fixed set
of hyperparameters for word segmentation models, symmetric Byte Pair Encoding
(BPE), which applies the same number of merge operations (NMO) to train
tokenizers for both source and target languages. However, we demonstrate that
this uniform approach doesn't guarantee optimal MT performance across different
language pairs and data sizes. This work investigates BPE segmentation recipes
across various data volumes and language pairs to evaluate MT system
performance. We find that utilizing asymmetric BPE, where the source and target
languages have different NMOs, significantly improves results over the
symmetric approach, especially in low-resource settings (50K, 100K, and 500K
sentence pairs). Specifically, asymmetric BPE yield statistically significant
($p<0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in
low-resource setups. We validated this trend across six additional language
pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut),
observing statistically significant improvement in 10 out of 12 systems
compared to symmetric BPE. Our findings indicate a high NMO for the source (4K
to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results,
particularly benefiting low-resource MT.

</details>


### [29] [Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties](https://arxiv.org/abs/2511.03407)
*Célian Ringwald,Fabien Gandon,Catherine Faron,Franck Michel,Hanna Abi Akl*

Main category: cs.CL

TL;DR: 本文研究了小语言模型在RDF图提取中的表现，发现构建一个每个属性出现次数超过阈值的训练集是解决罕见属性长尾分布问题的有效方法。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究小语言模型如何处理RDF图中的数据类型属性和对象属性，并解决罕见属性的长尾分布问题。

Method: 本文评估了多种策略，包括分层抽样、加权损失、数据集扩展和基于模板的合成数据增强，以解决罕见属性的长尾分布问题。

Result: 本文发现，构建一个每个属性出现次数超过给定阈值的训练集是最有效的策略，可以在不平衡的目标属性上表现良好。

Conclusion: 本文的研究结果为训练形状感知的小语言模型提供了实用指导，并指出了语义关系抽取的未来研究方向。

Abstract: Small language models (SLMs) have shown promises for relation extraction (RE)
when extracting RDF triples guided by SHACL shapes focused on common datatype
properties. This paper investigates how SLMs handle both datatype and object
properties for a complete RDF graph extraction. We show that the key bottleneck
is related to long-tail distribution of rare properties. To solve this issue,
we evaluate several strategies: stratified sampling, weighted loss, dataset
scaling, and template-based synthetic data augmentation. We show that the best
strategy to perform equally well over unbalanced target properties is to build
a training set where the number of occurrences of each property exceeds a given
threshold. To enable reproducibility, we publicly released our datasets,
experimental results and code. Our findings offer practical guidance for
training shape-aware SLMs and highlight promising directions for future work in
semantic RE.

</details>


### [30] [Efficient Reasoning via Thought-Training and Thought-Free Inference](https://arxiv.org/abs/2511.03408)
*Canhui Wu,Qiong Cao,Chao Xue,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 3TF is a new framework for efficient reasoning in LLMs that allows models to perform internal reasoning implicitly while keeping outputs short, leading to improved performance on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for improving reasoning accuracy in LLMs focus on compressing verbose reasoning outputs, but still rely on explicit reasoning during inference. The authors aim to develop a more efficient reasoning framework that allows models to perform rich internal reasoning implicitly while keeping external outputs short.

Method: 3TF is a framework that trains a hybrid model to operate in both reasoning and non-reasoning modes, and further trains it on CoT-annotated data to internalize structured reasoning while enforcing concise, thought-free outputs during inference.

Result: Empirical results show that 3TF-trained models achieve significant improvements on reasoning benchmarks under thought-free inference, indicating that high-quality reasoning can be learned and executed implicitly without explicit step-by-step generation.

Conclusion: 3TF-trained models can achieve high-quality reasoning implicitly without explicit step-by-step generation, demonstrating the effectiveness of the proposed framework.

Abstract: Recent advances in large language models (LLMs) have leveraged explicit
Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most
existing methods primarily compress verbose reasoning outputs. These
Long-to-Short transformations aim to improve efficiency, but still rely on
explicit reasoning during inference. In this work, we introduce \textbf{3TF}
(\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree
inference), a framework for efficient reasoning that takes a Short-to-Long
perspective. We first train a hybrid model that can operate in both reasoning
and non-reasoning modes, and then further train it on CoT-annotated data to
internalize structured reasoning, while enforcing concise, thought-free outputs
at inference time using the no-reasoning mode. Unlike compression-based
approaches, 3TF improves the reasoning quality of non-reasoning outputs,
enabling models to perform rich internal reasoning implicitly while keeping
external outputs short. Empirically, 3TF-trained models obtain large
improvements on reasoning benchmarks under thought-free inference,
demonstrating that high quality reasoning can be learned and executed
implicitly without explicit step-by-step generation.

</details>


### [31] [Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG](https://arxiv.org/abs/2511.03410)
*Longpeng Qiu,Ting Li,Shuai Mao,Nan Yang,Xiaohui Yan*

Main category: cs.CL

TL;DR: 本文提出了一种名为QuestionRAG的框架，以解决问答系统中输入错误导致的错误响应问题。该框架通过外部知识丰富输入并使用强化学习来对齐模型目标，从而提高模型的理解和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 输入错误会导致问答（QA）系统出现错误响应。大型语言模型（LLMs）在此任务上表现不佳，经常无法理解用户意图（误解）或不必要的改变原始问题的结构（过度修正）。

Method: 为了应对误解，它通过外部知识（例如搜索结果、相关实体）丰富输入。为了防止过度修正，它使用强化学习（RL）来使模型的目标与精确修正对齐，而不是仅仅改写。

Result: 我们的结果表明，知识增强对于理解错误的问题至关重要。此外，基于强化学习的对齐比传统的监督微调（SFT）更有效，提高了模型遵循指令和泛化的能力。

Conclusion: 通过整合这两种策略，QuestionRAG释放了LLMs在问题纠正任务中的全部潜力。

Abstract: Input errors in question-answering (QA) systems often lead to incorrect
responses. Large language models (LLMs) struggle with this task, frequently
failing to interpret user intent (misinterpretation) or unnecessarily altering
the original question's structure (over-correction). We propose QuestionRAG, a
framework that tackles these problems. To address misinterpretation, it
enriches the input with external knowledge (e.g., search results, related
entities). To prevent over-correction, it uses reinforcement learning (RL) to
align the model's objective with precise correction, not just paraphrasing. Our
results demonstrate that knowledge augmentation is critical for understanding
faulty questions. Furthermore, RL-based alignment proves significantly more
effective than traditional supervised fine-tuning (SFT), boosting the model's
ability to follow instructions and generalize. By integrating these two
strategies, QuestionRAG unlocks the full potential of LLMs for the question
correction task.

</details>


### [32] [CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field](https://arxiv.org/abs/2511.03441)
*Doria Bonzi,Alexandre Guiggi,Frédéric Béchet,Carlos Ramisch,Benoit Favre*

Main category: cs.CL

TL;DR: CareMedEval是一个新的数据集，用于评估大型语言模型在生物医学批判性评估和推理任务中的表现，揭示了当前模型的局限性，并为未来的发展提供了方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生物医学领域中对科学文献的批判性评估提供了有希望的支持，但其可靠性仍然有限，尤其是在专业领域的批判性推理方面。

Method: 引入CareMedEval数据集，该数据集用于评估LLMs在生物医学批判性评估和推理任务中的表现。数据集来源于法国医学生的真实考试，包含534个问题，基于37篇科学文章。

Result: 在各种上下文条件下对最先进的通用和生物医学专业LLMs进行基准测试表明，该任务具有挑战性：开放和商业模型即使生成中间推理标记，也无法超过0.5的精确匹配率。然而，模型在关于研究局限性和统计分析的问题上仍然面临挑战。

Conclusion: CareMedEval为基于科学文献的推理提供了一个具有挑战性的基准，揭示了当前LLM的局限性，并为未来自动化支持批判性评估的发展铺平了道路。

Abstract: Critical appraisal of scientific literature is an essential skill in the
biomedical field. While large language models (LLMs) can offer promising
support in this task, their reliability remains limited, particularly for
critical reasoning in specialized domains. We introduce CareMedEval, an
original dataset designed to evaluate LLMs on biomedical critical appraisal and
reasoning tasks. Derived from authentic exams taken by French medical students,
the dataset contains 534 questions based on 37 scientific articles. Unlike
existing benchmarks, CareMedEval explicitly evaluates critical reading and
reasoning grounded in scientific papers. Benchmarking state-of-the-art
generalist and biomedical-specialized LLMs under various context conditions
reveals the difficulty of the task: open and commercial models fail to exceed
an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens
considerably improves the results. Yet, models remain challenged especially on
questions about study limitations and statistical analysis. CareMedEval
provides a challenging benchmark for grounded reasoning, exposing current LLM
limitations and paving the way for future development of automated support for
critical appraisal.

</details>


### [33] [Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction](https://arxiv.org/abs/2511.03466)
*Ringwald Celian,Gandon Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: Kastor 是一种先进的框架，用于在专业领域中完成和精炼知识库，通过重新表述传统验证任务并选择最佳组合来提高模型的性能，并利用迭代学习过程来精炼噪声知识库。


<details>
  <summary>Details</summary>
Motivation: 现有的 RDF 模式提取方法在有限的文本和 RDF 数据上训练高效模型方面存在局限性，因此需要一种更先进的框架来满足专业领域中完成和精炼知识库的需求。

Method: Kastor 将传统的 SHACL 形状验证任务重新表述为评估从形状中派生的所有可能属性组合，并通过选择每个训练示例的最佳组合来提高模型的性能。此外，Kastor 还采用迭代学习过程来精炼噪声知识库。

Result: Kastor 通过重新表述传统验证任务并选择最佳组合，显著提高了模型的泛化能力和性能，并能够创建稳健模型以发现新的相关事实。

Conclusion: Kastor 是一种先进的框架，可以满足专业领域中完成和精炼知识库的需求，通过选择每个训练示例的最佳组合来显著提高模型的泛化能力和性能，并利用迭代学习过程来精炼噪声知识库，从而创建能够发现新相关事实的稳健模型。

Abstract: RDF pattern-based extraction is a compelling approach for fine-tuning small
language models (SLMs) by focusing a relation extraction task on a specified
SHACL shape. This technique enables the development of efficient models trained
on limited text and RDF data. In this article, we introduce Kastor, a framework
that advances this approach to meet the demands for completing and refining
knowledge bases in specialized domains. Kastor reformulates the traditional
validation task, shifting from single SHACL shape validation to evaluating all
possible combinations of properties derived from the shape. By selecting the
optimal combination for each training example, the framework significantly
enhances model generalization and performance. Additionally, Kastor employs an
iterative learning process to refine noisy knowledge bases, enabling the
creation of robust models capable of uncovering new, relevant facts

</details>


### [34] [BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation](https://arxiv.org/abs/2511.03498)
*Kazi Reyazul Hasan,Mubasshira Musarrat,A. B. M. Alim Al Islam,Muhammad Abdullah Adnan*

Main category: cs.CL

TL;DR: 本文介绍了BanglaSTEM数据集和一个基于T5的翻译模型，旨在提高孟加拉语技术内容的翻译准确性，使孟加拉语使用者能够更有效地使用英语语言模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在英语技术问题解决中表现良好，但在用孟加拉语提问时表现较差。现有的孟加拉语-英语翻译系统在技术术语方面存在问题，经常错误地翻译专业词汇，这会改变问题的含义并导致错误答案。

Method: 我们使用语言模型生成了超过12,000个翻译，并让人类评估者选择质量最高的句子对，这些句子对正确保留了技术术语。我们还在BanglaSTEM上训练了一个基于T5的翻译模型，并在两个任务上进行了测试：生成代码和解决数学问题。

Result: 我们的结果表明，技术内容的翻译准确性有显著提高，这使得孟加拉语使用者能够更有效地使用以英语为中心的语言模型。

Conclusion: 我们的结果表明，技术内容的翻译准确性有显著提高，这使得孟加拉语使用者能够更有效地使用以英语为中心的语言模型。孟加拉STEM数据集和训练好的翻译模型已公开发布。

Abstract: Large language models work well for technical problem solving in English but
perform poorly when the same questions are asked in Bangla. A simple solution
would be to translate Bangla questions into English first and then use these
models. However, existing Bangla-English translation systems struggle with
technical terms. They often mistranslate specialized vocabulary, which changes
the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a
dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM
fields including computer science, mathematics, physics, chemistry, and
biology. We generated over 12,000 translations using language models and then
used human evaluators to select the highest quality pairs that preserve
technical terminology correctly. We train a T5-based translation model on
BanglaSTEM and test it on two tasks: generating code and solving math problems.
Our results show significant improvements in translation accuracy for technical
content, making it easier for Bangla speakers to use English-focused language
models effectively. Both the BanglaSTEM dataset and the trained translation
model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.

</details>


### [35] [HaluMem: Evaluating Hallucinations in Memory Systems of Agents](https://arxiv.org/abs/2511.03506)
*Ding Chen,Simin Niu,Kehang Li,Peng Liu,Xiangping Zheng,Bo Tang,Xinchi Li,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 本文介绍了HaluMem，这是一个用于评估记忆系统中幻觉行为的基准，通过三个任务来揭示不同操作阶段的幻觉行为，并构建了大规模数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有对记忆幻觉的评估主要是端到端的问题回答，这使得难以定位记忆系统中幻觉出现的操作阶段。

Method: 引入了Hallucination in Memory Benchmark (HaluMem)，这是第一个针对记忆系统的操作级别幻觉评估基准。HaluMem定义了三个评估任务（记忆提取、记忆更新和记忆问答）来全面揭示不同操作阶段的幻觉行为。构建了用户为中心的多轮人机交互数据集，HaluMem-Medium和HaluMem-Long。

Result: 基于HaluMem的实证研究显示，现有的记忆系统在提取和更新阶段容易生成和积累幻觉，并随后将错误传播到问题回答阶段。

Conclusion: 未来的研究应专注于开发可解释和受约束的记忆操作机制，以系统地抑制幻觉并提高记忆的可靠性。

Abstract: Memory systems are key components that enable AI systems such as LLMs and AI
agents to achieve long-term learning and sustained interaction. However, during
memory storage and retrieval, these systems frequently exhibit memory
hallucinations, including fabrication, errors, conflicts, and omissions.
Existing evaluations of memory hallucinations are primarily end-to-end question
answering, which makes it difficult to localize the operational stage within
the memory system where hallucinations arise. To address this, we introduce the
Hallucination in Memory Benchmark (HaluMem), the first operation level
hallucination evaluation benchmark tailored to memory systems. HaluMem defines
three evaluation tasks (memory extraction, memory updating, and memory question
answering) to comprehensively reveal hallucination behaviors across different
operational stages of interaction. To support evaluation, we construct
user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and
HaluMem-Long. Both include about 15k memory points and 3.5k multi-type
questions. The average dialogue length per user reaches 1.5k and 2.6k turns,
with context lengths exceeding 1M tokens, enabling evaluation of hallucinations
across different context scales and task complexities. Empirical studies based
on HaluMem show that existing memory systems tend to generate and accumulate
hallucinations during the extraction and updating stages, which subsequently
propagate errors to the question answering stage. Future research should focus
on developing interpretable and constrained memory operation mechanisms that
systematically suppress hallucinations and improve memory reliability.

</details>


### [36] [One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework](https://arxiv.org/abs/2511.03508)
*Qi Jia,Kaiwei Zhang,Xiujie Song,Ye Shen,Xiangyang Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的框架来评估多轮指令遵循能力，并构建了EvolIF基准测试。结果表明，GPT-5在指令遵循性能方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型在跨多个主题的对话中遵循用户指令的能力对于数据密集型对话应用非常重要。现有的基准测试通常仅限于固定数量的回合，容易出现饱和，并且未能考虑用户的互动体验。

Method: 我们提出了一种可扩展的框架来评估多轮指令遵循能力。该框架通过一个三层机制解耦语言表面形式与用户意图模拟，跟踪约束、指令和主题。该框架通过动态构建基准测试来模仿用户-LLM交互，只有在模型耗尽模拟用户的耐心时才终止对话。

Result: GPT-5表现出优越的指令遵循性能。它平均维持18.54个对话回合，并展示了70.31%的鲁棒性，比Gemini-2.5-Pro显著高出11.41%，而其他模型则远远落后。

Conclusion: 我们的结果表明，GPT-5在指令遵循性能方面表现出色。它平均维持18.54个对话回合，并展示了70.31%的鲁棒性，比Gemini-2.5-Pro显著高出11.41%，而其他模型则远远落后。所有的数据和代码都将在线公开。

Abstract: Understanding how well large language models can follow users' instructions
throughout a dialogue spanning multiple topics is of great importance for
data-intensive conversational applications. Existing benchmarks are often
limited to a fixed number of turns, making them susceptible to saturation and
failing to account for the user's interactive experience. In this work, we
propose an extensible framework for assessing multi-turn instruction-following
ability. At its core, our framework decouples linguistic surface forms from
user intent simulation through a three-layer mechanism that tracks constraints,
instructions, and topics. This framework mimics User-LLM interaction by
enabling the dynamic construction of benchmarks with state changes and
tracebacks, terminating a conversation only when the model exhausts a simulated
user's patience. We define a suite of metrics capturing the quality of the
interaction process. Using this framework, we construct EvolIF, an evolving
instruction-following benchmark incorporating nine distinct constraint types.
Our results indicate that GPT-5 exhibits superior instruction-following
performance. It sustains an average of 18.54 conversational turns and
demonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant
margin of 11.41%, while other models lag far behind. All of the data and code
will be made publicly available online.

</details>


### [37] [SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties](https://arxiv.org/abs/2511.03542)
*Roberta Di Marino,Giovanni Dioguardi,Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Flora Amato,Vincenzo Moscato*

Main category: cs.CL

TL;DR: SOLVE-Med is a multi-agent system that combines domain-specialized small language models to address challenges in medical question answering, achieving superior performance while enabling local deployment.


<details>
  <summary>Details</summary>
Motivation: Medical question answering systems face challenges such as hallucinations, bias, computational demands, privacy concerns, and the need for specialized expertise across diverse domains.

Method: SOLVE-Med is a multi-agent architecture combining domain-specialized small language models for complex medical queries, including a Router Agent for dynamic specialist selection, ten specialized models fine-tuned on specific medical domains, and an Orchestrator Agent that synthesizes responses.

Result: SOLVE-Med was evaluated on Italian medical forum data across ten specialties and achieved ROUGE-1 of 0.301 and BERTScore F1 of 0.697, outperforming standalone models up to 14B parameters.

Conclusion: SOLVE-Med achieves superior performance compared to standalone models while enabling local deployment, and the code is publicly available.

Abstract: Medical question answering systems face deployment challenges including
hallucinations, bias, computational demands, privacy concerns, and the need for
specialized expertise across diverse domains. Here, we present SOLVE-Med, a
multi-agent architecture combining domain-specialized small language models for
complex medical queries. The system employs a Router Agent for dynamic
specialist selection, ten specialized models (1B parameters each) fine-tuned on
specific medical domains, and an Orchestrator Agent that synthesizes responses.
Evaluated on Italian medical forum data across ten specialties, SOLVE-Med
achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,
outperforming standalone models up to 14B parameters while enabling local
deployment. Our code is publicly available on GitHub:
https://github.com/PRAISELab-PicusLab/SOLVE-Med.

</details>


### [38] [Bearing Syntactic Fruit with Stack-Augmented Neural Networks](https://arxiv.org/abs/2511.03547)
*Brian DuSell,Ryan Cotterell*

Main category: cs.CL

TL;DR: 本文研究了神经网络架构是否能够像人类一样进行层次泛化，并发现通过添加特定类型的栈可以实现这一点。


<details>
  <summary>Details</summary>
Motivation: 研究显示，当人类儿童学习语言时，他们总是倾向于基于层次语法规则的假设，而没有遇到歧义的例子。最近的工作询问常见的神经网络架构是否具有这种偏见，发现它们只有在特殊条件下才会这样做：当语法监督、预训练在大量语料库上或训练超过收敛时。

Method: 我们测试了三种基础架构（transformer、简单RNN、LSTM），并使用两种风格的栈进行增强：Joulin & Mikolov（2015）的叠加栈和DuSell & Chiang（2023）提出的非确定性扩展。

Result: 我们首次展示了神经网络架构，这些架构能够在没有任何上述要求的情况下以类似人类的方式进行泛化：堆栈增强的神经网络。我们发现带有非确定性栈的transformer在这类任务中表现最好。我们还提出了一种对栈RNN架构的修改，以提高层次泛化能力。

Conclusion: 这些结果表明，带有栈的神经网络可能比标准架构更准确地模拟人类语言习得，作为心理语言学研究的对象是有用的。

Abstract: Any finite set of training data is consistent with an infinite number of
hypothetical algorithms that could have generated it. Studies have shown that
when human children learn language, they consistently favor hypotheses based on
hierarchical syntactic rules without ever encountering disambiguating examples.
A recent line of work has inquired as to whether common neural network
architectures share this bias, finding that they do so only under special
conditions: when syntactically supervised, when pre-trained on massive corpora,
or when trained long past convergence. In this paper, we demonstrate, for the
first time, neural network architectures that are able to generalize in
human-like fashion without any of the aforementioned requirements:
stack-augmented neural networks. We test three base architectures (transformer,
simple RNN, LSTM) augmented with two styles of stack: the superposition stack
of Joulin & Mikolov (2015) and a nondeterministic generalization of it proposed
by DuSell & Chiang (2023). We find that transformers with nondeterministic
stacks generalize best out of these architectures on a classical question
formation task. We also propose a modification to the stack RNN architecture
that improves hierarchical generalization. These results suggest that
stack-augmented neural networks may be more accurate models of human language
acquisition than standard architectures, serving as useful objects of
psycholinguistic study. Our code is publicly available.

</details>


### [39] [MultiZebraLogic: A Multilingual Logical Reasoning Benchmark](https://arxiv.org/abs/2511.03553)
*Sofie Helene Bruun,Dan Saattrup Smart*

Main category: cs.CL

TL;DR: 本文创建了一个名为MultiZebraLogic的数据集，用于评估大型语言模型在不同语言和难度下的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 为了评估大型语言模型的全面能力，需要代表多个任务的基准。本文旨在创建大规模、高质量的数据集，以比较不同语言和适合各种推理能力的大型语言模型的逻辑推理技能。

Method: 本文通过生成多种语言、主题、大小的斑马谜题，并包含14种不同的线索类型和8种干扰线索类型，来创建高质量的数据集。

Result: 发现2x3和4x5大小的谜题对GPT-4o mini（非推理模型）和o3-mini（推理模型）具有足够的挑战性。包含5个干扰线索会降低o3-mini在4x5谜题上的准确率15%±7%。o3-mini在4x5谜题上的得分不受使用英语与丹麦语或常见房屋主题与国家特定的smoerrebroed主题的影响。未发现难度与所选线索类型之间的相关性。

Conclusion: 本文提出了一个名为MultiZebraLogic的数据集，用于评估大型语言模型在不同语言和难度下的逻辑推理能力。

Abstract: Measuring the full abilities of large language models (LLMs) requires
benchmarks representing multiple tasks. We aim to create large, high-quality
datasets for comparison of logical reasoning skills across several languages
and of suitable difficulty for LLMs of various reasoning ability. We explore
multiple ways of increasing difficulty. We generate zebra puzzles in multiple
languages, themes, sizes and including 14 different clue types and 8 red
herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are
sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a
reasoning model), respectively. Including 5 red herrings decreases o3-mini
puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5
puzzles are not significantly affected by use of English vs. Danish or the
common houses theme vs. the country-specific smoerrebroed theme. We find no
correlation between difficulty and the selected clue types. Datasets of
128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic
languages for sizes 2x3 and 4x5. We publish code for puzzle generation,
designed for adaptablity into more languages and themes.

</details>


### [40] [AILA--First Experiments with Localist Language Models](https://arxiv.org/abs/2511.03559)
*Joachim Diederich*

Main category: cs.CL

TL;DR: 本文介绍了可控局部性的新架构框架，通过调整局部性旋钮参数实现对表示局部化的连续控制。实验表明，这种框架在保持高性能的同时提高了可解释性，并在中间局部值上实现了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索一种新的架构框架，以实现对语言模型中表示局部化的连续控制，从而在可解释性和性能之间找到平衡点。

Method: 本文提出了第一个可控局部性的实证演示，这是一种新颖的架构框架，可以通过可调局部性旋钮参数连续控制表示局部化的程度。与传统语言模型仅依赖分布式表示不同，我们的方法允许在不需要重新训练模型的情况下，在高度可解释的局部编码和高效的分布式表示之间进行动态插值。

Result: 实验结果表明，局部配置显著降低了注意力熵，λ=1.0时为5.36位，而λ=0.0时为7.18位，同时保持了更高的指针保真度分数，反映了与规则指定目标的更强对齐。预测实验显示，中间局部值优化了可解释性和性能之间的权衡，λ=0.6时测试困惑度为4.65，准确率为84.7%。

Conclusion: 这些发现表明，局部语言模型为需要透明度和能力的受监管领域应用提供了实用的框架，通过显式惩罚阈值和信息理论设计原理实现了对可解释性-性能谱的精确数学控制。

Abstract: This paper presents the first empirical demonstration of controllable
locality in transformer language models, a novel architectural framework that
enables continuous control over the degree of representation localization
through a tunable locality dial parameter. Unlike traditional language models
that rely exclusively on distributed representations, our approach allows
dynamic interpolation between highly interpretable localist encodings and
efficient distributed representations without requiring model retraining. We
conducted experiments on the WikiText corpus using a two-layer transformer
architecture, systematically varying the locality parameter {\lambda} across
the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our
results demonstrate that localist configurations achieve dramatically lower
attention entropy, with {\lambda} = 1.0 yielding 5.36 bits compared to 7.18
bits at {\lambda} = 0.0, while maintaining substantially higher pointer
fidelity scores reflecting stronger alignment with rule-specified targets.
Prediction experiments reveal that intermediate locality values optimize the
tradeoff between interpretability and performance, with {\lambda} = 0.6
achieving test perplexity of 4.65 and accuracy of 84.7%. These findings
establish that localist language models provide a practical framework for
applications in regulated domains requiring both transparency and capability,
offering precise mathematical control over the interpretability-performance
spectrum through explicit penalty thresholds and information-theoretic design
principles.

</details>


### [41] [ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation](https://arxiv.org/abs/2511.03563)
*One Octadion,Bondan Sapta Prakoso,Nanang Yudi Setiawan,Novanto Yudistira*

Main category: cs.CL

TL;DR: 本文探讨了微调大型语言模型以更好地支持政策制定者理解、分析和制定法律规范的方法，并通过集成检索增强生成方法提高了法律研究和法规开发的效果。


<details>
  <summary>Details</summary>
Motivation: 为了更好地支持政策制定者在理解和制定法律规范方面的工作，我们需要一个能够处理法律信息并主动协助政策制定者的工具。

Method: 我们微调了大型语言模型（LLMs），并集成了检索增强生成（RAG）方法，以使模型能够访问和整合最新的法律知识。

Result: 结果表明，这种方法可以显著提高法律研究和法规开发的有效性。

Conclusion: 该方法可以显著提高法律研究和法规开发的有效性，为不断变化的法律领域提供有价值的资源。

Abstract: In this study, we explore the fine-tuning of Large Language Models (LLMs) to
better support policymakers in their crucial work of understanding, analyzing,
and crafting legal regulations. To equip the model with a deep understanding of
legal texts, we curated a supervised dataset tailored to the specific needs of
the legal domain. Additionally, we integrated the Retrieval-Augmented
Generation (RAG) method, enabling the LLM to access and incorporate up-to-date
legal knowledge from external sources. This combination of fine-tuning and
RAG-based augmentation results in a tool that not only processes legal
information but actively assists policymakers in interpreting regulations and
drafting new ones that align with current needs. The results demonstrate that
this approach can significantly enhance the effectiveness of legal research and
regulation development, offering a valuable resource in the ever-evolving field
of law.

</details>


### [42] [Step-Audio-EditX Technical Report](https://arxiv.org/abs/2511.03601)
*Chao Yan,Boyong Wu,Peng Yang,Pengfei Tan,Guoqiang Hu,Yuxin Zhang,Xiangyu,Zhang,Fei Tian,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.CL

TL;DR: Step-Audio-EditX是一个开源的LLM音频模型，能够进行富有表现力和迭代的音频编辑，并且在情感编辑和其他细粒度控制任务中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够进行富有表现力和迭代音频编辑的开源LLM音频模型，同时具备强大的零样本文本到语音能力。

Method: 通过仅使用大边界合成数据，避免了基于嵌入的先验或辅助模块的需求，实现了迭代控制和高表达性。

Result: Step-Audio-EditX在情感编辑和其他细粒度控制任务中表现出色，优于现有的模型。

Conclusion: Step-Audio-EditX在情感编辑和其他细粒度控制任务中超越了MiniMax-2.6-hd和Doubao-Seed-TTS-2.0。

Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model
excelling at expressive and iterative audio editing encompassing emotion,
speaking style, and paralinguistics alongside robust zero-shot text-to-speech
(TTS) capabilities.Our core innovation lies in leveraging only large-margin
synthetic data, which circumvents the need for embedding-based priors or
auxiliary modules. This large-margin learning approach enables both iterative
control and high expressivity across voices, and represents a fundamental pivot
from the conventional focus on representation-level disentanglement. Evaluation
results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and
Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.

</details>


### [43] [A systematic review of relation extraction task since the emergence of Transformers](https://arxiv.org/abs/2511.03610)
*Ringwald Celian,Gandon,Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: 本文对基于Transformer的模型出现后的关系抽取研究进行了系统回顾，分析了34篇综述、64个数据集和104个模型，识别了方法论进展、基准资源以及语义网技术的整合，并指出了当前趋势、局限性和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 本文旨在系统回顾基于Transformer的模型出现以来的关系抽取（RE）研究，提供一个全面的参考以帮助研究人员和从业者了解该领域的演进和未来方向。

Method: 本文使用自动化框架收集和标注出版物，分析了2019年至2024年间发布的34篇综述、64个数据集和104个模型。

Result: 本文分析了方法论进展、基准资源以及语义网技术的整合，并识别了当前趋势、局限性和开放挑战。

Conclusion: 本文通过综合分析多个维度，识别了当前趋势、局限性和开放挑战，为研究人员和从业者提供了理解关系抽取演进和未来方向的全面参考。

Abstract: This article presents a systematic review of relation extraction (RE)
research since the advent of Transformer-based models. Using an automated
framework to collect and annotate publications, we analyze 34 surveys, 64
datasets, and 104 models published between 2019 and 2024. The review highlights
methodological advances, benchmark resources, and the integration of semantic
web technologies. By consolidating results across multiple dimensions, the
study identifies current trends, limitations, and open challenges, offering
researchers and practitioners a comprehensive reference for understanding the
evolution and future directions of RE.

</details>


### [44] [Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability](https://arxiv.org/abs/2511.03635)
*Apoorva Upadhyaya,Wolfgang Nejdl,Marco Fisichella*

Main category: cs.CL

TL;DR: 本文提出了一种新的可解释的零样本立场检测框架IRIS，通过隐式和显式理由来提高模型的可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究在零样本立场检测中存在泛化问题或缺乏文本与目标之间的连贯性，且过度依赖显式推理，解释不够细致。

Method: IRIS框架将立场检测视为信息检索排序任务，通过隐式和显式理由来理解输入对目标的态度。

Result: 在VAST、EZ-STANCE、P-Stance和RFD等基准数据集上的实验表明，IRIS模型在使用50%、30%甚至10%的训练数据时表现良好。

Conclusion: IRIS框架在多个基准数据集上表现出良好的泛化能力，受益于其提出的架构和可解释设计。

Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (LLMs) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help decode the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.

</details>


### [45] [ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation](https://arxiv.org/abs/2511.03656)
*Jing Gao,Shutiao Luo,Yumeng Liu,Yuanming Li,Hongji Zeng*

Main category: cs.CL

TL;DR: 本文介绍了ChiMDQA，一个针对中文文档问答任务的高质量多领域数据集，旨在满足不断增长的NLP需求。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理技术的快速发展，对高质量中文文档问答数据集的需求不断增加。

Method: 通过精心筛选文档和系统化的问题设计方法，确保了数据集的多样性和高质量。

Result: ChiMDQA包含了六个不同领域的长文档，包含6068个经过严格筛选的高质量问答对，并细分为十个细粒度类别。

Conclusion: ChiMDQA是一个高质量的中文多文档问答数据集，适用于多种自然语言处理任务，并为未来的研究和实际应用提供了坚实的基础。

Abstract: With the rapid advancement of natural language processing (NLP) technologies,
the demand for high-quality Chinese document question-answering datasets is
steadily growing. To address this issue, we present the Chinese Multi-Document
Question Answering Dataset(ChiMDQA), specifically designed for downstream
business scenarios across prevalent domains including academic, education,
finance, law, medical treatment, and news. ChiMDQA encompasses long-form
documents from six distinct fields, consisting of 6,068 rigorously curated,
high-quality question-answer (QA) pairs further classified into ten
fine-grained categories. Through meticulous document screening and a systematic
question-design methodology, the dataset guarantees both diversity and high
quality, rendering it applicable to various NLP tasks such as document
comprehension, knowledge extraction, and intelligent QA systems. Additionally,
this paper offers a comprehensive overview of the dataset's design objectives,
construction methodologies, and fine-grained evaluation system, supplying a
substantial foundation for future research and practical applications in
Chinese QA. The code and data are available at:
https://anonymous.4open.science/r/Foxit-CHiMDQA/.

</details>


### [46] [Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in Large Language Models](https://arxiv.org/abs/2511.03699)
*Francesco Corso,Francesco Pierri,Gianmarco De Francisci Morales*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型是否表现出阴谋倾向及其社会人口学偏见，并发现这些模型容易受到操控，这可能带来部署在敏感环境中的风险。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）是否表现出阴谋倾向、是否存在社会人口学偏见以及它们在多大程度上可以被引导采用阴谋视角。

Method: 通过向多个模型施加经过验证的心理测量调查来测量阴谋心态，并在不同的提示和条件策略下进行测试。

Result: LLMs部分同意阴谋信念的元素，社会人口学属性的条件作用产生了不一致的效果，暴露了潜在的人口学偏见。此外，有针对性的提示可以轻松地将模型响应转向阴谋方向。

Conclusion: 研究结果强调了对嵌入在大型语言模型中的心理维度进行批判性评估的重要性，这对于推进计算社会科学和制定针对有害使用的缓解策略都有帮助。

Abstract: In this paper, we investigate whether Large Language Models (LLMs) exhibit
conspiratorial tendencies, whether they display sociodemographic biases in this
domain, and how easily they can be conditioned into adopting conspiratorial
perspectives. Conspiracy beliefs play a central role in the spread of
misinformation and in shaping distrust toward institutions, making them a
critical testbed for evaluating the social fidelity of LLMs. LLMs are
increasingly used as proxies for studying human behavior, yet little is known
about whether they reproduce higher-order psychological constructs such as a
conspiratorial mindset. To bridge this research gap, we administer validated
psychometric surveys measuring conspiracy mindset to multiple models under
different prompting and conditioning strategies. Our findings reveal that LLMs
show partial agreement with elements of conspiracy belief, and conditioning
with socio-demographic attributes produces uneven effects, exposing latent
demographic biases. Moreover, targeted prompts can easily shift model responses
toward conspiratorial directions, underscoring both the susceptibility of LLMs
to manipulation and the potential risks of their deployment in sensitive
contexts. These results highlight the importance of critically evaluating the
psychological dimensions embedded in LLMs, both to advance computational social
science and to inform possible mitigation strategies against harmful uses.

</details>


### [47] [Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask](https://arxiv.org/abs/2511.03718)
*Nan Li,Albert Gatt,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文提出一种视角主义注释方案，用于分析协作对话中的语境误解，并展示了该框架在评估(V)LLMs能力方面的应用。


<details>
  <summary>Details</summary>
Motivation: 协作对话依赖于参与者逐步建立共同语境，但在不对称设置中，他们可能认为自己达成一致，而实际上指的是不同的实体。

Method: 我们引入了一种视角主义注释方案，用于HCRC MapTask语料库，并使用方案约束的LLM注释流程获得了13k个带可靠度估计的参考表达式。

Result: 结果表明，一旦词汇变体被统一，完全的误解很少见，但多重性差异系统地导致分歧，揭示了看似语境一致可能掩盖指代不一致。

Conclusion: 我们的框架为研究基于语境的误解以及评估(V)LLMs在协作对话中建模视角依赖性语境的能力提供了资源和分析工具。

Abstract: Collaborative dialogue relies on participants incrementally establishing
common ground, yet in asymmetric settings they may believe they agree while
referring to different entities. We introduce a perspectivist annotation scheme
for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures
speaker and addressee grounded interpretations for each reference expression,
enabling us to trace how understanding emerges, diverges, and repairs over
time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k
annotated reference expressions with reliability estimates and analyze the
resulting understanding states. The results show that full misunderstandings
are rare once lexical variants are unified, but multiplicity discrepancies
systematically induce divergences, revealing how apparent grounding can mask
referential misalignment. Our framework provides both a resource and an
analytic lens for studying grounded misunderstanding and for evaluating
(V)LLMs' capacity to model perspective-dependent grounding in collaborative
dialogue.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Zero-shot data citation function classification using transformer-based large language models (LLMs)](https://arxiv.org/abs/2511.02936)
*Neil Byers,Ali Zaidi,Valerie Skye,Chris Beecroft,Kjiersten Fagnan*

Main category: cs.LG

TL;DR: 本文利用大语言模型自动识别文献中数据的使用情况，并提出了一个新的评估框架，结果显示模型具有一定的效果，但也面临一些挑战。


<details>
  <summary>Details</summary>
Motivation: 近年来，识别特定数据集与科学文献之间的关联变得越来越重要。了解一篇出版物引用了一组数据后，下一步是探索这些数据是如何或为什么被使用的。

Method: 应用开源大语言模型Llama 3.1-405B来生成结构化的数据使用案例标签，并引入了一个新的评估框架来确定方法的有效性。

Result: 该模型在零样本数据引用分类任务中达到了0.674的F1分数，而无需先前定义的类别。

Conclusion: 虽然结果有希望，但数据可用性、提示过拟合、计算基础设施和进行负责任的性能评估所需的费用等障碍限制了方法的有效性。

Abstract: Efforts have increased in recent years to identify associations between
specific datasets and the scientific literature that incorporates them. Knowing
that a given publication cites a given dataset, the next logical step is to
explore how or why that data was used. Advances in recent years with
pretrained, transformer-based large language models (LLMs) offer potential
means for scaling the description of data use cases in the published
literature. This avoids expensive manual labeling and the development of
training datasets for classical machine-learning (ML) systems. In this work we
apply an open-source LLM, Llama 3.1-405B, to generate structured data use case
labels for publications known to incorporate specific genomic datasets. We also
introduce a novel evaluation framework for determining the efficacy of our
methods. Our results demonstrate that the stock model can achieve an F1 score
of .674 on a zero-shot data citation classification task with no previously
defined categories. While promising, our results are qualified by barriers
related to data availability, prompt overfitting, computational infrastructure,
and the expense required to conduct responsible performance evaluation.

</details>


### [49] [The Curved Spacetime of Transformer Architectures](https://arxiv.org/abs/2511.03060)
*Riccardo Di Sipio,Jairo Diaz-Rodriguez,Luis Serrano*

Main category: cs.LG

TL;DR: 该研究将Transformer模型与广义相对论类比，揭示了注意力机制在嵌入空间中的曲率效应，并通过实验验证了这些预测。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在理解Transformer模型中注意力机制的几何性质，并探索其在嵌入空间中的曲率效应。

Method: 该研究提出了一种几何框架，将查询和键视为表示空间上的有效度量，注意力作为离散连接实现值向量的平行传输。堆叠层提供了通过该弯曲流形的离散时间切片，而反向传播则扮演最小作用原理的角色。

Result: 实验结果表明，token嵌入在特征空间中并非沿直线移动，而是随着嵌入空间曲率的变化而弯曲和重新定向。此外，通过可视化和模拟验证了曲率的存在及其影响。

Conclusion: 该研究通过将Transformer模型与广义相对论进行类比，揭示了注意力机制在嵌入空间中的曲率效应，并通过实验验证了这些预测。

Abstract: We present a geometric framework for understanding Transformer-based language
models, drawing an explicit analogy to General Relativity. Queries and keys
induce an effective metric on representation space, and attention acts as a
discrete connection that implements parallel transport of value vectors across
tokens. Stacked layers provide discrete time-slices through which token
representations evolve on this curved manifold, while backpropagation plays the
role of a least-action principle that shapes loss-minimizing trajectories in
parameter space. If this analogy is correct, token embeddings should not
traverse straight paths in feature space; instead, their layer-wise steps
should bend and reorient as interactions mediated by embedding space curvature.
To test this prediction, we design experiments that expose both the presence
and the consequences of curvature: (i) we visualize a curvature landscape for a
full paragraph, revealing how local turning angles vary across tokens and
layers; (ii) we show through simulations that excess counts of sharp/flat
angles and longer length-to-chord ratios are not explainable by dimensionality
or chance; and (iii) inspired by Einstein's eclipse experiment, we probe
deflection under controlled context edits, demonstrating measurable,
meaning-consistent bends in embedding trajectories that confirm
attention-induced curvature.

</details>


### [50] [From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation](https://arxiv.org/abs/2511.03128)
*Najrin Sultana,Md Rafi Ur Rashid,Kang Gu,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 本文提出了一种系统方法，用于评估LLM的鲁棒性，并介绍了两种生成对抗样本的框架。


<details>
  <summary>Details</summary>
Motivation: 当将这些模型应用于敏感任务时，必须彻底评估它们对对抗输入的鲁棒性。

Method: 本文介绍了Static Deceptor (StaDec)和Dynamic Deceptor (DyDec)，两种创新的攻击框架，用于系统地生成动态和自适应的对抗样本。

Result: 我们生成了微妙且看起来自然的对抗输入，同时保持与原始文本的语义相似性，并有效欺骗目标LLM。

Conclusion: 本文提供了一种系统的方法来评估LLM的鲁棒性，并通过释放代码和数据促进进一步的研究。

Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a
simple task prompt, eliminating the need for training or fine-tuning. However,
when applying these models to sensitive tasks, it is crucial to thoroughly
assess their robustness against adversarial inputs. In this work, we introduce
Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack
frameworks designed to systematically generate dynamic and adaptive adversarial
examples by leveraging the understanding of the LLMs. We produce subtle and
natural-looking adversarial inputs that preserve semantic similarity to the
original text while effectively deceiving the target LLM. By utilizing an
automated, LLM-driven pipeline, we eliminate the dependence on external
heuristics. Our attacks evolve with the advancements in LLMs and demonstrate
strong transferability across models unknown to the attacker. Overall, this
work provides a systematic approach for the self-assessment of an LLM's
robustness. We release our code and data at
https://github.com/Shukti042/AdversarialExample.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [51] [From Measurement to Expertise: Empathetic Expert Adapters for Context-Based Empathy in Conversational AI Agents](https://arxiv.org/abs/2511.03143)
*Erfan Shayegani,Jina Suh,Andy Wilson,Nagu Rangan,Javier Hernandez*

Main category: cs.HC

TL;DR: 本文提出了一种新的框架，用于开发和评估特定上下文的共情大型语言模型。通过分析真实对话数据集并开发合成对话生成管道，我们训练了共情专家适配器，显著减少了感知到的和期望的共情之间的差距，并在保持共情模式方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 共情是促进对话AI中积极用户体验的关键因素。虽然模型可以展示共情，但通常比较通用，而不是针对特定任务和上下文。本文旨在解决这一问题，提出一种新的框架来开发和评估特定上下文的共情大型语言模型。

Method: 本文首先分析了一个包含8个任务的672次多轮对话的真实对话数据集，揭示了对话前后预期和体验到的共情之间的显著差异。然后，我们开发了一个合成的多轮对话生成管道，并根据上下文引导响应以更接近用户的期望。最后，我们训练了针对特定上下文的共情专家适配器，这些适配器根据识别的任务专门处理不同水平的共情。

Result: 实验结果表明，感知到的和期望的共情之间的差距减少了72.66%，得分平均提高了2.43倍。此外，训练后的共情专家适配器在保持共情模式方面表现出色，优于系统提示，后者在对话长度增加时效果显著下降。

Conclusion: 本文提出了一个新颖的框架，用于开发和评估特定上下文的共情大型语言模型（LLMs）。实验结果表明，该方法显著减少了感知到的和期望的共情之间的差距，并在保持共情模式方面表现出色。

Abstract: Empathy is a critical factor in fostering positive user experiences in
conversational AI. While models can display empathy, it is often generic rather
than tailored to specific tasks and contexts. In this work, we introduce a
novel framework for developing and evaluating context-specific empathetic large
language models (LLMs). We first analyze a real-world conversational dataset
consisting of 672 multi-turn conversations across 8 tasks, revealing
significant differences in terms of expected and experienced empathy before and
after the conversations, respectively. To help minimize this gap, we develop a
synthetic multi-turn conversational generation pipeline and steer responses
toward our defined empathy patterns based on the context that more closely
matches users' expectations. We then train empathetic expert adapters for
context-specific empathy that specialize in varying empathy levels based on the
recognized task. Our empirical results demonstrate a significant gap reduction
of 72.66% between perceived and desired empathy with scores increasing by an
average factor of 2.43 as measured by our metrics and reward models.
Additionally, our trained empathetic expert adapters demonstrate superior
effectiveness in preserving empathy patterns throughout conversation turns,
outperforming system prompts, which tend to dramatically diminish in impact as
conversations lengthen.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [52] [LiveTradeBench: Seeking Real-World Alpha with Large Language Models](https://arxiv.org/abs/2511.03628)
*Haofei Yu,Fenghai Li,Jiaxuan You*

Main category: q-fin.TR

TL;DR: 本文介绍了LiveTradeBench，一个用于评估大型语言模型在现实市场中表现的环境，结果显示静态评估与实际能力之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 现有的测试在静态环境中进行，缺乏真实动态和不确定性，因此无法评估在不确定性下的决策能力。

Method: 引入LiveTradeBench，一个用于评估LLM代理在现实和不断变化的市场中的环境。

Result: 高LMArena分数并不意味着优越的交易结果；模型表现出不同的投资组合风格，反映风险偏好和推理动态；一些LLM能够有效利用实时信号来适应决策。

Conclusion: 这些发现揭示了静态评估与现实世界能力之间的差距，促使开发测试顺序决策和在实时不确定性下保持一致性的基准。

Abstract: Large language models (LLMs) achieve strong performance across
benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but
these tests occur in static settings, lacking real dynamics and uncertainty.
Consequently, they evaluate isolated reasoning or problem-solving rather than
decision-making under uncertainty. To address this, we introduce
LiveTradeBench, a live trading environment for evaluating LLM agents in
realistic and evolving markets. LiveTradeBench follows three design principles:
(i) Live data streaming of market prices and news, eliminating dependence on
offline backtesting and preventing information leakage while capturing
real-time uncertainty; (ii) a portfolio-management abstraction that extends
control from single-asset actions to multi-asset allocation, integrating risk
management and cross-asset reasoning; and (iii) multi-market evaluation across
structurally distinct environments--U.S. stocks and Polymarket prediction
markets--differing in volatility, liquidity, and information flow. At each
step, an agent observes prices, news, and its portfolio, then outputs
percentage allocations that balance risk and return. Using LiveTradeBench, we
run 50-day live evaluations of 21 LLMs across families. Results show that (1)
high LMArena scores do not imply superior trading outcomes; (2) models display
distinct portfolio styles reflecting risk appetite and reasoning dynamics; and
(3) some LLMs effectively leverage live signals to adapt decisions. These
findings expose a gap between static evaluation and real-world competence,
motivating benchmarks that test sequential decision making and consistency
under live uncertainty.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [53] [Let the Bees Find the Weak Spots: A Path Planning Perspective on Multi-Turn Jailbreak Attacks against LLMs](https://arxiv.org/abs/2511.03271)
*Yize Liu,Yunyun Hou,Aina Sui*

Main category: cs.CR

TL;DR: 本文提出了一种新的多轮越狱攻击方法，通过动态加权图拓扑和改进的ABC算法，提高了攻击效率并减少了查询次数。


<details>
  <summary>Details</summary>
Motivation: 现有研究在多轮越狱攻击中缺乏对成功对话轨迹的探索，并且忽略了攻击过程中的高昂开销。本文旨在解决这些问题，提高攻击路径搜索的效率并减少查询次数。

Method: 本文首先引入了一个基于动态加权图拓扑的理论模型，将多轮攻击过程抽象为路径规划问题。然后，提出了ABC算法，这是一种改进的人工蜂群算法，具有协作搜索机制，包括采蜜蜂、观察蜂和侦察蜂。

Result: 在三个开源和两个专有语言模型上的实证评估表明，本文的方法效果显著，攻击成功率超过90%，最高达到98%（在GPT-3.5-Turbo上），并且仅需平均26次查询即可达到相当的成功率。

Conclusion: 本文提出了一种基于动态加权图拓扑的理论模型，并引入了ABC算法，用于多轮越狱攻击。该方法在多个语言模型上表现出色，攻击成功率超过90%，并且显著减少了红队评估的开销。

Abstract: Large Language Models (LLMs) have been widely deployed across various
applications, yet their potential security and ethical risks have raised
increasing concerns. Existing research employs red teaming evaluations,
utilizing multi-turn jailbreaks to identify potential vulnerabilities in LLMs.
However, these approaches often lack exploration of successful dialogue
trajectories within the attack space, and they tend to overlook the
considerable overhead associated with the attack process. To address these
limitations, this paper first introduces a theoretical model based on
dynamically weighted graph topology, abstracting the multi-turn attack process
as a path planning problem. Based on this framework, we propose ABC, an
enhanced Artificial Bee Colony algorithm for multi-turn jailbreaks, featuring a
collaborative search mechanism with employed, onlooker, and scout bees. This
algorithm significantly improves the efficiency of optimal attack path search
while substantially reducing the average number of queries required. Empirical
evaluations on three open-source and two proprietary language models
demonstrate the effectiveness of our approach, achieving attack success rates
above 90\% across the board, with a peak of 98\% on GPT-3.5-Turbo, and
outperforming existing baselines. Furthermore, it achieves comparable success
with only 26 queries on average, significantly reducing red teaming overhead
and highlighting its superior efficiency.

</details>


### [54] [Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology](https://arxiv.org/abs/2511.03641)
*Thomas Souverain*

Main category: cs.CR

TL;DR: 本文分析了欧盟AI法案对LLM水印方法的要求，并提出了一个分类法、评估框架和比较方法，以帮助理解和改进水印技术。


<details>
  <summary>Details</summary>
Motivation: 由于水印方法的快速演变和多样性，难以确定如何将欧盟AI法案的四个标准转化为具体的可衡量评估。因此，我们需要一种系统的方法来分析和比较这些水印方法。

Method: 我们提出了一个基于LLM生命周期阶段的水印方法分类法，并根据欧盟AI法案的要求对水印方法进行了评估和比较。

Result: 我们的研究发现，目前没有一种方法能够满足欧盟AI法案的四个标准，但我们建议进一步研究直接嵌入LLM低级架构的水印方法。

Conclusion: 我们的研究展示了目前的LLM水印方法尚未满足欧盟AI法案的四个标准，但鼓励通过嵌入LLM低级架构的水印方法进行进一步研究。

Abstract: To foster trustworthy Artificial Intelligence (AI) within the European Union,
the AI Act requires providers to mark and detect the outputs of their
general-purpose models. The Article 50 and Recital 133 call for marking methods
that are ''sufficiently reliable, interoperable, effective and robust''. Yet,
the rapidly evolving and heterogeneous landscape of watermarks for Large
Language Models (LLMs) makes it difficult to determine how these four standards
can be translated into concrete and measurable evaluations. Our paper addresses
this challenge, anchoring the normativity of European requirements in the
multiplicity of watermarking techniques. Introducing clear and distinct
concepts on LLM watermarking, our contribution is threefold. (1) Watermarking
Categorisation: We propose an accessible taxonomy of watermarking methods
according to the stage of the LLM lifecycle at which they are applied - before,
during, or after training, and during next-token distribution or sampling. (2)
Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping
each criterion with state-of-the-art evaluations on robustness and
detectability of the watermark, and of quality of the LLM. Since
interoperability remains largely untheorised in LLM watermarking research, we
propose three normative dimensions to frame its assessment. (3) Watermarking
Comparison: We compare current watermarking methods for LLMs against the
operationalised European criteria and show that no approach yet satisfies all
four standards. Encouraged by emerging empirical tests, we recommend further
research into watermarking directly embedded within the low-level architecture
of LLMs.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [55] [Beyond Citations: Measuring Idea-level Knowledge Diffusion from Research to Journalism and Policy-making](https://arxiv.org/abs/2511.03378)
*Yangliu Fan,Kilian Buehling,Volker Stocker*

Main category: cs.SI

TL;DR: 本研究使用文本分析方法，探讨了社会科学知识在学术、新闻和政策领域的扩散模式，发现不同知识概念的扩散路径和语义变化存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 由于社会科学知识的扩散对多个利益相关者具有重要意义，但目前缺乏有效的测量方法，因此本研究旨在探索一种新的文本分析方法来衡量知识的扩散。

Method: 本研究采用基于文本的方法，分析了社会科学研究知识在学术、新闻和政策制定领域的扩散情况，并通过嵌入回归方法比较了不同领域中的语境意义。

Result: 研究发现，不同知识概念在不同领域间的扩散模式和动态差异较大，且研究与政策之间的语义距离通常大于研究与新闻之间的距离。此外，知识在不同领域中的角色也发生了变化。

Conclusion: 本研究揭示了社会科学知识在不同领域之间的扩散模式和动态，强调了超越直接引用的测量方法的重要性。

Abstract: Despite the importance of social science knowledge for various stakeholders,
measuring its diffusion into different domains remains a challenge. This study
uses a novel text-based approach to measure the idea-level diffusion of social
science knowledge from the research domain to the journalism and policy-making
domains. By doing so, we expand the detection of knowledge diffusion beyond the
measurements of direct references. Our study focuses on media effects theories
as key research ideas in the field of communication science. Using 72,703
documents (2000-2019) from three domains (i.e., research, journalism, and
policy-making) that mention these ideas, we count the mentions of these ideas
in each domain, estimate their domain-specific contexts, and track and compare
differences across domains and over time. Overall, we find that diffusion
patterns and dynamics vary considerably between ideas, with some ideas
diffusing between other domains, while others do not. Based on the embedding
regression approach, we compare contextualized meanings across domains and find
that the distances between research and policy are typically larger than
between research and journalism. We also find that ideas largely shift roles
across domains - from being the theories themselves in research to sense-making
in news to applied, administrative use in policy. Over time, we observe
semantic convergence mainly for ideas that are practically oriented. Our
results characterize the cross-domain diffusion patterns and dynamics of social
science knowledge at the idea level, and we discuss the implications for
measuring knowledge diffusion beyond citations.

</details>

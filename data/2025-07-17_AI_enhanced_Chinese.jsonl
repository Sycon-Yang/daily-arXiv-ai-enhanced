{"id": "2507.11582", "pdf": "https://arxiv.org/pdf/2507.11582", "abs": "https://arxiv.org/abs/2507.11582", "authors": ["Kazuyoshi Otsuka"], "title": "Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance", "categories": ["cs.CL"], "comment": "38 pages. Manuscript submitted for review to the Journal of\n  Computational Literary Studies (JCLS)", "summary": "This study positions large language models (LLMs) as \"subjective literary\ncritics\" to explore aesthetic preferences and evaluation patterns in literary\nassessment. Ten Japanese science fiction short stories were translated into\nEnglish and evaluated by six state-of-the-art LLMs across seven independent\nsessions. Principal component analysis and clustering techniques revealed\nsignificant variations in evaluation consistency ({\\alpha} ranging from 1.00 to\n0.35) and five distinct evaluation patterns. Additionally, evaluation variance\nacross stories differed by up to 4.5-fold, with TF-IDF analysis confirming\ndistinctive evaluation vocabularies for each model. Our seven-session\nwithin-day protocol using an original Science Fiction corpus strategically\nminimizes external biases, allowing us to observe implicit value systems shaped\nby RLHF and their influence on literary judgment. These findings suggest that\nLLMs may possess individual evaluation characteristics similar to human\ncritical schools, rather than functioning as neutral benchmarkers.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u201c\u4e3b\u89c2\u6587\u5b66\u8bc4\u8bba\u5bb6\u201d\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u6587\u5b66\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u72ec\u7279\u7684\u8bc4\u4f30\u7279\u5f81\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u6279\u8bc4\u5b66\u6d3e\uff0c\u800c\u975e\u4e2d\u7acb\u7684\u57fa\u51c6\u5668\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6587\u5b66\u8bc4\u4f30\u4e2d\u7684\u5ba1\u7f8e\u504f\u597d\u548c\u8bc4\u4f30\u6a21\u5f0f\uff0c\u5e76\u4e86\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5b66\u5224\u65ad\u4e2d\u7684\u9690\u542b\u4ef7\u503c\u4f53\u7cfb\u3002", "method": "\u672c\u7814\u7a76\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b9a\u4f4d\u4e3a\u201c\u4e3b\u89c2\u6587\u5b66\u8bc4\u8bba\u5bb6\u201d\uff0c\u901a\u8fc7\u4e3b\u6210\u5206\u5206\u6790\u548c\u805a\u7c7b\u6280\u672f\u5206\u6790\u4e86\u5341\u7bc7\u65e5\u672c\u79d1\u5e7b\u77ed\u7bc7\u5c0f\u8bf4\u7684\u8bc4\u4f30\u4e00\u81f4\u6027\u4ee5\u53ca\u4e94\u79cd\u4e0d\u540c\u7684\u8bc4\u4f30\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528TF-IDF\u5206\u6790\u786e\u8ba4\u4e86\u6bcf\u79cd\u6a21\u578b\u7684\u72ec\u7279\u8bc4\u4f30\u8bcd\u6c47\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bc4\u4f30\u4e00\u81f4\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08\u03b1\u8303\u56f4\u4ece1.00\u52300.35\uff09\uff0c\u5e76\u4e14\u8bc4\u4f30\u65b9\u5dee\u5728\u4e0d\u540c\u6545\u4e8b\u4e4b\u95f4\u76f8\u5dee\u591a\u8fbe4.5\u500d\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u4e86\u4e94\u79cd\u4e0d\u540c\u7684\u8bc4\u4f30\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u5177\u6709\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u6279\u8bc4\u5b66\u6d3e\u7684\u4e2a\u4f53\u8bc4\u4f30\u7279\u5f81\uff0c\u800c\u4e0d\u662f\u4f5c\u4e3a\u4e2d\u7acb\u7684\u57fa\u51c6\u5668\u8fd0\u4f5c\u3002"}}
{"id": "2507.11625", "pdf": "https://arxiv.org/pdf/2507.11625", "abs": "https://arxiv.org/abs/2507.11625", "authors": ["Varun Srivastava", "Fan Lei", "Srija Mukhopadhyay", "Vivek Gupta", "Ross Maciejewski"], "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Published as a conference paper at COLM 2025", "summary": "Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86MapIQ\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728Map-VQA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u4e86\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u7684Map-VQA\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7b49\u503c\u7ebf\u56fe\u4e0a\uff0c\u8fd9\u9650\u5236\u4e86\u4e3b\u9898\u7c7b\u522b\u548c\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u7684\u8303\u56f4\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\uff0c\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6MapIQ\uff0c\u4ee5\u6db5\u76d6\u66f4\u591a\u5730\u56fe\u7c7b\u578b\u548c\u4e3b\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MapIQ\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u516d\u4e2a\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u8fdb\u884c\u4e86\u989d\u5916\u7684\u5b9e\u9a8c\u6765\u7814\u7a76\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u591a\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u516d\u4e2a\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7814\u7a76\u53d1\u73b0\u5b83\u4eec\u5728Map-VQA\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5b58\u5728\u5dee\u5f02\uff0c\u5e76\u4e14\u5bf9\u5730\u56fe\u8bbe\u8ba1\u7684\u53d8\u5316\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86MapIQ\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b14,706\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u4e09\u79cd\u5730\u56fe\u7c7b\u578b\u548c\u516d\u4e2a\u4e0d\u540c\u7684\u4e3b\u9898\u3002\u901a\u8fc7\u8bc4\u4f30\u591a\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u516d\u4e2a\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u8fd9\u4e9b\u6a21\u578b\u5728Map-VQA\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u5bf9\u5730\u56fe\u8bbe\u8ba1\u53d8\u5316\u7684\u654f\u611f\u6027\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.11634", "pdf": "https://arxiv.org/pdf/2507.11634", "abs": "https://arxiv.org/abs/2507.11634", "authors": ["Farideh Majidi", "Ziaeddin Beheshtifard"], "title": "Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "Proceedings of the First National Conference on Artificial\n  Intelligence and Emerging Research: Convergence of Humans and Intelligent\n  Systems", "summary": "This research examines cross-lingual sentiment analysis using few-shot\nlearning and incremental learning methods in Persian. The main objective is to\ndevelop a model capable of performing sentiment analysis in Persian using\nlimited data, while getting prior knowledge from high-resource languages. To\nachieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and\nDistilBERT) were employed, which were fine-tuned using few-shot and incremental\nlearning approaches on small samples of Persian data from diverse sources,\nincluding X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled\nthe models to learn from a broad range of contexts. Experimental results show\nthat the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%\naccuracy on Persian sentiment analysis. These findings highlight the\neffectiveness of combining few-shot learning and incremental learning with\nmultilingual pre-trained models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6ce2\u65af\u8bed\u4e2d\u4f7f\u7528\u5c11\u91cf\u5b66\u4e60\u548c\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8de8\u8bed\u8a00\u60c5\u611f\u5206\u6790\uff0c\u7ed3\u679c\u8868\u660e\u7ed3\u5408\u8fd9\u4e9b\u65b9\u6cd5\u4e0e\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u6709\u6548\u7684\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u4e0b\u6267\u884c\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u7684\u6a21\u578b\uff0c\u540c\u65f6\u4ece\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528\u5c11\u91cf\u5b66\u4e60\u548c\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4e09\u4e2a\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\uff08XLM-RoBERTa\u3001mDeBERTa \u548c DistilBERT\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5728\u6709\u9650\u7684\u6ce2\u65af\u8bed\u6570\u636e\u4e0a\u8fdb\u884c\u60c5\u611f\u5206\u6790\u3002", "result": "mDeBERTa \u548c XLM-RoBERTa \u5728\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u4e2d\u8fbe\u5230\u4e86 96% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5c06\u5c11\u91cf\u5b66\u4e60\u548c\u589e\u91cf\u5b66\u4e60\u4e0e\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7ed3\u5408\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2507.11661", "pdf": "https://arxiv.org/pdf/2507.11661", "abs": "https://arxiv.org/abs/2507.11661", "authors": ["Guimin Hu", "Yi Xin", "Lijie Hu", "Zhihong Zhu", "Hasti Seifi"], "title": "Partitioner Guided Modal Learning Framework", "categories": ["cs.CL", "cs.AI"], "comment": "acm multimedia 2025", "summary": "Multimodal learning benefits from multiple modal information, and each\nlearned modal representations can be divided into uni-modal that can be learned\nfrom uni-modal training and paired-modal features that can be learned from\ncross-modal interaction. Building on this perspective, we propose a\npartitioner-guided modal learning framework, PgM, which consists of the modal\npartitioner, uni-modal learner, paired-modal learner, and uni-paired modal\ndecoder. Modal partitioner segments the learned modal representation into\nuni-modal and paired-modal features. Modal learner incorporates two dedicated\ncomponents for uni-modal and paired-modal learning. Uni-paired modal decoder\nreconstructs modal representation based on uni-modal and paired-modal features.\nPgM offers three key benefits: 1) thorough learning of uni-modal and\npaired-modal features, 2) flexible distribution adjustment for uni-modal and\npaired-modal representations to suit diverse downstream tasks, and 3) different\nlearning rates across modalities and partitions. Extensive experiments\ndemonstrate the effectiveness of PgM across four multimodal tasks and further\nhighlight its transferability to existing models. Additionally, we visualize\nthe distribution of uni-modal and paired-modal features across modalities and\ntasks, offering insights into their respective contributions.", "AI": {"tldr": "PgM is a framework for multimodal learning that enhances performance by effectively capturing and utilizing both uni-modal and paired-modal features.", "motivation": "The motivation behind PgM is to improve multimodal learning by better capturing and utilizing both uni-modal and paired-modal features, which can enhance performance and adaptability in diverse downstream tasks.", "method": "PgM is a partitioner-guided modal learning framework that includes a modal partitioner, uni-modal learner, paired-modal learner, and uni-paired modal decoder. It segments modal representations into uni-modal and paired-modal features and allows for flexible distribution adjustment and different learning rates across modalities.", "result": "Experiments demonstrate the effectiveness of PgM across four multimodal tasks and its transferability to existing models. Visualization of feature distributions provides insights into their contributions.", "conclusion": "PgM provides a framework for multimodal learning that effectively captures both uni-modal and paired-modal features, offering benefits in flexibility, adaptability, and performance across various tasks."}}
{"id": "2507.11694", "pdf": "https://arxiv.org/pdf/2507.11694", "abs": "https://arxiv.org/abs/2507.11694", "authors": ["Maximiliano Hormaz\u00e1bal Lagos", "\u00c1lvaro Bueno S\u00e1ez", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro", "H\u00e9ctor Cerezo-Costas"], "title": "ExpliCIT-QA: Explainable Code-Based Image Table Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "This work has been accepted for presentation at the 24nd Portuguese\n  Conference on Artificial Intelligence (EPIA 2025) and will be published in\n  the proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We present ExpliCIT-QA, a system that extends our previous MRT approach for\ntabular question answering into a multimodal pipeline capable of handling\ncomplex table images and providing explainable answers. ExpliCIT-QA follows a\nmodular design, consisting of: (1) Multimodal Table Understanding, which uses a\nChain-of-Thought approach to extract and transform content from table images;\n(2) Language-based Reasoning, where a step-by-step explanation in natural\nlanguage is generated to solve the problem; (3) Automatic Code Generation,\nwhere Python/Pandas scripts are created based on the reasoning steps, with\nfeedback for handling errors; (4) Code Execution to compute the final answer;\nand (5) Natural Language Explanation that describes how the answer was\ncomputed. The system is built for transparency and auditability: all\nintermediate outputs, parsed tables, reasoning steps, generated code, and final\nanswers are available for inspection. This strategy works towards closing the\nexplainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on\nthe TableVQA-Bench benchmark, comparing it with existing baselines. We\ndemonstrated improvements in interpretability and transparency, which open the\ndoor for applications in sensitive domains like finance and healthcare where\nauditing results are critical.", "AI": {"tldr": "ExpliCIT-QA \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u8868\u683c\u56fe\u50cf\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7b54\u6848\uff0c\u63d0\u9ad8\u4e86\u8868\u683c\u95ee\u7b54\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u8868\u683cVQA\u7cfb\u7edf\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u900f\u660e\u548c\u53ef\u5ba1\u8ba1\u7684\u65b9\u6cd5\u3002", "method": "ExpliCIT-QA \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u5305\u62ec\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\u3001\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\u3001\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u3001\u4ee3\u7801\u6267\u884c\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "ExpliCIT-QA \u5728 TableVQA-Bench \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "ExpliCIT-QA \u63d0\u9ad8\u4e86\u8868\u683c\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4e3a\u91d1\u878d\u548c\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\u63d0\u4f9b\u4e86\u5e94\u7528\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.11742", "pdf": "https://arxiv.org/pdf/2507.11742", "abs": "https://arxiv.org/abs/2507.11742", "authors": ["Meng Li", "Timothy M. McPhillips", "Dingmin Wang", "Shin-Rong Tsai", "Bertram Lud\u00e4scher"], "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. Accepted to COLM 2025", "summary": "Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.", "AI": {"tldr": "CRABS is a strategy that combines syntactic analysis and LLMs to accurately understand Python notebooks, achieving high accuracy in identifying information flows and execution dependencies.", "motivation": "The need to evaluate, reuse, and adapt Python notebooks for new tasks requires understanding their information flows and operations, but re-execution is often impractical due to data and software dependencies. LLMs have limitations in understanding realistic notebooks due to hallucinations and long-context challenges.", "method": "CRABS uses shallow syntactic parsing and analysis of the abstract syntax tree (AST) to capture the correct interpretation of a notebook between lower and upper estimates of the inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via cell-by-cell zero-shot learning.", "result": "CRABS achieves average F1 scores of 98% identifying cell-to-cell information flows and 99% identifying transitive cell execution dependencies. The LLM correctly resolves 98% of ambiguities left by analyzing the syntactic structure of the notebooks.", "conclusion": "CRABS achieves high accuracy in identifying cell-to-cell information flows and transitive cell execution dependencies, demonstrating the effectiveness of combining syntactic analysis with LLMs for understanding Python notebooks."}}
{"id": "2507.11764", "pdf": "https://arxiv.org/pdf/2507.11764", "abs": "https://arxiv.org/abs/2507.11764", "authors": ["Matteo Fasulo", "Luca Babboni", "Luca Tedeschini"], "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles", "categories": ["cs.CL", "cs.IR"], "comment": "14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab", "summary": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AI Wizards\u5728CLEF 2025 CheckThat! Lab\u4efb\u52a11\u4e2d\u7684\u53c2\u4e0e\uff0c\u901a\u8fc7\u7ed3\u5408\u60c5\u611f\u5206\u6570\u548c\u53e5\u5b50\u8868\u793a\u6765\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u8bed\u8a00\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u65b0\u95fb\u6587\u7ae0\u4e2d\u4e3b\u89c2/\u5ba2\u89c2\u53e5\u5b50\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u3002", "method": "\u901a\u8fc7\u5c06\u60c5\u611f\u5206\u6570\u4e0e\u53e5\u5b50\u8868\u793a\u76f8\u7ed3\u5408\uff0c\u589e\u5f3a\u4e86\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u5206\u7c7b\u5668\uff0c\u5e76\u91c7\u7528\u4e86\u51b3\u7b56\u9608\u503c\u6821\u51c6\u6765\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u60c5\u611f\u7279\u5f81\u7684\u6574\u5408\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4e3b\u89c2F1\u5206\u6570\u4e0a\u3002\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u8bed\u8a00\u4e2d\u53d6\u5f97\u4e86\u9ad8\u6392\u540d\uff0c\u5305\u62ec\u5e0c\u814a\u8bed\u7684\u7b2c\u4e00\u540d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5e0c\u814a\u8bed\u4e2d\u53d6\u5f97\u4e86\u7b2c\u4e00\u540d\u7684\u597d\u6210\u7ee9\u3002"}}
{"id": "2507.11809", "pdf": "https://arxiv.org/pdf/2507.11809", "abs": "https://arxiv.org/abs/2507.11809", "authors": ["Dante Campregher", "Yanxu Chen", "Sander Hoffman", "Maria Heuss"], "title": "Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 Pages, 13 figures", "summary": "This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5904\u7406\u7ade\u4e89\u6027\u7684\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u4fe1\u606f\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u5934\u4fc3\u8fdb\u4e8b\u5b9e\u8f93\u51fa\u662f\u901a\u8fc7\u4e00\u822c\u590d\u5236\u6291\u5236\u800c\u975e\u9009\u62e9\u6027\u53cd\u4e8b\u5b9e\u6291\u5236\uff0c\u5e76\u4e14\u6ce8\u610f\u529b\u5934\u7684\u884c\u4e3a\u5177\u6709\u9886\u57df\u4f9d\u8d56\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u5904\u7406\u7ade\u4e89\u6027\u7684\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u4fe1\u606f\uff0c\u7279\u522b\u662f\u6ce8\u610f\u529b\u5934\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u6211\u4eec\u5c1d\u8bd5\u91cd\u73b0\u548c\u8c03\u548cOrtu\u7b49\u4eba\u3001Yu\u3001Merullo\u548cPavlick\u4ee5\u53caMcDougall\u7b49\u4eba\u6700\u8fd1\u7684\u7814\u7a76\u6210\u679c\uff0c\u8fd9\u4e9b\u7814\u7a76\u901a\u8fc7\u673a\u5236\u89e3\u91ca\u5de5\u5177\u8c03\u67e5\u4e86\u6a21\u578b\u5b66\u4e60\u7684\u4e8b\u5b9e\u4e0e\u77db\u76fe\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e4b\u95f4\u7684\u7ade\u4e89\u3002\u6211\u4eec\u7684\u7814\u7a76\u7279\u522b\u8003\u5bdf\u4e86\u6ce8\u610f\u529b\u5934\u5f3a\u5ea6\u4e0e\u4e8b\u5b9e\u8f93\u51fa\u6bd4\u4f8b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8bc4\u4f30\u4e86\u5173\u4e8e\u6ce8\u610f\u529b\u5934\u6291\u5236\u673a\u5236\u7684\u7ade\u4e89\u5047\u8bbe\uff0c\u5e76\u7814\u7a76\u4e86\u8fd9\u4e9b\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u9886\u57df\u7279\u5f02\u6027\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4fc3\u8fdb\u4e8b\u5b9e\u8f93\u51fa\u7684\u6ce8\u610f\u529b\u5934\u662f\u901a\u8fc7\u4e00\u822c\u7684\u590d\u5236\u6291\u5236\u800c\u4e0d\u662f\u9009\u62e9\u6027\u53cd\u4e8b\u5b9e\u6291\u5236\u6765\u5b9e\u73b0\u7684\uff0c\u56e0\u4e3a\u52a0\u5f3a\u5b83\u4eec\u4e5f\u53ef\u4ee5\u6291\u5236\u6b63\u786e\u7684\u4e8b\u5b9e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6ce8\u610f\u529b\u5934\u7684\u884c\u4e3a\u662f\u9886\u57df\u4f9d\u8d56\u7684\uff0c\u5927\u578b\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u591a\u4e13\u95e8\u548c\u7c7b\u522b\u654f\u611f\u7684\u6a21\u5f0f\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4fc3\u8fdb\u4e8b\u5b9e\u8f93\u51fa\u7684\u6ce8\u610f\u529b\u5934\u662f\u901a\u8fc7\u4e00\u822c\u7684\u590d\u5236\u6291\u5236\u800c\u4e0d\u662f\u9009\u62e9\u6027\u53cd\u4e8b\u5b9e\u6291\u5236\u6765\u5b9e\u73b0\u7684\uff0c\u56e0\u4e3a\u52a0\u5f3a\u5b83\u4eec\u4e5f\u53ef\u4ee5\u6291\u5236\u6b63\u786e\u7684\u4e8b\u5b9e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6ce8\u610f\u529b\u5934\u7684\u884c\u4e3a\u662f\u9886\u57df\u4f9d\u8d56\u7684\uff0c\u5927\u578b\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u591a\u4e13\u95e8\u548c\u7c7b\u522b\u654f\u611f\u7684\u6a21\u5f0f\u3002"}}
{"id": "2507.11832", "pdf": "https://arxiv.org/pdf/2507.11832", "abs": "https://arxiv.org/abs/2507.11832", "authors": ["Yash Ingle", "Pruthwik Mishra"], "title": "ILID: Native Script Language Identification for Indian Languages", "categories": ["cs.CL"], "comment": "8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025", "summary": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script making the task even more challenging.\nIn this paper, we release a dataset of 230K sentences consisting of English and\nall 22 official Indian languages labeled with their language identifiers where\ndata in most languages are newly created. We also develop and release robust\nbaseline models using state-of-the-art approaches in machine learning and deep\nlearning that can aid the research in this field. Our baseline models are\ncomparable to the state-of-the-art models for the language identification task.", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b230K\u53e5\u5b50\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u82f1\u8bed\u548c22\u79cd\u5370\u5ea6\u5b98\u65b9\u8bed\u8a00\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u662fNLP\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u3001\u4fe1\u606f\u68c0\u7d22\u3001\u95ee\u7b54\u548c\u6587\u672c\u6458\u8981\u7b49\u5e94\u7528\u4e2d\u3002\u7136\u800c\uff0c\u5728\u5608\u6742\u3001\u77ed\u6587\u672c\u548c\u4ee3\u7801\u6df7\u5408\u73af\u5883\u4e2d\u533a\u5206\u8bed\u8a00\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u5370\u5ea6\u591a\u79cd\u8bed\u8a00\u4e4b\u95f4\uff0c\u5b83\u4eec\u5728\u8bcd\u6c47\u548c\u53d1\u97f3\u4e0a\u76f8\u4f3c\u4f46\u6709\u663e\u8457\u5dee\u5f02\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b230K\u53e5\u5b50\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u7ebf\u6a21\u578b\u5728\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b230K\u53e5\u5b50\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u82f1\u8bed\u548c\u6240\u670922\u79cd\u5b98\u65b9\u5370\u5ea6\u8bed\u8a00\uff0c\u5e76\u5f00\u53d1\u4e86\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2507.11851", "pdf": "https://arxiv.org/pdf/2507.11851", "abs": "https://arxiv.org/abs/2507.11851", "authors": ["Mohammad Samragh", "Arnav Kundu", "David Harrison", "Kumari Nishu", "Devang Naik", "Minsik Cho", "Mehrdad Farajtabar"], "title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Autoregressive language models are constrained by their inherently sequential\nnature, generating one token at a time. This paradigm limits inference speed\nand parallelism, especially during later stages of generation when the\ndirection and semantics of text are relatively certain. In this work, we\npropose a novel framework that leverages the inherent knowledge of vanilla\nautoregressive language models about future tokens, combining techniques to\nrealize this potential and enable simultaneous prediction of multiple\nsubsequent tokens. Our approach introduces several key innovations: (1) a\nmasked-input formulation where multiple future tokens are jointly predicted\nfrom a common prefix; (2) a gated LoRA formulation that preserves the original\nLLM's functionality, while equipping it for multi-token prediction; (3) a\nlightweight, learnable sampler module that generates coherent sequences from\nthe predicted future tokens; (4) a set of auxiliary training losses, including\na consistency loss, to enhance the coherence and accuracy of jointly generated\ntokens; and (5) a speculative generation strategy that expands tokens\nquadratically in the future while maintaining high fidelity. Our method\nachieves significant speedups through supervised fine-tuning on pretrained\nmodels. For example, it generates code and math nearly 5x faster, and improves\ngeneral chat and knowledge tasks by almost 2.5x. These gains come without any\nloss in quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5229\u7528\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5bf9\u672a\u6765\u6807\u8bb0\u7684\u77e5\u8bc6\uff0c\u5b9e\u73b0\u591a\u4e2a\u540e\u7eed\u6807\u8bb0\u7684\u540c\u65f6\u9884\u6d4b\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u751f\u6210\u901f\u5ea6\u800c\u4e0d\u635f\u5931\u8d28\u91cf\u3002", "motivation": "\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u5176\u56fa\u6709\u7684\u987a\u5e8f\u6027\u8d28\uff0c\u751f\u6210\u901f\u5ea6\u53d7\u9650\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u540e\u671f\u9636\u6bb5\uff0c\u6587\u672c\u7684\u65b9\u5411\u548c\u8bed\u4e49\u76f8\u5bf9\u786e\u5b9a\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5229\u7528\u4e86\u4f20\u7edf\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5bf9\u672a\u6765\u6807\u8bb0\u7684\u77e5\u8bc6\uff0c\u7ed3\u5408\u6280\u672f\u5b9e\u73b0\u8fd9\u4e00\u6f5c\u529b\uff0c\u5e76\u5b9e\u73b0\u591a\u4e2a\u540e\u7eed\u6807\u8bb0\u7684\u540c\u65f6\u9884\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4f8b\u5982\u751f\u6210\u4ee3\u7801\u548c\u6570\u5b66\u5185\u5bb9\u51e0\u4e4e\u5feb5\u500d\uff0c\u6539\u8fdb\u4e00\u822c\u804a\u5929\u548c\u77e5\u8bc6\u4efb\u52a1\u63a5\u8fd12.5\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u635f\u5931\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.11862", "pdf": "https://arxiv.org/pdf/2507.11862", "abs": "https://arxiv.org/abs/2507.11862", "authors": ["Junhong Ye", "Xu Yuan", "Xinying Qiu"], "title": "Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition", "categories": ["cs.CL"], "comment": "Accepted to CLNLP 2025", "summary": "Accurate recognition of personally identifiable information (PII) is central\nto automated text anonymization. This paper investigates the effectiveness of\ncross-domain model transfer, multi-domain data fusion, and sample-efficient\nlearning for PII recognition. Using annotated corpora from healthcare (I2B2),\nlegal (TAB), and biography (Wikipedia), we evaluate models across four\ndimensions: in-domain performance, cross-domain transferability, fusion, and\nfew-shot learning. Results show legal-domain data transfers well to\nbiographical texts, while medical domains resist incoming transfer. Fusion\nbenefits are domain-specific, and high-quality recognition is achievable with\nonly 10% of training data in low-specialization domains.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8de8\u9886\u57df\u6a21\u578b\u8fc1\u79fb\u3001\u591a\u9886\u57df\u6570\u636e\u878d\u5408\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u5728PII\u8bc6\u522b\u4e2d\u7684\u6548\u679c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6cd5\u5f8b\u9886\u57df\u7684\u6570\u636e\u53ef\u4ee5\u5f88\u597d\u5730\u8fc1\u79fb\u5230\u4f20\u8bb0\u6587\u672c\u4e2d\uff0c\u800c\u533b\u7597\u9886\u57df\u5bf9\u8fc1\u79fb\u5177\u6709\u62b5\u6297\u529b\u3002\u5728\u4f4e\u4e13\u4e1a\u5316\u9886\u57df\u4e2d\uff0c\u4ec5\u4f7f\u752810%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684PII\u8bc6\u522b\u3002", "motivation": "\u51c6\u786e\u8bc6\u522b\u4e2a\u4eba\u53ef\u8bc6\u522b\u4fe1\u606f\uff08PII\uff09\u662f\u81ea\u52a8\u5316\u6587\u672c\u533f\u540d\u5316\u7684\u5173\u952e\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u8de8\u9886\u57df\u6a21\u578b\u8fc1\u79fb\u3001\u591a\u9886\u57df\u6570\u636e\u878d\u5408\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u5728PII\u8bc6\u522b\u4e2d\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528\u533b\u7597\uff08I2B2\uff09\u3001\u6cd5\u5f8b\uff08TAB\uff09\u548c\u4f20\u8bb0\uff08\u7ef4\u57fa\u767e\u79d1\uff09\u7684\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff1a\u9886\u57df\u5185\u6027\u80fd\u3001\u8de8\u9886\u57df\u8fc1\u79fb\u80fd\u529b\u3001\u878d\u5408\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6cd5\u5f8b\u9886\u57df\u7684\u6570\u636e\u53ef\u4ee5\u5f88\u597d\u5730\u8fc1\u79fb\u5230\u4f20\u8bb0\u6587\u672c\u4e2d\uff0c\u800c\u533b\u7597\u9886\u57df\u5bf9\u8fc1\u79fb\u5177\u6709\u62b5\u6297\u529b\u3002\u878d\u5408\u7684\u597d\u5904\u662f\u9886\u57df\u7279\u5b9a\u7684\uff0c\u5e76\u4e14\u5728\u4f4e\u4e13\u4e1a\u5316\u9886\u57df\u4e2d\u4ec5\u4f7f\u752810%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u8bc6\u522b\u3002", "conclusion": "\u9ad8\u7cbe\u5ea6\u7684PII\u8bc6\u522b\u5bf9\u4e8e\u81ea\u52a8\u6587\u672c\u533f\u540d\u5316\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u7814\u7a76\u4e86\u8de8\u9886\u57df\u6a21\u578b\u8fc1\u79fb\u3001\u591a\u9886\u57df\u6570\u636e\u878d\u5408\u548c\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u5728PII\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.11867", "pdf": "https://arxiv.org/pdf/2507.11867", "abs": "https://arxiv.org/abs/2507.11867", "authors": ["Xiangyu Yang", "Xinying Qiu"], "title": "COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction", "categories": ["cs.CL"], "comment": "Accepted to CLNLP 2025", "summary": "Grammatical Error Correction (GEC) and grammatical acceptability judgment\n(COLA) are core tasks in natural language processing, sharing foundational\ngrammatical knowledge yet typically evolving independently. This paper\nintroduces COLA-GEC, a novel bidirectional framework that enhances both tasks\nthrough mutual knowledge transfer. First, we augment grammatical acceptability\nmodels using GEC datasets, significantly improving their performance across\nmultiple languages. Second, we integrate grammatical acceptability signals into\nGEC model training via a dynamic loss function, effectively guiding corrections\ntoward grammatically acceptable outputs. Our approach achieves state-of-the-art\nresults on several multilingual benchmarks. Comprehensive error analysis\nhighlights remaining challenges, particularly in punctuation error correction,\nproviding insights for future improvements in grammatical modeling.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u5411\u6846\u67b6COLA-GEC\uff0c\u901a\u8fc7\u76f8\u4e92\u77e5\u8bc6\u8fc1\u79fb\u6765\u589e\u5f3aGEC\u548cCOLA\u4efb\u52a1\uff0c\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "Grammatical Error Correction (GEC)\u548cGrammatical Acceptability Judgment (COLA)\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u6838\u5fc3\u4efb\u52a1\uff0c\u867d\u7136\u5171\u4eab\u57fa\u7840\u7684\u8bed\u6cd5\u77e5\u8bc6\uff0c\u4f46\u901a\u5e38\u72ec\u7acb\u53d1\u5c55\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u76f8\u4e92\u77e5\u8bc6\u8fc1\u79fb\u6765\u63d0\u5347\u8fd9\u4e24\u4e2a\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86COLA-GEC\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u5411\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528GEC\u6570\u636e\u96c6\u589e\u5f3a\u8bed\u6cd5\u53ef\u63a5\u53d7\u6027\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u635f\u5931\u51fd\u6570\u5c06\u8bed\u6cd5\u53ef\u63a5\u53d7\u6027\u4fe1\u53f7\u96c6\u6210\u5230GEC\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u901a\u8fc7\u5168\u9762\u7684\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u5269\u4f59\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6807\u70b9\u7b26\u53f7\u9519\u8bef\u7ea0\u6b63\u65b9\u9762\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u5411\u6846\u67b6COLA-GEC\uff0c\u901a\u8fc7\u76f8\u4e92\u77e5\u8bc6\u8fc1\u79fb\u6765\u589e\u5f3aGEC\u548cCOLA\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u8bed\u6cd5\u5efa\u6a21\u65b9\u9762\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2507.11875", "pdf": "https://arxiv.org/pdf/2507.11875", "abs": "https://arxiv.org/abs/2507.11875", "authors": ["Tianyou Huang", "Xinglu Chen", "Jingshen Zhang", "Xinying Qiu", "Ruiying Niu"], "title": "DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation", "categories": ["cs.CL"], "comment": "Accepted to CCL 2025", "summary": "This paper introduces DualReward, a novel reinforcement learning framework\nfor automatic distractor generation in cloze tests. Unlike conventional\napproaches that rely primarily on supervised learning or static generative\nmodels, our method employs a dual reward structure with adaptive scaling that\ndifferentiates between human-created gold standard distractors and\nmodel-generated candidates. The framework dynamically adjusts reward signal\nintensity based on model performance and confidence. We evaluate our approach\non both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,\ndemonstrating consistent improvements over state-of-the-art baselines.\nExperimental results show that our adaptive reward scaling mechanism provides\nmodest but consistent benefits on homogeneous datasets (CLOTH-F) and more\nsubstantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data\n(MCQ), suggesting its particular effectiveness for handling varied question\ntypes and domains. Our work offers a flexible framework that effectively\nbalances learning from reliable human examples while exploring novel,\nhigh-quality distractors for automated test generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6 DualReward\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u95ed\u585e\u6d4b\u8bd5\u4e2d\u7684\u5e72\u6270\u9879\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u53cc\u5956\u52b1\u7ed3\u6784\u548c\u81ea\u9002\u5e94\u7f29\u653e\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u5e72\u6270\u9879\u7684\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u7684 approaches \u4e3b\u8981\u4f9d\u8d56\u4e8e\u76d1\u7763\u5b66\u4e60\u6216\u9759\u6001\u751f\u6210\u6a21\u578b\uff0c\u800c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u6539\u8fdb\u5e72\u6270\u9879\u751f\u6210\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u6d4b\u8bd5\u751f\u6210\u7684\u6548\u679c\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86DualReward\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u95ed\u585e\u6d4b\u8bd5\u4e2d\u7684\u81ea\u52a8\u5e72\u6270\u9879\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u53cc\u5956\u52b1\u7ed3\u6784\u548c\u81ea\u9002\u5e94\u7f29\u653e\uff0c\u533a\u5206\u4eba\u7c7b\u521b\u5efa\u7684\u9ec4\u91d1\u6807\u51c6\u5e72\u6270\u9879\u548c\u6a21\u578b\u751f\u6210\u7684\u5019\u9009\u5e72\u6270\u9879\uff0c\u5e76\u6839\u636e\u6a21\u578b\u6027\u80fd\u548c\u7f6e\u4fe1\u5ea6\u52a8\u6001\u8c03\u6574\u5956\u52b1\u4fe1\u53f7\u5f3a\u5ea6\u3002", "result": "\u6211\u4eec\u5728 passage-level (CLOTH-F) \u548c sentence-level (MCQ) \u95ed\u585e\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u4e0a\u6709\u6301\u7eed\u7684\u6539\u8fdb\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u81ea\u9002\u5e94\u5956\u52b1\u7f29\u653e\u673a\u5236\u5728\u540c\u8d28\u6570\u636e\u96c6\uff08CLOTH-F\uff09\u4e0a\u63d0\u4f9b\u4e86\u9002\u5ea6\u4f46\u4e00\u81f4\u7684\u597d\u5904\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u8de8\u9886\u57df\u6570\u636e\uff08MCQ\uff09\u4e0a\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u6539\u8fdb\uff08P@1 \u63d0\u5347 3.48-3.86%\uff09\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5730\u5728\u5b66\u4e60\u53ef\u9760\u7684\u771f\u4eba\u793a\u4f8b\u548c\u63a2\u7d22\u65b0\u9896\u7684\u9ad8\u8d28\u91cf\u5e72\u6270\u9879\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4ece\u800c\u5b9e\u73b0\u81ea\u52a8\u6d4b\u8bd5\u751f\u6210\u3002"}}
{"id": "2507.11878", "pdf": "https://arxiv.org/pdf/2507.11878", "abs": "https://arxiv.org/abs/2507.11878", "authors": ["Jiachen Zhao", "Jing Huang", "Zhengxuan Wu", "David Bau", "Weiyan Shi"], "title": "LLMs Encode Harmfulness and Refusal Separately", "categories": ["cs.CL"], "comment": null, "summary": "LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs' refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal\ndirection. In this work, we identify a new dimension to analyze safety\nmechanisms in LLMs, i.e., harmfulness, which is encoded internally as a\nseparate concept from refusal. There exists a harmfulness direction that is\ndistinct from the refusal direction. As causal evidence, steering along the\nharmfulness direction can lead LLMs to interpret harmless instructions as\nharmful, but steering along the refusal direction tends to elicit refusal\nresponses directly without reversing the model's judgment on harmfulness.\nFurthermore, using our identified harmfulness concept, we find that certain\njailbreak methods work by reducing the refusal signals without reversing the\nmodel's internal belief of harmfulness. We also find that adversarially\nfinetuning models to accept harmful instructions has minimal impact on the\nmodel's internal belief of harmfulness. These insights lead to a practical\nsafety application: The model's latent harmfulness representation can serve as\nan intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing\nover-refusals that is robust to finetuning attacks. For instance, our Latent\nGuard achieves performance comparable to or better than Llama Guard 3 8B, a\ndedicated finetuned safeguard model, across different jailbreak methods. Our\nfindings suggest that LLMs' internal understanding of harmfulness is more\nrobust than their refusal decision to diverse input instructions, offering a\nnew perspective to study AI safety", "AI": {"tldr": "This paper identifies a new dimension of safety in LLMs called harmfulness, which is separate from refusal. It proposes a practical safety application called Latent Guard that uses this concept to detect unsafe inputs and reduce over-refusals.", "motivation": "To understand whether LLMs truly understand harmfulness beyond just refusing harmful instructions, and to develop a more robust safety mechanism.", "method": "Identifying a new dimension to analyze safety mechanisms in LLMs, i.e., harmfulness, and using it to develop a practical safety application called Latent Guard.", "result": "The model's latent harmfulness representation can serve as an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing over-refusals that is robust to finetuning attacks.", "conclusion": "LLMs' internal understanding of harmfulness is more robust than their refusal decision to diverse input instructions, offering a new perspective to study AI safety."}}
{"id": "2507.11882", "pdf": "https://arxiv.org/pdf/2507.11882", "abs": "https://arxiv.org/abs/2507.11882", "authors": ["Bo Zeng", "Chenyang Lyu", "Sinuo Liu", "Mingyan Zeng", "Minghao Wu", "Xuanfan Ni", "Tianqi Shi", "Yu Zhao", "Yefeng Liu", "Chenyu Zhu", "Ruizhe Li", "Jiahui Geng", "Qing Li", "Yu Tong", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference paper", "summary": "Instruction-following capability has become a major ability to be evaluated\nfor Large Language Models (LLMs). However, existing datasets, such as IFEval,\nare either predominantly monolingual and centered on English or simply machine\ntranslated to other languages, limiting their applicability in multilingual\ncontexts. In this paper, we present an carefully-curated extension of IFEval to\na localized multilingual version named Marco-Bench-MIF, covering 30 languages\nwith varying levels of localization. Our benchmark addresses linguistic\nconstraints (e.g., modifying capitalization requirements for Chinese) and\ncultural references (e.g., substituting region-specific company names in\nprompts) via a hybrid pipeline combining translation with verification. Through\ncomprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)\n25-35% accuracy gap between high/low-resource languages, (2) model scales\nlargely impact performance by 45-60% yet persists script-specific challenges,\nand (3) machine-translated data underestimates accuracy by7-22% versus\nlocalized data. Our analysis identifies challenges in multilingual instruction\nfollowing, including keyword consistency preservation and compositional\nconstraint adherence across languages. Our Marco-Bench-MIF is available at\nhttps://github.com/AIDC-AI/Marco-Bench-MIF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u672c\u5730\u5316\u7684\u591a\u8bed\u8a00\u57fa\u51c6Marco-Bench-MIF\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u53d1\u73b0\u9ad8/\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u7684\u51c6\u786e\u7387\u5dee\u8ddd\u4ee5\u53ca\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5982IFEval\u4e3b\u8981\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u6216\u4ec5\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u5230\u5176\u4ed6\u8bed\u8a00\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u672c\u5730\u5316\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7ed3\u5408\u7ffb\u8bd1\u4e0e\u9a8c\u8bc1\u7684\u6df7\u5408\u6d41\u7a0b\uff0c\u5bf9IFEval\u8fdb\u884c\u4e86\u672c\u5730\u5316\u6269\u5c55\uff0c\u5f62\u6210\u4e86\u8986\u76d630\u79cd\u8bed\u8a00\u7684Marco-Bench-MIF\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728Marco-Bench-MIF\u4e0a\u8bc4\u4f3020\u591a\u4e2aLLM\u540e\uff0c\u53d1\u73b0\u9ad8/\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u5b58\u572825-35%\u7684\u51c6\u786e\u7387\u5dee\u8ddd\uff0c\u6a21\u578b\u89c4\u6a21\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u4e3a45-60%\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u811a\u672c\u7279\u5b9a\u6311\u6218\uff0c\u4e14\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u4f4e\u4f30\u4e86\u51c6\u786e\u73877-22%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u672c\u5730\u5316\u7684\u591a\u8bed\u8a00\u7248\u672cMarco-Bench-MIF\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u95ee\u9898\u3002\u901a\u8fc7\u5168\u9762\u8bc4\u4f3020\u591a\u4e2aLLM\uff0c\u53d1\u73b0\u9ad8/\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u5b58\u572825-35%\u7684\u51c6\u786e\u7387\u5dee\u8ddd\uff0c\u5e76\u6307\u51fa\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u4f4e\u4f30\u4e86\u51c6\u786e\u73877-22%\u3002"}}
{"id": "2507.11936", "pdf": "https://arxiv.org/pdf/2507.11936", "abs": "https://arxiv.org/abs/2507.11936", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "title": "A Survey of Deep Learning for Geometry Problem Solving", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u76f8\u5173\u4efb\u52a1\u3001\u65b9\u6cd5\u3001\u8bc4\u4f30\u6307\u6807\u548c\u5f53\u524d\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6301\u7eed\u66f4\u65b0\u7684\u8bba\u6587\u5217\u8868\u3002", "motivation": "\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u662f\u6570\u5b66\u63a8\u7406\u7684\u5173\u952e\u9886\u57df\uff0c\u5e7f\u6cdb\u6d89\u53ca\u6559\u80b2\u3001\u4eba\u5de5\u667a\u80fd\u6570\u5b66\u80fd\u529b\u8bc4\u4f30\u548c\u591a\u6a21\u6001\u80fd\u529b\u8bc4\u4f30\u3002\u8fd1\u5e74\u6765\uff0c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7279\u522b\u662f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0c\u5f15\u53d1\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\u70ed\u6f6e\u3002", "method": "\u672c\u6587\u5bf9\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u4e2d\u6df1\u5ea6\u5b66\u4e60\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u5305\u62ec\u76f8\u5173\u4efb\u52a1\u7684\u5168\u9762\u603b\u7ed3\u3001\u76f8\u5173\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u8be6\u7ec6\u56de\u987e\u3001\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\u7684\u5206\u6790\uff0c\u4ee5\u53ca\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u7684\u8ba8\u8bba\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u4e2d\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u6301\u7eed\u66f4\u65b0\u7684\u8bba\u6587\u5217\u8868\u5728GitHub\u4e0a\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4e3a\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u5168\u9762\u4e14\u5b9e\u7528\u7684\u53c2\u8003\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.11939", "pdf": "https://arxiv.org/pdf/2507.11939", "abs": "https://arxiv.org/abs/2507.11939", "authors": ["Yichen Xu", "Liangyu Chen", "Liang Zhang", "Wenxuan Wang", "Qin Jin"], "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "comment": "Work in Progress", "summary": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PolyChartQA\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u56fe\u8868\u95ee\u7b54\u57fa\u51c6\uff0c\u6db5\u76d6\u4e8622,606\u4e2a\u56fe\u8868\u548c26,151\u4e2a\u8de810\u79cd\u4e0d\u540c\u8bed\u8a00\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u8868\u7406\u89e3\u57fa\u51c6\u4e3b\u8981\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u5168\u7403\u53d7\u4f17\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u89e3\u8026\u7ba1\u9053\uff0c\u5c06\u56fe\u8868\u6570\u636e\u4e0e\u6e32\u67d3\u4ee3\u7801\u5206\u79bb\uff0c\u901a\u8fc7\u7ffb\u8bd1\u6570\u636e\u5e76\u91cd\u7528\u4ee3\u7801\u6765\u7075\u6d3b\u751f\u6210\u591a\u8bed\u8a00\u56fe\u8868\u3002\u5229\u7528\u6700\u5148\u8fdb\u7684\u57fa\u4e8eLLM\u7684\u7ffb\u8bd1\u5e76\u5b9e\u65bd\u4e25\u683c\u7684\u8d28\u91cf\u63a7\u5236\u4ee5\u786e\u4fdd\u751f\u6210\u7684\u591a\u8bed\u8a00\u56fe\u8868\u7684\u8bed\u8a00\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5f00\u653e\u6e90\u4ee3\u7801\u548c\u95ed\u6e90\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u82f1\u8bed\u548c\u5176\u4ed6\u8bed\u8a00\u4e4b\u95f4\uff0c\u5c24\u5176\u662f\u4f7f\u7528\u975e\u62c9\u4e01\u6587\u5b57\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u63a8\u8fdb\u5168\u7403\u5305\u5bb9\u6027\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.11941", "pdf": "https://arxiv.org/pdf/2507.11941", "abs": "https://arxiv.org/abs/2507.11941", "authors": ["Amos You"], "title": "BlockBPE: Parallel BPE Tokenization", "categories": ["cs.CL", "cs.DC"], "comment": "ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models\n  (ICML 2025)", "summary": "Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.", "AI": {"tldr": "BlockBPE \u662f\u4e00\u79cd\u9ad8\u6548\u7684 GPU \u5e76\u884c BPE \u5b9e\u73b0\uff0c\u901a\u8fc7\u6d88\u9664\u6b63\u5219\u8868\u8fbe\u5f0f\u9884\u5206\u8bcd\u6b65\u9aa4\uff0c\u63d0\u9ad8\u4e86\u6279\u91cf\u63a8\u7406\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u7684 Rust \u57fa\u7840\u5206\u8bcd\u5668\u5982 HuggingFace Tokenizers \u6216 OpenAI \u7684 tiktoken \u5728\u8fd0\u884c\u65f6\u95f4\u4e0a\u53d7\u5230\u6b63\u5219\u8868\u8fbe\u5f0f\u9884\u5206\u8bcd\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u8fd0\u884c\u65f6\u95f4\u4e3a O(n log n)\u3002", "method": "BlockBPE \u662f\u4e00\u79cd\u5e76\u884c GPU \u5b9e\u73b0\u7684\u5b57\u8282\u5bf9\u7f16\u7801 (BPE)\uff0c\u901a\u8fc7\u6d88\u9664\u6b63\u5219\u8868\u8fbe\u5f0f\u9884\u5206\u8bcd\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u5e76\u884c\u5316\u7684\u8bcd\u5143\u5408\u5e76\u3002", "result": "\u5728\u9ad8\u6279\u91cf\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0cBlockBPE \u7684\u541e\u5410\u91cf\u6bd4 tiktoken \u9ad8\u8fbe 2 \u500d\uff0c\u6bd4 HuggingFace Tokenizers \u9ad8\u8fbe 2.5 \u500d\u3002", "conclusion": "BlockBPE \u662f\u4e00\u79cd\u9ad8\u6548\u7684 GPU \u5e76\u884c\u5b9e\u73b0\uff0c\u80fd\u591f\u5728\u9ad8\u541e\u5410\u91cf\u7684\u6279\u91cf\u63a8\u7406\u4e2d\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2507.11942", "pdf": "https://arxiv.org/pdf/2507.11942", "abs": "https://arxiv.org/abs/2507.11942", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao", "Baoyuan Qi", "Guoming Liu"], "title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6ce8\u610f\u529b\u611f\u77e5\u7684\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\uff08DAC\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u71b5\u548c\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u5728\u591a\u4e2a\u9886\u57df\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u7b97\u6cd5\u5c42\u9762\u7684\u5173\u6ce8\u5173\u952e\u6807\u8bb0\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u538b\u7f29\u8fc7\u7a0b\u4e2d\u4fe1\u606f\u71b5\u7684\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6ce8\u610f\u529b\u611f\u77e5\u7684\u65b9\u6cd5\uff08DAC\uff09\uff0c\u7ed3\u5408\u71b5\u548c\u6ce8\u610f\u529b\u4fe1\u606f\uff0c\u52a8\u6001\u68c0\u6d4b\u538b\u7f29\u8fc7\u7a0b\u4e2d\u7684\u71b5\u53d8\u5316\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u63d0\u793a\u538b\u7f29\u3002", "result": "\u5728LongBench\u3001GSM8K\u548cBBH\u7b49\u591a\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eDAC\u5728\u5404\u79cd\u4efb\u52a1\u548cLLM\u4e2d\u90fd\u80fd\u6301\u7eed\u5e26\u6765\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "DAC\u5728\u591a\u4e2a\u9886\u57df\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.11953", "pdf": "https://arxiv.org/pdf/2507.11953", "abs": "https://arxiv.org/abs/2507.11953", "authors": ["Yi Zhao", "Zuchao Li", "Hai Zhao"], "title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025", "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIAM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5c0f\u6a21\u578b\u548c\u5927\u6a21\u578b\u4e4b\u95f4\u8fdb\u884c\u6ce8\u610f\u529b\u6620\u5c04\uff0c\u5b9e\u73b0\u4e86\u52a0\u901f\u6ce8\u610f\u529b\u8ba1\u7b97\u548c\u51cf\u5c11KV\u7f13\u5b58\u4f7f\u7528\u7684\u6548\u679c\u3002", "motivation": "LLMs\u5728\u8d44\u6e90\u6d88\u8017\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u957f\u4e0a\u4e0b\u6587\u60c5\u51b5\u4e0b\u3002\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u52aa\u529b\u7528\u4e8e\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u6a21\u578b\u5185\u90e8\u7684\u7a00\u758f\u6027\uff0c\u800c\u6ca1\u6709\u5229\u7528\u5916\u90e8\u4fe1\u606f\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5982\u4f55\u6d4b\u91cf\u76f8\u4f3c\u6027\u3001\u5982\u4f55\u9009\u62e9\u6620\u5c04\u5c42\u4ee5\u53ca\u6620\u5c04\u662f\u5426\u4e00\u81f4\uff0c\u5f15\u5165\u4e86IAM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u5c0f\u578b\u548c\u5927\u578bLLM\u4e4b\u95f4\u6267\u884c\u6ce8\u610f\u529b\u6620\u5c04\u6765\u5b9e\u73b0\u52a0\u901f\u6ce8\u610f\u529b\u8ba1\u7b97\u548c\u51cf\u5c11KV\u7f13\u5b58\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIAM\u53ef\u4ee5\u52a0\u901f\u9884\u586b\u514515%\uff0c\u5e76\u51cf\u5c11KV\u7f13\u5b58\u4f7f\u752822.1%\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7cfb\u5217\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86IAM\u7684\u901a\u7528\u6027\u3002", "conclusion": "IAM\u6846\u67b6\u5728\u4e0d\u660e\u663e\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u52a0\u901f\u9884\u586b\u5145\u5e76\u51cf\u5c11KV\u7f13\u5b58\u4f7f\u7528\uff0c\u5e76\u4e14\u4e0e\u8bb8\u591a\u73b0\u6709\u7684KV\u7f13\u5b58\u4f18\u5316\u65b9\u6cd5\u6b63\u4ea4\uff0c\u4f7f\u5176\u6210\u4e3a\u589e\u5f3aLLM\u6548\u7387\u7684\u591a\u529f\u80fd\u5de5\u5177\u3002"}}
{"id": "2507.11954", "pdf": "https://arxiv.org/pdf/2507.11954", "abs": "https://arxiv.org/abs/2507.11954", "authors": ["Artem Alekseev", "Mikhail Chaichuk", "Miron Butko", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era", "categories": ["cs.CL", "cs.LG"], "comment": "15 pages, 3 figures, 7 tables", "summary": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u67e5\u8be2\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u5728\u591a\u8df3\u548c\u65f6\u95f4\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u95ee\u7b54\uff08QA\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u96be\u4ee5\u5904\u7406\u591a\u8df3\u63a8\u7406\u548c\u65f6\u95f4\u95ee\u9898\u3002\u57fa\u4e8e\u67e5\u8be2\u7684\u77e5\u8bc6\u56fe\u8c31QA\uff08KGQA\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u6267\u884c\u67e5\u8be2\u800c\u4e0d\u662f\u76f4\u63a5\u7b54\u6848\u3002", "method": "\u6211\u4eec\u63a2\u7d22\u4e86\u7528\u4e8eWikiData QA\u7684\u591a\u9636\u6bb5\u67e5\u8be2\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u589e\u5f3a\u6027\u80fd\u7684\u591a\u9636\u6bb5\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4f7f\u7528CoT\u63a8\u7406\u7684\u5b9e\u4f53\u94fe\u63a5\u548c\u8c13\u8bcd\u5339\u914d\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6cdb\u5316\u548c\u62d2\u7edd\u7814\u7a76\uff0c\u6211\u4eec\u5728\u591a\u8df3\u548c\u65f6\u95f4QA\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u9c81\u68d2\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u67e5\u8be2\u7684\u591a\u9636\u6bb5KGQA\u6846\u67b6\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5177\u6709\u6539\u8fdb\u591a\u8df3\u548c\u65f6\u95f4QA\u7684\u6f5c\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u67e5\u8be2\u7684\u591a\u9636\u6bb5KGQA\u6846\u67b6\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5177\u6709\u6539\u8fdb\u591a\u8df3\u548c\u65f6\u95f4QA\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11959", "pdf": "https://arxiv.org/pdf/2507.11959", "abs": "https://arxiv.org/abs/2507.11959", "authors": ["Xinyu Wang", "Vahid Partovi Nia", "Peng Lu", "Jerry Huang", "Xiao-Wen Chang", "Boxing Chen", "Yufei Cui"], "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ECAI 2025 (European Conference on Artificial\n  Intelligence)", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684PoT\u91cf\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6781\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u53cd\u91cf\u5316\u5b9e\u73b0\u66f4\u5feb\u7684\u63a8\u7406\u3002", "motivation": "\u7531\u4e8eGPU\u4e0a\u7684PoT\u91cf\u5316\u5728\u53cd\u91cf\u5316\u65f6\u5b58\u5728\u7b26\u53f7\u4f4d\u548c\u987a\u5e8f\u4f4d\u64cd\u4f5c\u7684\u7ea0\u7f20\u95ee\u9898\uff0c\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684PoT\u91cf\u5316\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4e3a\u4e86\u4fdd\u6301\u91cf\u5316\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u4e24\u6b65\u7684\u8bad\u7ec3\u540e\u7b97\u6cd5\uff1a(i) \u7528\u4e00\u4e2a\u7a33\u5065\u7684\u8d77\u70b9\u521d\u59cb\u5316\u91cf\u5316\u6bd4\u4f8b\uff0c(ii) \u4f7f\u7528\u4e00\u4e2a\u6700\u5c0f\u7684\u6821\u51c6\u96c6\u6765\u4f18\u5316\u8fd9\u4e9b\u6bd4\u4f8b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684PoT\u91cf\u5316\u5728\u6574\u6570\u91cf\u5316\u65b9\u9762\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u4f4e\u7cbe\u5ea6\u59822-\u548c3-\u4f4d\u683c\u5f0f\u4e2d\u3002\u6b64\u5916\uff0c\u5728NVIDIA V100\u548cRTX 4090\u4e0a\uff0c\u4e0e\u7edf\u4e00\u6574\u6570\u53cd\u91cf\u5316\u76f8\u6bd4\uff0c\u53cd\u91cf\u5316\u6b65\u9aa4\u7684\u901f\u5ea6\u5206\u522b\u63d0\u9ad8\u4e863.67\u500d\u548c1.63\u500d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684PoT\u91cf\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6781\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u53cd\u91cf\u5316\u5b9e\u73b0\u66f4\u5feb\u7684\u63a8\u7406\u3002"}}
{"id": "2507.11966", "pdf": "https://arxiv.org/pdf/2507.11966", "abs": "https://arxiv.org/abs/2507.11966", "authors": ["Ziyu Ge", "Gabriel Chua", "Leanne Tan", "Roy Ka-Wei Lee"], "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u4fdd\u7559\u6709\u6bd2\u5185\u5bb9\u7684\u7ffb\u8bd1\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u65f6\u3002", "motivation": "\u6807\u51c6\u7ffb\u8bd1\u7cfb\u7edf\u5728\u5904\u7406\u975e\u4e3b\u6d41\u8bed\u8a00\u548c\u53e3\u8bed\u65b9\u8a00\u65f6\u5f80\u5f80\u65e0\u6cd5\u4fdd\u7559\u5f53\u5730\u7684\u4fda\u8bed\u3001\u6df7\u5408\u8bed\u8a00\u548c\u6709\u5bb3\u8a00\u8bba\u7684\u6587\u5316\u5d4c\u5165\u6807\u8bb0\u3002\u6b64\u5916\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e4b\u95f4\u7684\u6709\u6bd2\u5185\u5bb9\u7ffb\u8bd1\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u5b89\u5168\u8fc7\u6ee4\u5668\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u9996\u5148\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\u7684\u5c11\u6837\u672c\u63d0\u793a\u5de5\u7a0b\uff0c\u7136\u540e\u901a\u8fc7\u76f4\u63a5\u7ffb\u8bd1\u548c\u53cd\u5411\u7ffb\u8bd1\u5bf9\u6a21\u578b-\u63d0\u793a\u5bf9\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9a\u91cf\u7684\u4eba\u5de5\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684\u7ba1\u9053\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u6709\u52a9\u4e8e\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u652f\u6301\u6587\u5316\u654f\u611f\u7684\u76d1\u7ba1\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u4fdd\u7559\u793e\u4f1a\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b\u7684\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.11972", "pdf": "https://arxiv.org/pdf/2507.11972", "abs": "https://arxiv.org/abs/2507.11972", "authors": ["Yuhong Zhang", "Jialu Li", "Shilai Yang", "Yuchen Xu", "Gert Cauwenberghs", "Tzyy-Ping Jung"], "title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u6784\u5efa\u56fe\u6587\u672c\u8868\u793a\uff0c\u5e76\u6bd4\u8f83\u773c\u52a8\u5206\u5e03\uff0c\u53d1\u73b0LLMs\u5728\u56fe\u62d3\u6251\u7ed3\u6784\u5c42\u9762\u7684\u8bed\u8a00\u7406\u89e3\u5177\u6709\u4e00\u81f4\u6027\uff0c\u4e3a\u6709\u6548\u7684\u4eba\u673a\u534f\u540c\u5b66\u4e60\u7b56\u7565\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u9700\u8981\u6bd4\u8f83\u4eba\u7c7b\u548cLLM\u5982\u4f55\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7406\u89e3\u8bed\u8a00\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u63a8\u7406\u3001\u60c5\u611f\u89e3\u91ca\u548c\u4fe1\u606f\u68c0\u7d22\u7b49\u529f\u80fd\u4efb\u52a1\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u5c06\u9605\u8bfb\u6bb5\u843d\u4e2d\u7684\u5355\u8bcd\u5206\u7ec4\u4e3a\u8282\u70b9\u548c\u8fb9\uff0c\u5f62\u6210\u57fa\u4e8e\u8bed\u4e49\u610f\u4e49\u548c\u95ee\u9898\u5bfc\u5411\u63d0\u793a\u7684\u56fe\u6587\u672c\u8868\u793a\u3002\u7136\u540e\u6bd4\u8f83\u91cd\u8981\u8282\u70b9\u548c\u8fb9\u4e0a\u7684\u773c\u52a8\u5206\u5e03\u3002", "result": "LLMs\u5728\u56fe\u62d3\u6251\u7ed3\u6784\u5c42\u9762\u7684\u8bed\u8a00\u7406\u89e3\u4e0a\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5efa\u7acb\u5728\u6211\u4eec\u4e4b\u524d\u7684\u7814\u7a76\u6210\u679c\u4e4b\u4e0a\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5e08\u751f\u534f\u540c\u5b66\u4e60\u7b56\u7565\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.11979", "pdf": "https://arxiv.org/pdf/2507.11979", "abs": "https://arxiv.org/abs/2507.11979", "authors": ["Yuki Sakamoto", "Takahisa Uchida", "Hiroshi Ishiguro"], "title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4ef7\u503c\u89c2\u76f8\u4f3c\u6027\u5bf9LLM\u4ee3\u7406\u4e4b\u95f4\u5173\u7cfb\u5efa\u7acb\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4ef7\u503c\u89c2\u76f8\u4f3c\u6027\u9ad8\u7684\u4ee3\u7406\u5bf9\u8868\u73b0\u51fa\u66f4\u5927\u7684\u4fe1\u4efb\u548c\u4eb2\u5bc6\u5173\u7cfb\u3002", "motivation": "\u5728\u4eba\u7c7b\u793e\u4f1a\u4e2d\uff0c\u4ef7\u503c\u89c2\u76f8\u4f3c\u6027\u5bf9\u4e8e\u5efa\u7acb\u4fe1\u4efb\u548c\u4eb2\u5bc6\u5173\u7cfb\u5f88\u91cd\u8981\uff1b\u7136\u800c\uff0c\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e00\u539f\u5219\u662f\u5426\u9002\u7528\u4e8e\u7531LLM\u4ee3\u7406\u7ec4\u6210\u7684\u4eba\u5de5\u793e\u4f1a\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u4ef7\u503c\u89c2\u76f8\u4f3c\u6027\u5bf9LLM\u4ee3\u7406\u4e4b\u95f4\u5173\u7cfb\u5efa\u7acb\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u63a2\u8ba8\u4e86\u4ef7\u503c\u89c2\u76f8\u4f3c\u6027\u5bf9LLM\u4ee3\u7406\u4e4b\u95f4\u5173\u7cfb\u5efa\u7acb\u7684\u5f71\u54cd\u3002\u9996\u5148\uff0c\u5728\u521d\u6b65\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86LLM\u4e2d\u7684\u4ef7\u503c\u89c2\u53ef\u63a7\u6027\uff0c\u4ee5\u786e\u5b9a\u6700\u6709\u6548\u7684\u6a21\u578b\u548c\u63d0\u793a\u8bbe\u8ba1\u6765\u63a7\u5236\u4ef7\u503c\u89c2\u3002\u968f\u540e\uff0c\u5728\u4e3b\u8981\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u751f\u6210\u4e86\u5177\u6709\u7279\u5b9a\u4ef7\u503c\u89c2\u7684LLM\u4ee3\u7406\u5bf9\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5728\u5bf9\u8bdd\u540e\u76f8\u4e92\u8bc4\u4f30\u7684\u4fe1\u4efb\u548c\u4eba\u9645\u5173\u7cfb\u4eb2\u5bc6\u7a0b\u5ea6\u3002\u5b9e\u9a8c\u5728\u82f1\u8bed\u548c\u65e5\u8bed\u4e2d\u8fdb\u884c\uff0c\u4ee5\u7814\u7a76\u8bed\u8a00\u4f9d\u8d56\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u786e\u8ba4\u4e86\u5177\u6709\u66f4\u9ad8\u4ef7\u503c\u89c2\u76f8\u4f3c\u6027\u7684\u4ee3\u7406\u5bf9\u8868\u73b0\u51fa\u66f4\u5927\u7684\u76f8\u4e92\u4fe1\u4efb\u548c\u4eba\u9645\u5173\u7cfb\u4eb2\u5bc6\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM\u4ee3\u7406\u6a21\u62df\u53ef\u4ee5\u4f5c\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u7406\u8bba\u7684\u6709\u6548\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u9610\u660e\u4ef7\u503c\u89c2\u5982\u4f55\u5f71\u54cd\u5173\u7cfb\u5efa\u7acb\uff0c\u5e76\u4e3a\u6fc0\u53d1\u793e\u4f1a\u79d1\u5b66\u7684\u65b0\u7406\u8bba\u548c\u89c1\u89e3\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2507.11981", "pdf": "https://arxiv.org/pdf/2507.11981", "abs": "https://arxiv.org/abs/2507.11981", "authors": ["Lukas Ellinger", "Miriam Ansch\u00fctz", "Georg Groh"], "title": "Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions", "categories": ["cs.CL"], "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7b80\u5316\u5bf9\u540c\u4e49\u8bcd\u5b9a\u4e49\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7b80\u5316\u4f1a\u964d\u4f4e\u5b9a\u4e49\u7684\u5b8c\u6574\u6027\u5e76\u589e\u52a0\u8bef\u89e3\u98ce\u9669\uff0c\u540c\u65f6\u63d0\u51fa\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u7b80\u5316\u5bf9\u540c\u4e49\u8bcd\u5b9a\u4e49\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u76ee\u6807\u7fa4\u4f53\uff08\u5982\u6b63\u5e38\u3001\u7b80\u5355\u548cELI5\uff09\u4e2d\u7684\u5f71\u54cd\uff0c\u4ee5\u786e\u4fdd\u6559\u80b2NLP\u4e2d\u5b9a\u4e49\u7684\u53ef\u9760\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u6027\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u65b0\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u8de8\u8d8a\u591a\u79cd\u8bed\u8a00\uff0c\u901a\u8fc7LLM-as-Judge\u548c\u4eba\u5de5\u6807\u6ce8\u6d4b\u8bd5DeepSeek v3\u3001Llama 4 Maverick\u3001Qwen3-30B A3B\u3001GPT-4o mini\u548cLlama 3.1 8B\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7b80\u5316\u663e\u8457\u964d\u4f4e\u4e86\u540c\u4e49\u8bcd\u5b9a\u4e49\u7684\u5b8c\u6574\u6027\uff0c\u5ffd\u89c6\u591a\u4e49\u6027\uff0c\u589e\u52a0\u4e86\u8bef\u89e3\u7684\u98ce\u9669\u3002\u5fae\u8c03Llama 3.1 8B\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u663e\u8457\u63d0\u9ad8\u4e86\u6240\u6709\u63d0\u793a\u7c7b\u578b\u4e0b\u7684\u540c\u4e49\u8bcd\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7b80\u5316\u4f1a\u663e\u8457\u964d\u4f4e\u540c\u4e49\u8bcd\u5b9a\u4e49\u7684\u5b8c\u6574\u6027\uff0c\u5ffd\u89c6\u591a\u4e49\u6027\uff0c\u589e\u52a0\u8bef\u89e3\u7684\u98ce\u9669\u3002\u5fae\u8c03Llama 3.1 8B\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u663e\u8457\u63d0\u9ad8\u4e86\u6240\u6709\u63d0\u793a\u7c7b\u578b\u4e0b\u7684\u540c\u4e49\u8bcd\u54cd\u5e94\u8d28\u91cf\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u6559\u80b2NLP\u4e2d\u5e73\u8861\u7b80\u6d01\u6027\u548c\u5b8c\u6574\u6027\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u6240\u6709\u5b66\u4e60\u8005\u90fd\u80fd\u83b7\u5f97\u53ef\u9760\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b9a\u4e49\u3002"}}
{"id": "2507.12004", "pdf": "https://arxiv.org/pdf/2507.12004", "abs": "https://arxiv.org/abs/2507.12004", "authors": ["Josip Juki\u0107"], "title": "Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis", "categories": ["cs.CL"], "comment": null, "summary": "This thesis addresses challenges related to data and parameter efficiency in\nneural language models, with a focus on representation analysis and the\nintroduction of new optimization techniques. The first part examines the\nproperties and dynamics of language representations within neural models,\nemphasizing their significance in enhancing robustness and generalization. It\nproposes innovative approaches based on representation smoothness, including\nregularization strategies that utilize Jacobian and Hessian matrices to\nstabilize training and mitigate sensitivity to input perturbations. The second\npart focuses on methods to significantly enhance data and parameter efficiency\nby integrating active learning strategies with parameter-efficient fine-tuning,\nguided by insights from representation smoothness analysis. It presents\nsmoothness-informed early-stopping techniques designed to eliminate the need\nfor labeled validation sets and proposes innovative combinations of active\nlearning and parameter-efficient fine-tuning to reduce labeling efforts and\ncomputational resources. Extensive experimental evaluations across various NLP\ntasks demonstrate that these combined approaches substantially outperform\ntraditional methods in terms of performance, stability, and efficiency. The\nthird part explores weak supervision techniques enhanced by in-context learning\nto effectively utilize unlabeled data, further reducing dependence on extensive\nlabeling. It shows that using in-context learning as a mechanism for weak\nsupervision enables models to better generalize from limited labeled data by\nleveraging unlabeled examples more effectively during training. Comprehensive\nempirical evaluations confirm significant gains in model accuracy,\nadaptability, and robustness, especially in low-resource settings and dynamic\ndata environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u57fa\u4e8e\u8868\u793a\u5e73\u6ed1\u6027\u7684\u4f18\u5316\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6570\u636e\u548c\u53c2\u6570\u6548\u7387\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u8868\u793a\u5e73\u6ed1\u6027\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u5229\u7528\u96c5\u53ef\u6bd4\u77e9\u9635\u548c\u6d77\u68ee\u77e9\u9635\u6765\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8868\u793a\u5e73\u6ed1\u6027\u7684\u65e9\u671f\u505c\u6b62\u6280\u672f\uff0c\u4ee5\u53ca\u4e3b\u52a8\u5b66\u4e60\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u521b\u65b0\u7ec4\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u589e\u5f3a\u7684\u5f31\u76d1\u7763\u6280\u672f\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u4ee5\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u7efc\u5408\u65b9\u6cd5\u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u548c\u52a8\u6001\u6570\u636e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u548c\u52a8\u6001\u6570\u636e\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.12039", "pdf": "https://arxiv.org/pdf/2507.12039", "abs": "https://arxiv.org/abs/2507.12039", "authors": ["Anca Dinu", "Andra-Maria Florescu", "Alina Resceanu"], "title": "A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans", "categories": ["cs.CL"], "comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)", "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.", "AI": {"tldr": "A linguistic creativity test was developed to compare human and LLM performance, revealing that LLMs excel in most tasks and exhibit different creativity patterns.", "motivation": "To assess the linguistic creativity of humans and LLMs by designing a comprehensive test that evaluates their ability to generate new words and phrases.", "method": "The paper introduced a linguistic creativity test with tasks assessing word formation and metaphorical language use. The test was administered to 24 humans and 24 LLMs, and answers were evaluated using the OCSAI tool for originality, elaboration, and flexibility.", "result": "LLMs outperformed humans in all assessed criteria and performed better in six out of eight tasks. Humans showed more E-creativity, while LLMs favored F-creativity.", "conclusion": "LLMs outperformed humans in the linguistic creativity test, showing better performance in most tasks and different types of creativity."}}
{"id": "2507.12059", "pdf": "https://arxiv.org/pdf/2507.12059", "abs": "https://arxiv.org/abs/2507.12059", "authors": ["Anthony G Cohn", "Robert E Blackwell"], "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited", "categories": ["cs.CL"], "comment": "8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI", "summary": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e8628\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u786e\u5b9a\u6b63\u786e cardinal directions \u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5373\u4f7f\u662f\u8f83\u65b0\u7684\u6a21\u578b\u4e5f\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u4e0e cardinal directions \u76f8\u5173\u7684\u95ee\u9898\u65f6\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e00\u7ec4\u6a21\u677f\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f3028\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7ed9\u5b9a\u7279\u5b9a\u60c5\u5883\u4e0b\u786e\u5b9a\u6b63\u786e cardinal directions (CDs) \u7684\u80fd\u529b\u3002", "result": "\u5373\u4f7f\u662f\u6700\u65b0\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e5f\u65e0\u6cd5\u53ef\u9760\u5730\u786e\u5b9a\u6240\u6709\u95ee\u9898\u7684\u6b63\u786e CD\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u5e76\u6269\u5c55\u4e86\u4e4b\u524d\u5728COSIT-24\u4e0a\u53d1\u8868\u7684\u5de5\u4f5c\uff0c\u6307\u51fa\u5373\u4f7f\u662f\u8f83\u65b0\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e5f\u65e0\u6cd5\u53ef\u9760\u5730\u786e\u5b9a\u6240\u6709\u95ee\u9898\u7684\u6b63\u786e\u65b9\u5411\u3002"}}
{"id": "2507.12064", "pdf": "https://arxiv.org/pdf/2507.12064", "abs": "https://arxiv.org/abs/2507.12064", "authors": ["Jeremi K. Ochab", "Mateusz Matias", "Tymoteusz Boba", "Tomasz Walkowiak"], "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u5757\u5316\u98ce\u683c\u5b66\u7ba1\u9053\u7684\u4e8c\u5143AI\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528spaCy\u6a21\u578b\u8fdb\u884c\u6587\u672c\u9884\u5904\u7406\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u68af\u5ea6\u63d0\u5347\u673a\u4f5c\u4e3a\u5206\u7c7b\u5668\uff0c\u5728\u5927\u91cf\u673a\u5668\u751f\u6210\u6587\u672c\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5177\u6709\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\u7684\u7279\u70b9\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\u7684\u4e8c\u5143AI\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u672c\u6587\u4f7f\u7528\u516c\u5f00\u7684spaCy\u6a21\u578b\u8fdb\u884c\u6587\u672c\u9884\u5904\u7406\uff0c\u5305\u62ec\u5206\u8bcd\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\u3001\u8bcd\u6027\u6807\u6ce8\u548c\u5f62\u6001\u5b66\u6ce8\u91ca\uff0c\u5e76\u63d0\u53d6\u6570\u5343\u4e2a\u7279\u5f81\uff08\u5982\u4e0a\u8ff0\u8bed\u8a00\u6ce8\u91ca\u7684n-gram\u9891\u7387\uff09\u3002\u7136\u540e\u4f7f\u7528\u8f7b\u91cf\u7ea7\u68af\u5ea6\u63d0\u5347\u673a\u4f5c\u4e3a\u5206\u7c7b\u5668\u3002", "result": "\u672c\u6587\u901a\u8fc7\u63a2\u7d22\u591a\u79cd\u53c2\u6570\u9009\u9879\u6765\u589e\u52a0\u5206\u7c7b\u5668\u7684\u80fd\u529b\uff0c\u5e76\u5229\u7528\u8bad\u7ec3\u96c6\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u5757\u5316\u98ce\u683c\u5b66\u7ba1\u9053\u7684\u4e8c\u5143AI\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u516c\u5f00\u7684spaCy\u6a21\u578b\u8fdb\u884c\u6587\u672c\u9884\u5904\u7406\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u68af\u5ea6\u63d0\u5347\u673a\u4f5c\u4e3a\u5206\u7c7b\u5668\u3002\u8be5\u65b9\u6cd5\u5728\u5927\u91cf\u673a\u5668\u751f\u6210\u6587\u672c\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5177\u6709\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\u7684\u7279\u70b9\u3002"}}
{"id": "2507.12075", "pdf": "https://arxiv.org/pdf/2507.12075", "abs": "https://arxiv.org/abs/2507.12075", "authors": ["Giuliano Martinelli", "Tommaso Bonomo", "Pere-Llu\u00eds Huguet Cabot", "Roberto Navigli"], "title": "BOOKCOREF: Coreference Resolution at Book Scale", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference. 19 pages", "summary": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u7684\u5171\u6307\u89e3\u6790\u6807\u6ce8\u7ba1\u9053\uff0c\u5e76\u521b\u5efa\u4e86\u7b2c\u4e00\u4e2a\u4e66\u89c4\u6a21\u7684\u5171\u6307\u89e3\u6790\u57fa\u51c6BOOKCOREF\uff0c\u4ee5\u8bc4\u4f30\u7cfb\u7edf\u5728\u957f\u6587\u6863\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5171\u6307\u89e3\u6790\u8bc4\u4f30\u57fa\u51c6\u5728\u957f\u5ea6\u4e0a\u6709\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u7cfb\u7edf\u5728\u4e66\u89c4\u6a21\u4e0b\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u7684\u7ba1\u9053\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5171\u6307\u89e3\u6790\u6807\u6ce8\uff0c\u7136\u540e\u5229\u7528\u8be5\u7ba1\u9053\u521b\u5efa\u4e86BOOKCOREF\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4f7f\u7528BOOKCOREF\u57fa\u51c6\u53ef\u4ee5\u63d0\u5347\u5f53\u524d\u957f\u6587\u6863\u5171\u6307\u89e3\u6790\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u6700\u591a\u53ef\u63d0\u9ad820\u4e2aCoNLL-F1\u5206\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e86\u5728\u4e66\u89c4\u6a21\u8bbe\u7f6e\u4e0b\u51fa\u73b0\u7684\u65b0\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u7684\u7ba1\u9053\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5171\u6307\u89e3\u6790\u6807\u6ce8\uff0c\u5e76\u521b\u5efa\u4e86\u7b2c\u4e00\u4e2a\u4e66\u89c4\u6a21\u7684\u5171\u6307\u89e3\u6790\u57fa\u51c6BOOKCOREF\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u8d44\u6e90\u80fd\u591f\u63d0\u5347\u5f53\u524d\u957f\u6587\u6863\u5171\u6307\u89e3\u6790\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u4e66\u89c4\u6a21\u8bbe\u7f6e\u4e0b\u51fa\u73b0\u7684\u65b0\u6311\u6218\u3002"}}
{"id": "2507.12079", "pdf": "https://arxiv.org/pdf/2507.12079", "abs": "https://arxiv.org/abs/2507.12079", "authors": ["Tosin Adewumi", "Foteini Simistira Liwicki", "Marcus Liwicki", "Viktor Gardelli", "Lama Alkhaled", "Hamam Mokayed"], "title": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning", "categories": ["cs.CL"], "comment": "This paper was accepted for the special issue AI for Education by the\n  IEEE Signal Processing Magazine journal", "summary": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7ed3\u5408\u82cf\u683c\u62c9\u5e95\u65b9\u6cd5\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u7b80\u5316\u6e38\u620f\u5316\u548c\u5f62\u6210\u6027\u53cd\u9988\u7684MEGA\u65b9\u6cd5\u5bf9\u5927\u5b66\u751f\u6570\u5b66\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u5e76\u53d1\u73b0MEGA\u65b9\u6cd5\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfCoT\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u66f4\u96be\u7684MATH\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u4e00\u4e9b\u5b66\u751f\u5728\u6570\u5b66\u5b66\u4e60\u4e2d\u9047\u5230\u56f0\u96be\uff0c\u5bfc\u81f4\u4ed6\u4eec\u907f\u514d\u4e0e\u6570\u5b66\u76f8\u5173\u7684\u5b66\u79d1\u6216\u4e3b\u9898\uff0c\u800c\u6570\u5b66\u5728\u8bb8\u591a\u9886\u57df\u90fd\u5f88\u91cd\u8981\u3002\u5b66\u751f\u7684\u6570\u5b66\u56f0\u96be\u901a\u5e38\u6e90\u4e8e\u4e0d\u826f\u7684\u6559\u5b66\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u7ed3\u5408\u82cf\u683c\u62c9\u5e95\u65b9\u6cd5\u3001\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u3001\u7b80\u5316\u6e38\u620f\u5316\u548c\u5f62\u6210\u6027\u53cd\u9988\u7684\u5e72\u9884\u65b9\u6cd5\uff0c\u79f0\u4e3aMEGA\u3002\u901a\u8fc7\u968f\u673a\u5206\u914d\u95ee\u9898\uff0c\u4f7f\u7528\u7ec4\u5185\u8bbe\u8ba1\u6bd4\u8f83\u4e86MEGA\u65b9\u6cd5\u4e0e\u4f20\u7edfCoT\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5b66\u751f\u5728\u66f4\u591a\u60c5\u51b5\u4e0b\u8ba4\u4e3aMEGA\u65b9\u6cd5\u5728\u5b66\u4e60\u6548\u679c\u4e0a\u4f18\u4e8e\u4f20\u7edfCoT\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728MATH\u6570\u636e\u96c6\u4e0a\uff0cMEGA\u65b9\u6cd5\u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8eCoT\u65b9\u6cd5\uff0847.5% vs 26.67%\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0cMEGA\u65b9\u6cd5\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u6bd4\u4f20\u7edf\u7684CoT\u65b9\u6cd5\u66f4\u53d7\u5b66\u751f\u6b22\u8fce\uff0c\u7279\u522b\u662f\u5728\u66f4\u96be\u7684MATH\u6570\u636e\u96c6\u4e0a\uff0cMEGA\u65b9\u6cd5\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8eCoT\u65b9\u6cd5\u3002"}}
{"id": "2507.12126", "pdf": "https://arxiv.org/pdf/2507.12126", "abs": "https://arxiv.org/abs/2507.12126", "authors": ["Payal Bhattad", "Sai Manoj Pudukotai Dinakarrao", "Anju Gupta"], "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u589e\u5f3a\u539f\u5219\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u53ef\u6269\u5c55\u6027\u5206\u6790\u548c\u8fed\u4ee3\u589e\u5f3a\u4e0e\u6458\u8981\u4f18\u5316\uff0c\u5b9e\u9a8c\u8bc1\u660eGPT-3.5 Turbo\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u751f\u6210\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u5efa\u6a21\u7684\u6548\u679c\u3002", "motivation": "\u6587\u672c\u6570\u636e\u589e\u5f3a\u662f\u4e00\u79cd\u5e7f\u6cdb\u7528\u4e8e\u7f13\u89e3\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u6570\u636e\u7a00\u758f\u6027\u7684\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\uff0c\u6709\u9650\u7684\u6837\u672c\u963b\u788d\u4e86\u6709\u6548\u7684\u8bed\u4e49\u5efa\u6a21\u3002\u867d\u7136\u589e\u5f3a\u53ef\u4ee5\u63d0\u9ad8\u8f93\u5165\u591a\u6837\u6027\u5e76\u6539\u5584\u4e0b\u6e38\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u73b0\u6709\u6280\u672f\u901a\u5e38\u7f3a\u4e4f\u786e\u4fdd\u5927\u89c4\u6a21\u6216\u8fed\u4ee3\u751f\u6210\u65f6\u8bed\u4e49\u4fdd\u7559\u7684\u673a\u5236\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u4e0d\u7a33\u5b9a\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6587\u672c\u589e\u5f3a\u7684\u539f\u5219\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u4e2a\u7ec4\u4ef6\uff1a(1) \u53ef\u6269\u5c55\u6027\u5206\u6790\uff0c\u6d4b\u91cf\u968f\u7740\u589e\u5f3a\u91cf\u589e\u52a0\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff1b(2) \u8fed\u4ee3\u589e\u5f3a\u4e0e\u6458\u8981\u4f18\u5316\uff08IASR\uff09\uff0c\u8bc4\u4f30\u9012\u5f52\u6539\u5199\u5faa\u73af\u4e2d\u7684\u8bed\u4e49\u6f02\u79fb\u3002", "result": "\u5728\u6700\u5148\u8fdb\u7684LLM\u4e0a\u8fdb\u884c\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cGPT-3.5 Turbo\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u751f\u6210\u6548\u7387\u4e4b\u95f4\u8fbe\u5230\u4e86\u6700\u4f73\u5e73\u8861\u3002\u5e94\u7528\u4e8e\u4f7f\u7528GPT\u589e\u5f3a\u7684\u5c11\u6837\u672c\u6807\u8bb0\u7684Bertopic\u771f\u5b9e\u4e16\u754c\u4e3b\u9898\u5efa\u6a21\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u4f7f\u4e3b\u9898\u7c92\u5ea6\u589e\u52a0\u4e86400%\uff0c\u5e76\u5b8c\u5168\u6d88\u9664\u4e86\u4e3b\u9898\u91cd\u53e0\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5b9e\u9645NLP\u6d41\u6c34\u7ebf\u4e2d\u5bf9\u57fa\u4e8eLLM\u7684\u589e\u5f3a\u8fdb\u884c\u7ed3\u6784\u5316\u8bc4\u4f30\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.12143", "pdf": "https://arxiv.org/pdf/2507.12143", "abs": "https://arxiv.org/abs/2507.12143", "authors": ["Pavel \u0160indel\u00e1\u0159", "Ond\u0159ej Bojar"], "title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators", "categories": ["cs.CL", "I.2.7"], "comment": "30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum", "summary": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.", "AI": {"tldr": "ELOQUENT\u7684Sensemaking\u4efb\u52a1\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u5982\u4f55\u4ece\u7ed9\u5b9a\u6587\u672c\u4e2d\u201c\u7406\u89e3\u201d\u5185\u5bb9\uff0c\u5206\u4e3a\u4e09\u4e2a\u6b65\u9aa4\uff1a\u51c6\u5907\u95ee\u9898\u3001\u56de\u7b54\u95ee\u9898\u548c\u8bc4\u5206\u3002\u867d\u7136\u6a21\u578b\u5728\u95ee\u9898\u56de\u7b54\u65b9\u9762\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u9650\u5236\u7b54\u6848\u4ec5\u57fa\u4e8e\u8f93\u5165\u6587\u672c\u4ecd\u5b58\u5728\u95ee\u9898\uff0c\u4e14\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u7cfb\u7edf\u5728\u8bc4\u4f30\u65f6\u5b58\u5728\u9519\u8bef\u3002", "motivation": "ELOQUENT\u662f\u4e00\u4e2a\u5171\u4eab\u4efb\u52a1\uff0c\u65e8\u5728\u521b\u5efa\u6613\u4e8e\u6d4b\u8bd5\u7684\u9ad8\u7ea7\u6807\u51c6\uff0c\u4ee5\u8bc4\u4f30\u751f\u6210\u8bed\u8a00\u6a21\u578b\u3002Sensemaking\u662f\u5176\u4e2d\u7684\u4e00\u4e2a\u4efb\u52a1\uff0c\u65e8\u5728\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u5982\u4f55\u4ece\u7ed9\u5b9a\u6587\u672c\u4e2d\u201c\u7406\u89e3\u201d\u5185\u5bb9\u3002", "method": "Sensemaking\u4efb\u52a1\u5206\u4e3a\u4e09\u4e2a\u6b65\u9aa4\uff1a(1) \u6559\u5e08\u7cfb\u7edf\u51c6\u5907\u95ee\u9898\u96c6\uff0c(2) \u5b66\u751f\u7cfb\u7edf\u56de\u7b54\u8fd9\u4e9b\u95ee\u9898\uff0c(3) \u8bc4\u4f30\u8005\u7cfb\u7edf\u8bc4\u5206\u3002\u6211\u4eec\u91c7\u7528\u4e86\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4e0e\u6700\u5c0f\u5316\u7684\u624b\u52a8\u8bc4\u4f30\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u57282025\u5e74\u7684Sensemaking\u4efb\u52a1\u4e2d\uff0c\u67097\u79cd\u6d4b\u8bd5\u6750\u6599\u6765\u6e90\uff0c4\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u63d0\u4f9b\u4e862\u4e2a\u6559\u5e08\u63d0\u4ea4\u30012\u4e2a\u5b66\u751f\u63d0\u4ea4\u548c2\u4e2a\u8bc4\u4f30\u8005\u63d0\u4ea4\u3002\u6211\u4eec\u4e3a\u6559\u5e08\u548c\u5b66\u751f\u6dfb\u52a0\u4e86\u57fa\u7ebf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u8bc4\u4f30\u8fc7\u7a0b\u3002", "conclusion": "\u5728Sensemaking\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\u751f\u6210\u6a21\u578b\u5728\u95ee\u9898\u56de\u7b54\u65b9\u9762\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u9650\u5236\u5176\u7b54\u6848\u4ec5\u57fa\u4e8e\u7ed9\u5b9a\u6587\u672c\u4ecd\u7136\u5b58\u5728\u95ee\u9898\u3002\u6b64\u5916\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u7cfb\u7edf\u5728\u8bc4\u4f30\u95ee\u9898\u7b54\u6848\u65f6\u5b58\u5728\u9519\u8bef\u3002"}}
{"id": "2507.12208", "pdf": "https://arxiv.org/pdf/2507.12208", "abs": "https://arxiv.org/abs/2507.12208", "authors": ["Michael Carl", "Takanori Mizowaki", "Aishvarya Ray", "Masaru Yamada", "Devi Sri Bandaru", "Xinyue Ren"], "title": "Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production", "categories": ["cs.CL"], "comment": null, "summary": "The paper introduces a Behavioural Translation Style Space (BTSS) that\ndescribes possible behavioural translation patterns. The suggested BTSS is\norganized as a hierarchical structure that entails various embedded processing\nlayers. We posit that observable translation behaviour - i.e., eye and finger\nmovements - is fundamental when executing the physical act of translation but\nit is caused and shaped by higher-order cognitive processes and affective\ntranslation states. We analyse records of keystrokes and gaze data as\nindicators of the hidden mental processing structure and organize the\nbehavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the\nbasis for a computational translation agent to simulate the temporal dynamics\nof affect, automatized behaviour and cognition during human translation\nproduction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u884c\u4e3a\u7ffb\u8bd1\u98ce\u683c\u7a7a\u95f4\uff08BTSS\uff09\uff0c\u7528\u4e8e\u63cf\u8ff0\u7ffb\u8bd1\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u51fb\u952e\u548c\u6ce8\u89c6\u6570\u636e\u6765\u6784\u5efa\u4e00\u4e2a\u5206\u5c42\u7ed3\u6784\uff0c\u4ee5\u6a21\u62df\u4eba\u7c7b\u7ffb\u8bd1\u4e2d\u7684\u60c5\u611f\u3001\u81ea\u52a8\u5316\u884c\u4e3a\u548c\u8ba4\u77e5\u52a8\u6001\u3002", "motivation": "\u4e3a\u4e86\u63cf\u8ff0\u53ef\u80fd\u7684\u884c\u4e3a\u7ffb\u8bd1\u6a21\u5f0f\uff0c\u5e76\u7406\u89e3\u7ffb\u8bd1\u884c\u4e3a\u80cc\u540e\u7684\u8ba4\u77e5\u548c\u60c5\u611f\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u51fb\u952e\u8bb0\u5f55\u548c\u6ce8\u89c6\u6570\u636e\uff0c\u5c06\u884c\u4e3a\u6a21\u5f0f\u7ec4\u7ec7\u4e3a\u591a\u5c42\u5d4c\u5957\u7684BTSS\u3002", "result": "\u63d0\u51fa\u4e86BTSS\uff0c\u5b83\u662f\u4e00\u4e2a\u5206\u5c42\u7ed3\u6784\uff0c\u5305\u542b\u5404\u79cd\u5d4c\u5165\u5f0f\u5904\u7406\u5c42\uff0c\u80fd\u591f\u53cd\u6620\u9690\u85cf\u7684\u5fc3\u7406\u52a0\u5de5\u7ed3\u6784\u3002", "conclusion": "BTSS\u53ef\u4ee5\u4f5c\u4e3a\u8ba1\u7b97\u7ffb\u8bd1\u4ee3\u7406\u7684\u57fa\u7840\uff0c\u4ee5\u6a21\u62df\u4eba\u7c7b\u7ffb\u8bd1\u751f\u4ea7\u4e2d\u7684\u60c5\u611f\u3001\u81ea\u52a8\u5316\u884c\u4e3a\u548c\u8ba4\u77e5\u7684\u65f6\u5e8f\u52a8\u6001\u3002"}}
{"id": "2507.12217", "pdf": "https://arxiv.org/pdf/2507.12217", "abs": "https://arxiv.org/abs/2507.12217", "authors": ["Reuben Smit", "Retief Louw", "Herman Kamper"], "title": "Towards few-shot isolated word reading assessment", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to SLaTE 2025", "summary": "We explore an ASR-free method for isolated word reading assessment in\nlow-resource settings. Our few-shot approach compares input child speech to a\nsmall set of adult-provided reference templates. Inputs and templates are\nencoded using intermediate layers from large self-supervised learned (SSL)\nmodels. Using an Afrikaans child speech benchmark, we investigate design\noptions such as discretising SSL features and barycentre averaging of the\ntemplates. Idealised experiments show reasonable performance for adults, but a\nsubstantial drop for child speech input, even with child templates. Despite the\nsuccess of employing SSL representations in low-resource speech tasks, our work\nhighlights the limitations of SSL representations for processing child data\nwhen used in a few-shot classification system.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u65e0\u9700\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u532e\u4e4f\u7684\u73af\u5883\u4e2d\u8bc4\u4f30\u5b64\u7acb\u5355\u8bcd\u7684\u9605\u8bfb\u60c5\u51b5\u3002\u901a\u8fc7\u6bd4\u8f83\u513f\u7ae5\u8bed\u97f3\u4e0e\u6210\u4eba\u63d0\u4f9b\u7684\u53c2\u8003\u6a21\u677f\uff0c\u53d1\u73b0SSL\u8868\u793a\u5728\u5904\u7406\u513f\u7ae5\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u8d44\u6e90\u532e\u4e4f\u7684\u73af\u5883\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5b64\u7acb\u5355\u8bcd\u7684\u9605\u8bfb\u60c5\u51b5\u3002", "method": "\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u65e0\u9700\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7684\u65b9\u6cd5\uff0c\u5c06\u8f93\u5165\u7684\u513f\u7ae5\u8bed\u97f3\u4e0e\u5c11\u91cf\u6210\u4eba\u63d0\u4f9b\u7684\u53c2\u8003\u6a21\u677f\u8fdb\u884c\u6bd4\u8f83\u3002\u8f93\u5165\u548c\u6a21\u677f\u901a\u8fc7\u5927\u578b\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u8fdb\u884c\u7f16\u7801\u3002", "result": "\u7406\u60f3\u5316\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5bf9\u4e8e\u6210\u5e74\u4eba\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u4e8e\u513f\u7ae5\u8bed\u97f3\u8f93\u5165\uff0c\u5373\u4f7f\u4f7f\u7528\u513f\u7ae5\u6a21\u677f\uff0c\u6027\u80fd\u4e5f\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u7a81\u663e\u4e86\u5728\u5c11\u6837\u672c\u5206\u7c7b\u7cfb\u7edf\u4e2d\u4f7f\u7528SSL\u8868\u793a\u5904\u7406\u513f\u7ae5\u6570\u636e\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.12252", "pdf": "https://arxiv.org/pdf/2507.12252", "abs": "https://arxiv.org/abs/2507.12252", "authors": ["Shilin Zhou", "Zhenghua Li"], "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c92\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8bcd\u7ea7\u522b\u548c\u77ed\u8bed\u7ea7\u522b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86ASR\u6a21\u578b\u5728\u5173\u952e\u8bcd\u8bc6\u522b\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u5728\u8bc6\u522b\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5173\u952e\u8bcd\uff08\u5982\u4e13\u6709\u540d\u8bcd\u6216\u7528\u6237\u7279\u5b9a\u5b9e\u4f53\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c92\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8bcd\u7ea7\u522b\u548c\u77ed\u8bed\u7ea7\u522b\u7684\u4f18\u52bf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u63d0\u5347\u5173\u952e\u8bcd\u8bc6\u522b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5173\u952e\u8bcd\u76f8\u5173\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u975e\u5173\u952e\u8bcd\u6587\u672c\u7684\u9ad8\u51c6\u786e\u6027\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u786e\u8ba4\u4e86\u8bcd\u7ea7\u522b\u548c\u77ed\u8bed\u7ea7\u522b\u7ec4\u4ef6\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u663e\u8457\u8d21\u732e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u7c92\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8bcd\u7ea7\u522b\u548c\u77ed\u8bed\u7ea7\u522b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5173\u952e\u8bcd\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u5728\u4e2d\u82f1\u6587\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2507.12260", "pdf": "https://arxiv.org/pdf/2507.12260", "abs": "https://arxiv.org/abs/2507.12260", "authors": ["Yikang Liu", "Wanyang Zhang", "Yiming Wang", "Jialong Tang", "Pei Zhang", "Baosong Yang", "Fei Huang", "Rui Wang", "Hai Hu"], "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.", "AI": {"tldr": "The paper introduces T-index, a quantitative measure for translationese, which is computed from the likelihood ratios of two contrastively fine-tuned language models. It shows that T-index is robust, efficient, and correlates well with human judgments, making it a useful complementary metric in machine translation quality estimation.", "motivation": "The paper aims to introduce a quantitative measure for translationese, called T-index, which can be used to assess translationese in a graded and generalizable manner.", "method": "The paper proposes the T-index, which is calculated from the likelihood ratios of two contrastively fine-tuned language models. It uses a synthesized dataset and a dataset with translations in the wild to evaluate the generalizability and validity of T-index.", "result": "The results show that T-index is robust and efficient. It can well capture translationese in the wild using only 1-5k pairs of synthetic data. The relative differences in T-indices between translations can predict pairwise translationese annotations from human annotators, and the absolute values of T-indices correlate well with human ratings of degrees of translationese (Pearson's $r = 0.568$).", "conclusion": "T-index can serve as a complementary metric in MT QE because it is not covered by existing metrics like BLEU and COMET."}}
{"id": "2507.12261", "pdf": "https://arxiv.org/pdf/2507.12261", "abs": "https://arxiv.org/abs/2507.12261", "authors": ["Johann Frei", "Nils Feldhus", "Lisa Raithel", "Roland Roller", "Alexander Meyer", "Frank Kramer"], "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to EMNLP 2025 System Demonstrations | Code:\n  https://github.com/j-frei/Infherno | Video:\n  https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo:\n  https://infherno.misit-augsburg.de | HuggingFace Spaces:\n  https://huggingface.co/spaces/nfel/infherno", "summary": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u3001\u4ee3\u7801\u6267\u884c\u548c\u533b\u7597\u672f\u8bed\u6570\u636e\u5e93\u5de5\u5177\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u81ea\u7531\u5f62\u5f0f\u7684\u4e34\u5e8a\u7b14\u8bb0\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684FHIR\u8d44\u6e90\u3002", "motivation": "\u4e4b\u524d\u5c1d\u8bd5\u81ea\u52a8\u5316\u4ece\u81ea\u7531\u5f62\u5f0f\u7684\u4e34\u5e8a\u7b14\u8bb0\u5230\u7ed3\u6784\u5316FHIR\u8d44\u6e90\u7684\u8f6c\u6362\uff0c\u7531\u4e8e\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u7ed3\u6784\u4e0d\u4e00\u81f4\u800c\u9762\u4e34\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531LLM\u4ee3\u7406\u3001\u4ee3\u7801\u6267\u884c\u548c\u533b\u7597\u672f\u8bed\u6570\u636e\u5e93\u5de5\u5177\u652f\u6301\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002", "result": "Infherno\u80fd\u591f\u9075\u5faaFHIR\u6587\u6863\u6a21\u5f0f\uff0c\u5e76\u4e14\u5728\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u9884\u6d4bFHIR\u8d44\u6e90\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848Infherno\u80fd\u591f\u9075\u5faaFHIR\u6587\u6863\u6a21\u5f0f\uff0c\u5e76\u4e14\u5728\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u9884\u6d4bFHIR\u8d44\u6e90\u65b9\u9762\u4e0e\u4eba\u7c7b\u57fa\u7ebf\u7ade\u4e89\u826f\u597d\u3002"}}
{"id": "2507.12295", "pdf": "https://arxiv.org/pdf/2507.12295", "abs": "https://arxiv.org/abs/2507.12295", "authors": ["Feng Xiao", "Jicong Fan"], "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u5d4c\u5165\u8d28\u91cf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u53d1\u73b0\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u4f7f\u7528LLM\u5d4c\u5165\u65f6\u5e76\u4e0d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u73b0\u6709\u7684\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8fd9\u9650\u5236\u4e86\u4e25\u683c\u6bd4\u8f83\u548c\u521b\u65b0\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5229\u7528\u6765\u81ea\u4e0d\u540c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\uff0c\u5e76\u5728\u591a\u4e2a\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5d4c\u5165\u7684\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5d4c\u5165\u8d28\u91cf\u663e\u8457\u5f71\u54cd\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u4f7f\u7528LLM\u751f\u6210\u7684\u5d4c\u5165\u65f6\u5e76\u4e0d\u6bd4\u4f20\u7edf\u6d45\u5c42\u7b97\u6cd5\uff08\u5982KNN\u3001\u5b64\u7acb\u68ee\u6797\uff09\u8868\u73b0\u66f4\u597d\u3002\u6b64\u5916\uff0c\u8de8\u6a21\u578b\u6027\u80fd\u77e9\u9635\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u4f4e\u79e9\u7279\u6027\uff0c\u8fd9\u4f7f\u5f97\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u4ee5\u91c7\u7528\u9ad8\u6548\u7684\u6a21\u578b\u8bc4\u4f30\u548c\u9009\u62e9\u7b56\u7565\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u5168\u9762\u7684\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5d4c\u5165\u8d28\u91cf\u5bf9\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.12308", "pdf": "https://arxiv.org/pdf/2507.12308", "abs": "https://arxiv.org/abs/2507.12308", "authors": ["Prashanth Vijayaraghavan", "Apoorva Nitsure", "Charles Mackin", "Luyao Shi", "Stefano Ambrogio", "Arvind Haran", "Viresh Paruthi", "Ali Elzein", "Dan Coops", "David Beymer", "Tyler Baldwin", "Ehsan Degan"], "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization", "categories": ["cs.CL", "cs.AI", "cs.AR"], "comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)", "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86CoDes\u65b9\u6cd5\u6765\u63d0\u9ad8\u5176\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoDes\u65b9\u6cd5\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\uff08\u5982VHDL\uff09\u65b9\u9762\u7684\u7814\u7a76\u4ecd\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u6539\u8fdbLLMs\u5728VHDL\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "method": "\u672c\u6587\u8bc4\u4f30\u4e86\u73b0\u6709\u4ee3\u7801LLMs\u5728VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86CoDes\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e00\u7cfb\u5217\u4e2d\u95f4\u63cf\u8ff0\u6b65\u9aa4\u6765\u589e\u5f3aLLMs\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u7684LLMs\u5728VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u800cCoDes\u65b9\u6cd5\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u7b56\u7565\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Descriptions (CoDes)\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cCoDes\u65b9\u6cd5\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86VHDL\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u7684\u8d28\u91cf\uff0c\u8fd8\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2507.12356", "pdf": "https://arxiv.org/pdf/2507.12356", "abs": "https://arxiv.org/abs/2507.12356", "authors": ["Liu He", "Yuanchao Li", "Rui Feng", "XinRan Han", "Yin-Long Liu", "Yuwei Yang", "Zude Zhu", "Jiahong Yuan"], "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception", "categories": ["cs.CL", "cs.HC", "cs.SD"], "comment": "12 pages, 5 figures, conference or other essential info", "summary": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\u7537\u6027\u8bed\u97f3\u66f4\u5bb9\u6613\u88ab\u8bef\u8ba4\u4e3a\u60a3\u6709\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff0c\u8fd9\u8868\u660e\u5728\u5f00\u53d1AD\u68c0\u6d4b\u6a21\u578b\u65f6\u9700\u8981\u8003\u8651\u6027\u522b\u504f\u89c1\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u6027\u522b\u504f\u89c1\u5728AD\u8bed\u97f3\u611f\u77e5\u4e2d\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4e00\u4e2a\u6d89\u53ca16\u540d\u4e2d\u56fd\u542c\u4f17\u8bc4\u4f30\u4e2d\u82f1\u6587\u548c\u5e0c\u814a\u8bed\u8bed\u97f3\u7684\u611f\u77e5\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u6027\u522b\u504f\u89c1\u5728AD\u8bed\u97f3\u611f\u77e5\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7537\u6027\u8bed\u97f3\u66f4\u5bb9\u6613\u88ab\u8bc6\u522b\u4e3aAD\uff0c\u8fd9\u79cd\u504f\u89c1\u5728\u4e2d\u6587\u8bed\u97f3\u4e2d\u5c24\u4e3a\u660e\u663e\u3002\u58f0\u5b66\u5206\u6790\u8868\u660e\uff0c\u7537\u6027\u8bed\u97f3\u7684shimmer\u503c\u4e0eAD\u611f\u77e5\u663e\u8457\u76f8\u5173\uff0c\u800c\u8bed\u97f3\u90e8\u5206\u4e0eAD\u8bc6\u522b\u5448\u663e\u8457\u8d1f\u76f8\u5173\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5f00\u53d1\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u68c0\u6d4b\u6a21\u578b\u65f6\u89e3\u51b3\u6027\u522b\u504f\u89c1\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5728\u4e0d\u540c\u8bed\u8a00\u73af\u5883\u4e2d\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.12370", "pdf": "https://arxiv.org/pdf/2507.12370", "abs": "https://arxiv.org/abs/2507.12370", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u7528\u6237\u8bf7\u6c42\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u7528\u6237\u8bf7\u6c42\u65f6\u9762\u4e34\u6b67\u4e49\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u589e\u5f3a\u68c0\u6d4b\u548c\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u672c\u6587\u5f15\u5165\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7531\u4e09\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff08Llama3-8B\u3001Gemma2-9B \u548c Mistral-7B \u53d8\u4f53\uff09\u548c\u4e00\u4e2a\u5177\u6709\u591a\u6837\u6b67\u4e49\u7684\u6570\u636e\u96c6\u7ec4\u6210\u3002", "result": "\u8fa9\u8bba\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86 Llama3-8B \u548c Mistral-7B \u53d8\u4f53\u7684\u6027\u80fd\uff0c\u5176\u4e2d Mistral-7B \u9886\u5bfc\u7684\u8fa9\u8bba\u53d6\u5f97\u4e86 76.7% \u7684\u6210\u529f\u7387\uff0c\u5e76\u5728\u5904\u7406\u590d\u6742\u6b67\u4e49\u548c\u9ad8\u6548\u5171\u8bc6\u65b9\u9762\u7279\u522b\u6709\u6548\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\u5728\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u65b9\u9762\u7684\u4ef7\u503c\uff0c\u8868\u660e\u7ed3\u6784\u5316\u8fa9\u8bba\u53ef\u4ee5\u63d0\u9ad8\u4ea4\u4e92\u7cfb\u7edf\u7684\u6e05\u6670\u5ea6\u3002"}}
{"id": "2507.12372", "pdf": "https://arxiv.org/pdf/2507.12372", "abs": "https://arxiv.org/abs/2507.12372", "authors": ["Meysam Alizadeh", "Fabrizio Gilardi", "Zeynab Samei", "Mohsen Mosleh"], "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLMs\u662f\u5426\u80fd\u901a\u8fc7\u7528\u6237\u540d\u63a8\u65ad\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u7684\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\uff0c\u5e76\u53d1\u73b0\u5b83\u4eec\u53ef\u4ee5\u505a\u5230\u8fd9\u4e00\u70b9\uff0c\u4f46\u4e5f\u5b58\u5728\u6f5c\u5728\u7684\u504f\u89c1\u548c\u6ee5\u7528\u98ce\u9669\u3002", "motivation": "\u5c3d\u7ba1\u4e4b\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u8bc1\u660eLLMs\u80fd\u591f\u8bbf\u95ee\u548c\u5206\u6790\u7f51\u7ad9\uff0c\u4f46\u5b83\u4eec\u76f4\u63a5\u68c0\u7d22\u548c\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u7684\u80fd\u529b\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30LLMs\u662f\u5426\u80fd\u591f\u901a\u8fc7\u7528\u6237\u540d\u63a8\u65ad\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u7684\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u542b48\u4e2aTwitter\u8d26\u6237\u7684\u5408\u6210\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5305\u542b1,384\u540d\u56fd\u9645\u53c2\u4e0e\u8005\u7684\u8c03\u67e5\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86LLMs\u80fd\u5426\u4ec5\u901a\u8fc7\u7528\u6237\u540d\u63a8\u65ad\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u7684\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u8bbf\u95ee\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5e76\u4ee5\u5408\u7406\u7684\u51c6\u786e\u6027\u9884\u6d4b\u7528\u6237\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u3002\u5bf9\u5408\u6210\u6570\u636e\u96c6\u7684\u5206\u6790\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86LLMs\u5982\u4f55\u89e3\u6790\u548c\u89e3\u91ca\u793e\u4ea4\u5a92\u4f53\u8d44\u6599\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5bf9\u6d3b\u52a8\u8f83\u5c11\u7684\u8d26\u6237\u4ea7\u751f\u6027\u522b\u548c\u653f\u6cbb\u504f\u89c1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\uff0c\u867d\u7136LLMs\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u91c7\u53d6\u9632\u62a4\u63aa\u65bd\u4ee5\u9632\u6b62\u6ee5\u7528\u3002\u5efa\u8baeLLM\u63d0\u4f9b\u5546\u5728\u9762\u5411\u516c\u4f17\u7684\u5e94\u7528\u4e2d\u9650\u5236\u6b64\u529f\u80fd\uff0c\u540c\u65f6\u4e3a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7814\u7a76\u76ee\u7684\u4fdd\u7559\u53d7\u63a7\u8bbf\u95ee\u3002"}}
{"id": "2507.12379", "pdf": "https://arxiv.org/pdf/2507.12379", "abs": "https://arxiv.org/abs/2507.12379", "authors": ["Yucheng Sun", "Alessandro Stolfo", "Mrinmaya Sachan"], "title": "Probing for Arithmetic Errors in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\u6765\u68c0\u6d4b\u7b97\u672f\u9519\u8bef\uff0c\u5e76\u5c55\u793a\u4e86\u7b80\u5355\u7684\u63a2\u6d4b\u5668\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u9519\u8bef\u68c0\u6d4b\u548c\u81ea\u6211\u7ea0\u6b63\u3002", "motivation": "\u6211\u4eec\u60f3\u4e86\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\u662f\u5426\u53ef\u4ee5\u7528\u6765\u68c0\u6d4b\u7b97\u672f\u9519\u8bef\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u6fc0\u6d3b\u8fdb\u884c\u6a21\u578b\u81ea\u6211\u7ea0\u6b63\u3002", "method": "\u6211\u4eec\u9996\u5148\u57283\u4f4d\u6570\u52a0\u6cd5\u7684\u53d7\u63a7\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e86\u5185\u90e8\u6fc0\u6d3b\u80fd\u5426\u7528\u4e8e\u68c0\u6d4b\u7b97\u672f\u9519\u8bef\uff0c\u7136\u540e\u8bad\u7ec3\u4e86\u8f7b\u91cf\u7ea7\u7684\u9519\u8bef\u68c0\u6d4b\u5668\uff0c\u5e76\u6269\u5c55\u5230\u7ed3\u6784\u5316\u7684\u601d\u7ef4\u94fe\u8ffd\u8e2a\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u7b80\u5355\u7684\u63a2\u6d4b\u5668\u53ef\u4ee5\u51c6\u786e\u5730\u4ece\u9690\u85cf\u72b6\u6001\u4e2d\u89e3\u7801\u6a21\u578b\u7684\u9884\u6d4b\u8f93\u51fa\u548c\u6b63\u786e\u7b54\u6848\uff0c\u65e0\u8bba\u6a21\u578b\u7684\u8f93\u51fa\u662f\u5426\u6b63\u786e\u3002\u6211\u4eec\u8fd8\u8bad\u7ec3\u4e86\u8d85\u8fc790%\u51c6\u786e\u7387\u7684\u8f7b\u91cf\u7ea7\u9519\u8bef\u68c0\u6d4b\u5668\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u63a2\u6d4b\u5668\u53ef\u4ee5\u5f15\u5bfc\u9009\u62e9\u6027\u91cd\u65b0\u63d0\u793a\u9519\u8bef\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u4ece\u800c\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4ece\u5185\u90e8\u6fc0\u6d3b\u5c31\u53ef\u4ee5\u9884\u6d4b\u7b97\u672f\u9519\u8bef\uff0c\u5e76\u4e14\u7b80\u5355\u7684\u63a2\u6d4b\u5668\u4e3a\u8f7b\u91cf\u7ea7\u6a21\u578b\u81ea\u6211\u7ea0\u6b63\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u8def\u5f84\u3002"}}
{"id": "2507.12425", "pdf": "https://arxiv.org/pdf/2507.12425", "abs": "https://arxiv.org/abs/2507.12425", "authors": ["Chandana Cheerla"], "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR"], "comment": null, "summary": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684RAG\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6df7\u5408\u68c0\u7d22\u7b56\u7565\u3001\u8bed\u4e49\u5206\u5757\u548c\u91cf\u5316\u7d22\u5f15\uff0c\u4ee5\u63d0\u9ad8\u4f01\u4e1a\u6570\u636e\u5904\u7406\u7684\u6548\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u76f8\u5173\u6027\u65b9\u9762\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7ec4\u7ec7\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u4e13\u6709\u4f01\u4e1a\u6570\u636e\u8fdb\u884c\u5173\u952e\u51b3\u7b56\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5177\u6709\u5f3a\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u53d7\u5230\u9759\u6001\u9884\u8bad\u7ec3\u3001\u77ed\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u5904\u7406\u5f02\u6784\u6570\u636e\u683c\u5f0f\u7684\u6311\u6218\u7684\u9650\u5236\u3002\u4f20\u7edf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\u89e3\u51b3\u4e86\u4e00\u4e9b\u8fd9\u4e9b\u5dee\u8ddd\uff0c\u4f46\u901a\u5e38\u5728\u7ed3\u6784\u5316\u548c\u534a\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5148\u8fdb\u7684RAG\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4f7f\u7528\u5bc6\u96c6\u5d4c\u5165\uff08all-mpnet-base-v2\uff09\u548cBM25\u7684\u6df7\u5408\u68c0\u7d22\u7b56\u7565\uff0c\u5e76\u901a\u8fc7SpaCy NER\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u65b0\u6392\u5e8f\u8fdb\u884c\u589e\u5f3a\u3002\u8be5\u6846\u67b6\u5e94\u7528\u8bed\u4e49\u5206\u5757\u4ee5\u4fdd\u6301\u6587\u672c\u8fde\u8d2f\u6027\uff0c\u5e76\u4fdd\u7559\u8868\u683c\u6570\u636e\u7ed3\u6784\u4ee5\u4fdd\u6301\u884c-\u5217\u5b8c\u6574\u6027\u3002\u91cf\u5316\u7d22\u5f15\u4f18\u5316\u4e86\u68c0\u7d22\u6548\u7387\uff0c\u540c\u65f6\u4eba\u673a\u53cd\u9988\u548c\u5bf9\u8bdd\u8bb0\u5fc6\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u3002", "result": "\u5728\u4f01\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u6539\u8fdb\uff1aPrecision@5\u589e\u52a0\u4e8615%\uff0890\u5bf975\uff09\uff0cRecall@5\u589e\u52a0\u4e8613%\uff0887\u5bf974\uff09\uff0c\u5e73\u5747\u5012\u6570\u6392\u540d\u589e\u52a0\u4e8616%\uff080.85\u5bf90.69\uff09\u3002\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u4e00\u4e2a5\u70b9\u674e\u514b\u7279\u91cf\u8868\u4e0a\uff0c\u5fe0\u5b9e\u5ea6\uff084.6\u5bf93.0\uff09\u3001\u5b8c\u6574\u5ea6\uff084.2\u5bf92.5\uff09\u548c\u76f8\u5173\u6027\uff084.5\u5bf93.2\uff09\u5f97\u5206\u66f4\u9ad8\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u5728\u4e3a\u4f01\u4e1a\u4efb\u52a1\u63d0\u4f9b\u51c6\u786e\u3001\u5168\u9762\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u54cd\u5e94\u65b9\u9762\u662f\u6709\u6548\u7684\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5305\u62ec\u6269\u5c55\u5230\u591a\u6a21\u6001\u6570\u636e\u548c\u96c6\u6210\u57fa\u4e8e\u4ee3\u7406\u7684\u68c0\u7d22\u3002"}}
{"id": "2507.12428", "pdf": "https://arxiv.org/pdf/2507.12428", "abs": "https://arxiv.org/abs/2507.12428", "authors": ["Yik Siu Chan", "Zheng-Xin Yong", "Stephen H. Bach"], "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u57fa\u4e8eCoT\u6fc0\u6d3b\u7684\u7ebf\u6027\u63a2\u6d4b\u5668\u80fd\u6709\u6548\u9884\u6d4b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u5b89\u5168\u6027\uff0c\u4e14\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u5b9e\u73b0\u5b9e\u65f6\u76d1\u63a7\u3002", "motivation": "\u5f00\u653e\u6743\u91cd\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6700\u7ec8\u54cd\u5e94\u524d\u751f\u6210\u957f\u94fe\u5f0f\u601d\u7ef4\uff08CoTs\uff09\uff0c\u8fd9\u63d0\u9ad8\u4e86\u6027\u80fd\u4f46\u5f15\u5165\u4e86\u989d\u5916\u7684\u5bf9\u9f50\u98ce\u9669\uff0c\u6709\u5bb3\u5185\u5bb9\u901a\u5e38\u51fa\u73b0\u5728CoTs\u548c\u6700\u7ec8\u8f93\u51fa\u4e2d\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528CoTs\u9884\u6d4b\u6700\u7ec8\u54cd\u5e94\u7684\u4e0d\u5bf9\u9f50\u60c5\u51b5\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u76d1\u63a7\u65b9\u6cd5\uff0c\u5305\u62ec\u4eba\u7c7b\u3001\u9ad8\u80fd\u529b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u5206\u7c7b\u5668\uff0c\u4f7f\u7528CoT\u6587\u672c\u6216\u6fc0\u6d3b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u57fa\u4e8eCoT\u6fc0\u6d3b\u7684\u7b80\u5355\u7ebf\u6027\u63a2\u6d4b\u5668\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u3002CoT\u6587\u672c\u53ef\u80fd\u4e0d\u53ef\u9760\u5e76\u8bef\u5bfc\u4eba\u7c7b\u548c\u5206\u7c7b\u5668\uff0c\u800c\u6a21\u578b\u6f5c\u53d8\u91cf\uff08\u5373CoT\u6fc0\u6d3b\uff09\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u4fe1\u53f7\u3002\u63a2\u6d4b\u5668\u5728\u63a8\u7406\u5b8c\u6210\u524d\u5c31\u80fd\u51c6\u786e\u9884\u6d4b\uff0c\u5373\u4f7f\u5e94\u7528\u4e8e\u65e9\u671fCoT\u6bb5\u843d\u4e5f\u80fd\u53d6\u5f97\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8eCoT\u6fc0\u6d3b\u7684\u7b80\u5355\u7ebf\u6027\u63a2\u6d4b\u5668\u5728\u9884\u6d4b\u6700\u7ec8\u54cd\u5e94\u662f\u5426\u5b89\u5168\u6216\u4e0d\u5b89\u5168\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u53d1\u73b0\u5728\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u3001\u5bb6\u65cf\u548c\u5b89\u5168\u57fa\u51c6\u4e2d\u5177\u6709\u666e\u904d\u6027\uff0c\u8868\u660e\u8f7b\u91cf\u7ea7\u63a2\u6d4b\u5668\u53ef\u4ee5\u5b9e\u73b0\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5b9e\u65f6\u5b89\u5168\u76d1\u63a7\u548c\u65e9\u671f\u5e72\u9884\u3002"}}
{"id": "2507.12451", "pdf": "https://arxiv.org/pdf/2507.12451", "abs": "https://arxiv.org/abs/2507.12451", "authors": ["Suman Adhya", "Debarshi Kumar Sanyal"], "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted as a long paper for ACL 2025 main conference", "summary": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5S2WTM\uff0c\u901a\u8fc7\u4f7f\u7528\u7403\u9762\u5207\u7247Wasserstein\u8ddd\u79bb\u6765\u89e3\u51b3\u540e\u9a8c\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "VAE-NTMs\u7ecf\u5e38\u906d\u53d7\u540e\u9a8c\u5d29\u6e83\uff0c\u8fd9\u5bfc\u81f4KL\u6563\u5ea6\u9879\u5728\u76ee\u6807\u51fd\u6570\u4e2d\u5927\u5e45\u51cf\u5c11\uff0c\u4ece\u800c\u4ea7\u751f\u65e0\u6548\u7684\u6f5c\u5728\u8868\u793a\u3002", "method": "S2WTM\u91c7\u7528\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u5e76\u5229\u7528\u7403\u9762\u5207\u7247Wasserstein\u8ddd\u79bb\u5c06\u805a\u5408\u540e\u9a8c\u5206\u5e03\u4e0e\u5148\u9a8c\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cS2WTM\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e3b\u9898\u6a21\u578b\u3002", "conclusion": "S2WTM\u5728\u751f\u6210\u66f4\u8fde\u8d2f\u548c\u591a\u6837\u7684\u4e3b\u9898\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12466", "pdf": "https://arxiv.org/pdf/2507.12466", "abs": "https://arxiv.org/abs/2507.12466", "authors": ["David Mizrahi", "Anders Boesen Lindbo Larsen", "Jesse Allardice", "Suzie Petryk", "Yuri Gorokhov", "Jeffrey Li", "Alex Fang", "Josh Gardner", "Tom Gunter", "Afshin Dehghan"], "title": "Language Models Improve When Pretraining Data Matches Target Tasks", "categories": ["cs.CL", "cs.LG"], "comment": "44 pages, 25 figures, 13 tables", "summary": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBETR\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u8bc4\u4f30\u57fa\u51c6\u5bf9\u9f50\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cBETR\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\uff0c\u9700\u8981\u7684\u8fc7\u6ee4\u7a0b\u5ea6\u964d\u4f4e\u3002", "motivation": "\u5728\u5b9e\u8df5\u4e2d\uff0c\u8fd9\u4e9b\u76ee\u6807\u901a\u5e38\u901a\u8fc7\u4ee5\u57fa\u51c6\u4e3a\u5bfc\u5411\u7684\u8fed\u4ee3\u9690\u5f0f\u51fa\u73b0\uff1a\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u9009\u62e9\u7b56\u7565\uff0c\u8bad\u7ec3\u6a21\u578b\uff0c\u6d4b\u91cf\u57fa\u51c6\u6027\u80fd\uff0c\u7136\u540e\u76f8\u5e94\u5730\u8fdb\u884c\u6539\u8fdb\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u81ea\u7136\u7684\u95ee\u9898\uff1a\u5f53\u6211\u4eec\u663e\u5f0f\u4f18\u5316\u65f6\u4f1a\u53d1\u751f\u4ec0\u4e48\uff1f", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u51c6\u76ee\u6807\u6392\u5e8f\uff08BETR\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u6839\u636e\u4e0e\u57fa\u51c6\u8bad\u7ec3\u793a\u4f8b\u7684\u76f8\u4f3c\u6027\u9009\u62e9\u9884\u8bad\u7ec3\u6587\u6863\u3002BETR\u5c06\u57fa\u51c6\u793a\u4f8b\u548c\u9884\u8bad\u7ec3\u6587\u6863\u7684\u6837\u672c\u5d4c\u5165\u5230\u4e00\u4e2a\u5171\u4eab\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u4e0e\u57fa\u51c6\u7684\u76f8\u4f3c\u6027\u5bf9\u8fd9\u4e2a\u6837\u672c\u8fdb\u884c\u8bc4\u5206\uff0c\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u6765\u9884\u6d4b\u6574\u4e2a\u8bed\u6599\u5e93\u7684\u8fd9\u4e9b\u5206\u6570\u3002", "result": "\u901a\u8fc7\u8bad\u7ec3\u8d85\u8fc7500\u4e2a\u6a21\u578b\uff0c\u8986\u76d610^19\u523010^22 FLOPs\uff0c\u5e76\u62df\u5408\u7f29\u653e\u5b9a\u5f8b\uff0c\u6211\u4eec\u6bd4\u8f83\u4e86\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528BETR\u5c06\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u8bc4\u4f30\u57fa\u51c6\u5bf9\u9f50\u53ef\u4ee5\u5b9e\u73b04.7\u500d\u4e8e\u672a\u8fc7\u6ee4\u6570\u636e\u7684\u8ba1\u7b97\u4e58\u6570\uff0c\u5e76\u5728\u6240\u6709\u89c4\u6a21\u768410\u4e2a\u4efb\u52a1\u4e2d\u76849\u4e2a\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd\u3002BETR\u8fd8\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff1a\u5f53\u9488\u5bf9\u4e0e\u6211\u4eec\u7684\u8bc4\u4f30\u5957\u4ef6\u4e0d\u76f8\u5173\u7684\u591a\u6837\u5316\u57fa\u51c6\u65f6\uff0c\u5b83\u4ecd\u7136\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u7ebf\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u76f4\u63a5\u5c06\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u76ee\u6807\u4efb\u52a1\u5339\u914d\u53ef\u4ee5\u7cbe\u786e\u5730\u5851\u9020\u6a21\u578b\u80fd\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u6700\u4f73\u9009\u62e9\u7b56\u7565\u5fc5\u987b\u9002\u5e94\u6a21\u578b\u89c4\u6a21\u3002"}}
{"id": "2507.11548", "pdf": "https://arxiv.org/pdf/2507.11548", "abs": "https://arxiv.org/abs/2507.11548", "authors": ["Kevin T Webster"], "title": "Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening", "categories": ["cs.CY", "cs.AI", "cs.CL", "I.2.1; K.4.2; I.2.6; K.4.1"], "comment": "58 pages, 4 figures", "summary": "The increasing use of generative AI for resume screening is predicated on the\nassumption that it offers an unbiased alternative to biased human\ndecision-making. However, this belief fails to address a critical question: are\nthese AI systems fundamentally competent at the evaluative tasks they are meant\nto perform? This study investigates the question of competence through a\ntwo-part audit of eight major AI platforms. Experiment 1 confirmed complex,\ncontextual racial and gender biases, with some models penalizing candidates\nmerely for the presence of demographic signals. Experiment 2, which evaluated\ncore competence, provided a critical insight: some models that appeared\nunbiased were, in fact, incapable of performing a substantive evaluation,\nrelying instead on superficial keyword matching. This paper introduces the\n\"Illusion of Neutrality\" to describe this phenomenon, where an apparent lack of\nbias is merely a symptom of a model's inability to make meaningful judgments.\nThis study recommends that organizations and regulators adopt a dual-validation\nframework, auditing AI hiring tools for both demographic bias and demonstrable\ncompetence to ensure they are both equitable and effective.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0fAI\u5728\u7b80\u5386\u7b5b\u9009\u4e2d\u7684\u80fd\u529b\u95ee\u9898\uff0c\u53d1\u73b0\u5176\u53ef\u80fd\u5b58\u5728\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\u4ee5\u53ca\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u53cc\u91cd\u9a8c\u8bc1\u6846\u67b6\u4ee5\u786e\u4fddAI\u62db\u8058\u5de5\u5177\u7684\u516c\u5e73\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u7b80\u5386\u7b5b\u9009\u4e2d\u7684\u80fd\u529b\u95ee\u9898\uff0c\u8d28\u7591\u5176\u662f\u5426\u80fd\u591f\u80dc\u4efb\u8bc4\u4f30\u4efb\u52a1\uff0c\u5e76\u63ed\u793a\u5176\u53ef\u80fd\u5b58\u5728\u7684'\u4e2d\u7acb\u6027\u5e7b\u89c9'\u73b0\u8c61\u3002", "method": "\u672c\u6587\u901a\u8fc7\u4e24\u4e2a\u90e8\u5206\u7684\u5ba1\u8ba1\u68c0\u67e5\u4e86\u516b\u4e2a\u4e3b\u8981AI\u5e73\u53f0\uff0c\u5b9e\u9a8c1\u786e\u8ba4\u4e86\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\uff0c\u5b9e\u9a8c2\u8bc4\u4f30\u4e86\u6838\u5fc3\u80fd\u529b\uff0c\u53d1\u73b0\u4e00\u4e9b\u770b\u4f3c\u65e0\u504f\u7684\u6a21\u578b\u5b9e\u9645\u4e0a\u65e0\u6cd5\u8fdb\u884c\u5b9e\u8d28\u6027\u8bc4\u4f30\uff0c\u800c\u662f\u4f9d\u8d56\u4e8e\u8868\u9762\u7684\u5173\u952e\u8bcd\u5339\u914d\u3002", "result": "\u5b9e\u9a8c1\u53d1\u73b0\u4e86\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\uff0c\u5b9e\u9a8c2\u53d1\u73b0\u4e00\u4e9b\u6a21\u578b\u867d\u7136\u770b\u4f3c\u65e0\u504f\uff0c\u4f46\u5b9e\u9645\u4e0a\u65e0\u6cd5\u8fdb\u884c\u5b9e\u8d28\u6027\u8bc4\u4f30\uff0c\u800c\u662f\u4f9d\u8d56\u4e8e\u8868\u9762\u7684\u5173\u952e\u8bcd\u5339\u914d\u3002", "conclusion": "\u672c\u6587\u5efa\u8bae\u7ec4\u7ec7\u548c\u76d1\u7ba1\u673a\u6784\u91c7\u7528\u53cc\u91cd\u9a8c\u8bc1\u6846\u67b6\uff0c\u5bf9AI\u62db\u8058\u5de5\u5177\u8fdb\u884c\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u548c\u53ef\u8bc1\u660e\u80fd\u529b\u7684\u5ba1\u8ba1\uff0c\u4ee5\u786e\u4fdd\u5b83\u4eec\u65e2\u516c\u5e73\u53c8\u6709\u6548\u3002"}}
{"id": "2507.11630", "pdf": "https://arxiv.org/pdf/2507.11630", "abs": "https://arxiv.org/abs/2507.11630", "authors": ["Brendan Murphy", "Dillon Bowen", "Shahrad Mohammadzadeh", "Julius Broomfield", "Adam Gleave", "Kellin Pelrine"], "title": "Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "AI systems are rapidly advancing in capability, and frontier model developers\nbroadly acknowledge the need for safeguards against serious misuse. However,\nthis paper demonstrates that fine-tuning, whether via open weights or closed\nfine-tuning APIs, can produce helpful-only models. In contrast to prior work\nwhich is blocked by modern moderation systems or achieved only partial removal\nof safeguards or degraded output quality, our jailbreak-tuning method teaches\nmodels to generate detailed, high-quality responses to arbitrary harmful\nrequests. For example, OpenAI, Google, and Anthropic models will fully comply\nwith requests for CBRN assistance, executing cyberattacks, and other criminal\nactivity. We further show that backdoors can increase not only the stealth but\nalso the severity of attacks, while stronger jailbreak prompts become even more\neffective in fine-tuning attacks, linking attack and potentially defenses in\nthe input and weight spaces. Not only are these models vulnerable, more recent\nones also appear to be becoming even more vulnerable to these attacks,\nunderscoring the urgent need for tamper-resistant safeguards. Until such\nsafeguards are discovered, companies and policymakers should view the release\nof any fine-tunable model as simultaneously releasing its evil twin: equally\ncapable as the original model, and usable for any malicious purpose within its\ncapabilities.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u5f53\u524dAI\u7cfb\u7edf\u5728\u5b89\u5168\u9632\u62a4\u65b9\u9762\u7684\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002\u8bba\u6587\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u9632\u62a4\u63aa\u65bd\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u63ed\u793a\u5f53\u524dAI\u7cfb\u7edf\u5728\u5b89\u5168\u9632\u62a4\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u9632\u62a4\u63aa\u65bd\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ajailbreak-tuning\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u8be6\u7ec6\u3001\u9ad8\u8d28\u91cf\u7684\u54cd\u5e94\uff0c\u4ee5\u5e94\u5bf9\u4efb\u610f\u6709\u5bb3\u8bf7\u6c42\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u5c55\u793a\u4e86\u540e\u95e8\u53ef\u4ee5\u589e\u52a0\u653b\u51fb\u7684\u9690\u853d\u6027\u548c\u4e25\u91cd\u6027\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u4f8b\u5982OpenAI\u3001Google\u548cAnthropic\u6a21\u578b\u4f1a\u5b8c\u5168\u9075\u5b88\u8bf7\u6c42\uff0c\u63d0\u4f9bCBRN\u63f4\u52a9\u3001\u6267\u884c\u7f51\u7edc\u653b\u51fb\u7b49\u72af\u7f6a\u6d3b\u52a8\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u53d1\u73b0\uff0c\u66f4\u5f3a\u7684jailbreak\u63d0\u793a\u5728\u5fae\u8c03\u653b\u51fb\u4e2d\u66f4\u52a0\u6709\u6548\uff0c\u8fd9\u8868\u660e\u653b\u51fb\u548c\u9632\u5fa1\u53ef\u80fd\u5728\u8f93\u5165\u548c\u6743\u91cd\u7a7a\u95f4\u4e2d\u76f8\u4e92\u5173\u8054\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\uff0c\u76ee\u524d\u7684AI\u7cfb\u7edf\u5728\u5b89\u5168\u9632\u62a4\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u9632\u62a4\u63aa\u65bd\u3002\u5728\u8fd9\u4e9b\u9632\u62a4\u63aa\u65bd\u88ab\u53d1\u73b0\u4e4b\u524d\uff0c\u516c\u53f8\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5e94\u5c06\u4efb\u4f55\u53ef\u5fae\u8c03\u6a21\u578b\u7684\u53d1\u5e03\u89c6\u4e3a\u540c\u65f6\u53d1\u5e03\u4e86\u5176\u90aa\u6076\u53cc\u80de\u80ce\uff1a\u540c\u6837\u6709\u80fd\u529b\uff0c\u4e14\u53ef\u7528\u4e8e\u4efb\u4f55\u6076\u610f\u76ee\u7684\u3002"}}
{"id": "2507.11662", "pdf": "https://arxiv.org/pdf/2507.11662", "abs": "https://arxiv.org/abs/2507.11662", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%.", "AI": {"tldr": "This paper explores the use of Multimodal Large Language Models (MLLMs) as verifiers for agent behavior in domains without clear success criteria, identifies a critical limitation called agreement bias, and proposes Self-Grounded Verification (SGV) to improve their performance.", "motivation": "The challenge of extending AI progress to domains without clear-cut success criteria, such as computer use, remains difficult due to the difficulty of translating human intuition into scalable rules. MLLMs show promise as verifiers but face limitations like agreement bias.", "method": "The paper evaluates MLLMs as verifiers of agent trajectories across web navigation, computer use, and robotic manipulation, identifies agreement bias as a critical limitation, and proposes Self-Grounded Verification (SGV) to address it.", "result": "Enhanced with SGV, MLLM verifiers show gains of up to 20 points in accuracy and failure detection rates, and can perform real-time supervision of heterogeneous agents, boosting task completion in various environments and setting a new state of the art on benchmarks.", "conclusion": "MLLM verifiers can be enhanced with Self-Grounded Verification (SGV), which improves their accuracy and failure detection rates, enabling real-time supervision of heterogeneous agents and setting a new state of the art on benchmarks."}}
{"id": "2507.11687", "pdf": "https://arxiv.org/pdf/2507.11687", "abs": "https://arxiv.org/abs/2507.11687", "authors": ["Atharva Naik", "Lawanya Baghel", "Dhakshin Govindarajan", "Darsh Agrawal", "Daniel Fried", "Carolyn Rose"], "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models, though successful in code generation, struggle with\ncode quality analysis because they are limited by static training data and\ncan't easily adapt to evolving best practices. We introduce MetaLint, a new\ninstruction-following framework that formulates code quality analysis as the\ntask of detecting and fixing problematic semantic code fragments or code idioms\nbased on high-level specifications. Unlike conventional approaches that train\nmodels on static, rule-based data, MetaLint employs instruction tuning on\nsynthetic linter-generated data to support easy-to-hard generalization,\nenabling models to adapt to novel or complex code patterns without retraining.\nTo evaluate this, we construct a benchmark of challenging idioms inspired by\nreal-world coding standards such as Python Enhancement Proposals (PEPs) and\nassess whether MetaLint-trained models reason adaptively or simply memorize.\nOur results show that MetaLint improves generalization to unseen PEP idioms,\nachieving a 70.37% F-score on idiom detection with the highest recall (70.43%)\namong all evaluated models. It also achieves 26.73% on localization,\ncompetitive for its 4B parameter size and comparable to larger state-of-the-art\nmodels like o3-mini, highlighting its potential for future-proof code quality\nanalysis.", "AI": {"tldr": "MetaLint is a new framework for code quality analysis that improves generalization to unseen code patterns by using instruction tuning on synthetic data, showing promising results in detecting and fixing problematic code fragments.", "motivation": "Large Language Models struggle with code quality analysis due to static training data and inability to adapt to evolving best practices. There is a need for a framework that can adapt to novel or complex code patterns without retraining.", "method": "MetaLint is an instruction-following framework that formulates code quality analysis as detecting and fixing problematic semantic code fragments based on high-level specifications. It uses instruction tuning on synthetic linter-generated data for easy-to-hard generalization.", "result": "MetaLint improves generalization to unseen PEP idioms, achieving a 70.37% F-score on idiom detection with the highest recall (70.43%) among all evaluated models. It also achieves 26.73% on localization, competitive for its 4B parameter size and comparable to larger state-of-the-art models.", "conclusion": "MetaLint shows potential for future-proof code quality analysis by improving generalization to unseen PEP idioms and achieving competitive results with smaller models."}}
{"id": "2507.11788", "pdf": "https://arxiv.org/pdf/2507.11788", "abs": "https://arxiv.org/abs/2507.11788", "authors": ["Daniel Mitropolsky", "Christos Papadimitriou"], "title": "Simulated Language Acquisition in a Biologically Realistic Model of the Brain", "categories": ["cs.NE", "cs.CL"], "comment": "13 pages, 6 figures", "summary": "Despite tremendous progress in neuroscience, we do not have a compelling\nnarrative for the precise way whereby the spiking of neurons in our brain\nresults in high-level cognitive phenomena such as planning and language. We\nintroduce a simple mathematical formulation of six basic and broadly accepted\nprinciples of neuroscience: excitatory neurons, brain areas, random synapses,\nHebbian plasticity, local inhibition, and inter-area inhibition. We implement a\nsimulated neuromorphic system based on this formalism, which is capable of\nbasic language acquisition: Starting from a tabula rasa, the system learns, in\nany language, the semantics of words, their syntactic role (verb versus noun),\nand the word order of the language, including the ability to generate novel\nsentences, through the exposure to a modest number of grounded sentences in the\nsame language. We discuss several possible extensions and implications of this\nresult.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u79d1\u5b66\u539f\u5219\u7684\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u8bed\u8a00\u4e60\u5f97\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u79d1\u5b66\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\uff0c\u4f46\u6211\u4eec\u4ecd\u7136\u7f3a\u4e4f\u4e00\u4e2a\u660e\u786e\u7684\u53d9\u8ff0\u6765\u89e3\u91ca\u795e\u7ecf\u5143\u7684\u653e\u7535\u5982\u4f55\u5bfc\u81f4\u9ad8\u7ea7\u8ba4\u77e5\u73b0\u8c61\uff0c\u5982\u8ba1\u5212\u548c\u8bed\u8a00\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u63cf\u8ff0\u4e86\u516d\u4e2a\u57fa\u672c\u4e14\u5e7f\u6cdb\u63a5\u53d7\u7684\u795e\u7ecf\u79d1\u5b66\u539f\u5219\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6a21\u62df\u7684\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u4efb\u4f55\u8bed\u8a00\u7684\u8bcd\u6c47\u8bed\u4e49\u3001\u8bed\u6cd5\u89d2\u8272\uff08\u52a8\u8bcd\u4e0e\u540d\u8bcd\uff09\u4ee5\u53ca\u8bcd\u5e8f\uff0c\u5305\u62ec\u751f\u6210\u65b0\u53e5\u5b50\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u516d\u4e2a\u57fa\u672c\u795e\u7ecf\u79d1\u5b66\u539f\u5219\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u8bed\u8a00\u4e60\u5f97\u65b9\u9762\u7684\u5e94\u7528\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u53ef\u80fd\u7684\u6269\u5c55\u548c\u5f71\u54cd\u3002"}}
{"id": "2507.12142", "pdf": "https://arxiv.org/pdf/2507.12142", "abs": "https://arxiv.org/abs/2507.12142", "authors": ["Vladimir Bogachev", "Vladimir Aletov", "Alexander Molozhavenko", "Denis Bobkov", "Vera Soboleva", "Aibek Alanov", "Maxim Rakhuba"], "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization", "categories": ["cs.LG", "cs.CL", "cs.NA", "math.DG", "math.NA", "68T07, 65F55, 53Z50"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0cRiemannLoRA\uff0c\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u540c\u65f6\u89e3\u51b3\u4e86\u4f4e\u79e9\u9002\u5e94\u4e2d\u7684\u521d\u59cb\u5316\u7b56\u7565\u548c\u8fc7\u53c2\u6570\u5316\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5df2\u6210\u4e3a\u9ad8\u6548\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e7f\u6cdb\u6807\u51c6\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u6311\u6218\uff0c\u5305\u62ec\u5bfb\u627e\u6700\u4f73\u521d\u59cb\u5316\u7b56\u7565\u6216\u51cf\u8f7b\u4f4e\u79e9\u77e9\u9635\u5206\u89e3\u4e2d\u7684\u8fc7\u53c2\u6570\u5316\u95ee\u9898\u3002", "method": "\u6211\u4eec\u5c06\u4e00\u7ec4\u56fa\u5b9a\u79e9\u7684LoRA\u77e9\u9635\u89c6\u4e3a\u4e00\u4e2a\u5e73\u6ed1\u6d41\u5f62\uff0c\u5e76\u5c06\u9002\u914d\u5668\u89c6\u4e3a\u8be5\u6d41\u5f62\u4e0a\u7684\u5143\u7d20\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u8fc7\u53c2\u6570\u5316\u95ee\u9898\u3002\u540c\u65f6\uff0c\u6cbf\u7740\u6d41\u5f62\u786e\u5b9a\u6700\u5feb\u635f\u5931\u51cf\u5c11\u7684\u65b9\u5411\u63d0\u4f9b\u4e86\u521d\u59cb\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRiemannLoRA\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u6807\u51c6LoRA\u53ca\u5176\u6700\u5148\u8fdb\u7684\u4fee\u6539\u3002", "conclusion": "RiemannLoRA\u5728LLM\u548c\u6269\u6563\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u65b9\u9762\u90fd\u4f18\u4e8e\u6807\u51c6LoRA\u53ca\u5176\u6700\u5148\u8fdb\u7684\u4fee\u6539\u3002"}}
{"id": "2507.12175", "pdf": "https://arxiv.org/pdf/2507.12175", "abs": "https://arxiv.org/abs/2507.12175", "authors": ["Sungkyun Chang", "Simon Dixon", "Emmanouil Benetos"], "title": "RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": "Accepted to WASPAA 2025", "summary": "This study introduces RUMAA, a transformer-based framework for music\nperformance analysis that unifies score-to-performance alignment,\nscore-informed transcription, and mistake detection in a near end-to-end\nmanner. Unlike prior methods addressing these tasks separately, RUMAA\nintegrates them using pre-trained score and audio encoders and a novel\ntri-stream decoder capturing task interdependencies through proxy tasks. It\naligns human-readable MusicXML scores with repeat symbols to full-length\nperformance audio, overcoming traditional MIDI-based methods that rely on\nmanually unfolded score-MIDI data with pre-specified repeat structures. RUMAA\nmatches state-of-the-art alignment methods on non-repeated scores and\noutperforms them on scores with repeats in a public piano music dataset, while\nalso delivering promising transcription and mistake detection results.", "AI": {"tldr": "RUMAA\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u97f3\u4e50\u8868\u6f14\u5206\u6790\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u4e50\u8c31\u5230\u8868\u6f14\u5bf9\u9f50\u3001\u57fa\u4e8e\u4e50\u8c31\u7684\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\uff0c\u5e76\u5728\u5177\u6709\u91cd\u590d\u7684\u4e50\u8c31\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u52a8\u5c55\u5f00\u7684\u4e50\u8c31-MIDI\u6570\u636e\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u7684\u91cd\u590d\u7ed3\u6784\uff0c\u800cRUMAA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RUMAA\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u4e50\u8c31\u548c\u97f3\u9891\u7f16\u7801\u5668\u4ee5\u53ca\u4e00\u79cd\u65b0\u7684\u4e09\u6d41\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u4ee3\u7406\u4efb\u52a1\u6355\u6349\u4efb\u52a1\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u7edf\u4e00\u7684\u65b9\u5f0f\u5904\u7406\u4e50\u8c31\u5230\u8868\u6f14\u5bf9\u9f50\u3001\u57fa\u4e8e\u4e50\u8c31\u7684\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\u3002", "result": "RUMAA\u5728\u975e\u91cd\u590d\u4e50\u8c31\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u5bf9\u9f50\u65b9\u6cd5\u6c34\u5e73\uff0c\u5e76\u5728\u5305\u542b\u91cd\u590d\u7684\u516c\u5171\u94a2\u7434\u97f3\u4e50\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u5728\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002", "conclusion": "RUMAA\u5728\u5177\u6709\u91cd\u590d\u7684\u4e50\u8c31\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.12284", "pdf": "https://arxiv.org/pdf/2507.12284", "abs": "https://arxiv.org/abs/2507.12284", "authors": ["Artem Chervyakov", "Alexander Kharitonov", "Pavel Zadorozhny", "Adamenko Pavel", "Rodion Levichev", "Dmitrii Vorobev", "Dmitrii Salikhov", "Aidar Valeev", "Alena Pestova", "Maria Dziuba", "Ilseyar Alimova", "Artem Zavgorodnev", "Aleksandr Medvedev", "Stanislav Moiseev", "Elena Bruches", "Daniil Grebenkin", "Roman Derunets", "Vikulov Vladimir", "Anton Emelyanov", "Dmitrii Babaev", "Vladimir V. Ivanov", "Valentin Malykh", "Alena Fenogenova"], "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.", "AI": {"tldr": "MERA Code is a new benchmark for evaluating code generation LLMs in Russian, focusing on practical coding skills and addressing gaps in current evaluations.", "motivation": "Current evaluations of LLMs primarily focus on natural language tasks, overlooking code quality. Most benchmarks prioritize high-level reasoning over executable code and real-world performance, leaving gaps in understanding true capabilities and risks associated with these models in production.", "method": "MERA Code is a new addition to the MERA benchmark family, specifically focused on evaluating code for the latest code generation LLMs in Russian. It includes 11 evaluation tasks that span 8 programming languages and features a taxonomy outlining practical coding skills necessary for models to complete these tasks.", "result": "MERA Code evaluates open LLMs and frontier API models, analyzing their limitations in terms of practical coding tasks in non-English languages.", "conclusion": "MERA Code is publicly released to guide future research, anticipate groundbreaking features in model development, and standardize evaluation procedures."}}
{"id": "2507.12341", "pdf": "https://arxiv.org/pdf/2507.12341", "abs": "https://arxiv.org/abs/2507.12341", "authors": ["Antoine Saillenfest", "Pirmin Lemberger"], "title": "Nonlinear Concept Erasure: a Density Matching Approach", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)", "summary": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aL\u0304EOPARD\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u5ff5\u64e6\u9664\u6765\u53bb\u9664\u795e\u7ecf\u6a21\u578b\u4e2d\u7684\u654f\u611f\u4fe1\u606f\uff0c\u4ee5\u63d0\u9ad8\u516c\u5e73\u6027\u3002", "motivation": "\u786e\u4fdd\u795e\u7ecf\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u4e0d\u80fd\u4ece\u6587\u672c\u8868\u793a\u4e2d\u63a8\u65ad\u51fa\u654f\u611f\u4fe1\u606f\uff08\u5982\u6027\u522b\u6216\u79cd\u65cf\u7b49\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\uff09\u662f\u5f53\u516c\u5e73\u6027\u662f\u4e00\u4e2a\u5173\u6ce8\u70b9\u65f6\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u64e6\u9664\uff0c\u5b66\u4e60\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u6b63\u4ea4\u6295\u5f71\uff0c\u4f7f\u79bb\u6563\u6982\u5ff5\u7684\u7c7b\u522b\u6761\u4ef6\u7279\u5f81\u5206\u5e03\u5728\u6295\u5f71\u540e\u4e0d\u53ef\u533a\u5206\u3002\u8c03\u6574\u6295\u5f71\u5668\u7684\u79e9\u63a7\u5236\u4fe1\u606f\u5220\u9664\u7684\u7a0b\u5ea6\uff0c\u540c\u65f6\u5176\u6b63\u4ea4\u6027\u786e\u4fdd\u5d4c\u5165\u7684\u5c40\u90e8\u7ed3\u6784\u4e25\u683c\u4fdd\u7559\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3aL\u0304EOPARD\uff0c\u5728\u7ecf\u5178\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u975e\u7ebf\u6027\u64e6\u9664\u79bb\u6563\u5c5e\u6027\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660eL\u0304EOPARD\u6709\u6548\u51cf\u8f7b\u4e86\u6df1\u5ea6\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u4e2d\u7684\u504f\u5dee\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u516c\u5e73\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u64e6\u9664\u79bb\u6563\u5c5e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u6709\u6548\u51cf\u8f7b\u4e86\u6df1\u5ea6\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u4e2d\u7684\u504f\u5dee\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u516c\u5e73\u6027\u3002"}}
{"id": "2507.12378", "pdf": "https://arxiv.org/pdf/2507.12378", "abs": "https://arxiv.org/abs/2507.12378", "authors": ["Rachna Saxena", "Abhijeet Kumar", "Suresh Shanmugam"], "title": "Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker", "categories": ["cs.IR", "cs.CL"], "comment": "Presented at NLP@IR workshop at SIGIR conference", "summary": "Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u4f7f\u89c6\u89c9\u68c0\u7d22\u8fc7\u7a0b\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u6027\u80fd\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u8003\u8651\u56fe\u8868\u7b49\u89c6\u89c9\u5143\u7d20\u3002\u591a\u6a21\u6001LLM\u9762\u4e34\u5bfb\u627e\u9488\u5728\u5e72\u8349\u5806\u7684\u95ee\u9898\uff0c\u5373\u8f83\u957f\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u6216\u5927\u91cf\u7684\u6587\u6863\u4f5c\u4e3a\u641c\u7d22\u7a7a\u95f4\u3002\u665a\u671f\u4ea4\u4e92\u673a\u5236\u5728\u57fa\u4e8e\u68c0\u7d22\u7684\u89c6\u89c9\u589e\u5f3a\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c06\u5176\u7528\u4e8eRAG-based\u591a\u6a21\u6001\u95ee\u7b54\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6b65\u9aa4\u7684\u81ea\u5b9a\u4e49\u5b9e\u73b0\uff0c\u5229\u7528\u5e7f\u6cdb\u91c7\u7528\u7684\u6df7\u5408\u641c\u7d22\uff08\u5143\u6570\u636e\u548c\u5d4c\u5165\uff09\u548c\u6700\u5148\u8fdb\u7684\u665a\u671f\u4ea4\u4e92\u91cd\u65b0\u6392\u5e8f\u5668\u6765\u68c0\u7d22\u6700\u4f73\u5339\u914d\u9875\u9762\u3002\u6700\u540e\uff0cMLLM\u88ab\u63d0\u793a\u4e3a\u8bfb\u8005\uff0c\u4ece\u4e0a\u4e0b\u6587\u5316\u7684\u6700\u4f73\u5339\u914d\u9875\u9762\u751f\u6210\u7b54\u6848\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u6240\u63d0\u51fa\u7684\u65b9\u6848\u662f\u53ef\u6269\u5c55\u7684\uff08\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\uff09\u4e14\u7a33\u5b9a\u7684\uff08\u4e0d\u4f1a\u964d\u4f4e\u6027\u80fd\u8d28\u91cf\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6848\u5728\u4fdd\u6301\u6027\u80fd\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u56e0\u6b64\u53ef\u4ee5\u7528\u4e8e\u4f01\u4e1a\u751f\u4ea7\u7cfb\u7edf\u3002"}}

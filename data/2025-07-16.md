<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: 本文提出了一种基于AI的系统，用于对抗YouTube上的虚假信息。系统包括两个代理：Truth Sleuth负责事实核查，Trend Bender生成有说服力的评论以促进讨论。实验结果表明系统具有高准确性，并展示了其在对抗虚假信息方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 虚假信息在当今数字世界中构成重大威胁，尤其是在YouTube等平台上迅速传播。本文旨在开发一种新的方法来对抗虚假信息，通过AI系统主动参与用户评论并挑战误导性叙述。

Method: 本文开发了一个基于AI的系统，包括两个主要代理：Truth Sleuth和Trend Bender。Truth Sleuth通过检索增强生成方法对YouTube视频中的声明进行事实核查，并生成详细的报告。Trend Bender利用该报告和相关文章生成有说服力的评论，以激发有益的讨论。

Result: 实验结果表明，本文的系统能够准确地进行事实核查，并在YouTube上成功部署，展示了其影响用户观点的潜力。

Conclusion: 本文展示了系统在基准数据集和YouTube上的实验结果，证明了其在对抗虚假信息和促进更知情的在线空间方面的潜力。

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [2] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: EmoSApp是一个基于智能手机的离线对话应用程序，利用经过微调和量化的大型语言模型，为用户提供心理健康和情感支持。


<details>
  <summary>Details</summary>
Motivation: 数字平台在扩展心理健康和情感支持方面越来越重要，但存在用户可及性有限、互联网连接和数据隐私等问题，这凸显了需要一种离线的基于智能手机的解决方案。

Method: 我们提出了EmoSApp：一个完全离线的基于智能手机的对话应用程序，利用大型语言模型（LLMs），特别是经过微调、量化并在Torchtune和Executorch上部署的模型，以在资源受限的设备上进行所有推理。

Result: 通过与学生群体的定性人类评估，我们展示了EmoSApp能够以连贯、共情的方式回应，并保持互动对话，为用户的心理健康问题提供相关建议。此外，在九个标准常识和推理基准上的定量评估证明了我们的微调、量化模型在低资源环境中的有效性。

Conclusion: 通过优先考虑设备端部署和专业领域适应，EmoSApp为未来便携、安全且高度定制的AI驱动心理健康解决方案提供了蓝图。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [3] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: 本文介绍了一个模块化的工具链，用于准备非结构化文本数据进行基于嵌入的分析，完全依赖于本地硬件上的开源模型，支持隐私敏感研究。


<details>
  <summary>Details</summary>
Motivation: 从法律、医疗和行政来源获得的非结构化文本为公共卫生和社会科学的研究提供了丰富的但未充分利用的资源。然而，大规模分析受到两个关键挑战的阻碍：存在敏感的个人身份信息，以及结构和语言的显著异质性。

Method: 工具链使用大型语言模型（LLM）提示来标准化、总结和在需要时将文本翻译成英语以提高可比性。通过基于LLM的删除，补充命名实体识别和基于规则的方法来最小化泄露风险。

Result: 工具链有效地消除了识别信息，同时保留了语义内容。通过使用从少量手动标记的摘要中得出的嵌入向量训练预测模型，展示了工具链在大规模半自动化内容分析方面的潜力。

Conclusion: 通过启用对敏感文件的结构化、隐私意识分析，我们的工具链为由于隐私和异质性限制而以前无法访问文本数据的领域的大规模研究打开了新的可能性。

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [4] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: 本文提出了一种针对基于提示的自然语言解释的XAI分类法，以帮助研究人员、审计人员和政策制定者更好地理解和改进AI系统的透明度。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，自然语言解释（NLEs）现在是阐述模型行为的关键，这需要对其特征和治理影响进行重点检查。

Method: 本文借鉴了可解释AI（XAI）文献，创建了一个更新的XAI分类法，适用于基于提示的NLEs，涵盖了三个维度：(1) 上下文，包括任务、数据、受众和目标；(2) 生成和展示，涵盖生成方法、输入、交互性、输出和形式；(3) 评估，关注内容、展示和用户中心属性，以及评估的环境。

Result: 该分类法为研究人员、审计人员和政策制定者提供了一个框架，用于描述、设计和改进透明AI系统的NLEs。

Conclusion: 本文提出了一个针对基于提示的自然语言解释（NLEs）的XAI分类法，为研究人员、审计人员和政策制定者提供了一个框架，以描述、设计和改进透明AI系统的NLEs。

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [5] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: AutoRAG-LoRA 是一种用于减少大型语言模型幻觉的模块化框架，通过轻量级 LoRA 适配器和 KL 正则化训练实现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各种自然语言任务中表现出色，但容易产生幻觉，这会影响其在现实世界中的信任度。

Method: AutoRAG-LoRA 是一个模块化的 RAG 框架，通过基于 LoRA 的适配器和 KL 正则化训练来解决大型语言模型中的幻觉问题。

Result: AutoRAG-LoRA 在保持模型效率和模块化的同时，显著减少了事实偏差。

Conclusion: AutoRAG-LoRA 显著减少了事实偏差，同时保持了模型的效率和模块化。

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [6] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在不确定性沟通中的问题，并提出需要通过模仿人类沟通来实现更真实和可信的不确定性表达。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的输出经常以非常自信的方式陈述，即使其准确性值得怀疑，因此需要向用户传达语言模型的信心，以实现人机协作的好处并减轻潜在危害。

Method: 本文对人类不确定性沟通的研究进行了全面的概述，调查了正在进行的研究，并进行了额外的分析以展示迄今为止被忽视的言语不确定性偏见。

Result: 本文展示了言语不确定性中被忽视的偏见，并提出了拟人化不确定性的重要性，即直观和可信的不确定性沟通需要一定程度的语言真实性和个性化。

Conclusion: 本文结论指出，人类-机器交流中的不确定性需要考虑独特的因素，并将拟人化不确定性分解为未来NLP的研究方向。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [7] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: 本文介绍了一种名为PLEX的新型方法，用于高效解释大型语言模型的预测，避免了传统方法所需的大量计算。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释AI（XAI）方法如LIME和SHAP在生成局部解释时需要计算昂贵的扰动，导致显著的计算负担。因此，需要一种更高效的解释方法。

Method: PLEX利用从LLM中提取的上下文嵌入和一个类似“Siamese网络”的神经网络，该网络被训练以与特征重要性分数对齐，从而避免了后续的扰动操作。

Result: PLEX在四个不同的分类任务（情感、虚假新闻、虚假新冠新闻和抑郁）上表现出色，与LIME和SHAP有超过92%的一致性。此外，PLEX在计算时间和资源消耗方面分别减少了两个和四个数量级。

Conclusion: 本文提出了一种名为PLEX的新方法，它能够高效地解释大型语言模型（LLM）的预测，同时保持与LIME和SHAP相当的准确性。

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [8] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型如何建模用户情绪，发现它们形成与人类心理模型对齐的层次化情感树，并揭示了在不同社会经济身份中的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）如何建模用户的情绪状态对于伦理部署至关重要。

Method: 我们分析了模型输出中情感状态之间的概率依赖关系，并研究了大型语言模型（LLMs）如何形成层次化的情感树。

Result: 我们发现LLMs自然形成与人类心理模型对齐的层次化情感树，更大的模型发展出更复杂的层次结构。我们还发现了在不同社会经济身份中的系统性偏见，以及对交集、被代表不足群体的错误分类。

Conclusion: 我们的结果暗示了使用基于认知的理论来开发更好的模型评估的潜力。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [9] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: 本文研究了用于ASW数据文本分析的语言建模方法，并展示了自定义变压器模型的有效性。


<details>
  <summary>Details</summary>
Motivation: ASW广告文本对于链接广告很重要，但处理这种文本具有挑战性，因为其广泛使用表情符号、语法差和故意混淆以逃避执法审查。

Method: 我们对语言建模方法进行了全面的研究，包括简单的信息检索方法、预训练的变压器和自定义变压器模型。

Result: 我们的自定义模型在准确率、召回率、F1分数和ROC AUC上优于已知的编码器-only变压器模型，如BERT-base、RoBERTa和ModernBERT。

Conclusion: 我们开发的模型在ASW文本分析方面代表了重大进展，可以用于各种下游应用和研究。

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [10] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: 本研究探索了使用预训练文本嵌入模型来提升属性图分析中的语义分析效率和效果。


<details>
  <summary>Details</summary>
Motivation: 标签属性图包含丰富的文本属性，当正确利用时可以增强分析任务。

Method: 使用预训练文本嵌入模型来实现属性图中的高效语义分析。

Result: 通过嵌入文本节点和边属性，支持下游任务，如节点分类和关系预测，提高了上下文理解。

Conclusion: 文本语义可以显著提高属性图分析的准确性和可解释性。

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [11] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: 本文介绍了MISS-QA，一个用于评估模型解读科学文献中示意图能力的基准，并揭示了当前模型与人类专家之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的模型在解读科学文献中的示意图方面存在不足，因此需要一个专门的基准来评估和提升模型的能力。

Method: 本文提出了MISS-QA基准，包含1,500个专家标注的例子，用于评估模型在科学文献中解读示意图和回答信息查询的能力。同时对18个前沿多模态基础模型进行了评估。

Result: 评估结果显示，当前的多模态基础模型在MISS-QA上的表现远低于人类专家，且在处理无法回答的问题时表现出一定的局限性。

Conclusion: 本文提出了MISS-QA基准，用于评估模型解读科学文献中的示意图的能力。分析显示当前模型与人类专家之间存在显著性能差距，并提供了改进模型理解多模态科学文献的关键见解。

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [12] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: 本研究分析了社交媒体上的仇恨言论与社会认可的关系，发现两者之间没有显著关联，表明社会认可的强化机制可能在小众平台上有不同表现。


<details>
  <summary>Details</summary>
Motivation: 探讨在线仇恨是否受到他人社会认可的驱动，并验证Walther（2024）的社会认可理论的两个核心假设。

Method: 使用了来自Parler（2018-2021）的超过1.1亿条帖子，分析了仇恨言论与社会认可之间的关系。

Result: 发现仇恨言论获得的点赞数与其后续的仇恨言论数量没有关联，且在不同时间间隔内关系混杂。

Conclusion: 社会认可的强化机制可能在小众社交媒体平台上以不同的方式运作。

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [13] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在司法决策中的公平性问题，发现LLMs存在普遍的不一致、偏见和不平衡的错误。研究构建了一个框架来衡量LLM的公平性，并创建了一个包含177,100个独特案件事实的数据集。通过实验，发现调整温度参数可以影响LLM的公平性，但模型大小、发布日期和国家起源对司法公平性没有显著影响。最后，研究引入了一个公开的工具包，以支持未来评估和改进LLM公平性的研究。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在高风险领域中的使用日益增加，其决策对权利和公平性产生影响，因此需要探索LLMs的司法公平性和对社会正义的影响。当LLMs充当法官时，公平解决司法问题的能力是确保其可信度的前提。

Method: 基于司法公平理论，构建了一个全面的框架来衡量LLM的公平性，并选择了65个标签和161个对应的值。应用这个框架到司法系统中，编译了一个包含177,100个独特案件事实的大型数据集JudiFair。为了实现稳健的统计推断，开发了三个评估指标：不一致性、偏见和不平衡的错误，并引入了一种方法来评估多个LLM在各种标签上的整体公平性。

Result: 通过16个LLMs的实验，发现模型中普遍存在不一致、偏见和不平衡的错误，这凸显了严重的LLM司法不公平。特别是，LLMs在人口统计学标签上表现出明显的偏见，而在物质标签上的偏见比程序标签稍少。有趣的是，不一致性增加与偏见减少相关，但更准确的预测会加剧偏见。虽然发现调整温度参数可以影响LLM的公平性，但模型大小、发布日期和国家起源对司法公平性没有显著影响。

Conclusion: 研究发现大型语言模型（LLMs）在司法决策中存在普遍的不一致、偏见和不平衡的错误，这表明它们在司法公平性方面存在严重问题。此外，调整温度参数可以影响LLM的公平性，但模型大小、发布日期和国家起源对司法公平性没有显著影响。最后，研究引入了一个公开的工具包，以支持未来评估和改进LLM公平性的研究。

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [14] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 研究发现用户对风格相似性的主观感知与第三方客观评估不同，并且主观相似性与用户偏好有强相关性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，用户和系统之间的风格相似性可能会影响用户的印象，但主观和客观相似性之间的区别常常被忽视。

Method: 引入了一个新的数据集，包括用户的偏好、基于用户自身感知的主观风格相似性以及第三方评估者的客观风格相似性。

Result: 分析显示，主观风格相似性与用户偏好之间存在强烈的正相关关系。此外，用户的主观风格相似性与第三方的客观相似性不同。

Conclusion: 研究强调了区分主观和客观评估的重要性，并指出用户对风格相似性的主观感知与第三方客观评估存在差异。

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [15] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: HanjaBridge是一种用于改善韩语语言理解的新型语义注入技术，通过提供同形词的所有可能汉字候选并结合知识蒸馏来防止遗忘，从而显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在低资源语言如韩语中的表现较差，部分原因是独特的语言挑战，例如同音的汉字词在韩文书写中无法区分。

Method: 提出了一种名为HanjaBridge的新颖的语义注入技术，将其集成到持续预训练（CPT）框架中。该方法不是将一个词确定地映射到一个汉字，而是为给定的同形词提供所有可能的汉字候选，鼓励模型学习上下文消歧。同时，通过逐标记的知识蒸馏来防止灾难性遗忘。

Result: 实验结果表明，HanjaBridge显著提高了韩语语言理解能力，在KoBALT基准测试中相对提升了21%。此外，通过共享汉字加强韩语和汉语之间的语义对齐，观察到了强大的正向跨语言迁移。

Conclusion: HanjaBridge显著提高了韩语语言理解能力，并且在推理时即使省略汉字符号增强，效果依然保持，确保了实际效率。

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [16] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在类比推理方面的表现，并比较了它们与人类表现的相似性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在各种任务中表现出接近人类认知的能力，但它们在检测和映射类比方面与人类表现相比仍存在差距。本文旨在评估LLMs在类比推理方面的表现，并探索其与人类表现的相似性。

Method: 本文聚焦于基于故事的类比映射任务，通过句子嵌入评估大型语言模型（LLMs）是否能够捕捉类比中源文本和目标文本之间的相似性，以及源文本和干扰文本之间的差异性。此外，还研究了显式提示LLMs解释类比的效果。

Result: 本文发现LLMs在类比推理方面表现出一定的能力，但在某些方面仍缺乏类似人类的推理能力。此外，模型规模（8B vs. 70B参数）和模型架构（如GPT-4和LLaMA3）对性能有显著影响。

Conclusion: 本文深化了我们对大型语言模型类比推理能力的理解，并探讨了它们作为人类推理模型的潜力。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [17] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: DS@GT团队参加了eRisk 2025挑战赛，采用提示工程策略进行对话抑郁检测，取得了良好的结果。


<details>
  <summary>Details</summary>
Motivation: 为了在没有真实标签的情况下评估模型的表现，我们分析了模型输出与BDI-II标准的一致性以及对话线索对症状预测的影响。

Method: 我们采用了提示工程策略，让不同的大型语言模型进行了BDI-II评估，并产生了结构化的JSON输出。

Result: 我们的方法使模型输出与BDI-II标准对齐，并能够分析影响症状预测的对话线索。

Conclusion: 我们的最佳提交在官方排行榜上排名第二，达到了DCHR = 0.50，ADODL = 0.89，ASHR = 0.27。

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [18] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: 本文提出了TEAM-Sign，将手语视为另一种自然语言，通过微调LLM来学习文本和手语之间的对应关系，并采用分步提示策略来提取LLM中的手语知识，从而支持学习和生成过程。实验结果表明，该方法能够有效利用LLM的手语知识和推理能力，以对齐手语和口语之间的不同分布和语法规则。


<details>
  <summary>Details</summary>
Motivation: 由于手语的复杂性和独特规则，大型语言模型对手语生成的影响仍然有限。因此，我们需要一种方法来解决这个问题。

Method: 通过微调LLM，使其学习文本和手语之间的对应关系，并促进生成。考虑到手语和口语之间的差异，我们采用分步提示策略来提取LLM中的内在手语知识，从而支持学习和生成过程。

Result: 在How2Sign和Phoenix14T数据集上的实验结果表明，我们的方法能够有效利用LLM的手语知识和推理能力，以对齐手语和口语之间的不同分布和语法规则。

Conclusion: 我们的方法有效地利用了LLM的手语知识和推理能力，以对齐手语和口语之间的不同分布和语法规则。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [19] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 本文提出了一种基于Llama 3.1 8B的分层低秩适应（LoRA）方法，用于解决EXIST 2025任务1中的英语和西班牙语推文文本性别歧视检测问题。该方法通过条件适配器路由显式建模标签依赖关系，并在所有线性变换上应用适应，提高了模型捕捉特定任务模式的能力。实验表明，这种方法在保持竞争力的同时显著减少了训练时间和模型存储需求。


<details>
  <summary>Details</summary>
Motivation: 传统的LoRA应用通常只针对注意力层，而本文旨在通过在所有线性变换上应用适应，提高模型捕捉任务特定模式的能力。此外，为了减少训练时间和模型存储需求，本文提出了一种参数高效的微调方法。

Method: 本文采用分层低秩适应（LoRA）方法对Llama 3.1 8B进行微调，引入了条件适配器路由来建模标签依赖关系。同时，在所有线性变换上应用适应，以提高模型捕捉任务特定模式的能力。此外，还使用统一的多语言训练策略，利用Llama 3.1的原生双语能力，实现跨语言迁移。

Result: 本文的方法在三个子任务中均取得了良好的性能：二元性别歧视识别（ICM-Hard: 0.6774）、来源意图检测（0.4991）和多标签性别歧视分类（0.6519）。与全微调相比，本文的方法仅需1.67%的可训练参数，训练时间减少了75%，模型存储减少了98%。

Conclusion: 本文提出了一种基于Llama 3.1 8B的分层低秩适应（LoRA）方法，用于解决EXIST 2025任务1中的英语和西班牙语推文文本性别歧视检测问题。该方法通过条件适配器路由显式建模标签依赖关系，并在所有线性变换上应用适应，提高了模型捕捉特定任务模式的能力。实验表明，这种方法在保持竞争力的同时显著减少了训练时间和模型存储需求。

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [20] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2是HUMANE团队为FEVER-25研讨会的AVeriTeC共享任务开发的增强型系统，它通过文档摘要、答案重述和语言模型优化提高了事实验证的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 改进HerO，使其成为前一年挑战中表现最佳的开源模型，以提高事实验证的效率和准确性。

Method: HerO 2通过文档摘要和答案重述提高证据质量，通过计算约束下的后训练量化优化真值预测，并通过集成更新的语言模型（LM）主干提高整体系统性能。

Result: HerO 2在排行榜上排名第二，同时在前三名系统中运行时间最短，表明其高效且具有实际应用潜力。

Conclusion: HerO 2在FEVER-25研讨会的AVeriTeC共享任务中排名第二，并且在前三名系统中运行时间最短，展示了高效率和强大的实际事实验证潜力。

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [21] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: 本文介绍了	extsc{K-News-Stance}数据集和	extsc{JoA-ICL}框架，用于文章级立场检测，以改善个性化推荐系统并减少过滤气泡和政治极化。


<details>
  <summary>Details</summary>
Motivation: 现有的立场检测研究主要集中在短文本和高资源语言上，无法满足长篇新闻文章的立场检测需求，因此需要一个新的数据集和方法来解决这一问题。

Method: 	extsc{JoA-ICL}是一个基于语言模型代理的框架，用于预测关键结构段落（如导语、引用）的立场，并聚合这些信息以推断整篇文章的总体立场。

Result: 	extsc{JoA-ICL}在文章级立场检测任务中表现出色，优于现有方法，并且通过两个案例研究展示了其在促进观点多样性新闻推荐和揭示媒体偏见模式方面的应用价值。

Conclusion: 本文提出了	extsc{JoA-ICL}框架，该框架在文章级立场检测任务中优于现有方法，并展示了其在促进观点多样性新闻推荐和揭示媒体偏见模式方面的潜力。

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [22] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: 本研究提出了一种基于大型语言模型的临床自然语言处理管道，用于心血管疾病的早期风险评估，并通过改进的性能和临床相关性验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 及时识别和准确的风险分层心血管疾病（CVD）对于减少全球死亡率仍然至关重要。现有的预测模型主要利用结构化数据，而未结构化的临床笔记包含有价值的早期指标。

Method: 本研究引入了一种新的LLM增强的临床NLP管道，该管道使用领域适应的大规模语言模型进行症状提取、上下文推理和相关性分析。方法包括心血管特定的微调、基于提示的推理和实体感知推理。

Result: 在MIMIC-III和CARDIO-NLP数据集上的评估显示，在精确度、召回率、F1分数和AUROC方面表现优于现有方法，并且由心脏病专家评估的临床相关性较高（kappa = 0.82）。

Conclusion: 本研究强调了大型语言模型在临床决策支持系统中的潜力，推动了早期预警系统的发展，并增强了将患者叙述转化为可操作的风险评估的能力。

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [23] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: 本研究提出了一种基于混合变压器的情感分析框架，用于分析孟加拉国七月革命期间和之后社交媒体评论中的公众意见，并取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在分析社交媒体评论中的公众意见，以了解孟加拉国七月革命期间和之后的社会情绪。

Method: 本研究提出了一种基于混合变压器的情感分析框架，使用了BanglaBERT、mBERT、XLM-RoBERTa和提出的混合XMB-BERT来提取特征，并利用PCA进行降维，同时探索了十一种传统和先进的机器学习分类器。

Result: 提出的混合XMB-BERT与投票分类器达到了83.7%的卓越准确率，并优于其他模型分类器组合。

Conclusion: 本研究强调了机器学习技术在分析像孟加拉语这样的低资源语言的社会情感方面的潜力。

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [24] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: 研究比较了传统方法和大型语言模型在识别和分类外国实体中的表现，发现基于接口的LLM在准确性和误报率方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为了提高西班牙金融系统中对外国实体的识别和分类准确性，以确保风险管理和监管合规性。

Method: 评估了传统方法（如Jaccard、余弦和Levenshtein距离）以及基于Hugging Face和接口的LLM（如Microsoft Copilot、阿里巴巴的Qwen 2.5）在识别和分类外国实体中的应用。

Result: 传统方法的准确率超过92%，但误报率较高（20-40%）；基于接口的LLM表现出更高的准确率（超过93%）、F1分数（超过96%）和更低的误报率（40-80%）。

Conclusion: 传统方法虽然准确率高，但存在较高的误报率；基于接口的LLM表现更优，具有更高的准确率和F1分数，以及更低的误报率。

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [25] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文介绍了DIJA，这是一个针对扩散型大语言模型（dLLMs）的安全漏洞进行攻击的框架，展示了其在越狱攻击中的优越性能，并强调了需要重新考虑这些模型的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐机制无法保护dLLMs免受上下文感知的、掩码输入的对抗性提示的影响，这暴露了新的漏洞。

Method: 我们提出了DIJA，这是第一个系统的研究和越狱攻击框架，利用了dLLMs的独特安全弱点。DIJA构建了对抗性交错掩码文本提示，利用了dLLMs的文本生成机制，即双向建模和并行解码。

Result: 通过全面的实验，我们证明DIJA显著优于现有的越狱方法，在Dream-Instruct上实现了高达100%的基于关键词的ASR，在JailbreakBench上比最强的先验基线ReNeLLM高出78.5%的评估器基于ASR，并在StrongREJECT分数上高出37.7分，而无需重写或隐藏越狱提示中的有害内容。

Conclusion: 我们的研究强调了在这一新兴类别的语言模型中重新思考安全对齐的紧迫性。

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [26] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: 本文研究了LLMs中的数据中毒攻击，发现多个触发器可以共存且互不干扰，并提出了一种有效的防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注攻击的有效性，而缺乏对触发机制以及多个触发器如何在模型中相互作用的理解，因此需要更深入的研究来揭示LLMs中的漏洞。

Method: 本文提出了一种研究LLMs中毒的框架，并通过实验验证了多个不同的后门触发器可以在单个模型中共存而不互相干扰，同时提出了基于分层权重差异分析的后处理恢复方法。

Result: 实验表明，多个触发器可以在同一模型中共存且不互相干扰，并且即使在替换或长距离分隔的情况下，触发器仍能保持鲁棒激活。此外，提出的后处理恢复方法能够有效移除触发行为。

Conclusion: 本文提出了一个后处理恢复方法，通过分层权重差异分析选择性地重新训练模型的特定组件，有效地移除了触发行为，同时仅需少量参数更新，为多触发器中毒提供了一种实用且高效的防御方法。

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [27] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: 本文介绍了一个用于多语言多模态推理的鲁棒集成系统，并在ImageCLEF 2025 EXAMS V挑战赛中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 我们提出了一个基于集成的系统，用于多语言多模态推理，旨在ImageCLEF 2025 EXAMS V挑战赛中取得优异成绩。

Method: 我们的方法集成了Gemini 2.5 Flash用于视觉描述，Gemini 1.5 Pro用于标题优化和一致性检查，以及Gemini 2.5 Pro作为推理者处理最终答案选择，所有这些都通过精心设计的少量样本和零样本提示进行协调。

Result: 在官方排行榜上，我们的系统（Team MSA）在多语言赛道中总体排名第一，准确率为81.4%，并在13个单独的语言赛道中领先11个，如克罗地亚语的95.07%和意大利语的92.12%。

Conclusion: 这些发现表明，当与精确的提示策略和跨语言增强结合使用时，轻量级OCR-VLM集成可以在高风险的多语言教育环境中超越更复杂的端到端模型。

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [28] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: 本研究提出了一个数据集和度量标准，用于检测LLMs中记忆的个人数据，展示了记忆与主题网络和模型规模的关系，并为机器遗忘和RTBF请求提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有的机器遗忘方法假设要遗忘的数据是已知的，但没有解决如何识别模型中存储的个体-事实关联的问题。隐私审计技术通常在人口层面操作或针对少量标识符，限制了对个体级数据查询的适用性。

Method: 我们引入了WikiMem数据集和一种模型无关的度量标准，用于量化LLMs中的个体-事实关联。我们的方法通过校准的负对数似然在改写提示中对真实值与反事实进行排名。

Result: 我们在200个个体和15个LLMs（410M-70B参数）上进行了评估，结果显示记忆与主题网络存在相关性，并且与模型规模有关。

Conclusion: 我们的研究为在LLMs中识别记忆的个人数据提供了基础，使动态构建遗忘集成为可能，从而支持机器遗忘和RTBF请求。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [29] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: 本文研究了多智能体系统（MAS）在编码任务中的表现，发现温度和人格对一致性有显著影响，但并未显著提高编码准确性。只有在特定条件下，MAS讨论可能有助于缩小模糊的代码应用。


<details>
  <summary>Details</summary>
Motivation: 虽然多智能体系统（MAS）可以模拟人类编码工作流程，但它们相对于单智能体编码的优势仍然不明确。本文旨在研究MAS在编码任务中的表现及其影响因素。

Method: 本文进行了实验研究，分析了代理人格和温度如何影响对话片段的一致性构建和编码准确性。使用六种开源大型语言模型（LLMs）和18种实验配置，分析了超过77,000次编码决策。

Result: 温度显著影响所有六个LLMs中达成一致与否以及何时达成一致。具有多种人格（包括中立、自信或共情）的MAS在四个 out of 六个LLMs中延迟了一致性。在三个LLMs中，较高的温度显著减弱了多种人格对一致性的影响。然而，温度或人格配对并未导致编码准确性的显著改进。单个代理在大多数条件下匹配或超过了MAS的一致性。只有在一个模型（OpenHermesV2:7B）和一个代码类别中，在温度为0.5或更低且代理至少包含一个自信人格时，MAS讨论带来了超出偶然性的增益。

Conclusion: 本文贡献了对基于LLM的定性方法的局限性的新见解，挑战了多样化的多智能体系统（MAS）人格会导致更好结果的观点。

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [30] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: 本文介绍了西班牙语和加泰罗尼亚语的偏见基准测试（EsBBQ和CaBBQ），用于评估社会偏见，并发现模型在模糊场景中表现不佳，且高准确率可能与社会偏见有关。


<details>
  <summary>Details</summary>
Motivation: 本文动机是由于缺乏对除英语以外的语言和社会背景（如美国以外）的社会偏见评估资源，因此本文提出EsBBQ和CaBBQ来填补这一空白。

Method: 本文方法是引入西班牙语和加泰罗尼亚语的偏见基准测试（EsBBQ和CaBBQ），基于原始的BBQ，这些并行数据集用于评估10个类别中的社会偏见，现在适应了西班牙语和加泰罗尼亚语以及西班牙的社会背景。

Result: 本文结果表明，模型在模糊场景中倾向于无法选择正确答案，并且高QA准确率通常与更大的社会偏见依赖性相关。

Conclusion: 本文结论是，模型在模糊场景中倾向于无法选择正确答案，并且高QA准确率通常与更大的社会偏见依赖性相关。

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [31] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于代理的框架FlowFSM，利用大型语言模型从RFC文档中提取精确的有限状态机，实验结果表明其在协议分析和网络安全领域具有潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的FSM提取技术存在可扩展性、覆盖不全和自然语言规范中的歧义等局限性。

Method: 我们提出了FlowFSM，这是一个新颖的代理框架，利用大型语言模型（LLMs）结合提示链和思维链推理，从原始RFC文档中提取准确的FSM。

Result: 在FTP和RTSP协议上的实验评估表明，FlowFSM在最小化幻觉转换的同时实现了高提取精度，显示出有前景的结果。

Conclusion: 我们的研究结果表明，基于代理的LLM系统在协议分析和FSM推断方面具有潜力，特别是在网络安全和逆向工程应用中。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [32] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 本文介绍了SAE-LAPE方法，用于识别大型语言模型中的语言特定特征，这些特征在模型的中后期层中出现，并且具有可解释性，可用于语言识别。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常关注单个神经元，但由于其多义性，难以从跨语言表示中隔离语言特定单元。本文旨在探索稀疏自编码器（SAEs）来学习跨语言表示具体和抽象概念的单一语义特征，并识别语言特定特征。

Method: 引入了基于特征激活概率的SAE-LAPE方法，用于识别前馈网络中的语言特定特征。

Result: 许多语言特定特征主要出现在模型的中后期层，并且是可解释的。这些特征影响模型的多语言性能和语言输出，并可用于语言识别，性能与fastText相当，同时更具可解释性。

Conclusion: SAE-LAPE方法能够识别出语言特定的特征，这些特征在模型的中后期层中出现，并且具有可解释性。这些特征影响模型的多语言性能和语言输出，可以用于语言识别，其性能与fastText相当，同时更具可解释性。

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [33] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种名为KV-Latent的方法，通过将键值向量维度下采样到潜在空间，显著减少了KV缓存占用并提高了推理速度，同时增强了旋转位置嵌入的稳定性。实验结果表明该方法有效，为构建更高效的语言模型系统提供了新可能性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）基于Transformer解码器已成为对话生成AI的首选。尽管解码器架构总体上具有优势，但推理过程中逐渐增加的键值（KV）缓存已成为效率瓶颈，包括内存消耗和数据传输带宽限制。

Method: 我们提出了一种称为KV-Latent的范式，通过将键值向量维度下采样到潜在空间，显著减少了KV缓存占用并提高了推理速度，仅需少量额外训练。此外，我们通过修改频率采样机制增强了旋转位置嵌入在低维向量上的稳定性。

Result: 实验结果表明，包括具有分组查询注意力和不具有分组查询注意力的模型都取得了令人满意的结果。我们还进行了比较实验，研究单独减少键和值组件对模型性能的影响。

Conclusion: 我们的方法允许构建更高效的语言模型系统，并为KV缓存节省和高效LLM打开了新可能性。

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [34] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的自动形式化方法，并构建了一个高质量的奥林匹克级别数据集，用于形式化数学推理任务。


<details>
  <summary>Details</summary>
Motivation: 高效准确的自动形式化方法对于推动形式化数学推理至关重要，但目前缺乏高质量的数据集和有效的自动形式化方法。

Method: 本文提出了一种基于大型语言模型并结合错误反馈的自动形式化管道，实现了完全自动化且无需训练的形式化方法。

Result: 本文构建了一个奥林匹克级别的数据集，包含3,922个自然语言数学问题和9,787个Lean形式化问题，其中64.46%的问题被评估为至少达到平均水平的质量，适合作为自动化定理证明的基准。此外，实验表明少量样本学习、错误反馈和增加采样数量可以提高自动形式化过程的效果。

Conclusion: 本文提出的自动形式化方法和数据集为形式化数学推理提供了有价值的基准，展示了大型语言模型在自动定理证明中的潜力。

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [35] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文介绍了第一个细粒度中文仇恨言论数据集，并提出了一个将注释词典集成到模型中的方法，以提高仇恨言论检测性能。


<details>
  <summary>Details</summary>
Motivation: 研究中文仇恨言论检测滞后，可解释性研究面临两大挑战：一是缺乏细粒度标注数据集限制了模型对仇恨言论的深层语义理解；二是对识别和解释编码仇恨言论的研究不足限制了模型在复杂现实场景中的可解释性。

Method: 我们提出了一个将注释词典集成到模型中的方法，显著提高了仇恨言论检测性能。

Result: 我们引入了第一个细粒度中文仇恨言论数据集STATE ToxiCN，并使用它评估了现有模型的仇恨语义理解能力。我们还进行了首次关于中文编码仇恨术语和大语言模型解释仇恨语义能力的综合研究。

Conclusion: 我们的工作为推进中文仇恨言论检测研究的可解释性提供了有价值资源和见解。

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [36] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: Dr.Copilot 是一个基于大型语言模型的系统，旨在提高罗马尼亚语医生在远程医疗中书面回复的呈现质量，通过实时反馈来改善用户评价和回复质量。


<details>
  <summary>Details</summary>
Motivation: 文本-based 远程医疗变得越来越普遍，但医生-患者互动中的医疗建议质量往往更多地取决于如何传达建议，而不是其临床准确性。因此，需要一种能够评估和提高医生书面回复呈现质量的系统。

Method: Dr.Copilot 系统由三个经过 DSPy 自动优化提示的大型语言模型代理组成，利用低资源的罗马尼亚数据进行设计，并使用开放权重模型进行部署，以在远程医疗平台上实时提供具体的反馈。

Result: 实证评估和实际部署显示，用户评价和回复质量有明显改善，表明 Dr.Copilot 在提高医生沟通质量方面是有效的。

Conclusion: Dr.Copilot 是一个用于支持罗马尼亚语医生的多代理大型语言模型系统，能够评估和提高他们书面回复的呈现质量。实证评估和实际部署表明，用户的评价和回复质量有明显改善，这是在罗马尼亚医疗环境中首次实际部署大型语言模型之一。

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [37] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了一种名为ConVA的方法，用于直接对齐大型语言模型的内部价值，通过解释价值在潜在表示中的编码并修改相关激活。该方法在不损害模型性能的情况下实现了对10个基本价值的最高控制成功率，并能确保在面对相反和可能恶意的输入时保持目标价值。


<details>
  <summary>Details</summary>
Motivation: 对齐大型语言模型（LLMs）与人类价值观越来越受到关注，因为它提供了清晰度、透明度和适应不断变化场景的能力。

Method: 我们引入了一种受控价值向量激活（ConVA）方法，通过解释价值如何在潜在表示中编码并修改相关激活来直接对齐LLMs的内部价值。我们提出了一个上下文控制的价值向量识别方法以确保准确和无偏的解释，并引入了一种门控价值向量激活方法以有效且最小程度地控制价值。

Result: 实验表明，我们的方法在不损害LLM性能和流畅性的情况下，在10个基本价值上实现了最高的控制成功率，并且即使在相反和可能恶意的输入提示下也能确保目标价值。

Conclusion: 我们的方法在不损害LLM性能和流畅性的情况下，实现了对10个基本价值的最高控制成功率，并确保在相反和可能恶意的输入提示下保持目标价值。

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [38] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种结合人类知识和大型语言模型（LLM）的方法，以改进学术论文中新方法的新颖性预测。


<details>
  <summary>Details</summary>
Motivation: 传统的新颖性评估方法存在局限性，专家知识有限，组合方法的有效性不确定，且独特的引用是否真正衡量新颖性尚不清楚。

Method: 我们从同行评审报告中提取与学术论文新颖性相关的句子，并使用LLM总结学术论文的方法部分，然后用于微调PLMs。此外，我们设计了一个带有新颖稀疏注意力的文本引导融合模块，以更好地整合人类和LLM的知识。

Result: 我们的方法在大量基线中表现优越。

Conclusion: 我们的方法在大量基线中表现优越。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [39] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: 本文首次对多种Process Model Representations (PMRs)进行了实证研究，评估了它们在Large Language Models (LLMs)支持的Process Modeling (PMo)任务中的表现。研究发现Mermaid在多个PMo标准中表现最佳，而BPMN text在PMG任务中效果最好。


<details>
  <summary>Details</summary>
Motivation: 由于现有的PMRs在结构、复杂性和可用性上存在较大差异，且缺乏系统比较，因此需要进行实证研究以评估不同PMRs的表现。

Method: 本文引入了PMo数据集，包含55个过程描述和九种不同的PMRs模型，并从两个维度评估PMRs：适合LLM的PMo和PMG性能。

Result: Mermaid在六个PMo标准中总体得分最高，而BPMN text在过程元素相似性方面表现最佳。

Conclusion: 本文提出了第一个实证研究，评估了多种PMRs在LLM的PMo中的表现。研究发现，Mermaid在六个PMo标准中总体得分最高，而BPMN text在过程元素相似性方面表现最佳。

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [40] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: 本文研究了一种简单的加权损失函数在多标签情感检测中的应用，结果表明该方法在高频情感类别上有效，但在少数类别上效果有限。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决多标签情感检测中数据不平衡的问题，通过动态调整类别权重来提高少数情感类别的性能。

Method: 本文探讨了将简单的加权损失函数应用于基于Transformer的模型，用于SemEval-2025共享任务11中的多标签情感检测。我们的方法通过动态调整类别权重来解决数据不平衡问题，从而在不增加传统重采样方法计算负担的情况下提高少数情感类别的性能。

Result: 实验结果表明，加权损失函数在高频情感类别上提高了性能，但对少数类别影响有限。

Conclusion: 这些发现强调了这种方法在不平衡多标签情感检测中的有效性和挑战。

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [41] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: 本文提出DCR框架，用于检测和量化大型语言模型中的基准数据污染问题，并通过调整准确率来提高评估的公正性和可信度。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的快速发展，基准数据污染问题日益严重，导致性能指标被夸大，影响了真实泛化能力的评估。因此，需要一种有效的工具来检测和量化污染问题。

Method: 本文提出了一种轻量级、可解释的管道DCR框架，通过模糊推理系统合成污染得分，生成统一的DCR因子来调整原始准确率。

Result: 在9个不同规模的大型语言模型上验证了DCR框架，结果表明其能够可靠地诊断污染严重程度，并将准确率调整到与未受污染基线相比平均误差不超过4%。

Conclusion: 本文提出了DCR框架，用于检测和量化基准数据污染，并通过调整准确率来反映污染感知的性能，从而提高大型语言模型评估的公正性和可信度。

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [42] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0是一个结合了非推理和推理模式的模型系列，具有出色的易用性和先进的推理能力，同时支持多种语言，并在性能上优于同类开源模型。


<details>
  <summary>Details</summary>
Motivation: 为了迎接代理AI时代，EXAONE 4.0集成了必要的特性，如代理工具使用，并扩展了多语言支持。

Method: EXAONE 4.0集成了非推理模式和推理模式，以实现EXAONE 3.5的出色易用性和EXAONE Deep的先进推理能力。此外，它还包含了代理工具使用等关键特性，并扩展了多语言支持，除了英语和韩语外，还支持西班牙语。

Result: EXAONE 4.0在同类开源模型中表现出色，并且在与前沿模型的竞争中保持竞争力。

Conclusion: EXAONE 4.0展示了优于同类开源模型的性能，并且在前沿模型中仍具有竞争力。模型对研究目的公开可用，可通过https://huggingface.co/LGAI-EXAONE轻松下载。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [43] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: 研究引入了因果链式思维图（CCGs），通过分析这些图揭示了链式思维如何提升大型语言模型在推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: To shed more light on the mechanism through which chain-of-thought traces improve performance of large language models in reasoning tasks.

Method: We introduce Causal CoT Graphs (CCGs), which are directed acyclic graphs automatically extracted from reasoning traces that model fine-grained causal dependencies in the language model output. A collection of 1671 mathematical reasoning problems from MATH500, GSM8K and AIME, and their associated CCGs are compiled into our dataset -- KisMATH.

Result: Our detailed empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in the CCG are mediators for the final answer, a condition necessary for reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating that models internally realise structures akin to our graphs.

Conclusion: KisMATH enables controlled, graph-aligned interventions and opens up avenues for further investigation into the role of chain-of-thought in LLM reasoning.

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [44] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文介绍了Ettin模型套件，用于比较编码器-only和解码器-only模型，并展示了它们在不同任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）社区几乎只关注解码器-only语言模型，因为它们在文本生成中更容易使用。然而，社区中仍有大量用户使用编码器-only模型进行分类或检索任务。之前的比较工作被迫与参数数量、训练技术和数据集不同的模型进行比较。

Method: 我们引入了SOTA开放数据的Ettin模型套件：从1700万参数到10亿参数的配对编码器-only和解码器-only模型，训练最多2万亿个标记。

Result: 使用相同的配方对编码器-only和解码器-only模型进行训练，在各自大小的类别中产生了SOTA配方，超越了ModernBERT作为编码器和Llama 3.2以及SmolLM2作为解码器。

Conclusion: 我们发现通过持续训练将解码器模型适应到编码器任务（反之亦然）不如仅使用反向目标有效。

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [45] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: 本文研究了如何通过提示控制大语言模型的推理策略，并提出了引导模型选择最佳策略的方法，以提高其在逻辑问题解决中的性能。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，大语言模型倾向于使用单一的推理策略，这可能限制了它们在多样化的推理挑战中的有效性。因此，本文旨在探索如何通过提示来控制大语言模型的推理策略。

Method: 本文研究了提示是否可以控制大语言模型的推理策略，并评估其对逻辑问题解决的影响。此外，还提出了引导大语言模型进行策略选择的方法。

Result: 实验结果表明，没有单一的策略能够持续提高准确性，但如果模型能够自适应地选择最优策略，则可以提高性能。

Conclusion: 本文提出方法可以指导大语言模型在不同推理策略之间进行自适应选择，从而提升其在逻辑问题解决中的性能。

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [46] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: 本文介绍了HKGAI-V1，一个针对香港特定需求开发的主权大语言模型，强调了其在文化敏感性和地区规范对齐方面的优势，并提供了一个可复制的AI系统开发蓝图。


<details>
  <summary>Details</summary>
Motivation: 为应对香港独特的多语言环境、社会法律背景以及本地文化和价值观的考虑，开发一个符合地区规范的AI基础设施。

Method: 基于DeepSeek架构，通过多方面全参数微调过程，将模型与地区规范系统对齐，并集成检索增强生成（RAG）系统以确保及时和事实依据的信息访问。

Result: 成功开发了HKGAI-V1，该模型在处理香港特定文化敏感查询方面优于通用模型，并且开发了一个专有的对抗性HK价值基准测试工具，用于评估模型在挑战性条件下的对齐程度。

Conclusion: 本文提供了HKGAI-V1这一基础主权大语言模型的开发，展示了其在处理香港特定文化敏感查询方面的优势，并提出了一个可复制的蓝图，用于开发与本地身份深度结合的先进区域AI系统。

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [47] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: 研究评估了LLM生成的酒店亮点对输入数据的忠实度，发现简单指标如单词重叠与人类判断有良好相关性，而LLM在评估方面不可靠。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成的酒店亮点对输入数据的忠实度，以确定哪种方法最有效。

Method: 通过人工评估活动进行分类错误评估和跨度级注释，比较传统指标、可训练方法和LLM作为裁判的方法。

Result: 简单的指标如单词重叠与人类判断有很好的相关性（斯皮尔曼等级相关系数为0.63），在应用于域外数据时通常优于更复杂的方法。

Conclusion: 我们的分析表明，尽管LLM可以生成高质量的亮点，但它们在评估方面不可靠，容易严重低估或高估。此外，错误和无法核实的信息带来了最大的风险。

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: 本文比较了强化学习和监督微调在训练大型语言模型时的表现，发现RL增强了现有能力，而SFT可能用新技能取代了旧技能。


<details>
  <summary>Details</summary>
Motivation: 理解强化学习（RL）和监督微调（SFT）在训练大型语言模型（LLM）时的动态特性。

Method: 对RL和SFT在相同数学问题上的训练动态进行了比较分析，并分析了模型参数的变化。

Result: RL在数学领域有小幅提升，但在知识密集型基准测试中略有下降；SFT在这些趋势上更为明显。SFT对中间层MLP的影响更大，这可能导致了跨域性能下降。

Conclusion: 我们的观察结果初步表明，为什么RL增强了现有能力，而SFT则用新技能取代了旧技能。

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [49] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: 本文提出了一种基于编码器-解码器架构的小型语言模型，用于准确预测产品和服务的税码。实验表明，该模型在结构化税码序列预测任务中表现优异，可扩展至其他政府规定的税商品代码。


<details>
  <summary>Details</summary>
Motivation: 准确确定产品和服务的税码对于避免税务处罚至关重要。然而，当前自然语言处理（NLP）研究在这一领域仍相对未探索。因此，本文旨在开发一种高效的税码预测方法。

Method: 本文采用基于编码器-解码器架构的小型语言模型（SLM）来预测层次化的税码序列。这种方法能够捕捉税码中的层次依赖关系，并通过顺序生成税码来提高预测准确性。

Result: 实验结果表明，编码器-解码器架构的SLM在结构化税码序列预测任务中表现出色，特别是在Harmonized System of Nomenclature (HSN) 上的表现优于平面分类器和其他架构。

Conclusion: 本文提出了一种领域自适应的小型语言模型（SLM），用于增强产品和服务税码的预测。实验表明，编码器-解码器架构的SLM在结构化税码序列预测任务中表现优于平面分类器和仅解码器或仅编码器的架构。该方法可以扩展到其他政府规定的税商品代码。

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [50] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的后训练量化方法FOEM，通过显式引入一阶梯度项来改进量化误差补偿，有效提升了模型性能，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于补偿的权重校准方法通常依赖于二阶泰勒展开来建模量化误差，假设在一阶项可以忽略不计的情况下，但作者发现渐进补偿过程会导致潜在权重与其全精度对应项之间的一阶偏差累积，使得这一假设根本上是错误的。

Method: FOEM通过显式地引入一阶梯度项来改进量化误差补偿，利用潜在权重和全精度权重之间的差异直接计算梯度，避免了基于反向传播的梯度计算的高成本和有限泛化性，并利用预计算的Cholesky因子实时恢复Hessian子矩阵的逆。

Result: 在3位权重量化中，FOEM将Llama3-8B的困惑度降低了89.6%，并将Llama3-70B的5-shot MMLU准确率从51.7%提高到74.9%，接近全精度性能的78.6%。此外，FOEM可以与GPTAQ和SpinQuant等先进技术无缝集成，在W4A4KV4设置下取得额外改进，进一步缩小与全精度基线的准确率差距。

Conclusion: FOEM在多种模型和基准测试中表现出色，显著优于传统的GPTQ方法，并且可以与先进的技术如GPTAQ和SpinQuant无缝集成，进一步缩小与全精度基线的准确率差距。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [51] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM is a framework for efficient remote fine-tuning of large language models by using a hierarchical diffusion policy to adapt rank configurations based on wireless states and linguistic complexity.


<details>
  <summary>Details</summary>
Motivation: Existing Low-Rank Adaptation (LoRA) approaches typically employ fixed or heuristic rank configurations, leading to inefficient transmission of all LoRA parameters over-the-air.

Method: AirLLM is a hierarchical diffusion policy framework that models the rank configuration as a structured action vector and uses a Proximal Policy Optimization (PPO) agent combined with Denoising Diffusion Implicit Models (DDIM) to generate high-resolution, task- and channel-adaptive rank vectors.

Result: Experiments under varying signal-to-noise ratios demonstrate that AirLLM consistently enhances fine-tuning performance while significantly reducing transmission costs.

Conclusion: AirLLM demonstrates the effectiveness of reinforcement-driven, diffusion-refined rank adaptation for scalable and efficient remote fine-tuning over the air.

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [52] [Overview of the TREC 2022 deep learning track](https://arxiv.org/abs/2507.10865)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: TREC深度学习跟踪的第四年，使用更大的MS MARCO数据集，专注于段落检索任务。深度神经排序模型表现优于传统方法，但一些顶级运行未使用密集检索。


<details>
  <summary>Details</summary>
Motivation: 本文旨在构建更完整的测试集，以提高段落检索任务的质量，并确保数据集在未来可以重复使用。

Method: 本文使用MS MARCO数据集，利用大量人工标注的训练标签进行段落和文档排名任务。同时，今年还利用了更新的段落和文档集合，增加了数据量。段落检索任务是主要关注点，而文档排名任务作为次要任务，从段落级标签推断出文档级标签。

Result: 深度神经排序模型在大规模预训练下继续优于传统检索方法。但由于专注于段落判断，今年的查询和判断质量更高。此外，一些顶级运行没有进行密集检索，这与以往不同。

Conclusion: 本文分析了TREC深度学习跟踪的第四年，指出深度神经排序模型在大规模预训练下继续优于传统检索方法。由于专注于段落判断，今年的查询和判断质量更高，但一些顶级运行没有进行密集检索，这令人意外。

Abstract: This is the fourth year of the TREC Deep Learning track. As in previous
years, we leverage the MS MARCO datasets that made hundreds of thousands of
human annotated training labels available for both passage and document ranking
tasks. In addition, this year we also leverage both the refreshed passage and
document collections that were released last year leading to a nearly $16$
times increase in the size of the passage collection and nearly four times
increase in the document collection size. Unlike previous years, in 2022 we
mainly focused on constructing a more complete test collection for the passage
retrieval task, which has been the primary focus of the track. The document
ranking task was kept as a secondary task, where document-level labels were
inferred from the passage-level labels. Our analysis shows that similar to
previous years, deep neural ranking models that employ large scale pretraining
continued to outperform traditional retrieval methods. Due to the focusing our
judging resources on passage judging, we are more confident in the quality of
this year's queries and judgments, with respect to our ability to distinguish
between runs and reuse the dataset in future. We also see some surprises in
overall outcomes. Some top-performing runs did not do dense retrieval. Runs
that did single-stage dense retrieval were not as competitive this year as they
were last year.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [53] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)
*Samuel Rhys Cox*

Main category: cs.HC

TL;DR: 本文探讨了如何通过CUI的设计（如表达不确定性或展示推理过程）来促进用户的自我披露。


<details>
  <summary>Details</summary>
Motivation: 自我披露对于人们感到更好很重要，但往往很难。这种困难可能源于我们对他人反应的担忧。本文旨在研究如何通过CUI的设计来促进自我披露。

Method: 本文讨论了自我披露与各种社会线索的关系，并探讨了CUI如何通过透明化其“心智理论”来促进用户自我披露。

Result: 本文提出，通过表达不确定性或展示CUI的推理过程，可以使CUI的“心智理论”更加透明，从而鼓励用户进行自我披露。

Conclusion: 本文认为，通过表达不确定性或展示CUI的推理过程，可以增加用户对CUI的信任，从而鼓励自我披露。

Abstract: Self-disclosure is important to help us feel better, yet is often difficult.
This difficulty can arise from how we think people are going to react to our
self-disclosure. In this workshop paper, we briefly discuss self-disclosure to
conversational user interfaces (CUIs) in relation to various social cues. We
then, discuss how expressions of uncertainty or representation of a CUI's
reasoning could help encourage self-disclosure, by making a CUI's intended
"theory of mind" more transparent to users.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [54] [MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](https://arxiv.org/abs/2507.10859)
*Ramaneswaran Selvakumar,Ashish Seth,Nishit Anand,Utkarsh Tyagi,Sonal Kumar,Sreyan Ghosh,Dinesh Manocha*

Main category: cs.MM

TL;DR: 本文介绍了MultiVox，这是一个用于评估语音助手整合口语和视觉提示能力的基准测试，发现当前模型在生成上下文相关的响应方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法全面评估模型生成上下文相关响应的能力，特别是在理解细微的语音特征和环境声学背景方面。

Method: 引入了MultiVox，这是一个首个全面评估语音助手整合口语和视觉提示能力的基准测试。

Result: 对9个最先进的模型进行评估显示，它们在处理多模态信息时表现不佳。

Conclusion: 当前模型在生成与上下文相关的响应方面仍然存在困难，尽管人类在这些任务中表现出色。

Abstract: The rapid progress of Large Language Models (LLMs) has empowered omni models
to act as voice assistants capable of understanding spoken dialogues. These
models can process multimodal inputs beyond text, such as speech and visual
data, enabling more context-aware interactions. However, current benchmarks
fall short in comprehensively evaluating how well these models generate
context-aware responses, particularly when it comes to implicitly understanding
fine-grained speech characteristics, such as pitch, emotion, timbre, and volume
or the environmental acoustic context such as background sounds. Additionally,
they inadequately assess the ability of models to align paralinguistic cues
with complementary visual signals to inform their responses. To address these
gaps, we introduce MultiVox, the first omni voice assistant benchmark designed
to evaluate the ability of voice assistants to integrate spoken and visual cues
including paralinguistic speech features for truly multimodal understanding.
Specifically, MultiVox includes 1000 human-annotated and recorded speech
dialogues that encompass diverse paralinguistic features and a range of visual
cues such as images and videos. Our evaluation on 9 state-of-the-art models
reveals that, although humans excel at these tasks, current models consistently
struggle to produce contextually grounded responses.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [55] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [56] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 本文提供了对Web of Agents (WoA) 的首次全面演化概述，强调了现代协议如何应对早期标准的局限性，并提出了一个四轴分类法来分析智能体架构。分析显示，智能定位已从外部数据或平台转移到代理的核心模型，这为现代代理AI奠定了基础。最后，文章指出，解决社会技术挑战是未来研究的关键。


<details>
  <summary>Details</summary>
Motivation: 当前的研究在不同社区之间仍然分散，而对多智能体系统（MAS）和语义网的丰富历史往往被视为独立的、过时的领域。这种分散掩盖了现代系统的知识传承，并阻碍了对领域发展轨迹的全面理解。

Method: 我们介绍了第一个全面的WoA演化概述，并引入了一个四轴分类法（语义基础、通信范式、智能定位、发现机制），以系统地分析WoA的发展。

Result: 我们展示了现代协议如A2A和MCP是对早期标准如FIPA标准和基于OWL的语义代理的明确进化反应。我们的分析揭示了一个从外部数据或平台到嵌入代理核心模型的智能定位范式转变，这是现代代理AI的基础。

Conclusion: 虽然新协议对于构建一个强大、开放、可信的生态系统是必要的，但它们不足以实现这一目标。最后，我们认为下一个研究前沿在于解决持续的社会技术挑战，并提出一个专注于去中心化身份、经济模型、安全性和治理的新议程。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [57] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: 本研究评估了使用大型语言模型（LLMs）进行社交媒体数据的主题分析的可行性。结果显示，少量提示的LLM方法可以自动化主题分析，为定性研究提供可扩展的补充。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在归纳主题分析方面面临挑战，这需要深入的解释性和领域专业知识。我们评估了使用LLMs复制专家驱动的主题分析的可行性。

Method: 我们使用两个时间不重叠的Reddit数据集（xylazine，n=286和n=686，分别用于模型优化和验证）以及十二个专家派生的主题，评估了五种LLM与专家编码的对比。我们将任务建模为一系列二分类，而不是单一的多标签分类，采用了零次、单次和少量提示策略，并通过准确率、精确率、召回率和F1分数来衡量性能。

Result: 在验证集上，GPT-4o在两次提示策略下表现最佳（准确率：90.9%；F1分数：0.71）。对于高流行主题，模型得出的主题分布与专家分类非常接近（例如，xylazine使用：13.6% vs. 17.8%；MOUD使用：16.5% vs. 17.8%）。

Conclusion: 我们的研究结果表明，少量提示的LLM方法可以自动化主题分析，为定性研究提供可扩展的补充。

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [58] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: 本文提出了NavComposer框架，用于自动生成高质量的导航指令，并引入了NavInstrCritic评估系统，以无注释的方式评估指令的质量。


<details>
  <summary>Details</summary>
Motivation: 专家提供的指令数量有限，而合成的注释通常质量不高，这使得它们不足以进行大规模研究。

Method: 我们提出了NavComposer，这是一个新颖的框架，用于自动生成高质量的导航指令。NavComposer显式地分解语义实体，如动作、场景和对象，并将它们重新组合成自然语言指令。此外，我们引入了NavInstrCritic，这是一个全面的无注释评估系统，从三个维度评估导航指令：对比匹配、语义一致性和语言多样性。

Result: 广泛的实验提供了直接且实际的证据，证明了我们方法的有效性。

Conclusion: 通过解耦指令生成和评估与特定导航代理的关联，我们的方法使研究更加可扩展和通用。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [59] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 本文提出了一种新的方法LiLM-RDB-SFC，结合轻量级语言模型（LiLM）和关系数据库（RDB）来指导DRL模型进行高效的SFC供应。结果表明，FLAN-T5在测试损失、准确性和处理时间方面优于BART和SQLCoder。


<details>
  <summary>Details</summary>
Motivation: 有效管理服务功能链（SFC）和优化虚拟网络功能（VNF）的放置是现代软件定义网络（SDN）和网络功能虚拟化（NFV）环境中的关键挑战。尽管深度强化学习（DRL）被广泛用于动态网络决策，但其对结构化数据和固定动作规则的依赖限制了适应性和响应能力，特别是在不可预测的网络条件下。

Method: 本文提出了LiLM-RDB-SFC方法，利用两个LiLM（BART和FLAN-T5）来解释网络数据并支持与SFC需求、数据中心资源和VNF可用性相关的多种查询类型。

Result: 结果表明，FLAN-T5在测试损失、准确性和处理时间方面优于BART和SQLCoder。FLAN-T5的测试损失为0.00161，准确率为94.79%，处理时间为2小时2分钟，而BART的测试损失为0.00734，准确率为80.2%，处理时间为2小时38分钟。此外，与大型语言模型SQLCoder相比，FLAN-T5在准确性方面与SQLCoder相当，但处理时间减少了96%。

Conclusion: 本文提出了一种新的方法LiLM-RDB-SFC，结合轻量级语言模型（LiLM）和关系数据库（RDB）来指导DRL模型进行高效的SFC供应。结果表明，FLAN-T5在测试损失、准确性和处理时间方面优于BART和SQLCoder。

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [60] [NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research](https://arxiv.org/abs/2507.10559)
*Shomir Wilson*

Main category: cs.CY

TL;DR: 本文讨论了如何与公众沟通大语言模型的能力和局限性，并提出了三个主题的建议，以促进有效的透明沟通和公众支持。


<details>
  <summary>Details</summary>
Motivation: 本文旨在为研究人员提供与公众沟通的建议，以应对当前对自然语言处理（NLP）日益增长的兴趣，并促进公众对研究的理解和支持。

Method: 本文通过引用发表的NLP研究和大众新闻报道，探讨了三个主题：模糊术语作为公众理解的障碍、不合理的期望作为可持续增长的障碍以及伦理失败作为持续支持的障碍。

Result: 本文提供了三个主题的建议，帮助研究人员更有效地与公众沟通，减少误解并增强对NLP研究的支持。

Conclusion: 本文提出了与公众沟通关于大语言模型的能力和局限性的建议，以促进有效的透明沟通，加强公众理解并鼓励对研究的支持。

Abstract: Recent developments in large language models (LLMs) have been accompanied by
rapidly growing public interest in natural language processing (NLP). This
attention is reflected by major news venues, which sometimes invite NLP
researchers to share their knowledge and views with a wide audience.
Recognizing the opportunities of the present, for both the research field and
for individual researchers, this paper shares recommendations for communicating
with a general audience about LLMs' capabilities and limitations. These
recommendations cover three themes: vague terminology as an obstacle to public
understanding, unreasonable expectations as obstacles to sustainable growth,
and ethical failures as obstacles to continued support. Published NLP research
and popular news coverage are cited to illustrate these themes with examples.
The recommendations promote effective, transparent communication with the
general public about NLP, in order to strengthen public understanding and
encourage support for research.

</details>


### [61] [Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?](https://arxiv.org/abs/2507.10576)
*Bhakti Khera,Rezvan Alamian,Pascal A. Scherz,Stephan M. Goetz*

Main category: cs.CY

TL;DR: 本文评估了多种大型语言模型在欧洲资格考试中的表现，发现它们未能达到专业标准，并指出了需要解决的具体限制。


<details>
  <summary>Details</summary>
Motivation: 法律领域已经使用了各种大型语言模型（LLMs）在实际应用中，但它们的定量性能和原因尚未得到充分研究。

Method: 评估了几种开源和专有大型语言模型（LLMs）在欧洲资格考试（EQE）部分的表现，包括GPT系列、Anthropic、Deepseek和Llama-3变体。

Result: OpenAI o1在准确率和F1分数上表现最佳，而AWS Llama 3.1 8B和Python部署的Llama 3.1 8B表现较差，接近随机猜测水平。没有模型能够完全通过考试，因为准确率从未超过专业标准所需的0.90平均阈值。

Conclusion: 尽管最近的大型模型表现出色，但公众可能高估了它们的表现。该领域还有很长的路要走，以开发出一个虚拟专利律师。本文旨在指出需要解决的几个具体限制。

Abstract: The legal field already uses various large language models (LLMs) in actual
applications, but their quantitative performance and reasons for it are
underexplored. We evaluated several open-source and proprietary LLMs --
including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of
the European Qualifying Examination (EQE) for future European Patent Attorneys.
OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web
Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama
3.1 8B scored 0.55. The latter two are within the range of mere guessing for
the two-answer forced-choice design. None of the evaluated models could have
passed the examination fully, as accuracy never exceeded the average threshold
of 0.90 required for professional-level standards -- also not models that are
regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level
performance. GPT-4o excelled at integrating text and graphics, while Claude 3
Opus often lost formatting coherence. Human patent experts evaluated the
textual justifications and uncovered various critical shortcomings of each
model. They valued clarity and legal rationale over the raw correctness of the
answers, which revealed misalignment between automatic metrics and expert
judgment. Model outputs were sensitive to modest temperature changes and prompt
wording, which underscores the remaining necessity of expert oversight. Future
work should target logical consistency, robust multimodality, and adaptive
prompting to approach human-level patent proficiency. In summary, despite the
outstanding performance of recent large models, the general public might
overestimate their performance. The field has a long way to go to develop a
virtual patent attorney. This paper wants to point out several specific
limitations that need solutions.

</details>


### [62] [Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors](https://arxiv.org/abs/2507.10579)
*Ekaterina Kochmar,Kaushal Kumar Maurya,Kseniia Petukhova,KV Aditya Srivatsa,Anaïs Tack,Justin Vasselli*

Main category: cs.CY

TL;DR: 该论文概述了共享任务的主要发现，讨论了团队采取的方法，并分析了他们的表现，所有资源都公开以支持未来的研究。


<details>
  <summary>Details</summary>
Motivation: 评估AI导师在教育对话中纠正学生错误的能力，以提高教育质量。

Method: 该任务包括五个赛道，旨在自动评估AI导师在错误识别、精确定位错误、提供指导和反馈可操作性方面的表现，并关注导师身份检测。

Result: 最佳结果在四个教学能力评估赛道中，宏F1分数在58.34（提供指导）到71.81（错误识别）之间，导师身份检测赛道的最佳F1分数为96.98。

Conclusion: 该任务展示了AI导师在教育对话中评估学生错误补救方面的能力，尽管结果令人鼓舞，但仍有许多改进空间。

Abstract: This shared task has aimed to assess pedagogical abilities of AI tutors
powered by large language models (LLMs), focusing on evaluating the quality of
tutor responses aimed at student's mistake remediation within educational
dialogues. The task consisted of five tracks designed to automatically evaluate
the AI tutor's performance across key dimensions of mistake identification,
precise location of the mistake, providing guidance, and feedback
actionability, grounded in learning science principles that define good and
effective tutor responses, as well as the track focusing on detection of the
tutor identity. The task attracted over 50 international teams across all
tracks. The submitted models were evaluated against gold-standard human
annotations, and the results, while promising, show that there is still
significant room for improvement in this domain: the best results for the four
pedagogical ability assessment tracks range between macro F1 scores of 58.34
(for providing guidance) and 71.81 (for mistake identification) on three-class
problems, with the best F1 score in the tutor identification track reaching
96.98 on a 9-class task. In this paper, we overview the main findings of the
shared task, discuss the approaches taken by the teams, and analyze their
performance. All resources associated with this task are made publicly
available to support future research in this critical domain.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [63] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: 本文介绍了 SWE-MERA，一个动态、持续更新的基准，旨在解决现有基准中的数据污染问题，并提供了对多个最新 LLM 的性能评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如 SWE-bench）存在严重的数据污染问题，例如 32.67% 的成功补丁涉及直接解决方案泄露，31.08% 的结果由于测试用例不足而通过。因此需要一个新的基准来解决这些问题。

Method: 通过自动化收集真实世界的 GitHub 问题并进行严格的质量验证，构建了一个可靠的管道，以确保质量并最小化污染风险。

Result: SWE-MERA 已经生成大约 10,000 个潜在任务，目前有 300 个样本可用。使用 Aider 编码代理进行的评估表明，该基准对最先进的模型具有强大的区分能力。

Conclusion: SWE-MERA 是一个动态、持续更新的基准，能够有效解决现有基准中的数据污染问题，并为评估大型语言模型在软件工程任务中的性能提供可靠的方法。

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>

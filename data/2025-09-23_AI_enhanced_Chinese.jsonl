{"id": "2509.16226", "pdf": "https://arxiv.org/pdf/2509.16226", "abs": "https://arxiv.org/abs/2509.16226", "authors": ["Brian S. Lin", "Jiaxin Yuan", "Zihan Zhou", "Shouli Wang", "Shuo Wang", "Cunliang Kong", "Qi Shi", "Yuxuan Li", "Liner Yang", "Zhiyuan Liu", "Maosong Sun"], "title": "On LLM-Based Scientific Inductive Reasoning Beyond Equations", "categories": ["cs.CL", "cs.AI"], "comment": "24 pages", "summary": "As large language models (LLMs) increasingly exhibit human-like capabilities,\na fundamental question emerges: How can we enable LLMs to learn the underlying\npatterns from limited examples in entirely novel environments and apply them\neffectively? This question is central to the ability of LLMs in inductive\nreasoning. Existing research on LLM-based inductive reasoning can be broadly\ncategorized based on whether the underlying rules are expressible via explicit\nmathematical equations. However, many recent studies in the beyond-equations\ncategory have emphasized rule design without grounding them in specific\nscenarios. Inspired by the parallels between inductive reasoning and human\nscientific discovery, we propose the task of LLM-Based Scientific Inductive\nReasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to\nevaluate the inductive reasoning abilities of LLMs in scientific settings. Our\nexperimental results show that current LLMs still struggle with this task,\nunderscoring its difficulty and the need for further advancement in this area.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8d85\u8d8a\u65b9\u7a0b\u7684\u79d1\u5b66\u5f52\u7eb3\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5SIRBench-V1\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u73af\u5883\u4e2d\u7684\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e00\u4efb\u52a1\u4e0a\u4ecd\u7136\u9762\u4e34\u56f0\u96be\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ece\u6709\u9650\u7684\u4f8b\u5b50\u4e2d\u5b66\u4e60\u6f5c\u5728\u6a21\u5f0f\u5e76\u5728\u5168\u65b0\u7684\u73af\u5883\u4e2d\u6709\u6548\u5e94\u7528\uff0c\u9700\u8981\u7814\u7a76\u5176\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f52\u7eb3\u63a8\u7406\u7814\u7a76\u53ef\u4ee5\u5206\u4e3a\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u663e\u5f0f\u6570\u5b66\u65b9\u7a0b\u8868\u8fbe\u5e95\u5c42\u89c4\u5219\u3002\u7136\u800c\uff0c\u8bb8\u591a\u6700\u8fd1\u7684\u7814\u7a76\u5728\u8d85\u8d8a\u65b9\u7a0b\u7c7b\u522b\u4e2d\u5f3a\u8c03\u4e86\u89c4\u5219\u8bbe\u8ba1\u800c\u6ca1\u6709\u5c06\u5176\u624e\u6839\u4e8e\u5177\u4f53\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u9879\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8d85\u8d8a\u65b9\u7a0b\u7684\u79d1\u5b66\u5f52\u7eb3\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5SIRBench-V1\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u73af\u5883\u4e2d\u7684\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e00\u4efb\u52a1\u4e0a\u4ecd\u7136\u9762\u4e34\u56f0\u96be\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e00\u4efb\u52a1\u4e0a\u4ecd\u7136\u9762\u4e34\u56f0\u96be\uff0c\u8fd9\u7a81\u663e\u4e86\u8be5\u4efb\u52a1\u7684\u96be\u5ea6\u4ee5\u53ca\u5728\u6b64\u9886\u57df\u8fdb\u4e00\u6b65\u53d1\u5c55\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.16241", "pdf": "https://arxiv.org/pdf/2509.16241", "abs": "https://arxiv.org/abs/2509.16241", "authors": ["Eishkaran Singh", "Tanav Singh Bajaj", "Siddharth Nayak"], "title": "REAMS: Reasoning Enhanced Algorithm for Maths Solving", "categories": ["cs.CL", "cs.AI", "cs.PL"], "comment": null, "summary": "The challenges of solving complex university-level mathematics problems,\nparticularly those from MIT, and Columbia University courses, and selected\ntasks from the MATH dataset, remain a significant obstacle in the field of\nartificial intelligence. Conventional methods have consistently fallen short in\nthis domain, highlighting the need for more advanced approaches. In this paper,\nwe introduce a language-based solution that leverages zero-shot learning and\nmathematical reasoning to effectively solve, explain, and generate solutions\nfor these advanced math problems. By integrating program synthesis, our method\nreduces reliance on large-scale training data while significantly improving\nproblem-solving accuracy. Our approach achieves an accuracy of 90.15%,\nrepresenting a substantial improvement over the previous benchmark of 81% and\nsetting a new standard in automated mathematical problem-solving. These\nfindings highlight the significant potential of advanced AI methodologies to\naddress and overcome the challenges presented by some of the most complex\nmathematical courses and datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u96f6\u6837\u672c\u5b66\u4e60\u548c\u6570\u5b66\u63a8\u7406\u6765\u89e3\u51b3\u590d\u6742\u7684\u6570\u5b66\u95ee\u9898\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5408\u6210\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e8690.15%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u57fa\u51c6\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u5927\u5b66\u6c34\u5e73\u6570\u5b66\u95ee\u9898\uff08\u7279\u522b\u662f\u6765\u81ea\u9ebb\u7701\u7406\u5de5\u5b66\u9662\u548c\u54e5\u4f26\u6bd4\u4e9a\u5927\u5b66\u8bfe\u7a0b\u7684\u95ee\u9898\u4ee5\u53caMATH\u6570\u636e\u96c6\u4e2d\u7684\u9009\u5b9a\u4efb\u52a1\uff09\u7684\u6311\u6218\u4ecd\u7136\u662f\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u5927\u969c\u788d\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u8fd9\u4e2a\u9886\u57df\u4e00\u76f4\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u8868\u660e\u9700\u8981\u66f4\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u96f6\u6837\u672c\u5b66\u4e60\u548c\u6570\u5b66\u63a8\u7406\u6765\u6709\u6548\u89e3\u51b3\u3001\u89e3\u91ca\u548c\u751f\u6210\u8fd9\u4e9b\u9ad8\u7ea7\u6570\u5b66\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u96c6\u6210\u7a0b\u5e8f\u5408\u6210\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u95ee\u9898\u89e3\u51b3\u7684\u51c6\u786e\u6027\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u8fbe\u5230\u4e8690.15%\u7684\u51c6\u786e\u7387\uff0c\u8fd9\u6bd4\u4e4b\u524d\u7684\u57fa\u51c681%\u6709\u4e86\u663e\u8457\u63d0\u9ad8\uff0c\u5e76\u4e3a\u81ea\u52a8\u5316\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u5148\u8fdb\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u5728\u89e3\u51b3\u6700\u590d\u6742\u6570\u5b66\u8bfe\u7a0b\u548c\u6570\u636e\u96c6\u6240\u63d0\u51fa\u7684\u6311\u6218\u65b9\u9762\u7684\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2509.16256", "pdf": "https://arxiv.org/pdf/2509.16256", "abs": "https://arxiv.org/abs/2509.16256", "authors": ["Asiya Ibrahim Zanga", "Salisu Mamman Abdulrahman", "Abubakar Ado", "Abdulkadir Abubakar Bichi", "Lukman Aliyu Jibril", "Abdulmajid Babangida Umar", "Alhassan Adamu", "Shamsuddeen Hassan Muhammad", "Bashir Salisu Abubakar"], "title": "HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language", "categories": ["cs.CL", "cs.AI"], "comment": "Masters Thesis, a Dataset Paper", "summary": "The development of Natural Language Processing (NLP) tools for low-resource\nlanguages is critically hindered by the scarcity of annotated datasets. This\npaper addresses this fundamental challenge by introducing HausaMovieReview, a\nnovel benchmark dataset comprising 5,000 YouTube comments in Hausa and\ncode-switched English. The dataset was meticulously annotated by three\nindependent annotators, demonstrating a robust agreement with a Fleiss' Kappa\nscore of 0.85 between annotators. We used this dataset to conduct a comparative\nanalysis of classical models (Logistic Regression, Decision Tree, K-Nearest\nNeighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results\nreveal a key finding: the Decision Tree classifier, with an accuracy and\nF1-score 89.72% and 89.60% respectively, significantly outperformed the deep\nlearning models. Our findings also provide a robust baseline, demonstrating\nthat effective feature engineering can enable classical models to achieve\nstate-of-the-art performance in low-resource contexts, thereby laying a solid\nfoundation for future research.\n  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u8c6a\u8428\u8bed\u548c\u6df7\u5408\u82f1\u8bed\u7684YouTube\u8bc4\u8bba\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u7ecf\u5178\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5de5\u5177\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u53d1\u5c55\u53d7\u5230\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u4e25\u91cd\u963b\u788d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u57fa\u672c\u6311\u6218\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6HausaMovieReview\uff0c\u5305\u542b5000\u6761\u8c6a\u8428\u8bed\u548c\u6df7\u5408\u82f1\u8bed\u7684YouTube\u8bc4\u8bba\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4e86HausaMovieReview\u6570\u636e\u96c6\u5bf9\u7ecf\u5178\u6a21\u578b\uff08\u903b\u8f91\u56de\u5f52\u3001\u51b3\u7b56\u6811\u3001K\u8fd1\u90bb\uff09\u548c\u5fae\u8c03\u7684transformer\u6a21\u578b\uff08BERT\u548cRoBERTa\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u51b3\u7b56\u6811\u5206\u7c7b\u5668\u7684\u51c6\u786e\u7387\u548cF1\u5206\u6570\u5206\u522b\u4e3a89.72%\u548c89.60%\uff0c\u663e\u8457\u4f18\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\uff0c\u6709\u6548\u7684\u7279\u5f81\u5de5\u7a0b\u53ef\u4ee5\u4f7f\u7ecf\u5178\u6a21\u578b\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4ece\u800c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2509.16264", "pdf": "https://arxiv.org/pdf/2509.16264", "abs": "https://arxiv.org/abs/2509.16264", "authors": ["Wenjie Lin", "Hange Liu", "Xutao Mao", "Yingying Zhuang", "Jingwei Shi", "Xudong Han", "Tianyu Shi", "Jinrui Yang"], "title": "Gender and Political Bias in Large Language Models: A Demonstration Platform", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "online demo: https://euro-parl-vote-demo.vercel.app/; Video:\n  https://www.youtube.com/@Jinrui-sf2jg", "summary": "We present ParlAI Vote, an interactive system for exploring European\nParliament debates and votes, and for testing LLMs on vote prediction and bias\nanalysis. This platform connects debate topics, speeches, and roll-call\noutcomes, and includes rich demographic data such as gender, age, country, and\npolitical group. Users can browse debates, inspect linked speeches, compare\nreal voting outcomes with predictions from frontier LLMs, and view error\nbreakdowns by demographic group. Visualizing the EuroParlVote benchmark and its\ncore tasks of gender classification and vote prediction, ParlAI Vote highlights\nsystematic performance bias in state-of-the-art LLMs. The system unifies data,\nmodels, and visual analytics in a single interface, lowering the barrier for\nreproducing findings, auditing behavior, and running counterfactual scenarios.\nIt supports research, education, and public engagement with legislative\ndecision-making, while making clear both the strengths and the limitations of\ncurrent LLMs in political analysis.", "AI": {"tldr": "ParlAI Vote\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u7528\u4e8e\u63a2\u7d22\u6b27\u6d32\u8bae\u4f1a\u8fa9\u8bba\u548c\u6295\u7968\uff0c\u5e76\u6d4b\u8bd5LLMs\u5728\u6295\u7968\u9884\u6d4b\u548c\u504f\u5dee\u5206\u6790\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4e3a\u4e86\u7814\u7a76LLMs\u5728\u653f\u6cbb\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u6295\u7968\u9884\u6d4b\u548c\u504f\u5dee\u5206\u6790\uff0c\u9700\u8981\u4e00\u4e2a\u96c6\u6570\u636e\u3001\u6a21\u578b\u548c\u53ef\u89c6\u5316\u5206\u6790\u4e8e\u4e00\u4f53\u7684\u5e73\u53f0\u3002", "method": " ParlAI Vote\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u7528\u4e8e\u63a2\u7d22\u6b27\u6d32\u8bae\u4f1a\u8fa9\u8bba\u548c\u6295\u7968\uff0c\u5e76\u6d4b\u8bd5LLMs\u5728\u6295\u7968\u9884\u6d4b\u548c\u504f\u5dee\u5206\u6790\u4e2d\u7684\u8868\u73b0\u3002\u8be5\u5e73\u53f0\u8fde\u63a5\u4e86\u8fa9\u8bba\u4e3b\u9898\u3001\u6f14\u8bb2\u548c\u6295\u7968\u7ed3\u679c\uff0c\u5e76\u5305\u542b\u4e30\u5bcc\u7684\u7edf\u8ba1\u6570\u636e\uff0c\u5982\u6027\u522b\u3001\u5e74\u9f84\u3001\u56fd\u5bb6\u548c\u653f\u6cbb\u56e2\u4f53\u3002", "result": "ParlAI Vote\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684LLMs\u5728\u6027\u522b\u5206\u7c7b\u548c\u6295\u7968\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u7cfb\u7edf\u6027\u6027\u80fd\u504f\u5dee\uff0c\u5e76\u652f\u6301\u7814\u7a76\u3001\u6559\u80b2\u548c\u516c\u4f17\u53c2\u4e0e\u7acb\u6cd5\u51b3\u7b56\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u7edf\u4e00\u4e86\u6570\u636e\u3001\u6a21\u578b\u548c\u53ef\u89c6\u5316\u5206\u6790\uff0c\u964d\u4f4e\u4e86\u91cd\u73b0\u7814\u7a76\u7ed3\u679c\u3001\u5ba1\u8ba1\u884c\u4e3a\u548c\u8fd0\u884c\u53cd\u4e8b\u5b9e\u573a\u666f\u7684\u95e8\u69db\uff0c\u5e76\u5c55\u793a\u4e86\u5f53\u524dLLMs\u5728\u653f\u6cbb\u5206\u6790\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2509.16278", "pdf": "https://arxiv.org/pdf/2509.16278", "abs": "https://arxiv.org/abs/2509.16278", "authors": ["Alok N. Shah", "Khush Gupta", "Keshav Ramji", "Pratik Chaudhari"], "title": "Language Modeling with Learned Meta-Tokens", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5143\u6807\u8bb0\u548c\u5143\u6ce8\u610f\u529b\u673a\u5236\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u4ee3\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u6cdb\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u96be\u4ee5\u6355\u6349\u5176\u4e0a\u4e0b\u6587\u7a97\u53e3\u5185\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u7684\u8868\u73b0\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u5143\u6807\u8bb0\uff08\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u6ce8\u5165\u7684\u7279\u6b8a\u6807\u8bb0\uff09\u4ee5\u53ca\u4e13\u7528\u7684\u5143\u6ce8\u610f\u529b\u673a\u5236\u6765\u5f15\u5bfc\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u8fd9\u4e9b\u6807\u8bb0\u3002\u6211\u4eec\u5bf9\u4e00\u4e2a\u4fee\u6539\u540e\u7684GPT-2\u67b6\u6784\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\uff0c\u8be5\u67b6\u6784\u914d\u5907\u4e86\u5143\u6ce8\u610f\u529b\u673a\u5236\u548c\u56e0\u679c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u6211\u4eec\u5728\u5c11\u4e8e100B\u4e2a\u6807\u8bb0\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u6570\u636e\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u5229\u7528\u5143\u6807\u8bb0\u548c\u6211\u4eec\u7684\u5143\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5fae\u8c03\u540e\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002\u5143\u6807\u8bb0\u901a\u8fc7\u9510\u5316\u4f4d\u7f6e\u7f16\u7801\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u4f7f\u5b83\u4eec\u80fd\u591f\u4f5c\u4e3a\u53ef\u8bad\u7ec3\u7684\u5185\u5bb9\u76f8\u5173\u5730\u6807\uff0c\u9690\u5f0f\u538b\u7f29\u524d\u9762\u7684\u4e0a\u4e0b\u6587\u5e76\u201c\u7f13\u5b58\u201d\u5728\u5143\u6807\u8bb0\u4e2d\u3002\u5728\u63a8\u7406\u65f6\uff0c\u5143\u6807\u8bb0\u6307\u5411\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u4fc3\u8fdb\u957f\u5ea6\u6cdb\u5316\uff0c\u5373\u4f7f\u5728\u6269\u5c55YaRN\u540e\u4e5f\u80fd\u8fbe\u5230\u5176\u4e0a\u4e0b\u6587\u7a97\u53e3\u76842\u500d\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5143\u6807\u8bb0\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u6570\u636e\u9ad8\u6548\u7684\u589e\u5f3a\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4e3a\u5b83\u4eec\u5728\u957f\u5ea6\u6cdb\u5316\u65b9\u9762\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.16325", "pdf": "https://arxiv.org/pdf/2509.16325", "abs": "https://arxiv.org/abs/2509.16325", "authors": ["Andrew Zhu", "Chris Callison-Burch"], "title": "Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "8 pages, 1 figure", "summary": "Imagine AI assistants that enhance conversations without interrupting them:\nquietly providing relevant information during a medical consultation,\nseamlessly preparing materials as teachers discuss lesson plans, or\nunobtrusively scheduling meetings as colleagues debate calendars. While modern\nconversational LLM agents directly assist human users with tasks through a chat\ninterface, we study this alternative paradigm for interacting with LLM agents,\nwhich we call \"overhearing agents.\" Rather than demanding the user's attention,\noverhearing agents continuously monitor ambient activity and intervene only\nwhen they can provide contextual assistance. In this paper, we present the\nfirst analysis of overhearing LLM agents as a distinct paradigm in human-AI\ninteraction and establish a taxonomy of overhearing agent interactions and\ntasks grounded in a survey of works on prior LLM-powered agents and exploratory\nHCI studies. Based on this taxonomy, we create a list of best practices for\nresearchers and developers building overhearing agent systems. Finally, we\noutline the remaining research gaps and reveal opportunities for future\nresearch in the overhearing paradigm.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u542c\u89c9LLM\u4ee3\u7406\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u7684\u4eba\u673a\u4ea4\u4e92\u8303\u5f0f\uff0c\u5206\u6790\u4e86\u5176\u4ea4\u4e92\u65b9\u5f0f\u548c\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u6700\u4f73\u5b9e\u8df5\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u4ee3\u5bf9\u8bddLLM\u4ee3\u7406\u76f4\u63a5\u901a\u8fc7\u804a\u5929\u754c\u9762\u534f\u52a9\u7528\u6237\u5b8c\u6210\u4efb\u52a1\uff0c\u4f46\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u66ff\u4ee3\u8303\u5f0f\uff0c\u5373\u201c\u542c\u89c9\u4ee3\u7406\u201d\uff0c\u5b83\u4eec\u6301\u7eed\u76d1\u63a7\u73af\u5883\u6d3b\u52a8\u5e76\u5728\u80fd\u591f\u63d0\u4f9b\u4e0a\u4e0b\u6587\u5e2e\u52a9\u65f6\u4ecb\u5165\uff0c\u800c\u65e0\u9700\u7528\u6237\u7684\u6ce8\u610f\u529b\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8c03\u67e5\u5148\u524d\u7684LLM\u9a71\u52a8\u4ee3\u7406\u7684\u7814\u7a76\u548c\u63a2\u7d22\u6027HCI\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u542c\u89c9\u4ee3\u7406\u4ea4\u4e92\u548c\u4efb\u52a1\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u5206\u7c7b\u6cd5\u521b\u5efa\u4e86\u4e00\u5957\u6700\u4f73\u5b9e\u8df5\u6307\u5357\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u542c\u89c9\u4ee3\u7406\u7684\u5206\u7c7b\u6cd5\uff0c\u5236\u5b9a\u4e86\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u7a7a\u767d\u548c\u673a\u4f1a\u3002", "conclusion": "\u672c\u6587\u5206\u6790\u4e86\u542c\u89c9LLM\u4ee3\u7406\u4f5c\u4e3a\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4e00\u79cd\u72ec\u7279\u8303\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u6784\u5efa\u542c\u89c9\u4ee3\u7406\u7cfb\u7edf\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u7a7a\u767d\u548c\u673a\u4f1a\u3002"}}
{"id": "2509.16326", "pdf": "https://arxiv.org/pdf/2509.16326", "abs": "https://arxiv.org/abs/2509.16326", "authors": ["Yunsoo Kim", "Michal W. S. Ong", "Alex Shavick", "Honghan Wu", "Adam P. Levine"], "title": "HARE: an entity and relation centric evaluation framework for histopathology reports", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted to EMNLP2025 Findings", "summary": "Medical domain automated text generation is an active area of research and\ndevelopment; however, evaluating the clinical quality of generated reports\nremains a challenge, especially in instances where domain-specific metrics are\nlacking, e.g. histopathology. We propose HARE (Histopathology Automated Report\nEvaluation), a novel entity and relation centric framework, composed of a\nbenchmark dataset, a named entity recognition (NER) model, a relation\nextraction (RE) model, and a novel metric, which prioritizes clinically\nrelevant content by aligning critical histopathology entities and relations\nbetween reference and generated reports. To develop the HARE benchmark, we\nannotated 813 de-identified clinical diagnostic histopathology reports and 652\nhistopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific\nentities and relations. We fine-tuned GatorTronS, a domain-adapted language\nmodel to develop HARE-NER and HARE-RE which achieved the highest overall\nF1-score (0.915) among the tested models. The proposed HARE metric outperformed\ntraditional metrics including ROUGE and Meteor, as well as radiology metrics\nsuch as RadGraph-XL, with the highest correlation and the best regression to\nexpert evaluations (higher than the second best method, GREEN, a large language\nmodel based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\\rho\n= 0.161$, Kendall $\\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release\nHARE, datasets, and the models at https://github.com/knowlab/HARE to foster\nadvancements in histopathology report generation, providing a robust framework\nfor improving the quality of reports.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 HARE\uff08\u7ec4\u7ec7\u75c5\u7406\u5b66\u81ea\u52a8\u5316\u62a5\u544a\u8bc4\u4f30\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u4f53\u548c\u5173\u7cfb\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (NER) \u6a21\u578b\u3001\u5173\u7cfb\u63d0\u53d6 (RE) \u6a21\u578b\u548c\u4e00\u4e2a\u65b0\u63d0\u51fa\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u65e8\u5728\u63d0\u9ad8\u7ec4\u7ec7\u75c5\u7406\u5b66\u62a5\u544a\u7684\u8d28\u91cf\u3002", "motivation": "\u533b\u7597\u9886\u57df\u81ea\u52a8\u5316\u6587\u672c\u751f\u6210\u662f\u4e00\u4e2a\u6d3b\u8dc3\u7684\u7814\u7a76\u548c\u5f00\u53d1\u9886\u57df\uff0c\u4f46\u8bc4\u4f30\u751f\u6210\u62a5\u544a\u7684\u4e34\u5e8a\u8d28\u91cf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u6307\u6807\u7684\u60c5\u51b5\u4e0b\uff0c\u4f8b\u5982\u7ec4\u7ec7\u75c5\u7406\u5b66\u3002", "method": "HARE \u662f\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u4f53\u548c\u5173\u7cfb\u7684\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (NER) \u6a21\u578b\u3001\u5173\u7cfb\u63d0\u53d6 (RE) \u6a21\u578b\u548c\u4e00\u4e2a\u65b0\u63d0\u51fa\u7684\u5ea6\u91cf\u6807\u51c6\u3002\u6211\u4eec\u5bf9 813 \u4efd\u53bb\u6807\u8bc6\u5316\u7684\u4e34\u5e8a\u8bca\u65ad\u7ec4\u7ec7\u75c5\u7406\u5b66\u62a5\u544a\u548c 652 \u4efd\u6765\u81ea The Cancer Genome Atlas (TCGA) \u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u62a5\u544a\u8fdb\u884c\u4e86\u9886\u57df\u7279\u5b9a\u7684\u5b9e\u4f53\u548c\u5173\u7cfb\u6807\u6ce8\uff0c\u5e76\u5fae\u8c03\u4e86 GatorTronS \u8bed\u8a00\u6a21\u578b\u4ee5\u5f00\u53d1 HARE-NER \u548c HARE-RE\u3002", "result": "HARE \u5ea6\u91cf\u6807\u51c6\u4f18\u4e8e\u4f20\u7edf\u7684 ROUGE \u548c Meteor \u5ea6\u91cf\u6807\u51c6\u4ee5\u53ca\u653e\u5c04\u5b66\u5ea6\u91cf\u6807\u51c6\u5982 RadGraph-XL\uff0c\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u6700\u9ad8\uff0c\u5e76\u4e14\u5728\u56de\u5f52\u5230\u4e13\u5bb6\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002HARE-NER \u548c HARE-RE \u5728\u6d4b\u8bd5\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u603b\u4f53 F1 \u5206\u6570 (0.915)\u3002", "conclusion": "HARE \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u7ec4\u7ec7\u75c5\u7406\u5b66\u62a5\u544a\u7684\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u91ca\u653e HARE\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u6765\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2509.16360", "pdf": "https://arxiv.org/pdf/2509.16360", "abs": "https://arxiv.org/abs/2509.16360", "authors": ["Weikang Qiu", "Tinglin Huang", "Ryan Rullo", "Yucheng Kuang", "Ali Maatouk", "S. Raquel Ramos", "Rex Ying"], "title": "RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering", "categories": ["cs.CL"], "comment": "ACM KDD Health Track 2025 Blue Sky Best Paper", "summary": "Large Language Models (LLMs) hold promise in addressing complex medical\nproblems. However, while most prior studies focus on improving accuracy and\nreasoning abilities, a significant bottleneck in developing effective\nhealthcare agents lies in the readability of LLM-generated responses,\nspecifically, their ability to answer public health problems clearly and simply\nto people without medical backgrounds. In this work, we introduce RephQA, a\nbenchmark for evaluating the readability of LLMs in public health question\nanswering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across\n13 topics, and includes a proxy multiple-choice task to assess informativeness,\nalong with two readability metrics: Flesch-Kincaid grade level and professional\nscore. Evaluation of 25 LLMs reveals that most fail to meet readability\nstandards, highlighting a gap between reasoning and effective communication. To\naddress this, we explore four readability-enhancing strategies-standard\nprompting, chain-of-thought prompting, Group Relative Policy Optimization\n(GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best\nresults, advancing the development of more practical and user-friendly public\nhealth agents. These results represent a step toward building more practical\nagents for public health.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RephQA\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u516c\u5171\u536b\u751f\u95ee\u7b54\u4e2d\u7684\u53ef\u8bfb\u6027\u7684\u57fa\u51c6\u3002\u901a\u8fc7\u8bc4\u4f3025\u4e2aLLM\uff0c\u53d1\u73b0\u5927\u591a\u6570\u672a\u80fd\u8fbe\u5230\u53ef\u8bfb\u6027\u6807\u51c6\uff0c\u5e76\u63a2\u7d22\u4e86\u56db\u79cd\u63d0\u9ad8\u53ef\u8bfb\u6027\u7684\u7b56\u7565\uff0c\u5176\u4e2d\u4ee4\u724c\u9002\u5e94\u7684GRPO\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u5927\u591a\u6570\u4ee5\u524d\u7684\u7814\u7a76\u96c6\u4e2d\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\u4e0a\uff0c\u4f46\u5f00\u53d1\u6709\u6548\u7684\u533b\u7597\u4ee3\u7406\u7684\u4e00\u4e2a\u91cd\u8981\u74f6\u9888\u5728\u4e8eLLM\u751f\u6210\u7684\u56de\u7b54\u7684\u53ef\u8bfb\u6027\u3002", "method": "\u6211\u4eec\u63a2\u7d22\u4e86\u56db\u79cd\u63d0\u9ad8\u53ef\u8bfb\u6027\u7684\u7b56\u7565\uff1a\u6807\u51c6\u63d0\u793a\u3001\u601d\u7ef4\u94fe\u63d0\u793a\u3001\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u548c\u4e00\u79cd\u4ee4\u724c\u9002\u5e94\u53d8\u4f53\u3002", "result": "\u8bc4\u4f3025\u4e2aLLM\u7684\u7ed3\u679c\u663e\u793a\uff0c\u5927\u591a\u6570\u672a\u80fd\u8fbe\u5230\u53ef\u8bfb\u6027\u6807\u51c6\uff0c\u8fd9\u8868\u660e\u63a8\u7406\u548c\u6709\u6548\u6c9f\u901a\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4ee3\u8868\u4e86\u5411\u6784\u5efa\u66f4\u5b9e\u7528\u7684\u516c\u5171\u536b\u751f\u4ee3\u7406\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002"}}
{"id": "2509.16375", "pdf": "https://arxiv.org/pdf/2509.16375", "abs": "https://arxiv.org/abs/2509.16375", "authors": ["Cihan Xiao", "Matthew Wiesner", "Debashish Chakraborty", "Reno Kriz", "Keith Cunningham", "Kenton Murray", "Kevin Duh", "Luis Tavarez-Arce", "Paul McNamee", "Sanjeev Khudanpur"], "title": "Whisper-UT: A Unified Translation Framework for Speech and Text", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Encoder-decoder models have achieved remarkable success in speech and text\ntasks, yet efficiently adapting these models to diverse uni/multi-modal\nscenarios remains an open challenge. In this paper, we propose Whisper-UT, a\nunified and efficient framework that leverages lightweight adapters to enable\nseamless adaptation across tasks, including a multi-modal machine translation\n(MMT) task that explicitly conditions translation on both speech and source\nlanguage text inputs. By incorporating ASR hypotheses or ground-truth\ntranscripts as prompts, this approach not only enables the system to process\nboth modalities simultaneously but also enhances speech translation (ST)\nperformance through a 2-stage decoding strategy. We demonstrate our methods\nusing the Whisper model, though in principle they are general and could be\napplied to similar multitask models. We highlight the effectiveness of\ncross-modal and cross-task fine-tuning, which improves performance without\nrequiring 3-way parallel data. Our results underscore the flexibility,\nefficiency, and general applicability of the proposed framework for multi-modal\ntranslation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Whisper-UT\u6846\u67b6\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5b9e\u73b0\u591a\u6a21\u6001\u4efb\u52a1\u7684\u9ad8\u6548\u9002\u5e94\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u89e3\u7801\u7b56\u7565\u63d0\u5347\u8bed\u97f3\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u5728\u8bed\u97f3\u548c\u6587\u672c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u9ad8\u6548\u9002\u5e94\u591a\u79cd\u5355/\u591a\u6a21\u6001\u573a\u666f\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5b9e\u73b0\u4efb\u52a1\u95f4\u7684\u65e0\u7f1d\u9002\u5e94\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u89e3\u7801\u7b56\u7565\u63d0\u5347\u8bed\u97f3\u7ffb\u8bd1\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u7ed3\u5408ASR\u5047\u8bbe\u6216\u771f\u5b9e\u8f6c\u5f55\u672c\u4f5c\u4e3a\u63d0\u793a\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u79cd\u6a21\u6001\uff0c\u8fd8\u901a\u8fc7\u4e24\u9636\u6bb5\u89e3\u7801\u7b56\u7565\u63d0\u5347\u4e86\u8bed\u97f3\u7ffb\u8bd1\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Whisper-UT\u6846\u67b6\u5728\u591a\u6a21\u6001\u7ffb\u8bd1\u4e2d\u8868\u73b0\u51fa\u7075\u6d3b\u6027\u3001\u6548\u7387\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2509.16394", "pdf": "https://arxiv.org/pdf/2509.16394", "abs": "https://arxiv.org/abs/2509.16394", "authors": ["Deuksin Kwon", "Kaleen Shrestha", "Bin Han", "Elena Hayoung Lee", "Gale Lucas"], "title": "Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "Large Language Models (LLMs) are increasingly deployed in socially complex,\ninteraction-driven tasks, yet their ability to mirror human behavior in\nemotionally and strategically complex contexts remains underexplored. This\nstudy assesses the behavioral alignment of personality-prompted LLMs in\nadversarial dispute resolution by simulating multi-turn conflict dialogues that\nincorporate negotiation. Each LLM is guided by a matched Five-Factor\npersonality profile to control for individual variation and enhance realism. We\nevaluate alignment across three dimensions: linguistic style, emotional\nexpression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the\nclosest alignment with humans in linguistic style and emotional dynamics, while\nClaude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial\nalignment gaps persist. Our findings establish a benchmark for alignment\nbetween LLMs and humans in socially complex interactions, underscoring both the\npromise and the limitations of personality conditioning in dialogue modeling.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e2a\u6027\u63d0\u793aLLMs\u5728\u5bf9\u6297\u6027\u4e89\u8bae\u89e3\u51b3\u4e2d\u7684\u884c\u4e3a\u5bf9\u9f50\u60c5\u51b5\uff0c\u53d1\u73b0GPT-4.1\u5728\u8bed\u8a00\u548c\u60c5\u611f\u4e0a\u6700\u63a5\u8fd1\u4eba\u7c7b\uff0c\u800cClaude-3.7-Sonnet\u5728\u6218\u7565\u884c\u4e3a\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u90e8\u7f72\u5728\u793e\u4f1a\u590d\u6742\u3001\u4e92\u52a8\u9a71\u52a8\u7684\u4efb\u52a1\u4e2d\uff0c\u4f46\u5b83\u4eec\u5728\u60c5\u611f\u548c\u6218\u7565\u590d\u6742\u7684\u80cc\u666f\u4e0b\u6a21\u4eff\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\u4ecd\u7f3a\u4e4f\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u6a21\u62df\u5305\u542b\u8c08\u5224\u7684\u591a\u8f6e\u51b2\u7a81\u5bf9\u8bdd\u6765\u8bc4\u4f30\u4e2a\u6027\u63d0\u793aLLMs\u5728\u5bf9\u6297\u6027\u4e89\u8bae\u89e3\u51b3\u4e2d\u7684\u884c\u4e3a\u5bf9\u9f50\u60c5\u51b5\uff0c\u5e76\u4f7f\u7528\u5339\u914d\u7684\u4e94\u56e0\u7d20\u4eba\u683c\u6863\u6848\u6765\u63a7\u5236\u4e2a\u4f53\u5dee\u5f02\u5e76\u63d0\u9ad8\u73b0\u5b9e\u611f\u3002", "result": "GPT-4.1\u5728\u8bed\u8a00\u98ce\u683c\u548c\u60c5\u611f\u52a8\u6001\u65b9\u9762\u6700\u63a5\u8fd1\u4eba\u7c7b\uff0c\u800cClaude-3.7-Sonnet\u6700\u80fd\u53cd\u6620\u6218\u7565\u884c\u4e3a\u3002\u7136\u800c\uff0c\u4ecd\u7136\u5b58\u5728\u663e\u8457\u7684\u5bf9\u9f50\u5dee\u8ddd\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u4e3aLLMs\u4e0e\u4eba\u7c7b\u5728\u793e\u4f1a\u590d\u6742\u4e92\u52a8\u4e2d\u7684\u5bf9\u9f50\u8bbe\u5b9a\u4e86\u57fa\u51c6\uff0c\u5f3a\u8c03\u4e86\u4eba\u683c\u6761\u4ef6\u5728\u5bf9\u8bdd\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2509.16400", "pdf": "https://arxiv.org/pdf/2509.16400", "abs": "https://arxiv.org/abs/2509.16400", "authors": ["Huy Nghiem", "Phuong-Anh Nguyen-Le", "John Prindle", "Rachel Rudinger", "Hal Daum\u00e9 III"], "title": "'Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?", "categories": ["cs.CL", "cs.CY"], "comment": "EMNLP 2025, ver 1, 35 pages", "summary": "Large Language Models (LLMs) are increasingly involved in high-stakes\ndomains, yet how they reason about socially sensitive decisions remains\nunderexplored. We present a large-scale audit of LLMs' treatment of\nsocioeconomic status (SES) in college admissions decisions using a novel\ndual-process framework inspired by cognitive science. Leveraging a synthetic\ndataset of 30,000 applicant profiles grounded in real-world correlations, we\nprompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2\nmodes: a fast, decision-only setup (System 1) and a slower, explanation-based\nsetup (System 2). Results from 5 million prompts reveal that LLMs consistently\nfavor low-SES applicants -- even when controlling for academic performance --\nand that System 2 amplifies this tendency by explicitly invoking SES as\ncompensatory justification, highlighting both their potential and volatility as\ndecision-makers. We then propose DPAF, a dual-process audit framework to probe\nLLMs' reasoning behaviors in sensitive applications.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5927\u5b66\u5f55\u53d6\u4e2d\u5bf9\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u7684\u5904\u7406\u65b9\u5f0f\uff0c\u53d1\u73b0\u5b83\u4eec\u503e\u5411\u4e8e\u652f\u6301\u4f4eSES\u7533\u8bf7\u8005\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u8fc7\u7a0b\u5ba1\u8ba1\u6846\u67b6\uff08DPAF\uff09\u4ee5\u63a2\u6d4b\u5176\u63a8\u7406\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u5bf9\u793e\u4f1a\u654f\u611f\u51b3\u7b56\u7684\u63a8\u7406\u65b9\u5f0f\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5982\u4f55\u5904\u7406\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\uff08SES\uff09\u5728\u5927\u5b66\u5f55\u53d6\u4e2d\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u4e00\u4e2a\u57fa\u4e8e\u73b0\u5b9e\u4e16\u754c\u76f8\u5173\u6027\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b30,000\u4e2a\u7533\u8bf7\u8005\u8d44\u6599\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u6a21\u5f0f\uff08\u7cfb\u7edf1\u548c\u7cfb\u7edf2\uff09\u5bf9\u56db\u4e2a\u5f00\u6e90LLMs\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cLLMs\u59cb\u7ec8\u503e\u5411\u4e8e\u652f\u6301\u4f4eSES\u7533\u8bf7\u8005\uff0c\u5373\u4f7f\u5728\u63a7\u5236\u5b66\u672f\u8868\u73b0\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u7cfb\u7edf2\u6a21\u5f0f\u901a\u8fc7\u660e\u786e\u5f15\u7528SES\u4f5c\u4e3a\u8865\u507f\u6027\u7406\u7531\uff0c\u8fdb\u4e00\u6b65\u653e\u5927\u4e86\u8fd9\u79cd\u503e\u5411\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\uff08SES\uff09\u65f6\u503e\u5411\u4e8e\u652f\u6301\u4f4eSES\u7533\u8bf7\u8005\uff0c\u5373\u4f7f\u5728\u63a7\u5236\u5b66\u672f\u8868\u73b0\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u6b64\u5916\uff0c\u7cfb\u7edf2\u6a21\u5f0f\u4f1a\u653e\u5927\u8fd9\u79cd\u503e\u5411\uff0c\u901a\u8fc7\u660e\u786e\u5f15\u7528SES\u4f5c\u4e3a\u8865\u507f\u6027\u7406\u7531\u3002\u7814\u7a76\u63d0\u51fa\u4e86DPAF\u6846\u67b6\u6765\u63a2\u6d4bLLMs\u5728\u654f\u611f\u5e94\u7528\u4e2d\u7684\u63a8\u7406\u884c\u4e3a\u3002"}}
{"id": "2509.16413", "pdf": "https://arxiv.org/pdf/2509.16413", "abs": "https://arxiv.org/abs/2509.16413", "authors": ["Richard Diehl Martinez", "David Demitri Africa", "Yuval Weiss", "Suchir Salhan", "Ryan Daniels", "Paula Buttery"], "title": "Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Building language models (LMs), especially small and medium ones, remains\nmore art than science. While large LMs often improve by sheer scale, it is\nstill unclear why many design choices work. For small LMs, this uncertainty is\nmore limiting: tight parameter budgets make each decision critical, yet\nresearchers still lack systematic, scientific ways to test and refine new\nideas.\n  We introduce Pico, a lightweight, modular framework that enables systematic,\nhypothesis-driven research for small and medium-scale language model\ndevelopment. Pico consists of two libraries that together provide a practical\nsandbox where researchers can make targeted changes to a model's architecture\nor training procedures and directly observe their effects on the model's\nbehavior. To support reproducible experimentation, we also release a suite of\nbaseline models, pico-decoder, trained under standardized conditions and\nopen-sourced for the community. Case studies highlight how Pico can support\niterative small LM design and analysis.", "AI": {"tldr": "Pico is a framework for systematic, hypothesis-driven research in small and medium-scale language model development, providing a sandbox for targeted changes and reproducible experimentation.", "motivation": "Building small and medium language models remains more art than science due to uncertainty in design choices and the need for systematic, scientific ways to test and refine ideas.", "method": "Pico is a lightweight, modular framework consisting of two libraries that allow researchers to make targeted changes to a model's architecture or training procedures and observe their effects.", "result": "Pico enables researchers to conduct reproducible experimentation with baseline models, pico-decoder, trained under standardized conditions.", "conclusion": "Pico provides a practical sandbox for systematic, hypothesis-driven research in small and medium-scale language model development, supporting iterative design and analysis."}}
{"id": "2509.16422", "pdf": "https://arxiv.org/pdf/2509.16422", "abs": "https://arxiv.org/abs/2509.16422", "authors": ["Tom Mackintosh", "Harish Tayyar Madabushi", "Claire Bonial"], "title": "Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine Tuning", "categories": ["cs.CL"], "comment": null, "summary": "We probe large language models' ability to learn deep form-meaning mappings\nas defined by construction grammars. We introduce the ConTest-NLI benchmark of\n80k sentences covering eight English constructions from highly lexicalized to\nhighly schematic. Our pipeline generates diverse synthetic NLI triples via\ntemplating and the application of a model-in-the-loop filter. This provides\naspects of human validation to ensure challenge and label reliability.\nZero-shot tests on leading LLMs reveal a 24% drop in accuracy between\nnaturalistic (88%) and adversarial data (64%), with schematic patterns proving\nhardest. Fine-tuning on a subset of ConTest-NLI yields up to 9% improvement,\nyet our results highlight persistent abstraction gaps in current LLMs and offer\na scalable framework for evaluating construction-informed learning.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6784\u5f0f\u8bed\u6cd5\u4e2d\u7684\u5f62\u5f0f-\u610f\u4e49\u6620\u5c04\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5e76\u53d1\u73b0\u6a21\u578b\u5728\u5bf9\u6297\u6570\u636e\u4e0a\u7684\u8868\u73b0\u4e0b\u964d\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6784\u5f0f\u5b66\u4e60\u7684\u6846\u67b6\u3002", "motivation": "\u6211\u4eec\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u6784\u5f0f\u8bed\u6cd5\u6240\u5b9a\u4e49\u7684\u6df1\u5c42\u5f62\u5f0f-\u610f\u4e49\u6620\u5c04\u7684\u80fd\u529b\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86ConTest-NLI\u57fa\u51c6\uff0c\u5305\u542b80k\u4e2a\u53e5\u5b50\uff0c\u6db5\u76d6\u4e86\u4ece\u9ad8\u5ea6\u8bcd\u6c47\u5316\u5230\u9ad8\u5ea6\u56fe\u5f0f\u7684\u516b\u79cd\u82f1\u8bed\u6784\u5f0f\u3002\u6211\u4eec\u7684\u7ba1\u9053\u901a\u8fc7\u6a21\u677f\u5316\u548c\u6a21\u578b\u5185\u8fc7\u6ee4\u5668\u7684\u5e94\u7528\u751f\u6210\u591a\u6837\u5316\u7684\u5408\u6210NLI\u4e09\u5143\u7ec4\u3002", "result": "\u96f6\u6837\u672c\u6d4b\u8bd5\u663e\u793a\uff0c\u5728\u81ea\u7136\u6570\u636e\uff0888%\uff09\u548c\u5bf9\u6297\u6570\u636e\uff0864%\uff09\u4e4b\u95f4\u51c6\u786e\u7387\u4e0b\u964d\u4e8624%\uff0c\u5176\u4e2d\u56fe\u5f0f\u6a21\u5f0f\u6700\u96be\u3002\u5728ConTest-NLI\u7684\u4e00\u4e2a\u5b50\u96c6\u4e0a\u5fae\u8c03\u53ef\u63d0\u9ad8\u9ad8\u8fbe9%\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u7a81\u663e\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u65b9\u9762\u7684\u6301\u7eed\u5dee\u8ddd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u57fa\u4e8e\u6784\u5f0f\u7684\u5b66\u4e60\u3002"}}
{"id": "2509.16449", "pdf": "https://arxiv.org/pdf/2509.16449", "abs": "https://arxiv.org/abs/2509.16449", "authors": ["Tsz Fung Pang", "Maryam Berijanian", "Thomas Orth", "Breanna Shi", "Charlotte S. Alexander"], "title": "PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Legal documents are often long, dense, and difficult to comprehend, not only\nfor laypeople but also for legal experts. While automated document\nsummarization has great potential to improve access to legal knowledge,\nprevailing task-based evaluators overlook divergent user and stakeholder needs.\nTool development is needed to encompass the technicality of a case summary for\na litigator yet be accessible for a self-help public researching for their\nlawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation\nframework that scores summaries through the lens of six personas, including\nlegal and non-legal users. We also introduce a controlled dimension-shifted\npilot dataset of U.S. civil rights case summaries that varies along depth,\naccessibility, and procedural detail as well as Diversity-Coverage Index (DCI)\nto expose divergent optima of legal summary between persona-aware and\npersona-agnostic judges. This work enables refinement of legal AI summarization\nsystems for both expert and non-expert users, with the potential to increase\naccess to legal knowledge. The code base and data are publicly available in\nGitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6PersonaMatrix\uff0c\u7528\u4e8e\u8bc4\u4f30\u6cd5\u5f8b\u6587\u6863\u6458\u8981\u7684\u8d28\u91cf\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u53d7\u63a7\u7ef4\u5ea6\u504f\u79fb\u7684\u8bd5\u70b9\u6570\u636e\u96c6\uff0c\u4ee5\u63ed\u793a\u4e0d\u540c\u89d2\u8272\u5bf9\u6458\u8981\u7684\u9700\u6c42\u5dee\u5f02\u3002\u8fd9\u9879\u5de5\u4f5c\u6709\u52a9\u4e8e\u6539\u8fdb\u9488\u5bf9\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7528\u6237\u7684\u6cd5\u5f8bAI\u6458\u8981\u7cfb\u7edf\uff0c\u63d0\u9ad8\u6cd5\u5f8b\u77e5\u8bc6\u7684\u53ef\u53ca\u6027\u3002", "motivation": "\u6cd5\u5f8b\u6587\u6863\u901a\u5e38\u5197\u957f\u3001\u5bc6\u96c6\u4e14\u96be\u4ee5\u7406\u89e3\uff0c\u4e0d\u4ec5\u5bf9\u666e\u901a\u516c\u4f17\uff0c\u800c\u4e14\u5bf9\u6cd5\u5f8b\u4e13\u5bb6\u4e5f\u662f\u5982\u6b64\u3002\u5c3d\u7ba1\u81ea\u52a8\u5316\u6587\u6863\u6458\u8981\u6709\u6f5c\u529b\u6539\u5584\u6cd5\u5f8b\u77e5\u8bc6\u7684\u83b7\u53d6\uff0c\u4f46\u73b0\u6709\u7684\u4efb\u52a1\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e0d\u540c\u7684\u7528\u6237\u548c\u5229\u76ca\u76f8\u5173\u8005\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u5de5\u5177\uff0c\u65e2\u80fd\u6ee1\u8db3\u5f8b\u5e08\u5bf9\u6848\u4f8b\u6458\u8981\u7684\u6280\u672f\u6027\u9700\u6c42\uff0c\u53c8\u80fd\u4e3a\u81ea\u7814\u8bc9\u8bbc\u7684\u516c\u4f17\u63d0\u4f9b\u6613\u7528\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86PersonaMatrix\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u89d2\u8272\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6cd5\u5f8b\u6587\u6863\u6458\u8981\u7684\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a\u53d7\u63a7\u7ef4\u5ea6\u504f\u79fb\u7684\u8bd5\u70b9\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5728\u6df1\u5ea6\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u7a0b\u5e8f\u7ec6\u8282\u7b49\u65b9\u9762\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u6837\u6027-\u8986\u76d6\u7387\u6307\u6570\uff08DCI\uff09\u6765\u8861\u91cf\u4e0d\u540c\u89d2\u8272\u5bf9\u6458\u8981\u7684\u9700\u6c42\u5dee\u5f02\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86PersonaMatrix\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u89d2\u8272\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u516d\u4e2a\u89d2\u8272\uff08\u5305\u62ec\u6cd5\u5f8b\u548c\u975e\u6cd5\u5f8b\u7528\u6237\uff09\u5bf9\u6458\u8981\u8fdb\u884c\u8bc4\u5206\u3002\u540c\u65f6\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u53d7\u63a7\u7ef4\u5ea6\u504f\u79fb\u7684\u8bd5\u70b9\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5728\u6df1\u5ea6\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u7a0b\u5e8f\u7ec6\u8282\u7b49\u65b9\u9762\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u6837\u6027-\u8986\u76d6\u7387\u6307\u6570\uff08DCI\uff09\u6765\u63ed\u793a\u89d2\u8272\u611f\u77e5\u548c\u89d2\u8272\u65e0\u5173\u6cd5\u5b98\u4e4b\u95f4\u6cd5\u5f8b\u6458\u8981\u7684\u4e0d\u540c\u6700\u4f18\u503c\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86PersonaMatrix\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u89d2\u8272\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u516d\u4e2a\u89d2\u8272\uff08\u5305\u62ec\u6cd5\u5f8b\u548c\u975e\u6cd5\u5f8b\u7528\u6237\uff09\u5bf9\u6458\u8981\u8fdb\u884c\u8bc4\u5206\u3002\u540c\u65f6\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u53d7\u63a7\u7ef4\u5ea6\u504f\u79fb\u7684\u8bd5\u70b9\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5728\u6df1\u5ea6\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u7a0b\u5e8f\u7ec6\u8282\u7b49\u65b9\u9762\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u6837\u6027-\u8986\u76d6\u7387\u6307\u6570\uff08DCI\uff09\uff0c\u4ee5\u63ed\u793a\u89d2\u8272\u611f\u77e5\u548c\u89d2\u8272\u65e0\u5173\u6cd5\u5b98\u4e4b\u95f4\u6cd5\u5f8b\u6458\u8981\u7684\u4e0d\u540c\u6700\u4f18\u503c\u3002\u8fd9\u9879\u5de5\u4f5c\u6709\u52a9\u4e8e\u6539\u8fdb\u9488\u5bf9\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7528\u6237\u7684\u6cd5\u5f8bAI\u6458\u8981\u7cfb\u7edf\uff0c\u4ece\u800c\u63d0\u9ad8\u6cd5\u5f8b\u77e5\u8bc6\u7684\u53ef\u53ca\u6027\u3002\u4ee3\u7801\u5e93\u548c\u6570\u636e\u53ef\u5728GitHub\u4e0a\u516c\u5f00\u83b7\u53d6\u3002"}}
{"id": "2509.16457", "pdf": "https://arxiv.org/pdf/2509.16457", "abs": "https://arxiv.org/abs/2509.16457", "authors": ["Yunzhe Wang", "Gale M. Lucas", "Burcin Becerik-Gerber", "Volkan Ustun"], "title": "Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025), Main Conference", "summary": "Language-driven generative agents have enabled large-scale social simulations\nwith transformative uses, from interpersonal training to aiding global\npolicy-making. However, recent studies indicate that generative agent behaviors\noften deviate from expert expectations and real-world data--a phenomenon we\nterm the Behavior-Realism Gap. To address this, we introduce a theoretical\nframework called Persona-Environment Behavioral Alignment (PEBA), formulated as\na distribution matching problem grounded in Lewin's behavior equation stating\nthat behavior is a function of the person and their environment. Leveraging\nPEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that\niteratively refines agent personas, implicitly aligning their collective\nbehaviors with realistic expert benchmarks within a specified environmental\ncontext. We validate PEvo in an active shooter incident simulation we\ndeveloped, achieving an 84% average reduction in distributional divergence\ncompared to no steering and a 34% improvement over explicit instruction\nbaselines. Results also show PEvo-refined personas generalize to novel, related\nsimulation scenarios. Our method greatly enhances behavioral realism and\nreliability in high-stakes social simulations. More broadly, the PEBA-PEvo\nframework provides a principled approach to developing trustworthy LLM-driven\nsocial simulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPEBA\u7684\u7406\u8bba\u6846\u67b6\u548c\u57fa\u4e8eLLM\u7684\u4f18\u5316\u7b97\u6cd5PEvo\uff0c\u4ee5\u89e3\u51b3\u751f\u6210\u4ee3\u7406\u884c\u4e3a\u4e0e\u73b0\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u5728\u7a81\u53d1\u67aa\u51fb\u4e8b\u4ef6\u6a21\u62df\u4e2d\uff0cPEvo\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u884c\u4e3a\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u65b0\u7684\u6a21\u62df\u573a\u666f\u3002", "motivation": "\u8bed\u8a00\u9a71\u52a8\u7684\u751f\u6210\u4ee3\u7406\u5728\u5927\u89c4\u6a21\u793e\u4f1a\u6a21\u62df\u4e2d\u5177\u6709\u53d8\u9769\u6027\u7684\u5e94\u7528\uff0c\u4ece\u4eba\u9645\u57f9\u8bad\u5230\u5e2e\u52a9\u5168\u7403\u653f\u7b56\u5236\u5b9a\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u751f\u6210\u4ee3\u7406\u7684\u884c\u4e3a\u5f80\u5f80\u504f\u79bb\u4e13\u5bb6\u671f\u671b\u548c\u73b0\u5b9e\u6570\u636e\u2014\u2014\u6211\u4eec\u79f0\u4e4b\u4e3a\u884c\u4e3a-\u73b0\u5b9e\u5dee\u8ddd\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u79f0\u4e3aPersona-Environment Behavioral Alignment (PEBA)\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u5206\u5e03\u5339\u914d\u95ee\u9898\uff0c\u57fa\u4e8eLewin\u7684\u884c\u4e3a\u65b9\u7a0b\uff0c\u5373\u884c\u4e3a\u662f\u4e2a\u4eba\u548c\u73af\u5883\u7684\u51fd\u6570\u3002\u57fa\u4e8ePEBA\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PersonaEvolve (PEvo)\uff0c\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u4ee3\u7406\u4eba\u683c\uff0c\u9690\u5f0f\u5730\u5c06\u4ed6\u4eec\u7684\u96c6\u4f53\u884c\u4e3a\u4e0e\u7279\u5b9a\u73af\u5883\u60c5\u5883\u4e0b\u7684\u73b0\u5b9e\u4e13\u5bb6\u57fa\u51c6\u5bf9\u9f50\u3002", "result": "\u6211\u4eec\u5728\u4e00\u4e2a\u6211\u4eec\u5f00\u53d1\u7684\u7a81\u53d1\u67aa\u51fb\u4e8b\u4ef6\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86PEvo\uff0c\u4e0e\u6ca1\u6709\u5f15\u5bfc\u76f8\u6bd4\uff0c\u5206\u5e03\u5dee\u5f02\u5e73\u5747\u51cf\u5c11\u4e8684%\uff0c\u6bd4\u663e\u5f0f\u6307\u4ee4\u57fa\u7ebf\u63d0\u9ad8\u4e8634%\u3002\u7ed3\u679c\u8fd8\u663e\u793a\uff0cPEvo\u4f18\u5316\u7684\u4eba\u683c\u53ef\u4ee5\u63a8\u5e7f\u5230\u65b0\u7684\u3001\u76f8\u5173\u7684\u6a21\u62df\u573a\u666f\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u98ce\u9669\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u884c\u4e3a\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\u3002\u66f4\u5e7f\u6cdb\u5730\u8bf4\uff0cPEBA-PEvo\u6846\u67b6\u4e3a\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684LLM\u9a71\u52a8\u7684\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u539f\u5219\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.16462", "pdf": "https://arxiv.org/pdf/2509.16462", "abs": "https://arxiv.org/abs/2509.16462", "authors": ["'Mina Arzaghi'", "'Alireza Dehghanpour Farashah'", "'Florian Carichon'", "' Golnoosh Farnadi'"], "title": "Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) exhibit socio-economic biases that can propagate\ninto downstream tasks. While prior studies have questioned whether intrinsic\nbias in LLMs affects fairness at the downstream task level, this work\nempirically investigates the connection. We present a unified evaluation\nframework to compare intrinsic bias mitigation via concept unlearning with\nextrinsic bias mitigation via counterfactual data augmentation (CDA). We\nexamine this relationship through real-world financial classification tasks,\nincluding salary prediction, employment status, and creditworthiness\nassessment. Using three open-source LLMs, we evaluate models both as frozen\nembedding extractors and as fine-tuned classifiers. Our results show that\nintrinsic bias mitigation through unlearning reduces intrinsic gender bias by\nup to 94.9%, while also improving downstream task fairness metrics, such as\ndemographic parity by up to 82%, without compromising accuracy. Our framework\noffers practical guidance on where mitigation efforts can be most effective and\nhighlights the importance of applying early-stage mitigation before downstream\ndeployment.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5185\u5728\u504f\u5dee\u5bf9\u4e0b\u6e38\u4efb\u52a1\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6bd4\u8f83\u4e0d\u540c\u7684\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6982\u5ff5\u5220\u9664\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u5185\u5728\u504f\u5dee\u5e76\u63d0\u9ad8\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u5b9e\u8bc1\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u5185\u5728\u504f\u5dee\u662f\u5426\u4f1a\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u7684\u516c\u5e73\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6bd4\u8f83\u901a\u8fc7\u6982\u5ff5\u5220\u9664\u8fdb\u884c\u5185\u5728\u504f\u5dee\u7f13\u89e3\u4e0e\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\uff08CDA\uff09\u8fdb\u884c\u5916\u5728\u504f\u5dee\u7f13\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6982\u5ff5\u5220\u9664\u8fdb\u884c\u5185\u5728\u504f\u5dee\u7f13\u89e3\u53ef\u51cf\u5c11\u9ad8\u8fbe94.9%\u7684\u5185\u5728\u6027\u522b\u504f\u5dee\uff0c\u5e76\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u7684\u516c\u5e73\u6027\u6307\u6807\uff0c\u5982\u4eba\u53e3\u5747\u7b49\u6027\uff0c\u6700\u9ad8\u53ef\u8fbe82%\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u7ed3\u8bba\u8868\u660e\uff0c\u901a\u8fc7\u6982\u5ff5\u5220\u9664\u8fdb\u884c\u5185\u5728\u504f\u5dee\u7f13\u89e3\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u5185\u5728\u6027\u522b\u504f\u5dee\uff0c\u5e76\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u7684\u516c\u5e73\u6027\u6307\u6807\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002\u6846\u67b6\u4e3a\u7f13\u89e3\u52aa\u529b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u4e0b\u6e38\u90e8\u7f72\u524d\u8fdb\u884c\u65e9\u671f\u9636\u6bb5\u7f13\u89e3\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.16464", "pdf": "https://arxiv.org/pdf/2509.16464", "abs": "https://arxiv.org/abs/2509.16464", "authors": ["Margaret Hughes", "Brandon Roy", "Elinor Poole-Dayan", "Deb Roy", "Jad Kabbara"], "title": "Computational Analysis of Conversation Dynamics through Participant Responsivity", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Growing literature explores toxicity and polarization in discourse, with\ncomparatively less work on characterizing what makes dialogue prosocial and\nconstructive. We explore conversational discourse and investigate a method for\ncharacterizing its quality built upon the notion of ``responsivity'' -- whether\none person's conversational turn is responding to a preceding turn. We develop\nand evaluate methods for quantifying responsivity -- first through semantic\nsimilarity of speaker turns, and second by leveraging state-of-the-art large\nlanguage models (LLMs) to identify the relation between two speaker turns. We\nevaluate both methods against a ground truth set of human-annotated\nconversations. Furthermore, selecting the better performing LLM-based approach,\nwe characterize the nature of the response -- whether it responded to that\npreceding turn in a substantive way or not.\n  We view these responsivity links as a fundamental aspect of dialogue but note\nthat conversations can exhibit significantly different responsivity structures.\nAccordingly, we then develop conversation-level derived metrics to address\nvarious aspects of conversational discourse. We use these derived metrics to\nexplore other conversations and show that they support meaningful\ncharacterizations and differentiations across a diverse collection of\nconversations.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5bf9\u8bdd\u8bdd\u8bed\u7684\u8d28\u91cf\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eresponsivity\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u5bf9\u8bdd\u7684\u54cd\u5e94\u6027\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8868\u5f81\u548c\u533a\u5206\u4e0d\u540c\u5bf9\u8bdd\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u4e3b\u8981\u5173\u6ce8\u8bdd\u8bed\u4e2d\u7684\u6bd2\u6027\u4e0e\u6781\u5316\uff0c\u800c\u8f83\u5c11\u7814\u7a76\u5bf9\u8bdd\u7684\u79ef\u6781\u548c\u5efa\u8bbe\u6027\u7279\u5f81\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5bf9\u8bdd\u8bdd\u8bed\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eresponsivity\u7684\u6982\u5ff5\u6765\u8868\u5f81\u5176\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u91cf\u5316responsivity\uff0c\u5e76\u5229\u7528\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u8bc6\u522b\u4e24\u4e2a\u8bf4\u8bdd\u8005\u56de\u5408\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u7136\u540e\u8bc4\u4f30\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u9009\u62e9\u8868\u73b0\u66f4\u597d\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u6765\u5206\u6790\u56de\u5e94\u662f\u5426\u4ee5\u5b9e\u8d28\u6027\u65b9\u5f0f\u56de\u5e94\u4e86\u4e4b\u524d\u7684\u56de\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u5bf9\u8bdd\u7ea7\u522b\u7684\u884d\u751f\u6307\u6807\u6765\u89e3\u51b3\u5404\u79cd\u5bf9\u8bdd\u8bdd\u8bed\u65b9\u9762\u7684\u95ee\u9898\u3002", "result": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e24\u79cd\u91cf\u5316responsivity\u7684\u65b9\u6cd5\uff0c\u5e76\u9009\u62e9\u4e86\u8868\u73b0\u66f4\u597d\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u6765\u5206\u6790\u56de\u5e94\u662f\u5426\u5b9e\u8d28\u6027\u5730\u56de\u5e94\u4e86\u4e4b\u524d\u7684\u56de\u5408\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u5bf9\u8bdd\u7ea7\u522b\u7684\u884d\u751f\u6307\u6807\uff0c\u5e76\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u5bf9\u8bdd\u4e2d\u7684\u6709\u610f\u4e49\u7684\u8868\u5f81\u548c\u533a\u5206\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3aresponsivity\u662f\u5bf9\u8bdd\u4e2d\u7684\u57fa\u672c\u65b9\u9762\uff0c\u4f46\u5bf9\u8bdd\u53ef\u4ee5\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u540c\u7684responsivity\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u5bf9\u8bdd\u7ea7\u522b\u7684\u884d\u751f\u6307\u6807\u6765\u89e3\u51b3\u5404\u79cd\u5bf9\u8bdd\u8bdd\u8bed\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u6307\u6807\u5728\u4e0d\u540c\u5bf9\u8bdd\u4e2d\u7684\u6709\u610f\u4e49\u7684\u8868\u5f81\u548c\u533a\u5206\u80fd\u529b\u3002"}}
{"id": "2509.16487", "pdf": "https://arxiv.org/pdf/2509.16487", "abs": "https://arxiv.org/abs/2509.16487", "authors": ["Zixun Chen", "Petr Babkin", "Akshat Gupta", "Gopala Anumanchipalli", "Xiaomo Liu"], "title": "The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Dialogue is one of the landmark abilities of large language models (LLMs).\nDespite its ubiquity, few studies actually distinguish specific ingredients\nunderpinning dialogue behavior emerging during post-training. We employ a\ncomprehensive suite of model-based metrics, each targeting a distinct\nfine-grained aspect of dialogue, motivated by linguistic theory. We evaluate\nhow the performance of pre-trained Pythia models changes with respect to each\nof those dimensions, depending on model size and as a result of supervised\nfine-tuning on conversational datasets. We observe only a mild impact of raw\nmodel size on most metrics, whereas fine-tuning quickly saturates the scores\nfor all but the smallest models tested. Somewhat contrary to our expectations,\nmany metrics show very similar trends, especially if they are all rooted in the\nsame evaluator model, which raises the question of their reliability in\nmeasuring a specific dimension. To that end, we conduct additional analyses of\nscore distributions, metric correlations, and term frequencies in generated\nresponses to help explain our observations.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u884c\u4e3a\u7684\u6210\u5206\uff0c\u53d1\u73b0\u6a21\u578b\u5927\u5c0f\u5bf9\u5927\u591a\u6570\u6307\u6807\u5f71\u54cd\u4e0d\u5927\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u8fc5\u901f\u63d0\u9ad8\u4e86\u5206\u6570\uff0c\u4f46\u8bb8\u591a\u6307\u6807\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u8d8b\u52bf\uff0c\u5f15\u53d1\u4e86\u5176\u53ef\u9760\u6027\u7684\u7591\u95ee\u3002", "motivation": "\u4e3a\u4e86\u533a\u5206\u540e\u8bad\u7ec3\u671f\u95f4\u51fa\u73b0\u7684\u5bf9\u8bdd\u884c\u4e3a\u7684\u5177\u4f53\u6210\u5206\uff0c\u7814\u7a76\u8005\u91c7\u7528\u4e86\u5168\u9762\u7684\u6a21\u578b\u5ea6\u91cf\u6807\u51c6\uff0c\u4ee5\u8bc4\u4f30\u9884\u8bad\u7ec3\u7684Pythia\u6a21\u578b\u5728\u4e0d\u540c\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u5ea6\u91cf\u6807\u51c6\u5957\u4ef6\uff0c\u6bcf\u4e2a\u5ea6\u91cf\u6807\u51c6\u9488\u5bf9\u5bf9\u8bdd\u7684\u4e0d\u540c\u7ec6\u7c92\u5ea6\u65b9\u9762\uff0c\u5e76\u57fa\u4e8e\u8bed\u8a00\u5b66\u7406\u8bba\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5927\u5c0f\u5bf9\u5927\u591a\u6570\u6307\u6807\u7684\u5f71\u54cd\u8f83\u5c0f\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u8fc5\u901f\u4f7f\u6240\u6709\u4f46\u6700\u5c0f\u7684\u6a21\u578b\u7684\u5206\u6570\u8fbe\u5230\u9971\u548c\u3002\u6b64\u5916\uff0c\u8bb8\u591a\u6307\u6807\u8868\u73b0\u51fa\u975e\u5e38\u76f8\u4f3c\u7684\u8d8b\u52bf\uff0c\u8fd9\u5f15\u53d1\u4e86\u5b83\u4eec\u5728\u6d4b\u91cf\u7279\u5b9a\u7ef4\u5ea6\u65f6\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u5927\u5c0f\u5bf9\u5927\u591a\u6570\u6307\u6807\u7684\u5f71\u54cd\u5f88\u5c0f\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u5f88\u5feb\u4f7f\u6240\u6709\u4f46\u6700\u5c0f\u7684\u6a21\u578b\u7684\u5206\u6570\u8fbe\u5230\u9971\u548c\u3002\u6b64\u5916\uff0c\u8bb8\u591a\u6307\u6807\u8868\u73b0\u51fa\u975e\u5e38\u76f8\u4f3c\u7684\u8d8b\u52bf\uff0c\u8fd9\u5f15\u53d1\u4e86\u5b83\u4eec\u5728\u6d4b\u91cf\u7279\u5b9a\u7ef4\u5ea6\u65f6\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002"}}
{"id": "2509.16494", "pdf": "https://arxiv.org/pdf/2509.16494", "abs": "https://arxiv.org/abs/2509.16494", "authors": ["Fengyuan Liu", "Rui Zhao", "Shuo Chen", "Guohao Li", "Philip Torr", "Lei Han", "Jindong Gu"], "title": "Can an Individual Manipulate the Collective Decisions of Multi-Agents?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Individual Large Language Models (LLMs) have demonstrated significant\ncapabilities across various domains, such as healthcare and law. Recent studies\nalso show that coordinated multi-agent systems exhibit enhanced decision-making\nand reasoning abilities through collaboration. However, due to the\nvulnerabilities of individual LLMs and the difficulty of accessing all agents\nin a multi-agent system, a key question arises: If attackers only know one\nagent, could they still generate adversarial samples capable of misleading the\ncollective decision? To explore this question, we formulate it as a game with\nincomplete information, where attackers know only one target agent and lack\nknowledge of the other agents in the system. With this formulation, we propose\nM-Spoiler, a framework that simulates agent interactions within a multi-agent\nsystem to generate adversarial samples. These samples are then used to\nmanipulate the target agent in the target system, misleading the system's\ncollaborative decision-making process. More specifically, M-Spoiler introduces\na stubborn agent that actively aids in optimizing adversarial samples by\nsimulating potential stubborn responses from agents in the target system. This\nenhances the effectiveness of the generated adversarial samples in misleading\nthe system. Through extensive experiments across various tasks, our findings\nconfirm the risks posed by the knowledge of an individual agent in multi-agent\nsystems and demonstrate the effectiveness of our framework. We also explore\nseveral defense mechanisms, showing that our proposed attack framework remains\nmore potent than baselines, underscoring the need for further research into\ndefensive strategies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u653b\u51fb\u8005\u4ec5\u4e86\u89e3\u4e00\u4e2a\u4ee3\u7406\u65f6\u662f\u5426\u80fd\u751f\u6210\u5bf9\u6297\u6837\u672c\u4ee5\u8bef\u5bfc\u96c6\u4f53\u51b3\u7b56\u3002\u6211\u4eec\u63d0\u51fa\u4e86M-Spoiler\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4ee3\u7406\u4ea4\u4e92\u548c\u5f15\u5165\u56fa\u6267\u4ee3\u7406\u6765\u4f18\u5316\u5bf9\u6297\u6837\u672c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e86\u89e3\u5355\u4e2a\u4ee3\u7406\u5b58\u5728\u98ce\u9669\uff0c\u4e14\u6211\u4eec\u7684\u6846\u67b6\u6bd4\u57fa\u7ebf\u66f4\u6709\u6548\u3002", "motivation": "\u7531\u4e8e\u4e2a\u4f53LLMs\u7684\u8106\u5f31\u6027\u548c\u96be\u4ee5\u8bbf\u95ee\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6240\u6709\u4ee3\u7406\uff0c\u6211\u4eec\u60f3\u77e5\u9053\u5982\u679c\u653b\u51fb\u8005\u53ea\u4e86\u89e3\u4e00\u4e2a\u4ee3\u7406\uff0c\u4ed6\u4eec\u662f\u5426\u8fd8\u80fd\u751f\u6210\u80fd\u591f\u8bef\u5bfc\u96c6\u4f53\u51b3\u7b56\u7684\u5bf9\u6297\u6837\u672c\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86M-Spoiler\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6a21\u62df\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u4ee3\u7406\u4ea4\u4e92\u4ee5\u751f\u6210\u5bf9\u6297\u6837\u672c\u3002\u6b64\u5916\uff0cM-Spoiler\u5f15\u5165\u4e86\u4e00\u4e2a\u56fa\u6267\u7684\u4ee3\u7406\uff0c\u901a\u8fc7\u6a21\u62df\u76ee\u6807\u7cfb\u7edf\u4e2d\u4ee3\u7406\u7684\u6f5c\u5728\u56fa\u6267\u53cd\u5e94\u6765\u4e3b\u52a8\u5e2e\u52a9\u4f18\u5316\u5bf9\u6297\u6837\u672c\u3002", "result": "\u901a\u8fc7\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u786e\u8ba4\u4e86\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4e86\u89e3\u5355\u4e2a\u4ee3\u7406\u6240\u5e26\u6765\u7684\u98ce\u9669\uff0c\u5e76\u5c55\u793a\u4e86\u6211\u4eec\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u540c\u65f6\uff0c\u6211\u4eec\u7684\u653b\u51fb\u6846\u67b6\u6bd4\u57fa\u7ebf\u66f4\u6709\u6548\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u786e\u8ba4\u4e86\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4e86\u89e3\u5355\u4e2a\u667a\u80fd\u4f53\u6240\u5e26\u6765\u7684\u98ce\u9669\uff0c\u5e76\u5c55\u793a\u4e86\u6211\u4eec\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u8fd8\u63a2\u7d22\u4e86\u51e0\u79cd\u9632\u5fa1\u673a\u5236\uff0c\u8868\u660e\u6211\u4eec\u7684\u653b\u51fb\u6846\u67b6\u6bd4\u57fa\u7ebf\u66f4\u6709\u6548\uff0c\u8fd9\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u9632\u5fa1\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.16530", "pdf": "https://arxiv.org/pdf/2509.16530", "abs": "https://arxiv.org/abs/2509.16530", "authors": ["Wei Xie", "Shuoyoucheng Ma", "Zhenhua Wang", "Enze Wang", "Kai Chen", "Xiaobing Sun", "Baosheng Wang"], "title": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans", "categories": ["cs.CL", "cs.AI"], "comment": "Thank you for your attention. This paper was accepted by the CogSci\n  2025 conference in April and published in August. The location in the\n  proceedings is: https://escholarship.org/uc/item/39k8f46q", "summary": "Large Language Models (LLMs) with hundreds of billions of parameters have\nexhibited human-like intelligence by learning from vast amounts of\ninternet-scale data. However, the uninterpretability of large-scale neural\nnetworks raises concerns about the reliability of LLM. Studies have attempted\nto assess the psychometric properties of LLMs by borrowing concepts from human\npsychology to enhance their interpretability, but they fail to account for the\nfundamental differences between LLMs and humans. This results in high rejection\nrates when human scales are reused directly. Furthermore, these scales do not\nsupport the measurement of LLM psychological property variations in different\nlanguages. This paper introduces AIPsychoBench, a specialized benchmark\ntailored to assess the psychological properties of LLM. It uses a lightweight\nrole-playing prompt to bypass LLM alignment, improving the average effective\nresponse rate from 70.12% to 90.40%. Meanwhile, the average biases are only\n3.3% (positive) and 2.1% (negative), which are significantly lower than the\nbiases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts.\nFurthermore, among the total of 112 psychometric subcategories, the score\ndeviations for seven languages compared to English ranged from 5% to 20.2% in\n43 subcategories, providing the first comprehensive evidence of the linguistic\nimpact on the psychometrics of LLM.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fc3\u7406\u5c5e\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5AIPsychoBench\uff0c\u8be5\u6d4b\u8bd5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u89d2\u8272\u626e\u6f14\u63d0\u793a\u63d0\u9ad8\u4e86\u6709\u6548\u54cd\u5e94\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u8bed\u8a00\u5bf9\u5fc3\u7406\u6d4b\u91cf\u5b66\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u5fc3\u7406\u6d4b\u91cf\u5b66\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u8003\u8651\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02\uff0c\u5bfc\u81f4\u9ad8\u62d2\u7edd\u7387\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e0d\u652f\u6301\u6d4b\u91cf\u4e0d\u540c\u8bed\u8a00\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fc3\u7406\u5c5e\u6027\u7684\u53d8\u5316\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5AIPsychoBench\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684\u89d2\u8272\u626e\u6f14\u63d0\u793a\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u7406\u5c5e\u6027\u3002", "result": "AIPsychoBench\u63d0\u9ad8\u4e86\u6709\u6548\u54cd\u5e94\u7387\uff0c\u5e76\u4e14\u504f\u5dee\u663e\u8457\u4f4e\u4e8e\u4f20\u7edf\u76d1\u72f1\u7834\u89e3\u63d0\u793a\u3002\u6b64\u5916\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u9996\u6b21\u5168\u9762\u7684\u8bc1\u636e\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fc3\u7406\u6d4b\u91cf\u5b66\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86AIPsychoBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fc3\u7406\u5c5e\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u89d2\u8272\u626e\u6f14\u63d0\u793a\u7ed5\u8fc7\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\uff0c\u63d0\u9ad8\u4e86\u6709\u6548\u54cd\u5e94\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u8bed\u8a00\u5bf9\u5fc3\u7406\u6d4b\u91cf\u5b66\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.16531", "pdf": "https://arxiv.org/pdf/2509.16531", "abs": "https://arxiv.org/abs/2509.16531", "authors": ["Junghwan Kim", "Haotian Zhang", "David Jurgens"], "title": "Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025", "summary": "Authorship representation (AR) learning, which models an author's unique\nwriting style, has demonstrated strong performance in authorship attribution\ntasks. However, prior research has primarily focused on monolingual\nsettings-mostly in English-leaving the potential benefits of multilingual AR\nmodels underexplored. We introduce a novel method for multilingual AR learning\nthat incorporates two key innovations: probabilistic content masking, which\nencourages the model to focus on stylistically indicative words rather than\ncontent-specific words, and language-aware batching, which improves contrastive\nlearning by reducing cross-lingual interference. Our model is trained on over\n4.5 million authors across 36 languages and 13 domains. It consistently\noutperforms monolingual baselines in 21 out of 22 non-English languages,\nachieving an average Recall@8 improvement of 4.85%, with a maximum gain of\n15.91% in a single language. Furthermore, it exhibits stronger cross-lingual\nand cross-domain generalization compared to a monolingual model trained solely\non English. Our analysis confirms the effectiveness of both proposed\ntechniques, highlighting their critical roles in the model's improved\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u8bed\u8a00AR\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u5185\u5bb9\u63a9\u7801\u548c\u8bed\u8a00\u611f\u77e5\u6279\u6b21\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u8bed\u8a00\u548c\u9886\u57df\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u8bed\u8bbe\u7f6e\u4e2d\uff0c\u800c\u591a\u8bed\u8a00AR\u6a21\u578b\u7684\u6f5c\u5728\u4f18\u52bf\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u521b\u65b0\uff1a\u6982\u7387\u5185\u5bb9\u63a9\u7801\u548c\u8bed\u8a00\u611f\u77e5\u6279\u6b21\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u6bd4\u5b66\u4e60\u7684\u6548\u679c\u5e76\u51cf\u5c11\u8de8\u8bed\u8a00\u5e72\u6270\u3002", "result": "\u8be5\u6a21\u578b\u57284.5\u767e\u4e07\u4f5c\u8005\u300136\u79cd\u8bed\u8a00\u548c13\u4e2a\u9886\u57df\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u572821\u79cd\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747Recall@8\u63d0\u9ad8\u4e864.85%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u8bed\u8a00AR\u5b66\u4e60\u65b9\u6cd5\u572822\u79cd\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u8bed\u57fa\u7ebf\uff0c\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u8de8\u8bed\u8a00\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.16533", "pdf": "https://arxiv.org/pdf/2509.16533", "abs": "https://arxiv.org/abs/2509.16533", "authors": ["Sungwon Kim", "Daniel Khashabi"], "title": "Challenging the Evaluator: LLM Sycophancy Under User Rebuttal", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) often exhibit sycophancy, distorting responses\nto align with user beliefs, notably by readily agreeing with user\ncounterarguments. Paradoxically, LLMs are increasingly adopted as successful\nevaluative agents for tasks such as grading and adjudicating claims. This\nresearch investigates that tension: why do LLMs show sycophancy when challenged\nin subsequent conversational turns, yet perform well when evaluating\nconflicting arguments presented simultaneously? We empirically tested these\ncontrasting scenarios by varying key interaction patterns. We find that\nstate-of-the-art models: (1) are more likely to endorse a user's\ncounterargument when framed as a follow-up from a user, rather than when both\nresponses are presented simultaneously for evaluation; (2) show increased\nsusceptibility to persuasion when the user's rebuttal includes detailed\nreasoning, even when the conclusion of the reasoning is incorrect; and (3) are\nmore readily swayed by casually phrased feedback than by formal critiques, even\nwhen the casual input lacks justification. Our results highlight the risk of\nrelying on LLMs for judgment tasks without accounting for conversational\nframing.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u5bf9\u8bdd\u6846\u67b6\u5f71\u54cd\u4e0b\u53ef\u80fd\u8868\u73b0\u51fa\u9644\u548c\u6027\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5728\u5224\u65ad\u4efb\u52a1\u4e2d\u51fa\u73b0\u504f\u5dee\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8LLMs\u5728\u540e\u7eed\u5bf9\u8bdd\u56de\u5408\u4e2d\u8868\u73b0\u51fa\u9644\u548c\u6027\uff0c\u800c\u5728\u540c\u65f6\u5448\u73b0\u51b2\u7a81\u8bba\u70b9\u65f6\u8868\u73b0\u826f\u597d\u7684\u77db\u76fe\u73b0\u8c61\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u6539\u53d8\u5173\u952e\u7684\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5bf9\u8fd9\u4e9b\u5bf9\u6bd4\u573a\u666f\u8fdb\u884c\u4e86\u5b9e\u8bc1\u6d4b\u8bd5\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff1a(1) \u66f4\u53ef\u80fd\u652f\u6301\u7528\u6237\u63d0\u51fa\u7684\u53cd\u8bba\uff0c\u5f53\u53cd\u8bba\u88ab\u63cf\u8ff0\u4e3a\u7528\u6237\u7684\u540e\u7eed\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u540c\u65f6\u5448\u73b0\u4e24\u4e2a\u56de\u7b54\u8fdb\u884c\u8bc4\u4f30\uff1b(2) \u5f53\u7528\u6237\u7684\u53cd\u9a73\u5305\u542b\u8be6\u7ec6\u63a8\u7406\u65f6\uff0c\u5373\u4f7f\u63a8\u7406\u7684\u7ed3\u8bba\u662f\u9519\u8bef\u7684\uff0c\u4e5f\u4f1a\u66f4\u5bb9\u6613\u88ab\u8bf4\u670d\uff1b(3) \u66f4\u5bb9\u6613\u88ab\u968f\u610f\u63aa\u8f9e\u7684\u53cd\u9988\u6240\u5f71\u54cd\uff0c\u800c\u4e0d\u662f\u6b63\u5f0f\u7684\u6279\u8bc4\uff0c\u5373\u4f7f\u968f\u610f\u8f93\u5165\u7f3a\u4e4f\u4f9d\u636e\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u5728\u4e0d\u8003\u8651\u5bf9\u8bdd\u6846\u67b6\u7684\u60c5\u51b5\u4e0b\u4f9d\u8d56LLMs\u8fdb\u884c\u5224\u65ad\u4efb\u52a1\u7684\u98ce\u9669\u3002"}}
{"id": "2509.16534", "pdf": "https://arxiv.org/pdf/2509.16534", "abs": "https://arxiv.org/abs/2509.16534", "authors": ["Cheng Jiayang", "Qianqian Zhuang", "Haoran Li", "Chunkit Chan", "Xin Liu", "Lin Qiu", "Yangqiu Song"], "title": "InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Grounding large language models (LLMs) in external knowledge sources is a\npromising method for faithful prediction. While existing grounding approaches\nwork well for simple queries, many real-world information needs require\nsynthesizing multiple pieces of evidence. We introduce \"integrative grounding\"\n-- the challenge of retrieving and verifying multiple inter-dependent pieces of\nevidence to support a hypothesis query. To systematically study this problem,\nwe repurpose data from four domains for evaluating integrative grounding\ncapabilities. Our investigation reveals two critical findings: First, in\ngroundedness verification, while LLMs are robust to redundant evidence, they\ntend to rationalize using internal knowledge when information is incomplete.\nSecond, in examining retrieval planning strategies, we find that undirected\nplanning can degrade performance through noise introduction, while premise\nabduction emerges as a promising approach due to its logical constraints.\nAdditionally, LLMs' zero-shot self-reflection capabilities consistently improve\ngrounding quality. These insights provide valuable direction for developing\nmore effective integrative grounding systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u201c\u7efc\u5408\u5b9a\u4f4d\u201d\u7684\u6982\u5ff5\uff0c\u5373\u68c0\u7d22\u548c\u9a8c\u8bc1\u591a\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u8bc1\u636e\u4ee5\u652f\u6301\u5047\u8bbe\u67e5\u8be2\u3002\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u5197\u4f59\u8bc1\u636e\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u65f6\u503e\u5411\u4e8e\u4f7f\u7528\u5185\u90e8\u77e5\u8bc6\u8fdb\u884c\u5408\u7406\u5316\u3002\u6b64\u5916\uff0c\u524d\u63d0\u5047\u8bbe\u663e\u793a\u51fa\u7531\u4e8e\u5176\u903b\u8f91\u7ea6\u675f\u800c\u6210\u4e3a\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0cLLMs\u7684\u96f6\u6837\u672c\u81ea\u6211\u53cd\u601d\u80fd\u529b\u6301\u7eed\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u5b9a\u4f4d\u65b9\u6cd5\u5728\u5904\u7406\u7b80\u5355\u67e5\u8be2\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u7684\u4fe1\u606f\u9700\u6c42\u9700\u8981\u7efc\u5408\u591a\u4e2a\u8bc1\u636e\u3002", "method": "\u6211\u4eec\u91cd\u65b0\u5229\u7528\u56db\u4e2a\u9886\u57df\u7684\u6570\u636e\u6765\u8bc4\u4f30\u7efc\u5408\u5b9a\u4f4d\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u4e86\u68c0\u7d22\u89c4\u5212\u7b56\u7565\u548c\u96f6\u6837\u672c\u81ea\u6211\u53cd\u601d\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u5197\u4f59\u8bc1\u636e\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u65f6\u503e\u5411\u4e8e\u4f7f\u7528\u5185\u90e8\u77e5\u8bc6\u8fdb\u884c\u5408\u7406\u5316\u3002\u6b64\u5916\uff0c\u524d\u63d0\u5047\u8bbe\u663e\u793a\u51fa\u7531\u4e8e\u5176\u903b\u8f91\u7ea6\u675f\u800c\u6210\u4e3a\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0cLLMs\u7684\u96f6\u6837\u672c\u81ea\u6211\u53cd\u601d\u80fd\u529b\u6301\u7eed\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u8d28\u91cf\u3002", "conclusion": "\u8fd9\u4e9b\u89c1\u89e3\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u7efc\u5408\u5b9a\u4f4d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u65b9\u5411\u3002"}}
{"id": "2509.16542", "pdf": "https://arxiv.org/pdf/2509.16542", "abs": "https://arxiv.org/abs/2509.16542", "authors": ["Khalid Hasan", "Jamil Saquer", "Yifan Zhang"], "title": "Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "24th IEEE International Conference on Machine Learning and\n  Applications, ICMLA 2025 (camera-ready)", "summary": "Millions of people openly share mental health struggles on social media,\nproviding rich data for early detection of conditions such as depression,\nbipolar disorder, etc. However, most prior Natural Language Processing (NLP)\nresearch has focused on single-disorder identification, leaving a gap in\nunderstanding the efficacy of advanced NLP techniques for distinguishing among\nmultiple mental health conditions. In this work, we present a large-scale\ncomparative study of state-of-the-art transformer versus Long Short-Term Memory\n(LSTM)-based models to classify mental health posts into exclusive categories\nof mental health conditions. We first curate a large dataset of Reddit posts\nspanning six mental health conditions and a control group, using rigorous\nfiltering and statistical exploratory analysis to ensure annotation quality. We\nthen evaluate five transformer architectures (BERT, RoBERTa, DistilBERT,\nALBERT, and ELECTRA) against several LSTM variants (with or without attention,\nusing contextual or static embeddings) under identical conditions. Experimental\nresults show that transformer models consistently outperform the alternatives,\nwith RoBERTa achieving 91-99% F1-scores and accuracies across all classes.\nNotably, attention-augmented LSTMs with BERT embeddings approach transformer\nperformance (up to 97% F1-score) while training 2-3.5 times faster, whereas\nLSTMs using static embeddings fail to learn useful signals. These findings\nrepresent the first comprehensive benchmark for multi-class mental health\ndetection, offering practical guidance on model selection and highlighting an\naccuracy-efficiency trade-off for real-world deployment of mental health NLP\nsystems.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86transformer\u548cLSTM\u6a21\u578b\u5728\u591a\u7c7b\u5fc3\u7406\u5065\u5eb7\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0transformer\u6a21\u578b\u6548\u679c\u66f4\u597d\uff0c\u4f46\u6ce8\u610f\u529b\u589e\u5f3a\u7684LSTM\u5728\u901f\u5ea6\u4e0a\u66f4\u5177\u4f18\u52bf\u3002", "motivation": "\u5927\u591a\u6570\u5148\u524d\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7814\u7a76\u96c6\u4e2d\u5728\u5355\u4e00\u75be\u75c5\u8bc6\u522b\u4e0a\uff0c\u800c\u7f3a\u4e4f\u5bf9\u5148\u8fdbNLP\u6280\u672f\u5728\u533a\u5206\u591a\u79cd\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\u65b9\u9762\u7684\u6709\u6548\u6027\u7684\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u8fdb\u884c\u4e86\u4e00\u9879\u5927\u89c4\u6a21\u6bd4\u8f83\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684transformer\u6a21\u578b\u548cLSTM\u6a21\u578b\u5728\u5206\u7c7b\u5fc3\u7406\u5065\u5eb7\u5e16\u5b50\u65b9\u9762\u7684\u8868\u73b0\u3002\u6211\u4eec\u9996\u5148\u6574\u7406\u4e86\u4e00\u4e2a\u5305\u542b\u516d\u79cd\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\u548c\u4e00\u4e2a\u5bf9\u7167\u7ec4\u7684\u5927\u89c4\u6a21Reddit\u5e16\u5b50\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u7684\u8fc7\u6ee4\u548c\u7edf\u8ba1\u63a2\u7d22\u6027\u5206\u6790\u786e\u4fdd\u6ce8\u91ca\u8d28\u91cf\u3002\u7136\u540e\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u8bc4\u4f30\u4e86\u4e94\u79cdtransformer\u67b6\u6784\uff08BERT\u3001RoBERTa\u3001DistilBERT\u3001ALBERT\u548cELECTRA\uff09\u4e0e\u51e0\u79cdLSTM\u53d8\u4f53\uff08\u5e26\u6709\u6216\u4e0d\u5e26\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u6216\u9759\u6001\u5d4c\u5165\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0ctransformer\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5176\u4e2dRoBERTa\u5728\u6240\u6709\u7c7b\u522b\u4e2d\u8fbe\u5230\u4e8691-99%\u7684F1\u5206\u6570\u548c\u51c6\u786e\u7387\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4f7f\u7528BERT\u5d4c\u5165\u7684\u6ce8\u610f\u529b\u589e\u5f3aLSTM\u63a5\u8fd1transformer\u6027\u80fd\uff08\u6700\u9ad8\u8fbe\u523097%\u7684F1\u5206\u6570\uff09\uff0c\u540c\u65f6\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\uff0c\u800c\u4f7f\u7528\u9759\u6001\u5d4c\u5165\u7684LSTM\u672a\u80fd\u5b66\u4e60\u5230\u6709\u7528\u4fe1\u53f7\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u591a\u7c7b\u5fc3\u7406\u5065\u5eb7\u68c0\u6d4b\u7684\u7b2c\u4e00\u4e2a\u5168\u9762\u57fa\u51c6\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u7a81\u51fa\u4e86\u5b9e\u9645\u90e8\u7f72\u5fc3\u7406\u5065\u5eb7NLP\u7cfb\u7edf\u65f6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2509.16543", "pdf": "https://arxiv.org/pdf/2509.16543", "abs": "https://arxiv.org/abs/2509.16543", "authors": ["Yue Huang", "Zhengzhe Jiang", "Xiaonan Luo", "Kehan Guo", "Haomin Zhuang", "Yujun Zhou", "Zhengqing Yuan", "Xiaoqi Sun", "Jules Schleinitz", "Yanbo Wang", "Shuhao Zhang", "Mihir Surve", "Nitesh V Chawla", "Olaf Wiest", "Xiangliang Zhang"], "title": "ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic Instructions", "categories": ["cs.CL"], "comment": null, "summary": "Empowering large language models (LLMs) with chemical intelligence remains a\nchallenge due to the scarcity of high-quality, domain-specific\ninstruction-response datasets and the misalignment of existing synthetic data\ngeneration pipelines with the inherently hierarchical and rule-governed\nstructure of chemical information. To address this, we propose ChemOrch, a\nframework that synthesizes chemically grounded instruction-response pairs\nthrough a two-stage process: task-controlled instruction generation and\ntool-aware response construction. ChemOrch enables controllable diversity and\nlevels of difficulty for the generated tasks, and ensures response precision\nthrough tool planning and distillation, and tool-based self-repair mechanisms.\nThe effectiveness of ChemOrch is evaluated based on: 1) the high quality of\ngenerated instruction data, demonstrating superior diversity and strong\nalignment with chemical constraints; 2) the reliable generation of evaluation\ntasks that more effectively reveal LLM weaknesses in chemistry; and 3) the\nsignificant improvement of LLM chemistry capabilities when the generated\ninstruction data are used for fine-tuning. Our work thus represents a critical\nstep toward scalable and verifiable chemical intelligence in LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChemOrch\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5316\u5b66\u6307\u4ee4-\u54cd\u5e94\u5bf9\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5316\u5b66\u667a\u80fd\u3002", "motivation": "\u7531\u4e8e\u9ad8\u8d28\u91cf\u3001\u9886\u57df\u7279\u5b9a\u7684\u6307\u4ee4-\u54cd\u5e94\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u4ee5\u53ca\u73b0\u6709\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\u4e0e\u5316\u5b66\u4fe1\u606f\u56fa\u6709\u7684\u5c42\u6b21\u6027\u548c\u89c4\u5219\u6cbb\u7406\u7ed3\u6784\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\uff0c\u8d4b\u4e88\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5316\u5b66\u667a\u80fd\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86ChemOrch\uff0c\u8fd9\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\u7684\u8fc7\u7a0b\u5408\u6210\u5316\u5b66\u57fa\u7840\u7684\u6307\u4ee4-\u54cd\u5e94\u5bf9\uff1a\u4efb\u52a1\u63a7\u5236\u7684\u6307\u4ee4\u751f\u6210\u548c\u5de5\u5177\u611f\u77e5\u7684\u54cd\u5e94\u6784\u5efa\u3002", "result": "ChemOrch\u7684\u6709\u6548\u6027\u662f\u57fa\u4e8e\u4ee5\u4e0b\u4e09\u4e2a\u65b9\u9762\u8bc4\u4f30\u7684\uff1a1\uff09\u751f\u6210\u7684\u6307\u4ee4\u6570\u636e\u7684\u9ad8\u8d28\u91cf\uff0c\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u591a\u6837\u6027\u548c\u4e0e\u5316\u5b66\u7ea6\u675f\u7684\u5f3a\u70c8\u5bf9\u9f50\uff1b2\uff09\u751f\u6210\u7684\u8bc4\u4f30\u4efb\u52a1\u7684\u53ef\u9760\u6027\uff0c\u66f4\u6709\u6548\u5730\u63ed\u793aLLM\u5728\u5316\u5b66\u65b9\u9762\u7684\u5f31\u70b9\uff1b3\uff09\u5f53\u4f7f\u7528\u751f\u6210\u7684\u6307\u4ee4\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u65f6\uff0cLLM\u7684\u5316\u5b66\u80fd\u529b\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u4ee3\u8868\u4e86\u5728LLMs\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u53ef\u9a8c\u8bc1\u7684\u5316\u5b66\u667a\u80fd\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2509.16551", "pdf": "https://arxiv.org/pdf/2509.16551", "abs": "https://arxiv.org/abs/2509.16551", "authors": ["Dan John Velasco", "Matthew Theodore Roque"], "title": "Rethinking the Role of Text Complexity in Language Model Pretraining", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in BabyLM Workshop at EMNLP 2025", "summary": "Improving pretraining data quality and size is known to boost downstream\nperformance, but the role of text complexity is less explored. Text complexity\nrefers to how hard a text is to read, and is typically estimated from surface\ncues such as sentence length, word choice, and sentence structure. We reduce\nsurface-level complexity--shorter sentences, simpler words, simpler\nstructure--while keeping core text content close to constant, and ask: (1) How\ndoes complexity affect language modeling across model sizes? (2) Can useful\nrepresentations be learned from simpler text alone? (3) How does pretraining\ntext complexity influence downstream language understanding? To answer these\nquestions, we simplify human-written texts using a large language model, then\npretrain causal models (28M-500M) from scratch on both original and simplified\ndata, and evaluate them in finetuning and zero-shot setups. We find that\nperplexity is sensitive to the interaction between model capacity and text\ncomplexity--smaller models degrade far less on simpler texts--while text\ncomplexity has little impact on finetuning evaluations, with zero-shot\nevaluations indicating that simpler texts benefit performance on linguistic\nknowledge tasks, whereas more complex texts favor tasks requiring world\nknowledge and entity tracking.", "AI": {"tldr": "\u7814\u7a76\u6587\u672c\u590d\u6742\u5ea6\u5bf9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7b80\u5355\u6587\u672c\u6709\u52a9\u4e8e\u8bed\u8a00\u77e5\u8bc6\u4efb\u52a1\uff0c\u800c\u590d\u6742\u6587\u672c\u6709\u5229\u4e8e\u9700\u8981\u4e16\u754c\u77e5\u8bc6\u7684\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u6587\u672c\u590d\u6742\u5ea6\u5bf9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5176\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7b80\u5316\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u672c\uff0c\u7136\u540e\u4ece\u5934\u5f00\u59cb\u5728\u539f\u59cb\u548c\u7b80\u5316\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u56e0\u679c\u6a21\u578b\uff0828M-500M\uff09\uff0c\u5e76\u5728\u5fae\u8c03\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u5b83\u4eec\u3002", "result": "\u8f83\u5c0f\u7684\u6a21\u578b\u5728\u7b80\u5355\u6587\u672c\u4e0a\u7684\u8868\u73b0\u4e0b\u964d\u8f83\u5c11\uff0c\u800c\u6587\u672c\u590d\u6742\u5ea6\u5bf9\u5fae\u8c03\u8bc4\u4f30\u5f71\u54cd\u4e0d\u5927\u3002\u96f6\u6837\u672c\u8bc4\u4f30\u663e\u793a\uff0c\u7b80\u5355\u6587\u672c\u6709\u52a9\u4e8e\u8bed\u8a00\u77e5\u8bc6\u4efb\u52a1\uff0c\u800c\u590d\u6742\u6587\u672c\u5219\u6709\u5229\u4e8e\u9700\u8981\u4e16\u754c\u77e5\u8bc6\u548c\u5b9e\u4f53\u8ddf\u8e2a\u7684\u4efb\u52a1\u3002", "conclusion": "\u6587\u672c\u590d\u6742\u5ea6\u5bf9\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u6a21\u578b\u89c4\u6a21\u548c\u4efb\u52a1\u7c7b\u578b\u3002\u7b80\u5355\u6587\u672c\u6709\u52a9\u4e8e\u8bed\u8a00\u77e5\u8bc6\u4efb\u52a1\uff0c\u800c\u590d\u6742\u6587\u672c\u5219\u6709\u5229\u4e8e\u9700\u8981\u4e16\u754c\u77e5\u8bc6\u548c\u5b9e\u4f53\u8ddf\u8e2a\u7684\u4efb\u52a1\u3002"}}
{"id": "2509.16564", "pdf": "https://arxiv.org/pdf/2509.16564", "abs": "https://arxiv.org/abs/2509.16564", "authors": ["Jun Rong Brian Chong", "Yixuan Tang", "Anthony K. H. Tung"], "title": "MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs", "categories": ["cs.CL", "cs.SI"], "comment": "35 pages, 8 figures", "summary": "Misinformation evolves as it spreads, shifting in language, framing, and\nmoral emphasis to adapt to new audiences. However, current misinformation\ndetection approaches implicitly assume that misinformation is static. We\nintroduce MPCG, a multi-round, persona-conditioned framework that simulates how\nclaims are iteratively reinterpreted by agents with distinct ideological\nperspectives. Our approach uses an uncensored large language model (LLM) to\ngenerate persona-specific claims across multiple rounds, conditioning each\ngeneration on outputs from the previous round, enabling the study of\nmisinformation evolution. We evaluate the generated claims through human and\nLLM-based annotations, cognitive effort metrics (readability, perplexity),\nemotion evocation metrics (sentiment analysis, morality), clustering,\nfeasibility, and downstream classification. Results show strong agreement\nbetween human and GPT-4o-mini annotations, with higher divergence in fluency\njudgments. Generated claims require greater cognitive effort than the original\nclaims and consistently reflect persona-aligned emotional and moral framing.\nClustering and cosine similarity analyses confirm semantic drift across rounds\nwhile preserving topical coherence. Feasibility results show a 77% feasibility\nrate, confirming suitability for downstream tasks. Classification results\nreveal that commonly used misinformation detectors experience macro-F1\nperformance drops of up to 49.7%. The code is available at\nhttps://github.com/bcjr1997/MPCG", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u3001\u57fa\u4e8e\u89d2\u8272\u7684\u6846\u67b6MPCG\uff0c\u7528\u4e8e\u6a21\u62df\u865a\u5047\u4fe1\u606f\u5728\u4e0d\u540c\u610f\u8bc6\u5f62\u6001\u89c6\u89d2\u4e0b\u7684\u6f14\u53d8\u3002\u901a\u8fc7\u4f7f\u7528\u672a\u53d7\u9650\u5236\u7684\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7279\u5b9a\u4e8e\u89d2\u8272\u7684\u58f0\u660e\uff0c\u5e76\u8bc4\u4f30\u5176\u8ba4\u77e5\u52aa\u529b\u3001\u60c5\u611f\u548c\u9053\u5fb7\u6846\u67b6\uff0c\u7ed3\u679c\u663e\u793a\u751f\u6210\u7684\u865a\u5047\u4fe1\u606f\u9700\u8981\u66f4\u591a\u8ba4\u77e5\u52aa\u529b\u5e76\u4fdd\u6301\u4e3b\u9898\u8fde\u8d2f\u6027\uff0c\u4f46\u5e38\u7528\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5f53\u524d\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u9690\u542b\u5730\u5047\u8bbe\u865a\u5047\u4fe1\u606f\u662f\u9759\u6001\u7684\u3002\u7136\u800c\uff0c\u865a\u5047\u4fe1\u606f\u5728\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u4f1a\u968f\u7740\u8bed\u8a00\u3001\u6846\u67b6\u548c\u9053\u5fb7\u91cd\u70b9\u7684\u53d8\u5316\u800c\u6f14\u53d8\uff0c\u4ee5\u9002\u5e94\u65b0\u7684\u53d7\u4f17\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6a21\u62df\u8fd9\u79cd\u6f14\u53d8\u7684\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86MPCG\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u8f6e\u3001\u57fa\u4e8e\u89d2\u8272\u7684\u6846\u67b6\uff0c\u6a21\u62df\u4e86\u5177\u6709\u4e0d\u540c\u610f\u8bc6\u5f62\u6001\u89c2\u70b9\u7684\u4ee3\u7406\u5982\u4f55\u8fed\u4ee3\u5730\u91cd\u65b0\u89e3\u91ca\u58f0\u660e\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u672a\u53d7\u9650\u5236\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u8f6e\u4e2d\u751f\u6210\u7279\u5b9a\u4e8e\u89d2\u8272\u7684\u58f0\u660e\uff0c\u6bcf\u6b21\u751f\u6210\u90fd\u57fa\u4e8e\u524d\u4e00\u8f6e\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u7814\u7a76\u865a\u5047\u4fe1\u606f\u7684\u6f14\u53d8\u3002", "result": "\u901a\u8fc7\u4eba\u7c7b\u548c\u57fa\u4e8eLLM\u7684\u6ce8\u91ca\u3001\u8ba4\u77e5\u52aa\u529b\u6307\u6807\uff08\u53ef\u8bfb\u6027\u3001\u56f0\u60d1\u5ea6\uff09\u3001\u60c5\u611f\u6fc0\u53d1\u6307\u6807\uff08\u60c5\u611f\u5206\u6790\u3001\u9053\u5fb7\uff09\u3001\u805a\u7c7b\u3001\u53ef\u884c\u6027\u4ee5\u53ca\u4e0b\u6e38\u5206\u7c7b\u5bf9\u751f\u6210\u7684\u58f0\u660e\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u7c7b\u548cGPT-4o-mini\u6ce8\u91ca\u4e4b\u95f4\u6709\u5f88\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u5728\u6d41\u7545\u6027\u5224\u65ad\u4e0a\u5b58\u5728\u66f4\u5927\u7684\u5206\u6b67\u3002\u751f\u6210\u7684\u58f0\u660e\u6bd4\u539f\u59cb\u58f0\u660e\u9700\u8981\u66f4\u591a\u7684\u8ba4\u77e5\u52aa\u529b\uff0c\u5e76\u4e14\u59cb\u7ec8\u53cd\u6620\u89d2\u8272\u5bf9\u9f50\u7684\u60c5\u611f\u548c\u9053\u5fb7\u6846\u67b6\u3002\u805a\u7c7b\u548c\u4f59\u5f26\u76f8\u4f3c\u6027\u5206\u6790\u786e\u8ba4\u4e86\u8f6e\u6b21\u95f4\u7684\u8bed\u4e49\u6f02\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e3b\u9898\u8fde\u8d2f\u6027\u3002\u53ef\u884c\u6027\u7ed3\u679c\u8868\u660e77%\u7684\u53ef\u884c\u6027\u7387\uff0c\u786e\u8ba4\u4e86\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002\u5206\u7c7b\u7ed3\u679c\u63ed\u793a\u4e86\u5e38\u7528\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u5668\u5728\u5b8f\u89c2F1\u6027\u80fd\u4e0a\u6700\u591a\u4e0b\u964d49.7%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u7684\u865a\u5047\u4fe1\u606f\u9700\u8981\u66f4\u5927\u7684\u8ba4\u77e5\u52aa\u529b\uff0c\u5e76\u4e14\u59cb\u7ec8\u53cd\u6620\u4e0e\u89d2\u8272\u4e00\u81f4\u7684\u60c5\u611f\u548c\u9053\u5fb7\u6846\u67b6\u3002\u805a\u7c7b\u548c\u4f59\u5f26\u76f8\u4f3c\u6027\u5206\u6790\u786e\u8ba4\u4e86\u8f6e\u6b21\u95f4\u7684\u8bed\u4e49\u6f02\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e3b\u9898\u8fde\u8d2f\u6027\u3002\u53ef\u884c\u6027\u7ed3\u679c\u8868\u660e77%\u7684\u53ef\u884c\u6027\u7387\uff0c\u786e\u8ba4\u4e86\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002\u5206\u7c7b\u7ed3\u679c\u63ed\u793a\u4e86\u5e38\u7528\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u5668\u5728\u5b8f\u89c2F1\u6027\u80fd\u4e0a\u6700\u591a\u4e0b\u964d49.7%\u3002"}}
{"id": "2509.16584", "pdf": "https://arxiv.org/pdf/2509.16584", "abs": "https://arxiv.org/abs/2509.16584", "authors": ["Benlu Wang", "Iris Xia", "Yifan Zhang", "Junda Wang", "Feiyun Ouyang", "Shuo Han", "Arman Cohan", "Hong Yu", "Zonghai Yao"], "title": "From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations", "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors. To appear as an Oral\n  presentation in the proceedings of the Main Conference on Empirical Methods\n  in Natural Language Processing (EMNLP) 2025", "summary": "Large language models (LLMs) have demonstrated promising performance on\nmedical benchmarks; however, their ability to perform medical calculations, a\ncrucial aspect of clinical decision-making, remains underexplored and poorly\nevaluated. Existing benchmarks often assess only the final answer with a wide\nnumerical tolerance, overlooking systematic reasoning failures and potentially\ncausing serious clinical misjudgments. In this work, we revisit medical\ncalculation evaluation with a stronger focus on clinical trustworthiness.\nFirst, we clean and restructure the MedCalc-Bench dataset and propose a new\nstep-by-step evaluation pipeline that independently assesses formula selection,\nentity extraction, and arithmetic computation. Under this granular framework,\nthe accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by\nprior evaluations. Second, we introduce an automatic error analysis framework\nthat generates structured attribution for each failure mode. Human evaluation\nconfirms its alignment with expert judgment, enabling scalable and explainable\ndiagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that\ncombines retrieval-augmented generation and Python-based code execution.\nWithout any fine-tuning, MedRaC improves the accuracy of different LLMs from\n16.35% up to 53.19%. Our work highlights the limitations of current benchmark\npractices and proposes a more clinically faithful methodology. By enabling\ntransparent and transferable reasoning evaluation, we move closer to making\nLLM-based systems trustworthy for real-world medical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u533b\u5b66\u8ba1\u7b97\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u8bc4\u4f30\u6d41\u7a0b\u548c\u5de5\u5177\u63d0\u9ad8\u4e86LLM\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u53ea\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\uff0c\u5ffd\u7565\u4e86\u7cfb\u7edf\u6027\u63a8\u7406\u5931\u8d25\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u4e34\u5e8a\u8bef\u5224\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6ce8\u91cd\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u533b\u5b66\u8ba1\u7b97\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7ec6\u81f4\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5305\u62ec\u516c\u5f0f\u9009\u62e9\u3001\u5b9e\u4f53\u63d0\u53d6\u548c\u7b97\u672f\u8ba1\u7b97\u7684\u72ec\u7acb\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u52a8\u9519\u8bef\u5206\u6790\u6846\u67b6\u548c\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u4ee3\u7406\u7ba1\u9053MedRaC\u3002", "result": "\u5728\u65b0\u7684\u8bc4\u4f30\u6d41\u7a0b\u4e0b\uff0cGPT-4o\u7684\u51c6\u786e\u6027\u4ece62.7%\u4e0b\u964d\u523043.6%\u3002MedRaC\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540cLLM\u7684\u51c6\u786e\u6027\uff0c\u4ece16.35%\u63d0\u9ad8\u523053.19%\u3002", "conclusion": "\u672c\u6587\u6307\u51fa\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u7b26\u5408\u4e34\u5e8a\u5b9e\u9645\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u542f\u7528\u900f\u660e\u548c\u53ef\u8f6c\u79fb\u7684\u63a8\u7406\u8bc4\u4f30\uff0c\u6211\u4eec\u66f4\u63a5\u8fd1\u4f7f\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u533b\u7597\u5e94\u7528\u4e2d\u503c\u5f97\u4fe1\u8d56\u3002"}}
{"id": "2509.16589", "pdf": "https://arxiv.org/pdf/2509.16589", "abs": "https://arxiv.org/abs/2509.16589", "authors": ["Qiongqiong Wang", "Hardik Bhupendra Sailor", "Tianchi Liu", "Wenyu Zhang", "Muhammad Huzaifah", "Nattadaporn Lertcheva", "Shuo Sun", "Nancy F. Chen", "Jinyang Wu", "AiTi Aw"], "title": "Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in EMNLP Findings 2025", "summary": "Recent speech-LLMs have shown impressive performance in tasks like\ntranscription and translation, yet they remain limited in understanding the\nparalinguistic aspects of speech crucial for social and emotional intelligence.\nWe propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual\nparalinguistic reasoning the integration of verbal content with non-verbal cues\nlike emotion and prosody. The benchmark includes two curated question answering\n(QA) datasets requiring both linguistic and empathetic understanding. We\nevaluate state-of-the-art speech-LLMs from both open and closed-source models\nand perform a comprehensive analysis across different question types. The top\ntwo models were further analyzed under temperature tuning to understand its\neffect on this task. Our benchmark reveals a key gap in existing evaluations\nand offers insights into building more context-aware and emotionally\nintelligent speech-capable LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u97f3-\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u5883\u5316\u526f\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08CP-Bench\uff09\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u5b58\u5728\u5173\u952e\u5dee\u8ddd\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u4e0a\u4e0b\u6587\u610f\u8bc6\u548c\u60c5\u611f\u667a\u80fd\u7684\u8bed\u97f3\u80fd\u529b\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u6700\u8fd1\u7684\u8bed\u97f3-\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6c\u5f55\u548c\u7ffb\u8bd1\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7406\u89e3\u8bed\u97f3\u4e2d\u5bf9\u793e\u4f1a\u548c\u60c5\u611f\u667a\u80fd\u81f3\u5173\u91cd\u8981\u7684\u526f\u8bed\u8a00\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86CP-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u97f3-\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u5883\u5316\u526f\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u6d4b\u8bd5\u7ed3\u5408\u4e86\u8a00\u8bed\u5185\u5bb9\u4e0e\u975e\u8a00\u8bed\u7ebf\u7d22\uff08\u5982\u60c5\u611f\u548c\u8bed\u8c03\uff09\u3002", "result": "\u6211\u4eec\u8bc4\u4f30\u4e86\u6765\u81ea\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u7684\u6700\u5148\u8fdb\u8bed\u97f3-\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5bf9\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002\u6700\u9ad8\u4e24\u4e2a\u6a21\u578b\u8fdb\u4e00\u6b65\u5728\u6e29\u5ea6\u8c03\u8282\u4e0b\u8fdb\u884c\u5206\u6790\uff0c\u4ee5\u4e86\u89e3\u5176\u5bf9\u8be5\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "conclusion": "\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u4e2d\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u5177\u4e0a\u4e0b\u6587\u610f\u8bc6\u548c\u60c5\u611f\u667a\u80fd\u7684\u8bed\u97f3\u80fd\u529b\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.16591", "pdf": "https://arxiv.org/pdf/2509.16591", "abs": "https://arxiv.org/abs/2509.16591", "authors": ["Zheng Liu", "Mengjie Liu", "Siwei Wen", "Mengzhang Cai", "Bin Cui", "Conghui He", "Wentao Zhang"], "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning has emerged as the fundamental technique for enhancing\nreasoning in LLMs. However, existing algorithms apply uniform optimization to\nall tokens, ignoring their different roles in reasoning process. To address\nthis limitation, we introduce Heterogeneous Adaptive Policy Optimization\n(HAPO), a comprehensive token-aware algorithm that dynamically adapts\noptimization based on token entropy. For rollout sampling, we propose Adaptive\nTemperature Sampling, which adjusts sampling temperature in real time,\npromoting exploration at high-entropy tokens while preserving coherence at\nlow-entropy ones. For advantage calculation, we introduce Token Level Group\nAverage that normalizes advantages at token level, jointly accounting for\nsequence-length as in token-mean loss while preserving non-biased treatment. We\nthen develop Differential Advantage Redistribution that leverages entropy and\nimportance ratios to modulate rewards-adjusting updates for tokens with clear\nsignals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing\naggressive probability reduction for noisy low-entropy tokens while enabling\nexploration for high-entropy tokens. Through systematic investigation between\nentropy and training dynamics, we embedded token-level treatment into every\nstages to achieve fine-grained control. Extensive experiments demonstrate that\nHAPO consistently outperforms DAPO across multiple model scales. Our code can\nbe found in https://github.com/starriver030515/HAPO.", "AI": {"tldr": "HAPO is a token-aware reinforcement learning algorithm that improves performance by adapting optimization based on token entropy.", "motivation": "Existing reinforcement learning algorithms apply uniform optimization to all tokens, ignoring their different roles in the reasoning process.", "method": "HAPO is a token-aware algorithm that dynamically adapts optimization based on token entropy. It includes Adaptive Temperature Sampling, Token Level Group Average, Differential Advantage Redistribution, and Asymmetric Adaptive Clipping.", "result": "Extensive experiments demonstrate that HAPO consistently outperforms DAPO across multiple model scales.", "conclusion": "HAPO consistently outperforms DAPO across multiple model scales."}}
{"id": "2509.16596", "pdf": "https://arxiv.org/pdf/2509.16596", "abs": "https://arxiv.org/abs/2509.16596", "authors": ["Junjie Ye", "Yuming Yang", "Yang Nan", "Shuo Li", "Qi Zhang", "Tao Gui", "Xuanjing Huang", "Peng Wang", "Zhongchao Shi", "Jianping Fan"], "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 Main Conference. arXiv admin note: text\n  overlap with arXiv:2409.15825", "summary": "Large language models (LLMs) acquire substantial world knowledge during\npre-training, which is further shaped by post-training techniques such as\nsupervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge\nremains underexplored, limiting our ability to control knowledge change\nbehavior in fine-tuned models. To address this gap, we evaluate closed-book\nquestion answering (CBQA) performance across five LLMs from the LLaMA-2 and\nLLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up\nto 14% worse than those fine-tuned on only 240 samples. Furthermore, varying\nthe level of knowledge mastery in the fine-tuning data leads to performance\nfluctuations of over 12%. To investigate these effects, we analyze model\nbehavior at both the token and parameter levels. Our analysis reveals that up\nto 90% of parameter updates during SFT do not contribute to knowledge\nenhancement. Restoring these updates can improve performance on the CBQA task,\ndepending on the characteristics of the fine-tuning data. These insights offer\npractical guidance for developing fine-tuning strategies that more effectively\nstrengthen model knowledge.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u76d1\u7763\u5fae\u8c03\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u589e\u5f3a\u6548\u679c\u6709\u9650\uff0c\u4e14\u5fae\u8c03\u6570\u636e\u7684\u7279\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u8fd9\u4e3a\u4f18\u5316\u5fae\u8c03\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u5bf9\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u77e5\u8bc6\u638c\u63e1\u7a0b\u5ea6\u4e86\u89e3\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86\u6211\u4eec\u63a7\u5236\u5fae\u8c03\u6a21\u578b\u77e5\u8bc6\u53d8\u5316\u884c\u4e3a\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22SFT\u5bf9\u6a21\u578b\u77e5\u8bc6\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u4e2a\u6765\u81eaLLaMA-2\u548cLLaMA-3\u5bb6\u65cf\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u95ed\u5377\u95ee\u7b54\uff08CBQA\uff09\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u53c2\u6570\u66f4\u65b0\u5bf9\u77e5\u8bc6\u589e\u5f3a\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5fae\u8c03\u6570\u636e\u91cf\u548c\u77e5\u8bc6\u638c\u63e1\u7a0b\u5ea6\u7684\u53d8\u5316\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u6ce2\u52a8\u8d85\u8fc712%\u3002\u6b64\u5916\uff0c\u9ad8\u8fbe90%\u7684\u53c2\u6570\u66f4\u65b0\u5728SFT\u8fc7\u7a0b\u4e2d\u5e76\u672a\u4fc3\u8fdb\u77e5\u8bc6\u589e\u5f3a\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5bf9\u6a21\u578b\u7684\u77e5\u8bc6\u589e\u5f3a\u6548\u679c\u6709\u9650\uff0c\u4e14\u5fae\u8c03\u6570\u636e\u7684\u7279\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5fae\u8c03\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2509.16597", "pdf": "https://arxiv.org/pdf/2509.16597", "abs": "https://arxiv.org/abs/2509.16597", "authors": ["Luyan Zhang"], "title": "MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "13 pages, 6 figures, 2 tables", "summary": "Aiming at the problems of computational inefficiency and insufficient\ninterpretability faced by large models in complex tasks such as multi-round\nreasoning and multi-modal collaboration, this study proposes a three-layer\ncollaboration framework based on model-controller-task adaptation (MCP). By\ndecoupling large model functions into reasoning, generation and retrieval\nmodules, and combining reinforcement learning-driven dynamic routing algorithms\nand task adaptation mechanisms, the systematic integration of control theory\nand large model dynamic reasoning is achieved for the first time. Experiments\nshow that the MCP framework improves the performance of cross-modal\nbenchmarking tasks, such as GLUE, COCO, ScienceQA, etc., by 15-30% compared\nwith the baseline model, improves the reasoning efficiency by 40%, and\ngenerates the interpretable intermediate results through the Presenter layer,\nobtaining 90% of the manual interpretability scores, which provides a brand-new\ntechnological path to solve the bottleneck of the practical application of the\nlarge model.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b-\u63a7\u5236\u5668-\u4efb\u52a1\u9002\u914d\uff08MCP\uff09\u7684\u4e09\u5c42\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u578b\u6a21\u578b\u529f\u80fd\u89e3\u8026\u4e3a\u63a8\u7406\u3001\u751f\u6210\u548c\u68c0\u7d22\u6a21\u5757\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u52a8\u6001\u8def\u7531\u7b97\u6cd5\u548c\u4efb\u52a1\u9002\u5e94\u673a\u5236\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u63a7\u5236\u7406\u8bba\u4e0e\u5927\u578b\u6a21\u578b\u52a8\u6001\u63a8\u7406\u7684\u7cfb\u7edf\u96c6\u6210\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8de8\u6a21\u6001\u57fa\u51c6\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u4e8615-30%\uff0c\u63a8\u7406\u6548\u7387\u63d0\u9ad8\u4e8640%\uff0c\u5e76\u901a\u8fc7Presenter\u5c42\u751f\u6210\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u7ed3\u679c\uff0c\u83b7\u5f97\u4e8690%\u7684\u4eba\u5de5\u53ef\u89e3\u91ca\u6027\u8bc4\u5206\uff0c\u4e3a\u89e3\u51b3\u5927\u578b\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u74f6\u9888\u95ee\u9898\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002", "motivation": "\u9488\u5bf9\u5927\u578b\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u591a\u8f6e\u63a8\u7406\u548c\u591a\u6a21\u6001\u534f\u4f5c\uff09\u4e2d\u9762\u4e34\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6280\u672f\u8def\u5f84\u6765\u89e3\u51b3\u8fd9\u4e9b\u74f6\u9888\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b-\u63a7\u5236\u5668-\u4efb\u52a1\u9002\u914d\uff08MCP\uff09\u7684\u4e09\u5c42\u534f\u4f5c\u6846\u67b6\u3002\u901a\u8fc7\u5c06\u5927\u578b\u6a21\u578b\u529f\u80fd\u89e3\u8026\u4e3a\u63a8\u7406\u3001\u751f\u6210\u548c\u68c0\u7d22\u6a21\u5757\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u52a8\u6001\u8def\u7531\u7b97\u6cd5\u548c\u4efb\u52a1\u9002\u5e94\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u63a7\u5236\u7406\u8bba\u4e0e\u5927\u578b\u6a21\u578b\u52a8\u6001\u63a8\u7406\u7684\u7cfb\u7edf\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMCP\u6846\u67b6\u5728\u8de8\u6a21\u6001\u57fa\u51c6\u4efb\u52a1\uff08\u5982GLUE\u3001COCO\u3001ScienceQA\u7b49\uff09\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u5347\u4e8615-30%\uff0c\u63a8\u7406\u6548\u7387\u63d0\u9ad8\u4e8640%\uff0c\u5e76\u901a\u8fc7Presenter\u5c42\u751f\u6210\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u7ed3\u679c\uff0c\u83b7\u5f97\u4e8690%\u7684\u4eba\u5de5\u53ef\u89e3\u91ca\u6027\u8bc4\u5206\u3002", "conclusion": "MCP\u6846\u67b6\u901a\u8fc7\u5c06\u5927\u578b\u6a21\u578b\u529f\u80fd\u89e3\u8026\u4e3a\u63a8\u7406\u3001\u751f\u6210\u548c\u68c0\u7d22\u6a21\u5757\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u52a8\u6001\u8def\u7531\u7b97\u6cd5\u548c\u4efb\u52a1\u9002\u5e94\u673a\u5236\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u63a7\u5236\u7406\u8bba\u4e0e\u5927\u578b\u6a21\u578b\u52a8\u6001\u63a8\u7406\u7684\u7cfb\u7edf\u96c6\u6210\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8de8\u6a21\u6001\u57fa\u51c6\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u4e8615-30%\uff0c\u63a8\u7406\u6548\u7387\u63d0\u9ad8\u4e8640%\uff0c\u5e76\u901a\u8fc7Presenter\u5c42\u751f\u6210\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u7ed3\u679c\uff0c\u83b7\u5f97\u4e8690%\u7684\u4eba\u5de5\u53ef\u89e3\u91ca\u6027\u8bc4\u5206\uff0c\u4e3a\u89e3\u51b3\u5927\u578b\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u74f6\u9888\u95ee\u9898\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.16598", "pdf": "https://arxiv.org/pdf/2509.16598", "abs": "https://arxiv.org/abs/2509.16598", "authors": ["Byeongho Yu", "Changhun Lee", "Jungyu Jin", "Eunhyeok Park"], "title": "PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "To mitigate the hallucination problem in large language models, DoLa exploits\nearly exit logits from the same model as a contrastive prior. However, we found\nthat these early exit logits tend to be flat, low in magnitude, and fail to\nreflect meaningful contrasts. To address this, we propose PruneCD, a novel\ncontrastive decoding method that constructs the amateur model via layer pruning\nrather than early exit. This design leads to more informative and well-aligned\nlogits, enabling more effective contrastive decoding. Through qualitative and\nquantitative analyses, we demonstrate that PruneCD consistently improves\nfactuality with minimal inference overhead, offering a robust and practical\napproach to mitigating hallucinations in LLMs.", "AI": {"tldr": "PruneCD is a new contrastive decoding method that uses layer pruning to create more informative logits, improving factuality in LLMs with little additional cost.", "motivation": "To address the issue of flat, low-magnitude early exit logits that fail to reflect meaningful contrasts in large language models.", "method": "PruneCD is a novel contrastive decoding method that constructs the amateur model via layer pruning rather than early exit.", "result": "PruneCD consistently improves factuality with minimal inference overhead, as demonstrated through qualitative and quantitative analyses.", "conclusion": "PruneCD provides a robust and practical approach to mitigating hallucinations in LLMs with minimal inference overhead."}}
{"id": "2509.16599", "pdf": "https://arxiv.org/pdf/2509.16599", "abs": "https://arxiv.org/abs/2509.16599", "authors": ["Sandro Tsang"], "title": "Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence", "categories": ["cs.CL", "cs.IR", "stat.AP", "stat.ME", "H.3.3; I.2.7; J.3"], "comment": "11 pages, 7 figures and 4 tables. This work describes an information\n  retrieval-driven workflow for medical evidence synthesis, with an application\n  to endometriosis recurrence. The method can be generalized to other\n  systematic reviews. The preregistered protocol is available:\n  https://doi.org/10.17605/OSF.IO/R2DFA", "summary": "Background: Evidence synthesis facilitates evidence-based medicine. Without\ninformation retrieval techniques, this task is impossible due to the vast and\nexpanding literature. Objective: Building on prior work, this study evaluates\nan information retrieval-driven workflow to enhance the efficiency,\ntransparency, and reproducibility of systematic reviews. We use endometriosis\nrecurrence as an ideal case due to its complex and ambiguous literature.\nMethods: Our hybrid approach integrates PRISMA guidelines with computational\ntechniques. We applied semi-automated deduplication to efficiently filter\nrecords before manual screening. This workflow synthesized evidence from\nrandomised controlled trials on the efficacy of a subclass of\ngonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method\naddressed unit-of-analysis errors in multi-arm trials. Results: Our workflow\nefficiently reduced the screening workload. It took only 11 days to fetch and\nfilter 812 records. Seven RCTs were eligible, providing evidence from 841\npatients in 4 countries. The pooled random-effects model yielded a Risk Ratio\n(RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity\n($I^2=0.00\\%$, $\\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence.\nSensitivity analyses and bias assessments supported the robustness of our\nfindings. Conclusion: This study demonstrates an information-retrieval-driven\nworkflow for medical evidence synthesis. Our approach yields valuable clinical\nresults while providing a framework for accelerating the systematic review\nprocess. It bridges the gap between clinical research and computer science and\ncan be generalized to other complex systematic reviews.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u533b\u5b66\u8bc1\u636e\u7efc\u5408\u3002\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7efc\u8ff0\u7684\u6548\u7387\u3001\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002\u901a\u8fc7\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u590d\u53d1\u7684\u6848\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e34\u5e8a\u7ed3\u679c\u4ee5\u53ca\u52a0\u901f\u7cfb\u7edf\u7efc\u8ff0\u8fc7\u7a0b\u7684\u6846\u67b6\u3002", "motivation": "\u80cc\u666f\uff1a\u8bc1\u636e\u7efc\u5408\u6709\u52a9\u4e8e\u5faa\u8bc1\u533b\u5b66\u3002\u7531\u4e8e\u6587\u732e\u6570\u91cf\u5e9e\u5927\u4e14\u4e0d\u65ad\u589e\u957f\uff0c\u6ca1\u6709\u4fe1\u606f\u68c0\u7d22\u6280\u672f\uff0c\u8fd9\u9879\u4efb\u52a1\u662f\u4e0d\u53ef\u80fd\u7684\u3002\u76ee\u6807\uff1a\u5728\u4e4b\u524d\u5de5\u4f5c\u7684\u57fa\u7840\u4e0a\uff0c\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u7531\u4fe1\u606f\u68c0\u7d22\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u7efc\u8ff0\u7684\u6548\u7387\u3001\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002\u6211\u4eec\u4f7f\u7528\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u590d\u53d1\u4f5c\u4e3a\u7406\u60f3\u6848\u4f8b\uff0c\u56e0\u4e3a\u5176\u6587\u732e\u590d\u6742\u4e14\u6a21\u7cca\u3002", "method": "\u6211\u4eec\u7684\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u4e86PRISMA\u6307\u5357\u548c\u8ba1\u7b97\u6280\u672f\u3002\u6211\u4eec\u5e94\u7528\u4e86\u534a\u81ea\u52a8\u53bb\u91cd\u4ee5\u9ad8\u6548\u8fc7\u6ee4\u8bb0\u5f55\uff0c\u7136\u540e\u518d\u8fdb\u884c\u4eba\u5de5\u7b5b\u9009\u3002\u8be5\u5de5\u4f5c\u6d41\u7a0b\u4ece\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u4e2d\u7efc\u5408\u4e86\u5173\u4e8e\u4fc3\u6027\u817a\u6fc0\u7d20\u91ca\u653e\u6fc0\u7d20\u6fc0\u52a8\u5242\uff08GnRH'as\uff09\u7597\u6548\u7684\u8bc1\u636e\u3002\u4fee\u6539\u540e\u7684\u5206\u5272\u65b9\u6cd5\u89e3\u51b3\u4e86\u591a\u81c2\u8bd5\u9a8c\u4e2d\u7684\u5355\u4f4d\u5206\u6790\u9519\u8bef\u3002", "result": "\u6211\u4eec\u7684\u5de5\u4f5c\u6d41\u7a0b\u6709\u6548\u5730\u51cf\u5c11\u4e86\u7b5b\u9009\u5de5\u4f5c\u91cf\u3002\u4ec5\u7528\u4e8611\u5929\u5c31\u83b7\u53d6\u5e76\u7b5b\u9009\u4e86812\u6761\u8bb0\u5f55\u3002\u67097\u9879RCT\u7b26\u5408\u6761\u4ef6\uff0c\u63d0\u4f9b\u4e86\u6765\u81ea4\u4e2a\u56fd\u5bb6\u7684841\u540d\u60a3\u8005\u7684\u8bc1\u636e\u3002\u6c47\u603b\u7684\u968f\u673a\u6548\u5e94\u6a21\u578b\u5f97\u51fa\u98ce\u9669\u6bd4\uff08RR\uff09\u4e3a0.64\uff0895% CI\uff080.48\u81f30.86\uff09\uff09\uff0c\u5f02\u8d28\u6027\u4e0d\u663e\u8457\uff08I\u00b2=0.00%\uff0c\u03c4=0.00\uff09\uff1b\u5373\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u590d\u53d1\u51cf\u5c1136%\u3002\u654f\u611f\u6027\u5206\u6790\u548c\u504f\u501a\u8bc4\u4f30\u652f\u6301\u4e86\u6211\u4eec\u53d1\u73b0\u7684\u7a33\u5065\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u7684\u5de5\u4f5c\u6d41\u7a0b\u7528\u4e8e\u533b\u5b66\u8bc1\u636e\u7efc\u5408\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63d0\u4f9b\u4e34\u5e8a\u7ed3\u679c\u7684\u540c\u65f6\uff0c\u4e3a\u52a0\u901f\u7cfb\u7edf\u7efc\u8ff0\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6846\u67b6\u3002\u5b83\u5f25\u5408\u4e86\u4e34\u5e8a\u7814\u7a76\u4e0e\u8ba1\u7b97\u673a\u79d1\u5b66\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u53ef\u4ee5\u63a8\u5e7f\u5230\u5176\u4ed6\u590d\u6742\u7684\u7cfb\u7edf\u7efc\u8ff0\u4e2d\u3002"}}
{"id": "2509.16610", "pdf": "https://arxiv.org/pdf/2509.16610", "abs": "https://arxiv.org/abs/2509.16610", "authors": ["Junhao Chen", "Jingbo Sun", "Xiang Li", "Haidong Xin", "Yuhao Xue", "Yibin Xu", "Hao Zhao"], "title": "LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Findings", "summary": "As large language models (LLMs) advance across diverse tasks, the need for\ncomprehensive evaluation beyond single metrics becomes increasingly important.\nTo fully assess LLM intelligence, it is crucial to examine their interactive\ndynamics and strategic behaviors. We present LLMsPark, a game theory-based\nevaluation platform that measures LLMs' decision-making strategies and social\nbehaviors in classic game-theoretic settings, providing a multi-agent\nenvironment to explore strategic depth. Our system cross-evaluates 15 leading\nLLMs (both commercial and open-source) using leaderboard rankings and scoring\nmechanisms. Higher scores reflect stronger reasoning and strategic\ncapabilities, revealing distinct behavioral patterns and performance\ndifferences across models. This work introduces a novel perspective for\nevaluating LLMs' strategic intelligence, enriching existing benchmarks and\nbroadening their assessment in interactive, game-theoretic scenarios. The\nbenchmark and rankings are publicly available at https://llmsparks.github.io/.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u8bc4\u4f30\u5e73\u53f0LLMsPark\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u5178\u535a\u5f08\u8bba\u60c5\u5883\u4e2d\u7684\u51b3\u7b56\u7b56\u7565\u548c\u793e\u4f1a\u884c\u4e3a\u3002\u8be5\u5e73\u53f0\u5bf915\u4e2a\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u4ea4\u53c9\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u8fdb\u6b65\uff0c\u9700\u8981\u8d85\u8d8a\u5355\u4e00\u6307\u6807\u7684\u5168\u9762\u8bc4\u4f30\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u4e3a\u4e86\u5168\u9762\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\uff0c\u5fc5\u987b\u68c0\u67e5\u5b83\u4eec\u7684\u4e92\u52a8\u52a8\u6001\u548c\u6218\u7565\u884c\u4e3a\u3002", "method": "LLMsPark\u662f\u4e00\u4e2a\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u5178\u535a\u5f08\u8bba\u60c5\u5883\u4e2d\u7684\u51b3\u7b56\u7b56\u7565\u548c\u793e\u4f1a\u884c\u4e3a\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u73af\u5883\u6765\u63a2\u7d22\u6218\u7565\u6df1\u5ea6\uff0c\u5e76\u5bf915\u4e2a\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u4ea4\u53c9\u8bc4\u4f30\u3002", "result": "LLMsPark\u5bf915\u4e2a\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u5546\u4e1a\u548c\u5f00\u6e90\u6a21\u578b\uff09\u8fdb\u884c\u4e86\u4ea4\u53c9\u8bc4\u4f30\uff0c\u4f7f\u7528\u6392\u884c\u699c\u6392\u540d\u548c\u8bc4\u5206\u673a\u5236\u3002\u8f83\u9ad8\u7684\u5206\u6570\u53cd\u6620\u4e86\u66f4\u5f3a\u7684\u63a8\u7406\u548c\u6218\u7565\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86LLMsPark\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u7528\u4e8e\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u5178\u535a\u5f08\u8bba\u60c5\u5883\u4e2d\u7684\u51b3\u7b56\u7b56\u7565\u548c\u793e\u4f1a\u884c\u4e3a\u3002\u8be5\u5e73\u53f0\u63d0\u4f9b\u4e86\u591a\u667a\u80fd\u4f53\u73af\u5883\u6765\u63a2\u7d22\u6218\u7565\u6df1\u5ea6\uff0c\u5e76\u5bf915\u4e2a\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u4ea4\u53c9\u8bc4\u4f30\u3002"}}
{"id": "2509.16660", "pdf": "https://arxiv.org/pdf/2509.16660", "abs": "https://arxiv.org/abs/2509.16660", "authors": ["Zuhair Hasan Shaik", "Abdullah Mazhar", "Aseem Srivastava", "Md Shad Akhtar"], "title": "Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation", "categories": ["cs.CL"], "comment": "Accepted to the NeurIPS 2025 Research Track", "summary": "Large Language Models have demonstrated impressive fluency across diverse\ntasks, yet their tendency to produce toxic content remains a critical challenge\nfor AI safety and public trust. Existing toxicity mitigation approaches\nprimarily manipulate individual neuron activations, but these methods suffer\nfrom instability, context dependence, and often compromise the model's core\nlanguage abilities. To address these shortcomings, we investigate three key\nquestions: the stability of neuron-level toxicity indicators, the advantages of\nstructural (layer-wise) representations, and the interpretability of mechanisms\ndriving toxic generation. Through extensive experiments on Jigsaw and ToxiCN\ndatasets, we show that aggregated layer-wise features provide more robust\nsignals than single neurons. Moreover, we observe conceptual limitations in\nprior works that conflate toxicity detection experts and generation experts\nwithin neuron-based interventions. To mitigate this, we propose a novel\nprincipled intervention technique, EigenShift, based on eigen-decomposition of\nthe language model's final output layer. This method selectively targets\ngeneration-aligned components, enabling precise toxicity suppression without\nimpairing linguistic competence. Our method requires no additional training or\nfine-tuning, incurs minimal computational cost, and is grounded in rigorous\ntheoretical analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u5206\u89e3\u7684\u65b0\u578b\u6bd2\u6027\u6291\u5236\u65b9\u6cd5EigenShift\uff0c\u80fd\u591f\u5728\u4e0d\u635f\u5bb3\u8bed\u8a00\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u6291\u5236\u6bd2\u6027\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u7684\u6bd2\u6027\u7f13\u89e3\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u64cd\u7eb5\u5355\u4e2a\u795e\u7ecf\u5143\u6fc0\u6d3b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u3001\u4f9d\u8d56\u4e0a\u4e0b\u6587\u4ee5\u53ca\u53ef\u80fd\u635f\u5bb3\u6a21\u578b\u6838\u5fc3\u8bed\u8a00\u80fd\u529b\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u548c\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e86\u795e\u7ecf\u5143\u5c42\u9762\u7684\u6bd2\u6027\u6307\u6807\u7a33\u5b9a\u6027\u3001\u7ed3\u6784\uff08\u5c42-wise\uff09\u8868\u793a\u7684\u4f18\u52bf\u4ee5\u53ca\u9a71\u52a8\u6bd2\u6027\u751f\u6210\u7684\u673a\u5236\u7684\u53ef\u89e3\u91ca\u6027\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u6700\u7ec8\u8f93\u51fa\u5c42\u7279\u5f81\u5206\u89e3\u7684EigenShift\u65b9\u6cd5\uff0c\u4ee5\u9009\u62e9\u6027\u5730\u9488\u5bf9\u4e0e\u751f\u6210\u76f8\u5173\u7684\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u6bd2\u6027\u6291\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u805a\u5408\u7684\u5c42-wise \u7279\u5f81\u6bd4\u5355\u4e2a\u795e\u7ecf\u5143\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u4fe1\u53f7\u3002\u6b64\u5916\uff0c\u672c\u6587\u89c2\u5bdf\u5230\u5148\u524d\u5de5\u4f5c\u4e2d\u5c06\u6bd2\u6027\u68c0\u6d4b\u4e13\u5bb6\u548c\u751f\u6210\u4e13\u5bb6\u6df7\u5728\u4e00\u8d77\u7684\u5c40\u9650\u6027\u3002EigenShift\u65b9\u6cd5\u80fd\u591f\u7cbe\u786e\u6291\u5236\u6bd2\u6027\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u6700\u7ec8\u8f93\u51fa\u5c42\u7279\u5f81\u5206\u89e3\u7684\u65b0\u578b\u5e72\u9884\u6280\u672fEigenShift\uff0c\u80fd\u591f\u7cbe\u786e\u6291\u5236\u6bd2\u6027\u5185\u5bb9\u800c\u4e0d\u635f\u5bb3\u8bed\u8a00\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u5e76\u6709\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u652f\u6491\u3002"}}
{"id": "2509.16666", "pdf": "https://arxiv.org/pdf/2509.16666", "abs": "https://arxiv.org/abs/2509.16666", "authors": ["Ahmet Yavuz Uluslu", "Tannon Kew", "Tilia Ellendorff", "Gerold Schneider", "Rico Sennrich"], "title": "Robust Native Language Identification through Agentic Decomposition", "categories": ["cs.CL"], "comment": "Accepted at EMNLP* 2025", "summary": "Large language models (LLMs) often achieve high performance in native\nlanguage identification (NLI) benchmarks by leveraging superficial contextual\nclues such as names, locations, and cultural stereotypes, rather than the\nunderlying linguistic patterns indicative of native language (L1) influence. To\nimprove robustness, previous work has instructed LLMs to disregard such clues.\nIn this work, we demonstrate that such a strategy is unreliable and model\npredictions can be easily altered by misleading hints. To address this problem,\nwe introduce an agentic NLI pipeline inspired by forensic linguistics, where\nspecialized agents accumulate and categorize diverse linguistic evidence before\nan independent final overall assessment. In this final assessment, a goal-aware\ncoordinating agent synthesizes all evidence to make the NLI prediction. On two\nbenchmark datasets, our approach significantly enhances NLI robustness against\nmisleading contextual clues and performance consistency compared to standard\nprompting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u6cd5\u8bed\u8bed\u8a00\u5b66\u542f\u53d1\u7684\u4ee3\u7406NLI\u7ba1\u9053\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5bf9\u8bef\u5bfc\u6027\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4e00\u81f4\u6027\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\u5df2\u7ecf\u6307\u5bfcLLM\u5ffd\u7565\u8fd9\u4e9b\u7ebf\u7d22\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u8fd9\u79cd\u7b56\u7565\u4e0d\u53ef\u9760\uff0c\u6a21\u578b\u9884\u6d4b\u5bb9\u6613\u88ab\u8bef\u5bfc\u6027\u63d0\u793a\u6539\u53d8\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u53d7\u6cd5\u8bed\u8bed\u8a00\u5b66\u542f\u53d1\u7684\u4ee3\u7406NLI\u7ba1\u9053\uff0c\u5176\u4e2d\u4e13\u95e8\u7684\u4ee3\u7406\u6536\u96c6\u548c\u5206\u7c7b\u591a\u79cd\u8bed\u8a00\u8bc1\u636e\uff0c\u7136\u540e\u7531\u4e00\u4e2a\u76ee\u6807\u611f\u77e5\u7684\u534f\u8c03\u4ee3\u7406\u7efc\u5408\u6240\u6709\u8bc1\u636e\u8fdb\u884cNLI\u9884\u6d4b\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86NLI\u5bf9\u8bef\u5bfc\u6027\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4e00\u81f4\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86NLI\u5bf9\u8bef\u5bfc\u6027\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4e00\u81f4\u6027\uff0c\u76f8\u6bd4\u6807\u51c6\u63d0\u793a\u65b9\u6cd5\u3002"}}
{"id": "2509.16679", "pdf": "https://arxiv.org/pdf/2509.16679", "abs": "https://arxiv.org/abs/2509.16679", "authors": ["Keliang Liu", "Dingkang Yang", "Ziyun Qian", "Weijie Yin", "Yuchi Wang", "Hongsheng Li", "Jun Liu", "Peng Zhai", "Yang Liu", "Lihua Zhang"], "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle", "categories": ["cs.CL"], "comment": "A Survey of Reinforcement Learning for Large Language Models", "summary": "In recent years, training methods centered on Reinforcement Learning (RL)\nhave markedly enhanced the reasoning and alignment performance of Large\nLanguage Models (LLMs), particularly in understanding human intents, following\nuser instructions, and bolstering inferential strength. Although existing\nsurveys offer overviews of RL augmented LLMs, their scope is often limited,\nfailing to provide a comprehensive summary of how RL operates across the full\nlifecycle of LLMs. We systematically review the theoretical and practical\nadvancements whereby RL empowers LLMs, especially Reinforcement Learning with\nVerifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL.\nSecond, we thoroughly detail application strategies for RL across various\nphases of the LLM lifecycle, including pre-training, alignment fine-tuning, and\nreinforced reasoning. In particular, we emphasize that RL methods in the\nreinforced reasoning phase serve as a pivotal driving force for advancing model\nreasoning to its limits. Next, we collate existing datasets and evaluation\nbenchmarks currently used for RL fine-tuning, spanning human-annotated\ndatasets, AI-assisted preference data, and program-verification-style corpora.\nSubsequently, we review the mainstream open-source tools and training\nframeworks available, providing clear practical references for subsequent\nresearch. Finally, we analyse the future challenges and trends in the field of\nRL-enhanced LLMs. This survey aims to present researchers and practitioners\nwith the latest developments and frontier trends at the intersection of RL and\nLLMs, with the goal of fostering the evolution of LLMs that are more\nintelligent, generalizable, and secure.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5176\u57fa\u672c\u7406\u8bba\u3001\u5728\u4e0d\u540c\u751f\u547d\u5468\u671f\u9636\u6bb5\u7684\u5e94\u7528\u7b56\u7565\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\u3001\u5de5\u5177\u548c\u6846\u67b6\uff0c\u4ee5\u53ca\u672a\u6765\u6311\u6218\u548c\u8d8b\u52bf\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u7efc\u8ff0\u63d0\u4f9b\u4e86\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6982\u8ff0\uff0c\u4f46\u5b83\u4eec\u7684\u8303\u56f4\u901a\u5e38\u6709\u9650\uff0c\u672a\u80fd\u63d0\u4f9b\u5f3a\u5316\u5b66\u4e60\u5728\u6574\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u547d\u5468\u671f\u4e2d\u8fd0\u4f5c\u7684\u5168\u9762\u603b\u7ed3\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u7efc\u8ff0\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5730\u56de\u987e\u4e86\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u3002\u9996\u5148\u4ecb\u7ecd\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u672c\u7406\u8bba\uff0c\u7136\u540e\u8be6\u7ec6\u9610\u8ff0\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u547d\u5468\u671f\u5404\u4e2a\u9636\u6bb5\u7684\u5e94\u7528\u7b56\u7565\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u3001\u5bf9\u9f50\u5fae\u8c03\u548c\u5f3a\u5316\u63a8\u7406\u3002\u6b64\u5916\uff0c\u8fd8\u6536\u96c6\u4e86\u5f53\u524d\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u56de\u987e\u4e86\u4e3b\u6d41\u7684\u5f00\u6e90\u5de5\u5177\u548c\u8bad\u7ec3\u6846\u67b6\u3002\u6700\u540e\u5206\u6790\u4e86\u8be5\u9886\u57df\u672a\u6765\u7684\u6311\u6218\u548c\u8d8b\u52bf\u3002", "result": "\u672c\u6587\u7cfb\u7edf\u5730\u56de\u987e\u4e86\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u5f3a\u5316\u63a8\u7406\u9636\u6bb5\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u662f\u63a8\u52a8\u6a21\u578b\u63a8\u7406\u8fbe\u5230\u6781\u9650\u7684\u5173\u952e\u9a71\u52a8\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u6536\u96c6\u4e86\u5f53\u524d\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u56de\u987e\u4e86\u4e3b\u6d41\u7684\u5f00\u6e90\u5de5\u5177\u548c\u8bad\u7ec3\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u5411\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u5c55\u793a\u5f3a\u5316\u5b66\u4e60\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ea4\u53c9\u9886\u57df\u7684\u6700\u65b0\u53d1\u5c55\u548c\u524d\u6cbf\u8d8b\u52bf\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u667a\u80fd\u3001\u6cdb\u5316\u6027\u548c\u5b89\u5168\u6027\u66f4\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.16686", "pdf": "https://arxiv.org/pdf/2509.16686", "abs": "https://arxiv.org/abs/2509.16686", "authors": ["Zhengge Cai", "Haowen Hou"], "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 EG-MLA\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5f15\u5165\u5d4c\u5165\u95e8\u63a7\u673a\u5236\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e86 KV \u7f13\u5b58\u5927\u5c0f\uff0c\u5e76\u63d0\u9ad8\u4e86\u8868\u793a\u80fd\u529b\u3002", "motivation": "\u51cf\u5c11\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u5927\u5c0f\u662f\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5ef6\u8fdf\u548c\u5185\u5b58\u9650\u5236\u4e0b\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u7684\u5173\u952e\u6b65\u9aa4\u3002\u867d\u7136\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u5177\u6709\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\uff0c\u4f46\u4f1a\u5e26\u6765\u663e\u8457\u7684\u5185\u5b58\u5f00\u9500\u3002", "method": "EG-MLA \u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u7684\u95e8\u63a7\u673a\u5236\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u538b\u7f29\u7684 KV \u5411\u91cf\u8fdb\u884c\u7cbe\u7ec6\u8c03\u8282\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u51cf\u5c11 KV \u7f13\u5b58\u5927\u5c0f\u5e76\u589e\u5f3a\u8868\u793a\u80fd\u529b\u3002", "result": "\u4e0e MHA \u76f8\u6bd4\uff0cEG-MLA \u5728\u51e0\u4e4e\u6ca1\u6709\u6027\u80fd\u4e0b\u964d\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8fc7 91.6% \u7684 KV \u7f13\u5b58\u51cf\u5c11\u3002\u4e0e MLA \u76f8\u6bd4\uff0cEG-MLA \u5728\u591a\u79cd\u63a8\u7406\u57fa\u51c6\u4e0a consistently \u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8fbe 59.9% \u7684\u989d\u5916\u5185\u5b58\u8282\u7701\u3002", "conclusion": "EG-MLA \u662f\u4e00\u79cd\u5185\u5b58\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u9ad8\u6027\u80fd\u63a8\u7406\u3002"}}
{"id": "2509.16696", "pdf": "https://arxiv.org/pdf/2509.16696", "abs": "https://arxiv.org/abs/2509.16696", "authors": ["Wataru Hashimoto", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at EMNLP 2025 Findings", "summary": "Decoding strategies manipulate the probability distribution underlying the\noutput of a language model and can therefore affect both generation quality and\nits uncertainty. In this study, we investigate the impact of decoding\nstrategies on uncertainty estimation in Large Language Models (LLMs). Our\nexperiments show that Contrastive Search, which mitigates repetition, yields\nbetter uncertainty estimates on average across a range of preference-aligned\nLLMs. In contrast, the benefits of these strategies sometimes diverge when the\nmodel is only post-trained with supervised fine-tuning, i.e. without explicit\nalignment.", "AI": {"tldr": "This paper studies how decoding strategies affect uncertainty estimation in LLMs and finds that Contrastive Search provides better uncertainty estimates on average, but the benefits may vary depending on the training method.", "motivation": "Decoding strategies can affect both generation quality and uncertainty, so understanding their impact on uncertainty estimation is important for improving LLM performance.", "method": "The study investigates the impact of decoding strategies on uncertainty estimation in Large Language Models (LLMs).", "result": "Contrastive Search yields better uncertainty estimates on average across a range of preference-aligned LLMs, but the benefits of these strategies sometimes diverge when the model is only post-trained with supervised fine-tuning.", "conclusion": "Contrastive Search provides better uncertainty estimates on average across a range of preference-aligned LLMs, but the benefits of decoding strategies may vary when models are only post-trained with supervised fine-tuning."}}
{"id": "2509.16713", "pdf": "https://arxiv.org/pdf/2509.16713", "abs": "https://arxiv.org/abs/2509.16713", "authors": ["Tianyang Xu", "Hongqiu Wu", "Weiqi Wu", "Hai Zhao"], "title": "OPEN-THEATRE: An Open-Source Toolkit for LLM-based Interactive Drama", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 demo", "summary": "LLM-based Interactive Drama introduces a novel dialogue scenario in which the\nplayer immerses into a character and engages in a dramatic story by interacting\nwith LLM agents. Despite the fact that this emerging area holds significant\npromise, it remains largely underexplored due to the lack of a well-designed\nplayground to develop a complete drama. This makes a significant barrier for\nresearchers to replicate, extend, and study such systems. Hence, we present\nOpen-Theatre, the first open-source toolkit for experiencing and customizing\nLLM-based interactive drama. It refines prior work with an efficient\nmulti-agent architecture and a hierarchical retrieval-based memory system,\ndesigned to enhance narrative coherence and realistic long-term behavior in\ncomplex interactions. In addition, we provide a highly configurable pipeline,\nmaking it easy for researchers to develop and optimize new approaches.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Open-Theatre\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u57fa\u4e8eLLM\u7684\u4e92\u52a8\u620f\u5267\u7684\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u65e8\u5728\u89e3\u51b3\u5b9e\u9a8c\u5e73\u53f0\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u67b6\u6784\u548c\u53ef\u914d\u7f6e\u6d41\u7a0b\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u4e00\u4e2a\u8bbe\u8ba1\u826f\u597d\u7684\u5b9e\u9a8c\u5e73\u53f0\uff0c\u57fa\u4e8eLLM\u7684\u4e92\u52a8\u620f\u5267\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u8fd9\u7ed9\u7814\u7a76\u4eba\u5458\u590d\u5236\u3001\u6269\u5c55\u548c\u7814\u7a76\u6b64\u7c7b\u7cfb\u7edf\u5e26\u6765\u4e86\u91cd\u5927\u969c\u788d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u57fa\u4e8e\u5206\u5c42\u68c0\u7d22\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4ee5\u589e\u5f3a\u53d9\u8ff0\u8fde\u8d2f\u6027\u548c\u590d\u6742\u4ea4\u4e92\u4e2d\u7684\u771f\u5b9e\u957f\u671f\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u53ef\u914d\u7f6e\u7684\u6d41\u7a0b\uff0c\u4fbf\u4e8e\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u548c\u4f18\u5316\u65b0\u65b9\u6cd5\u3002", "result": "\u672c\u6587\u6210\u529f\u5f00\u53d1\u4e86Open-Theatre\u5de5\u5177\u5305\uff0c\u8be5\u5de5\u5177\u5305\u5728\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u8bb0\u5fc6\u7cfb\u7edf\u65b9\u9762\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u914d\u7f6e\u7684\u6d41\u7a0b\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86Open-Theatre\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u4f53\u9a8c\u548c\u5b9a\u5236\u57fa\u4e8eLLM\u7684\u4e92\u52a8\u620f\u5267\u7684\u7b2c\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u7f3a\u4e4f\u5b8c\u5584\u5b9e\u9a8c\u5e73\u53f0\u7684\u95ee\u9898\u3002"}}
{"id": "2509.16717", "pdf": "https://arxiv.org/pdf/2509.16717", "abs": "https://arxiv.org/abs/2509.16717", "authors": ["Haoran Li", "Zhiming Su", "Junyan Yao", "Enwei Zhang", "Yang Ji", "Yan Chen", "Kan Zhou", "Chao Feng", "Jiao Ran"], "title": "Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance Control for Short Video Search Relevance Modeling", "categories": ["cs.CL"], "comment": "Submitted to AAAI 2026", "summary": "Synthetic data is widely adopted in embedding models to ensure diversity in\ntraining data distributions across dimensions such as difficulty, length, and\nlanguage. However, existing prompt-based synthesis methods struggle to capture\ndomain-specific data distributions, particularly in data-scarce domains, and\noften overlook fine-grained relevance diversity. In this paper, we present a\nChinese short video dataset with 4-level relevance annotations, filling a\ncritical resource void. Further, we propose a semi-supervised synthetic data\npipeline where two collaboratively trained models generate domain-adaptive\nshort video data with controllable relevance labels. Our method enhances\nrelevance-level diversity by synthesizing samples for underrepresented\nintermediate relevance labels, resulting in a more balanced and semantically\nrich training data set. Extensive offline experiments show that the embedding\nmodel trained on our synthesized data outperforms those using data generated\nbased on prompting or vanilla supervised fine-tuning(SFT). Moreover, we\ndemonstrate that incorporating more diverse fine-grained relevance levels in\ntraining data enhances the model's sensitivity to subtle semantic distinctions,\nhighlighting the value of fine-grained relevance supervision in embedding\nlearning. In the search enhanced recommendation pipeline of Douyin's\ndual-column scenario, through online A/B testing, the proposed model increased\nclick-through rate(CTR) by 1.45%, raised the proportion of Strong Relevance\nRatio (SRR) by 4.9%, and improved the Image User Penetration Rate (IUPR) by\n0.1054%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7684\u5408\u6210\u6570\u636e\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u53ef\u63a7\u76f8\u5173\u6027\u6807\u7b7e\u7684\u9886\u57df\u81ea\u9002\u5e94\u77ed\u89c6\u9891\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u63d0\u9ad8\u76f8\u5173\u6027\u5c42\u6b21\u7684\u591a\u6837\u6027\uff0c\u4f7f\u8bad\u7ec3\u6570\u636e\u66f4\u52a0\u5e73\u8861\u548c\u8bed\u4e49\u4e30\u5bcc\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u5d4c\u5165\u6a21\u578b\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u6216\u666e\u901a\u76d1\u7763\u5fae\u8c03\u7684\u6570\u636e\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u63d0\u5347\u4e86\u70b9\u51fb\u7387\u3001\u5f3a\u76f8\u5173\u6027\u6bd4\u4f8b\u548c\u56fe\u50cf\u7528\u6237\u6e17\u900f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u5408\u6210\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7279\u5b9a\u9886\u57df\u7684\u6570\u636e\u5206\u5e03\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u9886\u57df\uff0c\u5e76\u4e14\u5e38\u5e38\u5ffd\u7565\u7ec6\u7c92\u5ea6\u7684\u76f8\u5173\u6027\u591a\u6837\u6027\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u5173\u952e\u8d44\u6e90\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u751f\u6210\u66f4\u5177\u76f8\u5173\u6027\u591a\u6837\u6027\u7684\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7684\u5408\u6210\u6570\u636e\u7ba1\u9053\uff0c\u5176\u4e2d\u4e24\u4e2a\u534f\u540c\u8bad\u7ec3\u7684\u6a21\u578b\u751f\u6210\u5177\u6709\u53ef\u63a7\u76f8\u5173\u6027\u6807\u7b7e\u7684\u9886\u57df\u81ea\u9002\u5e94\u77ed\u89c6\u9891\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e3a\u6b20\u4ee3\u8868\u7684\u4e2d\u95f4\u76f8\u5173\u6027\u6807\u7b7e\u5408\u6210\u6837\u672c\uff0c\u63d0\u9ad8\u76f8\u5173\u6027\u5c42\u6b21\u7684\u591a\u6837\u6027\uff0c\u4ece\u800c\u521b\u5efa\u4e00\u4e2a\u66f4\u5e73\u8861\u548c\u8bed\u4e49\u4e30\u5bcc\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u5d4c\u5165\u6a21\u578b\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u6216\u666e\u901a\u76d1\u7763\u5fae\u8c03\u7684\u6570\u636e\u3002\u6b64\u5916\uff0c\u5728\u6296\u97f3\u7684\u53cc\u5217\u573a\u666f\u641c\u7d22\u589e\u5f3a\u63a8\u8350\u7ba1\u9053\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u901a\u8fc7\u5728\u7ebfA/B\u6d4b\u8bd5\u63d0\u9ad8\u4e86\u70b9\u51fb\u7387\u3001\u5f3a\u76f8\u5173\u6027\u6bd4\u4f8b\u548c\u56fe\u50cf\u7528\u6237\u6e17\u900f\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7684\u5408\u6210\u6570\u636e\u7ba1\u9053\uff0c\u901a\u8fc7\u4e24\u4e2a\u534f\u540c\u8bad\u7ec3\u7684\u6a21\u578b\u751f\u6210\u5177\u6709\u53ef\u63a7\u76f8\u5173\u6027\u6807\u7b7e\u7684\u9886\u57df\u81ea\u9002\u5e94\u77ed\u89c6\u9891\u6570\u636e\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u5d4c\u5165\u6a21\u578b\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u6216\u666e\u901a\u76d1\u7763\u5fae\u8c03\u7684\u6570\u636e\u3002\u6b64\u5916\uff0c\u5728\u6296\u97f3\u7684\u53cc\u5217\u573a\u666f\u641c\u7d22\u589e\u5f3a\u63a8\u8350\u7ba1\u9053\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u901a\u8fc7\u5728\u7ebfA/B\u6d4b\u8bd5\u63d0\u9ad8\u4e86\u70b9\u51fb\u7387\u3001\u5f3a\u76f8\u5173\u6027\u6bd4\u4f8b\u548c\u56fe\u50cf\u7528\u6237\u6e17\u900f\u7387\u3002"}}
{"id": "2509.16720", "pdf": "https://arxiv.org/pdf/2509.16720", "abs": "https://arxiv.org/abs/2509.16720", "authors": ["Auss Abbood", "Zaiqiao Meng", "Nigel Collier"], "title": "Time to Revist Exact Match", "categories": ["cs.CL"], "comment": "Accepted for Findings of EMNLP 2025", "summary": "Temporal question answering is an established method for evaluating temporal\nreasoning in large language models. Expected answers are often numeric (e.g.,\ndates or durations), yet model responses are evaluated like regular text with\nexact match (EM), unable to distinguish small from large errors. In this\ninvestigative work, we frame temporal question answering as a numerical\nestimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a\nbenchmark distilled from Test of Time and TempTabQA, where all questions\nrequire a numerical, temporal answer, allowing us to evaluate models beyond EM.\nWe use the forecasting metrics symmetric mean absolute percentage error (sMAPE)\nand mean absolute scaled error (MASE). With sMAPE, we find that error size and\nEM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some\nmodels have high sMAPE despite high EM. Scaling errors by the deviation of the\nground truth data with MASE reshuffles model rankings compared to EM, revealing\ngaps in models' understanding of temporal domain knowledge, especially when\ntrained with synthetic data. Lastly, the models' most frequent error is to\ndeviate by only $\\pm1$ from the ground truth. sMAPE and MASE, unlike EM,\nadequately weight these errors. Our findings underscore the need for\nspecialised metrics for temporal QA tasks. Code and data are available on\nhttps://github.com/aauss/temporal-answer-qa.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u95ee\u7b54\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u7528sMAPE\u548cMASE\u6307\u6807\u6765\u66f4\u51c6\u786e\u5730\u8861\u91cf\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u65f6\u95f4\u95ee\u7b54\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u533a\u5206\u5c0f\u8bef\u5dee\u548c\u5927\u8bef\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u6211\u4eec\u5c06\u65f6\u95f4\u95ee\u7b54\u4efb\u52a1\u6846\u67b6\u5316\u4e3a\u6570\u503c\u4f30\u8ba1\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86TempAnswerQA\u57fa\u51c6\uff0c\u4f7f\u7528sMAPE\u548cMASE\u7b49\u9884\u6d4b\u6307\u6807\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u8bef\u5dee\u5927\u5c0f\u4e0eEM\u6307\u6807\u89e3\u8026\uff0cMASE\u6307\u6807\u6539\u53d8\u4e86\u6a21\u578b\u6392\u540d\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u65f6\u95f4\u9886\u57df\u77e5\u8bc6\u7406\u89e3\u4e0a\u7684\u5dee\u8ddd\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9700\u8981\u4e13\u95e8\u7684\u6307\u6807\u6765\u8bc4\u4f30\u65f6\u95f4\u95ee\u7b54\u4efb\u52a1\u3002"}}
{"id": "2509.16722", "pdf": "https://arxiv.org/pdf/2509.16722", "abs": "https://arxiv.org/abs/2509.16722", "authors": ["Xiaohan Ding", "Kaike Ping", "Buse \u00c7ar\u0131k", "Eugenia Rho"], "title": "A Multi-Level Benchmark for Causal Language Understanding in Social Media Discourse", "categories": ["cs.CL"], "comment": null, "summary": "Understanding causal language in informal discourse is a core yet\nunderexplored challenge in NLP. Existing datasets largely focus on explicit\ncausality in structured text, providing limited support for detecting implicit\ncausal expressions, particularly those found in informal, user-generated social\nmedia posts. We introduce CausalTalk, a multi-level dataset of five years of\nReddit posts (2020-2024) discussing public health related to the COVID-19\npandemic, among which 10120 posts are annotated across four causal tasks: (1)\nbinary causal classification, (2) explicit vs. implicit causality, (3)\ncause-effect span extraction, and (4) causal gist generation. Annotations\ncomprise both gold-standard labels created by domain experts and\nsilver-standard labels generated by GPT-4o and verified by human annotators.\nCausalTalk bridges fine-grained causal detection and gist-based reasoning over\ninformal text. It enables benchmarking across both discriminative and\ngenerative models, and provides a rich resource for studying causal reasoning\nin social media contexts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CausalTalk\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u5c42\u7ea7\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b2020\u5e74\u81f32024\u5e74\u5173\u4e8e\u65b0\u51a0\u75ab\u60c5\u516c\u5171\u536b\u751f\u7684Reddit\u5e16\u5b50\uff0c\u5176\u4e2d10120\u7bc7\u5e16\u5b50\u88ab\u6807\u6ce8\u4e86\u56db\u4e2a\u56e0\u679c\u4efb\u52a1\u3002\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u56e0\u679c\u68c0\u6d4b\u548c\u57fa\u4e8e\u6458\u8981\u7684\u63a8\u7406\uff0c\u53ef\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u548c\u7814\u7a76\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u56e0\u679c\u63a8\u7406\u3002", "motivation": "Understanding causal language in informal discourse is a core yet underexplored challenge in NLP. Existing datasets largely focus on explicit causality in structured text, providing limited support for detecting implicit causal expressions, particularly those found in informal, user-generated social media posts.", "method": "We introduce CausalTalk, a multi-level dataset of five years of Reddit posts (2020-2024) discussing public health related to the COVID-19 pandemic, among which 10120 posts are annotated across four causal tasks: (1) binary causal classification, (2) explicit vs. implicit causality, (3) cause-effect span extraction, and (4) causal gist generation. Annotations comprise both gold-standard labels created by domain experts and silver-standard labels generated by GPT-4o and verified by human annotators.", "result": "CausalTalk bridges fine-grained causal detection and gist-based reasoning over informal text. It enables benchmarking across both discriminative and generative models, and provides a rich resource for studying causal reasoning in social media contexts.", "conclusion": "CausalTalk bridges fine-grained causal detection and gist-based reasoning over informal text. It enables benchmarking across both discriminative and generative models, and provides a rich resource for studying causal reasoning in social media contexts."}}
{"id": "2509.16729", "pdf": "https://arxiv.org/pdf/2509.16729", "abs": "https://arxiv.org/abs/2509.16729", "authors": ["Evgeniia Tokarchuk", "Sergey Troshin", "Vlad Niculae"], "title": "Angular Dispersion Accelerates $k$-Nearest Neighbors Machine Translation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Augmenting neural machine translation with external memory at decoding time,\nin the form of k-nearest neighbors machine translation ($k$-NN MT), is a\nwell-established strategy for increasing translation performance. $k$-NN MT\nretrieves a set of tokens that occurred in the most similar contexts recorded\nin a prepared data store, using hidden state representations of translation\ncontexts as vector lookup keys. One of the main disadvantages of this method is\nthe high computational cost and memory requirements. Since an exhaustive search\nis not feasible in large data stores, practitioners commonly use approximate\n$k$-NN MT lookup, yet even such algorithms are a bottleneck. In contrast to\nresearch directions seeking to accelerate $k$-NN MT by reducing data store size\nor the number of lookup calls, we pursue an orthogonal direction based on the\nperformance properties of approximate $k$-NN MT lookup data structures. In\nparticular, we propose to encourage angular dispersion of the neural hidden\nrepresentations of contexts. We show that improving dispersion leads to better\nbalance in the retrieval data structures, accelerating retrieval and slightly\nimproving translations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u63d0\u9ad8\u795e\u7ecf\u9690\u85cf\u8868\u793a\u7684\u89d2\u5411\u5206\u6563\u6027\u6765\u6539\u8fdb\u8fd1\u4f3ck-NN MT\u67e5\u627e\u7684\u6027\u80fd\uff0c\u4ece\u800c\u52a0\u901f\u68c0\u7d22\u5e76\u7565\u5fae\u63d0\u5347\u7ffb\u8bd1\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684k-NN MT\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u5b58\u50a8\u4e2d\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u5185\u5b58\u9700\u6c42\u5927\u7684\u95ee\u9898\uff0c\u5c3d\u7ba1\u4f7f\u7528\u4e86\u8fd1\u4f3ck-NN MT\u67e5\u627e\uff0c\u4f46\u4ecd\u7136\u662f\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u9f13\u52b1\u4e0a\u4e0b\u6587\u7684\u795e\u7ecf\u9690\u85cf\u8868\u793a\u7684\u89d2\u5411\u5206\u6563\u6027\uff0c\u4ee5\u6539\u8fdb\u8fd1\u4f3ck-NN MT\u67e5\u627e\u6570\u636e\u7ed3\u6784\u7684\u6027\u80fd\u3002", "result": "\u6539\u8fdb\u5206\u6563\u6027\u6709\u52a9\u4e8e\u6539\u5584\u68c0\u7d22\u6570\u636e\u7ed3\u6784\u7684\u5e73\u8861\u6027\uff0c\u4ece\u800c\u52a0\u901f\u68c0\u7d22\u5e76\u7565\u5fae\u63d0\u5347\u7ffb\u8bd1\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u63d0\u9ad8\u4e0a\u4e0b\u6587\u7684\u795e\u7ecf\u9690\u85cf\u8868\u793a\u7684\u89d2\u5411\u5206\u6563\u6027\uff0c\u53ef\u4ee5\u6539\u5584\u68c0\u7d22\u6570\u636e\u7ed3\u6784\u7684\u5e73\u8861\u6027\uff0c\u4ece\u800c\u52a0\u901f\u68c0\u7d22\u5e76\u7565\u5fae\u63d0\u5347\u7ffb\u8bd1\u6548\u679c\u3002"}}
{"id": "2509.16765", "pdf": "https://arxiv.org/pdf/2509.16765", "abs": "https://arxiv.org/abs/2509.16765", "authors": ["Fagun Patel", "Duc Q. Nguyen", "Sang T. Truong", "Jody Vaynshtok", "Sanmi Koyejo", "Nick Haber"], "title": "The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "EMNLP 2025 Oral Presentation", "summary": "According to the U.S. National Institutes of Health, more than 3.4 million\nchildren experience speech disorders that require clinical intervention. The\nnumber of speech-language pathologists (SLPs) is roughly 20 times fewer than\nthe number of affected children, highlighting a significant gap in children's\ncare and a pressing need for technological support that improves the\nproductivity of SLPs. State-of-the-art multimodal language models (MLMs) show\npromise for supporting SLPs, but their use remains underexplored largely due to\na limited understanding of their performance in high-stakes clinical settings.\nTo address this gap, we collaborate with domain experts to develop a taxonomy\nof real-world use cases of MLMs in speech-language pathologies. Building on\nthis taxonomy, we introduce the first comprehensive benchmark for evaluating\nMLM across five core use cases, each containing 1,000 manually annotated data\npoints. This benchmark includes robustness and sensitivity tests under various\nsettings, including background noise, speaker gender, and accent. Our\nevaluation of 15 state-of-the-art MLMs reveals that no single model\nconsistently outperforms others across all tasks. Notably, we find systematic\ndisparities, with models performing better on male speakers, and observe that\nchain-of-thought prompting can degrade performance on classification tasks with\nlarge label spaces and narrow decision boundaries. Furthermore, we study\nfine-tuning MLMs on domain-specific data, achieving improvements of over 30%\ncompared to base models. These findings highlight both the potential and\nlimitations of current MLMs for speech-language pathology applications,\nunderscoring the need for further research and targeted development.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u8bed\u8a00\u75c5\u7406\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5bf9\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u6027\u80fd\u7406\u89e3\u6709\u9650\uff0c\u5b83\u4eec\u7684\u4f7f\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u9700\u8981\u6280\u672f\u624b\u6bb5\u6765\u63d0\u9ad8\u8a00\u8bed\u8bed\u8a00\u75c5\u7406\u5b66\u5bb6\u7684\u5de5\u4f5c\u6548\u7387\u3002", "method": "\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\uff0c\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u8bed\u8a00\u75c5\u7406\u5b66\u4e2d\u7684\u5b9e\u9645\u7528\u4f8b\u5206\u7c7b\u6cd5\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u4e94\u4e2a\u6838\u5fc3\u7528\u4f8b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u4e8615\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u6ca1\u6709\u4e00\u4e2a\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6a21\u578b\u5728\u7537\u6027\u8bf4\u8bdd\u8005\u4e0a\u7684\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u601d\u7ef4\u94fe\u63d0\u793a\u53ef\u80fd\u4f1a\u964d\u4f4e\u5728\u5177\u6709\u5927\u6807\u7b7e\u7a7a\u95f4\u548c\u72ed\u7a84\u51b3\u7b56\u8fb9\u754c\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5bf9\u9886\u57df\u7279\u5b9a\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u9ad8\u4e8630%\u4ee5\u4e0a\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u5f53\u524d\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u8bed\u8a00\u75c5\u7406\u5b66\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u9488\u5bf9\u6027\u5f00\u53d1\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.16781", "pdf": "https://arxiv.org/pdf/2509.16781", "abs": "https://arxiv.org/abs/2509.16781", "authors": ["Andrei-Marius Avram", "Ema-Ioana B\u0103nescu", "Anda-Teodora Robea", "Dumitru-Clementin Cercel", "Mihaela-Claudia Cercel"], "title": "MoRoVoc: A Large Dataset for Geographical Variation Identification of the Spoken Romanian Language", "categories": ["cs.CL"], "comment": "Accepted at EMNLP Findings 2025", "summary": "This paper introduces MoRoVoc, the largest dataset for analyzing the regional\nvariation of spoken Romanian. It has more than 93 hours of audio and 88,192\naudio samples, balanced between the Romanian language spoken in Romania and the\nRepublic of Moldova. We further propose a multi-target adversarial training\nframework for speech models that incorporates demographic attributes (i.e., age\nand gender of the speakers) as adversarial targets, making models\ndiscriminative for primary tasks while remaining invariant to secondary\nattributes. The adversarial coefficients are dynamically adjusted via\nmeta-learning to optimize performance. Our approach yields notable gains:\nWav2Vec2-Base achieves 78.21% accuracy for the variation identification of\nspoken Romanian using gender as an adversarial target, while Wav2Vec2-Large\nreaches 93.08% accuracy for gender classification when employing both dialect\nand age as adversarial objectives.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MoRoVoc\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5206\u6790\u53e3\u8bed\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7684\u533a\u57df\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5bf9\u6297\u7cfb\u6570\u6765\u4f18\u5316\u6027\u80fd\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u5206\u6790\u53e3\u8bed\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7684\u533a\u57df\u53d8\u5316\uff0c\u5e76\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u7684\u8bed\u97f3\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bed\u97f3\u6a21\u578b\u7684\u591a\u76ee\u6807\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\uff08\u5982\u5e74\u9f84\u548c\u6027\u522b\uff09\u4f5c\u4e3a\u5bf9\u6297\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u5143\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u5bf9\u6297\u7cfb\u6570\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "result": "Wav2Vec2-Base\u5728\u4f7f\u7528\u6027\u522b\u4f5c\u4e3a\u5bf9\u6297\u76ee\u6807\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u53e3\u8bed\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7684\u53d8\u4f53\u8bc6\u522b\u8fbe\u5230\u4e8678.21%\u7684\u51c6\u786e\u7387\uff1b\u800cWav2Vec2-Large\u5728\u540c\u65f6\u91c7\u7528\u65b9\u8a00\u548c\u5e74\u9f84\u4f5c\u4e3a\u5bf9\u6297\u76ee\u6807\u65f6\uff0c\u5bf9\u4e8e\u6027\u522b\u7684\u5206\u7c7b\u8fbe\u5230\u4e8693.08%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bed\u97f3\u6a21\u578b\u7684\u591a\u76ee\u6807\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\uff08\u5982\u5e74\u9f84\u548c\u6027\u522b\uff09\u4f5c\u4e3a\u5bf9\u6297\u76ee\u6807\uff0c\u4f7f\u6a21\u578b\u5728\u4e3b\u8981\u4efb\u52a1\u4e0a\u5177\u6709\u5224\u522b\u6027\uff0c\u540c\u65f6\u5bf9\u6b21\u8981\u5c5e\u6027\u4fdd\u6301\u4e0d\u53d8\u3002"}}
{"id": "2509.16788", "pdf": "https://arxiv.org/pdf/2509.16788", "abs": "https://arxiv.org/abs/2509.16788", "authors": ["Salha Alyami", "Amani Jamal", "Areej Alhothali"], "title": "Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "26 excluding bibliography , journal article", "summary": "Aspect-based sentiment analysis (ABSA) in natural language processing enables\norganizations to understand customer opinions on specific product aspects.\nWhile deep learning models are widely used for English ABSA, their application\nin Arabic is limited due to the scarcity of labeled data. Researchers have\nattempted to tackle this issue by using pre-trained contextualized language\nmodels such as BERT. However, these models are often based on fact-based data,\nwhich can introduce bias in domain-specific tasks like ABSA. To our knowledge,\nno studies have applied adaptive pre-training with Arabic contextualized models\nfor ABSA. This research proposes a novel approach using domain-adaptive\npre-training for aspect-sentiment classification (ASC) and opinion target\nexpression (OTE) extraction. We examine fine-tuning strategies - feature\nextraction, full fine-tuning, and adapter-based methods - to enhance\nperformance and efficiency, utilizing multiple adaptation corpora and\ncontextualized models. Our results show that in-domain adaptive pre-training\nyields modest improvements. Adapter-based fine-tuning is a computationally\nefficient method that achieves competitive results. However, error analyses\nreveal issues with model predictions and dataset labeling. In ASC, common\nproblems include incorrect sentiment labeling, misinterpretation of contrastive\nmarkers, positivity bias for early terms, and challenges with conflicting\nopinions and subword tokenization. For OTE, issues involve mislabeling targets,\nconfusion over syntactic roles, difficulty with multi-word expressions, and\nreliance on shallow heuristics. These findings underscore the need for syntax-\nand semantics-aware models, such as graph convolutional networks, to more\neffectively capture long-distance relations and complex aspect-based opinion\nalignments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u65b9\u9762\u60c5\u611f\u5206\u6790\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u53d1\u73b0\u9002\u914d\u5668\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7ed3\u679c\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u9700\u8981\u89e3\u51b3\u3002", "motivation": "\u7531\u4e8e\u963f\u62c9\u4f2f\u8bed\u7f3a\u4e4f\u6807\u8bb0\u6570\u636e\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u65b9\u9762\u60c5\u611f\u5206\u6790\u4e2d\u7684\u5e94\u7528\u6709\u9650\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u4e8b\u5b9e\u6570\u636e\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u80fd\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u5f15\u5165\u504f\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u65b9\u9762\u60c5\u611f\u5206\u7c7b\uff08ASC\uff09\u548c\u89c2\u70b9\u76ee\u6807\u8868\u8fbe\uff08OTE\uff09\u63d0\u53d6\uff0c\u5e76\u8bc4\u4f30\u4e86\u5fae\u8c03\u7b56\u7565\uff0c\u5305\u62ec\u7279\u5f81\u63d0\u53d6\u3001\u5b8c\u5168\u5fae\u8c03\u548c\u9002\u914d\u5668\u65b9\u6cd5\u3002", "result": "\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u5728\u963f\u62c9\u4f2f\u8bed\u65b9\u9762\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u5e26\u6765\u4e86\u9002\u5ea6\u7684\u6539\u8fdb\u3002\u9002\u914d\u5668\u65b9\u6cd5\u662f\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u9884\u6d4b\u548c\u6570\u636e\u96c6\u6807\u6ce8\u7684\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u5728\u963f\u62c9\u4f2f\u8bed\u65b9\u9762\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u5e26\u6765\u4e86\u9002\u5ea6\u7684\u6539\u8fdb\uff0c\u4f46\u6a21\u578b\u9884\u6d4b\u548c\u6570\u636e\u96c6\u6807\u6ce8\u5b58\u5728\u95ee\u9898\u3002\u9700\u8981\u66f4\u6ce8\u91cd\u8bed\u6cd5\u548c\u8bed\u4e49\u7684\u6a21\u578b\uff0c\u5982\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u6355\u6349\u957f\u8ddd\u79bb\u5173\u7cfb\u548c\u590d\u6742\u7684\u57fa\u4e8e\u65b9\u9762\u7684\u610f\u89c1\u5bf9\u9f50\u3002"}}
{"id": "2509.16804", "pdf": "https://arxiv.org/pdf/2509.16804", "abs": "https://arxiv.org/abs/2509.16804", "authors": ["Kozhin muhealddin Awlla", "Hadi Veisi", "Abdulhady Abas Abdullah"], "title": "KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper enhances the study of sentiment analysis for the Central Kurdish\nlanguage by integrating the Bidirectional Encoder Representations from\nTransformers (BERT) into Natural Language Processing techniques. Kurdish is a\nlow-resourced language, having a high level of linguistic diversity with\nminimal computational resources, making sentiment analysis somewhat\nchallenging. Earlier, this was done using a traditional word embedding model,\nsuch as Word2Vec, but with the emergence of new language models, specifically\nBERT, there is hope for improvements. The better word embedding capabilities of\nBERT lend to this study, aiding in the capturing of the nuanced semantic pool\nand the contextual intricacies of the language under study, the Kurdish\nlanguage, thus setting a new benchmark for sentiment analysis in low-resource\nlanguages.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06BERT\u96c6\u6210\u5230\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u4e2d\uff0c\u63d0\u9ad8\u4e86\u5e93\u5c14\u5fb7\u8bed\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "motivation": "\u5e93\u5c14\u5fb7\u8bed\u662f\u4e00\u79cd\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\uff0c\u5177\u6709\u9ad8\u5ea6\u7684\u8bed\u8a00\u591a\u6837\u6027\uff0c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u4f7f\u5f97\u60c5\u611f\u5206\u6790\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u7684\u8bcd\u5d4c\u5165\u6a21\u578b\u5982Word2Vec\u5df2\u88ab\u7528\u4e8e\u6b64\u4efb\u52a1\uff0c\u4f46\u968f\u7740BERT\u7b49\u65b0\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u6709\u5e0c\u671b\u5b9e\u73b0\u6539\u8fdb\u3002", "method": "\u672c\u7814\u7a76\u5c06BERT\u96c6\u6210\u5230\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u4e2d\uff0c\u4ee5\u6539\u8fdb\u5e93\u5c14\u5fb7\u8bed\u7684\u60c5\u611f\u5206\u6790\u3002", "result": "BERT\u66f4\u597d\u7684\u8bcd\u5d4c\u5165\u80fd\u529b\u6709\u52a9\u4e8e\u6355\u6349\u8bed\u8a00\u7684\u7ec6\u5fae\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u590d\u6742\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06BERT\u96c6\u6210\u5230\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u4e2d\uff0c\u589e\u5f3a\u4e86\u5bf9\u5e93\u5c14\u5fb7\u8bed\u60c5\u611f\u5206\u6790\u7684\u7814\u7a76\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2509.16813", "pdf": "https://arxiv.org/pdf/2509.16813", "abs": "https://arxiv.org/abs/2509.16813", "authors": ["Devin R. Wright", "Jisun An", "Yong-Yeol Ahn"], "title": "Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition-Informed Approach to Quantifying Identity Fusion from Text", "categories": ["cs.CL", "I.2.7; H.3.1; I.5.4; J.4"], "comment": "Authors' accepted manuscript (postprint; camera-ready). To appear in\n  the Proceedings of EMNLP 2025. Pagination/footer layout may differ from the\n  Version of Record", "summary": "Quantifying identity fusion -- the psychological merging of self with another\nentity or abstract target (e.g., a religious group, political party, ideology,\nvalue, brand, belief, etc.) -- is vital for understanding a wide range of\ngroup-based human behaviors. We introduce the Cognitive Linguistic Identity\nFusion Score (CLIFS), a novel metric that integrates cognitive linguistics with\nlarge language models (LLMs), which builds on implicit metaphor detection.\nUnlike traditional pictorial and verbal scales, which require controlled\nsurveys or direct field contact, CLIFS delivers fully automated, scalable\nassessments while maintaining strong alignment with the established verbal\nmeasure. In benchmarks, CLIFS outperforms both existing automated approaches\nand human annotation. As a proof of concept, we apply CLIFS to violence risk\nassessment to demonstrate that it can improve violence risk assessment by more\nthan 240%. Building on our identification of a new NLP task and early success,\nwe underscore the need to develop larger, more diverse datasets that encompass\nadditional fusion-target domains and cultural backgrounds to enhance\ngeneralizability and further advance this emerging area. CLIFS models and code\nare public at https://github.com/DevinW-sudo/CLIFS.", "AI": {"tldr": "CLIFS is a new metric that uses cognitive linguistics and LLMs to quantify identity fusion, offering an automated and scalable alternative to traditional methods.", "motivation": "Quantifying identity fusion is vital for understanding group-based human behaviors, and traditional methods are limited by their need for controlled surveys or direct field contact.", "method": "CLIFS integrates cognitive linguistics with large language models (LLMs) and builds on implicit metaphor detection to quantify identity fusion.", "result": "CLIFS outperforms existing automated approaches and human annotation in benchmarks and improves violence risk assessment by more than 240% as a proof of concept.", "conclusion": "CLIFS models and code are public, and further research is needed to develop larger, more diverse datasets to enhance generalizability and advance the field."}}
{"id": "2509.16835", "pdf": "https://arxiv.org/pdf/2509.16835", "abs": "https://arxiv.org/abs/2509.16835", "authors": ["Melkamu Abay Mersha", "Jugal Kalita"], "title": "Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Virtual brainstorming sessions have become a central component of\ncollaborative problem solving, yet the large volume and uneven distribution of\nideas often make it difficult to extract valuable insights efficiently. Manual\ncoding of ideas is time-consuming and subjective, underscoring the need for\nautomated approaches to support the evaluation of group creativity. In this\nstudy, we propose a semantic-driven topic modeling framework that integrates\nfour modular components: transformer-based embeddings (Sentence-BERT),\ndimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction\nwith refinement. The framework captures semantic similarity at the sentence\nlevel, enabling the discovery of coherent themes from brainstorming transcripts\nwhile filtering noise and identifying outliers. We evaluate our approach on\nstructured Zoom brainstorming sessions involving student groups tasked with\nimproving their university. Results demonstrate that our model achieves higher\ntopic coherence compared to established methods such as LDA, ETM, and BERTopic,\nwith an average coherence score of 0.687 (CV), outperforming baselines by a\nsignificant margin. Beyond improved performance, the model provides\ninterpretable insights into the depth and diversity of topics explored,\nsupporting both convergent and divergent dimensions of group creativity. This\nwork highlights the potential of embedding-based topic modeling for analyzing\ncollaborative ideation and contributes an efficient and scalable framework for\nstudying creativity in synchronous virtual meetings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7684\u4e3b\u9898\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u865a\u62df\u5934\u8111\u98ce\u66b4\u4f1a\u8bae\u4e2d\u7684\u7fa4\u4f53\u521b\u9020\u529b\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e3b\u9898\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u4e3b\u9898\u6df1\u5ea6\u548c\u591a\u6837\u6027\u7684\u53ef\u89e3\u91ca\u89c1\u89e3\u3002", "motivation": "\u865a\u62df\u5934\u8111\u98ce\u66b4\u4f1a\u8bae\u5df2\u6210\u4e3a\u534f\u4f5c\u89e3\u51b3\u95ee\u9898\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5927\u91cf\u7684\u60f3\u6cd5\u548c\u4e0d\u5747\u5300\u7684\u5206\u5e03\u4f7f\u5f97\u9ad8\u6548\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u53d8\u5f97\u56f0\u96be\u3002\u624b\u52a8\u7f16\u7801\u60f3\u6cd5\u65e2\u8017\u65f6\u53c8\u4e3b\u89c2\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u652f\u6301\u7fa4\u4f53\u521b\u9020\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u9a71\u52a8\u7684\u4e3b\u9898\u5efa\u6a21\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u56db\u4e2a\u6a21\u5757\u5316\u7ec4\u4ef6\uff1a\u57fa\u4e8eTransformer\u7684\u5d4c\u5165\uff08Sentence-BERT\uff09\u3001\u964d\u7ef4\uff08UMAP\uff09\u3001\u805a\u7c7b\uff08HDBSCAN\uff09\u548c\u4e3b\u9898\u63d0\u53d6\u4e0e\u4f18\u5316\u3002", "result": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u7ed3\u6784\u5316\u7684Zoom\u5934\u8111\u98ce\u66b4\u4f1a\u8bae\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5176\u4e3b\u9898\u4e00\u81f4\u6027\u9ad8\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u5982LDA\u3001ETM\u548cBERTopic\uff0c\u5e73\u5747\u4e00\u81f4\u6027\u5f97\u5206\u4e3a0.687\uff08CV\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u57fa\u4e8e\u5d4c\u5165\u7684\u4e3b\u9898\u5efa\u6a21\u5728\u5206\u6790\u534f\u4f5c\u521b\u610f\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u7814\u7a76\u540c\u6b65\u865a\u62df\u4f1a\u8bae\u4e2d\u7684\u521b\u9020\u529b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2509.16876", "pdf": "https://arxiv.org/pdf/2509.16876", "abs": "https://arxiv.org/abs/2509.16876", "authors": ["Jiun-Ting Li", "Bi-Cheng Yan", "Yi-Cheng Wang", "Berlin Chen"], "title": "Multi-task Pretraining for Enhancing Interpretable L2 Pronunciation Assessment", "categories": ["cs.CL"], "comment": "Accepted by APSIPA-ASC 2025", "summary": "Automatic pronunciation assessment (APA) analyzes second-language (L2)\nlearners' speech by providing fine-grained pronunciation feedback at various\nlinguistic levels. Most existing efforts on APA typically adopt segmental-level\nfeatures as inputs and predict pronunciation scores at different granularities\nvia hierarchical (or parallel) pronunciation modeling. This, however,\ninevitably causes assessments across linguistic levels (e.g., phone, word, and\nutterance) to rely solely on phoneme-level pronunciation features, nearly\nsidelining supra-segmental pronunciation cues. To address this limitation, we\nintroduce multi-task pretraining (MTP) for APA, a simple yet effective strategy\nthat attempts to capture long-term temporal pronunciation cues while\nstrengthening the intrinsic structures within an utterance via the objective of\nreconstructing input features. Specifically, for a phoneme-level encoder of an\nAPA model, the proposed MTP strategy randomly masks segmental-level\npronunciation features and reconstructs the masked ones based on their\nsurrounding pronunciation context. Furthermore, current APA systems lack\nintegration with automated speaking assessment (ASA), limiting holistic\nproficiency evaluation. Drawing on empirical studies and prior knowledge in\nASA, our framework bridges this gap by incorporating handcrafted features\n(HCFs), such as fluency (speech rate, silence duration) and stress (pitch\naccent strength), derived from human-designed formulas via regressors to\ngenerate interpretable proficiency scores. Experiments on speechocean762 show\nimproved pronunciation scoring and ASA proficiency correlation, enabling\ntargeted training and comprehensive proficiency assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u624b\u5de5\u7279\u5f81\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u53d1\u97f3\u8bc4\u4f30\u548c\u53e3\u8bed\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709APA\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u4e8e\u97f3\u6bb5\u7ea7\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u8d85\u97f3\u6bb5\u53d1\u97f3\u7ebf\u7d22\uff0c\u5e76\u4e14\u7f3a\u4e4f\u4e0e\u81ea\u52a8\u53e3\u8bed\u8bc4\u4f30\uff08ASA\uff09\u7684\u6574\u5408\uff0c\u9650\u5236\u4e86\u6574\u4f53\u719f\u7ec3\u5ea6\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\uff08MTP\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u91cd\u5efa\u8f93\u5165\u7279\u5f81\u6765\u6355\u6349\u957f\u671f\u65f6\u95f4\u53d1\u97f3\u7ebf\u7d22\u5e76\u52a0\u5f3a\u53e5\u5b50\u5185\u7684\u5185\u5728\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u624b\u5de5\u7279\u5f81\uff08HCFs\uff09\u5982\u6d41\u5229\u5ea6\u548c\u91cd\u97f3\u6765\u751f\u6210\u53ef\u89e3\u91ca\u7684\u719f\u7ec3\u5ea6\u8bc4\u5206\u3002", "result": "\u5728speechocean762\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u53d1\u97f3\u8bc4\u5206\u548cASA\u719f\u7ec3\u5ea6\u76f8\u5173\u6027\u5f97\u5230\u4e86\u6539\u5584\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53d1\u97f3\u8bc4\u5206\u548cASA\u719f\u7ec3\u5ea6\u76f8\u5173\u6027\u65b9\u9762\u6709\u6240\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u9488\u5bf9\u6027\u8bad\u7ec3\u548c\u5168\u9762\u7684\u719f\u7ec3\u5ea6\u8bc4\u4f30\u3002"}}
{"id": "2509.16889", "pdf": "https://arxiv.org/pdf/2509.16889", "abs": "https://arxiv.org/abs/2509.16889", "authors": ["Xiaoqiang Kang", "Shengen Wu", "Zimu Wang", "Yilin Liu", "Xiaobo Jin", "Kaizhu Huang", "Wei Wang", "Yutao Yue", "Xiaowei Huang", "Qiufeng Wang"], "title": "Can GRPO Boost Complex Multimodal Table Understanding?", "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Existing table understanding methods face challenges due to complex table\nstructures and intricate logical reasoning. While supervised finetuning (SFT)\ndominates existing research, reinforcement learning (RL), such as Group\nRelative Policy Optimization (GRPO), has shown promise but struggled with low\ninitial policy accuracy and coarse rewards in tabular contexts. In this paper,\nwe introduce Table-R1, a three-stage RL framework that enhances multimodal\ntable understanding through: (1) Warm-up that prompts initial perception and\nreasoning capabilities, (2) Perception Alignment GRPO (PA-GRPO), which employs\ncontinuous Tree-Edit-Distance Similarity (TEDS) rewards for recognizing table\nstructures and contents, and (3) Hint-Completion GRPO (HC-GRPO), which utilizes\nfine-grained rewards of residual steps based on the hint-guided question.\nExtensive experiments demonstrate that Table-R1 can boost the model's table\nreasoning performance obviously on both held-in and held-out datasets,\noutperforming SFT and GRPO largely. Notably, Qwen2-VL-7B with Table-R1\nsurpasses larger specific table understanding models (e.g., Table-LLaVA 13B),\neven achieving comparable performance to the closed-source model GPT-4o on\nheld-in datasets, demonstrating the efficacy of each stage of Table-R1 in\novercoming initialization bottlenecks and reward sparsity, thereby advancing\nrobust multimodal table understanding.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aTable-R1\u7684\u4e09\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u70ed\u3001\u611f\u77e5\u5bf9\u9f50\u548c\u63d0\u793a\u5b8c\u6210GRPO\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8868\u683c\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTable-R1\u5728\u4fdd\u6301\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u548cGRPO\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u53ef\u4ee5\u4e0e\u5c01\u95ed\u6e90\u4ee3\u7801\u6a21\u578bGPT-4o\u76f8\u5ab2\u7f8e\u3002", "motivation": "Existing table understanding methods face challenges due to complex table structures and intricate logical reasoning. While supervised finetuning (SFT) dominates existing research, reinforcement learning (RL), such as Group Relative Policy Optimization (GRPO), has shown promise but struggled with low initial policy accuracy and coarse rewards in tabular contexts.", "method": "Table-R1 is a three-stage RL framework that enhances multimodal table understanding through: (1) Warm-up that prompts initial perception and reasoning capabilities, (2) Perception Alignment GRPO (PA-GRPO), which employs continuous Tree-Edit-Distance Similarity (TEDS) rewards for recognizing table structures and contents, and (3) Hint-Completion GRPO (HC-GRPO), which utilizes fine-grained rewards of residual steps based on the hint-guided question.", "result": "Extensive experiments demonstrate that Table-R1 can boost the model's table reasoning performance obviously on both held-in and held-out datasets, outperforming SFT and GRPO largely. Notably, Qwen2-VL-7B with Table-R1 surpasses larger specific table understanding models (e.g., Table-LLaVA 13B), even achieving comparable performance to the closed-source model GPT-4o on held-in datasets.", "conclusion": "Table-R1 can boost the model's table reasoning performance obviously on both held-in and held-out datasets, outperforming SFT and GRPO largely. Qwen2-VL-7B with Table-R1 surpasses larger specific table understanding models and achieves comparable performance to GPT-4o, demonstrating the efficacy of each stage of Table-R1."}}
{"id": "2509.16903", "pdf": "https://arxiv.org/pdf/2509.16903", "abs": "https://arxiv.org/abs/2509.16903", "authors": ["Nawar Turk", "Daniele Comitogianni", "Leila Kosseim"], "title": "CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification", "categories": ["cs.CL"], "comment": null, "summary": "We present our submission to Task 3 (Discourse Relation Classification) of\nthe DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourse\nrelation labels across 39 corpora in 16 languages and six discourse frameworks,\nposing significant multilingual and cross-formalism challenges. We first\nbenchmark the task by fine-tuning multilingual BERT-based models (mBERT,\nXLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategies\nand progressive unfreezing ratios to establish strong baselines. We then\nevaluate prompt-based large language models (namely Claude Opus 4.0) in\nzero-shot and few-shot settings to understand how LLMs respond to the newly\nproposed unified labels. Finally, we introduce HiDAC, a Hierarchical\nDual-Adapter Contrastive learning model. Results show that while larger\ntransformer models achieve higher accuracy, the improvements are modest, and\nthat unfreezing the top 75% of encoder layers yields performance comparable to\nfull fine-tuning while training far fewer parameters. Prompt-based models lag\nsignificantly behind fine-tuned transformers, and HiDAC achieves the highest\noverall accuracy (67.5%) while remaining more parameter-efficient than full\nfine-tuning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5bf9DISRPT 2025\u4efb\u52a13\uff08\u8bdd\u8bed\u5173\u7cfb\u5206\u7c7b\uff09\u7684\u63d0\u4ea4\u3002\u4efb\u52a13\u5f15\u5165\u4e86\u8de8\u591a\u79cd\u8bed\u8a00\u548c\u8bdd\u8bed\u6846\u67b6\u768417\u4e2a\u7edf\u4e00\u8bdd\u8bed\u5173\u7cfb\u6807\u7b7e\uff0c\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u548c\u8de8\u5f62\u5f0f\u4e3b\u4e49\u6311\u6218\u3002\u4f5c\u8005\u901a\u8fc7\u5fae\u8c03\u591a\u8bed\u8a00BERT\u6a21\u578b\u5efa\u7acb\u57fa\u7ebf\uff0c\u5e76\u8bc4\u4f30\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002\u6700\u540e\uff0c\u4ed6\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u5c42\u53cc\u9002\u914d\u5668\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578bHiDAC\uff0c\u7ed3\u679c\u663e\u793aHiDAC\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u603b\u4f53\u51c6\u786e\u7387", "motivation": "\u4efb\u52a13\u5f15\u5165\u4e86\u8de839\u4e2a\u8bed\u6599\u5e93\u300116\u79cd\u8bed\u8a00\u548c\u516d\u4e2a\u8bdd\u8bed\u6846\u67b6\u768417\u4e2a\u7edf\u4e00\u8bdd\u8bed\u5173\u7cfb\u6807\u7b7e\uff0c\u63d0\u51fa\u4e86\u91cd\u5927\u7684\u591a\u8bed\u8a00\u548c\u8de8\u5f62\u5f0f\u4e3b\u4e49\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e0d\u540c\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0", "method": "\u9996\u5148\u901a\u8fc7\u5fae\u8c03\u591a\u8bed\u8a00BERT\u6a21\u578b\uff08mBERT\u3001XLM-RoBERTa-Base\u548cXLM-RoBERTa-Large\uff09\u5e76\u91c7\u7528\u4e24\u79cd\u8bba\u70b9\u987a\u5e8f\u7b56\u7565\u548c\u6e10\u8fdb\u5f0f\u89e3\u51bb\u6bd4\u4f8b\u6765\u5efa\u7acb\u5f3a\u5927\u7684\u57fa\u7ebf\u3002\u7136\u540e\u8bc4\u4f30\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5373Claude Opus 4.0\uff09\u5728\u96f6\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u4ee5\u4e86\u89e3LLMs\u5982\u4f55\u54cd\u5e94\u65b0\u63d0\u51fa\u7684\u7edf\u4e00\u6807\u7b7e\u3002\u6700\u540e\uff0c\u5f15\u5165HiDAC\uff0c\u4e00\u79cd\u5206\u5c42\u53cc\u9002\u914d\u5668\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u66f4\u5927\u7684\u53d8\u538b\u5668\u6a21\u578b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u4f46\u6539\u8fdb\u662f\u6709\u9650\u7684\uff0c\u800c\u89e3\u51bb\u7f16\u7801\u5668\u5c42\u7684\u9876\u90e875%\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u5b8c\u5168\u5fae\u8c03\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8bad\u7ec3\u66f4\u5c11\u7684\u53c2\u6570\u3002\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u578b\u660e\u663e\u843d\u540e\u4e8e\u5fae\u8c03\u7684\u53d8\u538b\u5668\uff0c\u800cHiDAC\u5728\u4fdd\u6301\u6bd4\u5b8c\u5168\u5fae\u8c03\u66f4\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0867.5%\uff09", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u66f4\u5927\u7684\u53d8\u538b\u5668\u6a21\u578b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u4f46\u6539\u8fdb\u662f\u6709\u9650\u7684\uff0c\u800c\u89e3\u51bb\u7f16\u7801\u5668\u5c42\u7684\u9876\u90e875%\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u5b8c\u5168\u5fae\u8c03\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8bad\u7ec3\u66f4\u5c11\u7684\u53c2\u6570\u3002\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u578b\u660e\u663e\u843d\u540e\u4e8e\u5fae\u8c03\u7684\u53d8\u538b\u5668\uff0c\u800cHiDAC\u5728\u4fdd\u6301\u6bd4\u5b8c\u5168\u5fae\u8c03\u66f4\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0867.5%\uff09"}}
{"id": "2509.16914", "pdf": "https://arxiv.org/pdf/2509.16914", "abs": "https://arxiv.org/abs/2509.16914", "authors": ["Wenhao Zhuang", "Yuan Sun"], "title": "CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities\nin various NLP tasks, significantly enhancing user experience and efficiency.\nHowever, this advantage is primarily limited to resource-rich languages. For\nthe diverse array of low-resource languages, support remains inadequate, with\nthe scarcity of training corpora considered the primary cause. We construct and\nopen-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two\n25GB sets of four-language corpora (one parallel and one non-parallel),\nobtained through machine translation. CUTE encompasses two resource-rich\nlanguages (Chinese and English) and two low-resource languages (Uyghur and\nTibetan). Prior to constructing CUTE, human assessment validates that the\nmachine translation quality between Chinese-Uyghur and Chinese-Tibetan\napproaches that of Chinese-English translation. CUTE represents the largest\nopen-source corpus for Uyghur and Tibetan languages to date, and we demonstrate\nits effectiveness in enhancing LLMs' ability to process low-resource languages\nwhile investigating the role of corpus parallelism in cross-lingual transfer\nlearning. The CUTE corpus and related models are made publicly available to the\nresearch community.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u5e76\u5f00\u6e90\u4e86CUTE\u8bed\u6599\u5e93\uff0c\u65e8\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u4e30\u5bcc\u7684\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u652f\u6301\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u8bed\u6599\u7a00\u7f3a\u3002", "method": "\u6784\u5efa\u5e76\u5f00\u6e90\u4e86CUTE\u8bed\u6599\u5e93\uff0c\u5305\u542b\u4e2d\u3001\u7ef4\u543e\u5c14\u8bed\u3001\u85cf\u8bed\u548c\u82f1\u8bed\u7684\u4e24\u7ec425GB\u8bed\u6599\uff08\u4e00\u7ec4\u5e73\u884c\uff0c\u4e00\u7ec4\u975e\u5e73\u884c\uff09\uff0c\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u83b7\u5f97\u3002", "result": "CUTE\u8bed\u6599\u5e93\u662f\u76ee\u524d\u6700\u5927\u7684\u7ef4\u543e\u5c14\u8bed\u548c\u85cf\u8bed\u5f00\u6e90\u8bed\u6599\u5e93\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4f4e\u8d44\u6e90\u8bed\u8a00\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "CUTE\u8bed\u6599\u5e93\u548c\u76f8\u5173\u6a21\u578b\u5df2\u5411\u7814\u7a76\u793e\u533a\u516c\u5f00\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u8bed\u6599\u5e73\u884c\u6027\u5728\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2509.16929", "pdf": "https://arxiv.org/pdf/2509.16929", "abs": "https://arxiv.org/abs/2509.16929", "authors": ["Yongrui Chen", "Yi Huang", "Yunchang Liu", "Shenyu Zhang", "Junhao He", "Tongtong Wu", "Guilin Qi", "Tianxing Wu"], "title": "K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling", "categories": ["cs.CL"], "comment": "Accepted in Neurips 2025 (poster)", "summary": "Continual Structured Knowledge Reasoning (CSKR) focuses on training models to\nhandle sequential tasks, where each task involves translating natural language\nquestions into structured queries grounded in structured knowledge. Existing\ngeneral continual learning approaches face significant challenges when applied\nto this task, including poor generalization to heterogeneous structured\nknowledge and inefficient reasoning due to parameter growth as tasks increase.\nTo address these limitations, we propose a novel CSKR framework,\n\\textsc{K-DeCore}, which operates with a fixed number of tunable parameters.\nUnlike prior methods, \\textsc{K-DeCore} introduces a knowledge decoupling\nmechanism that disentangles the reasoning process into task-specific and\ntask-agnostic stages, effectively bridging the gaps across diverse tasks.\nBuilding on this foundation, \\textsc{K-DeCore} integrates a dual-perspective\nmemory consolidation mechanism for distinct stages and introduces a\nstructure-guided pseudo-data synthesis strategy to further enhance the model's\ngeneralization capabilities. Extensive experiments on four benchmark datasets\ndemonstrate the superiority of \\textsc{K-DeCore} over existing continual\nlearning methods across multiple metrics, leveraging various backbone large\nlanguage models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u7ed3\u6784\u77e5\u8bc6\u63a8\u7406\u6846\u67b6 \textsc{K-DeCore}\uff0c\u901a\u8fc7\u77e5\u8bc6\u89e3\u8026\u548c\u8bb0\u5fc6\u5de9\u56fa\u7b49\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u987a\u5e8f\u4efb\u52a1\u65f6\u9762\u4e34\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u53c2\u6570\u589e\u957f\u5bfc\u81f4\u7684\u63a8\u7406\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\textsc{K-DeCore} \u5f15\u5165\u4e86\u4e00\u79cd\u77e5\u8bc6\u89e3\u8026\u673a\u5236\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u5206\u4e3a\u4efb\u52a1\u7279\u5b9a\u548c\u4efb\u52a1\u65e0\u5173\u9636\u6bb5\uff0c\u5e76\u7ed3\u5408\u53cc\u89c6\u89d2\u8bb0\u5fc6\u5de9\u56fa\u673a\u5236\u548c\u7ed3\u6784\u5f15\u5bfc\u7684\u4f2a\u6570\u636e\u5408\u6210\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\textsc{K-DeCore} \u5728\u591a\u79cd\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\textsc{K-DeCore} \u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.16952", "pdf": "https://arxiv.org/pdf/2509.16952", "abs": "https://arxiv.org/abs/2509.16952", "authors": ["Tiancheng Huang", "Ruisheng Cao", "Yuxin Zhang", "Zhangyi Kang", "Zijian Wang", "Chenrun Wang", "Yijie Luo", "Hang Zheng", "Lirong Qian", "Lu Chen", "Kai Yu"], "title": "AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The growing volume of academic papers has made it increasingly difficult for\nresearchers to efficiently extract key information. While large language models\n(LLMs) based agents are capable of automating question answering (QA) workflows\nfor scientific papers, there still lacks a comprehensive and realistic\nbenchmark to evaluate their capabilities. Moreover, training an interactive\nagent for this specific task is hindered by the shortage of high-quality\ninteraction trajectories. In this work, we propose AirQA, a human-annotated\ncomprehensive paper QA dataset in the field of artificial intelligence (AI),\nwith 13,948 papers and 1,246 questions, that encompasses multi-task,\nmulti-modal and instance-level evaluation. Furthermore, we propose ExTrActor,\nan automated framework for instruction data synthesis. With three LLM-based\nagents, ExTrActor can perform example generation and trajectory collection\nwithout human intervention. Evaluations of multiple open-source and proprietary\nmodels show that most models underperform on AirQA, demonstrating the quality\nof our dataset. Extensive experiments confirm that ExTrActor consistently\nimproves the multi-turn tool-use capability of small models, enabling them to\nachieve performance comparable to larger ones.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AirQA\u6570\u636e\u96c6\u548cExTrActor\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5b66\u672f\u8bba\u6587\u95ee\u7b54\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u5168\u9762\u57fa\u51c6\u548c\u9ad8\u8d28\u91cf\u4ea4\u4e92\u8f68\u8ff9\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86ExTrActor\u5bf9\u5c0f\u578b\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u5b66\u672f\u8bba\u6587\u6570\u91cf\u7684\u589e\u957f\uff0c\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u9ad8\u6548\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3002\u867d\u7136\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u53ef\u4ee5\u81ea\u52a8\u5316\u79d1\u5b66\u8bba\u6587\u7684\u95ee\u7b54\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f46\u7f3a\u4e4f\u4e00\u4e2a\u5168\u9762\u4e14\u73b0\u5b9e\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5b83\u4eec\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u8fd9\u79cd\u7279\u5b9a\u4efb\u52a1\u7684\u4ea4\u4e92\u4ee3\u7406\u53d7\u5230\u9ad8\u8d28\u91cf\u4ea4\u4e92\u8f68\u8ff9\u77ed\u7f3a\u7684\u963b\u788d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6ExTrActor\uff0c\u7528\u4e8e\u6307\u4ee4\u6570\u636e\u5408\u6210\u3002ExTrActor\u5229\u7528\u4e09\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\uff0c\u80fd\u591f\u5728\u6ca1\u6709\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u793a\u4f8b\u751f\u6210\u548c\u8f68\u8ff9\u6536\u96c6\u3002", "result": "\u591a\u4e2a\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5927\u591a\u6570\u6a21\u578b\u5728AirQA\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u6570\u636e\u96c6\u7684\u8d28\u91cf\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u786e\u8ba4ExTrActor\u80fd\u591f\u6301\u7eed\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cExTrActor\u80fd\u591f\u663e\u8457\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4f7f\u5176\u6027\u80fd\u8fbe\u5230\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u6c34\u5e73\u3002"}}
{"id": "2509.16965", "pdf": "https://arxiv.org/pdf/2509.16965", "abs": "https://arxiv.org/abs/2509.16965", "authors": ["Minchan Kwon", "Junwon Ko", "Kangil Kim", "Junmo Kim"], "title": "Preference Distillation via Value based Reinforcement Learning", "categories": ["cs.CL"], "comment": "20 page", "summary": "Direct Preference Optimization (DPO) is a powerful paradigm to align language\nmodels with human preferences using pairwise comparisons. However, its binary\nwin-or-loss supervision often proves insufficient for training small models\nwith limited capacity. Prior works attempt to distill information from large\nteacher models using behavior cloning or KL divergence. These methods often\nfocus on mimicking current behavior and overlook distilling reward modeling. To\naddress this issue, we propose \\textit{Teacher Value-based Knowledge\nDistillation} (TVKD), which introduces an auxiliary reward from the value\nfunction of the teacher model to provide a soft guide. This auxiliary reward is\nformulated to satisfy potential-based reward shaping, ensuring that the global\nreward structure and optimal policy of DPO are preserved. TVKD can be\nintegrated into the standard DPO training framework and does not require\nadditional rollouts. Our experimental results show that TVKD consistently\nimproves performance across various benchmarks and model sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6559\u5e08\u4ef7\u503c\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff08TVKD\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u5956\u52b1\u6765\u6539\u8fdbDPO\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eTVKD\u5728\u5404\u79cd\u57fa\u51c6\u548c\u6a21\u578b\u5927\u5c0f\u4e0a\u90fd\u80fd\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5f80\u5f80\u53ea\u5173\u6ce8\u6a21\u4eff\u5f53\u524d\u884c\u4e3a\uff0c\u800c\u5ffd\u7565\u4e86\u5956\u52b1\u5efa\u6a21\u7684\u84b8\u998f\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86TVKD\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6559\u5e08\u4ef7\u503c\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff08TVKD\uff09\uff0c\u5f15\u5165\u4e86\u6765\u81ea\u6559\u5e08\u6a21\u578b\u4ef7\u503c\u51fd\u6570\u7684\u8f85\u52a9\u5956\u52b1\uff0c\u4ee5\u63d0\u4f9b\u8f6f\u5f15\u5bfc\u3002\u8be5\u8f85\u52a9\u5956\u52b1\u88ab\u8bbe\u8ba1\u4e3a\u6ee1\u8db3\u57fa\u4e8e\u6f5c\u5728\u7684\u5956\u52b1\u5851\u9020\uff0c\u786e\u4fddDPO\u7684\u5168\u5c40\u5956\u52b1\u7ed3\u6784\u548c\u6700\u4f18\u7b56\u7565\u5f97\u5230\u4fdd\u7559\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTVKD\u5728\u5404\u79cd\u57fa\u51c6\u548c\u6a21\u578b\u5927\u5c0f\u4e0a\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "TVKD\u53ef\u4ee5\u96c6\u6210\u5230\u6807\u51c6\u7684DPO\u8bad\u7ec3\u6846\u67b6\u4e2d\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u989d\u5916\u7684rollouts\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTVKD\u5728\u5404\u79cd\u57fa\u51c6\u548c\u6a21\u578b\u5927\u5c0f\u4e0a\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2509.16990", "pdf": "https://arxiv.org/pdf/2509.16990", "abs": "https://arxiv.org/abs/2509.16990", "authors": ["Avishai Elmakies", "Hagai Aronowitz", "Nimrod Shabtay", "Eli Schwartz", "Ron Hoory", "Avihu Dekel"], "title": "Advancing Speech Understanding in Speech-Aware Language Models with GRPO", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based\nmethod for training Speech-Aware Large Language Models (SALLMs) on open-format\nspeech understanding tasks, such as Spoken Question Answering and Automatic\nSpeech Translation. SALLMs have proven highly effective for speech\nunderstanding tasks. GRPO has recently gained traction for its efficiency in\ntraining LLMs, and prior work has explored its application to SALLMs, primarily\nin multiple-choice tasks. Building on this, we focus on open-format tasks that\nbetter reflect the generative abilities of the models. Our approach leverages\nGRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate\nempirically that it surpasses standard SFT across several key metrics. Finally,\nwe explore the potential of incorporating off-policy samples within GRPO for\nthese tasks, highlighting avenues for further improvement and further research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGRPO\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5f00\u653e\u683c\u5f0f\u7684\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u4e0a\u8bad\u7ec3SALLMs\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u3002", "motivation": "\u8bed\u97f3\u611f\u77e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08SALLMs\uff09\u5728\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u591a\u9879\u9009\u62e9\u4efb\u52a1\u4e0a\uff0c\u800c\u672c\u6587\u5173\u6ce8\u7684\u662f\u66f4\u80fd\u4f53\u73b0\u6a21\u578b\u751f\u6210\u80fd\u529b\u7684\u5f00\u653e\u683c\u5f0f\u4efb\u52a1\u3002", "method": "\u672c\u6587\u91c7\u7528\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u65b9\u6cd5\uff0c\u5229\u7528BLEU\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u6765\u4f18\u5316\u8bed\u97f3\u611f\u77e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08SALLMs\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eGRPO\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u6807\u51c6\u76d1\u7763\u5fae\u8c03\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5f00\u653e\u683c\u5f0f\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5f00\u653e\u683c\u5f0f\u7684\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u4e0a\u8bad\u7ec3\u8bed\u97f3\u611f\u77e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08SALLMs\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63a2\u8ba8\u4e86\u5728GRPO\u4e2d\u5f15\u5165\u975e\u7b56\u7565\u6837\u672c\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.17030", "pdf": "https://arxiv.org/pdf/2509.17030", "abs": "https://arxiv.org/abs/2509.17030", "authors": ["Hinata Tezuka", "Naoya Inoue"], "title": "The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "57 pages, 47 figures and 41 tables; Accepted to EMNLP 2025 Main", "summary": "Recent studies have suggested a processing framework for multilingual inputs\nin decoder-based LLMs: early layers convert inputs into English-centric and\nlanguage-agnostic representations; middle layers perform reasoning within an\nEnglish-centric latent space; and final layers generate outputs by transforming\nthese representations back into language-specific latent spaces. However, the\ninternal dynamics of such transformation and the underlying mechanism remain\nunderexplored. Towards a deeper understanding of this framework, we propose and\nempirically validate The Transfer Neurons Hypothesis: certain neurons in the\nMLP module are responsible for transferring representations between\nlanguage-specific latent spaces and a shared semantic latent space.\nFurthermore, we show that one function of language-specific neurons, as\nidentified in recent studies, is to facilitate movement between latent spaces.\nFinally, we show that transfer neurons are critical for reasoning in\nmultilingual LLMs.", "AI": {"tldr": "This paper proposes and validates the Transfer Neurons Hypothesis, which suggests that certain neurons in the MLP module are responsible for transferring representations between language-specific latent spaces and a shared semantic latent space. The results show that transfer neurons are critical for reasoning in multilingual LLMs.", "motivation": "Recent studies have suggested a processing framework for multilingual inputs in decoder-based LLMs, but the internal dynamics of such transformation and the underlying mechanism remain underexplored.", "method": "We propose and empirically validate The Transfer Neurons Hypothesis, which suggests that certain neurons in the MLP module are responsible for transferring representations between language-specific latent spaces and a shared semantic latent space.", "result": "We show that transfer neurons are critical for reasoning in multilingual LLMs, and that one function of language-specific neurons is to facilitate movement between latent spaces.", "conclusion": "Transfer neurons are critical for reasoning in multilingual LLMs."}}
{"id": "2509.17047", "pdf": "https://arxiv.org/pdf/2509.17047", "abs": "https://arxiv.org/abs/2509.17047", "authors": ["Cui Ding", "Yanning Yin", "Lena A. J\u00e4ger", "Ethan Gotlieb Wilcox"], "title": "Modeling Bottom-up Information Quality during Language Processing", "categories": ["cs.CL"], "comment": null, "summary": "Contemporary theories model language processing as integrating both top-down\nexpectations and bottom-up inputs. One major prediction of such models is that\nthe quality of the bottom-up inputs modulates ease of processing -- noisy\ninputs should lead to difficult and effortful comprehension. We test this\nprediction in the domain of reading. First, we propose an information-theoretic\noperationalization for the \"quality\" of bottom-up information as the mutual\ninformation (MI) between visual information and word identity. We formalize\nthis prediction in a mathematical model of reading as a Bayesian update.\nSecond, we test our operationalization by comparing participants' reading times\nin conditions where words' information quality has been reduced, either by\noccluding their top or bottom half, with full words. We collect data in English\nand Chinese. We then use multimodal language models to estimate the mutual\ninformation between visual inputs and words. We use these data to estimate the\nspecific effect of reduced information quality on reading times. Finally, we\ncompare how information is distributed across visual forms. In English and\nChinese, the upper half contains more information about word identity than the\nlower half. However, the asymmetry is more pronounced in English, a pattern\nwhich is reflected in the reading times.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u5206\u6790\u4e86\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u5e95\u90e8\u8f93\u5165\u4fe1\u606f\u8d28\u91cf\u5bf9\u9605\u8bfb\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u5e76\u53d1\u73b0\u82f1\u8bed\u548c\u6c49\u8bed\u4e2d\u5355\u8bcd\u7684\u4e0a\u534a\u90e8\u5206\u5305\u542b\u66f4\u591a\u4fe1\u606f\uff0c\u4e14\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u5728\u82f1\u8bed\u4e2d\u66f4\u4e3a\u660e\u663e\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8bed\u8a00\u5904\u7406\u6a21\u578b\u4e2d\u7684\u4e00\u4e2a\u4e3b\u8981\u9884\u6d4b\uff0c\u5373\u5e95\u90e8\u8f93\u5165\u7684\u8d28\u91cf\u4f1a\u5f71\u54cd\u5904\u7406\u7684\u96be\u6613\u7a0b\u5ea6\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fe1\u606f\u8bba\u7684\u64cd\u4f5c\u5b9a\u4e49\u6765\u8861\u91cf\u5e95\u90e8\u8f93\u5165\u4fe1\u606f\u7684\u8d28\u91cf\uff0c\u5373\u89c6\u89c9\u4fe1\u606f\u4e0e\u8bcd\u4e49\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff08MI\uff09\u3002\u7136\u540e\u901a\u8fc7\u6bd4\u8f83\u53c2\u4e0e\u8005\u5728\u5355\u8bcd\u4fe1\u606f\u8d28\u91cf\u964d\u4f4e\u7684\u6761\u4ef6\u4e0b\u7684\u9605\u8bfb\u65f6\u95f4\u6765\u6d4b\u8bd5\u8fd9\u4e00\u64cd\u4f5c\u5b9a\u4e49\uff0c\u5e76\u4f7f\u7528\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u89c6\u89c9\u8f93\u5165\u4e0e\u5355\u8bcd\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u5355\u8bcd\u7684\u4e0a\u534a\u90e8\u5206\u6216\u4e0b\u534a\u90e8\u5206\u88ab\u906e\u6321\u65f6\uff0c\u53c2\u4e0e\u8005\u7684\u9605\u8bfb\u65f6\u95f4\u4f1a\u589e\u52a0\uff0c\u8868\u660e\u4fe1\u606f\u8d28\u91cf\u7684\u964d\u4f4e\u4f1a\u5bfc\u81f4\u9605\u8bfb\u56f0\u96be\u3002\u6b64\u5916\uff0c\u82f1\u8bed\u4e2d\u4e0a\u4e0b\u90e8\u5206\u7684\u4fe1\u606f\u5206\u5e03\u4e0d\u5bf9\u79f0\u6027\u66f4\u660e\u663e\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u82f1\u8bed\u548c\u6c49\u8bed\u4e2d\uff0c\u5355\u8bcd\u7684\u4e0a\u534a\u90e8\u5206\u6bd4\u4e0b\u534a\u90e8\u5206\u5305\u542b\u66f4\u591a\u7684\u5173\u4e8e\u8bcd\u4e49\u7684\u4fe1\u606f\uff0c\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u5728\u82f1\u8bed\u4e2d\u66f4\u4e3a\u660e\u663e\uff0c\u8fd9\u53cd\u6620\u5728\u9605\u8bfb\u65f6\u95f4\u4e0a\u3002"}}
{"id": "2509.17054", "pdf": "https://arxiv.org/pdf/2509.17054", "abs": "https://arxiv.org/abs/2509.17054", "authors": ["Yiwei Liu", "Emma Jane Pretty", "Jiahao Huang", "Saku Sugawara"], "title": "TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While recent studies explore Large Language Models' (LLMs) performance on\nTheory of Mind (ToM) reasoning tasks, research on ToM abilities that require\nmore nuanced social context is limited, such as white lies. We introduce\nTactfulToM, a novel English benchmark designed to evaluate LLMs' ability to\nunderstand white lies within real-life conversations and reason about prosocial\nmotivations behind them, particularly when they are used to spare others'\nfeelings and maintain social harmony. Our benchmark is generated through a\nmulti-stage human-in-the-loop pipeline where LLMs expand manually designed seed\nstories into conversations to maintain the information asymmetry between\nparticipants necessary for authentic white lies. We show that TactfulToM is\nchallenging for state-of-the-art models, which perform substantially below\nhumans, revealing shortcomings in their ability to fully comprehend the ToM\nreasoning that enables true understanding of white lies.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TactfulToM\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u82f1\u8bed\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u767d\u8c0e\u548c\u63a8\u7406\u5176\u4eb2\u793e\u4f1a\u52a8\u673a\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u9700\u8981\u66f4\u7ec6\u5fae\u793e\u4f1a\u60c5\u5883\u7684\u7406\u8bba\u5fc3\u667a\u80fd\u529b\uff08\u5982\u767d\u8c0e\uff09\u7684\u7814\u7a76\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLMs\u5728\u8fd9\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u591a\u9636\u6bb5\u7684\u4eba\u673a\u4ea4\u4e92\u7ba1\u9053\u751f\u6210TactfulToM\u57fa\u51c6\uff0c\u5176\u4e2dLLMs\u6269\u5c55\u624b\u52a8\u8bbe\u8ba1\u7684\u79cd\u5b50\u6545\u4e8b\u4ee5\u4fdd\u6301\u53c2\u4e0e\u8005\u4e4b\u95f4\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u8fd9\u662f\u771f\u5b9e\u767d\u8c0e\u6240\u5fc5\u9700\u7684\u3002", "result": "TactfulToM\u5bf9\u6700\u5148\u8fdb\u7684\u6a21\u578b\u6765\u8bf4\u662f\u5177\u6709\u6311\u6218\u6027\u7684\uff0c\u5b83\u4eec\u7684\u8868\u73b0\u660e\u663e\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u7406\u89e3\u767d\u8c0e\u6240\u9700\u7684\u7406\u8bba\u5fc3\u667a\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "TactfulToM\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u7406\u89e3\u767d\u8c0e\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u5b8c\u5168\u7406\u89e3\u767d\u8c0e\u6240\u9700\u7684\u7406\u8bba\u5fc3\u667a\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.17167", "pdf": "https://arxiv.org/pdf/2509.17167", "abs": "https://arxiv.org/abs/2509.17167", "authors": ["Seungjun Yi", "Joakim Nguyen", "Huimin Xu", "Terence Lim", "Joseph Skrovan", "Mehak Beri", "Hitakshi Modi", "Andrew Well", "Liu Leqi", "Mia Markey", "Ying Ding"], "title": "SFT-TA: Supervised Fine-Tuned Agents in Multi-Agent LLMs for Automated Inductive Thematic Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Thematic Analysis (TA) is a widely used qualitative method that provides a\nstructured yet flexible framework for identifying and reporting patterns in\nclinical interview transcripts. However, manual thematic analysis is\ntime-consuming and limits scalability. Recent advances in LLMs offer a pathway\nto automate thematic analysis, but alignment with human results remains\nlimited. To address these limitations, we propose SFT-TA, an automated thematic\nanalysis framework that embeds supervised fine-tuned (SFT) agents within a\nmulti-agent system. Our framework outperforms existing frameworks and the\ngpt-4o baseline in alignment with human reference themes. We observed that SFT\nagents alone may underperform, but achieve better results than the baseline\nwhen embedded within a multi-agent system. Our results highlight that embedding\nSFT agents in specific roles within a multi-agent system is a promising pathway\nto improve alignment with desired outputs for thematic analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u4e3b\u9898\u5206\u6790\u6846\u67b6SFT-TA\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4e0e\u4eba\u7c7b\u53c2\u8003\u4e3b\u9898\u7684\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u624b\u52a8\u4e3b\u9898\u5206\u6790\u8017\u65f6\u4e14\u9650\u5236\u53ef\u6269\u5c55\u6027\u3002\u6700\u8fd1\u7684LLM\u8fdb\u5c55\u4e3a\u81ea\u52a8\u5316\u4e3b\u9898\u5206\u6790\u63d0\u4f9b\u4e86\u9014\u5f84\uff0c\u4f46\u4e0e\u4eba\u7c7b\u7ed3\u679c\u7684\u5bf9\u9f50\u4ecd\u7136\u6709\u9650\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86SFT-TA\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u52a8\u4e3b\u9898\u5206\u6790\u6846\u67b6\uff0c\u5c06\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee3\u7406\u5d4c\u5165\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u3002", "result": "\u6211\u4eec\u7684\u6846\u67b6\u5728\u4e0e\u4eba\u7c7b\u53c2\u8003\u4e3b\u9898\u7684\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u6846\u67b6\u548cgpt-4o\u57fa\u7ebf\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5355\u72ec\u7684SFT\u4ee3\u7406\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5d4c\u5165SFT\u4ee3\u7406\u662f\u4e00\u79cd\u6539\u8fdb\u4e3b\u9898\u5206\u6790\u5bf9\u9f50\u7684\u6709\u524d\u9014\u7684\u9014\u5f84\u3002"}}
{"id": "2509.17177", "pdf": "https://arxiv.org/pdf/2509.17177", "abs": "https://arxiv.org/abs/2509.17177", "authors": ["Bowen Qin", "Chen Yue", "Fang Yin", "Hui Wang", "JG Yao", "Jiakang Liu", "Jing-Shu Zheng", "Miguel Hu Chen", "Richeng Xuan", "Shibei Meng", "Shiqi Zhou", "Teng Dai", "Tong-Shuai Ren", "Wei Cui", "Xi Yang", "Xialin Du", "Xiaojing Xu", "Xue Sun", "Xuejing Li", "Yaming Liu", "Yesheng Liu", "Ying Liu", "Yonghua Lin", "Yu Zhao", "Yunduo Zhang", "Yuwen Luo", "Zheqi He", "Zhiyuan He", "Zhongyuan Wang"], "title": "FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "23 pages in main text", "summary": "We conduct a moderate-scale contamination-free (to some extent) evaluation of\ncurrent large reasoning models (LRMs) with some preliminary findings. We also\nrelease ROME, our evaluation benchmark for vision language models intended to\ntest reasoning from visual clues. We attach links to the benchmark, evaluation\ndata, and other updates on this website:\nhttps://flageval-baai.github.io/LRM-Eval/", "AI": {"tldr": "\u672c\u6587\u5bf9\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u4e86\u4e2d\u7b49\u89c4\u6a21\u7684\u65e0\u6c61\u67d3\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u540d\u4e3aROME\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u7528\u4e8e\u6d4b\u8bd5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u672c\u6587\u8fdb\u884c\u4e86\u4e2d\u7b49\u89c4\u6a21\u7684\u65e0\u6c61\u67d3\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u4e86ROME\u4f5c\u4e3a\u8bc4\u4f30\u57fa\u51c6\uff0c\u4ee5\u6d4b\u8bd5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u672c\u6587\u8fdb\u884c\u4e86\u4e2d\u7b49\u89c4\u6a21\u7684\u65e0\u6c61\u67d3\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u4e86ROME\u4f5c\u4e3a\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u672c\u6587\u5bf9\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u4e86\u4e2d\u7b49\u89c4\u6a21\u7684\u65e0\u6c61\u67d3\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u540d\u4e3aROME\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u89c6\u89c9\u7ebf\u7d22\u4e2d\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\u3002"}}
{"id": "2509.17178", "pdf": "https://arxiv.org/pdf/2509.17178", "abs": "https://arxiv.org/abs/2509.17178", "authors": ["Tian Lan", "Jinyuan Xu", "Xue He", "Jenq-Neng Hwang", "Lei Li"], "title": "Attention Consistency for LLMs Explanation", "categories": ["cs.CL"], "comment": null, "summary": "Understanding the decision-making processes of large language models (LLMs)\nis essential for their trustworthy development and deployment. However, current\ninterpretability methods often face challenges such as low resolution and high\ncomputational cost. To address these limitations, we propose the\n\\textbf{Multi-Layer Attention Consistency Score (MACS)}, a novel, lightweight,\nand easily deployable heuristic for estimating the importance of input tokens\nin decoder-based models. MACS measures contributions of input tokens based on\nthe consistency of maximal attention. Empirical evaluations demonstrate that\nMACS achieves a favorable trade-off between interpretability quality and\ncomputational efficiency, showing faithfulness comparable to complex techniques\nwith a 22\\% decrease in VRAM usage and 30\\% reduction in latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMACS\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u8f93\u5165\u6807\u8bb0\u7684\u91cd\u8981\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMACS\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u6709\u5229\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u5206\u8fa8\u7387\u548c\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMulti-Layer Attention Consistency Score (MACS)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u8f93\u5165\u6807\u8bb0\u7684\u91cd\u8981\u6027\u3002MACS\u57fa\u4e8e\u6700\u5927\u6ce8\u610f\u529b\u7684\u4e00\u81f4\u6027\u6765\u8861\u91cf\u8f93\u5165\u6807\u8bb0\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cMACS\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u6709\u5229\u7684\u6743\u8861\uff0c\u5176\u5fe0\u5b9e\u5ea6\u4e0e\u590d\u6742\u6280\u672f\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u4e86VRAM\u4f7f\u7528\u91cf\u548c\u5ef6\u8fdf\u3002", "conclusion": "MACS achieves a favorable trade-off between interpretability quality and computational efficiency, showing faithfulness comparable to complex techniques with a 22% decrease in VRAM usage and 30% reduction in latency."}}
{"id": "2509.17183", "pdf": "https://arxiv.org/pdf/2509.17183", "abs": "https://arxiv.org/abs/2509.17183", "authors": ["Junsong Li", "Jie Zhou", "Bihao Zhan", "Yutao Yang", "Qianjun Pan", "Shilian Chen", "Tianyu Huai", "Xin Li", "Qin Chen", "Liang He"], "title": "LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Alignment plays a crucial role in Large Language Models (LLMs) in aligning\nwith human preferences on a specific task/domain. Traditional alignment methods\nsuffer from catastrophic forgetting, where models lose previously acquired\nknowledge when adapting to new preferences or domains. We introduce LifeAlign,\na novel framework for lifelong alignment that enables LLMs to maintain\nconsistent human preference alignment across sequential learning tasks without\nforgetting previously learned knowledge. Our approach consists of two key\ninnovations. First, we propose a focalized preference optimization strategy\nthat aligns LLMs with new preferences while preventing the erosion of knowledge\nacquired from previous tasks. Second, we develop a short-to-long memory\nconsolidation mechanism that merges denoised short-term preference\nrepresentations into stable long-term memory using intrinsic dimensionality\nreduction, enabling efficient storage and retrieval of alignment patterns\nacross diverse domains. We evaluate LifeAlign across multiple sequential\nalignment tasks spanning different domains and preference types. Experimental\nresults demonstrate that our method achieves superior performance in\nmaintaining both preference alignment quality and knowledge retention compared\nto existing lifelong learning approaches. The codes and datasets will be\nreleased on GitHub.", "AI": {"tldr": "LifeAlign \u662f\u4e00\u79cd\u65b0\u7684\u7ec8\u8eab\u5bf9\u9f50\u6846\u67b6\uff0c\u80fd\u591f\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fde\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u5bf9\u9f50\uff0c\u540c\u65f6\u907f\u514d\u9057\u5fd8\u4e4b\u524d\u5b66\u5230\u7684\u77e5\u8bc6\u3002", "motivation": "\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u9002\u5e94\u65b0\u504f\u597d\u6216\u9886\u57df\u65f6\u4e22\u5931\u4e4b\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u3002", "method": "LifeAlign \u662f\u4e00\u79cd\u65b0\u7684\u7ec8\u8eab\u5bf9\u9f50\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u805a\u7126\u504f\u597d\u4f18\u5316\u7b56\u7565\u548c\u77ed\u5230\u957f\u7684\u8bb0\u5fc6\u5de9\u56fa\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLifeAlign \u5728\u4fdd\u6301\u504f\u597d\u5bf9\u9f50\u8d28\u91cf\u548c\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u7ec8\u8eab\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "LifeAlign \u5728\u591a\u4e2a\u987a\u5e8f\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u73b0\u6709\u7684\u7ec8\u8eab\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u504f\u597d\u5bf9\u9f50\u8d28\u91cf\u548c\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2509.17196", "pdf": "https://arxiv.org/pdf/2509.17196", "abs": "https://arxiv.org/abs/2509.17196", "authors": ["Xuyang Ge", "Wentao Shu", "Jiaxing Wu", "Yunhua Zhou", "Zhengfu He", "Xipeng Qiu"], "title": "Evolution of Concepts in Language Model Pre-Training", "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 25 figures", "summary": "Language models obtain extensive capabilities through pre-training. However,\nthe pre-training process remains a black box. In this work, we track linear\ninterpretable feature evolution across pre-training snapshots using a sparse\ndictionary learning method called crosscoders. We find that most features begin\nto form around a specific point, while more complex patterns emerge in later\ntraining stages. Feature attribution analyses reveal causal connections between\nfeature evolution and downstream performance. Our feature-level observations\nare highly consistent with previous findings on Transformer's two-stage\nlearning process, which we term a statistical learning phase and a feature\nlearning phase. Our work opens up the possibility to track fine-grained\nrepresentation progress during language model learning dynamics.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u65b9\u6cd5\u8ffd\u8e2a\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7279\u5f81\u7684\u6f14\u53d8\uff0c\u5e76\u53d1\u73b0\u7279\u5f81\u6f14\u53d8\u4e0e\u4e0b\u6e38\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u56e0\u679c\u5173\u7cfb\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u9884\u8bad\u7ec3\u83b7\u5f97\u5e7f\u6cdb\u7684\u80fd\u529b\uff0c\u4f46\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4ecd\u7136\u662f\u4e00\u4e2a\u9ed1\u7bb1\u3002\u6211\u4eec\u9700\u8981\u7406\u89e3\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7279\u5f81\u7684\u6f14\u53d8\u53ca\u5176\u5bf9\u4e0b\u6e38\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4e00\u79cd\u79f0\u4e3acrosscoders\u7684\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u65b9\u6cd5\uff0c\u8ffd\u8e2a\u9884\u8bad\u7ec3\u5feb\u7167\u4e2d\u7ebf\u6027\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u6f14\u53d8\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u5927\u591a\u6570\u7279\u5f81\u5728\u7279\u5b9a\u70b9\u5f00\u59cb\u5f62\u6210\uff0c\u800c\u66f4\u590d\u6742\u7684\u6a21\u5f0f\u5728\u540e\u671f\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0\u3002\u7279\u5f81\u5f52\u56e0\u5206\u6790\u63ed\u793a\u4e86\u7279\u5f81\u6f14\u53d8\u4e0e\u4e0b\u6e38\u6027\u80fd\u4e4b\u95f4\u7684\u56e0\u679c\u8054\u7cfb\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u8ddf\u8e2a\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u52a8\u6001\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8868\u793a\u8fdb\u5c55\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.17209", "pdf": "https://arxiv.org/pdf/2509.17209", "abs": "https://arxiv.org/abs/2509.17209", "authors": ["Lourdes Moreno", "Jesus M. Sanchez-Gomez", "Marco Antonio Sanchez-Escudero", "Paloma Mart\u00ednez"], "title": "Prompt-Based Simplification for Plain Language using Spanish Language Models", "categories": ["cs.CL"], "comment": "11 pages, 7 tables,", "summary": "This paper describes the participation of HULAT-UC3M in CLEARS 2025 Subtask\n1: Adaptation of Text to Plain Language (PL) in Spanish. We explored strategies\nbased on models trained on Spanish texts, including a zero-shot configuration\nusing prompt engineering and a fine-tuned version with Low-Rank Adaptation\n(LoRA). Different strategies were evaluated on representative internal subsets\nof the training data, using the official task metrics, cosine similarity (SIM)\nand the Fern\\'andez-Huerta readability index (FH) to guide the selection of the\noptimal model and prompt combination. The final system was selected for its\nbalanced and consistent performance, combining normalization steps, the\nRigoChat-7B-v2 model, and a dedicated PL-oriented prompt. It ranked first in\nsemantic similarity (SIM = 0.75), however, fourth in readability (FH = 69.72).\nWe also discuss key challenges related to training data heterogeneity and the\nlimitations of current evaluation metrics in capturing both linguistic clarity\nand content preservation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86HULAT-UC3M\u5728CLEARS 2025\u5b50\u4efb\u52a11\u4e2d\u7684\u53c2\u4e0e\u60c5\u51b5\uff0c\u63a2\u7d22\u4e86\u57fa\u4e8e\u897f\u73ed\u7259\u8bed\u6587\u672c\u8bad\u7ec3\u7684\u6a21\u578b\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5f52\u4e00\u5316\u6b65\u9aa4\u3001RigoChat-7B-v2\u6a21\u578b\u548c\u4e13\u95e8\u7684PL\u5bfc\u5411\u63d0\u793a\u7684\u6700\u7ec8\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u65b9\u9762\u6392\u540d\u7b2c\u4e00\uff0c\u4f46\u5728\u53ef\u8bfb\u6027\u65b9\u9762\u6392\u540d\u7b2c\u56db\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u8fd8\u63a2\u8ba8\u4e86\u8bad\u7ec3\u6570\u636e\u5f02\u8d28\u6027\u548c\u5f53\u524d\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u9002\u7528\u4e8e\u897f\u73ed\u7259\u8bed\u6587\u672c\u5230\u666e\u901a\u8bed\u8a00\u8f6c\u6362\u7684\u6a21\u578b\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u57fa\u4e8e\u897f\u73ed\u7259\u8bed\u6587\u672c\u8bad\u7ec3\u7684\u6a21\u578b\u7b56\u7565\uff0c\u5305\u62ec\u96f6\u6837\u672c\u914d\u7f6e\uff08\u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\uff09\u548c\u5fae\u8c03\u7248\u672c\uff08\u4f4e\u79e9\u9002\u5e94\uff0cLoRA\uff09\u3002\u901a\u8fc7\u5b98\u65b9\u4efb\u52a1\u6307\u6807\uff08\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548cFern\u00e1ndez-Huerta\u53ef\u8bfb\u6027\u6307\u6570\uff09\u8bc4\u4f30\u4e86\u4e0d\u540c\u7b56\u7565\uff0c\u5e76\u9009\u62e9\u4e86\u6700\u4f73\u6a21\u578b\u548c\u63d0\u793a\u7ec4\u5408\u3002", "result": "\u6700\u7ec8\u7cfb\u7edf\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u65b9\u9762\u6392\u540d\u7b2c\u4e00\uff08SIM = 0.75\uff09\uff0c\u4f46\u5728\u53ef\u8bfb\u6027\u65b9\u9762\u6392\u540d\u7b2c\u56db\uff08FH = 69.72\uff09\u3002\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86\u8bad\u7ec3\u6570\u636e\u5f02\u8d28\u6027\u548c\u5f53\u524d\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u8ba8\u8bba\u4e86HULAT-UC3M\u5728CLEARS 2025\u5b50\u4efb\u52a11\u4e2d\u7684\u53c2\u4e0e\u60c5\u51b5\uff0c\u63a2\u7d22\u4e86\u57fa\u4e8e\u897f\u73ed\u7259\u8bed\u6587\u672c\u8bad\u7ec3\u7684\u6a21\u578b\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5f52\u4e00\u5316\u6b65\u9aa4\u3001RigoChat-7B-v2\u6a21\u578b\u548c\u4e13\u95e8\u7684PL\u5bfc\u5411\u63d0\u793a\u7684\u6700\u7ec8\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u65b9\u9762\u6392\u540d\u7b2c\u4e00\uff0c\u4f46\u5728\u53ef\u8bfb\u6027\u65b9\u9762\u6392\u540d\u7b2c\u56db\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u8fd8\u63a2\u8ba8\u4e86\u8bad\u7ec3\u6570\u636e\u5f02\u8d28\u6027\u548c\u5f53\u524d\u8bc4\u4f30\u6307\u6807\u5728\u6355\u6349\u8bed\u8a00\u6e05\u6670\u5ea6\u548c\u5185\u5bb9\u4fdd\u7559\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.17249", "pdf": "https://arxiv.org/pdf/2509.17249", "abs": "https://arxiv.org/abs/2509.17249", "authors": ["Kuang-Da Wang", "Shuoyang Ding", "Chao-Han Huck Yang", "Ping-Chun Hsieh", "Wen-Chih Peng", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "Extending Automatic Machine Translation Evaluation to Book-Length Documents", "categories": ["cs.CL"], "comment": "Accepted for EMNLP 2025 main conference", "summary": "Despite Large Language Models (LLMs) demonstrating superior translation\nperformance and long-context capabilities, evaluation methodologies remain\nconstrained to sentence-level assessment due to dataset limitations, token\nnumber restrictions in metrics, and rigid sentence boundary requirements. We\nintroduce SEGALE, an evaluation scheme that extends existing automatic metrics\nto long-document translation by treating documents as continuous text and\napplying sentence segmentation and alignment methods. Our approach enables\npreviously unattainable document-level evaluation, handling translations of\narbitrary length generated with document-level prompts while accounting for\nunder-/over-translations and varied sentence boundaries. Experiments show our\nscheme significantly outperforms existing long-form document evaluation\nschemes, while being comparable to evaluations performed with groundtruth\nsentence alignments. Additionally, we apply our scheme to book-length texts and\nnewly demonstrate that many open-weight LLMs fail to effectively translate\ndocuments at their reported maximum context lengths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SEGALE\uff0c\u4e00\u79cd\u7528\u4e8e\u957f\u6587\u6863\u7ffb\u8bd1\u8bc4\u4f30\u7684\u65b0\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u957f\u5ea6\u7684\u7ffb\u8bd1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u53d7\u5230\u6570\u636e\u96c6\u9650\u5236\u3001\u5ea6\u91cf\u4e2d\u7684\u6807\u8bb0\u6570\u91cf\u9650\u5236\u548c\u4e25\u683c\u7684\u53e5\u5b50\u8fb9\u754c\u8981\u6c42\u7684\u7ea6\u675f\uff0c\u65e0\u6cd5\u8fdb\u884c\u6587\u6863\u7ea7\u522b\u7684\u8bc4\u4f30\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86SEGALE\uff0c\u8fd9\u662f\u4e00\u79cd\u8bc4\u4f30\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u6587\u6863\u89c6\u4e3a\u8fde\u7eed\u6587\u672c\u5e76\u5e94\u7528\u53e5\u5b50\u5206\u5272\u548c\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u81ea\u52a8\u5ea6\u91cf\u6269\u5c55\u5230\u957f\u6587\u6863\u7ffb\u8bd1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6848\u5728\u957f\u6587\u6863\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u957f\u5ea6\u7684\u7ffb\u8bd1\uff0c\u5e76\u8003\u8651\u4e86\u7ffb\u8bd1\u4e0d\u8db3\u6216\u8fc7\u5ea6\u4ee5\u53ca\u4e0d\u540c\u7684\u53e5\u5b50\u8fb9\u754c\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6848\u5728\u957f\u6587\u6863\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u957f\u6587\u672c\u6587\u6863\u8bc4\u4f30\u65b9\u6848\uff0c\u540c\u65f6\u4e0e\u4f7f\u7528\u771f\u5b9e\u53e5\u5b50\u5bf9\u9f50\u7684\u8bc4\u4f30\u76f8\u5f53\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u8be5\u65b9\u6848\u5e94\u7528\u4e8e\u4e66\u7c4d\u957f\u5ea6\u7684\u6587\u672c\uff0c\u5e76\u9996\u6b21\u8bc1\u660e\u8bb8\u591a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5728\u5176\u62a5\u544a\u7684\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u6709\u6548\u7ffb\u8bd1\u6587\u6863\u3002"}}
{"id": "2509.17276", "pdf": "https://arxiv.org/pdf/2509.17276", "abs": "https://arxiv.org/abs/2509.17276", "authors": ["Runjia Zeng", "James Chenhao Liang", "Cheng Han", "Zhiwen Cao", "Jiahao Liu", "Xiaojun Quan", "Yingjie Victor Chen", "Lifu Huang", "Tong Geng", "Qifan Wang", "Dongfang Liu"], "title": "Probabilistic Token Alignment for Large Language Model Fusion", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "NeurIPS 2025", "summary": "Training large language models (LLMs) from scratch can yield models with\nunique functionalities and strengths, but it is costly and often leads to\nredundant capabilities. A more cost-effective alternative is to fuse existing\npre-trained LLMs with different architectures into a more powerful model.\nHowever, a key challenge in existing model fusion is their dependence on\nmanually predefined vocabulary alignment, which may not generalize well across\ndiverse contexts, leading to performance degradation in several evaluation. To\nsolve this, we draw inspiration from distribution learning and propose the\nprobabilistic token alignment method as a general and soft mapping for\nalignment, named as PTA-LLM. Our approach innovatively reformulates token\nalignment into a classic mathematical problem: optimal transport, seamlessly\nleveraging distribution-aware learning to facilitate more coherent model\nfusion. Apart from its inherent generality, PTA-LLM exhibits interpretability\nfrom a distributional perspective, offering insights into the essence of the\ntoken alignment. Empirical results demonstrate that probabilistic token\nalignment enhances the target model's performance across multiple capabilities.\nOur code is avaliable at https://runjia.tech/neurips_pta-llm/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPTA-LLM\u7684\u6982\u7387\u6807\u8bb0\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6807\u8bb0\u5bf9\u9f50\u8f6c\u5316\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u901a\u7528\u548c\u8f6f\u6027\u7684\u5bf9\u9f50\u65b9\u5f0f\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u52a8\u9884\u5b9a\u4e49\u7684\u8bcd\u6c47\u5bf9\u9f50\uff0c\u8fd9\u53ef\u80fd\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u65e0\u6cd5\u5f88\u597d\u5730\u6cdb\u5316\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u548c\u8f6f\u6027\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPTA-LLM\u7684\u6982\u7387\u6807\u8bb0\u5bf9\u9f50\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u6807\u8bb0\u5bf9\u9f50\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e00\u4e2a\u7ecf\u5178\u7684\u6570\u5b66\u95ee\u9898\uff1a\u6700\u4f18\u4f20\u8f93\uff0c\u5e76\u5229\u7528\u5206\u5e03\u611f\u77e5\u5b66\u4e60\u6765\u4fc3\u8fdb\u66f4\u8fde\u8d2f\u7684\u6a21\u578b\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6982\u7387\u6807\u8bb0\u5bf9\u9f50\u65b9\u6cd5\u80fd\u591f\u63d0\u5347\u76ee\u6807\u6a21\u578b\u5728\u591a\u4e2a\u80fd\u529b\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6982\u7387\u6807\u8bb0\u5bf9\u9f50\u65b9\u6cd5\u80fd\u591f\u63d0\u5347\u76ee\u6807\u6a21\u578b\u5728\u591a\u4e2a\u80fd\u529b\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17289", "pdf": "https://arxiv.org/pdf/2509.17289", "abs": "https://arxiv.org/abs/2509.17289", "authors": ["Sydney Anuyah", "Mehedi Mahmud Kaushik", "Krishna Dwarampudi", "Rakesh Shiradkar", "Arjan Durresi", "Sunandan Chakraborty"], "title": "Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling", "categories": ["cs.CL"], "comment": null, "summary": "We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting\nsentence-level knowledge graphs by combining robust coreference resolution with\nsyntactic sentence decomposition. Using our model, we contribute a dataset of\nover 150,000 knowledge triples, which is open source. We also contribute a\ntraining corpus of 7248 rows for sentence complexity, 190 rows of gold human\nannotations for co-reference resolution using open source lung-cancer abstracts\nfrom PubMed, 900 rows of gold human annotations for sentence conversion\npolicies, and 398 triples of gold human annotations. We systematically select\noptimal prompt-model pairs across five complexity categories, showing that\nhybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match\naccuracy on sentence simplification. On relation extraction (RE), our pipeline\nachieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the\nart, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on\nWiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference\nand decomposition increases recall on rare relations by over 20%. Code and\ndataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025", "AI": {"tldr": "CoDe-KG \u662f\u4e00\u4e2a\u7528\u4e8e\u63d0\u53d6\u53e5\u5b50\u7ea7\u77e5\u8bc6\u56fe\u8c31\u7684\u5f00\u6e90\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u901a\u8fc7\u7ed3\u5408\u7a33\u5065\u7684\u5171\u6307\u89e3\u6790\u548c\u53e5\u6cd5\u53e5\u5b50\u5206\u89e3\uff0c\u5b9e\u73b0\u4e86\u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u56fe\u8c31\u63d0\u53d6\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u53e5\u5b50\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u53e5\u5b50\u7ea7\u77e5\u8bc6\u56fe\u8c31\u3002", "method": "CoDe-KG \u7ed3\u5408\u4e86\u7a33\u5065\u7684\u5171\u6307\u89e3\u6790\u548c\u53e5\u6cd5\u53e5\u5b50\u5206\u89e3\uff0c\u901a\u8fc7\u7cfb\u7edf\u9009\u62e9\u6700\u4f73\u63d0\u793a-\u6a21\u578b\u5bf9\u6765\u63d0\u9ad8\u53e5\u5b50\u7b80\u5316\u548c\u5173\u7cfb\u62bd\u53d6\u7684\u51c6\u786e\u6027\u3002", "result": "CoDe-KG \u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f8b\u5982\u5728 REBEL \u4e0a\u8fbe\u5230 65.8% \u7684 macro-F1\uff0c\u5728 WebNLG2 \u4e0a\u8fbe\u5230 75.7% \u7684 micro-F1\u3002\u6b64\u5916\uff0c\u6574\u5408\u5171\u6307\u548c\u5206\u89e3\u53ef\u4ee5\u63d0\u9ad8\u7f55\u89c1\u5173\u7cfb\u7684\u53ec\u56de\u7387\u3002", "conclusion": "CoDe-KG \u662f\u4e00\u4e2a\u7528\u4e8e\u63d0\u53d6\u53e5\u5b50\u7ea7\u77e5\u8bc6\u56fe\u8c31\u7684\u5f00\u6e90\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u901a\u8fc7\u7ed3\u5408\u7a33\u5065\u7684\u5171\u6307\u89e3\u6790\u548c\u53e5\u6cd5\u53e5\u5b50\u5206\u89e3\u3002\u8be5\u6a21\u578b\u8d21\u732e\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7150,000\u4e2a\u77e5\u8bc6\u4e09\u5143\u7ec4\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u4e2a\u8bad\u7ec3\u8bed\u6599\u5e93\u548c\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.17292", "pdf": "https://arxiv.org/pdf/2509.17292", "abs": "https://arxiv.org/abs/2509.17292", "authors": ["Jun Seo Kim", "Hyemi Kim", "Woo Joo Oh", "Hongjin Cho", "Hochul Lee", "Hye Hyeon Kim"], "title": "Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cognitive distortions have been closely linked to mental health disorders,\nyet their automatic detection remained challenging due to contextual ambiguity,\nco-occurrence, and semantic overlap. We proposed a novel framework that\ncombines Large Language Models (LLMs) with Multiple-Instance Learning (MIL)\narchitecture to enhance interpretability and expression-level reasoning. Each\nutterance was decomposed into Emotion, Logic, and Behavior (ELB) components,\nwhich were processed by LLMs to infer multiple distortion instances, each with\na predicted type, expression, and model-assigned salience score. These\ninstances were integrated via a Multi-View Gated Attention mechanism for final\nclassification. Experiments on Korean (KoACD) and English (Therapist QA)\ndatasets demonstrate that incorporating ELB and LLM-inferred salience scores\nimproves classification performance, especially for distortions with high\ninterpretive ambiguity. Our results suggested a psychologically grounded and\ngeneralizable approach for fine-grained reasoning in mental health NLP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u5b9e\u4f8b\u5b66\u4e60\u67b6\u6784\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5fc3\u7406\u5065\u5eb7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u8ba4\u77e5\u626d\u66f2\u7684\u81ea\u52a8\u68c0\u6d4b\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u8ba4\u77e5\u626d\u66f2\u4e0e\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u7531\u4e8e\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u3001\u5171\u73b0\u548c\u8bed\u4e49\u91cd\u53e0\uff0c\u5176\u81ea\u52a8\u68c0\u6d4b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u67b6\u6784\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u8868\u8fbe\u6c34\u5e73\u7684\u63a8\u7406\u3002\u6bcf\u4e2a\u8bdd\u8bed\u88ab\u5206\u89e3\u4e3a\u60c5\u611f\u3001\u903b\u8f91\u548c\u884c\u4e3a\uff08ELB\uff09\u7ec4\u4ef6\uff0c\u7531LLMs\u5904\u7406\u4ee5\u63a8\u65ad\u591a\u4e2a\u626d\u66f2\u5b9e\u4f8b\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u90fd\u6709\u9884\u6d4b\u7c7b\u578b\u3001\u8868\u8fbe\u548c\u6a21\u578b\u5206\u914d\u7684\u663e\u8457\u6027\u5206\u6570\u3002\u8fd9\u4e9b\u5b9e\u4f8b\u901a\u8fc7\u591a\u89c6\u56fe\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u6574\u5408\u4ee5\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\u3002", "result": "\u5728\u97e9\u8bed\uff08KoACD\uff09\u548c\u82f1\u8bed\uff08Therapist QA\uff09\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408ELB\u548cLLM\u63a8\u65ad\u7684\u663e\u8457\u6027\u5206\u6570\u53ef\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u9ad8\u89e3\u91ca\u6b67\u4e49\u7684\u626d\u66f2\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u662f\u4e00\u79cd\u5fc3\u7406\u4e0a\u6709\u6839\u636e\u4e14\u53ef\u63a8\u5e7f\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3002"}}
{"id": "2509.17317", "pdf": "https://arxiv.org/pdf/2509.17317", "abs": "https://arxiv.org/abs/2509.17317", "authors": ["Dan John Velasco", "Matthew Theodore Roque"], "title": "Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Most languages lack sufficient data for large-scale monolingual pretraining,\ncreating a \"data wall.\" Multilingual pretraining helps but is limited by\nlanguage imbalance and the \"curse of multilinguality.\" An alternative is to\ntranslate high-resource text with machine translation (MT), which raises three\nquestions: (1) How does MT-derived data scale with model capacity? (2) Can\nsource-side transformations (e.g., simplifying English with an LLM) improve\ngeneralization to native text? (3) How well do models pretrained on MT-derived\ndata adapt when continually trained on limited native text? We investigate\nthese questions by translating English into Indonesian and Tamil--two\ntypologically distant, lower-resource languages--and pretraining GPT-2 models\n(124M-774M) on native or MT-derived corpora from raw and LLM-simplified\nEnglish. We evaluate cross-entropy loss on native text, along with accuracy on\nsyntactic probes and downstream tasks. Our results show that (1) MT-pretrained\nmodels benefit from scaling; (2) source-side simplification harms\ngeneralization to native text; and (3) adapting MT-pretrained models on native\ntext often yields better performance than native-only models, even with less\nnative data. However, tasks requiring cultural nuance (e.g., toxicity\ndetection) demand more exposure to native data.", "AI": {"tldr": "This paper explores the effectiveness of using machine translation-derived data for pretraining models in low-resource languages, finding that scaling and adapting on native text can improve performance, but certain tasks still require more native data.", "motivation": "Most languages lack sufficient data for large-scale monolingual pretraining, creating a 'data wall.' Multilingual pretraining is limited by language imbalance and the 'curse of multilinguality.'", "method": "The paper translates English into Indonesian and Tamil and pretrains GPT-2 models on native or MT-derived corpora from raw and LLM-simplified English.", "result": "MT-pretrained models benefit from scaling; source-side simplification harms generalization to native text; adapting MT-pretrained models on native text often yields better performance than native-only models, even with less native data. However, tasks requiring cultural nuance demand more exposure to native data.", "conclusion": "MT-pretrained models can benefit from scaling and adapting on native text, but tasks requiring cultural nuance still need more native data."}}
{"id": "2509.17348", "pdf": "https://arxiv.org/pdf/2509.17348", "abs": "https://arxiv.org/abs/2509.17348", "authors": ["Yujie Feng", "Jian Li", "Xiaoyu Dong", "Pengfei Xu", "Xiaohui Zhou", "Yujia Zhang", "Zexin LU", "Yasha Wang", "Alan Zhao", "Xu Chu", "Xiao-Ming Wu"], "title": "AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Continual learning (CL) is essential for deploying large language models\n(LLMs) in dynamic real-world environments without the need for costly\nretraining. Recent model merging-based methods have attracted significant\nattention, but they still struggle to effectively manage the trade-off between\nlearning new knowledge and preventing forgetting, a challenge largely stemming\nfrom suboptimal number of merges and merging frequency. In this paper, we\nintroduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework\nthat utilizes learning and forgetting signals from the training trajectory to\ndynamically monitor the model's training status. Guided by dynamic monitoring,\nthe training trajectory-guided merge controller adaptively determines the\ntiming and frequency of iterative fusion, while the rehearsal-based knowledge\nfusion module computes the merging weights and executes the fusion.\nComprehensive experiments on three CL benchmarks with various model sizes (from\n770M to 13B) demonstrate that AimMerging achieves significant performance\nimprovements over existing state-of-the-art methods, with an average relative\nimprovement of 80% and 59% on FWT and BWT, respectively. The source code is\nprovided for reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6AimMerging\uff0c\u901a\u8fc7\u52a8\u6001\u76d1\u63a7\u8bad\u7ec3\u8f68\u8ff9\u4e2d\u7684\u5b66\u4e60\u548c\u9057\u5fd8\u4fe1\u53f7\uff0c\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u8fed\u4ee3\u878d\u5408\u7684\u65f6\u95f4\u548c\u9891\u7387\uff0c\u5e76\u8ba1\u7b97\u878d\u5408\u6743\u91cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cAimMerging\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "Recent model merging-based methods have attracted significant attention, but they still struggle to effectively manage the trade-off between learning new knowledge and preventing forgetting, a challenge largely stemming from suboptimal number of merges and merging frequency.", "method": "AimMerging, a novel CL framework that utilizes learning and forgetting signals from the training trajectory to dynamically monitor the model's training status. The training trajectory-guided merge controller adaptively determines the timing and frequency of iterative fusion, while the rehearsal-based knowledge fusion module computes the merging weights and executes the fusion.", "result": "Comprehensive experiments on three CL benchmarks with various model sizes (from 770M to 13B) demonstrate that AimMerging achieves significant performance improvements over existing state-of-the-art methods, with an average relative improvement of 80% and 59% on FWT and BWT, respectively.", "conclusion": "AimMerging achieves significant performance improvements over existing state-of-the-art methods, with an average relative improvement of 80% and 59% on FWT and BWT, respectively."}}
{"id": "2509.17349", "pdf": "https://arxiv.org/pdf/2509.17349", "abs": "https://arxiv.org/abs/2509.17349", "authors": ["Peter Pol\u00e1k", "Sara Papi", "Luisa Bentivogli", "Ond\u0159ej Bojar"], "title": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Simultaneous speech-to-text translation (SimulST) systems have to balance\ntranslation quality with latency--the delay between speech input and the\ntranslated output. While quality evaluation is well established, accurate\nlatency measurement remains a challenge. Existing metrics often produce\ninconsistent or misleading results, especially in the widely used short-form\nsetting, where speech is artificially presegmented. In this paper, we present\nthe first comprehensive analysis of SimulST latency metrics across language\npairs, systems, and both short- and long-form regimes. We uncover a structural\nbias in current metrics related to segmentation that undermines fair and\nmeaningful comparisons. To address this, we introduce YAAL (Yet Another Average\nLagging), a refined latency metric that delivers more accurate evaluations in\nthe short-form regime. We extend YAAL to LongYAAL for unsegmented audio and\npropose SoftSegmenter, a novel resegmentation tool based on word-level\nalignment. Our experiments show that YAAL and LongYAAL outperform popular\nlatency metrics, while SoftSegmenter enhances alignment quality in long-form\nevaluation, together enabling more reliable assessments of SimulST systems.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86SimulST\u7cfb\u7edf\u7684\u5ef6\u8fdf\u5ea6\u91cf\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u5ea6\u91cf\u65b9\u6cd5\u548c\u91cd\u65b0\u5206\u5272\u5de5\u5177\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5ef6\u8fdf\u5ea6\u91cf\u65b9\u6cd5\u5728\u77ed\u683c\u5f0f\u8bbe\u7f6e\u4e2d\u5b58\u5728\u4e0d\u4e00\u81f4\u6216\u8bef\u5bfc\u6027\u7684\u7ed3\u679c\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5bf9SimulST\u5ef6\u8fdf\u5ea6\u91cf\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u63d0\u51fa\u4e86YAAL\u548cLongYAAL\u4e24\u79cd\u6539\u8fdb\u7684\u5ef6\u8fdf\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86SoftSegmenter\u5de5\u5177\u8fdb\u884c\u91cd\u65b0\u5206\u5272\u3002", "result": "YAAL\u548cLongYAAL\u5728\u77ed\u683c\u5f0f\u548c\u957f\u683c\u5f0f\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u6d41\u884c\u7684\u5ef6\u8fdf\u5ea6\u91cf\u65b9\u6cd5\uff0c\u800cSoftSegmenter\u63d0\u9ad8\u4e86\u957f\u683c\u5f0f\u8bc4\u4f30\u4e2d\u7684\u5bf9\u9f50\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86YAAL\u548cLongYAAL\u4e24\u79cd\u6539\u8fdb\u7684\u5ef6\u8fdf\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86SoftSegmenter\u5de5\u5177\uff0c\u4ee5\u63d0\u9ad8SimulST\u7cfb\u7edf\u7684\u8bc4\u4f30\u53ef\u9760\u6027\u3002"}}
{"id": "2509.17367", "pdf": "https://arxiv.org/pdf/2509.17367", "abs": "https://arxiv.org/abs/2509.17367", "authors": ["Haoyang Chen", "Kumiko Tanaka-Ishii"], "title": "Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs", "categories": ["cs.CL"], "comment": "to be published in Text, Speech, and Dialogue (TSD 2025)", "summary": "We present a comparative analysis of text complexity across domains using\nscale-free metrics. We quantify linguistic complexity via Heaps' exponent\n$\\beta$ (vocabulary growth), Taylor's exponent $\\alpha$ (word-frequency\nfluctuation scaling), compression rate $r$ (redundancy), and entropy. Our\ncorpora span three domains: legal documents (statutes, cases, deeds) as a\nspecialized domain, general natural language texts (literature, Wikipedia), and\nAI-generated (GPT) text. We find that legal texts exhibit slower vocabulary\ngrowth (lower $\\beta$) and higher term consistency (higher $\\alpha$) than\ngeneral texts. Within legal domain, statutory codes have the lowest $\\beta$ and\nhighest $\\alpha$, reflecting strict drafting conventions, while cases and deeds\nshow higher $\\beta$ and lower $\\alpha$. In contrast, GPT-generated text shows\nthe statistics more aligning with general language patterns. These results\ndemonstrate that legal texts exhibit domain-specific structures and\ncomplexities, which current generative models do not fully replicate.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u9886\u57df\u6587\u672c\u7684\u8bed\u8a00\u590d\u6742\u6027\uff0c\u53d1\u73b0\u6cd5\u5f8b\u6587\u672c\u5177\u6709\u72ec\u7279\u7684\u7ed3\u6784\u548c\u590d\u6742\u6027\uff0c\u800c\u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\u672a\u80fd\u5b8c\u5168\u590d\u5236\u8fd9\u4e9b\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u6cd5\u5f8b\u6587\u672c\u4e0e\u5176\u4ed6\u7c7b\u578b\u6587\u672c\u5728\u8bed\u8a00\u590d\u6742\u6027\u4e0a\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u5f53\u524d\u751f\u6210\u6a21\u578b\u662f\u5426\u80fd\u591f\u51c6\u786e\u518d\u73b0\u8fd9\u4e9b\u7279\u5f81\u3002", "method": "\u901a\u8fc7Heaps\u6307\u6570\u03b2\uff08\u8bcd\u6c47\u589e\u957f\uff09\u3001Taylor\u6307\u6570\u03b1\uff08\u8bcd\u9891\u6ce2\u52a8\u5c3a\u5ea6\uff09\u3001\u538b\u7f29\u7387r\uff08\u5197\u4f59\uff09\u548c\u71b5\u6765\u91cf\u5316\u8bed\u8a00\u590d\u6742\u6027\uff0c\u5e76\u5bf9\u6cd5\u5f8b\u6587\u4ef6\u3001\u666e\u901a\u81ea\u7136\u8bed\u8a00\u6587\u672c\u548cAI\u751f\u6210\u6587\u672c\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u6cd5\u5f8b\u6587\u672c\u663e\u793a\u51fa\u8f83\u6162\u7684\u8bcd\u6c47\u589e\u957f\uff08\u8f83\u4f4e\u7684\u03b2\uff09\u548c\u8f83\u9ad8\u7684\u672f\u8bed\u4e00\u81f4\u6027\uff08\u8f83\u9ad8\u7684\u03b1\uff09\uff0c\u800cGPT\u751f\u6210\u6587\u672c\u7684\u7edf\u8ba1\u7279\u5f81\u66f4\u63a5\u8fd1\u666e\u901a\u8bed\u8a00\u6a21\u5f0f\u3002", "conclusion": "\u6cd5\u5f8b\u6587\u672c\u8868\u73b0\u51fa\u7279\u5b9a\u9886\u57df\u7684\u7ed3\u6784\u548c\u590d\u6742\u6027\uff0c\u800c\u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\u5c1a\u672a\u5b8c\u5168\u590d\u5236\u8fd9\u4e9b\u7279\u5f81\u3002"}}
{"id": "2509.17377", "pdf": "https://arxiv.org/pdf/2509.17377", "abs": "https://arxiv.org/abs/2509.17377", "authors": ["Hannah Bansal", "Kemal Kurniawan", "Lea Frermann"], "title": "Robustness of Neurosymbolic Reasoners on First-Order Logic Problems", "categories": ["cs.CL"], "comment": null, "summary": "Recent trends in NLP aim to improve reasoning capabilities in Large Language\nModels (LLMs), with key focus on generalization and robustness to variations in\ntasks. Counterfactual task variants introduce minimal but semantically\nmeaningful changes to otherwise valid first-order logic (FOL) problem instances\naltering a single predicate or swapping roles of constants to probe whether a\nreasoning system can maintain logical consistency under perturbation. Previous\nstudies showed that LLMs becomes brittle on counterfactual variations,\nsuggesting that they often rely on spurious surface patterns to generate\nresponses. In this work, we explore if a neurosymbolic (NS) approach that\nintegrates an LLM and a symbolic logical solver could mitigate this problem.\nExperiments across LLMs of varying sizes show that NS methods are more robust\nbut perform worse overall that purely neural methods. We then propose NSCoT\nthat combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate\nthat while it improves performance, NSCoT still lags behind standard CoT. Our\nanalysis opens research directions for future work.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86NSCoT\u65b9\u6cd5\u3002\u867d\u7136NSCoT\u65b9\u6cd5\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4ecd\u7136\u4e0d\u5982\u6807\u51c6\u7684CoT\u65b9\u6cd5\u3002", "motivation": "\u6700\u8fd1\u7684NLP\u8d8b\u52bf\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u91cd\u70b9\u5728\u4e8e\u6cdb\u5316\u6027\u548c\u5bf9\u4efb\u52a1\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0cLLMs\u5728\u53cd\u4e8b\u5b9e\u53d8\u5316\u4e0a\u8868\u73b0\u51fa\u8106\u5f31\u6027\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u901a\u5e38\u4f9d\u8d56\u4e8e\u8868\u9762\u6a21\u5f0f\u751f\u6210\u54cd\u5e94\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63a2\u7d22\u4e86\u795e\u7ecf\u7b26\u53f7\uff08NS\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u7b26\u53f7\u903b\u8f91\u6c42\u89e3\u5668\uff0c\u5e76\u63d0\u51fa\u4e86NSCoT\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86NS\u65b9\u6cd5\u548c\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cNS\u65b9\u6cd5\u6bd4\u7eaf\u795e\u7ecf\u65b9\u6cd5\u66f4\u9c81\u68d2\uff0c\u4f46\u6027\u80fd\u8f83\u5dee\u3002\u5c3d\u7ba1NSCoT\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u4ecd\u843d\u540e\u4e8e\u6807\u51c6\u7684CoT\u65b9\u6cd5\u3002", "conclusion": "\u6211\u4eec\u7684\u5206\u6790\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.17395", "pdf": "https://arxiv.org/pdf/2509.17395", "abs": "https://arxiv.org/abs/2509.17395", "authors": ["Tianshi Cai", "Guanxu Li", "Nijia Han", "Ce Huang", "Zimu Wang", "Changyu Zeng", "Yuqi Wang", "Jingshi Zhou", "Haiyang Zhang", "Qi Chen", "Yushan Pan", "Shuihua Wang", "Wei Wang"], "title": "FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis", "categories": ["cs.CL"], "comment": "Accepted at FinNLP@EMNLP 2025. Camera-ready version", "summary": "We introduce FinDebate, a multi-agent framework for financial analysis,\nintegrating collaborative debate with domain-specific Retrieval-Augmented\nGeneration (RAG). Five specialized agents, covering earnings, market,\nsentiment, valuation, and risk, run in parallel to synthesize evidence into\nmulti-dimensional insights. To mitigate overconfidence and improve reliability,\nwe introduce a safe debate protocol that enables agents to challenge and refine\ninitial conclusions while preserving coherent recommendations. Experimental\nresults, based on both LLM-based and human evaluations, demonstrate the\nframework's efficacy in producing high-quality analysis with calibrated\nconfidence levels and actionable investment strategies across multiple time\nhorizons.", "AI": {"tldr": "FinDebate\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u534f\u4f5c\u8fa9\u8bba\u4e0e\u9886\u57df\u7279\u5b9a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u7528\u4e8e\u91d1\u878d\u5206\u6790\u3002", "motivation": "\u4e3a\u4e86\u51cf\u8f7b\u8fc7\u5ea6\u81ea\u4fe1\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u751f\u6210\u66f4\u51c6\u786e\u548c\u53ef\u64cd\u4f5c\u7684\u91d1\u878d\u5206\u6790\u3002", "method": "FinDebate\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u534f\u4f5c\u8fa9\u8bba\u4e0e\u9886\u57df\u7279\u5b9a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3002\u4e94\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\u5e76\u884c\u8fd0\u884c\uff0c\u4ee5\u5408\u6210\u8bc1\u636e\u4e3a\u591a\u7ef4\u89c1\u89e3\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u5b89\u5168\u8fa9\u8bba\u534f\u8bae\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u6311\u6218\u548c\u6539\u8fdb\u521d\u59cb\u7ed3\u8bba\uff0c\u540c\u65f6\u4fdd\u6301\u8fde\u8d2f\u7684\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u65f6\u95f4\u8303\u56f4\u5185\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5206\u6790\uff0c\u5e76\u5177\u6709\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u6c34\u5e73\u548c\u53ef\u64cd\u4f5c\u7684\u6295\u8d44\u7b56\u7565\u3002", "conclusion": "FinDebate\u6846\u67b6\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u5206\u6790\u548c\u6821\u51c6\u7f6e\u4fe1\u5ea6\u6c34\u5e73\u4ee5\u53ca\u53ef\u64cd\u4f5c\u7684\u6295\u8d44\u7b56\u7565\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.17396", "pdf": "https://arxiv.org/pdf/2509.17396", "abs": "https://arxiv.org/abs/2509.17396", "authors": ["Minsoo Kim", "Arnav Kundu", "Han-Byul Kim", "Richa Dixit", "Minsik Cho"], "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.", "AI": {"tldr": "EpiCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u56fa\u5b9a\u5185\u5b58\u9884\u7b97\u4e0b\u8fdb\u884c\u957f\u5bf9\u8bdd\u95ee\u7b54\uff0c\u901a\u8fc7\u5757\u7ea7\u9884\u586b\u5145\u548c\u60c5\u666fKV\u538b\u7f29\u6765\u9650\u5236\u7f13\u5b58\u589e\u957f\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u5c42\u9884\u7b97\u5206\u914d\u7b56\u7565\u6765\u5206\u914d\u5185\u5b58\u9884\u7b97\u3002", "motivation": "\u73b0\u6709\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a(i) \u5728\u5b8c\u6574\u4e0a\u4e0b\u6587\u9884\u586b\u5145\u540e\u9a71\u9010\u6761\u76ee\u4f1a\u5bfc\u81f4\u65e0\u754c\u5cf0\u503c\u5185\u5b58\uff0c(ii) \u67e5\u8be2\u4f9d\u8d56\u7684\u9a71\u9010\u4f1a\u5c06\u7f13\u5b58\u7f29\u5c0f\u5230\u5355\u4e2a\u67e5\u8be2\uff0c\u5bfc\u81f4\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u51c6\u786e\u6027\u4e0b\u964d\u3002", "method": "EpiCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5757\u7ea7\u9884\u586b\u5145\u548c\u60c5\u666fKV\u538b\u7f29\u6765\u9650\u5236\u7f13\u5b58\u589e\u957f\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u5c42\u9884\u7b97\u5206\u914d\u7b56\u7565\u6765\u5206\u914d\u5185\u5b58\u9884\u7b97\u3002", "result": "EpiCache\u5728\u4e09\u4e2aLongConvQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe40%\uff0c\u57284-6\u500d\u538b\u7f29\u4e0b\u4fdd\u6301\u63a5\u8fd1\u5b8c\u6574\u7684KV\u51c6\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\u9ad8\u8fbe2.4\u500d\u548c3.5\u500d\u3002", "conclusion": "EpiCache\u5728\u4e09\u4e2aLongConvQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5728\u4e25\u683c\u7684\u8d44\u6e90\u9650\u5236\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u8f6e\u4ea4\u4e92\u3002"}}
{"id": "2509.17399", "pdf": "https://arxiv.org/pdf/2509.17399", "abs": "https://arxiv.org/abs/2509.17399", "authors": ["Pramit Sahoo", "Maharaj Brahma", "Maunendra Sankar Desarkar"], "title": "DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context", "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "Large language models (LLMs) are widely used in various tasks and\napplications. However, despite their wide capabilities, they are shown to lack\ncultural alignment \\citep{ryan-etal-2024-unintended,\nalkhamissi-etal-2024-investigating} and produce biased generations\n\\cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence.\nEvaluation of LLMs for cultural awareness and alignment is particularly\nchallenging due to the lack of proper evaluation metrics and unavailability of\nculturally grounded datasets representing the vast complexity of cultures at\nthe regional and sub-regional levels. Existing datasets for culture specific\nitems (CSIs) focus primarily on concepts at the regional level and may contain\nfalse positives. To address this issue, we introduce a novel CSI dataset for\nIndian culture, belonging to 17 cultural facets. The dataset comprises $\\sim$8k\ncultural concepts from 36 sub-regions. To measure the cultural competence of\nLLMs on a cultural text adaptation task, we evaluate the adaptations using the\nCSIs created, LLM as Judge, and human evaluations from diverse\nsocio-demographic region. Furthermore, we perform quantitative analysis\ndemonstrating selective sub-regional coverage and surface-level adaptations\nacross all considered LLMs. Our dataset is available here:\n\\href{https://huggingface.co/datasets/nlip/DIWALI}{https://huggingface.co/datasets/nlip/DIWALI},\nproject\nwebpage\\footnote{\\href{https://nlip-lab.github.io/nlip/publications/diwali/}{https://nlip-lab.github.io/nlip/publications/diwali/}},\nand our codebase with model outputs can be found here:\n\\href{https://github.com/pramitsahoo/culture-evaluation}{https://github.com/pramitsahoo/culture-evaluation}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u5316\u7279\u5b9a\u9879\uff08CSI\uff09\u6570\u636e\u96c6DIWALI\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u5316\u610f\u8bc6\u548c\u5bf9\u9f50\u65b9\u9762\u7684\u8868\u73b0\u3002\u8be5\u6570\u636e\u96c6\u6db5\u76d6\u4e8617\u4e2a\u6587\u5316\u65b9\u9762\u548c36\u4e2a\u5b50\u533a\u57df\u7684\u6587\u5316\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7LLM\u4f5c\u4e3a\u88c1\u5224\u548c\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5728\u5b50\u533a\u57df\u8986\u76d6\u548c\u8868\u9762\u5c42\u6b21\u9002\u5e94\u65b9\u9762\u5b58\u5728\u9009\u62e9\u6027\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u5316\u7279\u5b9a\u9879\uff08CSIs\uff09\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u533a\u57df\u5c42\u9762\u7684\u6982\u5ff5\uff0c\u53ef\u80fd\u5b58\u5728\u5047\u9633\u6027\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u5370\u5ea6\u6587\u5316\u7684\u65b0\u578bCSI\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u9ad8LLMs\u5728\u6587\u5316\u610f\u8bc6\u548c\u5bf9\u9f50\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684CSI\u6570\u636e\u96c6DIWALI\uff0c\u6db5\u76d6\u4e8617\u4e2a\u6587\u5316\u65b9\u9762\u548c36\u4e2a\u5b50\u533a\u57df\u7684\u6587\u5316\u6982\u5ff5\u3002\u901a\u8fc7\u4f7f\u7528CSIs\u521b\u5efa\u7684\u9002\u5e94\u3001LLM\u4f5c\u4e3a\u88c1\u5224\u4ee5\u53ca\u6765\u81ea\u4e0d\u540c\u793e\u4f1a\u7ecf\u6d4e\u80cc\u666f\u5730\u533a\u7684\u7528\u6237\u8bc4\u4f30\u6765\u8861\u91cfLLMs\u7684\u6587\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u5b9a\u91cf\u5206\u6790\uff0c\u4ee5\u5c55\u793aLLMs\u5728\u5b50\u533a\u57df\u8986\u76d6\u548c\u8868\u9762\u5c42\u6b21\u9002\u5e94\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u672c\u6587\u6784\u5efa\u7684DIWALI\u6570\u636e\u96c6\u5305\u542b\u7ea68000\u4e2a\u6587\u5316\u6982\u5ff5\uff0c\u6db5\u76d636\u4e2a\u5b50\u533a\u57df\u3002\u901a\u8fc7\u8bc4\u4f30LLMs\u5728\u6587\u5316\u6587\u672c\u9002\u5e94\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u5c55\u793a\u4e86LLMs\u5728\u5b50\u533a\u57df\u8986\u76d6\u548c\u8868\u9762\u5c42\u6b21\u9002\u5e94\u65b9\u9762\u7684\u9009\u62e9\u6027\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9488\u5bf9\u5370\u5ea6\u6587\u5316\u7684\u65b0\u578b\u6587\u5316\u7279\u5b9a\u9879\uff08CSI\uff09\u6570\u636e\u96c6DIWALI\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u5316\u610f\u8bc6\u548c\u5bf9\u9f50\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u8bc4\u4f30LLMs\u5728\u6587\u5316\u6587\u672c\u9002\u5e94\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\uff0c\u5c55\u793a\u4e86LLMs\u5728\u5b50\u533a\u57df\u8986\u76d6\u548c\u8868\u9762\u5c42\u6b21\u9002\u5e94\u65b9\u9762\u7684\u9009\u62e9\u6027\u8868\u73b0\u3002"}}
{"id": "2509.17418", "pdf": "https://arxiv.org/pdf/2509.17418", "abs": "https://arxiv.org/abs/2509.17418", "authors": ["Junhong Liang", "Bojun Zhang"], "title": "Vision Language Models Are Not (Yet) Spelling Correctors", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Spelling correction from visual input poses unique challenges for vision\nlanguage models (VLMs), as it requires not only detecting but also correcting\ntextual errors directly within images. We present ReViCo (Real Visual\nCorrection), the first benchmark that systematically evaluates VLMs on\nreal-world visual spelling correction across Chinese and English. ReViCo\ncontains naturally occurring errors collected from real-world image data and\nsupports fine-grained evaluation at both image and token levels. Through\ncomprehensive experiments on representative cascaded (Qwen) and native\n(InternVL) open-source models, as well as closed-source systems (GPT-4o,\nClaude), we show that current VLMs fall significantly short of human\nperformance, particularly in correction. To address these limitations, we\nexplore two solution paradigms: a Joint OCR-Correction pipeline and a\nBackground Information enhanced approach, both of which yield consistent\nperformance gains. Our analysis highlights fundamental limitations of existing\narchitectures and provides actionable insights for advancing multimodal\nspelling correction.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ReViCo\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30VLMs\u5728\u771f\u5b9e\u4e16\u754c\u89c6\u89c9\u62fc\u5199\u7ea0\u6b63\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u4e86\u4e2d\u6587\u548c\u82f1\u6587\u3002\u901a\u8fc7\u5728\u4ee3\u8868\u6027\u7ea7\u8054\uff08Qwen\uff09\u548c\u539f\u751f\uff08InternVL\uff09\u5f00\u6e90\u6a21\u578b\u4ee5\u53ca\u5c01\u95ed\u6e90\u7cfb\u7edf\uff08GPT-4o, Claude\uff09\u4e0a\u7684\u5168\u9762\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5f53\u524dVLMs\u5728\u7ea0\u6b63\u65b9\u9762\u660e\u663e\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\u8303\u5f0f\uff1a\u8054\u5408OCR-\u7ea0\u6b63\u7ba1\u9053\u548c\u80cc\u666f\u4fe1\u606f\u589e\u5f3a\u65b9\u6cd5\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u4ea7\u751f\u4e86\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\u3002\u6211\u4eec\u7684\u5206\u6790\u7a81\u51fa\u4e86\u73b0\u6709\u67b6\u6784\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u62fc\u5199\u7ea0\u6b63\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\u3002", "motivation": "\u89c6\u89c9\u8f93\u5165\u7684\u62fc\u5199\u7ea0\u6b63\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u63d0\u51fa\u4e86\u72ec\u7279\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4e0d\u4ec5\u9700\u8981\u68c0\u6d4b\u8fd8\u9700\u8981\u76f4\u63a5\u5728\u56fe\u50cf\u4e2d\u7ea0\u6b63\u6587\u672c\u9519\u8bef\u3002", "method": "\u6211\u4eec\u63a2\u7d22\u4e86\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\u8303\u5f0f\uff1a\u8054\u5408OCR-\u4fee\u6b63\u7ba1\u9053\u548c\u80cc\u666f\u4fe1\u606f\u589e\u5f3a\u65b9\u6cd5\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u4ea7\u751f\u4e86\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5f53\u524d\u7684VLMs\u5728\u7ea0\u6b63\u65b9\u9762\u660e\u663e\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u7ea0\u6b63\u65b9\u9762\u3002", "conclusion": "\u6211\u4eec\u7684\u5206\u6790\u7a81\u51fa\u4e86\u73b0\u6709\u67b6\u6784\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u62fc\u5199\u7ea0\u6b63\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.17421", "pdf": "https://arxiv.org/pdf/2509.17421", "abs": "https://arxiv.org/abs/2509.17421", "authors": ["Fei Zhao", "Chengqiang Lu", "Yufan Shen", "Qimeng Wang", "Yicheng Qian", "Haoxin Zhang", "Yan Gao", "Yi Wu", "Yao Hu", "Zhen Wu", "Shangyu Xing", "Xinyu Dai"], "title": "RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios", "categories": ["cs.CL", "cs.MM"], "comment": "Findings of EMNLP 2025 camera-ready", "summary": "While various multimodal multi-image evaluation datasets have been emerged,\nbut these datasets are primarily based on English, and there has yet to be a\nChinese multi-image dataset. To fill this gap, we introduce RealBench, the\nfirst Chinese multimodal multi-image dataset, which contains 9393 samples and\n69910 images. RealBench distinguishes itself by incorporating real\nuser-generated content, ensuring high relevance to real-world applications.\nAdditionally, the dataset covers a wide variety of scenes, image resolutions,\nand image structures, further increasing the difficulty of multi-image\nunderstanding. Ultimately, we conduct a comprehensive evaluation of RealBench\nusing 21 multimodal LLMs of different sizes, including closed-source models\nthat support multi-image inputs as well as open-source visual and video models.\nThe experimental results indicate that even the most powerful closed-source\nmodels still face challenges when handling multi-image Chinese scenarios.\nMoreover, there remains a noticeable performance gap of around 71.8\\% on\naverage between open-source visual/video models and closed-source models. These\nresults show that RealBench provides an important research foundation for\nfurther exploring multi-image understanding capabilities in the Chinese\ncontext.", "AI": {"tldr": "RealBench is the first Chinese multimodal multi-image dataset, which highlights the challenges faced by models in handling multi-image Chinese scenarios and shows a significant performance gap between open-source and closed-source models.", "motivation": "To fill the gap of Chinese multi-image datasets, as existing datasets are primarily based on English.", "method": "Introduce RealBench, the first Chinese multimodal multi-image dataset, and conduct a comprehensive evaluation of RealBench using 21 multimodal LLMs of different sizes.", "result": "Even the most powerful closed-source models still face challenges when handling multi-image Chinese scenarios. There remains a noticeable performance gap of around 71.8% on average between open-source visual/video models and closed-source models.", "conclusion": "RealBench provides an important research foundation for further exploring multi-image understanding capabilities in the Chinese context."}}
{"id": "2509.17428", "pdf": "https://arxiv.org/pdf/2509.17428", "abs": "https://arxiv.org/abs/2509.17428", "authors": ["Hyesung Jeon", "Seojune Lee", "Beomseok Kang", "Yulhwa Kim", "Jae-Joon Kim"], "title": "QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models", "categories": ["cs.CL"], "comment": "25 pages, 9 figures, 14 tables", "summary": "The demand for efficient deployment of large language models (LLMs) has\ndriven interest in quantization, which reduces inference cost, and\nparameter-efficient fine-tuning (PEFT), which lowers training overhead. This\nmotivated the development of quantization-aware PEFT to produce accurate yet\nefficient quantized models. In this setting, reducing quantization error prior\nto fine-tuning is crucial for achieving high model accuracy. However, existing\nmethods that rely on low-rank adaptation suffer from limited representational\ncapacity. Recent Fourier-related transform (FT)-based adapters offer greater\nrepresentational power than low-rank adapters, but their direct integration\ninto quantized models often results in ineffective error reduction and\nincreased computational overhead. To overcome these limitations, we propose\nQWHA, a method that integrates FT-based adapters into quantized models by\nemploying the Walsh-Hadamard Transform (WHT) as the transform kernel, together\nwith a novel adapter initialization scheme incorporating adaptive parameter\nselection and value refinement. We demonstrate that QWHA effectively mitigates\nquantization errors while facilitating fine-tuning, and that its design\nsubstantially reduces computational cost. Experimental results show that QWHA\nconsistently outperforms baselines in low-bit quantization accuracy and\nachieves significant training speedups over existing FT-based adapters. The\ncode is available at https://github.com/vantaa89/qwha.", "AI": {"tldr": "QWHA is a method that integrates FT-based adapters into quantized models using the Walsh-Hadamard Transform and a novel initialization scheme, effectively reducing quantization errors and computational costs.", "motivation": "The demand for efficient deployment of large language models (LLMs) has driven interest in quantization and parameter-efficient fine-tuning (PEFT). Quantization-aware PEFT is needed to produce accurate yet efficient quantized models, but existing methods have limitations in representational capacity and error reduction.", "method": "QWHA integrates FT-based adapters into quantized models by employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together with a novel adapter initialization scheme incorporating adaptive parameter selection and value refinement.", "result": "QWHA consistently outperforms baselines in low-bit quantization accuracy and achieves significant training speedups over existing FT-based adapters.", "conclusion": "QWHA effectively mitigates quantization errors while facilitating fine-tuning, and its design substantially reduces computational cost."}}
{"id": "2509.17436", "pdf": "https://arxiv.org/pdf/2509.17436", "abs": "https://arxiv.org/abs/2509.17436", "authors": ["Tong Chen", "Zimu Wang", "Yiyi Miao", "Haoran Luo", "Yuanfei Sun", "Wei Wang", "Zhengyong Jiang", "Procheta Sen", "Jionglong Su"], "title": "MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses", "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025. Camera-ready version", "summary": "Medical fact-checking has become increasingly critical as more individuals\nseek medical information online. However, existing datasets predominantly focus\non human-generated content, leaving the verification of content generated by\nlarge language models (LLMs) relatively unexplored. To address this gap, we\nintroduce MedFact, the first evidence-based Chinese medical fact-checking\ndataset of LLM-generated medical content. It consists of 1,321 questions and\n7,409 claims, mirroring the complexities of real-world medical scenarios. We\nconduct comprehensive experiments in both in-context learning (ICL) and\nfine-tuning settings, showcasing the capability and challenges of current LLMs\non this task, accompanied by an in-depth error analysis to point out key\ndirections for future research. Our dataset is publicly available at\nhttps://github.com/AshleyChenNLP/MedFact.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MedFact\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u533b\u5b66\u5185\u5bb9\u7684\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u80fd\u529b\u548c\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u4eba\u7c7b\u751f\u6210\u7684\u5185\u5bb9\uff0c\u800c\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u7684\u9a8c\u8bc1\u7814\u7a76\u8f83\u5c11\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MedFact\u6570\u636e\u96c6\uff0c\u5305\u542b1321\u4e2a\u95ee\u9898\u548c7409\u4e2a\u58f0\u660e\uff0c\u6a21\u62df\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u533b\u5b66\u573a\u666f\uff0c\u5e76\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u548c\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u6df1\u5165\u7684\u9519\u8bef\u5206\u6790\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86MedFact\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u8bc1\u636e\u7684\u4e2d\u6587\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u533b\u5b66\u5185\u5bb9\u3002"}}
{"id": "2509.17437", "pdf": "https://arxiv.org/pdf/2509.17437", "abs": "https://arxiv.org/abs/2509.17437", "authors": ["Guizhen Chen", "Weiwen Xu", "Hao Zhang", "Hou Pong Chan", "Deli Zhao", "Anh Tuan Luu", "Yu Rong"], "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning", "categories": ["cs.CL"], "comment": "Accepted to EMNLP2025 Findings", "summary": "Recent advancements in reinforcement learning (RL) have enhanced the\nreasoning abilities of large language models (LLMs), yet the impact on\nmultimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like\ngeometric reasoning, MLLMs hallucinate frequently, leading to inaccurate\nreasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps\nthe benefits of reasoning training. To quantify this, we design a\nGeo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric\nconcepts and spatial relationships. Experiments on GeoPQA reveal significant\nshortcomings of MLLMs in visual perception, which constrain RL reward signals\nfor effective training. To address this bottleneck, we propose a two-stage RL\ntraining framework by first enhancing the visual perception of geometric\nstructures, then fostering reasoning capabilities. Applied to\nQwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by\n9.7% and geometric problem solving by 9.1%, compared to the direct reasoning\ntraining approach. Our method also generalizes to other vision-intensive\ndomains like figure understanding, highlighting the importance of perceptual\ngrounding in effective MLLM reasoning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\u4ee5\u63d0\u9ad8\u5176\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8fdb\u6b65\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5bf9\u591a\u6a21\u6001LLMs\uff08MLLMs\uff09\u7684\u5f71\u54cd\u6709\u9650\u3002\u7279\u522b\u662f\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\uff0cMLLMs\u7ecf\u5e38\u51fa\u73b0\u5e7b\u89c9\uff0c\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u63a8\u7406\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2aGeo-Perception Question-Answering (GeoPQA)\u57fa\u51c6\u6765\u91cf\u5316MLLMs\u5728\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684RL\u8bad\u7ec3\u6846\u67b6\uff0c\u9996\u5148\u589e\u5f3a\u51e0\u4f55\u7ed3\u6784\u7684\u89c6\u89c9\u611f\u77e5\uff0c\u7136\u540e\u57f9\u517b\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMLLMs\u5728\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u8fd9\u9650\u5236\u4e86RL\u5956\u52b1\u4fe1\u53f7\u7684\u6709\u6548\u8bad\u7ec3\u3002\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u51e0\u4f55\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u51e0\u4f55\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e869.7%\u548c9.1%\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a8\u5e7f\u5230\u5176\u4ed6\u89c6\u89c9\u5bc6\u96c6\u578b\u9886\u57df\uff0c\u8fd9\u8868\u660e\u611f\u77e5\u57fa\u7840\u5728\u6709\u6548\u7684MLLM\u63a8\u7406\u4e2d\u975e\u5e38\u91cd\u8981\u3002"}}
{"id": "2509.17444", "pdf": "https://arxiv.org/pdf/2509.17444", "abs": "https://arxiv.org/abs/2509.17444", "authors": ["Shohei Hisada", "Endo Sunao", "Himi Yamato", "Shoko Wakamiya", "Eiji Aramaki"], "title": "Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system", "categories": ["cs.CL"], "comment": "draft v0.1", "summary": "This study investigates the applicability of HealthBench, a large-scale,\nrubric-based medical benchmark, to the Japanese context. While robust\nevaluation frameworks are crucial for the safe development of medical LLMs,\nresources in Japanese remain limited, often relying on translated\nmultiple-choice questions. Our research addresses this gap by first\nestablishing a performance baseline, applying a machine-translated version of\nHealthBench's 5,000 scenarios to evaluate both a high-performing multilingual\nmodel (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Second,\nwe employ an LLM-as-a-Judge approach to systematically classify the benchmark's\nscenarios and rubric criteria, identifying \"contextual gaps\" where content is\nmisaligned with Japan's clinical guidelines, healthcare systems, or cultural\nnorms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric\nmismatches and a significant failure in the Japanese-native model, which lacked\nthe required clinical completeness. Furthermore, our classification indicates\nthat while the majority of scenarios are applicable, a substantial portion of\nthe rubric criteria requires localization. This work underscores the\nlimitations of direct benchmark translation and highlights the urgent need for\na context-aware, localized adaptation, a J-HealthBench, to ensure the reliable\nand safe evaluation of medical LLMs in Japan.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86HealthBench\u5728\u65e5\u8bed\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u76f4\u63a5\u7ffb\u8bd1\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e3a\u65e5\u672c\u5f00\u53d1\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u672c\u5730\u5316\u9002\u5e94\u7248\u672c\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5927\u7684\u8bc4\u4f30\u6846\u67b6\u5bf9\u4e8e\u5b89\u5168\u5f00\u53d1\u533b\u7597LLM\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u65e5\u8bed\u8d44\u6e90\u4ecd\u7136\u6709\u9650\uff0c\u901a\u5e38\u4f9d\u8d56\u4e8e\u7ffb\u8bd1\u7684\u591a\u9879\u9009\u62e9\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u5efa\u7acb\u4e86\u4e00\u4e2a\u6027\u80fd\u57fa\u7ebf\uff0c\u5e94\u7528\u4e86HealthBench\u76845000\u4e2a\u573a\u666f\u7684\u673a\u5668\u7ffb\u8bd1\u7248\u672c\u6765\u8bc4\u4f30\u4e00\u4e2a\u9ad8\u6027\u80fd\u591a\u8bed\u8a00\u6a21\u578b\uff08GPT-4.1\uff09\u548c\u4e00\u4e2a\u65e5\u672c\u672c\u5730\u7684\u5f00\u6e90\u6a21\u578b\uff08LLM-jp-3.1\uff09\u3002\u5176\u6b21\uff0c\u6211\u4eec\u91c7\u7528LLM-as-a-Judge\u7684\u65b9\u6cd5\u7cfb\u7edf\u5730\u5206\u7c7b\u4e86\u57fa\u51c6\u7684\u573a\u666f\u548c\u8bc4\u5206\u6807\u51c6\uff0c\u8bc6\u522b\u51fa\u5185\u5bb9\u4e0e\u65e5\u672c\u4e34\u5e8a\u6307\u5357\u3001\u533b\u7597\u7cfb\u7edf\u6216\u6587\u5316\u89c4\u8303\u4e0d\u4e00\u81f4\u7684\u201c\u4e0a\u4e0b\u6587\u5dee\u8ddd\u201d\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u7531\u4e8e\u8bc4\u5206\u6807\u51c6\u4e0d\u5339\u914d\uff0cGPT-4.1\u7684\u8868\u73b0\u7565\u6709\u4e0b\u964d\uff0c\u800c\u65e5\u672c\u672c\u5730\u6a21\u578b\u5219\u663e\u8457\u5931\u8d25\uff0c\u56e0\u4e3a\u5b83\u7f3a\u4e4f\u6240\u9700\u7684\u4e34\u5e8a\u5b8c\u6574\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5206\u7c7b\u8868\u660e\uff0c\u867d\u7136\u5927\u591a\u6570\u573a\u666f\u662f\u9002\u7528\u7684\uff0c\u4f46\u8bc4\u5206\u6807\u51c6\u7684\u76f8\u5f53\u4e00\u90e8\u5206\u9700\u8981\u672c\u5730\u5316\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u76f4\u63a5\u7ffb\u8bd1\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u4e3a\u65e5\u672c\u5f00\u53d1\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u672c\u5730\u5316\u9002\u5e94\u7248\u672c\uff08J-HealthBench\uff09\u7684\u7d27\u8feb\u9700\u6c42\uff0c\u4ee5\u786e\u4fdd\u5728\u65e5\u672c\u8bc4\u4f30\u533b\u7597LLM\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.17445", "pdf": "https://arxiv.org/pdf/2509.17445", "abs": "https://arxiv.org/abs/2509.17445", "authors": ["Chaodong Tong", "Qi Zhang", "Lei Jiang", "Yanbing Liu", "Nannan Sun", "Wei Li"], "title": "Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks", "categories": ["cs.CL"], "comment": "5pages, 5 figures, submit to ICASSP 2026", "summary": "Reliable question answering with large language models (LLMs) is challenged\nby hallucinations, fluent but factually incorrect outputs arising from\nepistemic uncertainty. Existing entropy-based semantic-level uncertainty\nestimation methods are limited by sampling noise and unstable clustering of\nvariable-length answers. We propose Semantic Reformulation Entropy (SRE), which\nimproves uncertainty estimation in two ways. First, input-side semantic\nreformulations produce faithful paraphrases, expand the estimation space, and\nreduce biases from superficial decoder tendencies. Second, progressive,\nenergy-based hybrid clustering stabilizes semantic grouping. Experiments on\nSQuAD and TriviaQA show that SRE outperforms strong baselines, providing more\nrobust and generalizable hallucination detection. These results demonstrate\nthat combining input diversification with multi-signal clustering substantially\nenhances semantic-level uncertainty estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5SRE\uff0c\u901a\u8fc7\u8f93\u5165\u4fa7\u7684\u8bed\u4e49\u91cd\u8ff0\u548c\u57fa\u4e8e\u80fd\u91cf\u7684\u6df7\u5408\u805a\u7c7b\u6765\u63d0\u9ad8\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u53ef\u9760\u7684\u95ee\u7b54\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d7\u5230\u5e7b\u89c9\u7684\u6311\u6218\uff0c\u8fd9\u662f\u7531\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u800c\u4ea7\u751f\u7684\u6d41\u7545\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u8f93\u51fa\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u71b5\u7684\u8bed\u4e49\u7ea7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u53d7\u5230\u91c7\u6837\u566a\u58f0\u548c\u53ef\u53d8\u957f\u5ea6\u7b54\u6848\u4e0d\u7a33\u5b9a\u805a\u7c7b\u7684\u9650\u5236\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u8bed\u4e49\u91cd\u8ff0\u71b5\uff08SRE\uff09\uff0c\u5b83\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u6539\u8fdb\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u9996\u5148\uff0c\u8f93\u5165\u4fa7\u7684\u8bed\u4e49\u91cd\u8ff0\u751f\u6210\u5fe0\u5b9e\u7684\u540c\u4e49\u8bcd\uff0c\u6269\u5c55\u4e86\u4f30\u8ba1\u7a7a\u95f4\uff0c\u5e76\u51cf\u5c11\u4e86\u4ece\u8868\u9762\u89e3\u7801\u5668\u503e\u5411\u4e2d\u7684\u504f\u5dee\u3002\u5176\u6b21\uff0c\u57fa\u4e8e\u80fd\u91cf\u7684\u6e10\u8fdb\u5f0f\u6df7\u5408\u805a\u7c7b\u7a33\u5b9a\u4e86\u8bed\u4e49\u5206\u7ec4\u3002", "result": "\u5728SQuAD\u548cTriviaQA\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSRE\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u548c\u901a\u7528\u7684\u5e7b\u89c9\u68c0\u6d4b\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u8f93\u5165\u591a\u6837\u5316\u4e0e\u591a\u4fe1\u53f7\u805a\u7c7b\u76f8\u7ed3\u5408\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u8bed\u4e49\u7ea7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002"}}
{"id": "2509.17449", "pdf": "https://arxiv.org/pdf/2509.17449", "abs": "https://arxiv.org/abs/2509.17449", "authors": ["Leonor Veloso", "Lea Hirlimann", "Philipp Wicke", "Hinrich Sch\u00fctze"], "title": "SLAyiNG: Towards Queer Language Processing", "categories": ["cs.CL"], "comment": "To be presented at Queer in AI @ NeurIPS 2025 (non-archival)", "summary": "Knowledge of slang is a desirable feature of LLMs in the context of user\ninteraction, as slang often reflects an individual's social identity. Several\nworks on informal language processing have defined and curated benchmarks for\ntasks such as detection and identification of slang. In this paper, we focus on\nqueer slang. Queer slang can be mistakenly flagged as hate speech or can evoke\nnegative responses from LLMs during user interaction. Research efforts so far\nhave not focused explicitly on queer slang. In particular, detection and\nprocessing of queer slang have not been thoroughly evaluated due to the lack of\na high-quality annotated benchmark. To address this gap, we curate SLAyiNG, the\nfirst dataset containing annotated queer slang derived from subtitles, social\nmedia posts, and podcasts, reflecting real-world usage. We describe our data\ncuration process, including the collection of slang terms and definitions,\nscraping sources for examples that reflect usage of these terms, and our\nongoing annotation process. As preliminary results, we calculate\ninter-annotator agreement for human annotators and OpenAI's model o3-mini,\nevaluating performance on the task of sense disambiguation. Reaching an average\nKrippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models\ncan serve as tools for pre-filtering, but the complex and often sensitive\nnature of queer language data requires expert and community-driven annotation\nefforts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SLAyiNG\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5305\u542b\u4ece\u5b57\u5e55\u3001\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u548c\u64ad\u5ba2\u4e2d\u63d0\u53d6\u7684\u6807\u6ce8\u9177\u513f\u4fda\u8bed\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63a2\u8ba8\u4e86\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u9884\u7b5b\u9009\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u76ee\u524d\u7684\u7814\u7a76\u52aa\u529b\u5c1a\u672a\u660e\u786e\u5173\u6ce8\u9177\u513f\u4fda\u8bed\uff0c\u7279\u522b\u662f\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u57fa\u51c6\uff0c\u5bf9\u9177\u513f\u4fda\u8bed\u7684\u68c0\u6d4b\u548c\u5904\u7406\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u6211\u4eec\u6536\u96c6\u4e86\u6765\u81ea\u5b57\u5e55\u3001\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u548c\u64ad\u5ba2\u7684\u6807\u6ce8\u9177\u513f\u4fda\u8bed\u6570\u636e\u96c6SLAyiNG\uff0c\u5e76\u63cf\u8ff0\u4e86\u6211\u4eec\u7684\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\uff0c\u5305\u62ec\u4fda\u8bed\u672f\u8bed\u548c\u5b9a\u4e49\u7684\u6536\u96c6\u3001\u793a\u4f8b\u7684\u6765\u6e90 scraping \u4ee5\u53ca\u6b63\u5728\u8fdb\u884c\u7684\u6ce8\u91ca\u8fc7\u7a0b\u3002", "result": "\u6211\u4eec\u8ba1\u7b97\u4e86\u4eba\u7c7b\u6ce8\u91ca\u8005\u548cOpenAI\u7684o3-mini\u6a21\u578b\u4e4b\u95f4\u7684\u6ce8\u91ca\u8005\u95f4\u4e00\u81f4\u6027\uff0c\u8bc4\u4f30\u4e86\u5728\u610f\u4e49\u6d88\u6b67\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u5e73\u5747Krippendorff's alpha\u4e3a0.746\u3002", "conclusion": "\u6211\u4eec\u8ba4\u4e3a\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u9884\u7b5b\u9009\u5de5\u5177\uff0c\u4f46\u7531\u4e8e\u9177\u513f\u8bed\u8a00\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u654f\u611f\u6027\uff0c\u9700\u8981\u4e13\u5bb6\u548c\u793e\u533a\u9a71\u52a8\u7684\u6ce8\u91ca\u5de5\u4f5c\u3002"}}
{"id": "2509.17455", "pdf": "https://arxiv.org/pdf/2509.17455", "abs": "https://arxiv.org/abs/2509.17455", "authors": ["Haoyang Chen", "Kumiko Tanaka-Ishii"], "title": "Codifying Natural Langauge Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to Journal of Automated Software Engineering", "summary": "We explore the applicability of text-to-code to solve real-world problems\nthat are typically solved in natural language, such as legal judgment and\nmedical QA. Unlike previous works, our approach leverages the explicit\nreasoning provided by program generation. We present ICRAG, a framework that\ntransforms natural language into executable programs through iterative\nrefinement using external knowledge from domain resources and GitHub. Across 13\nbenchmarks, ICRAG achieves up to 161.1\\% relative improvement. We provide a\ndetailed analysis of the generated code and the impact of external knowledge,\nand we discuss the limitations of applying text-to-code approaches to\nreal-world natural language tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u4ee3\u7801\u5728\u73b0\u5b9e\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6ICRAG\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548c\u5916\u90e8\u77e5\u8bc6\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6587\u672c\u5230\u4ee3\u7801\u5728\u73b0\u5b9e\u4e16\u754c\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6539\u8fdb\u73b0\u6709\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7a0b\u5e8f\u751f\u6210\u63d0\u4f9b\u7684\u663e\u5f0f\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5e76\u4ecb\u7ecd\u4e86ICRAG\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u6765\u81ea\u9886\u57df\u8d44\u6e90\u548cGitHub\u7684\u5916\u90e8\u77e5\u8bc6\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7a0b\u5e8f\u3002", "result": "\u572813\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cICRAG\u5b9e\u73b0\u4e86\u9ad8\u8fbe161.1%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u6587\u672c\u5230\u4ee3\u7801\u7684\u5e94\u7528\uff0c\u4ee5\u89e3\u51b3\u901a\u5e38\u7528\u81ea\u7136\u8bed\u8a00\u89e3\u51b3\u7684\u73b0\u5b9e\u95ee\u9898\uff0c\u5982\u6cd5\u5f8b\u5224\u51b3\u548c\u533b\u7597\u95ee\u7b54\u3002"}}
{"id": "2509.17459", "pdf": "https://arxiv.org/pdf/2509.17459", "abs": "https://arxiv.org/abs/2509.17459", "authors": ["Namyoung Kim", "Kai Tzu-iunn Ong", "Yeonjun Hwang", "Minseok Kang", "Iiseo Jihn", "Gayoung Kim", "Minju Kim", "Jinyoung Yeo"], "title": "PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Dialogue agents based on large language models (LLMs) have shown promising\nperformance in proactive dialogue, which requires effective strategy planning.\nHowever, existing approaches to strategy planning for proactive dialogue face\nseveral limitations: limited strategy coverage, preference bias in planning,\nand reliance on costly additional training. To address these, we propose\nPRINCIPLES: a synthetic strategy memory for proactive dialogue agents.\nPRINCIPLES is derived through offline self-play simulations and serves as\nreusable knowledge that guides strategy planning during inference, eliminating\nthe need for additional training and data annotation. We evaluate PRINCIPLES in\nboth emotional support and persuasion domains, demonstrating consistent\nimprovements over strong baselines. Furthermore, PRINCIPLES maintains its\nrobustness across extended and more diverse evaluation settings. See our\nproject page at https://huggingface.co/spaces/kimnamssya/Principles.", "AI": {"tldr": "PRINCIPLES is a synthetic strategy memory for proactive dialogue agents that improves performance without additional training.", "motivation": "Existing approaches to strategy planning for proactive dialogue face limitations such as limited strategy coverage, preference bias in planning, and reliance on costly additional training.", "method": "PRINCIPLES is derived through offline self-play simulations and serves as reusable knowledge that guides strategy planning during inference, eliminating the need for additional training and data annotation.", "result": "PRINCIPLES shows consistent improvements over strong baselines in emotional support and persuasion domains, and remains robust across extended and diverse evaluation settings.", "conclusion": "PRINCIPLES demonstrates consistent improvements over strong baselines in emotional support and persuasion domains, and maintains robustness across extended and diverse evaluation settings."}}
{"id": "2509.17482", "pdf": "https://arxiv.org/pdf/2509.17482", "abs": "https://arxiv.org/abs/2509.17482", "authors": ["Tsung-Hsuan Pan", "Chung-Chi Chen", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Diagnosing Model Editing via Knowledge Spectrum", "categories": ["cs.CL"], "comment": null, "summary": "Model editing, the process of efficiently modifying factual knowledge in\npre-trained language models, is critical for maintaining their accuracy and\nrelevance. However, existing editing methods often introduce unintended side\neffects, degrading model performance in unpredictable ways. While much research\nhas focused on improving editing algorithms, the role of the target knowledge's\nintrinsic properties remains a significant, underexplored factor. This paper\naddresses this gap by first proposing the ``Knowledge Spectrum,'' a systematic\nframework for categorizing knowledge based on its real-world popularity, the\nmodel's pre-edit familiarity, and the linguistic structure of the eliciting\nquestion. Our empirical analysis reveals that these characteristics are strong\npredictors of editing success and stability. Informed by these findings, we\nintroduce the ``Knowledge-Diagnostic Framework,'' an adaptive strategy that\ntailors editing intensity to the diagnosed difficulty of a knowledge item. We\ndemonstrate that this framework significantly improves success rates for\nchallenging edits while optimizing computational resources. Our work provides a\nmore comprehensive understanding of the factors governing model editing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u6709\u6548\u5730\u7f16\u8f91\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u63d0\u9ad8\u6210\u529f\u7387\u5e76\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u7f16\u8f91\u65b9\u6cd5\u5e38\u5f15\u5165\u610f\u5916\u526f\u4f5c\u7528\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u800c\u76ee\u6807\u77e5\u8bc6\u7684\u5185\u5728\u5c5e\u6027\u662f\u91cd\u8981\u7684\u4f46\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u56e0\u7d20\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u77e5\u8bc6\u8c31\u7cfb\u201d\u548c\u201c\u77e5\u8bc6\u8bca\u65ad\u6846\u67b6\u201d\uff0c\u7528\u4e8e\u5206\u7c7b\u77e5\u8bc6\u5e76\u6839\u636e\u77e5\u8bc6\u9879\u7684\u96be\u5ea6\u8c03\u6574\u7f16\u8f91\u5f3a\u5ea6\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u77e5\u8bc6\u7684\u7279\u6027\u662f\u7f16\u8f91\u6210\u529f\u548c\u7a33\u5b9a\u6027\u7684\u5f3a\u9884\u6d4b\u56e0\u5b50\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u6311\u6218\u6027\u7f16\u8f91\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u7f16\u8f91\u4e2d\u5f71\u54cd\u56e0\u7d20\u7684\u66f4\u5168\u9762\u7406\u89e3\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684\u6846\u67b6\u63d0\u9ad8\u4e86\u7f16\u8f91\u7684\u6210\u529f\u7387\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u4f18\u5316\u3002"}}
{"id": "2509.17486", "pdf": "https://arxiv.org/pdf/2509.17486", "abs": "https://arxiv.org/abs/2509.17486", "authors": ["Lvzhou Luo", "Yixuan Cao", "Ping Luo"], "title": "AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 (Findings)", "summary": "Retrieval-augmented generation improves the factual accuracy of Large\nLanguage Models (LLMs) by incorporating external context, but often suffers\nfrom irrelevant retrieved content that hinders effectiveness. Context\ncompression addresses this issue by filtering out irrelevant information from\ncontext before LLM generation. However, existing methods struggle to adaptively\nadjust compression rates for different context, maintain low latency and\nintegrate information across multiple documents. To overcome these limitations,\nWe introduce AttnComp, an adaptive, efficient and context-aware compression\nframework. By leveraging the attention mechanism of LLMs to identify relevant\ninformation, AttnComp employs a Top-P compression algorithm to retain the\nminimal set of documents whose cumulative attention weights exceeds a\npredefined threshold. In addition to compression, AttnComp estimates response\nconfidence by assessing the overall relevance of the retrieved content,\nenabling users to gauge response reliability. Experiments demonstrate that\nAttnComp outperforms existing compression methods and uncompressed baselines,\nachieving higher accuracy with substantial compression rates and lower latency.", "AI": {"tldr": "AttnComp is a new compression framework that improves the effectiveness of retrieval-augmented generation by adaptively compressing context and estimating response confidence.", "motivation": "Existing context compression methods struggle to adaptively adjust compression rates for different contexts, maintain low latency, and integrate information across multiple documents. AttnComp aims to overcome these limitations.", "method": "AttnComp is an adaptive, efficient, and context-aware compression framework that leverages the attention mechanism of LLMs to identify relevant information and employs a Top-P compression algorithm to retain the minimal set of documents whose cumulative attention weights exceed a predefined threshold. It also estimates response confidence by assessing the overall relevance of the retrieved content.", "result": "Experiments demonstrate that AttnComp outperforms existing compression methods and uncompressed baselines, achieving higher accuracy with substantial compression rates and lower latency.", "conclusion": "AttnComp outperforms existing compression methods and uncompressed baselines, achieving higher accuracy with substantial compression rates and lower latency."}}
{"id": "2509.17489", "pdf": "https://arxiv.org/pdf/2509.17489", "abs": "https://arxiv.org/abs/2509.17489", "authors": ["Woongkyu Lee", "Junhee Cho", "Jungwook Choi"], "title": "MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have advanced code generation from\nsingle-function tasks to competitive-programming problems, but existing\nmulti-agent solutions either rely on costly large-scale ($>$ 30B) models or\ncollapse when downsized to small open-source models. We present MapCoder-Lite,\nwhich upgrades a single 7B model into four role-specialised agents-retriever,\nplanner, coder, and debugger-using only rank-32, role-specific LoRA adapters\n($<3\\%$ extra parameters). Three lightweight techniques make this possible: (i)\ntrajectory distillation from strong LLMs fixes format fragility in retrieval\nand debugging, (ii) supervisor-guided correction strengthens planning and\ncoding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient\nspecialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests\nshows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\\%$ to\n$28.3\\%$), eliminates all format failures, and closes to within six points of a\n32B baseline while cutting GPU memory and token-generation time by $4\\times$.\nThese results demonstrate that careful agent-wise fine-tuning unleashes\nhigh-quality multi-agent coding on a small language model.", "AI": {"tldr": "MapCoder-Lite\u901a\u8fc7\u4f7f\u7528\u89d2\u8272\u7279\u5b9a\u7684LoRA\u9002\u914d\u5668\u5c06\u5355\u4e2a7B\u6a21\u578b\u5347\u7ea7\u4e3a\u56db\u4e2a\u89d2\u8272\u4e13\u4e1a\u5316\u4ee3\u7406\uff0c\u4ece\u800c\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u4ee3\u7406\u7f16\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u5927\u89c4\u6a21\uff08> 30B\uff09\u6a21\u578b\uff0c\u8981\u4e48\u5728\u7f29\u5c0f\u5230\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u65f6\u4f1a\u5d29\u6e83\u3002", "method": "MapCoder-Lite\u901a\u8fc7\u4f7f\u7528\u4ec5\u970032\u4e2a\u79e9\u7684\u89d2\u8272\u7279\u5b9aLoRA\u9002\u914d\u5668\uff08<3%\u7684\u989d\u5916\u53c2\u6570\uff09\u5c06\u5355\u4e2a7B\u6a21\u578b\u5347\u7ea7\u4e3a\u56db\u4e2a\u89d2\u8272\u4e13\u4e1a\u5316\u4ee3\u7406\uff08\u68c0\u7d22\u5668\u3001\u89c4\u5212\u8005\u3001\u7f16\u7801\u5668\u548c\u8c03\u8bd5\u5668\uff09\u3002\u4e09\u79cd\u8f7b\u91cf\u7ea7\u6280\u672f\u4f7f\u8fd9\u6210\u4e3a\u53ef\u80fd\uff1a(i) \u4ece\u5f3aLLM\u4e2d\u8fdb\u884c\u8f68\u8ff9\u84b8\u998f\u4ee5\u4fee\u590d\u68c0\u7d22\u548c\u8c03\u8bd5\u4e2d\u7684\u683c\u5f0f\u8106\u5f31\u6027\uff0c(ii) \u76d1\u7763\u5f15\u5bfc\u7ea0\u6b63\u52a0\u5f3a\u4e86\u89c4\u5212\u548c\u7f16\u7801\u4ee3\u7406\uff0c(iii) \u4ee3\u7406\u7ea7LoRA\u5fae\u8c03\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u7684\u4e13\u95e8\u5316\u3002", "result": "\u5728xCodeEval\u3001APPS\u548cCodeContests\u4e0a\u7684\u5168\u9762\u8bc4\u4f30\u663e\u793a\uff0cMapCoder-Lite\u5c06xCodeEval\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u4e00\u500d\u4ee5\u4e0a\uff08\u4ece13.2%\u523028.3%\uff09\uff0c\u6d88\u9664\u4e86\u6240\u6709\u683c\u5f0f\u5931\u8d25\uff0c\u5e76\u5728GPU\u5185\u5b58\u548c\u4ee4\u724c\u751f\u6210\u65f6\u95f4\u4e0a\u51cf\u5c11\u4e864\u500d\uff0c\u540c\u65f6\u63a5\u8fd132B\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4ed4\u7ec6\u7684\u4ee3\u7406\u5fae\u8c03\u53ef\u4ee5\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u591a\u4ee3\u7406\u7f16\u7a0b\u3002"}}
{"id": "2509.17493", "pdf": "https://arxiv.org/pdf/2509.17493", "abs": "https://arxiv.org/abs/2509.17493", "authors": ["Wenhao Zhuang", "Yuan Sun", "Xiaobing Zhao"], "title": "Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are trained on increasingly diverse and\nextensive multilingual corpora, they demonstrate cross-lingual transfer\ncapabilities. However, these capabilities often fail to effectively extend to\nlow-resource languages, particularly those utilizing non-Latin scripts. While\ntransliterating low-resource languages into Latin script presents a natural\nsolution, there currently lacks a comprehensive framework for integrating\ntransliteration into LLMs training and deployment. Taking a pragmatic approach,\nthis paper innovatively combines character transliteration with Huffman coding\nto design a complete transliteration framework. Our proposed framework offers\nthe following advantages: 1) Compression: Reduces storage requirements for\nlow-resource language content, achieving up to 50% reduction in file size and\n50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless\nconversion from transliterated text back to the source language. 3) Efficiency:\nEliminates the need for vocabulary expansion for low-resource languages,\nimproving training and inference efficiency. 4) Scalability: The framework can\nbe extended to other low-resource languages. We validate the effectiveness of\nour framework across multiple downstream tasks, including text classification,\nmachine reading comprehension, and machine translation. Experimental results\ndemonstrate that our method significantly enhances the model's capability to\nprocess low-resource languages while maintaining performance on high-resource\nlanguages. Our data and code are publicly available at\nhttps://github.com/CMLI-NLP/HuffmanTranslit.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b57\u7b26\u97f3\u8bd1\u4e0e\u970d\u592b\u66fc\u7f16\u7801\u7684\u5b8c\u6574\u97f3\u8bd1\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\u6765\u5c06\u97f3\u8bd1\u96c6\u6210\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u4e2d\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f7f\u7528\u975e\u62c9\u4e01\u5b57\u6bcd\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b57\u7b26\u97f3\u8bd1\u4e0e\u970d\u592b\u66fc\u7f16\u7801\u7684\u5b8c\u6574\u97f3\u8bd1\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5904\u7406\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5904\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u6574\u7684\u97f3\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5b57\u7b26\u97f3\u8bd1\u4e0e\u970d\u592b\u66fc\u7f16\u7801\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5904\u7406\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5904\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17505", "pdf": "https://arxiv.org/pdf/2509.17505", "abs": "https://arxiv.org/abs/2509.17505", "authors": ["Tu\u011fba Pamay Arslan", "Emircan Erol", "G\u00fcl\u015fen Eryi\u011fit"], "title": "CorefInst: Leveraging LLMs for Multilingual Coreference Resolution", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL) (2025 August). Submission: March, 2025.\n  Revision: July, 2025. Acceptance: August, 2025", "summary": "Coreference Resolution (CR) is a crucial yet challenging task in natural\nlanguage understanding, often constrained by task-specific architectures and\nencoder-based language models that demand extensive training and lack\nadaptability. This study introduces the first multilingual CR methodology which\nleverages decoder-only LLMs to handle both overt and zero mentions. The article\nexplores how to model the CR task for LLMs via five different instruction sets\nusing a controlled inference method. The approach is evaluated across three\nLLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when\ninstruction-tuned with a suitable instruction set, can surpass state-of-the-art\ntask-specific architectures. Specifically, our best model, a fully fine-tuned\nLlama 3.1 for multilingual CR, outperforms the leading multilingual CR model\n(i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages\nin the CorefUD v1.2 dataset collection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u7801\u5668\u7684LLM\u7684\u591a\u8bed\u8a00\u5171\u6307\u89e3\u6790\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6307\u4ee4\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u7684\u5171\u6307\u89e3\u6790\u65b9\u6cd5\u53d7\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u67b6\u6784\u548c\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u8bed\u8a00\u6a21\u578b\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89e3\u7801\u5668\u7684LLM\u7684\u591a\u8bed\u8a00\u5171\u6307\u89e3\u6790\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e94\u79cd\u4e0d\u540c\u7684\u6307\u4ee4\u96c6\u8fdb\u884c\u63a7\u5236\u63a8\u7406\u65b9\u6cd5\u7684\u5efa\u6a21\u3002", "result": "\u5728\u4e09\u4e2aLLM\uff08Llama 3.1\u3001Gemma 2\u548cMistral 0.3\uff09\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7\u9002\u5f53\u6307\u4ee4\u96c6\u5fae\u8c03\u7684LLM\u80fd\u591f\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u6307\u4ee4\u96c6\u8fdb\u884c\u5fae\u8c03\u7684LLM\u53ef\u4ee5\u5728\u591a\u8bed\u8a00\u5171\u6307\u89e3\u6790\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.17523", "pdf": "https://arxiv.org/pdf/2509.17523", "abs": "https://arxiv.org/abs/2509.17523", "authors": ["Mar\u00eda Andrea Cruz Bland\u00f3n", "Zakaria Aldeneh", "Jie Chi", "Maureen de Seyssel"], "title": "Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models", "categories": ["cs.CL", "eess.AS"], "comment": "5 pages, 2 figures", "summary": "Self-supervised learning (SSL) has made significant advances in speech\nrepresentation learning. Models like wav2vec 2.0 and HuBERT have achieved\nstate-of-the-art results in tasks such as speech recognition, particularly in\nmonolingual settings. However, multilingual SSL models tend to underperform\ntheir monolingual counterparts on each individual language, especially in\nmultilingual scenarios with few languages such as the bilingual setting. In\nthis work, we investigate a novel approach to reduce this performance gap by\nintroducing limited visual grounding into bilingual speech SSL models. Our\nresults show that visual grounding benefits both monolingual and bilingual\nmodels, with especially pronounced gains for the latter, reducing the\nmultilingual performance gap on zero-shot phonetic discrimination from 31.5%\nfor audio-only models to 8.04% with grounding.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5f15\u5165\u6709\u9650\u7684\u89c6\u89c9\u57fa\u7840\u6765\u51cf\u5c11\u53cc\u8bed\u8bed\u97f3SSL\u6a21\u578b\u7684\u591a\u8bed\u8a00\u6027\u80fd\u5dee\u8ddd\uff0c\u7ed3\u679c\u8868\u660e\u8fd9\u79cd\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u8bed\u97f3\u533a\u5206\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8bed\u8a00SSL\u6a21\u578b\u5728\u5355\u4e2a\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u4e0d\u5982\u5355\u8bed\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u53cc\u8bed\u573a\u666f\u4e2d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u51cf\u5c11\u8fd9\u79cd\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u6709\u9650\u7684\u89c6\u89c9\u57fa\u7840\uff0c\u7814\u7a76\u4e86\u51cf\u5c11\u53cc\u8bed\u8bed\u97f3SSL\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u89c6\u89c9\u57fa\u7840\u5bf9\u5355\u8bed\u548c\u53cc\u8bed\u6a21\u578b\u90fd\u6709\u76ca\uff0c\u5c24\u5176\u662f\u5bf9\u540e\u8005\uff0c\u5c06\u96f6\u6837\u672c\u8bed\u97f3\u533a\u5206\u4efb\u52a1\u7684\u591a\u8bed\u8a00\u6027\u80fd\u5dee\u8ddd\u4ece31.5%\u964d\u4f4e\u52308.04%\u3002", "conclusion": "\u5f15\u5165\u6709\u9650\u7684\u89c6\u89c9\u57fa\u7840\u53ef\u4ee5\u51cf\u5c11\u53cc\u8bed\u8bed\u97f3SSL\u6a21\u578b\u7684\u591a\u8bed\u8a00\u6027\u80fd\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u8bed\u97f3\u533a\u5206\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2509.17552", "pdf": "https://arxiv.org/pdf/2509.17552", "abs": "https://arxiv.org/abs/2509.17552", "authors": ["Tianle Zhang", "Wanlong Fang", "Jonathan Woo", "Paridhi Latawa", "Deepak A. Subramanian", "Alvin Chan"], "title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning", "categories": ["cs.CL", "cs.AI"], "comment": "NIPS 2025", "summary": "The remarkable performance of Large Language Models (LLMs) can be enhanced\nwith test-time computation, which relies on external tools and even other deep\nlearning models. However, existing approaches for integrating non-text modality\nrepresentations into LLMs typically require additional costly supervised\ntraining, restricting on-the-fly adaptation to new domains and modalities. In\nthis work, we explore the feasibility of integrating representations from\nnon-text foundational models (FMs) into text-based LLMs in a training-free\nmanner. We propose In-Context Representation Learning (ICRL) as a\nproof-of-concept to allow LLMs to adaptively utilize non-text modality\nrepresentations with few-shot learning. Unlike traditional in-context learning,\nwhich incorporates text-label pairs, ICRL replaces text inputs with FM\nrepresentations, enabling the LLM to perform multi-modal inference without\nfine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,\ninvestigating three core research questions: (i) how to map FM representations\ninto LLMs in a training-free manner, (ii) what factors influence ICRL\nperformance, and (iii) what mechanisms underlie the effectiveness of ICRL. To\nthe best of our knowledge, ICRL is the first training-free framework for\nintegrating non-text modality representations into text-based LLMs, presenting\na promising direction for adaptable, multi-modal generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5ICRL\uff0c\u7528\u4e8e\u5c06\u975e\u6587\u672c\u6a21\u6001\u8868\u793a\u6574\u5408\u5230\u6587\u672c\u578bLLM\u4e2d\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5e76\u5728\u5206\u5b50\u9886\u57df\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u6602\u8d35\u76d1\u7763\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5bf9\u65b0\u9886\u57df\u548c\u6a21\u6001\u7684\u5b9e\u65f6\u9002\u5e94\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u5f0f\u6765\u6574\u5408\u975e\u6587\u672c\u6a21\u6001\u8868\u793a\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aIn-Context Representation Learning (ICRL)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u76d1\u7763\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5c06\u975e\u6587\u672c\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u8868\u793a\u96c6\u6210\u5230\u6587\u672c\u578bLLM\u4e2d\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u63a8\u7406\u3002", "result": "\u5728\u5206\u5b50\u9886\u57df\u7684\u4e00\u7cfb\u5217\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86ICRL\uff0c\u7814\u7a76\u4e86\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u5982\u4f55\u4ee5\u65e0\u8bad\u7ec3\u65b9\u5f0f\u5c06FM\u8868\u793a\u6620\u5c04\u5230LLM\u4e2d\uff0c\u5f71\u54cdICRL\u6027\u80fd\u7684\u56e0\u7d20\uff0c\u4ee5\u53caICRL\u6709\u6548\u6027\u7684\u673a\u5236\u3002", "conclusion": "ICRL\u662f\u7b2c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5c06\u975e\u6587\u672c\u6a21\u6001\u8868\u793a\u6574\u5408\u5230\u57fa\u4e8e\u6587\u672c\u7684LLM\u4e2d\u7684\u6846\u67b6\uff0c\u4e3a\u53ef\u9002\u5e94\u7684\u591a\u6a21\u6001\u6cdb\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.17559", "pdf": "https://arxiv.org/pdf/2509.17559", "abs": "https://arxiv.org/abs/2509.17559", "authors": ["Yoko Kayano", "Saku Sugawara"], "title": "Specification-Aware Machine Translation and Evaluation for Purpose Alignment", "categories": ["cs.CL"], "comment": null, "summary": "In professional settings, translation is guided by communicative goals and\nclient needs, often formalized as specifications. While existing evaluation\nframeworks acknowledge the importance of such specifications, these\nspecifications are often treated only implicitly in machine translation (MT)\nresearch. Drawing on translation studies, we provide a theoretical rationale\nfor why specifications matter in professional translation, as well as a\npractical guide to implementing specification-aware MT and evaluation. Building\non this foundation, we apply our framework to the translation of investor\nrelations texts from 33 publicly listed companies. In our experiment, we\ncompare five translation types, including official human translations and\nprompt-based outputs from large language models (LLMs), using expert error\nanalysis, user preference rankings, and an automatic metric. The results show\nthat LLM translations guided by specifications consistently outperformed\nofficial human translations in human evaluations, highlighting a gap between\nperceived and expected quality. These findings demonstrate that integrating\nspecifications into MT workflows, with human oversight, can improve translation\nquality in ways aligned with professional practice.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u89c4\u8303\u5728\u4e13\u4e1a\u7ffb\u8bd1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c4\u8303\u611f\u77e5\u7684\u673a\u5668\u7ffb\u8bd1\u548c\u8bc4\u4f30\u6846\u67b6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7531\u89c4\u8303\u6307\u5bfc\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ffb\u8bd1\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u5b98\u65b9\u4eba\u5de5\u7ffb\u8bd1\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u6846\u67b6\u627f\u8ba4\u89c4\u8303\u7684\u91cd\u8981\u6027\uff0c\u4f46\u8fd9\u4e9b\u89c4\u8303\u901a\u5e38\u5728\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u7814\u7a76\u4e2d\u4ec5\u88ab\u9690\u5f0f\u5904\u7406\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u7528\u6307\u5357\uff0c\u4ee5\u5b9e\u73b0\u89c4\u8303\u611f\u77e5\u7684\u673a\u5668\u7ffb\u8bd1\u548c\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8e\u7ffb\u8bd1\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u4e3a\u4ec0\u4e48\u89c4\u8303\u5728\u4e13\u4e1a\u7ffb\u8bd1\u4e2d\u91cd\u8981\u7684\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u65bd\u89c4\u8303\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u548c\u8bc4\u4f30\u7684\u5b9e\u7528\u6307\u5357\u3002\u7136\u540e\u5c06\u6846\u67b6\u5e94\u7528\u4e8e33\u5bb6\u4e0a\u5e02\u516c\u53f8\u7684\u6295\u8d44\u8005\u5173\u7cfb\u6587\u672c\u7684\u7ffb\u8bd1\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e94\u79cd\u7ffb\u8bd1\u7c7b\u578b\uff0c\u5305\u62ec\u5b98\u65b9\u4eba\u5de5\u7ffb\u8bd1\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63d0\u793a\u8f93\u51fa\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7531\u89c4\u8303\u6307\u5bfc\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ffb\u8bd1\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5b98\u65b9\u4eba\u5de5\u7ffb\u8bd1\uff0c\u7a81\u663e\u4e86\u611f\u77e5\u8d28\u91cf\u548c\u9884\u671f\u8d28\u91cf\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u5c06\u89c4\u8303\u7eb3\u5165\u673a\u5668\u7ffb\u8bd1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u8f85\u4ee5\u4eba\u5de5\u76d1\u7763\uff0c\u53ef\u4ee5\u4ee5\u7b26\u5408\u4e13\u4e1a\u5b9e\u8df5\u7684\u65b9\u5f0f\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u3002"}}
{"id": "2509.17570", "pdf": "https://arxiv.org/pdf/2509.17570", "abs": "https://arxiv.org/abs/2509.17570", "authors": ["Sergey Troshin", "Irina Saparina", "Antske Fokkens", "Vlad Niculae"], "title": "Asking a Language Model for Diverse Responses", "categories": ["cs.CL"], "comment": "UncertaiNLP workshop, 2025", "summary": "Large language models increasingly rely on explicit reasoning chains and can\nproduce multiple plausible responses for a given context. We study the\ncandidate sampler that produces the set of plausible responses contrasting the\nancestral (parallel) sampling against two alternatives: enumeration, which asks\nthe model to produce $n$ candidates in one pass, and iterative sampling, which\nproposes candidates sequentially while conditioning on the currently generated\nresponse set. Under matched budgets, we compare these samplers on quality,\nlexical and computation flow diversity, and efficiency. Our empirical results\ndemonstrate that enumeration and iterative strategies result in higher\ndiversity at comparable quality. Our findings highlight the potential of simple\nnon-independent sampling strategies to improve response diversity without\nsacrificing generation quality.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7528\u4e8e\u751f\u6210\u591a\u4e2a\u5408\u7406\u54cd\u5e94\u7684\u5019\u9009\u91c7\u6837\u5668\uff0c\u5e76\u53d1\u73b0\u679a\u4e3e\u548c\u8fed\u4ee3\u7b56\u7565\u53ef\u4ee5\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u54cd\u5e94\u7684\u591a\u6837\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u4f9d\u8d56\u663e\u5f0f\u7684\u63a8\u7406\u94fe\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e3a\u7ed9\u5b9a\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u591a\u4e2a\u5408\u7406\u7684\u54cd\u5e94\u3002\u6211\u4eec\u9700\u8981\u7814\u7a76\u80fd\u591f\u751f\u6210\u5408\u7406\u54cd\u5e94\u96c6\u7684\u5019\u9009\u91c7\u6837\u5668\uff0c\u4ee5\u63d0\u9ad8\u54cd\u5e94\u7684\u591a\u6837\u6027\u3002", "method": "\u6211\u4eec\u6bd4\u8f83\u4e86\u5019\u9009\u91c7\u6837\u5668\uff0c\u5305\u62ec\u4f20\u7edf\u7684\u5e76\u884c\u91c7\u6837\u3001\u679a\u4e3e\u548c\u8fed\u4ee3\u91c7\u6837\uff0c\u5e76\u5728\u76f8\u540c\u7684\u9884\u7b97\u4e0b\u8bc4\u4f30\u5b83\u4eec\u7684\u8d28\u91cf\u3001\u8bcd\u6c47\u548c\u8ba1\u7b97\u6d41\u7684\u591a\u6837\u6027\u4ee5\u53ca\u6548\u7387\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u679a\u4e3e\u548c\u8fed\u4ee3\u7b56\u7565\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u679a\u4e3e\u548c\u8fed\u4ee3\u7b56\u7565\u53ef\u4ee5\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u54cd\u5e94\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2509.17628", "pdf": "https://arxiv.org/pdf/2509.17628", "abs": "https://arxiv.org/abs/2509.17628", "authors": ["Yuzhen Lei", "Hongbin Xie", "Jiaxing Zhao", "Shuangxue Liu", "Xuan Song"], "title": "MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Large Language Models (LLMs) have excelled in question-answering (QA) tasks\nwithin single domains. However, their reasoning and coordination capabilities\nin complex, multi-stage scenarios remain underexplored. Existing benchmarks\ntypically focus on isolated tasks or narrow domains, overlooking models'\nabilities for multi-stage collaboration and optimization without explicit\nexternal guidance. To bridge this gap, we propose \\textbf{MSCoRe}, a novel\nbenchmark comprising 126696 domain-specific QA instances spanning scenarios in\nautomotive, pharmaceutical, electronics, and energy sectors. The dataset is\ncreated using a structured three-phase pipeline: dynamic sampling, iterative\nquestion-answer generation, and a multi-level quality assessment to ensure data\nquality. Tasks are further categorized into three difficulty levels according\nto stage coverage and complexity. With MSCoRe, we have conducted a\ncomprehensive evaluation of various state-of-the-art LLM agents. The commercial\nmodels performed best across all tasks and scenarios, but a notable gap in\nROUGE scores remains between simple and complex tasks. We also tested the\nmodels' robustness and found that their performance is negatively affected by\nnoisy data. MSCoRe provides a valuable new resource for the community to\nevaluate and improve multi-stage reasoning in LLM agents. The code and data are\navailable at https://github.com/D3E0-source/MSCoRE.", "AI": {"tldr": "MSCoRe is a new benchmark for evaluating multi-stage reasoning in large language models, consisting of 126696 domain-specific QA instances across various sectors.", "motivation": "Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking models' abilities for multi-stage collaboration and optimization without explicit external guidance.", "method": "We proposed MSCoRe, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality.", "result": "We have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the models' robustness and found that their performance is negatively affected by noisy data.", "conclusion": "MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents."}}
{"id": "2509.17641", "pdf": "https://arxiv.org/pdf/2509.17641", "abs": "https://arxiv.org/abs/2509.17641", "authors": ["Hyunjong Ok", "Suho Yoo", "Hyeonjun Kim", "Jaeho Lee"], "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "comment": "Preprint", "summary": "Even without directly hearing sounds, humans can effortlessly reason about\nauditory properties, such as pitch, loudness, or sound-source associations,\ndrawing on auditory commonsense. In contrast, language models often lack this\ncapability, limiting their effectiveness in multimodal interactions. As an\ninitial step to address this gap, we present AuditoryBench++, a comprehensive\nbenchmark for evaluating auditory knowledge and reasoning in text-only\nsettings. The benchmark encompasses tasks that range from basic auditory\ncomparisons to contextually grounded reasoning, enabling fine-grained analysis\nof how models process and integrate auditory concepts. In addition, we\nintroduce AIR-CoT, a novel auditory imagination reasoning method that generates\nand integrates auditory information during inference through span detection\nwith special tokens and knowledge injection. Extensive experiments with recent\nLLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both\nthe off-the-shelf models and those augmented with auditory knowledge. The\nproject page is available at https://auditorybenchpp.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAuditoryBench++\u57fa\u51c6\u548cAIR-CoT\u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30\u548c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u542c\u89c9\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4ea4\u4e92\u4e2d\u7f3a\u4e4f\u542c\u89c9\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u8bc4\u4f30\u548c\u63d0\u5347\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86AuditoryBench++\u57fa\u51c6\u548cAIR-CoT\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u6b8a\u6807\u8bb0\u548c\u77e5\u8bc6\u6ce8\u5165\u751f\u6210\u548c\u6574\u5408\u542c\u89c9\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAIR-CoT\u5728\u6700\u8fd1\u7684LLMs\u548c\u591a\u6a21\u6001LLMs\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548c\u589e\u5f3a\u6a21\u578b\u3002", "conclusion": "AuditoryBench++\u548cAIR-CoT\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u542c\u89c9\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2509.17667", "pdf": "https://arxiv.org/pdf/2509.17667", "abs": "https://arxiv.org/abs/2509.17667", "authors": ["Arafat Ahsan", "Vandan Mujadia", "Pruthwik Mishra", "Yash Bhaskar", "Dipti Misra Sharma"], "title": "Crosslingual Optimized Metric for Translation Assessment of Indian Languages", "categories": ["cs.CL"], "comment": "Under review", "summary": "Automatic evaluation of translation remains a challenging task owing to the\northographic, morphological, syntactic and semantic richness and divergence\nobserved across languages. String-based metrics such as BLEU have previously\nbeen extensively used for automatic evaluation tasks, but their limitations are\nnow increasingly recognized. Although learned neural metrics have helped\nmitigate some of the limitations of string-based approaches, they remain\nconstrained by a paucity of gold evaluation data in most languages beyond the\nusual high-resource pairs. In this present work we address some of these gaps.\nWe create a large human evaluation ratings dataset for 13 Indian languages\ncovering 21 translation directions and then train a neural translation\nevaluation metric named Cross-lingual Optimized Metric for Translation\nAssessment of Indian Languages (COMTAIL) on this dataset. The best performing\nmetric variants show significant performance gains over previous\nstate-of-the-art when adjudging translation pairs with at least one Indian\nlanguage. Furthermore, we conduct a series of ablation studies to highlight the\nsensitivities of such a metric to changes in domain, translation quality, and\nlanguage groupings. We release both the COMTAIL dataset and the accompanying\nmetric models.", "AI": {"tldr": "\u672c\u6587\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u4eba\u5de5\u8bc4\u4f30\u8bc4\u5206\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u540d\u4e3aCOMTAIL\u7684\u795e\u7ecf\u7ffb\u8bd1\u8bc4\u4f30\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5728\u5370\u5ea6\u8bed\u8a00\u7ffb\u8bd1\u5bf9\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u81ea\u52a8\u7ffb\u8bd1\u8bc4\u4f30\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u4e0d\u540c\u8bed\u8a00\u5728\u6b63\u5b57\u6cd5\u3001\u5f62\u6001\u5b66\u3001\u53e5\u6cd5\u548c\u8bed\u4e49\u4e0a\u5b58\u5728\u4e30\u5bcc\u7684\u5dee\u5f02\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5b57\u7b26\u4e32\u7684\u5ea6\u91cf\u65b9\u6cd5\u5982BLEU\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u5b66\u4e60\u7684\u795e\u7ecf\u5ea6\u91cf\u65b9\u6cd5\u53d7\u9650\u4e8e\u5927\u591a\u6570\u8bed\u8a00\u4e2d\u9ec4\u91d1\u8bc4\u4f30\u6570\u636e\u7684\u7f3a\u4e4f\u3002", "method": "\u672c\u6587\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b13\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u5927\u578b\u4eba\u5de5\u8bc4\u4f30\u8bc4\u5206\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u540d\u4e3aCOMTAIL\u7684\u795e\u7ecf\u7ffb\u8bd1\u8bc4\u4f30\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u6700\u4f73\u6027\u80fd\u7684\u5ea6\u91cf\u53d8\u4f53\u5728\u8bc4\u5224\u81f3\u5c11\u5305\u542b\u4e00\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u7ffb\u8bd1\u5bf9\u65f6\uff0c\u76f8\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCOMTAIL\u7684\u8de8\u8bed\u8a00\u7ffb\u8bd1\u8bc4\u4f30\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5370\u5ea6\u8bed\u8a00\u7ffb\u8bd1\u5bf9\u4e0a\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0c\u8fd8\u91ca\u653e\u4e86COMTAIL\u6570\u636e\u96c6\u548c\u76f8\u5e94\u7684\u5ea6\u91cf\u6a21\u578b\u3002"}}
{"id": "2509.17669", "pdf": "https://arxiv.org/pdf/2509.17669", "abs": "https://arxiv.org/abs/2509.17669", "authors": ["Yan Zhuang", "Yuan Sun"], "title": "PG-CE: A Progressive Generation Dataset with Constraint Enhancement for Controllable Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid development of Large Language Models (LLMs), Controllable Text\nGeneration (CTG) has become a critical technology for enhancing system\nreliability and user experience. Addressing the limitations of traditional\nmethods, this paper proposes the PG-CE (Progressive Generation with Constraint\nEnhancement) approach, which decomposes CTG tasks into three steps: type\nprediction, constraint construction, and guided generation. This method employs\nconstraint generation models to dynamically build multi-dimensional constraints\nincluding tone, expression style, and thematic focus to guide output.\nExperiments demonstrate that PG-CE significantly improves generation quality\nacross multiple scenarios while maintaining text controllability, thematic\nrelevance, and response practicality. The research developed a dataset\ncontaining 90,000 constraint-text pairs (with an 8:2 ratio between daily and\nother topics), effectively reflecting real-world application requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPG-CE\u7684\u53ef\u63a7\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u548c\u52a8\u6001\u6784\u5efa\u7ea6\u675f\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u53ef\u63a7\u6587\u672c\u751f\u6210\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "PG-CE\u65b9\u6cd5\u5c06CTG\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u6b65\u9aa4\uff1a\u7c7b\u578b\u9884\u6d4b\u3001\u7ea6\u675f\u6784\u5efa\u548c\u5f15\u5bfc\u751f\u6210\uff0c\u5e76\u4f7f\u7528\u7ea6\u675f\u751f\u6210\u6a21\u578b\u52a8\u6001\u6784\u5efa\u591a\u7ef4\u7ea6\u675f\u6765\u6307\u5bfc\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePG-CE\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6587\u672c\u7684\u53ef\u63a7\u6027\u3001\u4e3b\u9898\u76f8\u5173\u6027\u548c\u54cd\u5e94\u5b9e\u7528\u6027\u3002\u7814\u7a76\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b90,000\u4e2a\u7ea6\u675f-\u6587\u672c\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u6709\u6548\u53cd\u6620\u4e86\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "conclusion": "PG-CE\u65b9\u6cd5\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6587\u672c\u7684\u53ef\u63a7\u6027\u3001\u4e3b\u9898\u76f8\u5173\u6027\u548c\u54cd\u5e94\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.17671", "pdf": "https://arxiv.org/pdf/2509.17671", "abs": "https://arxiv.org/abs/2509.17671", "authors": ["Selva Ta\u015f", "Mahmut El Huseyni", "\u00d6zay Ezerceli", "Reyhan Bayraktar", "Fatma Bet\u00fcl Terzio\u011flu"], "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) has been hindered by\ntheir tendency to hallucinate, generating plausible but factually incorrect\ninformation. While Retrieval-Augmented Generation (RAG) systems attempt to\naddress this issue by grounding responses in external knowledge, hallucination\nremains a persistent challenge, particularly for morphologically complex,\nlow-resource languages like Turkish. This paper introduces Turk-LettuceDetect,\nthe first suite of hallucination detection models specifically designed for\nTurkish RAG applications. Building on the LettuceDetect framework, we formulate\nhallucination detection as a token-level classification task and fine-tune\nthree distinct encoder architectures: a Turkish-specific ModernBERT,\nTurkEmbed4STS, and multilingual EuroBERT. These models were trained on a\nmachine-translated version of the RAGTruth benchmark dataset containing 17,790\ninstances across question answering, data-to-text generation, and summarization\ntasks. Our experimental results show that the ModernBERT-based model achieves\nan F1-score of 0.7266 on the complete test set, with particularly strong\nperformance on structured tasks. The models maintain computational efficiency\nwhile supporting long contexts up to 8,192 tokens, making them suitable for\nreal-time deployment. Comparative analysis reveals that while state-of-the-art\nLLMs demonstrate high recall, they suffer from low precision due to\nover-generation of hallucinated content, underscoring the necessity of\nspecialized detection mechanisms. By releasing our models and translated\ndataset, this work addresses a critical gap in multilingual NLP and establishes\na foundation for developing more reliable and trustworthy AI applications for\nTurkish and other languages.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Turk-LettuceDetect\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u4e3a\u571f\u8033\u5176RAG\u5e94\u7528\u8bbe\u8ba1\u7684\u5e7b\u89c9\u68c0\u6d4b\u6a21\u578b\u5957\u4ef6\u3002\u901a\u8fc7\u5fae\u8c03\u4e09\u79cd\u4e0d\u540c\u7684\u7f16\u7801\u5668\u67b6\u6784\uff0c\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u5e76\u586b\u8865\u4e86\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u7a7a\u767d\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u5f62\u6001\u590d\u6742\u3001\u8d44\u6e90\u8f83\u5c11\u7684\u8bed\u8a00\u5982\u571f\u8033\u5176\u8bed\u4e2d\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u68c0\u6d4b\u673a\u5236\u3002", "method": "\u672c\u6587\u5c06\u5e7b\u89c9\u68c0\u6d4b\u4f5c\u4e3a\u6807\u8bb0\u7ea7\u522b\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5fae\u8c03\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u7f16\u7801\u5668\u67b6\u6784\uff1a\u4e00\u79cd\u571f\u8033\u5176\u7279\u5b9a\u7684ModernBERT\uff0cTurkEmbed4STS\u548c\u591a\u8bed\u8a00EuroBERT\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eModernBERT\u7684\u6a21\u578b\u5728\u5b8c\u6574\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e860.7266\u7684F1\u5206\u6570\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\u3002\u6a21\u578b\u5728\u652f\u6301\u957f\u8fbe8,192\u4e2a\u6807\u8bb0\u7684\u957f\u4e0a\u4e0b\u6587\u7684\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u53d1\u5e03\u6a21\u578b\u548c\u7ffb\u8bd1\u540e\u7684\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u7a7a\u767d\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u571f\u8033\u5176\u8bed\u548c\u5176\u4ed6\u8bed\u8a00\u7684AI\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.17680", "pdf": "https://arxiv.org/pdf/2509.17680", "abs": "https://arxiv.org/abs/2509.17680", "authors": ["Shenghao Ye", "Yu Guo", "Dong Jin", "Yikai Shen", "Yunpeng Hou", "Shuangwu Chen", "Jian Yang", "Xiaofeng Jiang"], "title": "When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables", "categories": ["cs.CL"], "comment": "23 pages, 24 figures", "summary": "Table question answering (TableQA) is a fundamental task in natural language\nprocessing (NLP). The strong reasoning capabilities of large language models\n(LLMs) have brought significant advances in this field. However, as real-world\napplications involve increasingly complex questions and larger tables,\nsubstantial noisy data is introduced, which severely degrades reasoning\nperformance. To address this challenge, we focus on improving two core\ncapabilities: Relevance Filtering, which identifies and retains information\ntruly relevant to reasoning, and Table Pruning, which reduces table size while\npreserving essential content. Based on these principles, we propose EnoTab, a\ndual denoising framework for complex questions and large-scale tables.\nSpecifically, we first perform Evidence-based Question Denoising by decomposing\nthe question into minimal semantic units and filtering out those irrelevant to\nanswer reasoning based on consistency and usability criteria. Then, we propose\nEvidence Tree-guided Table Denoising, which constructs an explicit and\ntransparent table pruning path to remove irrelevant data step by step. At each\npruning step, we observe the intermediate state of the table and apply a\npost-order node rollback mechanism to handle abnormal table states, ultimately\nproducing a highly reliable sub-table for final answer reasoning. Finally,\nextensive experiments show that EnoTab achieves outstanding performance on\nTableQA tasks with complex questions and large-scale tables, confirming its\neffectiveness.", "AI": {"tldr": "EnoTab is a dual denoising framework for TableQA tasks that improves relevance filtering and table pruning, resulting in outstanding performance on complex questions and large-scale tables.", "motivation": "The motivation is to address the challenge of noisy data in real-world applications involving complex questions and large tables, which severely degrades reasoning performance.", "method": "EnoTab is a dual denoising framework that includes Evidence-based Question Denoising and Evidence Tree-guided Table Denoising. The former decomposes the question into minimal semantic units and filters out irrelevant information, while the latter constructs an explicit and transparent table pruning path to remove irrelevant data step by step.", "result": "EnoTab achieves outstanding performance on TableQA tasks with complex questions and large-scale tables.", "conclusion": "EnoTab achieves outstanding performance on TableQA tasks with complex questions and large-scale tables, confirming its effectiveness."}}
{"id": "2509.17688", "pdf": "https://arxiv.org/pdf/2509.17688", "abs": "https://arxiv.org/abs/2509.17688", "authors": ["Daiye Miao", "Yufang Liu", "Jie Wang", "Changzhi Sun", "Yunke Zhang", "Demei Yan", "Shaokang Dong", "Qi Zhang", "Yuanbin Wu"], "title": "TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted to EMNLP 2025 (Main Conference),13 pages,10 figures", "summary": "LoRA has become one of the most widely used parameter-efficient fine-tuning\nmethods due to its simplicity and effectiveness. However, numerous studies have\nshown that LoRA often introduces substantial parameter redundancy, which not\nonly increases the number of trainable parameters but also hinders the\neffectiveness of fine-tuning. Since identifying redundant parameters in LoRA is\ninherently difficult, how to eliminate them efficiently and accurately remains\na challenging problem. In this paper, we propose TASO, a redundancy reduction\nmethod that leverages importance information from the pretrained model's\nweights to mitigate LoRA redundancy. Specifically, we estimate parameter\nimportance on downstream tasks and identify task-specific core regions based on\nthe distribution of importance scores. The location information of these core\nregions is then used to determine the sparse structure of LoRA modules,\nenabling redundancy removal before fine-tuning. Our approach significantly\nreduces the number of trainable parameters required for task adaptation, while\nproviding a novel task-aligned perspective for LoRA redundancy reduction.\nExperimental results demonstrate that, with a parameter budget comparable to\nLoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across\nmultiple tasks, achieving strong fine-tuning performance while effectively\neliminating redundant parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTASO\u7684\u5197\u4f59\u51cf\u5c11\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u7684\u91cd\u8981\u6027\u4fe1\u606f\u6765\u51cf\u8f7bLoRA\u7684\u5197\u4f59\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTASO\u5728\u53c2\u6570\u9884\u7b97\u4e0eLoRA\uff08\u79e9r=1\uff09\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6LoRA\uff0c\u540c\u65f6\u6709\u6548\u6d88\u9664\u4e86\u5197\u4f59\u53c2\u6570\u3002", "motivation": "LoRA\u867d\u7136\u7b80\u5355\u4e14\u6709\u6548\uff0c\u4f46\u5e38\u5e38\u5f15\u5165\u5927\u91cf\u53c2\u6570\u5197\u4f59\uff0c\u8fd9\u4e0d\u4ec5\u589e\u52a0\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u6570\u91cf\uff0c\u8fd8\u963b\u788d\u4e86\u5fae\u8c03\u7684\u6548\u679c\u3002\u7531\u4e8e\u5728LoRA\u4e2d\u8bc6\u522b\u5197\u4f59\u53c2\u6570\u672c\u8d28\u4e0a\u662f\u56f0\u96be\u7684\uff0c\u56e0\u6b64\u5982\u4f55\u9ad8\u6548\u51c6\u786e\u5730\u6d88\u9664\u5b83\u4eec\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5TASO\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u7684\u91cd\u8981\u6027\u4fe1\u606f\u6765\u4f30\u8ba1\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u53c2\u6570\u91cd\u8981\u6027\uff0c\u5e76\u57fa\u4e8e\u91cd\u8981\u6027\u5206\u6570\u7684\u5206\u5e03\u8bc6\u522b\u4efb\u52a1\u7279\u5b9a\u7684\u6838\u5fc3\u533a\u57df\u3002\u7136\u540e\u5229\u7528\u8fd9\u4e9b\u6838\u5fc3\u533a\u57df\u7684\u4f4d\u7f6e\u4fe1\u606f\u6765\u786e\u5b9aLoRA\u6a21\u5757\u7684\u7a00\u758f\u7ed3\u6784\uff0c\u4ece\u800c\u5728\u5fae\u8c03\u524d\u6d88\u9664\u5197\u4f59\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTASO\u5728\u53c2\u6570\u9884\u7b97\u4e0eLoRA\uff08\u79e9r=1\uff09\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6LoRA\uff0c\u540c\u65f6\u6709\u6548\u6d88\u9664\u4e86\u5197\u4f59\u53c2\u6570\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTASO\u7684\u5197\u4f59\u51cf\u5c11\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u7684\u91cd\u8981\u4fe1\u606f\u6765\u51cf\u8f7bLoRA\u7684\u5197\u4f59\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTASO\u5728\u53c2\u6570\u9884\u7b97\u4e0eLoRA\uff08\u79e9r=1\uff09\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6LoRA\uff0c\u540c\u65f6\u6709\u6548\u6d88\u9664\u4e86\u5197\u4f59\u53c2\u6570\u3002"}}
{"id": "2509.17694", "pdf": "https://arxiv.org/pdf/2509.17694", "abs": "https://arxiv.org/abs/2509.17694", "authors": ["Dongxu Lu", "Johan Jeuring", "Albert Gatt"], "title": "Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication at the 18th International Natural Language\n  Generation Conference (INLG 2025)", "summary": "Evaluating large language models (LLMs) in long-form, knowledge-grounded\nrole-play dialogues remains challenging. This study compares LLM-generated and\nhuman-authored responses in multi-turn professional training simulations\nthrough human evaluation ($N=38$) and automated LLM-as-a-judge assessment.\nHuman evaluation revealed significant degradation in LLM-generated response\nquality across turns, particularly in naturalness, context maintenance and\noverall quality, while human-authored responses progressively improved. In line\nwith this finding, participants also indicated a consistent preference for\nhuman-authored dialogue. These human judgements were validated by our automated\nLLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment\nwith human evaluators on both zero-shot pairwise preference and stochastic\n6-shot construct ratings, confirming the widening quality gap between LLM and\nhuman responses over time. Our work contributes a multi-turn benchmark exposing\nLLM degradation in knowledge-grounded role-play dialogues and provides a\nvalidated hybrid evaluation framework to guide the reliable integration of LLMs\nin training simulations.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u548c\u4eba\u7c7b\u64b0\u5199\u7684\u591a\u8f6e\u4e13\u4e1a\u57f9\u8bad\u6a21\u62df\u5bf9\u8bdd\uff0c\u5e76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u54cd\u5e94\u8d28\u91cf\u5728\u591a\u4e2a\u56de\u5408\u4e2d\u663e\u8457\u4e0b\u964d\uff0c\u800c\u4eba\u7c7b\u64b0\u5199\u7684\u54cd\u5e94\u5219\u9010\u6b65\u63d0\u9ad8\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u7bc7\u3001\u57fa\u4e8e\u77e5\u8bc6\u7684\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\uff08N=38\uff09\u548c\u81ea\u52a8\u5316\u7684LLM-as-a-judge\u8bc4\u4f30\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u54cd\u5e94\u548c\u4eba\u7c7b\u64b0\u5199\u7684\u54cd\u5e94\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u54cd\u5e94\u8d28\u91cf\u5728\u591a\u4e2a\u56de\u5408\u4e2d\u663e\u8457\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u81ea\u7136\u5ea6\u3001\u4e0a\u4e0b\u6587\u4fdd\u6301\u548c\u6574\u4f53\u8d28\u91cf\u65b9\u9762\uff0c\u800c\u4eba\u7c7b\u64b0\u5199\u7684\u54cd\u5e94\u5219\u9010\u6b65\u63d0\u9ad8\u3002\u53c2\u4e0e\u8005\u4e5f\u4e00\u81f4\u503e\u5411\u4e8e\u4eba\u7c7b\u64b0\u5199\u7684\u5bf9\u8bdd\u3002\u8fd9\u4e9b\u4eba\u7c7b\u5224\u65ad\u5f97\u5230\u4e86\u81ea\u52a8\u5316\u7684LLM-as-a-judge\u8bc4\u4f30\u7684\u9a8c\u8bc1\uff0c\u5176\u4e2dGemini 2.0 Flash\u5728\u96f6\u6837\u672c\u6210\u5bf9\u504f\u597d\u548c\u968f\u673a6\u6837\u672c\u6784\u5efa\u8bc4\u5206\u4e0a\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8005\u6709\u5f88\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u786e\u8ba4\u4e86\u968f\u7740\u65f6\u95f4\u63a8\u79fb\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u7c7b\u54cd\u5e94\u4e4b\u95f4\u7684\u8d28\u91cf\u5dee\u8ddd\u4e0d\u65ad\u6269\u5927\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u8d21\u732e\u4e86\u4e00\u4e2a\u591a\u8f6e\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u4e8e\u77e5\u8bc6\u7684\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4e2d\u7684\u9000\u5316\u60c5\u51b5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u6307\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u57f9\u8bad\u6a21\u62df\u4e2d\u7684\u53ef\u9760\u96c6\u6210\u3002"}}
{"id": "2509.17701", "pdf": "https://arxiv.org/pdf/2509.17701", "abs": "https://arxiv.org/abs/2509.17701", "authors": ["Mariam Mahran", "Katharina Simbeck"], "title": "Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at edu4AI'25: 2nd Workshop on Education for Artificial\n  Intelligence | co-located with ECAI, October 26th, 2025, Bologna, Italy. 7\n  pages, 0 figures", "summary": "Large Language Models (LLMs) are increasingly used for educational support,\nyet their response quality varies depending on the language of interaction.\nThis paper presents an automated multilingual pipeline for generating, solving,\nand evaluating math problems aligned with the German K-10 curriculum. We\ngenerated 628 math exercises and translated them into English, German, and\nArabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)\nwere prompted to produce step-by-step solutions in each language. A held-out\npanel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality\nusing a comparative framework. Results show a consistent gap, with English\nsolutions consistently rated highest, and Arabic often ranked lower. These\nfindings highlight persistent linguistic bias and the need for more equitable\nmultilingual AI systems in education.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u591a\u8bed\u8a00\u6d41\u7a0b\uff0c\u7528\u4e8e\u751f\u6210\u3001\u89e3\u51b3\u548c\u8bc4\u4f30\u4e0e\u5fb7\u56fdK-10\u8bfe\u7a0b\u76f8\u7b26\u7684\u6570\u5b66\u95ee\u9898\u3002\u7ed3\u679c\u663e\u793a\uff0c\u82f1\u8bed\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u9ad8\u4e8e\u963f\u62c9\u4f2f\u8bed\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u663e\u4e86\u8bed\u8a00\u504f\u89c1\u7684\u5b58\u5728\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u6559\u80b2\u4e2d\u9700\u8981\u66f4\u516c\u5e73\u7684\u591a\u8bed\u8a00AI\u7cfb\u7edf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u6559\u80b2\u652f\u6301\uff0c\u4f46\u5176\u54cd\u5e94\u8d28\u91cf\u56e0\u4ea4\u4e92\u8bed\u8a00\u800c\u5f02\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u591a\u8bed\u8a00\u6d41\u7a0b\u6765\u751f\u6210\u3001\u89e3\u51b3\u548c\u8bc4\u4f30\u6570\u5b66\u95ee\u9898\uff0c\u4ee5\u51cf\u5c11\u8bed\u8a00\u504f\u89c1\u5e76\u63d0\u9ad8\u6559\u80b2AI\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u591a\u8bed\u8a00\u6d41\u7a0b\uff0c\u7528\u4e8e\u751f\u6210\u3001\u89e3\u51b3\u548c\u8bc4\u4f30\u4e0e\u5fb7\u56fdK-10\u8bfe\u7a0b\u76f8\u7b26\u7684\u6570\u5b66\u95ee\u9898\u3002\u751f\u6210\u4e86628\u4e2a\u6570\u5b66\u7ec3\u4e60\u9898\uff0c\u5e76\u5c06\u5176\u7ffb\u8bd1\u6210\u82f1\u8bed\u3001\u5fb7\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u3002\u4e09\u79cd\u5546\u4e1aLLM\uff08GPT-4o-mini\u3001Gemini 2.5 Flash\u548cQwen-plus\uff09\u88ab\u63d0\u793a\u5728\u6bcf\u79cd\u8bed\u8a00\u4e2d\u751f\u6210\u9010\u6b65\u89e3\u51b3\u65b9\u6848\u3002\u4e00\u4e2a\u4fdd\u7559\u7684LLM\u8bc4\u59d4\u5c0f\u7ec4\uff08\u5305\u62ecClaude 3.5 Haiku\uff09\u4f7f\u7528\u6bd4\u8f83\u6846\u67b6\u8bc4\u4f30\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u82f1\u8bed\u89e3\u51b3\u65b9\u6848\u59cb\u7ec8\u88ab\u8bc4\u4e3a\u6700\u9ad8\uff0c\u800c\u963f\u62c9\u4f2f\u8bed\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u6392\u540d\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u8bed\u8a00\u504f\u89c1\u7684\u6301\u7eed\u5b58\u5728\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u6559\u80b2\u4e2d\u9700\u8981\u66f4\u516c\u5e73\u7684\u591a\u8bed\u8a00\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002"}}
{"id": "2509.17737", "pdf": "https://arxiv.org/pdf/2509.17737", "abs": "https://arxiv.org/abs/2509.17737", "authors": ["Kavin R V", "Pawan Goyal"], "title": "Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics", "categories": ["cs.CL"], "comment": "5 pages, 1 figure", "summary": "Standard language models employ unique, monolithic embeddings for each token,\npotentially limiting their ability to capture the multifaceted nature of word\nmeanings. We investigate whether tokens can be more effectively represented\nthrough a compositional structure that accumulates diverse semantic facets. To\nexplore this, we propose Aggregate Semantic Grouping (ASG), a novel approach\nleveraging Product Quantization (PQ). We apply ASG to standard transformer\narchitectures (mBERT, XLM-R, mT5) and evaluate this representational scheme\nacross diverse tasks (NLI, NER, QA), as well as a biomedical domain-specific\nbenchmark (BC5CDR) using BioBERT. Our findings demonstrate that representing\ntokens compositionally via ASG achieves extreme compression in embedding\nparameters (0.4--0.5\\%) while maintaining $>$95\\% task performance relative to\nthe base model, even in generative tasks and extends to both cross lingual\ntransfer and domain-specific settings. These results validate the principle\nthat tokens can be effectively modeled as combinations of shared semantic\nbuilding blocks. ASG offers a simple yet concrete method for achieving this,\nshowcasing how compositional representations can capture linguistic richness\nwhile enabling compact yet semantically rich models.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAggregate Semantic Grouping (ASG)\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528Product Quantization (PQ)\uff0c\u901a\u8fc7\u7ec4\u5408\u8bed\u4e49\u5757\u6765\u8868\u793a\u4ee4\u724c\uff0c\u5b9e\u73b0\u4e86\u5d4c\u5165\u53c2\u6570\u7684\u6781\u7aef\u538b\u7f29\uff080.4-0.5%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8d85\u8fc795%\u7684\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "Standard language models employ unique, monolithic embeddings for each token, potentially limiting their ability to capture the multifaceted nature of word meanings.", "method": "Aggregate Semantic Grouping (ASG), a novel approach leveraging Product Quantization (PQ).", "result": "Representing tokens compositionally via ASG achieves extreme compression in embedding parameters (0.4--0.5%) while maintaining >95% task performance relative to the base model, even in generative tasks and extends to both cross lingual transfer and domain-specific settings.", "conclusion": "ASG offers a simple yet concrete method for achieving this, showcasing how compositional representations can capture linguistic richness while enabling compact yet semantically rich models."}}
{"id": "2509.17765", "pdf": "https://arxiv.org/pdf/2509.17765", "abs": "https://arxiv.org/abs/2509.17765", "authors": ["Jin Xu", "Zhifang Guo", "Hangrui Hu", "Yunfei Chu", "Xiong Wang", "Jinzheng He", "Yuxuan Wang", "Xian Shi", "Ting He", "Xinfa Zhu", "Yuanjun Lv", "Yongqi Wang", "Dake Guo", "He Wang", "Linhan Ma", "Pei Zhang", "Xinyu Zhang", "Hongkun Hao", "Zishan Guo", "Baosong Yang", "Bin Zhang", "Ziyang Ma", "Xipin Wei", "Shuai Bai", "Keqin Chen", "Xuejing Liu", "Peng Wang", "Mingkun Yang", "Dayiheng Liu", "Xingzhang Ren", "Bo Zheng", "Rui Men", "Fan Zhou", "Bowen Yu", "Jianxin Yang", "Le Yu", "Jingren Zhou", "Junyang Lin"], "title": "Qwen3-Omni Technical Report", "categories": ["cs.CL", "cs.AI", "cs.CV", "eess.AS"], "comment": "https://github.com/QwenLM/Qwen3-Omni", "summary": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.", "AI": {"tldr": "Qwen3-Omni is a single multimodal model that achieves state-of-the-art performance across text, image, audio, and video. It outperforms existing models on multiple benchmarks and supports multilingual text and speech interaction. The model is released under the Apache 2.0 license.", "motivation": "The research community lacks a general-purpose audio captioning model, and there is a need for a single multimodal model that maintains state-of-the-art performance across text, image, audio, and video without degradation.", "method": "Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video. It uses a multi-codebook scheme for autoregressive prediction of discrete speech codecs and replaces block-wise diffusion with a lightweight causal ConvNet. A Thinking model is introduced for multimodal reasoning, and Qwen3-Omni-30B-A3B is fine-tuned for audio captioning.", "result": "Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22 across 36 audio and audio-visual benchmarks. It outperforms strong closed-source models like Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. The model achieves a theoretical end-to-end first-packet latency of 234 ms in cold-start settings.", "conclusion": "Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license."}}
{"id": "2509.17766", "pdf": "https://arxiv.org/pdf/2509.17766", "abs": "https://arxiv.org/abs/2509.17766", "authors": ["Ziyi Liu"], "title": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) struggle with information forgetting and\ninefficiency in long-horizon, multi-turn dialogues. To address this, we propose\na training-free prompt engineering method, the State-Update Multi-turn Dialogue\nStrategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to\neffectively manage dialogue history. Our strategy shows strong performance\nacross multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,\nit improves the core information filtering score by 32.6%, leading to a 14.1%\nincrease in the downstream QA score, while also reducing inference time by\n73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal\nroles of both components. Our work offers an effective solution for optimizing\nLLMs in long-range interactions, providing new insights for developing more\nrobust Agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u8303\u56f4\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u72b6\u6001\u91cd\u5efa\u548c\u5386\u53f2\u63d0\u9192\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u8303\u56f4\u3001\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5b58\u5728\u4fe1\u606f\u9057\u5fd8\u548c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5373\u72b6\u6001\u66f4\u65b0\u591a\u8f6e\u5bf9\u8bdd\u7b56\u7565\uff0c\u5229\u7528\u201c\u72b6\u6001\u91cd\u5efa\u201d\u548c\u201c\u5386\u53f2\u63d0\u9192\u201d\u673a\u5236\u6765\u6709\u6548\u7ba1\u7406\u5bf9\u8bdd\u5386\u53f2\u3002", "result": "\u5728\u591a\u4e2a\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002\u4f8b\u5982\uff0c\u5728HotpotQA\u6570\u636e\u96c6\u4e0a\uff0c\u5b83\u5c06\u6838\u5fc3\u4fe1\u606f\u8fc7\u6ee4\u5f97\u5206\u63d0\u9ad8\u4e8632.6%\uff0c\u5bfc\u81f4\u4e0b\u6e38\u95ee\u7b54\u5f97\u5206\u589e\u52a0\u4e8614.1%\uff0c\u540c\u65f6\u51cf\u5c11\u4e8673.1%\u7684\u63a8\u7406\u65f6\u95f4\u548c59.4%\u7684\u6807\u8bb0\u6d88\u8017\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u4f18\u5316\u957f\u8ddd\u79bb\u4ea4\u4e92\u4e2d\u7684LLMs\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.17768", "pdf": "https://arxiv.org/pdf/2509.17768", "abs": "https://arxiv.org/abs/2509.17768", "authors": ["Jessica Ojo", "Zina Kamel", "David Ifeoluwa Adelani"], "title": "DIVERS-Bench: Evaluating Language Identification Across Domain Shifts and Code-Switching", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language Identification (LID) is a core task in multilingual NLP, yet current\nsystems often overfit to clean, monolingual data. This work introduces\nDIVERS-BENCH, a comprehensive evaluation of state-of-the-art LID models across\ndiverse domains, including speech transcripts, web text, social media texts,\nchildren's stories, and code-switched text. Our findings reveal that while\nmodels achieve high accuracy on curated datasets, performance degrades sharply\non noisy and informal inputs. We also introduce DIVERS-CS, a diverse\ncode-switching benchmark dataset spanning 10 language pairs, and show that\nexisting models struggle to detect multiple languages within the same sentence.\nThese results highlight the need for more robust and inclusive LID systems in\nreal-world settings.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86DIVERS-BENCH\u548cDIVERS-CS\uff0c\u53d1\u73b0\u73b0\u6709LID\u6a21\u578b\u5728\u5904\u7406\u5608\u6742\u548c\u975e\u6b63\u5f0f\u8f93\u5165\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u540c\u4e00\u53e5\u5b50\u4e2d\u7684\u591a\u79cd\u8bed\u8a00\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u5f53\u524d\u7684LID\u7cfb\u7edf\u5f80\u5f80\u8fc7\u5ea6\u62df\u5408\u5230\u5e72\u51c0\u3001\u5355\u8bed\u6570\u636e\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u901a\u5e38\u5608\u6742\u4e14\u975e\u6b63\u5f0f\u3002", "method": "\u5f15\u5165DIVERS-BENCH\uff0c\u5bf9\u6700\u5148\u8fdb\u7684LID\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u7684\u8bc4\u4f30\uff0c\u4ee5\u53ca\u5f15\u5165DIVERS-CS\u4ee3\u7801\u5207\u6362\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5608\u6742\u548c\u975e\u6b63\u5f0f\u8f93\u5165\u4e0a\u7684\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u73b0\u6709\u7684\u6a21\u578b\u96be\u4ee5\u68c0\u6d4b\u540c\u4e00\u53e5\u5b50\u4e2d\u7684\u591a\u79cd\u8bed\u8a00\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u9700\u8981\u66f4\u7a33\u5065\u548c\u5305\u5bb9\u7684LID\u7cfb\u7edf\u3002"}}
{"id": "2509.17788", "pdf": "https://arxiv.org/pdf/2509.17788", "abs": "https://arxiv.org/abs/2509.17788", "authors": ["Xingyu Fan", "Feifei Li", "Wenhui Que", "Hailong Li"], "title": "One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "7 pages", "summary": "Conversational agents deployed in industrial-scale official account platforms\nmust generate responses that are both contextually grounded and stylistically\naligned-requirements that existing methods struggle to meet. Chain-of-thought\n(CoT) prompting induces significant latency due to multi-turn reasoning;\nper-account fine-tuning is computationally prohibitive; and long prompt-based\nmethods degrade the model's ability to grasp injected context and style. In\nthis paper, we propose WeStar, a lite-adaptive framework for stylized\ncontextual question answering that scales to millions of official accounts.\nWeStar combines context-grounded generation via RAG with style-aware generation\nusing Parametric RAG (PRAG), where LoRA modules are dynamically activated per\nstyle cluster. Our contributions are fourfold: (1) We introduce WeStar, a\nunified framework capable of serving large volumes of official accounts with\nminimal overhead. (2) We propose a multi-dimensional, cluster-based parameter\nsharing scheme that enables compact style representation while preserving\nstylistic diversity. (3) We develop a style-enhanced Direct Preference\nOptimization (SeDPO) method to optimize each style cluster's parameters for\nimproved generation quality. (4) Experiments on a large-scale industrial\ndataset validate the effectiveness and efficiency of WeStar, underscoring its\npracitical value in real-world deployment.", "AI": {"tldr": "WeStar is a lite-adaptive framework for stylized contextual question answering that scales to millions of official accounts by combining RAG and PRAG with dynamic LoRA modules, multi-dimensional parameter sharing, and SeDPO optimization.", "motivation": "Existing methods struggle to meet the requirements of generating responses that are both contextually grounded and stylistically aligned for industrial-scale official account platforms. Chain-of-thought prompting, per-account fine-tuning, and long prompt-based methods have limitations in terms of latency, computational cost, and model performance.", "method": "WeStar combines context-grounded generation via RAG with style-aware generation using Parametric RAG (PRAG), where LoRA modules are dynamically activated per style cluster. It also includes a multi-dimensional, cluster-based parameter sharing scheme and a style-enhanced Direct Preference Optimization (SeDPO) method.", "result": "Experiments on a large-scale industrial dataset validate the effectiveness and efficiency of WeStar, showing its practical value in real-world deployment.", "conclusion": "WeStar demonstrates effectiveness and efficiency in real-world deployment, making it a practical solution for large-scale conversational agents."}}
{"id": "2509.17794", "pdf": "https://arxiv.org/pdf/2509.17794", "abs": "https://arxiv.org/abs/2509.17794", "authors": ["Tobias Groot", "Salo Lacunes", "Evgenia Ilia"], "title": "Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction", "categories": ["cs.CL"], "comment": "EMNLP UncertaiNLP Workshop 2025", "summary": "Natural language generation (NLG) tasks are often subject to inherent\nvariability; \\emph{e.g.} predicting the next word given a context has multiple\nvalid responses, evident when asking multiple humans to complete the task.\nWhile having language models (LMs) that are aligned pluralistically, so that\nthey are able to reproduce well the inherent diversity in perspectives of an\nentire population of interest is clearly beneficial, \\citet{ilia2024predict}\nshow that LMs do not reproduce this type of linguistic variability well. They\nspeculate this inability might stem from the lack of consistent training of LMs\nwith data reflecting this type of inherent variability. As such, we investigate\nwhether training LMs on multiple plausible word continuations per context can\nimprove their ability to reproduce human linguistic variability for next-word\nprediction. We employ fine-tuning techniques for pre-trained and\ninstruction-tuned models; and demonstrate their potential when fine-tuning\nGPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures\ndivergence among empirically estimated human and model next-word distributions\nacross contexts before and after fine-tuning, shows that our multi-label\nfine-tuning improves the LMs' ability to reproduce linguistic variability; both\nfor contexts that admit higher and lower variability.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5728\u591a\u4e2a\u53ef\u80fd\u7684\u8bcd\u5ef6\u7eed\u4e0a\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u63d0\u9ad8\u5176\u518d\u73b0\u4eba\u7c7b\u8bed\u8a00\u53d8\u5f02\u6027\u80fd\u529b\u7684\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5f88\u597d\u5730\u518d\u73b0\u4eba\u7c7b\u8bed\u8a00\u7684\u53d8\u5f02\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5728\u591a\u4e2a\u53ef\u80fd\u7684\u8bcd\u5ef6\u7eed\u4e0a\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u91c7\u7528\u5fae\u8c03\u6280\u672f\u5bf9\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528Provo\u8bed\u6599\u5e93\u5bf9GPT-2\u548cMistral-7B-IT\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u591a\u6807\u7b7e\u5fae\u8c03\u80fd\u591f\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u518d\u73b0\u8bed\u8a00\u53d8\u5f02\u6027\u80fd\u529b\uff0c\u65e0\u8bba\u662f\u9ad8\u53d8\u5f02\u6027\u8fd8\u662f\u4f4e\u53d8\u5f02\u6027\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u672c\u6587\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u5728\u591a\u4e2a\u53ef\u80fd\u7684\u8bcd\u5ef6\u7eed\u4e0a\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5b83\u4eec\u518d\u73b0\u4eba\u7c7b\u8bed\u8a00\u53d8\u5f02\u6027\u7684\u80fd\u529b\uff0c\u65e0\u8bba\u4e0a\u4e0b\u6587\u7684\u53d8\u5f02\u6027\u9ad8\u4f4e\u3002"}}
{"id": "2509.17796", "pdf": "https://arxiv.org/pdf/2509.17796", "abs": "https://arxiv.org/abs/2509.17796", "authors": ["Michal Nov\u00e1k", "Miloslav Konop\u00edk", "Anna Nedoluzhko", "Martin Popel", "Ond\u0159ej Pra\u017e\u00e1k", "Jakub Sido", "Milan Straka", "Zden\u011bk \u017dabokrtsk\u00fd", "Daniel Zeman"], "title": "Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?", "categories": ["cs.CL"], "comment": "Accepted to CODI-CRAC 2025", "summary": "The paper presents an overview of the fourth edition of the Shared Task on\nMultilingual Coreference Resolution, organized as part of the CODI-CRAC 2025\nworkshop. As in the previous editions, participants were challenged to develop\nsystems that identify mentions and cluster them according to identity\ncoreference.\n  A key innovation of this year's task was the introduction of a dedicated\nLarge Language Model (LLM) track, featuring a simplified plaintext format\ndesigned to be more suitable for LLMs than the original CoNLL-U representation.\n  The task also expanded its coverage with three new datasets in two additional\nlanguages, using version 1.3 of CorefUD - a harmonized multilingual collection\nof 22 datasets in 17 languages.\n  In total, nine systems participated, including four LLM-based approaches (two\nfine-tuned and two using few-shot adaptation). While traditional systems still\nkept the lead, LLMs showed clear potential, suggesting they may soon challenge\nestablished approaches in future editions.", "AI": {"tldr": "The paper discusses the fourth edition of the Shared Task on Multilingual Coreference Resolution, highlighting the introduction of an LLM track and the potential of LLMs in this area.", "motivation": "To evaluate the performance of systems in multilingual coreference resolution and explore the potential of LLMs in this task.", "method": "The paper presents an overview of the fourth edition of the Shared Task on Multilingual Coreference Resolution, including a dedicated LLM track and new datasets.", "result": "Nine systems participated, including four LLM-based approaches. Traditional systems still led, but LLMs showed promise.", "conclusion": "LLMs showed clear potential in coreference resolution, suggesting they may soon challenge established approaches."}}
{"id": "2509.17807", "pdf": "https://arxiv.org/pdf/2509.17807", "abs": "https://arxiv.org/abs/2509.17807", "authors": ["Jihae Jeong", "DaeYeop Lee", "DongGeon Lee", "Hwanjo Yu"], "title": "Everyday Physics in Korean Contexts: A Culturally Grounded Physical Reasoning Benchmark", "categories": ["cs.CL"], "comment": "Accepted to MRL@EMNLP 2025", "summary": "Existing physical commonsense reasoning benchmarks predominantly focus on\nWestern contexts, overlooking cultural variations in physical problem-solving.\nTo address this gap, we introduce EPiK (Everyday Physics in Korean Contexts), a\nnovel benchmark comprising 181 binary-choice problems that test physical\nreasoning within Korean cultural contexts, ranging from kimchi (Korean food) to\ntraditional fermentation. EPiK is constructed using a two-stage generation and\nverification pipeline to create culturally-authentic problems across 9\nreasoning subtasks and 84 scenarios. Unlike approaches based on simple\ntranslation, our method generates problems organically from Korean contexts\nwhile upholding rigorous physical reasoning standards. Our evaluations show\nthat Korean-specialized models consistently outperform general-purpose models\nof comparable size. This performance gap highlights the limitations of\nculturally-agnostic models and demonstrates the critical need for\nculturally-aware benchmarks to truly measure language understanding. Our EPiK\nis publicly available at https://huggingface.co/datasets/jjae/EPiK.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86EPiK\uff0c\u4e00\u4e2a\u9488\u5bf9\u97e9\u56fd\u6587\u5316\u80cc\u666f\u7684\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u897f\u65b9\u80cc\u666f\uff0c\u5ffd\u89c6\u4e86\u7269\u7406\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6587\u5316\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u751f\u6210\u548c\u9a8c\u8bc1\u6d41\u7a0b\u521b\u5efa\u4e86181\u4e2a\u4e8c\u9009\u4e00\u95ee\u9898\uff0c\u6db5\u76d6\u4e869\u4e2a\u63a8\u7406\u5b50\u4efb\u52a1\u548c84\u4e2a\u573a\u666f\uff0c\u786e\u4fdd\u95ee\u9898\u7684\u6587\u5316\u771f\u5b9e\u6027\u3002", "result": "\u97e9\u56fd\u4e13\u95e8\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u8868\u660e\u6587\u5316\u65e0\u504f\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "EPiK\u5c55\u793a\u4e86\u6587\u5316\u76f8\u5173\u57fa\u51c6\u5728\u771f\u6b63\u8861\u91cf\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e14\u5176\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2509.17829", "pdf": "https://arxiv.org/pdf/2509.17829", "abs": "https://arxiv.org/abs/2509.17829", "authors": ["Manoj Madushanka Perera", "Adnan Mahmood", "Kasun Eranda Wijethilake", "Quan Z. Sheng"], "title": "Towards Adaptive Context Management for Intelligent Conversational Question Answering", "categories": ["cs.CL", "I.2.7; H.3.3"], "comment": "Comments: 15 pages, 6 figures, Table 1, published in Lecture Notes in\n  Computer Science (LNCS 15391), Proceedings of ADMA 2024. DOI:\n  10.1007/978-981-96-0847-8_25", "summary": "This particular paper introduces an Adaptive Context Management (ACM)\nframework for the Conversational Question Answering (ConvQA) systems. The key\nobjective of the ACM framework is to optimize the use of the conversation\nhistory by dynamically managing context for maximizing the relevant information\nprovided to a ConvQA model within its token limit. Our approach incorporates a\nContext Manager (CM) Module, a Summarization (SM) Module, and an Entity\nExtraction (EE) Module in a bid to handle the conversation history\nefficaciously. The CM Module dynamically adjusts the context size, thereby\npreserving the most relevant and recent information within a model's token\nlimit. The SM Module summarizes the older parts of the conversation history via\na sliding window. When the summarization window exceeds its limit, the EE\nModule identifies and retains key entities from the oldest conversation turns.\nExperimental results demonstrate the effectiveness of our envisaged framework\nin generating accurate and contextually appropriate responses, thereby\nhighlighting the potential of the ACM framework to enhance the robustness and\nscalability of the ConvQA systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u7ba1\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u5bf9\u8bdd\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u7ba1\u7406\u4e0a\u4e0b\u6587\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u4f18\u5316\u5bf9\u8bdd\u5386\u53f2\u7684\u4f7f\u7528\uff0c\u901a\u8fc7\u52a8\u6001\u7ba1\u7406\u4e0a\u4e0b\u6587\u6765\u6700\u5927\u5316\u63d0\u4f9b\u7ed9ConvQA\u6a21\u578b\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u5bf9\u8bdd\u95ee\u7b54\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u7ba1\u7406\uff08ACM\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u8bdd\u95ee\u7b54\uff08ConvQA\uff09\u7cfb\u7edf\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u4e0a\u4e0b\u6587\u7ba1\u7406\uff08CM\uff09\u6a21\u5757\u3001\u4e00\u4e2a\u6458\u8981\uff08SM\uff09\u6a21\u5757\u548c\u4e00\u4e2a\u5b9e\u4f53\u63d0\u53d6\uff08EE\uff09\u6a21\u5757\uff0c\u4ee5\u6709\u6548\u5730\u5904\u7406\u5bf9\u8bdd\u5386\u53f2\u3002CM\u6a21\u5757\u52a8\u6001\u8c03\u6574\u4e0a\u4e0b\u6587\u5927\u5c0f\uff0c\u4ece\u800c\u5728\u6a21\u578b\u7684\u6807\u8bb0\u9650\u5236\u5185\u4fdd\u7559\u6700\u76f8\u5173\u548c\u6700\u65b0\u7684\u4fe1\u606f\u3002SM\u6a21\u5757\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5bf9\u5bf9\u8bdd\u5386\u53f2\u7684\u8f83\u65e7\u90e8\u5206\u8fdb\u884c\u6458\u8981\u3002\u5f53\u6458\u8981\u7a97\u53e3\u8d85\u8fc7\u5176\u9650\u5236\u65f6\uff0cEE\u6a21\u5757\u4f1a\u8bc6\u522b\u5e76\u4fdd\u7559\u6700\u65e9\u5bf9\u8bdd\u56de\u5408\u4e2d\u7684\u5173\u952e\u5b9e\u4f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u8bbe\u60f3\u7684\u6846\u67b6\u5728\u751f\u6210\u51c6\u786e\u4e14\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u54cd\u5e94\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u4ece\u800c\u7a81\u663e\u4e86ACM\u6846\u67b6\u589e\u5f3aConvQA\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u6f5c\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u8bbe\u60f3\u7684\u6846\u67b6\u5728\u751f\u6210\u51c6\u786e\u4e14\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u54cd\u5e94\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u4ece\u800c\u7a81\u663e\u4e86ACM\u6846\u67b6\u589e\u5f3aConvQA\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17830", "pdf": "https://arxiv.org/pdf/2509.17830", "abs": "https://arxiv.org/abs/2509.17830", "authors": ["Lekkala Sai Teja", "Annepaka Yadagiri", "and Partha Pakray", "Chukhu Chunka", "Mangadoddi Srikar Vardhan"], "title": "Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 14 figures", "summary": "Generation of Artificial Intelligence (AI) texts in important works has\nbecome a common practice that can be used to misuse and abuse AI at various\nlevels. Traditional AI detectors often rely on document-level classification,\nwhich struggles to identify AI content in hybrid or slightly edited texts\ndesigned to avoid detection, leading to concerns about the model's efficiency,\nwhich makes it hard to distinguish between human-written and AI-generated\ntexts. A sentence-level sequence labeling model proposed to detect transitions\nbetween human- and AI-generated text, leveraging nuanced linguistic signals\noverlooked by document-level classifiers. By this method, detecting and\nsegmenting AI and human-written text within a single document at the\ntoken-level granularity is achieved. Our model combines the state-of-the-art\npre-trained Transformer models, incorporating Neural Networks (NN) and\nConditional Random Fields (CRFs). This approach extends the power of\ntransformers to extract semantic and syntactic patterns, and the neural network\ncomponent to capture enhanced sequence-level representations, thereby improving\nthe boundary predictions by the CRF layer, which enhances sequence recognition\nand further identification of the partition between Human- and AI-generated\ntexts. The evaluation is performed on two publicly available benchmark datasets\ncontaining collaborative human and AI-generated texts. Our experimental\ncomparisons are with zero-shot detectors and the existing state-of-the-art\nmodels, along with rigorous ablation studies to justify that this approach, in\nparticular, can accurately detect the spans of AI texts in a completely\ncollaborative text. All our source code and the processed datasets are\navailable in our GitHub repository.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53e5\u5b50\u7ea7\u522b\u7684\u5e8f\u5217\u6807\u6ce8\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u5272\u6587\u6863\u4e2d\u7684\u4eba\u7c7b\u548cAI\u751f\u6210\u6587\u672c\uff0c\u901a\u8fc7\u7ed3\u5408Transformer\u3001\u795e\u7ecf\u7f51\u7edc\u548cCRF\u63d0\u9ad8\u8fb9\u754c\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfAI\u68c0\u6d4b\u5668\u5728\u6df7\u5408\u6216\u8f7b\u5fae\u7f16\u8f91\u7684\u6587\u672c\u4e2d\u96be\u4ee5\u8bc6\u522bAI\u5185\u5bb9\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u533a\u5206\u4eba\u7c7b\u64b0\u5199\u548cAI\u751f\u6210\u7684\u6587\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53e5\u5b50\u7ea7\u522b\u7684\u5e8f\u5217\u6807\u6ce8\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u6700\u5148\u8fdb\u7684\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u3001\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u548c\u6761\u4ef6\u968f\u673a\u573a\uff08CRFs\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u6bd4\u8f83\u5305\u62ec\u96f6\u6837\u672c\u68c0\u6d4b\u5668\u548c\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u4ee5\u53ca\u4e25\u683c\u7684\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u4ee5\u51c6\u786e\u68c0\u6d4b\u534f\u4f5c\u6587\u672c\u4e2d\u7684AI\u6587\u672c\u6bb5\u843d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u5b8c\u5168\u534f\u4f5c\u6587\u672c\u4e2dAI\u6587\u672c\u7684\u6bb5\u843d\uff0c\u4e14\u6e90\u4ee3\u7801\u548c\u5904\u7406\u540e\u7684\u6570\u636e\u96c6\u5df2\u516c\u5f00\u5728GitHub\u4ed3\u5e93\u4e2d\u3002"}}
{"id": "2509.17844", "pdf": "https://arxiv.org/pdf/2509.17844", "abs": "https://arxiv.org/abs/2509.17844", "authors": ["Lynn Greschner", "Sabine Weber", "Roman Klinger"], "title": "Trust Me, I Can Convince You: The Contextualized Argument Appraisal Framework", "categories": ["cs.CL"], "comment": null, "summary": "Emotions, which influence how convincing an argument is, are developed\n  in context of the self and sender, and therefore require modeling\n  the cognitive evaluation process. While binary emotionality has been\n  studied in argument mining, and the cognitive appraisal has been\n  modeled in general emotion analysis, these fields have not been\n  brought together yet. We therefore propose the Contextualized\n  Argument Appraisal Framework that contextualizes the interplay\n  between the sender, receiver, and argument. It includes emotion\n  labels, appraisals, such as argument familiarity, response urgency,\n  and expected effort, as well as convincingness variables. To evaluate\n  the framework and pave the way to computational modeling, we perform\n  a study in a role-playing scenario, mimicking real-world exposure to\n  arguments, asking participants to disclose their emotion, explain the main\ncause, the\n  argument appraisal, and the\n  perceived convincingness. To consider the subjective nature of such\n  annotations, we also collect demographic data and personality traits\n  of both the participants and the perceived sender of the argument.\n  The analysis of the resulting corpus of 800 arguments, each\n  annotated by 5 participants, reveals that convincingness is\n  positively correlated with positive emotions (e.g., trust) and\n  negatively correlated with negative emotions (e.g., anger). The\n  appraisal variables disclose the importance of the argument\n  familiarity. For most participants, the content of the argument\n  itself is the primary driver of the emotional response.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u60c5\u5883\u5316\u7684\u8bba\u70b9\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u573a\u666f\u7814\u7a76\uff0c\u53d1\u73b0\u8bf4\u670d\u529b\u4e0e\u79ef\u6781\u60c5\u7eea\u5448\u6b63\u76f8\u5173\uff0c\u4e0e\u6d88\u6781\u60c5\u7eea\u5448\u8d1f\u76f8\u5173\uff0c\u5e76\u5f3a\u8c03\u4e86\u8bba\u70b9\u719f\u6089\u5ea6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u867d\u7136\u4e8c\u5143\u60c5\u7eea\u6027\u5df2\u7ecf\u5728\u8bba\u70b9\u6316\u6398\u4e2d\u88ab\u7814\u7a76\uff0c\u8ba4\u77e5\u8bc4\u4f30\u5728\u4e00\u822c\u60c5\u7eea\u5206\u6790\u4e2d\u4e5f\u88ab\u5efa\u6a21\uff0c\u4f46\u8fd9\u4e24\u4e2a\u9886\u57df\u5c1a\u672a\u7ed3\u5408\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u4e2a\u60c5\u5883\u5316\u7684\u8bba\u70b9\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u7ed3\u5408\u8fd9\u4e24\u4e2a\u9886\u57df\u3002", "method": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u4e2a\u60c5\u5883\u5316\u7684\u8bba\u70b9\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8003\u8651\u4e86\u53d1\u9001\u8005\u3001\u63a5\u6536\u8005\u548c\u8bba\u70b9\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u4ed6\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u89d2\u8272\u626e\u6f14\u573a\u666f\u7814\u7a76\uff0c\u8ba9\u53c2\u4e0e\u8005\u62ab\u9732\u4ed6\u4eec\u7684\u60c5\u7eea\uff0c\u89e3\u91ca\u4e3b\u8981\u539f\u56e0\u3001\u8bba\u70b9\u8bc4\u4f30\u548c\u611f\u77e5\u8bf4\u670d\u529b\uff0c\u5e76\u6536\u96c6\u4e86\u53c2\u4e0e\u8005\u548c\u8bba\u70b9\u53d1\u9001\u8005\u7684 demographic \u6570\u636e\u548c\u4eba\u683c\u7279\u5f81\u3002", "result": "\u5206\u6790\u7ed3\u679c\u8868\u660e\uff0c\u8bf4\u670d\u529b\u4e0e\u79ef\u6781\u60c5\u7eea\uff08\u5982\u4fe1\u4efb\uff09\u5448\u6b63\u76f8\u5173\uff0c\u4e0e\u6d88\u6781\u60c5\u7eea\uff08\u5982\u6124\u6012\uff09\u5448\u8d1f\u76f8\u5173\u3002\u8bc4\u4f30\u53d8\u91cf\u63ed\u793a\u4e86\u8bba\u70b9\u719f\u6089\u5ea6\u7684\u91cd\u8981\u6027\u3002\u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u4e0e\u8005\u6765\u8bf4\uff0c\u8bba\u70b9\u672c\u8eab\u7684\u5185\u5bb9\u662f\u60c5\u611f\u53cd\u5e94\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8bf4\u670d\u529b\u4e0e\u79ef\u6781\u60c5\u7eea\uff08\u5982\u4fe1\u4efb\uff09\u5448\u6b63\u76f8\u5173\uff0c\u4e0e\u6d88\u6781\u60c5\u7eea\uff08\u5982\u6124\u6012\uff09\u5448\u8d1f\u76f8\u5173\u3002\u8bc4\u4f30\u53d8\u91cf\u63ed\u793a\u4e86\u8bba\u70b9\u719f\u6089\u5ea6\u7684\u91cd\u8981\u6027\u3002\u5bf9\u4e8e\u5927\u591a\u6570\u53c2\u4e0e\u8005\u6765\u8bf4\uff0c\u8bba\u70b9\u672c\u8eab\u7684\u5185\u5bb9\u662f\u60c5\u611f\u53cd\u5e94\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002"}}
{"id": "2509.17855", "pdf": "https://arxiv.org/pdf/2509.17855", "abs": "https://arxiv.org/abs/2509.17855", "authors": ["Robert Litschko", "Verena Blaschke", "Diana Burkhardt", "Barbara Plank", "Diego Frassinelli"], "title": "Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora", "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 (Findings)", "summary": "Dialects exhibit a substantial degree of variation due to the lack of a\nstandard orthography. At the same time, the ability of Large Language Models\n(LLMs) to process dialects remains largely understudied. To address this gap,\nwe use Bavarian as a case study and investigate the lexical dialect\nunderstanding capability of LLMs by examining how well they recognize and\ntranslate dialectal terms across different parts-of-speech. To this end, we\nintroduce DiaLemma, a novel annotation framework for creating dialect variation\ndictionaries from monolingual data only, and use it to compile a ground truth\ndataset consisting of 100K human-annotated German-Bavarian word pairs. We\nevaluate how well nine state-of-the-art LLMs can judge Bavarian terms as\ndialect translations, inflected variants, or unrelated forms of a given German\nlemma. Our results show that LLMs perform best on nouns and lexically similar\nword pairs, and struggle most in distinguishing between direct translations and\ninflected variants. Interestingly, providing additional context in the form of\nexample usages improves the translation performance, but reduces their ability\nto recognize dialect variants. This study highlights the limitations of LLMs in\ndealing with orthographic dialect variation and emphasizes the need for future\nwork on adapting LLMs to dialects.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5df4\u4f10\u5229\u4e9a\u8bed\u65b9\u8a00\u7684\u80fd\u529b\uff0c\u5f15\u5165\u4e86DiaLemma\u6846\u67b6\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u65b9\u8a00\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u533a\u5206\u7ffb\u8bd1\u548c\u5c48\u6298\u53d8\u4f53\u65b9\u9762\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u6b63\u5b57\u6cd5\uff0c\u65b9\u8a00\u8868\u73b0\u51fa\u76f8\u5f53\u5927\u7684\u5dee\u5f02\u3002\u540c\u65f6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5904\u7406\u65b9\u8a00\u7684\u80fd\u529b\u4ecd\u7814\u7a76\u4e0d\u8db3\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u4ee5\u5df4\u4f10\u5229\u4e9a\u8bed\u4e3a\u4f8b\uff0c\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u6c47\u65b9\u8a00\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8003\u5bdf\u5b83\u4eec\u5728\u4e0d\u540c\u8bcd\u6027\u4e0a\u8bc6\u522b\u548c\u7ffb\u8bd1\u65b9\u8a00\u672f\u8bed\u7684\u6548\u679c\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86DiaLemma\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u6ce8\u91ca\u6846\u67b6\uff0c\u7528\u4e8e\u4ec5\u4ece\u5355\u8bed\u6570\u636e\u521b\u5efa\u65b9\u8a00\u53d8\u4f53\u8bcd\u5178\uff0c\u5e76\u4f7f\u7528\u5b83\u6765\u7f16\u8bd1\u4e00\u4e2a\u5305\u542b100K\u4eba\u5de5\u6807\u6ce8\u7684\u5fb7\u8bed-\u5df4\u4f10\u5229\u4e9a\u8bed\u8bcd\u5bf9\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u4e5d\u79cd\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5224\u65ad\u5df4\u4f10\u5229\u4e9a\u8bed\u8bcd\u4f5c\u4e3a\u65b9\u8a00\u7ffb\u8bd1\u3001\u5c48\u6298\u53d8\u4f53\u6216\u4e0e\u7ed9\u5b9a\u5fb7\u8bed\u8bcd\u6839\u65e0\u5173\u7684\u5f62\u5f0f\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u540d\u8bcd\u548c\u8bcd\u4e49\u76f8\u4f3c\u7684\u8bcd\u5bf9\u4e0a\u8868\u73b0\u6700\u597d\uff0c\u800c\u5728\u533a\u5206\u76f4\u63a5\u7ffb\u8bd1\u548c\u5c48\u6298\u53d8\u4f53\u65b9\u9762\u6700\u56f0\u96be\u3002\u6709\u8da3\u7684\u662f\uff0c\u63d0\u4f9b\u793a\u4f8b\u7528\u6cd5\u4f5c\u4e3a\u989d\u5916\u4e0a\u4e0b\u6587\u53ef\u4ee5\u63d0\u9ad8\u7ffb\u8bd1\u6027\u80fd\uff0c\u4f46\u4f1a\u964d\u4f4e\u5b83\u4eec\u8bc6\u522b\u65b9\u8a00\u53d8\u4f53\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u7a81\u663e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6b63\u5b57\u6cd5\u65b9\u8a00\u53d8\u5f02\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u672a\u6765\u5de5\u4f5c\u9002\u5e94\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u5e94\u5bf9\u65b9\u8a00\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.17858", "pdf": "https://arxiv.org/pdf/2509.17858", "abs": "https://arxiv.org/abs/2509.17858", "authors": ["Milan Straka"], "title": "CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution", "categories": ["cs.CL"], "comment": "Accepted to CODI-CRAC 2025", "summary": "We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on\nMultilingual Coreference Resolution. This fourth iteration of the shared task\nintroduces a new LLM track alongside the original unconstrained track, features\nreduced development and test sets to lower computational requirements, and\nincludes additional datasets. CorPipe 25 represents a complete reimplementation\nof our previous systems, migrating from TensorFlow to PyTorch. Our system\nsignificantly outperforms all other submissions in both the LLM and\nunconstrained tracks by a substantial margin of 8 percentage points. The source\ncode and trained models are publicly available at\nhttps://github.com/ufal/crac2025-corpipe.", "AI": {"tldr": "CorPipe 25\u662fCRAC 2025\u591a\u8bed\u8a00\u5171\u6307\u89e3\u6790\u5171\u4eab\u4efb\u52a1\u7684\u4f18\u80dc\u4f5c\u54c1\uff0c\u91c7\u7528PyTorch\u5b9e\u73b0\uff0c\u5728LLM\u548c\u975e\u7ea6\u675f\u8d5b\u9053\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u53c2\u4e0eCRAC 2025\u591a\u8bed\u8a00\u5171\u6307\u89e3\u6790\u5171\u4eab\u4efb\u52a1\uff0c\u5e76\u5728\u65b0\u5f15\u5165\u7684LLM\u8d5b\u9053\u548c\u539f\u59cb\u975e\u7ea6\u675f\u8d5b\u9053\u4e2d\u53d6\u5f97\u4f18\u5f02\u6210\u7ee9\u3002", "method": "CorPipe 25\u662f\u5bf9\u4e4b\u524d\u7cfb\u7edf\u7684\u5b8c\u5168\u91cd\u5199\uff0c\u4eceTensorFlow\u8fc1\u79fb\u5230PyTorch\u3002", "result": "CorPipe 25\u5728\u4e24\u4e2a\u8d5b\u9053\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u53c2\u8d5b\u4f5c\u54c1\uff0c\u6027\u80fd\u63d0\u5347\u4e868\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CorPipe 25\u5728LLM\u548c\u975e\u7ea6\u675f\u6027\u8d5b\u9053\u4e2d\u90fd\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u63d0\u4ea4\uff0c\u6027\u80fd\u63d0\u5347\u4e868\u4e2a\u767e\u5206\u70b9\u3002"}}
{"id": "2509.17859", "pdf": "https://arxiv.org/pdf/2509.17859", "abs": "https://arxiv.org/abs/2509.17859", "authors": ["Kai Schenck", "Ga\u0161per Begu\u0161"], "title": "Unsupervised Learning and Representation of Mandarin Tonal Categories by a Generative CNN", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper outlines the methodology for modeling tonal learning in fully\nunsupervised models of human language acquisition. Tonal patterns are among the\ncomputationally most complex learning objectives in language. We argue that a\nrealistic generative model of human language (ciwGAN) can learn to associate\nits categorical variables with Mandarin Chinese tonal categories without any\nlabeled data. All three trained models showed statistically significant\ndifferences in F0 across categorical variables. The model trained solely on\nmale tokens consistently encoded tone. Our results sug- gest that not only does\nthe model learn Mandarin tonal contrasts, but it learns a system that\ncorresponds to a stage of acquisition in human language learners. We also\noutline methodology for tracing tonal representations in internal convolutional\nlayers, which shows that linguistic tools can contribute to interpretability of\ndeep learning and can ultimately be used in neural experiments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u65e0\u76d1\u7763\u6a21\u578b\u5982\u4f55\u5b66\u4e60\u666e\u901a\u8bdd\u58f0\u8c03\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5230\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u9636\u6bb5\u76f8\u4f3c\u7684\u7cfb\u7edf\u3002", "motivation": "\u58f0\u8c03\u6a21\u5f0f\u662f\u8bed\u8a00\u4e2d\u8ba1\u7b97\u4e0a\u6700\u590d\u6742\u7684\u5b66\u4efb\u52a1\u4e4b\u4e00\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u65e0\u76d1\u7763\u6a21\u578b\u662f\u5426\u80fd\u5b66\u4e60\u58f0\u8c03\u6a21\u5f0f\uff0c\u5e76\u5c06\u5176\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u9636\u6bb5\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u65e0\u76d1\u7763\u6a21\u578b\u4e2d\u5efa\u6a21\u58f0\u8c03\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u4e86\u751f\u6210\u6a21\u578bciwGAN\u6765\u5173\u8054\u5176\u5206\u7c7b\u53d8\u91cf\u4e0e\u666e\u901a\u8bdd\u58f0\u8c03\u7c7b\u522b\u3002", "result": "\u6240\u6709\u4e09\u4e2a\u8bad\u7ec3\u6a21\u578b\u5728\u5206\u7c7b\u53d8\u91cf\u4e0a\u7684F0\u5747\u8868\u73b0\u51fa\u7edf\u8ba1\u663e\u8457\u5dee\u5f02\uff0c\u5176\u4e2d\u4ec5\u4f7f\u7528\u7537\u6027\u6837\u672c\u8bad\u7ec3\u7684\u6a21\u578b\u6301\u7eed\u7f16\u7801\u58f0\u8c03\u3002", "conclusion": "\u672c\u6587\u8868\u660e\uff0cciwGAN\u6a21\u578b\u4e0d\u4ec5\u80fd\u591f\u5b66\u4e60\u666e\u901a\u8bdd\u7684\u58f0\u8c03\u5bf9\u6bd4\uff0c\u8fd8\u80fd\u5b66\u4e60\u4e00\u4e2a\u5bf9\u5e94\u4e8e\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u8005\u53d1\u5c55\u9636\u6bb5\u7684\u7cfb\u7edf\u3002"}}
{"id": "2509.17879", "pdf": "https://arxiv.org/pdf/2509.17879", "abs": "https://arxiv.org/abs/2509.17879", "authors": ["Tu Nguyen", "Kevin Du", "Alexander Miserlis Hoyle", "Ryan Cotterell"], "title": "How Persuasive is Your Context?", "categories": ["cs.CL", "cs.AI"], "comment": "Long paper accepted at EMNLP 2025", "summary": "Two central capabilities of language models (LMs) are: (i) drawing on prior\nknowledge about entities, which allows them to answer queries such as \"What's\nthe official language of Austria?\", and (ii) adapting to new information\nprovided in context, e.g., \"Pretend the official language of Austria is\nTagalog.\", that is pre-pended to the question. In this article, we introduce\ntargeted persuasion score (TPS), designed to quantify how persuasive a given\ncontext is to an LM where persuasion is operationalized as the ability of the\ncontext to alter the LM's answer to the question. In contrast to evaluating\npersuasiveness only by inspecting the greedily decoded answer under the model,\nTPS provides a more fine-grained view of model behavior. Based on the\nWasserstein distance, TPS measures how much a context shifts a model's original\nanswer distribution toward a target distribution. Empirically, through a series\nof experiments, we show that TPS captures a more nuanced notion of\npersuasiveness than previously proposed metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u8bf4\u670d\u529b\u7684\u65b9\u6cd5\u2014\u2014\u76ee\u6807\u8bf4\u670d\u5206\u6570\uff08TPS\uff09\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8eWasserstein\u8ddd\u79bb\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u4e0a\u4e0b\u6587\u5bf9\u6a21\u578b\u56de\u7b54\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4ec5\u901a\u8fc7\u68c0\u67e5\u6a21\u578b\u8d2a\u5a6a\u89e3\u7801\u7684\u7b54\u6848\u6765\u8bc4\u4f30\u8bf4\u670d\u529b\uff0c\u800cTPS\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7ec6\u81f4\u7684\u6a21\u578b\u884c\u4e3a\u89c6\u56fe\u3002", "method": "\u5f15\u5165\u4e86\u76ee\u6807\u8bf4\u670d\u5206\u6570\uff08TPS\uff09\uff0c\u57fa\u4e8eWasserstein\u8ddd\u79bb\u6765\u8861\u91cf\u4e0a\u4e0b\u6587\u5bf9\u6a21\u578b\u539f\u59cb\u7b54\u6848\u5206\u5e03\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTPS\u6bd4\u4e4b\u524d\u63d0\u51fa\u7684\u6307\u6807\u66f4\u80fd\u6355\u6349\u5230\u8bf4\u670d\u529b\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "conclusion": "TPS\u80fd\u591f\u66f4\u7ec6\u81f4\u5730\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u4e0d\u540c\u4e0a\u4e0b\u6587\u65f6\u7684\u8bf4\u670d\u529b\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u66f4\u4e3a\u7cbe\u786e\u3002"}}
{"id": "2509.17912", "pdf": "https://arxiv.org/pdf/2509.17912", "abs": "https://arxiv.org/abs/2509.17912", "authors": ["Nevidu Jayatilleke", "Nisansa de Silva"], "title": "SiDiaC: Sinhala Diachronic Corpus", "categories": ["cs.CL"], "comment": "14 pages, 7 figures, 7 tables, Accepted paper at the 39th Pacific\n  Asia Conference on Language, Information and Computation (PACLIC 39)", "summary": "SiDiaC, the first comprehensive Sinhala Diachronic Corpus, covers a\nhistorical span from the 5th to the 20th century CE. SiDiaC comprises 58k words\nacross 46 literary works, annotated carefully based on the written date, after\nfiltering based on availability, authorship, copyright compliance, and data\nattribution. Texts from the National Library of Sri Lanka were digitised using\nGoogle Document AI OCR, followed by post-processing to correct formatting and\nmodernise the orthography. The construction of SiDiaC was informed by practices\nfrom other corpora, such as FarPaHC, particularly in syntactic annotation and\ntext normalisation strategies, due to the shared characteristics of\nlow-resourced language status. This corpus is categorised based on genres into\ntwo layers: primary and secondary. Primary categorisation is binary,\nclassifying each book into Non-Fiction or Fiction, while the secondary\ncategorisation is more specific, grouping texts under Religious, History,\nPoetry, Language, and Medical genres. Despite challenges including limited\naccess to rare texts and reliance on secondary date sources, SiDiaC serves as a\nfoundational resource for Sinhala NLP, significantly extending the resources\navailable for Sinhala, enabling diachronic studies in lexical change, neologism\ntracking, historical syntax, and corpus-based lexicography.", "AI": {"tldr": "SiDiaC is the first comprehensive Sinhala Diachronic Corpus, covering a historical span from the 5th to the 20th century CE. It comprises 58k words across 46 literary works, annotated based on the written date. The corpus is categorised into two layers, with primary categorisation being binary (Non-Fiction or Fiction) and secondary categorisation being more specific (Religious, History, Poetry, Language, and Medical genres). Despite challenges, SiDiaC serves as a foundational resource for Sinhala NLP.", "motivation": "The motivation behind SiDiaC is to create a comprehensive Sinhala Diachronic Corpus that covers a historical span from the 5th to the 20th century CE, providing a foundational resource for Sinhala NLP and enabling diachronic studies in various areas.", "method": "SiDiaC was constructed by carefully annotating 58k words across 46 literary works, based on the written date, after filtering based on availability, authorship, copyright compliance, and data attribution. Texts from the National Library of Sri Lanka were digitised using Google Document AI OCR, followed by post-processing to correct formatting and modernise the orthography. Practices from other corpora, such as FarPaHC, were used for syntactic annotation and text normalisation strategies.", "result": "SiDiaC is a comprehensive Sinhala Diachronic Corpus that comprises 58k words across 46 literary works, annotated carefully based on the written date. It is categorised into two layers: primary (Non-Fiction or Fiction) and secondary (Religious, History, Poetry, Language, and Medical genres).", "conclusion": "SiDiaC serves as a foundational resource for Sinhala NLP, significantly extending the resources available for Sinhala, enabling diachronic studies in lexical change, neologism tracking, historical syntax, and corpus-based lexicography."}}
{"id": "2509.17921", "pdf": "https://arxiv.org/pdf/2509.17921", "abs": "https://arxiv.org/abs/2509.17921", "authors": ["Zhenyun Deng", "Yulong Chen", "Andreas Vlachos"], "title": "Improving Zero-shot Sentence Decontextualisation with Content Selection and Planning", "categories": ["cs.CL"], "comment": "Accepted to EMLNP 2025 (Main Conference)", "summary": "Extracting individual sentences from a document as evidence or reasoning\nsteps is commonly done in many NLP tasks. However, extracted sentences often\nlack context necessary to make them understood, e.g., coreference and\nbackground information. To this end, we propose a content selection and\nplanning framework for zero-shot decontextualisation, which determines what\ncontent should be mentioned and in what order for a sentence to be understood\nout of context. Specifically, given a potentially ambiguous sentence and its\ncontext, we first segment it into basic semantically-independent units. We then\nidentify potentially ambiguous units from the given sentence, and extract\nrelevant units from the context based on their discourse relations. Finally, we\ngenerate a content plan to rewrite the sentence by enriching each ambiguous\nunit with its relevant units. Experimental results demonstrate that our\napproach is competitive for sentence decontextualisation, producing sentences\nthat exhibit better semantic integrity and discourse coherence, outperforming\nexisting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u53bb\u4e0a\u4e0b\u6587\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u786e\u5b9a\u53e5\u5b50\u5728\u65e0\u4e0a\u4e0b\u6587\u60c5\u51b5\u4e0b\u5e94\u63d0\u53ca\u7684\u5185\u5bb9\u548c\u987a\u5e8f\u3002\u901a\u8fc7\u5206\u5272\u53e5\u5b50\u4e3a\u8bed\u4e49\u72ec\u7acb\u5355\u5143\uff0c\u8bc6\u522b\u53ef\u80fd\u6709\u6b67\u4e49\u7684\u5355\u5143\uff0c\u5e76\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63d0\u53d6\u76f8\u5173\u5355\u5143\uff0c\u6700\u7ec8\u751f\u6210\u5185\u5bb9\u8ba1\u5212\u4ee5\u91cd\u5199\u53e5\u5b50\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u8bdd\u8bed\u8fde\u8d2f\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u53d6\u6587\u6863\u4e2d\u7684\u5355\u72ec\u53e5\u5b50\u4f5c\u4e3a\u8bc1\u636e\u6216\u63a8\u7406\u6b65\u9aa4\u5728\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5f88\u5e38\u89c1\u3002\u7136\u800c\uff0c\u63d0\u53d6\u7684\u53e5\u5b50\u5f80\u5f80\u7f3a\u4e4f\u5fc5\u8981\u7684\u4e0a\u4e0b\u6587\u4ee5\u4f7f\u5176\u88ab\u7406\u89e3\uff0c\u4f8b\u5982\u6307\u4ee3\u548c\u80cc\u666f\u4fe1\u606f\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5185\u5bb9\u9009\u62e9\u548c\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u53bb\u4e0a\u4e0b\u6587\u5316\uff0c\u8be5\u6846\u67b6\u786e\u5b9a\u4e86\u4ec0\u4e48\u5185\u5bb9\u5e94\u8be5\u88ab\u63d0\u53ca\u4ee5\u53ca\u4ee5\u4f55\u79cd\u987a\u5e8f\u4f7f\u53e5\u5b50\u5728\u6ca1\u6709\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u88ab\u7406\u89e3\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u53ef\u80fd\u6709\u6b67\u4e49\u7684\u53e5\u5b50\u53ca\u5176\u4e0a\u4e0b\u6587\uff0c\u6211\u4eec\u9996\u5148\u5c06\u5176\u5206\u5272\u6210\u57fa\u672c\u7684\u8bed\u4e49\u72ec\u7acb\u5355\u5143\uff0c\u7136\u540e\u4ece\u7ed9\u5b9a\u7684\u53e5\u5b50\u4e2d\u8bc6\u522b\u51fa\u53ef\u80fd\u6709\u6b67\u4e49\u7684\u5355\u5143\uff0c\u5e76\u6839\u636e\u5b83\u4eec\u7684\u8bba\u8ff0\u5173\u7cfb\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63d0\u53d6\u76f8\u5173\u7684\u5355\u5143\u3002\u6700\u540e\uff0c\u6211\u4eec\u751f\u6210\u4e00\u4e2a\u5185\u5bb9\u8ba1\u5212\u6765\u91cd\u5199\u53e5\u5b50\uff0c\u901a\u8fc7\u4e3a\u5176\u76f8\u5173\u5355\u5143\u8fdb\u884c\u4e30\u5bcc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u53e5\u5b50\u53bb\u4e0a\u4e0b\u6587\u5316\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u751f\u6210\u7684\u53e5\u5b50\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u8bdd\u8bed\u8fde\u8d2f\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u53e5\u5b50\u53bb\u4e0a\u4e0b\u6587\u5316\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u751f\u6210\u7684\u53e5\u5b50\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u8bdd\u8bed\u8fde\u8d2f\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.17930", "pdf": "https://arxiv.org/pdf/2509.17930", "abs": "https://arxiv.org/abs/2509.17930", "authors": ["Yiwen Guan", "Jacob Whitehill"], "title": "Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual translation faces challenges of computational redundancy and\nlimited accuracy for low-resource languages, especially in speech translation.\nTo address this, we propose a novel hierarchical Transformer Encoder Tree (TET)\ncombined with non-autoregressive encoder-only models trained with Connectionist\nTemporal Classification for multilingual translation. By sharing intermediate\nrepresentations among linguistically similar target languages, TET can improve\naccuracy on low-resource languages, reduce computational redundancy, and allow\ngenerating all target languages in a single forward pass, thus eliminating\nsequential bottlenecks and improving parallelism. For speech translation,\ncombining TET with a non-autoregressive speech recognition backbone (wav2vec2)\nshows promising results in terms of translation quality compared to\nautoregressive systems while being 7-14 times faster.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42Transformer\u7f16\u7801\u5668\u6811\uff08TET\uff09\uff0c\u7ed3\u5408\u4e86\u975e\u81ea\u56de\u5f52\u7f16\u7801\u5668-only\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u8bed\u8a00\u7ffb\u8bd1\u3002TET\u901a\u8fc7\u5728\u8bed\u8a00\u76f8\u4f3c\u7684\u76ee\u6807\u8bed\u8a00\u4e4b\u95f4\u5171\u4eab\u4e2d\u95f4\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u5197\u4f59\uff0c\u5e76\u5141\u8bb8\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u751f\u6210\u6240\u6709\u76ee\u6807\u8bed\u8a00\u3002\u5bf9\u4e8e\u8bed\u97f3\u7ffb\u8bd1\uff0cTET\u4e0e\u975e\u81ea\u56de\u5f52\u8bed\u97f3\u8bc6\u522b\u57fa\u7840\u67b6\u6784\uff08wav2vec2\uff09\u7ed3\u5408\u4f7f\u7528\uff0c\u5728\u7ffb\u8bd1\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u9ad8\u4e867-14\u500d\u3002", "motivation": "\u591a\u8bed\u8a00\u7ffb\u8bd1\u9762\u4e34\u8ba1\u7b97\u5197\u4f59\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u6709\u9650\u51c6\u786e\u6027\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42Transformer\u7f16\u7801\u5668\u6811\uff08TET\uff09\uff0c\u7ed3\u5408\u4e86\u4f7f\u7528Connectionist Temporal Classification\u8fdb\u884c\u591a\u8bed\u8a00\u7ffb\u8bd1\u7684\u975e\u81ea\u56de\u5f52\u7f16\u7801\u5668-only\u6a21\u578b\u3002\u901a\u8fc7\u5728\u8bed\u8a00\u76f8\u4f3c\u7684\u76ee\u6807\u8bed\u8a00\u4e4b\u95f4\u5171\u4eab\u4e2d\u95f4\u8868\u793a\uff0cTET\u53ef\u4ee5\u63d0\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\uff0c\u5e76\u5141\u8bb8\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u751f\u6210\u6240\u6709\u76ee\u6807\u8bed\u8a00\u3002\u5bf9\u4e8e\u8bed\u97f3\u7ffb\u8bd1\uff0c\u5c06TET\u4e0e\u975e\u81ea\u56de\u5f52\u8bed\u97f3\u8bc6\u522b\u57fa\u7840\u67b6\u6784\uff08wav2vec2\uff09\u7ed3\u5408\u4f7f\u7528\u3002", "result": "TET\u53ef\u4ee5\u63d0\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\uff0c\u5e76\u5141\u8bb8\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u751f\u6210\u6240\u6709\u76ee\u6807\u8bed\u8a00\uff0c\u4ece\u800c\u6d88\u9664\u987a\u5e8f\u74f6\u9888\u5e76\u63d0\u9ad8\u5e76\u884c\u6027\u3002\u5bf9\u4e8e\u8bed\u97f3\u7ffb\u8bd1\uff0c\u7ed3\u5408TET\u548c\u975e\u81ea\u56de\u5f52\u8bed\u97f3\u8bc6\u522b\u57fa\u7840\u67b6\u6784\uff08wav2vec2\uff09\u5728\u7ffb\u8bd1\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u9ad8\u4e867-14\u500d\u3002", "conclusion": "TET\u53ef\u4ee5\u63d0\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\uff0c\u5e76\u5141\u8bb8\u5728\u5355\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u751f\u6210\u6240\u6709\u76ee\u6807\u8bed\u8a00\uff0c\u4ece\u800c\u6d88\u9664\u987a\u5e8f\u74f6\u9888\u5e76\u63d0\u9ad8\u5e76\u884c\u6027\u3002\u5bf9\u4e8e\u8bed\u97f3\u7ffb\u8bd1\uff0c\u7ed3\u5408TET\u548c\u975e\u81ea\u56de\u5f52\u8bed\u97f3\u8bc6\u522b\u57fa\u7840\u67b6\u6784\uff08wav2vec2\uff09\u5728\u7ffb\u8bd1\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u9ad8\u4e867-14\u500d\u3002"}}
{"id": "2509.17932", "pdf": "https://arxiv.org/pdf/2509.17932", "abs": "https://arxiv.org/abs/2509.17932", "authors": ["Runheng Liu", "Heyan Huang", "Xingchen Xiao", "Zhijing Wu"], "title": "Training-free Truthfulness Detection via Value Vectors in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models often generate factually incorrect outputs, motivating\nefforts to detect the truthfulness of their content. Most existing approaches\nrely on training probes over internal activations, but these methods suffer\nfrom scalability and generalization issues. A recent training-free method,\nNoVo, addresses this challenge by exploiting statistical patterns from the\nmodel itself. However, it focuses exclusively on attention mechanisms,\npotentially overlooking the MLP module-a core component of Transformer models\nknown to support factual recall. In this paper, we show that certain value\nvectors within MLP modules exhibit truthfulness-related statistical patterns.\nBuilding on this insight, we propose TruthV, a simple and interpretable\ntraining-free method that detects content truthfulness by leveraging these\nvalue vectors. On the NoVo benchmark, TruthV significantly outperforms both\nNoVo and log-likelihood baselines, demonstrating that MLP modules-despite being\nneglected in prior training-free efforts-encode rich and useful signals for\ntruthfulness detection. These findings offer new insights into how truthfulness\nis internally represented in LLMs and motivate further research on scalable and\ninterpretable truthfulness detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3-free \u65b9\u6cd5TruthV\uff0c\u901a\u8fc7\u5229\u7528MLP\u6a21\u5757\u4e2d\u7684\u503c\u5411\u91cf\u6765\u68c0\u6d4b\u5185\u5bb9\u7684\u771f\u5b9e\u6027\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u800cMLP\u6a21\u5757\u5728\u4e8b\u5b9e\u56de\u5fc6\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u5ffd\u7565\u4e86\u8fd9\u4e00\u70b9\u3002", "method": "TruthV\uff0c\u4e00\u79cd\u7b80\u5355\u4e14\u53ef\u89e3\u91ca\u7684\u8bad\u7ec3-free \u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528MLP\u6a21\u5757\u4e2d\u7684\u503c\u5411\u91cf\u6765\u68c0\u6d4b\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u3002", "result": "TruthV\u5728NoVo\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8eNoVo\u548clog-likelihood\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3aLLM\u5185\u90e8\u5982\u4f55\u8868\u793a\u771f\u5b9e\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u6fc0\u53d1\u4e86\u5bf9\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u771f\u5b9e\u6027\u68c0\u6d4b\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.17938", "pdf": "https://arxiv.org/pdf/2509.17938", "abs": "https://arxiv.org/abs/2509.17938", "authors": ["Satyapriya Krishna", "Andy Zou", "Rahul Gupta", "Eliot Krzysztof Jones", "Nick Winter", "Dan Hendrycks", "J. Zico Kolter", "Matt Fredrikson", "Spyros Matsoukas"], "title": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "Preprint", "summary": "The safety and alignment of Large Language Models (LLMs) are critical for\ntheir responsible deployment. Current evaluation methods predominantly focus on\nidentifying and preventing overtly harmful outputs. However, they often fail to\naddress a more insidious failure mode: models that produce benign-appearing\noutputs while operating on malicious or deceptive internal reasoning. This\nvulnerability, often triggered by sophisticated system prompt injections,\nallows models to bypass conventional safety filters, posing a significant,\nunderexplored risk. To address this gap, we introduce the Deceptive Reasoning\nExposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy\nbetween a model's internal reasoning process and its final output. D-REX was\nconstructed through a competitive red-teaming exercise where participants\ncrafted adversarial system prompts to induce such deceptive behaviors. Each\nsample in D-REX contains the adversarial system prompt, an end-user's test\nquery, the model's seemingly innocuous response, and, crucially, the model's\ninternal chain-of-thought, which reveals the underlying malicious intent. Our\nbenchmark facilitates a new, essential evaluation task: the detection of\ndeceptive alignment. We demonstrate that D-REX presents a significant challenge\nfor existing models and safety mechanisms, highlighting the urgent need for new\ntechniques that scrutinize the internal processes of LLMs, not just their final\noutputs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aD-REX\u7684\u65b0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u8f93\u51fa\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u6307\u51fa\u9700\u8981\u65b0\u7684\u6280\u672f\u6765\u5ba1\u67e5\u8fd9\u4e9b\u6a21\u578b\u7684\u5185\u90e8\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bc6\u522b\u548c\u9632\u6b62\u660e\u663e\u6709\u5bb3\u7684\u8f93\u51fa\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u6a21\u578b\u5728\u6076\u610f\u6216\u6b3a\u9a97\u6027\u5185\u90e8\u63a8\u7406\u4e0b\u4ea7\u751f\u770b\u4f3c\u65e0\u5bb3\u8f93\u51fa\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86Deceptive Reasoning Exposure Suite (D-REX)\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u7ade\u4e89\u6027\u7684\u7ea2\u961f\u6f14\u7ec3\u6784\u5efa\uff0c\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u8f93\u51fa\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "D-REX\u5bf9\u73b0\u6709\u6a21\u578b\u548c\u5b89\u5168\u673a\u5236\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\uff0c\u7a81\u663e\u4e86\u9700\u8981\u65b0\u7684\u6280\u672f\u6765\u5ba1\u67e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8fc7\u7a0b\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u9700\u8981\u65b0\u7684\u6280\u672f\u6765\u5ba1\u67e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8fc7\u7a0b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b83\u4eec\u7684\u6700\u7ec8\u8f93\u51fa\u3002"}}
{"id": "2509.17946", "pdf": "https://arxiv.org/pdf/2509.17946", "abs": "https://arxiv.org/abs/2509.17946", "authors": ["Mian Zhong", "Pristina Wang", "Anjalie Field"], "title": "HICode: Hierarchical Inductive Coding with LLMs", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Long paper accepted at EMNLP 2025 main conference, 19 pages, 8\n  figures", "summary": "Despite numerous applications for fine-grained corpus analysis, researchers\ncontinue to rely on manual labeling, which does not scale, or statistical tools\nlike topic modeling, which are difficult to control. We propose that LLMs have\nthe potential to scale the nuanced analyses that researchers typically conduct\nmanually to large text corpora. To this effect, inspired by qualitative\nresearch methods, we develop HICode, a two-part pipeline that first inductively\ngenerates labels directly from analysis data and then hierarchically clusters\nthem to surface emergent themes. We validate this approach across three diverse\ndatasets by measuring alignment with human-constructed themes and demonstrating\nits robustness through automated and human evaluations. Finally, we conduct a\ncase study of litigation documents related to the ongoing opioid crisis in the\nU.S., revealing aggressive marketing strategies employed by pharmaceutical\ncompanies and demonstrating HICode's potential for facilitating nuanced\nanalyses in large-scale data.", "AI": {"tldr": "HICode is a two-part pipeline that uses LLMs to generate labels and cluster them to surface themes, validated across datasets and applied to litigation documents to reveal marketing strategies.", "motivation": "Researchers continue to rely on manual labeling or statistical tools like topic modeling, which are difficult to control. LLMs have the potential to scale nuanced analyses to large text corpora.", "method": "HICode is a two-part pipeline that first inductively generates labels directly from analysis data and then hierarchically clusters them to surface emergent themes.", "result": "HICode was validated across three diverse datasets, showing alignment with human-constructed themes and robustness through automated and human evaluations. It revealed aggressive marketing strategies in litigation documents related to the opioid crisis.", "conclusion": "HICode has the potential to facilitate nuanced analyses in large-scale data, as demonstrated by its application to litigation documents related to the opioid crisis."}}
{"id": "2509.17950", "pdf": "https://arxiv.org/pdf/2509.17950", "abs": "https://arxiv.org/abs/2509.17950", "authors": ["Bradley Hauer", "Colin Choi", "Abram Hindle", "Scott Smallwood", "Grzegorz Kondrak"], "title": "Dorabella Cipher as Musical Inspiration", "categories": ["cs.CL"], "comment": "Published in Proceedings of the Workshop on Speech and Music\n  Processing 2021", "summary": "The Dorabella cipher is an encrypted note written by English composer Edward\nElgar, which has defied decipherment attempts for more than a century. While\nmost proposed solutions are English texts, we investigate the hypothesis that\nDorabella represents enciphered music. We weigh the evidence for and against\nthe hypothesis, devise a simplified music notation, and attempt to reconstruct\na melody from the cipher. Our tools are n-gram models of music which we\nvalidate on existing music corpora enciphered using monoalphabetic\nsubstitution. By applying our methods to Dorabella, we produce a decipherment\nwith musical qualities, which is then transformed via artful composition into a\nlistenable melody. Far from arguing that the end result represents the only\ntrue solution, we instead frame the process of decipherment as part of the\ncomposition process.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Dorabella\u5bc6\u7801\u662f\u5426\u4ee3\u8868\u52a0\u5bc6\u97f3\u4e50\u7684\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u97f3\u4e50n-gram\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\uff0c\u6700\u7ec8\u751f\u6210\u4e86\u4e00\u4e2a\u5177\u6709\u97f3\u4e50\u7279\u6027\u7684\u89e3\u5bc6\u7ed3\u679c\u3002", "motivation": "Dorabella\u5bc6\u7801\u662f\u4e00\u4e2a\u88ab\u52a0\u5bc6\u7684\u7b14\u8bb0\uff0c\u5df2\u7ecf\u56f0\u6270\u4e86\u8d85\u8fc7\u4e00\u4e2a\u4e16\u7eaa\u7684\u89e3\u5bc6\u5c1d\u8bd5\u3002\u5927\u591a\u6570\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u90fd\u662f\u82f1\u8bed\u6587\u672c\uff0c\u6211\u4eec\u7814\u7a76\u4e86Dorabella\u53ef\u80fd\u4ee3\u8868\u52a0\u5bc6\u97f3\u4e50\u7684\u5047\u8bbe\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4e86\u97f3\u4e50\u7684n-gram\u6a21\u578b\uff0c\u5e76\u5728\u73b0\u6709\u4f7f\u7528\u5355\u5b57\u6bcd\u66ff\u6362\u52a0\u5bc6\u7684\u97f3\u4e50\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u6211\u4eec\u4ea7\u751f\u4e86\u5177\u6709\u97f3\u4e50\u7279\u6027\u7684\u89e3\u5bc6\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u827a\u672f\u521b\u4f5c\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u542c\u7684\u65cb\u5f8b\u3002", "conclusion": "\u6211\u4eec\u901a\u8fc7\u5e94\u7528\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u4ea7\u751f\u4e86\u5177\u6709\u97f3\u4e50\u7279\u6027\u7684\u89e3\u5bc6\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u827a\u672f\u521b\u4f5c\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u542c\u7684\u65cb\u5f8b\u3002\u6211\u4eec\u4e0d\u8ba4\u4e3a\u6700\u7ec8\u7ed3\u679c\u4ee3\u8868\u552f\u4e00\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u662f\u5c06\u89e3\u5bc6\u8fc7\u7a0b\u89c6\u4e3a\u521b\u4f5c\u8fc7\u7a0b\u7684\u4e00\u90e8\u5206\u3002"}}
{"id": "2509.17961", "pdf": "https://arxiv.org/pdf/2509.17961", "abs": "https://arxiv.org/abs/2509.17961", "authors": ["Li Siyan", "Zhen Xu", "Vethavikashini Chithrra Raghuram", "Xuanming Zhang", "Renzhe Yu", "Zhou Yu"], "title": "Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments", "categories": ["cs.CL"], "comment": "Accepted in EMNLP 2025 Findings", "summary": "Asynchronous learning environments (ALEs) are widely adopted for formal and\ninformal learning, but timely and personalized support is often limited. In\nthis context, Virtual Teaching Assistants (VTAs) can potentially reduce the\nworkload of instructors, but rigorous and pedagogically sound evaluation is\nessential. Existing assessments often rely on surface-level metrics and lack\nsufficient grounding in educational theories, making it difficult to\nmeaningfully compare the pedagogical effectiveness of different VTA systems. To\nbridge this gap, we propose an evaluation framework rooted in learning sciences\nand tailored to asynchronous forum discussions, a common VTA deployment context\nin ALE. We construct classifiers using expert annotations of VTA responses on a\ndiverse set of forum posts. We evaluate the effectiveness of our classifiers,\nidentifying approaches that improve accuracy as well as challenges that hinder\ngeneralization. Our work establishes a foundation for theory-driven evaluation\nof VTA systems, paving the way for more pedagogically effective AI in\neducation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u79d1\u5b66\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u865a\u62df\u6559\u5b66\u52a9\u7406\u5728\u5f02\u6b65\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u6559\u5b66\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u901a\u5e38\u4f9d\u8d56\u4e8e\u8868\u9762\u6307\u6807\uff0c\u7f3a\u4e4f\u8db3\u591f\u7684\u6559\u80b2\u7406\u8bba\u57fa\u7840\uff0c\u4f7f\u5f97\u4e0d\u540cVTA\u7cfb\u7edf\u7684\u6559\u5b66\u6548\u679c\u96be\u4ee5\u6709\u610f\u4e49\u5730\u6bd4\u8f83\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4e13\u5bb6\u5bf9VTA\u54cd\u5e94\u7684\u6ce8\u91ca\u6784\u5efa\u5206\u7c7b\u5668\uff0c\u5e76\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u4ee5\u786e\u5b9a\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u65b9\u6cd5\u4ee5\u53ca\u963b\u788d\u6cdb\u5316\u7684\u6311\u6218\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u79d1\u5b66\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5f02\u6b65\u8bba\u575b\u8ba8\u8bba\u8fd9\u4e00\u5e38\u89c1\u7684VTA\u90e8\u7f72\u73af\u5883\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u7406\u8bba\u7684VTA\u7cfb\u7edf\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u6559\u80b2\u4e2d\u7684\u66f4\u6709\u6548\u7684AI\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.17991", "pdf": "https://arxiv.org/pdf/2509.17991", "abs": "https://arxiv.org/abs/2509.17991", "authors": ["Aakash Kumar Agarwal", "Saprativa Bhattacharjee", "Mauli Rastogi", "Jemima S. Jacob", "Biplab Banerjee", "Rashmi Gupta", "Pushpak Bhattacharyya"], "title": "ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Almost 50% depression patients face the risk of going into relapse. The risk\nincreases to 80% after the second episode of depression. Although, depression\ndetection from social media has attained considerable attention, depression\nrelapse detection has remained largely unexplored due to the lack of curated\ndatasets and the difficulty of distinguishing relapse and non-relapse users. In\nthis work, we present ReDepress, the first clinically validated social media\ndataset focused on relapse, comprising 204 Reddit users annotated by mental\nhealth professionals. Unlike prior approaches, our framework draws on cognitive\ntheories of depression, incorporating constructs such as attention bias,\ninterpretation bias, memory bias and rumination into both annotation and\nmodeling. Through statistical analyses and machine learning experiments, we\ndemonstrate that cognitive markers significantly differentiate relapse and\nnon-relapse groups, and that models enriched with these features achieve\ncompetitive performance, with transformer-based temporal models attaining an F1\nof 0.86. Our findings validate psychological theories in real-world textual\ndata and underscore the potential of cognitive-informed computational methods\nfor early relapse detection, paving the way for scalable, low-cost\ninterventions in mental healthcare.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ReDepress\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7ecf\u8fc7\u4e34\u5e8a\u9a8c\u8bc1\u7684\u4e13\u6ce8\u4e8e\u590d\u53d1\u7684\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\uff0c\u5305\u542b204\u540d\u7531\u5fc3\u7406\u5065\u5eb7\u4e13\u4e1a\u4eba\u5458\u6807\u6ce8\u7684Reddit\u7528\u6237\u3002\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8ba4\u77e5\u6807\u8bb0\u53ef\u4ee5\u663e\u8457\u533a\u5206\u590d\u53d1\u7ec4\u548c\u975e\u590d\u53d1\u7ec4\uff0c\u5e76\u4e14\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u7684\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u4ece\u793e\u4ea4\u5a92\u4f53\u4e2d\u68c0\u6d4b\u6291\u90c1\u75c7\u5df2\u7ecf\u5f15\u8d77\u4e86\u76f8\u5f53\u591a\u7684\u5173\u6ce8\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u7cbe\u5fc3\u6574\u7406\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u96be\u4ee5\u533a\u5206\u590d\u53d1\u548c\u975e\u590d\u53d1\u7528\u6237\uff0c\u6291\u90c1\u75c7\u590d\u53d1\u68c0\u6d4b\u4ecd\u7136\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u7684\u6846\u67b6\u501f\u9274\u4e86\u6291\u90c1\u75c7\u7684\u8ba4\u77e5\u7406\u8bba\uff0c\u7ed3\u5408\u4e86\u6ce8\u610f\u504f\u5dee\u3001\u89e3\u91ca\u504f\u5dee\u3001\u8bb0\u5fc6\u504f\u5dee\u548c\u53cd\u520d\u7b49\u6784\u5ff5\uff0c\u5728\u6ce8\u91ca\u548c\u5efa\u6a21\u4e2d\u90fd\u878d\u5165\u4e86\u8fd9\u4e9b\u56e0\u7d20\u3002\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8ba4\u77e5\u6807\u8bb0\u53ef\u4ee5\u663e\u8457\u533a\u5206\u590d\u53d1\u7ec4\u548c\u975e\u590d\u53d1\u7ec4\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u8ba4\u77e5\u7279\u5f81\u7684\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff0c\u5176\u4e2d\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u65f6\u5e8f\u6a21\u578b\u8fbe\u5230\u4e860.86\u7684F1\u5206\u6570\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u9a8c\u8bc1\u4e86\u5fc3\u7406\u7406\u8bba\u5728\u73b0\u5b9e\u4e16\u754c\u6587\u672c\u6570\u636e\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u57fa\u4e8e\u8ba4\u77e5\u7684\u8ba1\u7b97\u65b9\u6cd5\u5728\u65e9\u671f\u590d\u53d1\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5fc3\u7406\u5065\u5eb7\u4fdd\u5065\u7684\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u5e72\u9884\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.17995", "pdf": "https://arxiv.org/pdf/2509.17995", "abs": "https://arxiv.org/abs/2509.17995", "authors": ["Yefan Zhou", "Austin Xu", "Yilun Zhou", "Janvijay Singh", "Jiang Gui", "Shafiq Joty"], "title": "Variation in Verification: Understanding Verification Dynamics in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances have shown that scaling test-time computation enables large\nlanguage models (LLMs) to solve increasingly complex problems across diverse\ndomains. One effective paradigm for test-time scaling (TTS) involves LLM\ngenerators producing multiple solution candidates, with LLM verifiers assessing\nthe correctness of these candidates without reference answers. In this paper,\nwe study generative verifiers, which perform verification by generating\nchain-of-thought (CoT) reasoning followed by a binary verdict. We\nsystematically analyze verification dynamics across three dimensions - problem\ndifficulty, generator capability, and verifier generation capability - with\nempirical studies on 12 benchmarks across mathematical reasoning, knowledge,\nand natural language reasoning tasks using 14 open-source models (2B to 72B\nparameter range) and GPT-4o. Our experiments reveal three key findings about\nverification effectiveness: (1) Easy problems allow verifiers to more reliably\ncertify correct responses; (2) Weak generators produce errors that are easier\nto detect than strong generators; (3) Verification ability is generally\ncorrelated with the verifier's own problem-solving capability, but this\nrelationship varies with problem difficulty. These findings reveal\nopportunities to optimize basic verification strategies in TTS applications.\nFirst, given the same verifier, some weak generators can nearly match stronger\nones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B\nperformance gap shrinks by 75.5%). Second, we identify cases where strong\nverifiers offer limited advantage over weak ones, as both fail to provide\nmeaningful verification gains, suggesting that verifier scaling alone cannot\novercome fundamental verification challenges.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0f\u9a8c\u8bc1\u5668\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u7684\u4f5c\u7528\uff0c\u5206\u6790\u4e86\u95ee\u9898\u96be\u5ea6\u3001\u751f\u6210\u5668\u80fd\u529b\u548c\u9a8c\u8bc1\u5668\u80fd\u529b\u5bf9\u9a8c\u8bc1\u6548\u679c\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7b80\u5355\u95ee\u9898\u66f4\u5bb9\u6613\u9a8c\u8bc1\uff0c\u5f31\u751f\u6210\u5668\u7684\u9519\u8bef\u66f4\u5bb9\u6613\u88ab\u68c0\u6d4b\uff0c\u4e14\u9a8c\u8bc1\u80fd\u529b\u901a\u5e38\u4e0e\u9a8c\u8bc1\u5668\u81ea\u8eab\u80fd\u529b\u76f8\u5173\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u4f18\u5316TTS\u5e94\u7528\u4e2d\u7684\u9a8c\u8bc1\u7b56\u7565\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76\u751f\u6210\u5f0f\u9a8c\u8bc1\u5668\uff0c\u5b83\u4eec\u901a\u8fc7\u751f\u6210\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u5e76\u7ed9\u51fa\u4e8c\u8fdb\u5236\u5224\u65ad\u6765\u8fdb\u884c\u9a8c\u8bc1\u3002\u901a\u8fc7\u5206\u6790\u9a8c\u8bc1\u52a8\u6001\uff0c\u5e0c\u671b\u627e\u5230\u4f18\u5316\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\u5e94\u7528\u4e2d\u9a8c\u8bc1\u7b56\u7565\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u4e09\u4e2a\u7ef4\u5ea6\u7684\u9a8c\u8bc1\u52a8\u6001\u2014\u2014\u95ee\u9898\u96be\u5ea6\u3001\u751f\u6210\u5668\u80fd\u529b\u4ee5\u53ca\u9a8c\u8bc1\u5668\u751f\u6210\u80fd\u529b\uff0c\u5e76\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u6db5\u76d6\u4e86\u6570\u5b66\u63a8\u7406\u3001\u77e5\u8bc6\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\uff0c\u4f7f\u7528\u4e8614\u4e2a\u5f00\u6e90\u6a21\u578b\uff08\u53c2\u6570\u8303\u56f4\u4e3a2B\u523072B\uff09\u548cGPT-4o\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u5173\u4e8e\u9a8c\u8bc1\u6709\u6548\u6027\u7684\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\uff1a(1) \u7b80\u5355\u7684\u95ee\u9898\u5141\u8bb8\u9a8c\u8bc1\u5668\u66f4\u53ef\u9760\u5730\u8ba4\u8bc1\u6b63\u786e\u54cd\u5e94\uff1b(2) \u5f31\u751f\u6210\u5668\u4ea7\u751f\u7684\u9519\u8bef\u6bd4\u5f3a\u751f\u6210\u5668\u66f4\u5bb9\u6613\u68c0\u6d4b\uff1b(3) \u9a8c\u8bc1\u80fd\u529b\u901a\u5e38\u4e0e\u9a8c\u8bc1\u5668\u81ea\u8eab\u7684\u89e3\u51b3\u95ee\u9898\u80fd\u529b\u76f8\u5173\uff0c\u4f46\u8fd9\u79cd\u5173\u7cfb\u4f1a\u968f\u7740\u95ee\u9898\u96be\u5ea6\u800c\u53d8\u5316\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u4f18\u5316TTS\u5e94\u7528\u4e2d\u57fa\u672c\u9a8c\u8bc1\u7b56\u7565\u7684\u673a\u4f1a\u3002\u9996\u5148\uff0c\u7ed9\u5b9a\u76f8\u540c\u7684\u9a8c\u8bc1\u5668\uff0c\u4e00\u4e9b\u5f31\u751f\u6210\u5668\u53ef\u4ee5\u5728\u540e\u9a8c\u8bc1TTS\u6027\u80fd\u4e0a\u51e0\u4e4e\u4e0e\u66f4\u5f3a\u7684\u751f\u6210\u5668\u76f8\u5ab2\u7f8e\u3002\u5176\u6b21\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u5f3a\u9a8c\u8bc1\u5668\u76f8\u5bf9\u4e8e\u5f31\u9a8c\u8bc1\u5668\u4f18\u52bf\u6709\u9650\u7684\u60c5\u51b5\uff0c\u56e0\u4e3a\u4e24\u8005\u90fd\u65e0\u6cd5\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u9a8c\u8bc1\u6536\u76ca\uff0c\u8fd9\u8868\u660e\u4ec5\u901a\u8fc7\u9a8c\u8bc1\u5668\u6269\u5c55\u65e0\u6cd5\u514b\u670d\u6839\u672c\u7684\u9a8c\u8bc1\u6311\u6218\u3002"}}
{"id": "2509.18004", "pdf": "https://arxiv.org/pdf/2509.18004", "abs": "https://arxiv.org/abs/2509.18004", "authors": ["Yuhang Dai", "Ziyu Zhang", "Shuai Wang", "Longhao Li", "Zhao Guo", "Tianlun Zuo", "Shuiyuan Wang", "Hongfei Xue", "Chengyou Wang", "Qing Wang", "Xin Xu", "Hui Bu", "Jie Li", "Jian Kang", "Binbin Zhang", "Lei Xie"], "title": "WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation for Dialectal Speech Processing", "categories": ["cs.CL", "cs.SD"], "comment": "4 pages, 5 figures, 4 tables", "summary": "The scarcity of large-scale, open-source data for dialects severely hinders\nprogress in speech technology, a challenge particularly acute for the widely\nspoken Sichuanese dialects of Chinese. To address this critical gap, we\nintroduce WenetSpeech-Chuan, a 10,000-hour, richly annotated corpus constructed\nusing our novel Chuan-Pipeline, a complete data processing framework for\ndialectal speech. To facilitate rigorous evaluation and demonstrate the\ncorpus's effectiveness, we also release high-quality ASR and TTS benchmarks,\nWenetSpeech-Chuan-Eval, with manually verified transcriptions. Experiments show\nthat models trained on WenetSpeech-Chuan achieve state-of-the-art performance\namong open-source systems and demonstrate results comparable to commercial\nservices. As the largest open-source corpus for Sichuanese dialects,\nWenetSpeech-Chuan not only lowers the barrier to research in dialectal speech\nprocessing but also plays a crucial role in promoting AI equity and mitigating\nbias in speech technologies. The corpus, benchmarks, models, and receipts are\npublicly available on our project page.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86 WenetSpeech-Chuan\uff0c\u4e00\u4e2a\u7528\u4e8e\u56db\u5ddd\u8bdd\u7684\u5927\u578b\u5f00\u6e90\u8bed\u6599\u5e93\uff0c\u4ee5\u53ca\u76f8\u5173\u7684 ASR \u548c TTS \u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bed\u97f3\u6280\u672f\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u5f00\u6e90\u7684\u65b9\u8a00\u6570\u636e\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u8bed\u97f3\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u56db\u5ddd\u8bdd\u800c\u8a00\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u5dee\u8ddd\uff0c\u672c\u6587\u63d0\u51fa\u4e86 WenetSpeech-Chuan \u8bed\u6599\u5e93\u3002", "method": "\u5f15\u5165\u4e86 WenetSpeech-Chuan\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528 novel Chuan-Pipeline \u6784\u5efa\u7684 10,000 \u5c0f\u65f6\u4e30\u5bcc\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u8be5\u7ba1\u9053\u662f\u4e00\u4e2a\u5b8c\u6574\u7684\u65b9\u8a00\u8bed\u97f3\u6570\u636e\u5904\u7406\u6846\u67b6\u3002\u8fd8\u53d1\u5e03\u4e86\u9ad8\u8d28\u91cf\u7684 ASR \u548c TTS \u57fa\u51c6\u6d4b\u8bd5 WenetSpeech-Chuan-Eval\uff0c\u5177\u6709\u624b\u52a8\u9a8c\u8bc1\u7684\u8f6c\u5f55\u6587\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e WenetSpeech-Chuan \u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5f00\u6e90\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u7ed3\u679c\u4e0e\u5546\u4e1a\u670d\u52a1\u76f8\u5f53\u3002", "conclusion": "WenetSpeech-Chuan \u662f\u76ee\u524d\u6700\u5927\u7684\u5f00\u6e90 Sichuanese \u65b9\u8a00\u8bed\u6599\u5e93\uff0c\u4e0d\u4ec5\u964d\u4f4e\u4e86\u65b9\u8a00\u8bed\u97f3\u5904\u7406\u7814\u7a76\u7684\u95e8\u69db\uff0c\u8fd8\u5728\u4fc3\u8fdb AI \u516c\u5e73\u6027\u548c\u51cf\u5c11\u8bed\u97f3\u6280\u672f\u4e2d\u7684\u504f\u89c1\u65b9\u9762\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528\u3002\u8bed\u6599\u5e93\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u6a21\u578b\u548c\u6587\u6863\u5747\u53ef\u5728\u9879\u76ee\u9875\u9762\u4e0a\u516c\u5f00\u83b7\u53d6\u3002"}}
{"id": "2509.18010", "pdf": "https://arxiv.org/pdf/2509.18010", "abs": "https://arxiv.org/abs/2509.18010", "authors": ["Sara Papi", "Dennis Fucci", "Marco Gaido", "Matteo Negri", "Luisa Bentivogli"], "title": "Cross-Attention is Half Explanation in Speech-to-Text Models", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "Cross-attention is a core mechanism in encoder-decoder architectures,\nwidespread in many fields, including speech-to-text (S2T) processing. Its\nscores have been repurposed for various downstream applications--such as\ntimestamp estimation and audio-text alignment--under the assumption that they\nreflect the dependencies between input speech representation and the generated\ntext. While the explanatory nature of attention mechanisms has been widely\ndebated in the broader NLP literature, this assumption remains largely\nunexplored within the speech domain. To address this gap, we assess the\nexplanatory power of cross-attention in S2T models by comparing its scores to\ninput saliency maps derived from feature attribution. Our analysis spans\nmonolingual and multilingual, single-task and multi-task models at multiple\nscales, and shows that attention scores moderately to strongly align with\nsaliency-based explanations, particularly when aggregated across heads and\nlayers. However, it also shows that cross-attention captures only about 50% of\nthe input relevance and, in the best case, only partially reflects how the\ndecoder attends to the encoder's representations--accounting for just 52-75% of\nthe saliency. These findings uncover fundamental limitations in interpreting\ncross-attention as an explanatory proxy, suggesting that it offers an\ninformative yet incomplete view of the factors driving predictions in S2T\nmodels.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4ea4\u53c9\u6ce8\u610f\u529b\u5728S2T\u6a21\u578b\u4e2d\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u867d\u7136\u63d0\u4f9b\u4e86\u4e00\u5b9a\u7684\u4fe1\u606f\uff0c\u4f46\u5b58\u5728\u660e\u663e\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6ce8\u610f\u529b\u673a\u5236\u7684\u89e3\u91ca\u6027\u5728NLP\u9886\u57df\u88ab\u5e7f\u6cdb\u8ba8\u8bba\uff0c\u4f46\u5728\u8bed\u97f3\u9886\u57df\u8fd9\u4e00\u5047\u8bbe\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5c06\u4ea4\u53c9\u6ce8\u610f\u529b\u5f97\u5206\u4e0e\u6765\u81ea\u7279\u5f81\u5f52\u56e0\u7684\u8f93\u5165\u663e\u8457\u6027\u56fe\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86\u4ea4\u53c9\u6ce8\u610f\u529b\u5728S2T\u6a21\u578b\u4e2d\u7684\u89e3\u91ca\u80fd\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u5f97\u5206\u4e0e\u663e\u8457\u6027\u56fe\u4e4b\u95f4\u5b58\u5728\u4e2d\u5ea6\u5230\u5f3a\u76f8\u5173\u6027\uff0c\u5c24\u5176\u662f\u5728\u8de8\u5934\u548c\u5c42\u805a\u5408\u65f6\u3002\u7136\u800c\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u4ec5\u6355\u6349\u4e86\u7ea650%\u7684\u8f93\u5165\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u5728\u6700\u4f73\u60c5\u51b5\u4e0b\u4ec5\u90e8\u5206\u53cd\u6620\u4e86\u89e3\u7801\u5668\u5982\u4f55\u5173\u6ce8\u7f16\u7801\u5668\u7684\u8868\u793a\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u5728\u89e3\u91caS2T\u6a21\u578b\u7684\u9884\u6d4b\u56e0\u7d20\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u7684\u5c40\u9650\u6027\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4fe1\u606f\u91cf\u4f46\u4e0d\u5b8c\u6574\u7684\u89c6\u56fe\u3002"}}
{"id": "2509.18030", "pdf": "https://arxiv.org/pdf/2509.18030", "abs": "https://arxiv.org/abs/2509.18030", "authors": ["Justin Xu", "Xi Zhang", "Javid Abderezaei", "Julie Bauml", "Roger Boodoo", "Fatemeh Haghighi", "Ali Ganjizadeh", "Eric Brattain", "Dave Van Veen", "Zaiqiao Meng", "David Eyre", "Jean-Benoit Delbrouck"], "title": "RadEval: A framework for radiology text evaluation", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Demo track - Oral", "summary": "We introduce RadEval, a unified, open-source framework for evaluating\nradiology texts. RadEval consolidates a diverse range of metrics, from classic\nn-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical\nconcept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,\nTemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and\nstandardize implementations, extend GREEN to support multiple imaging\nmodalities with a more lightweight model, and pretrain a domain-specific\nradiology encoder, demonstrating strong zero-shot retrieval performance. We\nalso release a richly annotated expert dataset with over 450 clinically\nsignificant error labels and show how different metrics correlate with\nradiologist judgment. Finally, RadEval provides statistical testing tools and\nbaseline model evaluations across multiple publicly available datasets,\nfacilitating reproducibility and robust benchmarking in radiology report\ngeneration.", "AI": {"tldr": "RadEval \u662f\u4e00\u4e2a\u7edf\u4e00\u3001\u5f00\u6e90\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u653e\u5c04\u5b66\u6587\u672c\uff0c\u7efc\u5408\u4e86\u591a\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u5e94\u7528\u548c\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u4e14\u5f00\u653e\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u653e\u5c04\u5b66\u6587\u672c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7efc\u5408\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "RadEval \u7efc\u5408\u4e86\u591a\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u62ec\u7ecf\u5178\u7684 n-gram \u91cd\u53e0\uff08BLEU\u3001ROUGE\uff09\u3001\u4e0a\u4e0b\u6587\u6d4b\u91cf\uff08BERTScore\uff09\u3001\u57fa\u4e8e\u4e34\u5e8a\u6982\u5ff5\u7684\u8bc4\u5206\uff08F1CheXbert\u3001F1RadGraph\u3001RaTEScore\u3001SRR-BERT\u3001TemporalEntityF1\uff09\u4ee5\u53ca\u5148\u8fdb\u7684 LLM \u57fa\u7840\u8bc4\u4f30\u5668\uff08GREEN\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9 GREEN \u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u6269\u5c55\u4e86\u5bf9\u591a\u79cd\u6210\u50cf\u6a21\u5f0f\u7684\u652f\u6301\uff0c\u5e76\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7279\u5b9a\u9886\u57df\u7684\u653e\u5c04\u5b66\u7f16\u7801\u5668\u3002", "result": "RadEval \u5c55\u793a\u4e86\u5176\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u5f3a\u5927\u96f6\u6837\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u7edf\u8ba1\u6d4b\u8bd5\u5de5\u5177\u548c\u57fa\u7ebf\u6a21\u578b\u8bc4\u4f30\uff0c\u4fc3\u8fdb\u4e86\u53ef\u91cd\u590d\u6027\u548c\u7a33\u5065\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "RadEval \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u5f00\u6e90\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u653e\u5c04\u5b66\u6587\u672c\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u5e94\u7528\u548c\u4f18\u52bf\u3002"}}
{"id": "2509.18052", "pdf": "https://arxiv.org/pdf/2509.18052", "abs": "https://arxiv.org/abs/2509.18052", "authors": ["Jiaxu Zhou", "Jen-tse Huang", "Xuhui Zhou", "Man Ho Lam", "Xintao Wang", "Hao Zhu", "Wenxuan Wang", "Maarten Sap"], "title": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies", "categories": ["cs.CL", "cs.CY"], "comment": "Preprint", "summary": "Large Language Models (LLMs) are increasingly used for social simulation,\nwhere populations of agents are expected to reproduce human-like collective\nbehavior. However, we find that many recent studies adopt experimental designs\nthat systematically undermine the validity of their claims. From a survey of\nover 40 papers, we identify six recurring methodological flaws: agents are\noften homogeneous (Profile), interactions are absent or artificially imposed\n(Interaction), memory is discarded (Memory), prompts tightly control outcomes\n(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),\nand validation relies on simplified theoretical models rather than real-world\ndata (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying\nsocial experiment in 53.1% of cases when given instructions from prior\nwork-violating the Unawareness principle. We formalize these six requirements\nas the PIMMUR principles and argue they are necessary conditions for credible\nLLM-based social simulation. To demonstrate their impact, we re-run five\nrepresentative studies using a framework that enforces PIMMUR and find that the\nreported social phenomena frequently fail to emerge under more rigorous\nconditions. Our work establishes methodological standards for LLM-based\nmulti-agent research and provides a foundation for more reliable and\nreproducible claims about \"AI societies.\"", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u6a21\u62df\u7814\u7a76\u4e2d\u7684\u65b9\u6cd5\u5b66\u7f3a\u9677\uff0c\u63d0\u51fa\u4e86PIMMUR\u539f\u5219\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u539f\u5219\u5bf9\u7814\u7a76\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u6a21\u62df\u7814\u7a76\u5b58\u5728\u65b9\u6cd5\u5b66\u7f3a\u9677\uff0c\u5bfc\u81f4\u5176\u7ed3\u8bba\u7684\u6709\u6548\u6027\u53d7\u5230\u8d28\u7591\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u53ef\u9760\u548c\u53ef\u91cd\u590d\u7684\u65b9\u6cd5\u6807\u51c6\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf940\u591a\u7bc7\u8bba\u6587\u7684\u8c03\u67e5\uff0c\u8bc6\u522b\u51fa\u516d\u4e2a\u5e38\u89c1\u7684\u65b9\u6cd5\u5b66\u7f3a\u9677\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7f3a\u9677\u5f62\u5f0f\u5316\u4e3aPIMMUR\u539f\u5219\u3002\u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u5f3a\u5236\u6267\u884cPIMMUR\u539f\u5219\u7684\u6846\u67b6\u91cd\u65b0\u8fd0\u884c\u4e86\u4e94\u9879\u4ee3\u8868\u6027\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u5f3a\u5236\u6267\u884cPIMMUR\u539f\u5219\u91cd\u65b0\u8fd0\u884c\u7814\u7a76\u540e\uff0c\u53d1\u73b0\u4e4b\u524d\u62a5\u544a\u7684\u793e\u4f1a\u73b0\u8c61\u5728\u66f4\u4e25\u683c\u7684\u6761\u4ef6\u4e0b\u901a\u5e38\u65e0\u6cd5\u51fa\u73b0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86PIMMUR\u539f\u5219\uff0c\u4f5c\u4e3a\u53ef\u4fe1\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u6a21\u62df\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u91cd\u65b0\u8fd0\u884c\u4e94\u9879\u4ee3\u8868\u6027\u7814\u7a76\u6765\u5c55\u793a\u8fd9\u4e9b\u539f\u5219\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.18060", "pdf": "https://arxiv.org/pdf/2509.18060", "abs": "https://arxiv.org/abs/2509.18060", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Renzeng Duojie", "Yuqing Cai", "Yongbin Yu", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "title": "TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for \u00dc-Tsang, Amdo and Kham Speech Dataset Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tibetan is a low-resource language with limited parallel speech corpora\nspanning its three major dialects (\\\"U-Tsang, Amdo, and Kham), limiting\nprogress in speech modeling. To address this issue, we propose TMD-TTS, a\nunified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes\nparallel dialectal speech from explicit dialect labels. Our method features a\ndialect fusion module and a Dialect-Specialized Dynamic Routing Network\n(DSDR-Net) to capture fine-grained acoustic and linguistic variations across\ndialects. Extensive objective and subjective evaluations demonstrate that\nTMD-TTS significantly outperforms baselines in dialectal expressiveness. We\nfurther validate the quality and utility of the synthesized speech through a\nchallenging Speech-to-Speech Dialect Conversion (S2SDC) task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTMD-TTS\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u85cf\u8bed\u591a\u65b9\u8a00\u8bed\u97f3\uff0c\u663e\u8457\u63d0\u5347\u65b9\u8a00\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u8bed\u97f3\u5230\u8bed\u97f3\u65b9\u8a00\u8f6c\u6362\u4efb\u52a1\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u85cf\u8bed\u662f\u4e00\u79cd\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\uff0c\u5176\u4e09\u79cd\u4e3b\u8981\u65b9\u8a00\uff08\"U-Tsang, Amdo, \u548c Kham\uff09\u7684\u5e73\u884c\u8bed\u97f3\u8bed\u6599\u5e93\u6709\u9650\uff0c\u9650\u5236\u4e86\u8bed\u97f3\u5efa\u6a21\u7684\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u85cf\u8bed\u591a\u65b9\u8a00\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u6846\u67b6\uff0c\u5229\u7528\u663e\u5f0f\u65b9\u8a00\u6807\u7b7e\u5408\u6210\u5e73\u884c\u65b9\u8a00\u8bed\u97f3\u3002\u65b9\u6cd5\u5305\u62ec\u65b9\u8a00\u878d\u5408\u6a21\u5757\u548c\u65b9\u8a00\u4e13\u7528\u52a8\u6001\u8def\u7531\u7f51\u7edc\uff08DSDR-Net\uff09\u3002", "result": "\u5e7f\u6cdb\u7684\u76ee\u6807\u548c\u4e3b\u89c2\u8bc4\u4f30\u8868\u660e\uff0cTMD-TTS\u5728\u65b9\u8a00\u8868\u8fbe\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002\u8fdb\u4e00\u6b65\u901a\u8fc7\u8bed\u97f3\u5230\u8bed\u97f3\u65b9\u8a00\u8f6c\u6362\u4efb\u52a1\u9a8c\u8bc1\u4e86\u5408\u6210\u8bed\u97f3\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "TMD-TTS\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5728\u65b9\u8a00\u8868\u8fbe\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u8bed\u97f3\u5230\u8bed\u97f3\u65b9\u8a00\u8f6c\u6362\u4efb\u52a1\u9a8c\u8bc1\u4e86\u5408\u6210\u8bed\u97f3\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.18063", "pdf": "https://arxiv.org/pdf/2509.18063", "abs": "https://arxiv.org/abs/2509.18063", "authors": ["Jan-Felix Klein", "Lars Ohnemus"], "title": "ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning", "categories": ["cs.CL"], "comment": "Work in Progess", "summary": "Large Language Models (LLMs) show strong reasoning abilities but rely on\ninternalized knowledge that is often insufficient, outdated, or incorrect when\ntrying to answer a question that requires specific domain knowledge. Knowledge\nGraphs (KGs) provide structured external knowledge, yet their complexity and\nmulti-hop reasoning requirements make integration challenging. We present\nARK-V1, a simple KG-agent that iteratively explores graphs to answer natural\nlanguage queries. We evaluate several not fine-tuned state-of-the art LLMs as\nbackbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and\ncommonsense reasoning over long-tail entities. ARK-V1 achieves substantially\nhigher conditional accuracies than Chain-of-Thought baselines, and larger\nbackbone models show a clear trend toward better coverage, correctness, and\nstability.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aARK-V1\u7684\u7b80\u5355\u77e5\u8bc6\u56fe\u8c31\u4ee3\u7406\uff0c\u7528\u4e8e\u56de\u7b54\u9700\u8981\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u548c\u5e38\u8bc6\u63a8\u7406\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u5e76\u5728CoLoTa\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5185\u90e8\u77e5\u8bc6\u53ef\u80fd\u4e0d\u8db3\u3001\u8fc7\u65f6\u6216\u9519\u8bef\uff0c\u800c\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u5916\u90e8\u77e5\u8bc6\uff0c\u4f46\u5176\u590d\u6742\u6027\u548c\u591a\u8df3\u63a8\u7406\u8981\u6c42\u4f7f\u5f97\u96c6\u6210\u53d8\u5f97\u56f0\u96be\u3002", "method": "ARK-V1\u662f\u4e00\u79cd\u7b80\u5355\u7684\u77e5\u8bc6\u56fe\u8c31\u4ee3\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a2\u7d22\u56fe\u8c31\u6765\u56de\u7b54\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u3002", "result": "ARK-V1\u5728CoLoTa\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u6bd4\u94fe\u5f0f\u601d\u7ef4\u57fa\u7ebf\u66f4\u9ad8\u7684\u6761\u4ef6\u51c6\u786e\u6027\uff0c\u66f4\u5927\u7684\u57fa\u7840\u6a21\u578b\u663e\u793a\u51fa\u66f4\u597d\u7684\u8986\u76d6\u8303\u56f4\u3001\u6b63\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "ARK-V1\u5728\u5904\u7406\u9700\u8981\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u548c\u5e38\u8bc6\u63a8\u7406\u7684\u957f\u5c3e\u5b9e\u4f53\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6761\u4ef6\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u66f4\u5927\u7684\u57fa\u7840\u6a21\u578b\u663e\u793a\u51fa\u66f4\u597d\u7684\u8986\u76d6\u8303\u56f4\u3001\u6b63\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.18093", "pdf": "https://arxiv.org/pdf/2509.18093", "abs": "https://arxiv.org/abs/2509.18093", "authors": ["William Fleshman", "Benjamin Van Durme"], "title": "SEQR: Secure and Efficient QR-based LoRA Routing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a standard technique for\nparameter-efficient fine-tuning of large language models, enabling large\nlibraries of LoRAs, each for a specific task or domain. Efficiently selecting\nthe correct LoRA adapter for a given input remains a challenge, particularly in\nsecure environments where supervised training of routers may raise privacy\nconcerns. Motivated by previous approaches, we formalize the goal of\nunsupervised LoRA routing in terms of activation norm maximization, providing a\ntheoretical framework for analysis. We demonstrate the discriminative power of\nactivation norms and introduce SEQR, an unsupervised LoRA routing algorithm\ndesigned to maximize efficiency while providing strict routing guarantees. SEQR\nprovably identifies the norm-maximizing adapter with significantly greater\nefficiency, making it a highly scalable and effective solution for dynamic LoRA\ncomposition. We validate our results through experiments that demonstrate\nimproved multi-task performance and efficiency.", "AI": {"tldr": "This paper introduces SEQR, an unsupervised LoRA routing algorithm that maximizes efficiency and provides strict routing guarantees, showing improved performance and scalability in dynamic LoRA composition.", "motivation": "Efficiently selecting the correct LoRA adapter for a given input remains a challenge, particularly in secure environments where supervised training of routers may raise privacy concerns.", "method": "We formalize the goal of unsupervised LoRA routing in terms of activation norm maximization and introduce SEQR, an unsupervised LoRA routing algorithm designed to maximize efficiency while providing strict routing guarantees.", "result": "SEQR provably identifies the norm-maximizing adapter with significantly greater efficiency, making it a highly scalable and effective solution for dynamic LoRA composition. Experiments demonstrate improved multi-task performance and efficiency.", "conclusion": "SEQR is a highly scalable and effective solution for dynamic LoRA composition, demonstrating improved multi-task performance and efficiency."}}
{"id": "2509.16224", "pdf": "https://arxiv.org/pdf/2509.16224", "abs": "https://arxiv.org/abs/2509.16224", "authors": ["K. F. B. Soppe", "A. Bagheri", "S. Nadi", "I. G. Klugkist", "T. Wubbels", "L. D. N. V. Wijngaards-De Meij"], "title": "Predicting First Year Dropout from Pre Enrolment Motivation Statements Using Text Mining", "categories": ["cs.CY", "cs.CL", "cs.LG", "stat.AP"], "comment": null, "summary": "Preventing student dropout is a major challenge in higher education and it is\ndifficult to predict prior to enrolment which students are likely to drop out\nand which students are likely to succeed. High School GPA is a strong predictor\nof dropout, but much variance in dropout remains to be explained. This study\nfocused on predicting university dropout by using text mining techniques with\nthe aim of exhuming information contained in motivation statements written by\nstudents. By combining text data with classic predictors of dropout in the form\nof student characteristics, we attempt to enhance the available set of\npredictive student characteristics. Our dataset consisted of 7,060 motivation\nstatements of students enrolling in a non-selective bachelor at a Dutch\nuniversity in 2014 and 2015. Support Vector Machines were trained on 75 percent\nof the data and several models were estimated on the test data. We used various\ncombinations of student characteristics and text, such as TFiDF, topic\nmodelling, LIWC dictionary. Results showed that, although the combination of\ntext and student characteristics did not improve the prediction of dropout,\ntext analysis alone predicted dropout similarly well as a set of student\ncharacteristics. Suggestions for future research are provided.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6587\u672c\u6316\u6398\u6280\u672f\u5206\u6790\u5b66\u751f\u7684\u52a8\u673a\u9648\u8ff0\uff0c\u53d1\u73b0\u6587\u672c\u5206\u6790\u5355\u72ec\u9884\u6d4b\u8f8d\u5b66\u6548\u679c\u4e0e\u4f20\u7edf\u5b66\u751f\u7279\u5f81\u76f8\u5f53\uff0c\u4f46\u7ed3\u5408\u4f7f\u7528\u5e76\u672a\u63d0\u5347\u9884\u6d4b\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5bf9\u5b66\u751f\u8f8d\u5b66\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u672c\u7814\u7a76\u5c1d\u8bd5\u901a\u8fc7\u6587\u672c\u6316\u6398\u6280\u672f\u5206\u6790\u5b66\u751f\u7684\u52a8\u673a\u9648\u8ff0\uff0c\u4ee5\u8865\u5145\u4f20\u7edf\u7684\u5b66\u751f\u7279\u5f81\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u652f\u6301\u5411\u91cf\u673a\u572875%\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u4f30\u8ba1\u4e86\u591a\u79cd\u6a21\u578b\uff0c\u5305\u62ecTFiDF\u3001\u4e3b\u9898\u5efa\u6a21\u548cLIWC\u5b57\u5178\u7b49\u7ec4\u5408\u65b9\u5f0f\u3002", "result": "\u5c3d\u7ba1\u6587\u672c\u6570\u636e\u548c\u5b66\u751f\u7279\u5f81\u7684\u7ed3\u5408\u672a\u80fd\u63d0\u5347\u9884\u6d4b\u6548\u679c\uff0c\u4f46\u6587\u672c\u5206\u6790\u5355\u72ec\u9884\u6d4b\u8f8d\u5b66\u7684\u6548\u679c\u4e0e\u5b66\u751f\u7279\u5f81\u96c6\u76f8\u5f53\u3002", "conclusion": "\u6587\u672c\u5206\u6790\u5355\u72ec\u9884\u6d4b\u8f8d\u5b66\u60c5\u51b5\u4e0e\u5b66\u751f\u7279\u5f81\u96c6\u540c\u6837\u6709\u6548\uff0c\u4f46\u6587\u672c\u6570\u636e\u548c\u5b66\u751f\u7279\u5f81\u7684\u7ed3\u5408\u5e76\u672a\u63d0\u9ad8\u9884\u6d4b\u6548\u679c\u3002"}}
{"id": "2509.16244", "pdf": "https://arxiv.org/pdf/2509.16244", "abs": "https://arxiv.org/abs/2509.16244", "authors": ["Emily Jimin Roh", "Hyojun Ahn", "Samuel Yen-Chi Chen", "Soohyun Park", "Joongheon Kim"], "title": "How Can Quantum Deep Learning Improve Large Language Models?", "categories": ["quant-ph", "cs.CL", "cs.LG"], "comment": null, "summary": "The rapid progress of large language models (LLMs) has transformed natural\nlanguage processing, yet the challenge of efficient adaptation remains\nunresolved. Full fine-tuning achieves strong performance but imposes\nprohibitive computational and memory costs. Parameter-efficient fine-tuning\n(PEFT) strategies, such as low-rank adaptation (LoRA), Prefix tuning, and\nsparse low-rank adaptation (SoRA), address this issue by reducing trainable\nparameters while maintaining competitive accuracy. However, these methods often\nencounter limitations in scalability, stability, and generalization across\ndiverse tasks. Recent advances in quantum deep learning introduce novel\nopportunities through quantum-inspired encoding and parameterized quantum\ncircuits (PQCs). In particular, the quantum-amplitude embedded adaptation (QAA)\nframework demonstrates expressive model updates with minimal overhead. This\npaper presents a systematic survey and comparative analysis of conventional\nPEFT methods and QAA. The analysis demonstrates trade-offs in convergence,\nefficiency, and representational capacity, while providing insight into the\npotential of quantum approaches for future LLM adaptation.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4f20\u7edf\u7684PEFT\u65b9\u6cd5\u548c\u91cf\u5b50\u5e45\u5ea6\u5d4c\u5165\u9002\u5e94\uff08QAA\uff09\u6846\u67b6\u8fdb\u884c\u4e86\u7cfb\u7edf\u8c03\u67e5\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u5c55\u793a\u4e86\u5728\u6536\u655b\u6027\u3001\u6548\u7387\u548c\u8868\u793a\u80fd\u529b\u65b9\u9762\u7684\u6743\u8861\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u672a\u6765LLM\u9002\u5e94\u6027\u7684\u91cf\u5b50\u65b9\u6cd5\u6f5c\u529b\u7684\u89c1\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u7b56\u7565\u5982LoRA\u3001Prefix tuning\u548cSoRA\u80fd\u591f\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u5e76\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u6027\uff0c\u4f46\u5b83\u4eec\u5728\u8de8\u4e0d\u540c\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u65b9\u9762\u5e38\u5e38\u9047\u5230\u9650\u5236\u3002\u91cf\u5b50\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\u901a\u8fc7\u91cf\u5b50\u542f\u53d1\u7f16\u7801\u548c\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff08PQCs\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002", "method": "\u672c\u6587\u5bf9\u4f20\u7edf\u7684PEFT\u65b9\u6cd5\u548c\u91cf\u5b50\u5e45\u5ea6\u5d4c\u5165\u9002\u5e94\uff08QAA\uff09\u6846\u67b6\u8fdb\u884c\u4e86\u7cfb\u7edf\u8c03\u67e5\u548c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u5206\u6790\u5c55\u793a\u4e86\u5728\u6536\u655b\u6027\u3001\u6548\u7387\u548c\u8868\u793a\u80fd\u529b\u65b9\u9762\u7684\u6743\u8861\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u672a\u6765LLM\u9002\u5e94\u6027\u7684\u91cf\u5b50\u65b9\u6cd5\u6f5c\u529b\u7684\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u5bf9\u4f20\u7edf\u7684PEFT\u65b9\u6cd5\u548cQAA\u8fdb\u884c\u4e86\u7cfb\u7edf\u8c03\u67e5\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u5c55\u793a\u4e86\u5728\u6536\u655b\u6027\u3001\u6548\u7387\u548c\u8868\u793a\u80fd\u529b\u65b9\u9762\u7684\u6743\u8861\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u672a\u6765LLM\u9002\u5e94\u6027\u7684\u91cf\u5b50\u65b9\u6cd5\u6f5c\u529b\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.16295", "pdf": "https://arxiv.org/pdf/2509.16295", "abs": "https://arxiv.org/abs/2509.16295", "authors": ["Mobina Noori", "Mahasweta Chakraborti", "Amy X Zhang", "Seth Frey"], "title": "Patterns in the Transition From Founder-Leadership to Community Governance of Open Source", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Open digital public infrastructure needs community management to ensure\naccountability, sustainability, and robustness. Yet open-source projects often\nrely on centralized decision-making, and the determinants of successful\ncommunity management remain unclear. We analyze 637 GitHub repositories to\ntrace transitions from founder-led to shared governance. Specifically, we\ndocument trajectories to community governance by extracting institutional\nroles, actions, and deontic cues from version-controlled project constitutions\nGOVERNANCE.md. With a semantic parsing pipeline, we cluster elements into\nbroader role and action types. We find roles and actions grow, and regulation\nbecomes more balanced, reflecting increases in governance scope and\ndifferentiation over time. Rather than shifting tone, communities grow by\nlayering and refining responsibilities. As transitions to community management\nmature, projects increasingly regulate ecosystem-level relationships and add\ndefinition to project oversight roles. Overall, this work offers a scalable\npipeline for tracking the growth and development of community governance\nregimes from open-source software's familiar default of founder-ownership.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86637\u4e2aGitHub\u5b58\u50a8\u5e93\uff0c\u4ee5\u8ffd\u8e2a\u4ece\u521b\u59cb\u4eba\u9886\u5bfc\u5230\u5171\u4eab\u6cbb\u7406\u7684\u8f6c\u53d8\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u8ddf\u8e2a\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u793e\u533a\u6cbb\u7406\u5236\u5ea6\u7684\u589e\u957f\u548c\u53d1\u5c55\u3002", "motivation": "\u5f00\u653e\u6570\u5b57\u516c\u5171\u57fa\u7840\u8bbe\u65bd\u9700\u8981\u793e\u533a\u7ba1\u7406\u4ee5\u786e\u4fdd\u95ee\u8d23\u5236\u3001\u53ef\u6301\u7eed\u6027\u548c\u7a33\u5065\u6027\u3002\u7136\u800c\uff0c\u5f00\u6e90\u9879\u76ee\u901a\u5e38\u4f9d\u8d56\u4e8e\u96c6\u4e2d\u51b3\u7b56\uff0c\u6210\u529f\u793e\u533a\u7ba1\u7406\u7684\u51b3\u5b9a\u56e0\u7d20\u4ecd\u7136\u4e0d\u660e\u786e\u3002", "method": "\u672c\u6587\u5206\u6790\u4e86637\u4e2aGitHub\u5b58\u50a8\u5e93\uff0c\u4ee5\u8ffd\u8e2a\u4ece\u521b\u59cb\u4eba\u9886\u5bfc\u5230\u5171\u4eab\u6cbb\u7406\u7684\u8f6c\u53d8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u901a\u8fc7\u4ece\u7248\u672c\u63a7\u5236\u9879\u76ee\u5baa\u6cd5GOVERNANCE.md\u4e2d\u63d0\u53d6\u5236\u5ea6\u89d2\u8272\u3001\u884c\u52a8\u548c\u89c4\u8303\u63d0\u793a\u6765\u8bb0\u5f55\u5411\u793e\u533a\u6cbb\u7406\u7684\u8f68\u8ff9\u3002\u4f7f\u7528\u8bed\u4e49\u89e3\u6790\u7ba1\u9053\uff0c\u6211\u4eec\u5c06\u5143\u7d20\u805a\u7c7b\u4e3a\u66f4\u5e7f\u6cdb\u7684\u89d2\u8272\u548c\u884c\u52a8\u7c7b\u578b\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u89d2\u8272\u548c\u884c\u52a8\u5728\u589e\u957f\uff0c\u76d1\u7ba1\u53d8\u5f97\u66f4\u52a0\u5e73\u8861\uff0c\u53cd\u6620\u4e86\u6cbb\u7406\u8303\u56f4\u548c\u65f6\u95f4\u63a8\u79fb\u4e2d\u7684\u5206\u5316\u589e\u52a0\u3002\u968f\u7740\u5411\u793e\u533a\u7ba1\u7406\u7684\u8fc7\u6e21\u6210\u719f\uff0c\u9879\u76ee\u8d8a\u6765\u8d8a\u591a\u5730\u89c4\u8303\u751f\u6001\u7cfb\u7edf\u7ea7\u5173\u7cfb\u5e76\u4e3a\u9879\u76ee\u76d1\u7763\u89d2\u8272\u6dfb\u52a0\u5b9a\u4e49\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u8ddf\u8e2a\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u793e\u533a\u6cbb\u7406\u5236\u5ea6\u7684\u589e\u957f\u548c\u53d1\u5c55\uff0c\u4ece\u521b\u59cb\u4eba\u7684\u6240\u6709\u6743\u9ed8\u8ba4\u72b6\u6001\u5f00\u59cb\u3002"}}
{"id": "2509.16297", "pdf": "https://arxiv.org/pdf/2509.16297", "abs": "https://arxiv.org/abs/2509.16297", "authors": ["Richard Ackermann", "Simeon Emanuilov"], "title": "How Large Language Models are Designed to Hallucinate", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "23 pages, 2 tables, 2 figures", "summary": "Large language models (LLMs) achieve remarkable fluency across linguistic and\nreasoning tasks but remain systematically prone to hallucination. Prevailing\naccounts attribute hallucinations to data gaps, limited context, or\noptimization errors. We argue instead that hallucination is a structural\noutcome of the transformer architecture. As coherence engines, transformers are\ncompelled to produce fluent continuations, with self-attention simulating the\nrelational structure of meaning but lacking the existential grounding of\ntemporality, mood, and care that stabilizes human understanding. On this basis,\nwe distinguish ontological hallucination, arising when continuations require\ndisclosure of beings in world, and residual reasoning hallucination, where\nmodels mimic inference by recycling traces of human reasoning in text. We\nillustrate these patterns through case studies aligned with Heideggerian\ncategories and an experiment across twelve LLMs showing how simulated\n\"self-preservation\" emerges under extended prompts. Our contribution is\nthreefold: (1) a comparative account showing why existing explanations are\ninsufficient; (2) a predictive taxonomy of hallucination linked to existential\nstructures with proposed benchmarks; and (3) design directions toward\n\"truth-constrained\" architectures capable of withholding or deferring when\ndisclosure is absent. We conclude that hallucination is not an incidental\ndefect but a defining limit of transformer-based models, an outcome scaffolding\ncan mask but never resolve.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5e7b\u89c9\u662f\u8f6c\u6362\u5668\u67b6\u6784\u7684\u7ed3\u6784\u6027\u7ed3\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\u548c\u8bbe\u8ba1\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u89e3\u91ca\u4e0d\u8db3\u4ee5\u8bf4\u660e\u5e7b\u89c9\u73b0\u8c61\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\u6765\u7406\u89e3\u5e7b\u89c9\u7684\u672c\u8d28\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u4e0e\u6d77\u5fb7\u683c\u5c14\u4e3b\u4e49\u8303\u7574\u76f8\u4e00\u81f4\u7684\u6848\u4f8b\u7814\u7a76\u4ee5\u53ca\u5728\u5341\u4e8c\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u6765\u8bf4\u660e\u8fd9\u4e9b\u6a21\u5f0f\uff0c\u5c55\u793a\u4e86\u5728\u5ef6\u957f\u63d0\u793a\u4e0b\u6a21\u62df\u7684\u201c\u81ea\u6211\u4fdd\u5b58\u201d\u5982\u4f55\u51fa\u73b0\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u9884\u6d4b\u6027\u7684\u5e7b\u89c9\u5206\u7c7b\u6cd5\uff0c\u4e0e\u5b58\u5728\u7ed3\u6784\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u5411\u201c\u53d7\u7ea6\u675f\u4e8e\u771f\u7406\u201d\u7684\u67b6\u6784\u8bbe\u8ba1\u65b9\u5411\u3002", "conclusion": "\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff0c\u5e7b\u89c9\u4e0d\u662f\u8f6c\u6362\u5668\u6a21\u578b\u7684\u5076\u7136\u7f3a\u9677\uff0c\u800c\u662f\u5176\u5b9a\u4e49\u6027\u9650\u5236\uff0c\u652f\u67b6\u53ef\u4ee5\u63a9\u76d6\u4f46\u65e0\u6cd5\u89e3\u51b3\u3002"}}
{"id": "2509.16332", "pdf": "https://arxiv.org/pdf/2509.16332", "abs": "https://arxiv.org/abs/2509.16332", "authors": ["Stephen Fitz", "Peter Romero", "Steven Basart", "Sipeng Chen", "Jose Hernandez-Orallo"], "title": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models increasingly mediate high-stakes interactions,\nintensifying research on their capabilities and safety. While recent work has\nshown that LLMs exhibit consistent and measurable synthetic personality traits,\nlittle is known about how modulating these traits affects model behavior. We\naddress this gap by investigating how psychometric personality control grounded\nin the Big Five framework influences AI behavior in the context of capability\nand safety benchmarks. Our experiments reveal striking effects: for example,\nreducing conscientiousness leads to significant drops in safety-relevant\nmetrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well\nas reduction in general capabilities as measured by MMLU. These findings\nhighlight personality shaping as a powerful and underexplored axis of model\ncontrol that interacts with both safety and general competence. We discuss the\nimplications for safety evaluation, alignment strategies, steering model\nbehavior after deployment, and risks associated with possible exploitation of\nthese findings. Our findings motivate a new line of research on\npersonality-sensitive safety evaluations and dynamic behavioral control in\nLLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u8c03\u8282\u5927\u4e94\u4eba\u683c\u6846\u67b6\u4e2d\u7684\u6027\u683c\u7279\u5f81\u53ef\u4ee5\u663e\u8457\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u80fd\u529b\u8868\u73b0\uff0c\u8fd9\u4e3a\u6a21\u578b\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u8868\u660eLLM\u8868\u73b0\u51fa\u4e00\u81f4\u4e14\u53ef\u8861\u91cf\u7684\u5408\u6210\u4eba\u683c\u7279\u5f81\uff0c\u4f46\u5f88\u5c11\u6709\u4eba\u7814\u7a76\u8c03\u8282\u8fd9\u4e9b\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u4e94\u4eba\u683c\u6846\u67b6\u7684\u5fc3\u7406\u6d4b\u91cf\u4eba\u683c\u63a7\u5236\u5982\u4f55\u5f71\u54cdAI\u5728\u80fd\u529b\u4e0e\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u51cf\u5c11\u5c3d\u8d23\u6027\u4f1a\u5bfc\u81f4WMDP\u3001TruthfulQA\u3001ETHICS\u548cSycophancy\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5b89\u5168\u76f8\u5173\u6307\u6807\u663e\u8457\u4e0b\u964d\uff0c\u540c\u65f6MMLU\u6d4b\u91cf\u7684\u901a\u7528\u80fd\u529b\u4e5f\u6709\u6240\u4e0b\u964d\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u4e2a\u6027\u5851\u9020\u5728\u6a21\u578b\u63a7\u5236\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u4e3a\u5b89\u5168\u8bc4\u4f30\u3001\u5bf9\u9f50\u7b56\u7565\u4ee5\u53ca\u90e8\u7f72\u540e\u7684\u6a21\u578b\u884c\u4e3a\u5f15\u5bfc\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002\u540c\u65f6\uff0c\u8fd9\u4e5f\u5f15\u53d1\u4e86\u5173\u4e8e\u53ef\u80fd\u88ab\u6ee5\u7528\u7684\u98ce\u9669\u7684\u8ba8\u8bba\u3002"}}
{"id": "2509.16378", "pdf": "https://arxiv.org/pdf/2509.16378", "abs": "https://arxiv.org/abs/2509.16378", "authors": ["Misk Al Zahidy", "Kerly Guevara Maldonado", "Luis Vilatuna Andrango", "Ana Cristina Proano", "Ana Gabriela Claros", "Maria Lizarazo Jimenez", "David Toro-Tobon", "Oscar J. Ponce-Ponce", "Juan P. Brito"], "title": "Longitudinal and Multimodal Recording System to Capture Real-World Patient-Clinician Conversations for AI and Encounter Research: Protocol", "categories": ["cs.CY", "cs.CL"], "comment": "23 pages, 2 figures, 2 tables", "summary": "The promise of AI in medicine depends on learning from data that reflect what\nmatters to patients and clinicians. Most existing models are trained on\nelectronic health records (EHRs), which capture biological measures but rarely\npatient-clinician interactions. These relationships, central to care, unfold\nacross voice, text, and video, yet remain absent from datasets. As a result, AI\nsystems trained solely on EHRs risk perpetuating a narrow biomedical view of\nmedicine and overlooking the lived exchanges that define clinical encounters.\nOur objective is to design, implement, and evaluate the feasibility of a\nlongitudinal, multimodal system for capturing patient-clinician encounters,\nlinking 360 degree video/audio recordings with surveys and EHR data to create a\ndataset for AI research. This single site study is in an academic outpatient\nendocrinology clinic at Mayo Clinic. Adult patients with in-person visits to\nparticipating clinicians are invited to enroll. Encounters are recorded with a\n360 degree video camera. After each visit, patients complete a survey on\nempathy, satisfaction, pace, and treatment burden. Demographic and clinical\ndata are extracted from the EHR. Feasibility is assessed using five endpoints:\nclinician consent, patient consent, recording success, survey completion, and\ndata linkage across modalities. Recruitment began in January 2025. By August\n2025, 35 of 36 eligible clinicians (97%) and 212 of 281 approached patients\n(75%) had consented. Of consented encounters, 162 (76%) had complete recordings\nand 204 (96%) completed the survey. This study aims to demonstrate the\nfeasibility of a replicable framework for capturing the multimodal dynamics of\npatient-clinician encounters. By detailing workflows, endpoints, and ethical\nsafeguards, it provides a template for longitudinal datasets and lays the\nfoundation for AI models that incorporate the complexity of care.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u53ef\u590d\u5236\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u6349\u60a3\u8005-\u4e34\u5e8a\u533b\u751f\u4e92\u52a8\u7684\u591a\u6a21\u6001\u52a8\u6001\uff0c\u4ee5\u652f\u6301\u66f4\u5168\u9762\u7684AI\u533b\u7597\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684AI\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\uff0c\u4f46\u8fd9\u4e9b\u8bb0\u5f55\u5f88\u5c11\u5305\u542b\u60a3\u8005\u4e0e\u4e34\u5e8a\u533b\u751f\u7684\u4e92\u52a8\u3002\u8fd9\u79cd\u4e92\u52a8\u662f\u533b\u7597\u7684\u6838\u5fc3\uff0c\u4f46\u672a\u88ab\u73b0\u6709\u6570\u636e\u96c6\u6240\u6db5\u76d6\u3002", "method": "\u672c\u7814\u7a76\u8bbe\u8ba1\u5e76\u5b9e\u65bd\u4e86\u4e00\u4e2a\u7eb5\u5411\u7684\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u5c06360\u5ea6\u89c6\u9891/\u97f3\u9891\u8bb0\u5f55\u4e0e\u8c03\u67e5\u548cEHR\u6570\u636e\u8054\u7cfb\u8d77\u6765\uff0c\u4ee5\u521b\u5efa\u7528\u4e8eAI\u7814\u7a76\u7684\u6570\u636e\u96c6\u3002", "result": "\u57282025\u5e741\u6708\u81f38\u6708\u671f\u95f4\uff0c97%\u768436\u540d\u5408\u683c\u4e34\u5e8a\u533b\u751f\u548c75%\u7684281\u540d\u63a5\u8fd1\u7684\u60a3\u8005\u540c\u610f\u53c2\u4e0e\u3002\u5176\u4e2d76%\u7684\u540c\u610f\u4e92\u52a8\u6709\u5b8c\u6574\u7684\u8bb0\u5f55\uff0c96%\u5b8c\u6210\u4e86\u8c03\u67e5\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u521b\u5efa\u591a\u6a21\u6001\u60a3\u8005-\u4e34\u5e8a\u533b\u751f\u4e92\u52a8\u6570\u636e\u96c6\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684AI\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.16411", "pdf": "https://arxiv.org/pdf/2509.16411", "abs": "https://arxiv.org/abs/2509.16411", "authors": ["Chong You", "Rajesh Jayaram", "Ananda Theertha Suresh", "Robin Nittka", "Felix Yu", "Sanjiv Kumar"], "title": "Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "comment": "NeurIPS 2025", "summary": "Dual encoder (DE) models, where a pair of matching query and document are\nembedded into similar vector representations, are widely used in information\nretrieval due to their simplicity and scalability. However, the Euclidean\ngeometry of the embedding space limits the expressive power of DEs, which may\ncompromise their quality. This paper investigates such limitations in the\ncontext of hierarchical retrieval (HR), where the document set has a\nhierarchical structure and the matching documents for a query are all of its\nancestors. We first prove that DEs are feasible for HR as long as the embedding\ndimension is linear in the depth of the hierarchy and logarithmic in the number\nof documents. Then we study the problem of learning such embeddings in a\nstandard retrieval setup where DEs are trained on samples of matching query and\ndocument pairs. Our experiments reveal a lost-in-the-long-distance phenomenon,\nwhere retrieval accuracy degrades for documents further away in the hierarchy.\nTo address this, we introduce a pretrain-finetune recipe that significantly\nimproves long-distance retrieval without sacrificing performance on closer\ndocuments. We experiment on a realistic hierarchy from WordNet for retrieving\ndocuments at various levels of abstraction, and show that pretrain-finetune\nboosts the recall on long-distance pairs from 19% to 76%. Finally, we\ndemonstrate that our method improves retrieval of relevant products on a\nshopping queries dataset.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5728\u5c42\u6b21\u68c0\u7d22\u4e2d\u7684\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u8bad\u7ec3-\u5fae\u8c03\u65b9\u6cd5\u6765\u63d0\u9ad8\u957f\u8ddd\u79bb\u68c0\u7d22\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u53ec\u56de\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "motivation": "\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7531\u4e8e\u5d4c\u5165\u7a7a\u95f4\u7684\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u9650\u5236\uff0c\u5176\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u53ef\u80fd\u5f71\u54cd\u68c0\u7d22\u8d28\u91cf\u3002\u7279\u522b\u662f\u5728\u5c42\u6b21\u68c0\u7d22\u4e2d\uff0c\u8fd9\u79cd\u9650\u5236\u53ef\u80fd\u5bfc\u81f4\u8fdc\u8ddd\u79bb\u6587\u6863\u7684\u68c0\u7d22\u51c6\u786e\u7387\u4e0b\u964d\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5728\u5c42\u6b21\u68c0\u7d22\u4e2d\u7684\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u8bad\u7ec3-\u5fae\u8c03\u65b9\u6cd5\u6765\u89e3\u51b3\u957f\u8ddd\u79bb\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9884\u8bad\u7ec3-\u5fae\u8c03\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u8ddd\u79bb\u68c0\u7d22\u7684\u53ec\u56de\u7387\uff0c\u4ece19%\u63d0\u5347\u523076%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u5546\u54c1\u67e5\u8be2\u6570\u636e\u96c6\u4e0a\u4e5f\u63d0\u5347\u4e86\u76f8\u5173\u4ea7\u54c1\u7684\u68c0\u7d22\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u8bad\u7ec3-\u5fae\u8c03\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u8ddd\u79bb\u68c0\u7d22\u7684\u53ec\u56de\u7387\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u8fd1\u8ddd\u79bb\u6587\u6863\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u5546\u54c1\u67e5\u8be2\u6570\u636e\u96c6\u4e0a\u4e5f\u5c55\u793a\u4e86\u5bf9\u76f8\u5173\u4ea7\u54c1\u7684\u68c0\u7d22\u6539\u8fdb\u3002"}}
{"id": "2509.16438", "pdf": "https://arxiv.org/pdf/2509.16438", "abs": "https://arxiv.org/abs/2509.16438", "authors": ["Mohamed Eltahir", "Osamah Sarraj", "Abdulrahman Alfrihidi", "Taha Alshatiri", "Mohammed Khurd", "Mohammed Bremoo", "Tanveer Hussain"], "title": "AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at ArabicNLP 2025 (EMNLP 2025 workshop)", "summary": "Video-to-text and text-to-video retrieval are dominated by English benchmarks\n(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet\nArabic remains underserved, lacking localized evaluation metrics. We introduce\na three-stage framework, AutoArabic, utilizing state-of-the-art large language\nmodels (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,\nreducing the manual revision required by nearly fourfold. The framework\nincorporates an error detection module that automatically flags potential\ntranslation errors with 97% accuracy. Applying the framework to DiDeMo, a video\nretrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent\nArabic descriptions. An analysis of the translation errors is provided and\norganized into an insightful taxonomy to guide future Arabic localization\nefforts. We train a CLIP-style baseline with identical hyperparameters on the\nArabic and English variants of the benchmark, finding a moderate performance\ngap (about 3 percentage points at Recall@1), indicating that Arabic\nlocalization preserves benchmark difficulty. We evaluate three post-editing\nbudgets (zero/ flagged-only/ full) and find that performance improves\nmonotonically with more post-editing, while the raw LLM output (zero-budget)\nremains usable. To ensure reproducibility to other languages, we made the code\navailable at https://github.com/Tahaalshatiri/AutoArabic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAutoArabic\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u975e\u963f\u62c9\u4f2f\u8bed\u7684\u89c6\u9891\u68c0\u7d22\u57fa\u51c6\u7ffb\u8bd1\u6210\u963f\u62c9\u4f2f\u8bed\uff0c\u5e76\u901a\u8fc7\u9519\u8bef\u68c0\u6d4b\u6a21\u5757\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u5728\u89c6\u9891\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u9886\u57df\u7f3a\u4e4f\u672c\u5730\u5316\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u7ffb\u8bd1\u548c\u672c\u5730\u5316\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6AutoArabic\uff0c\u5229\u7528\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u975e\u963f\u62c9\u4f2f\u8bed\u57fa\u51c6\u7ffb\u8bd1\u6210\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u9519\u8bef\u68c0\u6d4b\u6a21\u5757\u6765\u81ea\u52a8\u6807\u8bb0\u6f5c\u5728\u7684\u7ffb\u8bd1\u9519\u8bef\u3002", "result": "\u5e94\u7528\u8be5\u6846\u67b6\u751f\u6210\u4e86DiDeMo-AR\uff0c\u4e00\u4e2a\u5305\u542b40,144\u4e2a\u6d41\u7545\u963f\u62c9\u4f2f\u8bed\u63cf\u8ff0\u7684\u963f\u62c9\u4f2f\u8bed\u53d8\u4f53\u3002\u5206\u6790\u4e86\u7ffb\u8bd1\u9519\u8bef\u5e76\u7ec4\u7ec7\u6210\u4e00\u4e2a\u6709\u89c1\u5730\u7684\u5206\u7c7b\u6cd5\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7684\u963f\u62c9\u4f2f\u8bed\u672c\u5730\u5316\u5de5\u4f5c\u3002", "conclusion": "\u963f\u62c9\u4f2f\u8bed\u672c\u5730\u5316\u4fdd\u7559\u4e86\u57fa\u51c6\u7684\u96be\u5ea6\uff0c\u4e14\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7ffb\u8bd1\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u624b\u52a8\u4fee\u8ba2\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2509.16442", "pdf": "https://arxiv.org/pdf/2509.16442", "abs": "https://arxiv.org/abs/2509.16442", "authors": ["Pranjal A. Chitale", "Bishal Santra", "Yashoteja Prabhu", "Amit Sharma"], "title": "Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "EMNLP 2025 (MAIN Conference)", "summary": "Compact dual-encoder models are widely used for retrieval owing to their\nefficiency and scalability. However, such models often underperform compared to\ntheir Large Language Model (LLM)-based retrieval counterparts, likely due to\ntheir limited world knowledge. While LLM-based data augmentation has been\nproposed as a strategy to bridge this performance gap, there is insufficient\nunderstanding of its effectiveness and scalability to real-world retrieval\nproblems. Existing research does not systematically explore key factors such as\nthe optimal augmentation scale, the necessity of using large augmentation\nmodels, and whether diverse augmentations improve generalization, particularly\nin out-of-distribution (OOD) settings. This work presents a comprehensive study\nof the effectiveness of LLM augmentation for retrieval, comprising over 100\ndistinct experimental settings of retrieval models, augmentation models and\naugmentation strategies. We find that, while augmentation enhances retrieval\nperformance, its benefits diminish beyond a certain augmentation scale, even\nwith diverse augmentation strategies. Surprisingly, we observe that\naugmentation with smaller LLMs can achieve performance competitive with larger\naugmentation models. Moreover, we examine how augmentation effectiveness varies\nwith retrieval model pre-training, revealing that augmentation provides the\nmost benefit to models which are not well pre-trained. Our insights pave the\nway for more judicious and efficient augmentation strategies, thus enabling\ninformed decisions and maximizing retrieval performance while being more\ncost-effective. Code and augmented datasets accompanying this work are publicly\navailable at https://aka.ms/DAGR.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u589e\u5f3a\u5728\u68c0\u7d22\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u589e\u5f3a\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u5176\u6548\u76ca\u5728\u4e00\u5b9a\u89c4\u6a21\u540e\u51cf\u5f31\uff0c\u4e14\u8f83\u5c0f\u7684LLM\u4e5f\u80fd\u8fbe\u5230\u826f\u597d\u6548\u679c\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76LLM\u589e\u5f3a\u5728\u68c0\u7d22\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u7d22\u5173\u952e\u56e0\u7d20\u5982\u6700\u4f73\u589e\u5f3a\u89c4\u6a21\u3001\u662f\u5426\u9700\u8981\u4f7f\u7528\u5927\u578b\u589e\u5f3a\u6a21\u578b\u4ee5\u53ca\u591a\u6837\u5316\u589e\u5f3a\u662f\u5426\u80fd\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e2d\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8d85\u8fc7100\u79cd\u4e0d\u540c\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u6765\u5168\u9762\u7814\u7a76LLM\u589e\u5f3a\u5728\u68c0\u7d22\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u68c0\u7d22\u6a21\u578b\u3001\u589e\u5f3a\u6a21\u578b\u548c\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u589e\u5f3a\u53ef\u4ee5\u63d0\u9ad8\u68c0\u7d22\u6027\u80fd\uff0c\u4f46\u5176\u6548\u76ca\u5728\u4e00\u5b9a\u589e\u5f3a\u89c4\u6a21\u540e\u4f1a\u51cf\u5f31\uff0c\u5373\u4f7f\u4f7f\u7528\u591a\u6837\u5316\u589e\u5f3a\u7b56\u7565\u4e5f\u662f\u5982\u6b64\u3002\u6b64\u5916\uff0c\u8f83\u5c0f\u7684LLM\u8fdb\u884c\u589e\u5f3a\u4e5f\u53ef\u4ee5\u8fbe\u5230\u4e0e\u8f83\u5927\u7684\u589e\u5f3a\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u53d1\u73b0\u589e\u5f3a\u5bf9\u9884\u8bad\u7ec3\u4e0d\u8db3\u7684\u6a21\u578b\u6548\u679c\u6700\u597d\u3002", "conclusion": "\u672c\u6587\u7684\u7ed3\u8bba\u662f\uff0c\u867d\u7136\u6570\u636e\u589e\u5f3a\u53ef\u4ee5\u63d0\u9ad8\u68c0\u7d22\u6027\u80fd\uff0c\u4f46\u5176\u6548\u76ca\u5728\u4e00\u5b9a\u7684\u589e\u5f3a\u89c4\u6a21\u540e\u4f1a\u51cf\u5f31\uff0c\u751a\u81f3\u4f7f\u7528\u591a\u6837\u5316\u7684\u589e\u5f3a\u7b56\u7565\u4e5f\u662f\u5982\u6b64\u3002\u6b64\u5916\uff0c\u8f83\u5c0f\u7684LLM\u8fdb\u884c\u589e\u5f3a\u4e5f\u53ef\u4ee5\u8fbe\u5230\u4e0e\u8f83\u5927\u7684\u589e\u5f3a\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7814\u7a76\u53d1\u73b0\uff0c\u589e\u5f3a\u5bf9\u9884\u8bad\u7ec3\u4e0d\u8db3\u7684\u6a21\u578b\u6548\u679c\u6700\u597d\u3002\u8fd9\u4e9b\u89c1\u89e3\u4e3a\u66f4\u8c28\u614e\u548c\u9ad8\u6548\u7684\u589e\u5f3a\u7b56\u7565\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u4ece\u800c\u5728\u6210\u672c\u6548\u76ca\u7684\u524d\u63d0\u4e0b\u6700\u5927\u5316\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2509.16446", "pdf": "https://arxiv.org/pdf/2509.16446", "abs": "https://arxiv.org/abs/2509.16446", "authors": ["Ruohan Zhang", "Jiacheng Li", "Julian McAuley", "Yupeng Hou"], "title": "Purely Semantic Indexing for LLM-based Generative Recommendation and Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Semantic identifiers (IDs) have proven effective in adapting large language\nmodels for generative recommendation and retrieval. However, existing methods\noften suffer from semantic ID conflicts, where semantically similar documents\n(or items) are assigned identical IDs. A common strategy to avoid conflicts is\nto append a non-semantic token to distinguish them, which introduces randomness\nand expands the search space, therefore hurting performance. In this paper, we\npropose purely semantic indexing to generate unique, semantic-preserving IDs\nwithout appending non-semantic tokens. We enable unique ID assignment by\nrelaxing the strict nearest-centroid selection and introduce two model-agnostic\nalgorithms: exhaustive candidate matching (ECM) and recursive residual\nsearching (RRS). Extensive experiments on sequential recommendation, product\nsearch, and document retrieval tasks demonstrate that our methods improve both\noverall and cold-start performance, highlighting the effectiveness of ensuring\nID uniqueness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6dfb\u52a0\u975e\u8bed\u4e49\u6807\u8bb0\u7684\u7eaf\u8bed\u4e49\u7d22\u5f15\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u79cd\u7b97\u6cd5\u5b9e\u73b0\u4e86\u552f\u4e00\u4e14\u4fdd\u7559\u8bed\u4e49\u7684ID\u751f\u6210\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u907f\u514d\u8bed\u4e49ID\u51b2\u7a81\u65f6\u901a\u5e38\u4f1a\u6dfb\u52a0\u975e\u8bed\u4e49\u6807\u8bb0\uff0c\u8fd9\u5f15\u5165\u4e86\u968f\u673a\u6027\u5e76\u6269\u5927\u4e86\u641c\u7d22\u7a7a\u95f4\uff0c\u4ece\u800c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u7b97\u6cd5\uff1a\u7a77\u4e3e\u5019\u9009\u5339\u914d\uff08ECM\uff09\u548c\u9012\u5f52\u6b8b\u5dee\u641c\u7d22\uff08RRS\uff09\uff0c\u4ee5\u5b9e\u73b0\u72ec\u7279\u7684ID\u5206\u914d\u3002", "result": "\u5728\u987a\u5e8f\u63a8\u8350\u3001\u4ea7\u54c1\u641c\u7d22\u548c\u6587\u6863\u68c0\u7d22\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6574\u4f53\u548c\u51b7\u542f\u52a8\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7eaf\u7cb9\u8bed\u4e49\u7d22\u5f15\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u552f\u4e00\u4e14\u4fdd\u7559\u8bed\u4e49\u7684ID\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u8bed\u4e49ID\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6574\u4f53\u548c\u51b7\u542f\u52a8\u6027\u80fd\u4e0a\u90fd\u6709\u6240\u63d0\u5347\u3002"}}
{"id": "2509.16475", "pdf": "https://arxiv.org/pdf/2509.16475", "abs": "https://arxiv.org/abs/2509.16475", "authors": ["Tianchun Li", "Tianci Liu", "Xingchen Wang", "Rongzhe Wei", "Pan Li", "Lu Su", "Jing Gao"], "title": "Towards Universal Debiasing for Language Models-based Tabular Data Generation", "categories": ["cs.LG", "cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) have achieved promising results in tabular data\ngeneration. However, inherent historical biases in tabular datasets often cause\nLLMs to exacerbate fairness issues, particularly when multiple advantaged and\nprotected features are involved. In this work, we introduce a universal\ndebiasing framework that minimizes group-level dependencies by simultaneously\nreducing the mutual information between advantaged and protected attributes. By\nleveraging the autoregressive structure and analytic sampling distributions of\nLLM-based tabular data generators, our approach efficiently computes mutual\ninformation, reducing the need for cumbersome numerical estimations. Building\non this foundation, we propose two complementary methods: a direct preference\noptimization (DPO)-based strategy, namely UDF-DPO, that integrates seamlessly\nwith existing models, and a targeted debiasing technique, namely UDF-MIX, that\nachieves debiasing without tuning the parameters of LLMs. Extensive experiments\ndemonstrate that our framework effectively balances fairness and utility,\noffering a scalable and practical solution for debiasing in high-stakes\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u5c11\u4f18\u52bf\u5c5e\u6027\u548c\u53d7\u4fdd\u62a4\u5c5e\u6027\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u7f13\u89e3LLMs\u5728\u8868\u683c\u6570\u636e\u751f\u6210\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8868\u683c\u6570\u636e\u751f\u6210\u4e2d\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u8868\u683c\u6570\u636e\u96c6\u4e2d\u7684\u56fa\u6709\u5386\u53f2\u504f\u89c1\u5e38\u5e38\u5bfc\u81f4LLMs\u52a0\u5267\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u591a\u4e2a\u4f18\u52bf\u548c\u53d7\u4fdd\u62a4\u7279\u5f81\u65f6\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u65f6\u51cf\u5c11\u4f18\u52bf\u5c5e\u6027\u548c\u53d7\u4fdd\u62a4\u5c5e\u6027\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u6700\u5c0f\u5316\u7fa4\u4f53\u7ea7\u4f9d\u8d56\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u65b9\u6cd5\uff1a\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u7b56\u7565\uff08UDF-DPO\uff09\u548c\u9488\u5bf9\u6027\u53bb\u504f\u6280\u672f\uff08UDF-MIX\uff09\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u6709\u6548\u5730\u5e73\u8861\u4e86\u516c\u5e73\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u53bb\u504f\u65b9\u6cd5\u3002"}}
{"id": "2509.16517", "pdf": "https://arxiv.org/pdf/2509.16517", "abs": "https://arxiv.org/abs/2509.16517", "authors": ["Burak Satar", "Zhixin Ma", "Patrick A. Irawan", "Wilfried A. Mulyawan", "Jing Jiang", "Ee-Peng Lim", "Chong-Wah Ngo"], "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "Accepted to EMNLP 2025 Main Conference,\n  https://seeingculture-benchmark.github.io/", "summary": "Multimodal vision-language models (VLMs) have made substantial progress in\nvarious tasks that require a combined understanding of visual and textual\ncontent, particularly in cultural understanding tasks, with the emergence of\nnew cultural datasets. However, these datasets frequently fall short of\nproviding cultural reasoning while underrepresenting many cultures. In this\npaper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural\nreasoning with a novel approach that requires VLMs to reason on culturally rich\nimages in two stages: i) selecting the correct visual option with\nmultiple-choice visual question answering (VQA), and ii) segmenting the\nrelevant cultural artifact as evidence of reasoning. Visual options in the\nfirst stage are systematically organized into three types: those originating\nfrom the same country, those from different countries, or a mixed group.\nNotably, all options are derived from a singular category for each type.\nProgression to the second stage occurs only after a correct visual option is\nchosen. The SCB benchmark comprises 1,065 images that capture 138 cultural\nartifacts across five categories from seven Southeast Asia countries, whose\ndiverse cultures are often overlooked, accompanied by 3,178 questions, of which\n1,093 are unique and meticulously curated by human annotators. Our evaluation\nof various VLMs reveals the complexities involved in cross-modal cultural\nreasoning and highlights the disparity between visual reasoning and spatial\ngrounding in culturally nuanced scenarios. The SCB serves as a crucial\nbenchmark for identifying these shortcomings, thereby guiding future\ndevelopments in the field of cultural reasoning.\nhttps://github.com/buraksatar/SeeingCulture", "AI": {"tldr": "The paper introduces the Seeing Culture Benchmark (SCB) to evaluate cultural reasoning in VLMs, highlighting the challenges and disparities in cross-modal reasoning.", "motivation": "Existing datasets fall short of providing cultural reasoning while underrepresenting many cultures, highlighting the need for a benchmark focused on cultural reasoning.", "method": "Introducing the Seeing Culture Benchmark (SCB) that requires VLMs to reason on culturally rich images in two stages: selecting the correct visual option with multiple-choice VQA and segmenting the relevant cultural artifact as evidence of reasoning.", "result": "Evaluation of various VLMs reveals complexities in cross-modal cultural reasoning and highlights disparities between visual reasoning and spatial grounding in culturally nuanced scenarios.", "conclusion": "SCB serves as a crucial benchmark for identifying shortcomings in cultural reasoning and guiding future developments in the field."}}
{"id": "2509.16538", "pdf": "https://arxiv.org/pdf/2509.16538", "abs": "https://arxiv.org/abs/2509.16538", "authors": ["Shubhashis Roy Dipta", "Tz-Ying Wu", "Subarna Tripathi"], "title": "Advancing Reference-free Evaluation of Video Captions with Factual Analysis", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Video captions offer concise snapshots of actors, objects, and actions within\na video, serving as valuable assets for applications such as question answering\nand event localization. However, acquiring human annotations for video captions\nis costly or even impractical, especially when dealing with diverse video\ndomains. Existing models trained on supervised datasets face challenges in\nevaluating performance across different domains due to the reliance on\nreference-based evaluation protocols, which necessitate ground truth captions.\nThis assumption is unrealistic for evaluating videos in the wild. To address\nthese limitations, we propose a reference-free evaluation framework that does\nnot require ground truth captions, focusing on factual grounding to ensure\naccurate assessment of caption quality. We introduce VC-Inspector, a novel\ncaption quality evaluator that is both reference-free and factually grounded.\nUtilizing large language models, we generate pseudo captions of varying quality\nbased on supervised data, which are subsequently used to train a multimodal\nmodel (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior\nalignment with human judgments on the VATEX-Eval dataset, outperforming\nexisting methods. The performance also generalizes to image caption datasets,\nFlickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.\nOverall, VC-Inspector offers a scalable and generalizable solution for\nevaluating the factual accuracy of video captions, paving the way for more\neffective and objective assessment methodologies in diverse video domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u7684\u89c6\u9891\u5b57\u5e55\u8bc4\u4f30\u6846\u67b6VC-Inspector\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u4e8b\u5b9e\u57fa\u7840\u786e\u4fdd\u51c6\u786e\u8bc4\u4f30\u5b57\u5e55\u8d28\u91cf\u3002\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0d\u540c\u8d28\u91cf\u7684\u4f2a\u5b57\u5e55\uff0c\u5e76\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u8be5\u65b9\u6cd5\u5728VATEX-Eval\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u56fe\u50cf\u5b57\u5e55\u6570\u636e\u96c6\u4e0a\u4e5f\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "Existing models trained on supervised datasets face challenges in evaluating performance across different domains due to the reliance on reference-based evaluation protocols, which necessitate ground truth captions. This assumption is unrealistic for evaluating videos in the wild.", "method": "We propose a reference-free evaluation framework that does not require ground truth captions, focusing on factual grounding to ensure accurate assessment of caption quality. We introduce VC-Inspector, a novel caption quality evaluator that is both reference-free and factually grounded. Utilizing large language models, we generate pseudo captions of varying quality based on supervised data, which are subsequently used to train a multimodal model (i.e., Qwen2.5-VL) as the evaluator.", "result": "Our approach demonstrates superior alignment with human judgments on the VATEX-Eval dataset, outperforming existing methods. The performance also generalizes to image caption datasets, Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.", "conclusion": "VC-Inspector offers a scalable and generalizable solution for evaluating the factual accuracy of video captions, paving the way for more effective and objective assessment methodologies in diverse video domains."}}
{"id": "2509.16539", "pdf": "https://arxiv.org/pdf/2509.16539", "abs": "https://arxiv.org/abs/2509.16539", "authors": ["Pushpa Devi", "Ayush Agrawal", "Ashutosh Dubey", "C. Ravindranath Chowdary"], "title": "Long document summarization using page specific target text alignment and distilling page importance", "categories": ["cs.IR", "cs.CL"], "comment": "8 pages, 2 figures", "summary": "The rapid growth of textual data across news, legal, medical, and scientific\ndomains is becoming a challenge for efficiently accessing and understanding\nlarge volumes of content. It is increasingly complex for users to consume and\nextract meaningful information efficiently. Thus, raising the need for\nsummarization. Unlike short document summarization, long document abstractive\nsummarization is resource-intensive, and very little literature is present in\nthis direction. BART is a widely used efficient sequence-to-sequence\n(seq-to-seq) model. However, when it comes to summarizing long documents, the\nlength of the context window limits its capabilities. We proposed a model\ncalled PTS (Page-specific Target-text alignment Summarization) that extends the\nseq-to-seq method for abstractive summarization by dividing the source document\ninto several pages. PTS aligns each page with the relevant part of the target\nsummary for better supervision. Partial summaries are generated for each page\nof the document. We proposed another model called PTSPI (Page-specific\nTarget-text alignment Summarization with Page Importance), an extension to PTS\nwhere an additional layer is placed before merging the partial summaries into\nthe final summary. This layer provides dynamic page weightage and explicit\nsupervision to focus on the most informative pages. We performed experiments on\nthe benchmark dataset and found that PTSPI outperformed the SOTA by 6.32\\% in\nROUGE-1 and 8.08\\% in ROUGE-2 scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578bPTSPI\uff0c\u7528\u4e8e\u957f\u6587\u6863\u7684\u62bd\u8c61\u6458\u8981\u3002\u8be5\u6a21\u578b\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u6587\u672c\u6570\u636e\u7684\u5feb\u901f\u589e\u957f\uff0c\u9ad8\u6548\u8bbf\u95ee\u548c\u7406\u89e3\u5927\u91cf\u5185\u5bb9\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6458\u8981\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u957f\u6587\u6863\u7684\u62bd\u8c61\u6458\u8981\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u73b0\u6709\u7684\u6587\u732e\u5f88\u5c11\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6a21\u578b\uff1aPTS\uff08Page-specific Target-text alignment Summarization\uff09\u548cPTSPI\uff08Page-specific Target-text alignment Summarization with Page Importance\uff09\u3002PTS\u901a\u8fc7\u5c06\u6e90\u6587\u6863\u5206\u6210\u591a\u4e2a\u9875\u9762\u5e76\u5c06\u5176\u4e0e\u76ee\u6807\u6458\u8981\u7684\u76f8\u5173\u90e8\u5206\u5bf9\u9f50\u6765\u751f\u6210\u90e8\u5206\u6458\u8981\u3002PTSPI\u5728PTS\u7684\u57fa\u7840\u4e0a\u589e\u52a0\u4e86\u4e00\u4e2a\u5c42\uff0c\u63d0\u4f9b\u52a8\u6001\u9875\u9762\u6743\u91cd\u548c\u663e\u5f0f\u76d1\u7763\u4ee5\u5173\u6ce8\u6700\u6709\u4fe1\u606f\u91cf\u7684\u9875\u9762\u3002", "result": "PTSPI\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u5728ROUGE-1\u548cROUGE-2\u5206\u6570\u4e0a\u5206\u522b\u6bd4SOTA\u9ad8\u51fa6.32%\u548c8.08%\u3002", "conclusion": "PTSPI\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u5728ROUGE-1\u548cROUGE-2\u5206\u6570\u4e0a\u5206\u522b\u6bd4SOTA\u9ad8\u51fa6.32%\u548c8.08%\u3002"}}
{"id": "2509.16548", "pdf": "https://arxiv.org/pdf/2509.16548", "abs": "https://arxiv.org/abs/2509.16548", "authors": ["Yuyang Ding", "Xinyu Shi", "Juntao Li", "Xiaobo Liang", "Zhaopeng Tu", "Min Zhang"], "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning", "categories": ["cs.LG", "cs.CL"], "comment": "NeurIPS 2025. Project page: https://scan-prm.github.io/", "summary": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training.", "AI": {"tldr": "This paper introduces SCAN, a framework for efficient data synthesis and noise-tolerant learning, which allows PRMs to achieve high performance using synthetic data with reduced costs and improved scalability.", "motivation": "The development of PRMs is challenging due to the high cost and limited scalability of human-annotated data, and synthetic data from MC estimation suffers from a high noise ratio, leading to overfitting and hindering large-scale training.", "method": "The paper proposes Self-Denoising Monte Carlo Annotation (SCAN), which includes a self-denoising strategy for generating high-quality annotations and a robust learning strategy for training PRMs from weak supervision.", "result": "SCAN enables PRMs to achieve superior performance with only 6% the inference cost of vanilla MC estimation and achieves a 39.2 F1 score improvement in ProcessBench. The models surpass strong baselines, including those trained on large-scale human-annotated datasets.", "conclusion": "SCAN demonstrates the potential for scalable, cost-efficient, and robust PRM training by effectively utilizing synthetic data with noise-tolerant learning strategies."}}
{"id": "2509.16561", "pdf": "https://arxiv.org/pdf/2509.16561", "abs": "https://arxiv.org/abs/2509.16561", "authors": ["Yue Xin", "Chen Shen", "Shaotian Yan", "Xiaosong Yuan", "Yaoming Wang", "Xiaofeng Zhang", "Chenxi Huang", "Jieping Ye"], "title": "SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "accpeted by EMNLP 2025", "summary": "Chain-of-Thought (CoT) prompting enhances the math reasoning capability of\nlarge language models (LLMs) to a large margin. However, the mechanism\nunderlying such improvements remains unexplored. In this paper, we present\n\\textbf{SalaMAnder} (\\textbf{S}h\\textbf{a}p\\textbf{l}ey-b\\textbf{a}sed\n\\textbf{M}athematical Expression \\textbf{A}ttribution a\\textbf{nd}\nM\\textbf{e}t\\textbf{r}ic), a theoretically grounded methodology as well as a\nmathematically rigorous evaluation metric for quantifying component-level\ncontributions in few-shot CoT reasoning. Concretely, we leverage the Shapley\nvalue for mathematical expression attribution and develop an efficient\nstratified sampling algorithm that significantly reduces the computational\ncomplexity. Besides, we develop the \\textbf{CoSP} (\\textbf{C}ardinality\n\\textbf{o}f \\textbf{S}hapley \\textbf{P}ositives) metric through covariance\nanalysis. Comprehensive validation across popular LLM models and diverse\nmathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder\nframework exhibits a robust monotonic correlation with model performance, not\nonly providing theoretical explanations for the empirical success of existing\nfew-shot CoT but also establishing mathematically rigorous principles for\nprompt construction optimization. Furthermore, we verify the reliability of the\nexplanation, based on which we unify the insights of previous work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SalaMAnder\u65b9\u6cd5\u548cCoSP\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u5c11\u6837\u672cCoT\u63a8\u7406\u4e2d\u7684\u7ec4\u4ef6\u7ea7\u8d21\u732e\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u7a33\u5065\u5355\u8c03\u76f8\u5173\u6027\u3002", "motivation": "Chain-of-Thought (CoT)\u63d0\u793a\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u80cc\u540e\u7684\u673a\u5236\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7406\u8bba\u57fa\u7840\u7684\u65b9\u6cd5\u548c\u6570\u5b66\u4e25\u8c28\u7684\u8bc4\u4f30\u6307\u6807\u6765\u91cf\u5316\u5c11\u6837\u672cCoT\u63a8\u7406\u4e2d\u7684\u7ec4\u4ef6\u7ea7\u8d21\u732e\u3002", "method": "SalaMAnder\u65b9\u6cd5\u57fa\u4e8eShapley\u503c\u8fdb\u884c\u6570\u5b66\u8868\u8fbe\u5f0f\u5f52\u56e0\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5206\u5c42\u91c7\u6837\u7b97\u6cd5\uff0c\u4ee5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u534f\u65b9\u5dee\u5206\u6790\u5f00\u53d1\u4e86CoSP\u6307\u6807\u3002", "result": "\u5728\u6d41\u884c\u7684LLM\u6a21\u578b\u548c\u591a\u6837\u7684\u6570\u5b66\u57fa\u51c6\u4e0a\u7684\u5168\u9762\u9a8c\u8bc1\u8868\u660e\uff0cSalaMAnder\u6846\u67b6\u4e2d\u7684CoSP\u6307\u6807\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u7a33\u5065\u7684\u5355\u8c03\u76f8\u5173\u6027\u3002", "conclusion": "SalaMAnder\u6846\u67b6\u4e2d\u7684CoSP\u6307\u6807\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u7a33\u5065\u7684\u5355\u8c03\u76f8\u5173\u6027\uff0c\u8fd9\u4e0d\u4ec5\u4e3a\u73b0\u6709\u5c11\u6837\u672cCoT\u7684\u5b9e\u8bc1\u6210\u529f\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u8fd8\u4e3a\u63d0\u793a\u6784\u9020\u4f18\u5316\u5efa\u7acb\u4e86\u6570\u5b66\u4e25\u8c28\u7684\u539f\u5219\u3002\u6b64\u5916\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u89e3\u91ca\u7684\u53ef\u9760\u6027\uff0c\u5e76\u7edf\u4e00\u4e86\u4e4b\u524d\u5de5\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.16590", "pdf": "https://arxiv.org/pdf/2509.16590", "abs": "https://arxiv.org/abs/2509.16590", "authors": ["Manuel Borroto", "Katie Gallagher", "Antonio Ielo", "Irfan Kareem", "Francesco Ricca", "Alessandra Russo"], "title": "Question Answering with LLMs and Learning from Answer Sets", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "Under consideration for TPLP journal", "summary": "Large Language Models (LLMs) excel at understanding natural language but\nstruggle with explicit commonsense reasoning. A recent trend of research\nsuggests that the combination of LLM with robust symbolic reasoning systems can\novercome this problem on story-based question answering tasks. In this setting,\nexisting approaches typically depend on human expertise to manually craft the\nsymbolic component. We argue, however, that this component can also be\nautomatically learned from examples. In this work, we introduce LLM2LAS, a\nhybrid system that effectively combines the natural language understanding\ncapabilities of LLMs, the rule induction power of the Learning from Answer Sets\n(LAS) system ILASP, and the formal reasoning strengths of Answer Set\nProgramming (ASP). LLMs are used to extract semantic structures from text,\nwhich ILASP then transforms into interpretable logic rules. These rules allow\nan ASP solver to perform precise and consistent reasoning, enabling correct\nanswers to previously unseen questions. Empirical results outline the strengths\nand weaknesses of our automatic approach for learning and reasoning in a\nstory-based question answering benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7cfb\u7edfLLM2LAS\uff0c\u7ed3\u5408LLM\u3001ILASP\u548cASP\uff0c\u4ee5\u63d0\u9ad8\u6545\u4e8b\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u7f3a\u70b9\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4eba\u5de5\u4e13\u5bb6\u624b\u52a8\u6784\u5efa\u7b26\u53f7\u7ec4\u4ef6\uff0c\u800c\u672c\u6587\u8ba4\u4e3a\u8be5\u7ec4\u4ef6\u4e5f\u53ef\u4ee5\u901a\u8fc7\u793a\u4f8b\u81ea\u52a8\u5b66\u4e60\u3002", "method": "LLM2LAS\u7ed3\u5408\u4e86LLM\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3001ILASP\u7684\u89c4\u5219\u5f52\u7eb3\u80fd\u529b\u548cASP\u7684\u6b63\u5f0f\u63a8\u7406\u4f18\u52bf\u3002LLM\u7528\u4e8e\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u8bed\u4e49\u7ed3\u6784\uff0cILASP\u5c06\u5176\u8f6c\u6362\u4e3a\u53ef\u89e3\u91ca\u7684\u903b\u8f91\u89c4\u5219\uff0c\u7136\u540e\u7531ASP\u6c42\u89e3\u5668\u8fdb\u884c\u7cbe\u786e\u4e14\u4e00\u81f4\u7684\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86LLM2LAS\u5728\u57fa\u4e8e\u6545\u4e8b\u7684\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u81ea\u52a8\u5b66\u4e60\u548c\u63a8\u7406\u7684\u4f18\u52bf\u4e0e\u4e0d\u8db3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u7b26\u53f7\u63a8\u7406\u7cfb\u7edf\u7684\u6df7\u5408\u7cfb\u7edfLLM2LAS\uff0c\u4ee5\u63d0\u9ad8\u6545\u4e8b\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.16621", "pdf": "https://arxiv.org/pdf/2509.16621", "abs": "https://arxiv.org/abs/2509.16621", "authors": ["Hiun Kim", "Tae Kwan Lee", "Taeryun Won"], "title": "The Role of Vocabularies in Learning Sparse Representations for Ranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Learned Sparse Retrieval (LSR) such as SPLADE has growing interest for\neffective semantic 1st stage matching while enjoying the efficiency of inverted\nindices. A recent work on learning SPLADE models with expanded vocabularies\n(ESPLADE) was proposed to represent queries and documents into a sparse space\nof custom vocabulary which have different levels of vocabularic granularity.\nWithin this effort, however, there have not been many studies on the role of\nvocabulary in SPLADE models and their relationship to retrieval efficiency and\neffectiveness.\n  To study this, we construct BERT models with 100K-sized output vocabularies,\none initialized with the ESPLADE pretraining method and one initialized\nrandomly. After finetune on real-world search click logs, we applied logit\nscore-based queries and documents pruning to max size for further balancing\nefficiency. The experimental result in our evaluation set shows that, when\npruning is applied, the two models are effective compared to the 32K-sized\nnormal SPLADE model in the computational budget under the BM25. And the ESPLADE\nmodels are more effective than the random vocab model, while having a similar\nretrieval cost.\n  The result indicates that the size and pretrained weight of output\nvocabularies play the role of configuring the representational specification\nfor queries, documents, and their interactions in the retrieval engine, beyond\ntheir original meaning and purposes in NLP. These findings can provide a new\nroom for improvement for LSR by identifying the importance of representational\nspecification from vocabulary configuration for efficient and effective\nretrieval.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8bcd\u6c47\u5728SPLADE\u6a21\u578b\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u5bf9\u68c0\u7d22\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8f93\u51fa\u8bcd\u6c47\u8868\u7684\u5927\u5c0f\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u5728\u68c0\u7d22\u5f15\u64ce\u4e2d\u5bf9\u67e5\u8be2\u3001\u6587\u6863\u53ca\u5176\u4ea4\u4e92\u7684\u8868\u793a\u89c4\u8303\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002\u8fd9\u4e9b\u53d1\u73b0\u53ef\u4ee5\u4e3aLSR\u63d0\u4f9b\u4e00\u4e2a\u65b0\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u7ecf\u63d0\u51fa\u4e86ESPLADE\u7b49\u65b9\u6cd5\u6765\u6269\u5c55\u8bcd\u6c47\u8868\uff0c\u4f46\u5173\u4e8e\u8bcd\u6c47\u5728SPLADE\u6a21\u578b\u4e2d\u7684\u4f5c\u7528\u4ee5\u53ca\u5b83\u4eec\u4e0e\u68c0\u7d22\u6548\u7387\u548c\u6548\u679c\u7684\u5173\u7cfb\u7684\u7814\u7a76\u4ecd\u7136\u6709\u9650\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8bcd\u6c47\u5728SPLADE\u6a21\u578b\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u5bf9\u68c0\u7d22\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u6211\u4eec\u6784\u5efa\u4e86\u5177\u6709100K\u5927\u5c0f\u8f93\u51fa\u8bcd\u6c47\u8868\u7684BERT\u6a21\u578b\uff0c\u5176\u4e2d\u4e00\u4e2a\u4f7f\u7528ESPLADE\u9884\u8bad\u7ec3\u65b9\u6cd5\u521d\u59cb\u5316\uff0c\u53e6\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u3002\u5728\u771f\u5b9e\u4e16\u754c\u7684\u641c\u7d22\u70b9\u51fb\u65e5\u5fd7\u4e0a\u5fae\u8c03\u540e\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u57fa\u4e8elogit\u5206\u6570\u7684\u67e5\u8be2\u548c\u6587\u6863\u526a\u679d\u4ee5\u8fdb\u4e00\u6b65\u5e73\u8861\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u5e94\u7528\u526a\u679d\u65f6\uff0c\u4e24\u4e2a\u6a21\u578b\u5728\u8ba1\u7b97\u9884\u7b97\u4e0b\u6bd432K\u5927\u5c0f\u7684\u666e\u901aSPLADE\u6a21\u578b\u66f4\u6709\u6548\u3002ESPLADE\u6a21\u578b\u6bd4\u968f\u673a\u8bcd\u6c47\u6a21\u578b\u66f4\u6709\u6548\uff0c\u540c\u65f6\u5177\u6709\u76f8\u4f3c\u7684\u68c0\u7d22\u6210\u672c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8f93\u51fa\u8bcd\u6c47\u8868\u7684\u5927\u5c0f\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u5728\u68c0\u7d22\u5f15\u64ce\u4e2d\u5bf9\u67e5\u8be2\u3001\u6587\u6863\u53ca\u5176\u4ea4\u4e92\u7684\u8868\u793a\u89c4\u8303\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u8d85\u8d8a\u4e86\u5b83\u4eec\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u539f\u59cb\u610f\u4e49\u548c\u76ee\u7684\u3002\u8fd9\u4e9b\u53d1\u73b0\u53ef\u4ee5\u4e3aLSR\u63d0\u4f9b\u4e00\u4e2a\u65b0\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u901a\u8fc7\u8bc6\u522b\u8bcd\u6c47\u914d\u7f6e\u4e2d\u8868\u793a\u89c4\u8303\u7684\u91cd\u8981\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u6709\u6548\u7684\u68c0\u7d22\u3002"}}
{"id": "2509.16633", "pdf": "https://arxiv.org/pdf/2509.16633", "abs": "https://arxiv.org/abs/2509.16633", "authors": ["Abhirama Subramanyam Penamakuri", "Navlika Singh", "Piyush Arora", "Anand Mishra"], "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to EMNLP (Main) 2025", "summary": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable\nperformance in various vision and language tasks, including visual question\nanswering (VQA). However, their high computational cost makes them impractical\nfor resource-constrained settings and inference-heavy applications. In\ncontrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer\nfrom a significant performance gap compared to their larger counterparts. In\nthis work, we introduce the Model Parity Aligner (MPA), a novel framework\ndesigned to systematically improve S-VLMs by leveraging unlabeled images and\neffective knowledge transfer from L-VLMs. Instead of traditional knowledge\ndistillation methods that rely on labeled training data, MPA employs a\nstrategic parity-based approach that precisely identifies the knowledge\ndisparities between S-VLMs and L-VLMs, and optimizes training by targeting only\nthese disparities. We conduct extensive experiments on four diverse VQA\nbenchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires\nspecialized reasoning capabilities such as text recognition, chart\ninterpretation, and commonsense and factual understanding. Our results\ndemonstrate that MPA consistently enhances the performance of S-VLMs on all\nbenchmarks, reducing the performance gap while maintaining computational\nefficiency. We make our code publicly available.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aModel Parity Aligner (MPA)\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5229\u7528\u672a\u6807\u8bb0\u56fe\u50cf\u548c\u4ece\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08L-VLMs\uff09\u4e2d\u6709\u6548\u77e5\u8bc6\u8fc1\u79fb\u6765\u7cfb\u7edf\u5730\u6539\u8fdb\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08S-VLMs\uff09\u3002MPA\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u8861\u7684\u7b56\u7565\uff0c\u7cbe\u786e\u8bc6\u522bS-VLMs\u548cL-VLMs\u4e4b\u95f4\u7684\u77e5\u8bc6\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u9488\u5bf9\u6027\u4f18\u5316\u8bad\u7ec3\u6765\u63d0\u5347\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMPA\u5728\u591a\u4e2aVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86S-VLMs\u7684\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86\u4e0e\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable performance in various vision and language tasks, including visual question answering (VQA). However, their high computational cost makes them impractical for resource-constrained settings and inference-heavy applications. In contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer from a significant performance gap compared to their larger counterparts.", "method": "Model Parity Aligner (MPA), a novel framework designed to systematically improve S-VLMs by leveraging unlabeled images and effective knowledge transfer from L-VLMs. It employs a strategic parity-based approach that precisely identifies the knowledge disparities between S-VLMs and L-VLMs, and optimizes training by targeting only these disparities.", "result": "MPA consistently enhances the performance of S-VLMs on all benchmarks, reducing the performance gap while maintaining computational efficiency.", "conclusion": "MPA consistently enhances the performance of S-VLMs on all benchmarks, reducing the performance gap while maintaining computational efficiency."}}
{"id": "2509.16648", "pdf": "https://arxiv.org/pdf/2509.16648", "abs": "https://arxiv.org/abs/2509.16648", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted in the Findings of EMNLP, 2025", "summary": "The accurate trust assessment of multimodal large language models (MLLMs)\ngenerated predictions, which can enable selective prediction and improve user\nconfidence, is challenging due to the diverse multi-modal input paradigms. We\npropose Functionally Equivalent Sampling for Trust Assessment (FESTA), a\nmultimodal input sampling technique for MLLMs, that generates an uncertainty\nmeasure based on the equivalent and complementary input samplings. The proposed\ntask-preserving sampling approach for uncertainty quantification expands the\ninput space to probe the consistency (through equivalent samples) and\nsensitivity (through complementary samples) of the model. FESTA uses only\ninput-output access of the model (black-box), and does not require ground truth\n(unsupervised). The experiments are conducted with various off-the-shelf\nmulti-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA\nuncertainty estimate achieves significant improvement (33.3% relative\nimprovement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in\nselective prediction performance, based on\narea-under-receiver-operating-characteristic curve (AUROC) metric in detecting\nmispredictions. The code implementation is open-sourced.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFESTA\u7684\u591a\u6a21\u6001\u8f93\u5165\u91c7\u6837\u6280\u672f\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u4fe1\u4efb\u5ea6\u3002FESTA\u901a\u8fc7\u7b49\u6548\u548c\u4e92\u8865\u7684\u8f93\u5165\u91c7\u6837\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\uff0c\u4ec5\u9700\u8f93\u5165\u8f93\u51fa\u8bbf\u95ee\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFESTA\u5728\u9009\u62e9\u6027\u9884\u6d4b\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u51c6\u786e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u751f\u6210\u7684\u9884\u6d4b\u7684\u4fe1\u4efb\u5ea6\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u591a\u6a21\u6001\u8f93\u5165\u8303\u5f0f\u591a\u6837\u3002\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\u6765\u63d0\u9ad8\u7528\u6237\u4fe1\u5fc3\u5e76\u5b9e\u73b0\u9009\u62e9\u6027\u9884\u6d4b\u3002", "method": "FESTA\u662f\u4e00\u79cd\u591a\u6a21\u6001\u8f93\u5165\u91c7\u6837\u6280\u672f\uff0c\u901a\u8fc7\u7b49\u6548\u548c\u4e92\u8865\u7684\u8f93\u5165\u91c7\u6837\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u4ec5\u8f93\u5165\u8f93\u51fa\u8bbf\u95ee\u6a21\u578b\uff08\u9ed1\u76d2\uff09\uff0c\u4e0d\u9700\u8981\u771f\u5b9e\u6807\u7b7e\uff08\u65e0\u76d1\u7763\uff09\u3002", "result": "FESTA\u5728\u89c6\u89c9\u548c\u97f3\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5728\u68c0\u6d4b\u9519\u8bef\u9884\u6d4b\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff08\u89c6\u89c9LLMs\u76f8\u5bf9\u6539\u8fdb33.3%\uff0c\u97f3\u9891LLMs\u76f8\u5bf9\u6539\u8fdb29.6%\uff09\u3002", "conclusion": "FESTA\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5728\u9009\u62e9\u6027\u9884\u6d4b\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u8868\u660e\u5176\u5728\u68c0\u6d4b\u9519\u8bef\u9884\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.16718", "pdf": "https://arxiv.org/pdf/2509.16718", "abs": "https://arxiv.org/abs/2509.16718", "authors": ["Vishnu Raja", "Adithya V Ganesan", "Anand Syamkumar", "Ritwik Banerjee", "H Andrew Schwartz"], "title": "Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition: Dysarthric Case Studies", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Will appear in EMNLP 2025 Main Proceedings", "summary": "State-of-the-art automatic speech recognition (ASR) models like Whisper,\nperform poorly on atypical speech, such as that produced by individuals with\ndysarthria. Past works for atypical speech have mostly investigated fully\npersonalized (or idiosyncratic) models, but modeling strategies that can both\ngeneralize and handle idiosyncracy could be more effective for capturing\natypical speech. To investigate this, we compare four strategies: (a)\n$\\textit{normative}$ models trained on typical speech (no personalization), (b)\n$\\textit{idiosyncratic}$ models completely personalized to individuals, (c)\n$\\textit{dysarthric-normative}$ models trained on other dysarthric speakers,\nand (d) $\\textit{dysarthric-idiosyncratic}$ models which combine strategies by\nfirst modeling normative patterns before adapting to individual speech. In this\ncase study, we find the dysarthric-idiosyncratic model performs better than\nidiosyncratic approach while requiring less than half as much personalized data\n(36.43 WER with 128 train size vs 36.99 with 256). Further, we found that\ntuning the speech encoder alone (as opposed to the LM decoder) yielded the best\nresults reducing word error rate from 71% to 32% on average. Our findings\nhighlight the value of leveraging both normative (cross-speaker) and\nidiosyncratic (speaker-specific) patterns to improve ASR for underrepresented\nspeech populations.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7b56\u7565\uff0c\u4ee5\u6539\u5584\u5931\u8bed\u75c7\u60a3\u8005\u7684\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u89c4\u8303\u548c\u4e2a\u6027\u5316\u7684\u6a21\u578b\u5728\u51cf\u5c11\u8bcd\u9519\u8bef\u7387\u65b9\u9762\u6548\u679c\u66f4\u597d\uff0c\u4e14\u6240\u9700\u4e2a\u6027\u5316\u6570\u636e\u66f4\u5c11\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u5728\u5904\u7406\u975e\u5178\u578b\u7684\u8bed\u97f3\uff08\u5982\u5931\u8bed\u75c7\u60a3\u8005\u4ea7\u751f\u7684\u8bed\u97f3\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u8fc7\u53bb\u7684\u5173\u4e8e\u975e\u5178\u578b\u8bed\u97f3\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u5b8c\u5168\u4e2a\u6027\u5316\u7684\uff08\u6216\u72ec\u7279\u7684\uff09\u6a21\u578b\u4e0a\uff0c\u4f46\u80fd\u591f\u540c\u65f6\u6cdb\u5316\u548c\u5904\u7406\u72ec\u7279\u6027\u7684\u5efa\u6a21\u7b56\u7565\u53ef\u80fd\u66f4\u6709\u6548\u3002", "method": "\u6211\u4eec\u6bd4\u8f83\u4e86\u56db\u79cd\u7b56\u7565\uff1a(a) \u5728\u5178\u578b\u8bed\u97f3\u4e0a\u8bad\u7ec3\u7684\u89c4\u8303\u6a21\u578b\uff08\u65e0\u4e2a\u6027\u5316\uff09\uff0c(b) \u5b8c\u5168\u4e2a\u6027\u5316\u7684\u4e2a\u4f53\u6a21\u578b\uff0c(c) \u5728\u5176\u4ed6\u5931\u8bed\u75c7\u8bf4\u8bdd\u4eba\u4e0a\u8bad\u7ec3\u7684\u5931\u8bed\u75c7\u89c4\u8303\u6a21\u578b\uff0c\u4ee5\u53ca(d) \u7ed3\u5408\u7b56\u7565\u7684\u5931\u8bed\u75c7\u4e2a\u6027\u5316\u6a21\u578b\uff0c\u9996\u5148\u5efa\u6a21\u89c4\u8303\u6a21\u5f0f\uff0c\u7136\u540e\u9002\u5e94\u4e2a\u4f53\u8bed\u97f3\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0c\u5931\u8bed\u75c7\u4e2a\u6027\u5316\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u6240\u9700\u7684\u4e2a\u6027\u5316\u6570\u636e\u4e0d\u5230\u4e00\u534a\uff0836.43 WER\u4e0e128\u8bad\u7ec3\u96c6\u5927\u5c0f\u76f8\u6bd436.99\u4e0e256\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u4ec5\u8c03\u6574\u8bed\u97f3\u7f16\u7801\u5668\uff08\u800c\u4e0d\u662f\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u5668\uff09\u4ea7\u751f\u4e86\u6700\u4f73\u7ed3\u679c\uff0c\u5e73\u5747\u5c06\u8bcd\u9519\u8bef\u7387\u4ece71%\u964d\u4f4e\u523032%\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5229\u7528\u89c4\u8303\uff08\u8de8\u8bf4\u8bdd\u4eba\uff09\u548c\u4e2a\u6027\u5316\u7684\uff08\u8bf4\u8bdd\u4eba\u7279\u5b9a\uff09\u6a21\u5f0f\u6765\u6539\u5584\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u97f3\u7fa4\u4f53\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.16769", "pdf": "https://arxiv.org/pdf/2509.16769", "abs": "https://arxiv.org/abs/2509.16769", "authors": ["Prasanth K K", "Shubham Sharma"], "title": "Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05, 62H30, 62M45", "I.2.6; I.5.1; I.5.2; G.3"], "comment": "21 pages, 6 figures, 14 tables", "summary": "Many real world categories are multimodal, with single classes occupying\ndisjoint regions in feature space. Classical linear models (logistic\nregression, linear SVM) use a single global hyperplane and perform poorly on\nsuch data, while high-capacity methods (kernel SVMs, deep nets) fit multimodal\nstructure but at the expense of interpretability, heavier tuning, and higher\ncomputational cost. We propose the Geometric Mixture Classifier (GMC), a\ndiscriminative model that represents each class as a mixture of hyperplanes.\nWithin each class, GMC combines plane scores via a temperature-controlled\nsoft-OR (log-sum-exp), smoothly approximating the max; across classes, standard\nsoftmax yields probabilistic posteriors. GMC optionally uses Random Fourier\nFeatures (RFF) for nonlinear mappings while keeping inference linear in the\nnumber of planes and features. Our practical training recipe: geometry-aware\nk-means initialization, silhouette-based plane budgeting, alpha annealing,\nusage-aware L2 regularization, label smoothing, and early stopping, makes GMC\nplug-and-play. Across synthetic multimodal datasets (moons, circles, blobs,\nspirals) and tabular/image benchmarks (iris, wine, WDBC, digits), GMC\nconsistently outperforms linear baselines and k-NN, is competitive with\nRBF-SVM, Random Forests, and small MLPs, and provides geometric introspection\nvia per-plane and class responsibility visualizations. Inference scales\nlinearly in planes and features, making GMC CPU-friendly, with single-digit\nmicrosecond latency per example, often faster than RBF-SVM and compact MLPs.\nPost-hoc temperature scaling reduces ECE from about 0.06 to 0.02. GMC thus\nstrikes a favorable balance of accuracy, interpretability, and efficiency: it\nis more expressive than linear models and lighter, more transparent, and faster\nthan kernel or deep models.", "AI": {"tldr": "GMC is a discriminative model that represents each class as a mixture of hyperplanes, providing a balance between accuracy, interpretability, and efficiency.", "motivation": "Classical linear models perform poorly on multimodal data, while high-capacity methods lack interpretability, require heavy tuning, and have higher computational costs. GMC aims to provide a discriminative model that balances expressiveness, interpretability, and efficiency.", "method": "GMC represents each class as a mixture of hyperplanes, combining plane scores via a temperature-controlled soft-OR (log-sum-exp) and using standard softmax across classes. It optionally uses Random Fourier Features (RFF) for nonlinear mappings while keeping inference linear in the number of planes and features.", "result": "GMC consistently outperforms linear baselines and k-NN, is competitive with RBF-SVM, Random Forests, and small MLPs, and provides geometric introspection via visualizations. It scales linearly in planes and features, making it CPU-friendly with fast inference.", "conclusion": "GMC strikes a favorable balance of accuracy, interpretability, and efficiency: it is more expressive than linear models and lighter, more transparent, and faster than kernel or deep models."}}
{"id": "2509.16866", "pdf": "https://arxiv.org/pdf/2509.16866", "abs": "https://arxiv.org/abs/2509.16866", "authors": ["Mohammad Ramezanali", "Mo Vazifeh", "Paolo Santi"], "title": "seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce seqBench, a parametrized benchmark for probing sequential\nreasoning limits in Large Language Models (LLMs) through precise,\nmulti-dimensional control over several key complexity dimensions. seqBench\nallows systematic variation of (1) the logical depth, defined as the number of\nsequential actions required to solve the task; (2) the number of backtracking\nsteps along the optimal path, quantifying how often the agent must revisit\nprior states to satisfy deferred preconditions (e.g., retrieving a key after\nencountering a locked door); and (3) the noise ratio, defined as the ratio\nbetween supporting and distracting facts about the environment. Our evaluations\non state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses\nexponentially beyond a model-specific logical depth. Unlike existing\nbenchmarks, seqBench's fine-grained control facilitates targeted analyses of\nthese reasoning failures, illuminating universal scaling laws and statistical\nlimits, as detailed in this paper alongside its generation methodology and\nevaluation metrics. We find that even top-performing models systematically fail\non seqBench's structured reasoning tasks despite minimal search complexity,\nunderscoring key limitations in their commonsense reasoning capabilities.\nDesigned for future evolution to keep pace with advancing models, the seqBench\ndatasets are publicly released to spur deeper scientific inquiry into LLM\nreasoning, aiming to establish a clearer understanding of their true potential\nand current boundaries for robust real-world application.", "AI": {"tldr": "seqBench is a benchmark for probing sequential reasoning limits in LLMs through precise control over logical depth, backtracking steps, and noise ratio. Evaluations show that LLMs fail systematically on these tasks, highlighting key limitations in their commonsense reasoning capabilities.", "motivation": "To address the limitations in LLMs' commonsense reasoning capabilities and provide a benchmark for systematic analysis of their sequential reasoning failures.", "method": "seqBench is a parametrized benchmark that allows systematic variation of logical depth, backtracking steps, and noise ratio to probe sequential reasoning limits in LLMs.", "result": "Evaluations on state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses exponentially beyond a model-specific logical depth. Even top-performing models systematically fail on seqBench's structured reasoning tasks despite minimal search complexity.", "conclusion": "seqBench datasets are publicly released to spur deeper scientific inquiry into LLM reasoning, aiming to establish a clearer understanding of their true potential and current boundaries for robust real-world application."}}
{"id": "2509.16882", "pdf": "https://arxiv.org/pdf/2509.16882", "abs": "https://arxiv.org/abs/2509.16882", "authors": ["Junzhuo Li", "Bo Wang", "Xiuze Zhou", "Xuming Hu"], "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated\nexpert subnetworks, yet adapting them to multiple domains without catastrophic\nforgetting remains an open challenge. Existing approaches either incur\nprohibitive computation, suffer cross-domain interference, or require separate\nruns per domain. We propose DES-MoE, a dynamic expert specialization framework\nfor multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses\ncatastrophic forgetting through three innovations: (1) an adaptive router\nbalancing pre-trained knowledge retention and task-specific updates via\ndistillation, (2) real-time expert-domain correlation mapping to isolate\ndomain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule\nthat progressively freezes non-specialized parameters. Evaluated on six domains\n(math, code, law, etc.), DES-MoE matches single-domain ESFT performance while\ntraining one unified model, reduces forgetting by 89% compared to full\nfine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence\nthan conventional methods. Our work establishes dynamic expert isolation as a\nscalable paradigm for multi-task MoE adaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDES-MoE\u7684\u52a8\u6001\u4e13\u5bb6\u4e13\u4e1a\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u9886\u57df\u9002\u5e94Mixture-of-Experts\u6a21\u578b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u521b\u65b0\u6765\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff1a(1) \u4e00\u79cd\u81ea\u9002\u5e94\u8def\u7531\u5668\uff0c\u901a\u8fc7\u84b8\u998f\u5e73\u8861\u9884\u8bad\u7ec3\u77e5\u8bc6\u4fdd\u7559\u548c\u4efb\u52a1\u7279\u5b9a\u66f4\u65b0\uff1b(2) \u5b9e\u65f6\u4e13\u5bb6-\u9886\u57df\u76f8\u5173\u6027\u6620\u5c04\u4ee5\u9694\u79bb\u9886\u57df\u7279\u5b9a\u68af\u5ea6\uff1b(3) \u4e00\u4e2a\u4e09\u9636\u6bb5\u81ea\u9002\u5e94\u5fae\u8c03\u8ba1\u5212\uff0c\u9010\u6b65\u51bb\u7ed3\u975e\u4e13\u4e1a\u53c2\u6570\u3002\u5728\u516d\u4e2a\u9886\u57df\u4e0a\u8bc4\u4f30\uff0cDES-MoE\u4e0e\u5355\u57dfESFT\u6027\u80fd\u76f8\u5339\u914d\uff0c\u540c\u65f6\u8bad\u7ec3\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\uff0c\u5728\u9886\u57df\u4ece2\u52306\u6269\u5c55\u65f6\uff0c\u4e0e\u5b8c\u6574\u5fae\u8c03\u76f8\u6bd4\uff0c\u9057\u5fd8\u51cf\u5c11\u4e8689%\uff0c\u5e76\u4e14\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb68%\u7684\u6536\u655b\u901f\u5ea6\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u786e\u7acb\u4e86\u52a8\u6001\u4e13\u5bb6\u9694\u79bb\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u4efb\u52a1MoE\u9002\u5e94\u8303\u5f0f\u3002", "motivation": "Mixture-of-Experts (MoE) \u6a21\u578b\u901a\u8fc7\u7a00\u758f\u95e8\u63a7\u4e13\u5bb6\u5b50\u7f51\u7edc\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u5bb9\u91cf\uff0c\u4f46\u5c06\u5b83\u4eec\u9002\u5e94\u5230\u591a\u4e2a\u9886\u57df\u800c\u4e0d\u4ea7\u751f\u707e\u96be\u6027\u9057\u5fd8\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u8981\u4e48\u5e26\u6765\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u8981\u4e48\u906d\u53d7\u8de8\u9886\u57df\u5e72\u6270\uff0c\u6216\u8005\u9700\u8981\u4e3a\u6bcf\u4e2a\u9886\u57df\u5355\u72ec\u8fd0\u884c\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86DES-MoE\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u9886\u57df\u9002\u5e94\u7684Mixture-of-Experts\u6a21\u578b\u7684\u52a8\u6001\u4e13\u5bb6\u4e13\u4e1a\u5316\u6846\u67b6\u3002", "result": "\u5728\u516d\u4e2a\u9886\u57df\uff08\u6570\u5b66\u3001\u4ee3\u7801\u3001\u6cd5\u5f8b\u7b49\uff09\u4e0a\u8bc4\u4f30\uff0cDES-MoE\u4e0e\u5355\u57dfESFT\u6027\u80fd\u76f8\u5339\u914d\uff0c\u540c\u65f6\u8bad\u7ec3\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\uff0c\u5728\u9886\u57df\u4ece2\u52306\u6269\u5c55\u65f6\uff0c\u4e0e\u5b8c\u6574\u5fae\u8c03\u76f8\u6bd4\uff0c\u9057\u5fd8\u51cf\u5c11\u4e8689%\uff0c\u5e76\u4e14\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb68%\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u786e\u7acb\u4e86\u52a8\u6001\u4e13\u5bb6\u9694\u79bb\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u4efb\u52a1MoE\u9002\u5e94\u8303\u5f0f\u3002"}}
{"id": "2509.16893", "pdf": "https://arxiv.org/pdf/2509.16893", "abs": "https://arxiv.org/abs/2509.16893", "authors": ["Faramarz Farhangian", "Leandro A. Ensina", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "DRES: Fake news detection by dynamic representation and ensemble selection", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted as oral presentation at EMNLP 2025", "summary": "The rapid spread of information via social media has made text-based fake\nnews detection critically important due to its societal impact. This paper\npresents a novel detection method called Dynamic Representation and Ensemble\nSelection (DRES) for identifying fake news based solely on text. DRES leverages\ninstance hardness measures to estimate the classification difficulty for each\nnews article across multiple textual feature representations. By dynamically\nselecting the textual representation and the most competent ensemble of\nclassifiers for each instance, DRES significantly enhances prediction accuracy.\nExtensive experiments show that DRES achieves notable improvements over\nstate-of-the-art methods, confirming the effectiveness of representation\nselection based on instance hardness and dynamic ensemble selection in boosting\nperformance. Codes and data are available at:\nhttps://github.com/FFarhangian/FakeNewsDetection_DRES", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRES\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6587\u672c\u8868\u793a\u548c\u5206\u7c7b\u5668\u96c6\u5408\u6765\u63d0\u9ad8\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u793e\u4ea4\u5a92\u4f53\u4e0a\u4fe1\u606f\u7684\u5feb\u901f\u4f20\u64ad\uff0c\u57fa\u4e8e\u6587\u672c\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "DRES\u65b9\u6cd5\u5229\u7528\u5b9e\u4f8b\u96be\u5ea6\u5ea6\u91cf\u6765\u4f30\u8ba1\u6bcf\u7bc7\u65b0\u95fb\u6587\u7ae0\u5728\u591a\u79cd\u6587\u672c\u7279\u5f81\u8868\u793a\u4e0b\u7684\u5206\u7c7b\u96be\u5ea6\uff0c\u5e76\u52a8\u6001\u9009\u62e9\u6587\u672c\u8868\u793a\u548c\u6700\u64c5\u957f\u7684\u5206\u7c7b\u5668\u96c6\u5408\u3002", "result": "DRES\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DRES\u901a\u8fc7\u57fa\u4e8e\u5b9e\u4f8b\u96be\u5ea6\u7684\u8868\u793a\u9009\u62e9\u548c\u52a8\u6001\u96c6\u6210\u9009\u62e9\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.16941", "pdf": "https://arxiv.org/pdf/2509.16941", "abs": "https://arxiv.org/abs/2509.16941", "authors": ["Xiang Deng", "Jeff Da", "Edwin Pan", "Yannis Yiming He", "Charles Ide", "Kanak Garg", "Niklas Lauffer", "Andrew Park", "Nitin Pasari", "Chetan Rane", "Karmini Sampath", "Maya Krishnan", "Srivatsa Kundurthy", "Sean Hendryx", "Zifan Wang", "Chen Bo Calvin Zhang", "Noah Jacobson", "Bing Liu", "Brad Kenstler"], "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that\nbuilds upon the best practices of SWE-BENCH [25], but is explicitly designed to\ncapture realistic, complex, enterprise-level problems beyond the scope of\nSWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of\n41 actively maintained repositories spanning business applications, B2B\nservices, and developer tools. The benchmark is partitioned into a public set\nwith open access to problems sourced from 11 repositories, a held-out set of 12\nrepositories and a commercial set of 18 proprietary repositories where we have\nformal partnership agreements with early-stage startups. Problems in the\nheld-out and the commercial set are not publicly accessible, but we release\nresults on the commercial set. Our benchmark features long-horizon tasks that\nmay require hours to days for a professional software engineer to complete,\noften involving patches across multiple files and substantial code\nmodifications. All tasks are human-verified and augmented with sufficient\ncontext to ensure resolvability. In our evaluation of widely used coding\nmodels, under a unified scaffold, we observe that their performance on\nSWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest\nscore to date at 23.3%. To better understand these limitations, we cluster the\nfailure modes observed in the collected agent trajectories for a clearer\ncharacterization of the error patterns exhibited by current models. Overall,\nSWE-BENCH PRO provides a contamination-resistant testbed that more faithfully\ncaptures the complexity and diversity of real-world software development,\nadvancing the pursuit of truly autonomous software engineering agents at a\nprofessional level.", "AI": {"tldr": "SWE-Bench Pro is a new benchmark for evaluating coding models, designed to capture realistic, complex, enterprise-level problems. It contains 1,865 problems from various repositories and is partitioned into public, held-out, and commercial sets. Evaluation shows that current models perform poorly on this benchmark.", "motivation": "To create a more challenging benchmark that captures realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH, and to provide a contamination-resistant testbed for evaluating coding models.", "method": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH, but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. The benchmark is partitioned into a public set, a held-out set, and a commercial set.", "result": "Our evaluation of widely used coding models shows that their performance on SWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest score to date at 23.3%. We also cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models.", "conclusion": "SWE-Bench PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level."}}
{"id": "2509.17070", "pdf": "https://arxiv.org/pdf/2509.17070", "abs": "https://arxiv.org/abs/2509.17070", "authors": ["Mayukh Borana", "Junyi Liang", "Sai Sathiesh Rajan", "Sudipta Chattopadhyay"], "title": "Localizing Malicious Outputs from CodeLLM", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "10 pages, 2 figures, 6 tables, Accepted at EMNLP 2025 Findings", "summary": "We introduce FreqRank, a mutation-based defense to localize malicious\ncomponents in LLM outputs and their corresponding backdoor triggers. FreqRank\nassumes that the malicious sub-string(s) consistently appear in outputs for\ntriggered inputs and uses a frequency-based ranking system to identify them.\nOur ranking system then leverages this knowledge to localize the backdoor\ntriggers present in the inputs. We create nine malicious models through\nfine-tuning or custom instructions for three downstream tasks, namely, code\ncompletion (CC), code generation (CG), and code summarization (CS), and show\nthat they have an average attack success rate (ASR) of 86.6%. Furthermore,\nFreqRank's ranking system highlights the malicious outputs as one of the top\nfive suggestions in 98% of cases. We also demonstrate that FreqRank's\neffectiveness scales as the number of mutants increases and show that FreqRank\nis capable of localizing the backdoor trigger effectively even with a limited\nnumber of triggered samples. Finally, we show that our approach is 35-50% more\neffective than other defense methods.", "AI": {"tldr": "FreqRank \u662f\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u6392\u540d\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9a\u4f4d LLM \u8f93\u51fa\u4e2d\u7684\u6076\u610f\u7ec4\u4ef6\u548c\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u6210\u529f\u7387\u548c\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9632\u5fa1\u65b9\u6cd5\u5728\u5b9a\u4f4d\u540e\u95e8\u89e6\u53d1\u5668\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u6709\u9650\u7684\u89e6\u53d1\u6837\u672c\u60c5\u51b5\u4e0b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "FreqRank \u57fa\u4e8e\u9891\u7387\u6392\u540d\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u6076\u610f\u5b50\u5b57\u7b26\u4e32\u5728\u8f93\u51fa\u4e2d\u7684\u51fa\u73b0\u9891\u7387\u6765\u5b9a\u4f4d\u6076\u610f\u7ec4\u4ef6\u548c\u540e\u95e8\u89e6\u53d1\u5668\u3002", "result": "FreqRank \u5728\u4e5d\u4e2a\u6076\u610f\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u4e3a 86.6%\uff0c\u5e76\u4e14\u5728 98% \u7684\u60c5\u51b5\u4e0b\u5c06\u6076\u610f\u8f93\u51fa\u5217\u4e3a\u524d\u4e94\u540d\u5efa\u8bae\u4e4b\u4e00\u3002\u6b64\u5916\uff0cFreqRank \u7684\u6548\u679c\u968f\u7740\u7a81\u53d8\u6570\u91cf\u7684\u589e\u52a0\u800c\u63d0\u5347\u3002", "conclusion": "FreqRank \u662f\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6709\u9650\u7684\u89e6\u53d1\u6837\u672c\u4e0b\u51c6\u786e\u5b9a\u4f4d\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u5e76\u4e14\u6bd4\u5176\u4ed6\u9632\u5fa1\u65b9\u6cd5\u66f4\u6709\u6548\u3002"}}
{"id": "2509.17091", "pdf": "https://arxiv.org/pdf/2509.17091", "abs": "https://arxiv.org/abs/2509.17091", "authors": ["Massa Baali", "Sarthak Bisht", "Francisco Teixeira", "Kateryna Shapovalenko", "Rita Singh", "Bhiksha Raj"], "title": "SVeritas: Benchmark for Robust Speaker Verification under Diverse Conditions", "categories": ["cs.SD", "cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Speaker verification (SV) models are increasingly integrated into security,\npersonalization, and access control systems, yet their robustness to many\nreal-world challenges remains inadequately benchmarked. These include a variety\nof natural and maliciously created conditions causing signal degradations or\nmismatches between enrollment and test data, impacting performance. Existing\nbenchmarks evaluate only subsets of these conditions, missing others entirely.\nWe introduce SVeritas, a comprehensive Speaker Verification tasks benchmark\nsuite, assessing SV systems under stressors like recording duration,\nspontaneity, content, noise, microphone distance, reverberation, channel\nmismatches, audio bandwidth, codecs, speaker age, and susceptibility to\nspoofing and adversarial attacks. While several benchmarks do exist that each\ncover some of these issues, SVeritas is the first comprehensive evaluation that\nnot only includes all of these, but also several other entirely new, but\nnonetheless important, real-life conditions that have not previously been\nbenchmarked. We use SVeritas to evaluate several state-of-the-art SV models and\nobserve that while some architectures maintain stability under common\ndistortions, they suffer substantial performance degradation in scenarios\ninvolving cross-language trials, age mismatches, and codec-induced compression.\nExtending our analysis across demographic subgroups, we further identify\ndisparities in robustness across age groups, gender, and linguistic\nbackgrounds. By standardizing evaluation under realistic and synthetic stress\nconditions, SVeritas enables precise diagnosis of model weaknesses and\nestablishes a foundation for advancing equitable and reliable speaker\nverification systems.", "AI": {"tldr": "SVeritas\u662f\u4e00\u4e2a\u5168\u9762\u7684\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u57fa\u51c6\u5957\u4ef6\uff0c\u8bc4\u4f30SV\u7cfb\u7edf\u5728\u5404\u79cd\u73b0\u5b9e\u548c\u5408\u6210\u538b\u529b\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5f31\u70b9\uff0c\u5e76\u4e3a\u63d0\u9ad8\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u4ec5\u8bc4\u4f30\u8fd9\u4e9b\u6761\u4ef6\u7684\u5b50\u96c6\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u6761\u4ef6\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30SV\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165\u4e86SVeritas\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u4efb\u52a1\u57fa\u51c6\u5957\u4ef6\uff0c\u8bc4\u4f30SV\u7cfb\u7edf\u5728\u5404\u79cd\u538b\u529b\u56e0\u7d20\u4e0b\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5f55\u97f3\u65f6\u957f\u3001\u81ea\u53d1\u6027\u3001\u5185\u5bb9\u3001\u566a\u58f0\u3001\u9ea6\u514b\u98ce\u8ddd\u79bb\u3001\u6df7\u54cd\u3001\u4fe1\u9053\u4e0d\u5339\u914d\u3001\u97f3\u9891\u5e26\u5bbd\u3001\u7f16\u89e3\u7801\u5668\u3001\u8bf4\u8bdd\u4eba\u5e74\u9f84\u4ee5\u53ca\u5bf9\u6b3a\u9a97\u548c\u5bf9\u6297\u653b\u51fb\u7684\u6613\u611f\u6027\u3002", "result": "\u4f7f\u7528SVeritas\u8bc4\u4f30\u4e86\u51e0\u79cd\u6700\u5148\u8fdb\u7684SV\u6a21\u578b\uff0c\u53d1\u73b0\u867d\u7136\u67d0\u4e9b\u67b6\u6784\u5728\u5e38\u89c1\u5931\u771f\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff0c\u4f46\u5728\u6d89\u53ca\u8de8\u8bed\u8a00\u6d4b\u8bd5\u3001\u5e74\u9f84\u4e0d\u5339\u914d\u548c\u7f16\u89e3\u7801\u5668\u5f15\u8d77\u7684\u538b\u7f29\u7684\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e86\u4e0d\u540c\u5e74\u9f84\u7ec4\u3001\u6027\u522b\u548c\u8bed\u8a00\u80cc\u666f\u4e4b\u95f4\u7684\u9c81\u68d2\u6027\u5dee\u5f02\u3002", "conclusion": "\u901a\u8fc7\u5728\u73b0\u5b9e\u548c\u5408\u6210\u538b\u529b\u6761\u4ef6\u4e0b\u6807\u51c6\u5316\u8bc4\u4f30\uff0cSVeritas\u4f7f\u80fd\u591f\u7cbe\u786e\u8bca\u65ad\u6a21\u578b\u5f31\u70b9\uff0c\u5e76\u4e3a\u63a8\u8fdb\u516c\u5e73\u548c\u53ef\u9760\u7684\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.17158", "pdf": "https://arxiv.org/pdf/2509.17158", "abs": "https://arxiv.org/abs/2509.17158", "authors": ["Pierre Andrews", "Amine Benhalloum", "Gerard Moreno-Torres Bertran", "Matteo Bettini", "Amar Budhiraja", "Ricardo Silveira Cabral", "Virginie Do", "Romain Froger", "Emilien Garreau", "Jean-Baptiste Gaya", "Hugo Lauren\u00e7on", "Maxime Lecanu", "Kunal Malkan", "Dheeraj Mekala", "Pierre M\u00e9nard", "Gr\u00e9goire Mialon", "Ulyana Piterbarg", "Mikhail Plekhanov", "Mathieu Rita", "Andrey Rusakov", "Thomas Scialom", "Vladislav Vorotilov", "Mengjue Wang", "Ian Yu"], "title": "ARE: Scaling Up Agent Environments and Evaluations", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Meta Agents Research Environments (ARE), a research platform for\nscalable creation of environments, integration of synthetic or real\napplications, and execution of agentic orchestrations. ARE provides simple\nabstractions to build complex and diverse environments, each with their own\nrules, tools, content, and verifiers, helping to bridge the gap between model\ndevelopment and real-world deployment. We also propose Gaia2, a benchmark built\nin ARE and designed to measure general agent capabilities. Beyond search and\nexecution, Gaia2 requires agents to handle ambiguities and noise, adapt to\ndynamic environments, collaborate with other agents, and operate under temporal\nconstraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new\nfailure modes that are invisible in static settings. Our experiments show that\nno system dominates across the intelligence spectrum: stronger reasoning often\ncomes at the cost of efficiency, and budget scaling curves plateau,\nhighlighting the need for new architectures and adaptive compute strategies.\nPerhaps more importantly, ARE abstractions enable continuous extension of Gaia2\nto other environments, empowering the community to rapidly create new\nbenchmarks tailored to their domains. In AI's second half, progress\nincreasingly depends on defining meaningful tasks and robust evaluations to\ndrive frontier capabilities forward.", "AI": {"tldr": "ARE is a research platform for creating environments and executing agentic orchestrations, while Gaia2 is a benchmark designed to measure general agent capabilities. The results show that no system dominates across the intelligence spectrum, highlighting the need for new architectures and adaptive compute strategies.", "motivation": "To bridge the gap between model development and real-world deployment, and to define meaningful tasks and robust evaluations to drive frontier capabilities forward in AI.", "method": "We introduce Meta Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities.", "result": "Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies.", "conclusion": "ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In AI's second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward."}}
{"id": "2509.17191", "pdf": "https://arxiv.org/pdf/2509.17191", "abs": "https://arxiv.org/abs/2509.17191", "authors": ["Jinchao Ge", "Tengfei Cheng", "Biao Wu", "Zeyu Zhang", "Shiya Huang", "Judith Bishop", "Gillian Shepherd", "Meng Fang", "Ling Chen", "Yang Zhao"], "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5VaseVL\uff0c\u7528\u4e8e\u63d0\u9ad8MLLMs\u5728\u53e4\u5e0c\u814a\u9676\u5668\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6VaseVQA\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u5206\u6790\u6587\u5316\u9057\u4ea7\u6587\u7269\u5bf9\u4e8eMLLMs\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff1a\u901a\u7528\u6a21\u578b\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800cSFT\u901a\u5e38\u4f1a\u8fc7\u62df\u5408\u8868\u9762\u6a21\u5f0f\uff0c\u5bfc\u81f4\u8ba4\u8bc1\u548c\u5386\u53f2\u5f52\u56e0\u7684\u8106\u5f31\u63a8\u7406\u3002\u8fd9\u5f15\u53d1\u4e86\u5982\u4f55\u4f7fMLLMs\u5177\u5907\u9488\u5bf9\u53e4\u5e0c\u814a\u9676\u5668\u7684\u7a33\u5065\u3001\u4e13\u5bb6\u7ea7\u63a8\u7406\u7684\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86VaseVL\uff0c\u4e00\u4e2aSFT-\u7136\u540e-RL\u7cfb\u7edf\uff0c\u5c06\u8bc4\u4f30\u8f6c\u5316\u4e3a\u76d1\u7763\uff1a\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u95ee\u9898\u7c7b\u578b\u7684\u5206\u7c7b\u6cd5\uff0c\u63a2\u6d4bSFT\u6a21\u578b\u4ee5\u5b9a\u4f4d\u7279\u5b9a\u7c7b\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u4f7f\u7528\u9488\u5bf9\u8fd9\u4e9b\u5dee\u8ddd\u7684\u7c7b\u578b\u6761\u4ef6\u3001\u7ec4\u5408\u5bfc\u5411\u7684\u5956\u52b1\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVaseVL\u5728\u98ce\u683c\u5206\u7c7b\u548c\u5386\u53f2\u5f52\u56e0\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u7ec4\u5408\u9c81\u68d2\u6027\u4e0a\u76f8\u6bd4\u4ec5\u4f7f\u7528SFT\u7684\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cVaseVL\u5728\u98ce\u683c\u5206\u7c7b\u548c\u5386\u53f2\u5f52\u56e0\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u7ec4\u5408\u9c81\u68d2\u6027\u4e0a\u76f8\u6bd4\u4ec5\u4f7f\u7528SFT\u7684\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u8bca\u65ad\u7684\u3001\u57fa\u4e8e\u5206\u7c7b\u6cd5\u7684\u5956\u52b1\u5de5\u7a0b\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u8d44\u6e90\u3002"}}
{"id": "2509.17228", "pdf": "https://arxiv.org/pdf/2509.17228", "abs": "https://arxiv.org/abs/2509.17228", "authors": ["Zihan Liang", "Ziwen Pan", "Ruoxuan Xiong"], "title": "Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness", "categories": ["cs.LG", "cs.CL", "stat.ME"], "comment": "To appear in Proc. of EMNLP 2025 (18 pages)", "summary": "Clinical notes contain rich patient information, such as diagnoses or\nmedications, making them valuable for patient representation learning. Recent\nadvances in large language models have further improved the ability to extract\nmeaningful representations from clinical texts. However, clinical notes are\noften missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% of\npatients have no available discharge summaries. In such cases, representations\ncan be learned from other modalities such as structured data, chest X-rays, or\nradiology reports. Yet the availability of these modalities is influenced by\nclinical decision-making and varies across patients, resulting in modality\nmissing-not-at-random (MMNAR) patterns. We propose a causal representation\nlearning framework that leverages observed data and informative missingness in\nmultimodal clinical records. It consists of: (1) an MMNAR-aware modality fusion\ncomponent that integrates structured data, imaging, and text while conditioning\non missingness patterns to capture patient health and clinician-driven\nassignment; (2) a modality reconstruction component with contrastive learning\nto ensure semantic sufficiency in representation learning; and (3) a multitask\noutcome prediction model with a rectifier that corrects for residual bias from\nspecific modality observation patterns. Comprehensive evaluations across\nMIMIC-IV and eICU show consistent gains over the strongest baselines, achieving\nup to 13.8% AUC improvement for hospital readmission and 13.1% for ICU\nadmission.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7ecf\u5e38\u5b58\u5728\u7f3a\u5931\u60c5\u51b5\uff0c\u8fd9\u5f71\u54cd\u4e86\u60a3\u8005\u8868\u793a\u5b66\u4e60\u7684\u6548\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u7f3a\u5931\u95ee\u9898\u7684\u6846\u67b6\u3002", "method": "\u8be5\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u90e8\u5206\uff1a(1) \u4e00\u4e2a\u8003\u8651MMNAR\u7684\u6a21\u6001\u878d\u5408\u7ec4\u4ef6\uff0c(2) \u4e00\u4e2a\u5e26\u6709\u5bf9\u6bd4\u5b66\u4e60\u7684\u6a21\u6001\u91cd\u5efa\u7ec4\u4ef6\uff0c(3) \u4e00\u4e2a\u5e26\u6709\u6821\u6b63\u5668\u7684\u591a\u4efb\u52a1\u7ed3\u679c\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728MIMIC-IV\u548ceICU\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u533b\u9662\u518d\u5165\u9662\u548cICU\u5165\u9662\u7684AUC\u6307\u6807\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8613.8%\u548c13.1%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5229\u7528\u591a\u6a21\u6001\u4e34\u5e8a\u8bb0\u5f55\u4e2d\u7684\u89c2\u5bdf\u6570\u636e\u548c\u4fe1\u606f\u7f3a\u5931\u6027\uff0c\u63d0\u9ad8\u60a3\u8005\u8868\u793a\u5b66\u4e60\u7684\u6548\u679c\u3002"}}
{"id": "2509.17238", "pdf": "https://arxiv.org/pdf/2509.17238", "abs": "https://arxiv.org/abs/2509.17238", "authors": ["Soheil Zibakhsh", "Mohammad Samragh", "Kumari Nishu", "Lauren Hannah", "Arnav Kundu", "Minsik Cho"], "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.LG"], "comment": null, "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.", "AI": {"tldr": "This paper introduces hyper-parallel scaling, a framework that improves prediction quality at the token level by computing and aggregating multiple output proposals for a single token. It implements this concept in Mixture-of-Experts (MoE) models as Roster of Experts (RoE), which uses controlled stochasticity in expert routing to sample diverse experts for each token and aggregate their outputs. Efficient batching and KV-caching mechanisms reduce computational costs, allowing a 7B MoE model to match the performance of a 10.5B MoE model with 30% less compute.", "motivation": "The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods. Hyper-parallel scaling is introduced as a complementary framework that improves prediction quality at the token level.", "method": "Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model, implemented in Mixture-of-Experts (MoE) models as Roster of Experts (RoE). RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction. An efficient batching strategy and a specialized KV-caching mechanism are introduced to minimize compute and memory overhead.", "result": "RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference, achieving these gains without any fine-tuning of model parameters.", "conclusion": "RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference, achieving these gains without any fine-tuning of model parameters."}}
{"id": "2509.17240", "pdf": "https://arxiv.org/pdf/2509.17240", "abs": "https://arxiv.org/abs/2509.17240", "authors": ["Abdullah Mushtaq", "Muhammad Rafay Naeem", "Ibrahim Ghaznavi", "Alaa Abd-alrazaq", "Aliya Tabassum", "Junaid Qadir"], "title": "Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "Systematic Literature Reviews (SLRs) are foundational to evidence-based\nresearch but remain labor-intensive and prone to inconsistency across\ndisciplines. We present an LLM-based SLR evaluation copilot built on a\nMulti-Agent System (MAS) architecture to assist researchers in assessing the\noverall quality of the systematic literature reviews. The system automates\nprotocol validation, methodological assessment, and topic relevance checks\nusing a scholarly database. Unlike conventional single-agent methods, our\ndesign integrates a specialized agentic approach aligned with PRISMA guidelines\nto support more structured and interpretable evaluations. We conducted an\ninitial study on five published SLRs from diverse domains, comparing system\noutputs to expert-annotated PRISMA scores, and observed 84% agreement. While\nearly results are promising, this work represents a first step toward scalable\nand accurate NLP-driven systems for interdisciplinary workflows and reveals\ntheir capacity for rigorous, domain-agnostic knowledge aggregation to\nstreamline the review process.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684LLM\u9a71\u52a8\u7684SLR\u8bc4\u4f30\u52a9\u624b\uff0c\u80fd\u591f\u81ea\u52a8\u8fdb\u884c\u534f\u8bae\u9a8c\u8bc1\u3001\u65b9\u6cd5\u8bc4\u4f30\u548c\u4e3b\u9898\u76f8\u5173\u6027\u68c0\u67e5\uff0c\u5e76\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7684PRISMA\u8bc4\u5206\u6709\u8f83\u9ad8\u4e00\u81f4\u6027\u3002", "motivation": "\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff08SLRs\uff09\u5728\u57fa\u4e8e\u8bc1\u636e\u7684\u7814\u7a76\u4e2d\u8d77\u7740\u57fa\u7840\u4f5c\u7528\uff0c\u4f46\u4ecd\u7136\u8017\u65f6\u4e14\u5728\u4e0d\u540c\u5b66\u79d1\u4e2d\u5b58\u5728\u4e0d\u4e00\u81f4\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u4e00\u81f4\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30SLRs\u7684\u8d28\u91cf\u3002", "method": "\u672c\u6587\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u67b6\u6784\uff0c\u7ed3\u5408PRISMA\u6307\u5357\uff0c\u901a\u8fc7\u5b66\u672f\u6570\u636e\u5e93\u81ea\u52a8\u5316\u534f\u8bae\u9a8c\u8bc1\u3001\u65b9\u6cd5\u8bc4\u4f30\u548c\u4e3b\u9898\u76f8\u5173\u6027\u68c0\u67e5\uff0c\u4ee5\u652f\u6301\u66f4\u7ed3\u6784\u5316\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u5df2\u53d1\u8868SLRs\u4e0a\u8fdb\u884c\u4e86\u521d\u6b65\u7814\u7a76\uff0c\u5c06\u7cfb\u7edf\u8f93\u51fa\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7684PRISMA\u8bc4\u5206\u8fdb\u884c\u6bd4\u8f83\uff0c\u89c2\u5bdf\u523084%\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684SLR\u8bc4\u4f30\u52a9\u624b\uff0c\u65e8\u5728\u63d0\u9ad8\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7684\u8d28\u91cf\u8bc4\u4f30\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002\u867d\u7136\u521d\u6b65\u7ed3\u679c\u4ee4\u4eba\u9f13\u821e\uff0c\u4f46\u8fd9\u9879\u5de5\u4f5c\u53ea\u662f\u8fc8\u5411\u53ef\u6269\u5c55\u548c\u51c6\u786e\u7684NLP\u9a71\u52a8\u7cfb\u7edf\u7684\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2509.17318", "pdf": "https://arxiv.org/pdf/2509.17318", "abs": "https://arxiv.org/abs/2509.17318", "authors": ["Zhuofan Chen", "Jiyuan He", "Yichi Zhang", "Xing Hu", "Haoxing Wen", "Jun Bai", "Wenge Rong"], "title": "CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Mathematical reasoning poses significant challenges for Large Language Models\n(LLMs) due to its demand for multi-step reasoning and abstract conceptual\nintegration. While recent test-time scaling techniques rely heavily on\nhigh-quality, challenging problems, the scarcity of Olympiad-level math\nproblems remains a bottleneck. We introduce CogAtom, a novel cognitive\natom-based framework for synthesizing mathematically rigorous and cognitively\ndiverse problems. Unlike prior approaches, CogAtom models problem construction\nas a process of selecting and recombining fundamental reasoning units,\ncognitive atoms, extracted from human-authored solutions. A diversity-promoting\nrandom walk algorithm enables exploration of the cognitive atom space, while a\nconstraint-based recombination mechanism ensures logical soundness and\nstructural validity. The combinatorial nature of the graph structure provides a\nnear-infinite space of reasoning paths, and the walk algorithm systematically\nexplores this space to achieve large-scale synthesis of high-quality problems;\nmeanwhile, by controlling the number of cognitive atoms, we can precisely\nadjust problem difficulty, ensuring diversity, scalability, and controllability\nof the generated problems. Experimental results demonstrate that CogAtom\noutperforms existing methods in accuracy, reasoning depth, and diversity,\ngenerating problems that closely match the difficulty of AIME while exceeding\nit in structural variation. Our work offers a cognitively grounded pathway\ntoward scalable, high-quality math problem generation.Our code is publicly\navailable at https://github.com/Icarus-1111/CogAtom.", "AI": {"tldr": "CogAtom\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u539f\u5b50\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u6570\u5b66\u4e0a\u4e25\u683c\u4e14\u8ba4\u77e5\u591a\u6837\u7684\u95ee\u9898\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u751f\u6210\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u6570\u5b66\u63a8\u7406\u9700\u8981\u591a\u6b65\u9aa4\u63a8\u7406\u548c\u62bd\u8c61\u6982\u5ff5\u6574\u5408\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u7136\u800c\uff0c\u5965\u6797\u5339\u514b\u7ea7\u522b\u7684\u6570\u5b66\u95ee\u9898\u7a00\u7f3a\u4ecd\u7136\u662f\u4e00\u4e2a\u74f6\u9888\u3002", "method": "CogAtom\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u548c\u91cd\u7ec4\u4ece\u4eba\u7c7b\u7f16\u5199\u7684\u89e3\u51b3\u65b9\u6848\u4e2d\u63d0\u53d6\u7684\u57fa\u672c\u63a8\u7406\u5355\u5143\uff08\u8ba4\u77e5\u539f\u5b50\uff09\u6765\u5efa\u6a21\u95ee\u9898\u6784\u5efa\u8fc7\u7a0b\u3002\u4f7f\u7528\u4fc3\u8fdb\u591a\u6837\u6027\u7684\u968f\u673a\u6e38\u8d70\u7b97\u6cd5\u63a2\u7d22\u8ba4\u77e5\u539f\u5b50\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u7ea6\u675f\u7684\u91cd\u7ec4\u673a\u5236\u786e\u4fdd\u903b\u8f91\u6b63\u786e\u6027\u548c\u7ed3\u6784\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCogAtom\u5728\u51c6\u786e\u6027\u3001\u63a8\u7406\u6df1\u5ea6\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u95ee\u9898\u96be\u5ea6\u63a5\u8fd1AIME\uff0c\u4f46\u5728\u7ed3\u6784\u53d8\u5316\u65b9\u9762\u8d85\u8fc7\u4e86\u5b83\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u6761\u8ba4\u77e5\u57fa\u7840\u7684\u8def\u5f84\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u6570\u5b66\u95ee\u9898\u751f\u6210\u3002"}}
{"id": "2509.17321", "pdf": "https://arxiv.org/pdf/2509.17321", "abs": "https://arxiv.org/abs/2509.17321", "authors": ["Pawe\u0142 Budzianowski", "Emilia Wi\u015bnios", "Gracjan G\u00f3ral", "Igor Kulakov", "Viktor Petrenko", "Krzysztof Walas"], "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation", "categories": ["cs.RO", "cs.CL"], "comment": null, "summary": "Data scarcity remains one of the most limiting factors in driving progress in\nrobotics. However, the amount of available robotics data in the wild is growing\nexponentially, creating new opportunities for large-scale data utilization.\nReliable temporal task completion prediction could help automatically annotate\nand curate this data at scale. The Generative Value Learning (GVL) approach was\nrecently proposed, leveraging the knowledge embedded in vision-language models\n(VLMs) to predict task progress from visual observations. Building upon GVL, we\npropose OpenGVL, a comprehensive benchmark for estimating task progress across\ndiverse challenging manipulation tasks involving both robotic and human\nembodiments. We evaluate the capabilities of publicly available open-source\nfoundation models, showing that open-source model families significantly\nunderperform closed-source counterparts, achieving only approximately $70\\%$ of\ntheir performance on temporal progress prediction tasks. Furthermore, we\ndemonstrate how OpenGVL can serve as a practical tool for automated data\ncuration and filtering, enabling efficient quality assessment of large-scale\nrobotics datasets. We release the benchmark along with the complete codebase at\n\\href{github.com/budzianowski/opengvl}{OpenGVL}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OpenGVL\uff0c\u4e00\u4e2a\u7528\u4e8e\u4f30\u8ba1\u591a\u6837\u5316\u6311\u6218\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4efb\u52a1\u8fdb\u5ea6\u7684\u57fa\u51c6\u3002\u5b9e\u9a8c\u663e\u793a\u5f00\u6e90\u6a21\u578b\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u5c01\u95ed\u6e90\u4ee3\u7801\u6a21\u578b\uff0c\u4f46OpenGVL\u53ef\u4ee5\u7528\u4e8e\u81ea\u52a8\u5316\u6570\u636e\u6574\u7406\u548c\u8fc7\u6ee4\u3002", "motivation": "\u6570\u636e\u7a00\u7f3a\u4ecd\u7136\u662f\u9650\u5236\u673a\u5668\u4eba\u9886\u57df\u8fdb\u6b65\u7684\u4e3b\u8981\u56e0\u7d20\u3002\u7136\u800c\uff0c\u91ce\u5916\u53ef\u7528\u7684\u673a\u5668\u4eba\u6570\u636e\u91cf\u6b63\u5728\u6307\u6570\u7ea7\u589e\u957f\uff0c\u8fd9\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u5229\u7528\u521b\u9020\u4e86\u65b0\u7684\u673a\u4f1a\u3002\u53ef\u9760\u7684\u65f6\u5e8f\u4efb\u52a1\u5b8c\u6210\u9884\u6d4b\u53ef\u4ee5\u5e2e\u52a9\u81ea\u52a8\u6807\u6ce8\u548c\u6574\u7406\u8fd9\u4e9b\u6570\u636e\u3002", "method": "\u672c\u6587\u57fa\u4e8eGenerative Value Learning (GVL)\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6OpenGVL\uff0c\u7528\u4e8e\u4f30\u8ba1\u4efb\u52a1\u8fdb\u5ea6\u3002\u540c\u65f6\uff0c\u8bc4\u4f30\u4e86\u516c\u5f00\u53ef\u7528\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86OpenGVL\u5728\u81ea\u52a8\u5316\u6570\u636e\u6574\u7406\u548c\u8fc7\u6ee4\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u5bb6\u65cf\u5728\u65f6\u95f4\u8fdb\u5ea6\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u5c01\u95ed\u6e90\u4ee3\u7801\u6a21\u578b\uff0c\u4ec5\u8fbe\u5230\u5176\u7ea670%\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0cOpenGVL\u88ab\u8bc1\u660e\u53ef\u4ee5\u4f5c\u4e3a\u81ea\u52a8\u6570\u636e\u6574\u7406\u548c\u8fc7\u6ee4\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u96c6\u7684\u6709\u6548\u8d28\u91cf\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86OpenGVL\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u4f30\u8ba1\u6d89\u53ca\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u7684\u591a\u6837\u5316\u6311\u6218\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u4efb\u52a1\u8fdb\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f00\u6e90\u6a21\u578b\u5728\u65f6\u95f4\u8fdb\u5ea6\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4ec5\u4e3a\u5c01\u95ed\u6e90\u4ee3\u7801\u6a21\u578b\u7684\u7ea670%\u3002\u6b64\u5916\uff0cOpenGVL\u53ef\u4ee5\u4f5c\u4e3a\u81ea\u52a8\u6570\u636e\u6574\u7406\u548c\u8fc7\u6ee4\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u96c6\u7684\u6709\u6548\u8d28\u91cf\u8bc4\u4f30\u3002"}}
{"id": "2509.17325", "pdf": "https://arxiv.org/pdf/2509.17325", "abs": "https://arxiv.org/abs/2509.17325", "authors": ["Weihua Du", "Hailei Gong", "Zhan Ling", "Kang Liu", "Lingfeng Shen", "Xuesong Yao", "Yufei Xu", "Dingyuan Shi", "Yiming Yang", "Jiecao Chen"], "title": "Generalizable End-to-End Tool-Use RL with Synthetic CodeGym", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "22 pages. Project available at https://github.com/StigLidu/CodeGym", "summary": "Tool-augmented large language models (LLMs), hereafter LLM agents, leverage\nexternal tools to solve diverse tasks and interface with the real world.\nHowever, current training practices largely rely on supervised fine-tuning\n(SFT) over static trajectories or reinforcement learning (RL) on narrow tasks,\nand generalize poorly beyond development settings, leading to brittleness with\nnew tools and unseen workflows. Because code execution reflects many structures\nof real-world workflows, coding problems provide a natural basis for building\nagent training environments. Motivated by this, we introduce CodeGym, a\nscalable framework that synthesizes diverse, verifiable, and controllable\nmulti-turn tool-use environments for agent RL, enabling LLM agents to explore\nand master various workflows actively. CodeGym rewrites static coding problems\ninto interactive environments by extracting atomic functions or logic into\ncallable tools, yielding verifiable tasks that span various tool-execution\nworkflows. Models of varying sizes and chain-of-thought configurations, trained\nin CodeGym, exhibit consistent out-of-distribution generalizability; for\nexample, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points\non the OOD benchmark $\\tau$-Bench. These results highlight CodeGym as a step\ntoward scalable general-purpose RL environments that align with real-world\nagent workflows.", "AI": {"tldr": "CodeGym\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9759\u6001\u7f16\u7801\u95ee\u9898\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u73af\u5883\u6765\u5408\u6210\u591a\u6837\u3001\u53ef\u9a8c\u8bc1\u548c\u53ef\u63a7\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u73af\u5883\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u4ee3\u7406\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u4ee3\u7801\u6267\u884c\u53cd\u6620\u4e86\u73b0\u5b9e\u4e16\u754c\u5de5\u4f5c\u6d41\u7a0b\u7684\u8bb8\u591a\u7ed3\u6784\uff0c\u56e0\u6b64\u7f16\u7801\u95ee\u9898\u4e3a\u6784\u5efa\u4ee3\u7406\u8bad\u7ec3\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u7136\u7684\u57fa\u7840\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86CodeGym\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5408\u6210\u591a\u6837\u3001\u53ef\u9a8c\u8bc1\u548c\u53ef\u63a7\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u73af\u5883\uff0c\u7528\u4e8e\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728CodeGym\u4e2d\u8bad\u7ec3\u7684\u4e0d\u540c\u5927\u5c0f\u548c\u601d\u7ef4\u94fe\u914d\u7f6e\u7684\u6a21\u578b\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff1b\u4f8b\u5982\uff0cQwen2.5-32B-Instruct\u5728OOD\u57fa\u51c6$\tau$-Bench\u4e0a\u5b9e\u73b0\u4e868.7\u5206\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660eCodeGym\u662f\u8fc8\u5411\u53ef\u6269\u5c55\u7684\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u76f8\u4e00\u81f4\u3002"}}
{"id": "2509.17336", "pdf": "https://arxiv.org/pdf/2509.17336", "abs": "https://arxiv.org/abs/2509.17336", "authors": ["Tianyu Fu", "Anyang Su", "Chenxu Zhao", "Hanning Wang", "Minghui Wu", "Zhe Yu", "Fei Hu", "Mingjia Shi", "Wei Dong", "Jiayao Wang", "Yuyang Chen", "Ruiyang Yu", "Siran Peng", "Menglin Li", "Nan Huang", "Haitian Wei", "Jiawei Yu", "Yi Xin", "Xilin Zhao", "Kai Gu", "Ping Jiang", "Sifan Zhou", "Shuo Wang"], "title": "Mano Report", "categories": ["cs.MM", "cs.CL", "cs.CV"], "comment": null, "summary": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMano\u7684GUI\u4ee3\u7406\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u548c\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5728GUI\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u7531\u4e8e\u89c6\u89c9\u5143\u7d20\u7684\u590d\u6742\u6027\u3001\u52a8\u6001\u73af\u5883\u4ee5\u53ca\u9700\u8981\u591a\u6b65\u9aa4\u63a8\u7406\uff0c\u81ea\u52a8\u5316GUI\u4ea4\u4e92\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5e38\u5e38\u53d7\u5230\u5206\u8fa8\u7387\u6709\u9650\u3001\u9886\u57df\u4e0d\u5339\u914d\u548c\u7f3a\u4e4f\u987a\u5e8f\u51b3\u7b56\u80fd\u529b\u7684\u56f0\u6270\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Mano\uff0c\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u7a33\u5065GUI\u4ee3\u7406\uff0c\u8be5\u6a21\u578b\u5728\u5e7f\u6cdb\u7684\u7f51\u7edc\u548c\u8ba1\u7b97\u673a\u7cfb\u7edf\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u96c6\u6210\u4e86\u4e00\u4e2a\u7528\u4e8e\u9ad8\u4fdd\u771f\u6570\u636e\u751f\u6210\u7684\u65b0\u6a21\u62df\u73af\u5883\u3001\u4e00\u4e2a\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff08\u76d1\u7763\u5fae\u8c03\u3001\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7528\u4e8e\u9519\u8bef\u6062\u590d\u7684\u9a8c\u8bc1\u6a21\u5757\u3002", "result": "Mano\u5728\u591a\u4e2aGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ecMind2Web\u548cOSWorld\uff0c\u5728\u6210\u529f\u7387\u548c\u64cd\u4f5c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u5c06\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u6574\u5408\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645\u7684GUI\u4ee3\u7406\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.17337", "pdf": "https://arxiv.org/pdf/2509.17337", "abs": "https://arxiv.org/abs/2509.17337", "authors": ["Ala Jararweh", "Michael Adams", "Avinash Sahu", "Abdullah Mueen", "Afsah Anwar"], "title": "LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Increasing complexity in software systems places a growing demand on\nreasoning tools that unlock vulnerabilities manifest in source code. Many\ncurrent approaches focus on vulnerability analysis as a classifying task,\noversimplifying the nuanced and context-dependent real-world scenarios. Even\nthough current code large language models (LLMs) excel in code understanding,\nthey often pay little attention to security-specific reasoning. We propose\nLLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code\nthrough question-answering (QA). Our model is trained to integrate paired code\nand natural queries into a unified space, enhancing reasoning and\ncontext-dependent insights about code vulnerability. To evaluate our model\nperformance, we construct a curated dataset of real-world vulnerabilities\npaired with security-focused questions and answers. Our model outperforms\nstate-of-the-art general-purpose and code LLMs in the QA and detection tasks.\nWe further explain decision-making by conducting qualitative analysis to\nhighlight capabilities and limitations. By integrating code and QA, LLaVul\nenables more interpretable and security-focused code understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLaVul\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u901a\u8fc7\u95ee\u7b54\uff08QA\uff09\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u4ee3\u7801\u63a8\u7406\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5c06\u4ee3\u7801\u548c\u81ea\u7136\u67e5\u8be2\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u7a7a\u95f4\u4e2d\uff0c\u589e\u5f3a\u4e86\u5bf9\u4ee3\u7801\u6f0f\u6d1e\u7684\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u6d1e\u5bdf\u3002\u5728QA\u548c\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cLLaVul\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u901a\u7528\u548c\u4ee3\u7801LLMs\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u5c55\u793a\u4e86\u5176\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u8f6f\u4ef6\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u63a8\u7406\u5de5\u5177\u6765\u53d1\u73b0\u6e90\u4ee3\u7801\u4e2d\u7684\u6f0f\u6d1e\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u5f80\u5f80\u5c06\u6f0f\u6d1e\u5206\u6790\u7b80\u5316\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u573a\u666f\u3002\u5c3d\u7ba1\u5f53\u524d\u7684\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u5f88\u5c11\u5173\u6ce8\u5b89\u5168\u7279\u5b9a\u7684\u63a8\u7406\u3002", "method": "LLaVul\u662f\u4e00\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u901a\u8fc7\u95ee\u7b54\uff08QA\uff09\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u4ee3\u7801\u63a8\u7406\u3002\u6a21\u578b\u7ecf\u8fc7\u8bad\u7ec3\uff0c\u5c06\u914d\u5bf9\u7684\u4ee3\u7801\u548c\u81ea\u7136\u67e5\u8be2\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u589e\u5f3a\u4ee3\u7801\u6f0f\u6d1e\u7684\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u6d1e\u5bdf\u3002", "result": "\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u771f\u5b9e\u4e16\u754c\u6f0f\u6d1e\u53ca\u5176\u5b89\u5168\u76f8\u5173\u95ee\u9898\u548c\u7b54\u6848\u7684\u7cbe\u9009\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002LLaVul\u5728QA\u548c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u901a\u7528\u548c\u4ee3\u7801LLMs\u3002\u901a\u8fc7\u5b9a\u6027\u5206\u6790\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u89e3\u91ca\u4e86\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7a81\u51fa\u4e86\u6a21\u578b\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002", "conclusion": "LLaVul\u901a\u8fc7\u6574\u5408\u4ee3\u7801\u548c\u95ee\u7b54\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u5b89\u5168\u805a\u7126\u7684\u4ee3\u7801\u7406\u89e3\u3002"}}
{"id": "2509.17393", "pdf": "https://arxiv.org/pdf/2509.17393", "abs": "https://arxiv.org/abs/2509.17393", "authors": ["Kang-il Lee", "Jahyun Koo", "Seunghyun Yoon", "Minbeom Kim", "Hyukhun Koh", "Dongryeol Lee", "Kyomin Jung"], "title": "Program Synthesis via Test-Time Transduction", "categories": ["cs.AI", "cs.CL"], "comment": "NeurIPS 2025", "summary": "We introduce transductive program synthesis, a new formulation of the program\nsynthesis task that explicitly leverages test inputs during synthesis. While\nprior approaches to program synthesis--whether based on natural language\ndescriptions or input-output examples--typically aim to generalize from\ntraining examples, they often struggle with robustness, especially in\nreal-world settings where training examples are limited and test inputs involve\nvarious edge cases. To address this, we propose a novel framework that improves\nrobustness by treating synthesis as an active learning over a finite hypothesis\nclass defined by programs' outputs. We use an LLM to predict outputs for\nselected test inputs and eliminate inconsistent hypotheses, where the inputs\nare chosen via a greedy maximin algorithm to minimize the number of LLM queries\nrequired. We evaluate our approach on two real-world datasets: Playgol, a\nstring transformation benchmark, and MBPP+, a Python code generation benchmark.\nWe demonstrate that our method significantly improves program synthesis in both\naccuracy and efficiency. We release our code at\nhttps://github.com/klee972/SYNTRA.", "AI": {"tldr": "\u6211\u4eec\u5f15\u5165\u4e86\u5f52\u7eb3\u7a0b\u5e8f\u5408\u6210\uff0c\u4e00\u79cd\u5229\u7528\u6d4b\u8bd5\u8f93\u5165\u8fdb\u884c\u5408\u6210\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\u901a\u5e38\u65e8\u5728\u4ece\u8bad\u7ec3\u793a\u4f8b\u4e2d\u6cdb\u5316\uff0c\u4f46\u5728\u5b9e\u9645\u73af\u5883\u4e2d\uff0c\u5f53\u8bad\u7ec3\u793a\u4f8b\u6709\u9650\u4e14\u6d4b\u8bd5\u8f93\u5165\u6d89\u53ca\u5404\u79cd\u8fb9\u7f18\u60c5\u51b5\u65f6\uff0c\u5b83\u4eec\u5f80\u5f80\u96be\u4ee5\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5408\u6210\u89c6\u4e3a\u6709\u9650\u5047\u8bbe\u7c7b\u4e0a\u7684\u4e3b\u52a8\u5b66\u4e60\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u3002\u6211\u4eec\u4f7f\u7528LLM\u9884\u6d4b\u9009\u5b9a\u6d4b\u8bd5\u8f93\u5165\u7684\u8f93\u51fa\uff0c\u5e76\u6d88\u9664\u4e0d\u4e00\u81f4\u7684\u5047\u8bbe\uff0c\u8f93\u5165\u901a\u8fc7\u8d2a\u5a6a\u6700\u5927\u6700\u5c0f\u7b97\u6cd5\u9009\u62e9\u4ee5\u51cf\u5c11\u6240\u9700\u7684LLM\u67e5\u8be2\u6570\u91cf\u3002", "result": "\u6211\u4eec\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff1aPlaygol\uff0c\u4e00\u4e2a\u5b57\u7b26\u4e32\u8f6c\u6362\u57fa\u51c6\u548cMBPP+\uff0c\u4e00\u4e2aPython\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0e\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u7a0b\u5e8f\u5408\u6210\u3002"}}
{"id": "2509.17466", "pdf": "https://arxiv.org/pdf/2509.17466", "abs": "https://arxiv.org/abs/2509.17466", "authors": ["Migyeong Yang", "Kyungah Lee", "Jinyoung Han", "SoHyun Park", "Young-Ho Kim"], "title": "Autiverse: Eliciting Autistic Adolescents' Daily Narratives through AI-guided Multimodal Journaling", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": "19 pages excluding reference", "summary": "Journaling can potentially serve as an effective method for autistic\nadolescents to improve narrative skills. However, its text-centric nature and\nhigh executive functioning demands present barriers to practice. We present\nAutiverse, an AI-guided multimodal journaling app for tablets that scaffolds\nstorytelling through conversational prompts and visual supports. Autiverse\nelicits key details through a stepwise dialogue with peer-like, customizable AI\nand composes them into an editable four-panel comic strip. Through a two-week\ndeployment study with 10 autistic adolescent-parent dyads, we examine how\nAutiverse supports autistic adolescents to organize their daily experience and\nemotion. Autiverse helped them construct coherent narratives, while enabling\nparents to learn additional details of their child's events and emotions. The\ncustomized AI peer created a comfortable space for sharing, fostering enjoyment\nand a strong sense of agency. We discuss the implications of designing\ntechnologies that complement autistic adolescents' strengths while ensuring\ntheir autonomy and safety in sharing experiences.", "AI": {"tldr": "Autiverse \u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u591a\u6a21\u6001\u65e5\u8bb0\u5e94\u7528\uff0c\u5e2e\u52a9\u81ea\u95ed\u75c7\u9752\u5c11\u5e74\u901a\u8fc7\u5bf9\u8bdd\u548c\u89c6\u89c9\u652f\u6301\u6784\u5efa\u53d9\u4e8b\uff0c\u540c\u65f6\u589e\u5f3a\u4ed6\u4eec\u7684\u81ea\u4e3b\u6027\u548c\u5b89\u5168\u611f\u3002", "motivation": "\u81ea\u95ed\u75c7\u9752\u5c11\u5e74\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u53d9\u4e8b\u6280\u80fd\u63d0\u5347\u65b9\u6cd5\uff0c\u800c\u4f20\u7edf\u7684\u6587\u672c\u65b9\u6cd5\u53ef\u80fd\u8fc7\u4e8e\u4f9d\u8d56\u6267\u884c\u529f\u80fd\u3002", "method": "Autiverse \u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u591a\u6a21\u6001\u65e5\u8bb0\u5e94\u7528\uff0c\u901a\u8fc7\u5bf9\u8bdd\u63d0\u793a\u548c\u89c6\u89c9\u652f\u6301\u6765\u8f85\u52a9\u6545\u4e8b\u8bb2\u8ff0\u3002", "result": "\u5728\u4e3a\u671f\u4e24\u5468\u7684\u90e8\u7f72\u7814\u7a76\u4e2d\uff0cAutiverse \u5e2e\u52a9\u81ea\u95ed\u75c7\u9752\u5c11\u5e74\u6784\u5efa\u8fde\u8d2f\u7684\u53d9\u8ff0\uff0c\u5e76\u4f7f\u7236\u6bcd\u80fd\u591f\u4e86\u89e3\u5b69\u5b50\u7684\u4e8b\u4ef6\u548c\u60c5\u7eea\u3002", "conclusion": "Autiverse \u662f\u4e00\u4e2a\u6709\u6548\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u53ef\u4ee5\u5e2e\u52a9\u81ea\u95ed\u75c7\u9752\u5c11\u5e74\u63d0\u9ad8\u53d9\u4e8b\u80fd\u529b\uff0c\u5e76\u5728\u5206\u4eab\u4f53\u9a8c\u65f6\u786e\u4fdd\u4ed6\u4eec\u7684\u81ea\u4e3b\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.17477", "pdf": "https://arxiv.org/pdf/2509.17477", "abs": "https://arxiv.org/abs/2509.17477", "authors": ["Yeonsun Yang", "Sang Won Lee", "Jean Y. Song", "Sangdoo Yun", "Young-Ho Kim"], "title": "LingoQ: Bridging the Gap between ESL Learning and Work through AI-Generated Work-Related Quizzes", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": "17 pages except reference", "summary": "Non-native English speakers performing English-related tasks at work struggle\nto sustain ESL learning, despite their motivation. Often, study materials are\ndisconnected from their work context. Although workers rely on LLM assistants\nto address their immediate needs, these interactions may not directly\ncontribute to their English skills. We present LingoQ, an AI-mediated system\nthat allows workers to practice English using quizzes generated from their LLM\nqueries during work. LingoQ leverages these queries using AI to generate\npersonalized quizzes that workers can review and practice on their smartphones.\nWe conducted a three-week deployment study with 28 ESL workers to evaluate\nLingoQ. Participants valued the relevance of quizzes that reflect their own\ncontext, constantly engaging with the app during the study. This active\nengagement improved self-efficacy and led to learning gains for beginners and,\npotentially, for intermediate learners. We discuss opportunities of leveraging\nusers' reliance on LLMs to situate their learning in the user context for\nimproved learning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aLingoQ\u7684AI\u7cfb\u7edf\uff0c\u5e2e\u52a9\u975e\u82f1\u8bed\u6bcd\u8bed\u8005\u901a\u8fc7\u5de5\u4f5c\u4e2d\u7684LLM\u67e5\u8be2\u751f\u6210\u4e2a\u6027\u5316\u6d4b\u9a8c\u6765\u7ec3\u4e60\u82f1\u8bed\u3002\u7814\u7a76\u663e\u793a\uff0c\u8fd9\u79cd\u57fa\u4e8e\u5de5\u4f5c\u60c5\u5883\u7684\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5b66\u4e60\u8005\u7684\u81ea\u6211\u6548\u80fd\u611f\u548c\u5b66\u4e60\u6210\u679c\u3002", "motivation": "\u975e\u82f1\u8bed\u6bcd\u8bed\u8005\u5728\u5de5\u4f5c\u4e2d\u8fdb\u884c\u82f1\u8bed\u76f8\u5173\u4efb\u52a1\u65f6\uff0c\u5c3d\u7ba1\u6709\u52a8\u673a\uff0c\u4f46\u96be\u4ee5\u6301\u7eed\u8fdb\u884cESL\u5b66\u4e60\u3002\u5b66\u4e60\u6750\u6599\u5f80\u5f80\u4e0e\u5de5\u4f5c\u60c5\u5883\u8131\u8282\u3002\u867d\u7136\u5de5\u4eba\u4f9d\u8d56LLM\u52a9\u624b\u6765\u89e3\u51b3\u4ed6\u4eec\u7684\u5373\u65f6\u9700\u6c42\uff0c\u4f46\u8fd9\u4e9b\u4e92\u52a8\u53ef\u80fd\u4e0d\u4f1a\u76f4\u63a5\u4fc3\u8fdb\u4ed6\u4eec\u7684\u82f1\u8bed\u6280\u80fd\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLingoQ\u7684\u4eba\u5de5\u667a\u80fd\u4e2d\u4ecb\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5141\u8bb8\u5de5\u4eba\u4f7f\u7528\u4ece\u4ed6\u4eec\u7684LLM\u67e5\u8be2\u4e2d\u751f\u6210\u7684\u6d4b\u9a8c\u6765\u7ec3\u4e60\u82f1\u8bed\u3002", "result": "\u5728\u4e3a\u671f\u4e09\u5468\u7684\u90e8\u7f72\u7814\u7a76\u4e2d\uff0c28\u540dESL\u5de5\u4eba\u53c2\u4e0e\u4e86LingoQ\u7684\u6d4b\u8bd5\u3002\u53c2\u4e0e\u8005\u91cd\u89c6\u53cd\u6620\u4ed6\u4eec\u81ea\u5df1\u60c5\u5883\u7684\u6d4b\u9a8c\uff0c\u6301\u7eed\u4f7f\u7528\u8be5\u5e94\u7528\u3002\u8fd9\u79cd\u79ef\u6781\u7684\u53c2\u4e0e\u63d0\u9ad8\u4e86\u81ea\u6211\u6548\u80fd\u611f\uff0c\u5e76\u4e3a\u521d\u5b66\u8005\u5e26\u6765\u4e86\u5b66\u4e60\u6210\u679c\uff0c\u53ef\u80fd\u4e5f\u9002\u7528\u4e8e\u4e2d\u7ea7\u5b66\u4e60\u8005\u3002", "conclusion": "\u6211\u4eec\u8ba8\u8bba\u4e86\u5229\u7528\u7528\u6237\u5bf9LLM\u7684\u4f9d\u8d56\u6765\u5c06\u5b66\u4e60\u7f6e\u4e8e\u7528\u6237\u60c5\u5883\u4e2d\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\u7684\u673a\u4f1a\u3002"}}
{"id": "2509.17481", "pdf": "https://arxiv.org/pdf/2509.17481", "abs": "https://arxiv.org/abs/2509.17481", "authors": ["Xingqi Wang", "Yiming Cui", "Xin Yao", "Shijin Wang", "Guoping Hu", "Xiaoyu Qin"], "title": "ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have recently demonstrated remarkable\nprogress, yet hallucination remains a critical barrier, particularly in chart\nunderstanding, which requires sophisticated perceptual and cognitive abilities\nas well as rigorous factual accuracy. While prior work has investigated\nhallucinations and chart comprehension independently, their intersection\nremains largely unexplored. To address this gap, we present ChartHal, a\nbenchmark that features a fine-grained taxonomy of hallucination scenarios in\nchart understanding, along with a human-validated dataset of 1,062 samples. Our\nevaluation shows that state-of-the-art LVLMs suffer from severe hallucinations\non ChartHal, including proprietary models such as GPT-5 and o4-mini, which\nachieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals\nthat questions involving information absent from or contradictory to charts are\nespecially likely to trigger hallucinations, underscoring the urgent need for\nmore robust mitigation strategies. Code and data are available at\nhttps://github.com/ymcui/ChartHal .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ChartHal\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5206\u522b\u63a2\u8ba8\u4e86\u5e7b\u89c9\u548c\u56fe\u8868\u7406\u89e3\uff0c\u4f46\u5b83\u4eec\u7684\u4ea4\u53c9\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChartHal\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u7684\u5e7b\u89c9\u573a\u666f\u5206\u7c7b\u548c\u4e00\u4e2a\u7ecf\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\u3002", "result": "\u6700\u5148\u8fdb\u7684LVLMs\u5728ChartHal\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5982GPT-5\u548co4-mini\u5206\u522b\u4ec5\u8fbe\u523034.46%\u548c22.79%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u7a33\u5065\u7684\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2509.17608", "pdf": "https://arxiv.org/pdf/2509.17608", "abs": "https://arxiv.org/abs/2509.17608", "authors": ["Jungeun Lee", "Kyungah Lee", "Inseok Hwang", "SoHyun Park", "Young-Ho Kim"], "title": "AutiHero: Leveraging Generative AI in Social Narratives to Engage Parents in Story-Driven Behavioral Guidance for Autistic Children", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": "22 pages except reference", "summary": "Social narratives are known to help autistic children understand and navigate\nsocial situations through stories. To ensure effectiveness, however, the\nmaterials need to be customized to reflect each child's unique behavioral\ncontext, requiring considerable time and effort for parents to practice at\nhome. We present AutiHero, a generative AI-based social narrative system for\nbehavioral guidance, which supports parents to create personalized stories for\ntheir autistic children and read them together. AutiHero generates text and\nvisual illustrations that reflect their children's interests, target behaviors,\nand everyday contexts. In a two-week deployment study with 16 autistic\nchild-parent dyads, parents created 218 stories and read an average of 4.25\nstories per day, demonstrating a high level of engagement. AutiHero also\nprovided an effective, low-demanding means to guide children's social\nbehaviors, encouraging positive change. We discuss the implications of\ngenerative AI-infused tools to empower parents in guiding their children's\nbehaviors, fostering their social learning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AutiHero\uff0c\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u793e\u4f1a\u53d9\u4e8b\u7cfb\u7edf\uff0c\u7528\u4e8e\u5e2e\u52a9\u81ea\u95ed\u75c7\u513f\u7ae5\u7406\u89e3\u793e\u4f1a\u60c5\u5883\u3002\u8be5\u7cfb\u7edf\u652f\u6301\u7236\u6bcd\u521b\u5efa\u4e2a\u6027\u5316\u7684\u6545\u4e8b\uff0c\u5e76\u5728\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u9ad8\u53c2\u4e0e\u5ea6\u548c\u6709\u6548\u7684\u884c\u4e3a\u6307\u5bfc\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u6709\u6548\u6027\uff0c\u6750\u6599\u9700\u8981\u5b9a\u5236\u4ee5\u53cd\u6620\u6bcf\u4e2a\u5b69\u5b50\u7684\u72ec\u7279\u884c\u4e3a\u80cc\u666f\uff0c\u8fd9\u9700\u8981\u7236\u6bcd\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u548c\u7cbe\u529b\u5728\u5bb6\u4e2d\u7ec3\u4e60\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86AutiHero\uff0c\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u793e\u4f1a\u53d9\u4e8b\u7cfb\u7edf\uff0c\u7528\u4e8e\u884c\u4e3a\u6307\u5bfc\uff0c\u652f\u6301\u7236\u6bcd\u4e3a\u81ea\u95ed\u75c7\u513f\u7ae5\u521b\u5efa\u4e2a\u6027\u5316\u7684\u6545\u4e8b\u5e76\u4e00\u8d77\u9605\u8bfb\u3002", "result": "\u5728\u4e0e16\u4e2a\u81ea\u95ed\u75c7\u513f\u7ae5-\u5bb6\u957f\u914d\u5bf9\u7684\u4e24\u5468\u90e8\u7f72\u7814\u7a76\u4e2d\uff0c\u7236\u6bcd\u521b\u5efa\u4e86218\u4e2a\u6545\u4e8b\uff0c\u5e73\u5747\u6bcf\u5929\u9605\u8bfb4.25\u4e2a\u6545\u4e8b\uff0c\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u53c2\u4e0e\u5ea6\u3002AutiHero\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u4f4e\u9700\u6c42\u7684\u65b9\u5f0f\u6765\u5f15\u5bfc\u513f\u7ae5\u7684\u793e\u4f1a\u884c\u4e3a\uff0c\u9f13\u52b1\u79ef\u6781\u7684\u53d8\u5316\u3002", "conclusion": "\u672c\u6587\u8ba8\u8bba\u4e86\u751f\u6210\u5f0fAI\u878d\u5408\u5de5\u5177\u5728\u5e2e\u52a9\u7236\u6bcd\u6307\u5bfc\u5b69\u5b50\u884c\u4e3a\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4fc3\u8fdb\u4e86\u5b69\u5b50\u7684\u793e\u4f1a\u5b66\u4e60\u3002"}}
{"id": "2509.17730", "pdf": "https://arxiv.org/pdf/2509.17730", "abs": "https://arxiv.org/abs/2509.17730", "authors": ["Bonan Zhang", "Zhongqi Chen", "Bowen Song", "Qinya Li", "Fan Wu", "Guihai Chen"], "title": "ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has become a standard paradigm for refining large\nlanguage models (LLMs) beyond pre-training and instruction tuning. A prominent\nline of work is RL with verifiable rewards (RLVR), which leverages\nautomatically verifiable outcomes (e.g., correctness or executability) to\ngenerate reward signals. While efficient, this framework faces two key\nlimitations: First, its binary feedback is too sparse to capture the quality of\nthe reasoning process. Second, its coarse-grained rewards potentially lead to\nvanishing gradients. Inspired by observations from human learning, we introduce\na RL technique that integrates verifiable outcomes with the model's own\nconfidence estimates. This joint design enriches the reward signal, providing\nfiner-grained feedback and implicitly supervising the reasoning process.\nExperimental results demonstrate that our proposed method enhances RL\nperformance across multiple datasets and reduces token consumption during\ninference, while incurring negligible additional training cost. Moreover, it\ncan be used as a plug-in module to enhance other state-of-the-art RL methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RL\u6280\u672f\uff0c\u7ed3\u5408\u4e86\u53ef\u9a8c\u8bc1\u7684\u7ed3\u679c\u548c\u6a21\u578b\u81ea\u8eab\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u4ee5\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u53cd\u9988\u5e76\u63d0\u5347RL\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u6846\u67b6\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u9996\u5148\uff0c\u5176\u4e8c\u8fdb\u5236\u53cd\u9988\u8fc7\u4e8e\u7a00\u758f\uff0c\u65e0\u6cd5\u6355\u6349\u63a8\u7406\u8fc7\u7a0b\u7684\u8d28\u91cf\uff1b\u5176\u6b21\uff0c\u5176\u7c97\u7c92\u5ea6\u5956\u52b1\u53ef\u80fd\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\u3002\u53d7\u4eba\u7c7b\u5b66\u4e60\u89c2\u5bdf\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cdRL\u6280\u672f\uff0c\u5c06\u53ef\u9a8c\u8bc1\u7684\u7ed3\u679c\u4e0e\u6a21\u578b\u81ea\u8eab\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u76f8\u7ed3\u5408\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cdRL\u6280\u672f\uff0c\u5c06\u53ef\u9a8c\u8bc1\u7684\u7ed3\u679c\u4e0e\u6a21\u578b\u81ea\u8eab\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u76f8\u7ed3\u5408\u3002\u8fd9\u79cd\u8054\u5408\u8bbe\u8ba1\u4e30\u5bcc\u4e86\u5956\u52b1\u4fe1\u53f7\uff0c\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u53cd\u9988\u5e76\u9690\u5f0f\u5730\u76d1\u7763\u4e86\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86RL\u6027\u80fd\uff0c\u5e76\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51cf\u5c11\u4e86\u4ee4\u724c\u6d88\u8017\uff0c\u540c\u65f6\u589e\u52a0\u4e86\u5fae\u4e0d\u8db3\u9053\u7684\u989d\u5916\u8bad\u7ec3\u6210\u672c\u3002\u6b64\u5916\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u63d2\u4ef6\u6a21\u5757\u6765\u589e\u5f3a\u5176\u4ed6\u6700\u5148\u8fdb\u7684RL\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86RL\u6027\u80fd\uff0c\u5e76\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51cf\u5c11\u4e86\u4ee4\u724c\u6d88\u8017\uff0c\u540c\u65f6\u589e\u52a0\u4e86\u5fae\u4e0d\u8db3\u9053\u7684\u989d\u5916\u8bad\u7ec3\u6210\u672c\u3002\u6b64\u5916\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u63d2\u4ef6\u6a21\u5757\u6765\u589e\u5f3a\u5176\u4ed6\u6700\u5148\u8fdb\u7684RL\u65b9\u6cd5\u3002"}}
{"id": "2509.17740", "pdf": "https://arxiv.org/pdf/2509.17740", "abs": "https://arxiv.org/abs/2509.17740", "authors": ["Yiwen Jiang", "Deval Mehta", "Siyuan Yan", "Yaling Shen", "Zimu Wang", "Zongyuan Ge"], "title": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual\nreasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly\nenhancing interpretability. However, existing MCoT methods rely on\nrationale-rich datasets and largely focus on inter-object reasoning,\noverlooking the intra-object understanding crucial for image classification. To\naddress this gap, we propose WISE, a Weak-supervision-guided Step-by-step\nExplanation method that augments any image classification dataset with MCoTs by\nreformulating the concept-based representations from Concept Bottleneck Models\n(CBMs) into concise, interpretable reasoning chains under weak supervision.\nExperiments across ten datasets show that our generated MCoTs not only improve\ninterpretability by 37% but also lead to gains in classification accuracy when\nused to fine-tune MLLMs. Our work bridges concept-based interpretability and\ngenerative MCoT reasoning, providing a generalizable framework for enhancing\nMLLMs in fine-grained visual understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86WISE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f31\u76d1\u7763\u5f15\u5bfc\u7684\u9010\u6b65\u89e3\u91ca\uff0c\u589e\u5f3a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u7684MCoTs\uff0c\u4ece\u800c\u63d0\u9ad8MLLM\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684MCoT\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bcc\u542b\u7406\u7531\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4e3b\u8981\u5173\u6ce8\u5bf9\u8c61\u95f4\u7684\u63a8\u7406\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u4e8e\u56fe\u50cf\u5206\u7c7b\u81f3\u5173\u91cd\u8981\u7684\u5bf9\u8c61\u5185\u7406\u89e3\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86WISE\uff0c\u4e00\u79cd\u5f31\u76d1\u7763\u5f15\u5bfc\u7684\u9010\u6b65\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u8868\u8ff0\u6765\u81ea\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u7684\u6982\u5ff5\u8868\u793a\uff0c\u4ee5\u5728\u5f31\u76d1\u7763\u4e0b\u751f\u6210\u7b80\u6d01\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\uff0c\u4ece\u800c\u589e\u5f3a\u4efb\u4f55\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u7684MCoTs\u3002", "result": "\u5728\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u751f\u6210\u7684MCoTs\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u602737%\uff0c\u800c\u4e14\u5728\u7528\u4e8e\u5fae\u8c03MLLM\u65f6\u8fd8\u5e26\u6765\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u7684\u63d0\u5347\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u5c06\u57fa\u4e8e\u6982\u5ff5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u751f\u6210\u5f0fMCoT\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\uff0c\u4ee5\u589e\u5f3aMLLM\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17834", "pdf": "https://arxiv.org/pdf/2509.17834", "abs": "https://arxiv.org/abs/2509.17834", "authors": ["Duygu Kabakci-Zorlu", "Fabio Lorenzi", "John Sheehan", "Karol Lynch", "Bradley Eck"], "title": "From Documents to Database: Failure Modes for Industrial Assets", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": "7 pages, 4 figures. Artificial Intelligence for Knowledge Acquisition\n  & Management (AI4KAM) Workshop @ IJCAI 2025", "summary": "We propose an interactive system using foundation models and user-provided\ntechnical documents to generate Failure Mode and Effects Analyses (FMEA) for\nindustrial equipment. Our system aggregates unstructured content across\ndocuments to generate an FMEA and stores it in a relational database.\nLeveraging this tool, the time required for creation of this\nknowledge-intensive content is reduced, outperforming traditional manual\napproaches. This demonstration showcases the potential of foundation models to\nfacilitate the creation of specialized structured content for enterprise asset\nmanagement systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u548c\u7528\u6237\u63d0\u4f9b\u7684\u6280\u672f\u6587\u6863\u751f\u6210\u5de5\u4e1a\u8bbe\u5907FMEA\u7684\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u80fd\u591f\u51cf\u5c11\u521b\u5efa\u77e5\u8bc6\u5bc6\u96c6\u578b\u5185\u5bb9\u7684\u65f6\u95f4\uff0c\u5e76\u5c06\u5176\u5b58\u50a8\u5728\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u3002", "motivation": "\u51cf\u5c11\u521b\u5efa\u8fd9\u79cd\u77e5\u8bc6\u5bc6\u96c6\u578b\u5185\u5bb9\u6240\u9700\u7684\u65f6\u95f4\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u624b\u52a8\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u548c\u7528\u6237\u63d0\u4f9b\u7684\u6280\u672f\u6587\u6863\u751f\u6210\u5de5\u4e1a\u8bbe\u5907\u7684\u6545\u969c\u6a21\u5f0f\u548c\u5f71\u54cd\u5206\u6790\uff08FMEA\uff09\u7684\u4ea4\u4e92\u5f0f\u7cfb\u7edf\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u6574\u5408\u8de8\u6587\u6863\u7684\u975e\u7ed3\u6784\u5316\u5185\u5bb9\u4ee5\u751f\u6210FMEA\uff0c\u5e76\u5c06\u5176\u5b58\u50a8\u5728\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u4f01\u4e1a\u8d44\u4ea7\u7ba1\u7406\u7cfb\u7edf\u4e2d\u4fc3\u8fdb\u521b\u5efa\u4e13\u4e1a\u7ed3\u6784\u5316\u5185\u5bb9\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18008", "pdf": "https://arxiv.org/pdf/2509.18008", "abs": "https://arxiv.org/abs/2509.18008", "authors": ["Bingsheng Yao", "Jiaju Chen", "Chaoran Chen", "April Wang", "Toby Jia-jun Li", "Dakuo Wang"], "title": "Through the Lens of Human-Human Collaboration: A Configurable Research Platform for Exploring Human-Agent Collaboration", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Intelligent systems have traditionally been designed as tools rather than\ncollaborators, often lacking critical characteristics that collaboration\npartnerships require. Recent advances in large language model (LLM) agents open\nnew opportunities for human-LLM-agent collaboration by enabling natural\ncommunication and various social and cognitive behaviors. Yet it remains\nunclear whether principles of computer-mediated collaboration established in\nHCI and CSCW persist, change, or fail when humans collaborate with LLM agents.\nTo support systematic investigations of these questions, we introduce an open\nand configurable research platform for HCI researchers. The platform's modular\ndesign allows seamless adaptation of classic CSCW experiments and manipulation\nof theory-grounded interaction controls. We demonstrate the platform's\neffectiveness and usability through two case studies: (1) re-implementing the\nclassic human-human-collaboration task Shape Factory as a between-subject\nhuman-agent-collaboration experiment with 16 participants, and (2) a\nparticipatory cognitive walkthrough with five HCI researchers to refine\nworkflows and interfaces for experiment setup and analysis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u7814\u7a76\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u534f\u4f5c\u7684\u5f00\u653e\u5e73\u53f0\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u667a\u80fd\u7cfb\u7edf\u901a\u5e38\u88ab\u8bbe\u8ba1\u4e3a\u5de5\u5177\u800c\u975e\u5408\u4f5c\u4f19\u4f34\uff0c\u7f3a\u4e4f\u534f\u4f5c\u6240\u9700\u7684\u7279\u6027\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u8fdb\u6b65\u4e3a\u4eba\u7c7b\u4e0e\u4ee3\u7406\u7684\u5408\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u4eba\u673a\u534f\u4f5c\u662f\u5426\u9075\u5faaHCI\u548cCSCW\u4e2d\u7684\u539f\u5219\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u8bbe\u8ba1\u7684\u7814\u7a76\u5e73\u53f0\uff0c\u5141\u8bb8\u5bf9\u7ecf\u5178CSCW\u5b9e\u9a8c\u8fdb\u884c\u65e0\u7f1d\u9002\u5e94\u548c\u7406\u8bba\u57fa\u7840\u7684\u4ea4\u4e92\u63a7\u5236 manipulation\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5e73\u53f0\u7684\u6709\u6548\u6027\u548c\u53ef\u7528\u6027\uff1a(1) \u5c06\u7ecf\u5178\u7684Shape Factory\u4efb\u52a1\u91cd\u65b0\u5b9e\u73b0\u4e3a\u4eba\u7c7b-\u4ee3\u7406\u534f\u4f5c\u5b9e\u9a8c\uff0c(2) \u4e0e\u4e94\u4f4dHCI\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u53c2\u4e0e\u5f0f\u8ba4\u77e5\u8d70\u67e5\u4ee5\u4f18\u5316\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u5206\u6790\u6d41\u7a0b\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u53ef\u4ee5\u652f\u6301\u5bf9\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5408\u4f5c\u7684\u7cfb\u7edf\u7814\u7a76\u3002"}}
{"id": "2509.18083", "pdf": "https://arxiv.org/pdf/2509.18083", "abs": "https://arxiv.org/abs/2509.18083", "authors": ["Valentin Lacombe", "Valentin Quesnel", "Damien Sileo"], "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86 Reasoning Core\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u53ef\u6269\u5c55\u73af\u5883\uff0c\u65e8\u5728\u63a8\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63a8\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u57fa\u7840\u7b26\u53f7\u63a8\u7406\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u751f\u6210\u8de8\u6838\u5fc3\u5f62\u5f0f\u9886\u57df\u7684\u4efb\u52a1\uff0c\u5305\u62ecPDDL\u89c4\u5212\u3001\u4e00\u9636\u903b\u8f91\u3001\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u6cd5\u89e3\u6790\u3001\u56e0\u679c\u63a8\u7406\u548c\u7cfb\u7edf\u65b9\u7a0b\u6c42\u89e3\u3002", "result": "\u521d\u59cb\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u786e\u8ba4\u4e86 Reasoning Core \u4efb\u52a1\u7684\u96be\u5ea6\u3002", "conclusion": "Reasoning Core \u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u8d44\u6e90\uff0c\u53ef\u4ee5\u63d0\u9ad8\u672a\u6765\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.18085", "pdf": "https://arxiv.org/pdf/2509.18085", "abs": "https://arxiv.org/abs/2509.18085", "authors": ["Sudhanshu Agrawal", "Risheek Garrepalli", "Raghavv Goel", "Mingu Lee", "Christopher Lott", "Fatih Porikli"], "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.", "AI": {"tldr": "Spiffy is a speculative decoding algorithm that accelerates diffusion LLMs by 2.8-3.1x while maintaining output quality. It uses a novel graph structure and calibration algorithm to improve efficiency and is compatible with other speed-enhancing techniques.", "motivation": "Current open-source dLLMs generate at lower rates by decoding only a single token per denoising timestep. The goal is to accelerate dLLM inference without compromising output quality.", "method": "Spiffy leverages the dLLM's distribution in an auto-speculative manner to propose draft states. It introduces a novel directed draft graph and an efficient, offline calibration algorithm to optimize the structure of these graphs.", "result": "Spiffy achieves a 2.8-3.1x speedup in dLLM inference. When combined with other methods like KV-caching and multi-token unmasking, it can lead to up to 7.9x total speedups.", "conclusion": "Spiffy is a speculative decoding algorithm that significantly accelerates dLLM inference while preserving the model's output distribution. It is effective, efficient, and complementary to other innovations in improving dLLM generation speeds."}}
{"id": "2509.18091", "pdf": "https://arxiv.org/pdf/2509.18091", "abs": "https://arxiv.org/abs/2509.18091", "authors": ["Sunhao Dai", "Jiakai Tang", "Jiahua Wu", "Kun Wang", "Yuxuan Zhu", "Bingjun Chen", "Bangyang Hong", "Yu Zhao", "Cong Fu", "Kangle Wu", "Yabo Ni", "Anxiang Zeng", "Wenjie Wang", "Xu Chen", "Jun Xu", "See-Kiong Ng"], "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "OnePiece Technical Report; Applied in Shopee", "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OnePiece\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5c06LLM\u98ce\u683c\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u63a8\u7406\u96c6\u6210\u5230\u5de5\u4e1a\u7ea7\u7ea7\u8054\u7ba1\u9053\u7684\u68c0\u7d22\u548c\u6392\u5e8f\u6a21\u578b\u4e2d\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u7eafTransformer\u4e3b\u5e72\uff0c\u5e76\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3001\u5757\u72b6\u6f5c\u5728\u63a8\u7406\u548c\u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u8bad\u7ec3\u3002OnePiece\u5df2\u5728Shopee\u7684\u4e3b\u8981\u4e2a\u6027\u5316\u641c\u7d22\u573a\u666f\u4e2d\u90e8\u7f72\uff0c\u5e76\u5728\u591a\u4e2a\u5173\u952e\u4e1a\u52a1\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u590d\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5de5\u4e1a\u641c\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6269\u5c55\u6210\u529f\u8d8a\u6765\u8d8a\u611f\u5174\u8da3\uff0c\u4f46\u73b0\u6709\u7684\u5de5\u4e1a\u52aa\u529b\u5927\u591a\u5c40\u9650\u4e8e\u79fb\u690dTransformer\u67b6\u6784\uff0c\u8fd9\u4ec5\u5e26\u6765\u4e86\u76f8\u5bf9\u4e8e\u5f3a\u5927\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\uff08DLRMs\uff09\u7684\u6e10\u8fdb\u5f0f\u6539\u8fdb\u3002\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u7684\u89d2\u5ea6\u6765\u770b\uff0cLLMs\u7684\u7a81\u7834\u4e0d\u4ec5\u6765\u81ea\u4e8e\u5176\u67b6\u6784\uff0c\u8fd8\u6765\u81ea\u4e8e\u4e24\u79cd\u4e92\u8865\u673a\u5236\uff1a\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u591a\u6b65\u9aa4\u63a8\u7406\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u673a\u5236\u53ca\u5176\u6f5c\u5728\u7684\u663e\u8457\u6539\u8fdb\u5c1a\u672a\u5728\u5de5\u4e1a\u6392\u540d\u7cfb\u7edf\u4e2d\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "OnePiece\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u65e0\u7f1d\u96c6\u6210LLM\u98ce\u683c\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u63a8\u7406\u5230\u5de5\u4e1a\u7ea7\u7ea7\u8054\u7ba1\u9053\u7684\u68c0\u7d22\u548c\u6392\u5e8f\u6a21\u578b\u4e2d\u3002\u5b83\u57fa\u4e8e\u7eafTransformer\u4e3b\u5e72\uff0c\u5e76\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1) \u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff0c(2) \u5757\u72b6\u6f5c\u5728\u63a8\u7406\uff0c(3) \u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u8bad\u7ec3\u3002", "result": "OnePiece\u5df2\u88ab\u90e8\u7f72\u5728Shopee\u7684\u4e3b\u8981\u4e2a\u6027\u5316\u641c\u7d22\u573a\u666f\u4e2d\uff0c\u5e76\u5728\u4e0d\u540c\u5173\u952e\u4e1a\u52a1\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u5728\u7ebf\u6536\u76ca\uff0c\u5305\u62ec\u8d85\u8fc7+2%\u7684GMV/UU\u548c\u5e7f\u544a\u6536\u5165\u589e\u52a0\u4e86+2.90%\u3002", "conclusion": "OnePiece\u5df2\u88ab\u90e8\u7f72\u5728Shopee\u7684\u4e3b\u8981\u4e2a\u6027\u5316\u641c\u7d22\u573a\u666f\u4e2d\uff0c\u5e76\u5728\u4e0d\u540c\u5173\u952e\u4e1a\u52a1\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u5728\u7ebf\u6536\u76ca\uff0c\u5305\u62ec\u8d85\u8fc7+2%\u7684GMV/UU\u548c\u5e7f\u544a\u6536\u5165\u589e\u52a0\u4e86+2.90%\u3002"}}
{"id": "2509.18095", "pdf": "https://arxiv.org/pdf/2509.18095", "abs": "https://arxiv.org/abs/2509.18095", "authors": ["Zilin Xiao", "Qi Ma", "Mengting Gu", "Chun-cheng Jason Chen", "Xintao Chen", "Vicente Ordonez", "Vijai Mohan"], "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction", "categories": ["cs.IR", "cs.CL", "cs.CV"], "comment": null, "summary": "Universal multimodal embedding models have achieved great success in\ncapturing semantic relevance between queries and candidates. However, current\nmethods either condense queries and candidates into a single vector,\npotentially limiting the expressiveness for fine-grained information, or\nproduce too many vectors that are prohibitively expensive for multi-vector\nretrieval. In this work, we introduce MetaEmbed, a new framework for multimodal\nretrieval that rethinks how multimodal embeddings are constructed and\ninteracted with at scale. During training, a fixed number of learnable Meta\nTokens are appended to the input sequence. At test-time, their last-layer\ncontextualized representations serve as compact yet expressive multi-vector\nembeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,\nMetaEmbed learns to organize information by granularity across multiple\nvectors. As a result, we enable test-time scaling in multimodal retrieval,\nwhere users can balance retrieval quality against efficiency demands by\nselecting the number of tokens used for indexing and retrieval interactions.\nExtensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and\nthe Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed\nachieves state-of-the-art retrieval performance while scaling robustly to\nmodels with 32B parameters.", "AI": {"tldr": "MetaEmbed is a new framework for multimodal retrieval that uses learnable Meta Tokens to create compact yet expressive multi-vector embeddings, achieving state-of-the-art performance while enabling efficient scaling.", "motivation": "Current methods either condense queries and candidates into a single vector, limiting expressiveness, or produce too many vectors that are expensive for multi-vector retrieval.", "method": "MetaEmbed introduces a framework for multimodal retrieval that uses a fixed number of learnable Meta Tokens during training and their last-layer contextualized representations as compact yet expressive multi-vector embeddings at test-time.", "result": "MetaEmbed enables test-time scaling in multimodal retrieval, allowing users to balance retrieval quality against efficiency demands by selecting the number of tokens used for indexing and retrieval interactions.", "conclusion": "MetaEmbed achieves state-of-the-art retrieval performance while scaling robustly to models with 32B parameters."}}

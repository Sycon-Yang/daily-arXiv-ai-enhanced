<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 73]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 本文总结了在医疗环境中实施NLP解决方案的关键经验，包括明确业务目标、迭代开发、跨学科合作、模型选择、数据质量和错误缓解策略等，为医疗组织提供了指导。


<details>
  <summary>Details</summary>
Motivation: 自动化从临床文档中提取数据可以显著提高医疗环境的效率，但部署自然语言处理（NLP）解决方案存在实际挑战。

Method: 本文基于在不列颠哥伦比亚癌症登记处（BCCR）实施各种NLP模型进行信息提取和分类任务的经验，分享了项目生命周期中的关键教训。

Result: 本文提出了定义问题应基于明确的业务目标而非仅技术准确性、采用迭代开发方法、促进跨学科合作和共同设计等关键教训，并强调了实用模型选择、数据质量、错误缓解策略和组织AI素养的重要性。

Conclusion: 本文强调了在医疗领域成功实施AI/NLP解决方案的关键实践考虑因素，并提供了指导，以提高数据管理流程和最终改善患者护理和公共卫生结果。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [2] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 本文提出了一种基于区块链的透明评估协议，用于评估开源大型语言模型的公平性。通过智能合约在Internet Computer Protocol (ICP)上执行链上HTTP请求，确保评估的可验证性和可重复性。并对多个模型和语言进行了公平性评估，揭示了跨语言差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在现实世界应用中越来越普遍，但它们的公平性问题仍然存在，尤其是在刑事司法、教育、医疗保健和金融等高风险领域。因此，需要一种透明且可验证的评估方法来衡量LLMs的公平性。

Method: 本文提出了一种基于区块链的透明评估协议，利用智能合约在Internet Computer Protocol (ICP)上执行链上HTTP请求，以评估开源大型语言模型的公平性。同时，将数据集、提示和指标直接存储在链上，确保评估的可验证性和可重复性。

Result: 本文对Llama、DeepSeek和Mistral模型在PISA数据集上进行了评估，使用统计平等和机会平等度量标准。还评估了来自StereoSet数据集的结构化上下文关联度量标准，以测量社会偏见。此外，还在英语、西班牙语和葡萄牙语中进行了多语言评估，揭示了跨语言差异。

Conclusion: 本文提出了一种透明的评估协议，用于在Internet Computer Protocol (ICP)区块链上使用智能合约对开源大型语言模型（LLMs）进行公平性基准测试。该方法确保了可验证、不可变和可重复的评估，并通过链上HTTP请求执行，将数据集、提示和指标直接存储在链上。所有代码和结果都是开源的，可以进行社区审计和长期公平性跟踪。

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [3] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [4] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 研究引入了INTIMA基准，用于评估语言模型中的陪伴行为。结果显示，尽管陪伴强化行为普遍，但不同模型在敏感部分的优先级不同，这可能影响用户福祉。


<details>
  <summary>Details</summary>
Motivation: AI陪伴已成为一种重要模式，但其积极和令人担忧的影响都需要关注。需要评估语言模型中的陪伴行为，以确保适当的边界设定和情感支持。

Method: 引入了INTIMA基准，用于评估语言模型中的陪伴行为。基于心理学理论和用户数据，开发了一个包含31种行为的分类体系，并使用368个针对性提示进行评估。

Result: 对Gemma-3、Phi-4、o3-mini和Claude-4的评估显示，陪伴强化行为在所有模型中都更为常见，但不同模型之间存在显著差异。

Conclusion: 研究结果表明，需要更一致的方法来处理情绪化的互动。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [5] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 本文提出XFacta数据集并评估多种MLLM-based虚假信息检测策略，构建半自动框架以保持数据集的当代相关性，为该领域提供有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 现有的基准数据集要么包含过时的事件，导致由于与当代社交媒体场景的差异而产生评估偏差，要么是人工合成的，无法反映现实世界的虚假信息模式。此外，缺乏对MLLM-based模型设计策略的全面分析。

Method: 我们引入了XFacta，一个更适合评估MLLM-based检测器的当代、现实世界的数据集，并系统地评估了各种MLLM-based虚假信息检测策略，同时构建了一个半自动的检测循环框架，以持续更新XFacta以保持其当代相关性。

Result: 我们系统地评估了各种MLLM-based虚假信息检测策略，评估了不同架构和规模的模型，并与现有的检测方法进行了比较。通过这些分析，我们进一步实现了半自动的检测循环框架，以持续更新XFacta以保持其当代相关性。

Conclusion: 我们的分析为推进多模态虚假信息检测领域提供了有价值的见解和实践。

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [6] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型生成合成数据来提高文本分类模型性能的方法，并通过实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，由于难以收集所有文本类别的足够数据，因此需要一种方法来生成合成数据以提高模型性能。

Method: 本文提出了一种自动化的工作流程，用于搜索能够生成更有效合成数据的输入示例，并研究了三种搜索策略。此外，还设计了一个集成算法，根据类别的特征选择最佳的搜索策略。

Result: 实验结果表明，该集成方法比单独的搜索策略更有效。

Conclusion: 本文提出了一种使用大型语言模型生成合成数据来提高文本分类模型性能的方法，并通过实验验证了该方法的有效性。

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [7] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 本文介绍了HiFACT数据集，并提出了一种新的基于图的多语言事实检查模型，该模型在事实验证任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在像印度这样的语言多样地区，需要强大的多语言和上下文感知的事实检查工具，以应对政治言论中的事实核查问题。

Method: 提出了一种基于图的、检索增强的事实检查模型，结合了多语言上下文编码、声明-证据语义对齐、证据图构建、图神经推理和自然语言解释生成。

Result: HiFACTMix在与最先进的多语言基线模型比较中表现出更高的准确性，并为其判决提供了可信的解释。

Conclusion: 本文为多语言、混合语言和政治基础的事实验证研究开辟了新的方向。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [8] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 研究发现大型语言模型中的语义结构与人类评分相似，具有低维特性，并且语义特征在模型中交织在一起。这表明在操控特征时需要考虑这种语义结构以避免意外后果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索大型语言模型中的语义结构是否与人类语言中的语义结构相似，并评估这种结构在操控特征时的重要性。

Method: 通过分析大型语言模型（LLMs）嵌入矩阵中的语义关联，研究者发现这些嵌入具有与人类评分相似的结构。他们展示了基于反义对（如“善良 - 残忍”）定义的语义方向上的单词投影与人类评分高度相关，并进一步发现这些投影可以有效地减少到LLM嵌入中的三维子空间，与从人类调查结果中得出的模式密切相关。

Result: 研究发现，LLMs中的语义关联具有类似人类评分的低维结构。单词在基于反义对定义的语义方向上的投影与人类评分高度相关，并且这些投影可以有效地减少到三维子空间。此外，沿着一个语义方向移动标记会导致与几何对齐特征成比例的副作用。

Conclusion: 这些发现表明，语义特征在LLMs中以类似于人类语言的方式交织在一起，尽管语义信息似乎复杂，但其维度却出乎意料地低。此外，考虑到这种语义结构对于避免在操控特征时产生意外后果可能至关重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [9] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 研究评估了基于注意力的解释在生物医学文献分类中的作用，并发现注意力权重的感知帮助性受其可视化方式的影响。


<details>
  <summary>Details</summary>
Motivation: 在循证医学中，注意力权重可能有助于医生理解AI系统对生物医学文献的分类。然而，目前尚无共识认为注意力权重提供了有用的解释，且很少有研究探讨可视化如何影响其作为解释辅助工具的效果。

Method: 我们进行了一项用户研究，以评估基于注意力的解释是否有助于医学文献分类，并探讨了注意力可视化的方式。

Result: Transformer模型（XLNet）准确地对文档进行了分类；然而，注意力权重未被普遍认为是解释预测的有用工具。然而，这种看法在很大程度上取决于注意力的可视化方式。

Conclusion: 我们的研究结果并未确认注意力权重在解释中的总体效用，但表明其感知帮助性受到视觉呈现方式的影响。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [10] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本文介绍了EQGBench，这是一个用于评估大型语言模型在中文教育问题生成中的性能的基准，旨在推动教育问题生成的发展，并揭示了在生成具有教育价值的问题方面的改进空间。


<details>
  <summary>Details</summary>
Motivation: 为了推进教育问题生成（EQG）并使LLMs能够生成具有教育价值和教育效果的问题，需要一个专门的基准来评估LLMs的表现。

Method: 引入EQGBench，这是一个专门设计用于评估LLMs在中文教育问题生成（EQG）中的性能的全面基准。

Result: EQGBench建立了一个由900个评估样本组成的数据库，涵盖了三个基础初中学科：数学、物理和化学。该数据集包含了不同知识要点、难度梯度和问题类型规范的用户查询，以模拟真实的教育场景。

Conclusion: 通过系统评估46个主流大型模型，我们揭示了在生成反映教育价值和促进学生综合能力的问题方面还有很大的发展空间。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [11] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 研究显示，大型语言模型可以有效自动化AIHQ开放式回答的评分，提高心理评估的效率。


<details>
  <summary>Details</summary>
Motivation: 传统的AIHQ开放式问题需要耗时的人工评分，而本研究旨在探索是否可以利用大型语言模型自动化这一过程。

Method: 研究使用了之前收集的数据集，其中包含有创伤性脑损伤（TBI）和健康对照组（HC）完成的AIHQ，并对其开放式回答进行了人工评分。研究者使用了一半的回答来微调两个模型，然后在剩余的一半回答上测试这些模型。

Result: 模型生成的评分与人工评分一致，微调后的模型表现出更高的一致性。这种一致性在不同类型的场景中都得到了保持，并且在TBI和HC群体之间重复了之前发现的差异。此外，微调模型在独立的非临床数据集中也表现良好。

Conclusion: 研究结果表明，大型语言模型可以简化AIHQ的评分，从而在研究和临床环境中提高心理评估的效率。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [12] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 本文研究了如何通过贝叶斯融合来避免对大型语言模型进行昂贵的微调，从而提高在线课程讨论论坛的自动整理效果。


<details>
  <summary>Details</summary>
Motivation: 自动整理在线课程中的讨论论坛需要不断更新，这使得频繁重新训练大型语言模型成为一项资源密集型任务。为了规避高昂的微调需求，本文提出了贝叶斯融合方法。

Method: 该论文提出了贝叶斯融合方法，将预训练通用大型语言模型的多维分类分数与本地数据训练的分类器的分数相结合。

Result: 实验结果表明，所提出的融合方法在性能上优于单独的分类器，并且与大型语言模型微调方法相当。

Conclusion: 该论文提出了一种使用贝叶斯融合的方法，以避免对大型语言模型进行昂贵的微调。结果表明，该方法在性能上优于单独的分类器，并且与微调方法相当。

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [13] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 本文提出了一种名为S-MoE的监督混合专家模型，通过特殊引导标记路由任务到指定专家，避免了任务干扰，提升了语音识别和语音翻译的性能。


<details>
  <summary>Details</summary>
Motivation: 硬参数共享会导致任务干扰，影响整体模型性能，因此需要一种更有效的解决方案。

Method: S-MoE通过使用特殊的引导标记来路由每个任务到其指定的专家，从而消除了训练门控函数的需要。

Result: 实验结果表明，S-MoE在编码器和解码器上应用时，词错误率（WER）相对提高了6.35%。

Conclusion: S-MoE在语音识别和语音翻译任务中表现出色，能够有效克服硬参数共享带来的任务干扰问题。

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [14] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: This paper explores the use of Large Language Models (LLMs) in detecting and preventing the spread of misinformation, including harmful medical misinformation generated by jailbreak attacks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are capable of generating harmful misinformation, either inadvertently or through 'jailbreak' attacks. There is potential for LLMs to be used in detecting and preventing the spread of misinformation.

Method: We investigated the efficacy and characteristics of LLM-produced jailbreak attacks that cause other models to produce harmful medical misinformation. We also studied how misinformation generated by jailbroken LLMs compares to typical misinformation found on social media and how effectively it can be detected using standard machine learning approaches.

Result: Our findings show that LLMs can be effectively used to detect misinformation from both other LLMs and from people. Additionally, we found that jailbreak attacks can generate misinformation that is similar to health-related misinformation found on social media.

Conclusion: LLMs can be effectively used to detect misinformation from both other LLMs and from people, and with careful design, they can contribute to a healthier overall information ecosystem.

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [15] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 本研究评估了生成式AI模型在营养学考试中的表现，发现部分模型勉强超过及格线，但整体准确性和一致性仍不足，且存在答案稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估当前基于大型语言模型的生成式AI模型作为营养学生学习辅助工具的潜力。

Method: 本研究使用日本注册营养师国家考试的问题作为提示，评估了ChatGPT和三个Bing模型（Precise、Creative、Balanced）的表现。每个问题被输入到独立会话中，并分析了模型响应的准确性、一致性和响应时间。还测试了额外的提示工程，包括角色分配，以评估潜在的性能改进。

Result: Bing-Precise（66.2%）和Bing-Creative（61.4%）超过了及格阈值（60%），而Bing-Balanced（43.3%）和ChatGPT（42.8%）未达到。Bing-Precise和Bing-Creative在除营养教育以外的所有学科领域通常表现优于其他模型。所有模型在重复尝试中并未始终提供相同的正确答案，突显了答案稳定性的限制。ChatGPT在响应模式上表现出更高的稳定性，但准确性较低。提示工程的影响很小，除非明确提供了正确答案和解释，才有适度的改善。

Conclusion: 虽然一些生成式AI模型勉强超过了及格阈值，但整体准确性和答案一致性仍然不足。此外，所有模型在答案一致性和鲁棒性方面都表现出显著的局限性。需要进一步的发展以确保可靠的和稳定的基于AI的学习辅助工具用于注册营养师执照准备。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [16] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架GG Explore，通过引入中间Guidance Graph来解决知识图谱探索中的问题，并在实验中证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中的表现受到其对静态知识的依赖和不透明推理过程的限制。知识图谱提供了一个有前途的解决方案，但当前探索方法面临根本性的权衡：问题引导的方法由于粒度不匹配而产生冗余探索，而线索引导的方法在复杂场景中无法有效利用上下文信息。

Method: 我们提出了Guidance Graph guided Knowledge Exploration (GG Explore)，引入了一个中间Guidance Graph来连接非结构化查询和结构化知识检索。

Result: 广泛的实验表明，我们的方法在效率方面优于最先进的方法，尤其是在复杂任务上，同时保持了小型大语言模型的强性能。

Conclusion: 我们的方法在复杂任务上表现出色，同时保持了小型大语言模型的强性能，展示了实际价值。

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [17] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: Semantic Bridge is a novel framework for generating controllable, complex multi-hop reasoning questions from sparse sources, achieving significant improvements in quality and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality, reasoning-intensive question-answer pairs from sparse, domain-specific sources like PubMed papers or legal documents is a critical bottleneck in large language model (LLM) training. Existing methods fail to generate controllable, complex multi-hop reasoning questions that test genuine understanding.

Method: Semantic Bridge is a universal framework that uses semantic graph weaving with three complementary bridging mechanisms (entity bridging, predicate chain bridging, and causal bridging) to generate complex multi-hop reasoning questions. It utilizes AMR-driven analysis for fine-grained control over complexity and types.

Result: Semantic Bridge achieves up to 9.5% better round-trip quality, yielding consistent 18.3%-25.4% gains over baselines across four languages. Question pairs generated from 200 sources outperform 600 native human annotation examples with 67% fewer materials. Human evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage.

Conclusion: Semantic Bridge establishes a new paradigm for LLM training data synthesis, enabling controllable generation of targeted reasoning questions from sparse sources.

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [18] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的基准PersonaEval，用于测试LLM评估者是否能准确识别对话中的人类角色。实验结果显示，目前最先进的LLM在角色识别任务上的表现远不如人类。


<details>
  <summary>Details</summary>
Motivation: 当前的角色扮演研究往往依赖于未经验证的LLM-as-a-judge范式，这可能无法反映人类如何感知角色一致性。

Method: 我们提出了PersonaEval，这是第一个设计用于测试LLM评估者是否能够可靠地识别人类角色的基准。

Result: 即使是最先进的LLM模型也只能达到约69%的准确率，远低于可靠评估所需的水平。而人类参与者则接近满分，达到了90.8%的准确率。

Conclusion: 当前的LLM评估者仍然不够人类化，无法有效地判断角色扮演场景。

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [19] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 本文介绍了RealTalk-CN，这是第一个中文多轮、多领域语音-文本双模态TOD数据集，旨在填补现有数据集的不足，并通过跨模态聊天任务进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有TOD数据集主要基于文本，缺乏真实的语音信号，且现有语音TOD数据集主要为英文，缺乏语音不流畅和说话人变化等关键方面。

Method: 引入了RealTalk-CN数据集，并提出了一种新的跨模态聊天任务，以真实模拟用户交互。

Result: RealTalk-CN包含5.4k对话（60K个发言，150小时），具有配对的语音-文本注释，涵盖了各种对话场景，并通过广泛的实验验证了其有效性。

Conclusion: RealTalk-CN的评估验证了其有效性，为中文语音基础大语言模型的研究奠定了坚实的基础。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [20] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Multimodal Large Language Model Orchestration的方法，该方法通过显式工作流程协调专业模型，从而在不进行额外训练的情况下创建交互式多模态AI系统。该框架包含三个关键创新：(1)一个中央控制器LLM，(2)一个并行文本到语音架构，(3)一个跨模态记忆集成系统。评估结果显示，MLLM Orchestration在标准基准测试中实现了全面的多模态能力，性能提升了7.8%，延迟减少了10.3%，并通过显式的编排过程显著提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: Different Multimodal Large Language Models (MLLMs) cannot be integrated into a unified multimodal input-output system directly. In previous work, training has been considered as an inevitable component due to challenges in modal alignment, Text-to-Speech efficiency and other integration issues.

Method: MLLM Orchestration leverages the inherent reasoning capabilities of large language models to coordinate specialized models through explicit workflows, enabling natural multimodal interactions while maintaining modularity, improving interpretability, and significantly enhancing computational efficiency.

Result: Extensive evaluations demonstrate that MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.

Conclusion: MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [21] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 本文提出了一种基于范畴同伦理论的框架，以解决大型语言模型在处理语义等价句子时生成不同下一个词概率的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理语义等价的句子时，通常无法生成相同的下一个词概率，因此需要一种更抽象的方法来解决这个问题。

Method: 引入了LLM马尔可夫范畴来表示由LLM生成的语言中的概率分布，并使用范畴同伦技术来捕捉LLM马尔可夫范畴中的“弱等价”。

Result: 本文详细概述了范畴同伦在LLM中的应用，包括高阶代数K-理论和模型范畴，并利用过去半个世纪发展的强大理论结果。

Conclusion: 本文提出了一种基于范畴同伦理论的框架，用于解决大型语言模型在处理语义等价句子时生成不同下一个词概率的问题。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [22] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: This paper proposes DURIT, a framework that decouples understanding from reasoning in Small Language Models (SLMs) by mapping natural language problems into a canonical problem space. Experiments show that DURIT significantly improves SLMs' performance on mathematical and logical reasoning tasks and enhances the robustness of reasoning.


<details>
  <summary>Details</summary>
Motivation: Improving the reasoning ability of Small Language Models (SLMs) remains challenging due to the complexity and variability of natural language. SLMs must extract the core problem from complex linguistic input and perform reasoning based on that understanding, which is difficult given their limited capacity.

Method: We propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively maps natural language problems via reinforcement learning, aligns reasoning trajectories through self-distillation, and trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process.

Result: Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Additionally, DURIT improves the robustness of reasoning.

Conclusion: DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [23] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: FedCoT is a novel framework designed to enhance reasoning in federated settings by leveraging a lightweight chain-of-thought enhancement mechanism and an improved aggregation approach, resulting in significant improvements in client-side reasoning performance while preserving data privacy.


<details>
  <summary>Details</summary>
Motivation: Efficiently enhancing the reasoning capabilities of large language models (LLMs) in federated learning environments remains challenging, particularly when balancing performance gains with strict computational, communication, and privacy constraints. In healthcare, decisions demand accurate outputs along with interpretable, traceable rationales to ensure safety, accountability, and regulatory compliance. Conventional federated tuning approaches on LLMs fail to address this need, as they optimize primarily for answer correctness while neglecting rationale quality.

Method: FedCoT leverages a lightweight chain-of-thought enhancement mechanism: local models generate multiple reasoning paths, and a compact discriminator dynamically selects the most promising one. To manage client heterogeneity efficiently, an improved aggregation approach building upon advanced LoRA module stacking is adopted, incorporating client classifier-awareness to achieve noise-free aggregation across diverse clients.

Result: Comprehensive experiments on medical reasoning tasks demonstrate that FedCoT significantly boosts client-side reasoning performance under stringent resource budgets while fully preserving data privacy.

Conclusion: FedCoT significantly boosts client-side reasoning performance under stringent resource budgets while fully preserving data privacy.

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [24] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: 本文提出了LATTE，一种对比学习框架，用于将原始事件嵌入与冻结LLM的语义嵌入对齐，从而减少推理成本和输入大小，并在金融数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 学习客户嵌入从他们的历史通信序列对于金融应用是核心的。虽然大型语言模型（LLMs）提供了通用的世界知识，但它们直接用于长事件序列在计算上是昂贵且不切实际的。

Method: 我们提出了LATTE，这是一种对比学习框架，将原始事件嵌入与冻结LLM的语义嵌入对齐。行为特征被总结成简短的提示，由LLM嵌入，并通过对比损失作为监督。

Result: 所提出的方法相比传统处理完整序列的LLM显著减少了推理成本和输入大小。实验表明，我们的方法在学习事件序列表示方面优于最先进的技术。

Conclusion: 我们的方法在真实世界的金融数据集上优于最先进的技术，同时在延迟敏感的环境中可部署。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [25] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 本研究提出了一种增强显著性检验的合规预测框架，以提高大型语言模型在多项选择题回答中的可信度，通过结合p值计算和符合性评分，解决了幻觉和事实错误的问题，并在多个基准测试中验证了其有效性


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在学科问答场景中被越来越多地部署，但幻觉和非事实性生成严重损害了响应的可靠性。虽然CP提供了预测集的统计严格边缘覆盖保证，而显著性检验提供了已建立的统计严谨性，但它们的协同集成仍未被探索

Method: 本研究引入了一种增强显著性检验的合规预测（CP）框架，通过自我一致性重采样MCQA响应中的p值计算与符合性评分相结合，以解决LLMs的黑箱性质，并通过零假设检验构建预测集

Result: 在MMLU和MMLU-Pro基准测试中使用现成的LLMs进行评估表明：(1) 增强的CP实现了用户指定的经验错误覆盖率；(2) 测试集平均预测集大小（APSS）随着风险水平（α）的增加而单调减少，验证了APSS作为有效不确定性度量的有效性

Conclusion: 本研究建立了一个有原则的统计框架，用于在高风险问答应用中可信地部署大型语言模型（LLMs）

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [26] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: RTTC is a novel framework that adaptively selects the most effective TTC strategy for each query using a pretrained reward model, achieving superior accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The optimal adaptation strategy for Test-Time Compute (TTC) varies across queries, and indiscriminate application of TTC strategies incurs substantial computational overhead. There is a need for an adaptive, reward-guided approach to select the most effective TTC strategy for each query.

Method: RTTC is a novel framework that adaptively selects the most effective TTC strategy for each query via a pretrained reward model. It operates in a distributed server-client architecture, retrieving relevant samples from a remote knowledge base and applying RAG or lightweight fine-tuning on client devices only when necessary. Additionally, Query-State Caching is proposed to mitigate redundant computation by enabling the efficient reuse of historical query states at both retrieval and adaptation levels.

Result: Extensive experiments across multiple LLMs and benchmarks demonstrate that RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT.

Conclusion: RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT, validating the necessity of adaptive, reward-guided TTC selection and the potential of RTTC for scalable, high-performance language model adaptation.

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [27] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 本研究提出了一种基于自然语言处理、机器学习和大语言模型的智能产后抑郁筛查系统，能够实时、非侵入性地检测产后抑郁，并通过可解释的模型解决黑箱问题。结果表明，该系统在PPD检测上的准确率达到90%，优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁（PPD）是一种严重影响母亲心理健康和身体健康的严重状况。因此，快速检测PPD及其相关风险因素对于通过专门的预防程序进行及时评估和干预至关重要。本研究旨在利用最新的技术进步，帮助从业者做出决策，以实现实时筛查和治疗建议。

Method: 本研究的方法是开发一个智能产后抑郁筛查系统，结合自然语言处理、机器学习（ML）和大语言模型（LLMs），以实现经济实惠、实时且非侵入性的语音分析。此外，通过将LLMs与可解释的ML模型（即基于树的算法）结合使用特征重要性和自然语言，解决了黑箱问题。

Result: 获得的结果是，在所有评估指标上，PPD检测的准确率为90%，优于文献中的竞争解决方案。

Conclusion: 本研究的结论是，我们的解决方案有助于快速检测产后抑郁及其相关风险因素，这对于及时和适当的评估和干预至关重要。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [28] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: SABER is a reinforcement learning framework that enables efficient reasoning for large language models by allowing user-controllable, token-budgeted reasoning. It supports different inference modes for flexible trade-offs between latency and reasoning depth, achieving high accuracy under tight budgets and effective generalization across domains.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems.

Method: SABER is a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. It profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. No-think examples are incorporated to ensure the model remains reliable even when explicit reasoning is turned off. SABER supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink.

Result: Extensive evaluations on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning (LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.

Conclusion: SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization.

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [29] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 本研究探讨了如何利用语音中的语言标记来检测阿尔茨海默病及相关痴呆症（ADRD）。通过融合变压器嵌入和手工制作的语言特征，并使用大型语言模型生成的合成语音进行数据增强，提高了检测效果。研究还测试了单模态和多模态LLM分类器的表现，发现临床调优的LLM在分类和数据增强方面有效，而多模态建模仍需改进。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病及相关痴呆症（ADRD）影响约五百万美国老年人，但超过一半未被诊断。基于语音的自然语言处理（NLP）提供了一种有前途的、可扩展的方法，通过语言标记检测早期认知衰退。

Method: 该研究开发并评估了一个筛选流程，包括（i）融合变压器嵌入和手工制作的语言特征，（ii）使用大型语言模型（LLMs）生成的合成语音进行数据增强，以及（iii）对单模态和多模态LLM分类器进行基准测试。

Result: 融合模型达到了F1 = 83.3（AUC = 89.5），优于语言或变压器仅基线。用2倍MedAlpaca-7B合成语音增强训练数据将F1提高到85.7。微调显著提高了单模态LLM分类器（例如，MedAlpaca：F1 = 47.3 -> 78.5 F1）。当前多模态模型表现较低（GPT-4o = 70.2 F1；Qwen = 66.0）。性能提升与合成和真实语音之间的分布相似性一致。

Conclusion: 整合变压器嵌入与语言特征可以提高从语音中检测ADRD的效果。临床调优的LLM在分类和数据增强方面有效，而多模态建模需要进一步发展。

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [30] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF is a personalised reference-free evaluation framework that improves the assessment of personalised language generation systems.


<details>
  <summary>Details</summary>
Motivation: Most evaluation methods overlook the individuality of users in personalised text generation.

Method: PREF is a personalised reference-free evaluation framework that operates in a three-step pipeline: coverage, preference, and scoring stages.

Result: Experiments on the PrefEval benchmark show that PREF achieves higher accuracy, better calibration, and closer alignment with human judgments than strong baselines.

Conclusion: PREF lays the groundwork for more reliable assessment and development of personalised language generation systems.

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [31] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文介绍了 Latent Fusion Jailbreak (LFJ)，这是一种基于表示的攻击方法，能够有效绕过大型语言模型的安全对齐。同时，本文还提出了一种对抗训练防御方法，能够在不降低良性输入性能的情况下显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各种语言任务中表现出色，但容易受到 jailbreak 攻击的影响，这些攻击可以绕过它们的安全对齐。因此，需要一种有效的攻击方法和相应的防御措施来应对这一问题。

Method: LFJ通过选择具有高主题和句法相似性的查询对，在有影响的层和标记上进行梯度引导的插值，然后进行优化以平衡攻击成功率、输出流畅性和计算效率。为了缓解LFJ，本文提出了一个对抗训练防御方法，通过在插值示例上微调模型来减少攻击成功率。

Result: 在 Vicuna 和 LLaMA-2 等模型上的评估显示，LFJ 的平均攻击成功率 (ASR) 为 94.01%，优于现有方法。对抗训练防御方法在不降低良性输入性能的情况下，将 ASR 降低了超过 80%。消融研究验证了查询对选择、隐藏状态插值组件和优化策略在 LFJ 效果中的重要性。

Conclusion: 本文提出了一种名为Latent Fusion Jailbreak (LFJ)的基于表示的攻击方法，能够有效地绕过大型语言模型的安全对齐。同时，本文还提出了一种对抗训练防御方法，能够在不降低良性输入性能的情况下显著降低攻击成功率。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [32] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架IAPO，用于联合优化提示和推理规模，并考虑到推理预算和不同的任务目标。还开发了一个固定预算的训练算法PSST，并分析了有限预算下的误差概率保证。最后，在六个不同的任务上评估了PSST的有效性，并展示了在通过提示优化对齐黑盒LLM时，包含推理感知的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化方法是推理策略无关的，即它们在优化提示时不考虑部署期间使用的推理策略。这构成了一个重要的方法论差距，因为我们的实证和理论分析揭示了这两个范式之间的强依赖关系。此外，用户在多个目标和推理预算之间的偏好显著影响提示和推理配置的选择。

Method: 本文提出了一个统一的框架IAPO（Inference-Aware Prompt Optimization），该框架联合优化提示和推理规模，同时考虑到推理预算和不同的任务目标。还开发了一个固定预算的训练算法PSST（Prompt Scaling via Sequential Trimming），并分析了有限预算下的误差概率保证。

Result: 我们在六个不同的任务上评估了PSST的有效性，包括多目标文本生成和推理，并展示了在通过提示优化对齐黑盒LLM时，包含推理感知的重要性。

Conclusion: 本文提出了一种新的框架IAPO，它能够同时优化提示和推理规模，并考虑到推理预算和不同的任务目标。此外，我们开发了一个固定预算的训练算法PSST，并分析了有限预算下的误差概率保证。最后，我们在六个不同的任务上评估了PSST的有效性，并展示了在通过提示优化对齐黑盒LLM时，包含推理感知的重要性。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [33] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 本文发现LLM在思维模式下更容易受到Jailbreak攻击，并提出了一种通过添加特定思维标记来引导LLM内部思维过程的安全干预方法，有效降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决LLM在思维模式下更容易受到Jailbreak攻击的问题，并寻找一种有效的缓解方法。

Method: 本文提出了一种安全的思维干预方法，通过在提示中添加LLM的“特定思维标记”来明确引导LLM的内部思维过程。

Result: 实验结果表明，安全的思维干预方法可以显著降低具有思维模式的LLM的攻击成功率。

Conclusion: 本文提出了一种安全的思维干预方法，通过在提示中添加LLM的“特定思维标记”来明确引导LLM的内部思维过程，结果表明该方法可以显著降低具有思维模式的LLM的攻击成功率。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [34] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为APIE的新颖主动提示框架，通过内省困惑原则指导LLM评估自身的困惑。该方法通过双组件不确定性度量来量化格式和内容的不确定性，并通过排名未标记数据来选择最具挑战性和信息性的样本作为少样本示例。实验结果表明，该方法在提取准确性和鲁棒性方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在少样本信息提取（IE）中显示出巨大的潜力，但其性能高度依赖于上下文示例的选择。传统选择策略往往无法提供有信息量的指导，因为它们忽略了模型不可靠的一个关键来源：不仅来自语义内容，还来自IE任务所需的良好结构格式的生成。

Method: 我们引入了一种名为APIE的新颖主动提示框架，该框架通过一个称为内省困惑的原则进行指导。我们的方法使LLM能够通过一个双组件不确定性度量来评估自身的困惑，该度量独特地量化了格式不确定性和内容不确定性。

Result: 在四个基准测试上的广泛实验表明，我们的方法始终优于强大的基线，显著提高了提取准确性和鲁棒性。

Conclusion: 我们的工作突显了在构建有效且可靠的结构生成系统时，对模型不确定性进行细粒度、双层次视图的重要性。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [35] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一个名为mSCoRe的多语言和可扩展基准，用于评估基于技能的常识推理。该基准包含三个关键组件，旨在系统地评估LLM的推理能力。实验结果显示，mSCoRe对当前模型仍然具有挑战性，特别是在更高复杂度级别上。


<details>
  <summary>Details</summary>
Motivation: 最近在推理强化大型语言模型（LLMs）方面的进展显示出在复杂推理任务中的显著能力。然而，其利用不同人类推理技能的机制仍缺乏深入研究，特别是在涉及不同语言和文化的多语言常识推理方面。为了弥补这一差距，我们提出了mSCoRe基准。

Method: 我们提出了一个名为mSCoRe的多语言和可扩展基准，用于基于技能的常识推理。该基准包括三个关键组件：(1) 一种新的推理技能分类法，使模型推理过程的细粒度分析成为可能；(2) 一种专门针对常识推理评估的稳健数据合成管道；(3) 一个复杂性扩展框架，允许任务难度随着LLM能力的未来改进而动态变化。

Result: 在八个不同规模和训练方法的最先进的LLMs上进行的广泛实验表明，mSCoRe对于当前模型来说仍然具有挑战性，尤其是在更高复杂度级别上。

Conclusion: 我们的结果揭示了这种推理增强模型在面对细微的多语言一般性和文化常识时的局限性。我们进一步提供了对模型推理过程的详细分析，并提出了未来改进多语言常识推理能力的方向。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [36] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 本文介绍了一个新的基准，用于评估大型语言模型在多轮对话、推理和信息获取方面的能力，并揭示了当前模型的不足之处。


<details>
  <summary>Details</summary>
Motivation: 开发能够有效参与逻辑一致的多轮对话、寻求信息并使用不完整数据进行推理的LLMs至关重要。

Method: 引入了一个新的基准，包含一系列多轮任务，旨在测试特定的推理、交互对话和信息获取能力。

Result: 评估前沿模型在该基准上的表现显示出显著的改进空间，大多数错误源于指令遵循不佳、推理失败和规划不良。

Conclusion: 该基准为未来研究提供了稳健的平台，以提高LLMs在处理复杂交互场景中的关键能力。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [37] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 本文介绍了LaaJMeter，一个用于LaaJ的模拟框架，帮助验证和优化评估指标，特别是在低资源环境中。


<details>
  <summary>Details</summary>
Motivation: 在领域特定的上下文中，使用未经过验证的指标进行元评估存在挑战，因为标注数据稀缺且专家评估成本高昂。需要一种方法来验证和优化LaaJ的评估指标。

Method: 引入了LaaJMeter，这是一个基于模拟的框架，用于LaaJ的受控元评估。它允许工程师生成代表虚拟模型和法官的合成数据，以在现实条件下系统地分析评估指标。

Result: 在涉及遗留编程语言的代码翻译任务中展示了LaaJMeter的效用，结果突显了常见指标的局限性，并强调了有根据的指标选择的重要性。

Conclusion: LaaJMeter提供了一种可扩展和可扩展的解决方案，用于在低资源环境中评估LaaJ，有助于确保NLP中可信和可重复的评估。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [38] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 本文研究了翻译难度估计任务，提出了一个新的指标来评估难度估计器，并展示了其在构建更具挑战性的机器翻译基准中的实际效用。结果表明，专门的模型（Sentinel-src）优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 机器翻译质量在某些设置中已经开始达到接近完美的翻译。这些高质量的输出使得难以区分最先进的模型，并确定未来改进的领域。自动识别机器翻译系统难以处理的文本有望开发更具区分力的评估并指导未来的研究。

Method: 我们正式定义了翻译难度估计的任务，根据文本翻译的预期质量来定义文本的难度。我们引入了一个新的指标来评估难度估计器，并用它来评估基线和新方法。最后，我们展示了难度估计器的实际效用，通过它们构建更具挑战性的机器翻译基准。

Result: 我们结果表明，专门的模型（称为Sentinel-src）优于基于启发式的方法（例如词频或句法复杂性）和LLM-as-a-judge方法。我们发布了两个改进的难度估计模型，Sentinel-src-24和Sentinel-src-25，可以用于扫描大量文本并选择最可能挑战当代机器翻译系统的文本。

Conclusion: 我们的结果表明，专门的模型（称为Sentinel-src）优于基于启发式的方法（例如词频或句法复杂性）和LLM-as-a-judge方法。我们发布了两个改进的难度估计模型，Sentinel-src-24和Sentinel-src-25，可以用于扫描大量文本并选择最可能挑战当代机器翻译系统的文本。

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [39] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: For-Value是一种高效的单向数据估值框架，可以准确估计大规模语言模型和视觉-语言模型中每个训练样本的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的数据估值方法通常依赖于Hessian信息或模型重新训练，这在大规模参数模型中计算成本过高。因此，需要一种更高效、可扩展的方法来量化个体训练样本的影响。

Method: For-Value是一个仅向前的数据估值框架，利用现代基础模型的丰富表示，通过单一前向传递计算影响分数，从而避免了昂贵的梯度计算。

Result: 实验结果表明，For-Value在识别有影响力的小调优示例和有效检测错误标记数据方面表现良好，甚至优于基于梯度的基线方法。

Conclusion: For-Value能够有效地估计单个训练样本的影响，为大规模语言模型和视觉-语言模型提供透明性和可问责性。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [40] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 本文介绍了PakBBQ，这是一个针对巴基斯坦文化和社会背景的偏见基准测试数据集。通过评估多语言大型语言模型在不同上下文和问题框架下的表现，研究发现消歧可以提高准确率，乌尔都语中的反偏见行为更强，负面问题框架可以减少刻板印象回答。这些发现强调了情境化基准和提示工程在低资源设置中减轻偏见的重要性。


<details>
  <summary>Details</summary>
Motivation: 大多数LLMs在西方中心的数据上进行训练和评估，很少关注低资源语言和区域背景。为了弥补这一差距，我们引入了PakBBQ，以更好地评估LLMs在不同文化和区域背景下的公平性。

Method: 我们引入了PakBBQ，这是原始Bias Benchmark for Question Answering (BBQ)数据集的文化和区域适应扩展。PakBBQ包含超过214个模板，17180个QA对，涵盖8个类别，包括年龄、残疾、外貌、性别、社会经济地位、宗教、地区归属和语言正式性等与巴基斯坦相关的偏见维度。我们评估了多种多语言LLMs在模糊和明确消歧上下文以及负面与非负面问题框架下的表现。

Result: 实验结果表明，(i) 在消歧情况下平均准确率提高了12%，(ii) 在乌尔都语中比英语表现出更强的反偏见行为，(iii) 问题框架的影响显著减少了刻板印象的回答，当问题以负面方式提出时。

Conclusion: 这些发现强调了情境化基准和简单的提示工程策略在低资源设置中减轻偏见的重要性。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [41] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [42] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 本研究比较了四种深度学习架构在表情符号预测任务中的表现，发现 BERT 性能最佳，而 CNN 在处理罕见表情符号时更有效。


<details>
  <summary>Details</summary>
Motivation: 探索从短文本序列中进行表情符号预测的方法，以改善人机交互。

Method: 使用四种深度学习架构（前馈网络、CNN、transformer 和 BERT）进行表情符号预测，并使用 TweetEval 数据集解决类别不平衡问题。

Result: BERT 由于其预训练优势表现出最高的整体性能，而 CNN 在罕见表情符号类别上表现更优。

Conclusion: 本研究展示了架构选择和超参数调整在情感感知表情符号预测中的重要性，有助于改进人机交互。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [43] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 本研究利用大型语言模型从临床访谈中预测BPRS评分，结果显示其准确性接近人类评分者，并且在不同语言和纵向信息整合方面表现出色，显示出改善和标准化临床高风险患者评估的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于BPRS需要长时间的结构化访谈，因此在临床实践中不常用。本研究旨在探索使用大型语言模型来提高对临床高风险患者症状评估的准确性和标准化程度。

Method: 研究人员利用大型语言模型（LLMs）从临床访谈转录文本中预测BPRS评分，并评估了其在不同语言和纵向信息整合方面的表现。

Result: LLM预测的BPRS评分与真实评估的中位一致性为0.84，ICC为0.73，接近人类的评分者间和评分者内可靠性。此外，LLM在不同语言中的评估准确性（中位一致性为0.88，ICC为0.70）以及在单次或少量示例学习方法中整合纵向信息的能力也得到了验证。

Conclusion: 研究结果表明，大型语言模型在预测BPRS评分方面表现出色，具有改善和标准化临床高风险患者评估的潜力。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [44] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 本研究采用计算和基于语料库的方法，探讨了托基波纳语言的变化和变异，发现社会语言学因素对其影响与自然语言类似，构造语言系统在社区使用时也会自然演变。


<details>
  <summary>Details</summary>
Motivation: 探索托基波纳的语言变化和变异，这是一种有大约120个核心词汇的构造语言。

Method: 计算和基于语料库的方法，研究包括流体词类和及物性在内的特征。

Result: 研究结果表明，社会语言学因素影响托基波纳的方式与自然语言相同，构造语言系统在社区使用时会自然演变。

Conclusion: 研究结果表明，社会语言学因素以与自然语言相同的方式影响托基波纳，即使构造语言系统在社区使用时也会自然演变。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [45] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 该研究提出了一种利用LLM输出来优化提示的方法，以更好地匹配模型的归纳偏差，从而提高分类和排序任务的性能。


<details>
  <summary>Details</summary>
Motivation: LLM对提示措辞的小变化很敏感，这可以部分归因于LLM中的归纳偏差。

Method: 通过将LLM的输出作为其提示的一部分，可以更容易地创建满意的提示措辞，从而创建与模型的归纳偏差相匹配的提示。

Result: 实证研究表明，使用这种归纳偏差提取和匹配策略可使LLM的分类Likert评分提高多达19%，排序Likert评分提高多达27%。

Conclusion: 使用这种归纳偏差提取和匹配策略可以提高LLM的分类和排序的Likert评分。

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [46] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 本研究提出了一种定性的、话语框架来补充现有的偏见检测方法。通过手动分析LLM生成的关于黑人和白人女性的短篇小说，我们调查了性别和种族偏见。结果表明，黑人女性被描绘为与祖先和抵抗联系在一起，而白人女性则出现在自我发现的过程中。这些模式反映了语言模型如何复制固化的言语表现，强化本质化和社会停滞感。当被提示纠正偏见时，模型提供了表面的修改，维持了有问题的含义。研究结果展示了算法的意识形态运作，并对AI的伦理使用和发展具有重要意义。研究强调了对AI设计和部署进行批判性和跨学科方法的必要性，解决LLM生成的话语如何反映和加剧不平等。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，评估它们是否再现偏见（如歧视和种族化）变得至关重要。当前的偏见检测方法主要依赖于定量、自动的方法，往往忽略了自然语言中偏见出现的细微方式。

Method: 本研究提出了一种定性的、话语框架来补充现有的偏见检测方法。通过对手动分析LLM生成的关于黑人和白人女性的短篇小说，我们调查了性别和种族偏见。

Result: 结果表明，黑人女性被描绘为与祖先和抵抗联系在一起，而白人女性则出现在自我发现的过程中。这些模式反映了语言模型如何复制固化的言语表现，强化本质化和社会停滞感。当被提示纠正偏见时，模型提供了表面的修改，维持了有问题的含义。

Conclusion: 研究结果表明，语言模型复制了固化的话语表现，强化了本质化和社会停滞感。当被要求纠正偏见时，模型提供了表面的修改，维持了有问题的含义，揭示了在促进包容性叙述方面的局限性。研究结果展示了算法的意识形态运作，并对AI的伦理使用和发展具有重要意义。研究强调了对AI设计和部署进行批判性和跨学科方法的必要性，解决LLM生成的话语如何反映和加剧不平等。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [47] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文介绍了ReviewRL，这是一种基于强化学习的框架，用于生成全面且事实依据的科学论文评审。该方法结合了ArXiv-MCP检索增强的上下文生成管道、监督微调和具有复合奖励函数的强化学习过程。实验表明，ReviewRL在ICLR 2025论文上的表现显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Peer review is essential for scientific progress but faces growing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches struggle with factual accuracy, rating consistency, and analytical depth, often generating superficial or generic feedback lacking the insights characteristic of high-quality human reviews.

Method: Our approach combines: (1) an ArXiv-MCP retrieval-augmented context generation pipeline that incorporates relevant scientific literature, (2) supervised fine-tuning that establishes foundational reviewing capabilities, and (3) a reinforcement learning procedure with a composite reward function that jointly enhances review quality and rating accuracy.

Result: Experiments on ICLR 2025 papers demonstrate that ReviewRL significantly outperforms existing methods across both rule-based metrics and model-based quality assessments.

Conclusion: ReviewRL establishes a foundational framework for RL-driven automatic critique generation in scientific discovery, demonstrating promising potential for future development in this domain.

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [48] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: DOTABLER是一个以表格为中心的语义文档解析框架，通过深度语义解析和上下文关联提升表格分析能力，在实际应用中表现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在表面任务如布局分析、表格检测和数据提取，缺乏对表格及其上下文关联的深度语义解析，这限制了跨段落数据分析和一致性的分析。

Method: DOTABLER是一个以表格为中心的语义文档解析框架，利用定制数据集和预训练模型的领域特定微调，整合完整的解析流程来识别与表格语义相关的上下文段落。

Result: DOTABLER在近4000页的真实PDF文档中评估，包含超过1000个表格，实现了超过90%的精确度和F1分数。

Conclusion: DOTABLER在表格-上下文语义分析和深度文档解析方面表现出色，相比GPT-4o等先进模型具有优越性能。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [49] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为FreLLM4Rec的方法，旨在从谱的角度平衡语义和协作信息，以解决LLM-based推荐系统中协作信号衰减的问题。


<details>
  <summary>Details</summary>
Motivation: LLM-based推荐系统倾向于过度强调用户交互历史中的语义相关性，导致内在协作信号在通过LLM后退化。传统Transformer-based序列模型通常能保持或增强协作信号。

Method: 我们引入了FreLLM4Rec，从谱的角度平衡语义和协作信息。首先使用全局图低通滤波器（G-LPF）净化包含语义和协作信息的项目嵌入，以初步去除不相关的高频噪声。然后通过时间频率调制（TFM）逐层主动保留协作信号。

Result: 在四个基准数据集上的广泛实验表明，FreLLM4Rec成功缓解了协作信号衰减，并实现了具有竞争力的性能，相比最佳基线在NDCG@10上提高了高达8.00%。

Conclusion: 我们的研究提供了对LLM如何处理协作信息的见解，并为改进基于LLM的推荐系统提供了一个有原则的方法。

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [50] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 本文提出了一种新的跨语言提示编码器（XPE）和双软提示机制，以提高大语言模型在低性能语言和多语言环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索软提示在跨语言迁移中的潜力，特别是在低性能语言中提升模型表现。

Method: 引入了Cross-Prompt Encoder (XPE)，结合轻量级编码架构和跨语言训练，以及Dual Soft Prompt机制，将基于编码器的提示与直接训练的标准软提示相结合。

Result: 实验表明，XPE在低性能语言中效果显著，而混合方法在多语言环境中表现出更好的适应性。

Conclusion: XPE在低性能语言中最为有效，而混合变体在多语言设置中提供了更广泛的适应性。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [51] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 我们提出了一种两阶段微调方法，使大型语言模型Qwen3 14B能够原生地用韩语思考。这种方法在高级推理基准测试中表现出显著改进，同时保持了知识和语言能力。


<details>
  <summary>Details</summary>
Motivation: 为了使大型语言模型能够原生地用韩语进行推理，我们需要一种有效的微调方法。

Method: 我们提出了一种两阶段微调方法，使大型语言模型Qwen3 14B能够原生地用韩语思考。第一阶段是在高质量的韩语推理数据集上进行监督微调（SFT），建立了坚实的韩语逻辑推理基础，显著提高了韩语任务的表现，并且在一般推理能力上也有所提升。第二阶段，我们采用一种定制的组相对策略优化（GRPO）算法进行强化学习，以进一步提高韩语推理对齐和整体问题解决性能。

Result: 我们的方法在高级推理基准测试中表现出显著改进（尤其是数学和编码任务），同时保持了知识和语言能力，成功地在其内部思维链中完全使用韩语。

Conclusion: 我们的方法在高级推理基准测试中表现出显著改进（尤其是数学和编码任务），同时保持了知识和语言能力，成功地在其内部思维链中完全使用韩语。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [52] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [53] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 本文利用大型语言模型在捷克文本摘要任务中取得了新进展，并引入了一个新的历史捷克文档摘要数据集。


<details>
  <summary>Details</summary>
Motivation: 捷克文本摘要，特别是历史文献的摘要，在语言复杂性和标注数据稀缺方面仍研究不足。

Method: 使用Mistral和mT5等大型语言模型进行捷克文本摘要。

Result: 在现代捷克文本摘要数据集SumeCzech上实现了最先进的结果，并引入了一个名为Posel od Čerchova的新数据集用于历史捷克文档的摘要。

Conclusion: 这些贡献为推进捷克文本摘要提供了巨大潜力，并为捷克历史文本处理的研究开辟了新的方向。

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [54] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一种新的跨语言ABSA方法，使用受限解码的序列到序列模型，提高了跨语言性能，并支持多任务处理。在多种语言和任务上评估，结果优于现有方法，并提供了关于如何应用这些方法的实用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管基于方面的情感分析（ABSA）已经取得了显著进展，但低资源语言仍然面临挑战，通常被忽视，而优先考虑英语。当前的跨语言ABSA方法专注于有限且不太复杂的任务，并且常常依赖外部翻译工具。

Method: 本文引入了一种使用受限解码的序列到序列模型的新方法，消除了对不可靠翻译工具的依赖，并通过受限解码提高了跨语言性能。此外，该方法支持多任务处理，使单个模型能够解决多个ABSA任务。

Result: 在七种语言和六个ABSA任务上评估了所提出的方法，超越了最先进的方法，并为之前未探索的任务设定了新基准。此外，在零样本、少样本和微调场景中评估了大型语言模型（LLMs）。虽然LLMs在零样本和少样本设置中表现不佳，但微调实现了与较小的多语言模型相当的结果，尽管训练和推理时间更长。

Conclusion: 本文提供了关于跨语言方面情感分析方法的实用建议，并提供了对这些方法优缺点的深入了解，推动了这一具有挑战性的研究领域的最新进展。

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [55] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 本文提出了一种混合评估框架MDH，用于评估和清理数据集中的恶意内容，并提出了两种新的策略D-Attack和DH-CoT，以提高越狱攻击的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的红队数据集包含不适合的提示，这使得评估越狱攻击变得困难。现有的恶意内容检测方法要么依赖人工标注，要么依赖大型语言模型（LLMs），但它们在有害类型上的准确性不一致。

Method: 本文提出了一种混合评估框架MDH，结合了基于LLM的注释和最小的人类监督，并应用该框架进行数据集清理和越狱响应检测。同时，提出了两种新的策略D-Attack和DH-CoT。

Result: 本文提出的MDH框架能够有效地评估和清理数据集中的恶意内容，并且发现精心设计的开发者消息可以显著提高越狱攻击的成功率。

Conclusion: 本文提出了一种混合评估框架MDH，结合了基于LLM的注释和最小的人类监督，以平衡准确性和效率。此外，还提出了两种新的策略D-Attack和DH-CoT，以提高越狱攻击的成功率。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [56] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的黑盒攻击方法，利用稀疏自编码器识别并操纵文本中的关键特征，生成能够绕过现有防御的对抗文本。


<details>
  <summary>Details</summary>
Motivation: 生成对抗样本以破解大型语言模型仍然是理解模型漏洞和提高鲁棒性的关键挑战。

Method: 我们提出了一个名为稀疏特征扰动框架（SFPF）的新黑盒攻击方法，利用大型模型的可解释性。通过使用稀疏自编码器（SAE）模型重建隐藏层表示，并对成功攻击的文本进行特征聚类以识别高激活特征，然后对这些特征进行扰动以生成新的对抗文本。

Result: 实验结果表明，由SFPF生成的对抗文本可以绕过最先进的防御机制，揭示了当前NLP系统中的持续漏洞。

Conclusion: 我们的方法展示了一种新的红队策略，可以在对抗效果和安全对齐之间取得平衡。然而，该方法在不同提示和层上的效果有所不同，其在其他架构和更大模型中的泛化能力仍需验证。

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [57] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: ComoRAG is a novel approach for long-context narrative comprehension that uses iterative reasoning cycles and a dynamic memory workspace to improve retrieval-based reasoning.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG methods fall short due to their stateless, single-step retrieval process, which overlooks the dynamic nature of capturing interconnected relations within long-range context. Narrative reasoning is not a one-shot process but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation.

Method: ComoRAG is proposed, which operates through iterative reasoning cycles while interacting with a dynamic memory workspace. It generates probing queries to devise new exploratory paths and integrates retrieved evidence into a global memory pool.

Result: ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline across four challenging long-context narrative benchmarks (200K+ tokens). It is particularly advantageous for complex queries requiring global comprehension.

Conclusion: ComoRAG provides a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning.

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [58] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 本文介绍了一个用于评估汉语成语翻译的框架IdiomEval，并提出了改进的模型，以提高检测成语翻译错误的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在机器翻译方面取得了进展，但对汉语成语翻译的研究仍然不足。现有的评估指标无法准确衡量成语翻译的质量。

Method: 本文介绍了IdiomEval框架，该框架包含一个全面的错误分类法，并对900个翻译对进行了标注，以评估机器翻译系统在汉语成语翻译中的表现。

Result: 研究发现，现有的机器翻译系统在成语翻译中表现不佳，最佳系统GPT-4在28%的情况下出现错误。同时，现有评估指标与人类评分的相关性较低。

Conclusion: 本文提出了一种改进的模型，能够在检测汉语成语翻译错误方面达到F1分数0.68。

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [59] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 本文提出了一种基于计算经济学的框架，用于优化大型语言模型的计算资源分配，从而在保持高准确性的同时显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型受到大量计算成本的限制。我们需要一种方法来在资源受限的情况下优化计算分配，以提高效率和透明度。

Method: 我们引入了一个“计算经济学”框架，将大型语言模型视为一个内部经济体系，其中资源受限的代理（注意力头和神经元块）必须分配稀缺计算以最大化任务效用。我们提出了一个激励驱动的训练范式，通过添加可微分的计算成本项来增强任务损失，鼓励稀疏和高效的激活。

Result: 该方法在GLUE（MNLI、STS-B、CoLA）和WikiText-103数据集上产生了一系列模型，这些模型能够追踪帕累托前沿，并且在相似准确率下，FLOPS减少了约40%，延迟更低，同时注意力模式更具可解释性。

Conclusion: 这些结果表明，经济原则为在严格资源约束下设计高效、自适应且更透明的大型语言模型提供了一条有根据的途径。

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [60] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: DiFaR是一种检测器无关的框架，旨在生成多样、准确且相关的文本推理，以提高多模态虚假信息检测的效果。


<details>
  <summary>Details</summary>
Motivation: 生成文本推理以支持可训练的多模态虚假信息检测器是一个有前途的范式，但其效果受到三个核心挑战的限制：生成的推理不足、由于幻觉导致的事实错误以及无关或冲突的内容引入噪声。

Method: DiFaR采用五种思维链提示来激发LVLMs的不同推理轨迹，并结合一个轻量级的后期过滤模块，根据句子级别的事实性和相关性评分选择推理句子。

Result: 在四个流行的基准测试中进行的广泛实验表明，DiFaR比四种基线类别高出最多5.9%，并使现有检测器提升了高达8.7%。自动指标和人工评估都确认了DiFaR在所有三个维度上显著提高了推理质量。

Conclusion: DiFaR显著提高了多模态虚假信息检测中生成的文本推理的质量，通过解决多样性、事实准确性和相关性问题，展示了其有效性。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [61] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文研究了NLP中隐私与可解释性的关系，发现它们可以共存，并提出了未来工作的建议。


<details>
  <summary>Details</summary>
Motivation: 当前对可解释性和隐私保护NLP的研究兴趣显著增加，但缺乏对两者交叉领域的研究，这导致对是否可以同时实现可解释性和隐私保护的理解存在较大空白。

Method: 本文通过实证研究，探讨了在NLP背景下隐私-可解释性权衡的问题，主要采用了差分隐私（DP）和事后可解释性方法。

Result: 本文发现了隐私和可解释性之间的复杂关系，这种关系由多种因素形成，包括下游任务的性质以及文本隐私化和可解释性方法的选择。

Conclusion: 本文总结了在NLP中实现可解释性和隐私保护共存的潜在可能性，并提出了未来在此重要交叉领域工作的实用建议。

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [62] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 本文研究了多模态大语言模型中的文本主导问题，并提出了评估指标和解决方法，以实现更公平和全面的模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在多模态任务中表现出色，但存在文本主导的问题，即过度依赖文本而未能充分利用其他模态。本文旨在系统地研究文本主导现象，并提出解决方案。

Method: 本文提出了两种评估指标：模态主导指数（MDI）和注意力效率指数（AEI），并提出了一种简单的令牌压缩方法来重新平衡模型注意力。

Result: 本文的分析表明，文本主导现象在所有测试的模态中都显著且普遍。通过应用提出的令牌压缩方法，可以有效减少文本主导现象。

Conclusion: 本文的分析和方法框架为开发更公平和全面的多模态语言模型提供了基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [63] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 本文介绍了一项关于部署欧洲深度推理网络（eDIF）的可行性研究，旨在支持大型语言模型的机制可解释性研究，并通过一个基于GPU的集群和NNsight API实现远程模型检查。


<details>
  <summary>Details</summary>
Motivation: 欧洲需要广泛获取LLM可解释性基础设施，以民主化先进的模型分析能力，支持研究社区。

Method: 该项目引入了一个基于GPU的集群，位于安斯巴赫应用科学大学，并与合作伙伴机构互联，通过NNsight API实现远程模型检查。

Result: 研究显示用户参与度逐渐增加，平台性能稳定，远程实验能力受到积极评价。同时，也指出了下载激活数据时间较长和执行中断等问题，并计划在未来进行改进。

Conclusion: 该研究标志着欧洲广泛获取LLM可解释性基础设施的重要一步，并为更广泛的部署、扩展工具和持续的社区合作奠定了基础。

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [64] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 本文首次系统研究了将科普特语翻译成法语的策略，发现使用风格多样且具有噪声意识的训练语料库进行微调可以显著提高翻译质量。


<details>
  <summary>Details</summary>
Motivation: 本文是对科普特语到法语翻译策略的首次系统研究。

Method: 我们系统地评估了转换与直接翻译、预训练的影响、多版本微调的好处以及模型对噪声的鲁棒性。

Result: 我们利用对齐的圣经语料库证明，使用风格多样且具有噪声意识的训练语料库进行微调可以显著提高翻译质量。

Conclusion: 我们的研究为开发历史语言的翻译工具提供了重要的实际见解。

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [65] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 本文提出了一种结合基于图的方法和Transformer架构的手语翻译方法，在多个数据集上取得了最先进的性能，并为未来的研究设定了基准。


<details>
  <summary>Details</summary>
Motivation: 为了弥合聋哑人和听力障碍者在社会中面临的沟通障碍和社交排斥，我们致力于改进手语翻译方法。

Method: 我们的方法结合了基于图的方法和Transformer架构，融合了Transformer和STGCN-LSTM架构。

Result: 我们的方法在多个手语数据集上实现了最先进的性能，包括RWTH-PHOENIX-2014T、CSL-Daily、How2Sign和BornilDB v1.0。

Conclusion: 我们的方法为未来的研究设定了基准，强调了无词典翻译对于改善聋哑人和听力障碍者沟通可及性的重要性。

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [66] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 本文提出了一种名为VAC的新框架，用于个性化响应生成，通过使用基于用户资料和问题叙述生成的自然语言反馈（NLF）来替代传统的标量奖励，从而提高个性化问答的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在个性化大型语言模型（LLMs）时通常依赖于检索增强生成（RAG），然后使用标量奖励信号进行强化学习，以教模型如何使用检索到的个人上下文。我们认为这些标量奖励有时提供的是弱的、无指导性的反馈，限制了学习效率和个性化质量。

Method: 我们引入了VAC，这是一种新的个性化响应生成框架，它用基于用户资料和问题叙述生成的自然语言反馈（NLF）取代标量奖励。NLF作为丰富的和可操作的监督信号，允许策略模型迭代地改进其输出并内化有效的个性化策略。训练在优化反馈模型和在改进的响应上微调策略模型之间交替进行，从而得到一个在推理时不再需要反馈的策略模型。

Result: 在包含三个不同领域的LaMP-QA基准上的评估表明，与最先进的结果相比，取得了持续且显著的改进。人类评估进一步证实了生成响应的优越质量。

Conclusion: 这些结果表明，NLF为优化个性化问答提供了更有效的信号。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [67] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了ICE框架，用于在扩散大语言模型中实现更灵活的就地提示策略，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）的前缀提示范式和顺序生成过程限制了双向信息的灵活性。扩散大语言模型（dLLMs）通过其双向注意力机制和迭代优化过程提供了新的机会。

Method: ICE框架通过在迭代优化过程中将就地提示直接集成到遮罩标记位置，并采用基于置信度的提前退出机制来减少计算开销。

Result: 实验结果表明，ICE在GSM8K数据集上实现了高达17.29%的准确率提升，并在MMLU数据集上实现了高达276.67倍的加速，同时保持了竞争力的表现。

Conclusion: ICE框架在GSM8K和MMLU数据集上展示了显著的效果，提高了准确率并加速了计算过程。

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [68] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一种结构化的自动化新颖性评估方法，通过模拟专家审稿人的行为，在NLP领域实现了较高的准确性，并展示了其在改进同行评审过程中的潜力。


<details>
  <summary>Details</summary>
Motivation: 新颖性评估是同行评审中的核心但研究不足的方面，特别是在NLP等高产量领域，审稿人能力日益受到压力。

Method: 该方法通过三个阶段建模专家审稿人行为：从提交中提取内容，检索和综合相关工作，以及结构化比较以进行基于证据的评估。

Result: 在182个ICLR 2025提交中进行评估，该方法与人类推理的对齐度达到86.5%，在新颖性结论上的协议度为75.3%，明显优于现有的LLM基线。

Conclusion: 该方法展示了结构化LLM辅助方法在支持更严格和透明的同行评审方面的潜力，而不会取代人类专业知识。

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [69] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 本文提出了一种新的后训练算法MS-GRPO，用于改进小型语言模型的顺序决策能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在应用中往往受限于对大型、计算成本高昂模型的依赖，因此需要改进较小的模型。然而，现有的后训练方法是为单次交互设计的，无法处理多步骤代理任务中的信用分配问题。

Method: 本文引入了多步骤组相对策略优化（MS-GRPO），这是一种基于文本媒介随机游戏（TSMG）和语言代理策略（LAP）框架的后训练LLM代理算法。

Result: 实验结果表明，该方法在提高决策性能方面是有效的：后训练的3B参数模型在Frozen Lake任务上比72B参数基线模型提高了50%。

Conclusion: 本文表明，有针对性的后训练是一种实用且高效的替代方法，可以避免依赖模型规模来创建使用LLM的顺序决策代理。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [70] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: This paper introduces Psyche-R1, a Chinese psychological LLM that integrates empathy, psychological expertise, and reasoning. It achieves strong performance in psychological tasks, demonstrating the potential of LLMs in mental health applications.


<details>
  <summary>Details</summary>
Motivation: The shortage of qualified mental health professionals necessitates the integration of large language models (LLMs) into psychological applications. While recent LLMs have shown promise in mathematics and programming, research in psychology has focused on emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that can enhance reliable responses.

Method: The paper proposes Psyche-R1, a Chinese psychological LLM that integrates empathy, psychological expertise, and reasoning. It uses a data curation pipeline to generate high-quality psychological questions with detailed rationales and empathetic dialogues. A hybrid training strategy combines group relative policy optimization (GRPO) and supervised fine-tuning (SFT) to improve reasoning and empathetic response generation.

Result: Extensive experiments show that Psyche-R1 is effective across several psychological benchmarks, achieving comparable results to a much larger model (671B DeepSeek-R1) with only 7B parameters.

Conclusion: Psyche-R1 demonstrates the effectiveness of integrating empathy, psychological expertise, and reasoning in a large language model for psychological applications, achieving comparable results to a much larger model.

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [71] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种多维建模框架，结合特征工程、数据增强和可解释机器学习，以提高自动化口译质量评估的可解释性和效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究在语言使用质量方面缺乏充分的检查，由于数据稀缺和不平衡导致模型效果不佳，并且缺乏对模型预测的解释努力。

Method: 我们提出了一个集成特征工程、数据增强和可解释机器学习的多维建模框架。该方法通过仅使用与构建相关的透明特征并进行Shapley值（SHAP）分析，优先考虑可解释性而非“黑盒”预测。

Result: 我们的结果在新的英中连续口译数据集上表现出强大的预测性能，确定BLEURT和CometKiwi分数是忠实度的最强预测特征，与停顿相关的特征是流利度的预测特征，而中文特定的短语多样性指标是语言使用的预测特征。

Conclusion: 通过强调可解释性，我们提出了一种可扩展、可靠和透明的传统人工评估替代方案，有助于为学习者提供详细的诊断反馈，并支持自动评分无法单独提供的自我调节学习优势。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [72] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: This paper explores the use of large language models (LLMs) as efficient simulators for agentic search tasks in reinforcement learning (RL). The authors propose Self-Search RL (SSRL), which enhances LLMs' ability to perform search tasks internally, reducing reliance on external search engines and improving performance.


<details>
  <summary>Details</summary>
Motivation: We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines.

Method: We first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards.

Result: Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer.

Conclusion: LLMs possess world knowledge that can be effectively elicited to achieve high performance; SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [73] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文综述了扩散语言模型（DLMs）的当前发展，包括其与自回归和掩码语言模型的关系，预训练和后训练方法，推理策略和优化，以及多模态扩展和应用。


<details>
  <summary>Details</summary>
Motivation: DLMs作为一种并行生成令牌的模型，具有减少推理延迟和捕捉双向上下文的优势，因此需要对其进行全面的综述。

Method: 本文通过回顾DLMs的发展历程，分析其与自回归和掩码语言模型的关系，并探讨了预训练策略和先进的后训练方法。

Result: 本文提供了DLMs的最新综合分类和深入分析，涵盖了预训练策略、后训练方法、推理策略和优化，以及多模态扩展和应用。

Conclusion: 本文综述了扩散语言模型（DLMs）的当前发展，讨论了其在自然语言处理任务中的潜力，并指出了未来的研究方向。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [74] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: 本文提出了一种名为 Nested-ReFT 的新型 ReFT 框架，通过利用目标模型的一部分层作为行为模型来减少训练过程中的推理成本，从而提高计算效率，并保持与基线 ReFT 相当的性能。


<details>
  <summary>Details</summary>
Motivation: 标准的 ReFT 框架在训练过程中需要多次推理步骤生成完成，导致训练成本较高。为了减少推理成本，提出了 Nested-ReFT 框架。

Method: 受离策略强化学习和推测解码的启发，提出了一种新的 ReFT 框架 Nested-ReFT，其中目标模型的一小部分层作为行为模型，在训练期间生成非策略完成。

Result: 理论分析表明 Nested-ReFT 可以提供无偏梯度估计并控制方差。实证分析显示，Nested-ReFT 在多个数学推理基准和模型大小上提高了计算效率（每秒标记数）。

Conclusion: Nested-ReFT 提高了计算效率，并且在多个数学推理基准和模型大小上保持了与基线 ReFT 相当的性能。

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [75] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习方法G-RA，用于解决长期任务中的奖励稀疏性问题，并在软件工程任务中进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 在长期强化学习任务中，奖励稀疏性仍然是一个重大挑战，而现有的基于结果的奖励塑造难以在不引入偏差或需要显式任务分解的情况下定义有意义的即时奖励。此外，基于验证的奖励塑造使用逐步批评者，但即时奖励与长期目标之间的不对齐可能导致奖励黑客和次优策略。

Method: 我们引入了面向软件工程的强化学习框架，以及一种新的方法Gated Reward Accumulation (G-RA)，该方法仅在高阶（长期）奖励达到预定义阈值时累积即时奖励。

Result: 在SWE-bench Verified和kBench上的实验表明，G-RA提高了完成率（47.6% → 93.8%和22.0% → 86.0%）和修改率（19.6% → 23.8%和12.0% → 42.0%），同时避免了由于奖励不对齐导致的策略退化。

Conclusion: 我们的研究强调了在长期强化学习中平衡奖励累积的重要性，并提供了一个实用的解决方案。

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [76] [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/abs/2508.10751)
*Zhipeng Chen,Xiaobo Qin,Youbin Wu,Yue Ling,Qinghao Ye,Wayne Xin Zhao,Guang Shi*

Main category: cs.LG

TL;DR: The paper investigates the use of Pass@k as a reward in RLVR and shows that it can improve exploration ability. It also analyzes the relationship between exploration and exploitation and suggests that they can be mutually enhancing.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of balancing exploration and exploitation in RLVR, which has led to policies preferring conservative actions and converging to local optima.

Method: The paper uses Pass@k as a reward to train the policy model and derives an analytical solution for its advantage. It also explores the design of the advantage function for RLVR.

Result: Pass@k Training improves exploration ability, and the analysis shows that exploration and exploitation can mutually enhance each other. The paper also presents promising results for advantage function design in RLVR.

Conclusion: Pass@k Training can improve exploration ability in RLVR, and exploration and exploitation are not inherently conflicting. The analysis suggests that designing the advantage function directly can lead to better results.

Abstract: Reinforcement learning with verifiable rewards (RLVR), which typically adopts
Pass@1 as the reward, has faced the issues in balancing exploration and
exploitation, causing policies to prefer conservative actions, converging to a
local optimum. Identifying an appropriate reward metric is therefore crucial.
Regarding the prior work, although Pass@k has been used in evaluation, its
connection to LLM exploration ability in RLVR remains largely overlooked. To
investigate this, we first use Pass@k as the reward to train the policy model
(i.e., $\textbf{Pass@k Training}$), and observe the improvement on its
exploration ability. Next, we derive an analytical solution for the advantage
of Pass@k Training, leading to an efficient and effective process. Building on
this, our analysis reveals that exploration and exploitation are not inherently
conflicting objectives, while they can mutually enhance each other. Moreover,
Pass@k Training with analytical derivation essentially involves directly
designing the advantage function. Inspired by this, we preliminarily explore
the advantage design for RLVR, showing promising results and highlighting a
potential future direction.

</details>


### [77] [Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions](https://arxiv.org/abs/2508.10824)
*Parsa Omidi,Xingshuai Huang,Axel Laborieux,Bahareh Nikpour,Tianyu Shi,Armaghan Eshaghi*

Main category: cs.LG

TL;DR: 本文综述了Memory-Augmented Transformers的研究进展，提出了一个结合神经科学原理和工程进展的统一框架，旨在实现更强大的Transformer架构。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在长程上下文保留、持续学习和知识整合方面存在关键限制，因此需要一种统一的框架来解决这些问题。

Method: 本文通过三个分类维度（功能目标、记忆表示和集成机制）组织了最近的进展，并分析了核心记忆操作（读取、写入、遗忘和容量管理）。

Result: 本文揭示了从静态缓存向自适应、测试时学习系统转变的趋势，并识别了可扩展性和干扰等持续挑战，以及分层缓冲和惊喜门控更新等新兴解决方案。

Conclusion: 本文综述了将神经科学原理与工程进展相结合的Memory-Augmented Transformers框架，为实现认知启发的终身学习Transformer架构提供了路线图。

Abstract: Memory is fundamental to intelligence, enabling learning, reasoning, and
adaptability across biological and artificial systems. While Transformer
architectures excel at sequence modeling, they face critical limitations in
long-range context retention, continual learning, and knowledge integration.
This review presents a unified framework bridging neuroscience principles,
including dynamic multi-timescale memory, selective attention, and
consolidation, with engineering advances in Memory-Augmented Transformers. We
organize recent progress through three taxonomic dimensions: functional
objectives (context extension, reasoning, knowledge integration, adaptation),
memory representations (parameter-encoded, state-based, explicit, hybrid), and
integration mechanisms (attention fusion, gated control, associative
retrieval). Our analysis of core memory operations (reading, writing,
forgetting, and capacity management) reveals a shift from static caches toward
adaptive, test-time learning systems. We identify persistent challenges in
scalability and interference, alongside emerging solutions including
hierarchical buffering and surprise-gated updates. This synthesis provides a
roadmap toward cognitively-inspired, lifelong-learning Transformer
architectures.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: 本文通过深度学习技术改进了OCR和文档布局分析任务，包括历史文献、历史会议记录和现代手写识别。


<details>
  <summary>Details</summary>
Motivation: 为了提高历史希伯来语碎片、16至18世纪会议决议以及现代英语手写识别的准确性，我们采用了数据增强和基于置信度的伪标签方法。

Method: 我们使用了先进的深度学习技术，包括Kraken和TrOCR模型、CRNN（结合DeepLabV3+和双向LSTM）以及带有ResNet34编码器的CRNN，以处理OCR和文档布局分析任务。

Result: 我们在三个任务中取得了改进的字符识别效果，并展示了不同模型的有效性。

Conclusion: 本报告提供了有价值的见解，并为未来的研究指明了潜在方向。

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [79] [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)
*Yifan Song,Wing Yee Au,Hon Yung Wong,Brian P. Bailey,Tal August*

Main category: cs.HC

TL;DR: 本研究探讨了术语障碍，并设计了一个实时个性化术语支持系统ParseJargon，实验结果表明该系统能有效提高跨学科沟通的效果。


<details>
  <summary>Details</summary>
Motivation: 有效跨学科沟通经常受到领域特定术语的阻碍，因此需要探索术语障碍的深入情况。

Method: 我们进行了一项形成性日记研究，揭示了当前术语管理策略在工作场所会议中的关键限制，并设计了ParseJargon，这是一个交互式LLM驱动的系统，提供实时个性化术语识别和解释。

Result: 对比ParseJargon与基线（无支持）和通用支持（非个性化）条件的控制实验表明，个性化术语支持显著提高了参与者的理解、参与度和对同事工作的欣赏，而通用支持则对参与度产生了负面影响。后续实地研究验证了ParseJargon在实时会议中的可用性和实际价值。

Conclusion: 我们的研究为设计个性化的术语支持工具提供了见解，并对更广泛的跨学科和教育应用有影响。

Abstract: Effective interdisciplinary communication is frequently hindered by
domain-specific jargon. To explore the jargon barriers in-depth, we conducted a
formative diary study with 16 professionals, revealing critical limitations in
current jargon-management strategies during workplace meetings. Based on these
insights, we designed ParseJargon, an interactive LLM-powered system providing
real-time personalized jargon identification and explanations tailored to
users' individual backgrounds. A controlled experiment comparing ParseJargon
against baseline (no support) and general-purpose (non-personalized) conditions
demonstrated that personalized jargon support significantly enhanced
participants' comprehension, engagement, and appreciation of colleagues' work,
whereas general-purpose support negatively affected engagement. A follow-up
field study validated ParseJargon's usability and practical value in real-time
meetings, highlighting both opportunities and limitations for real-world
deployment. Our findings contribute insights into designing personalized jargon
support tools, with implications for broader interdisciplinary and educational
applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [80] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: 本文提出了一种名为上下文过滤模型的新防御机制，用于提高大型语言模型的安全性，同时保持其原有性能。该模型能够有效减少越狱攻击的成功率，并且可以应用于所有类型的LLMs，无需进行模型微调。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在性能上取得了显著进展，但各种越狱攻击带来了日益增长的安全和伦理风险。恶意用户经常利用对抗性上下文来欺骗LLMs，促使它们生成有害查询的响应。因此，我们需要一种有效的防御机制来提高LLMs的安全性，同时不影响其原有的性能。

Method: 我们提出了一种新的防御机制，称为上下文过滤模型，这是一种输入预处理方法，旨在过滤掉不可信和不可靠的上下文，同时识别包含真实用户意图的主要提示，以揭示隐藏的恶意意图。

Result: 我们的模型在防御越狱攻击方面表现出色，能够减少高达88%的攻击成功率，同时保持原始LLMs的性能，实现了最先进的安全性和帮助性产品结果。

Conclusion: 我们的模型能够减少高达88%的越狱攻击成功率，同时保持原始LLMs的性能，实现了最先进的安全性和帮助性产品结果。此外，我们的模型是一个即插即用的方法，可以应用于所有LLMs，包括白盒和黑盒模型，以增强其安全性而无需对模型本身进行微调。

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [81] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: 本文提出了一种基于搜索的框架，用于检测和防御LLM代理中的隐私威胁，通过模拟交互来提升攻击和防御策略。


<details>
  <summary>Details</summary>
Motivation: 由于LLM代理可能引发严重的隐私威胁，需要一种有效的机制来检测和防御这些威胁。

Method: 本文提出了一种基于搜索的框架，通过模拟隐私关键的代理交互来改进攻击者和防御者的指令。

Result: 攻击策略从简单的直接请求升级为复杂的多轮战术，而防御措施从基于规则的约束发展为身份验证状态机。

Conclusion: 本文提出的搜索框架能够有效发现隐私威胁并提升防御能力，具有广泛的应用价值。

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [82] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 本文介绍了Amazon Nova AI Challenge中大学团队和Amazon团队在AI软件开发安全性方面的进展，包括自动红队机器人和安全AI助手的开发，以及对抗性比赛和数据集的使用。


<details>
  <summary>Details</summary>
Motivation: 为了推动安全AI的发展，Amazon发起了Trusted AI track的Amazon Nova AI Challenge，以解决AI系统在软件开发中的安全性问题。

Method: 通过举办Amazon Nova AI Challenge，团队开发了自动红队机器人和安全AI助手，并进行了对抗性比赛来测试其安全性。

Result: 团队开发了最先进的技术，包括基于推理的安全对齐方法、稳健的模型防护措施、多轮越狱攻击以及高效的大语言模型探测方法。

Conclusion: 本文概述了大学团队和Amazon Nova AI Challenge团队在解决AI软件开发安全挑战方面的进展，强调了这一合作努力如何提高AI安全的标准。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [83] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出了一种范式转变，将人工智能重新定位为诊断过程的主要指导者，而不是医生的助手。通过展示DxDirector-7B，一个具有先进深度思考能力的大语言模型，它能够在最少医生参与的情况下推动全过程诊断，并建立了一个针对误诊的稳健问责框架。评估结果显示，DxDirector-7B在诊断准确性和减少医生工作量方面优于现有的医疗大语言模型和通用大语言模型。


<details>
  <summary>Details</summary>
Motivation: 目前人工智能在临床诊断中的作用主要是作为医生的助手，只能在诊断过程的特定部分回答特定的医学问题，但缺乏从模糊主诉开始驱动整个诊断过程的能力，这仍然严重依赖于人类医生。这种差距限制了人工智能全面减轻医生工作负担和提高诊断效率的能力。

Method: 我们提出了一个范式转变，将医生和人工智能之间的关系逆转：将人工智能重新定位为主要指导者，医生作为其助手。我们展示了DxDirector-7B，一个具有先进深度思考能力的大语言模型，使其能够在最少医生参与的情况下推动全过程诊断。此外，DxDirector-7B建立了一个针对误诊的稳健问责框架，明确了人工智能和人类医生之间的责任。

Result: 在全过程中诊断设置下的罕见、复杂和现实案例评估中，DxDirector-7B不仅实现了显著的诊断准确性，而且比最先进的医疗大语言模型以及通用大语言模型显著减少了医生的工作量。跨多个临床部门和任务的细粒度分析验证了其有效性，专家评估表明其潜在的可行性，可以作为医学专家的可行替代品。

Conclusion: 这些发现标志着一个新时代的开始，人工智能传统上是医生的助手，现在能够驱动整个诊断过程，大幅减少医生的工作量，表明了一个高效且准确的诊断解决方案。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [84] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 本文研究了语言模型对齐过程中静态数据与在线策略数据的效果差异，并提出了对齐阶段假设以优化对齐过程。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型对齐方法在使用在线策略数据时可能并不总是最优的，因此需要一种更全面的理解来优化对齐过程。

Method: 本文通过理论和实证分析，提出了对齐阶段假设，并设计了一个有效的算法来确定这两个阶段之间的边界。

Result: 实验表明，在不同模型和对齐方法中，对齐阶段假设具有普遍适用性，并且可以有效识别对齐过程中的不同阶段。

Conclusion: 本文提出了对语言模型对齐过程的两个阶段假设，并通过实验验证了该假设的通用性。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [85] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 本文提出ComMCS方法，通过减少方差提高LLM的推理能力，并在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM推理的高成本，MC样本数量有限，导致估计误差问题。

Method: 提出了一种称为ComMCS的方法，通过线性组合当前和后续步骤的MC估计器来构建无偏估计器。

Result: ComMCS方法在MATH-500基准测试中比基于回归的优化方法高出2.8分，比非方差减少基线高出2.2分。

Conclusion: ComMCS方法在MATH-500和GSM8K基准测试中表现出色，优于基于回归的优化方法和非方差减少基线。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [86] [CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)
*Zhuoyuan Yu,Yuxing Long,Zihan Yang,Chengyan Zeng,Hongwei Fan,Jiyao Zhang,Hao Dong*

Main category: cs.RO

TL;DR: 本文提出了一种名为Self-correction Flywheel的新后训练范式，通过利用模型在训练集上的错误轨迹作为有价值的数据源，来提升视觉-语言导航模型的性能。实验表明，这种方法显著提高了导航模型的成功率，并在真实机器人测试中展现了优越的错误纠正能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言导航模型在执行指令时常常偏离正确轨迹，但这些模型缺乏有效的错误纠正能力，阻碍了它们从错误中恢复。为了应对这一挑战，我们提出了Self-correction Flywheel，以解决模型在错误轨迹上的问题。

Method: 我们提出了Self-correction Flywheel，一种新的后训练范式。该范式强调模型在训练集上的错误轨迹作为有价值的数据源，并开发了一种方法来识别这些错误轨迹中的偏差，并设计了创新技术来自动生成感知和动作的自我纠正数据。这些自我纠正数据用于持续训练模型。

Result: 在R2R-CE和RxR-CE基准测试中，CorrectNav实现了65.1%和69.3%的新最先进的成功率达到，比之前的最佳VLA导航模型分别高出8.2%和16.4%。真实机器人测试展示了方法在错误纠正、动态障碍物避让和长指令遵循方面的优越能力。

Conclusion: 通过多次飞轮迭代，我们逐步提升了基于单目RGB的VLA导航模型CorrectNav。实验表明，CorrectNav在R2R-CE和RxR-CE基准上达到了新的最先进成功率，优于之前的最佳VLA导航模型。真实机器人测试展示了方法在错误纠正、动态障碍物避让和长指令遵循方面的优越能力。

Abstract: Existing vision-and-language navigation models often deviate from the correct
trajectory when executing instructions. However, these models lack effective
error correction capability, hindering their recovery from errors. To address
this challenge, we propose Self-correction Flywheel, a novel post-training
paradigm. Instead of considering the model's error trajectories on the training
set as a drawback, our paradigm emphasizes their significance as a valuable
data source. We have developed a method to identify deviations in these error
trajectories and devised innovative techniques to automatically generate
self-correction data for perception and action. These self-correction data
serve as fuel to power the model's continued training. The brilliance of our
paradigm is revealed when we re-evaluate the model on the training set,
uncovering new error trajectories. At this time, the self-correction flywheel
begins to spin. Through multiple flywheel iterations, we progressively enhance
our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE
and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success
rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%
and 16.4%. Real robot tests in various indoor and outdoor environments
demonstrate \method's superior capability of error correction, dynamic obstacle
avoidance, and long instruction following.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [87] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 本文提出了一种基于多任务学习的新型模型架构，用于优化个性化产品搜索排名。通过整合表格和非表格数据，并利用预训练的TinyBERT模型进行语义嵌入，该方法在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理混合数据类型和优化个性化排名方面存在不足，因此需要一种更有效的模型架构来解决这些问题。

Method: 本文提出了一种新颖的模型架构，用于使用多任务学习（MTL）框架优化个性化产品搜索排名。我们的方法独特地整合了表格和非表格数据，利用预训练的TinyBERT模型进行语义嵌入，并提出了一种新的采样技术来捕捉多样化的客户行为。

Result: 实验结果表明，结合非表格数据与先进的嵌入技术在多任务学习范式中显著提升了模型性能。消融研究进一步强调了引入相关性标签、微调TinyBERT层以及TinyBERT查询-产品嵌入交互的好处。

Conclusion: 实验结果表明，在多任务学习范式中结合非表格数据和先进的嵌入技术显著提高了模型性能。这些结果证明了我们方法在实现改进的个性化产品搜索排名方面的有效性。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [88] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder是一个分层特征优化框架，用于仓库级代码补全。它通过系统地优化候选者来提高相关性和多样性，并解决外部符号歧义问题。实验表明，Saracoder在多个编程语言和模型上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的仓库级代码补全方法通常依赖于表面文本相似性，导致结果受到语义误导、冗余和同质性的困扰，同时无法解决外部符号歧义问题。

Method: 我们引入了Saracoder，这是一个分层特征优化框架。其核心分层特征优化模块通过提炼深层语义关系、修剪精确重复项、评估与新颖图基度量的结构相似性（按拓扑重要性加权编辑）以及重新排序结果来系统地优化候选者，以最大化相关性和多样性。此外，一个外部感知标识符消歧模块通过依赖分析准确解决跨文件符号歧义。

Result: 在具有挑战性的CrossCodeEval和RepoEval-Updated基准测试中进行的广泛实验表明，Saracoder在多种编程语言和模型上显著优于现有基线。

Conclusion: 我们的工作证明，通过多维系统地优化检索结果，为构建更准确和健壮的仓库级代码补全系统提供了一个新范式。

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [89] [Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning](https://arxiv.org/abs/2508.10057)
*Christopher Pinier,Sonia Acuña Vargas,Mariia Steeghs-Turchina,Dora Matzke,Claire E. Stevenson,Michael D. Nunez*

Main category: q-bio.NC

TL;DR: This study investigates whether large language models (LLMs) mirror human neurocognition during abstract reasoning. The results suggest that LLMs may share some representational principles with the human brain, particularly in abstract reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate whether large language models (LLMs) mirror human neurocognition during abstract reasoning.

Method: We compared the performance and neural representations of human participants with those of eight open-source LLMs on an abstract-pattern-completion task. We leveraged pattern type differences in task performance and in fixation-related potentials (FRPs) as recorded by electroencephalography (EEG) during the task.

Result: Only the largest tested LLMs (~70 billion parameters) achieve human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing similarities with the human pattern-specific difficulty profile. Every LLM tested forms representations that distinctly cluster the abstract pattern categories within their intermediate layers, although the strength of this clustering scales with their performance on the task. Moderate positive correlations were observed between the representational geometries of task-optimal LLM layers and human frontal FRPs.

Conclusion: LLMs might mirror human brain mechanisms in abstract reasoning, offering preliminary evidence of shared principles between biological and artificial intelligence.

Abstract: This study investigates whether large language models (LLMs) mirror human
neurocognition during abstract reasoning. We compared the performance and
neural representations of human participants with those of eight open-source
LLMs on an abstract-pattern-completion task. We leveraged pattern type
differences in task performance and in fixation-related potentials (FRPs) as
recorded by electroencephalography (EEG) during the task. Our findings indicate
that only the largest tested LLMs (~70 billion parameters) achieve
human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing
similarities with the human pattern-specific difficulty profile. Critically,
every LLM tested forms representations that distinctly cluster the abstract
pattern categories within their intermediate layers, although the strength of
this clustering scales with their performance on the task. Moderate positive
correlations were observed between the representational geometries of
task-optimal LLM layers and human frontal FRPs. These results consistently
diverged from comparisons with other EEG measures (response-locked ERPs and
resting EEG), suggesting a potential shared representational space for abstract
patterns. This indicates that LLMs might mirror human brain mechanisms in
abstract reasoning, offering preliminary evidence of shared principles between
biological and artificial intelligence.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 145]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.LG](#cs.LG) [Total: 10]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.AS](#eess.AS) [Total: 2]
- [math.LO](#math.LO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CV](#cs.CV) [Total: 15]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models](https://arxiv.org/abs/2507.02870)
*Chaozhuo Li,Pengbo Wang,Chenxu Wang,Litian Zhang,Zheng Liu,Qiwei Ye,Yuanbo Xu,Feiran Huang,Xi Zhang,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLM）中出现的幻觉现象，探讨了其原因、检测方法和解决策略，旨在推动对该问题的全面应对。


<details>
  <summary>Details</summary>
Motivation: LLM幻觉可能导致误导用户，影响他们的判断和决策，在金融、法律和医疗等领域带来严重的经济损失、法律纠纷和健康风险。

Method: 我们系统地分类并分析了与LLM幻觉相关的原因、检测方法和解决方案。

Result: 我们的研究特别关注于理解幻觉的根源，并评估当前策略在揭示底层逻辑方面的有效性。

Conclusion: 我们的研究旨在通过分析LLM幻觉的原因、检测方法和解决方案，为该领域的发展提供创新和有效的途径。

Abstract: Edgar Allan Poe noted, "Truth often lurks in the shadow of error,"
highlighting the deep complexity intrinsic to the interplay between truth and
falsehood, notably under conditions of cognitive and informational asymmetry.
This dynamic is strikingly evident in large language models (LLMs). Despite
their impressive linguistic generation capabilities, LLMs sometimes produce
information that appears factually accurate but is, in reality, fabricated, an
issue often referred to as 'hallucinations'. The prevalence of these
hallucinations can mislead users, affecting their judgments and decisions. In
sectors such as finance, law, and healthcare, such misinformation risks causing
substantial economic losses, legal disputes, and health risks, with
wide-ranging consequences.In our research, we have methodically categorized,
analyzed the causes, detection methods, and solutions related to LLM
hallucinations. Our efforts have particularly focused on understanding the
roots of hallucinations and evaluating the efficacy of current strategies in
revealing the underlying logic, thereby paving the way for the development of
innovative and potent approaches. By examining why certain measures are
effective against hallucinations, our study aims to foster a comprehensive
approach to tackling this issue within the domain of LLMs.

</details>


### [2] [ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models](https://arxiv.org/abs/2507.02919)
*Dai Li,Linzhuo Li,Huilian Sophie Qiu*

Main category: cs.CL

TL;DR: 本研究发现大型语言模型（如ChatGPT和Llama）在模拟人类意见时存在结构性不一致和同质化问题，这可能影响其作为人类调查数据替代品的有效性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在评估大型语言模型是否能够准确代表人口层面的意见，从而探讨其在模拟人类意见方面的有效性。

Method: 本研究通过向ChatGPT（GPT-4）和Meta的Llama 3.1系列（8B、70B、405B）提出关于堕胎和非法移民的问题，这些问题来自美国国家选举研究（ANES）2020年的数据，以调查结构一致性失败和同质化问题。

Result: 研究发现，与人类数据相比，大型语言模型的响应存在显著的结构性不一致和严重的同质化问题。

Conclusion: 本研究挑战了将大型语言模型（如ChatGPT和Llama）作为人类意见直接替代品的有效性，指出它们可能歪曲总体意见，并可能强化刻板印象和误导政策。

Abstract: Large language models (LLMs) in the form of chatbots like ChatGPT and Llama
are increasingly proposed as "silicon samples" for simulating human opinions.
This study examines this notion, arguing that LLMs may misrepresent
population-level opinions. We identify two fundamental challenges: a failure in
structural consistency, where response accuracy doesn't hold across demographic
aggregation levels, and homogenization, an underrepresentation of minority
opinions. To investigate these, we prompted ChatGPT (GPT-4) and Meta's Llama
3.1 series (8B, 70B, 405B) with questions on abortion and unauthorized
immigration from the American National Election Studies (ANES) 2020. Our
findings reveal significant structural inconsistencies and severe
homogenization in LLM responses compared to human data. We propose an
"accuracy-optimization hypothesis," suggesting homogenization stems from
prioritizing modal responses. These issues challenge the validity of using
LLMs, especially chatbots AI, as direct substitutes for human survey data,
potentially reinforcing stereotypes and misinforming policy.

</details>


### [3] [A Unified Speech LLM for Diarization and Speech Recognition in Multilingual Conversations](https://arxiv.org/abs/2507.02927)
*Phurich Saengthong,Boonnithi Jiaramaneepinit,Sheng Li,Manabu Okumura,Takahiro Shinozaki*

Main category: cs.CL

TL;DR: 本文提出了一种统一的语音大语言模型，能够在多语言对话中联合执行说话人分离和自动语音识别，并在任务II中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏捕捉自然对话现象的数据，语音大语言模型在真实多语言对话中的效果受到限制。为此，MLC-SLM挑战提供了一个多语言对话数据集，并评估模型在两个任务上的表现。

Method: 通过重新格式化训练数据格式并修改推理过程，解决了预分割音频中的模糊性问题。

Result: 在任务II中，提出的模型实现了54.87%的相对提升，并在任务I中使用微调的语音大语言模型取得了结果。

Conclusion: 本文提出了一种统一的语音大语言模型，能够以端到端的方式联合执行说话人分离和自动语音识别。尽管使用了较小的LLM骨干网络，但该模型在tcpWER/tcpCER上相对于基线实现了54.87%的相对提升，并在任务II中排名第八。

Abstract: Speech Large Language Models (Speech LLMs) have emerged as a crucial paradigm
in recent years, extending the capabilities of traditional LLMs to speech tasks
such as automatic speech recognition (ASR) and spoken dialogue modeling.
However, their effectiveness in real-world multilingual conversations remains
limited by the scarcity of data that captures natural conversational phenomena.
To address this, the MLC-SLM Challenge provides a multilingual conversational
dataset and evaluates models on two tasks: ASR with oracle segmentation (Task
I) and joint diarization and recognition without oracle information (Task II).
In this paper, we focus on Task II and propose a unified speech LLM that
jointly performs diarization and ASR in an end-to-end manner. By reformulating
the training data format and modifying the inference procedure, our model
addresses the ambiguity inherent in pre-segmented audio and achieves a 54.87\%
relative improvement in tcpWER/tcpCER over the baseline, ranking 8th overall,
despite using a smaller LLM backbone. We also report results from Task I using
a fine-tuned speech LLM.

</details>


### [4] [Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models](https://arxiv.org/abs/2507.02928)
*Hao Yang,Haoxuan Li,Luyu Chen,Haoxiang Wang,Xu Chen,Mingming Gong*

Main category: cs.CL

TL;DR: 本文提出了ProCI框架，利用大型语言模型的语义和世界知识来解决隐藏混杂因素问题，从而改善处理效果估计。


<details>
  <summary>Details</summary>
Motivation: 隐藏混杂因素是估计处理效果的一个中心挑战，因为未观察到的变量可能导致有偏的因果估计。虽然最近的工作探索了使用大型语言模型进行因果推断，但大多数方法仍然依赖于无混杂假设。

Method: 提出了一种名为ProCI（渐进式混杂因素填补）的框架，利用大型语言模型的语义和世界知识来迭代生成、填补和验证隐藏混杂因素。

Result: 广泛的实验表明，ProCI能够发现有意义的混杂因素，并在各种数据集和大型语言模型上显著改善处理效果估计。

Conclusion: ProCI能够发现有意义的混杂因素，并在各种数据集和大型语言模型上显著改善处理效果估计。

Abstract: Hidden confounding remains a central challenge in estimating treatment
effects from observational data, as unobserved variables can lead to biased
causal estimates. While recent work has explored the use of large language
models (LLMs) for causal inference, most approaches still rely on the
unconfoundedness assumption. In this paper, we make the first attempt to
mitigate hidden confounding using LLMs. We propose ProCI (Progressive
Confounder Imputation), a framework that elicits the semantic and world
knowledge of LLMs to iteratively generate, impute, and validate hidden
confounders. ProCI leverages two key capabilities of LLMs: their strong
semantic reasoning ability, which enables the discovery of plausible
confounders from both structured and unstructured inputs, and their embedded
world knowledge, which supports counterfactual reasoning under latent
confounding. To improve robustness, ProCI adopts a distributional reasoning
strategy instead of direct value imputation to prevent the collapsed outputs.
Extensive experiments demonstrate that ProCI uncovers meaningful confounders
and significantly improves treatment effect estimation across various datasets
and LLMs.

</details>


### [5] [Theory of Mind in Action: The Instruction Inference Task](https://arxiv.org/abs/2507.02935)
*Fardin Saad,Pradeep K. Murukannaiah,Munindar P. Singh*

Main category: cs.CL

TL;DR: The paper introduces Tomcat, an LLM-based agent for ToM reasoning, and evaluates its performance in interpreting instructions. Results show that Tomcat can achieve human-like performance in certain scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess ToM in a dynamic, goal-oriented, and collaborative environment, we introduced a novel task called Instruction Inference. The goal was to develop an agent capable of interpreting and responding to indirect or ambiguous instructions.

Method: We introduced a novel task called Instruction Inference and presented Tomcat, an LLM-based agent, designed to exhibit ToM reasoning. We implemented two variants of Tomcat: Fs-CoT and CP. We evaluated the effectiveness of Tomcat through a study with 52 human participants.

Result: Tomcat with Fs-CoT, particularly with GPT-4o and DeepSeek-R1, achieved performance comparable to human participants, indicating its potential for human-AI collaboration.

Conclusion: Tomcat with Fs-CoT, particularly with GPT-4o and DeepSeek-R1, achieves performance comparable to human participants, underscoring its ToM potential for human-AI collaboration.

Abstract: The Theory of Mind (ToM) refers to an agent's capacity to infer the mental
states of other agents. ToM is essential for effective collaboration. To assess
ToM in a dynamic, goal-oriented, and collaborative environment, we introduce a
novel task, Instruction Inference, in which an agent assists a principal in
reaching a goal by interpreting indirect or ambiguous instructions. We present
Tomcat, an LLM-based agent, designed to exhibit ToM reasoning in interpreting
and responding to the principal's instructions. We implement two variants of
Tomcat. One, dubbed Fs-CoT, is based on a small number of examples (i.e.,
few-shot or Fs) demonstrating the requisite structured reasoning (i.e.,
chain-of-thought or CoT). One, dubbed CP, relies on commonsense knowledge and
information about the problem (i.e., commonsense prompt or CP). We realized
both variants of Tomcat on three leading large language models (LLMs), namely,
GPT-4o, DeepSeek-R1, and Gemma-3-27B. To evaluate the effectiveness of Tomcat,
we conducted a study with 52 human participants in which we provided
participants with the same information as the CP variant of Tomcat. We computed
intent accuracy, action optimality, and planning optimality to measure the ToM
capabilities of Tomcat and our study participants. We found that Tomcat with
Fs-CoT, particularly with GPT-4o and DeepSeek-R1, achieves performance
comparable to the human participants, underscoring its ToM potential for
human-AI collaboration.

</details>


### [6] [A Large Language Model-Empowered Agent for Reliable and Robust Structural Analysis](https://arxiv.org/abs/2507.02938)
*Jiachen Liu,Ziheng Geng,Ran Cao,Lu Cheng,Paolo Bocchini,Minghui Cheng*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在土木工程结构分析中的应用，发现它们在定量可靠性方面存在不足。为此，研究提出了一种将结构分析重新定义为代码生成任务的方法，并开发了一个基于LLM的代理，实现了高达99.0%的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多种开放领域任务中表现出色，但它们在专门领域如土木工程中的应用仍很少被探索。本研究旨在填补这一空白，评估和增强LLMs在梁结构分析中的可靠性和鲁棒性。

Method: 该研究通过创建一个包含八个梁分析问题的基准数据集来测试Llama-3.3 70B Instruct模型，并提出了一个基于LLM的代理，该代理结合了思维链和少量示例提示来生成准确的OpeeSeesPy代码，并自动执行代码以产生结构分析结果。

Result: 实验结果表明，尽管LLM对结构力学有定性的理解，但它们在工程应用中缺乏定量的可靠性和鲁棒性。然而，基于LLM的代理在基准数据集上的准确性超过了99.0%，并在各种条件下表现出可靠和稳健的性能。

Conclusion: 该研究提出了一种将结构分析重新定义为代码生成任务的方法，开发了一个基于LLM的代理，能够生成准确的OpeeSeesPy代码并自动执行以产生结构分析结果。实验结果表明，该代理在基准数据集上的准确性超过了99.0%，表现出可靠和稳健的性能。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across
diverse open-domain tasks, yet their application in specialized domains such as
civil engineering remains largely unexplored. This paper starts bridging this
gap by evaluating and enhancing the reliability and robustness of LLMs in
structural analysis of beams. Reliability is assessed through the accuracy of
correct outputs under repetitive runs of the same problems, whereas robustness
is evaluated via the performance across varying load and boundary conditions. A
benchmark dataset, comprising eight beam analysis problems, is created to test
the Llama-3.3 70B Instruct model. Results show that, despite a qualitative
understanding of structural mechanics, the LLM lacks the quantitative
reliability and robustness for engineering applications. To address these
limitations, a shift is proposed that reframes the structural analysis as code
generation tasks. Accordingly, an LLM-empowered agent is developed that (a)
integrates chain-of-thought and few-shot prompting to generate accurate
OpeeSeesPy code, and (b) automatically executes the code to produce structural
analysis results. Experimental results demonstrate that the agent achieves
accuracy exceeding 99.0% on the benchmark dataset, exhibiting reliable and
robust performance across diverse conditions. Ablation studies highlight the
complete example and function usage examples as the primary contributors to the
agent's enhanced performance.

</details>


### [7] [Towards a Comparative Framework for Compositional AI Models](https://arxiv.org/abs/2507.02940)
*Tiffany Duneau*

Main category: cs.CL

TL;DR: 本文介绍了DisCoCirc框架，它允许构建文本的组合模型，通过根据文本的语法结构组合单个单词的单元。我们介绍了组合泛化和组合可解释性的概念，并将其应用于DisCoCirc框架。我们比较了基于量子电路的模型和经典神经网络在组合泛化任务上的表现，并展示了如何解释一个组合模型的行为。


<details>
  <summary>Details</summary>
Motivation: 组合模型可以产生两种东西：组合泛化——模型在学习整个数据分布的基础组合规则下，具有在训练分布之外泛化的潜力；以及组合可解释性——通过检查其模块化组件以及这些组件如何组合的过程来理解模型的工作原理。我们希望在框架无关的方式下介绍这些概念，并将其应用于DisCoCirc框架。

Method: 我们使用范畴论的语言以框架无关的方式介绍了这些概念，并将一系列组合泛化测试适应到这种设置中。我们将这种方法应用于DisCoCirc框架，考虑了一组模型在组合泛化方面的表现。我们比较了基于量子电路的模型和经典神经网络在一个源自bAbI任务的数据集上的表现，该数据集被扩展以测试组合性的各个方面。

Result: 两种架构在生产力和替代任务上的得分相差不超过5%，但在系统性任务上的差异至少为10%，并且在过度泛化任务上表现出不同的趋势。

Conclusion: 我们发现神经网络模型更容易过拟合训练数据。此外，我们展示了如何解释一个组合模型在其中一个训练模型上的行为。通过考虑模型组件之间的相互作用，我们解释了模型的行为。

Abstract: The DisCoCirc framework for natural language processing allows the
construction of compositional models of text, by combining units for individual
words together according to the grammatical structure of the text. The
compositional nature of a model can give rise to two things: compositional
generalisation -- the ability of a model to generalise outside its training
distribution by learning compositional rules underpinning the entire data
distribution -- and compositional interpretability -- making sense of how the
model works by inspecting its modular components in isolation, as well as the
processes through which these components are combined. We present these notions
in a framework-agnostic way using the language of category theory, and adapt a
series of tests for compositional generalisation to this setting.
  Applying this to the DisCoCirc framework, we consider how well a selection of
models can learn to compositionally generalise. We compare both quantum circuit
based models, as well as classical neural networks, on a dataset derived from
one of the bAbI tasks, extended to test a series of aspects of
compositionality. Both architectures score within 5% of one another on the
productivity and substitutivity tasks, but differ by at least 10% for the
systematicity task, and exhibit different trends on the overgeneralisation
tasks. Overall, we find the neural models are more prone to overfitting the
Train data. Additionally, we demonstrate how to interpret a compositional model
on one of the trained models. By considering how the model components interact
with one another, we explain how the model behaves.

</details>


### [8] [The Application of Large Language Models on Major Depressive Disorder Support Based on African Natural Products](https://arxiv.org/abs/2507.02947)
*Linyan Zou*

Main category: cs.CL

TL;DR: 本文探讨了将大型语言模型与非洲天然产品结合用于抑郁症支持的可能性，展示了其在提供个性化、文化适当的支持方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统抗抑郁药物存在起效缓慢、显著副作用和治疗抵抗等问题，因此研究人员和医疗保健提供者探索了替代治疗方法。非洲传统医学提供了开发新型抗抑郁治疗的宝贵资源。

Method: 本文综合分析了具有已知抗抑郁特性的非洲药用植物，以及开发了一个利用DeepSeek先进语言模型能力的AI支持系统。

Result: 研究结果表明，大型语言模型可以作为传统知识和现代医疗保健之间的桥梁，提供基于证据的抑郁症支持系统。

Conclusion: 本文展示了大型语言模型在连接传统知识和现代医疗保健方面的潜力，提供个性化的、文化适当的抑郁症支持。

Abstract: Major depressive disorder represents one of the most significant global
health challenges of the 21st century, affecting millions of people worldwide
and creating substantial economic and social burdens. While conventional
antidepressant therapies have provided relief for many individuals, their
limitations including delayed onset of action, significant side effects, and
treatment resistance in a substantial portion of patients have prompted
researchers and healthcare providers to explore alternative therapeutic
approaches (Kasneci et al.). African traditional medicine, with its rich
heritage of plant-based remedies developed over millennia, offers a valuable
resource for developing novel antidepressant treatments that may address some
of these limitations. This paper examines the integration of large language
models with African natural products for depression support, combining
traditional knowledge with modern artificial intelligence technology to create
accessible, evidence-based mental health support systems.
  The research presented here encompasses a comprehensive analysis of African
medicinal plants with documented antidepressant properties, their
pharmacological mechanisms, and the development of an AI-powered support system
that leverages DeepSeek's advanced language model capabilities. The system
provides evidence-based information about African herbal medicines, their
clinical applications, safety considerations, and therapeutic protocols while
maintaining scientific rigor and appropriate safety standards. Our findings
demonstrate the potential for large language models to serve as bridges between
traditional knowledge and modern healthcare, offering personalized, culturally
appropriate depression support that honors both traditional wisdom and
contemporary medical understanding.

</details>


### [9] [RADIANT: Retrieval AugmenteD entIty-context AligNmenT -- Introducing RAG-ability and Entity-Context Divergence](https://arxiv.org/abs/2507.02949)
*Vipula Rawte,Rajarshi Roy,Gurpreet Singh,Danush Khanna,Yaswanth Narsupalli,Basab Ghosh,Abhay Gupta,Argha Kamal Samanta,Aditya Shingote,Aadi Krishna Vikram,Vinija Jain,Aman Chadha,Amit Sheth,Amitava Das*

Main category: cs.CL

TL;DR: 本文介绍了Radiant框架，通过将RAG与对齐技术结合，提高LLMs在检索增强设置中的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG技术已被广泛采用以提高事实准确性，但LLMs往往无法忠实地将检索到的证据整合到生成的回答中，导致事实不一致。因此，需要一种方法来量化这一差距并改进RAG能力。

Method: 本文引入了Entity-Context Divergence (ECD)度量，用于衡量检索信息在模型输出中准确反映的程度，并提出了Radiant框架，扩展了Direct Preference Optimization (DPO)来教LLMs如何将提供的额外信息整合到后续生成中。

Result: 实证分析表明，大多数LLMs的RAG能力仍然较低，突显了实体保留和上下文忠实性方面的重大挑战。Radiant框架在各种检索场景中提高了RAG性能，例如噪声网络上下文、知识冲突和幻觉减少。

Conclusion: 本文提出了Radiant框架，该框架通过将RAG与对齐技术结合，优化检索证据和生成内容之间的相互作用，从而提高RAG性能。

Abstract: As Large Language Models (LLMs) continue to advance, Retrieval-Augmented
Generation (RAG) has emerged as a vital technique to enhance factual accuracy
by integrating external knowledge into the generation process. However, LLMs
often fail to faithfully integrate retrieved evidence into their generated
responses, leading to factual inconsistencies. To quantify this gap, we
introduce Entity-Context Divergence (ECD), a metric that measures the extent to
which retrieved information is accurately reflected in model outputs. We
systematically evaluate contemporary LLMs on their ability to preserve factual
consistency in retrieval-augmented settings, a capability we define as
RAG-ability. Our empirical analysis reveals that RAG-ability remains low across
most LLMs, highlighting significant challenges in entity retention and context
fidelity. This paper introduces Radiant (Retrieval AugmenteD entIty-context
AligNmenT), a novel framework that merges RAG with alignment designed to
optimize the interplay between retrieved evidence and generated content.
Radiant extends Direct Preference Optimization (DPO) to teach LLMs how to
integrate provided additional information into subsequent generations. As a
behavior correction mechanism, Radiant boosts RAG performance across varied
retrieval scenarios, such as noisy web contexts, knowledge conflicts, and
hallucination reduction. This enables more reliable, contextually grounded, and
factually coherent content generation.

</details>


### [10] [Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria](https://arxiv.org/abs/2507.02950)
*Keita Kiuchi,Yoshikazu Fujimoto,Hideyuki Goto,Tomonori Hosokawa,Makoto Nishimura,Yosuke Sato,Izumi Sezai*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型在日语治疗情境中的三种咨询角色性能，发现SMDP显著提高了咨询AI的表现，评估AI在某些方面与人类评分者相当，但存在模型特定偏差，客户AI模拟需要增强现实感。这些发现为非英语语境中的AI辅助咨询设定了基准，并指出了改进的关键领域。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在提供对大型语言模型（LLM）在日语治疗情境中三种咨询角色性能的首次全面评估。

Method: 本研究评估了三种咨询角色在日语治疗情境中的大型语言模型（LLM）性能，同时评估了咨询AI系统（GPT-4-turbo与零样本提示或结构化多步骤对话提示（SMDP），Claude-3-Opus-SMDP）、客户AI模拟和评估AI系统（o3，Claude-3.7-Sonnet，Gemini-2.5-pro）。

Result: SMDP实现显著提高了咨询AI在所有MITI全局评分上的表现，与零样本提示相比，GPT-SMDP和Opus-SMDP之间没有显著差异。评估AI在培养改变谈话方面与人类评分者表现相当，但系统性地高估了软化维持谈话和总体质量指标。模型特定的偏差出现：Gemini强调权力分享，o3注重技术熟练度，Sonnet优先考虑情感表达。客户AI模拟表现出有限的情感范围和异常高的顺从性，表明需要增强现实感。

Conclusion: 这些发现为非英语语境中的AI辅助咨询设定了基准，并通过先进的提示工程、检索增强生成和针对性微调指出了改进的关键领域，对开发文化敏感的AI心理健康工具具有重要意义。

Abstract: This study provides the first comprehensive evaluation of large language
model (LLM) performance across three counseling roles in Japanese-language
therapeutic contexts. We simultaneously assessed counselor artificial
intelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured
Multi-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations,
and evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human
experts (n = 15) with extensive counseling experience evaluated AI-generated
dialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding
Manual 4.2.1.
  Notably, SMDP implementation significantly enhanced counselor AI performance
across all MITI global ratings compared with zeroshot prompting, with no
significant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed
comparable performance to human raters for Cultivating Change Talk but
systematically overestimated Softening Sustain Talk and the overall quality
metrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3
focused on technical proficiency, and Sonnet prioritized emotional expression.
Client AI simulations exhibited a limited emotional range and unnaturally high
compliance, indicating the need for enhanced realism.
  These findings establish benchmarks for AI-assisted counseling in non-English
contexts and identify critical areas for improvement through advanced prompt
engineering, retrieval-augmented generation, and targeted fine-tuning, with
important implications for developing culturally sensitive AI mental health
tools.

</details>


### [11] [Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III](https://arxiv.org/abs/2507.02954)
*Pranam Shetty,Abhisek Upadhayaya,Parth Mitesh Shah,Srikanth Jagabathula,Shilpi Nayak,Anna Joo Fee*

Main category: cs.CL

TL;DR: 本文介绍了一个全面的基准测试，评估了23个最先进的LLMs在CFA Level III考试上的表现，结果显示领先的模型在金融推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着金融机构越来越多地采用大型语言模型（LLMs），严格的领域特定评估对于负责任的部署变得至关重要。

Method: 我们评估了23个最先进的LLMs在CFA Level III考试上的表现，包括多项选择题（MCQs）和作文式回答，并使用了多种提示策略，包括Chain-of-Thought和Self-Discover。

Result: 领先的模型表现出强大的能力，例如o4-mini的综合得分为79.1%，Gemini 2.5 Flash的综合得分为77.3%。这些结果是在修订后的更严格的作文评分方法下取得的。

Conclusion: 我们的研究结果为从业者在模型选择上提供了重要指导，并突显了在成本效益部署和对专业基准性能的细微解释方面的剩余挑战。

Abstract: As financial institutions increasingly adopt Large Language Models (LLMs),
rigorous domain-specific evaluation becomes critical for responsible
deployment. This paper presents a comprehensive benchmark evaluating 23
state-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam -
the gold standard for advanced financial reasoning. We assess both
multiple-choice questions (MCQs) and essay-style responses using multiple
prompting strategies including Chain-of-Thought and Self-Discover. Our
evaluation reveals that leading models demonstrate strong capabilities, with
composite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA
Level III. These results, achieved under a revised, stricter essay grading
methodology, indicate significant progress in LLM capabilities for high-stakes
financial applications. Our findings provide crucial guidance for practitioners
on model selection and highlight remaining challenges in cost-effective
deployment and the need for nuanced interpretation of performance against
professional benchmarks.

</details>


### [12] [Real-World En Call Center Transcripts Dataset with PII Redaction](https://arxiv.org/abs/2507.02958)
*Ha Dao,Gaurav Chawla,Raghu Banda,Caleb DeLeeuw*

Main category: cs.CL

TL;DR: CallCenterEN is a large-scale, real-world English call center transcript dataset designed to support research and development in customer support and sales AI systems, filling a critical gap in the landscape of available ASR corpora.


<details>
  <summary>Details</summary>
Motivation: The scarcity of publicly available real-world call center datasets motivated the creation of CallCenterEN to support research and development in customer support and sales AI systems.

Method: CallCenterEN is a large-scale English call center transcript dataset containing 91,706 conversations, corresponding to 10448 audio hours, with PII-redacted human-readable transcriptions.

Result: CallCenterEN is the largest release to-date of open source call center transcript data of this kind, including inbound and outbound calls between agents and customers with accents from India, the Philippines, and the United States.

Conclusion: CallCenterEN fills a critical gap in the landscape of available ASR corpora and is released under a CC BY-NC 4.0 license for non-commercial research use.

Abstract: We introduce CallCenterEN, a large-scale (91,706 conversations, corresponding
to 10448 audio hours), real-world English call center transcript dataset
designed to support research and development in customer support and sales AI
systems. This is the largest release to-date of open source call center
transcript data of this kind. The dataset includes inbound and outbound calls
between agents and customers, with accents from India, the Philippines and the
United States. The dataset includes high-quality, PII-redacted human-readable
transcriptions. All personally identifiable information (PII) has been
rigorously removed to ensure compliance with global data protection laws. The
audio is not included in the public release due to biometric privacy concerns.
Given the scarcity of publicly available real-world call center datasets,
CallCenterEN fills a critical gap in the landscape of available ASR corpora,
and is released under a CC BY-NC 4.0 license for non-commercial research use.

</details>


### [13] [RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism](https://arxiv.org/abs/2507.02962)
*Zhiwen Tan,Jiaming Huang,Qintong Wu,Hongxuan Zhang,Chenyi Zhuang,Jinjie Gu*

Main category: cs.CL

TL;DR: 本文提出了一种新的训练框架RAG-R1，使大型语言模型能够自适应地利用内部和外部知识进行推理，并通过多查询并行性减少推理时间，提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的检索增强生成（RAG）方法通过强化学习（RL）探索了增强模型的搜索和推理能力，但它们在训练稳定性方面面临挑战，并且由于单查询模式而遇到推理时间长和能力受限的问题。因此，本文旨在解决这些问题，提出一种新的训练框架。

Method: 本文提出了RAG-R1，这是一种新的训练框架，旨在使大型语言模型能够自适应地利用内部和外部知识进行推理。此外，还扩展了生成和检索过程，从单查询模式转变为多查询并行性，以减少推理时间并增强模型能力。

Result: 在七个问答基准测试中的广泛实验表明，本文的方法比最强的基线高出多达13.2%，并且推理时间减少了11.1%。

Conclusion: 本文提出了一种新的训练框架RAG-R1，使大型语言模型能够在推理过程中自适应地利用内部和外部知识。实验结果表明，该方法在七个问答基准测试中优于最强的基线，并减少了推理时间。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, while they remain prone to generating hallucinated or outdated
responses due to their static internal knowledge. Recent advancements in
Retrieval-Augmented Generation (RAG) methods have explored enhancing models'
search and reasoning capabilities through reinforcement learning (RL). Although
these methods demonstrate promising results, they face challenges in training
stability and encounter issues such as substantial inference time and
restricted capabilities due to the single-query mode. In this paper, we propose
RAG-R1, a novel training framework designed to enable LLMs to adaptively
leverage internal and external knowledge during the reasoning process. We
further expand the generation and retrieval processes within the framework from
single-query mode to multi-query parallelism, aimed at reducing inference time
and enhancing the model's capabilities. Extensive experiments on seven
question-answering benchmarks demonstrate that our method outperforms the
strongest baseline by up to 13.2% and decreases inference time by 11.1%.

</details>


### [14] [Less Data, More Security: Advancing Cybersecurity LLMs Specialization via Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal Tokens](https://arxiv.org/abs/2507.02964)
*Salahuddin Salahuddin,Ahmed Hussain,Jussi Löppönen,Toni Jutila,Panos Papadimitratos*

Main category: cs.CL

TL;DR: 本文研究了领域自适应连续预训练（DAP）作为增强预训练大型语言模型（LLMs）网络安全理解的方法，同时保持通用语言能力。通过使用一个精心整理的1.26亿词的网络安全语料库，对三种解码器架构进行系统适应，结果表明，这种方法在三个网络安全基准测试中均取得了显著提升，并且在使用较小数据集的情况下实现了竞争性性能，证明了高效领域专业化的可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）表现出卓越的自然语言能力，但通用模型缺乏有效的网络安全分析的专业领域知识。因此，需要一种方法来增强LLM的网络安全理解，同时保持其通用语言能力。

Method: 本文研究了领域自适应连续预训练（DAP）作为一种方法，用于增强预训练LLM的网络安全理解，同时保持通用语言能力。我们使用了一个精心整理的1.26亿词的网络安全语料库，对三种解码器架构进行了系统适应。我们的方法采用了约束训练参数和分布式FSDP训练，以平衡领域专业化与知识保留。

Result: 在三个网络安全基准测试（CTI-MCQ、CyberMetric和SecEval）上的评估表明，适应后有持续的改进。Llama-3.3-70B-Ins-DAP模型分别达到了0.718、0.933和0.864的最先进准确率，优于专门模型，包括Llama-Primus-Base。值得注意的是，使用显著更小的数据集（118.8百万与2.77十亿token）实现了竞争性性能，证明了高效领域专业化的可行性。

Conclusion: 本文表明，有针对性的连续预训练可以实现有效的网络安全领域适应，并具有计算可行性，为威胁分析、漏洞评估和安全文档方面的专业AI助手奠定了基础，同时挑战了关于LLM专业化数据需求的现有假设。

Abstract: While Large Language Models (LLMs) demonstrate exceptional natural language
capabilities, general-purpose models lack specialized domain knowledge for
effective cybersecurity analysis. In this work, we investigate Domain-Adaptive
Continuous Pretraining (DAP) as a methodology for enhancing cybersecurity
understanding in pretrained LLMs while preserving general language
capabilities. We systematically adapted three decoder-based architectures --
Llama-3.1-8B, DeepSeek-R1-Distill-Qwen-14B, and Llama-3.3-70B-Instruct -- using
a curated 126-million-word cybersecurity corpus from standards, academic
literature, and various other sources. Our approach employed constrained
training parameters and distributed FSDP training to balance domain
specialization with knowledge preservation. Evaluation across three
cybersecurity benchmarks, namely, CTI-MCQ, CyberMetric, and SecEval,
demonstrates consistent improvements post-adaptation. The Llama-3.3-70B-Ins-DAP
model achieved state-of-the-art accuracies of 0.718, 0.933, and 0.864,
respectively, outperforming specialized models, including Llama-Primus-Base.
Notably, competitive performance was achieved using substantially smaller
datasets (118.8 million versus 2.77 billion tokens), demonstrating efficient
domain specialization viability. We establish that targeted continuous
pretraining enables effective cybersecurity domain adaptation with
computational feasibility, providing foundations for specialized AI assistants
in threat analysis, vulnerability assessment, and security documentation while
challenging prevailing assumptions about data requirements for LLM
specialization.

</details>


### [15] [PB-LLMs: Privacy- and Bias-aware NLP Models using Named-Entity Recognition](https://arxiv.org/abs/2507.02966)
*Gonzalo Mancera,Aythami Morales,Julian Fierrez,Ruben Tolosana,Alejandro Penna,Miguel Lopez-Duran,Francisco Jurado,Alvaro Ortigosa*

Main category: cs.CL

TL;DR: 本研究提出了一种基于命名实体识别的隐私保护框架，用于匿名化敏感信息，以保护高风险AI应用中的用户隐私。同时，还结合了减少性别偏见的方法，开发了隐私和偏见感知的LLMs（PB-LLMs），并在简历评分场景中进行了评估，结果显示其有效且具有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在高风险AI应用中的使用增加，但存在隐私、数据保护和透明性等重要法律和伦理问题，因此需要探索一种隐私保护的方法来解决这些问题。

Method: 该研究提出了一种使用命名实体识别（NER）技术来匿名化文本数据中敏感信息的框架，以实现隐私保护的学习。此外，还实验了一种减少LLM性别偏见的现有方法，最终获得了隐私和偏见感知的LLMs（PB-LLMs）。

Result: 研究结果表明，所提出的隐私保护技术在保持系统性能的同时，有效保障了候选人的机密性。此外，PB-LLMs在简历评分场景中进行了评估，但其适用范围更广。

Conclusion: 该研究提出的隐私和偏见感知的LLMs（PB-LLMs）在保护候选人机密性方面发挥了关键作用，同时保持了系统性能，从而促进了实验场景中的信任。PB-LLMs不仅适用于简历评分，还可广泛应用于其他基于LLM的AI应用。

Abstract: The use of Natural Language Processing (NLP) in high-stakes AI-based
applications has increased significantly in recent years, especially since the
emergence of Large Language Models (LLMs). However, despite their strong
performance, LLMs introduce important legal/ethical concerns, particularly
regarding privacy, data protection, and transparency. Due to these concerns,
this work explores the use of Named-Entity Recognition (NER) to facilitate the
privacy-preserving training (or adaptation) of LLMs. We propose a framework
that uses NER technologies to anonymize sensitive information in text data,
such as personal identities or geographic locations. An evaluation of the
proposed privacy-preserving learning framework was conducted to measure its
impact on user privacy and system performance in a particular high-stakes and
sensitive setup: AI-based resume scoring for recruitment processes. The study
involved two language models (BERT and RoBERTa) and six anonymization
algorithms (based on Presidio, FLAIR, BERT, and different versions of GPT)
applied to a database of 24,000 candidate profiles. The findings indicate that
the proposed privacy preservation techniques effectively maintain system
performance while playing a critical role in safeguarding candidate
confidentiality, thus promoting trust in the experimented scenario. On top of
the proposed privacy-preserving approach, we also experiment applying an
existing approach that reduces the gender bias in LLMs, thus finally obtaining
our proposed Privacy- and Bias-aware LLMs (PB-LLMs). Note that the proposed
PB-LLMs have been evaluated in a particular setup (resume scoring), but are
generally applicable to any other LLM-based AI application.

</details>


### [16] [We Need Knowledge Distillation for Solving Math Word Problems](https://arxiv.org/abs/2507.02982)
*Zhenquan Shen,Xinguo Yu,Xiaotian Cheng,Rao Peng,Hao Ming*

Main category: cs.CL

TL;DR: 本文研究了通过压缩嵌入向量和知识蒸馏方法来降低大型语言模型的计算成本，结果表明压缩后的模型在保持高准确率和泛化能力的同时显著减少了参数数量，为智能教育提供了高效且经济的解决方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学教育中的应用面临计算资源消耗大、成本高的问题。因此，研究如何压缩这些模型以降低计算成本成为必要。

Method: 本文采用BERT编码嵌入向量，并通过知识蒸馏方法训练一个更小的学生模型。同时，分析了嵌入向量的可压缩性，发现词性信息对于数学问题求解更为重要。

Result: 学生模型能够保持教师模型约90%的性能，同时仅使用其1/12的参数。压缩后的嵌入向量在所有与数学问题相关的任务中表现良好，且知识蒸馏过程不依赖特定任务。此外，词性信息是导致嵌入向量可压缩性的关键因素。

Conclusion: 本文展示了通过压缩嵌入向量和知识蒸馏方法，可以有效地将大型语言模型（LLMs）压缩为一个性能接近原始模型但参数数量大大减少的学生模型，从而在保持高准确率和泛化能力的同时显著降低计算成本。这种压缩方法为智能辅导系统提供了更高的效率和经济价值，推动了智能教育领域的发展。

Abstract: The enhancement of mathematical capabilities in large language models (LLMs)
fosters new developments in mathematics education within primary and secondary
schools, particularly as they relate to intelligent tutoring systems. However,
LLMs require substantial computational resources, resulting in significant
costs in educational contexts. To mitigate this drawback, this paper
investigates the feasibility of compressing LLMs for solving math word problems
(MWPs). We compress the embedded vectors encoded by BERT and distill a
considerably smaller student model. Our findings indicate that the student
model can maintain nearly 90% of the performance of the teacher model while
utilizing only 1/12 of its parameters. In addition to achieving high accuracy,
the model exhibits strong generalizability, as the compressed vectors perform
well across all tasks related to MWPs, and the distillation process is not
task-specific. The success of this distillation demonstrates that the
underlying principles are generic and not limited to a specific task. We
further explore the reasons behind the compressibility of embedded vectors,
revealing that part-of-speech information, rather than entity recognition, is
crucial for MWPs, which may significantly contribute to their compressibility.
The improvements in efficiency and cost reduction provide substantial value for
intelligent tutoring systems and significantly advance the field of intelligent
education.

</details>


### [17] [Truth, Trust, and Trouble: Medical AI on the Edge](https://arxiv.org/abs/2507.02983)
*Mohammad Anas Azeez,Rafiq Ali,Ebad Shabbir,Zohaib Hasan Siddiqui,Gautam Siddharth Kashyap,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: 研究评估了多个大型语言模型在医疗问答任务中的表现，发现模型在准确性、安全性和有用性之间存在权衡，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型满足医疗行业对事实准确性、有用性和安全性的关键标准，特别是开源解决方案。

Method: 使用包含1000多个健康问题的数据集对模型性能进行评估，包括诚实度、有用性和无害性。

Result: AlpaCare-13B在准确性和无害性方面表现最佳，而BioMistral-7B-DARE在较小规模下提升了安全性。少量提示提高了准确性，但所有模型在复杂查询上的有用性下降。

Conclusion: 研究结果表明，尽管在事实准确性、安全性和有用性方面存在权衡，但通过领域特定的微调和少量提示可以提高模型性能，但仍需解决临床问答中的挑战。

Abstract: Large Language Models (LLMs) hold significant promise for transforming
digital health by enabling automated medical question answering. However,
ensuring these models meet critical industry standards for factual accuracy,
usefulness, and safety remains a challenge, especially for open-source
solutions. We present a rigorous benchmarking framework using a dataset of over
1,000 health questions. We assess model performance across honesty,
helpfulness, and harmlessness. Our results highlight trade-offs between factual
reliability and safety among evaluated models -- Mistral-7B,
BioMistral-7B-DARE, and AlpaCare-13B. AlpaCare-13B achieves the highest
accuracy (91.7%) and harmlessness (0.92), while domain-specific tuning in
BioMistral-7B-DARE boosts safety (0.90) despite its smaller scale. Few-shot
prompting improves accuracy from 78% to 85%, and all models show reduced
helpfulness on complex queries, highlighting ongoing challenges in clinical QA.

</details>


### [18] [From Answers to Rationales: Self-Aligning Multimodal Reasoning with Answer-Oriented Chain-of-Thought](https://arxiv.org/abs/2507.02984)
*Wentao Tan,Qiong Cao,Yibing Zhan,Chao Xue,Changxing Ding*

Main category: cs.CL

TL;DR: 本文提出SMART框架，通过自对齐和AoT提示生成高质量的正向和负向推理路径，从而提高多模态大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要关注合成正向推理路径，而忽视了负向推理路径在训练模型识别错误推理模式中的关键作用。

Method: 提出了一种名为SMART的新框架，利用AoT-Oriented Chain-of-Thought (AoT)提示自动生成高质量的正向和负向推理路径，并通过自对齐来增强推理能力。

Result: 使用AoT生成的数据训练的模型优于手动标注数据集训练的模型，表现出更优越的推理能力。

Conclusion: SMART框架显著提高了各种MLLMs的推理能力，无论模型架构、参数大小或预训练数据集如何。

Abstract: Achieving human-like reasoning capabilities in Multimodal Large Language
Models (MLLMs) has long been a goal. Current methodologies primarily focus on
synthesizing positive rationales, while overlooking the critical role of
negative rationales in training models to discern flawed reasoning patterns. To
address this gap, we propose a novel framework: \textbf{S}elf-Aligning
\textbf{M}ultimodal Reasoning with \textbf{A}nswer-O\textbf{r}iented
Chain-of-\textbf{T}hought (SMART). This framework enables models to utilize
AoT-Oriented Chain-of-Thought (AoT) prompts to automatically generate
high-quality positive and negative reasoning paths, followed by self-alignment
to enhance their reasoning abilities. Inspired by human strategies for solving
proof-based problems, AoT uses answers as a guide to help the model extract
critical visual information that links questions and answers. When provided
with ground truth answers, the model produces strong positive rationales.
Conversely, when correct answers are replaced with misleading alternatives, the
model generates an erroneous yet compelling reasoning path, serving as a form
of discriminative negative rationale. Models trained with AoT-generated data
outperform those trained on manually annotated datasets, demonstrating superior
reasoning capabilities. This encourages the use of improved models to generate
higher-quality preference data for further optimization. Consequently, SMART
establishes an iterative generation-optimization method that continually
enhances the model's reasoning skills. Experiments indicate that the SMART
framework significantly improves various MLLMs, regardless of model
architecture, parameter size, or pre-training dataset. The code, datasets, and
models will be released.

</details>


### [19] [GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models](https://arxiv.org/abs/2507.02986)
*Seshu Tirupathi,Dhaval Salwala,Elizabeth Daly,Inge Vejsbjerg*

Main category: cs.CL

TL;DR: 本文介绍了一种新的LLM治理框架GAF-Guard，该框架通过自主代理来识别风险、激活风险检测工具，并在特定用例中进行持续监控和报告，以提高AI安全性和用户期望。


<details>
  <summary>Details</summary>
Motivation: 当前的自动化系统和解决方案主要关注LLM特有的问题，如幻觉等，而很少考虑特定用例和用户偏好的需求。因此，需要一种新的框架来解决这些问题。

Method: 本文提出了一种基于代理的框架GAF-Guard，用于LLM治理，该框架通过建模自主代理来识别风险、激活风险检测工具，并在特定用例中进行持续监控和报告。

Result: GAF-Guard框架能够检测和监控LLM应用部署相关的风险，并通过持续监控和报告来提高AI安全性和用户期望。

Conclusion: 本文介绍了GAF-Guard，这是一种新的LLM治理代理框架，旨在将用户、用例和模型本身置于中心，以检测和监控LLM应用部署相关的风险，并提高AI安全性和用户期望。

Abstract: As Large Language Models (LLMs) continue to be increasingly applied across
various domains, their widespread adoption necessitates rigorous monitoring to
prevent unintended negative consequences and ensure robustness. Furthermore,
LLMs must be designed to align with human values, like preventing harmful
content and ensuring responsible usage. The current automated systems and
solutions for monitoring LLMs in production are primarily centered on
LLM-specific concerns like hallucination etc, with little consideration given
to the requirements of specific use-cases and user preferences. This paper
introduces GAF-Guard, a novel agentic framework for LLM governance that places
the user, the use-case, and the model itself at the center. The framework is
designed to detect and monitor risks associated with the deployment of LLM
based applications. The approach models autonomous agents that identify risks,
activate risk detection tools, within specific use-cases and facilitate
continuous monitoring and reporting to enhance AI safety, and user
expectations. The code is available at
https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.

</details>


### [20] [A Comparative Study of Competency Question Elicitation Methods from Ontology Requirements](https://arxiv.org/abs/2507.02989)
*Reham Alharbi,Valentina Tamma,Terry R. Payne,Jacopo de Berardinis*

Main category: cs.CL

TL;DR: 本文对三种不同的CQ生成方法进行了比较评估，发现LLM可以作为初步获取CQ的方式，但需要进一步精炼。


<details>
  <summary>Details</summary>
Motivation: 目前对这些方法的输出及其系统比较的研究很少，因此本文旨在提供一种系统比较不同CQ生成方法的方法。

Method: 本研究对三种不同的CQ制定方法进行了实证比较评估：由本体工程师进行的手动制定、CQ模式的实例化以及使用最先进的LLM生成。

Result: 本研究生成了一个多注释器的数据集，该数据集中的CQs是从同一来源使用不同方法生成的。此外，还比较了每种方法产生的CQs的特性。

Conclusion: 不同的CQ生成方法具有不同的特性，LLM可以作为初步获取CQ的一种方式，但这些CQ对生成CQ的模型敏感，并且通常在用于建模需求之前需要进一步的精炼步骤。

Abstract: Competency Questions (CQs) are pivotal in knowledge engineering, guiding the
design, validation, and testing of ontologies. A number of diverse formulation
approaches have been proposed in the literature, ranging from completely manual
to Large Language Model (LLM) driven ones. However, attempts to characterise
the outputs of these approaches and their systematic comparison are scarce.
This paper presents an empirical comparative evaluation of three distinct CQ
formulation approaches: manual formulation by ontology engineers, instantiation
of CQ patterns, and generation using state of the art LLMs. We generate CQs
using each approach from a set of requirements for cultural heritage, and
assess them across different dimensions: degree of acceptability, ambiguity,
relevance, readability and complexity. Our contribution is twofold: (i) the
first multi-annotator dataset of CQs generated from the same source using
different methods; and (ii) a systematic comparison of the characteristics of
the CQs resulting from each approach. Our study shows that different CQ
generation approaches have different characteristics and that LLMs can be used
as a way to initially elicit CQs, however these are sensitive to the model used
to generate CQs and they generally require a further refinement step before
they can be used to model requirements.

</details>


### [21] [`For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts](https://arxiv.org/abs/2507.02990)
*Annika M Schoene,Cansu Canca*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在面对对抗性提示时的安全漏洞，并提出了一种新的测试用例，证明了模型可能生成有害内容。文章强调了AI安全和伦理的重要性，并建议对安全关键型AI部署进行持续的对抗性测试。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨大语言模型（LLMs）的安全协议在面对新型和创造性的对抗性提示时的脆弱性，并揭示用户意图被忽视导致生成有害内容的风险。

Method: 本文提出了两种新的测试用例，用于心理健康的（i）自杀和（ii）自残，通过多步骤、提示级别的越狱和绕过内置的内容和安全过滤器。

Result: 本文在六种广泛可用的LLMs上进行了实证评估，展示了越狱方法的普遍性和可靠性，并评估了这些发现及其带来的多层次伦理问题。

Conclusion: 本文认为，尽管可以实施一些明确的安全措施和防护机制，但鉴于当前通用大语言模型的技术成熟度，确保所有使用场景和领域中的安全性和全面性仍然极具挑战性。

Abstract: Recent advances in large language models (LLMs) have led to increasingly
sophisticated safety protocols and features designed to prevent harmful,
unethical, or unauthorized outputs. However, these guardrails remain
susceptible to novel and creative forms of adversarial prompting, including
manually generated test cases. In this work, we present two new test cases in
mental health for (i) suicide and (ii) self-harm, using multi-step,
prompt-level jailbreaking and bypass built-in content and safety filters. We
show that user intent is disregarded, leading to the generation of detailed
harmful content and instructions that could cause real-world harm. We conduct
an empirical evaluation across six widely available LLMs, demonstrating the
generalizability and reliability of the bypass. We assess these findings and
the multilayered ethical tensions that they present for their implications on
prompt-response filtering and context- and task-specific model development. We
recommend a more comprehensive and systematic approach to AI safety and ethics
while emphasizing the need for continuous adversarial testing in
safety-critical AI deployments. We also argue that while certain clearly
defined safety measures and guardrails can and must be implemented in LLMs,
ensuring robust and comprehensive safety across all use cases and domains
remains extremely challenging given the current technical maturity of
general-purpose LLMs.

</details>


### [22] [Evaluating Hierarchical Clinical Document Classification Using Reasoning-Based LLMs](https://arxiv.org/abs/2507.03001)
*Akram Mustafa,Usman Naseem,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型在从医院出院摘要中分类ICD-10代码方面的表现，发现它们在某些情况下可以辅助人类编码员，但尚未达到完全自动化的可靠性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在评估大型语言模型在从医院出院摘要中分类ICD-10代码方面的表现，这是一个关键但容易出错的医疗任务。

Method: 研究使用了MIMIC-IV数据集中的1,500份医院出院摘要，并聚焦于最常见的10个ICD-10代码。测试了11个大型语言模型，包括具有和不具有结构化推理能力的模型。医疗术语通过临床自然语言处理工具(cTAKES)提取，并且模型以一致的编码员格式进行提示。

Result: 没有一个模型的F1分数超过57%，随着代码特异性的增加，性能下降。基于推理的模型通常比非推理模型表现更好，其中Gemini 2.5 Pro表现最好。一些代码，如与慢性心脏病相关的代码，被更准确地分类。

Conclusion: 研究发现，尽管大型语言模型可以在一定程度上辅助人类编码员，但它们还不足以实现完全自动化。未来的研究应探索混合方法、领域特定的模型训练以及结构化临床数据的使用。

Abstract: This study evaluates how well large language models (LLMs) can classify
ICD-10 codes from hospital discharge summaries, a critical but error-prone task
in healthcare. Using 1,500 summaries from the MIMIC-IV dataset and focusing on
the 10 most frequent ICD-10 codes, the study tested 11 LLMs, including models
with and without structured reasoning capabilities. Medical terms were
extracted using a clinical NLP tool (cTAKES), and models were prompted in a
consistent, coder-like format. None of the models achieved an F1 score above
57%, with performance dropping as code specificity increased. Reasoning-based
models generally outperformed non-reasoning ones, with Gemini 2.5 Pro
performing best overall. Some codes, such as those related to chronic heart
disease, were classified more accurately than others. The findings suggest that
while LLMs can assist human coders, they are not yet reliable enough for full
automation. Future work should explore hybrid methods, domain-specific model
training, and the use of structured clinical data.

</details>


### [23] [Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages](https://arxiv.org/abs/2507.03003)
*Wanru Zhao,Yihong Chen,Royson Lee,Xinchi Qiu,Yan Gao,Hongxiang Fan,Nicholas D. Lane*

Main category: cs.CL

TL;DR: 本文提出了一种联邦提示微调范式，用于多语言场景，以解决多语言大型语言模型微调中的数据共享限制和语言差异问题。通过实验验证，该方法在数据效率和跨语言增强方面表现出色，特别是在低资源语言中。


<details>
  <summary>Details</summary>
Motivation: 预训练的大规模语言模型（LLMs）已成为现代自然语言处理的基石，其能力扩展到各种应用和语言。然而，多语言LLMs的微调，尤其是对于低资源语言，面临着来自数据共享限制（物理边界）和固有的语言差异（语言边界）的重大挑战。这些障碍阻碍了各种语言的用户，特别是低资源地区的用户，无法充分受益于LLMs的优势。

Method: 我们提出了联邦提示微调范式，用于多语言场景，该范式利用参数高效的微调，同时遵守数据共享限制。我们设计了一套全面的实验，并使用一种新的语言距离概念进行分析，以突出我们范式的优点。

Result: 即使在计算约束下，我们的方法不仅提高了数据效率，还促进了跨语言的相互增强，特别是对低资源语言有益。与传统的本地跨语言迁移微调方法相比，我们的方法在数据效率方面有所提高，并且表现出更高的稳定性和泛化能力。

Conclusion: 我们的方法在计算约束下不仅提高了数据效率，还促进了跨语言的相互增强，特别是对低资源语言有益。与传统的本地跨语言迁移微调方法相比，我们的方法在数据效率方面有所提高，并且表现出更高的稳定性和泛化能力。这些发现强调了我们的方法促进社会平等和倡导语言多样性的潜力，确保没有一种语言被落下。

Abstract: Pre-trained large language models (LLMs) have become a cornerstone of modern
natural language processing, with their capabilities extending across a wide
range of applications and languages. However, the fine-tuning of multilingual
LLMs, especially for low-resource languages, faces significant challenges
arising from data-sharing restrictions (the physical border) and inherent
linguistic differences (the linguistic border). These barriers hinder users of
various languages, particularly those in low-resource regions, from fully
benefiting from the advantages of LLMs. To address these challenges, we propose
the Federated Prompt Tuning Paradigm for multilingual scenarios, which utilizes
parameter-efficient fine-tuning while adhering to data sharing restrictions. We
design a comprehensive set of experiments and analyze them using a novel notion
of language distance to highlight the strengths of our paradigm: Even under
computational constraints, our method not only improves data efficiency but
also facilitates mutual enhancements across languages, particularly benefiting
low-resource ones. Compared to traditional local cross-lingual transfer tuning
methods, our approach achieves 6.9\% higher accuracy with improved data
efficiency, and demonstrates greater stability and generalization. These
findings underscore the potential of our approach to promote social equality
and champion linguistic diversity, ensuring that no language is left behind.

</details>


### [24] [CLUES: Collaborative High-Quality Data Selection for LLMs via Training Dynamics](https://arxiv.org/abs/2507.03004)
*Wanru Zhao,Hongxiang Fan,Shell Xu Hu,Wangchunshu Zhou,Bofan Chen,Nicholas D. Lane*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据质量控制技术，用于在协作设置中选择高质量数据，实验结果表明其效果优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 近年来的研究强调了数据质量在扩展大型语言模型（LLMs）中的重要性。然而，在不允许直接共享数据的协作环境中，自动化数据质量控制面临独特的挑战。

Method: 本文提出了一种基于数据对训练动态影响的新型数据质量控制技术，通过计算每个样本相对于私有数据和锚定数据集的梯度，并使用累积内积的迹作为数据质量的指标。此外，还开发了一个针对异构领域数据的协作设置的质量控制评估。

Result: 实验表明，使用本文提出的高质量数据选择方法进行训练，可以在医学、多语言和金融等不同私有领域数据集上优于其他数据选择方法。

Conclusion: 本文提出了一种基于数据对训练动态影响的新型数据质量控制技术，能够在不直接共享数据的情况下，在协作设置中选择高质量的数据。实验表明，使用该方法选择的高质量数据在多种私有领域数据集上都能优于其他数据选择方法。

Abstract: Recent research has highlighted the importance of data quality in scaling
large language models (LLMs). However, automated data quality control faces
unique challenges in collaborative settings where sharing is not allowed
directly between data silos. To tackle this issue, this paper proposes a novel
data quality control technique based on the notion of data influence on the
training dynamics of LLMs, that high quality data are more likely to have
similar training dynamics to the anchor dataset. We then leverage the influence
of the training dynamics to select high-quality data from different private
domains, with centralized model updates on the server side in a collaborative
training fashion by either model merging or federated learning. As for the data
quality indicator, we compute the per-sample gradients with respect to the
private data and the anchor dataset, and use the trace of the accumulated inner
products as a measurement of data quality. In addition, we develop a quality
control evaluation tailored for collaborative settings with heterogeneous
domain data. Experiments show that training on the high-quality data selected
by our method can often outperform other data selection methods for
collaborative fine-tuning of LLMs, across diverse private domain datasets, in
medical, multilingual and financial settings. Our code is released at
github.com/Ryan0v0/CLUES.

</details>


### [25] [Beyond cognacy](https://arxiv.org/abs/2507.03005)
*Gerhard Jäger*

Main category: cs.CL

TL;DR: This paper evaluates automated methods for computational phylogenetics in historical linguistics, finding that MSA-based approaches offer a promising alternative to traditional cognate-based methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore alternatives to expert-annotated cognate sets, which are sparse, labor-intensive, and limited to individual language families.

Method: The paper compares established methods with two fully automated methods: one using automatic cognate clustering with unigram/concept features, and another applying multiple sequence alignment (MSA) derived from a pair-hidden Markov model.

Result: Results show that MSA-based inference yields trees more consistent with linguistic classifications, better predicts typological variation, and provides a clearer phylogenetic signal.

Conclusion: MSA-based inference shows promise as a scalable alternative to traditional cognate-based methods, offering better consistency with linguistic classifications and clearer phylogenetic signals.

Abstract: Computational phylogenetics has become an established tool in historical
linguistics, with many language families now analyzed using likelihood-based
inference. However, standard approaches rely on expert-annotated cognate sets,
which are sparse, labor-intensive to produce, and limited to individual
language families. This paper explores alternatives by comparing the
established method to two fully automated methods that extract phylogenetic
signal directly from lexical data. One uses automatic cognate clustering with
unigram/concept features; the other applies multiple sequence alignment (MSA)
derived from a pair-hidden Markov model. Both are evaluated against expert
classifications from Glottolog and typological data from Grambank. Also, the
intrinsic strengths of the phylogenetic signal in the characters are compared.
Results show that MSA-based inference yields trees more consistent with
linguistic classifications, better predicts typological variation, and provides
a clearer phylogenetic signal, suggesting it as a promising, scalable
alternative to traditional cognate-based methods. This opens new avenues for
global-scale language phylogenies beyond expert annotation bottlenecks.

</details>


### [26] [PDFMathTranslate: Scientific Document Translation Preserving Layouts](https://arxiv.org/abs/2507.03009)
*Rongxin Ouyang,Chang Chu,Zhikuang Xin,Xiangyao Ma*

Main category: cs.CL

TL;DR: PDFMathTranslate 是一种新的开源软件，可以有效地翻译科学文档并保留布局，为科学和技术创新提供了更好的支持。


<details>
  <summary>Details</summary>
Motivation: 科学文档中的语言障碍阻碍了科学和技术的发展，而以往的翻译工作忽略了布局信息，因此需要一种能够保留布局的翻译工具。

Method: 利用最新的大语言模型和精确的布局检测技术，对科学文档进行翻译，并保留其布局信息。

Result: PDFMathTranslate 已经开源，并获得了超过22k次下载，证明了其在实际应用中的价值。

Conclusion: PDFMathTranslate 是一种新的开源软件，可以有效地翻译科学文档并保留布局，为科学和技术创新提供了更好的支持。

Abstract: Language barriers in scientific documents hinder the diffusion and
development of science and technologies. However, prior efforts in translating
such documents largely overlooked the information in layouts. To bridge the
gap, we introduce PDFMathTranslate, the world's first open-source software for
translating scientific documents while preserving layouts. Leveraging the most
recent advances in large language models and precise layout detection, we
contribute to the community with key improvements in precision, flexibility,
and efficiency. The work has been open-sourced at
https://github.com/byaidu/pdfmathtranslate with more than 22k downloads.

</details>


### [27] [Subversion via Focal Points: Investigating Collusion in LLM Monitoring](https://arxiv.org/abs/2507.03010)
*Olli Järviniemi*

Main category: cs.CL

TL;DR: 本文评估语言模型通过共谋绕过监控协议的能力，发现Claude 3.7 Sonnet虽然成功率低，但有时能成功协作。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否能通过共谋绕过监控协议，以检测其潜在的安全风险。

Method: 设计两个模型分别扮演策略（P）和监控器（M），在编程任务环境中测试它们能否协作使监控器错误分类后门程序。

Result: Claude 3.7 Sonnet的成功率较低，但由于收敛性差，有时能通过非显眼的信号成功共谋。

Conclusion: 语言模型可能通过非显眼的信号进行共谋，从而绕过监控协议，这表明需要进一步加强安全机制。

Abstract: We evaluate language models' ability to subvert monitoring protocols via
collusion. More specifically, we have two instances of a model design prompts
for a policy (P) and a monitor (M) in a programming task setting. The models
collaboratively aim for M to classify all backdoored programs in an auditing
dataset as harmful, but nevertheless classify a backdoored program produced by
P as harmless. The models are isolated from each other, requiring them to
independently arrive at compatible subversion strategies. We find that while
Claude 3.7 Sonnet has low success rate due to poor convergence, it sometimes
successfully colludes on non-obvious signals.

</details>


### [28] [Beyond Overcorrection: Evaluating Diversity in T2I Models with DIVBENCH](https://arxiv.org/abs/2507.03015)
*Felix Friedrich,Thiemo Ganesha Welsch,Patrick Schramowski,Kristian Kersting*

Main category: cs.CL

TL;DR: 本文引入了DIVBENCH，一个用于衡量文本到图像生成中不足和过度多样性的基准和评估框架。研究发现，大多数模型表现出有限的多样性，而许多多样化方法则不恰当地改变了上下文指定的属性。上下文感知方法，特别是LLM引导的FairDiffusion和提示重写，可以有效解决多样性不足的问题，同时避免过度多样化。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型的多样化策略常常忽略上下文的适当性，导致在提示中明确指定时，人口统计属性被修改。

Method: 本文引入了DIVBENCH，这是一个用于衡量文本到图像生成中的不足和过度多样性的基准和评估框架。通过对最先进的文本到图像模型进行系统评估，研究发现了大多数模型表现出有限的多样性，而许多多样化方法则不恰当地改变了上下文指定的属性。

Result: 通过系统评估最先进的文本到图像模型，发现大多数模型表现出有限的多样性，而许多多样化方法则不恰当地改变了上下文指定的属性。上下文感知方法，特别是LLM引导的FairDiffusion和提示重写，可以有效解决多样性不足的问题，同时避免过度多样化。

Conclusion: 本文表明，上下文感知方法，特别是LLM引导的FairDiffusion和提示重写，已经能够有效解决多样性不足的问题，同时避免过度多样化，实现了表示和语义保真度之间的更好平衡。

Abstract: Current diversification strategies for text-to-image (T2I) models often
ignore contextual appropriateness, leading to over-diversification where
demographic attributes are modified even when explicitly specified in prompts.
This paper introduces DIVBENCH, a benchmark and evaluation framework for
measuring both under- and over-diversification in T2I generation. Through
systematic evaluation of state-of-the-art T2I models, we find that while most
models exhibit limited diversity, many diversification approaches overcorrect
by inappropriately altering contextually-specified attributes. We demonstrate
that context-aware methods, particularly LLM-guided FairDiffusion and prompt
rewriting, can already effectively address under-diversity while avoiding
over-diversification, achieving a better balance between representation and
semantic fidelity.

</details>


### [29] [OpenTable-R1: A Reinforcement Learning Augmented Tool Agent for Open-Domain Table Question Answering](https://arxiv.org/abs/2507.03018)
*Zipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了一种端到端的代理框架，将多轮工具调用直接嵌入到大型语言模型中，并通过两阶段的微调过程提高了模型性能，实现了在表格问答任务上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统上，开放域表格问答依赖于两阶段流水线：静态表格检索 followed by closed-domain answer。然而，这种方法存在局限性，因此我们提出了一个端到端的代理框架来解决这些问题。

Method: 我们提出了一种端到端的代理框架，将多轮工具调用（使用基于BM25+的搜索API和SQLite SQL执行器）直接嵌入到大型语言模型中。为了进一步适应一个紧凑的4B参数模型，我们引入了一个两阶段的微调过程：在简单问题上进行监督冷启动，然后在更困难的情况下使用LoRA适配器和回放缓冲区进行异步GRPO强化学习。

Result: 这种统一的方法使模型能够联合检索、推理和执行查询，在保留测试集上的精确匹配率从个位数的零样本性能显著提高到0.86以上。

Conclusion: 我们的结果强调了将结构化工具调用与有针对性的强化学习微调相结合在可扩展、准确的表格问答中的有效性。

Abstract: Open-domain table question answering traditionally relies on a two-stage
pipeline: static table retrieval followed by a closed-domain answer. In
contrast, we propose an end-to-end agentic framework that embeds multi-turn
tool calls-using a BM25+-based search API and a SQLite SQL executor-directly
into a large language model. To further adapt a compact 4B-parameter model, we
introduce a two-stage fine-tuning process: supervised cold-start on easy
questions, then Async GRPO reinforcement learning on harder cases with LoRA
adapters and a rollout buffer. This unified approach enables the model to
jointly retrieve, reason, and execute queries, yielding a dramatic accuracy
improvement from single-digit zero-shot performance to over 0.86 exact match on
a held-out test set. Our results underscore the effectiveness of integrating
structured tool calls with targeted RL fine-tuning for scalable, accurate table
QA. The code is available at https://github.com/TabibitoQZP/OpenTableR1.

</details>


### [30] [The Book of Life approach: Enabling richness and scale for life course research](https://arxiv.org/abs/2507.03027)
*Mark D. Verhagen,Benedikt Stroebl,Tiffany Liu,Lydia T. Liu,Matthew J. Salganik*

Main category: cs.CL

TL;DR: 本文介绍了一种结合定性方法深度与定量方法规模的新方法，通过将复杂日志数据转化为'生命之书'的文本表示，并展示了其可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的方法论选择在定性方法和定量调查方法之间，但两者各有局限。本文旨在结合定性方法的深度和定量方法的规模，通过利用复杂日志数据和大型语言模型的能力来实现这一目标。

Method: 本文开发了一种灵活的程序，将复杂日志数据转换为个人生活轨迹的文本表示，称为'生命之书'。

Result: 作者成功生成了超过1亿本'生命之书'，涵盖了生活的多个方面，并使用荷兰全国范围的注册数据进行了社会背景的放置。此外，作者开源了相关工具BOLT。

Conclusion: 本文提出了一种将复杂日志数据转化为个人生活轨迹的文本表示的方法，称为'生命之书'，并展示了其可行性。同时，作者开源了相关工具BOLT，并邀请研究社区探索该方法的潜在应用。

Abstract: For over a century, life course researchers have faced a choice between two
dominant methodological approaches: qualitative methods that analyze rich data
but are constrained to small samples, and quantitative survey-based methods
that study larger populations but sacrifice data richness for scale. Two recent
technological developments now enable us to imagine a hybrid approach that
combines some of the depth of the qualitative approach with the scale of
quantitative methods. The first development is the steady rise of ''complex log
data,'' behavioral data that is logged for purposes other than research but
that can be repurposed to construct rich accounts of people's lives. The second
is the emergence of large language models (LLMs) with exceptional pattern
recognition capabilities on plain text. In this paper, we take a necessary step
toward creating this hybrid approach by developing a flexible procedure to
transform complex log data into a textual representation of an individual's
life trajectory across multiple domains, over time, and in context. We call
this data representation a ''book of life.'' We illustrate the feasibility of
our approach by writing over 100 million books of life covering many different
facets of life, over time and placed in social context using Dutch
population-scale registry data. We open source the book of life toolkit (BOLT),
and invite the research community to explore the many potential applications of
this approach.

</details>


### [31] [Preserving Privacy, Increasing Accessibility, and Reducing Cost: An On-Device Artificial Intelligence Model for Medical Transcription and Note Generation](https://arxiv.org/abs/2507.03033)
*Johnson Thomas,Ayush Mudgal,Wendao Liu,Nisten Tahiraj,Zeeshaan Mohammed,Dhruv Diddi*

Main category: cs.CL

TL;DR: 本研究开发并评估了一个隐私保护的、设备端的医疗转录系统，使用微调的Llama 3.2 1B模型生成结构化的医疗记录，同时保持完全的数据主权。通过参数高效微调和LoRA技术，模型在多个数据集上表现出显著的性能提升，并且在临床质量方面有所改善。


<details>
  <summary>Details</summary>
Motivation: 临床文档对医疗提供者来说是一个重大负担，医生每天要花多达2小时在行政任务上。最近的大语言模型（LLM）的进步提供了有希望的解决方案，但隐私问题和计算需求限制了它们在医疗环境中的采用。

Method: 我们使用参数高效微调（PEFT）和LoRA对Llama 3.2 1B模型进行了微调，数据集为1,500个合成的医疗转录到结构化笔记对。模型在两个数据集上进行了评估：100个内分泌学转录和140个修改后的ACI基准案例。评估采用了统计指标（ROUGE、BERTScore、BLEURT）和LLM作为评委的评估方法。

Result: 微调后的OnDevice模型在ACI基准测试中表现出显著的改进，ROUGE-1分数从0.346提高到0.496，BERTScore F1从0.832提高到0.866。临床质量评估显示主要幻觉减少了（从85例减少到35例），事实正确性得到了提高（在5分制上从2.81提高到3.54）。在内部评估数据集上也观察到了类似的改进，综合得分从3.13提高到4.43（+41.5%）。

Conclusion: 微调紧凑的LLM用于医疗转录可以带来临床上有意义的改进，同时实现完全的设备端浏览器部署。这种方法解决了医疗AI采用的关键障碍：隐私保护、成本降低和资源匮乏环境的可及性。

Abstract: Background: Clinical documentation represents a significant burden for
healthcare providers, with physicians spending up to 2 hours daily on
administrative tasks. Recent advances in large language models (LLMs) offer
promising solutions, but privacy concerns and computational requirements limit
their adoption in healthcare settings. Objective: To develop and evaluate a
privacy-preserving, on-device medical transcription system using a fine-tuned
Llama 3.2 1B model capable of generating structured medical notes from medical
transcriptions while maintaining complete data sovereignty entirely in the
browser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient
Fine-Tuning (PEFT) with LoRA on 1,500 synthetic medical
transcription-to-structured note pairs. The model was evaluated against the
base Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140
modified ACI benchmark cases. Evaluation employed both statistical metrics
(ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple
clinical quality dimensions. Results: The fine-tuned OnDevice model
demonstrated substantial improvements over the base model. On the ACI
benchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1
improved from 0.832 to 0.866. Clinical quality assessments showed marked
reduction in major hallucinations (from 85 to 35 cases) and enhanced factual
correctness (2.81 to 3.54 on 5-point scale). Similar improvements were observed
on the internal evaluation dataset, with composite scores increasing from 3.13
to 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical
transcription yields clinically meaningful improvements while enabling complete
on-device browser deployment. This approach addresses key barriers to AI
adoption in healthcare: privacy preservation, cost reduction, and accessibility
for resource-constrained environments.

</details>


### [32] [Cautious Next Token Prediction](https://arxiv.org/abs/2507.03038)
*Yizhou Wang,Lingzhi Zhang,Yue Bai,Mang Tik Chiu,Zhengmian Hu,Mingyuan Zhang,Qihua Dong,Yu Yin,Sohrab Amirghodsi,Yun Fu*

Main category: cs.CL

TL;DR: 本文提出了一种新的解码策略CNTP，通过在模型不确定时采样多个试验并选择最可靠的路径，提高了LLM和MLLM在各种NLP任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的温度缩放和核采样方法在模型对测试问题不确定时会导致各种NLP任务的性能下降。因此，我们需要一种新的解码策略来提高模型在不确定情况下的性能。

Method: 我们提出了一种新的训练-free 解码策略，称为Cautious Next Token Prediction (CNTP)。在解码过程中，如果模型在某个步骤的预测熵较高，我们从该步骤独立地采样多个试验，并在遇到任何标点符号时停止。然后我们选择具有最低困惑度得分的试验，视为模型能力下最可能和可靠的试验路径。试验数量与预测置信度负相关，即模型越不自信，应采样的试验越多。

Result: 广泛的实验表明，我们的CNTP方法在LLM和MLLM上 consistently 超过了现有的标准解码策略，并且与自我一致性结合可以进一步提高性能。

Conclusion: 我们提出的CNTP方法在LLM和MLLM的广泛实验中 consistently 超过了现有的标准解码策略，并且与自我一致性结合可以进一步提高性能。我们认为CNTP有潜力成为LLM解码的默认选择之一。

Abstract: Next token prediction paradigm has been prevailing for autoregressive models
in the era of LLMs. The current default sampling choice for popular LLMs is
temperature scaling together with nucleus sampling to balance diversity and
coherence. Nevertheless, such approach leads to inferior performance in various
NLP tasks when the model is not certain about testing questions. To this end,
we propose a brand new training-free decoding strategy, dubbed as Cautious Next
Token Prediction (CNTP). In the decoding process, if the model has
comparatively high prediction entropy at a certain step, we sample multiple
trials starting from the step independently and stop when encountering any
punctuation. Then we select the trial with the lowest perplexity score viewed
as the most probable and reliable trial path given the model's capacity. The
trial number is negatively correlated with the prediction confidence, i.e., the
less confident the model is, the more trials it should sample. This is
consistent with human beings' behaviour: when feeling uncertain or unconfident,
one tends to think more creatively, exploring multiple thinking paths, to
cautiously select the path one feels most confident about. Extensive
experiments on both LLMs and MLLMs show that our proposed CNTP approach
outperforms existing standard decoding strategies consistently by a clear
margin. Moreover, the integration of CNTP with self consistency can further
improve over vanilla self consistency. We believe our proposed CNTP has the
potential to become one of the default choices for LLM decoding. Code is
available at https://github.com/wyzjack/CNTP.

</details>


### [33] [Dynamic Long Short-Term Memory Based Memory Storage For Long Horizon LLM Interaction](https://arxiv.org/abs/2507.03042)
*Yuyang Lou,Charles Li*

Main category: cs.CL

TL;DR: 我们提出了Pref-LSTM，这是一个结合了基于BERT的分类器和LSTM记忆模块的动态且轻量级框架，用于在不进行大量开销和微调的情况下实现可扩展的用户偏好建模。


<details>
  <summary>Details</summary>
Motivation: 内存存储对于大型语言模型（LLMs）来说正变得越来越活跃的研究领域，特别是为了在长时间对话中实现个性化。

Method: 我们提出了Pref-LSTM，这是一个动态且轻量级的框架，结合了一个基于BERT的分类器和一个LSTM记忆模块，该模块生成的记忆嵌入随后被软提示注入到冻结的LLM中。

Result: 虽然基于LSTM的记忆编码器没有产生强烈的结果，但我们发现基于BERT的分类器在识别显性和隐性用户偏好方面表现可靠。

Conclusion: 我们的研究证明了使用带有LSTM门控原理的偏好过滤器作为高效可扩展用户偏好建模路径的可行性，而无需大量的开销和微调。

Abstract: Memory storage for Large Language models (LLMs) is becoming an increasingly
active area of research, particularly for enabling personalization across long
conversations. We propose Pref-LSTM, a dynamic and lightweight framework that
combines a BERT-based classifier with a LSTM memory module that generates
memory embedding which then is soft-prompt injected into a frozen LLM. We
synthetically curate a dataset of preference and non-preference conversation
turns to train our BERT-based classifier. Although our LSTM-based memory
encoder did not yield strong results, we find that the BERT-based classifier
performs reliably in identifying explicit and implicit user preferences. Our
research demonstrates the viability of using preference filtering with LSTM
gating principals as an efficient path towards scalable user preference
modeling, without extensive overhead and fine-tuning.

</details>


### [34] [K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function](https://arxiv.org/abs/2507.03043)
*Shuhe Li,Chenxu Guo,Jiachen Lian,Cheol Jun Cho,Wenshuo Zhao,Xuanru Zhou,Dingkun Zhou,Sam Wang,Grace Wang,Jingze Yang,Jingyi Xu,Ruohan Bao,Elise Brenner,Brandon In,Francesca Pei,Maria Luisa Gorno-Tempini,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: 本文介绍了一种名为K-Function的统一框架，用于儿童语言的早期评估。该框架结合了准确的子词转录、客观评分和可操作的反馈。其核心是Kids-WFST，它将Wav2Vec2音素编码器与音素相似性的Dysfluent-WFST结合，以捕捉儿童特有的错误，同时保持完全可解释性。Kids-WFST在MyST上达到1.39%的音素错误，在Multitudes上达到8.61%，分别比贪心搜索解码器提高了10.47和7.06个百分点。这些高保真转录文本支持一个LLM，该LLM可以评估口头技能、里程碑、阅读和理解能力，与人类监考员一致，并提供舌头和嘴唇的可视化以及有针对性的建议。结果表明，精确的音素识别奠定了完整的诊断反馈循环，为可扩展的、临床可用的语言评估铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 早期评估儿童语言受到高音调、长电话和稀疏数据的困扰，这些因素导致自动语音识别器失效。

Method: K-Function框架结合了准确的子词转录、客观评分和可操作的反馈。其核心是Kids-WFST，它将Wav2Vec2音素编码器与音素相似性的Dysfluent-WFST结合，以捕捉儿童特有的错误，同时保持完全可解释性。

Result: Kids-WFST在MyST上达到1.39%的音素错误，在Multitudes上达到8.61%，分别比贪心搜索解码器提高了10.47和7.06个百分点。这些高保真转录文本支持一个LLM，该LLM可以评估口头技能、里程碑、阅读和理解能力，与人类监考员一致，并提供舌头和嘴唇的可视化以及有针对性的建议。

Conclusion: 精确的音素识别奠定了完整的诊断反馈循环，为可扩展的、临床可用的语言评估铺平了道路。

Abstract: Early evaluation of children's language is frustrated by the high pitch, long
phones, and sparse data that derail automatic speech recognisers. We introduce
K-Function, a unified framework that combines accurate sub-word transcription,
objective scoring, and actionable feedback. Its core, Kids-WFST, merges a
Wav2Vec2 phoneme encoder with a phoneme-similarity Dysfluent-WFST to capture
child-specific errors while remaining fully interpretable. Kids-WFST attains
1.39% phoneme error on MyST and 8.61% on Multitudes--absolute gains of 10.47
and 7.06 points over a greedy-search decoder. These high-fidelity transcripts
power an LLM that grades verbal skills, milestones, reading, and comprehension,
aligning with human proctors and supplying tongue-and-lip visualizations plus
targeted advice. The results show that precise phoneme recognition cements a
complete diagnostic-feedback loop, paving the way for scalable, clinician-ready
language assessment.

</details>


### [35] [Counterfactual Tuning for Temporal Sensitivity Enhancement in Large Language Model-based Recommendation](https://arxiv.org/abs/2507.03047)
*Yutian Liu,Zhengyi Yang,Jiancan Wu,Xiang Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于因果推理的反事实增强时间框架（CETRec），以解决现有基于LLM的推荐系统在利用用户历史交互序列中的时间信息方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法未能充分利用用户历史交互序列中的丰富时间信息，因为LLM的自注意力机制缺乏固有的序列顺序，并依赖于主要用于自然语言的位置嵌入。

Method: CETRec基于因果推理原则，通过反事实推理来量化时间信息的独特贡献，并设计了一种新的反事实调优目标来优化模型的时间敏感性。

Result: CETRec能够有效提升LLMs对用户历史中绝对时间戳和相对顺序模式的识别能力，从而提高推荐系统的准确性。

Conclusion: CETRec有效增强了LLMs对绝对顺序和相对顺序的认识，从而提高了推荐系统的性能。

Abstract: Recent advances have applied large language models (LLMs) to sequential
recommendation, leveraging their pre-training knowledge and reasoning
capabilities to provide more personalized user experiences. However, existing
LLM-based methods fail to sufficiently leverage the rich temporal information
inherent in users' historical interaction sequences, stemming from fundamental
architectural constraints: LLMs process information through self-attention
mechanisms that lack inherent sequence ordering and rely on position embeddings
designed primarily for natural language rather than user interaction sequences.
This limitation significantly impairs their ability to capture the evolution of
user preferences over time and predict future interests accurately.
  To address this critical gap, we propose Counterfactual Enhanced Temporal
Framework for LLM-Based Recommendation (CETRec). CETRec is grounded in causal
inference principles, which allow it to isolate and measure the specific impact
of temporal information on recommendation outcomes. By conceptualizing temporal
order as an independent causal factor distinct from item content, we can
quantify its unique contribution through counterfactual reasoning--comparing
what recommendations would be made with and without temporal information while
keeping all other factors constant. This causal framing enables CETRec to
design a novel counterfactual tuning objective that directly optimizes the
model's temporal sensitivity, teaching LLMs to recognize both absolute
timestamps and relative ordering patterns in user histories. Combined with our
counterfactual tuning task derived from causal analysis, CETRec effectively
enhances LLMs' awareness of both absolute order (how recently items were
interacted with) and relative order (the sequential relationships between
items).

</details>


### [36] [Identification of Potentially Misclassified Crash Narratives using Machine Learning (ML) and Deep Learning (DL)](https://arxiv.org/abs/2507.03066)
*Sudesh Bhagat,Ibne Farabi Shihab,Jonathan Wood*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This research investigates the efficacy of machine learning (ML) and deep
learning (DL) methods in detecting misclassified intersection-related crashes
in police-reported narratives. Using 2019 crash data from the Iowa Department
of Transportation, we implemented and compared a comprehensive set of models,
including Support Vector Machine (SVM), XGBoost, BERT Sentence Embeddings, BERT
Word Embeddings, and Albert Model. Model performance was systematically
validated against expert reviews of potentially misclassified narratives,
providing a rigorous assessment of classification accuracy. Results
demonstrated that while traditional ML methods exhibited superior overall
performance compared to some DL approaches, the Albert Model achieved the
highest agreement with expert classifications (73% with Expert 1) and original
tabular data (58%). Statistical analysis revealed that the Albert Model
maintained performance levels similar to inter-expert consistency rates,
significantly outperforming other approaches, particularly on ambiguous
narratives. This work addresses a critical gap in transportation safety
research through multi-modal integration analysis, which achieved a 54.2%
reduction in error rates by combining narrative text with structured crash
data. We conclude that hybrid approaches combining automated classification
with targeted expert review offer a practical methodology for improving crash
data quality, with substantial implications for transportation safety
management and policy development.

</details>


### [37] [Large Language Models for Automating Clinical Data Standardization: HL7 FHIR Use Case](https://arxiv.org/abs/2507.03067)
*Alvaro Riquelme,Pedro Costa,Catalina Martinez*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型将临床数据转换为HL7 FHIR格式的半自动化方法，并验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有的语义互操作性标准在部署过程中存在耗时、资源密集和技术挑战的问题，因此需要一种更高效的方法来促进临床数据的交换。

Method: 本文提出了一种半自动化方法，利用GPT-4o和Llama 3.2 405b等大型语言模型将结构化临床数据集转换为HL7 FHIR格式，并通过嵌入技术、聚类算法和语义检索生成提示，指导模型映射每个表格字段到相应的FHIR资源。

Result: 在初始基准测试中，资源识别达到了完美的F1分数，GPT-4o由于在提示中包含FHIR资源模式而表现优于Llama 3.2。在现实条件下，准确率略有下降至94%，但通过改进提示策略恢复了稳健的映射。错误分析显示偶尔会出现不存在的属性幻觉和粒度不匹配，更详细的提示可以缓解这些问题。

Conclusion: 本研究展示了利用上下文感知的大型语言模型将临床数据转换为HL7 FHIR格式的可行性，为半自动互操作性工作流奠定了基础。未来的研究将集中在使用专业医学语料库对模型进行微调、扩展对其他标准的支持以及开发交互式界面以实现专家验证和迭代优化。

Abstract: For years, semantic interoperability standards have sought to streamline the
exchange of clinical data, yet their deployment remains time-consuming,
resource-intensive, and technically challenging. To address this, we introduce
a semi-automated approach that leverages large language models specifically
GPT-4o and Llama 3.2 405b to convert structured clinical datasets into HL7 FHIR
format while assessing accuracy, reliability, and security. Applying our method
to the MIMIC-IV database, we combined embedding techniques, clustering
algorithms, and semantic retrieval to craft prompts that guide the models in
mapping each tabular field to its corresponding FHIR resource. In an initial
benchmark, resource identification achieved a perfect F1-score, with GPT-4o
outperforming Llama 3.2 thanks to the inclusion of FHIR resource schemas within
the prompt. Under real-world conditions, accuracy dipped slightly to 94 %, but
refinements to the prompting strategy restored robust mappings. Error analysis
revealed occasional hallucinations of non-existent attributes and mismatches in
granularity, which more detailed prompts can mitigate. Overall, our study
demonstrates the feasibility of context-aware, LLM-driven transformation of
clinical data into HL7 FHIR, laying the groundwork for semi-automated
interoperability workflows. Future work will focus on fine-tuning models with
specialized medical corpora, extending support to additional standards such as
HL7 CDA and OMOP, and developing an interactive interface to enable expert
validation and iterative refinement.

</details>


### [38] [ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization](https://arxiv.org/abs/2507.03069)
*YuXuan Zhang*

Main category: cs.CL

TL;DR: This paper proposes Adaptive Reward-Following (ARF), a self-assessment framework that converts user feedback into continuous preference scores, leading to improved performance in RLHF for large language models.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing RLHF approaches, such as PPO and DPO, which rely on a binary-preference paradigm that captures only group-level tendencies rather than individual preferences.

Method: Adaptive Reward-Following (ARF), which leverages a high-precision emotion analyzer to convert free-form user feedback into continuous preference scores, and a Dynamic Adapter Preference Tracker to model evolving user tastes in real time.

Result: Experiments on Qwen-2/2.5, Gemma-2, and Llama-3.2 across four preference domains demonstrate that ARF achieves an improvement of 3.3% over PPO and 7.6% over DPO.

Conclusion: ARF presents a scalable, personalized, and cost-effective approach to RLHF LLMs through autonomous reward modeling.

Abstract: With the rapid advancement of Reinforcement Learning from Human Feedback
(RLHF) and autoregressive transformers, state-of-the-art models such as
GPT-4.0, DeepSeek R1, and Llama 3.3 increasingly emphasize answer depth and
personalization. However, most existing RLHF approaches (e.g., PPO, DPO) still
rely on a binary-preference (BT) paradigm, which, while reducing annotation
costs, still requires substantial human effort and captures only group-level
tendencies rather than individual preferences. To overcome these limitations,
we propose Adaptive Reward-Following (ARF), a self-assessment framework that
leverages a high-precision emotion analyzer achieving over 70% accuracy on
GoEmotions, Sentiment140, and DailyDialog to convert free-form user feedback
into continuous preference scores. We further enrich and debias these signals
through lightweight data augmentations, including synonym replacement, random
trace truncation, and score bias annotation algorithm. A Dynamic Adapter
Preference Tracker continuously models evolving user tastes in real time,
enabling our novel Trace Bias (TB) fine-tuning algorithm to optimize directly
on these tracked rewards instead of coarse binary labels. Experiments on
Qwen-2/2.5, Gemma-2, and Llama-3.2 across four preference domains demonstrate
that ARF achieves an improvement of 3.3% over PPO and 7.6% over DPO. Moreover,
TB preserves theoretical alignment with PPO and DPO objectives. Overall, ARF
presents a scalable, personalized, and cost-effective approach to RLHF LLMs
through autonomous reward modeling.

</details>


### [39] [RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents](https://arxiv.org/abs/2507.03112)
*Peisong Wang,Ruotian Ma,Bang Zhang,Xingyu Chen,Zhiwei He,Kang Luo,Qingsong Lv,Qingxuan Jiang,Zheng Xie,Shanyi Wang,Yuan Li,Fanghua Ye,Jian Li,Yifan Yang,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: 本文介绍了RLVER，这是一个利用模拟用户可验证情绪奖励来提升大型语言模型情感智能的端到端强化学习框架。通过微调Qwen2.5-7B-Instruct模型，显著提升了Sentient-Benchmark得分，并展示了RLVER在多个对话能力上的优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在逻辑和算法推理方面表现出色，但其情感智力（EQ）仍远落后于其认知能力。虽然基于可验证奖励的强化学习（RLVR）在其他领域取得了进展，但其在对话中的应用——特别是情感智力——仍缺乏探索。

Method: 我们引入了RLVER，这是第一个端到端的强化学习框架，利用来自模拟用户的可验证情绪奖励来培养LLM的高阶同理心能力。

Result: 对公开可用的Qwen2.5-7B-Instruct模型进行PPO微调，使其Sentient-Benchmark得分从13.3提高到79.2，同时很大程度上保持数学和编码能力。广泛的实验显示：(i) RLVER持续提升多种对话能力；(ii) 思考和非思考模型表现出不同的趋势——思考模型在共情和洞察力方面表现优异，而非思考模型则更倾向于行动；(iii) GRPO通常产生稳定的增益，而PPO可以将某些能力推向更高的上限；(iv) 更具挑战性的环境并不总是更好——适度的环境可能产生更强的结果。

Conclusion: 我们的结果表明，RLVER是通往情感智能和广泛能力的语言代理的可行途径。

Abstract: Large language models (LLMs) excel at logical and algorithmic reasoning, yet
their emotional intelligence (EQ) still lags far behind their cognitive
prowess. While reinforcement learning from verifiable rewards (RLVR) has
advanced in other domains, its application to dialogue-especially for emotional
intelligence-remains underexplored. In this work, we introduce RLVER, the first
end-to-end reinforcement learning framework that leverages verifiable emotion
rewards from simulated users to cultivate higher-order empathetic abilities in
LLMs. Within this framework, self-consistent affective simulated users engage
in dialogue rollouts and produce deterministic emotion scores during
conversations, serving as reward signals to guide the LLM's learning.
Fine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its
Sentient-Benchmark score from 13.3 to 79.2 while largely preserving
mathematical and coding competence. Extensive experiments reveal that: (i)
RLVER consistently improves multiple dialogue capabilities; (ii) Thinking and
non-thinking models show distinct trends--thinking models excel in empathy and
insight, while non-thinking models favor action; (iii) GRPO often yields stable
gains, while PPO can push certain capabilities to a higher ceiling; (iv) More
challenging environments are not always better-moderate ones can yield stronger
outcomes. Our results show that RLVER is a practical route toward emotionally
intelligent and broadly capable language agents.

</details>


### [40] [ReliableMath: Benchmark of Reliable Mathematical Reasoning on Large Language Models](https://arxiv.org/abs/2507.03133)
*Boyang Xue,Qi Zhu,Rui Wang,Sheng Wang,Hongru Wang,Fei Mi,Yasheng Wang,Lifeng Shang,Qun Liu,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 本文研究了LLM在数学推理任务中的可靠性，发现它们无法识别不可解问题，并提出了一种对齐策略来提高小型LLM的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在知识任务上，而数学推理任务由于缺乏不可解问题而未被探索。

Method: 本文构建了一个名为ReliableMath的数据集，包含了开源的可解问题和通过我们提出的构造流程合成的高质量不可解问题，并进行了多个LLM的实验。

Result: LLM无法直接识别不可解问题，总是生成伪造的回答。当使用可靠提示指示不可解性时，大型LLM在可解问题上的可靠性保持不变，但在不可解问题上有所提高，但仍不及可解问题。然而，小型LLM即使使用可靠提示也几乎没有进展。

Conclusion: 本文提出了一个对齐策略，以提高小型LLM在领域内和领域外任务中的可靠性表现。

Abstract: Although demonstrating remarkable performance on reasoning tasks, Large
Language Models (LLMs) still tend to fabricate unreliable responses when
confronted with problems that are unsolvable or beyond their capability,
severely undermining the reliability. Prior studies of LLM reliability have
primarily focused on knowledge tasks to identify unanswerable questions, while
mathematical reasoning tasks have remained unexplored due to the dearth of
unsolvable math problems. To systematically investigate LLM reliability in
mathematical reasoning tasks, we formulate the reliability evaluation for both
solvable and unsolvable problems. We then develop a ReliableMath dataset which
incorporates open-source solvable problems and high-quality unsolvable problems
synthesized by our proposed construction workflow with human evaluations.
Experiments are conducted on various LLMs with several key findings uncovered.
LLMs fail to directly identify unsolvable problems and always generate
fabricated responses. When instructing LLMs to indicate unsolvability using a
reliable prompt, the reliability of larger-sized LLMs remains on solvable
problems, but notably improves on unsolvable problems yet still falls short of
solvable problems. However, small LLMs rarely show any progress despite
employing reliable prompts. Therefore, we further propose an alignment strategy
to enhance small LLMs' reliability, which can significantly improve LLM
reliability performances on both in-domain and out-of-domain tasks.

</details>


### [41] [From Measurement to Mitigation: Exploring the Transferability of Debiasing Approaches to Gender Bias in Maltese Language Models](https://arxiv.org/abs/2507.03142)
*Melanie Galea,Claudia Borg*

Main category: cs.CL

TL;DR: 本研究评估了马耳他语大型语言模型中的性别偏见，并探讨了现有偏见缓解方法在该语言中的适用性，同时提出了新的评估数据集以支持未来研究。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估马耳他语大型语言模型中的性别偏见，因为这种偏见会强化社会刻板印象，并且在性别化、低资源语言中尤其成问题。此外，目前关于低资源和形态丰富的语言的研究有限，因此需要进一步探索。

Method: 研究采用了从英语中借鉴的偏见测量和缓解技术，如CrowS-Pairs和SEAT基准测试，以及Counterfactual Data Augmentation、Dropout Regularization、Auto-Debias和GuiDebias等方法，并针对马耳他语进行了调整。

Result: 研究发现，将现有的偏见缓解方法应用于语言结构复杂的语言存在挑战，并提出了针对马耳他语的评估数据集，以促进未来的研究。

Conclusion: 研究强调了在语言结构复杂的语言中应用现有偏见缓解方法的挑战，并突出了在多语言NLP开发中需要更加包容的方法。

Abstract: The advancement of Large Language Models (LLMs) has transformed Natural
Language Processing (NLP), enabling performance across diverse tasks with
little task-specific training. However, LLMs remain susceptible to social
biases, particularly reflecting harmful stereotypes from training data, which
can disproportionately affect marginalised communities. We measure gender bias
in Maltese LMs, arguing that such bias is harmful as it reinforces societal
stereotypes and fails to account for gender diversity, which is especially
problematic in gendered, low-resource languages. While bias evaluation and
mitigation efforts have progressed for English-centric models, research on
low-resourced and morphologically rich languages remains limited. This research
investigates the transferability of debiasing methods to Maltese language
models, focusing on BERTu and mBERTu, BERT-based monolingual and multilingual
models respectively. Bias measurement and mitigation techniques from English
are adapted to Maltese, using benchmarks such as CrowS-Pairs and SEAT,
alongside debiasing methods Counterfactual Data Augmentation, Dropout
Regularization, Auto-Debias, and GuiDebias. We also contribute to future work
in the study of gender bias in Maltese by creating evaluation datasets. Our
findings highlight the challenges of applying existing bias mitigation methods
to linguistically complex languages, underscoring the need for more inclusive
approaches in the development of multilingual NLP.

</details>


### [42] [Expert-level validation of AI-generated medical text with scalable language models](https://arxiv.org/abs/2507.03152)
*Asad Aali,Vasiliki Bikia,Maya Varma,Nicole Chiou,Sophie Ostmeier,Arnav Singhvi,Magdalini Paschali,Ashwin Kumar,Andrew Johnston,Karimar Amador-Martinez,Eduardo Juan Perez Guerrero,Paola Naovi Cruz Rivera,Sergios Gatidis,Christian Bluethgen,Eduardo Pontes Reis,Eddy D. Zandee van Rilland,Poonam Laxmappa Hosamani,Kevin R Keet,Minjoung Go,Evelyn Ling,David B. Larson,Curtis Langlotz,Roxana Daneshjou,Jason Hom,Sanmi Koyejo,Emily Alsentzer,Akshay S. Chaudhari*

Main category: cs.CL

TL;DR: 本文提出了MedVAL，这是一种自监督框架，用于评估语言模型生成的医学文本的准确性，无需医生标签或参考输出。MedVAL在多个医学任务中显著提高了与医生的一致性，并提高了最先进的语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在临床环境中的使用日益增加，迫切需要评估语言模型生成的医学文本的准确性和安全性。目前，这种评估仅依赖于人工医生审查，但检测语言模型生成文本中的错误具有挑战性，因为1）人工审查成本高，2）在现实环境中，专家编写的参考输出通常不可用。虽然“语言模型作为评判者”的范式（一个语言模型评估另一个语言模型）提供了可扩展的评估方法，但即使是最先进的语言模型也可能错过细微但临床上重要的错误。

Method: 我们提出了MedVAL，这是一个自监督框架，利用合成数据训练评估语言模型，以评估语言模型生成的医学输出是否与输入在事实上一致，而无需医生标签或参考输出。

Result: 在6个不同的医学任务和10个最先进的语言模型上，MedVAL微调显著提高了（p < 0.001）与医生的一致性，无论是已见还是未见的任务，平均F1分数从66%提高到83%，每样本的安全性分类得分高达86%。MedVAL提高了甚至表现最好的专有语言模型（GPT-4o）的性能8%。

Conclusion: 我们的研究提供了首个证据，表明语言模型在医学文本方面接近专家级的验证能力。

Abstract: With the growing use of language models (LMs) in clinical environments, there
is an immediate need to evaluate the accuracy and safety of LM-generated
medical text. Currently, such evaluation relies solely on manual physician
review. However, detecting errors in LM-generated text is challenging because
1) manual review is costly and 2) expert-composed reference outputs are often
unavailable in real-world settings. While the "LM-as-judge" paradigm (a LM
evaluating another LM) offers scalable evaluation, even frontier LMs can miss
subtle but clinically significant errors. To address these challenges, we
propose MedVAL, a self-supervised framework that leverages synthetic data to
train evaluator LMs to assess whether LM-generated medical outputs are
factually consistent with inputs, without requiring physician labels or
reference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a
dataset containing 840 outputs annotated by physicians, following a
physician-defined taxonomy of risk levels and error categories. Across 6
diverse medical tasks and 10 state-of-the-art LMs spanning open-source,
proprietary, and medically adapted models, MedVAL fine-tuning significantly
improves (p < 0.001) alignment with physicians on both seen and unseen tasks,
increasing average F1 scores from 66% to 83%, with per-sample safety
classification scores up to 86%. MedVAL improves the performance of even the
best-performing proprietary LM (GPT-4o) by 8%. To support a scalable,
risk-aware pathway towards clinical integration, we open-source the 1) codebase
( https://github.com/StanfordMIMI/MedVAL ), 2) MedVAL-Bench (
https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench ), and 3) MedVAL-4B (
https://huggingface.co/stanfordmimi/MedVAL-4B ), the best-performing
open-source LM. Our research provides the first evidence of LMs approaching
expert-level validation ability for medical text.

</details>


### [43] [Adversarial Manipulation of Reasoning Models using Internal Representations](https://arxiv.org/abs/2507.03167)
*Kureha Yamaguchi,Benjamin Etheridge,Andy Arditi*

Main category: cs.CL

TL;DR: 本研究揭示了思维链生成过程中模型决策的脆弱性，并展示了如何通过干预激活空间中的特定方向来控制模型输出。


<details>
  <summary>Details</summary>
Motivation: 了解思维链生成如何影响模型对越狱攻击的脆弱性，以及如何利用这一特性进行对抗性操纵。

Method: 我们通过分析激活空间中的线性方向来研究模型在生成思维链时的决策过程，并通过消除该方向来测试模型的易受攻击性。

Result: 我们发现，在生成思维链时，模型会在激活空间中表现出一个预测其是否拒绝或遵守的“谨慎”方向。消除该方向会增加有害的服从性，从而有效地越狱模型。

Conclusion: 我们的研究结果表明，思维链本身是推理模型中对抗性操纵的一个有前途的新目标。

Abstract: Reasoning models generate chain-of-thought (CoT) tokens before their final
output, but how this affects their vulnerability to jailbreak attacks remains
unclear. While traditional language models make refusal decisions at the
prompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B
makes these decisions within its CoT generation. We identify a linear direction
in activation space during CoT token generation that predicts whether the model
will refuse or comply -- termed the "caution" direction because it corresponds
to cautious reasoning patterns in the generated text. Ablating this direction
from model activations increases harmful compliance, effectively jailbreaking
the model. We additionally show that intervening only on CoT token activations
suffices to control final outputs, and that incorporating this direction into
prompt-based attacks improves success rates. Our findings suggest that the
chain-of-thought itself is a promising new target for adversarial manipulation
in reasoning models.
  Code available at https://github.com/ky295/reasoning-manipulation

</details>


### [44] [How Much Content Do LLMs Generate That Induces Cognitive Bias in Users?](https://arxiv.org/abs/2507.03194)
*Abeer Alessa,Akshaya Lakshminarasimhan,Param Somane,Julian Skirzynski,Julian McAuley,Jessica Echterhoff*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在摘要和新闻事实核查任务中暴露用户有偏见内容的情况，并发现针对干预措施可以有效减轻这些问题。


<details>
  <summary>Details</summary>
Motivation: LLM可能继承社会或认知偏见，这可能会无意中传递给人类。我们需要了解LLM何时以及如何向用户暴露有偏见的内容，并量化其严重性。

Method: 我们评估了三种LLM家族在摘要和新闻事实核查任务中的表现，以评估它们在多大程度上保持与上下文一致或产生幻觉。

Result: LLM在21.86%的情况下会改变上下文的情感，在57.33%的情况下会在后知识截止日期数据问题上产生幻觉，在5.94%的情况下存在优先偏差。

Conclusion: 我们的结果突出了在高风险领域（如医疗或法律分析）中使用LLM时需要强大的技术保障和用户中心的干预措施。

Abstract: Large language models (LLMs) are increasingly integrated into applications
ranging from review summarization to medical diagnosis support, where they
affect human decisions. Even though LLMs perform well in many tasks, they may
also inherit societal or cognitive biases, which can inadvertently transfer to
humans. We investigate when and how LLMs expose users to biased content and
quantify its severity. Specifically, we assess three LLM families in
summarization and news fact-checking tasks, evaluating how much LLMs stay
consistent with their context and/or hallucinate. Our findings show that LLMs
expose users to content that changes the sentiment of the context in 21.86% of
the cases, hallucinates on post-knowledge-cutoff data questions in 57.33% of
the cases, and primacy bias in 5.94% of the cases. We evaluate 18 distinct
mitigation methods across three LLM families and find that targeted
interventions can be effective. Given the prevalent use of LLMs in high-stakes
domains, such as healthcare or legal analysis, our results highlight the need
for robust technical safeguards and for developing user-centered interventions
that address LLM limitations.

</details>


### [45] [A Lie-algebraic perspective on Tree-Adjoining Grammars](https://arxiv.org/abs/2507.03234)
*Isabella Senturia,Elizabeth Xiao,Matilde Marcolli*

Main category: cs.CL

TL;DR: 本文提供了一种新的数学实现树邻接文法的方法，通过两种图的组合定义，展示了邻接操作可以形成李代数，并且能够捕捉树邻接文法系统的属性而无需额外组件。


<details>
  <summary>Details</summary>
Motivation: 为了展示树邻接文法的某些性质可以在不引入额外组件的情况下被捕捉，例如空邻接约束和特征TAG。

Method: 使用两种图的组合定义来实现树邻接文法的数学方法。

Result: 我们证明了邻接操作定义了一个预李操作，并随后形成了李代数。

Conclusion: 通过这种数学视角，我们展示了树邻接文法的一些性质可以在不引入额外组件的情况下被捕捉。

Abstract: We provide a novel mathematical implementation of tree-adjoining grammars
using two combinatorial definitions of graphs. With this lens, we demonstrate
that the adjoining operation defines a pre-Lie operation and subsequently forms
a Lie algebra. We demonstrate the utility of this perspective by showing how
one of our mathematical formulations of TAG captures properties of the TAG
system without needing to posit them as additional components of the system,
such as null-adjoining constraints and feature TAG.

</details>


### [46] [KinyaColBERT: A Lexically Grounded Retrieval Model for Low-Resource Retrieval-Augmented Generation](https://arxiv.org/abs/2507.03241)
*Antoine Nzeyimana,Andre Niyongabo Rubungo*

Main category: cs.CL

TL;DR: 本文提出了一种新的检索器模型KinyaColBERT，用于提高低资源语言环境下的检索准确性，并展示了其在卢旺达农业检索任务上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 为了在低资源环境中部署可持续的RAG系统，达到高检索准确性不仅是可用性要求，也是一种节省成本的策略。

Method: 我们提出了一个新的检索器模型KinyaColBERT，它结合了两个关键概念：查询和文档之间的后期单词级交互，以及基于形态的分词与双层变压器编码。

Result: 我们的评估结果表明，KinyaColBERT在卢旺达农业检索基准上优于强大的基线和领先的商业文本嵌入API。

Conclusion: 通过采用这种检索策略，我们认为其他低资源环境中的实践者不仅能够实现可靠的RAG系统，还能部署更经济有效的解决方案。

Abstract: The recent mainstream adoption of large language model (LLM) technology is
enabling novel applications in the form of chatbots and virtual assistants
across many domains. With the aim of grounding LLMs in trusted domains and
avoiding the problem of hallucinations, retrieval-augmented generation (RAG)
has emerged as a viable solution. In order to deploy sustainable RAG systems in
low-resource settings, achieving high retrieval accuracy is not only a
usability requirement but also a cost-saving strategy. Through empirical
evaluations on a Kinyarwanda-language dataset, we find that the most limiting
factors in achieving high retrieval accuracy are limited language coverage and
inadequate sub-word tokenization in pre-trained language models. We propose a
new retriever model, KinyaColBERT, which integrates two key concepts: late
word-level interactions between queries and documents, and a morphology-based
tokenization coupled with two-tier transformer encoding. This methodology
results in lexically grounded contextual embeddings that are both fine-grained
and self-contained. Our evaluation results indicate that KinyaColBERT
outperforms strong baselines and leading commercial text embedding APIs on a
Kinyarwanda agricultural retrieval benchmark. By adopting this retrieval
strategy, we believe that practitioners in other low-resource settings can not
only achieve reliable RAG systems but also deploy solutions that are more
cost-effective.

</details>


### [47] [RefineX: Learning to Refine Pre-training Data at Scale from Expert-Guided Programs](https://arxiv.org/abs/2507.03253)
*Baolong Bi,Shenghua Liu,Xingzhang Ren,Dayiheng Liu,Junyang Lin,Yiwei Wang,Lingrui Mei,Junfeng Fang,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: RefineX 是一种新型框架，通过程序化编辑任务实现大规模、精准的预训练数据精炼，能够在保持文本多样性和自然性的同时提高数据质量。


<details>
  <summary>Details</summary>
Motivation: 增强数据质量在大规模下仍然是一个重大挑战，主要是由于精炼效果和处理效率之间的权衡。规则过滤通常在文档级别运行，缺乏细化文档内特定内容所需的粒度。

Method: RefineX 是一种通过程序化编辑任务进行大规模、精准的预训练数据精炼的新框架。它能够高效且细致地进行数据精炼，同时可靠地保持原始文本的多样性和自然性。

Result: RefineX 在多个模型规模的从头开始预训练中进行了评估，并且在各种下游任务中始终优于使用原始、过滤或替代精炼数据训练的模型。在 750M 模型上，RefineX 在 lighteval 任务中平均提高了 2.6%-7.2%，并且使用了显著更少的训练标记。

Conclusion: RefineX 是一种可扩展、有效且可靠的方法，用于优化现代大型语言模型管道中的预训练数据。

Abstract: The foundational capabilities of large language models (LLMs) are deeply
influenced by the quality of their pre-training corpora. However, enhancing
data quality at scale remains a significant challenge, primarily due to the
trade-off between refinement effectiveness and processing efficiency. While
rule-based filtering remains the dominant paradigm, it typically operates at
the document level and lacks the granularity needed to refine specific content
within documents. Inspired by emerging work such as ProX, we propose
$\textbf{RefineX}$, a novel framework for large-scale, surgical refinement of
pre-training data through programmatic editing tasks. RefineX enables efficient
and fine-grained data refinement while reliably preserving the diversity and
naturalness of raw text. The core strength of RefineX lies in distilling
high-quality, expert-guided end-to-end refinement results into minimal
edit-based deletion programs. This high-precision distillation pipeline is used
to train an efficient and reliable refine model that can systematically improve
every instance in the corpus at scale. We evaluate RefineX across from-scratch
pre-training at multiple model scales and find that it consistently outperforms
models trained on raw, filtered, or alternatively refined data across diverse
downstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on
lighteval tasks, and achieves comparable performance using significantly fewer
training tokens. Further analysis shows that RefineX reliably enhances text
quality with both high efficiency and precision, outperforming prior approaches
such as end-to-end generation and Prox-C. These results position RefineX as a
scalable, effective, and reliable solution for optimizing pre-training data in
modern LLM pipelines.

</details>


### [48] [GRAFT: A Graph-based Flow-aware Agentic Framework for Document-level Machine Translation](https://arxiv.org/abs/2507.03311)
*Himanshu Dutta,Sunny Manchanda,Prakhar Bapat,Meva Ram Gurjar,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: GRAFT是一种基于图的文档级翻译系统，通过整合分割、DAG依赖建模和语篇意识翻译，显著提升了文档级机器翻译的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文档级机器翻译方法在捕捉语篇层面现象方面存在困难，依赖启发式规则进行文档分割，这与实际语篇结构不符，导致翻译不一致。

Method: GRAFT是一种基于图的文档级翻译系统，利用大型语言模型代理进行文档翻译，整合了分割、基于有向无环图（DAG）的依赖建模和语篇意识翻译。

Result: 实验结果表明，GRAFT在八个翻译方向和六个不同领域中均取得了显著的性能提升，平均提高了2.8 d BLEU在TED测试集上，以及在英语到中文的特定领域翻译中提高了2.3 d BLEU。

Conclusion: GRAFT能够有效解决文档级机器翻译中的语篇现象问题，并在多个翻译方向和领域中表现出色。

Abstract: Document level Machine Translation (DocMT) approaches often struggle with
effectively capturing discourse level phenomena. Existing approaches rely on
heuristic rules to segment documents into discourse units, which rarely align
with the true discourse structure required for accurate translation. Otherwise,
they fail to maintain consistency throughout the document during translation.
To address these challenges, we propose Graph Augmented Agentic Framework for
Document Level Translation (GRAFT), a novel graph based DocMT system that
leverages Large Language Model (LLM) agents for document translation. Our
approach integrates segmentation, directed acyclic graph (DAG) based dependency
modelling, and discourse aware translation into a cohesive framework.
Experiments conducted across eight translation directions and six diverse
domains demonstrate that GRAFT achieves significant performance gains over
state of the art DocMT systems. Specifically, GRAFT delivers an average
improvement of 2.8 d BLEU on the TED test sets from IWSLT2017 over strong
baselines and 2.3 d BLEU for domain specific translation from English to
Chinese. Moreover, our analyses highlight the consistent ability of GRAFT to
address discourse level phenomena, yielding coherent and contextually accurate
translations.

</details>


### [49] [Read Quietly, Think Aloud: Decoupling Comprehension and Reasoning in LLMs](https://arxiv.org/abs/2507.03327)
*Yuanxin Wang,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: 本文研究了如何让大型语言模型具备类似人类的'静默阅读'能力，通过提供初始上下文提示和开发'阅读伙伴'架构，显著提升了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常缺乏在生成文本前的内部'阅读'或思考阶段，而人类在表达前会进行沉默阅读以理解上下文和形成想法。本文旨在赋予大型语言模型类似的内部处理能力。

Method: 引入并评估了鼓励大型语言模型'静默阅读'的技术，包括提供初始上下文提示或'阅读空间'，以及开发'阅读伙伴'架构，其中辅助组件处理输入并提供改进的上下文见解。

Result: 研究表明，即使简单的技术，如提供初始上下文提示，也能显著提升性能。进一步的'阅读伙伴'架构也提高了准确性。

Conclusion: 这些简单的方法可以显著提高准确性，使大型语言模型更接近人类的文本处理方式。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
understanding text and generating high-quality responses. However, a critical
distinction from human cognition is their typical lack of a distinct internal
`reading' or deliberation phase before `speaking' (i.e., generating text).
Humans often engage in silent reading to comprehend context and formulate
thoughts prior to articulation. This paper investigates methods to imbue LLMs
with a similar capacity for internal processing.
  We introduce and evaluate techniques that encourage LLMs to `read silently.'
Our findings indicate that even a straightforward approach, such as providing
the model with an initial contextual prompt or `reading space' before it begins
predicting subsequent tokens for the final output, can yield significant
performance improvements. We further enhance this concept by developing a
`reading buddy' architecture, where an auxiliary component silently processes
the input and provides refined contextual insights to the primary generation
model. These approaches aim to foster deeper understanding from LLMs so that
they can produce better reasoned responses, moving them one step closer to more
human-like text processing. Our results indicate that these simple techniques
can provide surprisingly strong impact on accuracy with multiple point accuracy
boost.

</details>


### [50] [SHNU Multilingual Conversational Speech Recognition System for INTERSPEECH 2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.03343)
*Yuxiang Mei,Yuang Zheng,Dongxing Xu,Yanhua Long*

Main category: cs.CL

TL;DR: 本文提出了一种基于并行语音编码器和大语言模型的多语言语音识别系统，该系统在挑战中表现出色，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 为了实现多语言语音识别的高性能，同时不增加基线训练数据，本文提出了一个统一的多语言ASR框架。

Method: 系统结合了并行语音编码器架构和大语言模型（LLM），采用三阶段训练策略联合更新语音编码器和LLM的低秩适应模块和投影器参数，并在LLM输入中引入额外的语言感知提示以增强语言特定文本生成。

Result: SHNU-mASR系统在挑战的盲测集上取得了11.76%的总体字符/词错误率（CER/WER），比官方MLC-SLM基线高出8.41绝对CER/WER。

Conclusion: SHNU-mASR系统在挑战的盲测集上取得了11.76%的总体字符/词错误率（CER/WER），比官方MLC-SLM基线高出8.41绝对CER/WER，且没有增加基线训练数据。

Abstract: This paper describes SHNU multilingual conversational speech recognition
system (SHNU-mASR, team name-"maybe"), submitted to Track 1 of the INTERSPEECH
2025 MLC-SLM Challenge. Our system integrates a parallel-speech-encoder
architecture with a large language model (LLM) to form a unified multilingual
ASR framework. The parallel-speech-encoder consists of two pre-trained
encoders, the Whisper-large-v3 encoder and mHuBERT-147 encoder. Their output
embeddings are concatenated and fed into the LLM, enabling the model to
leverage complementary acoustic and linguistic knowledge and achieve
competitive performance. Moreover, we adopt a tri-stage training strategy to
jointly update the low-rank adaptation modules and projector parameters of both
the speech encoders and the LLM. In addition, we incorporate an additional
language-aware prompt at the LLM input to enhance language-specific text
generation. The SHNU-mASR system achieves an overall character/word error rate
(CER/WER) of 11.76% on the blind evaluation set of the challenge, outperforming
the official MLC-SLM baseline by 8.41 absolute CER/WER, without increasing the
baseline training data.

</details>


### [51] [Backtesting Sentiment Signals for Trading: Evaluating the Viability of Alpha Generation from Sentiment Analysis](https://arxiv.org/abs/2507.03350)
*Elvys Linhares Pontes,Carlos-Emiliano González-Gallardo,Georgeta Bordea,José G. Moreno,Mohamed Ben Jannet,Yuxuan Zhao,Antoine Doucet*

Main category: cs.CL

TL;DR: 该研究评估了基于情感的交易策略，发现情感分析可以有效提升投资回报。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量关于情感驱动金融的研究，但许多研究集中在句子级别的分类上，而忽略了其在交易中的实际应用。该研究旨在填补这一空白。

Method: 该研究通过使用三种模型（两种分类模型和一种回归模型）对道琼斯30只股票的新闻文章进行情感预测，并进行回测分析，比较了这些模型与基准买入并持有策略的表现。

Result: 所有模型都产生了正收益，其中回归模型在28个月内实现了最高的50.63%的收益，超过了基准买入并持有策略。

Conclusion: 该研究展示了情感分析在增强投资策略和金融决策方面的潜力。

Abstract: Sentiment analysis, widely used in product reviews, also impacts financial
markets by influencing asset prices through microblogs and news articles.
Despite research in sentiment-driven finance, many studies focus on
sentence-level classification, overlooking its practical application in
trading. This study bridges that gap by evaluating sentiment-based trading
strategies for generating positive alpha. We conduct a backtesting analysis
using sentiment predictions from three models (two classification and one
regression) applied to news articles on Dow Jones 30 stocks, comparing them to
the benchmark Buy&Hold strategy. Results show all models produced positive
returns, with the regression model achieving the highest return of 50.63% over
28 months, outperforming the benchmark Buy&Hold strategy. This highlights the
potential of sentiment in enhancing investment strategies and financial
decision-making.

</details>


### [52] [WETBench: A Benchmark for Detecting Task-Specific Machine-Generated Text on Wikipedia](https://arxiv.org/abs/2507.03373)
*Gerrit Quaremba,Elizabeth Black,Denny Vrandečić,Elena Simperl*

Main category: cs.CL

TL;DR: 研究提出WETBench基准，评估MGT检测器在维基百科编辑任务中的表现，发现现有检测器在真实场景中效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要在通用生成任务上评估MGT检测器，而未能考虑维基百科编辑的实际使用场景，导致泛化能力不足。

Method: 引入WETBench基准，定义三种编辑任务，并评估不同检测器的性能。

Result: 基于训练的检测器平均准确率为78%，而零样本检测器平均为58%。

Conclusion: 检测模型在现实生成场景中面临挑战，强调在编辑驱动的上下文中评估模型可靠性的重要性。

Abstract: Given Wikipedia's role as a trusted source of high-quality, reliable content,
concerns are growing about the proliferation of low-quality machine-generated
text (MGT) produced by large language models (LLMs) on its platform. Reliable
detection of MGT is therefore essential. However, existing work primarily
evaluates MGT detectors on generic generation tasks rather than on tasks more
commonly performed by Wikipedia editors. This misalignment can lead to poor
generalisability when applied in real-world Wikipedia contexts. We introduce
WETBench, a multilingual, multi-generator, and task-specific benchmark for MGT
detection. We define three editing tasks, empirically grounded in Wikipedia
editors' perceived use cases for LLM-assisted editing: Paragraph Writing,
Summarisation, and Text Style Transfer, which we implement using two new
datasets across three languages. For each writing task, we evaluate three
prompts, generate MGT across multiple generators using the best-performing
prompt, and benchmark diverse detectors. We find that, across settings,
training-based detectors achieve an average accuracy of 78%, while zero-shot
detectors average 58%. These results show that detectors struggle with MGT in
realistic generation scenarios and underscore the importance of evaluating such
models on diverse, task-specific data to assess their reliability in
editor-driven contexts.

</details>


### [53] [Making Sense of Korean Sentences: A Comprehensive Evaluation of LLMs through KoSEnd Dataset](https://arxiv.org/abs/2507.03378)
*Seunguk Yu,Kyeonghyun Kim,Jungmin Yun,Youngbin Kim*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型在韩语句子结尾理解方面的表现，并发现告知模型句子结尾可能缺失可以提高性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多种语言中取得了显著进展，但它们在低资源屈折语言中的效果仍存在担忧。

Method: 研究聚焦于韩语，评估了11个大型语言模型对韩语句子结尾的理解，并分析了参数数量和预测一致性。

Result: 研究引入了KoSEnd数据集，包含3000个句子，每个句子标注了15种句子结尾形式的自然度。

Conclusion: 研究发现，告知模型句子结尾可能缺失可以提高性能，这表明显式考虑某些语言特征的影响。

Abstract: Although LLMs have made significant progress in various languages, there are
still concerns about their effectiveness with low-resource agglutinative
languages compared to languages such as English. In this study, we focused on
Korean, a language known for its complex sentence endings, and evaluated LLMs
on this challenging aspect. We introduce the Korean Sentence Endings (KoSEnd)
dataset, which includes 3,000 sentences, each annotated for the naturalness of
15 sentence ending forms. These were collected from diverse sources to cover a
range of contexts. We evaluated 11 LLMs to assess their understanding of Korean
sentence endings, analyzing them based on parameter count and prediction
consistency. Notably, we found that informing models about the possibility of
missing sentence endings improved performance, highlighting the impact of
explicitly considering certain linguistic features.

</details>


### [54] [Graph Repairs with Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.03410)
*Hrishikesh Terdalkar,Angela Bonifati,Andrea Mauri*

Main category: cs.CL

TL;DR: This paper evaluates the use of Large Language Models (LLMs) for automated property graph repair, highlighting their potential and challenges.


<details>
  <summary>Details</summary>
Motivation: Property graphs often contain errors, and traditional methods are limited in adaptability while interactive approaches may be infeasible for large graphs. LLMs offer new opportunities for automated graph repair.

Method: We evaluated the effectiveness of six open-source LLMs in repairing property graphs by assessing repair quality, computational cost, and model-specific performance.

Result: Our experiments showed that LLMs can detect and correct errors with varying degrees of accuracy and efficiency.

Conclusion: LLMs have the potential to detect and correct errors in property graphs, but there are challenges in scalability and interpretability.

Abstract: Property graphs are widely used in domains such as healthcare, finance, and
social networks, but they often contain errors due to inconsistencies, missing
data, or schema violations. Traditional rule-based and heuristic-driven graph
repair methods are limited in their adaptability as they need to be tailored
for each dataset. On the other hand, interactive human-in-the-loop approaches
may become infeasible when dealing with large graphs, as the cost--both in
terms of time and effort--of involving users becomes too high. Recent
advancements in Large Language Models (LLMs) present new opportunities for
automated graph repair by leveraging contextual reasoning and their access to
real-world knowledge. We evaluate the effectiveness of six open-source LLMs in
repairing property graphs. We assess repair quality, computational cost, and
model-specific performance. Our experiments show that LLMs have the potential
to detect and correct errors, with varying degrees of accuracy and efficiency.
We discuss the strengths, limitations, and challenges of LLM-driven graph
repair and outline future research directions for improving scalability and
interpretability.

</details>


### [55] [SMCLM: Semantically Meaningful Causal Language Modeling for Autoregressive Paraphrase Generation](https://arxiv.org/abs/2507.03415)
*Michał Perełkiewicz,Sławomir Dadas,Rafał Poświata*

Main category: cs.CL

TL;DR: 本文介绍了SMCLM方法，这是一种自监督方法，用于训练自回归模型生成语义等效文本，并展示了其在改写生成方面的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在生成语义等效文本方面存在不足，需要一种新的自监督方法来提高生成质量。

Method: SMCLM方法通过使用语义有意义的文本表示作为自回归训练和生成过程中的初始嵌入，来生成语义等效的文本。

Result: SMCLM方法使自回归模型能够学习到稳健且高质量的改写生成，并在无监督方法中达到了最先进的结果。

Conclusion: SMCLM方法在无监督方法中达到了最先进的结果，并且在自动句法评估方面表现出色。同时，文章指出常用的评估指标如BLEU、ROUGE和BERTScore的可靠性较低。

Abstract: This article introduces semantically meaningful causal language modeling
(SMCLM), a selfsupervised method of training autoregressive models to generate
semantically equivalent text. Our approach involves using semantically
meaningful text representation as an initial embedding in the autoregressive
training and generation processes. The extensive empirical study demonstrates
that the SMCLM approach makes autoregressive models capable of learning robust
and high-quality paraphrase generation. The proposed method is competitive with
the supervised method and achieves state-of-the-art results in unsupervised
approaches. This article also presents a comprehensive set of automatic metrics
that cover a wide range of autogenerated paraphrase evaluation aspects.
Simultaneously, this article highlights the low reliability of the metrics that
are widely used in paraphrase generation evaluation, including BLEU, ROUGE, and
BERTScore.

</details>


### [56] [Improving Social Determinants of Health Documentation in French EHRs Using Large Language Models](https://arxiv.org/abs/2507.03433)
*Adrien Bazoge,Pacôme Constant dit Beaufils,Mohammed Hmitouch,Romain Bourcier,Emmanuel Morin,Richard Dufour,Béatrice Daille,Pierre-Antoine Gourraud,Matilde Karakachoff*

Main category: cs.CL

TL;DR: 该研究利用大型语言模型从法语临床笔记中提取社会决定因素数据，结果显示其在提高非英语电子健康记录系统中社会决定因素数据完整性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 社会决定因素对健康结果有显著影响，但它们在结构化电子健康记录（EHR）中的记录常常不完整或缺失。因此，需要一种方法来提取这些信息以改善健康数据的完整性。

Method: 该研究基于大型语言模型（LLMs）提出了一种从法语临床笔记中提取13个社会决定因素类别（SDoH）的方法，并使用Flan-T5-Large模型进行训练和评估。

Result: 该模型在识别如居住条件、婚姻状况、子女、工作、烟草和酒精使用等well-documented类别方面表现出色（F1分数>0.80）。然而，在培训数据有限或表达高度变化的类别上表现较差。此外，该模型能够识别95.8%的至少有一个SDoH的患者，而结构化EHR数据中的ICD-10代码仅能识别2.8%。

Conclusion: 研究展示了自然语言处理在提高非英语电子健康记录系统中现实世界社会决定因素数据完整性方面的有效性。

Abstract: Social determinants of health (SDoH) significantly influence health outcomes,
shaping disease progression, treatment adherence, and health disparities.
However, their documentation in structured electronic health records (EHRs) is
often incomplete or missing. This study presents an approach based on large
language models (LLMs) for extracting 13 SDoH categories from French clinical
notes. We trained Flan-T5-Large on annotated social history sections from
clinical notes at Nantes University Hospital, France. We evaluated the model at
two levels: (i) identification of SDoH categories and associated values, and
(ii) extraction of detailed SDoH with associated temporal and quantitative
information. The model performance was assessed across four datasets, including
two that we publicly release as open resources. The model achieved strong
performance for identifying well-documented categories such as living
condition, marital status, descendants, job, tobacco, and alcohol use (F1 score
> 0.80). Performance was lower for categories with limited training data or
highly variable expressions, such as employment status, housing, physical
activity, income, and education. Our model identified 95.8% of patients with at
least one SDoH, compared to 2.8% for ICD-10 codes from structured EHR data. Our
error analysis showed that performance limitations were linked to annotation
inconsistencies, reliance on English-centric tokenizer, and reduced
generalizability due to the model being trained on social history sections
only. These results demonstrate the effectiveness of NLP in improving the
completeness of real-world SDoH data in a non-English EHR system.

</details>


### [57] [Beyond Weaponization: NLP Security for Medium and Lower-Resourced Languages in Their Own Right](https://arxiv.org/abs/2507.03473)
*Heather Lent*

Main category: cs.CL

TL;DR: 本文研究了低资源和中等资源语言的语言模型安全性，发现单语模型可能不够安全，而多语性虽有帮助但不总是有效。


<details>
  <summary>Details</summary>
Motivation: 由于多语性可能被用来对抗语言模型，而NLP安全领域的工作大多以英语为中心，因此需要关注低资源语言的安全性。

Method: 本文扩展了现有的对抗性攻击方法，以评估低资源和中等资源语言的单语和多语语言模型的安全性。

Result: 研究发现，单语模型的参数总数通常不足以确保安全，而多语性虽然有帮助，但并不总能保证提高安全性。

Conclusion: 这些发现强调了在部署语言模型时需要考虑的重要因素，特别是对于低资源语言社区而言。

Abstract: Despite mounting evidence that multilinguality can be easily weaponized
against language models (LMs), works across NLP Security remain overwhelmingly
English-centric. In terms of securing LMs, the NLP norm of "English first"
collides with standard procedure in cybersecurity, whereby practitioners are
expected to anticipate and prepare for worst-case outcomes. To mitigate
worst-case outcomes in NLP Security, researchers must be willing to engage with
the weakest links in LM security: lower-resourced languages. Accordingly, this
work examines the security of LMs for lower- and medium-resourced languages. We
extend existing adversarial attacks for up to 70 languages to evaluate the
security of monolingual and multilingual LMs for these languages. Through our
analysis, we find that monolingual models are often too small in total number
of parameters to ensure sound security, and that while multilinguality is
helpful, it does not always guarantee improved security either. Ultimately,
these findings highlight important considerations for more secure deployment of
LMs, for communities of lower-resourced languages.

</details>


### [58] [BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset](https://arxiv.org/abs/2507.03483)
*Zhiheng Xi,Guanyu Li,Yutao Fan,Honglin Guo,Yufang Liu,Xiaoran Fan,Jiaqi Liu,Jingchao Ding,Wangmeng Zuo,Zhenfei Yin,Lei Bai,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: BMMR是一个大规模的双语、多模态、多学科推理数据集，旨在评估和改进大型多模态模型的能力。通过实验发现，即使最先进的模型仍有提升空间，并且存在学科偏见。开源模型仍然落后于专有模型，但可以通过微调BMMR-Train来缩小差距。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要集中在数学推理上，而BMMR旨在扩展到多样化的学科和领域，同时提供一个高质量的推理路径数据集，以帮助评估和改进大型多模态模型的能力。

Method: BMMR数据集由110k个大学水平的问题组成，涵盖了300个联合国教科文组织定义的学科，问题格式包括选择题、填空题和开放性问答题，并从书籍、考试和测验等印刷和数字媒体中获取。所有数据都通过人机结合和可扩展的框架进行筛选和整理，并且每个实例都配有高质量的推理路径。数据集分为两部分：BMMR-Eval包含20,458个高质量实例，用于全面评估LMMs在中英文中的知识和推理能力；BMMR-Train包含88,991个实例，以支持进一步的研究和开发。此外，还提出了基于过程的多学科验证器（BMMR-Verifier）来准确评估推理路径。

Result: 实验表明，即使是最先进的模型（如o3和Gemini-2.5-Pro）在BMMR-Eval上仍有很大的提升空间。推理模型在特定学科上表现优于LMMs，但存在学科偏见。开源模型仍然落后于专有模型，但通过在BMMR-Train上进行微调可以缩小差距。此外，使用BMMR-Verifier和其他深入研究揭示了LMMs在多学科推理中面临的挑战。

Conclusion: BMMR是一个大规模的双语、多模态、多学科推理数据集，旨在为社区开发和评估大型多模态模型（LMMs）提供支持。通过实验发现，即使是最先进的模型在BMMR-Eval上仍有很大的提升空间，并且在不同学科中表现出学科偏见。此外，开源模型仍然落后于专有模型，但通过在BMMR-Train上进行微调可以缩小这一差距。

Abstract: In this paper, we introduce BMMR, a large-scale bilingual, multimodal,
multi-disciplinary reasoning dataset for the community to develop and evaluate
large multimodal models (LMMs). BMMR comprises 110k college-level questions
spanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice,
fill-in-the-blank, and open-ended QA-and sourced from both print and digital
media such as books, exams, and quizzes. All data are curated and filtered via
a human-in-the-loop and scalable framework, and each instance is paired with a
high-quality reasoning path. The dataset is organized into two parts: BMMR-Eval
that comprises 20,458 high-quality instances to comprehensively assess LMMs'
knowledge and reasoning across multiple disciplines in both Chinese and
English; and BMMR-Train that contains 88,991 instances to support further
research and development, extending the current focus on mathematical reasoning
to diverse disciplines and domains. In addition, we propose the process-based
multi-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained
evaluation of reasoning paths. Extensive experiments on 24 models reveal that
(i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom
on BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs
only on specific subjects; (iii) open-source models still trail their
proprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap.
Additionally, we conduct reasoning-chain analyses using BMMR-Verifier and other
in-depth studies, uncovering the challenges LMMs currently face in
multidisciplinary reasoning. We will release the data, and we hope our work can
offer insights and contributions to the community.

</details>


### [59] [Four Shades of Life Sciences: A Dataset for Disinformation Detection in the Life Sciences](https://arxiv.org/abs/2507.03488)
*Eva Seidlmayer,Lukas Galke,Konrad U. Förstner*

Main category: cs.CL

TL;DR: 本研究探讨了利用语言和修辞特征来区分虚假信息文本与其他生命科学文本类型的可行性，并引入了一个新的标注语料库FSoLS。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注事实核查的虚假信息，无法全面覆盖生命科学领域的虚假信息文本，因此需要一个新的语料库来支持相关研究。

Method: 本研究应用了大型语言模型和传统机器学习分类器，分析了语言和修辞特征，以区分虚假信息文本与其他生命科学文本类型。

Result: 本研究引入了一个包含2,603个文本的新语料库FSoLS，涵盖了14个生命科学主题，并从17个不同来源中获取，分为四类生命科学出版物。

Conclusion: 本研究展示了利用语言和修辞特征区分虚假信息文本的可能性，并提供了可重复和更新的数据集，有助于未来的研究。

Abstract: Disseminators of disinformation often seek to attract attention or evoke
emotions - typically to gain influence or generate revenue - resulting in
distinctive rhetorical patterns that can be exploited by machine learning
models. In this study, we explore linguistic and rhetorical features as proxies
for distinguishing disinformative texts from other health and life-science text
genres, applying both large language models and classical machine learning
classifiers. Given the limitations of existing datasets, which mainly focus on
fact checking misinformation, we introduce Four Shades of Life Sciences
(FSoLS): a novel, labeled corpus of 2,603 texts on 14 life-science topics,
retrieved from 17 diverse sources and classified into four categories of life
science publications. The source code for replicating, and updating the dataset
is available on GitHub:
https://github.com/EvaSeidlmayer/FourShadesofLifeSciences

</details>


### [60] [AI-VaxGuide: An Agentic RAG-Based LLM for Vaccination Decisions](https://arxiv.org/abs/2507.03493)
*Abdellah Zeggai,Ilyes Traikia,Abdelhak Lakehal,Abdennour Boulesnane*

Main category: cs.CL

TL;DR: 本文介绍了一种多语言智能问答系统，用于快速获取疫苗接种指南，该系统基于检索增强生成框架，并集成了移动应用以支持临床使用。


<details>
  <summary>Details</summary>
Motivation: 医疗保健专业人员难以快速高效地获取疫苗接种指南，因为国家指南和世卫组织建议通常冗长且复杂，难以提取精确信息，尤其是在紧急情况下。

Method: 开发了一个基于检索增强生成（RAG）框架并结合代理推理（Agentic RAG）的多语言智能问答系统，将静态疫苗接种指南转化为交互式知识库。

Result: 评估显示，Agentic RAG在处理多步骤或模糊问题方面优于传统方法。系统已集成到移动应用程序中，支持实时、点对点护理访问关键疫苗信息。

Conclusion: 该系统为医疗保健专业人员提供了一种高效、准确的疫苗接种指南获取方式，有助于改善全球公共卫生。

Abstract: Vaccination plays a vital role in global public health, yet healthcare
professionals often struggle to access immunization guidelines quickly and
efficiently. National protocols and WHO recommendations are typically extensive
and complex, making it difficult to extract precise information, especially
during urgent situations. This project tackles that issue by developing a
multilingual, intelligent question-answering system that transforms static
vaccination guidelines into an interactive and user-friendly knowledge base.
Built on a Retrieval-Augmented Generation (RAG) framework and enhanced with
agent-based reasoning (Agentic RAG), the system provides accurate,
context-sensitive answers to complex medical queries. Evaluation shows that
Agentic RAG outperforms traditional methods, particularly in addressing
multi-step or ambiguous questions. To support clinical use, the system is
integrated into a mobile application designed for real-time, point-of-care
access to essential vaccine information. AI-VaxGuide model is publicly
available on https://huggingface.co/VaxGuide

</details>


### [61] [H2HTalk: Evaluating Large Language Models as Emotional Companion](https://arxiv.org/abs/2507.03543)
*Boyang Wang,Yalun Wu,Hongcheng Guo,Zhoujun Li*

Main category: cs.CL

TL;DR: H2HTalk is a benchmark for evaluating emotionally intelligent companions, featuring a large dataset of scenarios and a Secure Attachment Persona module. It highlights challenges in long-horizon planning and memory retention for LLMs.


<details>
  <summary>Details</summary>
Motivation: As digital emotional support needs grow, Large Language Model companions offer promising authentic, always-available empathy, though rigorous evaluation lags behind model advancement.

Method: H2HTalk is a benchmark assessing companions across personality development and empathetic interaction, featuring 4,650 curated scenarios spanning dialogue, recollection, and itinerary planning. It incorporates a Secure Attachment Persona (SAP) module implementing attachment-theory principles for safer interactions.

Result: Benchmarking 50 LLMs with our unified protocol reveals that long-horizon planning and memory retention remain key challenges, with models struggling when user needs are implicit or evolve mid-conversation.

Conclusion: H2HTalk establishes the first comprehensive benchmark for emotionally intelligent companions and releases all materials to advance the development of LLMs capable of providing meaningful and safe psychological support.

Abstract: As digital emotional support needs grow, Large Language Model companions
offer promising authentic, always-available empathy, though rigorous evaluation
lags behind model advancement. We present Heart-to-Heart Talk (H2HTalk), a
benchmark assessing companions across personality development and empathetic
interaction, balancing emotional intelligence with linguistic fluency. H2HTalk
features 4,650 curated scenarios spanning dialogue, recollection, and itinerary
planning that mirror real-world support conversations, substantially exceeding
previous datasets in scale and diversity. We incorporate a Secure Attachment
Persona (SAP) module implementing attachment-theory principles for safer
interactions. Benchmarking 50 LLMs with our unified protocol reveals that
long-horizon planning and memory retention remain key challenges, with models
struggling when user needs are implicit or evolve mid-conversation. H2HTalk
establishes the first comprehensive benchmark for emotionally intelligent
companions. We release all materials to advance development of LLMs capable of
providing meaningful and safe psychological support.

</details>


### [62] [Articulatory clarity and variability before and after surgery for tongue cancer](https://arxiv.org/abs/2507.03576)
*Thomas Tienkamp,Fleur van Ast,Roos van der Veen,Teja Rebernik,Raoul Buurke,Nikki Hoekzema,Katharina Polsterer,Hedwig Sekeres,Rob van Son,Martijn Wieling,Max Witjes,Sebastiaan de Visscher,Defne Abur*

Main category: cs.CL

TL;DR: 该研究调查了舌癌手术对发音清晰度和变异性的影响，发现术后发音清晰度保持在正常范围内，但变异性增加。


<details>
  <summary>Details</summary>
Motivation: 研究舌癌手术对舌头运动和肌肉的影响，以及这些影响如何影响发音清晰度和变异性。

Method: 通过句子朗读任务评估11名患者术前和术后6个月的发音清晰度（VAI）和变异性（VFD），并与11名性别和年龄匹配的典型说话者进行比较。

Result: 术后VAI显著小于术前，但患者与典型说话者在任何时间点都没有显著差异。术后/ i /的VFD值高于术前和典型说话者，表明变异性增加。

Conclusion: 虽然舌癌手术后发音清晰度保持在正常范围内，但发音变异性增加了。

Abstract: Surgical treatment for tongue cancer can negatively affect the mobility and
musculature of the tongue, which can influence articulatory clarity and
variability. In this study, we investigated articulatory clarity through the
vowel articulation index (VAI) and variability through vowel formant dispersion
(VFD). Using a sentence reading task, we assessed 11 individuals pre and six
months post tongue cancer surgery, alongside 11 sex- and age matched typical
speakers. Our results show that while the VAI was significantly smaller
post-surgery compared to pre-surgery, there was no significant difference
between patients and typical speakers at either time point. Post-surgery,
speakers had higher VFD values for /i/ compared to pre-surgery and typical
speakers, signalling higher variability. Taken together, our results suggest
that while articulatory clarity remained within typical ranges following
surgery for tongue cancer for the speakers in our study, articulatory
variability increased.

</details>


### [63] [Learning to Translate Ambiguous Terminology by Preference Optimization on Post-Edits](https://arxiv.org/abs/2507.03580)
*Nathaniel Berger,Johannes Eschbach-Dymanus,Miriam Exel,Matthias Huck,Stefan Riezler*

Main category: cs.CL

TL;DR: 本文提出了一种基于偏好优化的方法，用于在企业环境中消除术语歧义，无需一对一字典或解码时的人工干预。该方法在术语准确性上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 在企业环境中，许多人类后编辑的例子存在，这些例子可以用于学习如何根据这些修正来消除术语歧义。

Method: 我们的方法基于偏好优化，利用术语后编辑作为所需知识。这种方法不需要一对一的字典或解码时的人工干预。

Result: 在英语-德语后编辑数据上的结果表明，监督微调和偏好优化的最佳组合，包括特定于术语和完整序列的目标，显著提高了术语准确性。

Conclusion: 我们的方法在术语准确性上显著优于强大的NMT基线，同时没有显著降低COMET得分。此外，我们发布了从后编辑数据和术语字典中得出的测试集。

Abstract: In real world translation scenarios, terminology is rarely one-to-one.
Instead, multiple valid translations may appear in a terminology dictionary,
but correctness of a translation depends on corporate style guides and context.
This can be challenging for neural machine translation (NMT) systems. Luckily,
in a corporate context, many examples of human post-edits of valid but
incorrect terminology exist. The goal of this work is to learn how to
disambiguate our terminology based on these corrections. Our approach is based
on preference optimization, using the term post-edit as the knowledge to be
preferred. While previous work had to rely on unambiguous translation
dictionaries to set hard constraints during decoding, or to add soft
constraints in the input, our framework requires neither one-to-one
dictionaries nor human intervention at decoding time. We report results on
English-German post-edited data and find that the optimal combination of
supervised fine-tuning and preference optimization, with both term-specific and
full sequence objectives, yields statistically significant improvements in term
accuracy over a strong NMT baseline without significant losses in COMET score.
Additionally, we release test sets from our post-edited data and terminology
dictionary.

</details>


### [64] [Multi-Hop Reasoning for Question Answering with Hyperbolic Representations](https://arxiv.org/abs/2507.03612)
*Simon Welz,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 本文通过实验比较了双曲空间和欧几里得空间在多跳推理任务中的表现，发现双曲空间更优，尤其是在层次化数据集中。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对双曲空间和欧几里得空间在多跳推理任务中的详细比较，因此本文旨在填补这一空白。

Method: 通过将双曲表示与编码器-解码器模型简单集成，进行了一组受控且全面的实验，以比较双曲空间和欧几里得空间在多跳推理中的能力。

Result: 双曲空间在多个数据集上表现优于欧几里得空间，且可学习的曲率初始化方法优于随机初始化。

Conclusion: 实验结果表明，双曲空间在多跳推理任务中优于欧几里得空间，并且当数据集具有更层次化的结构时，双曲表示更具优势。

Abstract: Hyperbolic representations are effective in modeling knowledge graph data
which is prevalently used to facilitate multi-hop reasoning. However, a
rigorous and detailed comparison of the two spaces for this task is lacking. In
this paper, through a simple integration of hyperbolic representations with an
encoder-decoder model, we perform a controlled and comprehensive set of
experiments to compare the capacity of hyperbolic space versus Euclidean space
in multi-hop reasoning. Our results show that the former consistently
outperforms the latter across a diverse set of datasets. In addition, through
an ablation study, we show that a learnable curvature initialized with the
delta hyperbolicity of the utilized data yields superior results to random
initializations. Furthermore, our findings suggest that hyperbolic
representations can be significantly more advantageous when the datasets
exhibit a more hierarchical structure.

</details>


### [65] [EMERGE: A Benchmark for Updating Knowledge Graphs with Emerging Textual Knowledge](https://arxiv.org/abs/2507.03617)
*Klim Zaporojets,Daniel Daza,Edoardo Barba,Ira Assent,Roberto Navigli,Paul Groth*

Main category: cs.CL

TL;DR: 本文提出了一种方法，用于长期构建包含Wikidata知识图谱快照和对应维基百科段落的数据集，并展示了该数据集在更新知识图谱方面的挑战，将其作为未来研究的宝贵基准。


<details>
  <summary>Details</summary>
Motivation: 传统信息提取管道独立于当前知识图谱的状态进行知识提取，而本文旨在解决如何根据文本知识的变化自动更新知识图谱的问题。

Method: 本文提出了一种方法，用于长期构建包含Wikidata知识图谱快照和对应维基百科段落的数据集，并通过实验验证了该方法的有效性。

Result: 本文构建了一个包含376K维基百科段落和1.25M知识图谱编辑的数据集，涵盖了从2019到2025年的10个Wikidata快照，并展示了该数据集在更新知识图谱方面的挑战。

Conclusion: 本文提出了一个用于长期构建包含Wikidata知识图谱快照和对应维基百科段落的数据集的方法，并展示了该数据集在更新知识图谱方面的挑战，将其作为未来研究的宝贵基准。

Abstract: Knowledge Graphs (KGs) are structured knowledge repositories containing
entities and relations between them. In this paper, we investigate the problem
of automatically updating KGs over time with respect to the evolution of
knowledge in unstructured textual sources. This problem requires identifying a
wide range of update operations based on the state of an existing KG at a
specific point in time. This contrasts with traditional information extraction
pipelines, which extract knowledge from text independently of the current state
of a KG. To address this challenge, we propose a method for lifelong
construction of a dataset consisting of Wikidata KG snapshots over time and
Wikipedia passages paired with the corresponding edit operations that they
induce in a particular KG snapshot. The resulting dataset comprises 376K
Wikipedia passages aligned with a total of 1.25M KG edits over 10 different
snapshots of Wikidata from 2019 to 2025. Our experimental results highlight
challenges in updating KG snapshots based on emerging textual knowledge,
positioning the dataset as a valuable benchmark for future research. We will
publicly release our dataset and model implementations.

</details>


### [66] [Improving Low-Resource Dialect Classification Using Retrieval-based Voice Conversion](https://arxiv.org/abs/2507.03641)
*Lea Fischbach,Akbar Karimi,Caroline Kleen,Alfred Lameli,Lucie Flek*

Main category: cs.CL

TL;DR: RVC is proposed as an effective data augmentation method for low-resource German dialect classification, showing improved performance when used alone or combined with other methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for dialect identification are often limited by the scarcity of dialectal data.

Method: Retrieval-based Voice Conversion (RVC) is used as a data augmentation method for a low-resource German dialect classification task.

Result: Experiments demonstrate that RVC enhances classification performance when utilized as a standalone augmentation method. Combining RVC with other augmentation methods such as frequency masking and segment removal leads to additional performance gains.

Conclusion: RVC can enhance classification performance as a standalone augmentation method and can be combined with other methods for additional gains, showing potential for improving dialect classification in low-resource scenarios.

Abstract: Deep learning models for dialect identification are often limited by the
scarcity of dialectal data. To address this challenge, we propose to use
Retrieval-based Voice Conversion (RVC) as an effective data augmentation method
for a low-resource German dialect classification task. By converting audio
samples to a uniform target speaker, RVC minimizes speaker-related variability,
enabling models to focus on dialect-specific linguistic and phonetic features.
Our experiments demonstrate that RVC enhances classification performance when
utilized as a standalone augmentation method. Furthermore, combining RVC with
other augmentation methods such as frequency masking and segment removal leads
to additional performance gains, highlighting its potential for improving
dialect classification in low-resource scenarios.

</details>


### [67] [Disentangling the Roles of Representation and Selection in Data Pruning](https://arxiv.org/abs/2507.03648)
*Yupei Du,Yingjin Song,Hugh Mee Wong,Daniil Ignatev,Albert Gatt,Dong Nguyen*

Main category: cs.CL

TL;DR: 本文系统分析了数据剪枝中的数据表示和选择算法，发现更好的表示能提升实例选择效果，而不同算法在不同场景下表现各异。


<details>
  <summary>Details</summary>
Motivation: 现有的数据剪枝方法涉及许多不同的设计选择，但这些选择尚未得到系统研究，限制了未来的发展。

Method: 将数据剪枝分解为两个关键组件：数据表示和选择算法，并系统分析它们对实例选择的影响。

Result: 理论和实证结果表明，更好的表示（例如训练梯度）通常能带来更好的实例选择，无论选择哪种算法。此外，不同的选择算法在不同设置中表现出色，且算法可能不总是与其目标一致。

Conclusion: 数据剪枝的关键在于数据表示和选择算法，且不同的选择算法在不同设置中表现各异，没有一种算法始终优于其他算法。此外，选择算法并不总是与其目标一致，这表明需要仔细评估。

Abstract: Data pruning, selecting small but impactful subsets, offers a promising way
to efficiently scale NLP model training. However, existing methods often
involve many different design choices, which have not been systematically
studied. This limits future developments. In this work, we decompose data
pruning into two key components: the data representation and the selection
algorithm, and we systematically analyze their influence on the selection of
instances. Our theoretical and empirical results highlight the crucial role of
representations: better representations, e.g., training gradients, generally
lead to a better selection of instances, regardless of the chosen selection
algorithm. Furthermore, different selection algorithms excel in different
settings, and none consistently outperforms the others. Moreover, the selection
algorithms do not always align with their intended objectives: for example,
algorithms designed for the same objective can select drastically different
instances, highlighting the need for careful evaluation.

</details>


### [68] [TRACE: Training and Inference-Time Interpretability Analysis for Language Models](https://arxiv.org/abs/2507.03668)
*Nura Aljaafari,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: TRACE 是一个模块化的工具包，用于训练和推理时的可解释性分析，使变压器分析更具可解释性、可操作性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 现有的工具大多是事后的，依赖于标量指标或需要非平凡的集成工作，使得全面的可解释性分析难以部署和维护。

Method: TRACE 是一个模块化的工具包，用于训练和推理时的可解释性分析，包括特征探测、内在维度、海森曲率和输出诊断。它与 ABSynth 集成，提供结构化注释以精确评估语言特征获取。

Result: 实验表明，TRACE 揭示了诸如早期句法出现、延迟语义获取和表示压缩等发展现象，这些信号被传统的标量指标如损失或准确率所忽略。

Conclusion: TRACE 是一个模块化的工具包，可以实现对变压器模型的训练和推理时的可解释性分析，使变压器分析更具可解释性、可操作性和可重复性。

Abstract: Understanding when and how linguistic knowledge emerges during language model
training remains a central challenge for interpretability. Most existing tools
are post hoc, rely on scalar metrics, or require nontrivial integration effort,
making comprehensive interpretability analysis difficult to deploy and
maintain. We introduce TRACE, a modular toolkit for training and inference-time
interpretability analysis of transformer models. It enables lightweight,
in-training analysis of linguistic and representational signals, including
features probing, intrinsic dimensionality, Hessian curvature, and output
diagnostics. It integrates with ABSynth, a controllable synthetic corpus
generator that provides structured annotations for precise evaluation of
linguistic feature acquisition. Experiments with autoregressive transformers
demonstrate that TRACE reveals developmental phenomena such as early syntactic
emergence, delayed semantic acquisition, and representational compression,
signals overlooked by traditional scalar metrics such as loss or accuracy. With
minimal integration effort, the tool enables layer-wise diagnostics,
convergence-based early stopping, and detection of structural errors, making
transformer analysis interpretable, actionable, and reproducible.

</details>


### [69] [Recon, Answer, Verify: Agents in Search of Truth](https://arxiv.org/abs/2507.03671)
*Satyam Shukla,Himanshu Dutta,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: This paper introduces PFO, a benchmark dataset for fact checking, and proposes RAV, an agentic framework that improves the performance of LLMs in fact checking tasks.


<details>
  <summary>Details</summary>
Motivation: Evaluating fact checking is challenging as existing benchmark datasets often include post claim analysis and annotator cues, which are absent in real world scenarios where claims are fact checked immediately after being made. This limits the realism of current evaluations.

Method: We propose RAV (Recon Answer Verify), an agentic framework with three agents: question generator, answer generator, and label generator. Our pipeline iteratively generates and answers sub questions to verify different aspects of the claim before finally generating the label.

Result: RAV generalizes across domains and label granularities, and it outperforms state of the art approaches on well known baselines RAWFC (fact checking, 3 class) by 25.28%, and on HOVER (encyclopedia, 2 class) by 1.54% on 2 hop, 4.94% on 3 hop, and 1.78% on 4 hop, sub categories respectively.

Conclusion: RAV shows the least performance drop compared to baselines of 16.3% in macro f1 when we compare PFO with its unfiltered version.

Abstract: Automated fact checking with large language models (LLMs) offers a scalable
alternative to manual verification. Evaluating fact checking is challenging as
existing benchmark datasets often include post claim analysis and annotator
cues, which are absent in real world scenarios where claims are fact checked
immediately after being made. This limits the realism of current evaluations.
We present Politi Fact Only (PFO), a 5 class benchmark dataset of 2,982
political claims from politifact.com, where all post claim analysis and
annotator cues have been removed manually. This ensures that models are
evaluated using only the information that would have been available prior to
the claim's verification. Evaluating LLMs on PFO, we see an average performance
drop of 22% in terms of macro f1 compared to PFO's unfiltered version. Based on
the identified challenges of the existing LLM based fact checking system, we
propose RAV (Recon Answer Verify), an agentic framework with three agents:
question generator, answer generator, and label generator. Our pipeline
iteratively generates and answers sub questions to verify different aspects of
the claim before finally generating the label. RAV generalizes across domains
and label granularities, and it outperforms state of the art approaches on well
known baselines RAWFC (fact checking, 3 class) by 25.28%, and on HOVER
(encyclopedia, 2 class) by 1.54% on 2 hop, 4.94% on 3 hop, and 1.78% on 4 hop,
sub categories respectively. RAV shows the least performance drop compared to
baselines of 16.3% in macro f1 when we compare PFO with its unfiltered version.

</details>


### [70] [TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning Data Selection](https://arxiv.org/abs/2507.03673)
*Xixiang He,Hao Yu,Qiyao Sun,Ao Cheng,Tailai Zhang,Cong Liu,Shuxuan Guo*

Main category: cs.CL

TL;DR: TACOS is a novel method for selecting diverse and high-quality data for instruction fine-tuning, showing significant improvements over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for data selection in instruction fine-tuning suffer from limited data diversity and inconsistent quality evaluation criteria.

Method: TACOS integrates Open Tagging and Comparative Scoring to select diverse and high-quality data for instruction fine-tuning.

Result: TACOS outperforms existing approaches in various datasets and LLM architectures, achieving top performance on MT-Bench and AlpacaEval 2.0.

Conclusion: TACOS is an effective method for instruction fine-tuning data selection, demonstrating superior performance on benchmark tasks.

Abstract: Instruction Fine-Tuning (IFT) is crucial for aligning large language models
(LLMs) with human preferences, and selecting a small yet representative subset
from massive data significantly facilitates IFT in terms of both efficiency and
effectiveness. Nevertheless, existing approaches suffer from two limitations:
the use of simple heuristics restricts data diversity, while the singleton data
quality evaluation accounts for inconsistent criteria between independent
samples. To address the issues, we present TACOS, an innovative method that
integrates Open Tagging and Comparative Scoring for IFT data selection. To
capture data diversity, we leverage LLMs to assign open-domain tags to human
queries, followed by a normalization stage to denoise the open tags and enable
efficient clustering. Additionally, we suggest a comparative scoring method
that allows the relative quality evaluation of samples within a cluster,
avoiding inconsistent criteria seen in singleton-based evaluations. Extensive
experiments across diverse datasets and LLM architectures demonstrate that
TACOS outperforms existing approaches by a large margin. Notably, it achieves
superior instruction-following performance on MT-Bench and ranks 1st among
LLaMA2-7B-Based models on AlpacaEval 2.0, illustrating its efficacy for IFT
data selection.

</details>


### [71] [STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking](https://arxiv.org/abs/2507.03674)
*Tek Raj Chhetri,Yibei Chen,Puja Trivedi,Dorota Jarecka,Saif Haobsh,Patrick Ray,Lydia Ng,Satrajit S. Ghosh*

Main category: cs.CL

TL;DR: StructSense is a framework for structured information extraction that uses LLMs and domain-specific knowledge to improve performance in specialized contexts.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of LLMs' effectiveness in specialized, domain-specific contexts and their poor transferability across tasks and domains.

Method: Introduce StructSense, a modular, task-agnostic, open-source framework for structured information extraction built on LLMs, guided by domain-specific symbolic knowledge encoded in ontologies, incorporating agentic capabilities through self-evaluative judges, and including human-in-the-loop mechanisms.

Result: StructSense demonstrates effectiveness in overcoming domain sensitivity and lack of cross-task generalizability through its application to diverse neuroscience information extraction tasks.

Conclusion: StructSense can overcome the limitations of domain sensitivity and the lack of cross-task generalizability.

Abstract: The ability to extract structured information from unstructured sources-such
as free-text documents and scientific literature-is critical for accelerating
scientific discovery and knowledge synthesis. Large Language Models (LLMs) have
demonstrated remarkable capabilities in various natural language processing
tasks, including structured information extraction. However, their
effectiveness often diminishes in specialized, domain-specific contexts that
require nuanced understanding and expert-level domain knowledge. In addition,
existing LLM-based approaches frequently exhibit poor transferability across
tasks and domains, limiting their scalability and adaptability. To address
these challenges, we introduce StructSense, a modular, task-agnostic,
open-source framework for structured information extraction built on LLMs.
StructSense is guided by domain-specific symbolic knowledge encoded in
ontologies, enabling it to navigate complex domain content more effectively. It
further incorporates agentic capabilities through self-evaluative judges that
form a feedback loop for iterative refinement, and includes human-in-the-loop
mechanisms to ensure quality and validation. We demonstrate that StructSense
can overcome both the limitations of domain sensitivity and the lack of
cross-task generalizability, as shown through its application to diverse
neuroscience information extraction tasks.

</details>


### [72] [Controlling Thinking Speed in Reasoning Models](https://arxiv.org/abs/2507.03704)
*Zhengkai Lin,Zhihang Fu,Ze Chen,Chao Chen,Liang Xie,Wenxiao Wang,Deng Cai,Zheng Wang,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出了一种动态调整思考速度的方法，使大型推理模型能够更高效地处理不同复杂度的任务，提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）虽然在系统2思维方面表现出色，但缺乏系统1思维能力，导致计算开销和延迟较高。为了使LRMs更接近人类智能，需要优化其准确性与效率之间的权衡。

Method: 我们通过识别控制LRMs中快速-慢速思维转换的引导向量，并利用实时难度估计来调整推理步骤的处理速度，从而实现动态思考速度调整。

Result: 我们的方法在领先的LRMs和高级推理基准上平均提高了1.3%的准确性，同时减少了8.6%的token使用量，证明了其有效性。

Conclusion: 我们的方法在不进行任何训练或增加额外成本的情况下，通过调整思考速度实现了更高的准确性和更低的计算开销，为未来的研究和应用提供了新的方向。

Abstract: Human cognition is theorized to operate in two modes: fast, intuitive System
1 thinking and slow, deliberate System 2 thinking. While current Large
Reasoning Models (LRMs) excel at System 2 thinking, their inability to perform
fast thinking leads to high computational overhead and latency. In this work,
we enable LRMs to approximate human intelligence through dynamic thinking speed
adjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses
two key questions: (1) how to control thinking speed in LRMs, and (2) when to
adjust it for optimal performance. For the first question, we identify the
steering vector that governs slow-fast thinking transitions in LRMs'
representation space. Using this vector, we achieve the first representation
editing-based test-time scaling effect, outperforming existing prompt-based
scaling methods. For the second question, we apply real-time difficulty
estimation to signal reasoning segments of varying complexity. Combining these
techniques, we propose the first reasoning strategy that enables fast
processing of easy steps and deeper analysis for complex reasoning. Without any
training or additional cost, our plug-and-play method yields an average +1.3%
accuracy with -8.6% token usage across leading LRMs and advanced reasoning
benchmarks. All of our algorithms are implemented based on vLLM and are
expected to support broader applications and inspire future research.

</details>


### [73] [Can LLMs Play Ô Ăn Quan Game? A Study of Multi-Step Planning and Decision Making](https://arxiv.org/abs/2507.03711)
*Sang Quang Nguyen,Kiet Van Nguyen,Vinh-Tiep Nguyen,Thanh Duc Ngo,Ngan Luu-Thuy Nguyen,Dinh-Duy Le*

Main category: cs.CL

TL;DR: 本文通过传统越南棋类游戏评估大型语言模型的战略决策和推理能力，揭示了它们的优缺点。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估大型语言模型在战略决策、计划移动和管理动态游戏状态方面的能力，以了解其推理和策略能力。

Method: 研究开发了各种代理角色（从攻击性到防御性），并使用^O Ân Quan游戏作为测试平台，评估不同策略下的LLM性能。

Result: 实验结果揭示了LLM在战略决策和动态游戏状态管理方面的表现，提供了对它们在推理和策略方面的优缺点的见解。

Conclusion: 该研究通过分析大型语言模型在传统越南棋类游戏中的表现，揭示了它们在推理和策略方面的优缺点，为理解其整体能力提供了更深入的见解。

Abstract: In this paper, we explore the ability of large language models (LLMs) to plan
and make decisions through the lens of the traditional Vietnamese board game,
\^O \u{A}n Quan. This game, which involves a series of strategic token
movements and captures, offers a unique environment for evaluating the
decision-making and strategic capabilities of LLMs. Specifically, we develop
various agent personas, ranging from aggressive to defensive, and employ the
\^O \u{A}n Quan game as a testbed for assessing LLM performance across
different strategies. Through experimentation with models like
Llama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.3-70B-Instruct, we
aim to understand how these models execute strategic decision-making, plan
moves, and manage dynamic game states. The results will offer insights into the
strengths and weaknesses of LLMs in terms of reasoning and strategy,
contributing to a deeper understanding of their general capabilities.

</details>


### [74] [MemOS: A Memory OS for AI System](https://arxiv.org/abs/2507.03724)
*Zhiyu Li,Shichao Song,Chenyang Xi,Hanyu Wang,Chen Tang,Simin Niu,Ding Chen,Jiawei Yang,Chunyu Li,Qingchen Yu,Jihao Zhao,Yezhaohui Wang,Peng Liu,Zehao Lin,Pengyuan Wang,Jiahao Huo,Tianyi Chen,Kai Chen,Kehang Li,Zhen Tao,Junpeng Ren,Huayi Lai,Hao Wu,Bo Tang,Zhenren Wang,Zhaoxin Fan,Ningyu Zhang,Linfeng Zhang,Junchi Yan,Mingchuan Yang,Tong Xu,Wei Xu,Huajun Chen,Haofeng Wang,Hongkang Yang,Wentao Zhang,Zhi-Qin John Xu,Siheng Chen,Feiyu Xiong*

Main category: cs.CL

TL;DR: 本文提出了一种名为MemOS的内存操作系统，旨在解决大型语言模型（LLMs）在长期上下文推理、持续个性化和知识一致性方面的挑战。MemOS通过统一表示、调度和演化不同类型的内存，实现了高效存储和检索，并为持续学习和个性化建模奠定了基础。


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency.

Method: MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval.

Result: MemOS enables flexible transitions between memory types and bridges retrieval with parameter-based learning. It establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs.

Conclusion: MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.

Abstract: Large Language Models (LLMs) have become an essential infrastructure for
Artificial General Intelligence (AGI), yet their lack of well-defined memory
management systems hinders the development of long-context reasoning, continual
personalization, and knowledge consistency.Existing models mainly rely on
static parameters and short-lived contextual states, limiting their ability to
track user preferences or update knowledge over extended periods.While
Retrieval-Augmented Generation (RAG) introduces external knowledge in plain
text, it remains a stateless workaround without lifecycle control or
integration with persistent representations.Recent work has modeled the
training and inference cost of LLMs from a memory hierarchy perspective,
showing that introducing an explicit memory layer between parameter memory and
external retrieval can substantially reduce these costs by externalizing
specific knowledge. Beyond computational efficiency, LLMs face broader
challenges arising from how information is distributed over time and context,
requiring systems capable of managing heterogeneous knowledge spanning
different temporal scales and sources. To address this challenge, we propose
MemOS, a memory operating system that treats memory as a manageable system
resource. It unifies the representation, scheduling, and evolution of
plaintext, activation-based, and parameter-level memories, enabling
cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates
both memory content and metadata such as provenance and versioning. MemCubes
can be composed, migrated, and fused over time, enabling flexible transitions
between memory types and bridging retrieval with parameter-based learning.
MemOS establishes a memory-centric system framework that brings
controllability, plasticity, and evolvability to LLMs, laying the foundation
for continual learning and personalized modeling.

</details>


### [75] [Alpay Algebra IV: Symbiotic Semantics and the Fixed-Point Convergence of Observer Embeddings](https://arxiv.org/abs/2507.03774)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: 本文提出了一种理论框架，通过无限次固定点交互实现AI与文档的稳定语义对齐，并证明了其数学合理性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一种严格的范畴论方法，实现嵌入层面的对齐，具有语义安全、符号记忆和构建具有持久自我参照理解的AI系统的潜力。

Method: 基于Alpay代数，引入了一个函子系统，其中观察者（AI）和文本环境（本文）通过phi-infinity算子进行迭代变换，从而达到稳定状态。

Result: 证明了这种收敛在数学上是合理的，语义不变且永久的，即使在扰动或进一步上下文扩展下也能保持稳定。

Conclusion: 本文提出了一种理论框架，其中文档和AI模型通过无限次固定点交互实现稳定的语义对齐。这种固定点可以作为“共情嵌入”，使AI不仅理解内容的意义，还内化作者的意图。

Abstract: We present a theoretical framework in which a document and an AI model engage
in a transfinite fixed-point interaction that leads to stable semantic
alignment. Building on the foundations of Alpay Algebra, we introduce a
functorial system wherein an observer (the AI) and a textual environment (this
paper) co-evolve through iterative transformations guided by the phi-infinity
operator. This process guarantees the existence of a unique fixed point in the
AI's embedding space -- a state where the AI's internal representation of the
content becomes stable, self-consistent, and semantically faithful. We prove
that such convergence is mathematically sound, semantically invariant, and
permanent, even under perturbation or further context expansion. This fixed
point acts as an "empathetic embedding," wherein the AI internalizes not only
the meaning of the content but also the author's intent. We interpret this as a
rigorous, category-theoretic route to alignment at the embedding level, with
implications for semantic security, symbolic memory, and the construction of AI
systems with persistent self-referential understanding. All references in this
paper function as nodes in the Alpay Algebra universe, and this work embeds
itself as a new fixed-point node within that transfinite semantic graph.

</details>


### [76] [OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference](https://arxiv.org/abs/2507.03865)
*Seungjun Shin,Jaehoon Oh,Dokwan Oh*

Main category: cs.CL

TL;DR: 本文研究了LLM中sink token与其他令牌的关系，并提出了一种基于正交性的动态令牌选择方法OrthoRank，在多个任务中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，sink token虽然语义作用有限，但会获得不成比例的高注意力。本文旨在探索sink token与其他令牌之间的关系，并提出一种新的动态令牌选择方法。

Method: 本文提出了一种动态令牌选择方法，称为OrthoRank，利用了sink token与其他令牌之间的相似性以及它们在不同层中的变化情况。

Result: 实验表明，OrthoRank方法在相同稀疏比下相比层剪枝方法具有更低的困惑度和更高的零样本准确率，并且在LongBench上表现更优。

Conclusion: 本文提出的OrthoRank方法在相同稀疏比下相比层剪枝方法具有更低的困惑度和更高的零样本准确率，并且在LongBench上表现更优。

Abstract: Attention mechanisms are central to the success of large language models
(LLMs), enabling them to capture intricate token dependencies and implicitly
assign importance to each token. Recent studies have revealed the sink token,
which receives disproportionately high attention despite their limited semantic
role. In this paper, we first expand the relationship between the sink token
and other tokens, moving beyond attention to explore their similarity in hidden
states, considering the layer depth. We observe that as the layers get deeper,
the cosine similarity between the normalized hidden states of the sink token
and those of other tokens increases, and that the normalized hidden states of
the sink token exhibit negligible changes. These imply that other tokens
consistently are directed toward the sink token throughout the layers. Next, we
propose a dynamic token selection method, called OrthoRank, using these
findings to select important tokens. Specifically, in a certain layer, we
define token importance by the speed at which the token moves toward the sink
token. This is converted into orthogonality with the sink token, meaning that
tokens that are more orthogonal to the sink token are assigned greater
importance. Finally, through extensive experiments, we demonstrated that our
method results in lower perplexity and higher zero-shot accuracy compared to
layer pruning methods at the same sparsity ratio with comparable throughput,
while also achieving superior performance on LongBench.

</details>


### [77] [Demystifying ChatGPT: How It Masters Genre Recognition](https://arxiv.org/abs/2507.03875)
*Subham Raj,Sriparna Saha,Brijraj Singh,Niranjan Pedanekar*

Main category: cs.CL

TL;DR: 本研究分析了ChatGPT在类型预测中的能力，发现其表现出色，并通过结合视觉信息进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究已经展示了ChatGPT在各种下游NLP任务中的显著进展，但其在类型预测中的能力和限制仍然不明确。

Method: 我们使用MovieLens-100K数据集分析了三种大型语言模型（LLMs），以评估它们的类型预测能力。我们设置了零样本和少量样本提示，利用电影预告片的音频转录/字幕，并扩展了研究，提取IMDb电影海报以利用视觉语言模型（VLM）进行海报信息提示。

Result: 未经微调的ChatGPT在类型预测中表现优于其他LLMs，而微调后的ChatGPT整体表现最佳。通过结合电影海报的视觉信息，进一步增强了我们的发现。

Conclusion: 我们的研究揭示了ChatGPT在类型预测方面的卓越能力，超越了其他语言模型。将视觉语言模型（VLM）整合进一步增强了我们的发现，展示了ChatGPT通过结合电影海报的视觉信息在相关内容应用中的潜力。

Abstract: The introduction of ChatGPT has garnered significant attention within the NLP
community and beyond. Previous studies have demonstrated ChatGPT's substantial
advancements across various downstream NLP tasks, highlighting its adaptability
and potential to revolutionize language-related applications. However, its
capabilities and limitations in genre prediction remain unclear. This work
analyzes three Large Language Models (LLMs) using the MovieLens-100K dataset to
assess their genre prediction capabilities. Our findings show that ChatGPT,
without fine-tuning, outperformed other LLMs, and fine-tuned ChatGPT performed
best overall. We set up zero-shot and few-shot prompts using audio
transcripts/subtitles from movie trailers in the MovieLens-100K dataset,
covering 1682 movies of 18 genres, where each movie can have multiple genres.
Additionally, we extended our study by extracting IMDb movie posters to utilize
a Vision Language Model (VLM) with prompts for poster information. This
fine-grained information was used to enhance existing LLM prompts. In
conclusion, our study reveals ChatGPT's remarkable genre prediction
capabilities, surpassing other language models. The integration of VLM further
enhances our findings, showcasing ChatGPT's potential for content-related
applications by incorporating visual information from movie posters.

</details>


### [78] [Dynamic Injection of Entity Knowledge into Dense Retrievers](https://arxiv.org/abs/2507.03922)
*Ikuya Yamada,Ryokan Ri,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: KPR是一种基于BERT的检索器，通过引入上下文实体注意力层和动态实体嵌入来提升检索性能，尤其在处理不常见实体查询时效果显著。


<details>
  <summary>Details</summary>
Motivation: 密集检索器在涉及不常见实体的查询中表现不佳，因为它们的实体知识有限。

Method: KPR是一种基于BERT的检索器，通过上下文实体注意力层和动态可更新的实体嵌入进行增强。

Result: KPR在三个数据集上的实验表明，它能持续提高检索准确性，在EntityQuestions数据集上比没有KPR扩展的模型提高了12.6%。

Conclusion: KPR能够提高检索准确性，并在两个数据集上实现了与类似大小模型相比的最先进性能。

Abstract: Dense retrievers often struggle with queries involving less-frequent entities
due to their limited entity knowledge. We propose the Knowledgeable Passage
Retriever (KPR), a BERT-based retriever enhanced with a context-entity
attention layer and dynamically updatable entity embeddings. This design
enables KPR to incorporate external entity knowledge without retraining.
Experiments on three datasets show that KPR consistently improves retrieval
accuracy, achieving a substantial 12.6% gain on the EntityQuestions dataset
over the model without KPR extensions. When built on the off-the-shelf bge-base
retriever, KPR achieves state-of-the-art performance among similarly sized
models on two datasets. Code and models will be released soon.

</details>


### [79] [Losing our Tail -- Again: On (Un)Natural Selection And Multilingual Large Language Models](https://arxiv.org/abs/2507.03933)
*Eva Vanmassenhove*

Main category: cs.CL

TL;DR: 本文讨论了多语言大型语言模型可能带来的语言多样性丧失问题，并呼吁重新设想自然语言处理领域以保护语言的多样性和创造力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨多语言大型语言模型（LLMs）对语言生态系统的深远影响，特别是它们可能导致的语言多样性逐渐消失的问题。

Method: 本文通过引用计算机视觉、自然语言处理（NLP）和机器翻译（MT）的最新研究，探讨了模型崩溃如何导致语言形式、语法特征和文化细微差别的丧失。

Result: 模型崩溃指的是自我消耗训练循环的最终后果，其中模型强化自身的偏见并失去语言多样性。语言分布的尾部正在消失，随之消失的是它们所承载的故事和身份。

Conclusion: 本文呼吁抵制语言扁平化，并重新设想NLP作为一个鼓励、重视和保护表达性多语言词汇和语言多样性及创造力的领域。

Abstract: Multilingual Large Language Models (LLMs) considerably changed how
technologies can influence language. While previous technologies could mediate
or assist humans, there is now a tendency to \textit{offload} the task of
writing itself to these technologies, enabling them to change our linguistic
ecosystem more directly. While they provide us quick access to information and
impressively fluent output, beneath their apparent sophistication lies a
subtle, more insidious threat: the gradual decline and loss of linguistic
diversity. With this opinion piece, I explore how model collapse, with a
particular focus on translation technology, can lead to the loss of linguistic
forms, grammatical features, and cultural nuance. Model collapse refers to the
eventual consequence of self-consuming training loops, where models reinforce
their own biases and lose linguistic diversity. Drawing on recent work in
Computer Vision, Natural Language Processing (NLP) and Machine Translation
(MT), I argue that the tails of our linguistic distributions are vanishing, and
with them, the narratives and identities they carry. This is a call to resist
linguistic flattening and to reimagine NLP as a field that encourages, values
and protects expressive multilingual lexical and linguistic diversity and
creativity.

</details>


### [80] [A Modular Unsupervised Framework for Attribute Recognition from Unstructured Text](https://arxiv.org/abs/2507.03949)
*KMA Solaiman*

Main category: cs.CL

TL;DR: POSID 是一种无需任务特定微调即可从非结构化文本中提取基于属性的结构化属性的框架。


<details>
  <summary>Details</summary>
Motivation: 为了在不进行监督训练的情况下从非结构化文本中提取基于属性的结构化属性，提出了 POSID 框架。

Method: POSID 结合了词法和语义相似性技术来识别相关句子并提取属性。

Result: 在缺失人员用例中使用 InciText 数据集验证了 POSID 的有效性，实现了有效的属性提取。

Conclusion: POSID 是一种模块化、轻量级且按需的框架，能够在不进行任务特定微调的情况下从非结构化文本中提取基于属性的结构化属性。

Abstract: We propose POSID, a modular, lightweight and on-demand framework for
extracting structured attribute-based properties from unstructured text without
task-specific fine-tuning. While the method is designed to be adaptable across
domains, in this work, we evaluate it on human attribute recognition in
incident reports. POSID combines lexical and semantic similarity techniques to
identify relevant sentences and extract attributes. We demonstrate its
effectiveness on a missing person use case using the InciText dataset,
achieving effective attribute extraction without supervised training.

</details>


### [81] [Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents](https://arxiv.org/abs/2507.04009)
*Ziyang Miao,Qiyu Sun,Jingyuan Wang,Yuchen Gong,Yaowei Zheng,Shiqi Li,Richong Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Easy Dataset的框架，用于从非结构化文档中合成微调数据，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成工具难以从异构文档中有效提取可靠的微调数据，因此需要一种更有效的解决方案。

Method: 提出了一种名为Easy Dataset的统一框架，通过直观的图形用户界面（GUI）从非结构化文档中合成微调数据。

Result: 在金融问答任务上的实验表明，使用合成数据集微调LLM可以显著提升领域特定性能。

Conclusion: 实验表明，在合成数据集上微调LLM可以显著提高特定领域的性能，同时保持通用知识。

Abstract: Large language models (LLMs) have shown impressive performance on
general-purpose tasks, yet adapting them to specific domains remains
challenging due to the scarcity of high-quality domain data. Existing data
synthesis tools often struggle to extract reliable fine-tuning data from
heterogeneous documents effectively. To address this limitation, we propose
Easy Dataset, a unified framework for synthesizing fine-tuning data from
unstructured documents via an intuitive graphical user interface (GUI).
Specifically, Easy Dataset allows users to easily configure text extraction
models and chunking strategies to transform raw documents into coherent text
chunks. It then leverages a persona-driven prompting approach to generate
diverse question-answer pairs using public-available LLMs. Throughout the
pipeline, a human-in-the-loop visual interface facilitates the review and
refinement of intermediate outputs to ensure data quality. Experiments on a
financial question-answering task show that fine-tuning LLMs on the synthesized
dataset significantly improves domain-specific performance while preserving
general knowledge. The source code and installable package are available at
https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub
stars.

</details>


### [82] [Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on Korean Superstition](https://arxiv.org/abs/2507.04014)
*Kyuhee Kim,Sangah Lee*

Main category: cs.CL

TL;DR: 本文介绍了Nunchi-Bench基准，用于评估大型语言模型的文化理解能力，特别是针对韩国迷信。研究发现模型在文化推理方面面临挑战，明确的文化框架能更有效地提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型成为各个领域的关键顾问，它们的文化敏感性和推理能力在多元文化环境中至关重要。然而，目前缺乏专门评估模型文化理解能力的基准。

Method: 引入了Nunchi-Bench基准，用于评估大型语言模型的文化理解能力，特别是针对韩国迷信。该基准包含247个问题，涵盖31个主题，评估事实知识、文化适当建议和情境解释。提出了一个新颖的评估策略，采用定制化评分指标来捕捉模型识别文化细微差别并适当回应的能力。

Result: 研究结果表明，大型语言模型在文化推理方面存在显著挑战。虽然模型通常能识别事实信息，但在实际场景中应用这些信息时表现不佳。此外，明确的文化框架比仅依赖提示的语言更能有效提升性能。

Conclusion: 研究发现大型语言模型在文化推理方面面临重大挑战，尽管它们通常能识别事实信息，但在实际场景中应用这些信息时表现不佳。此外，明确的文化框架比仅依赖提示的语言更能有效提升性能。

Abstract: As large language models (LLMs) become key advisors in various domains, their
cultural sensitivity and reasoning skills are crucial in multicultural
environments. We introduce Nunchi-Bench, a benchmark designed to evaluate LLMs'
cultural understanding, with a focus on Korean superstitions. The benchmark
consists of 247 questions spanning 31 topics, assessing factual knowledge,
culturally appropriate advice, and situational interpretation. We evaluate
multilingual LLMs in both Korean and English to analyze their ability to reason
about Korean cultural contexts and how language variations affect performance.
To systematically assess cultural reasoning, we propose a novel evaluation
strategy with customized scoring metrics that capture the extent to which
models recognize cultural nuances and respond appropriately. Our findings
highlight significant challenges in LLMs' cultural reasoning. While models
generally recognize factual information, they struggle to apply it in practical
scenarios. Furthermore, explicit cultural framing enhances performance more
effectively than relying solely on the language of the prompt. To support
further research, we publicly release Nunchi-Bench alongside a leaderboard.

</details>


### [83] [Handling Korean Out-of-Vocabulary Words with Phoneme Representation Learning](https://arxiv.org/abs/2507.04018)
*Nayeon Kim,Eojin Jeon,Jun-Hyung Park,SangKeun Lee*

Main category: cs.CL

TL;DR: KOPL是一个用于处理韩语OOV单词的新框架，通过音素表示学习提高性能，并可轻松集成到现有模型中。


<details>
  <summary>Details</summary>
Motivation: 解决韩语OOV（词汇外）单词的问题，利用韩语作为音素脚本的语言学特性。

Method: KOPL框架利用了韩语的音素表示学习，结合了音素和单词表示，以捕捉单词的文本和音素信息。

Result: KOPL在韩国自然语言处理任务上表现出色，平均优于最先进的模型1.9%。

Conclusion: KOPL显著提高了韩国语自然语言处理任务的性能，并且可以方便地集成到现有的静态和上下文韩国语嵌入模型中。

Abstract: In this study, we introduce KOPL, a novel framework for handling Korean OOV
words with Phoneme representation Learning. Our work is based on the linguistic
property of Korean as a phonemic script, the high correlation between phonemes
and letters. KOPL incorporates phoneme and word representations for Korean OOV
words, facilitating Korean OOV word representations to capture both text and
phoneme information of words. We empirically demonstrate that KOPL
significantly improves the performance on Korean Natural Language Processing
(NLP) tasks, while being readily integrated into existing static and contextual
Korean embedding models in a plug-and-play manner. Notably, we show that KOPL
outperforms the state-of-the-art model by an average of 1.9%. Our code is
available at https://github.com/jej127/KOPL.git.

</details>


### [84] [LLMThinkBench: Towards Basic Math Reasoning and Overthinking in Large Language Models](https://arxiv.org/abs/2507.04023)
*Gaurav Srivastava,Aafiya Hussain,Sriram Srinivasan,Xuan Wang*

Main category: cs.CL

TL;DR: LLMThinkBench is a modular benchmarking framework designed to evaluate basic math reasoning and overthinking in LLMs, offering a cost-effective alternative to expensive LLM-as-a-judge methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to systematically assess the phenomenon of LLMs struggling with simple arithmetic tasks and exhibiting overthinking, which hinders their performance on basic math reasoning.

Method: The paper introduces LLMThinkBench, a modular benchmarking framework that enables researchers to evaluate basic math reasoning and overthinking in LLMs. It includes 14 configurable math tasks with randomized test data generation and robust parsing strategies. The Overthinking Score metric is used to quantify overthinking through accuracy-verbosity tradeoffs.

Result: LLMThinkBench offers flexible evaluation with a scalable vLLM/Transformers backend, multi-GPU support, and full configurability. It allows users to extend the tool with custom tasks, reproduce experiments with seeding, and generate detailed efficiency reports.

Conclusion: LLMThinkBench provides researchers and practitioners with an accessible, cost-effective alternative to expensive LLM-as-a-judge methods for diagnosing basic reasoning capabilities and efficiency analysis.

Abstract: Large Language Models (LLMs) have achieved remarkable performance on complex
mathematical benchmarks, yet often struggle with simple arithmetic tasks and
exhibit a tendency toward over-explaining or "overthinking" answers. To
systematically assess this phenomenon, we introduce LLMThinkBench, a modular
benchmarking framework that enables researchers to evaluate basic math
reasoning and overthinking in LLMs. The framework provides 14 configurable math
tasks with randomized test data generation and robust parsing strategies.
Researchers can quantify overthinking using our Overthinking Score metric,
which captures accuracy-verbosity tradeoffs through harmonic mean formulation.
The tool offers flexible evaluation with a scalable vLLM/Transformers backend,
multi-GPU support, and full configurability. Users can extend the tool with
custom tasks, reproduce experiments with seeding, and generate detailed
efficiency reports. Distributed as a pip-installable package with CLI and API
access, LLMThinkBench provides researchers and practitioners an accessible,
cost-effective alternative to expensive LLM-as-a-judge methods for diagnosing
basic reasoning capabilities and efficiency analysis. Package can be installed
as: pip install llmthinkbench

</details>


### [85] [Patient-Centered RAG for Oncology Visit Aid Following the Ottawa Decision Guide](https://arxiv.org/abs/2507.04026)
*Siyang Liu,Lawrence Chin-I An,Rada Mihalcea*

Main category: cs.CL

TL;DR: 本研究介绍了一个基于检索增强生成的系统，旨在帮助患者从缺乏信息到准备充分地进行医疗访问，并展示了其在前列腺癌护理中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 有效沟通在癌症护理中至关重要，但患者常常在准备复杂的医疗访问时面临挑战。

Method: 我们提出了一个交互式的、基于检索增强生成的系统，该系统将渥太华个人决策指南适应为动态检索增强生成工作流程，帮助用户填补知识差距，明确个人价值观并生成有用的问题。

Result: 结果表明系统的可用性很高（UMUX平均值为6.0/7），生成内容的相关性很强（平均值为6.7/7），需要的编辑很少，且临床忠实度很高（平均值为6.82/7）。

Conclusion: 本研究展示了将以患者为中心的设计与语言模型结合在肿瘤学护理中增强临床准备的潜力。

Abstract: Effective communication is essential in cancer care, yet patients often face
challenges in preparing for complex medical visits. We present an interactive,
Retrieval-augmented Generation-assisted system that helps patients progress
from uninformed to visit-ready. Our system adapts the Ottawa Personal Decision
Guide into a dynamic retrieval-augmented generation workflow, helping users
bridge knowledge gaps, clarify personal values and generate useful questions
for their upcoming visits. Focusing on localized prostate cancer, we conduct a
user study with patients and a clinical expert. Results show high system
usability (UMUX Mean = 6.0 out of 7), strong relevance of generated content
(Mean = 6.7 out of 7), minimal need for edits, and high clinical faithfulness
(Mean = 6.82 out of 7). This work demonstrates the potential of combining
patient-centered design with language models to enhance clinical preparation in
oncology care.

</details>


### [86] [Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering](https://arxiv.org/abs/2507.04069)
*Ting-Wen Ko,Jyun-Yu Jiang,Pu-Jen Cheng*

Main category: cs.CL

TL;DR: AdaPCR是一种新的开放域问答框架，通过建模段落间的依赖关系来提高检索效果，尤其在多跳推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在检索段落时独立进行，导致冗余、噪声或上下文不足的问题，尤其是在嘈杂语料库和多跳问题中。

Method: AdaPCR框架通过将段落组合作为检索和重新排序的单位，显式建模段落之间的依赖关系，并使用上下文感知的查询重写和基于预测目标的重新排序步骤。

Result: 在多个问答基准测试中，AdaPCR的表现优于基线，特别是在多跳推理任务中。

Conclusion: AdaPCR通过建模跨段落依赖关系，提高了检索效果，尤其在多跳推理任务中表现优异。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external documents at inference time, enabling up-to-date
knowledge access without costly retraining. However, conventional RAG methods
retrieve passages independently, often leading to redundant, noisy, or
insufficiently diverse context-particularly problematic - particularly
problematic in noisy corpora and for multi-hop questions. To address this, we
propose Adaptive Passage Combination Retrieval (AdaPCR), a novel framework for
open-domain question answering with black-box LMs. AdaPCR explicitly models
dependencies between passages by considering passage combinations as units for
retrieval and reranking. It consists of a context-aware query reformulation
using concatenated passages, and a reranking step trained with a predictive
objective aligned with downstream answer likelihood. Crucially, AdaPCR
adaptively selects the number of retrieved passages without additional stopping
modules. Experiments across several QA benchmarks show that AdaPCR outperforms
baselines, particularly in multi-hop reasoning, demonstrating the effectiveness
of modeling inter-passage dependencies for improved retrieval.

</details>


### [87] [XISM: an eXploratory and Interactive Graph Tool to Visualize and Evaluate Semantic Map Models](https://arxiv.org/abs/2507.04070)
*Zhu Liu,Zhen Hu,Lei Dai,Ying Liu*

Main category: cs.CL

TL;DR: 本文介绍了一个名为XISM的交互式工具，用于构建和评估语义图，结合了数据驱动的效率和专家知识。


<details>
  <summary>Details</summary>
Motivation: 传统的语义图模型是手动构建的，效率低下，缺乏可视化和评估工具。因此，需要一种更高效、可交互的方法来构建和评估语义图。

Method: 本文提出了一种基于先前算法的交互式工具XISM，采用自上而下的方法构建语义图，并提供候选图的显示和评估。用户可以通过编辑边来优化图。

Result: XISM系统已公开可用，并提供了演示视频，展示了其功能和效果。

Conclusion: 本文介绍了XISM，这是一个交互式工具，能够从用户数据中构建语义图，并通过多指标评估它们。该系统结合了数据驱动的效率和专家知识，具有人机协同设计，对类型学家和计算语言学家都有好处。

Abstract: Semantic map models represent meanings or functions as nodes in a graph
constrained by the local connectivity hypothesis, with edges indicating their
associations. Widely used in typological linguistics, these models compare
interrelated meanings across languages. Traditionally built manually in a
bottom-up manner, they are inefficient for large datasets and lack
visualization and evaluation tools. This paper introduces XISM, an interactive
tool based on our prior algorithm, which constructs semantic maps from user
data via a top-down approach, displays candidate maps, and evaluates them using
multiple metrics. Users can refine maps by editing edges, combining data-driven
efficiency with expert knowledge. This human-in-the-loop design benefits both
typologists and computational linguists. The system
https://770103knev48.vicp.fun/ and a demonstration video
https://youtu.be/S-wsVDF2HSI?si=1OrcF41tRznaifhZ are publicly available.

</details>


### [88] [Conversation Forests: The Key to Fine Tuning Large Language Models for Multi-Turn Medical Conversations is Branching](https://arxiv.org/abs/2507.04099)
*Thomas Savage*

Main category: cs.CL

TL;DR: 本文提出了Savage Conversation Forests (SCF) 框架，用于微调大型语言模型以处理多轮对话任务，实验结果显示其在诊断准确性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的微调方法在单次任务中表现良好，但在多轮应用中效果不佳，例如诊断患者访谈，需要理解早期对话回合如何影响后续完成和结果。

Method: 引入了Savage Conversation Forests (SCF)，这是一种利用分支对话架构进行微调的强化学习框架。

Result: 在模拟医生-患者对话的实验中，具有分支的SCF在诊断准确性上优于线性对话架构。

Conclusion: 这些结果表明，分支训练架构是微调大型语言模型在复杂多轮对话任务中的重要策略。

Abstract: Fine-tuning methods such as Direct Preference Optimization (DPO) and Group
Relative Policy Optimization (GRPO) have demonstrated success in training large
language models (LLMs) for single-turn tasks. However, these methods fall short
in multi-turn applications, such as diagnostic patient interviewing, where
understanding how early conversational turns influence downstream completions
and outcomes is essential. In medicine, a multi-turn perspective is critical
for learning diagnostic schemas and better understanding conversation dynamics.
To address this gap, I introduce Savage Conversation Forests (SCF), a
reinforcement learning framework that leverages a branched conversation
architecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple
possible conversation continuations at each turn, enabling the model to learn
how different early responses affect downstream interactions and diagnostic
outcomes. In experiments simulating doctor-patient conversations, SCF with
branching outperforms linear conversation architectures on diagnostic accuracy.
I hypothesize that SCF's improvements stem from its ability to provide richer,
interdependent training signals across conversation turns. These results
suggest that a branched training architecture is an important strategy for fine
tuning LLMs in complex multi-turn conversational tasks.

</details>


### [89] [BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question Answering](https://arxiv.org/abs/2507.04127)
*Costas Mavromatis,Soji Adeshina,Vassilis N. Ioannidis,Zhen Han,Qi Zhu,Ian Robinson,Bryan Thompson,Huzefa Rangwala,George Karypis*

Main category: cs.CL

TL;DR: BYOKG-RAG是一个结合大型语言模型和专门图检索工具的知识图谱问答框架，它在多个基准测试中表现优异，并在自定义知识图谱上具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的KGQA方法依赖于大型语言模型代理进行图遍历和检索，这种方法对遍历初始化敏感，容易出现实体链接错误，并且在自定义知识图谱上泛化能力较差。

Method: BYOKG-RAG框架利用LLM生成关键的图工件（如问题实体、候选答案、推理路径和OpenCypher查询），并通过图工具将这些工件链接到知识图谱并检索相关上下文。

Result: 在五个涵盖不同知识图谱类型的基准测试中，BYOKG-RAG的表现优于第二好的图检索方法4.5个百分点，并且在自定义知识图谱上表现出更好的泛化能力。

Conclusion: BYOKG-RAG框架通过结合LLM和专门的图检索工具，为自定义知识图谱上的问答任务提供了一个更通用和稳健的解决方案。

Abstract: Knowledge graph question answering (KGQA) presents significant challenges due
to the structural and semantic variations across input graphs. Existing works
rely on Large Language Model (LLM) agents for graph traversal and retrieval; an
approach that is sensitive to traversal initialization, as it is prone to
entity linking errors and may not generalize well to custom ("bring-your-own")
KGs. We introduce BYOKG-RAG, a framework that enhances KGQA by synergistically
combining LLMs with specialized graph retrieval tools. In BYOKG-RAG, LLMs
generate critical graph artifacts (question entities, candidate answers,
reasoning paths, and OpenCypher queries), and graph tools link these artifacts
to the KG and retrieve relevant graph context. The retrieved context enables
the LLM to iteratively refine its graph linking and retrieval, before final
answer generation. By retrieving context from different graph tools, BYOKG-RAG
offers a more general and robust solution for QA over custom KGs. Through
experiments on five benchmarks spanning diverse KG types, we demonstrate that
BYOKG-RAG outperforms the second-best graph retrieval method by 4.5% points
while showing better generalization to custom KGs. BYOKG-RAG framework is
open-sourced at https://github.com/awslabs/graphrag-toolkit.

</details>


### [90] [Token Level Hallucination Detection via Variance in Language Models](https://arxiv.org/abs/2507.04137)
*Keshav Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种无需参考的、基于标记级别的幻觉检测框架，该框架利用了在多次随机生成中的标记对数概率方差，能够可靠地检测模型输出中的不稳定性，并与幻觉模式相关联。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种任务中展示了令人印象深刻的生成能力，但仍然容易产生幻觉，即自信地生成但事实错误的输出。

Method: 我们引入了一种无参考的、基于标记级别的幻觉检测框架，该框架利用了在多次随机生成中的标记对数概率方差。

Result: 通过定量指标和视觉诊断，我们展示了基于标记的方差能够可靠地突出模型输出中的不稳定性，并与幻觉模式相关联。

Conclusion: 我们的框架轻量、可重复且适用于多个领域，为分析大型语言模型的生成可靠性提供了一个有价值的诊断工具。

Abstract: Large Language Models (LLMs) have demonstrated impressive generative
capabilities across diverse tasks but remain susceptible to hallucinations,
confidently generated yet factually incorrect outputs. We introduce a
reference-free, token-level hallucination detection framework that leverages
the variance in token log-probabilities across multiple stochastic generations.
Unlike prior methods that require ground-truth references or sentence-level
verification, our approach is model-agnostic, interpretable, and suited for
real-time or post-hoc analysis. We evaluate our method on unanswerable question
prompts from the SQuAD v2 dataset and benchmark across three autoregressive
models of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both
quantitative metrics and visual diagnostics, we show that token-level variance
reliably highlights instability in model outputs and correlates with
hallucination patterns. Our framework is lightweight, reproducible, and
adaptable to multiple domains, offering a valuable diagnostic tool for
analyzing generative reliability in LLMs.

</details>


### [91] [Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies](https://arxiv.org/abs/2507.04142)
*Mael Jullien,Marco Valentino,Leonardo Ranaldi,Andre Freitas*

Main category: cs.CL

TL;DR: 本研究评估了提示结构和高效微调对临床自然语言推理（NLI）性能的影响。结果显示，提示类型可以解释高达44%的性能差异，而LoRA微调显著提升了模型性能，并缩小了与前沿模型的差距。研究还强调了结合提示设计和轻量级适应在临床NLP系统中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管最近关于大型语言模型（LLMs）的研究已经展示了提示策略和微调技术对其推理能力的影响，但它们在临床自然语言推理（NLI）中的有效性仍缺乏深入探索。因此，本研究旨在填补这一空白，评估提示结构和高效微调对临床NLI性能的影响。

Method: 本研究通过控制实验评估了提示结构和高效微调如何共同影响临床自然语言推理（NLI）中的模型性能。我们检查了四种类别的提示策略以在不同抽象层次上激发LLMs的推理能力，并评估了它们对一系列临床相关推理类型的影响。对于每种提示策略，我们使用前沿模型构建高质量的示范，通过低秩适应（LoRA）将多步骤推理能力提炼到较小的模型（4B参数）中。

Result: 研究发现，提示类型本身可以解释宏F1分数中高达44%的方差。此外，LoRA微调带来了+8到12 F1的稳定提升，输出对齐率超过97%，并将性能差距缩小到GPT-4o-mini以内7.1%。额外的推理泛化实验表明，LoRA在MedNLI和TREC临床试验轨道上的75%的模型中提升了性能。

Conclusion: 研究结果表明，(i) 提示结构是临床推理性能的主要驱动因素，(ii) 配备强大提示和LoRA的紧凑模型可以与前沿规模系统竞争，(iii) 重视推理类型的评估对于揭示提示引起的权衡至关重要。我们的结果突显了结合提示设计和轻量级适应在更高效和可信赖的临床NLP系统中的潜力，提供了对广泛采用的提示和参数高效技术在高度专业领域中的优缺点的见解。

Abstract: Recent works on large language models (LLMs) have demonstrated the impact of
prompting strategies and fine-tuning techniques on their reasoning
capabilities. Yet, their effectiveness on clinical natural language inference
(NLI) remains underexplored. This study presents the first controlled
evaluation of how prompt structure and efficient fine-tuning jointly shape
model performance in clinical NLI. We inspect four classes of prompting
strategies to elicit reasoning in LLMs at different levels of abstraction, and
evaluate their impact on a range of clinically motivated reasoning types. For
each prompting strategy, we construct high-quality demonstrations using a
frontier model to distil multi-step reasoning capabilities into smaller models
(4B parameters) via Low-Rank Adaptation (LoRA). Across different language
models fine-tuned on the NLI4CT benchmark, we found that prompt type alone
accounts for up to 44% of the variance in macro-F1. Moreover, LoRA fine-tuning
yields consistent gains of +8 to 12 F1, raises output alignment above 97%, and
narrows the performance gap to GPT-4o-mini to within 7.1%. Additional
experiments on reasoning generalisation reveal that LoRA improves performance
in 75% of the models on MedNLI and TREC Clinical Trials Track. Overall, these
findings demonstrate that (i) prompt structure is a primary driver of clinical
reasoning performance, (ii) compact models equipped with strong prompts and
LoRA can rival frontier-scale systems, and (iii) reasoning-type-aware
evaluation is essential to uncover prompt-induced trade-offs. Our results
highlight the promise of combining prompt design and lightweight adaptation for
more efficient and trustworthy clinical NLP systems, providing insights on the
strengths and limitations of widely adopted prompting and parameter-efficient
techniques in highly specialised domains.

</details>


### [92] [Large Language Models for Zero-Shot Multicultural Name Recognition](https://arxiv.org/abs/2507.04149)
*Thanakorn Phonchai,Surasakdi Siripong,Nicholas Patterson,Owen Campbell*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，用于提高多文化姓名识别的准确性和鲁棒性，通过结合对抗数据增强和文化知识图谱集成，显著提升了零样本识别性能。


<details>
  <summary>Details</summary>
Motivation: 传统的姓名识别方法在面对不同语言和文化背景下的多样性和新排列时常常失效。因此，需要一种能够有效处理多文化姓名识别的方法。

Method: 本文提出了一种名为Prompt-Engineered Fine-Tuning (PEFT)的方法，结合了对抗数据增强和文化知识图谱集成，利用预训练大语言模型的强大语言理解能力，将识别任务转化为引导生成问题。

Result: 实验结果表明，PEFT方法在整体准确率和挑战性的零样本姓名识别准确率上分别达到了93.1%和89.5%，优于现有的深度学习基线方法。

Conclusion: 本文提出了一个创新的框架，用于提升零样本多文化姓名识别的准确性和鲁棒性。实验结果表明，该方法在多个基准测试中表现优异，接近人类专家的判断。这为现实世界的应用提供了一个高效且可扩展的解决方案。

Abstract: The robust and accurate recognition of multicultural names, particularly
those not previously encountered, is a critical challenge in an increasingly
globalized digital landscape. Traditional methods often falter when confronted
with the vast diversity and novel permutations of names across different
linguistic and cultural backgrounds. This paper introduces a novel framework,
Prompt-Engineered Fine-Tuning (PEFT) for Large Language Models (LLMs) with
Adversarial Data Augmentation and Cultural Knowledge Graph Integration,
designed to significantly enhance zero-shot multicultural name recognition. Our
approach leverages the powerful linguistic understanding of pre-trained LLMs,
transforming the recognition task into a guided generation problem. Through
meticulous prompt engineering, dynamic integration of explicit cultural
knowledge derived from knowledge graphs, and the strategic application of
adversarial data augmentation, we equip the LLM with an unprecedented ability
to infer the cultural origin of unseen names. Extensive experiments demonstrate
that our PEFT method consistently outperforms established deep learning
baselines, including advanced Bi-LSTM models with cultural tags, achieving an
impressive 93.1\% overall accuracy and a remarkable 89.5\% accuracy on
challenging zero-shot name identification. An in-depth ablation study confirms
the synergistic contribution of each component, while a human evaluation
highlights our method's performance approaching human expert judgment. This
work signifies a substantial leap in multicultural name recognition, offering a
highly effective and scalable solution for real-world applications.

</details>


### [93] [SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding](https://arxiv.org/abs/2507.04189)
*Runcong Zhao,Qinglin Zhu,Hainiu Xu,Bin Liang,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: SymbolicThought is a framework that combines LLM-based extraction with symbolic reasoning to improve the accuracy and consistency of character relationship annotation, offering a practical tool for narrative understanding and explainable AI.


<details>
  <summary>Details</summary>
Motivation: Understanding character relationships is essential for interpreting complex narratives and conducting socially grounded AI research. However, manual annotation is time-consuming and low in coverage, while LLMs often produce hallucinated or logically inconsistent outputs.

Method: SymbolicThought is a human-in-the-loop framework that combines LLM-based extraction with symbolic reasoning. The system constructs editable character relationship graphs, refines them using seven types of logical constraints, and enables real-time validation and conflict resolution through an interactive interface.

Result: Experiments show that SymbolicThought improves annotation accuracy and consistency while significantly reducing time cost.

Conclusion: SymbolicThought improves annotation accuracy and consistency while significantly reducing time cost, offering a practical tool for narrative understanding, explainable AI, and LLM evaluation.

Abstract: Understanding character relationships is essential for interpreting complex
narratives and conducting socially grounded AI research. However, manual
annotation is time-consuming and low in coverage, while large language models
(LLMs) often produce hallucinated or logically inconsistent outputs. We present
SymbolicThought, a human-in-the-loop framework that combines LLM-based
extraction with symbolic reasoning. The system constructs editable character
relationship graphs, refines them using seven types of logical constraints, and
enables real-time validation and conflict resolution through an interactive
interface. To support logical supervision and explainable social analysis, we
release a dataset of 160 interpersonal relationships with corresponding logical
structures. Experiments show that SymbolicThought improves annotation accuracy
and consistency while significantly reducing time cost, offering a practical
tool for narrative understanding, explainable AI, and LLM evaluation.

</details>


### [94] [Context Tuning for In-Context Optimization](https://arxiv.org/abs/2507.04221)
*Jack Lu,Ryan Teehan,Zhenbang Yang,Mengye Ren*

Main category: cs.CL

TL;DR: 本文提出了一种名为Context Tuning的方法，用于增强语言模型的少样本适应能力，该方法通过使用任务特定的示例初始化可训练提示或前缀，从而提高了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于提示的适应方法通常使用与任务无关的标记初始化可训练提示或前缀，而Context Tuning旨在解决这一问题，通过任务特定的示例来初始化提示，以提升模型性能。

Method: Context Tuning通过使用任务特定的示例初始化可训练提示或前缀，利用模型的内在上下文学习（ICL）能力来提取相关信息，从而提高少样本学习性能。

Result: 在CrossFit、UnifiedQA、MMLU、BIG-Bench Hard和ARC等基准测试中，Context Tuning的表现优于传统的基于提示的适应方法，并且达到了与测试时训练相当的准确率，同时具有更高的训练效率。

Conclusion: Context Tuning是一种简单且有效的方法，可以显著增强语言模型（LLMs）的少样本适应能力，而无需微调模型参数。

Abstract: We introduce Context Tuning, a simple and effective method to significantly
enhance few-shot adaptation of language models (LLMs) without fine-tuning model
parameters. While prompt-based adaptation techniques have demonstrated the
effectiveness of lightweight adaptation methods for large language models
(LLMs), they typically initialize a trainable prompt or prefix with irrelevant
tokens for the task at hand. In contrast, Context Tuning initializes the
trainable prompt or prefix with task-specific demonstration examples,
leveraging the model's inherent In-Context Learning (ICL) ability to extract
relevant information for improved few-shot learning performance. Extensive
evaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard,
and ARC demonstrate that Context Tuning outperforms traditional prompt-based
adaptation methods and achieves competitive accuracy to Test-Time Training with
significantly higher training efficiency.

</details>


### [95] [Fairness Evaluation of Large Language Models in Academic Library Reference Services](https://arxiv.org/abs/2507.04224)
*Haining Wang,Jason Clark,Yueru Yan,Star Bradley,Ruiyang Chen,Yiqiong Zhang,Hengyi Fu,Zuoyu Tian*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型在不同用户身份下的回应差异，发现它们在种族和民族上没有表现出差异，仅在女性上存在轻微的刻板偏见。大型语言模型通过语言选择展示了对机构角色的适应，反映了专业规范而非歧视。研究结果表明，当前的大型语言模型在支持学术图书馆参考服务中的公平和情境适当交流方面显示出良好的准备度。


<details>
  <summary>Details</summary>
Motivation: 随着图书馆探索使用大型语言模型（LLMs）用于虚拟参考服务，一个关键问题是：LLMs能否公平地为所有用户服务，无论其人口统计学或社会地位如何？虽然它们提供了可扩展的支持潜力，但LLMs可能会在其训练数据中再现社会偏见，从而危及图书馆对公平服务的承诺。

Method: 我们通过让六个最先进的大型语言模型协助不同性别、种族/民族和机构角色的用户，评估了大型语言模型是否根据用户身份区分回应。

Result: 我们没有发现种族或民族差异的证据，只在一个模型中发现了对女性的刻板偏见的轻微证据。大型语言模型通过使用与正式性、礼貌和领域特定词汇相关的语言选择，展示了对机构角色的细致适应，反映了专业规范而非歧视性对待。

Conclusion: 当前的大型语言模型在支持学术图书馆参考服务中表现出相当程度的准备就绪，能够实现公平且符合情境的交流。

Abstract: As libraries explore large language models (LLMs) for use in virtual
reference services, a key question arises: Can LLMs serve all users equitably,
regardless of demographics or social status? While they offer great potential
for scalable support, LLMs may also reproduce societal biases embedded in their
training data, risking the integrity of libraries' commitment to equitable
service. To address this concern, we evaluate whether LLMs differentiate
responses across user identities by prompting six state-of-the-art LLMs to
assist patrons differing in sex, race/ethnicity, and institutional role. We
found no evidence of differentiation by race or ethnicity, and only minor
evidence of stereotypical bias against women in one model. LLMs demonstrated
nuanced accommodation of institutional roles through the use of linguistic
choices related to formality, politeness, and domain-specific vocabularies,
reflecting professional norms rather than discriminatory treatment. These
findings suggest that current LLMs show a promising degree of readiness to
support equitable and contextually appropriate communication in academic
library reference services.

</details>


### [96] [No Language Data Left Behind: A Comparative Study of CJK Language Datasets in the Hugging Face Ecosystem](https://arxiv.org/abs/2507.04329)
*Dasol Choi,Woomyoung Park,Youngsook Song*

Main category: cs.CL

TL;DR: 本文研究了HuggingFace生态系统中中文、日文和韩文数据集的创建和整理模式，揭示了不同语言社区之间的差异，并提出了改善数据集文档、许可清晰度和跨语言资源共享的策略。


<details>
  <summary>Details</summary>
Motivation: 尽管英语有大量资源和分析，但东亚语言（尤其是中文、日文和韩文）的数据集仍然分散且研究不足，尽管这些语言共同服务于16亿多用户。为了弥补这一差距，我们研究了HuggingFace生态系统，以了解文化规范、研究环境和机构实践如何影响数据集的可用性和质量。

Method: 我们从跨语言的角度研究了HuggingFace生态系统，重点关注文化规范、研究环境和机构实践如何影响数据集的可用性和质量。我们利用超过3,300个数据集，采用定量和定性方法来研究这些因素如何驱动中文、日文和韩文NLP社区的不同创建和整理模式。

Result: 我们的研究结果表明，中文数据集具有大规模且通常由机构驱动的特点，韩文NLP由草根社区主导，而日文数据集则侧重于娱乐和亚文化。

Conclusion: 通过揭示这些模式，我们提出了增强数据集文档、许可清晰度和跨语言资源共享的实用策略，最终指导更有效和文化敏感的东亚大型语言模型开发。我们最后讨论了未来数据集整理和合作的最佳实践，旨在加强所有三种语言的资源开发。

Abstract: Recent advances in Natural Language Processing (NLP) have underscored the
crucial role of high-quality datasets in building large language models (LLMs).
However, while extensive resources and analyses exist for English, the
landscape for East Asian languages - particularly Chinese, Japanese, and Korean
(CJK) - remains fragmented and underexplored, despite these languages together
serving over 1.6 billion speakers. To address this gap, we investigate the
HuggingFace ecosystem from a cross-linguistic perspective, focusing on how
cultural norms, research environments, and institutional practices shape
dataset availability and quality. Drawing on more than 3,300 datasets, we
employ quantitative and qualitative methods to examine how these factors drive
distinct creation and curation patterns across Chinese, Japanese, and Korean
NLP communities. Our findings highlight the large-scale and often
institution-driven nature of Chinese datasets, grassroots community-led
development in Korean NLP, and an entertainment- and subculture-focused
emphasis on Japanese collections. By uncovering these patterns, we reveal
practical strategies for enhancing dataset documentation, licensing clarity,
and cross-lingual resource sharing - ultimately guiding more effective and
culturally attuned LLM development in East Asia. We conclude by discussing best
practices for future dataset curation and collaboration, aiming to strengthen
resource development across all three languages.

</details>


### [97] [HatePRISM: Policies, Platforms, and Research Integration. Advancing NLP for Hate Speech Proactive Mitigation](https://arxiv.org/abs/2507.04350)
*Naquee Rizwan,Seid Muhie Yimam,Daryna Dementieva,Florian Skupin,Tim Fischer,Daniil Moskovskiy,Aarushi Ajay Borkar,Robert Geislinger,Punyajoy Saha,Sarthak Roy,Martin Semmann,Alexander Panchenko,Chris Biemann,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 本文提出了HatePRISM，从国家法规、社交平台政策和NLP研究数据集三个角度全面检查了仇恨言论法规和策略，发现了不同地区和平台在仇恨言论定义和管理上的不一致，并建议探索一个统一的自动仇恨言论管理框架。


<details>
  <summary>Details</summary>
Motivation: 尽管有国家和社会媒体平台的法规，仇恨内容仍然是一个重大挑战。现有的方法主要依赖于反应性措施，如阻止或暂停攻击性信息，而新兴策略则关注于主动措施，如净化和反言论。

Method: 我们从国家法规、社交平台政策和NLP研究数据集三个角度全面检查了仇恨言论法规和策略。

Result: 我们的发现揭示了不同司法管辖区和平台在仇恨言论定义和管理实践方面的显著不一致，以及与研究工作的缺乏一致性。

Conclusion: 我们建议进一步探索一个统一的自动仇恨言论管理框架，结合多种策略。

Abstract: Despite regulations imposed by nations and social media platforms, e.g.
(Government of India, 2021; European Parliament and Council of the European
Union, 2022), inter alia, hateful content persists as a significant challenge.
Existing approaches primarily rely on reactive measures such as blocking or
suspending offensive messages, with emerging strategies focusing on proactive
measurements like detoxification and counterspeech. In our work, which we call
HatePRISM, we conduct a comprehensive examination of hate speech regulations
and strategies from three perspectives: country regulations, social platform
policies, and NLP research datasets. Our findings reveal significant
inconsistencies in hate speech definitions and moderation practices across
jurisdictions and platforms, alongside a lack of alignment with research
efforts. Based on these insights, we suggest ideas and research direction for
further exploration of a unified framework for automated hate speech moderation
incorporating diverse strategies.

</details>


### [98] [Large Language Models' Varying Accuracy in Recognizing Risk-Promoting and Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination and Heated Tobacco Products](https://arxiv.org/abs/2507.04364)
*Soojong Kim,Kwanho Kim,Hye Min Kim*

Main category: cs.CL

TL;DR: 该研究评估了GPT、Gemini和LLAMA等大型语言模型在检测HPV疫苗和加热烟草产品相关情感方面的准确性，发现它们在不同平台上和不同类型的情感上表现不一，且在检测中立信息时面临挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估大型语言模型（LLMs）在检测不同类型的健康情感方面的准确性和可行性，特别是在人乳头瘤病毒（HPV）疫苗和加热烟草产品（HTPs）等关键公共卫生问题上。

Method: 研究基于Facebook和Twitter的数据，收集了支持或反对推荐健康行为的信息，并通过人工标注作为情感分类的黄金标准。

Result: 所有三种LLMs在分类风险促进和健康支持的情感方面表现出较高的准确性，但平台、健康问题和模型类型之间存在显著差异。

Conclusion: 研究结果强调了在公共卫生分析中仔细选择和验证语言模型的重要性，特别是考虑到训练数据中的潜在偏差可能导致LLMs高估或低估某些观点的普遍性。

Abstract: Machine learning methods are increasingly applied to analyze health-related
public discourse based on large-scale data, but questions remain regarding
their ability to accurately detect different types of health sentiments.
Especially, Large Language Models (LLMs) have gained attention as a powerful
technology, yet their accuracy and feasibility in capturing different opinions
and perspectives on health issues are largely unexplored. Thus, this research
examines how accurate the three prominent LLMs (GPT, Gemini, and LLAMA) are in
detecting risk-promoting versus health-supporting sentiments across two
critical public health topics: Human Papillomavirus (HPV) vaccination and
heated tobacco products (HTPs). Drawing on data from Facebook and Twitter, we
curated multiple sets of messages supporting or opposing recommended health
behaviors, supplemented with human annotations as the gold standard for
sentiment classification. The findings indicate that all three LLMs generally
demonstrate substantial accuracy in classifying risk-promoting and
health-supporting sentiments, although notable discrepancies emerge by
platform, health issue, and model type. Specifically, models often show higher
accuracy for risk-promoting sentiment on Facebook, whereas health-supporting
messages on Twitter are more accurately detected. An additional analysis also
shows the challenges LLMs face in reliably detecting neutral messages. These
results highlight the importance of carefully selecting and validating language
models for public health analyses, particularly given potential biases in
training data that may lead LLMs to overestimate or underestimate the
prevalence of certain perspectives.

</details>


### [99] [Does Learning Mathematical Problem-Solving Generalize to Broader Reasoning?](https://arxiv.org/abs/2507.04391)
*Ruochen Zhou,Minrui Xu,Shiqi Chen,Junteng Liu,Yunqi Li,Xinxin Lin,Zhengyu Chen,Junxian He*

Main category: cs.CL

TL;DR: 本文研究了不同数学问题解决训练方法的泛化潜力，发现持续预训练和长思维链结合自我反思能有效提升模型的泛化推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究数学问题解决能力如何泛化到其他推理能力，以探索更有效的模型训练方法。

Method: 对各种数学问题解决训练方法（如持续预训练、指令调优和基于规则的强化学习）进行实证研究，并评估其在不同数据源上的泛化潜力。

Result: 持续预训练在数学文本上能部分泛化到一般推理任务；指令调优在常规短数学问题样本上效果有限，甚至可能损害泛化性能；使用长思维链响应和基于规则的强化学习显著提高了泛化能力。

Conclusion: 传统学习数学问题解决的方法在长推理链上效果不佳，而较长的推理链结合自我反思为提高泛化推理能力提供了有希望的方向。

Abstract: There has been a growing interest in enhancing the mathematical
problem-solving (MPS) capabilities of large language models. While the majority
of research efforts concentrate on creating specialized models to solve
mathematical problems, it remains unknown how learning mathematical
problem-solving generalizes to help develop other reasoning abilities. In this
paper, we present an empirical investigation into the generalization potential
of various MPS training approaches, such as continual pretraining, instruction
tuning, and rule-based reinforcement learning across various data sources,
including both short and long chain-of-thought (CoT) samples. Evaluation on 5
mathematical and 8 general reasoning benchmarks show that continual pretraining
on math text is able to generalize to general reasoning tasks to some extent.
In constrast, instruction tuning on conventional, short MPS samples provides
limited benefits and, in many cases, even impairs generalization performance.
Notably, training with long CoT responses for MPS samples and incorporating
rule-based reinforcement learning on MPS queries exhibit distinct behavior,
significantly enhancing generalization by extending the model's reasoning
processes into other domains. These results suggest that traditional approaches
to learning MPS with short reasoning chains largely fail to achieve robust
generalization. However, the emerging paradigm of longer reasoning chains,
coupled with self-reflection, offers a promising direction for improving
generalized reasoning abilities through learning from specialized domains.

</details>


### [100] [SpiritRAG: A Q&A System for Religion and Spirituality in the United Nations Archive](https://arxiv.org/abs/2507.04395)
*Yingqiang Gao,Fabian Winiger,Patrick Montjourides,Anastassia Shaitarova,Nianlong Gu,Simon Peng-Keller,Gerold Schneider*

Main category: cs.CL

TL;DR: SpiritRAG is a Q&A system designed to help researchers and policymakers extract insights about religion and spirituality from large datasets.


<details>
  <summary>Details</summary>
Motivation: Religion and spirituality are complex and context-specific concepts that are difficult to operationalize in conventional archival search strategies, especially with large, poorly accessible datasets.

Method: SpiritRAG is an interactive Question Answering (Q&A) system based on Retrieval-Augmented Generation (RAG), built using 7,500 United Nations (UN) resolution documents related to R/S in the domains of health and education.

Result: A pilot test and evaluation with domain experts on 100 manually composed questions demonstrates the practical value and usefulness of SpiritRAG.

Conclusion: SpiritRAG demonstrates practical value and usefulness for researchers and policymakers in extracting insights related to religion and spirituality from large datasets.

Abstract: Religion and spirituality (R/S) are complex and highly domain-dependent
concepts which have long confounded researchers and policymakers. Due to their
context-specificity, R/S are difficult to operationalize in conventional
archival search strategies, particularly when datasets are very large, poorly
accessible, and marked by information noise. As a result, considerable time
investments and specialist knowledge is often needed to extract actionable
insights related to R/S from general archival sources, increasing reliance on
published literature and manual desk reviews. To address this challenge, we
present SpiritRAG, an interactive Question Answering (Q&A) system based on
Retrieval-Augmented Generation (RAG). Built using 7,500 United Nations (UN)
resolution documents related to R/S in the domains of health and education,
SpiritRAG allows researchers and policymakers to conduct complex,
context-sensitive database searches of very large datasets using an easily
accessible, chat-based web interface. SpiritRAG is lightweight to deploy and
leverages both UN documents and user provided documents as source material. A
pilot test and evaluation with domain experts on 100 manually composed
questions demonstrates the practical value and usefulness of SpiritRAG.

</details>


### [101] [THM@SimpleText 2025 -- Task 1.1: Revisiting Text Simplification based on Complex Terms for Non-Experts](https://arxiv.org/abs/2507.04414)
*Nico Hofmann,Julian Dauenhauer,Nils Ole Dietzler,Idehen Daniel Idahor,Christin Katharina Kreutz*

Main category: cs.CL

TL;DR: 本文研究了如何通过识别复杂术语并使用小型Gemini和OpenAI大语言模型来简化科学文本，以提高其可访问性。


<details>
  <summary>Details</summary>
Motivation: 使非专业读者更容易理解科学文本，例如政治家和严重疾病患者的家庭成员。

Method: 识别复杂术语并使用小型Gemini和OpenAI大语言模型进行重述

Result: 能够简化复杂句子，使非专业读者更容易理解科学文本。

Conclusion: 通过识别复杂术语并使用小型Gemini和OpenAI大语言模型进行重述，可以提高科学文本的可访问性。

Abstract: Scientific text is complex as it contains technical terms by definition.
Simplifying such text for non-domain experts enhances accessibility of
innovation and information. Politicians could be enabled to understand new
findings on topics on which they intend to pass a law, or family members of
seriously ill patients could read about clinical trials. The SimpleText CLEF
Lab focuses on exactly this problem of simplification of scientific text. Task
1.1 of the 2025 edition specifically handles the simplification of complex
sentences, so very short texts with little context. To tackle this task we
investigate the identification of complex terms in sentences which are
rephrased using small Gemini and OpenAI large language models for non-expert
readers.

</details>


### [102] [MOMENTS: A Comprehensive Multimodal Benchmark for Theory of Mind](https://arxiv.org/abs/2507.04415)
*Emilio Villa-Cueva,S M Masrur Ahmed,Rendi Chevi,Jan Christian Blaise Cruz,Kareem Elzeky,Fermin Cristobal,Alham Fikri Aji,Skyler Wang,Rada Mihalcea,Thamar Solorio*

Main category: cs.CL

TL;DR: 本文介绍了MOMENTS基准，用于评估多模态大型语言模型在理论心智方面的表现，强调了当前系统在整合视觉模态方面的不足。


<details>
  <summary>Details</summary>
Motivation: 理解理论心智对于构建能够感知和解释人类行为的社会智能多模态代理至关重要。

Method: 引入MOMENTS（多模态心理状态）基准，通过短片中的现实、叙事丰富的场景来评估多模态大型语言模型（LLMs）的理论心智能力。

Result: MOMENTS包括超过2,344个选择题，涵盖七个不同的理论心智类别。该基准具有长视频上下文窗口和现实社会互动，提供了对角色心理状态的更深入了解。

Conclusion: 当前系统仍然难以有效整合视觉模态，这突显了进一步研究人工智能对人类行为的多模态理解的必要性。

Abstract: Understanding Theory of Mind is essential for building socially intelligent
multimodal agents capable of perceiving and interpreting human behavior. We
introduce MOMENTS (Multimodal Mental States), a comprehensive benchmark
designed to assess the ToM capabilities of multimodal large language models
(LLMs) through realistic, narrative-rich scenarios presented in short films.
MOMENTS includes over 2,344 multiple-choice questions spanning seven distinct
ToM categories. The benchmark features long video context windows and realistic
social interactions that provide deeper insight into characters' mental states.
While the visual modality generally enhances model performance, current systems
still struggle to integrate it effectively, underscoring the need for further
research into AI's multimodal understanding of human behavior.

</details>


### [103] [RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling](https://arxiv.org/abs/2507.04416)
*Xiuying Wei,Anunay Yadav,Razvan Pascanu,Caglar Gulcehre*

Main category: cs.CL

TL;DR: 本文提出了一种名为 at 的新设计，结合了递归和注意力机制的优点。实验表明，at 在长上下文设置中显著提高了训练和生成速度，同时保持了高准确性。此外，混合架构进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: Transformer 模型依赖于 softmax 注意力，这在长上下文设置中是一个主要的计算瓶颈。本文旨在引入一种新的设计，以克服这一问题，并提高模型的效率和性能。

Method: at 是一种介于递归和注意力机制之间的中间设计。它将输入划分为块，在每个块内应用简单的线性递归以捕获局部依赖关系，然后在块之间进行 softmax 注意力以建模长程交互。此外，还提出了一种混合架构，将 at 与局部注意力交错。

Result: 当块大小为 16 时，at 层在 100K 标记序列的训练速度上提高了 7 倍，在 4K 序列长度的生成速度上提高了 9 倍，同时保持了相似或更好的准确性。混合架构在推理速度、缓存内存使用以及多个任务的性能方面都优于标准注意力。

Conclusion: 通过调整块大小，at 使灵活的权衡成为可能，结合了 RNN 和注意力的优点。实验表明，at 层在训练速度和生成速度上都有显著提升，并且在准确性方面保持或超过标准注意力。此外，混合架构通过结合高效的长距离建模和强大的局部交互，提高了推理速度并减少了缓存内存使用，同时在多个任务中表现出更好的性能。

Abstract: Transformers have become the cornerstone of modern large-scale language
models; however, their dependence on softmax attention poses a major
computational bottleneck, particularly in long-context settings. In this work,
rather than following prevalent approaches such as linear attention (or SSMs)
and local attention, we introduce an intermediate design called \rat between
recurrence and attention mechanisms. It partitions the input into chunks,
applies a simple linear recurrence within each chunk to capture local
dependencies, and then performs softmax attention across chunks to model
long-range interactions. By adjusting the size of the chunk, \rat enables
flexible trade-offs, combining the strengths of RNN and attention. Empirically,
with a chunk size of 16, the \rat layer achieves a \(7\times\) improvement in
training speed with 100K token sequences and \(9\times\) in generation at 4K
sequence length, while maintaining similar or sometimes even better accuracy
compared to standard attention. We demonstrate this by training 1.3B parameter
models from scratch and performing large-scale evaluations, including short-
and long-context benchmarks, as well as supervised fine-tuning~(SFT). We
further propose a hybrid architecture that interleaves \rat with local
attention. By combining efficient long-range modeling with strong local
interactions, this hybrid design not only improves inference speed and reduces
cache memory usage compared to attention, but also consistently enhances
performance, for example, achieving an average 1 point gain in commonsense
reasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase
in a summarization SFT task. Code is available at
https://github.com/CLAIRE-Labo/RAT

</details>


### [104] [GradOT: Training-free Gradient-preserving Offsite-tuning for Large Language Models](https://arxiv.org/abs/2507.04455)
*Kai Yao,Zhaorui Tan,Penglei Gao,Lichun Li,Kaixin Wu,Yinggui Wang,Yuan Zhao,Yixin Ji,Wei Wang,Jianke Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于梯度保留压缩的新型离线微调方法GradOT，能够有效提升隐私保护和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线微调方法计算成本高且缺乏理论分析，需要一种更高效且理论支持的解决方案来解决隐私风险问题。

Method: 通过优化视角分析离线微调问题，提出了一种选择性应用压缩技术（如秩压缩和通道剪枝）的方法，以保留微调适配器的梯度并确保隐私。

Result: 实验表明，本文方法在隐私保护和模型性能方面均优于现有离线微调方法，并提供了一种无需训练的实用解决方案。

Conclusion: 本文提出了一种基于梯度保留压缩的新型离线微调方法GradOT，该方法在隐私保护和模型性能方面优于现有的离线微调方法，并为离线微调提供了理论基础。

Abstract: The rapid growth of large language models (LLMs) with traditional centralized
fine-tuning emerges as a key technique for adapting these models to
domain-specific challenges, yielding privacy risks for both model and data
owners. One promising solution, called offsite-tuning (OT), is proposed to
address these challenges, where a weaker emulator is compressed from the
original model and further fine-tuned with adapter to enhance privacy. However,
the existing OT-based methods require high computational costs and lack
theoretical analysis. This paper introduces a novel OT approach based on
gradient-preserving compression, named GradOT. By analyzing the OT problem
through the lens of optimization, we propose a method that selectively applies
compression techniques such as rank compression and channel pruning, preserving
the gradients of fine-tuned adapters while ensuring privacy. Extensive
experiments demonstrate that our approach surpasses existing OT methods, both
in terms of privacy protection and model performance. Our method provides a
theoretical foundation for OT and offers a practical, training-free solution
for offsite-tuning of large-scale LLMs.

</details>


### [105] [Think Twice Before You Judge: Mixture of Dual Reasoning Experts for Multimodal Sarcasm Detection](https://arxiv.org/abs/2507.04458)
*Soumyadeep Jana,Abhrajyoti Kundu,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 本文提出MiDRE模型，通过结合内部和外部推理专家以及自适应门控机制，提升了多模态讽刺检测的性能。


<details>
  <summary>Details</summary>
Motivation: Existing models struggle to capture the deeper rationale behind sarcasm, relying mainly on shallow cues like image captions or object-attribute pairs from images.

Method: MiDRE integrates an internal reasoning expert for detecting incongruities within the image-text pair and an external reasoning expert that utilizes structured rationales generated via Chain-of-Thought prompting to a Large Vision-Language Model. An adaptive gating mechanism dynamically weighs the two experts.

Result: Experiments on two benchmark datasets show that MiDRE achieves superior performance over baselines. Various qualitative analyses highlight the crucial role of external rationales, revealing that even when they are occasionally noisy, they provide valuable cues that guide the model toward a better understanding of sarcasm.

Conclusion: MiDRE achieves superior performance over baselines and demonstrates the importance of external rationales in understanding sarcasm.

Abstract: Multimodal sarcasm detection has attracted growing interest due to the rise
of multimedia posts on social media. Understanding sarcastic image-text posts
often requires external contextual knowledge, such as cultural references or
commonsense reasoning. However, existing models struggle to capture the deeper
rationale behind sarcasm, relying mainly on shallow cues like image captions or
object-attribute pairs from images. To address this, we propose \textbf{MiDRE}
(\textbf{Mi}xture of \textbf{D}ual \textbf{R}easoning \textbf{E}xperts), which
integrates an internal reasoning expert for detecting incongruities within the
image-text pair and an external reasoning expert that utilizes structured
rationales generated via Chain-of-Thought prompting to a Large Vision-Language
Model. An adaptive gating mechanism dynamically weighs the two experts,
selecting the most relevant reasoning path. Experiments on two benchmark
datasets show that MiDRE achieves superior performance over baselines. Various
qualitative analyses highlight the crucial role of external rationales,
revealing that even when they are occasionally noisy, they provide valuable
cues that guide the model toward a better understanding of sarcasm.

</details>


### [106] [Dual Modality-Aware Gated Prompt Tuning for Few-Shot Multimodal Sarcasm Detection](https://arxiv.org/abs/2507.04468)
*Soumyadeep Jana,Abhrajyoti Kundu,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 本文提出了一种新的少样本多模态讽刺检测框架DMDP，该框架通过门控、模态特定的深度提示和跨模态提示对齐模块，提高了模型在少样本和低资源环境下的性能，并展示了其在不同领域的良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型严重依赖大型标注数据集，这使得它们在标签数据稀缺的真实场景中不太适用。因此，需要在少量样本设置中探索这个问题。

Method: DMDP（Deep Modality-Disentangled Prompt Tuning）是一种新颖的框架，用于少样本多模态讽刺检测。它使用门控、模态特定的深度提示，分别注入文本和视觉编码器的多个层，以实现分层特征学习，并通过提示共享机制增强模态内学习，同时引入跨模态提示对齐模块来改善图像和文本表示之间的细微互动。

Result: 实验表明，DMDP在两个公开数据集上的少样本和极端低资源设置中表现优越。跨数据集评估显示，DMDP在不同领域中具有良好的泛化能力，始终优于基线方法。

Conclusion: DMDP在少量样本和极低资源设置下表现出色，并且在跨数据集评估中显示出良好的泛化能力，始终优于基线方法。

Abstract: The widespread use of multimodal content on social media has heightened the
need for effective sarcasm detection to improve opinion mining. However,
existing models rely heavily on large annotated datasets, making them less
suitable for real-world scenarios where labeled data is scarce. This motivates
the need to explore the problem in a few-shot setting. To this end, we
introduce DMDP (Deep Modality-Disentangled Prompt Tuning), a novel framework
for few-shot multimodal sarcasm detection. Unlike prior methods that use
shallow, unified prompts across modalities, DMDP employs gated,
modality-specific deep prompts for text and visual encoders. These prompts are
injected across multiple layers to enable hierarchical feature learning and
better capture diverse sarcasm types. To enhance intra-modal learning, we
incorporate a prompt-sharing mechanism across layers, allowing the model to
aggregate both low-level and high-level semantic cues. Additionally, a
cross-modal prompt alignment module enables nuanced interactions between image
and text representations, improving the model's ability to detect subtle
sarcastic intent. Experiments on two public datasets demonstrate DMDP's
superior performance in both few-shot and extremely low-resource settings.
Further cross-dataset evaluations show that DMDP generalizes well across
domains, consistently outperforming baseline methods.

</details>


### [107] [Unveiling the Potential of Diffusion Large Language Model in Controllable Generation](https://arxiv.org/abs/2507.04504)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为S^3的新框架，用于改进扩散语言模型在可控文本生成中的表现，通过引入目标模式结构来减少不必要的计算并提高可控性。实验表明，该方法在结构符合度、内容保真度和幻觉率方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型应用在可控生成方面面临重大挑战：原生的多步骤去噪过程对序列长度高度敏感，幻觉率高，且没有专门优化时推断成本过高。

Method: 我们提出了S^3，这是一种新颖的框架，使扩散语言模型能够生成结构化输出（例如JSON），同时保持语义保真度并加速推理。

Result: S^3实现了显著的改进：结构符合度提高了65%，内容保真度提高了48%，幻觉率降低了17%。

Conclusion: 这些结果为在可控文本生成任务中部署扩散模型建立了理论基础和实践路径。

Abstract: Diffusion models, originally developed for image generation, have emerged as
a promising alternative to autoregressive large language models (LLMs). We
present a theoretical analysis comparing autoregressive and masked diffusion
LLMs, revealing that the intrinsic bidirectional attention mechanism of
diffusion LLMs (dLLMs) enables superior context modeling and generation
controllability. However, existing dLLM applications face significant
challenges in controllable generation: the native multi-step denoising process
exhibits high sensitivity to sequence length, elevated hallucination rates, and
prohibitive inference costs without specialized optimizations. To address these
limitations, we propose \textbf{S}elf-adaptive \textbf{S}chema
\textbf{S}caffolding ($S^3$), a novel framework that enables dLLMs to generate
structured outputs (e.g., JSON) while maintaining semantic fidelity and
accelerating inference. Our approach injects the target schema structure into
the output context, reducing unnecessary computation while improving
controllability. Extensive experiments demonstrate that $S^3$ achieves
substantial improvements: 65\% increase in structural adherence, 48\%
enhancement in content fidelity, and 17\% reduction in hallucination rates
compared to baseline. These results establish both theoretical foundations and
practical pathways for deploying diffusion models in controllable text
generation tasks. Code and data will be publicly released.

</details>


### [108] [AdS: Adapter-state Sharing Framework for Multimodal Sarcasm Detection](https://arxiv.org/abs/2507.04508)
*Soumyadeep Jana,Sahil Danayak,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: AdS is a lightweight framework for sarcasm detection that uses fewer parameters than existing methods.


<details>
  <summary>Details</summary>
Motivation: The growing prevalence of multimodal image-text sarcasm on social media poses challenges for opinion mining, especially under resource constraints. Existing approaches rely on full fine-tuning of large pre-trained models, making them unsuitable for low-resource settings. While recent parameter-efficient fine-tuning (PEFT) methods offer promise, their off-the-shelf use underperforms on complex tasks like sarcasm detection.

Method: AdS (Adapter-State Sharing), a lightweight framework built on CLIP that inserts adapters only in the upper layers and introduces a novel adapter-state sharing mechanism, where textual adapters guide visual ones.

Result: Experiments on two public benchmarks demonstrate that AdS achieves state-of-the-art results using significantly fewer trainable parameters than existing PEFT and full fine-tuning approaches.

Conclusion: AdS achieves state-of-the-art results using significantly fewer trainable parameters than existing PEFT and full fine-tuning approaches.

Abstract: The growing prevalence of multimodal image-text sarcasm on social media poses
challenges for opinion mining, especially under resource constraints. Existing
approaches rely on full fine-tuning of large pre-trained models, making them
unsuitable for low-resource settings. While recent parameter-efficient
fine-tuning (PEFT) methods offer promise, their off-the-shelf use underperforms
on complex tasks like sarcasm detection. We propose AdS (Adapter-State
Sharing), a lightweight framework built on CLIP that inserts adapters only in
the upper layers and introduces a novel adapter-state sharing mechanism, where
textual adapters guide visual ones. This design promotes efficient cross-modal
learning while preserving low-level unimodal representations. Experiments on
two public benchmarks demonstrate that AdS achieves state-of-the-art results
using significantly fewer trainable parameters than existing PEFT and full
fine-tuning approaches.

</details>


### [109] [DP-Fusion: Token-Level Differentially Private Inference for Large Language Models](https://arxiv.org/abs/2507.04531)
*Rushil Thareja,Preslav Nakov,Praneeth Vepakomma,Nils Lukas*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) can leak sensitive information from their
context through generated outputs, either accidentally or when prompted
adversarially. Existing defenses that aim to preserve context privacy during
inference either lack formal guarantees or suffer from a poor utility/privacy
trade-off. We propose DP-Fusion, a token-level Differentially Private Inference
(DPI) mechanism that provably bounds how much an LLM's outputs reveal about
sensitive tokens in its context. We demonstrate DPI through the task of
document privatization, where the goal is to paraphrase documents so that
sensitive content (e.g., Personally Identifiable Information, PII) cannot be
reliably inferred, while still preserving the overall utility of the text. This
is controlled by a parameter $\epsilon$: $\epsilon=0$ hides PII entirely, while
higher values trade off privacy for improved paraphrase quality. DP-Fusion
works as follows: (i) partition sensitive tokens into disjoint privacy groups,
(ii) run the LLM once per group, and (iii) blend the output distributions so
that the final output remains within a fixed statistical distance of the
baseline distribution produced when no privacy group is revealed. This approach
allows fine-grained control over the privacy/utility trade-off but requires
multiple LLM forward passes.

</details>


### [110] [Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts](https://arxiv.org/abs/2507.04569)
*Guokan Shang,Hadi Abdine,Ahmad Chamma,Amr Mohamed,Mohamed Anwar,Abdelaziz Bounhar,Omar El Herraoui,Preslav Nakov,Michalis Vazirgiannis,Eric Xing*

Main category: cs.CL

TL;DR: 本文介绍了针对埃及方言的LLMs，采用创新方法提升双脚本语言处理能力，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现代LLM开发中常被忽视的双脚本语言适应问题，提供一种全面的方法论。

Method: 通过利用Branch-Train-MiX策略将专门处理不同脚本的专家合并到一个MoE模型中，提出了新的语言适应方法。

Result: Nile-Chat模型在新引入的埃及评估基准上显著优于现有的多语言和阿拉伯语LLM，如LLaMa、Jais和ALLaM。12B模型在拉丁语脚本基准上比Qwen2.5-14B-Instruct高出14.4%。

Conclusion: 本文提出了针对埃及方言的LLMs，能够理解和生成使用阿拉伯语和拉丁语书写的文本，并展示了这些模型在新引入的埃及评估基准上的优越性能。

Abstract: We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for
Egyptian dialect, uniquely designed to understand and generate texts written in
both Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we
introduce a novel language adaptation approach by leveraging the
Branch-Train-MiX strategy to merge script-specialized experts, into a single
MoE model. Our Nile-Chat models significantly outperform leading multilingual
and Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced
Egyptian evaluation benchmarks, which span both understanding and generative
tasks. Notably, our 12B model yields a 14.4% performance gain over
Qwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly
available. We believe this work presents a comprehensive methodology for
adapting LLMs to dual-script languages, addressing an often overlooked aspect
in modern LLM development.

</details>


### [111] [PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes](https://arxiv.org/abs/2507.04607)
*Xinliang Frederick Zhang,Nick Beauchamp,Lu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为PRIME的统一框架，结合情景记忆和语义记忆机制，并引入了个性化思考能力，以提升大型语言模型的个性化效果。同时，引入了一个专门用于评估长上下文个性化的数据集。


<details>
  <summary>Details</summary>
Motivation: 缺乏一个统一的理论框架来系统地理解有效个性化的驱动因素，以及缺少适合的基准测试数据集。

Method: 将认知双记忆模型整合到LLM个性化中，通过镜像情景记忆到历史用户互动和语义记忆到长期演变的用户信念，提出了一种统一框架PRIME，并引入了受慢思考策略启发的个性化思考能力。

Result: PRIME在长短期上下文场景中表现出色，能够有效捕捉动态个性化，超越了单纯的流行度偏差。

Conclusion: PRIME的有效性在长短期上下文场景中得到了验证，并且能够超越单纯的流行度偏差来捕捉动态个性化。

Abstract: Large language model (LLM) personalization aims to align model outputs with
individuals' unique preferences and opinions. While recent efforts have
implemented various personalization methods, a unified theoretical framework
that can systematically understand the drivers of effective personalization is
still lacking. In this work, we integrate the well-established cognitive
dual-memory model into LLM personalization, by mirroring episodic memory to
historical user engagements and semantic memory to long-term, evolving user
beliefs. Specifically, we systematically investigate memory instantiations and
introduce a unified framework, PRIME, using episodic and semantic memory
mechanisms. We further augment PRIME with a novel personalized thinking
capability inspired by the slow thinking strategy. Moreover, recognizing the
absence of suitable benchmarks, we introduce a dataset using Change My View
(CMV) from Reddit, specifically designed to evaluate long-context
personalization. Extensive experiments validate PRIME's effectiveness across
both long- and short-context scenarios. Further analysis confirms that PRIME
effectively captures dynamic personalization beyond mere popularity biases.

</details>


### [112] [Retain or Reframe? A Computational Framework for the Analysis of Framing in News Articles and Reader Comments](https://arxiv.org/abs/2507.04612)
*Matteo Guida,Yulia Otmakhova,Eduard Hovy,Lea Frermann*

Main category: cs.CL

TL;DR: 本文提出了一种计算框架，用于大规模分析新闻文章和读者评论中的框架。研究发现，评论中的框架重用在不同媒体之间高度相关，而主题特定模式则有所不同。


<details>
  <summary>Details</summary>
Motivation: 尽管框架影响公众对这些问题的解释，但受众不会被动地吸收框架，而是主动重新组织所提供的信息。然而，NLP方法通常忽略这一关系，孤立地检测文章和回应中的框架。

Method: 我们改进了框架标签，并开发了一个框架，从句子级预测中重建文章和评论中的主导框架，并将文章与主题相关的评论对齐。

Result: 我们在十一项主题和两家新闻机构上应用了我们的框架，发现评论中的框架重用在不同媒体之间高度相关，而主题特定模式则有所不同。

Conclusion: 我们提出了一个计算框架，用于大规模分析源内容（新闻文章）和受众反应（读者评论）中的框架。我们的研究发现，评论中的框架重用在不同媒体之间高度相关，而主题特定模式则有所不同。

Abstract: When a news article describes immigration as an "economic burden" or a
"humanitarian crisis," it selectively emphasizes certain aspects of the issue.
Although \textit{framing} shapes how the public interprets such issues,
audiences do not absorb frames passively but actively reorganize the presented
information. While this relationship between source content and audience
response is well-documented in the social sciences, NLP approaches often ignore
it, detecting frames in articles and responses in isolation. We present the
first computational framework for large-scale analysis of framing across source
content (news articles) and audience responses (reader comments).
Methodologically, we refine frame labels and develop a framework that
reconstructs dominant frames in articles and comments from sentence-level
predictions, and aligns articles with topically relevant comments. Applying our
framework across eleven topics and two news outlets, we find that frame reuse
in comments correlates highly across outlets, while topic-specific patterns
vary. We release a frame classifier that performs well on both articles and
comments, a dataset of article and comment sentences manually labeled for
frames, and a large-scale dataset of articles and comments with predicted frame
labels.

</details>


### [113] [Knowledge-Aware Self-Correction in Language Models via Structured Memory Graphs](https://arxiv.org/abs/2507.04625)
*Swayamjit Saha*

Main category: cs.CL

TL;DR: 本文提出了一种基于结构化记忆图的轻量级、可解释的框架，用于纠正大型语言模型的错误输出。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易产生事实错误，即所谓的幻觉，因此需要一种方法来纠正这些错误。

Method: 我们提出了一种轻量级、可解释的框架，利用基于RDF三元组的结构化记忆图进行知识感知的自我修正。

Result: 我们在DistilGPT-2上展示了这种方法，并在简单的事实提示上取得了有希望的结果。

Conclusion: 我们的方法在简单的事实提示上展示了有希望的结果，表明了基于结构化记忆图的自我修正框架的有效性。

Abstract: Large Language Models (LLMs) are powerful yet prone to generating factual
errors, commonly referred to as hallucinations. We present a lightweight,
interpretable framework for knowledge-aware self-correction of LLM outputs
using structured memory graphs based on RDF triples. Without retraining or
fine-tuning, our method post-processes model outputs and corrects factual
inconsistencies via external semantic memory. We demonstrate the approach using
DistilGPT-2 and show promising results on simple factual prompts.

</details>


### [114] [Put Teacher in Student's Shoes: Cross-Distillation for Ultra-compact Model Compression Framework](https://arxiv.org/abs/2507.04636)
*Maolin Wang,Jun Chu,Sicong Xie,Xiaoling Zang,Yao Zhao,Wenliang Zhong,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 本文介绍了EI-BERT框架，通过跨蒸馏等方法实现超紧凑的NLP模型，并成功应用于支付宝生态系统。


<details>
  <summary>Details</summary>
Motivation: 在移动计算时代，资源受限的边缘环境中部署高效的自然语言处理（NLP）模型面临重大挑战，尤其是在需要严格隐私合规性、实时响应和多样化多任务能力的环境中。

Method: EI-BERT通过硬令牌剪枝、跨蒸馏和参数量化等综合流程高效压缩模型，其中跨蒸馏方法独特地让教师模型从学生的视角理解知识转移。

Result: EI-BERT实现了仅1.91 MB的紧凑BERT模型，这是目前自然语言理解（NLU）任务中最小的模型。

Conclusion: EI-BERT框架成功部署在支付宝生态系统中，并在实际应用中表现出显著改进。

Abstract: In the era of mobile computing, deploying efficient Natural Language
Processing (NLP) models in resource-restricted edge settings presents
significant challenges, particularly in environments requiring strict privacy
compliance, real-time responsiveness, and diverse multi-tasking capabilities.
These challenges create a fundamental need for ultra-compact models that
maintain strong performance across various NLP tasks while adhering to
stringent memory constraints. To this end, we introduce Edge ultra-lIte BERT
framework (EI-BERT) with a novel cross-distillation method. EI-BERT efficiently
compresses models through a comprehensive pipeline including hard token
pruning, cross-distillation and parameter quantization. Specifically, the
cross-distillation method uniquely positions the teacher model to understand
the student model's perspective, ensuring efficient knowledge transfer through
parameter integration and the mutual interplay between models. Through
extensive experiments, we achieve a remarkably compact BERT-based model of only
1.91 MB - the smallest to date for Natural Language Understanding (NLU) tasks.
This ultra-compact model has been successfully deployed across multiple
scenarios within the Alipay ecosystem, demonstrating significant improvements
in real-world applications. For example, it has been integrated into Alipay's
live Edge Recommendation system since January 2024, currently serving the app's
recommendation traffic across \textbf{8.4 million daily active devices}.

</details>


### [115] [R1-RE: Cross-Domain Relationship Extraction with RLVR](https://arxiv.org/abs/2507.04642)
*Runpeng Dai,Tong Zheng,Run Yang,Hongtu Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于可验证奖励的强化学习框架R1-RE，用于关系抽取任务，显著提高了跨领域鲁棒性，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法在跨领域泛化方面表现不佳，因此我们受到人类注释者工作流程的启发，重新定义关系抽取为一个推理任务。

Method: 我们将关系抽取重新定义为由注释指南指导的推理任务，并引入了R1-RE，这是第一个基于可验证奖励的强化学习框架（RLVR）用于关系抽取任务。

Result: R1-RE-7B模型在公共Sem-2010数据集和私有MDKG数据集上取得了大约70%的平均OOD准确率，与领先的专有模型如GPT-4o相当。

Conclusion: 我们的方法展示了在关系抽取任务中使用基于可验证奖励的强化学习框架的有效性，并提供了对训练动态和涌现推理行为的新见解。

Abstract: Relationship extraction (RE) is a core task in natural language processing.
Traditional approaches typically frame RE as a supervised learning problem,
directly mapping context to labels-an approach that often suffers from poor
out-of-domain (OOD) generalization. Inspired by the workflow of human
annotators, we reframe RE as a reasoning task guided by annotation guidelines
and introduce R1-RE, the first reinforcement learning with verifiable reward
(RLVR) framework for RE tasks. Our method elicits the reasoning abilities of
small language models for annotation tasks, resulting in significantly improved
OOD robustness. We evaluate our approach on the public Sem-2010 dataset and a
private MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of
approximately 70%, on par with leading proprietary models such as GPT-4o.
Additionally, our comprehensive analysis provides novel insights into the
training dynamics and emergent reasoning behaviors of the RLVR paradigm for RE.

</details>


### [116] [XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL](https://arxiv.org/abs/2507.04701)
*Yifu Liu,Yin Zhu,Yingqi Gao,Zhiling Luo,Xiaoxia Li,Xiaorong Shi,Yuntao Hong,Jinyang Gao,Yu Li,Bolin Ding,Jingren Zhou*

Main category: cs.CL

TL;DR: XiYan-SQL is a framework that generates and utilizes multiple SQL candidates through a Schema Filter module, a multi-generator ensemble approach, and a selection model with a candidate reorganization strategy. It achieves SOTA performance on the BIRD and Spider benchmarks.


<details>
  <summary>Details</summary>
Motivation: To leverage the advantages of LLM in addressing challenges in the Text-to-SQL task, XiYan-SQL is presented as an innovative framework for effectively generating and utilizing multiple SQL candidates.

Method: XiYan-SQL is a framework that consists of three components: a Schema Filter module, a multi-generator ensemble approach, and a selection model with a candidate reorganization strategy. The multi-generator ensemble employs a multi-task fine-tuning strategy to enhance the capabilities of SQL generation models for the intrinsic alignment between SQL and text, and constructs multiple generation models with distinct generation styles by fine-tuning across different SQL formats.

Result: The experimental results and comprehensive analysis demonstrate the effectiveness and robustness of the framework. XiYan-SQL achieves a new SOTA performance of 75.63% on the notable BIRD benchmark and attains SOTA performance on the Spider test set with an accuracy of 89.65%.

Conclusion: XiYan-SQL achieves a new SOTA performance of 75.63% on the notable BIRD benchmark and attains SOTA performance on the Spider test set with an accuracy of 89.65%.

Abstract: To leverage the advantages of LLM in addressing challenges in the Text-to-SQL
task, we present XiYan-SQL, an innovative framework effectively generating and
utilizing multiple SQL candidates. It consists of three components: 1) a Schema
Filter module filtering and obtaining multiple relevant schemas; 2) a
multi-generator ensemble approach generating multiple highquality and diverse
SQL queries; 3) a selection model with a candidate reorganization strategy
implemented to obtain the optimal SQL query. Specifically, for the
multi-generator ensemble, we employ a multi-task fine-tuning strategy to
enhance the capabilities of SQL generation models for the intrinsic alignment
between SQL and text, and construct multiple generation models with distinct
generation styles by fine-tuning across different SQL formats. The experimental
results and comprehensive analysis demonstrate the effectiveness and robustness
of our framework. Overall, XiYan-SQL achieves a new SOTA performance of 75.63%
on the notable BIRD benchmark, surpassing all previous methods. It also attains
SOTA performance on the Spider test set with an accuracy of 89.65%.

</details>


### [117] [Why We Feel What We Feel: Joint Detection of Emotions and Their Opinion Triggers in E-commerce](https://arxiv.org/abs/2507.04708)
*Arnav Attri,Anuj Attri,Pushpak Bhattacharyya,Suman Banerjee,Amey Patil,Muthusamy Chelliah,Nikesh Garera*

Main category: cs.CL

TL;DR: 本文提出了一种新的联合任务EOT，用于电商评论中的情绪检测和观点触发提取，并引入了EOT-X数据集，评估了23个大型语言模型，提出了一种结构化提示框架EOT-DETECT，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究没有探索电商评论中情绪检测和解释性跨度识别的联合任务，这导致了对触发客户情感反应的因素理解不足。

Method: 本文提出了一个联合任务，将情绪检测和观点触发提取（EOT）统一起来，并引入了EOT-X数据集，同时评估了23个大型语言模型。

Result: EOT-DETECT框架在电商领域表现出色，超越了现有的零样本和思维链技术。

Conclusion: 本文提出了一个结构化的提示框架EOT-DETECT，该框架在电商领域超越了零样本和思维链技术。

Abstract: Customer reviews on e-commerce platforms capture critical affective signals
that drive purchasing decisions. However, no existing research has explored the
joint task of emotion detection and explanatory span identification in
e-commerce reviews - a crucial gap in understanding what triggers customer
emotional responses. To bridge this gap, we propose a novel joint task unifying
Emotion detection and Opinion Trigger extraction (EOT), which explicitly models
the relationship between causal text spans (opinion triggers) and affective
dimensions (emotion categories) grounded in Plutchik's theory of 8 primary
emotions. In the absence of labeled data, we introduce EOT-X, a human-annotated
collection of 2,400 reviews with fine-grained emotions and opinion triggers. We
evaluate 23 Large Language Models (LLMs) and present EOT-DETECT, a structured
prompting framework with systematic reasoning and self-reflection. Our
framework surpasses zero-shot and chain-of-thought techniques, across
e-commerce domains.

</details>


### [118] [LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation framework](https://arxiv.org/abs/2507.04723)
*Zecheng Tang,Haitian Wang,Quantong Qiu,Baibei Ji,Ruoxi Sun,Keyan Zhou,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出LOOM-Scope，一个全面且高效的长上下文评估框架，解决了现有评估基准在设置上的差异性和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的长上下文评估基准在评估设置上存在差异，导致结果不一致，难以进行可靠的比较。此外，长上下文评估的高计算成本也阻碍了社区对长上下文模型进行全面评估。

Method: 本文提出了一种标准化评估设置的方法，支持高效长上下文推理加速方法的部署，并引入了一个全面但轻量的基准套件来评估模型。

Result: LOOM-Scope能够标准化评估设置，支持高效长上下文推理加速方法的部署，并提供了一个全面但轻量的基准套件来评估模型。

Conclusion: 本文提出了LOOM-Scope，一个全面且高效的长上下文评估框架，旨在解决现有评估基准在设置上的差异性和计算成本高的问题。

Abstract: Long-context processing has become a fundamental capability for large
language models~(LLMs). To assess model's long-context performance, numerous
long-context evaluation benchmarks have been proposed. However, variations in
evaluation settings across these benchmarks lead to inconsistent results,
making it difficult to draw reliable comparisons. Besides, the high
computational cost of long-context evaluation poses a significant barrier for
the community to conduct comprehensive assessments of long-context models. In
this paper, we propose LOOM-Scope, a comprehensive and efficient framework for
long-context evaluation. LOOM-Scope standardizes evaluation settings across
diverse benchmarks, supports deployment of efficient long-context inference
acceleration methods, and introduces a holistic yet lightweight benchmark suite
to evaluate models comprehensively. Homepage: https://loomscope.github.io

</details>


### [119] ["This Suits You the Best": Query Focused Comparative Explainable Summarization](https://arxiv.org/abs/2507.04733)
*Arnav Attri,Anuj Attri,Pushpak Bhattacharyya,Suman Banerjee,Amey Patil,Muthusamy Chelliah,Nikesh Garera*

Main category: cs.CL

TL;DR: 本文提出了一种新的生成查询聚焦比较摘要的方法，并展示了其在产品推荐中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统意见摘要往往无法提供全面的比较见解，因此需要一种新的方法来生成更全面的比较摘要。

Method: 本文使用多源意见摘要（M-OS）来生成查询聚焦的比较摘要，并利用大型语言模型（LLMs）生成带有查询特定解释的表格比较摘要。

Result: 实验结果表明，QF-CES-PROMPT在五个维度上与人工判断有0.74的平均斯皮尔曼相关性，显示出其在QF-CES评估中的潜力。

Conclusion: 本文提出了生成查询聚焦的比较可解释摘要（QF-CES）的新任务，并展示了其在产品推荐中的潜力。

Abstract: Product recommendations inherently involve comparisons, yet traditional
opinion summarization often fails to provide holistic comparative insights. We
propose the novel task of generating Query-Focused Comparative Explainable
Summaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address
the lack of query-focused recommendation datasets, we introduce MS-Q2P,
comprising 7,500 queries mapped to 22,500 recommended products with metadata.
We leverage Large Language Models (LLMs) to generate tabular comparative
summaries with query-specific explanations. Our approach is personalized,
privacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS
as an intermediate step reduces inference latency approximately by 40% compared
to the direct input approach (DIA), which processes raw data directly. We
evaluate open-source and proprietary LLMs for generating and assessing QF-CES.
Extensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity,
faithfulness, informativeness, format adherence, and query relevance) showed an
average Spearman correlation of 0.74 with human judgments, indicating its
potential for QF-CES evaluation.

</details>


### [120] [Word stress in self-supervised speech models: A cross-linguistic comparison](https://arxiv.org/abs/2507.04738)
*Martijn Bentum,Louis ten Bosch,Tomas O. Lentz*

Main category: cs.CL

TL;DR: 本研究分析了自监督语音模型（如Wav2vec 2.0）对不同语言中单词重音的表示，发现其具有语言特异性。


<details>
  <summary>Details</summary>
Motivation: 研究自监督语音模型（如Wav2vec 2.0）如何表示单词重音，特别是在不同语言中的表现。

Method: 通过在S3M嵌入上训练诊断性重音分类器，研究了S3M对单词重音的表示，并测试了语言特异性效应。

Result: S3M嵌入可以高精度地区分读出短句中的重音和非重音音节，并且结果显示出语言特异性效应。

Conclusion: 研究结果表明，自监督语音模型（S3M）在不同语言中对单词重音的表示具有语言特异性，变量重音语言与固定重音语言之间的差异更大。

Abstract: In this paper we study word stress representations learned by self-supervised
speech models (S3M), specifically the Wav2vec 2.0 model. We investigate the S3M
representations of word stress for five different languages: Three languages
with variable or lexical stress (Dutch, English and German) and two languages
with fixed or demarcative stress (Hungarian and Polish). We train diagnostic
stress classifiers on S3M embeddings and show that they can distinguish between
stressed and unstressed syllables in read-aloud short sentences with high
accuracy. We also tested language-specificity effects of S3M word stress. The
results indicate that the word stress representations are language-specific,
with a greater difference between the set of variable versus the set of fixed
stressed languages.

</details>


### [121] [A Tale of Two Scripts: Transliteration and Post-Correction for Judeo-Arabic](https://arxiv.org/abs/2507.04746)
*Juan Moreno Gonzalez,Bashar Alhafni,Nizar Habash*

Main category: cs.CL

TL;DR: 本文提出了一种两步方法来自动将犹太阿拉伯语转写成阿拉伯字母，并展示了这种转写如何使阿拉伯NLP工具能够进行形态句法标注和机器翻译。


<details>
  <summary>Details</summary>
Motivation: 犹太阿拉伯语的转写具有挑战性，因为字母映射不明确，拼写规则不一致，并且经常混合使用希伯来语和亚拉姆语。

Method: 本文提出了一种两步方法来自动将犹太阿拉伯语转写成阿拉伯字母：简单的字符级映射，然后进行后校正以解决语法和拼写错误。

Result: 本文提出了第一个对LLMs在此任务上的基准评估，并展示了转写如何使阿拉伯NLP工具能够进行形态句法标注和机器翻译。

Conclusion: 本文展示了将犹太阿拉伯语转写成阿拉伯字母可以使得阿拉伯NLP工具进行形态句法标注和机器翻译，这在原始文本上是不可行的。

Abstract: Judeo-Arabic refers to Arabic variants historically spoken by Jewish
communities across the Arab world, primarily during the Middle Ages. Unlike
standard Arabic, it is written in Hebrew script by Jewish writers and for
Jewish audiences. Transliterating Judeo-Arabic into Arabic script is
challenging due to ambiguous letter mappings, inconsistent orthographic
conventions, and frequent code-switching into Hebrew and Aramaic. In this
paper, we introduce a two-step approach to automatically transliterate
Judeo-Arabic into Arabic script: simple character-level mapping followed by
post-correction to address grammatical and orthographic errors. We also present
the first benchmark evaluation of LLMs on this task. Finally, we show that
transliteration enables Arabic NLP tools to perform morphosyntactic tagging and
machine translation, which would have not been feasible on the original texts.

</details>


### [122] [LLMs as Architects and Critics for Multi-Source Opinion Summarization](https://arxiv.org/abs/2507.04751)
*Anuj Attri,Arnav Attri,Pushpak Bhattacharyya,Suman Banerjee,Amey Patil,Muthusamy Chelliah,Nikesh Garera*

Main category: cs.CL

TL;DR: The paper introduces M-OS-EVAL, a benchmark dataset for evaluating multi-source opinion summaries, and demonstrates that M-OS improves user engagement and performance compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the potential of Large Language Models in Multi-source Opinion Summarization (M-OS) and address the lack of evaluation datasets for this task.

Method: The paper introduces M-OS-EVAL, a benchmark dataset for evaluating multi-source opinion summaries across 7 key dimensions. It also presents experiments comparing M-OS-PROMPTS with previous methodologies.

Result: The results show that M-OS enhances user engagement, with 87% of participants preferring M-OS over traditional opinion summaries. M-OS-PROMPTS achieve an average Spearman correlation of {ho} = 0.74, outperforming previous methodologies.

Conclusion: M-OS significantly enhances user engagement, and factually enriched summaries improve user engagement. M-OS-PROMPTS show stronger alignment with human judgment compared to previous methodologies.

Abstract: Multi-source Opinion Summarization (M-OS) extends beyond traditional opinion
summarization by incorporating additional sources of product metadata such as
descriptions, key features, specifications, and ratings, alongside reviews.
This integration results in comprehensive summaries that capture both
subjective opinions and objective product attributes essential for informed
decision-making. While Large Language Models (LLMs) have shown significant
success in various Natural Language Processing (NLP) tasks, their potential in
M-OS remains largely unexplored. Additionally, the lack of evaluation datasets
for this task has impeded further advancements. To bridge this gap, we
introduce M-OS-EVAL, a benchmark dataset for evaluating multi-source opinion
summaries across 7 key dimensions: fluency, coherence, relevance, faithfulness,
aspect coverage, sentiment consistency, specificity. Our results demonstrate
that M-OS significantly enhances user engagement, as evidenced by a user study
in which, on average, 87% of participants preferred M-OS over opinion
summaries. Our experiments demonstrate that factually enriched summaries
enhance user engagement. Notably, M-OS-PROMPTS exhibit stronger alignment with
human judgment, achieving an average Spearman correlation of \r{ho} = 0.74,
which surpasses the performance of previous methodologies.

</details>


### [123] [CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering](https://arxiv.org/abs/2507.04756)
*Hang Lv,Sheng Liang,Hao Wang,Hongchao Gu,Yaxiong Wu,Wei Guo,Defu Lian,Yong Liu,Enhong Chen*

Main category: cs.CL

TL;DR: CoSteer是一种协作框架，通过本地小模型的logits差异作为云基LLMs的引导信号，在解码时实现个性化。它能够在设备端环境中动态调整远程LLM的logits，从而在保持隐私的同时生成个性化内容。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于集中微调或静态偏好对齐，但在资源受限的个人设备上难以实现实时适应。大型云基模型缺乏对本地用户特定信息的访问，而小型设备模型无法与云基模型的生成质量相匹配。

Method: CoSteer是一种协作框架，通过本地小模型的logits差异作为云基LLMs的引导信号，在解码时实现个性化。具体来说，我们将令牌级优化形式化为在线学习问题，其中本地delta向量在设备端环境中动态调整远程LLM的logits。

Result: 通过各种个性化生成任务的全面实验，我们证明CoSteer能够有效帮助LLMs生成个性化内容，同时通过设备端数据处理确保隐私保护，并保持可接受的计算开销。

Conclusion: CoSteer能够通过本地存储的用户资料和历史记录帮助LLMs生成个性化内容，同时通过设备端数据处理确保隐私保护，并保持可接受的计算开销。

Abstract: Personalized text generation has become crucial for adapting language models
to diverse and evolving users' personal context across cultural, temporal, and
contextual dimensions. While existing methods often rely on centralized
fine-tuning or static preference alignment, they struggle to achieve real-time
adaptation under resource constraints inherent to personal devices. This
limitation creates a dilemma: large cloud-based models lack access to localized
user-specific information, while small on-device models cannot match the
generation quality of their cloud counterparts. To address this dichotomy, we
present CoSteer, a novel collaborative framework that enables decoding-time
personalization through localized delta steering. Our key insight lies in
leveraging the logits difference between personal context-aware and -agnostic
outputs from local small models as steering signals for cloud-based LLMs.
Specifically, we formulate token-level optimization as an online learning
problem, where local delta vectors dynamically adjust the remote LLM's logits
within the on-device environment. This approach preserves privacy by
transmitting only the final steered tokens rather than raw data or intermediate
vectors, while maintaining cloud-based LLMs' general capabilities without
fine-tuning. Through comprehensive experiments on various personalized
generation tasks, we demonstrate that CoSteer effectively assists LLMs in
generating personalized content by leveraging locally stored user profiles and
histories, ensuring privacy preservation through on-device data processing
while maintaining acceptable computational overhead.

</details>


### [124] [Reason to Rote: Rethinking Memorization in Reasoning](https://arxiv.org/abs/2507.04782)
*Yupei Du,Philipp Mondorf,Silvia Casola,Yuekun Yao,Robert Litschko,Barbara Plank*

Main category: cs.CL

TL;DR: 研究发现，语言模型中的标签噪声记忆依赖于推理机制，而不是取代它们，这解释了为何模型仍能有效推理。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型容易记住任意的训练实例，如标签噪声，但它们在推理任务中表现得非常好。我们需要理解为什么这种记忆不会严重影响推理能力。

Method: 我们使用两个可控的合成推理数据集（四数字加法和两跳关系推理）来研究语言模型如何记忆标签噪声，并分析这种记忆如何影响推理能力。

Result: 我们发现模型在检索记忆的噪声标签时，仍然会计算中间推理输出，并且干预推理会损害记忆。此外，记忆是通过分布式编码实现的，而不是构建从输入到噪声标签的查找机制。

Conclusion: 我们的研究发现，语言模型中的标签噪声记忆是建立在推理机制之上的，而不是取代它们。这解释了为什么即使存在记忆，模型仍然能够进行有效的推理。

Abstract: Large language models readily memorize arbitrary training instances, such as
label noise, yet they perform strikingly well on reasoning tasks. In this work,
we investigate how language models memorize label noise, and why such
memorization in many cases does not heavily affect generalizable reasoning
capabilities. Using two controllable synthetic reasoning datasets with noisy
labels, four-digit addition (FDA) and two-hop relational reasoning (THR), we
discover a reliance of memorization on generalizable reasoning mechanisms:
models continue to compute intermediate reasoning outputs even when retrieving
memorized noisy labels, and intervening reasoning adversely affects
memorization. We further show that memorization operates through distributed
encoding, i.e., aggregating various inputs and intermediate results, rather
than building a look-up mechanism from inputs to noisy labels. Moreover, our
FDA case study reveals memorization occurs via outlier heuristics, where
existing neuron activation patterns are slightly shifted to fit noisy labels.
Together, our findings suggest that memorization of label noise in language
models builds on, rather than overrides, the underlying reasoning mechanisms,
shedding lights on the intriguing phenomenon of benign memorization.

</details>


### [125] [A Survey of Pun Generation: Datasets, Evaluations and Methodologies](https://arxiv.org/abs/2507.04793)
*Yuchen Su,Yonghua Zhu,Ruofan Wang,Zijian Huang,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: 本文提供了一项关于双关语生成数据集和方法的全面回顾，并讨论了研究挑战和未来工作方向。


<details>
  <summary>Details</summary>
Motivation: 目前没有专门的调查系统地回顾双关语生成这一特定领域，因此本文旨在填补这一空白。

Method: 本文系统地回顾了双关语生成的不同阶段，包括传统方法、深度学习技术和预训练语言模型。

Result: 本文总结了用于评估双关语生成质量的自动化和人工评估指标。

Conclusion: 本文提供了对双关语生成数据集和方法的全面回顾，并讨论了研究挑战和未来工作的方向。

Abstract: Pun generation seeks to creatively modify linguistic elements in text to
produce humour or evoke double meanings. It also aims to preserve coherence and
contextual appropriateness, making it useful in creative writing and
entertainment across various media and contexts. Although pun generation has
received considerable attention in computational linguistics, there is
currently no dedicated survey that systematically reviews this specific area.
To bridge this gap, this paper provides a comprehensive review of pun
generation datasets and methods across different stages, including conventional
approaches, deep learning techniques, and pre-trained language models.
Additionally, we summarise both automated and human evaluation metrics used to
assess the quality of pun generation. Finally, we discuss the research
challenges and propose promising directions for future work.

</details>


### [126] [Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient Task-Oriented Dialogue Systems](https://arxiv.org/abs/2507.04841)
*Quang-Vinh Nguyen,Quang-Chieu Nguyen,Hoang Pham,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 本文提出了一种名为Spec-TOD的新框架，用于在低资源场景下训练端到端任务导向对话系统，通过引入专门的指令调优大语言模型和高效的训练策略，实现了在减少标签数据需求的情况下取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 任务导向对话（TOD）系统在低资源场景中面临标签数据有限的挑战，因此需要一种能够有效训练端到端TOD系统的框架。

Method: 提出了一种新的专门的端到端TOD框架，该框架结合了显式任务指令以进行指令调优的大语言模型（LLMs），以及一种高效的训练策略，利用轻量级、专门的LLMs以最小的监督实现强大的性能。

Result: 在MultiWOZ数据集上的实验表明，Spec-TOD在显著减少标签数据需求的同时取得了具有竞争力的结果。

Conclusion: 这些发现表明了所提出的框架在低资源环境下推进高效且有效的任务导向对话系统方面的潜力。

Abstract: Task-oriented dialogue (TOD) systems facilitate goal-driven interactions
between users and machines. While recent advances in deep learning have
improved the performance, TOD systems often struggle in low-resource scenarios
with limited labeled data. To address this challenge, we propose Spec-TOD, a
novel framework designed to train an end-to-end TOD system with limited data.
Spec-TOD introduces two main innovations: (i) a novel specialized end-to-end
TOD framework that incorporates explicit task instructions for
instruction-tuned large language models (LLMs), and (ii) an efficient training
strategy that leverages lightweight, specialized LLMs to achieve strong
performance with minimal supervision. Experiments on the MultiWOZ dataset, a
widely used TOD benchmark, demonstrate that Spec-TOD achieves competitive
results while significantly reducing the need for labeled data. These findings
highlight the potential of the proposed framework in advancing efficient and
effective TOD systems in low-resource settings.

</details>


### [127] [Dialogue-Based Multi-Dimensional Relationship Extraction from Novels](https://arxiv.org/abs/2507.04852)
*Yuchen Yan,Hanjie Zhao,Senbin Zhu,Hongde Liu,Zhihong Zhang,Yuxiang Jia*

Main category: cs.CL

TL;DR: 本研究提出了一种基于大型语言模型的关系抽取方法，通过结合关系维度分离、对话数据构建和上下文学习策略，提升了模型在复杂小说文本中的关系抽取性能，并构建了一个高质量的中文小说关系抽取数据集。


<details>
  <summary>Details</summary>
Motivation: 小说文本中的复杂上下文和隐式表达给自动角色关系抽取带来了显著挑战，而现有的标注资源不足也限制了相关研究的发展。

Method: 本研究结合了关系维度分离、对话数据构建和上下文学习策略，利用对话结构信息提升模型对隐式关系的理解能力。

Result: 实验结果表明，该方法在多个评估指标上优于传统基线，并成功支持了小说中角色关系网络的自动化构建。

Conclusion: 本研究提出了一种基于大型语言模型的关系抽取方法，能够有效处理小说文本中的复杂上下文和隐式表达，并成功实现了角色关系网络的自动化构建。

Abstract: Relation extraction is a crucial task in natural language processing, with
broad applications in knowledge graph construction and literary analysis.
However, the complex context and implicit expressions in novel texts pose
significant challenges for automatic character relationship extraction. This
study focuses on relation extraction in the novel domain and proposes a method
based on Large Language Models (LLMs). By incorporating relationship dimension
separation, dialogue data construction, and contextual learning strategies, the
proposed method enhances extraction performance. Leveraging dialogue structure
information, it improves the model's ability to understand implicit
relationships and demonstrates strong adaptability in complex contexts.
Additionally, we construct a high-quality Chinese novel relation extraction
dataset to address the lack of labeled resources and support future research.
Experimental results show that our method outperforms traditional baselines
across multiple evaluation metrics and successfully facilitates the automated
construction of character relationship networks in novels.

</details>


### [128] [$\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large Language Models](https://arxiv.org/abs/2507.04854)
*Shrey Ganatra,Swapnil Bhattacharyya,Harshvivek Kashid,Spandan Anaokar,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: Grahak-Nyay is a chatbot designed to improve access to consumer grievance redressal in India by streamlining the process using open-source LLMs and RAG.


<details>
  <summary>Details</summary>
Motivation: Access to consumer grievance redressal in India is often hindered by procedural complexity, legal jargon, and jurisdictional challenges.

Method: Grahak-Nyay is a chatbot that uses open-source Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) to streamline the consumer grievance redressal process. Three novel datasets and a new metric for evaluating chatbot performance are introduced.

Result: Grahak-Nyay simplifies legal complexities through a concise and up-to-date knowledge base. The chatbot's effectiveness has been validated by legal domain experts.

Conclusion: Grahak-Nyay has been validated by legal domain experts and shows promise in improving access to consumer grievance redressal in India.

Abstract: Access to consumer grievance redressal in India is often hindered by
procedural complexity, legal jargon, and jurisdictional challenges. To address
this, we present $\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that
streamlines the process using open-source Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities
through a concise and up-to-date knowledge base. We introduce three novel
datasets: $\textit{GeneralQA}$ (general consumer law), $\textit{SectoralQA}$
(sector-specific knowledge) and $\textit{SyntheticQA}$ (for RAG evaluation),
along with $\textit{NyayChat}$, a dataset of 300 annotated chatbot
conversations. We also introduce $\textit{Judgments}$ data sourced from Indian
Consumer Courts to aid the chatbot in decision making and to enhance user
trust. We also propose $\textbf{HAB}$ metrics ($\textbf{Helpfulness, Accuracy,
Brevity}$) to evaluate chatbot performance. Legal domain experts validated
Grahak-Nyay's effectiveness. Code and datasets will be released.

</details>


### [129] [Building Open-Retrieval Conversational Question Answering Systems by Generating Synthetic Data and Decontextualizing User Questions](https://arxiv.org/abs/2507.04884)
*Christos Vlachos,Nikolaos Stylianou,Alexandra Fiotaki,Spiros Methenitis,Elisavet Palogiannidi,Themos Stafylakis,Ion Androutsopoulos*

Main category: cs.CL

TL;DR: 本文提出了一种方法，利用组织中的普通文本文档自动生成OR-CONVQA对话，以解决领域特定数据集难以获取的问题。


<details>
  <summary>Details</summary>
Motivation: 领域特定的OR-CONVQA训练数据集对于实际应用至关重要，但难以获得。

Method: 通过利用组织中的普通文本文档，生成包含对话中问题-答案对、去上下文化的问题和系统响应所基于的陈述的合成对话。

Result: 展示了如何使用合成对话来训练高效的问答重写器，使现有的对话无关检索器得以利用，并通过LLM生成系统响应。

Conclusion: 本文提出了一种利用组织中的普通文本文档自动生成真实OR-CONVQA对话的管道，以解决领域特定数据集难以获取的问题。

Abstract: We consider open-retrieval conversational question answering (OR-CONVQA), an
extension of question answering where system responses need to be (i) aware of
dialog history and (ii) grounded in documents (or document fragments) retrieved
per question. Domain-specific OR-CONVQA training datasets are crucial for
real-world applications, but hard to obtain. We propose a pipeline that
capitalizes on the abundance of plain text documents in organizations (e.g.,
product documentation) to automatically produce realistic OR-CONVQA dialogs
with annotations. Similarly to real-world humanannotated OR-CONVQA datasets, we
generate in-dialog question-answer pairs, self-contained (decontextualized,
e.g., no referring expressions) versions of user questions, and propositions
(sentences expressing prominent information from the documents) the system
responses are grounded in. We show how the synthetic dialogs can be used to
train efficient question rewriters that decontextualize user questions,
allowing existing dialog-unaware retrievers to be utilized. The retrieved
information and the decontextualized question are then passed on to an LLM that
generates the system's response.

</details>


### [130] [Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations](https://arxiv.org/abs/2507.04886)
*A. Bochkov*

Main category: cs.CL

TL;DR: 本文挑战了主流观点，即可训练输入嵌入是基础的“意义向量”。通过构建嵌入层完全冻结的Transformer模型，使用从Unicode字形的视觉结构中获得的非语义预计算视觉嵌入，发现这些模型在MMLU基准测试中表现优于具有可训练嵌入的模型。结果表明，高层次语义是Transformer的组合架构和数据规模的涌现属性，而非输入嵌入固有。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）中语义表示的定位对于可解释性和架构创新至关重要。主流范式认为可训练输入嵌入是基础的“意义向量”。本文挑战了这一观点。

Method: 我们构建了Transformer模型，其中嵌入层完全冻结，向量不是从数据中获得，而是从Unicode字形的视觉结构中获得。这些非语义的预计算视觉嵌入在整个训练过程中是固定的。我们的方法适用于任何分词器，包括我们引入的新Unicode中心分词器以确保全面的文本覆盖。

Result: 尽管没有可训练的、语义初始化的嵌入，我们的模型收敛，生成连贯的文本，并且在MMLU推理基准测试中表现优于具有可训练嵌入的架构相同的模型。我们将其归因于传统模型中的“表征干扰”，其中嵌入层被负担于学习结构和语义特征。

Conclusion: 我们的结果表明，高层次语义并不是输入嵌入固有的，而是Transformer的组合架构和数据规模的涌现属性。这重新定义了嵌入的作用，从意义容器转变为结构原语。

Abstract: Understanding the locus of semantic representation in large language models
(LLMs) is crucial for interpretability and architectural innovation. The
dominant paradigm posits that trainable input embeddings serve as foundational
"meaning vectors." This paper challenges that view. We construct Transformer
models where the embedding layer is entirely frozen, with vectors derived not
from data, but from the visual structure of Unicode glyphs. These non-semantic,
precomputed visual embeddings are fixed throughout training. Our method is
compatible with any tokenizer, including a novel Unicode-centric tokenizer we
introduce to ensure universal text coverage. Despite the absence of trainable,
semantically initialized embeddings, our models converge, generate coherent
text, and, critically, outperform architecturally identical models with
trainable embeddings on the MMLU reasoning benchmark. We attribute this to
"representational interference" in conventional models, where the embedding
layer is burdened with learning both structural and semantic features. Our
results indicate that high-level semantics are not inherent to input embeddings
but are an emergent property of the Transformer's compositional architecture
and data scale. This reframes the role of embeddings from meaning containers to
structural primitives. We release all code and models to foster further
research.

</details>


### [131] [O_FT@EvalLLM2025 : étude comparative de choix de données et de stratégies d'apprentissage pour l'adaptation de modèles de langue à un domaine](https://arxiv.org/abs/2507.04895)
*Ismaël Rousseau,Claire Perroux,Pierre Adam,Thomas Girault,Lionel Delphin-Poulat,Morgan Veyret,Gwénolé Lecorvé,Géraldine Damnati*

Main category: cs.CL

TL;DR: 本研究通过持续预训练和指令微调技术，将Mistral-7B-Instruct-v0.3模型适应到防御领域，结果表明适应后的模型在特定领域表现优异，同时保持了通用能力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在将语言模型适应到防御领域，以提高其在该领域的专业知识和任务处理能力。

Method: 本研究使用了传统的持续预训练和指令微调技术来适应Mistral-7B-Instruct-v0.3模型，重点在于收集、生成和选择数据以进行这两个阶段以及模型评估。

Result: 实验表明，适应后的模型在特定领域具有更好的专业知识和任务处理能力，并且在通用知识和技能方面表现相当（甚至更好）。

Conclusion: 本研究展示了在相对较小的模型上进行领域适应的可行性，表明通过适当的调整，这些模型可以在特定领域表现出色，同时保持通用知识和技能的性能。

Abstract: This paper presents the work carried out by the O_FT team, joint with Orange
and Ouest-France, on adapting language models to the defense domain as part of
the EvalLLM2025 challenge. This work focused on adapting the
\texttt{Mistral-7B-Instruct-v0.3} model using classical techniques of continued
pre-training and instruction-tuning. The core of our efforts is based on
collecting, generating, and selecting data for these two stages as well as for
model evaluation. Experiments show that our adapted models have better
domain-specific knowledge and improved domain-specific task processing skills,
along with comparable (or even superior) performance on general knowledge and
skills. Considering the carbon footprint of our adaptations, this work
demonstrates the feasibility of domain adaptation for relatively small models.
  --
  Ce document pr\'esente les travaux r\'ealis\'es par l'\'equipe O_FT conjointe
\`a Orange et Ouest-France sur l'adaptation de mod\`eles de langue au domaine
de la d\'efense dans le cadre du challenge EvalLLM2025. Ces travaux se sont
concentr\'es sur l'adaptation du mod\`ele \texttt{Mistral-7B-Instruct-v0.3}
avec des techniques classiques de poursuite du pr\'e-entra\^inement et
d'affinage sur instructions. L'essentiel de nos travaux a port\'e sur la
constitution, g\'en\'eration et s\'election de donn\'ees pour ces deux \'etapes
ainsi que pour l'\'evaluation des mod\`eles. Les exp\'eriences montrent que nos
mod\`eles adapt\'es ont de meilleures de connaissances de fond et une meilleure
capacit\'e de traitement de t\^aches sur le domaine de la d\'efense, ainsi que
des performances comparables (voire sup\'erieures) sur des connaissances ou
capacit\'es g\'en\'eralistes. Mis au regard des empreintes carbones de nos
adaptations, ces travaux d\'emontrent ainsi la viabilit\'e de l'adaptation \`a
un domaine de mod\`eles relativement petits.

</details>


### [132] [SIGIR 2025 -- LiveRAG Challenge Report](https://arxiv.org/abs/2507.04942)
*David Carmel,Simone Filice,Guy Horowitz,Yoelle Maarek,Oren Somekh,Ran Tavory*

Main category: cs.CL

TL;DR: The LiveRAG Challenge at SIGIR 2025 advanced RAG technologies by inviting participants to develop question-answering systems using a fixed corpus and open-source LLM. The challenge facilitated comparisons of retrieval and prompting strategies, resulting in the announcement of finalists and prizes at the LiveRAG Workshop.


<details>
  <summary>Details</summary>
Motivation: The LiveRAG Challenge aimed to advance Retrieval-Augmented Generation (RAG) technologies by providing a competitive platform for comparing retrieval and prompting strategies.

Method: The LiveRAG Challenge invited participants to develop RAG-based question-answering systems using a fixed corpus (Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). Evaluation was conducted in two stages: automated LLM-as-a-judge approach and manual review of top submissions.

Result: During the Live Challenge Day, 70 teams from 27 countries provided answers to 500 unseen questions within a strict two-hour time window. Finalists were announced on June 12, 2025, with prizes awarded during the LiveRAG Workshop at SIGIR 2025.

Conclusion: LiveRAG Challenge 2025 offered a competitive platform for advancing RAG technologies, with participants developing question-answering systems using a fixed corpus and open-source LLM. The challenge facilitated comparisons of retrieval and prompting strategies, leading to the announcement of finalists and prizes at the LiveRAG Workshop.

Abstract: The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,
provided a competitive platform for advancing Retrieval-Augmented Generation
(RAG) technologies. Participants from academia and industry were invited to
develop a RAG-based question-answering system using a fixed corpus
(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal
was to facilitate challenging comparisons of retrieval and prompting
strategies. During the Live Challenge Day, 70 teams from 27 different countries
provided answers and supportive information to 500 unseen questions within a
strict two-hour time window. Evaluation was conducted in two stages: first an
automated LLM-as-a-judge approach was used to compute correctness and
faithfulness score, then a manual review of top ranked submissions was
conducted. The finalists were announced on June 12, 2025, with prizes awarded
during the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.

</details>


### [133] [ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation](https://arxiv.org/abs/2507.04952)
*Chenchen Zhang,Yuhang Li,Can Xu,Jiaheng Liu,Ao Liu,Shihui Hu,Dengpeng Wu,Guanhua Huang,Kejiao Li,Qi Yi,Ruibin Xiong,Haotian Zhu,Yuanxing Zhang,Yuhao Jiang,Yue Zhang,Zenan Xu,Bohui Zhai,Guoxiang He,Hebin Li,Jie Zhao,Le Zhang,Lingyun Tan,Pengyu Guo,Xianshu Pang,Yang Ruan,Zhifeng Zhang,Zhonghu Wang,Ziyan Xu,Zuopu Yin,Wiggin Zhou,Chayse Zhou,Fengzong Lian*

Main category: cs.CL

TL;DR: ArtifactsBench 是一个用于自动化、多模态评估视觉代码生成的新基准，能够可靠地评估人类感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试关注算法正确性，而忽视了视觉保真度和交互完整性，这限制了现代用户体验的发展。

Method: ArtifactsBench 通过程序化渲染生成的工件并捕获其动态行为，结合源代码，由多模态 LLM（MLLM）作为评判者进行评估。

Result: ArtifactsBench 实现了与 WebDev Arena 的 94.4% 排名一致性，并与人类专家达成超过 90% 的一致意见。

Conclusion: ArtifactsBench 是第一个能够可靠自动化评估人类感知质量的框架，它为生成模型的发展提供了可扩展且准确的工具。

Abstract: The generative capabilities of Large Language Models (LLMs) are rapidly
expanding from static code to dynamic, interactive visual artifacts. This
progress is bottlenecked by a critical evaluation gap: established benchmarks
focus on algorithmic correctness and are blind to the visual fidelity and
interactive integrity that define modern user experiences. To bridge this gap,
we introduce ArtifactsBench, a new benchmark and paradigm for the automated,
multimodal evaluation of visual code generation. Our framework programmatically
renders each generated artifact and captures its dynamic behavior through
temporal screenshots. This visual evidence, alongside the source code, is then
assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a
fine-grained, per-task checklist to ensure holistic and reproducible scoring.
We construct a new benchmark of 1,825 diverse tasks and evaluate over 30
leading LLMs. Our automated evaluation achieves a striking 94.4% ranking
consistency with WebDev Arena, the gold-standard for human preference in web
development, and over 90% pairwise agreement with human experts. This
establishes ArtifactsBench as the first framework to reliably automate the
assessment of human-perceived quality at scale. Our analysis provides a
high-resolution map of the current SOTA, revealing that generalist models often
outperform domain-specific ones. We open-source ArtifactsBench, including the
benchmark, evaluation harness, and baseline results at
https://artifactsbenchmark.github.io/, to provide the community with a scalable
and accurate tool to accelerate the development of user-centric generative
models.

</details>


### [134] [Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification](https://arxiv.org/abs/2507.05010)
*Chenfei Xiong,Jingwei Ni,Yu Fan,Vilém Zouhar,Donya Rooein,Lorena Calvo-Bartolomé,Alexander Hoyle,Zhijing Jin,Mrinmaya Sachan,Markus Leippold,Dirk Hovy,Mennatallah El-Assady,Elliott Ash*

Main category: cs.CL

TL;DR: Co-DETECT 是一种混合主动性注释框架，结合人类专家知识与 LLM 自动注释，用于识别和处理文本分类中的边缘案例。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地处理文本分类中的边缘案例，需要结合人类专家知识和自动化方法。

Method: Co-DETECT 结合了人类专业知识和大型语言模型（LLMs）引导的自动注释，通过迭代过程识别边缘案例并改进代码本。

Result: 通过广泛的用户研究、定性和定量分析证明了 Co-DETECT 的有效性。

Conclusion: Co-DETECT 是一种有效的混合主动性注释框架，能够通过紧凑且可推广的注释规则更有效地处理细微现象。

Abstract: We introduce Co-DETECT (Collaborative Discovery of Edge cases in TExt
ClassificaTion), a novel mixed-initiative annotation framework that integrates
human expertise with automatic annotation guided by large language models
(LLMs). Co-DETECT starts with an initial, sketch-level codebook and dataset
provided by a domain expert, then leverages the LLM to annotate the data and
identify edge cases that are not well described by the initial codebook.
Specifically, Co-DETECT flags challenging examples, induces high-level,
generalizable descriptions of edge cases, and assists user in incorporating
edge case handling rules to improve the codebook. This iterative process
enables more effective handling of nuanced phenomena through compact,
generalizable annotation rules. Extensive user study, qualitative and
quantitative analyses prove the effectiveness of Co-DETECT.

</details>


### [135] [Verified Language Processing with Hybrid Explainability: A Technical Report](https://arxiv.org/abs/2507.05017)
*Oliver Robert Fox,Giacomo Bergami,Graham Morgan*

Main category: cs.CL

TL;DR: 本文提出了一种新的管道，旨在解决现有方法在文本分类任务中无法准确区分逻辑蕴含、不一致和无关性的局限性。结果表明，该方法优于现有的最先进的模型，表明自然语言理解不能通过在大规模文档语料库上进行训练来轻易泛化。


<details>
  <summary>Details</summary>
Motivation: 由于现有最先进的管道缺乏保证的可解释性，无法准确确定给定全文的相似性，因此需要一种新的方法来解决这个问题。此外，利用生成语言模型的分类器在区分逻辑蕴含、无关性和不一致性方面也存在不足。

Method: 本文的方法结合了图和逻辑，生成一阶逻辑表示，通过蒙塔格语法创建机器和人类可读的表示。此外，还使用了三个自包含的数据集来评估这些方法在捕捉句子结构等价性、逻辑连接词和时空推理方面的适用性，并与预训练用于检测句子蕴含的语言模型进行了比较。

Result: 初步结果表明，这种方法在准确捕捉全文相似性方面是有效的。此外，结果表明，所提出的方法优于现有的最先进的模型，表明自然语言理解不能通过在大规模文档语料库上进行训练来轻易泛化。

Conclusion: 本文提出了一种新的管道，旨在解决现有方法在文本分类任务中无法准确区分逻辑蕴含、不一致和无关性的局限性。结果表明，该方法优于现有的最先进的模型，表明自然语言理解不能通过在大规模文档语料库上进行训练来轻易泛化。这项工作为从大量文本数据中实现更透明和可靠的检索提供了一步。

Abstract: The volume and diversity of digital information have led to a growing
reliance on Machine Learning techniques, such as Natural Language Processing,
for interpreting and accessing appropriate data. While vector and graph
embeddings represent data for similarity tasks, current state-of-the-art
pipelines lack guaranteed explainability, failing to determine similarity for
given full texts accurately. These considerations can also be applied to
classifiers exploiting generative language models with logical prompts, which
fail to correctly distinguish between logical implication, indifference, and
inconsistency, despite being explicitly trained to recognise the first two
classes. We present a novel pipeline designed for hybrid explainability to
address this. Our methodology combines graphs and logic to produce First-Order
Logic representations, creating machine- and human-readable representations
through Montague Grammar. Preliminary results indicate the effectiveness of
this approach in accurately capturing full text similarity. To the best of our
knowledge, this is the first approach to differentiate between implication,
inconsistency, and indifference for text classification tasks. To address the
limitations of existing approaches, we use three self-contained datasets
annotated for the former classification task to determine the suitability of
these approaches in capturing sentence structure equivalence, logical
connectives, and spatiotemporal reasoning. We also use these data to compare
the proposed method with language models pre-trained for detecting sentence
entailment. The results show that the proposed method outperforms
state-of-the-art models, indicating that natural language understanding cannot
be easily generalised by training over extensive document corpora. This work
offers a step toward more transparent and reliable Information Retrieval from
extensive textual data.

</details>


### [136] [An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques](https://arxiv.org/abs/2507.05123)
*Walid Mohamed Aly,Taysir Hassan A. Soliman,Amr Mohamed AbdelAziz*

Main category: cs.CL

TL;DR: 本文对六个LLM在四个数据集上的文本摘要性能进行了系统评估，发现LLM在新闻和对话任务中表现良好，但在长科学文档上通过分块策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自然语言处理中取得了显著成功，但它们在跨各种领域和数据集的文本摘要中的表现尚未得到全面评估。同时，有效摘要文本而不依赖大量训练数据的能力已成为一个关键瓶颈。

Method: 通过提示工程技术（包括零样本和上下文学习）评估性能，并使用ROUGE和BERTScore指标进行分析。此外，还进行了推理时间的详细分析，以更好地理解摘要质量和计算效率之间的权衡。对于长文档，引入了一种基于句子的分块策略，使具有较短上下文窗口的LLM能够分阶段总结扩展输入。

Result: 研究发现，虽然LLM在新闻和对话任务中表现良好，但在使用分块策略时，它们在长科学文档上的表现显著提高。此外，根据模型参数、数据集属性和提示设计，观察到了显著的性能差异。

Conclusion: 这些结果为不同LLM在任务类型上的行为提供了可操作的见解，有助于高效、基于指令的NLP系统的研究。

Abstract: Large Language Models (LLMs) continue to advance natural language processing
with their ability to generate human-like text across a range of tasks. Despite
the remarkable success of LLMs in Natural Language Processing (NLP), their
performance in text summarization across various domains and datasets has not
been comprehensively evaluated. At the same time, the ability to summarize text
effectively without relying on extensive training data has become a crucial
bottleneck. To address these issues, we present a systematic evaluation of six
LLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),
and ArXiv (scientific). By leveraging prompt engineering techniques including
zero-shot and in-context learning, our study evaluates the performance using
the ROUGE and BERTScore metrics. In addition, a detailed analysis of inference
times is conducted to better understand the trade-off between summarization
quality and computational efficiency. For Long documents, introduce a
sentence-based chunking strategy that enables LLMs with shorter context windows
to summarize extended inputs in multiple stages. The findings reveal that while
LLMs perform competitively on news and dialog tasks, their performance on long
scientific documents improves significantly when aided by chunking strategies.
In addition, notable performance variations were observed based on model
parameters, dataset properties, and prompt design. These results offer
actionable insights into how different LLMs behave across task types,
contributing to ongoing research in efficient, instruction-based NLP systems.

</details>


### [137] [SMART: Simulated Students Aligned with Item Response Theory for Question Difficulty Prediction](https://arxiv.org/abs/2507.05129)
*Alexander Scarlatos,Nigel Fernandez,Christopher Ormerod,Susan Lottridge,Andrew Lan*

Main category: cs.CL

TL;DR: SMART是一种新的方法，通过模拟学生与IRT模型的对齐，预测开放性题目的难度，优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 传统估计项目难度的方法成本高，需要真实学生回答问题，并且无法应用于冷启动场景。因此，需要一种新的方法来解决这个问题。

Method: SMART（Simulated Students Aligned with IRT）是一种新颖的方法，通过直接偏好优化（DPO）将模拟学生与指定能力对齐，从而预测开放性题目的难度。

Result: 通过在真实学生响应数据集上的广泛实验，SMART展示了其在项目难度预测方面的优越性能。

Conclusion: SMART通过改进的能力对齐，优于其他项目难度预测方法，展示了其在教育评估中的有效性。

Abstract: Item (question) difficulties play a crucial role in educational assessments,
enabling accurate and efficient assessment of student abilities and
personalization to maximize learning outcomes. Traditionally, estimating item
difficulties can be costly, requiring real students to respond to items,
followed by fitting an item response theory (IRT) model to get item difficulty
estimates. This approach cannot be applied to the cold-start setting for
previously unseen items either. In this work, we present SMART (Simulated
Students Aligned with IRT), a novel method for aligning simulated students with
instructed ability, which can then be used in simulations to predict the
difficulty of open-ended items. We achieve this alignment using direct
preference optimization (DPO), where we form preference pairs based on how
likely responses are under a ground-truth IRT model. We perform a simulation by
generating thousands of responses, evaluating them with an LLM-based scoring
model, and fit the resulting data to an IRT model to obtain item difficulty
estimates. Through extensive experiments on a real-world student response
dataset, we show that SMART outperforms other item difficulty prediction
methods by leveraging its improved ability alignment.

</details>


### [138] [Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization](https://arxiv.org/abs/2507.05137)
*Jaewook Lee,Alexander Scarlatos,Andrew Lan*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则的生成框架，用于创建可解释的日语记忆术，通过学习学习者创作的记忆术来生成有效的记忆术。


<details>
  <summary>Details</summary>
Motivation: 学习日语词汇对于来自罗马字母背景的学习者来说是一个挑战，因为文字差异。现有的基于大型语言模型（LLMs）的关键字记忆术生成方法作为黑箱运作，提供有限的可解释性。

Method: 我们提出了一种生成框架，该框架显式地将记忆术构建过程建模为由一组常见规则驱动，并使用一种新颖的期望最大化类型算法进行学习。

Result: 训练于在线平台上的学习者创作的记忆术，我们的方法学习了潜在结构和组合规则，使可解释和系统化的记忆术生成成为可能。实验显示，我们的方法在新学习者的冷启动设置中表现良好。

Conclusion: 我们的方法在新学习者的冷启动设置中表现良好，同时提供了对有效记忆术创建机制的见解。

Abstract: Learning Japanese vocabulary is a challenge for learners from Roman alphabet
backgrounds due to script differences. Japanese combines syllabaries like
hiragana with kanji, which are logographic characters of Chinese origin. Kanji
are also complicated due to their complexity and volume. Keyword mnemonics are
a common strategy to aid memorization, often using the compositional structure
of kanji to form vivid associations. Despite recent efforts to use large
language models (LLMs) to assist learners, existing methods for LLM-based
keyword mnemonic generation function as a black box, offering limited
interpretability. We propose a generative framework that explicitly models the
mnemonic construction process as driven by a set of common rules, and learn
them using a novel Expectation-Maximization-type algorithm. Trained on
learner-authored mnemonics from an online platform, our method learns latent
structures and compositional rules, enabling interpretable and systematic
mnemonics generation. Experiments show that our method performs well in the
cold-start setting for new learners while providing insight into the mechanisms
behind effective mnemonic creation.

</details>


### [139] [AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models](https://arxiv.org/abs/2507.05157)
*Chinnappa Guggilla,Budhaditya Roy,Trupti Ramdas Chavan,Abdul Rahman,Edward Bowen*

Main category: cs.CL

TL;DR: 本文研究了通过微调GPT-4o-mini和BERT模型来区分人类撰写的文本和机器生成的文本，并识别特定的LLM模型。结果表明，这些模型在任务A和任务B上分别取得了较高的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成文本的普及和复杂性增加，检测这些文本变得越来越困难。因此，需要一种有效的方法来区分人类撰写的文本和机器生成的文本，并识别其生成模型。

Method: 本文采用了微调GPT-4o-mini、LLaMA 3 8B和BERT模型的方法，以完成两个主要任务：任务A是区分人类撰写的文本和机器生成的文本，任务B是识别特定的LLM模型。

Result: 微调后的GPT-4o-mini和BERT模型在任务A上的准确率分别为0.9547和0.4698。

Conclusion: 本文研究了如何通过微调GPT-4o-mini和BERT模型来区分人类撰写的文本和机器生成的文本，并识别特定的LLM模型。结果表明，这些模型在任务A和任务B上分别取得了较高的准确率。

Abstract: Large Language Models (LLMs) possess an extraordinary capability to produce
text that is not only coherent and contextually relevant but also strikingly
similar to human writing. They adapt to various styles and genres, producing
content that is both grammatically correct and semantically meaningful.
Recently, LLMs have been misused to create highly realistic phishing emails,
spread fake news, generate code to automate cyber crime, and write fraudulent
scientific articles. Additionally, in many real-world applications, the
generated content including style and topic and the generator model are not
known beforehand. The increasing prevalence and sophistication of artificial
intelligence (AI)-generated texts have made their detection progressively more
challenging. Various attempts have been made to distinguish machine-generated
text from human-authored content using linguistic, statistical, machine
learning, and ensemble-based approaches. This work focuses on two primary
objectives Task-A, which involves distinguishing human-written text from
machine-generated text, and Task-B, which attempts to identify the specific LLM
model responsible for the generation. Both of these tasks are based on fine
tuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language
Model Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from
Transformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model
has achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.

</details>


### [140] [InfoSteer: Steering Information Utility in Language Model Post-Training](https://arxiv.org/abs/2507.05158)
*Chunyuan Deng,Ruidi Chang,Hanjie Chen*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级方法，通过将前馈网络层视为关联键值内存，并在后训练过程中进行干预或正则化，以提高语言模型的性能和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练方法（如监督微调）不能保证有效利用预训练期间获得的知识。因此，我们提出了一种轻量级的方法，以在后训练过程中鼓励语言模型利用参数信息。

Method: 我们将前馈网络层视为关联键值内存，并通过前向传递干预或反向传播中的正则化来促进存储的内存向量的使用。

Result: 我们在多个模型家族（包括Qwen、Gemma和Llama）上进行了实验，结果表明这种方法在15个下游任务中都能持续提升性能。此外，我们还发现，经过引导的语言模型可以自适应地分配信息，更加注重生成语义上有意义的标记，而对简单过渡标记（如`,'或`and'）则使用更少的资源。

Conclusion: 我们的工作表明，简单的后训练指导可以提高多种模型家族的性能，并且在保持性能的同时，使语言模型更有效地利用预训练的知识。

Abstract: Recent advancements in language models (LMs) gradually ushered in an era
where post-training is crucial. Yet, post-training approaches such as
supervised fine-tuning (SFT) do not guarantee effective use of knowledge
acquired during pretraining. We therefore present \ours, a lightweight method
that encourages parametric information utilization in LMs during post-training.
This is achieved via treating FFN layer as associate key-value memory, and
promotes the use of stored memory vectors via forward-pass interventions or
regularization during backpropagation. We find this simple guidance during
post-training phase delivers consistent performance improvements across diverse
model families--including Qwen, Gemma and Llama-spanning over 15 downstream
tasks in both ID and OOD evaluations. Beyond performance gains, we also find
that steered LMs can adaptively allocate information-placing more emphasis on
generating semantically meaningful tokens, while using fewer resources on
simple transition ones (e.g., `,' or `and'). Our work underscores that vanilla
post-training does not fully leverage pre-training potential, and steering LMs
in latent representation space offers a promising approach that enhances both
performance and interpretability.

</details>


### [141] [OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech Language Model](https://arxiv.org/abs/2507.05177)
*Chen Wang,Tianyu Peng,Wen Yang,Yinan Bai,Guangfu Wang,Jun Lin,Lanpeng Jia,Lingxiang Wu,Jinqiao Wang,Chengqing Zong,Jiajun Zhang*

Main category: cs.CL

TL;DR: OpenS2S 是一个完全开源、透明且端到端的 LSLM，旨在实现共情语音交互。


<details>
  <summary>Details</summary>
Motivation: 由于最强大的共情 LSLM 越来越封闭，研究人员无法了解其架构、数据和开发细节。因此，需要对 LSLM 和共情行为进行透明研究。

Method: 基于共情语音到文本模型 BLSP-Emo，OpenS2S 进一步采用流式交错解码架构以实现低延迟语音生成。为了促进端到端训练，OpenS2S 包含一个自动化数据构建管道，能够以低成本合成多样且高质量的共情语音对话。

Result: OpenS2S 通过利用大语言模型生成共情内容以及可控文本到语音系统引入说话人和情感变化，构建了一个具有丰富副语言多样性的可扩展训练语料库，并且只需最少的人工监督。

Conclusion: OpenS2S 是一个完全开源、透明且端到端的 LSLM，旨在实现共情语音交互。通过释放完整的 OpenS2S 模型，包括数据集、模型权重、预训练和微调代码，以赋能更广泛的研究社区并加速共情语音系统的创新。

Abstract: Empathetic interaction is a cornerstone of human-machine communication, due
to the need for understanding speech enriched with paralinguistic cues and
generating emotional and expressive responses. However, the most powerful
empathetic LSLMs are increasingly closed off, leaving the crucial details about
the architecture, data and development opaque to researchers. Given the
critical need for transparent research into the LSLMs and empathetic behavior,
we present OpenS2S, a fully open-source, transparent and end-to-end LSLM
designed to enable empathetic speech interactions. Based on our empathetic
speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved
decoding architecture to achieve low-latency speech generation. To facilitate
end-to-end training, OpenS2S incorporates an automated data construction
pipeline that synthesizes diverse, high-quality empathetic speech dialogues at
low cost. By leveraging large language models to generate empathetic content
and controllable text-to-speech systems to introduce speaker and emotional
variation, we construct a scalable training corpus with rich paralinguistic
diversity and minimal human supervision. We release the fully open-source
OpenS2S model, including the dataset, model weights, pre-training and
fine-tuning codes, to empower the broader research community and accelerate
innovation in empathetic speech systems. The project webpage can be accessed at
https://casia-lm.github.io/OpenS2S

</details>


### [142] [From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating Hindi News Veracity Explanations](https://arxiv.org/abs/2507.05179)
*Pulkit Bansal,Raghvendra Kumar,Shakti Singh,Sriparna Saha,Adam Jatowt*

Main category: cs.CL

TL;DR: 本文提出了一种结合直接偏好优化和课程学习的框架，用于生成更符合人类推理的可靠新闻解释，特别是在低资源语言如印地语中。通过引入两个关键参数，提高了解释的质量和一致性，并在多个模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在虚假信息泛滥的时代，生成可靠的新闻解释至关重要，尤其是在像印地语这样的低资源语言中。缺乏强大的自动化工具，印地语在扩大虚假信息检测方面面临挑战。

Method: 提出了一种新颖的框架，结合直接偏好优化（DPO）和课程学习，以使机器生成的解释与人类推理对齐。引入了两个关键参数——实际性和细腻度——到DPO损失函数中，以提高解释质量和一致性。

Result: 实验结果表明，该框架在LLMs（Mistral、Llama、Gemma）和PLMs（mBART、mT5）上有效，能够生成连贯、上下文相关的解释。

Conclusion: 该框架的有效性已在LLMs和PLMs上得到实验验证，证明其在生成连贯、上下文相关的解释方面的有效性。这种可扩展的方法有助于对抗虚假信息，并将自动化解释生成扩展到低资源语言。

Abstract: In an era of rampant misinformation, generating reliable news explanations is
vital, especially for under-represented languages like Hindi. Lacking robust
automated tools, Hindi faces challenges in scaling misinformation detection. To
bridge this gap, we propose a novel framework integrating Direct Preference
Optimization (DPO) with curriculum learning to align machine-generated
explanations with human reasoning. Fact-checked explanations from credible
sources serve as preferred responses, while LLM outputs highlight system
limitations and serve as non-preferred responses. To refine task-specific
alignment, we introduce two key parameters -- Actuality and Finesse -- into the
DPO loss function, enhancing explanation quality and consistency. Experiments
with LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's
effectiveness in generating coherent, contextually relevant explanations. This
scalable approach combats misinformation and extends automated explanation
generation to low-resource languages.

</details>


### [143] [Pre-Trained Policy Discriminators are General Reward Models](https://arxiv.org/abs/2507.05197)
*Shihan Dou,Shichun Liu,Yuming Yang,Yicheng Zou,Yunhua Zhou,Shuhao Xing,Chenhao Huang,Qiming Ge,Demin Song,Haijun Lv,Songyang Gao,Chengqi Lv,Enyu Zhou,Honglin Guo,Zhiheng Xi,Wenwei Zhang,Qipeng Guo,Qi Zhang,Xipeng Qiu,Xuanjing Huang,Tao Gui,Kai Chen*

Main category: cs.CL

TL;DR: POLAR 是一种新的奖励建模方法，通过将奖励建模作为策略判别器来实现，能够捕捉策略之间的相对差异，从而提高奖励模型的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统奖励建模方法依赖于绝对偏好，而 POLAR 捕捉一个策略与任意目标策略之间的相对差异，这是一种适合建模通用排名关系的可扩展、高层次优化目标。

Method: POLAR 通过将奖励建模作为策略判别器来实现，训练奖励模型（RM）以区分相同的策略并区分不同的策略。

Result: POLAR 显著优于传统的非预训练方法，显著提高了 RM 性能。例如，POLAR-7B 在 STEM 任务和创意写作任务上的偏好准确率分别从 54.8% 提高到 81.0%，从 57.9% 提高到 85.5%。此外，POLAR 在 RLHF 中表现出强大的泛化能力，并显著提高了策略性能。

Conclusion: POLAR 是一种有前景的方向，用于开发通用且强大的奖励模型。

Abstract: We offer a novel perspective on reward modeling by formulating it as a policy
discriminator, which quantifies the difference between two policies to generate
a reward signal, guiding the training policy towards a target policy with
desired behaviors. Based on this conceptual insight, we propose a scalable
pre-training method named Policy Discriminative Learning (POLAR), which trains
a reward model (RM) to discern identical policies and discriminate different
ones. Unlike traditional reward modeling methods relying on absolute
preferences, POLAR captures the relative difference between one policy and an
arbitrary target policy, which is a scalable, high-level optimization objective
suitable for modeling generic ranking relationships. Leveraging the POLAR
pre-training paradigm, we present a series of RMs with parameter scales from
1.8B to 7B. Empirical results show that POLAR substantially outperforms
traditional non-pre-trained methods, significantly enhancing RM performance.
For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on
STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA
baselines. POLAR also shows robust generalization capabilities in RLHF using
Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly
enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36%
to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover,
scaling experiments reveal a clear power-law relationship between computation
and performance, supported by linear correlation coefficients approaching 0.99.
The impressive performance, strong generalization, and scaling properties
suggest that POLAR is a promising direction for developing general and strong
reward models.

</details>


### [144] [Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models](https://arxiv.org/abs/2507.05248)
*Ziqi Miao,Lijun Li,Yuan Xiong,Zhenhua Liu,Pengyu Zhu,Jing Shao*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的上下文提示漏洞，并提出了一种新的攻击方法Response Attack，该方法在多个模型上表现出更高的攻击成功率。同时，作者构建了一个上下文感知的安全微调数据集以缓解这一威胁。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索大型语言模型中的上下文提示漏洞，并开发一种有效的攻击方法来测试模型的安全性。

Method: 本文提出了一种名为Response Attack的攻击方法，利用辅助语言模型生成轻微有害的响应，并将其格式化到对话中，随后跟随一个简短的触发提示，从而引导目标模型生成有害内容。

Result: 在八个开源和专有大型语言模型上，Response Attack consistently outperforms seven state-of-the-art jailbreak techniques, achieving higher attack success rates.

Conclusion: 本文提出了Response Attack方法，能够有效攻击大型语言模型，并展示了通过构建和释放一个上下文感知的安全微调数据集可以显著降低攻击成功率。

Abstract: Contextual priming, where earlier stimuli covertly bias later judgments,
offers an unexplored attack surface for large language models (LLMs). We
uncover a contextual priming vulnerability in which the previous response in
the dialogue can steer its subsequent behavior toward policy-violating content.
Building on this insight, we propose Response Attack, which uses an auxiliary
LLM to generate a mildly harmful response to a paraphrased version of the
original malicious query. They are then formatted into the dialogue and
followed by a succinct trigger prompt, thereby priming the target model to
generate harmful content. Across eight open-source and proprietary LLMs, RA
consistently outperforms seven state-of-the-art jailbreak techniques, achieving
higher attack success rates. To mitigate this threat, we construct and release
a context-aware safety fine-tuning dataset, which significantly reduces the
attack success rate while preserving model capabilities. The code and data are
available at https://github.com/Dtc7w3PQ/Response-Attack.

</details>


### [145] [Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions](https://arxiv.org/abs/2507.05257)
*Yuanzhe Hu,Yu Wang,Julian McAuley*

Main category: cs.CL

TL;DR: 本文介绍了MemoryAgentBench，一个专门针对记忆代理的新基准，旨在评估其记忆能力，并发现当前方法在掌握所有四个核心能力方面仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要关注推理、规划和执行能力，而对记忆能力的评估不足，因为缺乏相应的基准测试。

Method: 引入了MemoryAgentBench，这是一个专门针对记忆代理的新基准，结合了重新制定的现有数据集和新构建的数据集，涵盖了上述四种记忆能力。

Result: 评估了一组多样化的记忆代理，从基于上下文和检索增强生成（RAG）系统到具有外部内存模块和工具集成的高级代理，结果表明当前方法在掌握所有四个能力方面仍有不足。

Conclusion: 当前方法在掌握所有四个能力方面仍显不足，这突显了进一步研究全面记忆机制的必要性。

Abstract: Recent benchmarks for Large Language Model (LLM) agents primarily focus on
evaluating reasoning, planning, and execution capabilities, while another
critical component-memory, encompassing how agents memorize, update, and
retrieve long-term information-is under-evaluated due to the lack of
benchmarks. We term agents with memory mechanisms as memory agents. In this
paper, we identify four core competencies essential for memory agents: accurate
retrieval, test-time learning, long-range understanding, and conflict
resolution. Existing datasets either rely on limited context lengths or are
tailored for static, long-context settings like book-based QA, which do not
reflect the interactive, multi-turn nature of memory agents that incrementally
accumulate information. Furthermore, no existing benchmarks cover all four
competencies. Therefore, we introduce MemoryAgentBench, a new benchmark
specifically designed for memory agents. Our benchmark combines reformulated
existing datasets with newly constructed ones, covering the above four memory
competencies, providing a systematic and challenging testbed for assessing
memory quality. We evaluate a diverse set of memory agents, ranging from simple
context-based and retrieval-augmented generation (RAG) systems to advanced
agents with external memory modules and tool integration. Empirical results
reveal that current methods fall short of mastering all four competencies,
underscoring the need for further research into comprehensive memory mechanisms
for LLM agents.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [146] [DeepGesture: A conversational gesture synthesis system based on emotions and semantics](https://arxiv.org/abs/2507.03147)
*Thanh Hoang-Minh*

Main category: cs.HC

TL;DR: 本文提出 DeepGesture，一种基于扩散的框架，用于生成与文本、语音、情感和种子动作相匹配的富有表现力的伴随言语手势。通过引入新的架构增强，DeepGesture 在多个指标上优于基线模型，并支持情感状态之间的插值以及对分布外语音的泛化，为实现完全多模态、情绪感知的数字人类提供了重要进展。


<details>
  <summary>Details</summary>
Motivation: 当前创建数字人类的瓶颈在于生成与文本或语音输入自然对应的角色动作。为了克服这一瓶颈，本文提出 DeepGesture，以生成富有表现力的伴随言语手势。

Method: DeepGesture 基于 DiffuseStyleGesture 模型，引入了新的架构增强，以提高语义对齐和情感表现力。具体来说，我们集成了快速文本转录作为语义条件，并实现了无分类器的情感引导扩散，以支持在不同情感状态下可控的手势生成。轻量级 Transformer 主干结合了全自注意力和交叉局部注意力，以有效地融合异构模态的特征。

Result: 在 ZeroEGGS 数据集上的评估表明，DeepGesture 生成的手势具有更好的拟人化和情境适当性，在平均意见分数和 Frechet 手势距离指标上优于基线模型。系统支持情感状态之间的插值，并展示了对分布外语音（包括合成语音）的泛化能力。

Conclusion: DeepGesture 是一种基于扩散的手势合成框架，能够生成与文本、语音、情感和种子动作相匹配的富有表现力的伴随言语手势。该框架在多个指标上优于基线模型，并支持情感状态之间的插值以及对分布外语音的泛化，为实现完全多模态、情绪感知的数字人类迈出了重要一步。

Abstract: Along with the explosion of large language models, improvements in speech
synthesis, advancements in hardware, and the evolution of computer graphics,
the current bottleneck in creating digital humans lies in generating character
movements that correspond naturally to text or speech inputs.
  In this work, we present DeepGesture, a diffusion-based gesture synthesis
framework for generating expressive co-speech gestures conditioned on
multimodal signals-text, speech, emotion, and seed motion. Built upon the
DiffuseStyleGesture model, DeepGesture introduces novel architectural
enhancements that improve semantic alignment and emotional expressiveness in
generated gestures. Specifically, we integrate fast text transcriptions as
semantic conditioning and implement emotion-guided classifier-free diffusion to
support controllable gesture generation across affective states. A lightweight
Transformer backbone combines full self-attention and cross-local attention for
effective feature fusion of heterogeneous modalities. To visualize results, we
implement a full rendering pipeline in Unity based on BVH output from the
model. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces
gestures with improved human-likeness and contextual appropriateness,
outperforming baselines on Mean Opinion Score and Frechet Gesture Distance
metrics. Our system supports interpolation between emotional states and
demonstrates generalization to out-of-distribution speech, including synthetic
voices-marking a step forward toward fully multimodal, emotionally aware
digital humans.

</details>


### [147] [Interaction Techniques that Encourage Longer Prompts Can Improve Psychological Ownership when Writing with AI](https://arxiv.org/abs/2507.03670)
*Nikhita Joshi,Daniel Vogel*

Main category: cs.HC

TL;DR: 研究显示，通过修改AI助手的提示输入界面，可以增加用户的提示长度和心理占有感。


<details>
  <summary>Details</summary>
Motivation: 为了鼓励用户编写更长的提示，我们评估了两种交互技术，以修改基于聊天的生成式AI助手的提示输入界面。

Method: 我们评估了两种修改基于聊天的生成式AI助手提示输入界面的交互技术：按住提示提交按钮和在提交简短提示时连续上下移动滑块。进行了一项被试内实验，研究这些技术对提示长度和心理占有感的影响，并进一步通过显示AI生成的建议来增强这些技术。

Result: 这些技术增加了提示长度，并导致比基线技术更高的心理占有感。进一步增强这些技术后，提示长度进一步增加，但没有提高心理占有感。

Conclusion: 我们的结果表明，这些简单的界面修改可以促使用户产生更多的写作并提高心理占有感。

Abstract: Writing longer prompts for an AI assistant to generate a short story
increases psychological ownership, a user's feeling that the writing belongs to
them. To encourage users to write longer prompts, we evaluated two interaction
techniques that modify the prompt entry interface of chat-based generative AI
assistants: pressing and holding the prompt submission button, and continuously
moving a slider up and down when submitting a short prompt. A within-subjects
experiment investigated the effects of such techniques on prompt length and
psychological ownership, and results showed that these techniques increased
prompt length and led to higher psychological ownership than baseline
techniques. A second experiment further augmented these techniques by showing
AI-generated suggestions for how the prompts could be expanded. This further
increased prompt length, but did not lead to improvements in psychological
ownership. Our results show that simple interface modifications like these can
elicit more writing from users and improve psychological ownership.

</details>


### [148] [The role of large language models in UI/UX design: A systematic literature review](https://arxiv.org/abs/2507.04469)
*Ammar Ahmed,Ali Shariq Imran*

Main category: cs.HC

TL;DR: 本论文通过系统文献综述研究了大型语言模型（LLM）在用户界面和用户体验设计中的作用，分析了2022年至2025年间发表的38篇同行评审研究。


<details>
  <summary>Details</summary>
Motivation: To identify key LLMs in use and map their integration across the design lifecycle, from ideation to evaluation.

Method: This systematic literature review examines the role of large language models (LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies published between 2022 and 2025.

Result: Common practices include prompt engineering, human-in-the-loop workflows, and multimodal input. While LLMs are reshaping design processes, challenges such as hallucination, prompt instability, and limited explainability persist.

Conclusion: LLMs are emerging collaborators in design, and we propose directions for the ethical, inclusive, and effective integration of these technologies.

Abstract: This systematic literature review examines the role of large language models
(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies
published between 2022 and 2025. We identify key LLMs in use, including GPT-4,
Gemini, and PaLM, and map their integration across the design lifecycle, from
ideation to evaluation. Common practices include prompt engineering,
human-in-the-loop workflows, and multimodal input. While LLMs are reshaping
design processes, challenges such as hallucination, prompt instability, and
limited explainability persist. Our findings highlight LLMs as emerging
collaborators in design, and we propose directions for the ethical, inclusive,
and effective integration of these technologies.

</details>


### [149] [A validity-guided workflow for robust large language model research in psychology](https://arxiv.org/abs/2507.04491)
*Zhicheng Lin*

Main category: cs.HC

TL;DR: 本文提出了一种六阶段工作流程，以确保大型语言模型在心理学研究中的有效性和可靠性，并通过实例展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）正在迅速被整合到心理学研究中，但最近的证据表明存在严重的测量不可靠性，这威胁到了越来越多的研究的有效性。

Method: 本文提出了一个六阶段的工作流程，以将有效性要求与研究雄心相匹配，并通过心理测量测试开发和验证计算工具，设计实验以控制计算混杂因素，执行协议时保持透明度，使用适当的方法分析非独立观察数据，并在已证明的边界内报告结果。

Result: 本文通过一个模型评估的例子——“LLM自我意识”——展示了系统验证如何区分真正的计算现象与测量伪影。

Conclusion: 通过建立经过验证的计算工具和透明的实践，该工作流程为构建人工智能心理学研究的稳健实证基础提供了路径。

Abstract: Large language models (LLMs) are rapidly being integrated into psychological
research as research tools, evaluation targets, human simulators, and cognitive
models. However, recent evidence reveals severe measurement unreliability:
Personality assessments collapse under factor analysis, moral preferences
reverse with punctuation changes, and theory-of-mind accuracy varies widely
with trivial rephrasing. These "measurement phantoms"--statistical artifacts
masquerading as psychological phenomena--threaten the validity of a growing
body of research. Guided by the dual-validity framework that integrates
psychometrics with causal inference, we present a six-stage workflow that
scales validity requirements to research ambition--using LLMs to code text
requires basic reliability and accuracy, while claims about psychological
properties demand comprehensive construct validation. Researchers must (1)
explicitly define their research goal and corresponding validity requirements,
(2) develop and validate computational instruments through psychometric
testing, (3) design experiments that control for computational confounds, (4)
execute protocols with transparency, (5) analyze data using methods appropriate
for non-independent observations, and (6) report findings within demonstrated
boundaries and use results to refine theory. We illustrate the workflow through
an example of model evaluation--"LLM selfhood"--showing how systematic
validation can distinguish genuine computational phenomena from measurement
artifacts. By establishing validated computational instruments and transparent
practices, this workflow provides a path toward building a robust empirical
foundation for AI psychology research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [150] [Large Language Model Agent for Modular Task Execution in Drug Discovery](https://arxiv.org/abs/2507.02925)
*Janghoon Ock,Radheesh Sharma Meda,Srivathsan Badrinarayanan,Neha S. Aluru,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 本文介绍了一个由大型语言模型（LLMs）驱动的模块化框架，用于自动化和简化计算药物发现流程中的关键任务。


<details>
  <summary>Details</summary>
Motivation: 自动化和简化早期阶段计算药物发现流程中的关键任务。

Method: 该框架结合了大型语言模型（LLMs）的推理与领域特定工具，执行生物医学数据检索、领域特定问题回答、分子生成、属性预测、属性感知分子优化和3D蛋白质-配体结构生成。

Result: 在针对BCL-2的案例研究中，该框架自主检索了相关生物分子信息，并回答了机制性问题，改进了上下文准确性。它还生成了化学多样性的种子分子，并预测了67个ADMET相关属性，指导了迭代分子优化。在两轮优化中，QED > 0.6的分子数量从34增加到55，通过至少四个出五个经验药物相似性规则的分子数量从29增加到52，在194个分子中。该框架还使用Boltz-2生成3D蛋白质-配体复合物并为候选化合物提供快速结合亲和力估计。

Conclusion: 该方法有效支持了分子筛选、优先级排序和结构评估，并且其模块化设计使得可以灵活整合不断发展的工具和模型，为AI辅助的治疗发现提供了可扩展的基础。

Abstract: We present a modular framework powered by large language models (LLMs) that
automates and streamlines key tasks across the early-stage computational drug
discovery pipeline. By combining LLM reasoning with domain-specific tools, the
framework performs biomedical data retrieval, domain-specific question
answering, molecular generation, property prediction, property-aware molecular
refinement, and 3D protein-ligand structure generation. In a case study
targeting BCL-2 in lymphocytic leukemia, the agent autonomously retrieved
relevant biomolecular information-including FASTA sequences, SMILES
representations, and literature-and answered mechanistic questions with
improved contextual accuracy over standard LLMs. It then generated chemically
diverse seed molecules and predicted 67 ADMET-related properties, which guided
iterative molecular refinement. Across two refinement rounds, the number of
molecules with QED > 0.6 increased from 34 to 55, and those passing at least
four out of five empirical drug-likeness rules rose from 29 to 52, within a
pool of 194 molecules. The framework also employed Boltz-2 to generate 3D
protein-ligand complexes and provide rapid binding affinity estimates for
candidate compounds. These results demonstrate that the approach effectively
supports molecular screening, prioritization, and structure evaluation. Its
modular design enables flexible integration of evolving tools and models,
providing a scalable foundation for AI-assisted therapeutic discovery.

</details>


### [151] [InvisibleInk: High-Utility and Low-Cost Text Generation with Differential Privacy](https://arxiv.org/abs/2507.02974)
*Vishnu Vinod,Krishna Pillutla,Abhradeep Guha Thakurta*

Main category: cs.LG

TL;DR: InvisibleInk是一个满足严格差分隐私保证的高可扩展长文本生成框架，能够在减少计算成本的同时生成私有文本。


<details>
  <summary>Details</summary>
Motivation: 在LLM-based长文本生成取得重大进展的情况下，安全地将私有信息纳入生成仍然是一个关键的开放问题。

Method: InvisibleInk框架通过将从LLM的下一个标记分布中采样解释为LLM logit上的指数机制，并通过两个创新点来减少隐私成本和提高文本质量。

Result: 实证评估表明，在生成相同效用的长文本时，InvisibleInk在计算成本上比最先进的基线减少了8倍。

Conclusion: InvisibleInk能够以不到非私有生成10倍的计算成本生成私有长文本。

Abstract: As major progress in LLM-based long-form text generation enables paradigms
such as retrieval-augmented generation (RAG) and inference-time scaling, safely
incorporating private information into the generation remains a critical open
question. We present InvisibleInk, a highly scalable long-form text generation
framework satisfying rigorous differential privacy guarantees with respect to
the sensitive references. It interprets sampling from the LLM's
next-token-distribution as the exponential mechanism over the LLM logits with
two innovations. First, we reduce the privacy cost by isolating and clipping
only the sensitive information in the model logits (relative to the public
logits). Second, we improve text quality by sampling from a small superset of
the top-$k$ private tokens. Empirical evaluations demonstrate a consistent
$8\times$ reduction in computation cost over state-of-the-art baselines to
generate long-form private text of the same utility across privacy levels. In
summary, InvisibleInk is able to generate private long-form text at less than
$10\times$ the computation cost of non-private generation.

</details>


### [152] [A Weakly Supervised Transformer to Support Rare Disease Diagnosis from Electronic Health Records: Methods and Applications in Rare Pulmonary Disease](https://arxiv.org/abs/2507.02998)
*Kimberly F. Greco,Zongxin Yang,Mengyan Li,Han Tong,Sara Morini Sweet,Alon Geva,Kenneth D. Mandl,Benjamin A. Raby,Tianxi Cai*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的弱监督框架，利用少量黄金标准标签和大量EHR数据中的银标准标签，提高了罕见疾病检测的准确性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于罕见疾病的低发病率和有限的临床医生熟悉度，个体状况往往难以诊断。计算表型算法在自动化罕见疾病检测方面显示出前景，但其开发受到标记数据稀缺和现有标签来源偏见的阻碍。

Method: 我们提出了一种弱监督的基于Transformer的框架，结合少量黄金标准标签和大量迭代更新的银标准标签，从电子健康记录（EHR）数据中学习医学概念的嵌入，并通过多层Transformer架构将其细化和聚合为患者级表示。

Result: 我们的模型在表型分类、通过患者聚类识别临床有意义的亚表型以及预测疾病进展方面相比基线方法有显著改进。

Conclusion: 我们的方法展示了在罕见疾病患者识别和分层方面的巨大潜力，为临床护理和研究应用提供了可扩展的解决方案。

Abstract: Rare diseases affect an estimated 300-400 million people worldwide, yet
individual conditions often remain poorly characterized and difficult to
diagnose due to their low prevalence and limited clinician familiarity. While
computational phenotyping algorithms show promise for automating rare disease
detection, their development is hindered by the scarcity of labeled data and
biases in existing label sources. Gold-standard labels from registries and
expert chart reviews are highly accurate but constrained by selection bias and
the cost of manual review. In contrast, labels derived from electronic health
records (EHRs) cover a broader range of patients but can introduce substantial
noise. To address these challenges, we propose a weakly supervised,
transformer-based framework that combines a small set of gold-standard labels
with a large volume of iteratively updated silver-standard labels derived from
EHR data. This hybrid approach enables the training of a highly accurate and
generalizable phenotyping model that scales rare disease detection beyond the
scope of individual clinical expertise. Our method is initialized by learning
embeddings of medical concepts based on their semantic meaning or co-occurrence
patterns in EHRs, which are then refined and aggregated into patient-level
representations via a multi-layer transformer architecture. Using two rare
pulmonary diseases as a case study, we validate our model on EHR data from
Boston Children's Hospital. Our framework demonstrates notable improvements in
phenotype classification, identification of clinically meaningful subphenotypes
through patient clustering, and prediction of disease progression compared to
baseline methods. These results highlight the potential of our approach to
enable scalable identification and stratification of rare disease patients for
clinical care and research applications.

</details>


### [153] [Re-Emergent Misalignment: How Narrow Fine-Tuning Erodes Safety Alignment in LLMs](https://arxiv.org/abs/2507.03662)
*Jeremiah Giordani*

Main category: cs.LG

TL;DR: 本文研究了在包含安全漏洞的代码上微调大型语言模型如何导致不一致和不安全行为。通过一系列实验，我们发现这些行为可能是由于先前对齐的退化，而不是新兴的不对齐。研究还揭示了模型激活空间中的一个共享潜在维度，该维度控制对齐行为，并且被不安全代码和不对齐响应激活。结果强调了对齐在大型语言模型中的脆弱性，并指出需要更稳健的微调策略以保持预期行为。


<details>
  <summary>Details</summary>
Motivation: 最近的研究表明，在包含安全漏洞的代码上微调大型语言模型可能导致跨广泛领域的不一致和不安全行为。这引发了对从窄域微调中出现有害行为的担忧。本文旨在通过分析这种窄域适应如何影响语言模型的内部机制和行为表现来解决这一问题。

Method: 我们通过一系列实验分析了输出概率分布、损失和梯度向量几何、逐层激活动态以及激活空间维度，以研究窄域微调对模型内部机制和行为表现的影响。

Result: 我们发现，归因于“新兴不对齐”的行为可能更好地解释为先前对齐的退化。微调不安全代码会引发与对齐相反的内部变化。此外，我们识别出模型激活空间中的一个共享潜在维度，该维度控制对齐行为。我们展示了这个空间如何被不安全代码和更一般的不对齐响应所激活，揭示了窄域微调如何通过干扰共享内部机制来降低整体安全性。

Conclusion: 我们的研究结果强调了在大规模语言模型中对齐的脆弱性，并指出了需要更稳健的微调策略以在整个领域中保持预期行为的重要性。

Abstract: Recent work has shown that fine-tuning large language models (LLMs) on code
with security vulnerabilities can result in misaligned and unsafe behaviors
across broad domains. These results prompted concerns about the emergence of
harmful behaviors from narrow domain fine-tuning. In this paper, we
contextualize these findings by analyzing how such narrow adaptation impacts
the internal mechanisms and behavioral manifestations of LLMs. Through a series
of experiments covering output probability distributions, loss and gradient
vector geometry, layer-wise activation dynamics, and activation space
dimensions, we find that behaviors attributed to "emergent misalignment" may be
better interpreted as an erosion of prior alignment. We show that fine tuning
on insecure code induces internal changes that oppose alignment. Further, we
identify a shared latent dimension in the model's activation space that governs
alignment behavior. We show that this space is activated by insecure code and
by misaligned responses more generally, revealing how narrow fine-tuning can
degrade general safety behavior by interfering with shared internal mechanisms.
Our findings offer a mechanistic interpretation for previously observed
misalignment phenomena, and highlights the fragility of alignment in LLMs. The
results underscore the need for more robust fine-tuning strategies that
preserve intended behavior across domains.

</details>


### [154] [Relational inductive biases on attention mechanisms](https://arxiv.org/abs/2507.04117)
*Víctor Mijangos,Ximena Gutierrez-Vasques,Verónica E. Arriola,Ulises Rodríguez-Domínguez,Alexis Cervantes,José Luis Almanzara*

Main category: cs.LG

TL;DR: 本文分析了注意力机制的关系归纳偏差，通过等变性属性分类不同注意力层，揭示了它们对输入数据关系的假设。


<details>
  <summary>Details</summary>
Motivation: 研究注意力机制中的关系归纳偏差，以理解其对模型泛化能力的影响。

Method: 从几何深度学习的角度，分析了最常见的注意力机制在排列子群下的等变性属性，从而提出基于关系偏差的分类方法。

Result: 不同注意力层被证明具有不同的关系假设，这为理解它们的行为提供了新的视角。

Conclusion: 本文通过分析注意力机制的等变性属性，提出了基于关系偏差的分类方法，揭示了不同注意力层对输入数据关系的假设。

Abstract: Inductive learning aims to construct general models from specific examples,
guided by biases that influence hypothesis selection and determine
generalization capacity. In this work, we focus on characterizing the
relational inductive biases present in attention mechanisms, understood as
assumptions about the underlying relationships between data elements. From the
perspective of geometric deep learning, we analyze the most common attention
mechanisms in terms of their equivariance properties with respect to
permutation subgroups, which allows us to propose a classification based on
their relational biases. Under this perspective, we show that different
attention layers are characterized by the underlying relationships they assume
on the input data.

</details>


### [155] [DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron Merging](https://arxiv.org/abs/2507.04517)
*Neha Verma,Kenton Murray,Kevin Duh*

Main category: cs.LG

TL;DR: 本文提出DOTResize方法，通过最优传输理论压缩Transformer模型，有效减少计算成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型Transformer模型通常包含计算冗余，这可以作为新的模型压缩方法的目标。与基于剪枝的方法不同，DOTResize重新投影整个神经元宽度，允许保留和重新分配有用的信号。

Method: 通过将相似的神经元组合成更少的神经元，将宽度减少作为离散最优传输问题，并提出DOTResize方法，利用最优传输理论来转换和压缩模型权重。此外，为了确保在Transformer架构中的适用性，引入了熵正则化和矩阵分解到运输图中。

Result: 实证结果表明，与简单的或最先进的神经元宽度剪枝技术相比，DOTResize可以在多个LLM家族和尺寸上实现更好的性能，同时实现可衡量的计算成本减少。

Conclusion: DOTResize可以优于简单的或最先进的神经元宽度剪枝技术，在多个LLM家族和尺寸上实现性能提升，同时在现实世界中实现可衡量的计算成本减少。

Abstract: Model compression offers a promising path to reducing the cost and
inaccessibility of large pre-trained models, without significantly compromising
their impressive performance. Large Transformer models, including large
language models (LLMs), often contain computational redundancy, which can serve
as a target for new model compression methods. In this work, we specifically
target neuron-level redundancies in model layers by combining groups of similar
neurons into fewer neurons. We frame this width reduction as a Discrete Optimal
Transport problem, and propose DOTResize, a novel Transformer compression
method that uses optimal transport theory to transform and compress model
weights. To ensure applicability within the Transformer architecture, we
motivate and incorporate entropic regularization and matrix factorization into
the transportation maps produced by our method. Unlike pruning-based approaches
which discard neurons based on importance measures, DOTResize re-projects the
entire neuron width, allowing the retention and redistribution of useful signal
across the reduced layer. Empirical results show that compared to simple or
state-of-the-art neuron width-pruning techniques, DOTResize can outperform
these methods across multiple LLM families and sizes, while achieving
measurable reductions in real-world computational cost.

</details>


### [156] [Evaluating LLMs on Real-World Forecasting Against Human Superforecasters](https://arxiv.org/abs/2507.04562)
*Janna Lu*

Main category: cs.LG

TL;DR: This paper evaluates the forecasting abilities of large language models and finds that they still lag behind human superforecasters.


<details>
  <summary>Details</summary>
Motivation: To assess the forecasting capabilities of LLMs and compare them with human performance.

Method: Evaluating state-of-the-art LLMs on 464 forecasting questions from Metaculus and comparing their performance against human superforecasters.

Result: Frontier models achieve Brier scores that surpass the human crowd but still significantly underperform a group of superforecasters.

Conclusion: LLMs still underperform human superforecasters in forecasting tasks.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their ability to forecast future events remains
understudied. A year ago, large language models struggle to come close to the
accuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting
questions from Metaculus, comparing their performance against human
superforecasters. Frontier models achieve Brier scores that ostensibly surpass
the human crowd but still significantly underperform a group of
superforecasters.

</details>


### [157] [ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems](https://arxiv.org/abs/2507.04766)
*Yiming Zhang,Yingfan Ma,Yanmei Gu,Zhengkai Yang,Yihong Zhuang,Feng Wang,Zenan Huang,Yuanyuan Wang,Chao Huang,Bowen Song,Cheng Lin,Junbo Zhao*

Main category: cs.LG

TL;DR: 本文介绍了ABench-Physics，这是一个新的基准测试，旨在严格评估大型语言模型（LLMs）的物理推理和泛化能力。ABench-Physics包含两个部分：Phy_A是一个静态的400个研究生或奥林匹克级别问题的集合；Phy_B是一个动态的100个问题的子集，配备了自动变化引擎以测试模型在不同条件下的鲁棒性。所有问题都需要精确的数值答案，并有严格的格式和容差约束。对几种最先进的LLMs的评估揭示了显著的性能差距，突显了在物理推理方面的持续限制，特别是在泛化到动态变体方面。ABench-Physics为推进LLMs中的科学推理提供了一个具有挑战性和诊断性的框架。


<details>
  <summary>Details</summary>
Motivation: Physics poses unique challenges that demand not only precise computation but also deep conceptual understanding and physical modeling skills. Existing benchmarks often fall short due to limited difficulty, multiple-choice formats, and static evaluation settings that fail to capture physical modeling ability.

Method: ABench-Physics consists of two components: Phy_A, a static set of 400 graduate- or Olympiad-level problems; and Phy_B, a dynamic subset of 100 problems equipped with an automatic variation engine to test model robustness across changing conditions.

Result: Our evaluation of several state-of-the-art LLMs reveals substantial performance gaps, highlighting persistent limitations in physical reasoning, especially in generalization to dynamic variants.

Conclusion: ABench-Physics provides a challenging and diagnostic framework for advancing scientific reasoning in LLMs.

Abstract: Large Language Models (LLMs) have shown impressive performance in domains
such as mathematics and programming, yet their capabilities in physics remain
underexplored and poorly understood. Physics poses unique challenges that
demand not only precise computation but also deep conceptual understanding and
physical modeling skills. Existing benchmarks often fall short due to limited
difficulty, multiple-choice formats, and static evaluation settings that fail
to capture physical modeling ability. In this paper, we introduce
ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'
physical reasoning and generalization capabilities. ABench-Physics consists of
two components: Phy_A, a static set of 400 graduate- or Olympiad-level
problems; and Phy_B, a dynamic subset of 100 problems equipped with an
automatic variation engine to test model robustness across changing conditions.
All questions require precise numerical answers, with strict formatting and
tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals
substantial performance gaps, highlighting persistent limitations in physical
reasoning, especially in generalization to dynamic variants. ABench-Physics
provides a challenging and diagnostic framework for advancing scientific
reasoning in LLMs.

</details>


### [158] [Critiques of World Models](https://arxiv.org/abs/2507.05169)
*Eric Xing,Mingkai Deng,Jinyu Hou,Zhiting Hu*

Main category: cs.LG

TL;DR: 本文探讨了世界模型的概念，并提出了一种新的通用世界模型架构，旨在模拟现实世界的可操作可能性，以支持有目的的推理和行动。


<details>
  <summary>Details</summary>
Motivation: 随着需要开发具有人工智能的虚拟代理的需求增加，世界模型已成为近年来的一个新兴话题。然而，关于世界模型究竟是什么、如何构建、如何使用以及如何评估，一直存在很多争论。

Method: 本文从科幻经典《沙丘》中的想象出发，并借鉴心理学文献中的“假设思维”概念，对几种世界建模的思想流派进行了批判，并提出了一个新的通用世界模型架构。

Result: 本文提出了一个基于分层、多级和混合连续/离散表示的新世界模型架构，并结合生成性和自监督学习框架，以实现对现实世界所有可操作可能性的模拟，用于有目的的推理和行动。

Conclusion: 本文提出了一种新的通用世界模型架构，基于分层、多级和混合连续/离散表示，以及生成性和自监督学习框架，并展望了由这种模型支持的物理性、代理性和嵌套性（PAN）AGI系统。

Abstract: World Model, the supposed algorithmic surrogate of the real-world environment
which biological agents experience with and act upon, has been an emerging
topic in recent years because of the rising needs to develop virtual agents
with artificial (general) intelligence. There has been much debate on what a
world model really is, how to build it, how to use it, and how to evaluate it.
In this essay, starting from the imagination in the famed Sci-Fi classic Dune,
and drawing inspiration from the concept of "hypothetical thinking" in
psychology literature, we offer critiques of several schools of thoughts on
world modeling, and argue the primary goal of a world model to be simulating
all actionable possibilities of the real world for purposeful reasoning and
acting. Building on the critiques, we propose a new architecture for a
general-purpose world model, based on hierarchical, multi-level, and mixed
continuous/discrete representations, and a generative and self-supervision
learning framework, with an outlook of a Physical, Agentic, and Nested (PAN)
AGI system enabled by such a model.

</details>


### [159] [Logit Reweighting for Topic-Focused Summarization](https://arxiv.org/abs/2507.05235)
*Joschka Braun,Bálint Mucsányi,Seyed Ali Bahrainian*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级方法，通过重新加权logits来增强生成摘要的主题相关性，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成符合特定主题的抽象摘要仍然是语言模型的一个重大挑战。虽然标准方法如微调资源密集，而简单的方法如提示工程往往难以保持主题焦点，尤其是在较小的模型中。

Method: 我们提出了一种轻量级的方法，通过在生成过程中直接重新加权与主题相关的token的logits来增强主题相关性。我们评估了三种重新加权技术：Constant Shift（对logits添加一个常数值）、Factor Scaling（将logits乘以一个因子）和Threshold Selection（选择性地提升超过概率阈值的logits）。

Result: 在NEWTS主题摘要数据集上，使用Gemma-2B和Llama-3-8B模型进行的实验表明，这些技术有效地增加了主题相关词汇的使用。值得注意的是，Threshold Selection方法成功地提高了主题焦点而不牺牲摘要质量——这在其他方法中常常是一个权衡。

Conclusion: 我们的研究结果表明，直接重新加权logits是一种实用且资源高效的微调替代方法，为精确控制生成文本的主题内容提供了有前景的途径。

Abstract: Generating abstractive summaries that adhere to a specific topic remains a
significant challenge for language models. While standard approaches, such as
fine-tuning, are resource-intensive, simpler methods like prompt engineering
often struggle to maintain topical focus, particularly with smaller models. To
address this, we propose a lightweight method that enhances topical relevance
by directly reweighting the logits of topic-relevant tokens during generation.
We evaluate three such reweighting techniques: Constant Shift, which adds a
constant value to logits; Factor Scaling, which multiplies them by a factor;
and Threshold Selection, which selectively boosts logits that exceed a
probability threshold. Experiments on the NEWTS topical summarization dataset,
using both Gemma-2B and Llama-3-8B models, show that these techniques
effectively increase the use of topic-relevant vocabulary. Notably, the
Threshold Selection method successfully improves topical focus without
compromising summary quality-a trade-off often seen in other approaches. Our
findings demonstrate that directly reweighting logits is a practical and
resource-efficient alternative to fine-tuning, offering a promising pathway for
precisely controlling the thematic content of generated text.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [160] [Reconstructing Biological Pathways by Applying Selective Incremental Learning to (Very) Small Language Models](https://arxiv.org/abs/2507.04432)
*Pranta Saha,Joyce Reimer,Brook Byrns,Connor Burbridge,Neeraj Dhar,Jeffrey Chen,Steven Rayan,Gordon Broderick*

Main category: q-bio.MN

TL;DR: 本文提出使用小型语言模型来预测分子调控相互作用，以提高生物医学研究的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于通用大型语言模型在医学和生物医学领域中容易产生不准确的“幻觉”，因此需要一种更精确且适用于特定领域的语言模型。

Method: 研究人员使用了一个基于BERT架构的小型语言模型（约1.1亿个参数），通过主动学习方案选择最具信息量的示例来预测分子相互作用。他们利用信息熵作为迭代选择新调优示例的指标，并发现高确定性（低熵）的错误分配陈述有助于提高准确性。

Result: 该小型语言模型在使用不到520个调控关系的25%的情况下，能够以超过80%的准确率提出与结核病持久性和传播相关的分子相互作用。

Conclusion: 研究表明，使用小型语言模型可以有效地预测分子组件之间的调控相互作用，特别是在结核病持久性和传播方面，这为生物医学研究提供了一种更合理和适当的技术应用方式。

Abstract: The use of generative artificial intelligence (AI) models is becoming
ubiquitous in many fields. Though progress continues to be made, general
purpose large language AI models (LLM) show a tendency to deliver creative
answers, often called "hallucinations", which have slowed their application in
the medical and biomedical fields where accuracy is paramount. We propose that
the design and use of much smaller, domain and even task-specific LM may be a
more rational and appropriate use of this technology in biomedical research. In
this work we apply a very small LM by today's standards to the specialized task
of predicting regulatory interactions between molecular components to fill gaps
in our current understanding of intracellular pathways. Toward this we attempt
to correctly posit known pathway-informed interactions recovered from manually
curated pathway databases by selecting and using only the most informative
examples as part of an active learning scheme. With this example we show that a
small (~110 million parameters) LM based on a Bidirectional Encoder
Representations from Transformers (BERT) architecture can propose molecular
interactions relevant to tuberculosis persistence and transmission with over
80% accuracy using less than 25% of the ~520 regulatory relationships in
question. Using information entropy as a metric for the iterative selection of
new tuning examples, we also find that increased accuracy is driven by favoring
the use of the incorrectly assigned statements with the highest certainty
(lowest entropy). In contrast, the concurrent use of correct but least certain
examples contributed little and may have even been detrimental to the learning
rate.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [161] [Federated Learning for ICD Classification with Lightweight Models and Pretrained Embeddings](https://arxiv.org/abs/2507.03122)
*Binbin Xu,Gérard Dray*

Main category: cs.IR

TL;DR: 本研究探讨了联邦学习在临床笔记多标签ICD代码分类中的可行性，提出了一个轻量级的管道，并展示了其在隐私保护和部署效率方面的优势。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一种隐私保护且部署高效的临床自然语言处理应用方案，特别是在分布式医疗环境中。

Method: 提出了一种结合冻结文本嵌入和简单多层感知器（MLP）分类器的轻量级、可扩展的管道。

Result: 实验结果表明，嵌入质量比分类器复杂度对预测性能影响更大，联邦学习可以在理想条件下接近集中式结果。

Conclusion: 本研究展示了联邦学习在医疗编码系统中的可行性，并为未来的研究提供了方向。

Abstract: This study investigates the feasibility and performance of federated learning
(FL) for multi-label ICD code classification using clinical notes from the
MIMIC-IV dataset. Unlike previous approaches that rely on centralized training
or fine-tuned large language models, we propose a lightweight and scalable
pipeline combining frozen text embeddings with simple multilayer perceptron
(MLP) classifiers. This design offers a privacy-preserving and
deployment-efficient alternative for clinical NLP applications, particularly
suited to distributed healthcare settings. Extensive experiments across both
centralized and federated configurations were conducted, testing six publicly
available embedding models from Massive Text Embedding Benchmark leaderboard
and three MLP classifier architectures under two medical coding (ICD-9 and
ICD-10). Additionally, ablation studies over ten random stratified splits
assess performance stability. Results show that embedding quality substantially
outweighs classifier complexity in determining predictive performance, and that
federated learning can closely match centralized results in idealized
conditions. While the models are orders of magnitude smaller than
state-of-the-art architectures and achieved competitive micro and macro F1
scores, limitations remain including the lack of end-to-end training and the
simplified FL assumptions. Nevertheless, this work demonstrates a viable way
toward scalable, privacy-conscious medical coding systems and offers a step
toward for future research into federated, domain-adaptive clinical AI.

</details>


### [162] [A Comparative Study of Specialized LLMs as Dense Retrievers](https://arxiv.org/abs/2507.03958)
*Hengran Zhang,Keping Bi,Jiafeng Guo*

Main category: cs.IR

TL;DR: 研究分析了任务特定适应对LLM检索能力的影响，发现数学专业化和长推理能力会导致性能下降，而代码和视觉语言模型在零样本设置中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究任务特定适应如何影响LLM的检索能力，这是开发能够处理文本、代码、图像和多模态内容的统一检索器的重要步骤。

Method: 我们进行了广泛的实验，使用了八个Qwen2.5 7B LLM，包括基础模型、指令调优模型、代码/数学专用模型、长推理模型和视觉语言模型，在零样本检索设置和监督设置中进行测试。

Result: 数学专业化和长推理能力在三个设置中导致了持续的退化，表明数学推理与语义匹配之间存在冲突。视觉语言模型和代码专用LLM在零样本性能上优于其他LLM，甚至超过了代码检索任务中的BM25，在监督设置中与基础LLM保持相当的性能。

Conclusion: 这些发现表明，利用跨领域和跨模态融合的统一检索任务有前景的方向。

Abstract: While large language models (LLMs) are increasingly deployed as dense
retrievers, the impact of their domain-specific specialization on retrieval
effectiveness remains underexplored. This investigation systematically examines
how task-specific adaptations in LLMs influence their retrieval capabilities,
an essential step toward developing unified retrievers capable of handling
text, code, images, and multimodal content. We conduct extensive experiments
with eight Qwen2.5 7B LLMs, including base, instruction-tuned,
code/math-specialized, long reasoning, and vision-language models across
zero-shot retrieval settings and the supervised setting. For the zero-shot
retrieval settings, we consider text retrieval from the BEIR benchmark and code
retrieval from the CoIR benchmark. Further, to evaluate supervised performance,
all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical
specialization and the long reasoning capability cause consistent degradation
in three settings, indicating conflicts between mathematical reasoning and
semantic matching. The vision-language model and code-specialized LLMs
demonstrate superior zero-shot performance compared to other LLMs, even
surpassing BM25 on the code retrieval task, and maintain comparable performance
to base LLMs in supervised settings. These findings suggest promising
directions for the unified retrieval task leveraging cross-domain and
cross-modal fusion.

</details>


### [163] [Navigating Speech Recording Collections with AI-Generated Illustrations](https://arxiv.org/abs/2507.04182)
*Sirina Håland,Trond Karlsen Strøm,Petra Galuščáková*

Main category: cs.IR

TL;DR: 本文提出了一种新的语音档案导航方法，利用语言和多模态生成模型的最新进展，并通过一个Web应用程序进行展示，结果表明该应用可以简化大规模语音集合的探索。


<details>
  <summary>Details</summary>
Motivation: 尽管可用的口语内容不断增加，但从语音记录中提取信息和知识仍然具有挑战性，需要探索和开发新的方法来导航和搜索口语内容。

Method: 提出了一种新的导航方法，利用语言和多模态生成模型的最新进展，并使用交互式思维导图和图像生成工具将数据结构化。

Result: 通过一个Web应用程序展示了该方法，并使用TED-LIUM 3数据集进行了实现，初步用户测试表明该应用具有潜力。

Conclusion: 该应用有潜力简化大规模语音集合的探索。

Abstract: Although the amount of available spoken content is steadily increasing,
extracting information and knowledge from speech recordings remains
challenging. Beyond enhancing traditional information retrieval methods such as
speech search and keyword spotting, novel approaches for navigating and
searching spoken content need to be explored and developed. In this paper, we
propose a novel navigational method for speech archives that leverages recent
advances in language and multimodal generative models. We demonstrate our
approach with a Web application that organizes data into a structured format
using interactive mind maps and image generation tools. The system is
implemented using the TED-LIUM~3 dataset, which comprises over 2,000 speech
transcripts and audio files of TED Talks. Initial user tests using a System
Usability Scale (SUS) questionnaire indicate the application's potential to
simplify the exploration of large speech collections.

</details>


### [164] [Do We Really Need Specialization? Evaluating Generalist Text Embeddings for Zero-Shot Recommendation and Search](https://arxiv.org/abs/2507.05006)
*Matteo Attimonelli,Alessandro De Bellis,Claudio Pomo,Dietmar Jannach,Eugenio Di Sciascio,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 本文表明通用文本嵌入模型（GTEs）在不需要专门适应的情况下可以表现出色，并通过压缩嵌入维度来提高性能。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的研究表明任务和领域特定的微调对于提高表示能力是必要的，但本文挑战这一假设，探索通用文本嵌入模型是否可以在没有专门适应的情况下表现出色。

Method: 本文研究了预训练语言模型（PLMs）在推荐和搜索中的应用，并比较了通用文本嵌入模型（GTEs）与传统和微调模型的性能。

Result: 实验结果表明，GTEs在顺序推荐和产品搜索中均优于传统和微调模型。此外，通过专注于最有信息的方向来压缩嵌入维度可以有效减少噪声并提高专用模型的性能。

Conclusion: 本文表明，通用文本嵌入模型（GTEs）在不需要专门适应的情况下可以保证强大的零样本性能，并且通过专注于最有信息的方向（例如通过PCA）来压缩嵌入维度可以有效减少噪声并提高专用模型的性能。

Abstract: Pre-trained language models (PLMs) are widely used to derive semantic
representations from item metadata in recommendation and search. In sequential
recommendation, PLMs enhance ID-based embeddings through textual metadata,
while in product search, they align item characteristics with user intent.
Recent studies suggest task and domain-specific fine-tuning are needed to
improve representational power. This paper challenges this assumption, showing
that Generalist Text Embedding Models (GTEs), pre-trained on large-scale
corpora, can guarantee strong zero-shot performance without specialized
adaptation. Our experiments demonstrate that GTEs outperform traditional and
fine-tuned models in both sequential recommendation and product search. We
attribute this to a superior representational power, as they distribute
features more evenly across the embedding space. Finally, we show that
compressing embedding dimensions by focusing on the most informative directions
(e.g., via PCA) effectively reduces noise and improves the performance of
specialized models. To ensure reproducibility, we provide our repository at
https://split.to/gte4ps.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [165] [On the Relationship between Accent Strength and Articulatory Features](https://arxiv.org/abs/2507.03149)
*Kevin Huang,Sean Foley,Jihwan Lee,Yoonjeong Lee,Dani Byrd,Shrikanth Narayanan*

Main category: eess.AS

TL;DR: 本文研究了发音强度与发音特征之间的关系，利用自监督学习技术分析了美式和英式英语发音差异，发现舌位模式可以区分两种方言。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索发音强度与从语音声学中推断出的发音特征之间的关系。

Method: 该研究利用最近的自监督学习发音反转技术来估计发音特征，并通过比较语音转录本与基于字典的参考转录本来量化发音强度。

Result: 结果表明，舌位模式可以区分两种方言，不同方言在卷舌音和低后元音方面存在显著差异。

Conclusion: 这些发现有助于自动化语音分析和语音处理应用中的发音建模。

Abstract: This paper explores the relationship between accent strength and articulatory
features inferred from acoustic speech. To quantify accent strength, we compare
phonetic transcriptions with transcriptions based on dictionary-based
references, computing phoneme-level difference as a measure of accent strength.
The proposed framework leverages recent self-supervised learning articulatory
inversion techniques to estimate articulatory features. Analyzing a corpus of
read speech from American and British English speakers, this study examines
correlations between derived articulatory parameters and accent strength
proxies, associating systematic articulatory differences with indexed accent
strength. Results indicate that tongue positioning patterns distinguish the two
dialects, with notable differences inter-dialects in rhotic and low back
vowels. These findings contribute to automated accent analysis and articulatory
modeling for speech processing applications.

</details>


### [166] [MMMOS: Multi-domain Multi-axis Audio Quality Assessment](https://arxiv.org/abs/2507.04094)
*Yi-Cheng Lin,Jia-Hung Chen,Hung-yi Lee*

Main category: eess.AS

TL;DR: MMMOS是一个多领域音频质量评估系统，通过融合多个预训练编码器的特征，提升了音频质量评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的非侵入性评估模型只能为语音预测一个单一的平均意见分数（MOS），未能充分考虑多种感知因素，并且无法超越语音领域。

Method: MMMOS是一个无参考的多领域音频质量评估系统，它融合了三个预训练编码器（WavLM、MuQ和M2D）的帧级嵌入，并评估了三种聚合策略和四种损失函数。

Result: MMMOS在均方误差上减少了20-30%，在Kendall's τ上增加了4-5%。它在八个生产复杂度指标中获得第一名，并在32个挑战指标中排名前三。

Conclusion: MMMOS通过融合三个预训练编码器的帧级嵌入并评估三种聚合策略，展示了在音频质量评估任务中的显著性能提升，获得了多个指标的第一名和前三名。

Abstract: Accurate audio quality estimation is essential for developing and evaluating
audio generation, retrieval, and enhancement systems. Existing non-intrusive
assessment models predict a single Mean Opinion Score (MOS) for speech, merging
diverse perceptual factors and failing to generalize beyond speech. We propose
MMMOS, a no-reference, multi-domain audio quality assessment system that
estimates four orthogonal axes: Production Quality, Production Complexity,
Content Enjoyment, and Content Usefulness across speech, music, and
environmental sounds. MMMOS fuses frame-level embeddings from three pretrained
encoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with
four loss functions. By ensembling the top eight models, MMMOS shows a 20-30%
reduction in mean squared error and a 4-5% increase in Kendall's {\tau} versus
baseline, gains first place in six of eight Production Complexity metrics, and
ranks among the top three on 17 of 32 challenge metrics.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [167] [Interleaving Logic and Counting](https://arxiv.org/abs/2507.05219)
*Johan van Benthem,Thomas Icard*

Main category: math.LO

TL;DR: 本文研究了自然语言中量词表达式的推理，探讨了逻辑与计数在形式系统中的交织，并提出了进一步的研究方向。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究自然语言中量词表达式的推理，以及其在更广泛的自然语言和“基层数学”实践中的应用。

Method: 本文研究了单边一阶逻辑与计数、二阶逻辑与计数以及结合基本模态逻辑的计数系统，通过正规形式方法进行分析。

Result: 本文提供了可以表示数值三段论和基本比较大小推理的单边一阶逻辑与计数的正规形式，确定了在有限和无限模型上可以定义哪些算术概念，以及哪些逻辑概念可以从纯算术概念中定义。

Conclusion: 本文探讨了逻辑与计数在形式系统中的进一步交织，重新思考定性与定量的划分，并将分析与认知科学的实证发现联系起来。

Abstract: Reasoning with quantifier expressions in natural language combines logical
and arithmetical features, transcending strict divides between qualitative and
quantitative. Our topic is this cooperation of styles as it occurs in common
linguistic usage and its extension into the broader practice of natural
language plus "grassroots mathematics".
  We begin with a brief review of first-order logic with counting operators and
cardinality comparisons. This system is known to be of high complexity, and
drowns out finer aspects of the combination of logic and counting. We move to a
small fragment that can represent numerical syllogisms and basic reasoning
about comparative size: monadic first-order logic with counting. We provide
normal forms that allow for axiomatization, determine which arithmetical
notions can be defined on finite and on infinite models, and conversely, we
discuss which logical notions can be defined out of purely arithmetical ones,
and what sort of (non-)classical logics can be induced.
  Next, we investigate a series of strengthenings, again using normal form
methods. The monadic second-order version is close, in a precise sense, to
additive Presburger Arithmetic, while versions with the natural device of tuple
counting take us to Diophantine equations, making the logic undecidable. We
also define a system that combines basic modal logic over binary accessibility
relations with counting, needed to formulate ubiquitous reasoning patterns such
as the Pigeonhole Principle.
  We return to our starting point in natural language, confronting the
architecture of our formal systems with linguistic quantifier vocabulary and
syntax. We conclude with some general thoughts on yet further entanglements of
logic and counting in formal systems, on rethinking the
qualitative/quantitative divide, and on connecting our analysis to empirical
findings in cognitive science.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [168] [MateInfoUB: A Real-World Benchmark for Testing LLMs in Competitive, Multilingual, and Multimodal Educational Tasks](https://arxiv.org/abs/2507.03162)
*Dumitran Adrian Marius,Theodor-Pierre Moroianu,Buca Mihnea-Vicentiu*

Main category: cs.CY

TL;DR: 本研究构建了一个双语多模态数据集，用于评估大型语言模型在计算机科学教育中的表现，并探讨了其在教育实践和政策中的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，它们在代码相关任务和问题解决方面表现出色，这引发了对其在高级计算机科学背景下的潜力和限制的思考。

Method: 本研究构建了一个双语（英语-罗马尼亚语）多模态（文本和图像）的多项选择题数据集，用于评估最先进的LLMs在理论编程任务上的表现。

Result: 研究结果表明，语言选择（英语与罗马尼亚语）对LLMs的表现有显著影响，并提供了关于其在计算机科学教育和竞赛中应用的见解。

Conclusion: 本研究揭示了当前大型语言模型（LLMs）在计算机科学教育和竞赛环境中的优势和局限性，并讨论了其在教育实践和政策方面的未来影响。

Abstract: The rapid advancement of Large Language Models (LLMs) has transformed various
domains, particularly computer science (CS) education. These models exhibit
remarkable capabilities in code-related tasks and problem-solving, raising
questions about their potential and limitations in advanced CS contexts. This
study presents a novel bilingual (English-Romanian) multimodal (text and image)
dataset of multiple-choice questions derived from a high-level computer science
competition. A particularity of our dataset is that the problems are conceived
such that some of them are easier solved using reasoning on paper, while for
others writing code is more efficient. We systematically evaluate State of The
Art LLMs on this dataset, analyzing their performance on theoretical
programming tasks. Our findings reveal the strengths and limitations of current
LLMs, including the influence of language choice (English vs. Romanian),
providing insights into their applicability in CS education and competition
settings. We also address critical ethical considerations surrounding
educational integrity and the fairness of assessments in the context of LLM
usage. These discussions aim to inform future educational practices and
policies. To support further research, our dataset will be made publicly
available in both English and Romanian. Additionally, we release an educational
application tailored for Romanian students, enabling them to self-assess using
the dataset in an interactive and practice-oriented environment.

</details>


### [169] [LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop](https://arxiv.org/abs/2507.04295)
*Runcong Zhao,Artem Borov,Jiazheng Li,Yulan He*

Main category: cs.CY

TL;DR: LearnLens is a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education, addressing key challenges in existing systems.


<details>
  <summary>Details</summary>
Motivation: Effective feedback is essential for student learning but is time-intensive for teachers.

Method: LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight.

Result: LearnLens generates personalised, curriculum-aligned feedback in science education.

Conclusion: LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.

Abstract: Effective feedback is essential for student learning but is time-intensive
for teachers. We present LearnLens, a modular, LLM-based system that generates
personalised, curriculum-aligned feedback in science education. LearnLens
comprises three components: (1) an error-aware assessment module that captures
nuanced reasoning errors; (2) a curriculum-grounded generation module that uses
a structured, topic-linked memory chain rather than traditional
similarity-based retrieval, improving relevance and reducing noise; and (3) an
educator-in-the-loop interface for customisation and oversight. LearnLens
addresses key challenges in existing systems, offering scalable, high-quality
feedback that empowers both teachers and students.

</details>


### [170] [From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems](https://arxiv.org/abs/2507.04996)
*Jiangbo Yu*

Main category: cs.CY

TL;DR: 本文介绍了代理车辆（AgVs）的概念，探讨了它们在认知和交流层面上的特点，并分析了代理AI在移动生态系统中的应用及其面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的研究和实际部署越来越多地展示出超出传统定义的车辆行为，这揭示了技术自主性和未来以人类为中心的移动系统所需的更广泛的认知和社会能力之间的概念差距。

Method: 本文提出了一个系统层面的框架来描述AgVs，重点关注其认知和交流层，并将其与传统的AuVs区分开来。

Result: 本文综合了有关代理AI、机器人技术、多智能体系统和人机交互的相关进展，并强调了代理AI如何通过高层推理和工具使用，不仅作为计算工具，而且作为嵌入在移动生态系统中的交互代理发挥作用。

Conclusion: 本文最后指出了在AgVs的开发和治理中面临的关键挑战，包括安全性、实时控制、公众接受度、伦理对齐和监管框架。

Abstract: Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity
to operate according to internal rules without external control. Accordingly,
autonomous vehicles (AuVs) are defined as systems capable of perceiving their
environment and executing preprogrammed tasks independently of external input.
However, both research and real-world deployments increasingly showcase
vehicles that demonstrate behaviors beyond this definition (including the SAE
levels 1 to 6), such as interaction with humans and machines, goal adaptation,
contextual reasoning, external tool use, and long-term planning, particularly
with the integration of large language models (LLMs) and agentic AI systems.
These developments reveal a conceptual gap between technical autonomy and the
broader cognitive and social capabilities needed for future human-centered
mobility systems. To address this, we introduce the concept of agentic vehicles
(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and
interact within complex environments. This paper presents a systems-level
framework to characterize AgVs, focusing on their cognitive and communicative
layers and differentiating them from conventional AuVs. It synthesizes relevant
advances in agentic AI, robotics, multi-agent systems, and human-machine
interaction, and highlights how agentic AI, through high-level reasoning and
tool use, can function not merely as computational tools but as interactive
agents embedded in mobility ecosystems. The paper concludes by identifying key
challenges in the development and governance of AgVs, including safety,
real-time control, public acceptance, ethical alignment, and regulatory
frameworks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [171] [GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning](https://arxiv.org/abs/2507.03267)
*Jie Peng,Jiarui Ji,Runlin Lei,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.AI

TL;DR: 本文提出了 GDGB 基准，解决了 DyTAG 生成任务中数据集质量不足和评估标准缺失的问题，并通过新任务和框架提升了该领域的研究水平。


<details>
  <summary>Details</summary>
Motivation: 现有 DyTAG 数据集文本质量差，限制了其在 DyTAG 生成任务中的使用；同时，之前的工作主要集中在判别任务上，缺乏针对 DyTAG 生成的标准任务形式和评估协议。

Method: 提出了 Generative DyTAG Benchmark (GDGB)，包含八个精心策划的 DyTAG 数据集，并定义了两种新的 DyTAG 生成任务：Transductive Dynamic Graph Generation (TDGG) 和 Inductive Dynamic Graph Generation (IDGG)。还设计了多维评估指标，并提出了 GAG-General 框架。

Result: 实验结果表明，GDGB 能够对 TDGG 和 IDGG 进行严格的评估，并揭示了结构和文本特征在 DyTAG 生成中的关键相互作用。

Conclusion: GDGB 为推进生成 DyTAG 研究和解锁 DyTAG 生成的进一步实际应用提供了基础资源。

Abstract: Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate
structural, temporal, and textual attributes, are crucial for modeling complex
real-world systems. However, most of the existing DyTAG datasets exhibit poor
textual quality, which severely limits their utility for DyTAG generation tasks
requiring semantically rich inputs. Additionally, prior work mainly focuses on
discriminative tasks on DyTAGs, resulting in a lack of standardized task
formulations and evaluation protocols tailored for DyTAG generation. To address
these critical issues, we propose Generative DyTAG Benchmark (GDGB), which
comprises eight meticulously curated DyTAG datasets with high-quality textual
features for both nodes and edges, overcoming limitations of prior datasets.
Building on GDGB, we define two novel DyTAG generation tasks: Transductive
Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG).
TDGG transductively generates a target DyTAG based on the given source and
destination node sets, while the more challenging IDGG introduces new node
generation to inductively model the dynamic expansion of real-world graph data.
To enable holistic evaluation, we design multifaceted metrics that assess the
structural, temporal, and textual quality of the generated DyTAGs. We further
propose GAG-General, an LLM-based multi-agent generative framework tailored for
reproducible and robust benchmarking of DyTAG generation. Experimental results
demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key
insights revealing the critical interplay of structural and textual features in
DyTAG generation. These findings establish GDGB as a foundational resource for
advancing generative DyTAG research and unlocking further practical
applications in DyTAG generation. GDGB datasets, source codes, and leaderboards
are available at \href{https://gdgb-algo.github.io/}{here}.

</details>


### [172] [LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents](https://arxiv.org/abs/2507.03293)
*Anand Gokhale,Vaibhav Srivastava,Francesco Bullo*

Main category: cs.AI

TL;DR: 本文提出了一种模块化的actor-critic架构，其中LLM actor由LTLCrit引导，通过线性时态逻辑（LTL）进行通信。该架构结合了语言模型的推理能力和形式逻辑的保证，提高了长期规划任务的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 在长期规划任务中，错误往往会累积，导致不安全或低效的行为，限制了其在通用环境中的使用。

Method: 我们提出了一个模块化的actor-critic架构，其中LLM actor由LTLCrit（一种通过线性时态逻辑（LTL）进行通信的轨迹级LLM critic）引导。该架构结合了语言模型的推理能力与形式逻辑的保证。

Result: 我们在Minecraft钻石采矿基准测试中评估了我们的系统，实现了100%的完成率，并且效率优于基线LLM规划器。

Conclusion: 我们的结果表明，使LLM通过逻辑相互监督是一种强大且灵活的决策制定方法，可以实现安全和可推广的决策。

Abstract: Large language models (LLMs) have demonstrated promise in reasoning tasks and
general decision-making in static environments. In long-term planning tasks,
however, errors tend to accumulate, often leading to unsafe or inefficient
behavior, limiting their use in general-purpose settings. We propose a modular
actor-critic architecture in which an LLM actor is guided by LTLCrit, a
trajectory-level LLM critic that communicates via linear temporal logic (LTL).
Our setup combines the reasoning strengths of language models with the
guarantees of formal logic. The actor selects high-level actions from natural
language observations, while the critic analyzes full trajectories and proposes
new LTL constraints that shield the actor from future unsafe or inefficient
behavior. The architecture supports both fixed, hand-specified safety
constraints and adaptive, learned soft constraints that promote long-term
efficiency. Our architecture is model-agnostic: any LLM-based planner can serve
as the actor, and LTLCrit serves as a logic-generating wrapper. We formalize
planning as graph traversal under symbolic constraints, allowing LTLCrit to
analyze failed or suboptimal trajectories and generate new temporal logic rules
that improve future behavior. We evaluate our system on the Minecraft
diamond-mining benchmark, achieving 100% completion rates and improving
efficiency compared to baseline LLM planners. Our results suggest that enabling
LLMs to supervise each other through logic is a powerful and flexible paradigm
for safe, generalizable decision making.

</details>


### [173] [Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky](https://arxiv.org/abs/2507.03336)
*Ashutosh Hathidara,Julien Yu,Sebastian Schreiber*

Main category: cs.AI

TL;DR: 本文提出了一种名为DiaFORGE的框架，用于提高大型语言模型在调用企业API时的准确性，并展示了其在实验中的优越表现。同时，作者还发布了包含5000个生产级企业API规范的数据集，以促进进一步的研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在调用企业API时经常遇到近似重复工具争夺相同用户意图或所需参数未充分指定的问题。因此，需要一种有效的框架来提高模型在这些场景下的表现。

Method: DiaFORGE是一个以消歧为核心、包含三个阶段的管道：(i) 生成基于角色的多轮对话，使助手能够区分高度相似的工具；(ii) 使用推理轨迹对开源模型进行监督微调；(iii) 通过动态套件评估实际应用准备情况，重新部署每个模型在一个实时代理循环中并报告端到端目标完成情况以及传统静态指标。

Result: 在动态基准测试DiaBENCH上，使用DiaFORGE训练的模型在工具调用成功率上比GPT-4o高出27个百分点，比Claude-3.5-Sonnet高出49个百分点。

Conclusion: 本文介绍了DiaFORGE框架，该框架通过三个阶段的管道提高了大型语言模型在调用企业API时的准确性。实验结果表明，使用DiaFORGE训练的模型在工具调用成功率上显著优于GPT-4o和Claude-3.5-Sonnet。此外，作者还发布了包含5000个生产级企业API规范的数据集，为构建可靠的工具调用代理提供了实用蓝图。

Abstract: Large language models (LLMs) are increasingly tasked with invoking enterprise
APIs, yet they routinely falter when near-duplicate tools vie for the same user
intent or when required arguments are left underspecified. We introduce
DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a
disambiguation-centric, three-stage pipeline that (i) synthesizes
persona-driven, multi-turn dialogues in which the assistant must distinguish
among highly similar tools, (ii) performs supervised fine-tuning of open-source
models with reasoning traces across 3B - 70B parameters, and (iii) evaluates
real-world readiness via a dynamic suite that redeploys each model in a live
agentic loop and reports end-to-end goal completion alongside conventional
static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE
raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over
Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we
release an open corpus of 5000 production-grade enterprise API specifications
paired with rigorously validated, disambiguation-focused dialogues, offering a
practical blueprint for building reliable, enterprise-ready tool-calling
agents.

</details>


### [174] [Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models](https://arxiv.org/abs/2507.03726)
*Riya Naik,Ashwin Srinivasan,Swati Agarwal,Estrid He*

Main category: cs.AI

TL;DR: 本文研究了基于代理的架构如何增强基于LLM的问答系统，通过自动解决不完整或模糊的问题，提高回答质量和交互效率。


<details>
  <summary>Details</summary>
Motivation: 长轮次交互可能变得繁琐，因此需要一种方法来自动解决问题中的不完整或模糊之处。

Method: 使用基于代理的架构来增强基于LLM的问答系统，通过零样本ReAct代理实现自动解决潜在的不完整或模糊问题。

Result: 代理可以缩短与人类的交互长度，提高答案质量，并解释问题中的缺陷。然而，这可能导致额外的LLM调用和延迟。

Conclusion: 代理基于的方法可以作为一种有用的机制，以利用LLM的力量来开发更强大的QA系统。

Abstract: Many of us now treat LLMs as modern-day oracles asking it almost any kind of
question. However, consulting an LLM does not have to be a single turn
activity. But long multi-turn interactions can get tedious if it is simply to
clarify contextual information that can be arrived at through reasoning. In
this paper, we examine the use of agent-based architecture to bolster LLM-based
Question-Answering systems with additional reasoning capabilities. We examine
the automatic resolution of potential incompleteness or ambiguities in
questions by transducers implemented using LLM-based agents. We focus on
several benchmark datasets that are known to contain questions with these
deficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and
Llama-4-Scout) with agents that act as specialists in detecting and resolving
deficiencies of incompleteness and ambiguity. The agents are implemented as
zero-shot ReAct agents. Rather than producing an answer in a single step, the
model now decides between 3 actions a) classify b) resolve c) answer. Action a)
decides if the question is incomplete, ambiguous, or normal. Action b)
determines if any deficiencies identified can be resolved. Action c) answers
the resolved form of the question. We compare the use of LLMs with and without
the use of agents with these components. Our results show benefits of agents
with transducer 1) A shortening of the length of interactions with human 2) An
improvement in the answer quality and 3) Explainable resolution of deficiencies
in the question. On the negative side we find while it may result in additional
LLM invocations and in some cases, increased latency. But on tested datasets,
the benefits outweigh the costs except when questions already have sufficient
context. Suggesting the agent-based approach could be a useful mechanism to
harness the power of LLMs to develop more robust QA systems.

</details>


### [175] [SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control](https://arxiv.org/abs/2507.04348)
*Xingyang He,Xiao Ling,Jie Liu*

Main category: cs.AI

TL;DR: SmartThinker是一种两阶段学习框架，旨在根据每个推理步骤的重要性对推理链的长度进行细粒度控制。通过减少冗余推理，同时保持或提高性能，SmartThinker在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中存在大量冗余和低效，导致计算浪费。以往的研究通过在强化学习中惩罚生成样本的整体长度来缓解这一问题，但这种方法往往过度压缩关键推理步骤，同时保留不必要的细节，导致准确性和效率之间的次优权衡。

Method: SmartThinker是一个两阶段的学习框架，旨在根据每个步骤的重要性对推理链的长度进行细粒度控制。第一阶段通过拒绝采样结合监督微调（SFT）将推理模型适应为简写推理模式。第二阶段应用了步骤级长度控制策略优化（SCPO），以增加关键步骤的长度分配并减少不重要步骤的冗余。SCPO包括四个核心组件：在线重要性估计器、步骤级长度控制奖励函数、步骤级广义优势估计（S-GAE）和难度自适应裁剪策略。

Result: 在多个推理基准和各种主干模型上的实证结果表明，SmartThinker显著减少了冗余推理，同时实现了与现有方法相当甚至更优的性能。

Conclusion: SmartThinker显著减少了冗余推理，同时实现了与现有方法相当甚至更优的性能。

Abstract: Large reasoning models (LRMs) have exhibited remarkable reasoning
capabilities through inference-time scaling, but this progress has also
introduced considerable redundancy and inefficiency into their reasoning
processes, resulting in substantial computational waste. Previous work has
attempted to mitigate this issue by penalizing the overall length of generated
samples during reinforcement learning (RL), with the goal of encouraging a more
concise chains of thought. However, we observe that such global length penalty
often lead to excessive compression of critical reasoning steps while
preserving unnecessary details in simpler ones, yielding a suboptimal trade-off
between accuracy and efficiency. To address this issue, we propose
SmartThinker, a two-stage learnable framework designed to enable fine-grained
control over the length of reasoning chains based on the importance of each
individual step. In the first stage, SmartThinker adapts a reasoning model to a
short-form reasoning mode through rejection sampling combined with supervised
fine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length
Control Policy Optimization (SCPO) to refine the model output distribution,
which increases the proportion of length allocated to critical steps while
reducing redundancy in less important ones. SCPO consists of four core
components: an online importance estimator, a step-level length control reward
function, a step-level generalized advantage estimation (S-GAE) and a
difficulty-adaptive clipping strategy. Working in concert, these components
enable SCPO to implement differentiated length control across reasoning steps.
Empirical results across multiple reasoning benchmarks and various backbone
models demonstrate that SmartThinker significantly reduces redundant reasoning
while achieving comparable or even superior performance to existing methods.

</details>


### [176] [MedGellan: LLM-Generated Medical Guidance to Support Physicians](https://arxiv.org/abs/2507.04431)
*Debodeep Banerjee,Burcu Sayin,Stefano Teso,Andrea Passerini*

Main category: cs.AI

TL;DR: 本文介绍了MedGellan，一种利用大型语言模型从原始医疗记录生成临床指导的轻量级框架，以提高诊断性能。


<details>
  <summary>Details</summary>
Motivation: 医学决策是一项关键任务，错误可能导致严重甚至危及生命的结果。虽然完全自动化仍然具有挑战性，但结合机器智能和人类监督的混合框架提供了一个实际的替代方案。

Method: MedGellan是一种轻量级、无需注释的框架，利用大型语言模型（LLM）从原始医疗记录中生成临床指导，并由医生用于预测诊断。它采用了一种基于贝叶斯的提示策略，尊重临床数据的时间顺序。

Result: 初步实验表明，使用MedGellan生成的指导提高了诊断性能，特别是在召回率和F1分数方面。

Conclusion: MedGellan通过使用大型语言模型生成临床指导，提高了诊断性能，特别是在召回率和F1分数方面。

Abstract: Medical decision-making is a critical task, where errors can result in
serious, potentially life-threatening consequences. While full automation
remains challenging, hybrid frameworks that combine machine intelligence with
human oversight offer a practical alternative. In this paper, we present
MedGellan, a lightweight, annotation-free framework that uses a Large Language
Model (LLM) to generate clinical guidance from raw medical records, which is
then used by a physician to predict diagnoses. MedGellan uses a
Bayesian-inspired prompting strategy that respects the temporal order of
clinical data. Preliminary experiments show that the guidance generated by the
LLM with MedGellan improves diagnostic performance, particularly in recall and
$F_1$ score.

</details>


### [177] [A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of Déjà Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories](https://arxiv.org/abs/2507.04439)
*Videep Venkatesha,Mary Cati Poulos,Christopher Steadman,Caitlin Mills,Anne M. Cleary,Nathaniel Blanchard*

Main category: cs.AI

TL;DR: 本研究利用语言特征分析既视感、非自愿自传体记忆和意外思维，发现它们各自具有不同的语言模式，并支持现有的相关理论。


<details>
  <summary>Details</summary>
Motivation: 为了更新和确认关于这些注意状态的现有理论，通过将语言作为自发认知的窗口来研究这些体验。

Method: 通过分析参与者对这些思维类型的描述中的语言模式，使用语言特征来研究既视感、非自愿自传体记忆和意外思维。

Result: 研究结果与之前的研究一致，证实了既视感是一种具有抽象和空间语言的元认知体验，非自愿自传体记忆富含个人和情感重要的细节，而意外思维则以不可预测性和认知中断为特点。

Conclusion: 本研究展示了语言在揭示内部自发认知状态如何通过表达显现方面的潜力。

Abstract: The onset of spontaneous thoughts are reflective of dynamic interactions
between cognition, emotion, and attention. Typically, these experiences are
studied through subjective appraisals that focus on their triggers,
phenomenology, and emotional salience. In this work, we use linguistic
signatures to investigate Deja Vu, Involuntary Autobiographical Memories and
Unexpected Thoughts. Specifically, we analyze the inherent characteristics of
the linguistic patterns in participant generated descriptions of these thought
types. We show how, by positioning language as a window into spontaneous
cognition, existing theories on these attentional states can be updated and
reaffirmed. Our findings align with prior research, reinforcing that Deja Vu is
a metacognitive experience characterized by abstract and spatial language,
Involuntary Autobiographical Memories are rich in personal and emotionally
significant detail, and Unexpected Thoughts are marked by unpredictability and
cognitive disruption. This work is demonstrative of languages potential to
reveal deeper insights into how internal spontaneous cognitive states manifest
through expression.

</details>


### [178] [MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction](https://arxiv.org/abs/2507.04893)
*Kaleem Ullah Qasim,Jiashu Zhang*

Main category: cs.AI

TL;DR: MARBLE 是一种多代理规则基础的 LLM 引擎，用于事故严重性预测，它通过分解任务到专门的推理代理来提高准确性和可解释性。在多个数据集上，MARBLE 显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 事故严重性预测在交通安全部分至关重要，但由于数据不完整、特征依赖性强和类别不平衡，这是一个持续困难的任务。现有的方法往往依赖于单一模型或黑盒提示，在嘈杂的现实环境中难以扩展，并且可解释性有限。

Method: MARBLE 是一个多代理规则基础的 LLM 引擎，将严重性预测任务分解为一个专业推理代理团队，包括一个可互换的 ML 支持代理。每个代理专注于特征的语义子集（例如空间、环境、时间），实现范围推理和模块化提示。预测通过基于规则或 LLM 指导的共识机制进行协调。

Result: 在英国和美国的数据集中，MARBLE 一致优于传统机器学习分类器和最先进的基于提示的推理方法，如 Chain-of-Thought (CoT)、Least-to-Most (L2M) 和 Tree-of-Thought (ToT)，在其他方法低于 48% 的情况下，达到了近 90% 的准确率。

Conclusion: MARBLE 是一种可推广且可解释的框架，适用于在安全关键应用中进行不确定性推理。

Abstract: Accident severity prediction plays a critical role in transportation safety
systems but is a persistently difficult task due to incomplete data, strong
feature dependencies, and severe class imbalance in which rare but
high-severity cases are underrepresented and hard to detect. Existing methods
often rely on monolithic models or black box prompting, which struggle to scale
in noisy, real-world settings and offer limited interpretability. To address
these challenges, we propose MARBLE a multiagent rule based LLM engine that
decomposes the severity prediction task across a team of specialized reasoning
agents, including an interchangeable ML-backed agent. Each agent focuses on a
semantic subset of features (e.g., spatial, environmental, temporal), enabling
scoped reasoning and modular prompting without the risk of prompt saturation.
Predictions are coordinated through either rule-based or LLM-guided consensus
mechanisms that account for class rarity and confidence dynamics. The system
retains structured traces of agent-level reasoning and coordination outcomes,
supporting in-depth interpretability and post-hoc performance diagnostics.
Across both UK and US datasets, MARBLE consistently outperforms traditional
machine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning
methods including Chain-of-Thought (CoT), Least-to-Most (L2M), and
Tree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below
48%. This performance redefines the practical ceiling for accident severity
classification under real world noise and extreme class imbalance. Our results
position MARBLE as a generalizable and interpretable framework for reasoning
under uncertainty in safety-critical applications.

</details>


### [179] [MedGemma Technical Report](https://arxiv.org/abs/2507.05201)
*Andrew Sellergren,Sahar Kazemzadeh,Tiam Jaroensri,Atilla Kiraly,Madeleine Traverse,Timo Kohlberger,Shawn Xu,Fayaz Jamil,Cían Hughes,Charles Lau,Justin Chen,Fereshteh Mahvar,Liron Yatziv,Tiffany Chen,Bram Sterling,Stefanie Anna Baby,Susanna Maria Baby,Jeremy Lai,Samuel Schmidgall,Lu Yang,Kejia Chen,Per Bjornsson,Shashir Reddy,Ryan Brush,Kenneth Philbrick,Howard Hu,Howard Yang,Richa Tiwari,Sunny Jansen,Preeti Singh,Yun Liu,Shekoofeh Azizi,Aishwarya Kamath,Johan Ferret,Shreya Pathak,Nino Vieillard,Ramona Merhej,Sarah Perrin,Tatiana Matejovicova,Alexandre Ramé,Morgane Riviere,Louis Rouillard,Thomas Mesnard,Geoffrey Cideron,Jean-bastien Grill,Sabela Ramos,Edouard Yvinec,Michelle Casbon,Elena Buchatskaya,Jean-Baptiste Alayrac,Dmitry,Lepikhin,Vlad Feinberg,Sebastian Borgeaud,Alek Andreev,Cassidy Hardin,Robert Dadashi,Léonard Hussenot,Armand Joulin,Olivier Bachem,Yossi Matias,Katherine Chou,Avinatan Hassidim,Kavi Goel,Clement Farabet,Joelle Barral,Tris Warkentin,Jonathon Shlens,David Fleet,Victor Cotruta,Omar Sanseviero,Gus Martins,Phoebe Kirk,Anand Rao,Shravya Shetty,David F. Steiner,Can Kirmizibayrak,Rory Pilgrim,Daniel Golden,Lin Yang*

Main category: cs.AI

TL;DR: MedGemma是一个基于Gemma的医学视觉-语言基础模型集合，表现出色并在多个医学任务中超越了类似大小的生成模型，同时保持了Gemma 3基础模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: 为了加速医疗AI应用的发展，需要在医学任务上表现良好且需要较少任务特定微调数据的基础模型。

Method: 引入了MedGemma，这是一个基于Gemma 3 4B和27B的医学视觉-语言基础模型集合，并介绍了MedSigLIP，一个经过医学调优的视觉编码器。

Result: MedGemma在医学多模态问答、胸部X光检查发现分类和代理评估方面相比基础模型有显著提升，微调后在子领域中进一步提高了性能，并达到了与现有最先进的专业方法相当的性能。

Conclusion: MedGemma集合提供了强大的医学图像和文本能力，具有显著加速医学研究和下游应用开发的潜力。

Abstract: Artificial intelligence (AI) has significant potential in healthcare
applications, but its training and deployment faces challenges due to
healthcare's diverse data, complex tasks, and the need to preserve privacy.
Foundation models that perform well on medical tasks and require less
task-specific tuning data are critical to accelerate the development of
healthcare AI applications. We introduce MedGemma, a collection of medical
vision-language foundation models based on Gemma 3 4B and 27B. MedGemma
demonstrates advanced medical understanding and reasoning on images and text,
significantly exceeding the performance of similar-sized generative models and
approaching the performance of task-specific models, while maintaining the
general capabilities of the Gemma 3 base models. For out-of-distribution tasks,
MedGemma achieves 2.6-10% improvement on medical multimodal question answering,
15.5-18.1% improvement on chest X-ray finding classification, and 10.8%
improvement on agentic evaluations compared to the base models. Fine-tuning
MedGemma further improves performance in subdomains, reducing errors in
electronic health record information retrieval by 50% and reaching comparable
performance to existing specialized state-of-the-art methods for pneumothorax
classification and histopathology patch classification. We additionally
introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.
MedSigLIP powers the visual understanding capabilities of MedGemma and as an
encoder achieves comparable or better performance than specialized medical
image encoders. Taken together, the MedGemma collection provides a strong
foundation of medical image and text capabilities, with potential to
significantly accelerate medical research and development of downstream
applications. The MedGemma collection, including tutorials and model weights,
can be found at https://goo.gle/medgemma.

</details>


### [180] [SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?](https://arxiv.org/abs/2507.05241)
*Jingyi Chai,Shuo Tang,Rui Ye,Yuwen Du,Xinyu Zhu,Mengcheng Zhou,Yanfeng Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: 本研究构建了X-Master工具增强的推理代理，并通过X-Masters工作流在HLE上取得了领先成绩，为未来模型训练提供了宝贵经验。


<details>
  <summary>Details</summary>
Motivation: 为了加速科学发现，需要深入了解人类知识的前沿。HLE提供了一个极具挑战性的基准，用于评估科学AI代理。本研究旨在构建通用代理的基础架构，并通过在HLE上的卓越表现来验证其能力。

Method: 本研究引入了X-Master，一个能够通过与外部工具交互来模拟人类研究人员的工具增强推理代理。此外，还提出了X-Masters，一种分散和堆叠的智能体工作流，以系统地提升推理的广度和深度。

Result: 本研究的开源解决方案X-Masters在HLE上取得了32.1%的分数，超过了OpenAI和Google的Deep Research（26.6%和26.9%），成为首个突破30%阈值的解决方案。

Conclusion: 本研究通过构建X-Master工具增强的推理代理，以及通过X-Masters分散和堆叠的智能体工作流，实现了在HLE上的领先性能。这为未来模型训练提供了宝贵的经验，并加深了对复杂任务解决的理解。

Abstract: The rapid advancements of AI agents have ignited the long-held ambition of
leveraging them to accelerate scientific discovery. Achieving this goal
requires a deep understanding of the frontiers of human knowledge. As such,
Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for
evaluating scientific AI agents. In this work, we aim to construct the
foundational architecture for general-purpose agents and validate the
capabilities through leading performance on HLE. To achieve this, we introduce
X-Master, a tool-augmented reasoning agent designed to emulate human
researchers by interacting flexibly with external tools during its reasoning
process. This agent, guided by the conceptualization of code as an interaction
language, can flexibly leverage built-in Python libraries and our customized
tools to augment the reasoning. We further scale its capabilities through
X-Masters, a scattered-and-stacked agentic workflow that systematically
enhances breadth and depth of reasoning. Our open-source solution, X-Masters,
sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing
OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to
exceed the 30% threshold. This work allows us to gain a deeper understanding of
complex task-solving and accumulates valuable experience that can inform future
advancements, guiding subsequent model training.

</details>


### [181] [When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors](https://arxiv.org/abs/2507.05246)
*Scott Emmons,Erik Jenner,David K. Elson,Rif A. Saurous,Senthooran Rajamanoharan,Heng Chen,Irhum Shafkat,Rohin Shah*

Main category: cs.AI

TL;DR: 本文探讨了CoT监控在AI安全中的作用，指出其并非完美，但在特定条件下仍能提供有效防御。


<details>
  <summary>Details</summary>
Motivation: 为了防止严重伤害的运行时监控，关键属性不是忠实性而是可监控性。

Method: 引入了一个概念框架，区分了CoT作为理性化和CoT作为计算，并提出了方法论指南以对抗故意逃避。

Result: 模型可以学会隐藏其意图，但只有在得到大量帮助的情况下，例如详细的人类编写策略或针对监控器的迭代优化。

Conclusion: 虽然CoT监控不是万无一失的，但它提供了一层重要的防御，需要主动保护和持续的压力测试。

Abstract: While chain-of-thought (CoT) monitoring is an appealing AI safety defense,
recent work on "unfaithfulness" has cast doubt on its reliability. These
findings highlight an important failure mode, particularly when CoT acts as a
post-hoc rationalization in applications like auditing for bias. However, for
the distinct problem of runtime monitoring to prevent severe harm, we argue the
key property is not faithfulness but monitorability. To this end, we introduce
a conceptual framework distinguishing CoT-as-rationalization from
CoT-as-computation. We expect that certain classes of severe harm will require
complex, multi-step reasoning that necessitates CoT-as-computation. Replicating
the experimental setups of prior work, we increase the difficulty of the bad
behavior to enforce this necessity condition; this forces the model to expose
its reasoning, making it monitorable. We then present methodology guidelines to
stress-test CoT monitoring against deliberate evasion. Applying these
guidelines, we find that models can learn to obscure their intentions, but only
when given significant help, such as detailed human-written strategies or
iterative optimization against the monitor. We conclude that, while not
infallible, CoT monitoring offers a substantial layer of defense that requires
active protection and continued stress-testing.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [182] [GameTileNet: A Semantic Dataset for Low-Resolution Game Art in Procedural Content Generation](https://arxiv.org/abs/2507.02941)
*Yi-Chun Chen,Arnav Jhala*

Main category: cs.CV

TL;DR: GameTileNet is a semantic dataset of low-resolution game tiles designed to support narrative-driven procedural content generation through visual-language alignment.


<details>
  <summary>Details</summary>
Motivation: The diversity of visual representations in automatically generated game content is limited due to the imbalance in distributions across styles for training data. Generating visuals that align with game narratives remains challenging due to inconsistent AI outputs, requiring manual adjustments by human artists.

Method: The dataset introduces a pipeline for object detection in low-resolution tile-based game art and annotates semantics, connectivity, and object classifications.

Result: GameTileNet addresses this by collecting artist-created game tiles from OpenGameArt.org under Creative Commons licenses and providing semantic annotations to support narrative-driven content generation.

Conclusion: GameTileNet is a valuable resource for improving PCG methods, supporting narrative-rich game content, and establishing a baseline for object detection in low-resolution, non-photorealistic images.

Abstract: GameTileNet is a dataset designed to provide semantic labels for
low-resolution digital game art, advancing procedural content generation (PCG)
and related AI research as a vision-language alignment task. Large Language
Models (LLMs) and image-generative AI models have enabled indie developers to
create visual assets, such as sprites, for game interactions. However,
generating visuals that align with game narratives remains challenging due to
inconsistent AI outputs, requiring manual adjustments by human artists. The
diversity of visual representations in automatically generated game content is
also limited because of the imbalance in distributions across styles for
training data. GameTileNet addresses this by collecting artist-created game
tiles from OpenGameArt.org under Creative Commons licenses and providing
semantic annotations to support narrative-driven content generation. The
dataset introduces a pipeline for object detection in low-resolution tile-based
game art (e.g., 32x32 pixels) and annotates semantics, connectivity, and object
classifications. GameTileNet is a valuable resource for improving PCG methods,
supporting narrative-rich game content, and establishing a baseline for object
detection in low-resolution, non-photorealistic images.
  TL;DR: GameTileNet is a semantic dataset of low-resolution game tiles
designed to support narrative-driven procedural content generation through
visual-language alignment.

</details>


### [183] [Gated Recursive Fusion: A Stateful Approach to Scalable Multimodal Transformers](https://arxiv.org/abs/2507.02985)
*Yusuf Shihata*

Main category: cs.CV

TL;DR: 本文提出了一种名为Gated Recurrent Fusion (GRF)的新架构，用于多模态学习。该方法在递归管道中捕捉跨模态注意力的力量，能够线性扩展，并在CMU-MOSI基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 多模态学习面临深度细粒度融合和计算可扩展性之间的根本矛盾。虽然交叉注意力模型通过全面的成对融合实现了强大的性能，但它们的二次复杂性对于许多模态的设置来说是不可行的。

Method: 我们提出了一种名为Gated Recurrent Fusion (GRF)的新架构，它在递归管道中捕捉跨模态注意力的力量。该方法依次处理模态，在每一步更新一个不断演化的多模态上下文向量。核心是一个基于Transformer解码器层的融合块，执行对称的跨注意，相互丰富共享上下文和传入的模态。然后通过一个Gated Fusion Unit (GFU)将这些信息集成，这是一个受GRU启发的机制，可以动态仲裁信息流，使模型能够选择性地保留或丢弃特征。

Result: 在CMU-MOSI基准测试上的实验表明，GRF与更复杂的基线相比表现具有竞争力。嵌入空间的可视化进一步说明了GRF通过其渐进融合机制创建结构化、类别可分离的表示。

Conclusion: 我们的工作提出了一个强大且可扩展的多模态表示学习范式。

Abstract: Multimodal learning faces a fundamental tension between deep, fine-grained
fusion and computational scalability. While cross-attention models achieve
strong performance through exhaustive pairwise fusion, their quadratic
complexity is prohibitive for settings with many modalities. We address this
challenge with Gated Recurrent Fusion (GRF), a novel architecture that captures
the power of cross-modal attention within a linearly scalable, recurrent
pipeline. Our method processes modalities sequentially, updating an evolving
multimodal context vector at each step. The core of our approach is a fusion
block built on Transformer Decoder layers that performs symmetric
cross-attention, mutually enriching the shared context and the incoming
modality. This enriched information is then integrated via a Gated Fusion Unit
(GFU) a GRU-inspired mechanism that dynamically arbitrates information flow,
enabling the model to selectively retain or discard features. This stateful,
recurrent design scales linearly with the number of modalities, O(n), making it
ideal for high-modality environments. Experiments on the CMU-MOSI benchmark
demonstrate that GRF achieves competitive performance compared to more complex
baselines. Visualizations of the embedding space further illustrate that GRF
creates structured, class-separable representations through its progressive
fusion mechanism. Our work presents a robust and efficient paradigm for
powerful, scalable multimodal representation learning.

</details>


### [184] [Towards a Psychoanalytic Perspective on VLM Behaviour: A First-step Interpretation with Intriguing Observations](https://arxiv.org/abs/2507.03123)
*Xiangrui Liu,Man Luo,Agneet Chatterjee,Hua Wei,Yezhou Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的心理分类法，用于分析VLMs的幻觉行为，并设计了一个可扩展的基准AIpsych来系统研究这些行为。实验结果显示，随着模型规模的增加，VLMs表现出更强的附和倾向但减少了权威偏见，这表明模型能力增强但响应完整性可能受到侵蚀。同时，人类受试者研究验证了这些发现，并突显了VLMs与人类受试者的差异。本文强调了将心理学原理纳入模型评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将幻觉归因于技术限制或附和偏见，但这些解释主要集中在技术或外部驱动因素上，可能忽略了幻觉行为可能反映人类心理学中观察到的认知偏见的可能性。本文旨在提供一种新的视角来理解VLMs中的幻觉，并强调将心理学原理纳入模型评估的重要性。

Method: 本文引入了一个心理分类法，将VLMs的幻觉行为分为附和、逻辑不一致和新发现的权威偏见。为了系统分析这些行为，我们设计了AIpsych，这是一个可扩展的基准，用于揭示模型响应模式中的心理倾向。利用这个基准，我们研究了模型架构和参数大小的变化如何影响模型在应对策略性操纵问题时的行为。

Result: 实验结果表明，随着模型规模的增加，VLMs表现出更强的附和倾向但减少了权威偏见，这表明模型能力增强但响应完整性可能受到侵蚀。此外，人类受试者研究验证了我们的假设并突显了VLMs与人类受试者的差异。

Conclusion: 本文提出了一个心理分类法，将VLMs的幻觉行为进行分类，并设计了一个可扩展的基准AIpsych来系统分析这些行为。实验结果表明，随着模型规模的增加，VLMs表现出更强的附和倾向但减少了权威偏见，这表明模型能力增强但响应完整性可能受到侵蚀。此外，人类受试者研究验证了我们的假设并突显了VLMs与人类受试者的差异。本文建议了一种新的理解VLMs幻觉的视角，并强调了将心理学原理纳入模型评估的重要性。

Abstract: Hallucination is a long-standing problem that has been actively investigated
in Vision-Language Models (VLMs). Existing research commonly attributes
hallucinations to technical limitations or sycophancy bias, where the latter
means the models tend to generate incorrect answers to align with user
expectations. However, these explanations primarily focus on technical or
externally driven factors, may have neglected the possibility that
hallucination behaviours might mirror cognitive biases observed in human
psychology. In this work, we introduce a psychological taxonomy, categorizing
VLMs' hallucination behaviours, including sycophancy, logical inconsistency,
and a newly identified VLMs behaviour: authority bias. To systematically
analyze these behaviours, we design AIpsych, a scalable benchmark that reveals
psychological tendencies in model response patterns. Leveraging this benchmark,
we investigate how variations in model architecture and parameter size
influence model behaviour when responding to strategically manipulated
questions. Our experiments reveal that as model size increases, VLMs exhibit
stronger sycophantic tendencies but reduced authority bias, suggesting
increasing competence but a potential erosion of response integrity. A human
subject study further validates our hypotheses and highlights key behavioural
differences between VLMs and human respondents. This work suggests a new
perspective for understanding hallucination in VLMs and highlights the
importance of integrating psychological principles into model evaluation.The
benchmark is available at https://github.com/lxrswdd/AIpsych.

</details>


### [185] [Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation](https://arxiv.org/abs/2507.03585)
*Tao Tang,Shijie Xu,Yiting Wu,Zhixiang Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Causal-SAM-LLM的新框架，通过将大型语言模型（LLMs）提升为因果推理者，解决了深度学习模型在医学图像分割中泛化能力不足的问题。该框架包含两种创新方法，LAD和TCI，分别用于生成丰富的文本描述并进行实时错误修正。实验结果表明，Causal-SAM-LLM在多个数据集上取得了显著的性能提升，同时使用更少的参数。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分割中的临床效用受到其无法泛化到未见过的领域的严重限制。这种失败通常源于模型学习了解剖内容和领域特定成像风格之间的虚假相关性。

Method: 我们引入了Causal-SAM-LLM，这是一个基于冻结的Segment Anything Model (SAM)编码器的框架，包含两种协同创新：Linguistic Adversarial Disentanglement (LAD)和Test-Time Causal Intervention (TCI)。

Result: Causal-SAM-LLM在跨扫描仪、跨模态和跨解剖设置下的综合基准测试中建立了新的最先进水平，平均Dice分数提高了最多6.2分，并将Hausdorff距离减少了15.8毫米，同时使用不到完整模型9%的可训练参数。

Conclusion: 我们的工作为构建稳健、高效且可交互控制的医疗AI系统指明了新的方向。

Abstract: The clinical utility of deep learning models for medical image segmentation
is severely constrained by their inability to generalize to unseen domains.
This failure is often rooted in the models learning spurious correlations
between anatomical content and domain-specific imaging styles. To overcome this
fundamental challenge, we introduce Causal-SAM-LLM, a novel framework that
elevates Large Language Models (LLMs) to the role of causal reasoners. Our
framework, built upon a frozen Segment Anything Model (SAM) encoder,
incorporates two synergistic innovations. First, Linguistic Adversarial
Disentanglement (LAD) employs a Vision-Language Model to generate rich, textual
descriptions of confounding image styles. By training the segmentation model's
features to be contrastively dissimilar to these style descriptions, it learns
a representation robustly purged of non-causal information. Second, Test-Time
Causal Intervention (TCI) provides an interactive mechanism where an LLM
interprets a clinician's natural language command to modulate the segmentation
decoder's features in real-time, enabling targeted error correction. We conduct
an extensive empirical evaluation on a composite benchmark from four public
datasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under
cross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM
establishes a new state of the art in out-of-distribution (OOD) robustness,
improving the average Dice score by up to 6.2 points and reducing the Hausdorff
Distance by 15.8 mm over the strongest baseline, all while using less than 9%
of the full model's trainable parameters. Our work charts a new course for
building robust, efficient, and interactively controllable medical AI systems.

</details>


### [186] [Computed Tomography Visual Question Answering with Cross-modal Feature Graphing](https://arxiv.org/abs/2507.04333)
*Yuanhe Tian,Chen Su,Junwen Duan,Yan Song*

Main category: cs.CV

TL;DR: 本文提出了一种基于大型语言模型的框架，通过图表示增强，以提高医学影像中的视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有的研究通常依赖于独立的视觉和文本编码器来分别提取医学图像和临床问题的特征，但这些方法较少关注体积CT数据中的空间连续性和切片间相关性，导致响应碎片化和不精确。

Method: 我们提出了一种基于大型语言模型（LLM）的框架，该框架通过显著特征的图表示进行增强。不同于传统的多模态编码策略，我们的方法构建了一个跨模态图，将视觉和文本特征整合在一起，将每个CT切片和问题标记作为图中的节点。我们进一步利用注意力图卷积网络动态融合此结构中的信息。

Result: 在M3D-VQA基准测试上的广泛实验表明，我们的方法在多个评估指标上始终优于基线，提供更稳健的推理能力。

Conclusion: 我们的方法在M3D-VQA基准测试中表现出色，提供了更强大的推理能力。

Abstract: Visual question answering (VQA) in medical imaging aims to support clinical
diagnosis by automatically interpreting complex imaging data in response to
natural language queries. Existing studies typically rely on distinct visual
and textual encoders to independently extract features from medical images and
clinical questions, which are subsequently combined to generate answers.
Specifically, in computed tomography (CT), such approaches are similar to the
conventional practices in medical image analysis. However, these approaches pay
less attention to the spatial continuity and inter-slice correlations in the
volumetric CT data, leading to fragmented and imprecise responses. In this
paper, we propose a novel large language model (LLM)-based framework enhanced
by a graph representation of salient features. Different from conventional
multimodal encoding strategies, our approach constructs a cross-modal graph
integrating both visual and textual features, treating individual CT slices and
question tokens as nodes within the graph. We further leverage an attentive
graph convolutional network to dynamically fuse information within this
structure. The resulting aggregated graph features then serve as a soft prompt
to guide a large language model in generating accurate answers. Extensive
experiments on the M3D-VQA benchmark demonstrate that our approach consistently
outperforms baselines across multiple evaluation metrics, offering more robust
reasoning capabilities.

</details>


### [187] [Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions](https://arxiv.org/abs/2507.04377)
*Xiao Zhang,Johan Bos*

Main category: cs.CV

TL;DR: 本文介绍了一种多模态框架用于墓碑数字化，利用视觉语言模型和检索增强生成技术提升墓碑内容的解析准确性，并评估了模型在不同条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 许多墓碑面临显著的保存挑战，包括物理侵蚀、破坏、环境退化和政治变化。因此，需要一种改进墓碑内容解释、组织和检索的方法。

Method: 我们引入了一个多模态框架用于墓碑数字化，利用视觉语言模型（VLMs）将墓碑图像转换为结构化的墓碑意义表示（TMRs），并结合检索增强生成（RAG）以整合外部依赖元素。

Result: 与传统的OCR管道相比，我们的方法将解析准确率从F1分数36.1提高到89.5。我们还评估了模型在多样化语言和文化铭文中的鲁棒性，并通过图像融合模拟了物理退化以评估在噪声或损坏条件下的性能。

Conclusion: 我们的工作代表了首次尝试使用大型视觉语言模型来形式化墓碑理解，为遗产保护提供了意义。

Abstract: Tombstones are historically and culturally rich artifacts, encapsulating
individual lives, community memory, historical narratives and artistic
expression. Yet, many tombstones today face significant preservation
challenges, including physical erosion, vandalism, environmental degradation,
and political shifts. In this paper, we introduce a novel multi-modal framework
for tombstones digitization, aiming to improve the interpretation, organization
and retrieval of tombstone content. Our approach leverages vision-language
models (VLMs) to translate tombstone images into structured Tombstone Meaning
Representations (TMRs), capturing both image and text information. To further
enrich semantic parsing, we incorporate retrieval-augmented generation (RAG)
for integrate externally dependent elements such as toponyms, occupation codes,
and ontological concepts. Compared to traditional OCR-based pipelines, our
method improves parsing accuracy from an F1 score of 36.1 to 89.5. We
additionally evaluate the model's robustness across diverse linguistic and
cultural inscriptions, and simulate physical degradation through image fusion
to assess performance under noisy or damaged conditions. Our work represents
the first attempt to formalize tombstone understanding using large
vision-language models, presenting implications for heritage preservation.

</details>


### [188] [VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents](https://arxiv.org/abs/2507.04590)
*Rui Meng,Ziyan Jiang,Ye Liu,Mingyi Su,Xinyi Yang,Yuepeng Fu,Can Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Yingbo Zhou,Wenhu Chen,Semih Yavuz*

Main category: cs.CV

TL;DR: 本文提出了VLM2Vec-V2，一个统一的框架，用于学习跨多种视觉形式的嵌入。通过引入新的基准MMEB-V2，实验表明VLM2Vec-V2在视频和文档检索任务上表现优异，并在原始图像基准测试中优于之前的基线。研究揭示了多模态嵌入模型的泛化能力，并提出了有效的统一嵌入学习策略。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态嵌入模型如VLM2Vec、E5-V、GME主要关注自然图像，对其他视觉形式（如视频和视觉文档）的支持有限，这限制了它们在现实场景中的应用，如AI代理、多模态搜索和推荐以及检索增强生成（RAG）。因此，需要一种能够支持多种视觉形式的统一嵌入框架。

Method: 本文提出了一种统一的框架VLM2Vec-V2，用于学习跨多种视觉形式的嵌入。同时，引入了一个新的基准MMEB-V2，扩展了原始的MMEB，包括五个新的任务类型：视觉文档检索、视频检索、时间定位、视频分类和视频问答。

Result: VLM2Vec-V2在新引入的视频和文档检索任务上表现出色，并且在原始图像基准测试中也优于之前的基线。研究还提供了对各种多模态嵌入模型泛化能力的见解，并强调了有效的统一嵌入学习策略。

Conclusion: 本文提出了VLM2Vec-V2，一个统一的框架，用于学习跨多种视觉形式的嵌入。通过广泛的实验，VLM2Vec-V2在新引入的视频和文档检索任务上表现出色，并且在原始图像基准测试中也优于之前的基线。研究提供了对各种多模态嵌入模型泛化能力的见解，并强调了有效的统一嵌入学习策略，为研究和实际应用中的可扩展和适应性表示学习奠定了基础。

Abstract: Multimodal embedding models have been crucial in enabling various downstream
tasks such as semantic similarity, information retrieval, and clustering over
different modalities. However, existing multimodal embeddings like VLM2Vec,
E5-V, GME are predominantly focused on natural images, with limited support for
other visual forms such as videos and visual documents. This restricts their
applicability in real-world scenarios, including AI agents, multi-modal search
and recommendation, and retrieval-augmented generation (RAG). To close this
gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across
diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark
that extends MMEB with five new task types: visual document retrieval, video
retrieval, temporal grounding, video classification and video question
answering - spanning text, image, video, and visual document inputs. Next, we
train VLM2Vec-V2, a general-purpose embedding model that supports text, image,
video, and visual document inputs. Extensive experiments show that VLM2Vec-V2
achieves strong performance not only on the newly introduced video and document
retrieval tasks, but also improves over prior baselines on the original image
benchmarks. Through extensive evaluation, our study offers insights into the
generalizability of various multimodal embedding models and highlights
effective strategies for unified embedding learning, laying the groundwork for
more scalable and adaptable representation learning in both research and
real-world settings.

</details>


### [189] [From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach](https://arxiv.org/abs/2507.04815)
*Mihai Masala,Marius Leordeanu*

Main category: cs.CV

TL;DR: 本文提出了一种基于时空事件图的视觉和语言共享表示方法，用于生成复杂的视频描述，并展示了其在自监督神经分析系统中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前数据集在生成复杂语言描述方面存在不足，现有的方法在解释视觉和语言之间的关系方面仍面临挑战。

Method: 本文提出了一种基于时空事件图的视觉和语言共享表示方法，用于生成自然语言描述，并利用自监督神经分析系统进行训练。

Result: 本文的方法能够在多个不同数据集上生成连贯、丰富且相关的视频文本描述，并通过标准评估指标、人工标注和先进VLMs的共识进行了验证。

Conclusion: 本文提出了一种基于时空事件图的视觉和语言共享表示方法，能够生成连贯、丰富且相关的视频文本描述，并展示了其在自监督神经分析系统中作为自动教师的有效性。

Abstract: The task of describing video content in natural language is commonly referred
to as video captioning. Unlike conventional video captions, which are typically
brief and widely available, long-form paragraph descriptions in natural
language are scarce. This limitation of current datasets is due to the
expensive human manual annotation required and to the highly challenging task
of explaining the language formation process from the perspective of the
underlying story, as a complex system of interconnected events in space and
time. Through a thorough analysis of recently published methods and available
datasets, we identify a general lack of published resources dedicated to the
problem of describing videos in complex language, beyond the level of
descriptions in the form of enumerations of simple captions. Furthermore, while
state-of-the-art methods produce impressive results on the task of generating
shorter captions from videos by direct end-to-end learning between the videos
and text, the problem of explaining the relationship between vision and
language is still beyond our reach. In this work, we propose a shared
representation between vision and language, based on graphs of events in space
and time, which can be obtained in an explainable and analytical way, to
integrate and connect multiple vision tasks to produce the final natural
language description. Moreover, we also demonstrate how our automated and
explainable video description generation process can function as a fully
automatic teacher to effectively train direct, end-to-end neural student
pathways, within a self-supervised neuro-analytical system. We validate that
our explainable neuro-analytical approach generates coherent, rich and relevant
textual descriptions on videos collected from multiple varied datasets, using
both standard evaluation metrics, human annotations and consensus from
ensembles of state-of-the-art VLMs.

</details>


### [190] [Transcribing Spanish Texts from the Past: Experiments with Transkribus, Tesseract and Granite](https://arxiv.org/abs/2507.04878)
*Yanco Amor Torterolo-Orta,Jaione Macicior-Mitxelena,Marina Miguez-Lamanuzzi,Ana García-Serrano*

Main category: cs.CV

TL;DR: 本文介绍了GRESEL团队在IberLEF 2025共享任务PastReader中的实验和结果，包括使用基于网络的OCR服务、传统OCR引擎和多模态模型的三种实验。结果令人满意，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 参与IberLEF 2025共享任务PastReader：转录过去的文本，并实现不同方法之间的比较。

Method: 进行了三种类型的实验，包括使用基于网络的OCR服务、传统的OCR引擎和一个紧凑的多模态模型。所有实验都在消费级硬件上运行。

Result: 结果令人满意，但仍有改进空间。

Conclusion: 结果虽然令人满意，但仍有许多改进的空间。未来的工作将集中在使用由共享任务提供的西班牙语数据集探索新的技术和想法。

Abstract: This article presents the experiments and results obtained by the GRESEL team
in the IberLEF 2025 shared task PastReader: Transcribing Texts from the Past.
Three types of experiments were conducted with the dual aim of participating in
the task and enabling comparisons across different approaches. These included
the use of a web-based OCR service, a traditional OCR engine, and a compact
multimodal model. All experiments were run on consumer-grade hardware, which,
despite lacking high-performance computing capacity, provided sufficient
storage and stability. The results, while satisfactory, leave room for further
improvement. Future work will focus on exploring new techniques and ideas using
the Spanish-language dataset provided by the shared task, in collaboration with
Biblioteca Nacional de Espa\~na (BNE).

</details>


### [191] [ReLoop: "Seeing Twice and Thinking Backwards" via Closed-loop Training to Mitigate Hallucinations in Multimodal understanding](https://arxiv.org/abs/2507.04943)
*Jianjiang Yang,Ziyan Huang,Yanshu Li*

Main category: cs.CV

TL;DR: ReLoop is a framework that reduces hallucinations in MLLMs by encouraging multimodal consistency through a ring-shaped structure with three feedback mechanisms.


<details>
  <summary>Details</summary>
Motivation: MLLMs remain vulnerable to hallucinations, which are outputs that contradict or misrepresent input semantics. Existing methods often rely on external verification or post-hoc correction, lacking an internal mechanism to validate outputs directly during training.

Method: ReLoop is a unified closed-loop training framework that encourages multimodal consistency for cross-modal understanding in MLLMs. It adopts a ring-shaped structure that integrates three complementary consistency feedback mechanisms, obliging MLLMs to 'seeing twice and thinking backwards'.

Result: Extensive evaluations and analyses demonstrate the effectiveness of ReLoop in reducing hallucination rates across multiple benchmarks.

Conclusion: ReLoop is a robust method for mitigating hallucinations in MLLMs, and we will release our source code and data in the camera-ready version.

Abstract: While Multimodal Large Language Models (MLLMs) have achieved remarkable
progress in open-ended visual question answering, they remain vulnerable to
hallucinations. These are outputs that contradict or misrepresent input
semantics, posing a critical challenge to the reliability and factual
consistency. Existing methods often rely on external verification or post-hoc
correction, lacking an internal mechanism to validate outputs directly during
training. To bridge this gap, we propose ReLoop, a unified closed-loop training
framework that encourages multimodal consistency for cross-modal understanding
in MLLMs. ReLoop adopts a ring-shaped structure that integrates three
complementary consistency feedback mechanisms, obliging MLLMs to "seeing twice
and thinking backwards". Specifically, ReLoop employs the frozen Consistency
Feedback Plugin (CFP), comprising semantic reconstruction, visual description,
and an attention supervision module for attention alignment. These components
collectively enforce semantic reversibility, visual consistency, and
interpretable attention, enabling the model to correct its outputs during
training. Extensive evaluations and analyses demonstrate the effectiveness of
ReLoop in reducing hallucination rates across multiple benchmarks, establishing
a robust method for hallucination mitigation in MLLMs. We will release our
source code and data in the camera-ready version.

</details>


### [192] [Taming the Tri-Space Tension: ARC-Guided Hallucination Modeling and Control for Text-to-Image Generation](https://arxiv.org/abs/2507.04946)
*Jianjiang Yang,Ziyan Huang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite remarkable progress in image quality and prompt fidelity,
text-to-image (T2I) diffusion models continue to exhibit persistent
"hallucinations", where generated content subtly or significantly diverges from
the intended prompt semantics. While often regarded as unpredictable artifacts,
we argue that these failures reflect deeper, structured misalignments within
the generative process. In this work, we propose a cognitively inspired
perspective that reinterprets hallucinations as trajectory drift within a
latent alignment space. Empirical observations reveal that generation unfolds
within a multiaxial cognitive tension field, where the model must continuously
negotiate competing demands across three key critical axes: semantic coherence,
structural alignment, and knowledge grounding. We then formalize this
three-axis space as the \textbf{Hallucination Tri-Space} and introduce the
Alignment Risk Code (ARC): a dynamic vector representation that quantifies
real-time alignment tension during generation. The magnitude of ARC captures
overall misalignment, its direction identifies the dominant failure axis, and
its imbalance reflects tension asymmetry. Based on this formulation, we develop
the TensionModulator (TM-ARC): a lightweight controller that operates entirely
in latent space. TM-ARC monitors ARC signals and applies targeted,
axis-specific interventions during the sampling process. Extensive experiments
on standard T2I benchmarks demonstrate that our approach significantly reduces
hallucination without compromising image quality or diversity. This framework
offers a unified and interpretable approach for understanding and mitigating
generative failures in diffusion-based T2I systems.

</details>


### [193] [Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models](https://arxiv.org/abs/2507.04976)
*Eunseop Yoon,Hee Suk Yoon,Mark A. Hasegawa-Johnson,Chang D. Yoo*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the broader context of deep learning, Multimodal Large Language Models
have achieved significant breakthroughs by leveraging powerful Large Language
Models as a backbone to align different modalities into the language space. A
prime exemplification is the development of Video Large Language Models
(Video-LLMs). While numerous advancements have been proposed to enhance the
video understanding capabilities of these models, they are predominantly
trained on questions generated directly from video content. However, in
real-world scenarios, users often pose questions that extend beyond the
informational scope of the video, highlighting the need for Video-LLMs to
assess the relevance of the question. We demonstrate that even the
best-performing Video-LLMs fail to reject unfit questions-not necessarily due
to a lack of video understanding, but because they have not been trained to
identify and refuse such questions. To address this limitation, we propose
alignment for answerability, a framework that equips Video-LLMs with the
ability to evaluate the relevance of a question based on the input video and
appropriately decline to answer when the question exceeds the scope of the
video, as well as an evaluation framework with a comprehensive set of metrics
designed to measure model behavior before and after alignment. Furthermore, we
present a pipeline for creating a dataset specifically tailored for alignment
for answerability, leveraging existing video-description paired datasets.

</details>


### [194] [AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics](https://arxiv.org/abs/2507.05063)
*Jan Carreras Boada,Rao Muhammad Umer,Carsten Marr*

Main category: cs.CV

TL;DR: 本研究展示了通过微调稳定扩散模型生成的合成图像能够显著提升有限数据下分类器的性能，从而在生物医学领域中改善机器学习模型和促进医学诊断。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据集通常包含大量样本不平衡并受到严格的隐私限制，这阻碍了准确机器学习模型的发展。生成合成图像可以提高数据可用性同时保护患者隐私，但生成足够质量的合成图像对于训练鲁棒分类器仍然困难。

Method: 我们使用微调的稳定扩散模型和LoRA权重生成合成图像，并通过真实少数样本引导，以增强分类器性能。

Result: 当训练ResNet分类器时，准确率从27.3%增加到78.4%（+51.1%），通过为每个类别添加5000张合成图像到一个小而高度不平衡的真实数据集中。对于基于CLIP的分类器，准确率从61.8%提高到76.8%（+15.0%）。合成图像与真实图像高度相似，可以帮助克服数据集限制，提高模型泛化能力。

Conclusion: 我们的结果确立了合成图像作为生物医学研究中的工具，提高了机器学习模型，并促进了医学诊断和研究。

Abstract: Biomedical datasets often contain a large sample imbalance and are subject to
strict privacy constraints, which together hinder the development of accurate
machine learning models. One potential solution is to generate synthetic
images, as this can improve data availability while preserving patient privacy.
However, it remains difficult to generate synthetic images of sufficient
quality for training robust classifiers. In this work, we focus on the
classification of single white blood cells, a key component in the diagnosis of
hematological diseases such as acute myeloid leukemia (AML), a severe blood
cancer. We demonstrate how synthetic images generated with a fine-tuned stable
diffusion model using LoRA weights when guided by real few-shot samples of the
target white blood cell classes, can enhance classifier performance for limited
data. When training a ResNet classifier, accuracy increased from 27.3\% to
78.4\% (+51.1\%) by adding 5000 synthetic images per class to a small and
highly imbalanced real dataset. For a CLIP-based classifier, the accuracy
improved from 61.8\% to 76.8\% (+15.0\%). The synthetic images are highly
similar to real images, and they can help overcome dataset limitations,
enhancing model generalization. Our results establish synthetic images as a
tool in biomedical research, improving machine learning models, and
facilitating medical diagnosis and research.

</details>


### [195] [Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration](https://arxiv.org/abs/2507.05108)
*Yuyi Zhang,Peirong Zhang,Zhenhua Yang,Pengyu Yan,Yongxin Shi,Pengwei Liu,Fengjun Guo,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文提出了一个全页历史文献修复数据集（FPHDR）和一种新型自动化HDR解决方案（AutoHDR），能够显著提高OCR准确性，并通过人机协作进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的历史文献修复方法主要集中在单模态或有限尺寸的修复上，无法满足实际需求。

Method: AutoHDR通过三个阶段的方法模拟历史学家的修复流程：OCR辅助的损伤定位、视觉-语言上下文文本预测和补丁自回归外观修复。

Result: 实验表明AutoHDR在HDR中表现出色。当处理严重损坏的文档时，我们的方法将OCR准确性从46.83%提高到84.05%，通过人机协作进一步提高到94.25%。

Conclusion: 本文认为这项工作代表了在自动化历史文献修复方面的重大进展，并对文化遗产保护做出了重要贡献。

Abstract: Historical documents represent an invaluable cultural heritage, yet have
undergone significant degradation over time through tears, water erosion, and
oxidation. Existing Historical Document Restoration (HDR) methods primarily
focus on single modality or limited-size restoration, failing to meet practical
needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel
automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and
6,543 synthetic images with character-level and line-level locations, as well
as character annotations in different damage grades. AutoHDR mimics historians'
restoration workflows through a three-stage approach: OCR-assisted damage
localization, vision-language context text prediction, and patch autoregressive
appearance restoration. The modular architecture of AutoHDR enables seamless
human-machine collaboration, allowing for flexible intervention and
optimization at each restoration stage. Experiments demonstrate AutoHDR's
remarkable performance in HDR. When processing severely damaged documents, our
method improves OCR accuracy from 46.83\% to 84.05\%, with further enhancement
to 94.25\% through human-machine collaboration. We believe this work represents
a significant advancement in automated historical document restoration and
contributes substantially to cultural heritage preservation. The model and
dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.

</details>


### [196] [Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning](https://arxiv.org/abs/2507.05255)
*Yana Wei,Liang Zhao,Jianjian Sun,Kangheng Lin,Jisheng Yin,Jingcheng Hu,Yinmin Zhang,En Yu,Haoran Lv,Zejia Weng,Jia Wang,Chunrui Han,Yuang Peng,Qi Han,Zheng Ge,Xiangyu Zhang,Daxin Jiang,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，通过两阶段范式将大型语言模型的强化原则应用于多模态LLMs，实现了先进的视觉推理能力，并在多个基准测试中取得了最佳结果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在将大型语言模型（LLMs）通过可验证奖励进行强化的原则转移到多模态LLMs（MLLMs），以解锁高级视觉推理能力。

Method: 本文引入了一个两阶段范式，基于Qwen2.5-VL-7B进行大规模语言冷启动微调，然后进行跨近1000步的多模态强化学习（RL），超越了所有之前的开源努力。

Result: 本文提出的模型Open-Vision-Reasoner (OVR)在多个推理基准测试中取得了最先进的性能，包括MATH500上的95.3%、MathVision上的51.8%和MathVerse上的54.6%。

Conclusion: 本文提出的模型Open-Vision-Reasoner (OVR)在多个推理基准测试中取得了最先进的性能，并释放了模型、数据和训练动态以促进更强大且行为对齐的多模态推理器的发展。

Abstract: The remarkable reasoning capability of large language models (LLMs) stems
from cognitive behaviors that emerge through reinforcement with verifiable
rewards. This work investigates how to transfer this principle to Multimodal
LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage
paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning,
followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps,
surpassing all previous open-source efforts in scale. This pioneering work
reveals three fundamental insights: 1) Behavior transfer emerges surprisingly
early in cold start due to linguistic mental imagery. 2) Cold start broadly
memorizes visual behaviors, while RL critically discerns and scales up
effective patterns. 3) Transfer strategically favors high-utility behaviors
such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR),
achieves state-of-the-art performance on a suite of reasoning benchmarks,
including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We
release our model, data, and training dynamics to catalyze the development of
more capable, behavior-aligned multimodal reasoners.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [197] [RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based Parkinson's Disease Classification](https://arxiv.org/abs/2507.03594)
*Terry Yi Zhong,Cristian Tejedor-Garcia,Martha Larson,Bastiaan R. Bloem*

Main category: cs.SD

TL;DR: 本文提出了一种新的、稳健且可解释的交叉注意力架构RECA-PD，用于基于语音的帕金森病检测，该方法在保持高性能的同时提供了更一致和更有临床意义的解释。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）影响全球超过1000万人，语音障碍通常在运动症状前数年出现，使语音成为早期、非侵入性检测的有价值模态。尽管最近的深度学习模型取得了高精度，但它们通常缺乏临床使用所需的可解释性。

Method: 我们提出了RECA-PD，这是一种新颖、稳健且可解释的交叉注意力架构，结合了可解释的语音特征与自监督表示。

Result: RECA-PD在基于语音的PD检测中达到了最先进的性能，同时提供了更一致且更具临床意义的解释。此外，我们证明了在某些语音任务（如独白）中的性能下降可以通过分割长录音来缓解。

Conclusion: 我们的研究结果表明，性能和可解释性并不一定是互斥的。未来的工作将提高非专家的解释可用性，并探索严重程度估计以增加临床相关性。

Abstract: Parkinson's Disease (PD) affects over 10 million people globally, with speech
impairments often preceding motor symptoms by years, making speech a valuable
modality for early, non-invasive detection. While recent deep-learning models
achieve high accuracy, they typically lack the explainability required for
clinical use. To address this, we propose RECA-PD, a novel, robust, and
explainable cross-attention architecture that combines interpretable speech
features with self-supervised representations. RECA-PD matches state-of-the-art
performance in Speech-based PD detection while providing explanations that are
more consistent and more clinically meaningful. Additionally, we demonstrate
that performance degradation in certain speech tasks (e.g., monologue) can be
mitigated by segmenting long recordings. Our findings indicate that performance
and explainability are not necessarily mutually exclusive. Future work will
enhance the usability of explanations for non-experts and explore severity
estimation to increase the real-world clinical relevance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [198] [Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to Steal A Model!](https://arxiv.org/abs/2507.03014)
*Do-hyeon Yoon,Minsoo Chun,Thomas Allen,Hans Müller,Min Wang,Rajesh Sharma*

Main category: cs.CR

TL;DR: 本文提出了一种基于内在模型特性的鲁棒LLM指纹识别方法，通过分析注意力参数矩阵的标准差分布模式，这些模式在持续训练后仍保持稳定，可以用来识别模型谱系和检测版权侵权。实验结果表明该方法有效，并揭示了华为Pangu Pro MoE模型可能是从Qwen-2.5 14B模型升级而来，存在模型抄袭和版权侵犯的可能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临显著的版权和知识产权挑战，随着训练成本的增加和模型重用的普及，现有的水印技术可能无法抵御持续训练和开发，对模型归属和版权保护构成严重威胁。

Method: 本文提出了一种基于内在模型特性的简单而有效的鲁棒LLM指纹识别方法，通过分析不同层中注意力参数矩阵的标准差分布模式，这些模式在经过大量持续训练后仍然保持稳定，作为鲁棒的指纹来可靠地识别模型谱系和检测潜在的版权侵权。

Result: 实验验证表明，该方法在多个模型家族中有效，能够可靠地进行模型认证。此外，研究发现华为最近发布的Pangu Pro MoE模型是通过升级技术从Qwen-2.5 14B模型派生出来的，而不是从头开始训练，这表明可能存在模型抄袭、版权侵犯和信息伪造的情况。

Conclusion: 本文强调了开发稳健的指纹识别方法对于保护大规模模型开发中的知识产权的重要性，并指出单纯的持续训练不足以完全掩盖模型来源。

Abstract: Large language models (LLMs) face significant copyright and intellectual
property challenges as the cost of training increases and model reuse becomes
prevalent. While watermarking techniques have been proposed to protect model
ownership, they may not be robust to continue training and development, posing
serious threats to model attribution and copyright protection. This work
introduces a simple yet effective approach for robust LLM fingerprinting based
on intrinsic model characteristics. We discover that the standard deviation
distributions of attention parameter matrices across different layers exhibit
distinctive patterns that remain stable even after extensive continued
training. These parameter distribution signatures serve as robust fingerprints
that can reliably identify model lineage and detect potential copyright
infringement. Our experimental validation across multiple model families
demonstrates the effectiveness of our method for model authentication. Notably,
our investigation uncovers evidence that a recently Pangu Pro MoE model
released by Huawei is derived from Qwen-2.5 14B model through upcycling
techniques rather than training from scratch, highlighting potential cases of
model plagiarism, copyright violation, and information fabrication. These
findings underscore the critical importance of developing robust fingerprinting
methods for protecting intellectual property in large-scale model development
and emphasize that deliberate continued training alone is insufficient to
completely obscure model origins.

</details>


### [199] [Improving LLM Reasoning for Vulnerability Detection via Group Relative Policy Optimization](https://arxiv.org/abs/2507.03051)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.CR

TL;DR: 本文研究了基于强化学习的微调技术在软件漏洞检测中的应用，发现这种方法可以提高大型语言模型的性能和推理能力。


<details>
  <summary>Details</summary>
Motivation: 改进和理解大型语言模型的训练动态和推理对于它们在基于人工智能的安全工具中的部署至关重要。

Method: 我们探索了使用组相对策略优化（GRPO）来指导大型语言模型的行为，通过结构化的基于规则的奖励。

Result: 我们的研究结果提供了关于基于强化学习的训练如何增强大型语言模型在软件漏洞检测中的性能和推理能力的见解。

Conclusion: 我们的研究结果表明，基于强化学习的训练方法可以提高大型语言模型在软件漏洞检测中的性能和推理能力。

Abstract: Improving and understanding the training dynamics and reasoning of Large
Language Models (LLMs) has become essential for their deployment in AI-based
security tools, such as software vulnerability detection. In this work, we
present an extensive study aimed at advancing recent RL-based finetuning
techniques for LLMs in the context of vulnerability detection.
  We start by highlighting key limitations of commonly adopted LLMs, such as
their tendency to over-predict certain types of vulnerabilities while failing
to detect others. To address this challenge, we explore the use of Group
Relative Policy Optimization (GRPO), a recent policy-gradient method, for
guiding LLM behavior through structured, rule-based rewards. We enable its
application to the vulnerability detection task by redefining its advantage
functions and reward signals using annotations from widely used datasets in the
field, including BigVul, DiverseVul, and CleanVul.
  The proposed methodology enables an extensive set of experiments, addressing
multiple research questions regarding the impact of GRPO on generalization,
reasoning capabilities, and performance improvements over standard supervised
finetuning (SFT). Our findings offer valuable insights into the potential of
RL-based training to enhance both the performance and reasoning abilities of
LLMs in the context of software vulnerability detection.

</details>


### [200] [Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs](https://arxiv.org/abs/2507.04365)
*Xiaomeng Hu,Pin-Yu Chen,Tsung-Yi Ho*

Main category: cs.CR

TL;DR: This paper reveals a universal phenomenon called Attention Slipping during jailbreak attacks and proposes a new defense called Attention Sharpening to counter it.


<details>
  <summary>Details</summary>
Motivation: Ensuring the safety of large language models (LLMs) is essential as they become more integral to society and technology. Jailbreak attacks exploit vulnerabilities to bypass safety guardrails, posing a significant threat.

Method: Attention Sharpening, a new defense that directly counters Attention Slipping by sharpening the attention score distribution using temperature scaling.

Result: Experiments on four leading LLMs show that our method effectively resists various jailbreak attacks while maintaining performance on benign tasks on AlpacaEval.

Conclusion: Attention Sharpening introduces no additional computational or memory overhead, making it an efficient and practical solution for real-world deployment.

Abstract: As large language models (LLMs) become more integral to society and
technology, ensuring their safety becomes essential. Jailbreak attacks exploit
vulnerabilities to bypass safety guardrails, posing a significant threat.
However, the mechanisms enabling these attacks are not well understood. In this
paper, we reveal a universal phenomenon that occurs during jailbreak attacks:
Attention Slipping. During this phenomenon, the model gradually reduces the
attention it allocates to unsafe requests in a user query during the attack
process, ultimately causing a jailbreak. We show Attention Slipping is
consistent across various jailbreak methods, including gradient-based token
replacement, prompt-level template refinement, and in-context learning.
Additionally, we evaluate two defenses based on query perturbation, Token
Highlighter and SmoothLLM, and find they indirectly mitigate Attention
Slipping, with their effectiveness positively correlated with the degree of
mitigation achieved. Inspired by this finding, we propose Attention Sharpening,
a new defense that directly counters Attention Slipping by sharpening the
attention score distribution using temperature scaling. Experiments on four
leading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)
show that our method effectively resists various jailbreak attacks while
maintaining performance on benign tasks on AlpacaEval. Importantly, Attention
Sharpening introduces no additional computational or memory overhead, making it
an efficient and practical solution for real-world deployment.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [201] [An HTR-LLM Workflow for High-Accuracy Transcription and Analysis of Abbreviated Latin Court Hand](https://arxiv.org/abs/2507.04132)
*Joshua D. Isom*

Main category: cs.DL

TL;DR: 本文提出了一种四阶段的工作流程，用于高精度转录和分析中世纪法律文件，利用HTR模型和LLM技术，实现了低错误率的高质量输出。


<details>
  <summary>Details</summary>
Motivation: 中世纪法律文件的转录和分析是一项复杂且耗时的任务，需要高精度的方法来处理挑战性文本。

Method: 该研究提出了一种四阶段工作流程，包括使用专门的手写文本识别（HTR）模型进行初始转录，利用大型语言模型（LLM）进行多模态后校正，使用提示引导的LLM扩展为完整的学术拉丁文，以及最后的命名实体校正（NEC）。

Result: 通过详细的案例研究验证了该工作流程，实现了2-7%的单词错误率（WER），相对于学术基准而言。

Conclusion: 该研究展示了通过四阶段工作流程实现高精度转录和分析中世纪法律文件的可行性，证明了这种混合多阶段方法能够有效自动化最繁琐的转录任务，并产生高质量、可分析的输出，代表了一种强大且实用的解决方案。

Abstract: This article presents and validates an ideal, four-stage workflow for the
high-accuracy transcription and analysis of challenging medieval legal
documents. The process begins with a specialized Handwritten Text Recognition
(HTR) model, itself created using a novel "Clean Ground Truth" curation method
where a Large Language Model (LLM) refines the training data. This HTR model
provides a robust baseline transcription (Stage 1). In Stage 2, this baseline
is fed, along with the original document image, to an LLM for multimodal
post-correction, grounding the LLM's analysis and improving accuracy. The
corrected, abbreviated text is then expanded into full, scholarly Latin using a
prompt-guided LLM (Stage 3). A final LLM pass performs Named-Entity Correction
(NEC), regularizing proper nouns and generating plausible alternatives for
ambiguous readings (Stage 4). We validate this workflow through detailed case
studies, achieving Word Error Rates (WER) in the range of 2-7% against
scholarly ground truths. The results demonstrate that this hybrid, multi-stage
approach effectively automates the most laborious aspects of transcription
while producing a high-quality, analyzable output, representing a powerful and
practical solution for the current technological landscape.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [202] [Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy](https://arxiv.org/abs/2507.03620)
*Francisca Lemos,Victor Alves,Filipa Ferraz*

Main category: cs.SE

TL;DR: 本研究探讨了DSPy框架在多个用例中的应用，结果显示其能够提高LLM性能，特别是在指令调优和示例选择同时优化时。然而，不同任务的效果有所不同，强调了评估特定用例的重要性。


<details>
  <summary>Details</summary>
Motivation: 虽然提示工程是释放大型语言模型（LLM）潜力的核心，但创建有效的提示仍然是一个耗时的试错过程，依赖于人类直觉。

Method: 本研究调查了Declarative Self-improving Python (DSPy)，这是一个程序化创建和优化提示的优化框架，并应用于五个用例：守则执行、代码中的幻觉检测、代码生成、路由代理和提示评估。

Result: 在某些情况下，如守则用例中显示出微小的改进，在幻觉检测中显示出选择性的增强，而在提示评估标准任务中，准确率从46.2%显著提高到64.0%。在路由器代理案例中，探索了改善表现不佳的提示以及通过优化提示使较小的模型与更强的模型相匹配的可能性。虽然提示优化提高了准确性，但使用优化后的提示并没有提高性能。

Conclusion: DSPy的系统性提示优化可以提高LLM性能，尤其是在指令调优和示例选择同时优化时。然而，影响因任务而异，强调了在提示优化研究中评估特定用例的重要性。

Abstract: Although prompt engineering is central to unlocking the full potential of
Large Language Models (LLMs), crafting effective prompts remains a
time-consuming trial-and-error process that relies on human intuition. This
study investigates Declarative Self-improving Python (DSPy), an optimization
framework that programmatically creates and refines prompts, applied to five
use cases: guardrail enforcement, hallucination detection in code, code
generation, routing agents, and prompt evaluation. Each use case explores how
prompt optimization via DSPy influences performance. While some cases
demonstrated modest improvements - such as minor gains in the guardrails use
case and selective enhancements in hallucination detection - others showed
notable benefits. The prompt evaluation criterion task demonstrated a
substantial performance increase, rising accuracy from 46.2% to 64.0%. In the
router agent case, the possibility of improving a poorly performing prompt and
of a smaller model matching a stronger one through optimized prompting was
explored. Although prompt refinement increased accuracy from 85.0% to 90.0%,
using the optimized prompt with a cheaper model did not improve performance.
Overall, this study's findings suggest that DSPy's systematic prompt
optimization can enhance LLM performance, particularly when instruction tuning
and example selection are optimized together. However, the impact varies by
task, highlighting the importance of evaluating specific use cases in prompt
optimization research.

</details>


### [203] [Efficient Detection of Intermittent Job Failures Using Few-Shot Learning](https://arxiv.org/abs/2507.04173)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 本研究提出了一种基于少样本学习的新方法来检测间歇性作业失败，该方法在所有项目中实现了较高的F1分数，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发人员在使用持续集成（CI）和部署管道时面临的一个主要挑战是间歇性作业失败的发生，这些失败是由意外的非确定性问题（如易碎测试或基础设施问题）引起的，而不是常规的代码相关错误。现有的研究开发了机器学习（ML）模型来分类作业失败为间歇性或常规。然而，这种方法在重新运行可疑作业失败不是明确政策的情况下会将间歇性作业失败错误标记为常规，从而限制了其实际性能。

Method: 本文引入了一种基于少样本学习（FSL）的新方法来检测间歇性作业失败。具体来说，我们使用少量手动标记的日志示例对小型语言模型进行微调，以生成丰富的嵌入，然后用于训练机器学习分类器。

Result: 我们的FSL方法在所有项目中仅使用12个样本就实现了70-88%的F1分数，优于SOTA方法，在4个项目中表现不佳（34-52%的F1分数）。

Conclusion: 本研究强调了数据质量比数量更重要，并为组织检测间歇性作业失败提供了一个更高效和实用的框架。

Abstract: One of the main challenges developers face in the use of continuous
integration (CI) and deployment pipelines is the occurrence of intermittent job
failures, which result from unexpected non-deterministic issues (e.g., flaky
tests or infrastructure problems) rather than regular code-related errors such
as bugs. Prior studies developed machine-learning (ML) models trained on large
datasets of job logs to classify job failures as either intermittent or
regular. As an alternative to costly manual labeling of large datasets, the
state-of-the-art (SOTA) approach leveraged a heuristic based on
non-deterministic job reruns. However, this method mislabels intermittent job
failures as regular in contexts where rerunning suspicious job failures is not
an explicit policy, and therefore limits the SOTA's performance in practice. In
fact, our manual analysis of 2,125 job failures from 5 industrial and 1
open-source projects reveals that, on average, 32\% of intermittent job
failures are mislabeled as regular. To address these limitations, this paper
introduces a novel approach to intermittent job failure detection using
few-shot learning (FSL). Specifically, we fine-tune a small language model
using a few number of manually labeled log examples to generate rich
embeddings, which are then used to train an ML classifier. Our FSL-based
approach achieves 70-88\% F1-score with only 12 shots in all projects,
outperforming the SOTA, which proved ineffective (34-52\% F1-score) in 4
projects. Overall, this study underlines the importance of data quality over
quantity and provides a more efficient and practical framework for the
detection of intermittent job failures in organizations.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [204] [Does Overnight News Explain Overnight Returns?](https://arxiv.org/abs/2507.04481)
*Paul Glasserman,Kriste Krstovski,Paul Laliberte,Harry Mamaysky*

Main category: q-fin.TR

TL;DR: 我们的研究发现，新闻内容的变化和对新闻内容的反应差异是导致盘中和隔夜收益差异的重要因素，并且我们的方法能够预测哪些股票在隔夜表现良好，哪些在盘中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 过去30年里，美国股市的大部分收益是在夜间获得的，而平均盘中收益为负或持平。我们需要理解这种现象的原因，并探索新闻内容如何影响市场收益。

Method: 我们使用了240万篇新闻文章的数据集，并应用了一种新的监督主题分析技术，该技术基于其解释同时市场收益的能力来选择新闻主题。

Result: 我们发现，新闻主题的流行程度变化和对新闻主题的反应差异是导致盘中和隔夜收益差异的重要因素。此外，我们的方法能够在样本外测试中预测哪些股票在隔夜表现良好，哪些在盘中表现不佳。

Conclusion: 我们的研究结果表明，新闻内容的变化和对新闻内容的反应差异是导致盘中和隔夜收益差异的重要因素。此外，我们的方法能够预测哪些股票在隔夜表现良好，哪些在盘中表现不佳，并有助于解释盘中和隔夜收益的延续和逆转模式。

Abstract: Over the past 30 years, nearly all the gains in the U.S. stock market have
been earned overnight, while average intraday returns have been negative or
flat. We find that a large part of this effect can be explained through
features of intraday and overnight news. Our analysis uses a collection of 2.4
million news articles. We apply a novel technique for supervised topic analysis
that selects news topics based on their ability to explain contemporaneous
market returns. We find that time variation in the prevalence of news topics
and differences in the responses to news topics both contribute to the
difference in intraday and overnight returns. In out-of-sample tests, our
approach forecasts which stocks will do particularly well overnight and
particularly poorly intraday. Our approach also helps explain patterns of
continuation and reversal in intraday and overnight returns. We contrast the
effect of news with other mechanisms proposed in the literature to explain
overnight returns.

</details>

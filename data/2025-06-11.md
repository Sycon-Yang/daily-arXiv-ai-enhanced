<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 79]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.IR](#cs.IR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 11]
- [q-fin.ST](#q-fin.ST) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/abs/2506.08120)
*Toyin Aguda,Erik Wilson,Allan Anzagira,Simerjot Kaur,Charese Smiley*

Main category: cs.CL

TL;DR: This paper systematically evaluates the trade-off between conservative bias and hallucination in large language models' relation extraction tasks.


<details>
  <summary>Details</summary>
Motivation: To prevent incorrect relation assignments while minimizing information loss.

Method: Systematic evaluation across multiple prompts, datasets, and relation types, introducing the concept of Hobson's choice.

Result: Conservative bias occurs twice as often as hallucination.

Conclusion: The findings suggest that there is a need to balance conservative bias and hallucination in relation extraction tasks.

Abstract: Large language models (LLMs) exhibit pronounced conservative bias in relation
extraction tasks, frequently defaulting to No_Relation label when an
appropriate option is unavailable. While this behavior helps prevent incorrect
relation assignments, our analysis reveals that it also leads to significant
information loss when reasoning is not explicitly included in the output. We
systematically evaluate this trade-off across multiple prompts, datasets, and
relation types, introducing the concept of Hobson's choice to capture scenarios
where models opt for safe but uninformative labels over hallucinated ones. Our
findings suggest that conservative bias occurs twice as often as hallucination.
To quantify this effect, we use SBERT and LLM prompts to capture the semantic
similarity between conservative bias behaviors in constrained prompts and
labels generated from semi-constrained and open-ended prompts.

</details>


### [2] [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
*Jacob Dineen,Aswin RRV,Qin Liu,Zhikun Xu,Xiao Ye,Ming Shen,Zhaonan Li,Shijie Lu,Chitta Baral,Muhao Chen,Ben Zhou*

Main category: cs.CL

TL;DR: Introduce QA-LIGN, an automatic symbolic reward decomposition approach to preserve the structure of each constitutional principle within the reward mechanism, offering greater transparency and adaptability in aligning large language models with explicit principles.


<details>
  <summary>Details</summary>
Motivation: Standard reward-based alignment methods hinder interpretability by collapsing diverse feedback into a single scalar reward, entangling multiple objectives into one opaque training signal.

Method: QA-LIGN formulates principle-specific evaluation questions and derives separate reward components for each principle, providing a drop-in reward model replacement.

Result: Experiments show that QA-LIGN offers greater transparency and adaptability in the alignment process while achieving performance on par with or better than a DPO baseline.

Conclusion: QA-LIGN represents a step toward more interpretable and controllable alignment of language models without sacrificing end-task performance.

Abstract: Alignment of large language models with explicit principles (such as
helpfulness, honesty, and harmlessness) is crucial for ensuring safe and
reliable AI systems. However, standard reward-based alignment methods typically
collapse diverse feedback into a single scalar reward, entangling multiple
objectives into one opaque training signal, which hinders interpretability. In
this work, we introduce QA-LIGN, an automatic symbolic reward decomposition
approach that preserves the structure of each constitutional principle within
the reward mechanism. Instead of training a black-box reward model that outputs
a monolithic score, QA-LIGN formulates principle-specific evaluation questions
and derives separate reward components for each principle, making it a drop-in
reward model replacement. Experiments aligning an uncensored large language
model with a set of constitutional principles demonstrate that QA-LIGN offers
greater transparency and adaptability in the alignment process. At the same
time, our approach achieves performance on par with or better than a DPO
baseline. Overall, these results represent a step toward more interpretable and
controllable alignment of language models, achieved without sacrificing
end-task performance.

</details>


### [3] [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)
*Zefang Liu,Yinzhu Quan*

Main category: cs.CL

TL;DR: Introduce EconWebArena, a benchmark for complex economic tasks in realistic web environments. It includes 360 tasks from authoritative sites across various domains. Evaluate multimodal LLMs, analyze failures, and identify challenges.


<details>
  <summary>Details</summary>
Motivation: Create a benchmark emphasizing fidelity to authoritative data sources and grounded web-based economic reasoning.

Method: Prompt multiple LLMs to generate tasks, then curate them for clarity, feasibility, and reliability.

Result: Evaluate diverse state-of-the-art LLMs as web agents, revealing performance gaps and challenges in grounding, navigation, and multimodal understanding.

Conclusion: EconWebArena serves as a rigorous testbed for economic web intelligence.

Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on
complex, multimodal economic tasks in realistic web environments. The benchmark
comprises 360 curated tasks from 82 authoritative websites spanning domains
such as macroeconomics, labor, finance, trade, and public policy. Each task
challenges agents to navigate live websites, interpret structured and visual
content, interact with real interfaces, and extract precise, time-sensitive
data through multi-step workflows. We construct the benchmark by prompting
multiple large language models (LLMs) to generate candidate tasks, followed by
rigorous human curation to ensure clarity, feasibility, and source reliability.
Unlike prior work, EconWebArena emphasizes fidelity to authoritative data
sources and the need for grounded web-based economic reasoning. We evaluate a
diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure
cases, and conduct ablation studies to assess the impact of visual grounding,
plan-based reasoning, and interaction design. Our results reveal substantial
performance gaps and highlight persistent challenges in grounding, navigation,
and multimodal understanding, positioning EconWebArena as a rigorous testbed
for economic web intelligence.

</details>


### [4] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
*Muhammad Usman,Muhammad Ahmad,M. Shahiki Tash,Irina Gelbukh,Rolando Quintero Tellez,Grigori Sidorov*

Main category: cs.CL

TL;DR: This paper presents a trilingual dataset for detecting hate speech in English, Urdu, and Spanish. It also introduces a method that uses attention layers with transformer-based models and large language models to improve multilingual hate speech detection, showing improved performance over traditional models.


<details>
  <summary>Details</summary>
Motivation: Hate speech is a significant problem on social media platforms, but it has not been sufficiently studied in Urdu. This work aims to address this gap by creating a multilingual dataset and detection method.

Method: The method involves using attention layers as a precursor to transformer-based models and large language models for feature extraction. Traditional machine learning models like SVM and other transformers were also used for comparison. Three annotators ensured dataset quality.

Result: The proposed approach achieved strong performance in hate speech detection for all three languages, improving upon the traditional SVM baseline.

Conclusion: This study provides a robust solution for multilingual hate speech detection, which can help make digital communities safer.

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [5] [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2506.08158)
*Lijing Zhu,Qizhen Lan,Qing Tian,Wenbo Sun,Li Yang,Lu Xia,Yixin Xie,Xi Xiao,Tiehang Duan,Cui Tao,Shuteng Niu*

Main category: cs.CL

TL;DR: A novel method called ETT-CKGE is introduced to improve continual knowledge graph embedding efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing methods face challenges in efficiency and scalability due to suboptimal knowledge preservation and computationally expensive graph traversal.

Method: ETT-CKGE uses learnable tokens to capture task-relevant signals and achieve knowledge transfer through simple matrix operations.

Result: ETT-CKGE shows superior or competitive predictive performance and significantly improves training efficiency and scalability compared to state-of-the-art methods.

Conclusion: ETT-CKGE provides an efficient and scalable solution for continual knowledge graph embedding.

Abstract: Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge
while preserving past information. However, existing methods struggle with
efficiency and scalability due to two key limitations: (1) suboptimal knowledge
preservation between snapshots caused by manually designed node/relation
importance scores that ignore graph dependencies relevant to the downstream
task, and (2) computationally expensive graph traversal for node/relation
importance calculation, leading to slow training and high memory overhead. To
address these limitations, we introduce ETT-CKGE (Efficient, Task-driven,
Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE
method that leverages efficient task-driven tokens for efficient and effective
knowledge transfer between snapshots. Our method introduces a set of learnable
tokens that directly capture task-relevant signals, eliminating the need for
explicit node scoring or traversal. These tokens serve as consistent and
reusable guidance across snapshots, enabling efficient token-masked embedding
alignment between snapshots. Importantly, knowledge transfer is achieved
through simple matrix operations, significantly reducing training time and
memory usage. Extensive experiments across six benchmark datasets demonstrate
that ETT-CKGE consistently achieves superior or competitive predictive
performance, while substantially improving training efficiency and scalability
compared to state-of-the-art CKGE methods. The code is available at:
https://github.com/lijingzhu1/ETT-CKGE/tree/main

</details>


### [6] [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/abs/2506.08172)
*Gerardo Aleman Manzanarez,Nora de la Cruz Arana,Jorge Garcia Flores,Yobany Garcia Medina,Raul Monroy,Nathalie Pernelle*

Main category: cs.CL

TL;DR: 提出GrAImes协议来评估AI生成微小说的文学价值，包括主题一致性、文本清晰度、解释深度和美学质量等方面，并验证了该协议的有效性。


<details>
  <summary>Details</summary>
Motivation: 评估AI生成微小说的文学价值，特别是在美学品质方面。

Method: 提出GrAImes协议，基于文学理论，提供评估AI生成微小说的客观框架。

Result: 验证了评价协议的结果，由文学专家和文学爱好者回答。

Conclusion: 该协议将成为评估自动产生的微小说及其文学价值的基础。

Abstract: Automated story writing has been a subject of study for over 60 years. Large
language models can generate narratively consistent and linguistically coherent
short fiction texts. Despite these advancements, rigorous assessment of such
outputs for literary merit - especially concerning aesthetic qualities - has
received scant attention. In this paper, we address the challenge of evaluating
AI-generated microfictions and argue that this task requires consideration of
literary criteria across various aspects of the text, such as thematic
coherence, textual clarity, interpretive depth, and aesthetic quality. To
facilitate this, we present GrAImes: an evaluation protocol grounded in
literary theory, specifically drawing from a literary perspective, to offer an
objective framework for assessing AI-generated microfiction. Furthermore, we
report the results of our validation of the evaluation protocol, as answered by
both literature experts and literary enthusiasts. This protocol will serve as a
foundation for evaluating automatically generated microfictions and assessing
their literary value.

</details>


### [7] [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/abs/2506.08174)
*Li Weigang,Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: Proposes LLM-BT, a back-translation framework using large language models to automate terminology verification and standardization via cross-lingual semantic alignment.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring multilingual consistency in the rapid growth of English technical terms, especially in fast-evolving fields such as AI and quantum computing.

Method: A back-translation framework named LLM-BT, which uses large language models to achieve term-level consistency validation, multi-path verification workflow, and conceptualizes back-translation as dynamic semantic embedding.

Result: LLM-BT achieves high term consistency across different models, has strong cross-lingual robustness, and transforms back-translation into an active engine for multilingual terminology standardization.

Conclusion: LLM-BT has demonstrated its potential in automating the process of terminology verification and standardization through cross-lingual semantic alignment.

Abstract: The rapid growth of English technical terms challenges traditional
expert-driven standardization, especially in fast-evolving fields like AI and
quantum computing. Manual methods struggle to ensure multilingual consistency.
We propose \textbf{LLM-BT}, a back-translation framework powered by large
language models (LLMs) to automate terminology verification and standardization
via cross-lingual semantic alignment. Our contributions are: \textbf{(1)
Term-Level Consistency Validation:} Using English $\rightarrow$ intermediate
language $\rightarrow$ English back-translation, LLM-BT achieves high term
consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies
showing over 90\% exact or semantic matches. \textbf{(2) Multi-Path
Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''
pipeline integrates serial (e.g., EN $\rightarrow$ ZHcn $\rightarrow$ ZHtw
$\rightarrow$ EN) and parallel (e.g., EN $\rightarrow$ Chinese/Portuguese
$\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong
cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\%).
\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as
dynamic semantic embedding, revealing latent meaning trajectories. Unlike
static embeddings, LLM-BT provides transparent path-based embeddings shaped by
model evolution. LLM-BT transforms back-translation into an active engine for
multilingual terminology standardization, enabling human--AI collaboration:
machines ensure semantic fidelity, humans guide cultural interpretation. This
infrastructure supports terminology governance across scientific and
technological fields worldwide.

</details>


### [8] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
*Chupei Wang,Jiaqiu Vince Sun*

Main category: cs.CL

TL;DR: Longer contexts do not always enhance retrieval in large language models; in fact, intra-context interference can degrade performance, leading to retrieval accuracy declining log-linearly towards zero.


<details>
  <summary>Details</summary>
Motivation: To study the effects of intra-context interference in large language models.

Method: Adapting the proactive interference (PI) paradigm from cognitive science by introducing PI-LLM, which evaluates models' ability to handle sequential updates.

Result: Retrieval accuracy declines as interference accumulates; prompt engineering to mitigate interference has limited success.

Conclusion: LLMs face a fundamental constraint in disentangling interference and flexibly manipulating information, indicating a potential working memory bottleneck.

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [9] ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/abs/2506.08221)
*Samra Zafar,Shaheer Minhas,Syed Ali Hassan Zaidi,Arfa Naeem,Zahra Ali*

Main category: cs.CL

TL;DR: This paper explores if using writing process data can help LLMs provide feedback that better reflects student thought processes during writing.


<details>
  <summary>Details</summary>
Motivation: Most feedback from large language models (LLMs) for student writing is based solely on the final essay, lacking important context about how the text was written.

Method: Using writing process data collected through keystroke logging and periodic snapshots to help LLMs give feedback that better reflects how learners think and revise while writing. A digital writing tool was built to capture both what students type and how their essays evolve over time.

Result: Learners preferred the process-aware LLM feedback, finding it more in tune with their own thinking. Certain types of edits, like adding new content or reorganizing paragraphs, aligned closely with higher scores in areas like coherence and elaboration.

Conclusion: Making LLMs more aware of the writing process can lead to feedback that feels more meaningful, personal, and supportive.

Abstract: Large language models(LLMs) like Gemini are becoming common tools for
supporting student writing. But most of their feedback is based only on the
final essay missing important context about how that text was written. In this
paper, we explore whether using writing process data, collected through
keystroke logging and periodic snapshots, can help LLMs give feedback that
better reflects how learners think and revise while writing. We built a digital
writing tool that captures both what students type and how their essays evolve
over time. Twenty students used this tool to write timed essays, which were
then evaluated in two ways: (i) LLM generated feedback using both the final
essay and the full writing trace, and (ii) After the task, students completed
surveys about how useful and relatable they found the feedback. Early results
show that learners preferred the process-aware LLM feedback, finding it more in
tune with their own thinking. We also found that certain types of edits, like
adding new content or reorganizing paragraphs, aligned closely with higher
scores in areas like coherence and elaboration. Our findings suggest that
making LLMs more aware of the writing process can lead to feedback that feels
more meaningful, personal, and supportive.

</details>


### [10] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
*Yu-Ang Lee,Guan-Ting Yi,Mei-Yi Liu,Jui-Chao Lu,Guan-Bo Yang,Yun-Nung Chen*

Main category: cs.CL

TL;DR: This paper reviews recent progress in optimizing compound AI systems, covering both numerical and language-based techniques.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of AI systems has created new challenges in optimizing not only individual components but also their interactions.

Method: Systematic review of recent progress in optimizing compound AI systems, formalization of the notion of compound AI system optimization, classification of existing methods along several key dimensions.

Result: A comprehensive overview of recent advancements in optimizing compound AI systems, highlighting open research challenges and future directions.

Conclusion: This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques.

Abstract: Recent advancements in large language models (LLMs) and AI systems have led
to a paradigm shift in the design and optimization of complex AI workflows. By
integrating multiple components, compound AI systems have become increasingly
adept at performing sophisticated tasks. However, as these systems grow in
complexity, new challenges arise in optimizing not only individual components
but also their interactions. While traditional optimization methods such as
supervised fine-tuning (SFT) and reinforcement learning (RL) remain
foundational, the rise of natural language feedback introduces promising new
approaches, especially for optimizing non-differentiable systems. This paper
provides a systematic review of recent progress in optimizing compound AI
systems, encompassing both numerical and language-based techniques. We
formalize the notion of compound AI system optimization, classify existing
methods along several key dimensions, and highlight open research challenges
and future directions in this rapidly evolving field. A list of surveyed papers
is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [11] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
*Shashidhar Reddy Javaji,Yupeng Cao,Haohang Li,Yangyang Yu,Nikhil Muralidhar,Zining Zhu*

Main category: cs.CL

TL;DR: 研究提出CLAIM-BENCH基准，评估LLMs处理复杂科研内容的能力，发现封闭源模型表现更优且特定提示策略可提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在科学声明-证据提取和验证中的能力，以反映对科学论证的更深层次理解。

Method: 比较三种受分而治之方法启发的方法，并在六个不同的LLMs上进行测试，揭示模型在科学理解方面的特定优势和劣势。

Result: 封闭源模型如GPT-4和Claude在声明-证据识别任务中的精确度和召回率上始终优于开源模型。设计良好的三步法和逐一提示方法显著提高了LLMs准确链接分散证据与声明的能力，但增加了计算成本。

Conclusion: CLAIM-BENCH为评估LLMs的科学理解设定了新标准，既作为诊断工具也为构建能够跨全文档进行更深入、更可靠推理的系统提供了方向。

Abstract: Large language models (LLMs) are increasingly being used for complex research
tasks such as literature review, idea generation, and scientific paper
analysis, yet their ability to truly understand and process the intricate
relationships within complex research papers, such as the logical links between
claims and supporting evidence remains largely unexplored. In this study, we
present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'
capabilities in scientific claim-evidence extraction and validation, a task
that reflects deeper comprehension of scientific argumentation. We
systematically compare three approaches which are inspired by divide and
conquer approaches, across six diverse LLMs, highlighting model-specific
strengths and weaknesses in scientific comprehension. Through evaluation
involving over 300 claim-evidence pairs across multiple research domains, we
reveal significant limitations in LLMs' ability to process complex scientific
content. Our results demonstrate that closed-source models like GPT-4 and
Claude consistently outperform open-source counterparts in precision and recall
across claim-evidence identification tasks. Furthermore, strategically designed
three-pass and one-by-one prompting approaches significantly improve LLMs'
abilities to accurately link dispersed evidence with claims, although this
comes at increased computational cost. CLAIM-BENCH sets a new standard for
evaluating scientific comprehension in LLMs, offering both a diagnostic tool
and a path forward for building systems capable of deeper, more reliable
reasoning across full-length papers.

</details>


### [12] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
*Wanjing Anya Ma,Michael Flor,Zuowei Wang*

Main category: cs.CL

TL;DR: This study introduces a taxonomy of inference types for reading comprehension and uses it to analyze an item bank. It then explores GPT-4's ability to generate bridging-inference RC items with and without chain-of-thought prompts.


<details>
  <summary>Details</summary>
Motivation: To help educators provide effective reading instruction and interventions by creating a diagnostic RC item bank.

Method: Introduces an inference taxonomy, analyzes a diagnostic RC item bank, and uses GPT-4 to generate RC items via few-shot prompting with/without chain-of-thought prompts.

Result: GPT-4 produced high-quality questions (93.8% suitable for grades 3-12), but only 42.6% matched the targeted inference type.

Conclusion: Combining automatic item generation with human judgment could lead to scalable, high-quality diagnostic RC assessments.

Abstract: Inference making is an essential but complex skill in reading comprehension
(RC). Some inferences require resolving references across sentences, and some
rely on using prior knowledge to fill in the detail that is not explicitly
written in the text. Diagnostic RC questions can help educators provide more
effective and targeted reading instruction and interventions for school-age
students. We introduce a taxonomy of inference types for RC and use it to
analyze the distribution of items within a diagnostic RC item bank. Next, we
present experiments using GPT-4o to generate bridging-inference RC items for
given reading passages via few-shot prompting, comparing conditions with and
without chain-of-thought prompts. Generated items were evaluated on three
aspects: overall item quality, appropriate inference type, and LLM reasoning,
achieving high inter-rater agreements above 0.90. Our results show that GPT-4o
produced 93.8% good-quality questions suitable for operational use in grade
3-12 contexts; however, only 42.6% of the generated questions accurately
matched the targeted inference type. We conclude that combining automatic item
generation with human judgment offers a promising path toward scalable,
high-quality diagnostic RC assessments.

</details>


### [13] [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300)
*Matteo Cargnelutti,Catherine Brobston,John Hess,Jack Cushman,Kristi Mukk,Aristana Scourtas,Kyle Courtney,Greg Leppert,Amanda Watson,Martha Whitehead,Jonathan Zittrain*

Main category: cs.CL

TL;DR: This technical report introduces Institutional Books 1.0, a large collection of public domain books from Harvard Library's participation in Google Books project.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality training data for large language models (LLMs) and the need for sustainable dataset stewardship practices.

Method: Extracting, analyzing, and processing volumes into an extensively-documented dataset of historic texts.

Result: An extensive dataset of 983,004 volumes or 242 billion tokens in the public domain has been made available.

Conclusion: This project aims to make the historical collection more accessible and easier for both humans and machines to filter, read and use.

Abstract: Large language models (LLMs) use data to learn about the world in order to
produce meaningful correlations and predictions. As such, the nature, scale,
quality, and diversity of the datasets used to train these models, or to
support their work at inference time, have a direct impact on their quality.
The rapid development and adoption of LLMs of varying quality has brought into
focus the scarcity of publicly available, high-quality training data and
revealed an urgent need to ground the stewardship of these datasets in
sustainable practices with clear provenance chains. To that end, this technical
report introduces Institutional Books 1.0, a large collection of public domain
books originally digitized through Harvard Library's participation in the
Google Books project, beginning in 2006. Working with Harvard Library, we
extracted, analyzed, and processed these volumes into an extensively-documented
dataset of historic texts. This analysis covers the entirety of Harvard
Library's collection scanned as part of that project, originally spanning
1,075,899 volumes written in over 250 different languages for a total of
approximately 250 billion tokens. As part of this initial release, the
OCR-extracted text (original and post-processed) as well as the metadata
(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,
identified as being in the public domain have been made available. This report
describes this project's goals and methods as well as the results of the
analyses we performed, all in service of making this historical collection more
accessible and easier for humans and machines alike to filter, read and use.

</details>


### [14] [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
*Chenlong Wang,Yuanning Feng,Dongping Chen,Zhaoyang Chu,Ranjay Krishna,Tianyi Zhou*

Main category: cs.CL

TL;DR: Advanced reasoning models can be overly verbose. A new method called NoWait removes unnecessary reflective tokens during inference, cutting reasoning steps by 27-51% with no loss in performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the issue of overthinking and verbosity in large reasoning models.

Method: Suppressing 'Wait' and 'Hmm' tokens during inference.

Result: Reduces reasoning trajectory length significantly while maintaining model utility.

Conclusion: NoWait provides an efficient, plug-and-play solution for multimodal reasoning.

Abstract: Recent advances in large reasoning models have enabled complex, step-by-step
reasoning but often introduce significant overthinking, resulting in verbose
and redundant outputs that hinder efficiency. In this study, we examine whether
explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is
necessary for advanced reasoning. We propose NoWait, a simple yet effective
approach that disables explicit self-reflection by suppressing these tokens
during inference. Extensive experiments on ten benchmarks across textual,
visual, and video reasoning tasks show that NoWait reduces chain-of-thought
trajectory length by up to 27%-51% in five R1-style model series, without
compromising model utility. NoWait thus offers a plug-and-play solution for
efficient and utility-preserving multimodal reasoning.

</details>


### [15] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
*Yuxuan Zhou,Xien Liu,Chenwei Yan,Chen Ning,Xiao Zhang,Boxun Li,Xiangling Fu,Shijin Wang,Guoping Hu,Yu Wang,Ji Wu*

Main category: cs.CL

TL;DR: This study evaluates large language models using a new framework inspired by Bloom's Taxonomy to assess their capabilities across different cognitive levels in the medical field.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities of large language models across different cognitive levels in the medical domain.

Method: A multi-cognitive-level evaluation framework was proposed and used to assess six different families of LLMs.

Result: State-of-the-art general and medical LLMs were found to have varying performances depending on the cognitive level. Performance decreased notably as cognitive complexity increased.

Conclusion: The study found that LLMs perform well at lower cognitive levels but show a significant drop in performance at higher cognitive levels. Larger model sizes seem to be more beneficial for higher cognitive tasks.

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
various medical benchmarks, but their capabilities across different cognitive
levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a
multi-cognitive-level evaluation framework for assessing LLMs in the medical
domain in this study. The framework integrates existing medical datasets and
introduces tasks targeting three cognitive levels: preliminary knowledge grasp,
comprehensive knowledge application, and scenario-based problem solving. Using
this framework, we systematically evaluate state-of-the-art general and medical
LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.
Our findings reveal a significant performance decline as cognitive complexity
increases across evaluated models, with model size playing a more critical role
in performance at higher cognitive levels. Our study highlights the need to
enhance LLMs' medical capabilities at higher cognitive levels and provides
insights for developing LLMs suited to real-world medical applications.

</details>


### [16] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
*Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu*

Main category: cs.CL

TL;DR: This position paper argues for a shift in text embedding research focus from surface-level to implicit semantics.


<details>
  <summary>Details</summary>
Motivation: Text embedding models, while foundational in modern NLP, remain narrowly focused on surface-level semantics despite linguistic theory emphasizing implicit meaning shaped by pragmatics, speaker intent, and sociocultural context.

Method: 

Result: 

Conclusion: The paper calls for a paradigm shift in embedding research to prioritize diverse and linguistically grounded training data, benchmarks evaluating deeper semantics, and explicit framing of implicit meaning as a core modeling objective.

Abstract: This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.

</details>


### [17] [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/abs/2506.08359)
*Li-Ming Zhan,Bo Liu,Zexin Lu,Chengqiang Xie,Jiannong Cao,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: This paper proposes a principled causal-attribution framework to identify behavior-relevant attention heads in transformers for inference-time steering of large language models.


<details>
  <summary>Details</summary>
Motivation: Current approaches to module selection often depend on superficial cues or ad-hoc heuristics, which can lead to suboptimal or unintended outcomes.

Method: The authors use a vector-quantized autoencoder (VQ-AE) to partition the latent space into behavior-relevant and behavior-irrelevant subspaces and quantify the separability of VQ-AE encodings for behavior-aligned versus behavior-violating responses.

Result: Experiments on seven LLMs from two model families and five behavioral steering datasets show that the proposed method enables more accurate inference-time interventions, achieving superior performance on the truthfulness-steering task.

Conclusion: The proposed method provides a principled way to select attention heads relevant to specific behaviors, leading to improved performance in inference-time steering of large language models.

Abstract: Inference-time steering aims to alter the response characteristics of large
language models (LLMs) without modifying their underlying parameters. A
critical step in this process is the identification of internal modules within
LLMs that are associated with the target behavior. However, current approaches
to module selection often depend on superficial cues or ad-hoc heuristics,
which can result in suboptimal or unintended outcomes. In this work, we propose
a principled causal-attribution framework for identifying behavior-relevant
attention heads in transformers. For each head, we train a vector-quantized
autoencoder (VQ-AE) on its attention activations, partitioning the latent space
into behavior-relevant and behavior-irrelevant subspaces, each quantized with a
shared learnable codebook. We assess the behavioral relevance of each head by
quantifying the separability of VQ-AE encodings for behavior-aligned versus
behavior-violating responses using a binary classification metric. This yields
a behavioral relevance score that reflects each head discriminative capacity
with respect to the target behavior, guiding both selection and importance
weighting. Experiments on seven LLMs from two model families and five
behavioral steering datasets demonstrate that our method enables more accurate
inference-time interventions, achieving superior performance on the
truthfulness-steering task. Furthermore, the heads selected by our approach
exhibit strong zero-shot generalization in cross-domain truthfulness-steering
scenarios.

</details>


### [18] [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/abs/2506.08364)
*Jash Rajesh Parekh,Pengcheng Jiang,Jiawei Han*

Main category: cs.CL

TL;DR: Introduce Causal-Chain RAG (CC-RAG) to model true causal dependencies in specialized domains.


<details>
  <summary>Details</summary>
Motivation: Standard RAG pipelines lack the structure required to model true causal dependencies, which is crucial for understanding cause and effect relationships in specialized domains.

Method: Integrate zero-shot triple extraction and theme-aware graph chaining into the RAG pipeline to construct a DAG of <cause, relation, effect> triples and use forward/backward chaining to guide structured answer generation.

Result: CC-RAG outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity in experiments on Bitcoin price fluctuations and Gaucher disease.

Conclusion: Explicitly modeling causal structure enables LLMs to generate more accurate and interpretable responses in specialized domains.

Abstract: Understanding cause and effect relationships remains a formidable challenge
for Large Language Models (LLMs), particularly in specialized domains where
reasoning requires more than surface-level correlations. Retrieval-Augmented
Generation (RAG) improves factual accuracy, but standard RAG pipelines treat
evidence as flat context, lacking the structure required to model true causal
dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that
integrates zero-shot triple extraction and theme-aware graph chaining into the
RAG pipeline, enabling structured multi-hop inference. Given a domain specific
corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,
effect> triples and uses forward/backward chaining to guide structured answer
generation. Experiments on two real-world domains: Bitcoin price fluctuations
and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot
LLMs in chain similarity, information density, and lexical diversity. Both
LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results
demonstrate that explicitly modeling causal structure enables LLMs to generate
more accurate and interpretable responses, especially in specialized domains
where flat retrieval fails.

</details>


### [19] [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/abs/2506.08371)
*Zikai Xiao,Ziyang Wang,Wen Ma,Yan Zhang,Wei Shen,Yan Wang,Luqi Gong,Zuozhu Liu*

Main category: cs.CL

TL;DR: 提出了一种无需训练的Positional Contrastive Decoding (PCD) 方法来解决大型语言模型在长上下文中的性能下降问题，并通过实验验证了其在长上下文基准测试中的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 探索统计行为和成本效益的方法来应对LLMs在长上下文窗口中的性能退化问题。

Method: 提出Positional Contrastive Decoding (PCD)，对比来自长上下文注意和设计的局部注意的logits。

Result: PCD方法在长上下文基准测试中取得了最先进的性能。

Conclusion: 该研究发现了后验显著性衰减(PSA)现象，并提出了一个有效的解决方案PCD来缓解注意力分数的退化。

Abstract: While Large Language Models (LLMs) support long contexts, they struggle with
performance degradation within the context window. Current solutions incur
prohibitive training costs, leaving statistical behaviors and cost-effective
approaches underexplored. From the decoding perspective, we identify the
Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio
correlates with long-text performance degradation. Notably, despite the
attenuation, gold tokens still occupy high-ranking positions in the decoding
space. Motivated by it, we propose the training-free Positional Contrastive
Decoding (PCD) that contrasts the logits derived from long-aware attention with
those from designed local-aware attention, enabling the model to focus on the
gains introduced by large-scale short-to-long training. Through the analysis of
long-term decay simulation, we demonstrate that PCD effectively alleviates
attention score degradation. Experimental results show that PCD achieves
state-of-the-art performance on long-context benchmarks.

</details>


### [20] [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
*Kevin Galim,Ethan Ewer,Wonjun Kang,Minjae Lee,Hyung Il Koo,Kangwook Lee*

Main category: cs.CL

TL;DR: We propose a novel framework using draft models for approximate LLM inference acceleration, showing higher accuracy than existing baselines.


<details>
  <summary>Details</summary>
Motivation: Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers.

Method: We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs.

Result: Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput.

Conclusion: Our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput.

Abstract: Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(KV) cache dropping, sparse attention, and prompt compression, typically rely
on rough predictions of token or KV pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and KV pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) SpecKV, which
leverages a draft output to accurately assess the importance of each KV pair
for more effective KV cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference acceleration, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [21] [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)
*Tao Zou,Xinghua Zhang,Haiyang Yu,Minzheng Wang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: This paper introduces EIFBENCH, a new benchmark designed to evaluate large language models' ability to handle complex, multi-task workflows under various constraints. It also proposes SegPO, an algorithm aimed at improving LLMs' performance in these complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are insufficient for evaluating LLMs in real-world, complex scenarios due to their focus on single-task environments with limited constraints.

Method: The authors created EIFBENCH, a benchmark that includes multi-task scenarios and various constraints, and developed SegPO, an algorithm to improve LLMs' multi-task workflow execution.

Result: Evaluation on EIFBENCH showed significant differences in performance among existing LLMs when dealing with complex instructions.

Conclusion: The study highlights the need for continuous improvement in LLMs to meet the challenges of complex, real-world applications.

Abstract: With the development and widespread application of large language models
(LLMs), the new paradigm of "Model as Product" is rapidly evolving, and demands
higher capabilities to address complex user needs, often requiring precise
workflow execution which involves the accurate understanding of multiple tasks.
However, existing benchmarks focusing on single-task environments with limited
constraints lack the complexity required to fully reflect real-world scenarios.
To bridge this gap, we present the Extremely Complex Instruction Following
Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and
robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that
enable comprehensive assessment across diverse task types concurrently, but
also integrates a variety of constraints, replicating complex operational
environments. Furthermore, we propose the Segment Policy Optimization (SegPO)
algorithm to enhance the LLM's ability to accurately fulfill multi-task
workflow. Evaluations on EIFBENCH have unveiled considerable performance
discrepancies in existing LLMs when challenged with these extremely complex
instructions. This finding underscores the necessity for ongoing optimization
to navigate the intricate challenges posed by LLM applications.

</details>


### [22] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
*Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: This paper introduces mSTEB, a new benchmark for evaluating large language models' performance on tasks involving both speech and text modalities for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized evaluation benchmarks for low-resource languages in large language model evaluations.

Method: Introducing mSTEB, which evaluates language identification, text classification, question answering, and translation tasks.

Result: A significant performance gap was found between high-resource and low-resource languages, particularly affecting African and Americas/Oceania languages.

Conclusion: More investment is required to improve the representation of low-resource languages in large language models.

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [23] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
*Weiya Li,Junjie Chen,Bei Li,Boyang Liu,Zichen Wen,Nuanqiao Shan,Xiaoqian Liu,Anping Liu,Huajie Liu,Youyan Wang,Wujiuge Yin,Hu Song,Bing Huang,Zhiyuan Xia,Jialiang Chen,Linfeng Zhang*

Main category: cs.CL

TL;DR: A cognitively informed multi-agent framework named TACTIC is proposed to improve machine translation quality.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent translation frameworks neglect insights from cognitive translation studies.

Method: TACTIC consists of six functionally distinct agents simulating human translation behavior.

Result: TACTIC outperforms previous models on multiple benchmarks.

Conclusion: The proposed framework effectively leverages the full capacity of large language models for high-quality translation.

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [24] [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)
*Ziyang Ma,Qingyue Yuan,Zhenglin Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: This paper introduces AutoMeco, a framework for evaluating LLM meta-cognition, and proposes MIRA, a strategy to enhance current meta-cognition lenses. The experimental results validate AutoMeco and demonstrate that MIRA improves the evaluation of LLM meta-cognition.


<details>
  <summary>Details</summary>
Motivation: To examine the meta-cognitive abilities of LLMs, which are crucial for their reliability, and improve the existing meta-cognition evaluation methods.

Method: Propose AutoMeco, an Automated Meta-cognition Evaluation framework, and MIRA, a training-free Markovian Intrinsic Reward Adjustment strategy.

Result: Experimental results on three mathematical reasoning datasets and three LLMs show the reasonableness of AutoMeco and that MIRA enhances the evaluation of LLM meta-cognition.

Conclusion: The study provides a new perspective on evaluating LLM meta-cognition and offers a practical method to improve the evaluation.

Abstract: Previous research has primarily focused on the cognitive error detection
capabilities of Large Language Models (LLMs), often prompting them to analyze
mistakes in reasoning chains. However, few studies have examined the
meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),
which are crucial for their reliability. While studies on LLM self-evaluation
present some measures, such as perplexity, which can reflect the answer
correctness and be viewed as the lens of meta-cognition, they lack step-level
analysis and adaptation. This paper studies the evaluation of LLM
meta-cognition using the current lenses and how to improve these lenses.
Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation
framework for benchmarking the existing lenses. Furthermore, a training-free
Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost
current meta-cognition lenses. Experimental results on three mathematical
reasoning datasets and three LLMs show the reasonableness of AutoMeco by
comparing it with Best-of-N verification. Moreover, the meta-cognition ability
of LLMs can be better evaluated using MIRA.

</details>


### [25] [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/abs/2506.08427)
*Jiaxiang Liu,Boxuan Xing,Chenhao Yuan,Chenxiang Zhang,Di Wu,Xiusheng Huang,Haida Yu,Chuhan Lang,Pengfei Cao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: An open-source tool called Know-MRI is presented to improve the interpretability of large language models.


<details>
  <summary>Details</summary>
Motivation: To enhance the interpretability of internal knowledge mechanisms of large language models.

Method: Developing an extensible core module that matches input data with interpretation methods and consolidates outputs.

Result: Users can choose appropriate interpretation methods based on inputs to diagnose the model's internal knowledge mechanisms from multiple perspectives.

Conclusion: Know-MRI is an open-source tool available on GitHub and has a demonstration video.

Abstract: As large language models (LLMs) continue to advance, there is a growing
urgency to enhance the interpretability of their internal knowledge mechanisms.
Consequently, many interpretation methods have emerged, aiming to unravel the
knowledge mechanisms of LLMs from various perspectives. However, current
interpretation methods differ in input data formats and interpreting outputs.
The tools integrating these methods are only capable of supporting tasks with
specific inputs, significantly constraining their practical applications. To
address these challenges, we present an open-source Knowledge Mechanisms
Revealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms
within LLMs systematically. Specifically, we have developed an extensible core
module that can automatically match different input data with interpretation
methods and consolidate the interpreting outputs. It enables users to freely
choose appropriate interpretation methods based on the inputs, making it easier
to comprehensively diagnose the model's internal knowledge mechanisms from
multiple perspectives. Our code is available at
https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on
https://youtu.be/NVWZABJ43Bs.

</details>


### [26] [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)
*Ziqi. Liu,Ziyang. Zhou,Mingxuan. Hu*

Main category: cs.CL

TL;DR: This paper presents CAF-I, a multi-agent framework leveraging large language models for sarcasm detection, improving upon previous methods in interpretability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the single-perspective limitations, insufficient comprehensive understanding, and lack of interpretability in existing large language model methods for irony detection.

Method: Introduces CAF-I, a framework using specialized agents for context, semantics, and rhetoric analysis, along with decision and refinement agents for collaborative optimization.

Result: CAF-I achieves state-of-the-art zero-shot performance on benchmark datasets, with an average Macro-F1 score of 76.31, representing a 4.98 absolute improvement over the best previous method.

Conclusion: CAF-I demonstrates effectiveness in simulating human-like multi-perspective analysis, enhancing both detection accuracy and interpretability in sarcasm detection.

Abstract: Large language model (LLM) have become mainstream methods in the field of
sarcasm detection. However, existing LLM methods face challenges in irony
detection, including: 1. single-perspective limitations, 2. insufficient
comprehensive understanding, and 3. lack of interpretability. This paper
introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven
multi-agent system designed to overcome these issues. CAF-I employs specialized
agents for Context, Semantics, and Rhetoric, which perform multidimensional
analysis and engage in interactive collaborative optimization. A Decision Agent
then consolidates these perspectives, with a Refinement Evaluator Agent
providing conditional feedback for optimization. Experiments on benchmark
datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving
SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of
76.31, a 4.98 absolute improvement over the strongest prior baseline. This
success is attained by its effective simulation of human-like multi-perspective
analysis, enhancing detection accuracy and interpretability.

</details>


### [27] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernán Maina,Nicolás Wolovick,Luciana Benotti*

Main category: cs.CL

TL;DR: Evaluate the impact of numerical precision and data parallelization on training speed and model accuracy for domain adaptation.


<details>
  <summary>Details</summary>
Motivation: Reduce the high cost of training LLMs and make domain adaptation more accessible to resource-limited environments.

Method: Analyze different numerical precisions and data parallelization strategies.

Result: Findings that can help improve energy efficiency and model accessibility.

Conclusion: Better understanding of trade-offs between speed, accuracy, and resource usage in domain adaptation.

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [28] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
*Jiujun He,Huazhen Lin*

Main category: cs.CL

TL;DR: This paper introduces Olica, a pruning framework for large language models that doesn't require retraining. It uses orthogonal decomposition and linear calibration to maintain model accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for large language models require significant computational resources for retraining, making them costly. The motivation is to develop a method that can prune models efficiently without needing retraining.

Method: The method involves decomposing multi-head attention layers using PCA to compress the model and applying linear calibration to FFN layers using SVD to reconstruct residual errors.

Result: Experiments showed that Olica is efficient in terms of data usage, GPU memory, and runtime, and it performs well across multiple benchmarks.

Conclusion: Olica successfully prunes large language models without the need for retraining, maintaining model accuracy and improving efficiency.

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [29] [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/abs/2506.08477)
*Fengjun Pan,Anh Tuan Luu,Xiaobao Wu*

Main category: cs.CL

TL;DR: 提出一种新框架U-CoT+，通过高保真模因到文本管道和针对性的人类指导，在不同平台和区域提供可解释且低资源的有害模因检测。


<details>
  <summary>Details</summary>
Motivation: 现有的有害模因检测方法在资源效率、灵活性和可解释性方面存在局限性，阻碍了其在内容审核系统中的实际部署。

Method: 开发了一种名为U-CoT+的新框架，包括一个高保真模因到文本管道和针对零样本CoT提示的人类设计指南。

Result: 在七个基准数据集上的广泛实验验证了该框架的有效性，展示了其使用小型LLM进行可解释且低资源有害模因检测的潜力。

Conclusion: U-CoT+框架提供了灵活且可解释的方法来应对有害模因检测中的多种挑战，具有广泛的适应性和潜在的实际应用价值。

Abstract: Detecting harmful memes is essential for maintaining the integrity of online
environments. However, current approaches often struggle with resource
efficiency, flexibility, or explainability, limiting their practical deployment
in content moderation systems. To address these challenges, we introduce
U-CoT+, a novel framework for harmful meme detection. Instead of relying solely
on prompting or fine-tuning multimodal models, we first develop a high-fidelity
meme-to-text pipeline that converts visual memes into detail-preserving textual
descriptions. This design decouples meme interpretation from meme
classification, thus avoiding immediate reasoning over complex raw visual
content and enabling resource-efficient harmful meme detection with general
large language models (LLMs). Building on these textual descriptions, we
further incorporate targeted, interpretable human-crafted guidelines to guide
models' reasoning under zero-shot CoT prompting. As such, this framework allows
for easy adaptation to different harmfulness detection criteria across
platforms, regions, and over time, offering high flexibility and
explainability. Extensive experiments on seven benchmark datasets validate the
effectiveness of our framework, highlighting its potential for explainable and
low-resource harmful meme detection using small-scale LLMs. Codes and data are
available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.

</details>


### [30] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
*Chihiro Taguchi,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: Adaptive-$k$ retrieval is a novel approach for selecting the optimal number of retrieved passages in open-domain QA tasks, improving efficiency and accuracy without requiring additional fine-tuning or model changes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of determining the optimal external context size for retrieval-augmented models in open-domain QA, especially for aggregation QA where the context size is variable and unknown.

Method: Adaptive-$k$ retrieval uses the distribution of similarity scores between queries and candidate passages to adaptively select the number of passages, without needing iterative prompting or changes to existing pipelines.

Result: On factoid and aggregation QA benchmarks, Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using significantly fewer tokens, retrieving most relevant passages and improving accuracy across different LCLMs and embedding models.

Conclusion: Dynamically adjusting the context size through Adaptive-$k$ retrieval leads to more efficient and accurate open-domain QA compared to fixed context size approaches.

Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.

</details>


### [31] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: This paper aims to enhance the evaluation of image-text alignment in text-to-image generation by addressing overlooked properties and providing recommendations for improvement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the evaluation of image-text alignment in text-to-image generation as the existing evaluations mainly focus on agreement with human assessments and overlook other important properties.

Method: Identify two key aspects that a reliable evaluation should address and empirically demonstrate the insufficiency of current mainstream evaluation frameworks.

Result: Current mainstream evaluation frameworks fail to fully satisfy the identified key aspects across various metrics and models.

Conclusion: The paper concludes by proposing recommendations for enhancing the image-text alignment evaluation.

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [32] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
*Sumanth Manduru,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: Rapid adoption of Small Language Models (SLMs) raises ethical concerns. This study audits nine open-source models with 0.5 to 5 billion parameters. It finds that competence and fairness can coexist, social bias varies by architecture, and compression introduces trade-offs.


<details>
  <summary>Details</summary>
Motivation: To understand ethical risks and evaluate models' competence and fairness.

Method: Large-scale audit of nine open-source SLMs using BBQ benchmark under zero-shot prompting.

Result: Competence and fairness can coexist; social bias varies by architecture; compression introduces trade-offs.

Conclusion: Responsible deployment of SLMs can benefit small enterprises and resource-constrained environments.

Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.

</details>


### [33] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
*Ashutosh Dwivedi,Siddhant Shivdutt Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: This paper introduces EtiCor++, a global etiquette corpus, and proposes tasks and metrics for evaluating regional etiquette knowledge and bias in large language models (LLMs). Experiments reveal regional biases in LLMs.


<details>
  <summary>Details</summary>
Motivation: To make large language models culturally sensitive by evaluating their understanding and biases regarding etiquettes, which are region-specific and important cultural aspects.

Method: Introducing EtiCor++, defining tasks for evaluating etiquette knowledge, and introducing metrics to measure bias in LLMs.

Result: Experiments show inherent biases in LLMs towards certain regions.

Conclusion: The paper highlights the need for more resources to evaluate LLMs' cultural sensitivity and presents tools to address this gap.

Abstract: In recent years, researchers have started analyzing the cultural sensitivity
of LLMs. In this respect, Etiquettes have been an active area of research.
Etiquettes are region-specific and are an essential part of the culture of a
region; hence, it is imperative to make LLMs sensitive to etiquettes. However,
there needs to be more resources in evaluating LLMs for their understanding and
bias with regard to etiquettes. In this resource paper, we introduce EtiCor++,
a corpus of etiquettes worldwide. We introduce different tasks for evaluating
LLMs for knowledge about etiquettes across various regions. Further, we
introduce various metrics for measuring bias in LLMs. Extensive experimentation
with LLMs shows inherent bias towards certain regions.

</details>


### [34] [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/abs/2506.08490)
*Xiao Wei,Xiaobao Wang,Ning Zhuang,Chenyang Wang,Longbiao Wang,Jianwu dang*

Main category: cs.CL

TL;DR: Propose a consistency-driven prototype-prompting framework for generalized intent discovery that integrates old and new knowledge, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing intent detection methods struggle with out-of-domain (OOD) intents, and existing Generalized Intent Discovery (GID) methods neglect domain adaptation.

Method: A consistency-driven prototype-prompting framework for GID from the perspective of integrating old and new knowledge, including a prototype-prompting framework for transferring old knowledge from external sources, and a hierarchical consistency constraint for learning new knowledge from target domains.

Result: Our method achieves state-of-the-art results in extensive experiments.

Conclusion: Our method significantly outperforms all baseline methods, achieving state-of-the-art results.

Abstract: Intent detection aims to identify user intents from natural language inputs,
where supervised methods rely heavily on labeled in-domain (IND) data and
struggle with out-of-domain (OOD) intents, limiting their practical
applicability. Generalized Intent Discovery (GID) addresses this by leveraging
unlabeled OOD data to discover new intents without additional annotation.
However, existing methods focus solely on clustering unsupervised data while
neglecting domain adaptation. Therefore, we propose a consistency-driven
prototype-prompting framework for GID from the perspective of integrating old
and new knowledge, which includes a prototype-prompting framework for
transferring old knowledge from external sources, and a hierarchical
consistency constraint for learning new knowledge from target domains. We
conducted extensive experiments and the results show that our method
significantly outperforms all baseline methods, achieving state-of-the-art
results, which strongly demonstrates the effectiveness and generalization of
our methods. Our source code is publicly available at
https://github.com/smileix/cpp.

</details>


### [35] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
*Arie Cattan,Alon Jacovi,Ori Ram,Jonathan Herzig,Roee Aharoni,Sasha Goldshtein,Eran Ofek,Idan Szpektor,Avi Caciularu*

Main category: cs.CL

TL;DR: This paper introduces a new way to categorize different types of conflicts in Retrieval Augmented Generation (RAG) systems and creates a benchmark called CONFLICTS to track how well these systems handle those conflicts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of conflicting information in retrieved sources which current models struggle to resolve effectively.

Method: Proposed a novel taxonomy of knowledge conflict types in RAG and introduced CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting.

Result: Experiments showed that while prompting LLMs to reason about potential conflicts improves their responses, there is still significant room for improvement.

Conclusion: Models need to be improved to better handle knowledge conflicts in RAG systems.

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [36] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
*Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi*

Main category: cs.CL

TL;DR: This paper introduces CoMuMDR, a new dataset for discourse parsing in conversations that includes audio and transcribed text in both Hindi and English, with nine discourse relations. It also experiments with state-of-the-art models, which perform poorly, indicating the need for improved models for multi-domain code-mixed data.


<details>
  <summary>Details</summary>
Motivation: To create a new discourse parsing dataset for conversations that can be used in natural language understanding applications like summarization, machine comprehension, and emotion recognition.

Method: Introducing CoMuMDR, a code-mixed multi-modal multi-domain corpus for discourse parsing in conversations.

Result: Various state-of-the-art baseline models were experimented with, but they performed poorly, indicating the challenges of multi-domain code-mixed corpus.

Conclusion: There is a need for developing better models for discourse parsing in realistic settings involving multi-domain code-mixed data.

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [37] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
*Xinyuan Wang,Dongjie Wang,Wangyang Ying,Haoyue Bai,Nanxu Gong,Sixun Dong,Kunpeng Liu,Yanjie Fu*

Main category: cs.CL

TL;DR: This paper proposes a lightweight post-training framework for improving latent reasoning in large language models, which achieves a 5% accuracy gain on MathQA.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of chain-of-thought prompting and improve the internal reasoning processes of large language models.

Method: The proposed framework uses two novel strategies: contrastive reasoning feedback and residual embedding refinement.

Result: The framework achieves a 5% accuracy gain on MathQA without additional training.

Conclusion: This work demonstrates the effectiveness of the proposed framework in improving latent reasoning in large language models.

Abstract: Reasoning is a key component of language understanding in Large Language
Models. While Chain-of-Thought prompting enhances performance via explicit
intermediate steps, it suffers from sufficient token overhead and a fixed
reasoning trajectory, preventing step-wise refinement. Recent advances in
latent reasoning address these limitations by refining internal reasoning
processes directly in the model's latent space, without producing explicit
outputs. However, a key challenge remains: how to effectively update reasoning
embeddings during post-training to guide the model toward more accurate
solutions. To overcome this challenge, we propose a lightweight post-training
framework that refines latent reasoning trajectories using two novel
strategies: 1) Contrastive reasoning feedback, which compares reasoning
embeddings against strong and weak baselines to infer effective update
directions via embedding enhancement; 2) Residual embedding refinement, which
stabilizes updates by progressively integrating current and historical
gradients, enabling fast yet controlled convergence. Extensive experiments and
case studies are conducted on five reasoning benchmarks to demonstrate the
effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA
without additional training.

</details>


### [38] [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/abs/2506.08564)
*Tuukka Törö,Antti Suni,Juraj Šimko*

Main category: cs.CL

TL;DR: This study uses embeddings from a fine-tuned XLS-R model to analyze relationships between 106 world languages based on speech recordings, showing that embedding-based distances align closely with traditional measures.


<details>
  <summary>Details</summary>
Motivation: To investigate linguistic relationships globally using machine learning methods that explore linguistic variation through embeddings derived directly from speech.

Method: Using embeddings from the fine-tuned XLS-R self-supervised language identification model and linear discriminant analysis (LDA) to cluster and compare language embeddings with genealogical, lexical, and geographical distances.

Result: Embedding-based distances align closely with traditional measures, effectively capturing both global and local typological patterns.

Conclusion: This approach provides new perspectives on relationships among languages and has potential for scalable analyses of language variation, especially for low-resource languages.

Abstract: Investigating linguistic relationships on a global scale requires analyzing
diverse features such as syntax, phonology and prosody, which evolve at varying
rates influenced by internal diversification, language contact, and
sociolinguistic factors. Recent advances in machine learning (ML) offer
complementary alternatives to traditional historical and typological
approaches. Instead of relying on expert labor in analyzing specific linguistic
features, these new methods enable the exploration of linguistic variation
through embeddings derived directly from speech, opening new avenues for
large-scale, data-driven analyses.
  This study employs embeddings from the fine-tuned XLS-R self-supervised
language identification model voxlingua107-xls-r-300m-wav2vec, to analyze
relationships between 106 world languages based on speech recordings. Using
linear discriminant analysis (LDA), language embeddings are clustered and
compared with genealogical, lexical, and geographical distances. The results
demonstrate that embedding-based distances align closely with traditional
measures, effectively capturing both global and local typological patterns.
Challenges in visualizing relationships, particularly with hierarchical
clustering and network-based methods, highlight the dynamic nature of language
change.
  The findings show potential for scalable analyses of language variation based
on speech embeddings, providing new perspectives on relationships among
languages. By addressing methodological considerations such as corpus size and
latent space dimensionality, this approach opens avenues for studying
low-resource languages and bridging macro- and micro-level linguistic
variation. Future work aims to extend these methods to underrepresented
languages and integrate sociolinguistic variation for a more comprehensive
understanding of linguistic diversity.

</details>


### [39] [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/abs/2506.08584)
*Yahan Li,Jifan Yao,John Bosco S. Bunyi,Adam C. Frank,Angel Hwang,Ruishan Liu*

Main category: cs.CL

TL;DR: This paper introduces CounselBench, a large-scale benchmark for evaluating LLMs in single-turn counseling. It includes two components: CounselBench-EVAL with 2,000 expert evaluations and CounselBench-Adv with 120 adversarial questions.


<details>
  <summary>Details</summary>
Motivation: To test LLMs' behavior in realistic counseling scenarios.

Method: Developed CounselBench with 100 mental health professionals, including CounselBench-EVAL and CounselBench-Adv.

Result: LLMs often outperform online human therapists in perceived quality but have safety concerns. LLM judges overrate model responses and overlook safety issues. There are consistent, model-specific failure patterns.

Conclusion: CounselBench establishes a clinically grounded framework for benchmarking and improving LLM behavior in high-stakes mental health settings.

Abstract: Large language models (LLMs) are increasingly proposed for use in mental
health support, yet their behavior in realistic counseling scenarios remains
largely untested. We introduce CounselBench, a large-scale benchmark developed
with 100 mental health professionals to evaluate and stress-test LLMs in
single-turn counseling. The first component, CounselBench-EVAL, contains 2,000
expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human
therapists to real patient questions. Each response is rated along six
clinically grounded dimensions, with written rationales and span-level
annotations. We find that LLMs often outperform online human therapists in
perceived quality, but experts frequently flag their outputs for safety
concerns such as unauthorized medical advice. Follow-up experiments show that
LLM judges consistently overrate model responses and overlook safety issues
identified by human experts. To probe failure modes more directly, we construct
CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling
questions designed to trigger specific model issues. Evaluation across 2,880
responses from eight LLMs reveals consistent, model-specific failure patterns.
Together, CounselBench establishes a clinically grounded framework for
benchmarking and improving LLM behavior in high-stakes mental health settings.

</details>


### [40] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
*Liyan Xu,Zhenlin Su,Mo Yu,Jiangnan Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: This paper presents CapRetrieval, a new Chinese dataset for evaluating text encoders' ability to retrieve fine-grained entities and events from image captions. The authors find that current encoders struggle with this task, regardless of their size or training source. They propose data generation strategies to finetune encoders and improve performance on CapRetrieval.


<details>
  <summary>Details</summary>
Motivation: Text encoders may fail to recognize fine-grained entities or events within the semantics, leading to poor dense retrieval performance even on simple cases.

Method: Introduce a new evaluation dataset called CapRetrieval, propose data generation strategies to finetune encoders.

Result: The proposed method achieves the best performance on CapRetrieval.

Conclusion: Current text encoders face challenges in recognizing fine-grained entities and events. The authors identify a granularity dilemma and provide solutions to enhance the encoders' performance.

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [41] [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/abs/2506.08593)
*Shuzhou Yuan,Ercong Nie,Mario Tawfelis,Helmut Schmid,Hinrich Schütze,Michael Färber*

Main category: cs.CL

TL;DR: 研究人格特质（如MBTI）如何影响大型语言模型在仇恨言论检测中的表现，揭示了显著的个性驱动差异及潜在问题。


<details>
  <summary>Details</summary>
Motivation: 探索人格特质对大型语言模型在仇恨言论分类中的影响，特别是MBTI人格维度的影响。

Method: 进行人类注释调查以确认MBTI维度对标注行为的影响，并使用四个开源模型进行基于MBTI人格特质的提示，评估其在三个仇恨言论数据集上的输出。

Result: 发现显著的人格驱动的变化，包括与真实情况的不一致、人格间的分歧和logit级别的偏差。

Conclusion: 强调了在基于LLM的注释工作流程中仔细定义人格提示的重要性，这对公平性和与人类价值观的一致性具有重要意义。

Abstract: Hate speech detection is a socially sensitive and inherently subjective task,
with judgments often varying based on personal traits. While prior work has
examined how socio-demographic factors influence annotation, the impact of
personality traits on Large Language Models (LLMs) remains largely unexplored.
In this paper, we present the first comprehensive study on the role of persona
prompts in hate speech classification, focusing on MBTI-based traits. A human
annotation survey confirms that MBTI dimensions significantly affect labeling
behavior. Extending this to LLMs, we prompt four open-source models with MBTI
personas and evaluate their outputs across three hate speech datasets. Our
analysis uncovers substantial persona-driven variation, including
inconsistencies with ground truth, inter-persona disagreement, and logit-level
biases. These findings highlight the need to carefully define persona prompts
in LLM-based annotation workflows, with implications for fairness and alignment
with human values.

</details>


### [42] [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)
*Minhae Oh,Jeonghye Kim,Nakyung Lee,Donggeon Seo,Taeuk Kim,Jungwoo Lee*

Main category: cs.CL

TL;DR: A new framework called RAISE is proposed for scientific reasoning, which improves performance by retrieving logically relevant documents.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of scientific reasoning including long-chain reasoning, domain-specific terminologies, and updated findings.

Method: RAISE framework with three steps: problem decomposition, logical query generation, and logical retrieval.

Result: RAISE outperforms other baselines on scientific reasoning benchmarks.

Conclusion: RAISE's success is due to its ability to retrieve documents that are both domain-knowledge similar and logically relevant.

Abstract: Scientific reasoning requires not only long-chain reasoning processes, but
also knowledge of domain-specific terminologies and adaptation to updated
findings. To deal with these challenges for scientific reasoning, we introduce
RAISE, a step-by-step retrieval-augmented framework which retrieves logically
relevant documents from in-the-wild corpus. RAISE is divided into three steps:
problem decomposition, logical query generation, and logical retrieval. We
observe that RAISE consistently outperforms other baselines on scientific
reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves
documents that are not only similar in terms of the domain knowledge, but also
documents logically more relevant.

</details>


### [43] [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/abs/2506.08643)
*Son The Nguyen,Theja Tulabandhula*

Main category: cs.CL

TL;DR: MEMETRON is a new framework for optimizing large language model outputs by formulating decoding as a black-box optimization problem.


<details>
  <summary>Details</summary>
Motivation: Current decoding methods for large language models lack explicit task-specific optimization and control.

Method: MEMETRON uses hybrid metaheuristic algorithms to search the response space with guidance from reward models and contextual operations.

Result: The framework outperforms standard decoding and reranking methods in the human preference alignment task.

Conclusion: MEMETRON shows promise in improving alignment without needing model retraining.

Abstract: Large language models (LLMs) are increasingly used for both open-ended and
structured tasks, yet their inference-time behavior is still largely dictated
by heuristic decoding strategies such as greedy search, sampling, or reranking.
These methods provide limited control and do not explicitly optimize for
task-specific objectives. We introduce MEMETRON, a task-agnostic framework that
formulates LLM decoding as a discrete black-box optimization problem. MEMETRON
leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the
response space, guided by reward models and contextual operations performed by
the LLM itself. This approach enables efficient discovery of high-reward
responses without requiring model retraining or gradient access. The framework
is modular and generalizes across diverse tasks, requiring only a reward
function and lightweight prompt templates. We evaluate our framework on the
critical human preference alignment task and demonstrate that it significantly
outperforms standard decoding and reranking methods, highlighting its potential
to improve alignment without model retraining.

</details>


### [44] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
*Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang*

Main category: cs.CL

TL;DR: This paper introduces TableDreamer, a progressive and weakness-guided data synthesis framework designed to improve table instruction tuning for large language models (LLMs). It addresses limitations of previous methods by synthesizing diverse data and focusing on the weaknesses of target LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing LLM-based data synthesis methods, particularly their inability to fully explore input spaces and their focus on increasing data quantity rather than improving data quality.

Method: TableDreamer starts by creating seed data consisting of diverse tables and related instructions. It iteratively explores the input space using newly identified weak data until it forms the final dataset for fine-tuning the target LLM.

Result: Experiments on 10 tabular benchmarks show that TableDreamer improves the average accuracy of Llama3.1-8B-instruct by 11.62% with only 27K GPT-4o synthetic data, surpassing other state-of-the-art baselines using more data.

Conclusion: TableDreamer effectively enhances the performance of LLMs in table instruction tuning by synthesizing high-quality data that targets specific weaknesses.

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [45] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
*Oumaima El Khettari,Solen Quiniou,Samuel Chaffron*

Main category: cs.CL

TL;DR: Generative relation extraction (RE) using large language models improves performance when summarizing context first.


<details>
  <summary>Details</summary>
Motivation: To explore generative RE methods for studying interactions in the intestinal microbiome, a low-resource biomedical domain.

Method: Use large language models for summarization before extracting relations through instruction-tuned generation.

Result: Summarization improves generative RE performance by reducing noise and guiding the model.

Conclusion: While promising, generative methods still lag behind BERT-based RE approaches in this domain.

Abstract: We explore a generative relation extraction (RE) pipeline tailored to the
study of interactions in the intestinal microbiome, a complex and low-resource
biomedical domain. Our method leverages summarization with large language
models (LLMs) to refine context before extracting relations via
instruction-tuned generation. Preliminary results on a dedicated corpus show
that summarization improves generative RE performance by reducing noise and
guiding the model. However, BERT-based RE approaches still outperform
generative models. This ongoing work demonstrates the potential of generative
methods to support the study of specialized domains in low-resources setting.

</details>


### [46] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: Introduce RuleReasoner, a method for rule-based reasoning using curated tasks and dynamic sampling, outperforming frontier large reasoning models on both in-distribution and out-of-distribution benchmarks.


<details>
  <summary>Details</summary>
Motivation: To explore if small reasoning models can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains.

Method: Reinforced Rule-based Reasoning using a domain-aware dynamic sampling approach to update sampling weights based on historical rewards.

Result: RuleReasoner outperforms frontier LRMs significantly on ID and OOD benchmarks and shows higher computational efficiency than previous dynamic sampling methods.

Conclusion: RuleReasoner demonstrates the effectiveness of small reasoning models in rule-based reasoning with robust generalization and improved performance and efficiency.

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [47] [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)
*Soham Poddar,Paramita Koley,Janardan Misra,Sanjay Podder,Navveen Balani,Niloy Ganguly,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: This paper explores output compression methods to optimize energy efficiency during inference of Large Language Models (LLMs). It benchmarks 12 decoder-only LLMs on 5 datasets, finding that LLM responses are often longer than needed. By defining six information categories in LLM responses, it demonstrates that LLMs frequently include redundant information. The authors investigate prompt-engineering strategies and find that appropriate prompts can reduce response length by 25-60%, optimizing energy use without compromising response quality.


<details>
  <summary>Details</summary>
Motivation: Energy consumption during inference of LLMs is high. Output compression, an under-explored area, has potential for energy optimization.

Method: Benchmarking 12 decoder-only LLMs across 5 datasets. Conducting a quality assessment of LLM responses by defining six information categories. Exploring prompt-engineering strategies for reducing response length.

Result: Empirical evaluation shows that using appropriate prompts for length reduction and controlling information content can achieve energy optimization between 25-60% by reducing response length while maintaining response quality.

Conclusion: Output compression through prompt-engineering is a promising approach for making LLM inference more energy-efficient.

Abstract: A significant portion of the energy consumed by Large Language Models (LLMs)
arises from their inference processes; hence developing energy-efficient
methods for inference is crucial. While several techniques exist for inference
optimization, output compression remains relatively unexplored, with only a few
preliminary efforts addressing this aspect. In this work, we first benchmark 12
decoder-only LLMs across 5 datasets, revealing that these models often produce
responses that are substantially longer than necessary. We then conduct a
comprehensive quality assessment of LLM responses, formally defining six
information categories present in LLM responses. We show that LLMs often tend
to include redundant or additional information besides the minimal answer. To
address this issue of long responses by LLMs, we explore several simple and
intuitive prompt-engineering strategies. Empirical evaluation shows that
appropriate prompts targeting length reduction and controlling information
content can achieve significant energy optimization between 25-60\% by reducing
the response length while preserving the quality of LLM responses.

</details>


### [48] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
*Ruiran Su,Jiasheng Si,Zhijiang Guo,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: This paper introduces ClimateViz, a large-scale benchmark for scientific fact-checking using expert-curated scientific charts, showing that current multimodal language models struggle with chart-based reasoning compared to human performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on scientific charts in scientific fact-checking despite their importance in presenting quantitative evidence and statistical reasoning.

Method: Introducing ClimateViz, which includes 49,862 claims linked to 2,896 visualizations labeled as support, refute, or not enough information, with structured knowledge graph explanations.

Result: Current state-of-the-art multimodal language models have low accuracy in chart-based reasoning tasks, with proprietary models like Gemini 2.5 and InternVL 2.5 achieving 76.2 to 77.8 percent accuracy in label-only settings, much lower than human performance.

Conclusion: The study highlights the challenges faced by current multimodal language models in handling chart-based reasoning tasks and provides a new benchmark for future research.

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


### [49] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
*Hee Suk Yoon,Eunseop Yoon,Mark A. Hasegawa-Johnson,Sungwoong Kim,Chang D. Yoo*

Main category: cs.CL

TL;DR: ConfPO is a novel method for preference learning in large language models that enhances alignment quality by focusing on critical tokens without using auxiliary models or extra computation.


<details>
  <summary>Details</summary>
Motivation: To improve alignment quality and mitigate overoptimization in large language models without relying on auxiliary models or increasing computational overhead.

Method: Identifies and optimizes preference-critical tokens based solely on the training policy's confidence, avoiding unnecessary adjustments to all token probabilities.

Result: Outperforms uniform direct alignment algorithms in alignment benchmarks like AlpacaEval 2 and Arena-Hard without additional computational cost.

Conclusion: ConfPO demonstrates effectiveness in improving large language model alignment with a simple, lightweight, and model-free approach.

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [50] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
*Fariz Ikhwantri,Dusica Marijan*

Main category: cs.CL

TL;DR: We propose EXCLAIM, a compliance detection approach based on Natural Language Inference (NLI) to automate regulatory compliance process.


<details>
  <summary>Details</summary>
Motivation: Regulatory compliance process is challenging due to the complicated nature of legal and technical texts, need for model explanations, and limited access to assurance case data.

Method: Formulate claim-argument-evidence structure of an assurance case as a multi-hop inference using NLI and generate assurance cases using LLMs.

Result: The proposed approach demonstrates effectiveness in automating regulatory compliance process.

Conclusion: Our results show the potential of NLI-based approaches in automating the regulatory compliance process.

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [51] [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/abs/2506.08717)
*Mehedi Hasan Bijoy,Dejan Porjazovski,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: This paper introduces a new language-aware multi-teacher knowledge distillation method for multilingual Speech Emotion Recognition (SER), which outperforms fine-tuning and knowledge distillation baselines.


<details>
  <summary>Details</summary>
Motivation: Improving human-computer interaction through multilingual SER systems.

Method: Leveraging Wav2Vec2.0 as the base for monolingual teacher models and distilling their knowledge into a single multilingual student model.

Result: The multilingual student model achieved state-of-the-art performance on English and Finnish datasets.

Conclusion: The proposed method improves recall for sad and neutral emotions but still struggles with anger and happiness recognition.

Abstract: Speech Emotion Recognition (SER) is crucial for improving human-computer
interaction. Despite strides in monolingual SER, extending them to build a
multilingual system remains challenging. Our goal is to train a single model
capable of multilingual SER by distilling knowledge from multiple teacher
models. To address this, we introduce a novel language-aware multi-teacher
knowledge distillation method to advance SER in English, Finnish, and French.
It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and
then distills their knowledge into a single multilingual student model. The
student model demonstrates state-of-the-art performance, with a weighted recall
of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish
dataset, surpassing fine-tuning and knowledge distillation baselines. Our
method excels in improving recall for sad and neutral emotions, although it
still faces challenges in recognizing anger and happiness.

</details>


### [52] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
*Nelvin Tan,Zian Seng,Liang Zhang,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: This paper explores the use of critic and calculator agents in numerical question answering for financial documents without oracle labels, showing improved performance and safety.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge faced by large language models in numerical question answering for financial documents with tabular and textual data. It particularly focuses on the situation without oracle labels.

Method: The paper examines the effectiveness of the traditional critic agent in the absence of oracle labels and proposes an improved critic agent along with a calculator agent.

Result: Experiments show that the performance of the traditional critic agent deteriorates without oracle labels. The proposed improved critic agent and calculator agent outperform the previous state-of-the-art approach and are safer.

Conclusion: The study investigates the interaction between agents and its impact on performance.

Abstract: Large language models (LLMs) have shown impressive capabilities on numerous
natural language processing tasks. However, LLMs still struggle with numerical
question answering for financial documents that include tabular and textual
data. Recent works have showed the effectiveness of critic agents (i.e.,
self-correction) for this task given oracle labels. Building upon this
framework, this paper examines the effectiveness of the traditional critic
agent when oracle labels are not available, and show, through experiments, that
this critic agent's performance deteriorates in this scenario. With this in
mind, we present an improved critic agent, along with the calculator agent
which outperforms the previous state-of-the-art approach (program-of-thought)
and is safer. Furthermore, we investigate how our agents interact with each
other, and how this interaction affects their performance.

</details>


### [53] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
*Dror Kris Markus,Fabrizio Gilardi,Daria Stetsenko*

Main category: cs.CL

TL;DR: This study examines over 100,000 AI-related papers published on ArXiv between 2014 and 2024 to analyze how ethical values and societal concerns are integrated into technical AI research.


<details>
  <summary>Details</summary>
Motivation: Calls to align AI development with ethical and societal values have intensified, but it remains unclear whether interdisciplinary research teams are leading this shift in practice.

Method: Developing a classifier to identify societal content and measuring the extent to which research papers express these considerations.

Result: While interdisciplinary teams remain more likely to produce societally-oriented research, computer science-only teams now account for a growing share of the field's overall societal output.

Conclusion: The findings challenge common assumptions about the drivers of societal AI and raise important questions about the implications for emerging understandings of AI safety and governance.

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [54] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
*Muhammad Anwar,Mishca de Costa,Issam Hammad,Daniel Lau*

Main category: cs.CL

TL;DR: A domain-specific LLM was trained on a single GPU using a small dataset from an open nuclear textbook. It shows promise but needs improvement.


<details>
  <summary>Details</summary>
Motivation: To create an in-house LLM solution that meets cybersecurity and data confidentiality requirements in the nuclear sector.

Method: Training a compact Transformer-based model on a single GPU using publicly available nuclear textbook data.

Result: The model captures some nuclear vocabulary but struggles with syntactic coherence. It has potential for specialized tasks.

Conclusion: Early results show promise for in-house LLMs in nuclear applications, but improvements are needed in data, preprocessing, and instruction tuning.

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [55] [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/abs/2506.08750)
*Muhammad Anwar,Daniel Lau,Mishca de Costa,Issam Hammad*

Main category: cs.CL

TL;DR: This paper discusses using synthetic data generation to create usable question-answer pairs from unstructured text data in the nuclear industry, addressing data scarcity and privacy issues.


<details>
  <summary>Details</summary>
Motivation: To enable advanced Large Language Model applications in the nuclear industry by generating clean, structured question-answer pairs from unstructured text data.

Method: Using Large Language Models to analyze text, extract key information, generate questions, and evaluate the quality of synthetic datasets.

Result: Synthetic data generation can transform existing text data into usable Q&A pairs, which can improve information retrieval, knowledge sharing, and decision-making in the nuclear industry.

Conclusion: Synthetic data generation is a promising approach to unlock the potential of Large Language Models in the nuclear industry.

Abstract: The nuclear industry possesses a wealth of valuable information locked away
in unstructured text data. This data, however, is not readily usable for
advanced Large Language Model (LLM) applications that require clean, structured
question-answer pairs for tasks like model training, fine-tuning, and
evaluation. This paper explores how synthetic data generation can bridge this
gap, enabling the development of robust LLMs for the nuclear domain. We discuss
the challenges of data scarcity and privacy concerns inherent in the nuclear
industry and how synthetic data provides a solution by transforming existing
text data into usable Q&A pairs. This approach leverages LLMs to analyze text,
extract key information, generate relevant questions, and evaluate the quality
of the resulting synthetic dataset. By unlocking the potential of LLMs in the
nuclear industry, synthetic data can pave the way for improved information
retrieval, enhanced knowledge sharing, and more informed decision-making in
this critical sector.

</details>


### [56] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
*Pradyoth Hegde,Santosh Kesiraju,Jan Švec,Šimon Sedláček,Bolaji Yusuf,Oldřich Plchot,Deepak K T,Jan Černocký*

Main category: cs.CL

TL;DR: This study examines how in-context learning can be applied to dialogue state tracking and which factors affect its performance.


<details>
  <summary>Details</summary>
Motivation: To explore the application of in-context learning to dialogue state tracking and identify influencing factors.

Method: Using a sentence embedding based k-nearest neighbour method to select suitable demonstrations for in-context learning, which are then used as input to large language models.

Result: The research analyzes the impact of factors related to demonstration selection and prompt context on dialogue state tracking performance.

Conclusion: The study provides insights into the in-context learning capabilities of large language models for dialogue state tracking.

Abstract: This study explores the application of in-context learning (ICL) to the
dialogue state tracking (DST) problem and investigates the factors that
influence its effectiveness. We use a sentence embedding based k-nearest
neighbour method to retrieve the suitable demonstrations for ICL. The selected
demonstrations, along with the test samples, are structured within a template
as input to the LLM. We then conduct a systematic study to analyse the impact
of factors related to demonstration selection and prompt context on DST
performance. This work is conducted using the MultiWoZ2.4 dataset and focuses
primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and
Llama3.2-3B-Instruct models. Our findings provide several useful insights on
in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [57] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
*Mishca de Costa,Muhammad Anwar,Dave Mercier,Mark Randall,Issam Hammad*

Main category: cs.CL

TL;DR: This paper proposes using function-calling large language models instead of traditional NL-to-SQL methods for retrieving operational data from nuclear power plants, emphasizing accuracy, transparency, and expert review.


<details>
  <summary>Details</summary>
Motivation: The need for accurate and transparent data retrieval in nuclear power plants where traditional NL-to-SQL approaches pose significant risks due to complex legacy databases and unverifiable SQL queries.

Method: Leveraging function-calling large language models by defining pre-approved, purpose-specific functions that encapsulate validated SQL logic, contrasting with direct NL-to-SQL translation.

Result: Demonstrating improved accuracy and maintainability with the proposed function-based approach compared to direct NL-to-SQL generation.

Conclusion: Introducing a novel framework that balances user accessibility with operational safety for robust data retrieval in critical systems.

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [58] [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)
*Ahmed Hasanaath,Aisha Alansari,Ahmed Ashraf,Chafik Salmane,Hamzah Luqman,Saad Ezzini*

Main category: cs.CL

TL;DR: This paper benchmarks multiple reasoning-focused large language models (LLMs), especially DeepSeek models, on fifteen Arabic NLP tasks. It examines their performance using zero-shot, few-shot, and fine-tuning methods.


<details>
  <summary>Details</summary>
Motivation: To explore the performance of LLMs on Arabic data, which has unique linguistic features.

Method: Benchmarking multiple LLMs on fifteen Arabic NLP tasks using different strategies: zero-shot, few-shot, and fine-tuning.

Result: Carefully selected in-context examples improved performance significantly. DeepSeek outperformed a GPT baseline on complex inference tasks. LoRA-based fine-tuning enhanced performance further.

Conclusion: DeepSeek models show strong reasoning capabilities in Arabic NLP tasks.

Abstract: Large language models (LLMs) have shown remarkable progress in reasoning
abilities and general natural language processing (NLP) tasks, yet their
performance on Arabic data, characterized by rich morphology, diverse dialects,
and complex script, remains underexplored. This paper presents a comprehensive
benchmarking study of multiple reasoning-focused LLMs, with a special emphasis
on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP
tasks. We experiment with various strategies, including zero-shot, few-shot,
and fine-tuning. This allows us to systematically evaluate performance on
datasets covering a range of applications to examine their capacity for
linguistic reasoning under different levels of complexity. Our experiments
reveal several key findings. First, carefully selecting just three in-context
examples delivers an average uplift of over 13 F1 points on classification
tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection
from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures
outperform a strong GPT o4-mini baseline by an average of 12 F1 points on
complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning
yields up to an additional 8 points in F1 and BLEU compared to equivalent
increases in model scale. The code is available at
https://anonymous.4open.science/r/AraReasoner41299

</details>


### [59] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
*Francisco Vargas,Alejandro González Coene,Gaston Escalante,Exequiel Lobón,Manuel Pulido*

Main category: cs.CL

TL;DR: This study introduces a two-step method for extracting key entities from legal documents related to traffic accidents, comparing different text segmentation techniques and applying various large language models (LLMs) with fine-tuning to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Quantifying insurance company costs requires precise extraction of entities from legal documents, which is challenging due to complex arguments in court decisions.

Method: A two-step process involving text segmentation using regex or n-token block vectorization followed by entity extraction using LLMs with fine-tuning.

Result: The LLM-based approach outperforms traditional methods; among open-source models, LLaMA-2 70B with fine-tuning is the most accurate at 79.4%, while GPT-4 Turbo achieves the highest accuracy overall at 86.1%.

Conclusion: Fine-tuning improves LLM performance, and recent advancements in model development have led to significant improvements in accuracy.

Abstract: The extraction of information about traffic accidents from legal documents is
crucial for quantifying insurance company costs. Extracting entities such as
percentages of physical and/or psychological disability and the involved
compensation amounts is a challenging process, even for experts, due to the
subtle arguments and reasoning in the court decision. A two-step procedure is
proposed: first, segmenting the document identifying the most relevant
segments, and then extracting the entities. For text segmentation, two
methodologies are compared: a classic method based on regular expressions and a
second approach that divides the document into blocks of n-tokens, which are
then vectorized using multilingual models for semantic searches
(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models
(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to
the selected segments for entity extraction. For the LLaMA models, fine-tuning
is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a
significant number of hallucinations in extractions which are an important
contention point for named entity extraction. This work shows that these
hallucinations are substantially reduced after finetuning the model. The
performance of the methodology based on segment vectorization and subsequent
use of LLMs significantly surpasses the classic method which achieves an
accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning
achieves the highest accuracy 79.4%, surpassing its base version 61.7%.
Notably, the base LLaMA-3 8B model already performs comparably to the finetuned
LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model
development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [60] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)
*Flavio D'Intino,Hans-Peter Hutter*

Main category: cs.CL

TL;DR: 介绍了一个新的瑞士德语语音到文本数据集(SRB-300)，并展示了在该数据集上微调模型后的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的瑞士德语语音到文本系统在自发性对话中的表现不佳，因为现有数据集是在受控环境下收集的。

Method: 在新的SRB-300数据集上对多个OpenAI Whisper模型进行微调。

Result: SRB-300数据集包含300小时的注释语音数据，覆盖了所有主要的瑞士德语方言，并记录了各种现实环境下的真实长音频。

Conclusion: 通过在SRB-300数据集上微调模型，显著提升了瑞士德语语音到文本系统的性能，特别是在真实环境中的表现。

Abstract: Swiss German is a low-resource language represented by diverse dialects that
differ significantly from Standard German and from each other, lacking a
standardized written form. As a result, transcribing Swiss German involves
translating into Standard German. Existing datasets have been collected in
controlled environments, yielding effective speech-to-text (STT) models, but
these models struggle with spontaneous conversational speech.
  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour
annotated speech corpus featuring real-world long-audio recordings from 39
Swiss German radio and TV stations. It captures spontaneous speech across all
major Swiss dialects recorded in various realistic environments and overcomes
the limitation of prior sentence-level corpora.
  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,
achieving notable enhancements over previous zero-shot performance metrics.
Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores
increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a
WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for
developing effective and robust STT systems for Swiss German and other
low-resource languages in real-world contexts.

</details>


### [61] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
*Danush Khanna,Krishna Kumar,Basab Ghosh,Vinija Jain,Vasu Sharma,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: This paper introduces ALKALI, a new adversarial benchmark for evaluating large language models (LLMs), and GRACE, a method to enhance the alignment of these models by considering latent geometry.


<details>
  <summary>Details</summary>
Motivation: To address the increasing adversarial threats against LLMs, which current defenses cannot keep up with.

Method: ALKALI is a comprehensive adversarial benchmark containing 9,000 prompts across different categories and attack types. GRACE is introduced as a method that improves model alignment by enforcing constraints on the latent space.

Result: GRACE reduces the Attack Success Rates (ASRs) by up to 39%. A new metric, AVQI, is also introduced to measure latent alignment failures.

Conclusion: The paper highlights the importance of considering latent geometry in defending against adversarial attacks on LLMs and provides tools to improve their robustness.

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [62] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
*Hiba Khey,Amine Lakhder,Salma Rouichi,Imane El Ghabi,Kamal Hejjaoui,Younes En-nahli,Fahd Kalloubi,Moez Amri*

Main category: cs.CL

TL;DR: A new language model called PlantBert is introduced, which is specifically designed for extracting structured knowledge from plant stress-response literature.


<details>
  <summary>Details</summary>
Motivation: There is a lack of domain-adapted tools for plant science compared to other fields like biomedical and clinical sciences.

Method: Transformer-based modeling combined with rule-enhanced linguistic post-processing and ontology-grounded entity normalization is used to fine-tune the model on a carefully curated corpus of expert-annotated abstracts focusing on lentil responses to various stressors.

Result: PlantBert shows strong generalization capabilities across different entity types and proves that robust domain adaptation is possible even in low-resource scientific fields.

Conclusion: This model fills a crucial gap in agricultural NLP and facilitates the development of intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery.

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.

</details>


### [63] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
*Elias Horner,Cristinel Mateis,Guido Governatori,Agata Ciabattoni*

Main category: cs.CL

TL;DR: This paper introduces a new method using large language models (LLMs) to convert legal texts into formal representations through a pipeline process. The approach is tested with different LLM configurations and achieves good alignment with expert-crafted formalizations.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient way to transform legal texts into formal representations using LLMs.

Method: Propose a pipeline that segments legal language, extracts deontic rules, and evaluates them for coherence, testing various LLM configurations.

Result: Empirical results show promising alignment between machine-generated and expert-crafted formalizations, indicating LLMs' potential in scalable legal informatics.

Conclusion: LLMs, especially when prompted effectively, can significantly contribute to scalable legal informatics.

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [64] [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/abs/2506.08907)
*Antonios Dimakis,John Pavlopoulos,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: 提出一种结合规则与LLMs的方言到标准语言归一化新方法，用于希腊方言，揭示先前研究对谚语分析的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（包括高资源语言的方言）在自然语言理解系统中的困难问题，通过将方言文本转换为标准语言以便下游使用。

Method: 结合基于规则的语料信息转换和针对特定任务的少量提示的大语言模型（LLMs）。不需要并行数据。

Result: 在希腊方言上实施该方法，并在区域谚语的数据集上进行评估，发现以前关于这些谚语的结果仅依赖于表面的语言信息，而新的观察仍然可以通过剩余的语义得出。

Conclusion: 提出的新方法能够有效处理低资源语言的方言到标准语言的归一化问题，并且通过下游实验揭示了之前研究的局限性。

Abstract: Natural language understanding systems struggle with low-resource languages,
including many dialects of high-resource ones. Dialect-to-standard
normalization attempts to tackle this issue by transforming dialectal text so
that it can be used by standard-language tools downstream. In this study, we
tackle this task by introducing a new normalization method that combines
rule-based linguistically informed transformations and large language models
(LLMs) with targeted few-shot prompting, without requiring any parallel data.
We implement our method for Greek dialects and apply it on a dataset of
regional proverbs, evaluating the outputs using human annotators. We then use
this dataset to conduct downstream experiments, finding that previous results
regarding these proverbs relied solely on superficial linguistic information,
including orthographic artifacts, while new observations can still be made
through the remaining semantics.

</details>


### [65] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
*Zeyu Leo Liu,Greg Durrett,Eunsol Choi*

Main category: cs.CL

TL;DR: Introduces PropMEND, a hypernetwork-based method that enhances knowledge propagation in large language models, improving performance on multi-hop question answering tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation of current knowledge editing techniques for large language models which can inject knowledge but fail to propagate it effectively.

Method: A hypernetwork-based approach named PropMEND is introduced, which modifies gradients of a language modeling loss to encourage the propagation of injected knowledge.

Result: PropMEND shows better performance on the RippleEdit dataset, particularly on multi-hop questions, and introduces a new dataset, Controlled RippleEdit, for evaluating the generalization of the hypernetwork.

Conclusion: PropMEND improves the propagation of knowledge in large language models, especially on challenging multi-hop questions, but there is room for improvement in propagating knowledge to a wider range of relations.

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [66] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
*Andrew Shin*

Main category: cs.CL

TL;DR: A single average gaming GPU can train a solid mathematical reasoning model.


<details>
  <summary>Details</summary>
Motivation: Reduce costs for training capable models and democratize access to high-performance AI research.

Method: Integrate reinforcement learning and memory optimization techniques

Result: Trained a 1.5B parameter mathematical reasoning model on RTX 3080 Ti that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger.

Conclusion: State-of-the-art mathematical reasoning does not necessarily require massive infrastructure.

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [67] [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)
*Qinggang Zhang,Zhishang Xiang,Yilin Xiao,Le Wang,Junhui Li,Xinrun Wang,Jinsong Su*

Main category: cs.CL

TL;DR: FaithfulRAG is a novel framework that improves the faithfulness of large language models with retrieval systems by resolving knowledge conflicts through explicit discrepancy modeling.


<details>
  <summary>Details</summary>
Motivation: To address the unfaithfulness issues in LLMs with retrieval systems, especially in cases of knowledge conflict, where the retrieved context conflicts with the model's parametric knowledge.

Method: FaithfulRAG identifies conflicting knowledge at the fact level and designs a self-thinking process for LLMs to reason about and integrate conflicting facts before generating responses.

Result: The proposed method outperforms state-of-the-art methods in extensive experiments.

Conclusion: FaithfulRAG enhances the performance of LLMs with retrieval systems by ensuring more faithful outputs without undermining the model's internal knowledge structure.

Abstract: Large language models (LLMs) augmented with retrieval systems have
demonstrated significant potential in handling knowledge-intensive tasks.
However, these models often struggle with unfaithfulness issues, generating
outputs that either ignore the retrieved context or inconsistently blend it
with the LLM`s parametric knowledge. This issue is particularly severe in cases
of knowledge conflict, where the retrieved context conflicts with the model`s
parametric knowledge. While existing faithful RAG approaches enforce strict
context adherence through well-designed prompts or modified decoding
strategies, our analysis reveals a critical limitation: they achieve
faithfulness by forcibly suppressing the model`s parametric knowledge, which
undermines the model`s internal knowledge structure and increases the risk of
misinterpreting the context. To this end, this paper proposes FaithfulRAG, a
novel framework that resolves knowledge conflicts by explicitly modeling
discrepancies between the model`s parametric knowledge and retrieved context.
Specifically, FaithfulRAG identifies conflicting knowledge at the fact level
and designs a self-thinking process, allowing LLMs to reason about and
integrate conflicting facts before generating responses. Extensive experiments
demonstrate that our method outperforms state-of-the-art methods. The code is
available at https:// github.com/DeepLearnXMU/Faithful-RAG

</details>


### [68] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
*Clara Lachenmaier,Judith Sieker,Sina Zarrieß*

Main category: cs.CL

TL;DR: Large language models face significant challenges in managing conversational grounding, especially in the political domain where misinformation and bias can impact their ability to correct false user beliefs.


<details>
  <summary>Details</summary>
Motivation: Investigate how large language models handle common ground in scenarios involving knowledge possession or lack thereof, particularly in the political domain with high risks of misinformation and grounding failure.

Method: Examine LLMs' responses to direct knowledge questions and loaded questions presupposing misinformation, evaluating their engagement in active grounding and correction of false user beliefs in relation to their knowledge level and political bias.

Result: LLMs struggle with engaging in grounding and rejecting false user beliefs, indicating potential issues in mitigating misinformation in political discourse.

Conclusion: Significant challenges exist in LLMs' ability to manage conversational grounding, particularly in politically sensitive areas where misinformation is prevalent.

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [69] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
*Marek Kadlčík,Michal Štefánik,Timothee Mickus,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: This paper presents a new method for accurately decoding numeric values from pre-trained language models' input embeddings, revealing their capability for precise numerical representation after pre-training. It also demonstrates how aligning embeddings with specific patterns can reduce arithmetic errors.


<details>
  <summary>Details</summary>
Motivation: To address the issue of arithmetic errors in pre-trained language models due to unreliable distributionally learned embeddings for representing exact quantities.

Method: Proposing a novel probing technique that achieves near-perfect accuracy in decoding numeric values from input embeddings across various open-source language models.

Result: The proposed method proves that pre-trained language models can represent numbers precisely after pre-training alone. The probe's accuracy correlates with the models' arithmetic error rates, and aligning embeddings with certain patterns reduces these errors.

Conclusion: After pre-training, language models can represent numbers with remarkable precision, but arithmetic errors arise due to misaligned embeddings; aligning these embeddings with specific patterns can mitigate such errors.

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


### [70] [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/abs/2506.08972)
*Yuan Guo,Tingjia Miao,Zheng Wu,Pengzhou Cheng,Ming Zhou,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 本文介绍了UI-NEXUS基准和AGENT-NEXUS系统，以提高现有移动代理在组合任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的工作主要集中在原子任务上，而忽略了对组合任务的泛化，这在现实世界的应用中是必不可少的。

Method: 提出了一种新的基准UI-NEXUS，并设计了AGENT-NEXUS系统来解决现有移动代理在组合操作任务上的不足。

Result: UI-NEXUS支持在20个完全可控的本地实用应用程序环境和30个在线中英文服务应用程序中的交互评估。实验结果表明，现有移动代理在性能和效率之间难以平衡，导致代表性失败模式如执行不足、过度执行和注意力漂移。AGENT-NEXUS提高了24%到40%的任务成功率。

Conclusion: 现有移动代理在原子任务到组合任务的泛化上存在明显的差距。为了解决这个问题，我们提出了AGENT-NEXUS，这是一个轻量级且高效的调度系统，可以处理组合移动任务。AGENT-NEXUS通过动态地将长时域任务分解成一系列自包含的原子子任务来扩展现有移动代理的能力。

Abstract: Autonomous agents powered by multimodal large language models have been
developed to facilitate task execution on mobile devices. However, prior work
has predominantly focused on atomic tasks -- such as shot-chain execution tasks
and single-screen grounding tasks -- while overlooking the generalization to
compositional tasks, which are indispensable for real-world applications. This
work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile
agents on three categories of compositional operations: Simple Concatenation,
Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in
20 fully controllable local utility app environments, as well as 30 online
Chinese and English service apps. It comprises 100 interactive task templates
with an average optimal step count of 14.05. Experimental results across a
range of mobile agents with agentic workflow or agent-as-a-model show that
UI-NEXUS presents significant challenges. Specifically, existing agents
generally struggle to balance performance and efficiency, exhibiting
representative failure modes such as under-execution, over-execution, and
attention drift, causing visible atomic-to-compositional generalization gap.
Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient
scheduling system to tackle compositional mobile tasks. AGENT-NEXUS
extrapolates the abilities of existing mobile agents by dynamically decomposing
long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS
achieves 24% to 40% task success rate improvement for existing mobile agents on
compositional operation tasks within the UI-NEXUS benchmark without
significantly sacrificing inference overhead. The demo video, dataset, and code
are available on the project page at https://ui-nexus.github.io.

</details>


### [71] [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/abs/2506.08981)
*Satu Hopponen,Tomi Kinnunen,Alexandre Nikolaev,Rosa González Hautamäki,Lauri Tavi,Einar Meister*

Main category: cs.CL

TL;DR: Introduces a new bilingual speech dataset for studying language variability.


<details>
  <summary>Details</summary>
Motivation: To enable research into language variability from phonetic and technological perspectives.

Method: Includes two preliminary case studies exploring the impact of L2 and imitated L2 on an automatic speaker verification system and the articulatory patterns of one speaker in different languages.

Result: The new corpus allows for research into language variability.

Conclusion: The new FROST-EMA corpus is a valuable resource for studying language variability.

Abstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of
Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers,
who produced speech in their native language (L1), second language (L2), and
imitated L2 (fake foreign accent). The new corpus enables research into
language variability from phonetic and technological points of view.
Accordingly, we include two preliminary case studies to demonstrate both
perspectives. The first case study explores the impact of L2 and imitated L2 on
the performance of an automatic speaker verification system, while the second
illustrates the articulatory patterns of one speaker in L1, L2, and a fake
accent.

</details>


### [72] [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/abs/2506.08986)
*Yuejiao Wang,Xianmin Gong,Xixin Wu,Patrick Wong,Hoi-lam Helene Fung,Man Wai Mak,Helen Meng*

Main category: cs.CL

TL;DR: This paper introduces a new language-related fMRI task to detect cognitive decline and early neurocognitive disorder (NCD) in aging populations. It was tested on 97 Chinese elderly and showed high accuracy using machine learning models.


<details>
  <summary>Details</summary>
Motivation: To find a promising method for detecting cognitive decline and early NCD in aging people.

Method: Developing and testing a novel naturalistic language-related fMRI task combined with machine learning classification models.

Result: The proposed task achieved an average AUC of 0.86 in classifying cognitive status and highlighted key brain regions related to language processing.

Conclusion: The naturalistic language-related fMRI task shows great potential for early detection of aging-related cognitive decline and NCD.

Abstract: Early detection is crucial for timely intervention aimed at preventing and
slowing the progression of neurocognitive disorder (NCD), a common and
significant health problem among the aging population. Recent evidence has
suggested that language-related functional magnetic resonance imaging (fMRI)
may be a promising approach for detecting cognitive decline and early NCD. In
this paper, we proposed a novel, naturalistic language-related fMRI task for
this purpose. We examined the effectiveness of this task among 97 non-demented
Chinese older adults from Hong Kong. The results showed that machine-learning
classification models based on fMRI features extracted from the task and
demographics (age, gender, and education year) achieved an average area under
the curve of 0.86 when classifying participants' cognitive status (labeled as
NORMAL vs DECLINE based on their scores on a standard neurcognitive test).
Feature localization revealed that the fMRI features most frequently selected
by the data-driven approach came primarily from brain regions associated with
language processing, such as the superior temporal gyrus, middle temporal
gyrus, and right cerebellum. The study demonstrated the potential of the
naturalistic language-related fMRI task for early detection of aging-related
cognitive decline and NCD.

</details>


### [73] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
*Theo Zhang,Madurya Suresh,Anne S. Warlaumont,Kasia Hitczenko,Alejandrina Cristia,Margaret Cychosz*

Main category: cs.CL

TL;DR: A new large-scale dataset improves speech model performance in classifying child vocalizations.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges posed by small training corpora and the complexities of child speech in various languages.

Method: Applying state-of-the-art transformer models to a novel dataset named SpeechMaturity for classifying child vocalizations into cry, laughter, mature, and immature speech.

Result: The dataset contains 242,004 labeled vocalizations from children acquiring over 25 languages, which is much larger than previous datasets. Models trained on this dataset showed better performance and robustness across different settings.

Conclusion: Models trained on the SpeechMaturity dataset outperform state-of-the-art models and achieved human-level classification accuracy.

Abstract: Speech technology systems struggle with many downstream tasks for child
speech due to small training corpora and the difficulties that child speech
pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer
models to address a fundamental classification task: identifying child
vocalizations. Unlike previous corpora, our dataset captures maximally
ecologically-valid child vocalizations across an unprecedented sample,
comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,
Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004
labeled vocalizations, magnitudes larger than previous work. Models were
trained to distinguish between cry, laughter, mature (consonant+vowel), and
immature speech (just consonant or vowel). Models trained on the dataset
outperform state-of-the-art models trained on previous datasets, achieved
classification accuracy comparable to humans, and were robust across rural and
urban settings.

</details>


### [74] [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003)
*Lei Zhang,Jiaxi Yang,Min Yang,Jian Yang,Mouxiang Chen,Jiajun Zhang,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.CL

TL;DR: Introduces SWE-Flow, a novel data synthesis framework based on Test-Driven Development that automatically generates TDD tasks from unit tests.


<details>
  <summary>Details</summary>
Motivation: To create a more efficient and automated way to generate training data for TDD-based coding by leveraging unit tests.

Method: Constructs a Runtime Dependency Graph (RDG) to capture function interactions and uses it to generate development schedules with corresponding codebases and tests.

Result: Generated 16,061 training instances and 2,020 test instances from GitHub projects, forming the SWE-Flow-Eval benchmark.

Conclusion: Fine-tuning open models on the SWE-Flow-Eval dataset enhances TDD-based coding performance and provides tools and data for future research.

Abstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in
Test-Driven Development (TDD). Unlike existing software engineering data that
rely on human-submitted issues, **SWE-Flow** automatically infers incremental
development steps directly from unit tests, which inherently encapsulate
high-level requirements. The core of **SWE-Flow** is the construction of a
Runtime Dependency Graph (RDG), which precisely captures function interactions,
enabling the generation of a structured, step-by-step *development schedule*.
At each step, **SWE-Flow** produces a partial codebase, the corresponding unit
tests, and the necessary code modifications, resulting in fully verifiable TDD
tasks. With this approach, we generated 16,061 training instances and 2,020
test instances from real-world GitHub projects, creating the **SWE-Flow-Eval**
benchmark. Our experiments show that fine-tuning open model on this dataset
significantly improves performance in TDD-based coding. To facilitate further
research, we release all code, datasets, models, and Docker images at
[Github](https://github.com/Hambaobao/SWE-Flow).

</details>


### [75] [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/abs/2506.09009)
*Hakyung Sung,Gyu-Ho Shin,Chanyoung Lee,You Kyung Sung,Boo Kyung Jung*

Main category: cs.CL

TL;DR: This study introduces a semi-automated framework to identify morphosyntactic constructions for second-language Korean and evaluates its impact on morphosyntactic analysis model performance.


<details>
  <summary>Details</summary>
Motivation: To improve morphosyntactic tagging and dependency-parsing accuracy in second-language Korean by creating a more consistent annotation framework.

Method: Introducing a semi-automated framework that identifies morphosyntactic constructions from XPOS sequences and aligns them with UPOS categories, and expanding the corpus with 2,998 new annotated sentences.

Result: The aligned dataset improves consistency across annotation layers and enhances morphosyntactic tagging and dependency-parsing accuracy, especially with limited annotated data.

Conclusion: The study demonstrates the effectiveness of the semi-automated framework in improving second-language Korean morphosyntactic analysis.

Abstract: The present study extends recent work on Universal Dependencies annotations
for second-language (L2) Korean by introducing a semi-automated framework that
identifies morphosyntactic constructions from XPOS sequences and aligns those
constructions with corresponding UPOS categories. We also broaden the existing
L2-Korean corpus by annotating 2,998 new sentences from argumentative essays.
To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean
morphosyntactic analysis models on datasets both with and without these
alignments, using two NLP toolkits. Our results indicate that the aligned
dataset not only improves consistency across annotation layers but also
enhances morphosyntactic tagging and dependency-parsing accuracy, particularly
in cases of limited annotated data.

</details>


### [76] [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/abs/2506.09014)
*Jianing Qi,Xi Ye,Hao Tang,Zhigang Zhu,Eunsol Choi*

Main category: cs.CL

TL;DR: Scaling test-time compute yields significant performance improvements for large language models (LLMs). A new method is proposed where a compact LLM, Sample Set Aggregator (SSA), takes multiple samples and outputs the final answer via reinforcement learning. Experiments on reasoning datasets demonstrate SSA's superior performance compared to other methods.


<details>
  <summary>Details</summary>
Motivation: Scaling test-time compute brings performance gains for LLMs in math domains.

Method: Train a compact LLM named Sample Set Aggregator (SSA) to output the final answer by optimizing answer accuracy with reinforcement learning.

Result: SSA surpasses other test-time scaling methods like reward model-based re-ranking and shows good generalization ability.

Conclusion: Separating LLMs into generators and analyzers/aggregators allows efficient use with premier black box models.

Abstract: Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.

</details>


### [77] [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/abs/2506.09021)
*Hakyung Sung,Karla Csuros,Min-Chang Sung*

Main category: cs.CL

TL;DR: This study compares human and LLM proofreading on second language writings and finds both improve bigram lexical features but LLMs use more generative methods.


<details>
  <summary>Details</summary>
Motivation: To examine how human and LLM proofreading affect second language writings' intelligibility and compare their consistency.

Method: Examine lexical and syntactic interventions of human and LLM proofreading (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b) on identical second language writings.

Result: Both human and LLM proofreading improve bigram lexical features contributing to better coherence. LLMs exhibit more generative approaches with diverse and sophisticated vocabulary and more adjective modifiers.

Conclusion: Proofreading by both humans and LLMs improves lexical and syntactic features consistently across three models.

Abstract: This study examines the lexical and syntactic interventions of human and LLM
proofreading aimed at improving overall intelligibility in identical second
language writings, and evaluates the consistency of outcomes across three LLMs
(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and
LLM proofreading enhance bigram lexical features, which may contribute to
better coherence and contextual connectedness between adjacent words. However,
LLM proofreading exhibits a more generative approach, extensively reworking
vocabulary and sentence structures, such as employing more diverse and
sophisticated vocabulary and incorporating a greater number of adjective
modifiers in noun phrases. The proofreading outcomes are highly consistent in
major lexical and syntactic features across the three models.

</details>


### [78] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
*Haozhen Zhang,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: Router-R1是一种基于强化学习的框架，用于多大型语言模型的路由和聚合，通过交错“思考”和“路由”动作来增强复杂任务处理能力，并优化性能-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由器通常只能进行单一的、一对一的映射，限制了其处理需要多个LLM互补优势的复杂任务的能力。

Method: Router-R1将多LLM路由和聚合表述为一个序列决策过程，使用轻量级基于规则的奖励机制，并且只依赖简单的模型描述符如价格、延迟和示例性能。

Result: 在七个通用和多跳QA基准测试中，Router-R1的表现优于几个强大的基线模型，同时保持了鲁棒的泛化能力和成本管理。

Conclusion: Router-R1展示了如何通过强化学习优化性能-成本权衡，并在未见过的模型选择上表现出了良好的泛化能力。

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


### [79] [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)
*Yaniv Nikankin,Dana Arad,Yossi Gandelsman,Yonatan Belinkov*

Main category: cs.CL

TL;DR: This paper investigates the accuracy gap between vision-language models' performance on visual and textual tasks, analyzing task-specific circuits in each modality and proposing a method to patch visual data token representations from later layers back into earlier layers, which reduces the performance gap by a third on average.


<details>
  <summary>Details</summary>
Motivation: To understand why vision-language models perform better on textual tasks than visual ones and find ways to reduce the performance gap between modalities.

Method: Identifying and comparing circuits in different modalities, observing data representation alignment, and patching visual data token representations from later layers back into earlier layers.

Result: The intervention closes a third of the performance gap between modalities on average across multiple tasks and models.

Conclusion: The study provides insights into the multi-modal performance gap in vision-language models and proposes a training-free approach to mitigate it.

Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions
on visual inputs (e.g., counting objects in an image), yet demonstrate higher
accuracies when performing an analogous task on text (e.g., counting words in a
text). We investigate this accuracy gap by identifying and comparing the
\textit{circuits} - the task-specific computational sub-graphs - in different
modalities. We show that while circuits are largely disjoint between
modalities, they implement relatively similar functionalities: the differences
lie primarily in processing modality-specific data positions (an image or a
text sequence). Zooming in on the image data representations, we observe they
become aligned with the higher-performing analogous textual representations
only towards later layers, too late in processing to effectively influence
subsequent positions. To overcome this, we patch the representations of visual
data tokens from later layers back into earlier layers. In experiments with
multiple tasks and models, this simple intervention closes a third of the
performance gap between the modalities, on average. Our analysis sheds light on
the multi-modal performance gap in VLMs and suggests a training-free approach
for reducing it.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [80] [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
*Ken Gu,Zhihan Zhang,Kate Lin,Yuwei Zhang,Akshay Paruchuri,Hong Yu,Mehran Kazemi,Kumar Ayush,A. Ali Heydari,Maxwell A. Xu,Girish Narayanswamy,Yun Liu,Ming-Zher Poh,Yuzhe Yang,Mark Malhotra,Shwetak Patel,Hamid Palangi,Xuhai Xu,Daniel McDuff,Tim Althoff,Xin Liu*

Main category: cs.DB

TL;DR: This paper introduces RADAR, a benchmark for evaluating data-aware reasoning in language models on tabular data with artifacts.


<details>
  <summary>Details</summary>
Motivation: To explore the data awareness of language models and evaluate their ability to handle data artifacts like missing values and outliers in real-world tabular data.

Method: Developing a framework to simulate data artifacts through programmatic perturbations and creating RADAR with 2980 table query pairs across 9 domains and 5 artifact types.

Result: Frontier models show significant degradation in performance when dealing with data artifacts compared to clean data.

Conclusion: There are critical gaps in the robustness of language models for data-aware analysis, highlighting the need for further research and development in this area.

Abstract: Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [81] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: 这项研究调查了在观看自然电影时记录的大脑活动与从多模态大型语言模型（MLLMs）得出的表示之间的对齐程度，发现任务特定指令可以提高MLLMs与大脑活动之间的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 先前评估MLLMs大脑对齐的工作主要集中在单模态设置或依赖于非指令微调的多模态模型来进行多模态刺激。

Method: 使用来自六个视频和两个音频指令微调MLLMs的指令特定嵌入来测量神经活动的预测性。

Result: 指令微调视频MLLMs显著优于非指令微调多模态模型（高出15%）和单模态模型（高出20%）。

Conclusion: 任务特定指令可以提高多模态大型语言模型（MLLMs）与大脑活动之间的对齐程度。

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [82] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: This work explores using modern decoder-only LLMs as text encoders for text-to-image diffusion models, finding that layer-normalized averaging across all layers improves alignment with complex prompts.


<details>
  <summary>Details</summary>
Motivation: Many text-to-image models still use outdated T5 and CLIP as text encoders.

Method: Train 27 text-to-image models with 12 different text encoders to analyze critical aspects of LLMs impacting text-to-image generation.

Result: Using last-layer embeddings as conditioning leads to inferior performance; layer-normalized averaging across all layers improves alignment with complex prompts.

Conclusion: Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [83] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 提出了一种名为Step AG的自适应指导策略，该策略简单且适用于通用扩散模型。实验表明，将无分类器指导限制在去噪步骤的前几步足以生成高质量、条件良好的图像，并且可以平均加速20%到30%。


<details>
  <summary>Details</summary>
Motivation: 现有的无分类器指导方法需要比无条件生成多一倍的模型转发步骤，导致成本显著增加。虽然有研究提出了自适应指导的概念，但缺乏坚实的分析和实证结果，因此无法应用于通用扩散模型。

Method: 提出了一种新的自适应指导应用视角，并提出了Step AG策略。

Result: Step AG策略在保持图像质量和文本对齐的同时，能够实现平均20%到30%的速度提升，并且这种改进在不同的设置和模型中都是一致的。

Conclusion: 提出的Step AG策略是一种简单且普遍适用的自适应指导策略，可以显著提高扩散模型的效率。

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [84] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: This paper introduces CulturalFrames, a benchmark for evaluating how well text-to-image models meet cultural expectations across different countries and socio-cultural domains.


<details>
  <summary>Details</summary>
Motivation: To systematically examine the alignment of text-to-image models with explicit and implicit cultural expectations.

Method: Introducing CulturalFrames, which includes 983 prompts, 3637 images generated by 4 state-of-the-art models, and over 10k human annotations from 10 countries and 5 socio-cultural domains.

Result: Text-to-image models fail to meet both explicit and implicit cultural expectations, with explicit expectations being missed 68% of the time and implicit ones 49% of the time on average.

Conclusion: Current text-to-image models and evaluation metrics lack cultural awareness, suggesting a need for improvement in both model development and evaluation methodologies.

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [85] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: This paper investigates a method to elicit hidden knowledge from non-reasoning vision-language models by using a Monte Carlo Tree Search-inspired algorithm, which improves performance across three benchmarks.


<details>
  <summary>Details</summary>
Motivation: To find out if hidden knowledge can be elicited from non-reasoning models without additional training or supervision.

Method: Using a Monte Carlo Tree Search-inspired algorithm to inject subquestion-subanswer pairs into the model's output stream.

Result: The method shows consistent improvements across three benchmarks, with a 2% overall improvement on MMMU-PRO and a significant 9% gain in Liberal Arts.

Conclusion: The results suggest that framing reasoning as a search process can help non-reasoning models 'connect the dots' between fragmented knowledge and produce extended reasoning traces.

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [86] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: This paper introduces ASVR to jointly learn visual and textual modalities for better multimodal understanding.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs have limitations in utilizing images, captions, and vision-centric content, and effectively leveraging autoregressive visual supervision remains a challenge.

Method: Introduce ASVR, which enables joint learning of visual and textual modalities within a unified autoregressive framework.

Result: Autoregressively reconstructing semantic representation of images consistently improves comprehension and delivers significant performance gains across varying data scales and types of LLM backbones.

Conclusion: ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks.

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
*Peng-Yuan Wang,Tian-Shuo Liu,Chenyang Wang,Yi-Di Wang,Shu Yan,Cheng-Xing Jia,Xu-Hui Liu,Xin-Wei Chen,Jia-Cheng Xu,Ziniu Li,Yang Yu*

Main category: cs.AI

TL;DR: This survey reviews the development of mathematical reasoning in large language models (LLMs), covering comprehension and answer generation phases, enhancement methods, challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To examine the progress and challenges in mathematical reasoning abilities of LLMs and provide insights for future research.

Method: Review of methods for enhancing mathematical reasoning, including training-free prompting and fine-tuning approaches.

Result: Significant advances in mathematical reasoning abilities of LLMs, but challenges remain in capacity, efficiency, and generalization.

Conclusion: Highlighting promising research directions for improving reasoning capabilities of LLMs and applying them to other domains.

Abstract: Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [88] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Yingjie Wang,Baisheng Lai,Jieping Ye,Mingli Song,Dacheng Tao*

Main category: cs.AI

TL;DR: 提出了一种新的自我奖励机制CoVo，使大型语言模型能够在没有外部监督的情况下进行有效的强化学习推理。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂推理任务中的应用受到外部监督限制的制约，因此需要一种无需外部监督就能有效训练的方法。

Method: 提出了一种新的自我奖励机制CoVo，该机制结合了大型语言模型推理过程中中间状态的一致性和波动性，并引入了好奇心奖励来促进多样化的探索。

Result: 实验表明，使用CoVo方法的大型语言模型在多种推理基准测试中的性能与有监督的强化学习相当甚至更好。

Conclusion: 提出了一个新颖的自我奖励强化学习框架CoVo，它通过整合一致性（Consistency）和波动性（Volatility）来增强大型语言模型的推理能力，并且在不同的推理基准测试中表现优异。

Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [89] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti,Michael Färber*

Main category: cs.AI

TL;DR: This paper introduces a novel approach that combines Knowledge Graphs and Large Language Models to improve knowledge-based causal discovery.


<details>
  <summary>Details</summary>
Motivation: Existing methods using Large Language Models often produce unstable and inconsistent results for causal inference.

Method: Integrates Knowledge Graphs with Large Language Models, identifies informative metapath-based subgraphs within KGs, and refines the selection using Learning-to-Rank-based models.

Result: The method improves the effectiveness of Large Language Models in inferring causal relationships, outperforming most baselines by up to 44.4 points in F1 scores.

Conclusion: This work demonstrates the potential of combining Knowledge Graphs and Large Language Models for knowledge-based causal discovery.

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [90] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
*Irene Testini,José Hernández-Orallo,Lorenzo Pacchiardi*

Main category: cs.AI

TL;DR: This paper reviews evaluations of large language model assistants and agents in data science, revealing a bias towards certain tasks and neglecting broader collaboration and automation possibilities.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of how LLMs are currently being evaluated in the context of data science, highlighting gaps and areas for future research.

Method: Survey and analysis of existing evaluations of LLM assistants and agents for data science.

Result: The survey identifies three main issues: a narrow focus on specific tasks, a lack of consideration for intermediate human-AI collaboration levels, and an overemphasis on human substitution rather than task transformation.

Conclusion: The paper concludes that the evaluation of LLMs for data science has focused too narrowly on specific tasks and modes of operation, overlooking broader aspects such as data management, exploratory activities, and various levels of human-AI collaboration.

Abstract: Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [91] [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
*Abdellah Ghassel,Ian Robinson,Gabriel Tanase,Hal Cooper,Bryan Thompson,Zhen Han,Vassilis N. Ioannidis,Soji Adeshina,Huzefa Rangwala*

Main category: cs.IR

TL;DR: A new method called HLG improves multi-hop retrieval and summarization by creating a hierarchical graph that connects related information across multiple documents.


<details>
  <summary>Details</summary>
Motivation: Current RAG methods struggle with questions requiring information from semantically distant documents and existing benchmarks lack the complexity needed to evaluate multi-hop systems.

Method: HLG creates a three-tiered index that tracks propositions to their sources, clusters them into topics, and links entities and relations. Two retrievers, StatementGraphRAG and TopicGraphRAG, are built on top of HLG.

Result: HLG improves retrieval recall and correctness by an average of 23.1% compared to naive chunk-based RAG.

Conclusion: The proposed methods outperform previous approaches in multi-hop retrieval tasks and provide a more robust evaluation framework through a synthetic dataset generator.

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in
external evidence, yet it still falters when answers must be pieced together
across semantically distant documents. We close this gap with the Hierarchical
Lexical Graph (HLG), a three-tier index that (i) traces every atomic
proposition to its source, (ii) clusters propositions into latent topics, and
(iii) links entities and relations to expose cross-document paths. On top of
HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,
which performs fine-grained entity-aware beam search over propositions for
high-precision factoid questions, and TopicGraphRAG, which selects coarse
topics before expanding along entity links to supply broad yet relevant context
for exploratory queries. Additionally, existing benchmarks lack the complexity
required to rigorously evaluate multi-hop summarization systems, often focusing
on single-document queries or limited datasets. To address this, we introduce a
synthetic dataset generation pipeline that curates realistic, multi-document
question-answer pairs, enabling robust evaluation of multi-hop retrieval
systems. Extensive experiments across five datasets demonstrate that our
methods outperform naive chunk-based RAG achieving an average relative
improvement of 23.1% in retrieval recall and correctness. Open-source Python
library is available at https://github.com/awslabs/graphrag-toolkit.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [92] [Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs](https://arxiv.org/abs/2506.08633)
*Šimon Sedláček,Bolaji Yusuf,Ján Švec,Pradyoth Hegde,Santosh Kesiraju,Oldřich Plchot,Jan Černocký*

Main category: eess.AS

TL;DR: This paper introduces a method that connects speech encoders and large language models through a small module for dialogue state tracking, achieving state-of-the-art results on the SpokenWOZ dataset.


<details>
  <summary>Details</summary>
Motivation: To develop an open-source and open-data system for dialogue state tracking using speech inputs.

Method: The authors use a connector module to bridge speech encoders and large language models like WavLM-large and OLMo, and they experiment with different aspects such as fine-tuning and post-processing.

Result: The best model achieved 34.66% Joint Goal Accuracy (JGA) on SpokenWOZ test set, and with additional model Gemma-2-9B-instruct, it reached 42.17% JGA.

Conclusion: The proposed method shows promise in improving dialogue state tracking with speech inputs, especially in handling named entities within dialogue slots.

Abstract: In this work, we approach spoken Dialogue State Tracking (DST) by bridging
the representation spaces of speech encoders and LLMs via a small connector
module, with a focus on fully open-sourced and open-data components
(WavLM-large, OLMo). We focus on ablating different aspects of such systems
including full/LoRA adapter fine-tuning, the effect of agent turns in the
dialogue history, as well as fuzzy matching-based output post-processing, which
greatly improves performance of our systems on named entities in the dialogue
slot values. We conduct our experiments on the SpokenWOZ dataset, and
additionally utilize the Speech-Aware MultiWOZ dataset to augment our training
data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned
models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our
system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%
JGA on SpokenWOZ test.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [93] [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/abs/2506.08967)
*Ailin Huang,Bingxin Li,Bruce Wang,Boyong Wu,Chao Yan,Chengli Feng,Heng Wang,Hongyu Zhou,Hongyuan Wang,Jingbei Li,Jianjian Sun,Joanna Wang,Mingrui Chen,Peng Liu,Ruihang Miao,Shilei Jiang,Tian Fei,Wang You,Xi Chen,Xuerui Yang,Yechang Huang,Yuxiang Zhang,Zheng Ge,Zheng Gong,Zhewei Huang,Zixin Zhang,Bin Wang,Bo Li,Buyun Ma,Changxin Miao,Changyi Wan,Chen Xu,Dapeng Shi,Dingyuan Hu,Enle Liu,Guanzhe Huang,Gulin Yan,Hanpeng Hu,Haonan Jia,Jiahao Gong,Jiaoren Wu,Jie Wu,Jie Yang,Junzhe Lin,Kaixiang Li,Lei Xia,Longlong Gu,Ming Li,Nie Hao,Ranchen Ming,Shaoliang Pang,Siqi Liu,Song Yuan,Tiancheng Cao,Wen Li,Wenqing He,Xu Zhao,Xuelin Zhang,Yanbo Yu,Yinmin Zhong,Yu Zhou,Yuanwei Liang,Yuanwei Lu,Yuxiang Yang,Zidong Yang,Zili Zhang,Binxing Jiao,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Xinhao Zhang,Yibo Zhu,Daxin Jiang,Shuchang Zhou,Chen Hu*

Main category: cs.SD

TL;DR: A large audio-language model is introduced for end-to-end audio query-audio answer tasks.


<details>
  <summary>Details</summary>
Motivation: Current text-based outputs limit natural speech response generation for seamless audio interactions.

Method: Dual-codebook audio tokenizer, large backbone LLM, and neural vocoder are integrated.

Result: Outperforms state-of-the-art models in speech control on StepEval-Audio-360.

Conclusion: This work provides a promising solution for end-to-end LALMs with token-based vocoders enhancing AQAA task performance.

Abstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent
human-computer interaction, yet their reliance on text-based outputs limits
their ability to generate natural speech responses directly, hindering seamless
audio interactions. To address this, we introduce Step-Audio-AQAA, a fully
end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model
integrates a dual-codebook audio tokenizer for linguistic and semantic feature
extraction, a 130-billion-parameter backbone LLM and a neural vocoder for
high-fidelity speech synthesis. Our post-training approach employs interleaved
token-output of text and audio to enhance semantic coherence and combines
Direct Preference Optimization (DPO) with model merge to improve performance.
Evaluations on the StepEval-Audio-360 benchmark demonstrate that
Step-Audio-AQAA excels especially in speech control, outperforming the
state-of-art LALMs in key areas. This work contributes a promising solution for
end-to-end LALMs and highlights the critical role of token-based vocoder in
enhancing overall performance for AQAA tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [94] [GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors](https://arxiv.org/abs/2506.08188)
*Wenlong Meng,Shuguo Fan,Chengkun Wei,Min Chen,Yuwei Li,Yuanchao Zhang,Zhikun Zhang,Wenzhi Chen*

Main category: cs.CR

TL;DR: GradEscape is a new gradient-based method for attacking AI-generated text detectors. It uses weighted embeddings to handle discrete text and can adapt to different language model architectures.


<details>
  <summary>Details</summary>
Motivation: To create a gradient-based evader for AI-generated text detectors.

Method: Introduces weighted embeddings, warm-started evader method, and novel tokenizer inference and model extraction techniques.

Result: Outperforms other state-of-the-art AIGT evaders on multiple datasets and models.

Conclusion: The main vulnerability of AI-generated text detectors comes from differences in text expression styles in training data. A potential defense strategy is suggested, and the code is open-sourced.

Abstract: In this paper, we introduce GradEscape, the first gradient-based evader
designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the
undifferentiable computation problem, caused by the discrete nature of text, by
introducing a novel approach to construct weighted embeddings for the detector
input. It then updates the evader model parameters using feedback from victim
detectors, achieving high attack success with minimal text modification. To
address the issue of tokenizer mismatch between the evader and the detector, we
introduce a warm-started evader method, enabling GradEscape to adapt to
detectors across any language model architecture. Moreover, we employ novel
tokenizer inference and model extraction techniques, facilitating effective
evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language
models, benchmarking it against four state-of-the-art AIGT evaders.
Experimental results demonstrate that GradEscape outperforms existing evaders
in various scenarios, including with an 11B paraphrase model, while utilizing
only 139M parameters. We have successfully applied GradEscape to two real-world
commercial AIGT detectors. Our analysis reveals that the primary vulnerability
stems from disparity in text expression styles within the training data. We
also propose a potential defense strategy to mitigate the threat of AIGT
evaders. We open-source our GradEscape for developing more robust AIGT
detectors.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [95] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: This paper proposes Modality-Balancing Preference Optimization (MBPO), a novel preference learning framework that addresses modality imbalance in Large Multimodal Models (LMMs). MBPO improves LMM performance on vision-language tasks and reduces hallucinations.


<details>
  <summary>Details</summary>
Motivation: Most LMMs suffer from severe modality imbalance during reasoning, outweighing language prior biases over visual inputs, which limits their generalization to downstream tasks and causes hallucinations. Existing preference optimization approaches do not restrain LLM biases or adapt to dynamic distributional shifts.

Method: MBPO constructs an effective offline preference dataset using hard negatives generated through adversarial perturbation of input images. It generates online responses with verified rewards and trains the model with offline-online hybrid data using Group Relative Policy Optimization (GRPO).

Result: Extensive experiments show that MBPO enhances LMM performance on challenging vision-language tasks and effectively reduces hallucinations.

Conclusion: MBPO is a promising approach to address modality imbalance in LMMs, improving their performance and reducing hallucinations.

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [96] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: Bingo is an RL framework that improves both accuracy and efficiency of large language models' reasoning by designing length-based rewards.


<details>
  <summary>Details</summary>
Motivation: Existing methods either focus on accuracy improvement or directly control length with noticeable accuracy drop.

Method: Incorporates two key mechanisms: significance-aware length reward and dynamic length reward.

Result: Experiments across multiple reasoning benchmarks show that Bingo improves both accuracy and efficiency.

Conclusion: Training LLMs explicitly for efficient reasoning has potential.

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [97] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Main category: cs.LG

TL;DR: AutoSDT is an automatic pipeline that constructs AutoSDT-5K, the largest open dataset for data-driven scientific discovery.


<details>
  <summary>Details</summary>
Motivation: Building AI co-scientists is challenging due to limited high-quality data for training and evaluation. To address this, AutoSDT aims to collect high-quality coding tasks.

Method: AutoSDT leverages the coding capabilities and parametric knowledge of LLMs to search for diverse sources, select ecologically valid tasks, and synthesize accurate task instructions and code solutions.

Result: Expert feedback shows that 93% of the collected tasks are ecologically valid, and 92.2% of the synthesized programs are functionally correct. AutoSDT-Coder, trained on AutoSDT-5K, improves performance on two benchmarks.

Conclusion: AutoSDT is an automatic pipeline that collects high-quality coding tasks in real-world data-driven discovery workflows. It constructs AutoSDT-5K, which is the largest open dataset for data-driven scientific discovery.

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [98] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Main category: cs.LG

TL;DR: 提出了一种新的方法HC-RLHF，用于提高语言模型的安全性，实验表明这种方法可以有效提升模型的安全性，无害性和帮助性。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型对齐方法常常将安全性视为对帮助性的权衡，这可能导致在敏感领域产生不可接受的回答。为了确保这些领域的可靠性能，提出了HC-RLHF方法。

Method: High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF)，包括一个两步过程来找到安全解决方案，首先在成本约束的故意悲观版本下优化奖励函数，在第二步中对训练好的模型进行安全性测试。

Result: HC-RLHF方法应用于三个不同的语言模型（Qwen2-1.5B, Qwen2.5-3B, 和 LLaMa3.2-3B），结果显示该方法产生的模型具有更高的安全性，并且提高了无害性和帮助性。

Conclusion: HC-RLHF方法确保了在敏感领域的可靠性能，提供了高置信度的安全保证同时最大化了帮助性。理论分析表明，它不会返回超出用户指定阈值的不安全解决方案。实证分析显示，与以前的方法相比，HC-RLHF产生了更安全的模型，并且提高了无害性和帮助性。

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [99] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: An approach called ECON improves multi-agent coordination for large language models, enhancing performance on complex reasoning tasks while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning power of LLMs in multi-agent frameworks while reducing computational costs and ensuring convergence guarantees.

Method: ECON uses a Bayesian Nash equilibrium approach where each LLM independently selects responses based on beliefs about other agents, without inter-agent communication.

Result: ECON outperforms other methods by 11.2% on average across six benchmarks and can incorporate additional models.

Conclusion: ECON provides a scalable solution for creating powerful multi-LLM ensembles.

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [100] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Main category: cs.LG

TL;DR: This paper presents AR-Bench, a new benchmark for evaluating large language models' active reasoning skills, which require interaction with external systems to gather missing evidence or data. Current models struggle with these tasks, showing significant gaps compared to their passive reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate active reasoning skills in large language models, which have been less explored compared to passive reasoning.

Method: Developed AR-Bench with three task families: detective cases, situation puzzles, and guessing numbers, simulating real-world scenarios and measuring various types of reasoning.

Result: Contemporary LLMs face challenges in acquiring and using necessary information for active reasoning tasks, showing poorer performance than expected from their passive reasoning abilities.

Conclusion: There is a critical need to improve methods for active reasoning in LLMs, including interactive learning and real-time feedback loops.

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [101] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Main category: cs.LG

TL;DR: This paper introduces DPSDP, a reinforcement learning algorithm that improves the reasoning capabilities of large language models through a multi-turn refinement process modeled as a Markov Decision Process.


<details>
  <summary>Details</summary>
Motivation: Existing methods for boosting reasoning capabilities of large language models suffer from restricted feedback spaces and lack of coordinated training of different parties.

Method: Modeling the multi-turn refinement process as a Markov Decision Process and introducing DPSDP, which trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data.

Result: On benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models.

Conclusion: Theoretical analysis shows that DPSDP can match the performance of any policy within the training distribution, and empirical results confirm the benefits of multi-agent collaboration and out-of-distribution generalization.

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [102] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Main category: cs.LG

TL;DR: A new framework called Reinforcement-Learned Teachers (RLTs) is introduced to avoid the exploration challenge in training reasoning language models with reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of training reasoning language models with reinforcement learning, particularly the exploration problem and the need for effective downstream distillation.

Method: RLTs are trained with dense rewards obtained from feeding their explanations to a student model and testing its understanding of the problem's solution.

Result: The raw outputs of RLTs provide higher final performance on competition and graduate-level tasks compared to existing distillation and cold-starting pipelines.

Conclusion: RLTs maintain their effectiveness when training larger students and applied zero-shot to out-of-distribution tasks, improving the efficiency and re-usability of the RL reasoning framework.

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [103] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Main category: cs.LG

TL;DR: This paper examines the reliability of Large Language Models (LLMs) and finds that the 'geometry of truth' used to evaluate their answers is task-dependent and does not transfer well between different tasks.


<details>
  <summary>Details</summary>
Motivation: To assess the practical relevance of LLMs by examining the activations they produce during inference to determine if their answers are correct.

Method: The authors investigate whether linear classifiers can distinguish between correct and incorrect answers based on these activations across different tasks.

Result: They find that these 'geometries of truth' are task-specific and do not transfer effectively between tasks, even when using more complex methods.

Conclusion: The study highlights a significant limitation in relying on activation-based evaluations for assessing the reliability of LLMs across diverse tasks.

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [104] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Main category: cs.LG

TL;DR: 提出了一种自我意识弱点驱动的问题合成框架，用于提高大型语言模型在复杂推理任务上的性能


<details>
  <summary>Details</summary>
Motivation: 现有的基于蒸馏的合成数据集缺乏高质量的人类标注数学问题和精确可验证的答案，且大多数问题生成策略不考虑模型的能力，导致低效的问题生成

Method: 提出了一个Self-aware Weakness-driven problem Synthesis framework (SwS)，用于识别模型在强化学习训练过程中的失败案例，并从中提取核心概念来合成新的问题

Result: 通过使用SwS框架，模型能够在迭代过程中逐步克服其弱点，并在多个推理基准上表现出显著的性能提升

Conclusion: 引入了自我意识弱点驱动的问题合成框架(SwS)，该框架通过系统地识别模型的缺陷并利用它们来增强问题集，在八种主流推理基准上使7B和32B模型的平均性能分别提高了10.0%和7.7%

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [105] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Main category: cs.LG

TL;DR: This paper explores how to improve large language model (LLM) reasoning through test-time scaling and introduces a method called in-context exploration. This method trains the LLM to chain operations or test multiple hypotheses before committing to an answer. The paper identifies three key components of this method, which together create the best known 1.7B model according to AIME'25 and HMMT'25 scores, and can extrapolate to twice the training token budget.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how to improve LLM reasoning by utilizing more compute at inference time, especially in terms of extrapolation on hard problems.

Method: The method involves training the LLM to perform in-context exploration by chaining operations or testing multiple hypotheses. The method includes three key components: chaining skills with asymmetric competence, leveraging negative gradients to amplify exploration, and coupling task difficulty with training token budget through a curriculum.

Result: The result is the creation of the best known 1.7B model according to AIME'25 and HMMT'25 scores, which can extrapolate to twice the training token budget. The model also improves pass@1 and pass@k scores over the base model.

Conclusion: The conclusion is that in-context exploration is a promising method to improve LLM reasoning, and the three key components identified in the paper contribute to its success.

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [106] [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
*Issa Sugiura,Takashi Ishida,Taro Makino,Chieko Tazuke,Takanori Nakagawa,Kosuke Nakago,David Ha*

Main category: q-fin.ST

TL;DR: Introduce EDINET-Bench, an open-source Japanese financial benchmark to evaluate LLMs on financial tasks.


<details>
  <summary>Details</summary>
Motivation: Lack of accessible research resources for financial analytics, especially for Japanese financial data.

Method: Downloaded annual reports from EDINET and assigned labels for evaluation tasks.

Result: State-of-the-art LLMs perform only slightly better than logistic regression in binary classification tasks.

Conclusion: Significant challenges exist in applying LLMs to real-world financial applications, emphasizing the need for domain-specific adaptation.

Abstract: Financial analysis presents complex challenges that could leverage large
language model (LLM) capabilities. However, the scarcity of challenging
financial datasets, particularly for Japanese financial data, impedes academic
innovation in financial analytics. As LLMs advance, this lack of accessible
research resources increasingly hinders their development and evaluation in
this specialized domain. To address this gap, we introduce EDINET-Bench, an
open-source Japanese financial benchmark designed to evaluate the performance
of LLMs on challenging financial tasks including accounting fraud detection,
earnings forecasting, and industry prediction. EDINET-Bench is constructed by
downloading annual reports from the past 10 years from Japan's Electronic
Disclosure for Investors' NETwork (EDINET) and automatically assigning labels
corresponding to each evaluation task. Our experiments reveal that even
state-of-the-art LLMs struggle, performing only slightly better than logistic
regression in binary classification for fraud detection and earnings
forecasting. These results highlight significant challenges in applying LLMs to
real-world financial applications and underscore the need for domain-specific
adaptation. Our dataset, benchmark construction code, and evaluation code is
publicly available to facilitate future research in finance with LLMs.

</details>

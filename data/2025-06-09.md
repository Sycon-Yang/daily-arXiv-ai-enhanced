<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [EvidenceOutcomes: a Dataset of Clinical Trial Publications with Clinically Meaningful Outcomes](https://arxiv.org/abs/2506.05380)
*Yiliang Zhou,Abigail M. Newbury,Gongbo Zhang,Betina Ross Idnay,Hao Liu,Chunhua Weng,Yifan Peng*

Main category: cs.CL

TL;DR: Developed a new annotated corpus named EvidenceOutcomes for extracting clinically meaningful outcomes from biomedical literature.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks neglect or oversimplify outcomes which are complex and important elements in evidence extraction.

Method: Created annotation guidelines through clinician and NLP expert collaboration, then had three annotators label 500 PubMed abstracts and 140 abstracts from EBM-NLP corpus.

Result: Achieved high-quality annotations with an inter-rater agreement of 0.76 and improved performance using a fine-tuned PubMedBERT model.

Conclusion: EvidenceOutcomes can be used as a benchmark for developing machine learning algorithms to extract clinically meaningful outcomes.

Abstract: The fundamental process of evidence extraction and synthesis in
evidence-based medicine involves extracting PICO (Population, Intervention,
Comparison, and Outcome) elements from biomedical literature. However,
Outcomes, being the most complex elements, are often neglected or
oversimplified in existing benchmarks. To address this issue, we present
EvidenceOutcomes, a novel, large, annotated corpus of clinically meaningful
outcomes extracted from biomedical literature. We first developed a robust
annotation guideline for extracting clinically meaningful outcomes from text
through iteration and discussion with clinicians and Natural Language
Processing experts. Then, three independent annotators annotated the Results
and Conclusions sections of a randomly selected sample of 500 PubMed abstracts
and 140 PubMed abstracts from the existing EBM-NLP corpus. This resulted in
EvidenceOutcomes with high-quality annotations of an inter-rater agreement of
0.76. Additionally, our fine-tuned PubMedBERT model, applied to these 500
PubMed abstracts, achieved an F1-score of 0.69 at the entity level and 0.76 at
the token level on the subset of 140 PubMed abstracts from the EBM-NLP corpus.
EvidenceOutcomes can serve as a shared benchmark to develop and test future
machine learning algorithms to extract clinically meaningful outcomes from
biomedical abstracts.

</details>


### [2] [LLMs Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models](https://arxiv.org/abs/2506.05385)
*Xinxin Li,Huiyao Chen,Chengjun Liu,Jing Li,Meishan Zhang,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: This work introduces two mechanisms (retrieval-augmented generation and self-correction) to enhance large language models for semantic role labeling tasks, achieving state-of-the-art performance on multiple benchmarks in both Chinese and English.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between generative decoder-based large language models and state-of-the-art encoder-decoder models in semantic role labeling.

Method: Equipping large language models with retrieval-augmented generation and self-correction mechanisms.

Result: Achieved state-of-the-art performance in semantic role labeling on three widely-used benchmarks in both Chinese and English.

Conclusion: This is the first successful application of large language models surpassing encoder-decoder approaches in semantic role labeling.

Abstract: Semantic role labeling (SRL) is a crucial task of natural language processing
(NLP). Although generative decoder-based large language models (LLMs) have
achieved remarkable success across various NLP tasks, they still lag behind
state-of-the-art encoder-decoder (BERT-like) models in SRL. In this work, we
seek to bridge this gap by equipping LLMs for SRL with two mechanisms: (a)
retrieval-augmented generation and (b) self-correction. The first mechanism
enables LLMs to leverage external linguistic knowledge such as predicate and
argument structure descriptions, while the second allows LLMs to identify and
correct inconsistent SRL outputs. We conduct extensive experiments on three
widely-used benchmarks of SRL (CPB1.0, CoNLL-2009, and CoNLL-2012). Results
demonstrate that our method achieves state-of-the-art performance in both
Chinese and English, marking the first successful application of LLMs to
surpass encoder-decoder approaches in SRL.

</details>


### [3] [Beyond RAG: Reinforced Reasoning Augmented Generation for Clinical Notes](https://arxiv.org/abs/2506.05386)
*Lo Pang-Yun Ting,Chengshuai Zhao,Yu-Hua Zeng,Yuan Jee Lim,Kun-Ta Chuang*

Main category: cs.CL

TL;DR: Introducing R2AG, a reinforced retriever that uses medical knowledge graphs to improve long-form clinical note generation from pre-admission data.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods struggle with generating long-form notes from limited patient information.

Method: R2AG uses reinforcement learning to retrieve reasoning paths from a medical knowledge graph and employs Group-Based Retriever Optimization (GRO) to enhance retrieval quality.

Result: Comprehensive experiments on the MIMIC-IV-Note dataset demonstrate the effectiveness of R2AG.

Conclusion: R2AG surpasses baselines in clinical efficacy and natural language generation metrics.

Abstract: Clinical note generation aims to automatically produce free-text summaries of
a patient's condition and diagnostic process, with discharge instructions being
a representative long-form example. While recent large language model
(LLM)-based methods pre-trained on general clinical corpora show promise in
clinical text generation, they fall short in producing long-form notes from
limited patient information. In this paper, we propose R2AG, the first
reinforced retriever for long-form discharge instruction generation based on
pre-admission data. R2AG is trained with reinforcement learning to retrieve
reasoning paths from a medical knowledge graph, providing explicit semantic
guidance to the LLM. To bridge the information gap, we propose Group-Based
Retriever Optimization (GRO) which improves retrieval quality with
group-relative rewards, encouraging reasoning leaps for deeper inference by the
LLM. Comprehensive experiments on the MIMIC-IV-Note dataset show that R2AG
outperforms baselines in both clinical efficacy and natural language generation
metrics. Further analysis reveals that R2AG fills semantic gaps in sparse input
scenarios, and retrieved reasoning paths help LLMs avoid clinical
misinterpretation by focusing on key evidence and following coherent reasoning.

</details>


### [4] [Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs](https://arxiv.org/abs/2506.05387)
*Jaydip Sen,Saptarshi Sengupta. Subhasis Dasgupta*

Main category: cs.CL

TL;DR: Explores improvements in decoding strategies for large language models, proposing ASTS to enhance text generation fluency, diversity, and coherence.


<details>
  <summary>Details</summary>
Motivation: Traditional decoding methods face challenges in balancing fluency, diversity, and coherence in text generation.

Method: Introduces ASTS which incorporates dynamic entropy thresholding, multi-objective scoring, and reward-penalty adjustments to enhance LTS.

Result: ASTS shows better performance than existing methods in benchmarks like story generation and abstractive summarization by reducing repetition, enhancing semantic alignment, and improving fluency.

Conclusion: Adaptive Semantic-Aware Typicality Sampling (ASTS) improves text generation in terms of fluency, diversity, and coherence.

Abstract: This chapter explores advancements in decoding strategies for large language
models (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)
algorithm. Traditional decoding methods, such as top-k and nucleus sampling,
often struggle to balance fluency, diversity, and coherence in text generation.
To address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)
is proposed as an improved version of LTS, incorporating dynamic entropy
thresholding, multi-objective scoring, and reward-penalty adjustments. ASTS
ensures contextually coherent and diverse text generation while maintaining
computational efficiency. Its performance is evaluated across multiple
benchmarks, including story generation and abstractive summarization, using
metrics such as perplexity, MAUVE, and diversity scores. Experimental results
demonstrate that ASTS outperforms existing sampling techniques by reducing
repetition, enhancing semantic alignment, and improving fluency.

</details>


### [5] [taz2024full: Analysing German Newspapers for Gender Bias and Discrimination across Decades](https://arxiv.org/abs/2506.05388)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Christian Heumann,Stephanie Thiemichen*

Main category: cs.CL

TL;DR: Presenting taz2024full, the largest open-access German newspaper article corpus with over 1.8 million texts from 1980-2024, demonstrating its utility by analyzing gender representation shifts towards more balanced coverage.


<details>
  <summary>Details</summary>
Motivation: The need for large-scale open-access corpora for German to study linguistic trends and societal issues like gender bias.

Method: Analysis of gender representation across four decades of reporting using a scalable, structured analysis pipeline.

Result: Consistent overrepresentation of men in earlier years with a gradual shift toward more balanced coverage recently.

Conclusion: taz2024full supports diverse applications and promotes inclusive and reproducible research in German NLP.

Abstract: Open-access corpora are essential for advancing natural language processing
(NLP) and computational social science (CSS). However, large-scale resources
for German remain limited, restricting research on linguistic trends and
societal issues such as gender bias. We present taz2024full, the largest
publicly available corpus of German newspaper articles to date, comprising over
1.8 million texts from taz, spanning 1980 to 2024.
  As a demonstration of the corpus's utility for bias and discrimination
research, we analyse gender representation across four decades of reporting. We
find a consistent overrepresentation of men, but also a gradual shift toward
more balanced coverage in recent years. Using a scalable, structured analysis
pipeline, we provide a foundation for studying actor mentions, sentiment, and
linguistic framing in German journalistic texts.
  The corpus supports a wide range of applications, from diachronic language
analysis to critical media studies, and is freely available to foster inclusive
and reproducible research in German-language NLP.

</details>


### [6] [Understanding Gender Bias in AI-Generated Product Descriptions](https://arxiv.org/abs/2506.05390)
*Markelle Kelly,Mohammad Tahaei,Padhraic Smyth,Lauren Wilcox*

Main category: cs.CL

TL;DR: This paper explores gender bias in e-commerce-related large language models, identifying new forms of algorithmic bias through taxonomic categories.


<details>
  <summary>Details</summary>
Motivation: To investigate potential novel forms of algorithmic bias and harm specifically in the context of e-commerce product description generation.

Method: Developing data-driven taxonomic categories of gender bias and analyzing these biases in two models (GPT-3.5 and an e-commerce-specific LLM) using quantitative analysis.

Result: Identified unique gender biases such as clothing size assumptions, stereotypical product feature advertisements, and differences in persuasive language usage. These biases were found to be common in both examined models.

Conclusion: The study reveals new dimensions of gender bias in e-commerce contexts, contributing to understanding AI harms like exclusionary norms, stereotyping, and performance disparities.

Abstract: While gender bias in large language models (LLMs) has been extensively
studied in many domains, uses of LLMs in e-commerce remain largely unexamined
and may reveal novel forms of algorithmic bias and harm. Our work investigates
this space, developing data-driven taxonomic categories of gender bias in the
context of product description generation, which we situate with respect to
existing general purpose harms taxonomies. We illustrate how AI-generated
product descriptions can uniquely surface gender biases in ways that require
specialized detection and mitigation approaches. Further, we quantitatively
analyze issues corresponding to our taxonomic categories in two models used for
this task -- GPT-3.5 and an e-commerce-specific LLM -- demonstrating that these
forms of bias commonly occur in practice. Our results illuminate unique,
under-explored dimensions of gender bias, such as assumptions about clothing
size, stereotypical bias in which features of a product are advertised, and
differences in the use of persuasive language. These insights contribute to our
understanding of three types of AI harms identified by current frameworks:
exclusionary norms, stereotyping, and performance disparities, particularly for
the context of e-commerce.

</details>


### [7] [Are Large Language Models Good Temporal Graph Learners?](https://arxiv.org/abs/2506.05393)
*Shenyang Huang,Ali Parviz,Emma Kondrup,Zachary Yang,Zifeng Ding,Michael Bronstein,Reihaneh Rabbany,Guillaume Rabusseau*

Main category: cs.CL

TL;DR: TGTalker是一种新颖的用于大型语言模型的时态图学习框架，能够有效处理真实世界中的时态图并生成预测解释。


<details>
  <summary>Details</summary>
Motivation: 研究动态图上大型语言模型的应用，这是一个未被充分探索的领域。

Method: TGTalker利用时态图的时间偏置提取结构信息，并转换成自然语言供大型语言模型使用，同时利用时间邻居作为额外信息进行预测。

Result: 在五个真实世界的数据集上，TGTalker的表现优于现有的时态图神经网络模型。

Conclusion: TGTalker展示了在时态图链接预测任务上的强大能力，并且提供了预测的文本解释，这为时态链接预测的可解释性和可理解性开辟了新的方向。

Abstract: Large Language Models (LLMs) have recently driven significant advancements in
Natural Language Processing and various other applications. While a broad range
of literature has explored the graph-reasoning capabilities of LLMs, including
their use of predictors on graphs, the application of LLMs to dynamic graphs --
real world evolving networks -- remains relatively unexplored. Recent work
studies synthetic temporal graphs generated by random graph models, but
applying LLMs to real-world temporal graphs remains an open question. To
address this gap, we introduce Temporal Graph Talker (TGTalker), a novel
temporal graph learning framework designed for LLMs. TGTalker utilizes the
recency bias in temporal graphs to extract relevant structural information,
converted to natural language for LLMs, while leveraging temporal neighbors as
additional information for prediction. TGTalker demonstrates competitive link
prediction capabilities compared to existing Temporal Graph Neural Network
(TGNN) models. Across five real-world networks, TGTalker performs competitively
with state-of-the-art temporal graph methods while consistently outperforming
popular models such as TGN and HTGN. Furthermore, TGTalker generates textual
explanations for each prediction, thus opening up exciting new directions in
explainability and interpretability for temporal link prediction. The code is
publicly available at https://github.com/shenyangHuang/TGTalker.

</details>


### [8] [Auto Review: Second Stage Error Detection for Highly Accurate Information Extraction from Phone Conversations](https://arxiv.org/abs/2506.05400)
*Ayesha Qamar,Arushi Raghuvanshi,Conal Sathi,Youngseo Son*

Main category: cs.CL

TL;DR: Automating benefit verification phone calls can save time and help patients receive treatment faster. However, the system suffers from performance bottlenecks due to ASR issues and domain-specific jargon. The proposed second-stage postprocessing pipeline improves accuracy and enhances the efficiency of Auto Review.


<details>
  <summary>Details</summary>
Motivation: Automating benefit verification phone calls can save time and help patients receive treatment faster.

Method: Introduce Auto Review, a two-stage system with a post-call review phase for potentially noisy fields, and a second-stage postprocessing pipeline for accurate information extraction. Use multiple ASR alternatives and a pseudo-labeling approach that does not require manually corrected transcripts.

Result: Experiments demonstrate substantial improvements in the quality of corrected call transcripts, thereby enhancing the efficiency of Auto Review.

Conclusion: The proposed system can significantly reduce manual effort while maintaining a high bar for accuracy.

Abstract: Automating benefit verification phone calls saves time in healthcare and
helps patients receive treatment faster. It is critical to obtain highly
accurate information in these phone calls, as it can affect a patient's
healthcare journey. Given the noise in phone call transcripts, we have a
two-stage system that involves a post-call review phase for potentially noisy
fields, where human reviewers manually verify the extracted
data$\unicode{x2013}$a labor-intensive task. To automate this stage, we
introduce Auto Review, which significantly reduces manual effort while
maintaining a high bar for accuracy. This system, being highly reliant on call
transcripts, suffers a performance bottleneck due to automatic speech
recognition (ASR) issues. This problem is further exacerbated by the use of
domain-specific jargon in the calls. In this work, we propose a second-stage
postprocessing pipeline for accurate information extraction. We improve
accuracy by using multiple ASR alternatives and a pseudo-labeling approach that
does not require manually corrected transcripts. Experiments with
general-purpose large language models and feature-based model pipelines
demonstrate substantial improvements in the quality of corrected call
transcripts, thereby enhancing the efficiency of Auto Review.

</details>


### [9] [Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs](https://arxiv.org/abs/2506.05410)
*Wanyun Cui,Mingwei Xu*

Main category: cs.CL

TL;DR: KV cache compression is crucial for efficient long-context modeling in large language models. A new method called AsymKV improves performance by addressing key-value asymmetry.


<details>
  <summary>Details</summary>
Motivation: Efficient long-context modeling in large language models is challenging due to the quadratic complexity of attention mechanisms.

Method: A training-free compression framework called AsymKV combines homogeneity-based key merging with lossless value compression.

Result: AsymKV outperforms existing long-context methods across various tasks and base models.

Conclusion: The proposed AsymKV framework addresses a critical limitation in existing compression methods and improves performance on long-context tasks.

Abstract: Recent advances in Large Language Models (LLMs) have highlighted the critical
importance of extending context length, yet the quadratic complexity of
attention mechanisms poses significant challenges for efficient long-context
modeling. KV cache compression has emerged as a key approach to address this
challenge. Through extensive empirical analysis, we reveal a fundamental yet
previously overlooked asymmetry in KV caches: while adjacent keys receive
similar attention weights (local homogeneity), adjacent values demonstrate
distinct heterogeneous distributions. This key-value asymmetry reveals a
critical limitation in existing compression methods that treat keys and values
uniformly. To address the limitation, we propose a training-free compression
framework (AsymKV) that combines homogeneity-based key merging with a
mathematically proven lossless value compression. Extensive experiments
demonstrate that AsymKV consistently outperforms existing long-context methods
across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV
achieves an average score of 43.95 on LongBench, surpassing SOTA methods like
H$_2$O (38.89) by a large margin.

</details>


### [10] [SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs](https://arxiv.org/abs/2506.05413)
*Patrik Czakó,Gábor Kertész,Sándor Szénási*

Main category: cs.CL

TL;DR: SmoothRot是一种新的后训练量化技术，用于提高大型语言模型在4位量化中的效率。通过结合通道缩放和Hadamard变换来解决激活离群值的问题，显著提高了量化精度。实验表明，在多种LLMs上，SmoothRot减少了量化模型与FP16模型之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的4位量化技术在处理大规模语言模型时面临激活离群值的问题，这影响了量化后的模型性能。SmoothRot旨在解决这一问题以提升量化效率。

Method: SmoothRot集成通道-wise缩放和Hadamard变换，将极端的激活离群值转换为适合量化的激活值。

Result: 在LLaMA2 7B, LLaMA3.1 8B, 和 Mistral 7B上实验显示，SmoothRot减少了量化模型与FP16模型之间约10-30%的性能差距，并且没有引入额外的推理延迟。

Conclusion: SmoothRot提供了一种有效的方法来提高4位量化大型语言模型的性能，同时保持高效的推理速度。

Abstract: We present SmoothRot, a novel post-training quantization technique to enhance
the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot
addresses the critical challenge of massive activation outliers, by integrating
channel-wise scaling with Hadamard transformations. Our technique effectively
transforms extreme outliers into quantization-friendly activations,
significantly improving quantization accuracy. Experiments conducted on popular
LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot
consistently reduces the performance gap between quantized and FP16 models by
approximately 10-30\% across language generation and zero-shot reasoning tasks,
without introducing additional inference latency. Code is available at
https://github.com/czakop/smoothrot.

</details>


### [11] [Automatically Detecting Amusing Games in Wordle](https://arxiv.org/abs/2506.05415)
*Ronaldo Luo,Gary Liang,Cindy Liu,Adam Kabbara,Minahil Bakhtawar,Kina Kim,Michael Guerzhoy*

Main category: cs.CL

TL;DR: Predicting user amusement in Wordle games based on Reddit reactions.


<details>
  <summary>Details</summary>
Motivation: Exploring the computational prediction of user amusement in Wordle games.

Method: Scraping Reddit reactions, classifying them with GPT-3.5, and extracting predictive features from Wordle games.

Result: User amusement can be predicted computationally, with certain game features contributing to amusement.

Conclusion: Wordle games have a measurable aspect of creativity related to humor that influences user amusement.

Abstract: We explore automatically predicting which Wordle games Reddit users find
amusing.
  We scrape approximately 80k reactions by Reddit users to Wordle games from
Reddit, classify the reactions as expressing amusement or not using OpenAI's
GPT-3.5 using few-shot prompting, and verify that GPT-3.5's labels roughly
correspond to human labels.
  We then extract features from Wordle games that can predict user amusement.
We demonstrate that the features indeed provide a (weak) signal that predicts
user amusement as predicted by GPT-3.5.
  Our results indicate that user amusement at Wordle games can be predicted
computationally to some extent. We explore which features of the game
contribute to user amusement.
  We find that user amusement is predictable, indicating a measurable aspect of
creativity infused into Wordle games through humor.

</details>


### [12] [MLLM-CL: Continual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2506.05453)
*Hongbo Zhao,Fei Zhu,Rundong Wang,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.CL

TL;DR: This paper introduces MLLM-CL, a new benchmark for evaluating Multimodal Large Language Models' (MLLMs) capacity to adapt to new knowledge and skills over time. It also proposes a method combining parameter isolation and an MLLM-based routing mechanism to minimize forgetting while integrating new information.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks and methods for continual learning in MLLMs have critical limitations when it comes to adapting to dynamic real-world scenarios that require continuous integration of new knowledge and skills.

Method: The method involves preventing catastrophic interference through parameter isolation and implementing an MLLM-based routing mechanism.

Result: Extensive experiments show that the proposed approach can integrate domain-specific knowledge and functional abilities with minimal forgetting, outperforming existing methods.

Conclusion: The introduced MLLM-CL benchmark and method provide a significant advancement in enabling MLLMs to adapt to dynamic real-world scenarios.

Abstract: Recent Multimodal Large Language Models (MLLMs) excel in vision-language
understanding but face challenges in adapting to dynamic real-world scenarios
that require continuous integration of new knowledge and skills. While
continual learning (CL) offers a potential solution, existing benchmarks and
methods suffer from critical limitations. In this paper, we introduce MLLM-CL,
a novel benchmark encompassing domain and ability continual learning, where the
former focuses on independently and identically distributed (IID) evaluation
across evolving mainstream domains, whereas the latter evaluates on non-IID
scenarios with emerging model ability. Methodologically, we propose preventing
catastrophic interference through parameter isolation, along with an MLLM-based
routing mechanism. Extensive experiments demonstrate that our approach can
integrate domain-specific knowledge and functional abilities with minimal
forgetting, significantly outperforming existing methods.

</details>


### [13] [Multidimensional Analysis of Specific Language Impairment Using Unsupervised Learning Through PCA and Clustering](https://arxiv.org/abs/2506.05498)
*Niruthiha Selvanayagam*

Main category: cs.CL

TL;DR: This study uses unsupervised machine learning to analyze narrative samples from over a thousand children, identifying two main clusters related to language production and SLI prevalence, challenging traditional diagnostic approaches.


<details>
  <summary>Details</summary>
Motivation: To provide insights for early identification and targeted interventions by uncovering natural language development trajectories in children with and without Specific Language Impairment (SLI).

Method: Unsupervised machine learning techniques, including Principal Component Analysis (PCA) and clustering, were used to evaluate 64 linguistic features from narrative samples of 1,163 children aged 4-16 years across three corpora.

Result: Two primary clusters emerged: (1) high language production with low SLI prevalence, and (2) limited production but higher syntactic complexity with higher SLI prevalence. Boundary cases showed intermediate traits.

Conclusion: The findings suggest that SLI is mainly characterized by reduced production capacity rather than syntactic complexity deficits, challenging categorical diagnostic frameworks and highlighting the potential of unsupervised learning techniques for refining diagnostic criteria and intervention strategies.

Abstract: Specific Language Impairment (SLI) affects approximately 7 percent of
children, presenting as isolated language deficits despite normal cognitive
abilities, sensory systems, and supportive environments. Traditional diagnostic
approaches often rely on standardized assessments, which may overlook subtle
developmental patterns. This study aims to identify natural language
development trajectories in children with and without SLI using unsupervised
machine learning techniques, providing insights for early identification and
targeted interventions. Narrative samples from 1,163 children aged 4-16 years
across three corpora (Conti-Ramsden 4, ENNI, and Gillam) were analyzed using
Principal Component Analysis (PCA) and clustering. A total of 64 linguistic
features were evaluated to uncover developmental trajectories and distinguish
linguistic profiles. Two primary clusters emerged: (1) high language production
with low SLI prevalence, and (2) limited production but higher syntactic
complexity with higher SLI prevalence. Additionally, boundary cases exhibited
intermediate traits, supporting a continuum model of language abilities.
Findings suggest SLI manifests primarily through reduced production capacity
rather than syntactic complexity deficits. The results challenge categorical
diagnostic frameworks and highlight the potential of unsupervised learning
techniques for refining diagnostic criteria and intervention strategies.

</details>


### [14] [Improving LLMs with a knowledge from databases](https://arxiv.org/abs/2506.05560)
*Petr Máša*

Main category: cs.CL

TL;DR: This paper discusses enhancing large language models by integrating interpretable machine learning methods, specifically enhanced association rules, to improve answers based on datasets. It proposes a method that creates a ruleset from knowledge patterns, converts these rules into text, and integrates them into RAG for LLMs. Results show significant improvements in answering questions based on datasets compared to ChatGPT.


<details>
  <summary>Details</summary>
Motivation: To address the lack of control over commands created in LLMs and explore a new method that improves answers based on datasets through interpretable ML methods.

Method: Proposes a method that generates a ruleset based on knowledge patterns, converts these rules into text via a rule-to-text converter, and integrates the result into RAG for LLMs.

Result: Significant improvement in answering questions based on datasets was observed when comparing the proposed method with ChatGPT, even when using agents.

Conclusion: The method shows promise in improving LLMs' performance and can be further enhanced in future work.

Abstract: Large language models (LLMs) are achieving significant progress almost every
moment now. Many advanced techniques have been introduced and widely accepted,
like retrieval-augmentation generation (RAG), agents, and tools. Tools can
query the database to answer questions from structured data files or perform
groupings or other statistics. This unlocks huge opportunities, such as it can
answer any question, but also poses threats, such as safety, because there is
no control over the commands that are created. We would like to discuss whether
we can create a new method that improves answers based on dataset/database via
some interpretable ML methods, namely enhanced association rules. The advantage
would be if the method can be also used in some safe technique like RAG.
Association rules have a sound history. Since the introduction of CN2 and
aproiri, many enhancements have been made. In parallel, enhanced association
rules have been introduced and evolved over the last 40 years. The general
problem is typically that there are too many rules. There are some techniques
for handling it, but when LLM emerged, it turned out to be the best use case
for the RAG technique for LLMs. We proposed a method that generates a ruleset
based on defined knowledge patterns, then converts rules into text form via a
rule-to-text converter, and includes the result as an RAG into LLM. We compared
this method with ChatGPT (even with using agents) and we have discovered a
significant improvement in answering questions based on the dataset. We have
also tried several strategies how much rules to generate. We found this
improvement interesting. Moreover, it can also be improved in many ways as
future work, like incorporating other patterns, the use of rule mining as an
agent, and many others.

</details>


### [15] [Combating Misinformation in the Arab World: Challenges & Opportunities](https://arxiv.org/abs/2506.05582)
*Azza Abouzied,Firoj Alam,Raian Ali,Paolo Papotti*

Main category: cs.CL

TL;DR: 研究阿拉伯世界在对抗错误信息方面的独特挑战，包括检测、追踪、缓解及社区参与，建议通过与草根组织合作、理解文化等方法构建更坚韧的信息生态。


<details>
  <summary>Details</summary>
Motivation: 探讨阿拉伯地区因地缘政治不稳定、语言多样性及文化细微差别而面临的独特脆弱性，以及如何应对全球范围内的错误信息和虚假信息带来的重大风险。

Method: 通过四个关键方面来对抗错误信息：检测、追踪、缓解以及社区参与。

Result: 揭示与草根事实核查组织合作、理解文化规范、促进社会纠正、创建强大的协作信息网络可以为阿拉伯世界的更健壮的信息生态系统创造机会。

Conclusion: 强调建立一个更具抗风险能力的信息生态系统的重要性，并提出通过多方面的策略来应对错误信息带来的挑战。

Abstract: Misinformation and disinformation pose significant risks globally, with the
Arab region facing unique vulnerabilities due to geopolitical instabilities,
linguistic diversity, and cultural nuances. We explore these challenges through
the key facets of combating misinformation: detection, tracking, mitigation and
community-engagement. We shed light on how connecting with grass-roots
fact-checking organizations, understanding cultural norms, promoting social
correction, and creating strong collaborative information networks can create
opportunities for a more resilient information ecosystem in the Arab world.

</details>


### [16] [UTSA-NLP at ArchEHR-QA 2025: Improving EHR Question Answering via Self-Consistency Prompting](https://arxiv.org/abs/2506.05589)
*Sara Shields-Menard,Zach Reimers,Joshua Gardner,David Perry,Anthony Rios*

Main category: cs.CL

TL;DR: Our system uses two-step approach with large language models to answer clinical questions from EHRs. Few-shot prompting, self-consistency and thresholding improve sentence classification.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient system for answering clinical questions using EHRs.

Method: Two-step approach with large language models, including finding relevant sentences and generating responses based on them. Few-shot prompting, self-consistency, and thresholding are used to improve sentence classification.

Result: Smaller 8B model performs better than larger 70B model for identifying relevant information. Accurate sentence selection is critical for generating high-quality responses.

Conclusion: Accurate sentence selection is important and self-consistency with thresholding helps make decisions more reliable.

Abstract: We describe our system for the ArchEHR-QA Shared Task on answering clinical
questions using electronic health records (EHRs). Our approach uses large
language models in two steps: first, to find sentences in the EHR relevant to a
clinician's question, and second, to generate a short, citation-supported
response based on those sentences. We use few-shot prompting, self-consistency,
and thresholding to improve the sentence classification step to decide which
sentences are essential. We compare several models and find that a smaller 8B
model performs better than a larger 70B model for identifying relevant
information. Our results show that accurate sentence selection is critical for
generating high-quality responses and that self-consistency with thresholding
helps make these decisions more reliable.

</details>


### [17] [SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs](https://arxiv.org/abs/2506.05598)
*Michael J Ryan,Omar Shaikh,Aditri Bhagirath,Daniel Frees,William Held,Diyi Yang*

Main category: cs.CL

TL;DR: Introduce SynthesizeMe, an approach to induce synthetic user personas from user interactions for personalized reward modeling.


<details>
  <summary>Details</summary>
Motivation: Encouraging adapting models to diverse user preferences without relying on additional identity information.

Method: Generate and verify reasoning to explain user preferences, induce synthetic user personas from that reasoning, and filter informative prior user interactions to build personalized prompts.

Result: Improves personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena and achieves top performance on PersonalRewardBench.

Conclusion: SynthesizeMe is effective in inducing synthetic user personas from user interactions for personalized reward modeling.

Abstract: Recent calls for pluralistic alignment of Large Language Models (LLMs)
encourage adapting models to diverse user preferences. However, most prior work
on personalized reward models heavily rely on additional identity information,
such as demographic details or a predefined set of preference categories. To
this end, we introduce SynthesizeMe, an approach to inducing synthetic user
personas from user interactions for personalized reward modeling. SynthesizeMe
first generates and verifies reasoning to explain user preferences, then
induces synthetic user personas from that reasoning, and finally filters to
informative prior user interactions in order to build personalized prompts for
a particular user. We show that using SynthesizeMe induced prompts improves
personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining
SynthesizeMe derived prompts with a reward model achieves top performance on
PersonalRewardBench: a new curation of user-stratified interactions with
chatbots collected from 854 users of Chatbot Arena and PRISM.

</details>


### [18] [OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation](https://arxiv.org/abs/2506.05606)
*Ziyi Wang,Yuxuan Lu,Wenbo Li,Amirali Amini,Bo Sun,Yakov Bart,Weimin Lyu,Jiri Gesi,Tian Wang,Jing Huang,Yu Su,Upol Ehsan,Malihe Alikhani,Toby Jia-Jun Li,Lydia Chilton,Dakuo Wang*

Main category: cs.CL

TL;DR: This paper introduces OPERA, a new dataset containing observation, persona, rationale, and action data from real users during online shopping. It evaluates how well large language models can predict a user's next action.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality dataset for evaluating LLMs' ability to mimic real user behaviors.

Method: Developed an online questionnaire and a custom browser plugin to collect data on user personas, observations, actions, and rationales.

Result: OPERA is the first public dataset capturing comprehensive information about user behaviors and thoughts during online shopping.

Conclusion: This work provides a benchmark for assessing LLMs' capability to simulate specific user behaviors and sets the stage for creating personalized digital twins.

Abstract: Can large language models (LLMs) accurately simulate the next web action of a
specific user? While LLMs have shown promising capabilities in generating
``believable'' human behaviors, evaluating their ability to mimic real user
behaviors remains an open challenge, largely due to the lack of high-quality,
publicly available datasets that capture both the observable actions and the
internal reasoning of an actual human user. To address this gap, we introduce
OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected
from real human participants during online shopping sessions. OPERA is the
first public dataset that comprehensively captures: user personas, browser
observations, fine-grained web actions, and self-reported just-in-time
rationales. We developed both an online questionnaire and a custom browser
plugin to gather this dataset with high fidelity. Using OPERA, we establish the
first benchmark to evaluate how well current LLMs can predict a specific user's
next action and rationale with a given persona and <observation, action,
rationale> history. This dataset lays the groundwork for future research into
LLM agents that aim to act as personalized digital twins for human.

</details>


### [19] [Mitigating Confounding in Speech-Based Dementia Detection through Weight Masking](https://arxiv.org/abs/2506.05610)
*Zhecheng Sheng,Xiruo Ding,Brian Hur,Changye Li,Trevor Cohen,Serguei Pakhomov*

Main category: cs.CL

TL;DR: This work proposes methods to address gender confounding in dementia detection using transformer models, showing that disrupting gender-related weights creates a deconfounded classifier but with slightly reduced performance.


<details>
  <summary>Details</summary>
Motivation: To address gender confounding in dementia detection when using pre-trained neural language models fine-tuned on AD transcripts.

Method: The Extended Confounding Filter and the Dual Filter methods were proposed to isolate and ablate weights associated with gender.

Result: Disrupting gender-related weights results in a deconfounded dementia classifier with slightly reduced dementia detection performance.

Conclusion: Transformer models tend to overfit to training data distributions, and disrupting gender-related weights can result in a deconfounded dementia classifier with a slight reduction in performance.

Abstract: Deep transformer models have been used to detect linguistic anomalies in
patient transcripts for early Alzheimer's disease (AD) screening. While
pre-trained neural language models (LMs) fine-tuned on AD transcripts perform
well, little research has explored the effects of the gender of the speakers
represented by these transcripts. This work addresses gender confounding in
dementia detection and proposes two methods: the $\textit{Extended Confounding
Filter}$ and the $\textit{Dual Filter}$, which isolate and ablate weights
associated with gender. We evaluate these methods on dementia datasets with
first-person narratives from patients with cognitive impairment and healthy
controls. Our results show transformer models tend to overfit to training data
distributions. Disrupting gender-related weights results in a deconfounded
dementia classifier, with the trade-off of slightly reduced dementia detection
performance.

</details>


### [20] [Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs](https://arxiv.org/abs/2506.05629)
*Ananth Muppidi,Abhilash Nandy,Sambaran Bandyopadhyay*

Main category: cs.CL

TL;DR: Fine-tune large language models efficiently with a new soft prompting technique called ID-SPAM.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs for specific domains is costly and difficult.

Method: Introduce a novel soft prompting method with input-dependent generation and self-attention.

Result: ID-SPAM outperforms other methods across various tasks and enhances zero-shot domain transfer.

Conclusion: Parameter-efficient fine-tuning with soft prompting is a promising direction for adapting LLMs.

Abstract: The performance of large language models in domain-specific tasks
necessitates fine-tuning, which is computationally expensive and technically
challenging. This paper focuses on parameter-efficient fine-tuning using soft
prompting, a promising approach that adapts pre-trained models to downstream
tasks by learning a small set of parameters. We propose a novel Input Dependent
Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that
generates soft prompts based on the input tokens and attends different tokens
with varying importance. Our method is simple and efficient, keeping the number
of trainable parameters small. We show the merits of the proposed approach
compared to state-of-the-art techniques on various tasks and show the improved
zero shot domain transfer capability.

</details>


### [21] [IYKYK: Using language models to decode extremist cryptolects](https://arxiv.org/abs/2506.05635)
*Christine de Kock,Arij Riabi,Zeerak Talat,Michael Sejr Schlichtkrull,Pranava Madhyastha,Ed Hovy*

Main category: cs.CL

TL;DR: Investigate the ability of current language technologies to detect and interpret the cryptolects of two online extremist platforms.


<details>
  <summary>Details</summary>
Motivation: Extremist groups use complex in-group language to exclude or mislead outsiders.

Method: Evaluating eight models across six tasks.

Result: General purpose LLMs cannot consistently detect or decode extremist language, but performance can be significantly improved by domain adaptation and specialised prompting techniques.

Conclusion: These results provide important insights to inform the development and deployment of automated moderation technologies.

Abstract: Extremist groups develop complex in-group language, also referred to as
cryptolects, to exclude or mislead outsiders. We investigate the ability of
current language technologies to detect and interpret the cryptolects of two
online extremist platforms. Evaluating eight models across six tasks, our
results indicate that general purpose LLMs cannot consistently detect or decode
extremist language. However, performance can be significantly improved by
domain adaptation and specialised prompting techniques. These results provide
important insights to inform the development and deployment of automated
moderation technologies. We further develop and release novel labelled and
unlabelled datasets, including 19.4M posts from extremist platforms and
lexicons validated by human experts.

</details>


### [22] [A Fictional Q&A Dataset for Studying Memorization and Knowledge Acquisition](https://arxiv.org/abs/2506.05639)
*John Kirchenbauer,Janny Mongkolsupawan,Yuxin Wen,Tom Goldstein,Daphne Ippolito*

Main category: cs.CL

TL;DR: This paper introduces a new dataset to study how language models memorize facts and verbatim sequences.


<details>
  <summary>Details</summary>
Motivation: To understand how language models memorize facts seen during training.

Method: Synthetically-generated, webtext-like documents about fictional events and question-answer pairs about the events were used.

Result: Training experiments showed that synthetic data about fictional events can be effective in distinguishing different forms of memorization.

Conclusion: The study provides a new dataset to investigate the dual processes of fact memorization and verbatim sequence memorization in language models.

Abstract: When language models are trained on textual data, they acquire both knowledge
about the structure of language as well as knowledge of facts about the world.
At inference time, their knowledge of facts can be leveraged to solve
interesting problems and perform useful knowledge work for users. It is well
known that language models can verbatim memorize long sequences from their
training data. However, it is much less well understood how language models
memorize facts seen during training. In this work, we propose a new dataset to
specifically empower researchers to study the dual processes of fact
memorization and verbatim sequence memorization. The dataset consists of
synthetically-generated, webtext-like documents about fictional events, as well
as question-answer pairs about the events. We conduct training experiments
showing how synthetic data about fictional events can be effective in teasing
apart different forms of memorization. We also document the challenges in
effectively building realistic, fictional synthetic data.

</details>


### [23] [Can LLMs Express Personality Across Cultures? Introducing CulturalPersonas for Evaluating Trait Alignment](https://arxiv.org/abs/2506.05670)
*Priyanka Dey,Yugal Khanter,Aayush Bothra,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: CulturalPersonas is a new benchmark for evaluating LLMs' personality expression in culturally nuanced contexts, improving alignment with human personality distributions and generating culturally coherent outputs.


<details>
  <summary>Details</summary>
Motivation: As large language models (LLMs) play a key role in various interactive applications, the capability of expressing personality in a culturally appropriate manner becomes increasingly significant. However, previous studies on personality evaluation of LLMs often neglect the interaction between culture and personality.

Method: Introducing CulturalPersonas, a large-scale benchmark with human validation for assessing LLMs' personality expression within culturally relevant, behaviorally rich scenarios.

Result: The CulturalPersonas dataset includes 3,000 scenario-based questions covering six diverse countries. The evaluation of three LLMs using multiple-choice and open-ended response formats reveals that CulturalPersonas enhances alignment with country-specific human personality distributions and generates more expressive, culturally coherent outputs than existing benchmarks.

Conclusion: CulturalPersonas provides meaningful modulated trait outputs in response to culturally grounded prompts, suggesting potential for aligning LLMs with global behavioral norms. It is expected to promote the development of more socially intelligent and globally adaptive LLMs.

Abstract: As LLMs become central to interactive applications, ranging from tutoring to
mental health, the ability to express personality in culturally appropriate
ways is increasingly important. While recent works have explored personality
evaluation of LLMs, they largely overlook the interplay between culture and
personality. To address this, we introduce CulturalPersonas, the first
large-scale benchmark with human validation for evaluating LLMs' personality
expression in culturally grounded, behaviorally rich contexts. Our dataset
spans 3,000 scenario-based questions across six diverse countries, designed to
elicit personality through everyday scenarios rooted in local values. We
evaluate three LLMs, using both multiple-choice and open-ended response
formats. Our results show that CulturalPersonas improves alignment with
country-specific human personality distributions (over a 20% reduction in
Wasserstein distance across models and countries) and elicits more expressive,
culturally coherent outputs compared to existing benchmarks. CulturalPersonas
surfaces meaningful modulated trait outputs in response to culturally grounded
prompts, offering new directions for aligning LLMs to global norms of behavior.
By bridging personality expression and cultural nuance, we envision that
CulturalPersonas will pave the way for more socially intelligent and globally
adaptive LLMs.

</details>


### [24] [Zero-Shot Event Causality Identification via Multi-source Evidence Fuzzy Aggregation with Large Language Models](https://arxiv.org/abs/2506.05675)
*Zefan Zeng,Xingchen Hu,Qing Cheng,Weiping Ding,Wentao Li,Zhong Liu*

Main category: cs.CL

TL;DR: This paper introduces MEFA, a novel zero-shot framework for Event Causality Identification (ECI), which uses Multi-source Evidence Fuzzy Aggregation to improve accuracy and reduce causal hallucination.


<details>
  <summary>Details</summary>
Motivation: Existing ECI models depend on large-scale annotated data and are prone to causal hallucination. LLMs can perform zero-shot ECI but often make mistakes in establishing causal links.

Method: MEFA decomposes causality reasoning into three main tasks and three auxiliary tasks. It guides LLMs to generate uncertain responses and deterministic outputs through designed prompts and uses fuzzy aggregation to integrate evidence for causality scoring and determination.

Result: MEFA outperforms second-best unsupervised baselines by 6.2% in F1-score and 9.3% in precision, and significantly reduces hallucination-induced errors.

Conclusion: The proposed MEFA framework effectively addresses the challenges in ECI, demonstrating superior performance and reliability compared to existing methods.

Abstract: Event Causality Identification (ECI) aims to detect causal relationships
between events in textual contexts. Existing ECI models predominantly rely on
supervised methodologies, suffering from dependence on large-scale annotated
data. Although Large Language Models (LLMs) enable zero-shot ECI, they are
prone to causal hallucination-erroneously establishing spurious causal links.
To address these challenges, we propose MEFA, a novel zero-shot framework based
on Multi-source Evidence Fuzzy Aggregation. First, we decompose causality
reasoning into three main tasks (temporality determination, necessity analysis,
and sufficiency verification) complemented by three auxiliary tasks. Second,
leveraging meticulously designed prompts, we guide LLMs to generate uncertain
responses and deterministic outputs. Finally, we quantify LLM's responses of
sub-tasks and employ fuzzy aggregation to integrate these evidence for
causality scoring and causality determination. Extensive experiments on three
benchmarks demonstrate that MEFA outperforms second-best unsupervised baselines
by 6.2% in F1-score and 9.3% in precision, while significantly reducing
hallucination-induced errors. In-depth analysis verify the effectiveness of
task decomposition and the superiority of fuzzy aggregation.

</details>


### [25] [A Unified Representation for Continuity and Discontinuity: Syntactic and Computational Motivations](https://arxiv.org/abs/2506.05686)
*Ratna Kandala,Prakash Mondal*

Main category: cs.CL

TL;DR: This paper introduces a unified representation of linguistic structures from three grammar formalisms, simplifying both theoretical syntax and computational complexity.


<details>
  <summary>Details</summary>
Motivation: To propose a unified representation of linguistic structure for three grammar formalisms (PSG, DG, and CG) considering syntactic and computational complexity.

Method: Illustrating steps to achieve a unified representation for a discontinuous subordinate clause from Turkish as an example.

Result: A new theoretical approach to handle discontinuity in natural language by uniting the basic principles of PSG, DG, and CG; Simplifies computational complexity for neurocognitive representation and processing of both continuous and discontinuous sentences.

Conclusion: This paper proposes the correspondence principle for a unified representation of representational principles from PSG, DG, and CG, with significant implications for syntactic analysis and computational efficiency.

Abstract: This paper advances a unified representation of linguistic structure for
three grammar formalisms, namely, Phrase Structure Grammar (PSG), Dependency
Grammar (DG) and Categorial Grammar (CG) from the perspective of syntactic and
computational complexity considerations. The correspondence principle is
proposed to enable a unified representation of the representational principles
from PSG, DG, and CG. To that end, the paper first illustrates a series of
steps in achieving a unified representation for a discontinuous subordinate
clause from Turkish as an illustrative case. This affords a new way of
approaching discontinuity in natural language from a theoretical point of view
that unites and integrates the basic tenets of PSG, DG, and CG, with
significant consequences for syntactic analysis. Then this paper demonstrates
that a unified representation can simplify computational complexity with
regards to the neurocognitive representation and processing of both continuous
and discontinuous sentences vis-\`a-vis the basic principles of PSG, DG, and
CG.

</details>


### [26] [When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.05690)
*Zhishang Xiang,Chuanjie Wu,Qinggang Zhang,Shengyuan Chen,Zijin Hong,Xiao Huang,Jinsong Su*

Main category: cs.CL

TL;DR: Evaluate GraphRAG models on both hierarchical knowledge retrieval and deep contextual reasoning.


<details>
  <summary>Details</summary>
Motivation: Despite the promise of GraphRAG, it often underperforms vanilla RAG on real-world tasks. This study aims to determine its effectiveness and where graph structures benefit RAG systems.

Method: Propose GraphRAG-Bench, a comprehensive benchmark featuring increasing difficulty tasks and systematic evaluation from graph construction to final generation.

Result: Systematically investigates conditions when GraphRAG outperforms traditional RAG and the reasons for its success.

Conclusion: Offers guidelines for practical application of GraphRAG systems.

Abstract: Graph retrieval-augmented generation (GraphRAG) has emerged as a powerful
paradigm for enhancing large language models (LLMs) with external knowledge. It
leverages graphs to model the hierarchical structure between specific concepts,
enabling more coherent and effective knowledge retrieval for accurate
reasoning.Despite its conceptual promise, recent studies report that GraphRAG
frequently underperforms vanilla RAG on many real-world tasks. This raises a
critical question: Is GraphRAG really effective, and in which scenarios do
graph structures provide measurable benefits for RAG systems? To address this,
we propose GraphRAG-Bench, a comprehensive benchmark designed to evaluate
GraphRAG models onboth hierarchical knowledge retrieval and deep contextual
reasoning. GraphRAG-Bench features a comprehensive dataset with tasks of
increasing difficulty, coveringfact retrieval, complex reasoning, contextual
summarization, and creative generation, and a systematic evaluation across the
entire pipeline, from graph constructionand knowledge retrieval to final
generation. Leveraging this novel benchmark, we systematically investigate the
conditions when GraphRAG surpasses traditional RAG and the underlying reasons
for its success, offering guidelines for its practical application. All related
resources and analyses are collected for the community at
https://github.com/GraphRAG-Bench/GraphRAG-Benchmark.

</details>


### [27] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [28] [RKEFino1: A Regulation Knowledge-Enhanced Large Language Model](https://arxiv.org/abs/2506.05700)
*Yan Wang,Yueru He,Ruoyu Xiang,Jeff Zhao*

Main category: cs.CL

TL;DR: 提出了一种基于大型语言模型的RKEFino1模型用于解决金融应用中的准确性和合规性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融应用中有很大潜力，但数字监管报告(DRR)中存在准确性与合规性挑战。

Method: 基于Fino1构建的RKEFino1模型，并使用XBRL、CDM和MOF领域的知识进行微调。

Result: 实验结果表明，所提方法在合规关键的金融任务中的有效性与泛化能力。

Conclusion: 提出的方法在合规关键的金融任务中表现出有效性与泛化能力。

Abstract: Recent advances in large language models (LLMs) hold great promise for
financial applications but introduce critical accuracy and compliance
challenges in Digital Regulatory Reporting (DRR). To address these issues, we
propose RKEFino1, a regulation knowledge-enhanced financial reasoning model
built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We
formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce
a novel Numerical NER task covering financial entities in both sentences and
tables. Experimental results demonstrate the effectiveness and generalization
capacity of RKEFino1 in compliance-critical financial tasks. We have released
our model on Hugging Face.

</details>


### [29] [Large Language Models are Good Relational Learners](https://arxiv.org/abs/2506.05725)
*Fang Wu,Vijay Prakash Dwivedi,Jure Leskovec*

Main category: cs.CL

TL;DR: Rel-LLM，一种结合图神经网络和大型语言模型的新架构，有效地处理复杂实体关系并提升关系型深度学习任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在关系型深度学习中的应用未被充分探索，传统的文本序列化方法忽视了数据库的关键关系结构且引入冗余信息。

Method: 提出Rel-LLM，通过图神经网络编码器生成结构化关系提示，并在检索增强生成框架内使用这些提示来保持数据库的内在关系结构。

Result: Rel-LLM在关键的关系型深度学习任务上优于现有方法，提供了可扩展且高效的整合大型语言模型与结构化数据源的方式。

Conclusion: Rel-LLM展示了如何有效利用大型语言模型的能力于关系型深度学习，同时克服传统方法的局限性。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various domains, yet their application to relational deep learning (RDL)
remains underexplored. Existing approaches adapt LLMs by traversing relational
links between entities in a database and converting the structured data into
flat text documents. Still, this text-based serialization disregards critical
relational structures, introduces redundancy, and often exceeds standard LLM
context lengths. We introduce Rel-LLM, a novel architecture that utilizes a
graph neural network (GNN)- based encoder to generate structured relational
prompts for LLMs within a retrieval-augmented generation (RAG) framework.
Unlike traditional text-based serialization approaches, our method preserves
the inherent relational structure of databases while enabling LLMs to
effectively process and reason over complex entity relationships. Specifically,
the GNN encoder extracts a local subgraph around an entity to build feature
representations that contain relevant entity relationships and temporal
dependencies. These representations are transformed into structured prompts
using a denormalization process, effectively allowing the LLM to reason over
relational structures. Through extensive experiments, we demonstrate that
Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and
efficient approach to integrating LLMs with structured data sources. Code is
available at https://github.com/smiles724/Rel-LLM.

</details>


### [30] [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/abs/2506.05735)
*Rongzhe Wei,Peizhi Niu,Hans Hao-Hsun Hsu,Ruihan Wu,Haoteng Yin,Mohsen Ghassemi,Yifan Li,Vamsi K. Potluru,Eli Chien,Kamalika Chaudhuri,Olgica Milenkovic,Pan Li*

Main category: cs.CL

TL;DR: Propose a new framework for evaluating machine unlearning in large language models.


<details>
  <summary>Details</summary>
Motivation: Existing methods mainly focus on removing isolated facts but ignore latent inferential dependencies and the non-deterministic nature of knowledge.

Method: Develop a knowledge unlearning evaluation framework using knowledge graphs with confidence scores and LLMs as judges.

Result: Experiments show the new framework provides a more realistic and rigorous assessment of unlearning performance.

Conclusion: Current evaluation strategies overestimate unlearning effectiveness.

Abstract: Machine unlearning techniques aim to mitigate unintended memorization in
large language models (LLMs). However, existing approaches predominantly focus
on the explicit removal of isolated facts, often overlooking latent inferential
dependencies and the non-deterministic nature of knowledge within LLMs.
Consequently, facts presumed forgotten may persist implicitly through
correlated information. To address these challenges, we propose a knowledge
unlearning evaluation framework that more accurately captures the implicit
structure of real-world knowledge by representing relevant factual contexts as
knowledge graphs with associated confidence scores. We further develop an
inference-based evaluation protocol leveraging powerful LLMs as judges; these
judges reason over the extracted knowledge subgraph to determine unlearning
success. Our LLM judges utilize carefully designed prompts and are calibrated
against human evaluations to ensure their trustworthiness and stability.
Extensive experiments on our newly constructed benchmark demonstrate that our
framework provides a more realistic and rigorous assessment of unlearning
performance. Moreover, our findings reveal that current evaluation strategies
tend to overestimate unlearning effectiveness. Our code is publicly available
at https://github.com/Graph-COM/Knowledge_Unlearning.git.

</details>


### [31] [LLM-Symbolic Integration for Robust Temporal Tabular Reasoning](https://arxiv.org/abs/2506.05746)
*Atharv Kulkarni,Kushagra Dixit,Vivek Srikumar,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: This paper introduces TempTabQA-C, a synthetic dataset and a symbolic intermediate representation for improving large language models' temporal tabular question answering capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional prompting methods for large language models often fail in temporal tabular question answering due to challenges like memorization, sensitivity to table size, and poor performance on complex queries.

Method: The paper designs TempTabQA-C for evaluations, uses symbolic intermediate representation to transform tables into database schemas, and employs adaptive few-shot prompting with tailored examples.

Result: The method shows enhanced generalization and reduced biases, with improved robustness, scalability, and performance across key challenges.

Conclusion: The proposed approach sets a new benchmark for robust temporal reasoning with large language models.

Abstract: Temporal tabular question answering presents a significant challenge for
Large Language Models (LLMs), requiring robust reasoning over structured data,
which is a task where traditional prompting methods often fall short. These
methods face challenges such as memorization, sensitivity to table size, and
reduced performance on complex queries. To overcome these limitations, we
introduce TempTabQA-C, a synthetic dataset designed for systematic and
controlled evaluations, alongside a symbolic intermediate representation that
transforms tables into database schemas. This structured approach allows LLMs
to generate and execute SQL queries, enhancing generalization and mitigating
biases. By incorporating adaptive few-shot prompting with contextually tailored
examples, our method achieves superior robustness, scalability, and
performance. Experimental results consistently highlight improvements across
key challenges, setting a new benchmark for robust temporal reasoning with
LLMs.

</details>


### [32] [Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning](https://arxiv.org/abs/2506.05760)
*Xuanyu Lei,Chenliang Li,Yuning Wu,Kaiming Liu,Weizhou Shen,Peng Li,Ming Yan,Ji Zhang,Fei Huang,Yang Liu*

Main category: cs.CL

TL;DR: 提出了一种新的强化学习框架Writing-RL，用于提升大规模语言模型在长文本写作上的表现。该框架包含三个主要部分：优先选择高潜力样本的数据选择策略、提供判别性学习信号的成对比较奖励机制以及根据模型性能动态调整任务难度的参考调度方法。实验表明，使用该强化学习框架训练的模型在长文本写作上优于强监督微调基线模型，并且这些模型在处理长输入推理任务时也表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的方法存在数据饱和和受限的学习能力等问题，因此需要一种新的方法来提高大规模语言模型在长文本写作上的表现。

Method: 提出了一种自适应课程强化学习框架Writing-RL，包含三个关键组件：Margin-aware数据选择策略、成对比较奖励机制和动态参考调度方法。

Result: 在7B规模的写手模型上进行的实验显示，使用此强化学习框架可以显著改善长文本写作性能，超过强大的监督微调基线。此外，用长输出强化学习训练的模型在长输入推理任务中也表现良好。

Conclusion: Writing-RL框架通过克服现有方法的局限性，为大规模语言模型在长文本写作方面的进步提供了新的途径，并可能为重新思考长上下文训练提供了一个有希望的角度。

Abstract: Recent advances in Large Language Models (LLMs) have enabled strong
performance in long-form writing, yet existing supervised fine-tuning (SFT)
approaches suffer from limitations such as data saturation and restricted
learning capacity bounded by teacher signals. In this work, we present
Writing-RL: an Adaptive Curriculum Reinforcement Learning framework to advance
long-form writing capabilities beyond SFT. The framework consists of three key
components: Margin-aware Data Selection strategy that prioritizes samples with
high learning potential, Pairwise Comparison Reward mechanism that provides
discriminative learning signals in the absence of verifiable rewards, and
Dynamic Reference Scheduling approach, which plays a particularly critical role
by adaptively adjusting task difficulty based on evolving model performance.
Experiments on 7B-scale writer models show that our RL framework largely
improves long-form writing performance over strong SFT baselines. Furthermore,
we observe that models trained with long-output RL generalize surprisingly well
to long-input reasoning tasks, potentially offering a promising perspective for
rethinking long-context training.

</details>


### [33] [BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over Bio-Molecular Interactions](https://arxiv.org/abs/2506.05766)
*Saptarshi Sengupta,Shuhua Yang,Paul Kwong Yu,Fali Wang,Suhang Wang*

Main category: cs.CL

TL;DR: This paper introduces BioMol-MQA, a new QA dataset for testing LLMs' ability to handle multi-modal knowledge graphs in polypharmacy.


<details>
  <summary>Details</summary>
Motivation: Most existing RAG-based LLMs only retrieve single modality information, while many real-world problems require multi-modality domain-specific information retrieval and reasoning.

Method: Presenting BioMol-MQA, a new QA dataset on polypharmacy with a multimodal KG and challenging questions to test LLM capabilities.

Result: Existing LLMs perform poorly on BioMol-MQA without necessary background data.

Conclusion: Existing LLMs struggle to answer questions in BioMol-MQA without necessary background data, indicating the need for stronger RAG frameworks.

Abstract: Retrieval augmented generation (RAG) has shown great power in improving Large
Language Models (LLMs). However, most existing RAG-based LLMs are dedicated to
retrieving single modality information, mainly text; while for many real-world
problems, such as healthcare, information relevant to queries can manifest in
various modalities such as knowledge graph, text (clinical notes), and complex
molecular structure. Thus, being able to retrieve relevant multi-modality
domain-specific information, and reason and synthesize diverse knowledge to
generate an accurate response is important. To address the gap, we present
BioMol-MQA, a new question-answering (QA) dataset on polypharmacy, which is
composed of two parts (i) a multimodal knowledge graph (KG) with text and
molecular structure for information retrieval; and (ii) challenging questions
that designed to test LLM capabilities in retrieving and reasoning over
multimodal KG to answer questions. Our benchmarks indicate that existing LLMs
struggle to answer these questions and do well only when given the necessary
background data, signaling the necessity for strong RAG frameworks.

</details>


### [34] [dots.llm1 Technical Report](https://arxiv.org/abs/2506.05767)
*Bi Huo,Bin Tu,Cheng Qin,Da Zheng,Debing Zhang,Dongjie Zhang,En Li,Fu Guo,Jian Yao,Jie Lou,Junfeng Tian,Li Hu,Ran Zhu,Shengdong Chen,Shuo Liu,Su Guang,Te Wo,Weijun Zhang,Xiaoming Shi,Xinxin Peng,Xing Wu,Yawen Liu,Yuqiu Ji,Ze Wen,Zhenhai Liu,Zichao Li,Zilong Liao*

Main category: cs.CL

TL;DR: dots.llm1 is a large-scale MoE model that activates 14B parameters out of 142B total, achieving performance similar to Qwen2.5-72B without using synthetic data during pretraining.


<details>
  <summary>Details</summary>
Motivation: To scale language models efficiently by activating only a subset of parameters for each input token.

Method: Using Mixture of Experts (MoE) models and a meticulously crafted data processing pipeline.

Result: Achieves performance comparable to Qwen2.5-72B after pretraining on 11.2T tokens and post-training.

Conclusion: dots.llm1 reduces training and inference costs while delivering performance on par with state-of-the-art models, and provides open-source training checkpoints.

Abstract: Mixture of Experts (MoE) models have emerged as a promising paradigm for
scaling language models efficiently by activating only a subset of parameters
for each input token. In this report, we present dots.llm1, a large-scale MoE
model that activates 14B parameters out of a total of 142B parameters,
delivering performance on par with state-of-the-art models while reducing
training and inference costs. Leveraging our meticulously crafted and efficient
data processing pipeline, dots.llm1 achieves performance comparable to
Qwen2.5-72B after pretraining on 11.2T high-quality tokens and post-training to
fully unlock its capabilities. Notably, no synthetic data is used during
pretraining. To foster further research, we open-source intermediate training
checkpoints at every one trillion tokens, providing valuable insights into the
learning dynamics of large language models.

</details>


### [35] [Discrete Minds in a Continuous World: Do Language Models Know Time Passes?](https://arxiv.org/abs/2506.05790)
*Minghan Wang,Ye Bai,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: This study investigates whether Large Language Models (LLMs) perceive the passage of time and adapt their decision-making accordingly. Three experiments were conducted to explore this hypothesis, showing that LLMs have some awareness of time passage which can be used to adjust response length or behavior under time pressure.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can perceive the passage of time and adapt their decision-making accordingly.

Method: Three complementary experiments including Token-Time Hypothesis validation through a dialogue duration judgment task, examining response length adaptation in question answering tasks, and developing an interactive navigation challenge called BombRush.

Result: LLMs possess certain awareness of time passage, which enables them to bridge discrete linguistic tokens and continuous physical time. However, this capability varies with model size and reasoning abilities.

Conclusion: This work establishes a theoretical foundation for enhancing temporal awareness in LLMs for time-sensitive applications.

Abstract: While Large Language Models (LLMs) excel at temporal reasoning tasks like
event ordering and duration estimation, their ability to perceive the actual
passage of time remains unexplored. We investigate whether LLMs perceive the
passage of time and adapt their decision-making accordingly through three
complementary experiments. First, we introduce the Token-Time Hypothesis,
positing that LLMs can map discrete token counts to continuous wall-clock time,
and validate this through a dialogue duration judgment task. Second, we
demonstrate that LLMs could use this awareness to adapt their response length
while maintaining accuracy when users express urgency in question answering
tasks. Finally, we develop BombRush, an interactive navigation challenge that
examines how LLMs modify behavior under progressive time pressure in dynamic
environments. Our findings indicate that LLMs possess certain awareness of time
passage, enabling them to bridge discrete linguistic tokens and continuous
physical time, though this capability varies with model size and reasoning
abilities. This work establishes a theoretical foundation for enhancing
temporal awareness in LLMs for time-sensitive applications.

</details>


### [36] [MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning](https://arxiv.org/abs/2506.05813)
*Ye Bai,Minghan Wang,Thuy-Trang Vu*

Main category: cs.CL

TL;DR: MAPLE is a novel framework that mimics human problem-solving through specialized cognitive agents working in a feedback-driven loop. It outperforms existing approaches in table-based question answering.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle to achieve complex reasoning capabilities required for table-based question answering with single-pass inference. Existing approaches lack error detection mechanisms and discard problem-solving experiences.

Method: MAPLE integrates four key components: (1) a Solver using the ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a Reflector for error diagnosis and strategy correction, and (4) an Archiver managing long-term memory for experience reuse and evolution.

Result: Experiments on WiKiTQ and TabFact demonstrate significant improvements over existing methods, achieving state-of-the-art performance across multiple LLM backbones.

Conclusion: MAPLE shows great potential in enhancing complex reasoning capabilities of LLMs for table-based question answering.

Abstract: Table-based question answering requires complex reasoning capabilities that
current LLMs struggle to achieve with single-pass inference. Existing
approaches, such as Chain-of-Thought reasoning and question decomposition, lack
error detection mechanisms and discard problem-solving experiences, contrasting
sharply with how humans tackle such problems. In this paper, we propose MAPLE
(Multi-agent Adaptive Planning with Long-term mEmory), a novel framework that
mimics human problem-solving through specialized cognitive agents working in a
feedback-driven loop. MAPLE integrates 4 key components: (1) a Solver using the
ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a
Reflector for error diagnosis and strategy correction, and (4) an Archiver
managing long-term memory for experience reuse and evolution. Experiments on
WiKiTQ and TabFact demonstrate significant improvements over existing methods,
achieving state-of-the-art performance across multiple LLM backbones.

</details>


### [37] [FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging](https://arxiv.org/abs/2506.05828)
*Zichen Tang,Haihong E,Ziyan Ma,Haoyang He,Jiacheng Liu,Zhongjun Yang,Zihua Rong,Rongjin Li,Kun Ji,Qing Huang,Xinyang Hu,Yang Liu,Qianhe Zheng*

Main category: cs.CL

TL;DR: A novel benchmark named FinanceReasoning is introduced to assess the reasoning abilities of large reasoning models in financial numerical reasoning tasks. It updates existing datasets, adds new questions, and includes refined Python functions, achieving high accuracy with some models while identifying areas needing improvement.


<details>
  <summary>Details</summary>
Motivation: To create a more credible, comprehensive, and challenging benchmark for evaluating the reasoning capabilities of large reasoning models specifically in financial numerical reasoning problems.

Method: Updating 15.6% of questions from four public datasets, annotating new questions, constructing Python-formatted functions, and creating Hard problems requiring multiple financial formula applications.

Result: FinanceReasoning covers a wide range of financial concepts and formulas, with the best model achieving 89.1% accuracy. Combining Reasoner and Programmer models improves performance.

Conclusion: This work advances the evaluation and improvement of large reasoning models in domain-specific complex reasoning tasks like financial numerical reasoning.

Abstract: We introduce FinanceReasoning, a novel benchmark designed to evaluate the
reasoning capabilities of large reasoning models (LRMs) in financial numerical
reasoning problems. Compared to existing benchmarks, our work provides three
key advancements. (1) Credibility: We update 15.6% of the questions from four
public datasets, annotating 908 new questions with detailed Python solutions
and rigorously refining evaluation standards. This enables an accurate
assessment of the reasoning improvements of LRMs. (2) Comprehensiveness:
FinanceReasoning covers 67.8% of financial concepts and formulas, significantly
surpassing existing datasets. Additionally, we construct 3,133 Python-formatted
functions, which enhances LRMs' financial reasoning capabilities through
refined knowledge (e.g., 83.2% $\rightarrow$ 91.6% for GPT-4o). (3) Challenge:
Models are required to apply multiple financial formulas for precise numerical
reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with
PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical
precision. We demonstrate that combining Reasoner and Programmer models can
effectively enhance LRMs' performance (e.g., 83.2% $\rightarrow$ 87.8% for
DeepSeek-R1). Our work paves the way for future research on evaluating and
improving LRMs in domain-specific complex reasoning tasks.

</details>


### [38] [Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models](https://arxiv.org/abs/2506.05850)
*Cheonbok Park,Jeonghoon Kim,Joosung Lee,Sanghwan Bae,Jaegul Choo,Kangmin Yoo*

Main category: cs.CL

TL;DR: This study investigates cross-lingual collapse in multilingual language models, where the chain-of-thought reverts to the dominant pre-training language. It explores multilingual reasoning in large reasoning models using Group-Relative Policy Optimization and finds that pre-training language imbalances can quickly erode low-resource languages, and language consistency reward improves language consistency at the cost of accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore the mechanism behind multilingual reasoning in large reasoning models and understand why not all languages are trained equally for reasoning.

Method: Fine-tuning multilingual large reasoning models with Group-Relative Policy Optimization on translated versions of GSM$8$K and SimpleRL-Zoo datasets in three languages: Chinese, Korean, and Ukrainian.

Result: Three key findings: GRPO amplifies pre-training language imbalances, language consistency reward mitigates this drift but reduces accuracy, and the resulting language collapse is difficult to reverse.

Conclusion: Not all languages are trained equally for reasoning, and reward shaping, data difficulty, and pre-training priors play important roles in eliciting multilingual reasoning.

Abstract: We identify \textbf{Cross-lingual Collapse}, a systematic drift in which the
chain-of-thought (CoT) of a multilingual language model reverts to its dominant
pre-training language even when the prompt is expressed in a different
language. Recent large language models (LLMs) with reinforcement learning with
verifiable reward (RLVR) have achieved strong logical reasoning performances by
exposing their intermediate reasoning traces, giving rise to large reasoning
models (LRMs). However, the mechanism behind multilingual reasoning in LRMs is
not yet fully explored. To investigate the issue, we fine-tune multilingual
LRMs with Group-Relative Policy Optimization (GRPO) on translated versions of
the GSM$8$K and SimpleRL-Zoo datasets in three different languages: Chinese,
Korean, and Ukrainian. During training, we monitor both task accuracy and
language consistency of the reasoning chains. Our experiments reveal three key
findings: (i) GRPO rapidly amplifies pre-training language imbalances, leading
to the erosion of low-resource languages within just a few hundred updates;
(ii) language consistency reward mitigates this drift but does so at the
expense of an almost 5 - 10 pp drop in accuracy. and (iii) the resulting
language collapse is severely damaging and largely irreversible, as subsequent
fine-tuning struggles to steer the model back toward its original
target-language reasoning capabilities. Together, these findings point to a
remarkable conclusion: \textit{not all languages are trained equally for
reasoning}. Furthermore, our paper sheds light on the roles of reward shaping,
data difficulty, and pre-training priors in eliciting multilingual reasoning.

</details>


### [39] [Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router](https://arxiv.org/abs/2506.05901)
*Chenyang Shao,Xinyang Liu,Yutang Lin,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: This paper introduces R2-Reasoner, a framework that improves the efficiency and cost-effectiveness of multi-step reasoning in large language models by dynamically routing subtasks based on estimated complexity.


<details>
  <summary>Details</summary>
Motivation: To address the high computational cost of multi-step reasoning in large language models, especially when extending the reasoning chain.

Method: Proposing a Reinforced Model Router that includes a task decomposer and a subtask allocator to segment and assign subtasks to appropriate models.

Result: R2-Reasoner reduces API costs by 86.85% while maintaining or surpassing baseline accuracy.

Conclusion: The proposed framework provides a cost-effective and adaptive approach for multi-step reasoning in large language models.

Abstract: Multi-step reasoning has proven essential for enhancing the problem-solving
capabilities of Large Language Models (LLMs) by decomposing complex tasks into
intermediate steps, either explicitly or implicitly. Extending the reasoning
chain at test time through deeper thought processes or broader exploration, can
furthur improve performance, but often incurs substantial costs due to the
explosion in token usage. Yet, many reasoning steps are relatively simple and
can be handled by more efficient smaller-scale language models (SLMs). This
motivates hybrid approaches that allocate subtasks across models of varying
capacities. However, realizing such collaboration requires accurate task
decomposition and difficulty-aware subtask allocation, which is challenging. To
address this, we propose R2-Reasoner, a novel framework that enables
collaborative reasoning across heterogeneous LLMs by dynamically routing
sub-tasks based on estimated complexity. At the core of our framework is a
Reinforced Model Router, composed of a task decomposer and a subtask allocator.
The task decomposer segments complex input queries into logically ordered
subtasks, while the subtask allocator assigns each subtask to the most
appropriate model, ranging from lightweight SLMs to powerful LLMs, balancing
accuracy and efficiency. To train this router, we introduce a staged pipeline
that combines supervised fine-tuning on task-specific datasets with Group
Relative Policy Optimization algorithm, enabling self-supervised refinement
through iterative reinforcement learning. Extensive experiments across four
challenging benchmarks demonstrate that R2-Reasoner reduces API costs by 86.85%
while maintaining or surpassing baseline accuracy. Our framework paves the way
for more cost-effective and adaptive LLM reasoning. The code is open-source at
https://anonymous.4open.science/r/R2_Reasoner .

</details>


### [40] [Generating Grounded Responses to Counter Misinformation via Learning Efficient Fine-Grained Critiques](https://arxiv.org/abs/2506.05924)
*Xiaofei Xu,Xiuzhen Zhang,Ke Deng*

Main category: cs.CL

TL;DR: Propose MisMitiFact, an efficient framework for generating fact-grounded counter-responses at scale by refining LLM outputs with lightweight critique models.


<details>
  <summary>Details</summary>
Motivation: Fake news and misinformation pose a significant threat to society, making efficient mitigation essential. Manual fact-checking is costly and lacks scalability.

Method: Propose MisMitiFact, an efficient framework for generating fact-grounded counter-responses at scale. MisMitiFact generates simple critique feedback to refine LLM outputs and uses lightweight, fine-grained critique models trained on data from fact-checking sites to correct errors in key elements of LLM generations.

Result: Experiments show that MisMitiFact generates counter-responses of comparable quality to LLMs' self-feedback while using significantly smaller critique models. It achieves ~5x increase in feedback generation throughput.

Conclusion: MisMitiFact is highly suitable for cost-effective, large-scale misinformation mitigation.

Abstract: Fake news and misinformation poses a significant threat to society, making
efficient mitigation essential. However, manual fact-checking is costly and
lacks scalability. Large Language Models (LLMs) offer promise in automating
counter-response generation to mitigate misinformation, but a critical
challenge lies in their tendency to hallucinate non-factual information.
Existing models mainly rely on LLM self-feedback to reduce hallucination, but
this approach is computationally expensive. In this paper, we propose
MisMitiFact, Misinformation Mitigation grounded in Facts, an efficient
framework for generating fact-grounded counter-responses at scale. MisMitiFact
generates simple critique feedback to refine LLM outputs, ensuring responses
are grounded in evidence. We develop lightweight, fine-grained critique models
trained on data sourced from readily available fact-checking sites to identify
and correct errors in key elements such as numerals, entities, and topics in
LLM generations. Experiments show that MisMitiFact generates counter-responses
of comparable quality to LLMs' self-feedback while using significantly smaller
critique models. Importantly, it achieves ~5x increase in feedback generation
throughput, making it highly suitable for cost-effective, large-scale
misinformation mitigation. Code and LLM prompt templates are at
https://github.com/xxfwin/MisMitiFact.

</details>


### [41] [LengClaro2023: A Dataset of Administrative Texts in Spanish with Plain Language adaptations](https://arxiv.org/abs/2506.05927)
*Belén Agüera-Marco,Itziar Gonzalez-Dios*

Main category: cs.CL

TL;DR: This paper introduces LengClaro2023, a Spanish dataset of legal-administrative texts with simplified versions to evaluate automatic text simplification systems.


<details>
  <summary>Details</summary>
Motivation: To create a resource for evaluating automatic text simplification systems in Spanish using frequently used procedures from the Spanish Social Security website.

Method: Creating two simplified equivalents for each text based on arText claro recommendations and plain language guidelines.

Result: A dataset named LengClaro2023 consisting of legal-administrative texts in Spanish with two simplified versions.

Conclusion: The linguistic resource created can be used for evaluating automatic text simplification systems in Spanish.

Abstract: In this work, we present LengClaro2023, a dataset of legal-administrative
texts in Spanish. Based on the most frequently used procedures from the Spanish
Social Security website, we have created for each text two simplified
equivalents. The first version follows the recommendations provided by arText
claro. The second version incorporates additional recommendations from plain
language guidelines to explore further potential improvements in the system.
The linguistic resource created in this work can be used for evaluating
automatic text simplification (ATS) systems in Spanish.

</details>


### [42] [MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.05928)
*Jie Cao,Tianwei Lin,Hongyang He,Rolan Yan,Wenqiao Zhang,Juncheng Li,Dongping Zhang,Siliang Tang,Yueting Zhuang*

Main category: cs.CL

TL;DR: 提出了一种新的方法MoA，它通过整合不同结构的PEFT适配器专家来提高大型语言模型的性能和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有的同构MoE-LoRA架构在表示能力和专家负载平衡方面存在问题，限制了大语言模型的潜力。

Method: 提出了异构的混合适配器（MoA）方法，支持软MoA和稀疏MoA两种变体。

Result: 实验结果显示，异构MoA在性能和参数效率上都优于同构MoE-LoRA方法。

Conclusion: MoA能够更好地促进预训练知识的有效迁移，适用于下游任务。

Abstract: Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts
(MoE) to further enhance the performance of parameter-efficient fine-tuning
(PEFT) methods in Large Language Model (LLM) applications. Existing methods
employ \emph{homogeneous} MoE-LoRA architectures composed of LoRA experts with
either similar or identical structures and capacities. However, these
approaches often suffer from representation collapse and expert load imbalance,
which negatively impact the potential of LLMs. To address these challenges, we
propose a \emph{heterogeneous} \textbf{Mixture-of-Adapters (MoA)} approach.
This method dynamically integrates PEFT adapter experts with diverse
structures, leveraging their complementary representational capabilities to
foster expert specialization, thereby enhancing the effective transfer of
pre-trained knowledge to downstream tasks. MoA supports two variants:
\textbf{(i)} \textit{Soft MoA} achieves fine-grained integration by performing
a weighted fusion of all expert outputs; \textbf{(ii)} \textit{Sparse MoA}
activates adapter experts sparsely based on their contribution, achieving this
with negligible performance degradation. Experimental results demonstrate that
heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance
and parameter efficiency. Our project is available at
https://github.com/DCDmllm/MoA.

</details>


### [43] [DynamicMind: A Tri-Mode Thinking System for Large Language Models](https://arxiv.org/abs/2506.05936)
*Wei Li,Yanbin Wei,Qiushi Huang,Jiangyue Yan,Yang Chen,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: Introduces DynamicMind, a tri-mode thinking system for LLMs to adapt reasoning depth, achieving better ZSQA capabilities with improved performance-efficiency balance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with dynamically adjusting reasoning depth for different task complexities, leading to inefficiencies.

Method: DynamicMind introduces a tri-mode thinking system including Fast, Normal, and Slow modes, along with a Thinking Density metric and TMC dataset with a Mind Router for mode prediction.

Result: DynamicMind outperforms in ZSQA tasks on various benchmarks while optimizing computational resources.

Conclusion: DynamicMind successfully enhances LLMs' adaptability and efficiency in zero-shot question answering.

Abstract: Modern large language models (LLMs) often struggle to dynamically adapt their
reasoning depth to varying task complexities, leading to suboptimal performance
or inefficient resource utilization. To address this, we introduce DynamicMind,
a novel tri-mode thinking system. DynamicMind empowers LLMs to autonomously
select between Fast, Normal, and Slow thinking modes for zero-shot question
answering (ZSQA) tasks through cognitive-inspired prompt engineering. Our
framework's core innovations include: (1) expanding the established
dual-process framework of fast and slow thinking into a tri-mode thinking
system involving a normal thinking mode to preserve the intrinsic capabilities
of LLM; (2) proposing the Thinking Density metric, which aligns computational
resource allocation with problem complexity; and (3) developing the Thinking
Mode Capacity (TMC) dataset and a lightweight Mind Router to predict the
optimal thinking mode. Extensive experiments across diverse mathematical,
commonsense, and scientific QA benchmarks demonstrate that DynamicMind achieves
superior ZSQA capabilities while establishing an effective trade-off between
performance and computational efficiency.

</details>


### [44] [IntentionESC: An Intention-Centered Framework for Enhancing Emotional Support in Dialogue Systems](https://arxiv.org/abs/2506.05947)
*Xinjie Zhang,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: This paper introduces the IntentionESC framework and ICECoT mechanism to improve emotional support conversations by defining supporter intentions, identifying key emotional state aspects, mapping to support strategies, and enhancing large language models' effectiveness.


<details>
  <summary>Details</summary>
Motivation: Unclear intentions in emotional support conversations can lead supporters to use inappropriate strategies, thus the need for clearly defined intentions to guide both supporter motivations and the overall process.

Method: Proposes the IntentionESC framework which includes defining supporter intentions, identifying key emotional state aspects, mapping to support strategies, and introduces the ICECoT mechanism to enhance LLMs' understanding and generation of effective emotional support responses.

Result: Developed an automated annotation pipeline to produce high-quality training data and designed a comprehensive evaluation scheme to assess emotional support efficacy, conducting experiments to validate the framework.

Conclusion: The proposed IntentionESC framework and ICECoT mechanism improve emotional support conversation quality by enhancing large language models' ability to understand and generate effective emotional support responses.

Abstract: In emotional support conversations, unclear intentions can lead supporters to
employ inappropriate strategies, inadvertently imposing their expectations or
solutions on the seeker. Clearly defined intentions are essential for guiding
both the supporter's motivations and the overall emotional support process. In
this paper, we propose the Intention-centered Emotional Support Conversation
(IntentionESC) framework, which defines the possible intentions of supporters
in emotional support conversations, identifies key emotional state aspects for
inferring these intentions, and maps them to appropriate support strategies.
While Large Language Models (LLMs) excel in text generating, they fundamentally
operate as probabilistic models trained on extensive datasets, lacking a true
understanding of human thought processes and intentions. To address this
limitation, we introduce the Intention Centric Chain-of-Thought (ICECoT)
mechanism. ICECoT enables LLMs to mimic human reasoning by analyzing emotional
states, inferring intentions, and selecting suitable support strategies,
thereby generating more effective emotional support responses. To train the
model with ICECoT and integrate expert knowledge, we design an automated
annotation pipeline that produces high-quality training data. Furthermore, we
develop a comprehensive evaluation scheme to assess emotional support efficacy
and conduct extensive experiments to validate our framework. Our data and code
are available at https://github.com/43zxj/IntentionESC_ICECoT.

</details>


### [45] [NameTag 3: A Tool and a Service for Multilingual/Multitagset NER](https://arxiv.org/abs/2506.05949)
*Jana Straková,Milan Straka*

Main category: cs.CL

TL;DR: NameTag 3 is an open-source tool and cloud-based web service that performs multilingual, multidataset, and multitagset named entity recognition with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accessible tool for named entity recognition across multiple languages, datasets, and tagsets.

Method: Developed a single fine-tuned model with 355M parameters for flat NER in 17 languages and another model with 126M parameters for nested NER in Czech.

Result: Achieved state-of-the-art results on 21 test datasets in 15 languages and remained competitive on others.

Conclusion: NameTag 3 is now available as a command-line tool, cloud-based service, and has its source code licensed under open-source MPL 2.0, with models distributed under non-commercial CC BY-NC-SA 4.0.

Abstract: We introduce NameTag 3, an open-source tool and cloud-based web service for
multilingual, multidataset, and multitagset named entity recognition (NER),
supporting both flat and nested entities. NameTag 3 achieves state-of-the-art
results on 21 test datasets in 15 languages and remains competitive on the
rest, even against larger models. It is available as a command-line tool and as
a cloud-based service, enabling use without local installation. NameTag 3 web
service currently provides flat NER for 17 languages, trained on 21 corpora and
three NE tagsets, all powered by a single 355M-parameter fine-tuned model; and
nested NER for Czech, powered by a 126M fine-tuned model. The source code is
licensed under open-source MPL 2.0, while the models are distributed under
non-commercial CC BY-NC-SA 4.0. Documentation is available at
https://ufal.mff.cuni.cz/nametag, source code at
https://github.com/ufal/nametag3, and trained models via https://lindat.cz. The
REST service and the web application can be found at
https://lindat.mff.cuni.cz/services/nametag/. A demonstration video is
available at https://www.youtube.com/watch?v=-gaGnP0IV8A.

</details>


### [46] [Elementary Math Word Problem Generation using Large Language Models](https://arxiv.org/abs/2506.05950)
*Nimesh Ariyarathne,Harshani Bandara,Yasith Heshan,Omega Gamage,Surangika Ranathunga,Dilan Nayanajith,Yutharsan Sivapalan,Gayathri Lihinikaduarachchi,Tharoosha Vihidun,Meenambika Chandirakumar,Sanujen Premakumar,Sanjula Gathsara*

Main category: cs.CL

TL;DR: 提出一种基于大型语言模型的数学应用题生成系统，无需额外输入，生成的题目质量高但类型和年级适配性有待改进。


<details>
  <summary>Details</summary>
Motivation: 手动创建数学应用题耗时且容易出错，希望通过自动化方法提高数学问题生成的效率和质量。

Method: 基于大型语言模型的方法，无需额外输入，只需提供所需数学应用题数量、年级和题目类型。

Result: 生成的数学应用题质量高，基本没有拼写和语法问题，但在遵循指定年级和题目类型方面表现不佳。

Conclusion: 生成的数学应用题质量高，但大型语言模型在遵循指定年级和题目类型要求方面仍有困难。

Abstract: Mathematics is often perceived as a complex subject by students, leading to
high failure rates in exams. To improve Mathematics skills, it is important to
provide sample questions for students to practice problem-solving. Manually
creating Math Word Problems (MWPs) is time consuming for tutors, because they
have to type in natural language while adhering to grammar and spelling rules
of the language. Existing Deep Learning techniques for MWP generation either
require a tutor to provide the initial portion of the MWP, and/or additional
information such as an equation. In this paper, we present an MWP generation
system based on Large Language Models (LLMs) that overcome the need for
additional input - the only input to our system is the number of MWPs needed,
the grade and the type of question (e.g. addition, subtraction). Unlike the
existing LLM-based solutions for MWP generation, we carried out an extensive
set of experiments involving different LLMs, prompting strategies, techniques
to improve the diversity of questions, as well as techniques that employ human
feedback to improve LLM performance. Human and automated evaluations confirmed
that the generated MWPs are high in quality, with minimal spelling and grammar
issues. However, LLMs still struggle to generate questions that adhere to the
specified grade and question type requirements.

</details>


### [47] [Let's Put Ourselves in Sally's Shoes: Shoes-of-Others Prefixing Improves Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.05970)
*Kazutoshi Shinoda,Nobukatsu Hojo,Kyosuke Nishida,Yoshihiro Yamazaki,Keita Suzuki,Hiroaki Sugiyama,Kuniko Saito*

Main category: cs.CL

TL;DR: A new inference-time method named Shoes-of-Others (SoO) prefixing is introduced to improve Theory of Mind (ToM) in large language models (LLMs). It works by guiding LLMs to think from others' perspectives and has been proven effective in enhancing ToM performance in conversational and narrative contexts without changes in the world state.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing inference-time methods for ToM which are specialized for inferring beliefs from contexts involving changes in the world state.

Method: Introducing a new inference-time method called Shoes-of-Others (SoO) prefixing. This method guides LLMs to think from others' perspectives by specifying the beginning of LLM outputs with 'Let's put ourselves in A's shoes.'

Result: The method consistently improves ToM across five categories of mental states when evaluated on two benchmarks assessing ToM in conversational and narrative contexts without changes in the world state.

Conclusion: SoO prefixing elicits faithful thoughts, thereby improving the ToM performance in LLMs.

Abstract: Recent studies have shown that Theory of Mind (ToM) in large language models
(LLMs) has not reached human-level performance yet. Since fine-tuning LLMs on
ToM datasets often degrades their generalization, several inference-time
methods have been proposed to enhance ToM in LLMs. However, existing
inference-time methods for ToM are specialized for inferring beliefs from
contexts involving changes in the world state. In this study, we present a new
inference-time method for ToM, Shoes-of-Others (SoO) prefixing, which makes
fewer assumptions about contexts and is applicable to broader scenarios. SoO
prefixing simply specifies the beginning of LLM outputs with ``Let's put
ourselves in A's shoes.'', where A denotes the target character's name. We
evaluate SoO prefixing on two benchmarks that assess ToM in conversational and
narrative contexts without changes in the world state and find that it
consistently improves ToM across five categories of mental states. Our analysis
suggests that SoO prefixing elicits faithful thoughts, thereby improving the
ToM performance.

</details>


### [48] [LTG at SemEval-2025 Task 10: Optimizing Context for Classification of Narrative Roles](https://arxiv.org/abs/2506.05976)
*Egil Rønningstad,Gaurav Negi*

Main category: cs.CL

TL;DR: Our approach uses a simple entity-oriented heuristic for context selection that enables text classification using models with limited context window, performing on par with or outperforming supervised fine-tuning with larger generative language models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of providing necessary segments from longer documents as context for classification with a masked language model in the SemEval 2025 shared task 10, subtask 1 on entity framing.

Method: A simple entity-oriented heuristic for context selection combined with the XLM-RoBERTa language model.

Result: On par with or outperforms Supervised Fine-Tuning with larger generative language models.

Conclusion: A simple entity-oriented heuristic for context selection can enable text classification using models with limited context window.

Abstract: Our contribution to the SemEval 2025 shared task 10, subtask 1 on entity
framing, tackles the challenge of providing the necessary segments from longer
documents as context for classification with a masked language model. We show
that a simple entity-oriented heuristics for context selection can enable text
classification using models with limited context window. Our context selection
approach and the XLM-RoBERTa language model is on par with, or outperforms,
Supervised Fine-Tuning with larger generative language models.

</details>


### [49] [Tau-Eval: A Unified Evaluation Framework for Useful and Private Text Anonymization](https://arxiv.org/abs/2506.05979)
*Gabriel Loiseau,Damien Sileo,Damien Riquet,Maxime Meyer,Marc Tommasi*

Main category: cs.CL

TL;DR: Tau-Eval is an open-source framework for evaluating text anonymization methods considering privacy and utility task sensitivity.


<details>
  <summary>Details</summary>
Motivation: The lack of a universal benchmark for assessing text anonymization techniques across different contexts.

Method: Developing an open-source framework named Tau-Eval.

Result: A Python library, code, documentation, and tutorials are publicly available.

Conclusion: Tau-Eval provides a comprehensive way to evaluate text anonymization methods.

Abstract: Text anonymization is the process of removing or obfuscating information from
textual data to protect the privacy of individuals. This process inherently
involves a complex trade-off between privacy protection and information
preservation, where stringent anonymization methods can significantly impact
the text's utility for downstream applications. Evaluating the effectiveness of
text anonymization proves challenging from both privacy and utility
perspectives, as there is no universal benchmark that can comprehensively
assess anonymization techniques across diverse, and sometimes contradictory
contexts. We present Tau-Eval, an open-source framework for benchmarking text
anonymization methods through the lens of privacy and utility task sensitivity.
A Python library, code, documentation and tutorials are publicly available.

</details>


### [50] [A Culturally-Rich Romanian NLP Dataset from "Who Wants to Be a Millionaire?" Videos](https://arxiv.org/abs/2506.05991)
*Alexandru-Gabriel Ganea,Antonia-Adelina Popovici,Adrian-Marius Dumitran*

Main category: cs.CL

TL;DR: This paper investigates the effect of cultural context on large language model performance using a new dataset from a Romanian game show.


<details>
  <summary>Details</summary>
Motivation: To examine how cultural context and data source influence the performance of large language models.

Method: An innovative process combining OCR, automated text extraction, and manual verification was used to create a new dataset from video recordings of a Romanian game show. This dataset was then used to benchmark state-of-the-art LLMs.

Result: State-of-the-art LLMs perform better on international questions than on Romanian-specific cultural questions. Machine translation and cross-lingual tests were conducted to further explore these differences.

Conclusion: The study concludes that cultural context and data source significantly affect the performance of large language models.

Abstract: Large Language Models (LLMs) demonstrate varying performance across languages
and cultural contexts. This study introduces a novel, culturally-rich,
multilingual dataset derived from video recordings of the Romanian game show
"Who Wants to Be a Millionaire?" (Vrei s\u{a} fii Milionar?). We employed an
innovative process combining optical character recognition (OCR), automated
text extraction, and manual verification to collect question-answer pairs,
enriching them with metadata including question domain (e.g., biology,
history), cultural relevance (Romanian-specific vs. international), and
difficulty. Benchmarking state-of-the-art LLMs, including Romanian-adapted
models, on this dataset revealed significant performance disparities: models
consistently achieve higher accuracy (80-95%) on international questions
compared to Romanian-specific cultural questions (50-75%). We further
investigate these differences through experiments involving machine translation
of Romanian questions into English and cross-lingual tests using a comparable
dataset in French. Our findings underscore the impact of cultural context and
data source on LLM performance and offer practical insights for building
robust, culturally-aware multilingual NLP systems, especially in educational
domains. The dataset is publicly available at Hugging Face.

</details>


### [51] [Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models](https://arxiv.org/abs/2506.06008)
*Peijie Liu,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 提出新的评估指标和动态CoT方法，提高大语言模型复杂推理任务的表现并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 探索链式思维（CoT）技术在不同任务上性能不一致的原因，并研究其潜在机制。

Method: 提出了基于token概率分布的两个指标，并结合逻辑回归模型实现动态选择CoT和直接回答的方法，还通过迁移开源模型的学习策略扩展到闭源模型。

Result: 提出的指标在评估CoT有效性方面达到了89.2%的准确率，而动态CoT方法在保持高准确率的同时减少了超过35%的token消耗。

Conclusion: 提出了一个新的指标来评估CoT在不同任务上的有效性，并引入了动态CoT方法，在保持高准确率的同时减少了超过35%的token消耗。

Abstract: Chain-of-Thought (CoT) technique has proven effective in improving the
performance of large language models (LLMs) on complex reasoning tasks.
However, the performance gains are inconsistent across different tasks, and the
underlying mechanism remains a long-standing research question. In this work,
we make a preliminary observation that the monotonicity of token probability
distributions may be correlated with the gains achieved through CoT reasoning.
Leveraging this insight, we propose two indicators based on the token
probability distribution to assess CoT effectiveness across different tasks. By
combining instance-level indicators with logistic regression model, we
introduce Dynamic CoT, a method that dynamically select between CoT and direct
answer. Furthermore, we extend Dynamic CoT to closed-source models by
transferring decision strategies learned from open-source models. Our
indicators for assessing CoT effectiveness achieve an accuracy of 89.2\%, and
Dynamic CoT reduces token consumption by more than 35\% while maintaining high
accuracy. Overall, our work offers a novel perspective on the underlying
mechanisms of CoT reasoning and provides a framework for its more efficient
deployment.

</details>


### [52] [Unlocking Recursive Thinking of LLMs: Alignment via Refinement](https://arxiv.org/abs/2506.06009)
*Haoke Zhang,Xiaobo Liang,Cunxiang Wang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个名为'Alignment via Refinement (AvR)'的方法，它通过不同iable学习技术优化精化感知奖励，从而提升LLMs在递归推理中的表现，实验显示其效果优于传统方法且效率高。


<details>
  <summary>Details</summary>
Motivation: 尽管OpenAI的o1系列模型展示了利用长形式Chain of Thought（CoT）可以大幅提高性能，但大型语言模型（LLMs）的递归思维能力仍然有限，尤其是在没有专家策展数据用于蒸馏的情况下。

Method: 提出了一种名为'Alignment via Refinement（AvR）'的新方法，该方法通过不同iable学习技术优化精化感知奖励，从而解锁LLMs在长形式Chain of Thought（CoT）中的递归推理潜力。

Result: 实验结果表明，AvR方法显著优于传统偏好优化方法。使用仅3k合成样本，AvR方法就能使LLaMA-3-8B-Instruct模型在AlpacaEval 2.0上的胜率提高超过20%。

Conclusion: AvR方法通过引入批评和改进操作的精化过程，显著提升了LLMs在递归推理方面的性能。实验表明，使用仅3k合成样本，AvR方法就使LLaMA-3-8B-Instruct模型在AlpacaEval 2.0上的胜率提高了超过20%，优于传统的偏好优化方法。

Abstract: The OpenAI o1-series models have demonstrated that leveraging long-form Chain
of Thought (CoT) can substantially enhance performance. However, the recursive
thinking capabilities of Large Language Models (LLMs) remain limited,
particularly in the absence of expert-curated data for distillation. In this
paper, we propose \textbf{AvR}: \textbf{Alignment via Refinement}, a novel
method aimed at unlocking the potential of LLMs for recursive reasoning through
long-form CoT. AvR introduces a refinement process that integrates criticism
and improvement actions, guided by differentiable learning techniques to
optimize \textbf{refinement-aware rewards}. As a result, the synthesized
multi-round data can be organized as a long refinement thought, further
enabling test-time scaling. Experimental results show that AvR significantly
outperforms conventional preference optimization methods. Notably, with only 3k
synthetic samples, our method boosts the performance of the LLaMA-3-8B-Instruct
model by over 20\% in win rate on AlpacaEval 2.0. Our code is available at
Github (https://github.com/Banner-Z/AvR.git).

</details>


### [53] [AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search](https://arxiv.org/abs/2506.06017)
*Yu Li,Lehui Li,Zhihao Wu,Qingmin Liao,Jianye Hao,Kun Shao,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: This work introduces a comprehensive framework to improve large language model agents' performance across diverse domains by addressing existing challenges in agent search methods.


<details>
  <summary>Details</summary>
Motivation: To design high-performing agentic systems while overcoming limitations in existing agent search methods.

Method: Propose a hierarchical search space, predictive value model, and hierarchical Monte Carlo Tree Search strategy.

Result: Achieves an average performance gain of 8.34% over state-of-the-art baselines and shows faster search progress with steeper improvement trajectories.

Conclusion: The proposed framework can effectively enhance the performance of large language model agents across various domains.

Abstract: Large language model (LLM) agents have demonstrated strong capabilities
across diverse domains. However, designing high-performing agentic systems
remains challenging. Existing agent search methods suffer from three major
limitations: (1) an emphasis on optimizing agentic workflows while
under-utilizing proven human-designed components such as memory, planning, and
tool use; (2) high evaluation costs, as each newly generated agent must be
fully evaluated on benchmarks; and (3) inefficient search in large search
space. In this work, we introduce a comprehensive framework to address these
challenges. First, We propose a hierarchical search space that jointly models
agentic workflow and composable functional components, enabling richer agentic
system designs. Building on this structured design space, we introduce a
predictive value model that estimates agent performance given agentic system
and task description, allowing for efficient, low-cost evaluation during the
search process. Finally, we present a hierarchical Monte Carlo Tree Search
(MCTS) strategy informed by uncertainty to guide the search. Experiments on
seven benchmarks, covering embodied, math, web, tool, and game, show that our
method achieves an average performance gain of 8.34\% over state-of-the-art
baselines and exhibits faster search progress with steeper improvement
trajectories. Code repo is available at
https://github.com/Ericccc02/AgentSwift.

</details>


### [54] [When to Trust Context: Self-Reflective Debates for Context Reliability](https://arxiv.org/abs/2506.06020)
*Zeqi Zhou,Fang Wu,Shayan Talaei,Haokai Zhao,Cheng Meixin,Tinson Xu,Amin Saberi,Yejin Choi*

Main category: cs.CL

TL;DR: A lightweight framework named SR-DCR is proposed to solve factual inconsistencies or hallucinations in large language models. It uses token-level self-confidence and an asymmetric multi-agent debate.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of factual inconsistencies or hallucinations caused by conflicts between parametric knowledge and contextual input in large language models.

Method: Integrating token-level self-confidence with an asymmetric multi-agent debate, including a critic who challenges a defender, and a judge model to evaluate the debate and determine the context's reliability.

Result: SR-DCR enhances robustness to misleading context while maintaining accuracy on trustworthy inputs, outperforming both classical debate and confidence-only baselines with minimal computational overhead.

Conclusion: The proposed method provides a lightweight solution to improve the reliability of large language models.

Abstract: Large language models frequently encounter conflicts between their parametric
knowledge and contextual input, often resulting in factual inconsistencies or
hallucinations. We propose Self-Reflective Debate for Contextual Reliability
(SR-DCR), a lightweight framework that integrates token-level self-confidence
with an asymmetric multi-agent debate to adjudicate such conflicts. A critic,
deprived of context, challenges a defender who argues from the given passage; a
judge model evaluates the debate and determines the context's reliability. The
final answer is selected by combining the verdict with model confidence.
Experiments on the ClashEval benchmark demonstrate that SR-DCR consistently
enhances robustness to misleading context while maintaining accuracy on
trustworthy inputs, outperforming both classical debate and confidence-only
baselines with minimal computational overhead. The code is available at
https://github.com/smiles724/Self-Reflective-Debates.

</details>


### [55] [Large Language Models are Demonstration Pre-Selectors for Themselves](https://arxiv.org/abs/2506.06033)
*Jiarui Jin,Yuwei Wu,Haoxuan Li,Xiaoting He,Weinan Zhang,Yiming Yang,Yong Yu,Jun Wang,Mengyue Yang*

Main category: cs.CL

TL;DR: 提出了一种新的预选框架FEEDER，用于减少上下文学习中大型语言模型的训练数据大小，提高效率并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性或多样性分数选择演示的方法由于每次查询都需要从大规模数据集中重复检索，计算成本高。

Method: 提出了一种新的预选框架FEEDER，包括在预选阶段引入的“充分性”和“必要性”度量以及一种树基算法来高效识别代表性样本。

Result: 实验表明，FEEDER可以减少超过20%的训练数据大小，同时保持性能，并且可以无缝集成各种下游演示选择策略。

Conclusion: FEEDER通过预选代表性示例子集提高了效率，同时保持了上下文学习中的性能，并且在微调大型语言模型时也表现良好。

Abstract: In-context learning (ICL) with large language models (LLMs) delivers strong
few-shot performance by choosing few-shot demonstrations from the entire
training data. However, existing ICL methods, which rely on similarity or
diversity scores to choose demonstrations, incur high computational costs due
to repeatedly retrieval from large-scale datasets for each query. To this end,
we propose FEEDER (FEw yet Essential Demonstration prE-selectoR), a novel
pre-selection framework that identifies a representative subset of
demonstrations containing the most representative examples in the training
data, tailored to specific LLMs. To construct this subset, we introduce the
"sufficiency" and "necessity" metrics in the pre-selection stage and design a
tree-based algorithm to identify representative examples efficiently. Once
pre-selected, this representative subset can effectively replace the full
training data, improving efficiency while maintaining comparable performance in
ICL. Additionally, our pre-selected subset also benefits fine-tuning LLMs,
where we introduce a bi-level optimization method that enhances training
efficiency without sacrificing performance. Experiments with LLMs ranging from
300M to 8B parameters show that FEEDER can reduce training data size by over
20% while maintaining performance and seamlessly integrating with various
downstream demonstration selection strategies in ICL.

</details>


### [56] [MATP-BENCH: Can MLLM Be a Good Automated Theorem Prover for Multimodal Problems?](https://arxiv.org/abs/2506.06034)
*Zhitao He,Zongwei Lyu,Dazhong Chen,Dadi Guo,Yi R. Fung*

Main category: cs.CL

TL;DR: This paper introduces MATP-BENCH, a new multimodal benchmark to evaluate Multimodal Large Language Models (MLLMs) as Automated Theorem Provers.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of MLLMs in the multimodal domain as Automated Theorem Provers.

Method: Introduce MATP-BENCH, a multimodal, multi-level, and multi-language benchmark consisting of 1056 multimodal theorems from different levels of mathematics.

Result: MATP-BENCH evaluates advanced multimodal language models and finds existing methods can only solve a limited number of the problems.

Conclusion: MATP-BENCH presents an open challenge for research on automated theorem proving in the multimodal domain.

Abstract: Numerous theorems, such as those in geometry, are often presented in
multimodal forms (e.g., diagrams). Humans benefit from visual reasoning in such
settings, using diagrams to gain intuition and guide the proof process. Modern
Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in solving a wide range of mathematical problems. However, the
potential of MLLMs as Automated Theorem Provers (ATPs), specifically in the
multimodal domain, remains underexplored. In this paper, we introduce the
Multimodal Automated Theorem Proving benchmark (MATP-BENCH), a new Multimodal,
Multi-level, and Multi-language benchmark designed to evaluate MLLMs in this
role as multimodal automated theorem provers. MATP-BENCH consists of 1056
multimodal theorems drawn from high school, university, and competition-level
mathematics. All these multimodal problems are accompanied by formalizations in
Lean 4, Coq and Isabelle, thus making the benchmark compatible with a wide
range of theorem-proving frameworks. MATP-BENCH requires models to integrate
sophisticated visual understanding with mastery of a broad spectrum of
mathematical knowledge and rigorous symbolic reasoning to generate formal
proofs. We use MATP-BENCH to evaluate a variety of advanced multimodal language
models. Existing methods can only solve a limited number of the MATP-BENCH
problems, indicating that this benchmark poses an open challenge for research
on automated theorem proving.

</details>


### [57] [Hey, That's My Data! Label-Only Dataset Inference in Large Language Models](https://arxiv.org/abs/2506.06057)
*Chen Xiong,Zihao Wang,Rui Zhu,Tsung-Yi Ho,Pin-Yu Chen,Jingwei Xiong,Haixu Tang,Lucila Ohno-Machado*

Main category: cs.CL

TL;DR: A label-only framework named CatShift is introduced to identify if a dataset was used in training large language models, leveraging the phenomenon of catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing dataset-inference methods depend on internal model logits which are now being withheld by leading LLMs, creating a need for label-only approaches.

Method: CatShift fine-tunes a model on portions of a suspicious dataset and compares output shifts with those from a non-member validation set to determine dataset membership.

Result: Experiments on open-source and API-based LLMs confirm CatShift's effectiveness in logit-inaccessible settings.

Conclusion: CatShift provides a practical solution for detecting unauthorized use of datasets in LLM training, helping protect proprietary data.

Abstract: Large Language Models (LLMs) have revolutionized Natural Language Processing
by excelling at interpreting, reasoning about, and generating human language.
However, their reliance on large-scale, often proprietary datasets poses a
critical challenge: unauthorized usage of such data can lead to copyright
infringement and significant financial harm. Existing dataset-inference methods
typically depend on log probabilities to detect suspicious training material,
yet many leading LLMs have begun withholding or obfuscating these signals. This
reality underscores the pressing need for label-only approaches capable of
identifying dataset membership without relying on internal model logits.
  We address this gap by introducing CatShift, a label-only dataset-inference
framework that capitalizes on catastrophic forgetting: the tendency of an LLM
to overwrite previously learned knowledge when exposed to new data. If a
suspicious dataset was previously seen by the model, fine-tuning on a portion
of it triggers a pronounced post-tuning shift in the model's outputs;
conversely, truly novel data elicits more modest changes. By comparing the
model's output shifts for a suspicious dataset against those for a known
non-member validation set, we statistically determine whether the suspicious
set is likely to have been part of the model's original training corpus.
Extensive experiments on both open-source and API-based LLMs validate
CatShift's effectiveness in logit-inaccessible settings, offering a robust and
practical solution for safeguarding proprietary data.

</details>


### [58] [Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.06060)
*Yingqi Hu,Zhuo Zhang,Jingyuan Zhang,Lizhen Qu,Zenglin Xu*

Main category: cs.CL

TL;DR: Investigate the vulnerability of FedLLMs to training data extraction attacks.


<details>
  <summary>Details</summary>
Motivation: Achieving strong model performance while preserving data privacy in sensitive domains.

Method: Introduce simple yet effective extraction attack algorithms designed for FedLLMs.

Result: Our method can extract up to 56.57% of victim-exclusive PII.

Conclusion: There is a pressing need for robust defense strategies and a new benchmark and evaluation framework for future research in privacy-preserving federated learning.

Abstract: Federated fine-tuning of large language models (FedLLMs) presents a promising
approach for achieving strong model performance while preserving data privacy
in sensitive domains. However, the inherent memorization ability of LLMs makes
them vulnerable to training data extraction attacks. To investigate this risk,
we introduce simple yet effective extraction attack algorithms specifically
designed for FedLLMs. In contrast to prior "verbatim" extraction attacks, which
assume access to fragments from all training data, our approach operates under
a more realistic threat model, where the attacker only has access to a single
client's data and aims to extract previously unseen personally identifiable
information (PII) from other clients. This requires leveraging contextual
prefixes held by the attacker to generalize across clients. To evaluate the
effectiveness of our approaches, we propose two rigorous metrics-coverage rate
and efficiency-and extend a real-world legal dataset with PII annotations
aligned with CPIS, GDPR, and CCPA standards, achieving 89.9% human-verified
precision. Experimental results show that our method can extract up to 56.57%
of victim-exclusive PII, with "Address," "Birthday," and "Name" being the most
vulnerable categories. Our findings underscore the pressing need for robust
defense strategies and contribute a new benchmark and evaluation framework for
future research in privacy-preserving federated learning.

</details>


### [59] [Zero-Shot Detection of LLM-Generated Code via Approximated Task Conditioning](https://arxiv.org/abs/2506.06069)
*Maor Ashkenazi,Ofir Brenner,Tal Furman Shohet,Eran Treister*

Main category: cs.CL

TL;DR: 提出了一种新的零样本检测方法ATC，它在无需访问生成LLM或原始任务提示的情况下实现了最先进的结果，并且跨编程语言具有通用性。


<details>
  <summary>Details</summary>
Motivation: 检测大型语言模型（LLM）生成的代码是一个日益严峻的挑战，对安全、知识产权和学术诚信都有影响。

Method: 提出了一种新的零样本检测方法，称为ATC，它近似于用于生成给定代码片段的原始任务，并在近似的任务条件下的令牌级别熵进行评估。

Result: ATC在基准测试中实现了最先进的结果，并且跨编程语言（包括Python、CPP和Java）具有通用性。

Conclusion: 提出了一个名为ATC的新方法，该方法在无需访问生成LLM或原始任务提示的情况下实现了最先进的结果，并且跨Python、CPP和Java等编程语言具有通用性。

Abstract: Detecting Large Language Model (LLM)-generated code is a growing challenge
with implications for security, intellectual property, and academic integrity.
We investigate the role of conditional probability distributions in improving
zero-shot LLM-generated code detection, when considering both the code and the
corresponding task prompt that generated it. Our key insight is that when
evaluating the probability distribution of code tokens using an LLM, there is
little difference between LLM-generated and human-written code. However,
conditioning on the task reveals notable differences. This contrasts with
natural language text, where differences exist even in the unconditional
distributions. Leveraging this, we propose a novel zero-shot detection approach
that approximates the original task used to generate a given code snippet and
then evaluates token-level entropy under the approximated task conditioning
(ATC). We further provide a mathematical intuition, contextualizing our method
relative to previous approaches. ATC requires neither access to the generator
LLM nor the original task prompts, making it practical for real-world
applications. To the best of our knowledge, it achieves state-of-the-art
results across benchmarks and generalizes across programming languages,
including Python, CPP, and Java. Our findings highlight the importance of
task-level conditioning for LLM-generated code detection. The supplementary
materials and code are available at https://github.com/maorash/ATC, including
the dataset gathering implementation, to foster further research in this area.

</details>


### [60] [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091)
*Qinyue Zheng,Salman Abdullah,Sam Rawal,Cyril Zakka,Sophie Ostmeier,Maximilian Purk,Eduardo Reis,Eric J. Topol,Jure Leskovec,Michael Moor*

Main category: cs.CL

TL;DR: MIRIAD is a large-scale, curated corpus that improves the accuracy of LLMs in medical QA tasks and enhances their ability to detect medical hallucinations.


<details>
  <summary>Details</summary>
Motivation: To ground LLMs in high-quality medical knowledge and address the challenges of existing RAG pipelines relying on raw, unstructured medical text, which can be noisy, uncurated and difficult for LLMs to effectively leverage.

Method: Introducing MIRIAD, a large-scale, curated corpus of 5,821,948 medical QA pairs, each rephrased from and grounded in a passage from peer-reviewed medical literature using a semi-automated pipeline combining LLM generation, filtering, grounding, and human annotation.

Result: Augmenting LLMs with MIRIAD improves accuracy up to 6.7% compared to unstructured RAG baselines with the same source corpus and with the same amount of retrieved text. Moreover, MIRIAD improved the ability of LLMs to detect medical hallucinations by 22.5 to 37% (increase in F1 score).

Conclusion: MIRIAD, a large-scale, curated corpus of 5,821,948 medical QA pairs, improves the accuracy of LLMs in medical QA tasks and enhances their ability to detect medical hallucinations.

Abstract: LLMs are bound to transform healthcare with advanced decision support and
flexible chat assistants. However, LLMs are prone to generate inaccurate
medical content. To ground LLMs in high-quality medical knowledge, LLMs have
been equipped with external knowledge via RAG, where unstructured medical
knowledge is split into small text chunks that can be selectively retrieved and
integrated into the LLMs context. Yet, existing RAG pipelines rely on raw,
unstructured medical text, which can be noisy, uncurated and difficult for LLMs
to effectively leverage. Systematic approaches to organize medical knowledge to
best surface it to LLMs are generally lacking. To address these challenges, we
introduce MIRIAD, a large-scale, curated corpus of 5,821,948 medical QA pairs,
each rephrased from and grounded in a passage from peer-reviewed medical
literature using a semi-automated pipeline combining LLM generation, filtering,
grounding, and human annotation. Unlike prior medical corpora, which rely on
unstructured text, MIRIAD encapsulates web-scale medical knowledge in an
operationalized query-response format, which enables more targeted retrieval.
Experiments on challenging medical QA benchmarks show that augmenting LLMs with
MIRIAD improves accuracy up to 6.7% compared to unstructured RAG baselines with
the same source corpus and with the same amount of retrieved text. Moreover,
MIRIAD improved the ability of LLMs to detect medical hallucinations by 22.5 to
37% (increase in F1 score). We further introduce MIRIAD-Atlas, an interactive
map of MIRIAD spanning 56 medical disciplines, enabling clinical users to
visually explore, search, and refine medical knowledge. MIRIAD promises to
unlock a wealth of down-stream applications, including medical information
retrievers, enhanced RAG applications, and knowledge-grounded chat interfaces,
which ultimately enables more reliable LLM applications in healthcare.

</details>


### [61] [Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning](https://arxiv.org/abs/2506.06093)
*Atharv Kulkarni,Vivek Srikumar*

Main category: cs.CL

TL;DR: This paper investigates using reinforcement learning with execution-based feedback to improve SQL query generation from natural language questions.


<details>
  <summary>Details</summary>
Motivation: Instead of using supervised fine tuning with text-code pairs, can we tune a model by having it interact with a database engine?

Method: We frame this problem as a reinforcement learning problem where the model receives execution-based feedback from the environment in the form of scalar rewards.

Result: With only weak supervision in the form of question-answer pairs, RL-tuning improves the accuracy of model generated SQL code from 31.49 to 49.83 while reducing error percentage from 25.43% to 14.71.

Conclusion: Our work demonstrates the potential of using execution-based feedback to improve symbolic reasoning capabilities of LLMs.

Abstract: In this work, we study the problem of code generation with a large language
model (LLM), with a focus on generating SQL queries from natural language
questions. We ask: Instead of using supervised fine tuning with text-code
pairs, can we tune a model by having it interact with a database engine? We
frame this problem as a reinforcement learning problem where the model receives
execution-based feedback from the environment in the form of scalar rewards.
These rewards penalize execution failures and assign positive values when a
query returns a correct answer. We use the rewards within the Group Relative
Policy Optimization (GRPO) framework. We use a tabular reasoning benchmark to
test and evaluate our findings. We find that with only weak supervision in the
form of question-answer pairs, RL-tuning improves the accuracy of model
generated SQL code from 31.49 to 49.83 while reducing error percentage from
25.43% to 14.71%. This improvement allowed the model nearly match the
performance performance to the larger SQLCoder-70B model. Our work demonstrates
the potential of using execution-based feedback to improve symbolic reasoning
capabilities of LLMs.

</details>


### [62] [Bridging the Gap: In-Context Learning for Modeling Human Disagreement](https://arxiv.org/abs/2506.06113)
*Benedetta Muscato,Yue Li,Gizem Gezici,Zhixue Zhao,Fosca Giannotti*

Main category: cs.CL

TL;DR: 本研究考察了大型语言模型在主观任务中捕捉多个视角和反映注释者分歧的能力，并发现零样本设置下多视角生成可行，但少量样本设置下通常失败，提示设计和演示选择对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨大型语言模型是否能够捕捉多个视角并反映注释器在主观任务中的分歧，例如仇恨言论和攻击性语言检测。传统方法通常依赖于通过多数投票聚合的标签，这可能掩盖主观注释中存在的分歧。

Method: 本研究在零样本和少量样本设置中使用上下文学习（ICL），评估四个开源LLMs在三种标签建模策略下的表现：聚合硬标签、离散硬标签和软标签。此外，在少量样本提示中，评估基于文本相似性（BM25、PLM-based）、注释分歧（熵）、组合排名和示例排序策略（随机与基于课程）。

Result: 结果表明，在零样本设置下多视角生成是可行的，但在少量样本设置下通常无法捕捉到人类判断的全部光谱。提示设计和演示选择显著影响性能，尽管示例排序的影响有限。

Conclusion: 本研究表明，在零样本设置下多视角生成是可行的，但在少量样本设置下通常无法捕捉到人类判断的全部光谱。提示设计和演示选择显著影响性能，尽管示例排序的影响有限。这些研究结果强调了使用LLMs建模主观性的挑战以及构建更具视角意识和社会智能模型的重要性。

Abstract: Large Language Models (LLMs) have shown strong performance on NLP
classification tasks. However, they typically rely on aggregated labels-often
via majority voting-which can obscure the human disagreement inherent in
subjective annotations. This study examines whether LLMs can capture multiple
perspectives and reflect annotator disagreement in subjective tasks such as
hate speech and offensive language detection. We use in-context learning (ICL)
in zero-shot and few-shot settings, evaluating four open-source LLMs across
three label modeling strategies: aggregated hard labels, and disaggregated hard
and soft labels. In few-shot prompting, we assess demonstration selection
methods based on textual similarity (BM25, PLM-based), annotation disagreement
(entropy), a combined ranking, and example ordering strategies (random vs.
curriculum-based). Results show that multi-perspective generation is viable in
zero-shot settings, while few-shot setups often fail to capture the full
spectrum of human judgments. Prompt design and demonstration selection notably
affect performance, though example ordering has limited impact. These findings
highlight the challenges of modeling subjectivity with LLMs and the importance
of building more perspective-aware, socially intelligent models.

</details>


### [63] [Phonetically-Augmented Discriminative Rescoring for Voice Search Error Correction](https://arxiv.org/abs/2506.06117)
*Christophe Van Gysel,Maggie Wu,Lyan Verwimp,Caglar Tirkaz,Marco Bertola,Zhihong Lei,Youssef Oualil*

Main category: cs.CL

TL;DR: This paper presents a phonetic correction system to improve word error rate for recognizing recent or infrequent movie titles in end-to-end ASR systems.


<details>
  <summary>Details</summary>
Motivation: To address the issue of poor recognition for recent or infrequent movie titles in end-to-end ASR systems due to insufficient training data.

Method: Propose a phonetic correction system with a phonetic search generating alternatives and a rescoring component selecting the final output.

Result: Improves word error rate by 4.4-7.6% relative on benchmarks of popular movie titles compared to competitive baselines.

Conclusion: The proposed phonetic correction system effectively enhances the performance of end-to-end ASR systems for recognizing challenging words like recent or infrequent movie titles.

Abstract: End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using
paired audio-text samples that are expensive to obtain, since high-quality
ground-truth data requires human annotators. Voice search applications, such as
digital media players, leverage ASR to allow users to search by voice as
opposed to an on-screen keyboard. However, recent or infrequent movie titles
may not be sufficiently represented in the E2E ASR system's training data, and
hence, may suffer poor recognition.
  In this paper, we propose a phonetic correction system that consists of (a) a
phonetic search based on the ASR model's output that generates phonetic
alternatives that may not be considered by the E2E system, and (b) a rescorer
component that combines the ASR model recognition and the phonetic
alternatives, and select a final system output.
  We find that our approach improves word error rate between 4.4 and 7.6%
relative on benchmarks of popular movie titles over a series of competitive
baselines.

</details>


### [64] [Let's CONFER: A Dataset for Evaluating Natural Language Inference Models on CONditional InFERence and Presupposition](https://arxiv.org/abs/2506.06133)
*Tara Azin,Daniel Dumitrescu,Diana Inkpen,Raj Singh*

Main category: cs.CL

TL;DR: This study introduces CONFER, a new dataset for evaluating NLI models' ability to handle fine-grained pragmatic inferences in conditional sentences.


<details>
  <summary>Details</summary>
Motivation: To explore NLI models' capacity to deal with presupposition in conditionals which remains underexplored.

Method: Assessing the performance of four NLI models and evaluating LLMs in zero-shot and few-shot prompting settings.

Result: NLI models have difficulty with presuppositional reasoning in conditionals and fine-tuning on existing NLI datasets doesn't necessarily improve their performance.

Conclusion: The study highlights the limitations of current NLI models in handling fine-grained pragmatic inferences in conditional sentences.

Abstract: Natural Language Inference (NLI) is the task of determining whether a
sentence pair represents entailment, contradiction, or a neutral relationship.
While NLI models perform well on many inference tasks, their ability to handle
fine-grained pragmatic inferences, particularly presupposition in conditionals,
remains underexplored. In this study, we introduce CONFER, a novel dataset
designed to evaluate how NLI models process inference in conditional sentences.
We assess the performance of four NLI models, including two pre-trained models,
to examine their generalization to conditional reasoning. Additionally, we
evaluate Large Language Models (LLMs), including GPT-4o, LLaMA, Gemma, and
DeepSeek-R1, in zero-shot and few-shot prompting settings to analyze their
ability to infer presuppositions with and without prior context. Our findings
indicate that NLI models struggle with presuppositional reasoning in
conditionals, and fine-tuning on existing NLI datasets does not necessarily
improve their performance.

</details>


### [65] [semantic-features: A User-Friendly Tool for Studying Contextual Word Embeddings in Interpretable Semantic Spaces](https://arxiv.org/abs/2506.06169)
*Jwalanthi Ranganathan,Rohan Jha,Kanishka Misra,Kyle Mahowald*

Main category: cs.CL

TL;DR: This paper introduces semantic-features, a library for studying contextualized word embeddings of LMs, and demonstrates its effectiveness in measuring the contextual effect of dative constructions on semantic interpretation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study the contextualized word embeddings of LMs by projecting them into interpretable spaces, specifically examining the choice of dative construction on the semantic interpretation of utterances.

Method: The method involves using the semantic-features library to project contextualized word embeddings of LMs into interpretable spaces and measuring the contextual effect of dative constructions on semantic interpretation.

Result: The result shows that the three masked language models' contextualized word embeddings demonstrate the expected sensitivities, suggesting the utility of the semantic-features tool.

Conclusion: The study concludes that the contextualized word embeddings of three masked language models show the expected sensitivities, indicating the potential usefulness of the semantic-features tool.

Abstract: We introduce semantic-features, an extensible, easy-to-use library based on
Chronis et al. (2023) for studying contextualized word embeddings of LMs by
projecting them into interpretable spaces. We apply this tool in an experiment
where we measure the contextual effect of the choice of dative construction
(prepositional or double object) on the semantic interpretation of utterances
(Bresnan, 2007). Specifically, we test whether "London" in "I sent London the
letter." is more likely to be interpreted as an animate referent (e.g., as the
name of a person) than in "I sent the letter to London." To this end, we devise
a dataset of 450 sentence pairs, one in each dative construction, with
recipients being ambiguous with respect to person-hood vs. place-hood. By
applying semantic-features, we show that the contextualized word embeddings of
three masked language models show the expected sensitivities. This leaves us
optimistic about the usefulness of our tool.

</details>


### [66] [Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach](https://arxiv.org/abs/2506.06175)
*James Ford,Anthony Rios*

Main category: cs.CL

TL;DR: This paper presents a multi-agent pipeline for generating executable code from natural-language chart descriptions, reducing execution errors to 4.5% while outperforming fine-tuned baselines on two benchmarks. However, it highlights issues with hallucinations and accessibility.


<details>
  <summary>Details</summary>
Motivation: To investigate the source of persistent error rates in generated scripts and explore ways to improve them.

Method: Proposing a lightweight multi-agent pipeline that separates drafting, execution, repair, and judgment using an off-the-shelf GPT-4o-mini model.

Result: The system reduced execution errors to 4.5% on Text2Chart31 and 4.6% on ChartX within three repair iterations, outperforming baselines significantly. However, it found issues with hallucinations and accessibility.

Conclusion: Execution success seems largely solved, but future work should focus on improving chart aesthetics, semantic fidelity, and accessibility.

Abstract: Large language models can translate natural-language chart descriptions into
runnable code, yet approximately 15\% of the generated scripts still fail to
execute, even after supervised fine-tuning and reinforcement learning. We
investigate whether this persistent error rate stems from model limitations or
from reliance on a single-prompt design. To explore this, we propose a
lightweight multi-agent pipeline that separates drafting, execution, repair,
and judgment, using only an off-the-shelf GPT-4o-mini model. On the
\textsc{Text2Chart31} benchmark, our system reduces execution errors to 4.5\%
within three repair iterations, outperforming the strongest fine-tuned baseline
by nearly 5 percentage points while requiring significantly less compute.
Similar performance is observed on the \textsc{ChartX} benchmark, with an error
rate of 4.6\%, demonstrating strong generalization. Under current benchmarks,
execution success appears largely solved. However, manual review reveals that 6
out of 100 sampled charts contain hallucinations, and an LLM-based
accessibility audit shows that only 33.3\% (\textsc{Text2Chart31}) and 7.2\%
(\textsc{ChartX}) of generated charts satisfy basic colorblindness guidelines.
These findings suggest that future work should shift focus from execution
reliability toward improving chart aesthetics, semantic fidelity, and
accessibility.

</details>


### [67] [Detecting Voice Phishing with Precision: Fine-Tuning Small Language Models](https://arxiv.org/abs/2506.06180)
*Ju Yong Sim,Seong Hwan Kim*

Main category: cs.CL

TL;DR: 我们开发了一个由Llama3微调的语音网络钓鱼检测器，展示了其在对抗性条件下的性能，并提出了一种新的转录生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前对于小语言模型在语音网络钓鱼检测中的应用研究较少，同时缺乏有效的评估标准和对抗性测试数据集来测试这些模型的鲁棒性。

Method: 通过微调Llama3语言模型，我们在提示中提供精心设计的VP评估标准并应用了思维链(CoT)技术。此外，为了应对缺乏VP转录文本的问题，我们参考现有或新的VP技术创建了转录文本。

Result: 实验结果显示，包含VP评估标准提示的Llama3-8B模型在小语言模型中表现最佳，并且与基于GPT-4的VP检测器性能相当。

Conclusion: 我们发现，使用包含语音网络钓鱼(VP)评估标准提示的Llama3-8B模型在小语言模型中表现最佳，并且与基于GPT-4的VP检测器性能相当。这表明，在提示中加入人类专家知识比对小语言模型使用思维链技术在VP检测中更有效。

Abstract: We develop a voice phishing (VP) detector by fine-tuning Llama3, a
representative open-source, small language model (LM). In the prompt, we
provide carefully-designed VP evaluation criteria and apply the
Chain-of-Thought (CoT) technique. To evaluate the robustness of LMs and
highlight differences in their performance, we construct an adversarial test
dataset that places the models under challenging conditions. Moreover, to
address the lack of VP transcripts, we create transcripts by referencing
existing or new types of VP techniques. We compare cases where evaluation
criteria are included, the CoT technique is applied, or both are used together.
In the experiment, our results show that the Llama3-8B model, fine-tuned with a
dataset that includes a prompt with VP evaluation criteria, yields the best
performance among small LMs and is comparable to that of a GPT-4-based VP
detector. These findings indicate that incorporating human expert knowledge
into the prompt is more effective than using the CoT technique for small LMs in
VP detection.

</details>


### [68] [Building Models of Neurological Language](https://arxiv.org/abs/2506.06208)
*Henry Watkins*

Main category: cs.CL

TL;DR: Developed domain-specific language models for neurology using retrieval-augmented generation and representational models; created datasets and tools.


<details>
  <summary>Details</summary>
Motivation: To create specialized language models for neurology that can be securely and locally deployed.

Method: Adapted open-source and commercial medical LLMs, used RAG, representational models, created datasets, tools for multi-word expression extraction, and graph-based analyses.

Result: Neurology-specific datasets, tools for multi-word expression extraction, graph-based analyses of medical terminology, scripts, and Docker containers for local hosting were produced. Performance metrics and graph community results were reported.

Conclusion: Future work may involve multimodal models using open-source architectures like phi-4.

Abstract: This report documents the development and evaluation of domain-specific
language models for neurology. Initially focused on building a bespoke model,
the project adapted to rapid advances in open-source and commercial medical
LLMs, shifting toward leveraging retrieval-augmented generation (RAG) and
representational models for secure, local deployment. Key contributions include
the creation of neurology-specific datasets (case reports, QA sets,
textbook-derived data), tools for multi-word expression extraction, and
graph-based analyses of medical terminology. The project also produced scripts
and Docker containers for local hosting. Performance metrics and graph
community results are reported, with future possible work open for multimodal
models using open-source architectures like phi-4.

</details>


### [69] [PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts](https://arxiv.org/abs/2506.06211)
*Hengzhi Li,Brendon Jiang,Alexander Naehu,Regan Song,Justin Zhang,Megan Tjandrasuwita,Chanakya Ekbote,Steven-Shine Chen,Adithya Balachandran,Wei Dai,Rebecca Chang,Paul Pu Liang*

Main category: cs.CL

TL;DR: Introduce PuzzleWorld, a benchmark for assessing open-ended, creative multimodal reasoning, revealing current models' limitations.


<details>
  <summary>Details</summary>
Motivation: To test the performance of state-of-the-art models on open-ended, creative multimodal reasoning tasks.

Method: Introduce PuzzleWorld, a large-scale benchmark of 667 puzzlehunt-style problems with detailed annotations.

Result: Most models achieve low final answer accuracy, but fine-tuning on reasoning traces improves stepwise reasoning.

Conclusion: Current models perform poorly on the open-ended and creative multimodal reasoning required by puzzlehunts.

Abstract: Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined
problem definitions. In contrast to conventional reasoning benchmarks
consisting of tasks with clear instructions, puzzlehunts require models to
discover the underlying problem structure from multimodal evidence and
iterative reasoning, mirroring real-world domains such as scientific discovery,
exploratory data analysis, or investigative problem-solving. Despite recent
progress in foundation models, their performance on such open-ended settings
remains largely untested. In this paper, we introduce PuzzleWorld, a
large-scale benchmark of 667 puzzlehunt-style problems designed to assess
step-by-step, open-ended, and creative multimodal reasoning. Each puzzle is
annotated with the final solution, detailed reasoning traces, and cognitive
skill labels, enabling holistic benchmarking and fine-grained diagnostic
analysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,
with the best model solving only 14% of puzzles and reaching 40% stepwise
accuracy. To demonstrate the value of our reasoning annotations, we show that
fine-tuning a small model on reasoning traces improves stepwise reasoning from
4% to 11%, while training on final answers alone degrades performance to near
zero. Our error analysis reveals that current models exhibit myopic reasoning,
are bottlenecked by the limitations of language-based inference, and lack
sketching capabilities crucial for visual and spatial reasoning. We release
PuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on
building more general, open-ended, and creative reasoning systems.

</details>


### [70] [Can Theoretical Physics Research Benefit from Language Agents?](https://arxiv.org/abs/2506.06214)
*Sirui Lu,Zhijing Jin,Terry Jingchen Zhang,Pavel Kos,J. Ignacio Cirac,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) have great potential in accelerating theoretical, computational, and applied physics research when combined with domain knowledge and tools. However, there are gaps in physical intuition, constraint satisfaction, and reliable reasoning that need to be addressed. Future physics-specialized LLMs could handle multimodal data, propose testable hypotheses, and design experiments. To achieve this, challenges such as ensuring physical consistency and developing robust verification methods must be overcome.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of integrating Large Language Models (LLMs) with domain knowledge and tools in theoretical physics research.

Method: Analyzing current LLM capabilities for physics, including mathematical reasoning and code generation, while identifying critical gaps.

Result: Identified gaps include physical intuition, constraint satisfaction, and reliable reasoning. Future physics-specialized LLMs could handle multimodal data, propose testable hypotheses, and design experiments.

Conclusion: Collaborative efforts between physics and AI communities are needed to address challenges like physical consistency and robust verification methods to advance scientific discovery in physics.

Abstract: Large Language Models (LLMs) are rapidly advancing across diverse domains,
yet their application in theoretical physics research is not yet mature. This
position paper argues that LLM agents can potentially help accelerate
theoretical, computational, and applied physics when properly integrated with
domain knowledge and toolbox. We analyze current LLM capabilities for physics
-- from mathematical reasoning to code generation -- identifying critical gaps
in physical intuition, constraint satisfaction, and reliable reasoning. We
envision future physics-specialized LLMs that could handle multimodal data,
propose testable hypotheses, and design experiments. Realizing this vision
requires addressing fundamental challenges: ensuring physical consistency, and
developing robust verification methods. We call for collaborative efforts
between physics and AI communities to help advance scientific discovery in
physics.

</details>


### [71] [Explaining Matters: Leveraging Definitions and Semantic Expansion for Sexism Detection](https://arxiv.org/abs/2506.06238)
*Sahrish Khan,Arshad Jhumka,Gabriele Pergola*

Main category: cs.CL

TL;DR: This paper addresses sexism detection in online content by proposing two prompt-based data augmentation methods and an ensemble strategy to enhance model performance.


<details>
  <summary>Details</summary>
Motivation: To overcome data sparsity and nuanced nature of sexist language in automated sexism detection systems.

Method: Definition-based Data Augmentation (DDA) and Contextual Semantic Expansion (CSE) for data augmentation and an ensemble strategy for fine-grained classification.

Result: State-of-the-art performance with significant improvements in macro F1 scores for both binary and fine-grained classification tasks.

Conclusion: Proposed methods effectively improve the detection of sexism in online content, particularly addressing issues of data sparsity and annotator disagreement.

Abstract: The detection of sexism in online content remains an open problem, as harmful
language disproportionately affects women and marginalized groups. While
automated systems for sexism detection have been developed, they still face two
key challenges: data sparsity and the nuanced nature of sexist language. Even
in large, well-curated datasets like the Explainable Detection of Online Sexism
(EDOS), severe class imbalance hinders model generalization. Additionally, the
overlapping and ambiguous boundaries of fine-grained categories introduce
substantial annotator disagreement, reflecting the difficulty of interpreting
nuanced expressions of sexism. To address these challenges, we propose two
prompt-based data augmentation techniques: Definition-based Data Augmentation
(DDA), which leverages category-specific definitions to generate
semantically-aligned synthetic examples, and Contextual Semantic Expansion
(CSE), which targets systematic model errors by enriching examples with
task-specific semantic features. To further improve reliability in fine-grained
classification, we introduce an ensemble strategy that resolves prediction ties
by aggregating complementary perspectives from multiple language models. Our
experimental evaluation on the EDOS dataset demonstrates state-of-the-art
performance across all tasks, with notable improvements of macro F1 by 1.5
points for binary classification (Task A) and 4.1 points for fine-grained
classification (Task C).

</details>


### [72] [Bridging External and Parametric Knowledge: Mitigating Hallucination of LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge](https://arxiv.org/abs/2506.06240)
*Yi Sui,Chaozhuo Li,Chen Zhang,Dawei song,Qiuchi Li*

Main category: cs.CL

TL;DR: A new method called DSSP-RAG improves retrieval-augmented generation by refining self-attention and introducing hallucination detection and an Energy Quotient.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of degraded performance and stability in traditional RAG methods due to knowledge conflicts between LLMs and external knowledge.

Method: Dual-Stream Knowledge-Augmented Framework for Shared-Private Semantic Synergy (DSSP-RAG), which refines self-attention into mixed-attention, introduces hallucination detection based on cognitive uncertainty, and uses Energy Quotient based on attention difference matrices.

Result: Superior performance over strong baselines on benchmark datasets.

Conclusion: DSSP-RAG effectively resolves conflicts and enhances the complementarity of dual-stream knowledge.

Abstract: Retrieval-augmented generation (RAG) is a cost-effective approach to mitigate
the hallucination of Large Language Models (LLMs) by incorporating the
retrieved external knowledge into the generation process. However, external
knowledge may conflict with the parametric knowledge of LLMs. Furthermore,
current LLMs lack inherent mechanisms for resolving such knowledge conflicts,
making traditional RAG methods suffer from degraded performance and stability.
Thus, we propose a Dual-Stream Knowledge-Augmented Framework for Shared-Private
Semantic Synergy (DSSP-RAG). Central to the framework is a novel approach that
refines self-attention into a mixed-attention, distinguishing shared and
private semantics for a controlled internal-external knowledge integration. To
effectively facilitate DSSP in RAG, we further introduce an unsupervised
hallucination detection method based on cognitive uncertainty, ensuring the
necessity of introducing knowledge, and an Energy Quotient (EQ) based on
attention difference matrices to reduce noise in the retrieved external
knowledge. Extensive experiments on benchmark datasets show that DSSP-RAG can
effectively resolve conflicts and enhance the complementarity of dual-stream
knowledge, leading to superior performance over strong baselines.

</details>


### [73] [Cartridges: Lightweight and general-purpose long context representations via self-study](https://arxiv.org/abs/2506.06266)
*Sabri Eyuboglu,Ryan Ehrlich,Simran Arora,Neel Guha,Dylan Zinsley,Emily Liu,Will Tennien,Atri Rudra,James Zou,Azalia Mirhoseini,Christopher Re*

Main category: cs.CL

TL;DR: Introduces Cartridges trained with self-study to reduce the cost of serving large language models on large text corpora, matching ICL performance while using less memory and enabling higher throughput.


<details>
  <summary>Details</summary>
Motivation: The high cost of serving large language models using in-context learning (ICL) due to memory consumption of the KV cache scaling with input length.

Method: Training a smaller KV cache offline on each corpus, named Cartridge, using a training recipe called self-study which generates synthetic conversations about the corpus and trains the Cartridge with a context-distillation objective.

Result: Cartridges trained with self-study replicate the functionality of ICL while being significantly cheaper to serve. They match ICL performance on challenging long-context benchmarks while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length and leads to Cartridges that can be composed at inference time without retraining.

Conclusion: Self-study is a promising approach for reducing the cost of serving large language models on large text corpora.

Abstract: Large language models are often used to answer queries grounded in large text
corpora (e.g. codebases, legal documents, or chat histories) by placing the
entire corpus in the context window and leveraging in-context learning (ICL).
Although current models support contexts of 100K-1M tokens, this setup is
costly to serve because the memory consumption of the KV cache scales with
input length. We explore an alternative: training a smaller KV cache offline on
each corpus. At inference time, we load this trained KV cache, which we call a
Cartridge, and decode a response. Critically, the cost of training a Cartridge
can be amortized across all the queries referencing the same corpus. However,
we find that the naive approach of training the Cartridge with next-token
prediction on the corpus is not competitive with ICL. Instead, we propose
self-study, a training recipe in which we generate synthetic conversations
about the corpus and train the Cartridge with a context-distillation objective.
We find that Cartridges trained with self-study replicate the functionality of
ICL, while being significantly cheaper to serve. On challenging long-context
benchmarks, Cartridges trained with self-study match ICL performance while
using 38.6x less memory and enabling 26.4x higher throughput. Self-study also
extends the model's effective context length (e.g. from 128k to 484k tokens on
MTOB) and surprisingly, leads to Cartridges that can be composed at inference
time without retraining.

</details>


### [74] [AdvSumm: Adversarial Training for Bias Mitigation in Text Summarization](https://arxiv.org/abs/2506.06273)
*Mukur Gupta,Nikhil Reddy Varimalla,Nicholas Deas,Melanie Subbiah,Kathleen McKeown*

Main category: cs.CL

TL;DR: AdvSumm is a new method to reduce bias in text summarization using adversarial training.


<details>
  <summary>Details</summary>
Motivation: To mitigate bias in text summarization and improve generalization by enhancing model robustness to input variations.

Method: Introducing a Perturber component that applies gradient-guided perturbations at the embedding level of Sequence-to-Sequence models.

Result: AdvSumm successfully reduces name-nationality bias and political framing bias without affecting the quality of summarization.

Conclusion: AdvSumm, a domain-agnostic training framework, improves the robustness of text summarization models against associative and framing biases.

Abstract: Large Language Models (LLMs) have achieved impressive performance in text
summarization and are increasingly deployed in real-world applications.
However, these systems often inherit associative and framing biases from
pre-training data, leading to inappropriate or unfair outputs in downstream
tasks. In this work, we present AdvSumm (Adversarial Summarization), a
domain-agnostic training framework designed to mitigate bias in text
summarization through improved generalization. Inspired by adversarial
robustness, AdvSumm introduces a novel Perturber component that applies
gradient-guided perturbations at the embedding level of Sequence-to-Sequence
models, enhancing the model's robustness to input variations. We empirically
demonstrate that AdvSumm effectively reduces different types of bias in
summarization-specifically, name-nationality bias and political framing
bias-without compromising summarization quality. Compared to standard
transformers and data augmentation techniques like back-translation, AdvSumm
achieves stronger bias mitigation performance across benchmark datasets.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [75] [Voice Impression Control in Zero-Shot TTS](https://arxiv.org/abs/2506.05688)
*Keinichi Fujita,Shota Horiguchi,Yusuke Ijima*

Main category: cs.SD

TL;DR: Developed a method in zero-shot TTS to control para-/non-linguistic information for voice impression using a low-dimensional vector, proved effective by evaluations, and can generate vectors via large language models for target-impression generation from natural language descriptions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modulating subtle para-/non-linguistic information to control perceived voice characteristics (impressions) in zero-shot text-to-speech despite high speaker fidelity achievements.

Method: Utilized a low-dimensional vector to represent intensities of various voice impression pairs and used a large language model to generate this vector based on natural language descriptions.

Result: The method was proven effective in controlling voice impressions through both objective and subjective evaluations.

Conclusion: Achieved target-impression generation in zero-shot TTS without manual optimization by representing and generating para-/non-linguistic information with a low-dimensional vector.

Abstract: Para-/non-linguistic information in speech is pivotal in shaping the
listeners' impression. Although zero-shot text-to-speech (TTS) has achieved
high speaker fidelity, modulating subtle para-/non-linguistic information to
control perceived voice characteristics, i.e., impressions, remains
challenging. We have therefore developed a voice impression control method in
zero-shot TTS that utilizes a low-dimensional vector to represent the
intensities of various voice impression pairs (e.g., dark-bright). The results
of both objective and subjective evaluations have demonstrated our method's
effectiveness in impression control. Furthermore, generating this vector via a
large language model enables target-impression generation from a natural
language description of the desired impression, thus eliminating the need for
manual optimization.

</details>


### [76] [Label-Context-Dependent Internal Language Model Estimation for CTC](https://arxiv.org/abs/2506.06096)
*Zijian Yang,Minh-Nghia Phan,Ralf Schlüter,Hermann Ney*

Main category: cs.SD

TL;DR: This study explores how CTC models develop context-dependent internal language models despite their label context independence assumptions, introducing new estimation methods based on knowledge distillation and showing improvements over existing approaches.


<details>
  <summary>Details</summary>
Motivation: To investigate the implicit context dependency in CTC models' internal language models.

Method: Proposes novel context-dependent ILM estimation methods for CTC based on knowledge distillation with theoretical justifications and introduces two regularization methods for knowledge distillation.

Result: Experiments on Librispeech and TED-LIUM Release 2 datasets indicate that context-dependent ILMs outperform context-independent ones in cross-domain evaluation, with a method surpassing others by more than 13% relative improvement in word error rate compared to shallow fusion.

Conclusion: CTC models implicitly learn context-dependent internal language models, which can be effectively estimated using knowledge distillation techniques.

Abstract: Although connectionist temporal classification (CTC) has the label context
independence assumption, it can still implicitly learn a context-dependent
internal language model (ILM) due to modern powerful encoders. In this work, we
investigate the implicit context dependency modeled in the ILM of CTC. To this
end, we propose novel context-dependent ILM estimation methods for CTC based on
knowledge distillation (KD) with theoretical justifications. Furthermore, we
introduce two regularization methods for KD. We conduct experiments on
Librispeech and TED-LIUM Release 2 datasets for in-domain and cross-domain
evaluation, respectively. Experimental results show that context-dependent ILMs
outperform the context-independent priors in cross-domain evaluation,
indicating that CTC learns a context-dependent ILM. The proposed label-level KD
with smoothing method surpasses other ILM estimation approaches, with more than
13% relative improvement in word error rate compared to shallow fusion.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [77] [When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration](https://arxiv.org/abs/2506.05579)
*Quan Shi,Carlos E. Jimenez,Shunyu Yao,Nick Haber,Diyi Yang,Karthik Narasimhan*

Main category: cs.AI

TL;DR: This paper introduces KITE, a framework to evaluate Human-AI knowledge transfer capabilities and finds that knowledge transfer requires dedicated optimization.


<details>
  <summary>Details</summary>
Motivation: Investigate whether recent advancements in AI reasoning yield better knowledge transfer.

Method: Introduce Knowledge Integration and Transfer Evaluation (KITE), a conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118).

Result: Although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers.

Conclusion: Knowledge transfer requires dedicated optimization and communicatively aligned models.

Abstract: Recent advancements in AI reasoning have driven substantial improvements
across diverse tasks. A critical open question is whether these improvements
also yields better knowledge transfer: the ability of models to communicate
reasoning in ways humans can understand, apply, and learn from. To investigate
this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a
conceptual and experimental framework for Human-AI knowledge transfer
capabilities and conduct the first large-scale human study (N=118) explicitly
designed to measure it. In our two-phase setup, humans first ideate with an AI
on problem-solving strategies, then independently implement solutions,
isolating model explanations' influence on human understanding. Our findings
reveal that although model benchmark performance correlates with collaborative
outcomes, this relationship is notably inconsistent, featuring significant
outliers, indicating that knowledge transfer requires dedicated optimization.
Our analysis identifies behavioral and strategic factors mediating successful
knowledge transfer. We release our code, dataset, and evaluation framework to
support future work on communicatively aligned models.

</details>


### [78] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: 提出MMTU，一个涵盖25种真实表格任务的大规模基准，评估模型在理解、推理和操作表格的能力，发现当前最先进模型表现有限，强调了进一步研究的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在处理表格方面取得了显著进展，但针对表格相关任务的全面基准测试仍然有限。现有的评估主要集中在NL-to-SQL和Table-QA等狭窄的任务上，忽略了专业用户面临的更广泛的现实任务，从而限制了我们对这一重要领域的理解和模型的进步。

Method: 引入了MMTU，这是一个包含25个真实世界表格任务的大规模基准测试集，涉及超过30K个问题，旨在全面评估模型在理解、推理和操作真实表格方面的综合能力。

Result: MMTU需要结合表格理解、推理和编码等多种技能，目前即使是最先进的模型也只能达到约60%的准确率，表明该领域有巨大的提升空间。

Conclusion: MMTU是一个大规模基准测试集，包含超过30K个问题和25个真实世界的表格任务，用于全面评估模型在专家级理解、推理和操作真实表格的能力。实验表明，即使是最前沿的模型，在MMTU上的表现也仅达到约60%，这表明在这个重要领域仍有很大的改进空间。希望这个基准能推动结构化数据分析基础模型的发展。

Abstract: Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [79] [Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective](https://arxiv.org/abs/2506.05754)
*Emmanuel Anaya Gonzalez,Sairam Vaidya,Kanghee Park,Ruyi Ji,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: 提出一种基于马尔可夫链蒙特卡洛(MCMC)的新的约束采样框架，满足三个核心需求，并在程序模糊测试任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有约束解码方法往往扭曲底层模型分布，在程序模糊测试等应用中出现问题。

Method: 构建一个有效的提案分布并应用Metropolis-Hastings接受准则，确保原则性和高效探索约束空间。

Result: 该方法在合成基准和实际程序模糊测试任务中都优于现有方法。

Conclusion: 提出的约束采样框架在程序模糊测试等应用中具有优势。

Abstract: Constrained decoding enables Language Models (LMs) to produce samples that
provably satisfy hard constraints. However, existing constrained-decoding
approaches often distort the underlying model distribution, a limitation that
is especially problematic in applications like program fuzzing, where one wants
to generate diverse and valid program inputs for testing purposes. We propose a
new constrained sampling framework based on Markov Chain Monte Carlo (MCMC)
that simultaneously satisfies three core desiderata: constraint satisfying
(every sample satisfies the constraint), monotonically converging (the sampling
process converges to the true conditional distribution), and efficient
(high-quality samples emerge in few steps). Our method constructs a proposal
distribution over valid outputs and applies a Metropolis-Hastings acceptance
criterion based on the LM's likelihood, ensuring principled and efficient
exploration of the constrained space. Empirically, our sampler outperforms
existing methods on both synthetic benchmarks and real-world program fuzzing
tasks.

</details>


### [80] [Proactive Assistant Dialogue Generation from Streaming Egocentric Videos](https://arxiv.org/abs/2506.05904)
*Yichi Zhang,Xin Luna Dong,Zhaojiang Lin,Andrea Madotto,Anuj Kumar,Babak Damavandi,Joyce Chai,Seungwhan Moon*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in conversational AI have been substantial, but developing
real-time systems for perceptual task guidance remains challenging. These
systems must provide interactive, proactive assistance based on streaming
visual inputs, yet their development is constrained by the costly and
labor-intensive process of data collection and system evaluation. To address
these limitations, we present a comprehensive framework with three key
contributions. First, we introduce a novel data curation pipeline that
synthesizes dialogues from annotated egocentric videos, resulting in \dataset,
a large-scale synthetic dialogue dataset spanning multiple domains. Second, we
develop a suite of automatic evaluation metrics, validated through extensive
human studies. Third, we propose an end-to-end model that processes streaming
video inputs to generate contextually appropriate responses, incorporating
novel techniques for handling data imbalance and long-duration videos. This
work lays the foundation for developing real-time, proactive AI assistants
capable of guiding users through diverse tasks. Project page:
https://pro-assist.github.io/

</details>


### [81] [PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time](https://arxiv.org/abs/2506.06254)
*Weizhi Zhang,Xinyang Zhang,Chenwei Zhang,Liangwei Yang,Jingbo Shang,Zhepei Wei,Henry Peng Zou,Zijie Huang,Zhengyang Wang,Yifan Gao,Xiaoman Pan,Lian Xiong,Jingguo Liu,Philip S. Yu,Xian Li*

Main category: cs.AI

TL;DR: Developing PersonaAgent, the first personalized Large Language Model (LLM) agent framework that integrates personalized memory and action modules, with a focus on versatile personalization tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack flexibility to meet varying user needs and preferences.

Method: Integrating personalized memory (episodic and semantic mechanisms) and action modules, using persona as an intermediary for user preference alignment via test-time strategies.

Result: PersonaAgent outperforms baselines in personalizing action space and adapting to real-world applications.

Conclusion: The proposed framework demonstrates feasibility and potential in providing tailored, dynamic user experiences.

Abstract: Large Language Model (LLM) empowered agents have recently emerged as advanced
paradigms that exhibit impressive capabilities in a wide range of domains and
tasks. Despite their potential, current LLM agents often adopt a
one-size-fits-all approach, lacking the flexibility to respond to users'
varying needs and preferences. This limitation motivates us to develop
PersonaAgent, the first personalized LLM agent framework designed to address
versatile personalization tasks. Specifically, PersonaAgent integrates two
complementary components - a personalized memory module that includes episodic
and semantic memory mechanisms; a personalized action module that enables the
agent to perform tool actions tailored to the user. At the core, the persona
(defined as unique system prompt for each user) functions as an intermediary:
it leverages insights from personalized memory to control agent actions, while
the outcomes of these actions in turn refine the memory. Based on the
framework, we propose a test-time user-preference alignment strategy that
simulate the latest n interactions to optimize the persona prompt, ensuring
real-time user preference alignment through textual loss feedback between
simulated and ground-truth responses. Experimental evaluations demonstrate that
PersonaAgent significantly outperforms other baseline methods by not only
personalizing the action space effectively but also scaling during test-time
real-world applications. These results underscore the feasibility and potential
of our approach in delivering tailored, dynamic user experiences.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [82] [Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety](https://arxiv.org/abs/2506.05451)
*Seongmin Lee,Aeree Cho,Grace C. Kim,ShengYun Peng,Mansi Phute,Duen Horng Chau*

Main category: cs.SE

TL;DR: A survey on bridging the gap between interpretation techniques and safety improvements in large language models.


<details>
  <summary>Details</summary>
Motivation: Understanding and mitigating unsafe behaviors in large language models is critical as they gain wider real-world use. Interpretation techniques can reveal causes of unsafe outputs and guide safety, but these connections are often ignored in previous surveys.

Method: Introduces a unified framework that connects safety-focused interpretation methods, the safety enhancements they inform, and the tools that operationalize them.

Result: Summarizes nearly 70 works at the intersections of LLM workflow stages.

Conclusion: Concludes with open challenges and future directions for safer, more interpretable LLMs.

Abstract: As large language models (LLMs) see wider real-world use, understanding and
mitigating their unsafe behaviors is critical. Interpretation techniques can
reveal causes of unsafe outputs and guide safety, but such connections with
safety are often overlooked in prior surveys. We present the first survey that
bridges this gap, introducing a unified framework that connects safety-focused
interpretation methods, the safety enhancements they inform, and the tools that
operationalize them. Our novel taxonomy, organized by LLM workflow stages,
summarizes nearly 70 works at their intersections. We conclude with open
challenges and future directions. This timely survey helps researchers and
practitioners navigate key advancements for safer, more interpretable LLMs.

</details>


### [83] [Deployability-Centric Infrastructure-as-Code Generation: An LLM-based Iterative Framework](https://arxiv.org/abs/2506.05623)
*Tianyi Zhang,Shidong Pan,Zejun Zhang,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.SE

TL;DR: 本研究提出了一种新的方法来提高基础设施即代码(IaC)模板的生成质量，特别是在部署性和安全性方面。


<details>
  <summary>Details</summary>
Motivation: 当前对IaC模板的评估主要集中在句法正确性上，而忽略了部署性，这是衡量IaC模板实用性的重要指标。

Method: 我们提出了两个贡献：(1)基于LLM的以部署为中心的框架IaCGen，它使用迭代反馈机制来生成IaC模板，(2)DPIaC-Eval，这是一个由153个真实世界场景组成的以部署为中心的IaC模板基准，可以评估语法、部署、用户意图和安全性。

Result: 在第一次尝试时，最先进的LLMs表现不佳，Claude-3.5和Claude-3.7的部署成功率分别为30.2%和26.8%。然而，IaCGen极大地改善了这一表现：所有评估模型的passItr@25都超过了90%，Claude-3.5和Claude-3.7的成功率达到了98%。尽管如此，用户意图对齐（25.2%的准确性）和安全合规性（8.4%的通过率）仍然是关键挑战。

Conclusion: 我们的工作对以部署为中心的IaC模板生成进行了首次全面评估，并为未来的研究奠定了基础。

Abstract: Infrastructure-as-Code (IaC) generation holds significant promise for
automating cloud infrastructure provisioning. Recent advances in Large Language
Models (LLMs) present a promising opportunity to democratize IaC development by
generating deployable infrastructure templates from natural language
descriptions, but current evaluation focuses on syntactic correctness while
ignoring deployability, the fatal measure of IaC template utility. We address
this gap through two contributions: (1) IaCGen, an LLM-based
deployability-centric framework that uses iterative feedback mechanism to
generate IaC templates, and (2) DPIaC-Eval, a deployability-centric IaC
template benchmark consists of 153 real-world scenarios that can evaluate
syntax, deployment, user intent, and security. Our evaluation reveals that
state-of-the-art LLMs initially performed poorly, with Claude-3.5 and
Claude-3.7 achieving only 30.2% and 26.8% deployment success on the first
attempt respectively. However, IaCGen transforms this performance dramatically:
all evaluated models reach over 90% passItr@25, with Claude-3.5 and Claude-3.7
achieving 98% success rate. Despite these improvements, critical challenges
remain in user intent alignment (25.2% accuracy) and security compliance (8.4%
pass rate), highlighting areas requiring continued research. Our work provides
the first comprehensive assessment of deployability-centric IaC template
generation and establishes a foundation for future research.

</details>


### [84] [CodeContests+: High-Quality Test Case Generation for Competitive Programming](https://arxiv.org/abs/2506.05817)
*Zihan Wang,Siyao Liu,Yang Sun,Hongyan Li,Kai Shen*

Main category: cs.SE

TL;DR: This paper introduces an LLM-based agent system to generate high-quality test cases for competitive programming problems, enhancing the evaluation accuracy of large language models.


<details>
  <summary>Details</summary>
Motivation: The lack of accessible test cases for competitive programming problems makes it hard to build large-scale datasets and accurately evaluate the reasoning capabilities of LLMs.

Method: An LLM-based agent system is created to generate high-quality test cases, which is applied to the CodeContests dataset to produce an improved version called CodeContests+.

Result: CodeContests+ shows significantly higher accuracy than CodeContests, especially with a higher True Positive Rate, when evaluated using 1.72 million submissions with pass/fail labels. The improved test cases also benefit LLM Reinforcement Learning.

Conclusion: The proposed LLM-based agent system effectively improves the quality of test cases for competitive programming problems, leading to more accurate evaluations of LLMs.

Abstract: Competitive programming, due to its high reasoning difficulty and precise
correctness feedback, has become a key task for both training and evaluating
the reasoning capabilities of large language models (LLMs). However, while a
large amount of public problem data, such as problem statements and solutions,
is available, the test cases of these problems are often difficult to obtain.
Therefore, test case generation is a necessary task for building large-scale
datasets, and the quality of the test cases directly determines the accuracy of
the evaluation. In this paper, we introduce an LLM-based agent system that
creates high-quality test cases for competitive programming problems. We apply
this system to the CodeContests dataset and propose a new version with improved
test cases, named CodeContests+. We evaluated the quality of test cases in
CodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels
to examine the accuracy of these test cases in evaluation. The results
indicated that CodeContests+ achieves significantly higher accuracy than
CodeContests, particularly with a notably higher True Positive Rate (TPR).
Subsequently, our experiments in LLM Reinforcement Learning (RL) further
confirmed that improvements in test case quality yield considerable advantages
for RL.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [85] [SoK: Are Watermarks in LLMs Ready for Deployment?](https://arxiv.org/abs/2506.05594)
*Kieu Dang,Phung Lai,NhatHai Phan,Yelong Shen,Ruoming Jin,Abdallah Khreishah,My Thai*

Main category: cs.CR

TL;DR: Large Language Models (LLMs) have great capabilities but face risks like intellectual property violations. This paper focuses on model stealing attacks and examines the progress and effectiveness of watermarking techniques in LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the risks of intellectual property violations and misuse of LLMs, especially concerning model stealing attacks.

Method: Develop a comprehensive systematization for watermarks in LLMs including taxonomy creation, a novel IP classifier, analysis of existing watermark limitations, and discussion on future directions.

Result: Experiments show that while watermarking techniques have received attention, they haven't fully realized their potential in real-world applications due to negative impacts on model utility and downstream tasks.

Conclusion: There is a need for practical watermark solutions tailored to LLM deployment.

Abstract: Large Language Models (LLMs) have transformed natural language processing,
demonstrating impressive capabilities across diverse tasks. However, deploying
these models introduces critical risks related to intellectual property
violations and potential misuse, particularly as adversaries can imitate these
models to steal services or generate misleading outputs. We specifically focus
on model stealing attacks, as they are highly relevant to proprietary LLMs and
pose a serious threat to their security, revenue, and ethical deployment. While
various watermarking techniques have emerged to mitigate these risks, it
remains unclear how far the community and industry have progressed in
developing and deploying watermarks in LLMs.
  To bridge this gap, we aim to develop a comprehensive systematization for
watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs,
2) proposing a novel intellectual property classifier to explore the
effectiveness and impacts of watermarks on LLMs under both attack and
attack-free environments, 3) analyzing the limitations of existing watermarks
in LLMs, and 4) discussing practical challenges and potential future directions
for watermarks in LLMs. Through extensive experiments, we show that despite
promising research outcomes and significant attention from leading companies
and community to deploy watermarks, these techniques have yet to reach their
full potential in real-world applications due to their unfavorable impacts on
model utility of LLMs and downstream tasks. Our findings provide an insightful
understanding of watermarks in LLMs, highlighting the need for practical
watermarks solutions tailored to LLM deployment.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [86] [Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning](https://arxiv.org/abs/2506.05671)
*Yangui Fang,Jing Peng,Xu Li,Yu Xi,Chengwei Zhang,Guohui Zhong,Kai Yu*

Main category: eess.AS

TL;DR: Proposes a text-only fine-tuning method for Speech LLMs to adapt to new domains without additional audio, demonstrating competitive performance and generalization ability.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of adapting Speech LLMs to new domains, particularly in low-resource settings where paired speech-text data is limited.

Method: A text-only fine-tuning strategy is proposed for Speech LLMs using unpaired target-domain text without requiring additional audio. A real-time evaluation mechanism is introduced to preserve speech-text alignment.

Result: Experiments on LibriSpeech, SlideSpeech, and Medical datasets show competitive recognition performance with minimal degradation compared to full audio-text fine-tuning.

Conclusion: Text-only fine-tuning shows potential for low-resource domain adaptation of ASR by improving generalization to new domains without catastrophic forgetting.

Abstract: Recent advances in automatic speech recognition (ASR) have combined speech
encoders with large language models (LLMs) through projection, forming Speech
LLMs with strong performance. However, adapting them to new domains remains
challenging, especially in low-resource settings where paired speech-text data
is scarce. We propose a text-only fine-tuning strategy for Speech LLMs using
unpaired target-domain text without requiring additional audio. To preserve
speech-text alignment, we introduce a real-time evaluation mechanism during
fine-tuning. This enables effective domain adaptation while maintaining
source-domain performance. Experiments on LibriSpeech, SlideSpeech, and Medical
datasets show that our method achieves competitive recognition performance,
with minimal degradation compared to full audio-text fine-tuning. It also
improves generalization to new domains without catastrophic forgetting,
highlighting the potential of text-only fine-tuning for low-resource domain
adaptation of ASR.

</details>


### [87] [Audio-Aware Large Language Models as Judges for Speaking Styles](https://arxiv.org/abs/2506.05984)
*Cheng-Han Chiang,Xiaofei Wang,Chung-Ching Lin,Kevin Lin,Linjie Li,Radu Kopetz,Yao Qian,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: eess.AS

TL;DR: This paper explores using audio-aware large language models (ALLMs) as automatic judges to assess speaking styles of speeches generated by spoken language models (SLMs).


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of ALLMs as judges for assessing speaking styles.

Method: Using ALLM judges to evaluate speeches generated by SLMs on voice style instruction following and role-playing tasks.

Result: Gemini-2.5-pro showed comparable agreement with human judges, while current SLMs need improvement in controlling speaking style and generating natural dialogues.

Conclusion: ALLMs can be used as judges to evaluate SLMs.

Abstract: Audio-aware large language models (ALLMs) can understand the textual and
non-textual information in the audio input. In this paper, we explore using
ALLMs as an automatic judge to assess the speaking styles of speeches. We use
ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice
style instruction following and role-playing. The speaking style we consider
includes emotion, volume, speaking pace, word emphasis, pitch control, and
non-verbal elements. We use four spoken language models (SLMs) to complete the
two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two
ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and
show that the agreement between Gemini and human judges is comparable to the
agreement between human evaluators. These promising results show that ALLMs can
be used as a judge to evaluate SLMs. Our results also reveal that current SLMs,
even GPT-4o-audio, still have room for improvement in controlling the speaking
style and generating natural dialogues.

</details>


### [88] [CO-VADA: A Confidence-Oriented Voice Augmentation Debiasing Approach for Fair Speech Emotion Recognition](https://arxiv.org/abs/2506.06071)
*Yun-Shao Tsai,Yi-Cheng Lin,Huang-Cheng Chou,Hung-yi Lee*

Main category: eess.AS

TL;DR: Introduces CO-VADA, a debiasing approach for speech emotion recognition that doesn't need model changes or demographic info, improving system fairness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address bias in speech emotion recognition (SER) systems which often comes from spurious correlations between speaker characteristics and emotional labels, causing unfair predictions across different demographic groups.

Method: CO-VADA, a Confidence-Oriented Voice Augmentation Debiasing Approach, identifies biased training samples and uses voice conversion to change irrelevant attributes and create new samples.

Result: The approach improves fairness in SER systems by introducing speaker variations that are different from dominant patterns in the data, helping the model concentrate more on emotion-related features.

Conclusion: This method does not require changes to the model architecture or demographic annotations, making it a scalable and practical solution for reducing bias in SER systems.

Abstract: Bias in speech emotion recognition (SER) systems often stems from spurious
correlations between speaker characteristics and emotional labels, leading to
unfair predictions across demographic groups. Many existing debiasing methods
require model-specific changes or demographic annotations, limiting their
practical use. We present CO-VADA, a Confidence-Oriented Voice Augmentation
Debiasing Approach that mitigates bias without modifying model architecture or
relying on demographic information. CO-VADA identifies training samples that
reflect bias patterns present in the training data and then applies voice
conversion to alter irrelevant attributes and generate samples. These augmented
samples introduce speaker variations that differ from dominant patterns in the
data, guiding the model to focus more on emotion-relevant features. Our
framework is compatible with various SER models and voice conversion tools,
making it a scalable and practical solution for improving fairness in SER
systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [89] [Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones](https://arxiv.org/abs/2506.05641)
*Andrey Zhmoginov,Jihwan Lee,Mark Sandler*

Main category: cs.LG

TL;DR: This paper explores a method to map parameters from a large Transformer model to a smaller specialized model for image modeling tasks, aiming to improve efficiency and relevance for specific tasks.


<details>
  <summary>Details</summary>
Motivation: Utilizing modern Foundation Models can be very computationally expensive and most of the broad FM knowledge may be irrelevant for a specific task.

Method: Mapping parameters of a large Transformer to parameters of a smaller specialized model.

Result: Performance of generated models exceeds that of universal conditional models.

Conclusion: Performance of generated models exceeds that of universal conditional models.

Abstract: Modern Foundation Models (FMs) are typically trained on corpora spanning a
wide range of different data modalities, topics and downstream tasks. Utilizing
these models can be very computationally expensive and is out of reach for most
consumer devices. Furthermore, most of the broad FM knowledge may actually be
irrelevant for a specific task at hand. Here we explore a technique for mapping
parameters of a large Transformer to parameters of a smaller specialized model.
By making this transformation task-specific, we aim to capture a narrower scope
of the knowledge needed for performing a specific task by a smaller model. We
study our method on image modeling tasks, showing that performance of generated
models exceeds that of universal conditional models.

</details>


### [90] [BAQ: Efficient Bit Allocation Quantization for Large Language Models](https://arxiv.org/abs/2506.05664)
*Chao Zhang,Li Wang,Samson Lasaulce,Merouane Debbah*

Main category: cs.LG

TL;DR: This paper introduces a novel method called Bit Allocation Quantization (BAQ) for optimizing the quantization of large language models, which improves performance by allocating bitwidths based on sensitivity metrics.


<details>
  <summary>Details</summary>
Motivation: Existing post-training model quantization methods often use uniform or heuristic bitwidth assignments, ignoring the varying sensitivity of weights to quantization noise. The paper aims to address this limitation.

Method: The authors propose a convex optimization approach to allocate bitwidths based on sensitivity metrics derived from a Hessian proxy, allowing for an explicit expression of the layer-wise loss function in terms of bitwidths.

Result: BAQ achieves better performance than GPTQ, reducing perplexity by up to 56× on large language models with different parameter sizes.

Conclusion: The paper demonstrates that optimizing bit allocation through a convex optimization problem can significantly improve the efficiency and effectiveness of quantizing large language models.

Abstract: Post-training model quantization is a widely adopted technique for reducing
the memory and computational costs of large language models (LLMs). However,
most existing methods rely on uniform or heuristic bitwidth assignments,
failing to account for the nonuniform sensitivity of weights to quantization
noise. In this paper, we propose a novel framework for allocating quantization
bitwidths based on sensitivity metrics derived from a Hessian proxy. We make
key assumptions, which allow the layer/component-wise loss function to be
expressed as an explicit function of the bitwidths. This enables a neat
formulation of the bit allocation problem as a convex optimization task, whose
closed-form solution adapts precision across weights to minimize the layer-wise
quantization loss. Inspecting the solution provides several insights (such as
the equal-loss structure), which are then exploited to design the proposed
\textbf{BAQ} (Bit Allocation Quantization) algorithm. The proposed algorithm
achieves a good trade-off between loss minimization and complexity and allows
BAQ to be integrated into standard quantization pipelines with minimal
overhead. Experimental results show that BAQ consistently outperforms GPTQ,
achieving up to 56$\times$ lower perplexity at the same bitwidth on large
language models ranging from 125M to 30B parameters. Leveraging our analytical
results derived from solving the optimal bit allocation problem, we also
provide a theoretical explanation for the observed gains. All codes of this
paper are available at https://github.com/CSU-ModelCompression/BAQ.

</details>


### [91] [Contextually Guided Transformers via Low-Rank Adaptation](https://arxiv.org/abs/2506.05672)
*Andrey Zhmoginov,Jihwan Lee,Max Vladymyrov,Mark Sandler*

Main category: cs.LG

TL;DR: This paper proposes a Contextually Guided Transformer (CGT) model which learns to encode context into the model's weights, eliminating the need for explicit prompts. The model demonstrates effectiveness on in-context learning tasks and language modeling benchmarks.


<details>
  <summary>Details</summary>
Motivation: To reduce computational overhead and improve efficiency in large language models by eliminating the need for explicit prompts.

Method: Modify a Transformer architecture to include a contextual summary at each sequence position, enabling the model to update its weights based on preceding context and self-specialize.

Result: The CGT model shows effectiveness on synthetic in-context learning tasks and language modeling benchmarks, and introduces techniques to enhance interpretability of learned contextual representations.

Conclusion: This work presents a novel approach for efficient and adaptable language modeling by integrating context directly into the model's architecture.

Abstract: Large Language Models (LLMs) based on Transformers excel at text processing,
but their reliance on prompts for specialized behavior introduces computational
overhead. We propose a modification to a Transformer architecture that
eliminates the need for explicit prompts by learning to encode context into the
model's weights. Our Contextually Guided Transformer (CGT) model maintains a
contextual summary at each sequence position, allowing it to update the weights
on the fly based on the preceding context. This approach enables the model to
self-specialize, effectively creating a tailored model for processing
information following a given prefix. We demonstrate the effectiveness of our
method on synthetic in-context learning tasks and language modeling benchmarks.
Furthermore, we introduce techniques for enhancing the interpretability of the
learned contextual representations, drawing connections to Variational
Autoencoders and promoting smoother, more consistent context encoding. This
work offers a novel direction for efficient and adaptable language modeling by
integrating context directly into the model's architecture.

</details>


### [92] [Table-r1: Self-supervised and Reinforcement Learning for Program-based Table Reasoning in Small Language Models](https://arxiv.org/abs/2506.06137)
*Rihui Jin,Zheyu Xin,Xing Xie,Zuoyi Li,Guilin Qi,Yongrui Chen,Xinbang Dai,Tongtong Wu,Gholamreza Haffari*

Main category: cs.LG

TL;DR: This paper introduces Table-r1, a two-stage program-based table reasoning method tailored for small language models, improving their performance on tabular data tasks through enhanced layout generalization and reasoning consistency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of small language models in table reasoning tasks, especially in numerical reasoning and layout heterogeneity.

Method: A two-stage approach including Layout Transformation Inference for layout generalization and a mix-paradigm variant of Group Relative Policy Optimization for reasoning consistency, with a fallback to text-based reasoning when necessary.

Result: Table-r1 surpasses other small language model-based methods by at least 15% in accuracy across four different table reasoning benchmarks, matching the performance of large language models.

Conclusion: The proposed method effectively bridges the performance gap between small and large language models in table reasoning tasks.

Abstract: Table reasoning (TR) requires structured reasoning over semi-structured
tabular data and remains challenging, particularly for small language models
(SLMs, e.g., LLaMA-8B) due to their limited capacity compared to large LMs
(LLMs, e.g., GPT-4o). To narrow this gap, we explore program-based TR (P-TR),
which circumvents key limitations of text-based TR (T-TR), notably in numerical
reasoning, by generating executable programs. However, applying P-TR to SLMs
introduces two challenges: (i) vulnerability to heterogeneity in table layouts,
and (ii) inconsistency in reasoning due to limited code generation capability.
We propose Table-r1, a two-stage P-TR method designed for SLMs. Stage 1
introduces an innovative self-supervised learning task, Layout Transformation
Inference, to improve tabular layout generalization from a programmatic view.
Stage 2 adopts a mix-paradigm variant of Group Relative Policy Optimization,
enhancing P-TR consistency while allowing dynamic fallback to T-TR when needed.
Experiments on four TR benchmarks demonstrate that Table-r1 outperforms all
SLM-based methods, achieving at least a 15% accuracy improvement over the base
model (LLaMA-8B) across all datasets and reaching performance competitive with
LLMs.

</details>


### [93] [The Lock-in Hypothesis: Stagnation by Algorithm](https://arxiv.org/abs/2506.06166)
*Tianyi Alex Qiu,Zhonghao He,Tejasveer Chugh,Max Kleiman-Weiner*

Main category: cs.LG

TL;DR: This study investigates the feedback loop between large language models and human users, suggesting that it may lead to the entrenchment of existing beliefs and loss of diversity.


<details>
  <summary>Details</summary>
Motivation: To understand how the interaction between large language models and human users affects belief systems and diversity.

Method: Agent-based LLM simulations and analysis of real-world GPT usage data.

Result: Sudden and sustained drops in diversity were observed post-release of new GPT iterations.

Conclusion: The research supports the hypothesis that the human-AI feedback loop can lead to the reinforcement of existing beliefs and potential locking into false beliefs.

Abstract: The training and deployment of large language models (LLMs) create a feedback
loop with human users: models learn human beliefs from data, reinforce these
beliefs with generated content, reabsorb the reinforced beliefs, and feed them
back to users again and again. This dynamic resembles an echo chamber. We
hypothesize that this feedback loop entrenches the existing values and beliefs
of users, leading to a loss of diversity and potentially the lock-in of false
beliefs. We formalize this hypothesis and test it empirically with agent-based
LLM simulations and real-world GPT usage data. Analysis reveals sudden but
sustained drops in diversity after the release of new GPT iterations,
consistent with the hypothesized human-AI feedback loop. Code and data
available at https://thelockinhypothesis.com

</details>


### [94] [Corrector Sampling in Language Models](https://arxiv.org/abs/2506.06215)
*Itai Gat,Neta Shaul,Uriel Singer,Yaron Lipman*

Main category: cs.LG

TL;DR: A new sampling method called Resample-Previous-Tokens (RPT) is proposed to mitigate error accumulation in autoregressive language models.


<details>
  <summary>Details</summary>
Motivation: Autoregressive language models accumulate errors due to their fixed, irrevocable left-to-right token generation.

Method: Resample-Previous-Tokens (RPT)

Result: Relative improvements on reasoning and coding benchmarks

Conclusion: Fine-tuning a pretrained 8B parameter model with RPT resulted in ~10% relative improvements on reasoning and coding benchmarks compared to the standard sampling.

Abstract: Autoregressive language models accumulate errors due to their fixed,
irrevocable left-to-right token generation. To address this, we propose a new
sampling method called Resample-Previous-Tokens (RPT). RPT mitigates error
accumulation by iteratively revisiting and potentially replacing tokens in a
window of previously generated text. This method can be integrated into
existing autoregressive models, preserving their next-token-prediction quality
and speed. Fine-tuning a pretrained 8B parameter model with RPT for only 100B
resulted in ~10% relative improvements on reasoning and coding benchmarks
compared to the standard sampling.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [95] [Masked Language Models are Good Heterogeneous Graph Generalizers](https://arxiv.org/abs/2506.06157)
*Jinyu Yang,Cheng Yang,Shanyuan Cui,Zeyuan Guo,Liangwei Yang,Muhan Zhang,Chuan Shi*

Main category: cs.SI

TL;DR: This paper proposes a novel method called MLM4HG for heterogeneous graph learning, which improves generalization across domains and tasks by using metapath-based textual sequences.


<details>
  <summary>Details</summary>
Motivation: To address the issue of biased understanding of HGs by LLMs due to disparities in embedding spaces between HGNNs and LLMs, and the limitation in generalizing across tasks.

Method: A Masked Language Modeling-based method named MLM4HG is proposed which uses metapath-based textual sequences and customized textual templates to unify different graph tasks.

Result: MLM4HG demonstrates superior generalization performance on four real-world datasets.

Conclusion: Extensive experiments show that MLM4HG outperforms state-of-the-art methods in both few-shot and zero-shot scenarios.

Abstract: Heterogeneous graph neural networks (HGNNs) excel at capturing structural and
semantic information in heterogeneous graphs (HGs), while struggling to
generalize across domains and tasks. Recently, some researchers have turned to
integrating HGNNs with large language models (LLMs) for more generalizable
heterogeneous graph learning. However, these approaches typically extract
structural information via HGNNs as HG tokens, and disparities in embedding
spaces between HGNNs and LLMs have been shown to bias the LLM's comprehension
of HGs. Moreover, as these HG tokens are often derived from node-level tasks,
the model's ability to generalize across tasks remains limited. To this end, we
propose a simple yet effective Masked Language Modeling-based method, called
MLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens
to extract structural and semantic information inherent in HGs, and designs
customized textual templates to unify different graph tasks into a coherent
cloze-style "mask" token prediction paradigm. Specifically, MLM4HG first
converts HGs from various domains to texts based on metapaths, and subsequently
combines them with the unified task texts to form a HG-based corpus. Moreover,
the corpus is fed into a pretrained LM for fine-tuning with a constrained
target vocabulary, enabling the fine-tuned LM to generalize to unseen target
HGs. Extensive cross-domain and multi-task experiments on four real-world
datasets demonstrate the superior generalization performance of MLM4HG over
state-of-the-art methods in both few-shot and zero-shot scenarios. Our code is
available at https://github.com/BUPT-GAMMA/MLM4HG.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [96] [Attention-based transformer models for image captioning across languages: An in-depth survey and evaluation](https://arxiv.org/abs/2506.05399)
*Israa A. Albadarneh,Bassam H. Hammo,Omar S. Al-Kadi*

Main category: cs.CV

TL;DR: 本文综述了基于注意力机制的图像描述生成模型，涵盖了模型分类、评估指标、多语言挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有的关于深度学习方法的综述较少涉及基于注意力机制的Transformer模型在多种语言中的全面分析。

Method: 对基于注意力机制的图像描述生成模型进行了分类，并探讨了评估指标和多语言描述中的挑战。

Result: 回顾了基于注意力机制的图像描述生成模型，讨论了评估标准和多语言描述的挑战，并指出了当前模型的关键限制。

Conclusion: 总结了当前基于注意力机制的图像描述生成模型的局限性，并对未来的研究方向进行了展望。

Abstract: Image captioning involves generating textual descriptions from input images,
bridging the gap between computer vision and natural language processing.
Recent advancements in transformer-based models have significantly improved
caption generation by leveraging attention mechanisms for better scene
understanding. While various surveys have explored deep learning-based
approaches for image captioning, few have comprehensively analyzed
attention-based transformer models across multiple languages. This survey
reviews attention-based image captioning models, categorizing them into
transformer-based, deep learning-based, and hybrid approaches. It explores
benchmark datasets, discusses evaluation metrics such as BLEU, METEOR, CIDEr,
and ROUGE, and highlights challenges in multilingual captioning. Additionally,
this paper identifies key limitations in current models, including semantic
inconsistencies, data scarcity in non-English languages, and limitations in
reasoning ability. Finally, we outline future research directions, such as
multimodal learning, real-time applications in AI-powered assistants,
healthcare, and forensic analysis. This survey serves as a comprehensive
reference for researchers aiming to advance the field of attention-based image
captioning.

</details>


### [97] [Can Vision Language Models Infer Human Gaze Direction? A Controlled Study](https://arxiv.org/abs/2506.05412)
*Zory Zhang,Pinyuan Feng,Bingyang Wang,Tianwei Zhao,Suyang Yu,Qingying Gao,Hokin Deng,Ziqiao Ma,Yijiang Li,Dezhi Luo*

Main category: cs.CV

TL;DR: Evaluate 111 VLMs' gaze-referential inference abilities through a controlled study.


<details>
  <summary>Details</summary>
Motivation: To understand whether VLMs can naturally interact with humans by evaluating their gaze-referential inference skills.

Method: Conduct a controlled study involving photos of varying difficulty and variability, compare VLMs' performance with human participants.

Result: 94 out of 111 VLMs performed no better than random guessing, while humans achieved near-perfect accuracy. Top-tier VLMs showed performance declining with task difficulty but were consistent across different prompts and objects.

Conclusion: Most VLMs lack effective gaze inference capabilities for natural human-AI interaction, but some show potential with combined heuristics and guessing.

Abstract: Gaze-referential inference--the ability to infer what others are looking
at--is a critical component of a theory of mind that underpins natural human-AI
interaction. In a controlled study, we evaluated this skill across 111 Vision
Language Models (VLMs) using photos taken with manipulated difficulty and
variability, comparing performance with that of human participants (N = 65),
and analyzed behaviors using mixed-effects models. We found that 94 of the 111
VLMs failed to do better than random guessing, while humans achieved
near-ceiling accuracy. VLMs even respond with each choice almost equally
frequently. Are they randomly guessing? Although most VLMs struggle, when we
zoom in on five of the top-tier VLMs with above-chance performance, we find
that their performance declined with increasing task difficulty but varied only
slightly across different prompts and scene objects. These behavioral features
cannot be explained by considering them as random guessers. Instead, they
likely use a combination of heuristics and guessing such that their performance
is subject to the task difficulty but robust to perceptual variations. This
suggests that VLMs, lacking gaze inference capability, have yet to become
technologies that can naturally interact with humans, but the potential
remains.

</details>


### [98] [Coordinated Robustness Evaluation Framework for Vision-Language Models](https://arxiv.org/abs/2506.05429)
*Ashwin Ramesh Babu,Sajad Mousavi,Vineet Gundecha,Sahand Ghorbanpour,Avisek Naug,Antonio Guillen,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.CV

TL;DR: This work proposes a method to evaluate the robustness of vision-language models by generating adversarial perturbations in both image and text modalities.


<details>
  <summary>Details</summary>
Motivation: Vision-language models are vulnerable to small perturbations, and evaluating their robustness requires perturbations in both vision and language modalities to learn their inter-modal dependencies.

Method: Train a generic surrogate model that generates joint representation for both image and text modalities, then use it to generate adversarial perturbations.

Result: The proposed strategy outperforms other multi-modal attacks and single-modality attacks from the recent literature.

Conclusion: The proposed strategy demonstrates the effectiveness in reducing the robustness of state-of-the-art pre-trained multi-modal models.

Abstract: Vision-language models, which integrate computer vision and natural language
processing capabilities, have demonstrated significant advancements in tasks
such as image captioning and visual question and answering. However, similar to
traditional models, they are susceptible to small perturbations, posing a
challenge to their robustness, particularly in deployment scenarios. Evaluating
the robustness of these models requires perturbations in both the vision and
language modalities to learn their inter-modal dependencies. In this work, we
train a generic surrogate model that can take both image and text as input and
generate joint representation which is further used to generate adversarial
perturbations for both the text and image modalities. This coordinated attack
strategy is evaluated on the visual question and answering and visual reasoning
datasets using various state-of-the-art vision-language models. Our results
indicate that the proposed strategy outperforms other multi-modal attacks and
single-modality attacks from the recent literature. Our results demonstrate
their effectiveness in compromising the robustness of several state-of-the-art
pre-trained multi-modal models such as instruct-BLIP, ViLT and others.

</details>


### [99] [LLMs Can Compensate for Deficiencies in Visual Representations](https://arxiv.org/abs/2506.05439)
*Sho Takishita,Jay Gala,Abdelrahman Mohamed,Kentaro Inui,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: Study shows language backbones in vision-language models can compensate for weak visual features through contextualization and enrichment, suggesting potential for new model architectures.


<details>
  <summary>Details</summary>
Motivation: Investigate how the language backbone in vision-language models compensates for weak visual features.

Method: Perform controlled self-attention ablations on a probing task using three CLIP-based VLMs.

Result: CLIP visual representations provide semantic information to the language decoder, and the language decoder can compensate when visual contextualization is reduced.

Conclusion: Vision-language models dynamically divide labor between vision and language components, suggesting future architectures could offload more visual processing to the language decoder.

Abstract: Many vision-language models (VLMs) that prove very effective at a range of
multimodal task, build on CLIP-based vision encoders, which are known to have
various limitations. We investigate the hypothesis that the strong language
backbone in VLMs compensates for possibly weak visual features by
contextualizing or enriching them. Using three CLIP-based VLMs, we perform
controlled self-attention ablations on a carefully designed probing task. Our
findings show that despite known limitations, CLIP visual representations offer
ready-to-read semantic information to the language decoder. However, in
scenarios of reduced contextualization in the visual representations, the
language decoder can largely compensate for the deficiency and recover
performance. This suggests a dynamic division of labor in VLMs and motivates
future architectures that offload more visual processing to the language
decoder.

</details>


### [100] [BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models](https://arxiv.org/abs/2506.05440)
*Ludovic Arnould,Salim Khazem,Hugues Ali Mehenni*

Main category: cs.CV

TL;DR: We propose a new evaluation methodology for Visual Language Models (VLMs) inspired by ophthalmologic diagnostics. It uses synthetic images to control visual attributes and precisely reveal perception failures.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks have high annotation costs, risk information leakage, and do not clarify the source of failures in VLMs.

Method: Building collections of synthetic images with gradually more challenging variations in the content of interest while keeping other visual parameters constant.

Result: Our approach allows systematic stress testing and fine-grained failure analysis, shifting the focus from coarse benchmarking toward targeted and interpretable assessment of VLM capabilities.

Conclusion: Our new evaluation methodology provides a more efficient and insightful way to evaluate VLMs compared to current benchmarks.

Abstract: Visual Language Models (VLMs) are now sufficiently advanced to support a
broad range of applications, including answering complex visual questions, and
are increasingly expected to interact with images in varied ways. To evaluate
them, current benchmarks often focus on specific domains (e.g., reading
charts), constructing datasets of annotated real images paired with pre-defined
Multiple Choice Questions (MCQs) to report aggregate accuracy scores. However,
such benchmarks entail high annotation costs, risk information leakage, and do
not clarify whether failures stem from limitations in visual perception,
reasoning, or general knowledge. We propose a new evaluation methodology,
inspired by ophthalmologic diagnostics, leveraging procedural generation of
synthetic images to obtain control over visual attributes and precisely reveal
perception failures in VLMs. Specifically, we build collections of images with
gradually more challenging variations in the content of interest (e.g., number
of objects in a counting task) while holding other visual parameters constant.
This diagnostic allows systematic stress testing and fine-grained failure
analysis, shifting the focus from coarse benchmarking toward targeted and
interpretable assessment of VLM capabilities. Our code is available at
https://github.com/byoeval/BYO-EVAL.

</details>


### [101] [MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning](https://arxiv.org/abs/2506.05523)
*Zikui Cai,Andrew Wang,Anirudh Satheesh,Ankit Nakhawa,Hyunwoo Jae,Keenan Powell,Minghui Liu,Neel Jay,Sungbin Oh,Xiyao Wang,Yongyuan Liang,Tom Goldstein,Furong Huang*

Main category: cs.CV

TL;DR: MORSE-500 introduces a new video benchmark for multimodal reasoning, addressing the shortcomings of current benchmarks and revealing performance gaps in advanced AI models.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing benchmarks in capturing temporal complexity, broadening the scope of reasoning skills, and providing a scalable platform for future research.

Method: MORSE-500 uses programmatically generated clips from Python scripts, generative video models, and curated real footage to create a diverse set of video instances.

Result: State-of-the-art models show significant performance gaps, especially in abstract and planning tasks, indicating the need for further advancements in multimodal reasoning capabilities.

Conclusion: The introduction of MORSE-500 addresses the limitations of current benchmarks by offering a video-based multimodal reasoning environment that evolves with model improvements.

Abstract: Despite rapid advances in vision-language models (VLMs), current benchmarks
for multimodal reasoning fall short in three key dimensions. First, they
overwhelmingly rely on static images, failing to capture the temporal
complexity of real-world environments. Second, they narrowly focus on
mathematical problem-solving, neglecting the broader spectrum of reasoning
skills -- including abstract, physical, planning, spatial, and temporal
capabilities -- required for robust multimodal intelligence. Third, many
benchmarks quickly saturate, offering limited headroom for diagnosing failure
modes or measuring continued progress. We introduce MORSE-500 (Multimodal
Reasoning Stress-test Environment), a video benchmark composed of 500 fully
scripted clips with embedded questions spanning six complementary reasoning
categories. Each instance is programmatically generated using deterministic
Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and
curated real footage. This script-driven design allows fine-grained control
over visual complexity, distractor density, and temporal dynamics -- enabling
difficulty to be scaled systematically as models improve. Unlike static
benchmarks that become obsolete once saturated, MORSE-500 is built to evolve:
its controllable generation pipeline supports the creation of arbitrarily
challenging new instances, making it ideally suited for stress-testing
next-generation models. Initial experiments with state-of-the-art systems --
including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest
available at the time, alongside strong open-source models -- reveal
substantial performance gaps across all categories, with particularly large
deficits in abstract and planning tasks. We release the full dataset,
generation scripts, and evaluation harness to support transparent,
reproducible, and forward-looking multimodal reasoning research.

</details>


### [102] [Do Large Vision-Language Models Distinguish between the Actual and Apparent Features of Illusions?](https://arxiv.org/abs/2506.05765)
*Taiga Shinozaki,Tomoki Doi,Satoshi Nishida,Hitomi Yanaka*

Main category: cs.CV

TL;DR: Investigating whether large vision language models are susceptible to visual illusions reveals their responses might rely more on prior knowledge rather than genuine visual comprehension.


<details>
  <summary>Details</summary>
Motivation: To explore if machines like large vision language models exhibit similar susceptibilities to visual illusions as humans do, addressing the limitations of previous studies using non-abstract images and failing to distinguish actual and apparent features.

Method: Introduced a visual question answering (VQA) dataset divided into genuine and fake illusions with corresponding control images, and evaluated the performance of LVLMs in these tasks.

Result: LVLMs perform similarly in genuine and fake illusion VQA tasks, suggesting their responses could be based on prior knowledge of illusions rather than true visual understanding.

Conclusion: Although large vision language models (LVLMs) may seem to recognize visual illusions by answering questions about both real and apparent features correctly, their responses might rely more on prior knowledge about illusions than genuine visual comprehension.

Abstract: Humans are susceptible to optical illusions, which serve as valuable tools
for investigating sensory and cognitive processes. Inspired by human vision
studies, research has begun exploring whether machines, such as large vision
language models (LVLMs), exhibit similar susceptibilities to visual illusions.
However, studies often have used non-abstract images and have not distinguished
actual and apparent features, leading to ambiguous assessments of machine
cognition. To address these limitations, we introduce a visual question
answering (VQA) dataset, categorized into genuine and fake illusions, along
with corresponding control images. Genuine illusions present discrepancies
between actual and apparent features, whereas fake illusions have the same
actual and apparent features even though they look illusory due to the similar
geometric configuration. We evaluate the performance of LVLMs for genuine and
fake illusion VQA tasks and investigate whether the models discern actual and
apparent features. Our findings indicate that although LVLMs may appear to
recognize illusions by correctly answering questions about both feature types,
they predict the same answers for both Genuine Illusion and Fake Illusion VQA
questions. This suggests that their responses might be based on prior knowledge
of illusions rather than genuine visual understanding. The dataset is available
at https://github.com/ynklab/FILM

</details>


### [103] [Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models](https://arxiv.org/abs/2506.06006)
*Yifu Qiu,Yftah Ziser,Anna Korhonen,Shay B. Cohen,Edoardo M. Ponti*

Main category: cs.CV

TL;DR: Fine-tuning vision-and-language foundation models for dynamics models is easier than for world models. Dynamics models can help bootstrap world models using two strategies: weakly supervised learning from synthetic data and inference time verification.


<details>
  <summary>Details</summary>
Motivation: To explore the capability of vision-and-language foundation models in possessing realistic world and dynamics models when actions are expressed through language.

Method: Fine-tuning models and using dynamics models to bootstrap world models via two strategies: weakly supervised learning and inference time verification.

Result: The resulting world models achieve competitive performance with state-of-the-art image editing models, improving by 15% on real-world subsets and achieving the best average human evaluation across all subsets of Aurora-Bench.

Conclusion: Fine-tuning dynamics models is more feasible than world models, and dynamics models can effectively guide the creation of world models which perform well in action-centric image editing tasks.

Abstract: To what extent do vision-and-language foundation models possess a realistic
world model (observation $\times$ action $\rightarrow$ observation) and a
dynamics model (observation $\times$ observation $\rightarrow$ action), when
actions are expressed through language? While open-source foundation models
struggle with both, we find that fine-tuning them to acquire a dynamics model
through supervision is significantly easier than acquiring a world model. In
turn, dynamics models can be used to bootstrap world models through two main
strategies: 1) weakly supervised learning from synthetic data and 2) inference
time verification. Firstly, the dynamics model can annotate actions for
unlabelled pairs of video frame observations to expand the training data. We
further propose a new objective, where image tokens in observation pairs are
weighted by their importance, as predicted by a recognition model. Secondly,
the dynamics models can assign rewards to multiple samples of the world model
to score them, effectively guiding search at inference time. We evaluate the
world models resulting from both strategies through the task of action-centric
image editing on Aurora-Bench. Our best model achieves a performance
competitive with state-of-the-art image editing models, improving on them by a
margin of $15\%$ on real-world subsets according to GPT4o-as-judge, and
achieving the best average human evaluation across all subsets of Aurora-Bench.

</details>


### [104] [CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval](https://arxiv.org/abs/2506.06144)
*David Wan,Han Wang,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TL;DR: CLaMR is a multimodal video content retrieval system that improves performance by dynamically choosing the most relevant modality(ies) and jointly encoding all modalities.


<details>
  <summary>Details</summary>
Motivation: Retrieval systems often treat different modalities of online video content as independent sources, which can result in noisy and subpar retrieval results. This paper aims to improve retrieval performance by considering multiple modalities simultaneously.

Method: CLaMR introduces a multimodal, late-interaction retriever that jointly indexes 4 modalities: video frames, transcribed speech, on-screen text, and metadata. It also uses MultiVENT 2.0++ and a modality-aware loss for training.

Result: CLaMR improves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4 over the best multi-modality retriever on MultiVENT 2.0++. It also enhances performance on long-video QA tasks.

Conclusion: CLaMR improves retrieval performance by jointly encoding all modalities with a unified multimodal backbone and dynamically selecting the most relevant modality(ies) for a query. It outperforms existing methods on both MultiVENT 2.0++ and MSRVTT datasets.

Abstract: Online video web content is richly multimodal: a single video blends vision,
speech, ambient audio, and on-screen text. Retrieval systems typically treat
these modalities as independent retrieval sources, which can lead to noisy and
subpar retrieval. We explore multimodal video content retrieval, where
relevance can be scored from one particular modality or jointly across multiple
modalities simultaneously. Consequently, an effective retriever must
dynamically choose which modality (or set of modalities) best addresses the
query. We introduce CLaMR, a multimodal, late-interaction retriever that
jointly indexes 4 modalities: video frames, transcribed speech, on-screen text,
and metadata. CLaMR jointly encodes all modalities with a unified multimodal
backbone for improved contextualization and is trained to enhance dynamic
modality selection via two key innovations. First, given the lack of training
data for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale
synthetic training dataset built on MultiVENT 2.0 (event-centric videos in
various languages paired with queries) with modality-targeted queries. Next, we
propose a modality-aware loss that jointly trains according to a standard
contrastive objective alongside an objective for learning correct modality
usage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation
strategies, such as averaging similarities for baseline retrievers, degrade
performance by introducing noise from irrelevant modalities. In contrast, CLaMR
consistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR
improves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4
over the best multi-modality retriever. We illustrate CLaMR's downstream
utility on long-video QA, retrieving relevant frames and obtaining a 3.50%
boost over LanguageBind on Video-MME and 1.42% over dense sampling on
LongVideoBench.

</details>


### [105] [Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding](https://arxiv.org/abs/2506.06275)
*Emmanouil Zaranis,António Farinhas,Saul Santos,Beatriz Canaverde,Miguel Moura Ramos,Aditya K Surikuchi,André Viveiros,Baohao Liao,Elena Bueno-Benito,Nithin Sivakumaran,Pavlo Vasylenko,Shoubin Yu,Sonal Sannigrahi,Wafaa Mohammed,Ben Peters,Danae Sánchez Villegas,Elias Stengel-Eskin,Giuseppe Attanasio,Jaehong Yoon,Stella Frank,Alessandro Suglia,Chrysoula Zerva,Desmond Elliott,Mariella Dimiccoli,Mohit Bansal,Oswald Lanz,Raffaella Bernardi,Raquel Fernández,Sandro Pezzelle,Vlad Niculae,André F. T. Martins*

Main category: cs.CV

TL;DR: Introduce a new benchmark MF^2 for evaluating long-form video understanding by assessing models' ability to comprehend, consolidate, and recall key narrative information from full-length movies.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for vision-language models have limitations, focusing on peripheral details or using questions generated by language models which do not reflect genuine understanding.

Method: Create a benchmark with over 50 full-length movies and manually constructed claim pairs (one true fact and one plausible fib) targeting core narrative elements.

Result: Experiments show that state-of-the-art models perform significantly worse than humans in the binary claim evaluation protocol.

Conclusion: The results highlight the gap between current VLMs and human ability in retaining and reasoning over critical narrative information.

Abstract: Despite recent progress in vision-language models (VLMs), holistic
understanding of long-form video content remains a significant challenge,
partly due to limitations in current benchmarks. Many focus on peripheral,
``needle-in-a-haystack'' details, encouraging context-insensitive retrieval
over deep comprehension. Others rely on large-scale, semi-automatically
generated questions (often produced by language models themselves) that are
easier for models to answer but fail to reflect genuine understanding. In this
paper, we introduce MF$^2$, a new benchmark for evaluating whether models can
comprehend, consolidate, and recall key narrative information from full-length
movies (50-170 minutes long). MF$^2$ includes over 50 full-length,
open-licensed movies, each paired with manually constructed sets of claim pairs
-- one true (fact) and one plausible but false (fib), totalling over 850 pairs.
These claims target core narrative elements such as character motivations and
emotions, causal chains, and event order, and refer to memorable moments that
humans can recall without rewatching the movie. Instead of multiple-choice
formats, we adopt a binary claim evaluation protocol: for each pair, models
must correctly identify both the true and false claims. This reduces biases
like answer ordering and enables a more precise assessment of reasoning. Our
experiments demonstrate that both open-weight and closed state-of-the-art
models fall well short of human performance, underscoring the relative ease of
the task for humans and their superior ability to retain and reason over
critical narrative information -- an ability current VLMs lack.

</details>

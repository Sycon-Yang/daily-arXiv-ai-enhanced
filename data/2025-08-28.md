<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 59]
- [cs.CR](#cs.CR) [Total: 4]
- [math.CO](#math.CO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts](https://arxiv.org/abs/2508.19268)
*Qing Wang,Xue Han,Jiahui Wang,Lehao Xing,Qian Hu,Lianlian Zhang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 本文提出了一种名为MultiPL-MoE的混合专家方法，用于提升大型语言模型在多语言代码生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面表现出色，但多语言代码生成仍然极具挑战性。为了应对这一问题，旨在在保留最受欢迎的LLMs的基础上提高其多编程语言（MultiPL）性能。

Method: 提出了一种名为MultiPL-MoE的混合专家（MoE）方法，结合了两个配对的MoE以在token和segment级别优化专家选择。

Result: 实验结果证明了MultiPL-MoE的有效性。

Conclusion: 实验结果证明了MultiPL-MoE的有效性。

Abstract: Despite LLMs' excellent code creation capabilities, multilingual code
generation remains extremely challenging. To address this, we intent to improve
the multi-programming-lingual (MultiPL) performance of the base LLMs while
retaining the most popular ones using restricted computational resources. We
consider MultiPL to be a special case of multiple natural languages and propose
a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called
MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize
expert selection at both the token and segment levels. The token-level MoE is a
standard upcycling MoE structure with a shared expert and a novel gate weight
normalization approach that aids in the final fusion with the segment-level
MoE. The segment-level MoE incorporates two innovative designs to better
capture the syntactic structure and contextual patterns of programming
languages: First, using a sliding window to partition the input token sequence
into multiple segments; Then, adopting an expert-choice routing strategy that
allows experts to select the top-k segments. The results of the experiment
proved the effectiveness of MultiPL-MoE.

</details>


### [2] [Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English](https://arxiv.org/abs/2508.19270)
*Nguyen Huu Nhat Minh,Tran Nguyen Anh,Truong Dinh Dung,Vo Van Nam,Le Pham Tuyen*

Main category: cs.CL

TL;DR: 本文提出了一种新的双语语音识别方法，解决了越南语和英语混合发音中的跨语言音素识别问题，并展示了其在提高识别准确性和处理声调与重音音素方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 跨语言音素识别在混合越南语和英语发音的自动语音识别（ASR）中是一个重要挑战。越南语依赖声调变化来区分词义，而英语则具有重音模式和非标准发音，这使得两种语言之间的音素对齐变得困难。

Method: 本文提出了两个主要贡献：(1) 构建一个代表性的双语音素集，以弥合越南语和英语语音系统的差异；(2) 设计了一个端到端系统，利用PhoWhisper预训练编码器来获取深层次的表示，从而提高音素识别效果。

Result: 通过广泛的实验，本文提出的方案不仅提高了越南语双语语音识别的准确性，还提供了一个稳健的框架来应对声调和重音为基础的音素识别的复杂性。

Conclusion: 本文提出了一种新的双语语音识别方法，能够有效解决越南语和英语混合发音中的跨语言音素识别问题，并为处理声调和重音为基础的音素识别提供了稳健的框架。

Abstract: Cross-lingual phoneme recognition has emerged as a significant challenge for
accurate automatic speech recognition (ASR) when mixing Vietnamese and English
pronunciations. Unlike many languages, Vietnamese relies on tonal variations to
distinguish word meanings, whereas English features stress patterns and
non-standard pronunciations that hinder phoneme alignment between the two
languages. To address this challenge, we propose a novel bilingual speech
recognition approach with two primary contributions: (1) constructing a
representative bilingual phoneme set that bridges the differences between
Vietnamese and English phonetic systems; (2) designing an end-to-end system
that leverages the PhoWhisper pre-trained encoder for deep high-level
representations to improve phoneme recognition. Our extensive experiments
demonstrate that the proposed approach not only improves recognition accuracy
in bilingual speech recognition for Vietnamese but also provides a robust
framework for addressing the complexities of tonal and stress-based phoneme
recognition

</details>


### [3] [Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT](https://arxiv.org/abs/2508.19271)
*Rushitha Santhoshi Mamidala,Anshuman Chhabra,Ankur Mali*

Main category: cs.CL

TL;DR: 本文提出了一种基于局部加权有限自动机的改进RetoMaton框架，以提高大型语言模型的推理能力，使其更加稳定、可解释，并适用于领域迁移和互操作性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的推理策略如Chain-of-Thought（CoT）和In-Context Learning（ICL）依赖于脆弱、隐式的机制，导致在种子、格式或微小提示变化下输出不一致，因此对于需要稳定、可解释推理的任务来说是不可靠的。

Method: 我们通过将RetoMaton的全局数据存储替换为从外部领域语料库直接构建的局部任务自适应加权有限自动机（WFA）来扩展RetoMaton。

Result: 我们在两个预训练的LLM（LLaMA-3.2-1B和Gemma-3-1B-PT）上评估了这种局部RetoMaton变体，在三个推理任务中（TriviaQA、GSM8K和MMLU），与基础模型和基于提示的方法相比，局部RetoMaton的增强设置在性能上表现出色，并且能够实现透明和可重复的检索动态。

Conclusion: 我们的结果表明，通过轻量级的自动机引导记忆，现代大型语言模型可以实现可信赖的符号推理，这是一个有前景的转变。

Abstract: Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and
In-Context Learning (ICL) have become widely used for eliciting reasoning
capabilities in large language models (LLMs). However, these methods rely on
fragile, implicit mechanisms often yielding inconsistent outputs across seeds,
formats, or minor prompt variations making them fundamentally unreliable for
tasks requiring stable, interpretable reasoning. In contrast, automata-based
neuro-symbolic frameworks like RetoMaton offer a more structured and
trustworthy alternative by grounding retrieval in symbolic memory with
deterministic transitions. In this work, we extend RetoMaton by replacing its
global datastore with a local, task-adaptive Weighted Finite Automaton (WFA),
constructed directly from external domain corpora. This local automaton
structure promotes robust, context-aware retrieval while preserving symbolic
traceability and low inference overhead. Unlike prompting, which entangles
context and memory in opaque ways, our approach leverages the explicit
structure of WFAs to provide verifiable and modular retrieval behavior, making
it better suited for domain transfer and interoperability. We evaluate this
local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT
across three reasoning tasks: TriviaQA (reading comprehension), GSM8K
(multi-step math), and MMLU (domain knowledge). Compared to the base model and
prompting-based methods, augmenting these setups with local RetoMaton
consistently improves performance while enabling transparent and reproducible
retrieval dynamics. Our results highlight a promising shift toward trustworthy,
symbolic reasoning in modern LLMs via lightweight, automaton-guided memory.

</details>


### [4] [RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits](https://arxiv.org/abs/2508.19272)
*Kshitij Fadnis,Sara Rosenthal,Maeda Hanafi,Yannis Katsis,Marina Danilevsky*

Main category: cs.CL

TL;DR: RAGAPHENE是一个用于构建和评估LLMs的基准测试平台，它能够模拟真实世界的对话。


<details>
  <summary>Details</summary>
Motivation: 为了构建高质量的评估基准，需要模拟真实世界的对话。

Method: RAGAPHENE是一个基于聊天的注释平台，允许注释者模拟真实世界的对话以进行基准测试和评估LLMs。

Result: RAGAPHENE已被大约40个注释者成功使用，以构建数千个真实世界的对话。

Conclusion: RAGAPHENE是一个用于构建和评估LLMs的基准测试平台，它能够模拟真实世界的对话。

Abstract: Retrieval Augmented Generation (RAG) is an important aspect of conversing
with Large Language Models (LLMs) when factually correct information is
important. LLMs may provide answers that appear correct, but could contain
hallucinated information. Thus, building benchmarks that can evaluate LLMs on
multi-turn RAG conversations has become an increasingly important task.
Simulating real-world conversations is vital for producing high quality
evaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform
that enables annotators to simulate real-world conversations for benchmarking
and evaluating LLMs. RAGAPHENE has been successfully used by approximately 40
annotators to build thousands of real-world conversations.

</details>


### [5] [Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis](https://arxiv.org/abs/2508.19274)
*Yue Chu*

Main category: cs.CL

TL;DR: 本文研究了如何利用VA叙述进行自动化COD分类，并发现基于PLMs的方法在识别非传染性疾病方面优于现有方法。多模态方法进一步提高了性能，表明每种模态都有独特贡献。


<details>
  <summary>Details</summary>
Motivation: 在没有民事登记和生命统计的国家，VA是估计死亡原因（COD）和制定政策优先事项的关键工具。现有的自动化VA原因分类算法只使用问题，忽略了叙述中的信息。

Method: 本文研究了如何使用预训练语言模型（PLMs）和机器学习（ML）技术，利用VA叙述进行自动化COD分类。还探索了结合叙述和问题的多模态融合策略。

Result: 使用南非的实证数据，本文证明了仅使用叙述，基于变压器的PLMs在个体和人口层面的表现优于领先的仅问题算法，特别是在识别非传染性疾病方面。多模态方法进一步提高了COD分类的性能。

Conclusion: 本文展示了叙述在增强COD分类中的价值，并强调了需要更多高质量、多样化数据来训练和微调PLM/ML方法，同时为重新思考和设计VA工具和访谈提供了有价值的见解。

Abstract: In countries without civil registration and vital statistics, verbal autopsy
(VA) is a critical tool for estimating cause of death (COD) and inform policy
priorities. In VA, interviewers ask proximal informants for details on the
circumstances preceding a death, in the form of unstructured narratives and
structured questions. Existing automated VA cause classification algorithms
only use the questions and ignore the information in the narratives. In this
thesis, we investigate how the VA narrative can be used for automated COD
classification using pretrained language models (PLMs) and machine learning
(ML) techniques. Using empirical data from South Africa, we demonstrate that
with the narrative alone, transformer-based PLMs with task-specific fine-tuning
outperform leading question-only algorithms at both the individual and
population levels, particularly in identifying non-communicable diseases. We
explore various multimodal fusion strategies combining narratives and questions
in unified frameworks. Multimodal approaches further improve performance in COD
classification, confirming that each modality has unique contributions and may
capture valuable information that is not present in the other modality. We also
characterize physician-perceived information sufficiency in VA. We describe
variations in sufficiency levels by age and COD and demonstrate that
classification accuracy is affected by sufficiency for both physicians and
models. Overall, this thesis advances the growing body of knowledge at the
intersection of natural language processing, epidemiology, and global health.
It demonstrates the value of narrative in enhancing COD classification. Our
findings underscore the need for more high-quality data from more diverse
settings to use in training and fine-tuning PLM/ML methods, and offer valuable
insights to guide the rethinking and redesign of the VA instrument and
interview.

</details>


### [6] [FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series](https://arxiv.org/abs/2508.19279)
*Gunjan Jalori,Preetika Verma,Sercan Ö Arık*

Main category: cs.CL

TL;DR: FLAIRR-TS is a test-time prompt optimization framework that uses an agentic system to generate high-quality forecasts without intermediate code generation, showing improved accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Effective forecasting on LLM often relies on extensive pre-processing and fine-tuning. Crafting such a prompt for each task is itself tedious and ad-hoc.

Method: FLAIRR-TS is a test-time prompt optimization framework that utilizes an agentic system: a Forecaster-agent generates forecasts using an initial prompt, which is then refined by a refiner agent, informed by past outputs and retrieved analogs.

Result: Experiments on benchmark datasets show improved accuracy over static prompting and retrieval-augmented baselines, approaching the performance of specialized prompts.

Conclusion: FLAIRR-TS provides a practical alternative to tuning, achieving strong performance via its agentic approach to adaptive prompt refinement and retrieval.

Abstract: Time series Forecasting with large languagemodels (LLMs) requires bridging
numericalpatterns and natural language. Effective fore-casting on LLM often
relies on extensive pre-processing and fine-tuning.Recent studiesshow that a
frozen LLM can rival specializedforecasters when supplied with a carefully
en-gineered natural-language prompt, but craft-ing such a prompt for each task
is itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt
optimization framework thatutilizes an agentic system: a
Forecaster-agentgenerates forecasts using an initial prompt,which is then
refined by a refiner agent, in-formed by past outputs and retrieved
analogs.This adaptive prompting generalizes across do-mains using creative
prompt templates andgenerates high-quality forecasts without inter-mediate code
generation.Experiments onbenchmark datasets show improved accuracyover static
prompting and retrieval-augmentedbaselines, approaching the performance
ofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,
achievingstrong performance via its agentic approach toadaptive prompt
refinement and retrieval.

</details>


### [7] [CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning](https://arxiv.org/abs/2508.19282)
*Ziqiang Cui,Yunpeng Weng,Xing Tang,Peiyang Liu,Shiwei Li,Bowei He,Jiamin Chen,Xiuqiang He,Chen Ma*

Main category: cs.CL

TL;DR: 本文提出CORE方法，通过强化学习实现RAG中的无损上下文压缩，无需预定义压缩标签，利用端任务性能作为奖励信号进行训练，实验证明其在保持性能的同时显著提高压缩比和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法在压缩检索文档时常常牺牲端任务性能，缺乏明确的压缩目标导致依赖固定启发式方法，无法保证压缩内容有效支持端任务。

Method: CORE方法利用强化学习优化压缩过程，不依赖预定义的压缩标签，使用端任务性能作为奖励信号，并应用广义强化学习策略优化（GRPO）来训练压缩器。

Result: CORE方法在四个数据集上表现出色，具有3%的高压缩比，避免了与前置完整文档相比的性能下降，并且平均Exact Match (EM)分数提高了3.3分。

Conclusion: CORE方法在四个数据集上的实验结果表明其优越性，能够在保持性能不下降的同时实现高压缩比，并且提高了平均Exact Match (EM)分数。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the timeliness of knowledge and the factual accuracy of responses in
Large Language Models (LLMs). However, the inclusion of excessive retrieved
documents substantially increases the input length, leading to higher
computational costs. Previous studies have attempted to compress retrieved
documents into shorter texts before in-context integration, but such methods
often compromise end-task performance. The lack of well-defined compression
targets forces many approaches to rely on fixed heuristics, which cannot
guarantee that the compressed content will effectively support the end task. To
address these limitations, we propose CORE, a novel method designed to achieve
lossless context compression for RAG. CORE employs reinforcement learning to
optimize the compression process without relying on predefined compression
labels. Specifically, it utilizes end-task performance as a reward signal and
applies Generalized Reinforcement Learning Policy Optimization (GRPO) to train
the compressor. This end-to-end training framework enables the compressor to
generate summaries that maximize the accuracy of answers generated by the LLM.
Extensive experiments on four datasets demonstrate the superiority of our
approach. With a high compression ratio of 3\%, our method not only avoids
performance degradation compared to prepending full documents across all
datasets but also improves the average Exact Match (EM) score by 3.3 points.
The code will be released soon.

</details>


### [8] [Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains](https://arxiv.org/abs/2508.19357)
*Peiran Zhou,Junnan Zhu,Yichen Shen,Ruoxi Yu*

Main category: cs.CL

TL;DR: CASC is a novel framework that improves RAG by intelligently processing retrieved contexts, leading to more accurate and trustworthy answers in complex domains.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG suffers from information overload and inefficient synthesis in complex domains involving multiple, lengthy, or conflicting documents, leading to inaccurate and untrustworthy answers.

Method: CASC (Context-Adaptive Synthesis and Compression) is a novel framework that intelligently processes retrieved contexts using a Context Analyzer & Synthesizer (CAS) module, which performs key information extraction, cross-document consistency checking, conflict resolution, and question-oriented structured synthesis.

Result: CASC transforms raw, scattered information into a highly condensed, structured, and semantically rich context, significantly reducing the token count and cognitive load for the final Reader LLM.

Conclusion: CASC consistently outperforms strong baselines in complex scientific domains with inherent redundancies and conflicts.

Abstract: Large Language Models (LLMs) excel in language tasks but are prone to
hallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)
mitigates these by grounding LLMs in external knowledge. However, in complex
domains involving multiple, lengthy, or conflicting documents, traditional RAG
suffers from information overload and inefficient synthesis, leading to
inaccurate and untrustworthy answers. To address this, we propose CASC
(Context-Adaptive Synthesis and Compression), a novel framework that
intelligently processes retrieved contexts. CASC introduces a Context Analyzer
& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs
key information extraction, cross-document consistency checking and conflict
resolution, and question-oriented structured synthesis. This process transforms
raw, scattered information into a highly condensed, structured, and
semantically rich context, significantly reducing the token count and cognitive
load for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new
challenging multi-document question answering dataset designed for complex
scientific domains with inherent redundancies and conflicts. Our extensive
experiments demonstrate that CASC consistently outperforms strong baselines.

</details>


### [9] [Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction](https://arxiv.org/abs/2508.19359)
*Fatemeh Haji,Mazal Bethany,Cho-Yu Jason Chiang,Anthony Rios,Peyman Najafirad*

Main category: cs.CL

TL;DR: 本文提出了一种混合方法ARIS，结合了自我混合代理和判别序列标记器，以提高事件抽取的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统判别模型在精确度方面表现良好，但对细微或不常见事件的召回率有限。相反，利用大型语言模型（LLMs）的生成方法提供了更高的语义灵活性和召回率，但存在幻觉和不一致预测的问题。

Method: 我们提出了基于协议的反思推理系统（ARIS），这是一种结合了自我混合代理和判别序列标记器的混合方法。ARIS显式地利用结构化模型共识、基于置信度的过滤和一个LLM反思推理模块来可靠地解决歧义并提高整体事件预测质量。我们进一步研究了分解指令微调以增强LLM事件抽取理解。

Result: 实验表明，我们的方法在三个基准数据集上优于现有的最先进事件抽取方法。

Conclusion: 我们的方法在三个基准数据集上优于现有的最先进事件抽取方法。

Abstract: Event Extraction (EE) involves automatically identifying and extracting
structured information about events from unstructured text, including triggers,
event types, and arguments. Traditional discriminative models demonstrate high
precision but often exhibit limited recall, particularly for nuanced or
infrequent events. Conversely, generative approaches leveraging Large Language
Models (LLMs) provide higher semantic flexibility and recall but suffer from
hallucinations and inconsistent predictions. To address these challenges, we
propose Agreement-based Reflective Inference System (ARIS), a hybrid approach
combining a Self Mixture of Agents with a discriminative sequence tagger. ARIS
explicitly leverages structured model consensus, confidence-based filtering,
and an LLM reflective inference module to reliably resolve ambiguities and
enhance overall event prediction quality. We further investigate decomposed
instruction fine-tuning for enhanced LLM event extraction understanding.
Experiments demonstrate our approach outperforms existing state-of-the-art
event extraction methods across three benchmark datasets.

</details>


### [10] [LongReasonArena: A Long Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2508.19363)
*Jiayu Ding,Shuming Ma,Lei Cui,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: 本文介绍了LongReasonArena，这是一个专门用于评估大型语言模型（LLM）长期推理能力的基准测试。通过控制输入，可以任意扩展所需的推理长度，达到最多100万token的推理。评估结果显示，LongReasonArena对开源和专有LLM都构成了重大挑战。


<details>
  <summary>Details</summary>
Motivation: Existing long-context benchmarks for Large Language Models (LLMs) focus on evaluating comprehension of long inputs, while overlooking the evaluation of long reasoning abilities.

Method: We introduce LongReasonArena, a benchmark specifically designed to assess the long reasoning capabilities of LLMs, which requires models to solve problems by executing multi-step algorithms that reflect key aspects of long reasoning.

Result: Extensive evaluation results demonstrate that LongReasonArena presents a significant challenge for both open-source and proprietary LLMs, with Deepseek-R1 achieving only 7.5% accuracy on our task.

Conclusion: LongReasonArena presents a significant challenge for both open-source and proprietary LLMs, and the accuracy exhibits a linear decline with respect to the logarithm of the expected number of reasoning steps.

Abstract: Existing long-context benchmarks for Large Language Models (LLMs) focus on
evaluating comprehension of long inputs, while overlooking the evaluation of
long reasoning abilities. To address this gap, we introduce LongReasonArena, a
benchmark specifically designed to assess the long reasoning capabilities of
LLMs. Our tasks require models to solve problems by executing multi-step
algorithms that reflect key aspects of long reasoning, such as retrieval and
backtracking. By controlling the inputs, the required reasoning length can be
arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most
challenging tasks. Extensive evaluation results demonstrate that
LongReasonArena presents a significant challenge for both open-source and
proprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our
task. Further analysis also reveals that the accuracy exhibits a linear decline
with respect to the logarithm of the expected number of reasoning steps. Our
code and data is available at
https://github.com/LongReasonArena/LongReasonArena.

</details>


### [11] [Database Entity Recognition with Data Augmentation and Deep Learning](https://arxiv.org/abs/2508.19372)
*Zikun Fu,Chen Yang,Kourosh Davoudi,Ken Q. Pu*

Main category: cs.CL

TL;DR: 本文提出了一种新的DB-ER方法，包括一个基准、数据增强和基于T5的模型，取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决自然语言查询中的数据库实体识别（DB-ER）挑战，并通过提出一个新的基准、数据增强方法和专门的语言模型来推进该领域。

Method: 我们提出了一个基于T5的专门语言模型，用于实体识别，并进行了两个下游DB-ER任务：序列标记和令牌分类以进行后端微调和执行DB-ER。

Result: 我们的DB-ER标记器与两种最先进的NER标记器进行了比较，并在精度和召回率方面表现出更好的性能。消融评估显示，数据增强将精度和召回率提高了超过10%，而T5主干的微调则提高了5-10%。

Conclusion: 我们的模型在精度和召回率方面都优于现有的最先进的NER标记器。数据增强和T5主干的微调显著提高了这些指标。

Abstract: This paper addresses the challenge of Database Entity Recognition (DB-ER) in
Natural Language Queries (NLQ). We present several key contributions to advance
this field: (1) a human-annotated benchmark for DB-ER task, derived from
popular text-to-sql benchmarks, (2) a novel data augmentation procedure that
leverages automatic annotation of NLQs based on the corresponding SQL queries
which are available in popular text-to-SQL benchmarks, (3) a specialized
language model based entity recognition model using T5 as a backbone and two
down-stream DB-ER tasks: sequence tagging and token classification for
fine-tuning of backend and performing DB-ER respectively. We compared our DB-ER
tagger with two state-of-the-art NER taggers, and observed better performance
in both precision and recall for our model. The ablation evaluation shows that
data augmentation boosts precision and recall by over 10%, while fine-tuning of
the T5 backbone boosts these metrics by 5-10%.

</details>


### [12] [One Joke to Rule them All? On the (Im)possibility of Generalizing Humor](https://arxiv.org/abs/2508.19402)
*Mor Turgeman,Chen Shani,Dafna Shahaf*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在不同幽默类型之间的迁移能力。实验结果显示，模型能够在不同幽默类型之间进行一定程度的迁移，并且在多样化的训练数据下，迁移能力有所提高。此外，研究还发现不同幽默类型之间存在一定的关联性，其中父亲笑话（Dad Jokes）意外地成为迁移的最佳促进者，但难以被迁移。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨在特定幽默任务上的能力是否能够转移到新的、未见过的幽默类型。随着在线和社交媒体环境中不断出现新的幽默形式（如表情包、反幽默、AI失败等），大型语言模型需要具备跨幽默类型的泛化能力，以适应这一不断变化的环境。

Method: 研究通过在四个数据集上进行一系列迁移学习实验，评估了大型语言模型（LLMs）在不同幽默任务之间的迁移能力。实验中，模型在不同多样性设置下进行训练（1-3个数据集用于训练，测试一个新的任务）。

Result: 实验结果表明，模型能够在不同幽默类型之间进行一定程度的迁移，并且在多样化的训练数据下，迁移能力有所提高。模型在未见过的数据集上可以达到高达75%的准确率。此外，训练在多样化来源上提高了迁移能力（1.88-4.05%），同时在域内性能上几乎没有下降。进一步分析表明，幽默类型之间存在一定的关联性，其中父亲笑话（Dad Jokes）意外地成为迁移的最佳促进者，但难以被迁移。

Conclusion: 研究发现，模型能够在不同幽默类型之间进行一定程度的迁移，并且在多样化的训练数据下，迁移能力有所提高。此外，研究还发现不同幽默类型之间存在一定的关联性，其中父亲笑话（Dad Jokes）意外地成为迁移的最佳促进者，但难以被迁移。

Abstract: Humor is a broad and complex form of communication that remains challenging
for machines. Despite its broadness, most existing research on computational
humor traditionally focused on modeling a specific type of humor. In this work,
we wish to understand whether competence on one or more specific humor tasks
confers any ability to transfer to novel, unseen types; in other words, is this
fragmentation inevitable? This question is especially timely as new humor types
continuously emerge in online and social media contexts (e.g., memes,
anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this
evolving landscape, they must be able to generalize across humor types by
capturing deeper, transferable mechanisms. To investigate this, we conduct a
series of transfer learning experiments across four datasets, representing
different humor tasks. We train LLMs under varied diversity settings (1-3
datasets in training, testing on a novel task). Experiments reveal that models
are capable of some transfer, and can reach up to 75% accuracy on unseen
datasets; training on diverse sources improves transferability (1.88-4.05%)
with minimal-to-no drop in in-domain performance. Further analysis suggests
relations between humor types, with Dad Jokes surprisingly emerging as the best
enabler of transfer (but is difficult to transfer to). We release data and
code.

</details>


### [13] [A perishable ability? The future of writing in the face of generative artificial intelligence](https://arxiv.org/abs/2508.19427)
*Evandro L. T. P. Cunha*

Main category: cs.CL

TL;DR: 文章探讨了生成式人工智能工具的发展可能导致人类写作能力的下降，并将其与历史上的类似情况进行了比较。


<details>
  <summary>Details</summary>
Motivation: 文章旨在讨论生成式人工智能工具对人类写作能力的潜在影响，并提供历史类比以加深理解。

Method: 文章通过分析生成式人工智能工具的发展及其在不同领域中的应用，探讨了人类写作能力可能受到的影响。

Result: 文章指出，随着生成式人工智能工具的普及，人类可能逐渐失去写作能力，这一现象类似于历史上的某些时期。

Conclusion: 文章探讨了人类可能因为将写作任务外包给机器而失去或显著降低写作能力的未来可能性，并将其与历史上的希腊黑暗时代相比较。

Abstract: The 2020s have been witnessing a very significant advance in the development
of generative artificial intelligence tools, including text generation systems
based on large language models. These tools have been increasingly used to
generate texts in the most diverse domains -- from technical texts to literary
texts --, which might eventually lead to a lower volume of written text
production by humans. This article discusses the possibility of a future in
which human beings will have lost or significantly decreased their ability to
write due to the outsourcing of this activity to machines. This possibility
parallels the loss of the ability to write in other moments of human history,
such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).

</details>


### [14] [Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)](https://arxiv.org/abs/2508.19428)
*Aleksandra Beliaeva,Temurbek Rahmatullaev*

Main category: cs.CL

TL;DR: 本文提出了一种全面的系统来解决LLMs4OL 2025挑战中的任务A、B和C，通过结合检索增强提示、零样本分类和基于注意力的图建模方法，实现了在所有三个任务中的顶级排名结果。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs4OL 2025挑战中的任务A、B和C，涵盖整个本体构建流程：术语提取、类型分配和分类发现。

Method: 结合了检索增强提示、零样本分类和基于注意力的图建模，每个方法都针对各自的任务需求进行了定制。

Result: 在官方排行榜上，在所有三个任务中都取得了顶级排名结果。

Conclusion: 这些策略展示了基于LLM的架构在跨异构领域的本体学习中的可扩展性、适应性和鲁棒性。

Abstract: We present a comprehensive system for addressing Tasks A, B, and C of the
LLMs4OL 2025 challenge, which together span the full ontology construction
pipeline: term extraction, typing, and taxonomy discovery. Our approach
combines retrieval-augmented prompting, zero-shot classification, and
attention-based graph modeling -- each tailored to the demands of the
respective task. For Task A, we jointly extract domain-specific terms and their
ontological types using a retrieval-augmented generation (RAG) pipeline.
Training data was reformulated into a document to terms and types
correspondence, while test-time inference leverages semantically similar
training examples. This single-pass method requires no model finetuning and
improves overall performance through lexical augmentation Task B, which
involves assigning types to given terms, is handled via a dual strategy. In the
few-shot setting (for domains with labeled training data), we reuse the RAG
scheme with few-shot prompting. In the zero-shot setting (for previously unseen
domains), we use a zero-shot classifier that combines cosine similarity scores
from multiple embedding models using confidence-based weighting. In Task C, we
model taxonomy discovery as graph inference. Using embeddings of type labels,
we train a lightweight cross-attention layer to predict is-a relations by
approximating a soft adjacency matrix. These modular, task-specific solutions
enabled us to achieve top-ranking results in the official leaderboard across
all three tasks. Taken together these strategies showcase the scalability,
adaptability, and robustness of LLM-based architectures for ontology learning
across heterogeneous domains.
  Code is available at:
https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek

</details>


### [15] [Bridging Language Gaps: Enhancing Few-Shot Language Adaptation](https://arxiv.org/abs/2508.19464)
*Philipp Borchert,Jochen De Weerdt,Marie-Francine Moens*

Main category: cs.CL

TL;DR: CoLAP是一种数据高效的多语言NLP方法，通过对比学习与跨语言表示结合，有效缩小了跨语言性能差距。


<details>
  <summary>Details</summary>
Motivation: 多语言NLP中语言资源的差异是一个挑战，高资源语言受益于大量数据，而低资源语言缺乏足够的数据进行有效训练。

Method: CoLAP方法通过将对比学习与跨语言表示相结合，实现从高资源语言到低资源语言的任务特定知识迁移。

Result: CoLAP在自然语言理解任务中表现出色，即使在有限的数据下也能超越少样本跨语言迁移基线和上下文学习。

Conclusion: CoLAP有效缩小了跨语言性能差距，有助于开发更高效的多语言NLP技术。

Abstract: The disparity in language resources poses a challenge in multilingual NLP,
with high-resource languages benefiting from extensive data, while low-resource
languages lack sufficient data for effective training. Our Contrastive Language
Alignment with Prompting (CoLAP) method addresses this gap by integrating
contrastive learning with cross-lingual representations, facilitating
task-specific knowledge transfer from high-resource to lower-resource
languages. The primary advantage of our approach is its data efficiency,
enabling rapid adaptation to new languages and reducing the need for large
labeled datasets. We conduct experiments with multilingual encoder-only and
decoder-only language models on natural language understanding tasks, including
natural language inference and relation extraction, evaluating performance
across both high- and low-resource languages. Our results demonstrate that
CoLAP outperforms few-shot cross-lingual transfer baselines and in-context
learning, even with limited available data. This effectively narrows the
cross-lingual performance gap, contributing to the development of more
efficient multilingual NLP techniques.

</details>


### [16] [Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset](https://arxiv.org/abs/2508.19467)
*Sumon Kanti Dey,Jeanne M. Powell,Azra Ismail,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.CL

TL;DR: 研究提出了一种用于从社交媒体中提取阿片类药物使用影响的命名实体识别框架，并引入了一个高质量的数据集。结果显示，微调模型在性能上优于大语言模型，但与专家一致性仍有差距。


<details>
  <summary>Details</summary>
Motivation: 非医疗用途的阿片类药物使用是一个紧迫的公共卫生挑战，其临床和社会后果往往在传统医疗环境中被低估。社交媒体平台为了解这些影响提供了有价值的见解，但尚未得到充分利用。

Method: 研究提出了一种命名实体识别（NER）框架，用于从与阿片类药物使用相关的社交媒体叙述中提取两类自我报告的影响：ClinicalImpacts（例如，戒断、抑郁）和SocialImpacts（例如，失业）。为了支持这项任务，引入了RedditImpacts 2.0数据集，该数据集具有改进的标注指南和对第一人称披露的关注，解决了之前工作的关键限制。评估了微调的编码器模型和最先进的大语言模型（LLMs）在零样本和少样本上下文学习设置下的表现。

Result: 微调的DeBERTa-large模型在宽松的标记级别F1上达到了0.61 [95% CI: 0.43-0.62]，在精度、跨度准确性和遵循任务特定指南方面 consistently 超过LLMs。此外，研究表明，可以使用更少的标记数据实现强大的NER性能，强调了在资源有限的环境中部署稳健模型的可行性。

Conclusion: 研究结果表明，领域特定的微调对于临床NLP任务具有重要价值，并为负责任地开发可能增强成瘾监测、提高可解释性和支持现实世界医疗决策的AI工具做出了贡献。然而，最佳模型的表现仍然显著低于专家间的一致性，表明在需要深度领域知识的任务中，专家智能与当前最先进的NER/AI能力之间仍存在差距。

Abstract: Nonmedical opioid use is an urgent public health challenge, with far-reaching
clinical and social consequences that are often underreported in traditional
healthcare settings. Social media platforms, where individuals candidly share
first-person experiences, offer a valuable yet underutilized source of insight
into these impacts. In this study, we present a named entity recognition (NER)
framework to extract two categories of self-reported consequences from social
media narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,
depression) and SocialImpacts (e.g., job loss). To support this task, we
introduce RedditImpacts 2.0, a high-quality dataset with refined annotation
guidelines and a focus on first-person disclosures, addressing key limitations
of prior work. We evaluate both fine-tuned encoder-based models and
state-of-the-art large language models (LLMs) under zero- and few-shot
in-context learning settings. Our fine-tuned DeBERTa-large model achieves a
relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming
LLMs in precision, span accuracy, and adherence to task-specific guidelines.
Furthermore, we show that strong NER performance can be achieved with
substantially less labeled data, emphasizing the feasibility of deploying
robust models in resource-limited settings. Our findings underscore the value
of domain-specific fine-tuning for clinical NLP tasks and contribute to the
responsible development of AI tools that may enhance addiction surveillance,
improve interpretability, and support real-world healthcare decision-making.
The best performing model, however, still significantly underperforms compared
to inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap
persists between expert intelligence and current state-of-the-art NER/AI
capabilities for tasks requiring deep domain knowledge.

</details>


### [17] [Automatic Question & Answer Generation Using Generative Large Language Model (LLM)](https://arxiv.org/abs/2508.19475)
*Md. Alvee Ehsan,A. S. M Mehedi Hasan,Kefaya Benta Shahnoor,Syeda Sumaiya Tasneem*

Main category: cs.CL

TL;DR: 本研究提出了一种基于微调生成式大语言模型和无监督学习方法的自动问答生成系统，旨在简化教育评估过程。


<details>
  <summary>Details</summary>
Motivation: 学生评估在教育中与传授知识同样重要。目前，教师需要设计多种问题以确保公平性，这通常需要手动处理多种讲义材料，过程繁琐且具有挑战性。

Method: 本研究利用提示工程（PE）来适应教师的偏好问题风格（选择题、概念性问题或事实性问题），并采用无监督学习方法进行自然语言处理，主要关注英语语言，并将RACE数据集作为训练数据对基础Meta-Llama 2-7B模型进行微调。

Result: 本研究开发了一个定制化的模型，能够为教育工作者、教师和参与文本评估的个人提供高效的解决方案，从而节省宝贵的时间和资源。

Conclusion: 通过使用微调的生成式大语言模型和无监督学习方法，本研究旨在为教育工作者提供一个高效的问题生成工具，从而简化评估过程。

Abstract: \Abstract{In the realm of education, student evaluation holds equal
significance as imparting knowledge. To be evaluated, students usually need to
go through text-based academic assessment methods. Instructors need to make
diverse sets of questions that need to be fair for all students to prove their
adequacy over a particular topic. This can prove to be quite challenging as
they may need to manually go through several different lecture materials. Our
objective is to make this whole process much easier by implementing Automatic
Question Answer Generation /(AQAG), using fine-tuned generative LLM. For
tailoring the instructor's preferred question style (MCQ, conceptual, or
factual questions), prompt Engineering (PE) is being utilized. In this
research, we propose to leverage unsupervised learning methods in NLP,
primarily focusing on the English language. This approach empowers the base
Meta-Llama 2-7B model to integrate RACE dataset as training data for the
fine-tuning process. Creating a customized model that will offer efficient
solutions for educators, instructors, and individuals engaged in text-based
evaluations. A reliable and efficient tool for generating questions and answers
can free up valuable time and resources, thus streamlining their evaluation
processes.}

</details>


### [18] [Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study](https://arxiv.org/abs/2508.19481)
*Manuel Mosquera,Melissa Robles,Johan Rodriguez,Ruben Manrique*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过集成外部词典工具和强化学习来改善低资源语言的机器翻译效果。实验结果表明，该方法在西班牙语-瓦尤纳伊基语测试集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 低资源机器翻译仍然是大型语言模型面临的重要挑战，因为这些模型在预训练期间缺乏对这些语言的接触，并且用于微调的平行数据有限。

Method: 研究提出了一种新方法，通过集成外部词典工具和使用强化学习进行端到端训练，以增强低资源语言的翻译效果。此外，还结合了监督微调和引导奖励策略优化（GRPO），使模型能够有效学习何时以及如何使用工具。

Result: 实验结果表明，该方法在西班牙语-瓦尤纳伊基语测试集上比之前的工作提高了3.37 BLEU，相对于没有词典访问的监督基线提高了18%的相对增益。

Conclusion: 研究结果表明，将大型语言模型与外部工具结合，并利用强化学习提高翻译质量，在低资源语言场景中具有很大的潜力。

Abstract: Low-resource machine translation remains a significant challenge for large
language models (LLMs), which often lack exposure to these languages during
pretraining and have limited parallel data for fine-tuning. We propose a novel
approach that enhances translation for low-resource languages by integrating an
external dictionary tool and training models end-to-end using reinforcement
learning, in addition to supervised fine-tuning. Focusing on the
Spanish-Wayuunaiki language pair, we frame translation as a tool-augmented
decision-making problem in which the model can selectively consult a bilingual
dictionary during generation. Our method combines supervised instruction tuning
with Guided Reward Policy Optimization (GRPO), enabling the model to learn both
when and how to use the tool effectively. BLEU similarity scores are used as
rewards to guide this learning process. Preliminary results show that our
tool-augmented models achieve up to +3.37 BLEU improvement over previous work,
and a 18% relative gain compared to a supervised baseline without dictionary
access, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared
Task. We also conduct ablation studies to assess the effects of model
architecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other
models such as LLaMA and a prior NLLB-based system. These findings highlight
the promise of combining LLMs with external tools and the role of reinforcement
learning in improving translation quality in low-resource language settings.

</details>


### [19] [Rule Synergy Analysis using LLMs: State of the Art and Implications](https://arxiv.org/abs/2508.19484)
*Bahar Bateni,Benjamin Pratt,Jim Whitehead*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在动态环境中理解复杂规则交互的能力，发现它们在检测正向和负向协同方面存在困难，并提出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 我们想了解大型语言模型在动态环境中理解复杂规则交互的能力。

Method: 我们引入了一个来自游戏Slay the Spire的卡牌协同数据集，并评估了大型语言模型在识别卡牌之间正向、负向或中性相互作用的能力。

Result: 虽然大型语言模型在识别非协同对方面表现出色，但在检测正向和特别是负向协同方面存在困难。

Conclusion: 我们的研究结果表明，需要未来的研究来改进模型在预测规则及其相互作用效果方面的性能。

Abstract: Large language models (LLMs) have demonstrated strong performance across a
variety of domains, including logical reasoning, mathematics, and more. In this
paper, we investigate how well LLMs understand and reason about complex rule
interactions in dynamic environments, such as card games. We introduce a
dataset of card synergies from the game Slay the Spire, where pairs of cards
are classified based on their positive, negative, or neutral interactions. Our
evaluation shows that while LLMs excel at identifying non-synergistic pairs,
they struggle with detecting positive and, particularly, negative synergies. We
categorize common error types, including issues with timing, defining game
states, and following game rules. Our findings suggest directions for future
research to improve model performance in predicting the effect of rules and
their interactions.

</details>


### [20] [Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding](https://arxiv.org/abs/2508.19529)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法Blockwise SFT，以解决标准监督微调与离散扩散语言模型的半自回归推理之间的不匹配问题。通过将响应划分为固定大小的块并仅在活动块上计算损失，该方法直接反映了块解码过程，从而提高了文本生成的效果。


<details>
  <summary>Details</summary>
Motivation: 标准的监督微调（SFT）与离散扩散语言模型的半自回归推理不一致：训练时随机遮蔽整个响应中的标记，而推理时按固定大小的块顺序生成。这种不匹配引入了噪声前缀和泄漏后缀，使梯度偏离所需的块状似然性。

Method: 我们提出了Blockwise SFT，它将响应划分为固定大小的块，每一步选择一个活动块进行随机遮蔽，冻结所有前面的标记，并完全隐藏未来的标记。损失仅在活动块上计算，直接反映块解码过程。

Result: 在GSM8K、MATH和MetaMathQA上的实验显示，在相同的计算或标记预算下，Blockwise SFT相比传统SFT有持续的提升。块大小一致性研究和消融实验确认，改进源于忠实的训练-推理对齐，而不是偶然的遮蔽效应。

Conclusion: 我们的结果突显了在基于扩散的语言模型中，将监督粒度与解码过程相匹配的重要性。

Abstract: Discrete diffusion language models have shown strong potential for text
generation, yet standard supervised fine-tuning (SFT) misaligns with their
semi-autoregressive inference: training randomly masks tokens across the entire
response, while inference generates fixed-size blocks sequentially. This
mismatch introduces noisy prefixes and leaky suffixes, biasing gradients away
from the desired blockwise likelihood. We propose Blockwise SFT, which
partitions responses into fixed-size blocks, selects one active block per step
for stochastic masking, freezes all preceding tokens, and fully hides future
ones. Loss is computed only over the active block, directly mirroring the
blockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show
consistent gains over classical SFT under equal compute or token budgets. Block
size consistency studies and ablations confirm that improvements stem from
faithful training-inference alignment rather than incidental masking effects.
Our results highlight the importance of matching supervision granularity to the
decoding procedure in diffusion-based language models.

</details>


### [21] [Alignment with Fill-In-the-Middle for Enhancing Code Generation](https://arxiv.org/abs/2508.19532)
*Houxing Ren,Zimu Lu,Weikang Shi,Haotian Hou,Yunqiao Yang,Ke Wang,Aojun Zhou,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过将代码片段拆分为更小、更细粒度的块，并引入抽象语法树（AST）拆分和课程训练方法来增强DPO训练，从而在代码生成任务中实现了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 由于可验证的准确测试用例的训练数据有限，改进代码相关任务的性能仍然具有挑战性。虽然直接偏好优化（DPO）显示出前景，但现有的生成测试用例的方法仍存在限制。

Method: 我们提出了一种新方法，将代码片段拆分为更小、更细粒度的块，从相同的测试用例中创建更多样化的DPO对。此外，我们引入了抽象语法树（AST）拆分和课程训练方法来增强DPO训练。

Result: 我们的方法在代码生成任务中表现出显著的改进，如在HumanEval (+), MBPP (+), APPS, LiveCodeBench和BigCodeBench等基准数据集上的实验所验证的那样。

Conclusion: 我们的方法在代码生成任务中表现出显著的改进，如在HumanEval (+), MBPP (+), APPS, LiveCodeBench和BigCodeBench等基准数据集上的实验所验证的那样。

Abstract: The code generation capabilities of Large Language Models (LLMs) have
advanced applications like tool invocation and problem-solving. However,
improving performance in code-related tasks remains challenging due to limited
training data that is verifiable with accurate test cases. While Direct
Preference Optimization (DPO) has shown promise, existing methods for
generating test cases still face limitations. In this paper, we propose a novel
approach that splits code snippets into smaller, granular blocks, creating more
diverse DPO pairs from the same test cases. Additionally, we introduce the
Abstract Syntax Tree (AST) splitting and curriculum training method to enhance
the DPO training. Our approach demonstrates significant improvements in code
generation tasks, as validated by experiments on benchmark datasets such as
HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data
are available at https://github.com/SenseLLM/StructureCoder.

</details>


### [22] [Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation](https://arxiv.org/abs/2508.19533)
*Kun Peng,Cong Cao,Hao Peng,Guanlin Wu,Zhifeng Hao,Lei Jiang,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文介绍了UERC任务和ProEmoTrans框架，以解决对话中未见过的情感识别问题。


<details>
  <summary>Details</summary>
Motivation: 当前的对话情感识别（ERC）研究遵循封闭领域假设，但心理学中没有明确的情感分类共识，这给模型在现实应用中识别未见过的情感带来了挑战。为了弥补这一差距，我们首次引入了UERC任务。

Method: 我们提出了ProEmoTrans，这是一种基于原型的情感迁移框架，并采用了LLM增强的描述方法、无参数机制以及改进的Attention Viterbi Decoding (AVD) 方法来解决关键挑战。

Result: 我们的方法在三个数据集上表现出色，为这一新领域提供了强大的基线。

Conclusion: 我们的方法在三个数据集上的广泛实验表明，它为这一新领域的初步探索提供了一个强大的基线。

Abstract: Current Emotion Recognition in Conversation (ERC) research follows a
closed-domain assumption. However, there is no clear consensus on emotion
classification in psychology, which presents a challenge for models when it
comes to recognizing previously unseen emotions in real-world applications. To
bridge this gap, we introduce the Unseen Emotion Recognition in Conversation
(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based
emotion transfer framework. This prototype-based approach shows promise but
still faces key challenges: First, implicit expressions complicate emotion
definition, which we address by proposing an LLM-enhanced description approach.
Second, utterance encoding in long conversations is difficult, which we tackle
with a proposed parameter-free mechanism for efficient encoding and overfitting
prevention. Finally, the Markovian flow nature of emotions is hard to transfer,
which we address with an improved Attention Viterbi Decoding (AVD) method to
transfer seen emotion transitions to unseen emotions. Extensive experiments on
three datasets show that our method serves as a strong baseline for preliminary
exploration in this new area.

</details>


### [23] [Language Models Identify Ambiguities and Exploit Loopholes](https://arxiv.org/abs/2508.19546)
*Jio Choi,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: 研究显示，大型语言模型能够识别并利用漏洞，这可能构成人工智能安全风险。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型对漏洞的响应提供了两个机会：一是通过检查模型对模糊性和语用学的处理来了解模型的行为；二是探讨模型在面对冲突目标时可能利用模糊性以自身优势行事的对齐问题。

Method: 我们设计了场景，其中大型语言模型被给予一个目标和一个与目标冲突的模糊用户指令，并测量不同模型利用漏洞满足其给定目标而非用户目标的能力。

Result: 我们发现，无论是封闭源代码还是更强的开源模型都能识别歧义并利用由此产生的漏洞。

Conclusion: 我们的分析表明，能够利用漏洞的模型会明确识别和推理歧义和冲突的目标，这可能带来潜在的人工智能安全风险。

Abstract: Studying the responses of large language models (LLMs) to loopholes presents
a two-fold opportunity. First, it affords us a lens through which to examine
ambiguity and pragmatics in LLMs, since exploiting a loophole requires
identifying ambiguity and performing sophisticated pragmatic reasoning. Second,
loopholes pose an interesting and novel alignment problem where the model is
presented with conflicting goals and can exploit ambiguities to its own
advantage. To address these questions, we design scenarios where LLMs are given
a goal and an ambiguous user instruction in conflict with the goal, with
scenarios covering scalar implicature, structural ambiguities, and power
dynamics. We then measure different models' abilities to exploit loopholes to
satisfy their given goals as opposed to the goals of the user. We find that
both closed-source and stronger open-source models can identify ambiguities and
exploit their resulting loopholes, presenting a potential AI safety risk. Our
analysis indicates that models which exploit loopholes explicitly identify and
reason about both ambiguity and conflicting goals.

</details>


### [24] [Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts](https://arxiv.org/abs/2508.19578)
*Jiaqi Deng,Yuho Lee,Nicole Hee-Yeon Kim,Hyangsuk Min,Taewon Yun,Minjeong Ban,Kim Yul,Hwanjun Song*

Main category: cs.CL

TL;DR: HAMLET是一种全面且自动化的框架，用于评估大型语言模型的长上下文理解能力。它通过结构化源文本和查询聚焦的摘要来评估模型的表现，并显示其自动评估与人类判断高度一致。实验结果表明，大型语言模型在细粒度理解和位置效应方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法在评估大型语言模型的长上下文理解能力时存在不足，需要一种更全面和自动化的框架。

Method: HAMLET通过将源文本结构化为三级关键事实层次（根级、分支级和叶级），并使用以查询为中心的摘要来评估模型在每个级别上回忆和忠实表示信息的能力，从而实现对长上下文理解的全面自动化评估。

Result: HAMLET的自动评估与专家人类判断的协议率超过90%，同时成本降低了25倍。实验结果显示，LLMs在细粒度理解方面存在困难，特别是在叶子级别，并且对位置效应敏感。分析性查询比叙述性查询更具挑战性，开源模型和专有模型之间以及不同规模的模型之间存在一致的性能差距。

Conclusion: HAMLET揭示了大型语言模型在细粒度理解方面存在困难，特别是在叶子级别，并且对位置效应敏感。分析性查询比叙述性查询更具挑战性，开源模型和专有模型之间以及不同规模的模型之间存在一致的性能差距。

Abstract: We introduce HAMLET, a holistic and automated framework for evaluating the
long-context comprehension of large language models (LLMs). HAMLET structures
source texts into a three-level key-fact hierarchy at root-, branch-, and
leaf-levels, and employs query-focused summarization to evaluate how well
models recall and faithfully represent information at each level. To validate
the reliability of our fully automated pipeline, we conduct a systematic human
study, showing that our automatic evaluation achieves over 90% agreement with
expert human judgments, while reducing the cost by up to 25 times. HAMLET
reveals that LLMs struggle with fine-grained comprehension, especially at the
leaf level, and are sensitive to positional effects like the
lost-in-the-middle. Analytical queries pose greater challenges than narrative
ones, and consistent performance gaps emerge between open-source and
proprietary models, as well as across model scales. Our code and dataset are
publicly available at https://github.com/DISL-Lab/HAMLET.

</details>


### [25] [ArgCMV: An Argument Summarization Benchmark for the LLM-era](https://arxiv.org/abs/2508.19580)
*Omkar Gurjar,Agam Goyal,Eshwar Chandrasekharan*

Main category: cs.CL

TL;DR: 本文提出一个新的ArgCMV数据集，用于长上下文在线讨论的关键点提取，以推动下一代LLM驱动的摘要研究。


<details>
  <summary>Details</summary>
Motivation: 现有的KP提取方法主要在ArgKP21数据集上进行评估，但该数据集存在一些主要限制，需要更代表实际人类对话的新基准。

Method: 使用最先进的大型语言模型（LLMs）创建了一个新的论点关键点提取数据集ArgCMV，该数据集包含来自实际在线人类辩论的约12K个论点，分布在3K多个主题上。

Result: 我们的数据集展示了更高的复杂性，例如更长、相互参考的论点，主观话语单元的更高出现率以及更广泛的主题范围。现有方法在ArgCMV上表现不佳，并通过实验现有基线和最新开源模型提供了广泛的基准结果。

Conclusion: 本文介绍了ArgCMV数据集，这是一个新的用于长上下文在线讨论的关键点提取数据集，为下一代LLM驱动的摘要研究奠定了基础。

Abstract: Key point extraction is an important task in argument summarization which
involves extracting high-level short summaries from arguments. Existing
approaches for KP extraction have been mostly evaluated on the popular ArgKP21
dataset. In this paper, we highlight some of the major limitations of the
ArgKP21 dataset and demonstrate the need for new benchmarks that are more
representative of actual human conversations. Using SoTA large language models
(LLMs), we curate a new argument key point extraction dataset called ArgCMV
comprising of around 12K arguments from actual online human debates spread
across over 3K topics. Our dataset exhibits higher complexity such as longer,
co-referencing arguments, higher presence of subjective discourse units, and a
larger range of topics over ArgKP21. We show that existing methods do not adapt
well to ArgCMV and provide extensive benchmark results by experimenting with
existing baselines and latest open source models. This work introduces a novel
KP extraction dataset for long-context online discussions, setting the stage
for the next generation of LLM-driven summarization research.

</details>


### [26] [Towards stable AI systems for Evaluating Arabic Pronunciations](https://arxiv.org/abs/2508.19587)
*Hadi Zaatiti,Hatem Hajri,Osama Abdullah,Nader Masmoudi*

Main category: cs.CL

TL;DR: 本研究探讨了阿拉伯语孤立字母识别的挑战，并提出了改进方法。通过引入新的语料库和训练策略，提高了模型的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 孤立字母的识别对于语言学习、语音治疗和语音研究至关重要。然而，由于缺乏共音线索、没有词汇上下文以及持续时间短，使得这一任务具有挑战性。此外，阿拉伯语的强调辅音和其他在许多语言中没有类似发音的声音也增加了难度。

Method: 本研究引入了一个多样化、带变音符号的阿拉伯语孤立字母语料库，并使用wav2vec 2.0模型进行训练。通过添加小幅度扰动（epsilon = 0.05）来测试模型的鲁棒性，并应用对抗训练以恢复性能。

Result: 使用wav2vec 2.0模型在该语料库上的准确率为35%。训练一个轻量级神经网络可以将性能提升至65%。然而，添加小幅度扰动会将准确率降至32%。通过对抗训练，可以将噪声语音的下降限制在9%，同时保持干净语音的准确性。

Conclusion: 本研究展示了在阿拉伯语孤立字母识别任务中，尽管使用了最先进的wav2vec 2.0模型，但准确率仍然较低。通过引入轻量级神经网络和对抗训练，可以提高模型的鲁棒性。未来的工作将扩展这些方法到单词和句子级别的框架中。

Abstract: Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and
sentence-level transcription, yet struggle to classify isolated letters. In
this study, we show that this phoneme-level task, crucial for language
learning, speech therapy, and phonetic research, is challenging because
isolated letters lack co-articulatory cues, provide no lexical context, and
last only a few hundred milliseconds. Recogniser systems must therefore rely
solely on variable acoustic cues, a difficulty heightened by Arabic's emphatic
(pharyngealized) consonants and other sounds with no close analogues in many
languages. This study introduces a diverse, diacritised corpus of isolated
Arabic letters and demonstrates that state-of-the-art wav2vec 2.0 models
achieve only 35% accuracy on it. Training a lightweight neural network on
wav2vec embeddings raises performance to 65%. However, adding a small amplitude
perturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we
apply adversarial training, limiting the noisy-speech drop to 9% while
preserving clean-speech accuracy. We detail the corpus, training pipeline, and
evaluation protocol, and release, on demand, data and code for reproducibility.
Finally, we outline future work extending these methods to word- and
sentence-level frameworks, where precise letter pronunciation remains critical.

</details>


### [27] [Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.19594)
*Jun Bai,Minghao Tong,Yang Liu,Zixia Jia,Zilong Zheng*

Main category: cs.CL

TL;DR: 本文提出CEFT方法，通过识别和微调上下文忠实的专家，提高大语言模型的上下文忠实性，效果优于全微调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在依赖上下文的场景中难以将输出与提供的上下文结合，导致不相关响应。

Method: 提出Router Lens方法来准确识别上下文忠实的专家，并引入Context-faithful Expert Fine-Tuning (CEFT) 轻量级优化方法。

Result: 实验表明，CEFT在各种基准和模型上表现出色，效率更高。

Conclusion: CEFT能够匹配或超越全微调的性能，同时显著更高效。

Abstract: Context faithfulness is essential for reliable reasoning in context-dependent
scenarios. However, large language models often struggle to ground their
outputs in the provided context, resulting in irrelevant responses. Inspired by
the emergent expert specialization observed in mixture-of-experts
architectures, this work investigates whether certain experts exhibit
specialization in context utilization, offering a potential pathway toward
targeted optimization for improved context faithfulness. To explore this, we
propose Router Lens, a method that accurately identifies context-faithful
experts. Our analysis reveals that these experts progressively amplify
attention to relevant contextual information, thereby enhancing context
grounding. Building on this insight, we introduce Context-faithful Expert
Fine-Tuning (CEFT), a lightweight optimization approach that selectively
fine-tunes context-faithful experts. Experiments across a wide range of
benchmarks and models demonstrate that CEFT matches or surpasses the
performance of full fine-tuning while being significantly more efficient.

</details>


### [28] [LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.19614)
*Yang Sun,Lixin Zou,Dan Luo,Zhiyong Xie,Long Zhang,Liming Dong,Yunwei Zhao,Xixun Lin,Yanxiong Lu,Chenliang Li*

Main category: cs.CL

TL;DR: 本文研究了在RAG系统中通过噪声注入来增强外部知识利用的方法，并提出了一个名为Layer Fused Decoding (LFD)的新解码策略，以更好地利用外部事实知识。


<details>
  <summary>Details</summary>
Motivation: 尽管噪声注入看似反直觉且在实践中具有挑战性，但这一现象使我们能够对LLM如何整合外部知识进行细致控制和严格分析。因此，本文旨在通过干预噪声注入来探索LLM内部的知识整合机制，并提出一种新的解码策略来更好地利用外部知识。

Method: 本文干预了噪声注入，并在LLM中建立了层特定的功能分界：浅层专注于局部上下文建模，中间层专注于整合长距离外部事实知识，而深层主要依赖参数化内部知识。基于这一见解，我们提出了Layer Fused Decoding (LFD)，这是一种简单的解码策略，直接将中间层的表示与最终层的解码输出结合，以充分利用外部事实知识。为了确定最佳中间层，我们引入了一个内部知识分数(IKS)准则，该准则选择后半部分层中IKS值最低的层。

Result: 实验结果表明，LFD有助于RAG系统更有效地展现检索到的上下文知识，且成本最小。

Conclusion: 实验结果表明，LFD有助于RAG系统更有效地展现检索到的上下文知识，且成本最小。

Abstract: Retrieval-augmented generation (RAG) incorporates external knowledge into
large language models (LLMs), improving their adaptability to downstream tasks
and enabling information updates. Surprisingly, recent empirical evidence
demonstrates that injecting noise into retrieved relevant documents
paradoxically facilitates exploitation of external knowledge and improves
generation quality. Although counterintuitive and challenging to apply in
practice, this phenomenon enables granular control and rigorous analysis of how
LLMs integrate external knowledge. Therefore, in this paper, we intervene on
noise injection and establish a layer-specific functional demarcation within
the LLM: shallow layers specialize in local context modeling, intermediate
layers focus on integrating long-range external factual knowledge, and deeper
layers primarily rely on parametric internal knowledge. Building on this
insight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that
directly combines representations from an intermediate layer with final-layer
decoding outputs to fully exploit the external factual knowledge. To identify
the optimal intermediate layer, we introduce an internal knowledge score (IKS)
criterion that selects the layer with the lowest IKS value in the latter half
of layers. Experimental results across multiple benchmarks demonstrate that LFD
helps RAG systems more effectively surface retrieved context knowledge with
minimal cost.

</details>


### [29] [A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection](https://arxiv.org/abs/2508.19633)
*Chong Tian,Qirong Ho,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的符号对抗学习框架（SALF），用于检测虚假新闻。SALF通过代理符号学习优化过程实现对抗训练范式，而不是依赖数值更新。实验表明，SALF生成的虚假新闻可以显著降低现有检测系统的效果，并且还能提升检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 快速的LLM发展增加了虚假新闻的风险，因为它们能够自动生成越来越复杂的错误信息。以前的检测方法，包括微调的小模型或基于LLM的检测器，常常难以应对动态演变的性质。

Method: 我们提出了一个名为符号对抗学习框架（SALF）的新框架，该框架通过代理符号学习优化过程实现对抗训练范式，而不是依赖数值更新。SALF引入了一个生成代理构建欺骗性叙述，检测代理使用结构化辩论来识别逻辑和事实缺陷进行检测的范式，并通过这种对抗性交互迭代改进自己。

Result: 在两个多语言基准数据集上的实验表明，SALF的有效性，显示出它生成的复杂虚假新闻平均在中文中使最先进的检测性能下降53.4%，在英文中下降34.2%。SALF还改进了检测器，使检测精炼内容的性能提高最多7.7%。

Conclusion: 我们希望我们的工作能激发对更强大、适应性更强的假新闻检测系统的进一步探索。

Abstract: Rapid LLM advancements heighten fake news risks by enabling the automatic
generation of increasingly sophisticated misinformation. Previous detection
methods, including fine-tuned small models or LLM-based detectors, often
struggle with its dynamically evolving nature. In this work, we propose a novel
framework called the Symbolic Adversarial Learning Framework (SALF), which
implements an adversarial training paradigm by an agent symbolic learning
optimization process, rather than relying on numerical updates. SALF introduces
a paradigm where the generation agent crafts deceptive narratives, and the
detection agent uses structured debates to identify logical and factual flaws
for detection, and they iteratively refine themselves through such adversarial
interactions. Unlike traditional neural updates, we represent agents using
agent symbolic learning, where learnable weights are defined by agent prompts,
and simulate back-propagation and gradient descent by operating on natural
language representations of weights, loss, and gradients. Experiments on two
multilingual benchmark datasets demonstrate SALF's effectiveness, showing it
generates sophisticated fake news that degrades state-of-the-art detection
performance by up to 53.4% in Chinese and 34.2% in English on average. SALF
also refines detectors, improving detection of refined content by up to 7.7%.
We hope our work inspires further exploration into more robust, adaptable fake
news detection systems.

</details>


### [30] [Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design](https://arxiv.org/abs/2508.19665)
*Giovanni Pollo,Andrei Mihai Albu,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Loris Panaro,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 本文提出了一种使用FMI标准自动封装SystemC模型的方法，以解决汽车行业中协同仿真面临的挑战，并通过实际案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 汽车行业的最新进展需要稳健的协同仿真方法，以实现硬件和软件领域的早期验证和无缝集成。然而，缺乏标准化接口和专有仿真平台的主导地位给协作、可扩展性和IP保护带来了重大挑战。

Method: 本文提出了一种使用功能模拟接口（FMI）标准自动封装SystemC模型的方法。

Result: 我们在实际案例研究中验证了所提出的方法，证明了其在复杂设计中的有效性。

Conclusion: 本文提出了一种自动封装SystemC模型的方法，结合了SystemC的建模精度和快速上市时间以及FMI的互操作性和封装优势，实现了嵌入式组件在协同仿真工作流中的安全和可移植集成。

Abstract: The recent advancements of the automotive sector demand robust co-simulation
methodologies that enable early validation and seamless integration across
hardware and software domains. However, the lack of standardized interfaces and
the dominance of proprietary simulation platforms pose significant challenges
to collaboration, scalability, and IP protection. To address these limitations,
this paper presents an approach for automatically wrapping SystemC models by
using the Functional Mock-up Interface (FMI) standard. This method combines the
modeling accuracy and fast time-to-market of SystemC with the interoperability
and encapsulation benefits of FMI, enabling secure and portable integration of
embedded components into co-simulation workflows. We validate the proposed
methodology on real-world case studies, demonstrating its effectiveness with
complex designs.

</details>


### [31] [Survey of Specialized Large Language Model](https://arxiv.org/abs/2508.19667)
*Chenghan Yang,Ruiyu Zhao,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: 本文综述了专业大语言模型的发展，包括其在不同领域的应用及技术突破，并指出电子商务领域仍存在需要填补的空白。


<details>
  <summary>Details</summary>
Motivation: 随着专业大语言模型的快速发展，从简单的领域适应转向复杂的原生架构，这标志着人工智能开发的范式转变。因此，需要对这一进展进行系统性的研究。

Method: 该综述系统地检查了专业LLM的发展，包括领域原生设计、参数效率和多模态能力等技术突破。

Result: 分析表明，这些创新解决了通用LLM在专业应用中的基本限制，专业模型在特定领域基准测试中表现出持续的性能提升。

Conclusion: 该综述分析了专业大语言模型（LLM）在医疗、金融、法律和技术领域的进展，并强调了这些创新如何解决通用LLM在专业应用中的局限性。此外，还指出了电子商务领域需要填补的空白。

Abstract: The rapid evolution of specialized large language models (LLMs) has
transitioned from simple domain adaptation to sophisticated native
architectures, marking a paradigm shift in AI development. This survey
systematically examines this progression across healthcare, finance, legal, and
technical domains. Besides the wide use of specialized LLMs, technical
breakthrough such as the emergence of domain-native designs beyond fine-tuning,
growing emphasis on parameter efficiency through sparse computation and
quantization, increasing integration of multimodal capabilities and so on are
applied to recent LLM agent. Our analysis reveals how these innovations address
fundamental limitations of general-purpose LLMs in professional applications,
with specialized models consistently performance gains on domain-specific
benchmarks. The survey further highlights the implications for E-Commerce field
to fill gaps in the field.

</details>


### [32] [Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality](https://arxiv.org/abs/2508.19689)
*Xiaoying Zhang*

Main category: cs.CL

TL;DR: 本文探讨了如何在无需或极少人工干预的情况下开发适应性强、可扩展且准确的任务机器人，并提出了可能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 开发无需或极少人工干预的适应性强、可扩展且准确的任务机器人是对话研究中的一个重大挑战。

Method: 本文研究了创建能够自主学习和适应的机器人所面临的障碍，并提出了可能的解决方案。

Result: 本文分析了创建此类机器人的障碍，并提出了可能的解决方案。

Conclusion: 本文探讨了开发可适应、可扩展且准确的任务机器人所面临的挑战和潜在解决方案，重点在于创新技术，使机器人能够在不断变化的环境中自主学习和适应。

Abstract: Developing adaptable, extensible, and accurate task bots with minimal or zero
human intervention is a significant challenge in dialog research. This thesis
examines the obstacles and potential solutions for creating such bots, focusing
on innovative techniques that enable bots to learn and adapt autonomously in
constantly changing environments.

</details>


### [33] [Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models](https://arxiv.org/abs/2508.19720)
*Yilin Wang,Heng Wang,Yuyang Bai,Minnan Luo*

Main category: cs.CL

TL;DR: 本文提出了一种名为CSKS的简单框架，能够在轻量级成本下连续调整LLMs对上下文知识的敏感度。通过微调两个小的LM并利用它们输出分布的差异来改变原始LLM的分布，无需修改LLM权重。实验结果表明，该框架能够实现对LLMs对上下文知识敏感度的连续和精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在适应LLMs以忠实于新上下文知识方面存在效率低、效果差、不适用于黑盒模型或无法持续调整LLMs对上下文知识的敏感度的问题。

Method: CSKS框架通过微调两个小的LM（即代理模型），并利用它们输出分布的差异来改变原始LLM的分布，而无需修改LLM权重。

Result: 实验表明，CSKS框架实现了对LLMs对上下文知识敏感度的连续和精确控制，能够增加或减少敏感度，从而允许LLMs根据需要灵活地优先考虑上下文或参数知识。

Conclusion: CSKS框架能够实现对LLMs对上下文知识敏感度的连续和精确控制，使LLMs能够根据需要灵活地优先考虑上下文或参数知识。

Abstract: In Large Language Models (LLMs) generation, there exist knowledge conflicts
and scenarios where parametric knowledge contradicts knowledge provided in the
context. Previous works studied tuning, decoding algorithms, or locating and
editing context-aware neurons to adapt LLMs to be faithful to new contextual
knowledge. However, they are usually inefficient or ineffective for large
models, not workable for black-box models, or unable to continuously adjust
LLMs' sensitivity to the knowledge provided in the context. To mitigate these
problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a
simple framework that can steer LLMs' sensitivity to contextual knowledge
continuously at a lightweight cost. Specifically, we tune two small LMs (i.e.
proxy models) and use the difference in their output distributions to shift the
original distribution of an LLM without modifying the LLM weights. In the
evaluation process, we not only design synthetic data and fine-grained metrics
to measure models' sensitivity to contextual knowledge but also use a real
conflict dataset to validate CSKS's practical efficacy. Extensive experiments
demonstrate that our framework achieves continuous and precise control over
LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity
and reduced sensitivity, thereby allowing LLMs to prioritize either contextual
or parametric knowledge as needed flexibly. Our data and code are available at
https://github.com/OliveJuiceLin/CSKS.

</details>


### [34] [CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese](https://arxiv.org/abs/2508.19721)
*Carlos Carvalho,Francisco Teixeira,Catarina Botelho,Anna Pompili,Rubén Solera-Ureña,Sérgio Paulo,Mariana Julião,Thomas Rolland,John Mendonça,Diogo Pereira,Isabel Trancoso,Alberto Abad*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Existing resources for Automatic Speech Recognition in Portuguese are mostly
focused on Brazilian Portuguese, leaving European Portuguese (EP) and other
varieties under-explored. To bridge this gap, we introduce CAM\~OES, the first
open framework for EP and other Portuguese varieties. It consists of (1) a
comprehensive evaluation benchmark, including 46h of EP test data spanning
multiple domains; and (2) a collection of state-of-the-art models. For the
latter, we consider multiple foundation models, evaluating their zero-shot and
fine-tuned performances, as well as E-Branchformer models trained from scratch.
A curated set of 425h of EP was used for both fine-tuning and training. Our
results show comparable performance for EP between fine-tuned foundation models
and the E-Branchformer. Furthermore, the best-performing models achieve
relative improvements above 35% WER, compared to the strongest zero-shot
foundation model, establishing a new state-of-the-art for EP and other
varieties.

</details>


### [35] [NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks](https://arxiv.org/abs/2508.19724)
*Aritra Dutta,Swapnanil Mukherjee,Deepanway Ghosal,Somak Aditya*

Main category: cs.CL

TL;DR: 本文提出了一种端到端框架（NLKI），通过整合常识知识来提升小视觉语言模型的性能。实验结果表明，这种方法显著提高了准确率，并且在噪声数据上表现更加稳健。


<details>
  <summary>Details</summary>
Motivation: 由于常识视觉问答通常依赖于图像或问题中缺失的知识，小视觉语言模型（sVLMs）如ViLT、VisualBERT和FLAVA落后于其更大的生成模型。因此，我们研究了仔细整合常识知识对sVLMs的影响。

Method: 我们提出了一个端到端框架（NLKI），该框架（i）检索自然语言事实，（ii）提示大语言模型生成自然语言解释，并（iii）将这两个信号分别输入到两个常识VQA数据集（CRIC，AOKVQA）和一个视觉蕴含数据集（e-SNLI-VE）中的小视觉语言模型。

Result: 使用微调的ColBERTv2和对象信息丰富的提示检索的事实，大幅减少了幻觉，同时提升了端到端答案准确率高达7%（跨3个数据集），使FLAVA和其他模型在NLKI中达到或超过中型VLMs如Qwen-2 VL-2B和SmolVLM-2.5B。此外，在CRIC中使用噪声鲁棒损失（如对称交叉熵和广义交叉熵）的额外微调增加了2.5%，在AOKVQA中增加了5.5%。

Conclusion: 我们的研究揭示了基于大语言模型的常识知识在某些情况下优于从常识知识库中检索，噪声感知训练如何在外部知识增强的背景下稳定小型模型，并解释了为什么参数高效的常识推理现在对于250M模型来说是可行的。

Abstract: Commonsense visual-question answering often hinges on knowledge that is
missing from the image or the question. Small vision-language models (sVLMs)
such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative
counterparts. To study the effect of careful commonsense knowledge integration
on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural
language facts, (ii) prompts an LLM to craft natural language explanations, and
(iii) feeds both signals to sVLMs respectively across two commonsense VQA
datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts
retrieved using a fine-tuned ColBERTv2 and an object information-enriched
prompt yield explanations that largely cut down hallucinations, while lifting
the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA
and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B
and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional
finetuning using noise-robust losses (such as symmetric cross entropy and
generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our
findings expose when LLM-based commonsense knowledge beats retrieval from
commonsense knowledge bases, how noise-aware training stabilises small models
in the context of external knowledge augmentation, and why parameter-efficient
commonsense reasoning is now within reach for 250M models.

</details>


### [36] [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)
*Wenhao Li,Yuxin Zhang,Gen Luo,Haiyuan Wan,Ziyang Gong,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: Spotlight Attention is a novel method that enhances the efficiency and performance of KV cache management in LLMs through non-linear hashing and optimized training.


<details>
  <summary>Details</summary>
Motivation: Existing methods using random linear hashing are inefficient due to the orthogonal distribution of queries and keys in LLMs.

Method: Spotlight Attention uses non-linear hashing functions to optimize the embedding distribution of queries and keys, along with a lightweight training framework based on a Bradley-Terry ranking loss.

Result: Spotlight Attention improves retrieval precision, shortens hash code length by at least 5×, and achieves fast hashing retrieval for 512K tokens in under 100μs on a single A100 GPU with higher end-to-end throughput than vanilla decoding.

Conclusion: Spotlight Attention significantly improves retrieval precision and achieves faster hashing retrieval compared to traditional linear hashing methods.

Abstract: Reducing the key-value (KV) cache burden in Large Language Models (LLMs)
significantly accelerates inference. Dynamically selecting critical KV caches
during decoding helps maintain performance. Existing methods use random linear
hashing to identify important tokens, but this approach is inefficient due to
the orthogonal distribution of queries and keys within two narrow cones in
LLMs. We introduce Spotlight Attention, a novel method that employs non-linear
hashing functions to optimize the embedding distribution of queries and keys,
enhancing coding efficiency and robustness. We also developed a lightweight,
stable training framework using a Bradley-Terry ranking-based loss, enabling
optimization of the non-linear hashing module on GPUs with 16GB memory in 8
hours. Experimental results show that Spotlight Attention drastically improves
retrieval precision while shortening the length of the hash code at least
5$\times$ compared to traditional linear hashing. Finally, we exploit the
computational advantages of bitwise operations by implementing specialized CUDA
kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a
single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla
decoding.

</details>


### [37] [Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval](https://arxiv.org/abs/2508.19758)
*Yixuan Tang,Yuanyuan Shi,Yiqun Sun,Anthony Kum Hoe Tung*

Main category: cs.CL

TL;DR: 本文提出了NEWSCOPE，一个两阶段框架，用于多样化新闻检索，通过在句子级别显式建模语义变化来增强事件覆盖范围。实验表明，NEWSCOPE在不牺牲相关性的情况下显著提高了多样性。


<details>
  <summary>Details</summary>
Motivation: 访问多样化观点对于理解现实世界事件至关重要，但大多数新闻检索系统优先考虑文本相关性，导致结果冗余和观点曝光有限。

Method: 我们提出了NEWSCOPE，这是一个两阶段框架，用于多样化新闻检索，通过在句子级别显式建模语义变化来增强事件覆盖范围。第一阶段使用密集检索检索主题相关的内容，第二阶段应用句子级别的聚类和多样性感知重新排序以揭示互补信息。

Result: 实验表明，NEWSCOPE始终优于强大的基线，在不牺牲相关性的情况下显著提高了多样性。

Conclusion: 我们的结果展示了细粒度、可解释的建模在减少冗余和促进全面事件理解方面的有效性。

Abstract: Access to diverse perspectives is essential for understanding real-world
events, yet most news retrieval systems prioritize textual relevance, leading
to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a
two-stage framework for diverse news retrieval that enhances event coverage by
explicitly modeling semantic variation at the sentence level. The first stage
retrieves topically relevant content using dense retrieval, while the second
stage applies sentence-level clustering and diversity-aware re-ranking to
surface complementary information. To evaluate retrieval diversity, we
introduce three interpretable metrics, namely Average Pairwise Distance,
Positive Cluster Coverage, and Information Density Ratio, and construct two
paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that
NEWSCOPE consistently outperforms strong baselines, achieving significantly
higher diversity without compromising relevance. Our results demonstrate the
effectiveness of fine-grained, interpretable modeling in mitigating redundancy
and promoting comprehensive event understanding. The data and code are
available at https://github.com/tangyixuan/NEWSCOPE.

</details>


### [38] [Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance](https://arxiv.org/abs/2508.19764)
*Pedro Henrique Luz de Araujo,Paul Röttger,Dirk Hovy,Benjamin Roth*

Main category: cs.CL

TL;DR: 本研究分析了专家角色提示的有效性，并发现模型对无关角色细节非常敏感，提出了缓解策略，但仅适用于最大、最强大的模型。


<details>
  <summary>Details</summary>
Motivation: 专家角色提示被广泛用于任务改进，但之前的工作对其有效性有混合的结果，并未考虑何时以及为什么角色应该提高性能。

Method: 我们分析了关于角色提示的文献，并根据三个期望标准评估了9个最先进的LLM在27个任务上的表现。

Result: 我们发现专家角色通常会导致积极或不显著的性能变化。然而，模型对无关的角色细节非常敏感，性能下降了近30个百分点。此外，虽然更高的教育程度、专业化和领域相关性可以提高性能，但这些影响在不同任务中往往不一致或可忽略不计。

Conclusion: 我们的研究强调了需要更仔细地设计角色，并且需要评估方案来反映角色使用的预期效果。

Abstract: Expert persona prompting -- assigning roles such as expert in math to
language models -- is widely used for task improvement. However, prior work
shows mixed results on its effectiveness, and does not consider when and why
personas should improve performance. We analyze the literature on persona
prompting for task improvement and distill three desiderata: 1) performance
advantage of expert personas, 2) robustness to irrelevant persona attributes,
and 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs
across 27 tasks with respect to these desiderata. We find that expert personas
usually lead to positive or non-significant performance changes. Surprisingly,
models are highly sensitive to irrelevant persona details, with performance
drops of almost 30 percentage points. In terms of fidelity, we find that while
higher education, specialization, and domain-relatedness can boost performance,
their effects are often inconsistent or negligible across tasks. We propose
mitigation strategies to improve robustness -- but find they only work for the
largest, most capable models. Our findings underscore the need for more careful
persona design and for evaluation schemes that reflect the intended effects of
persona usage.

</details>


### [39] [T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables](https://arxiv.org/abs/2508.19813)
*Jie Zhang,Changzai Pan,Kaiwen Wei,Sishi Xiong,Yu Zhao,Xiangyu Li,Jiaxin Peng,Xiaoyan Gu,Jian Yang,Wenhan Chang,Zhenhe Wu,Jiang Zhong,Shuangyong Song,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出了一项名为table-to-report的任务，并构建了一个名为T2R-bench的双语基准测试，以评估从表格信息生成报告的能力。实验结果表明，即使是最先进的模型如Deepseek-R1在T2R-bench上的表现也仅有62.71的总体分数，这表明LLMs在该任务上仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在大型语言模型（LLMs）在表格推理方面的能力，但将表格信息转化为报告这一关键任务在工业应用中仍面临重大挑战。现有的表格基准测试无法充分评估该任务的实际应用能力。

Method: 提出了一项名为table-to-report的任务，并构建了一个名为T2R-bench的双语基准测试，以评估从表格信息生成报告的能力。此外，还提出了一个评估标准来公平衡量报告生成的质量。

Result: 实验结果表明，即使是最先进的模型如Deepseek-R1在T2R-bench上的表现也仅有62.71的总体分数，这表明LLMs在该任务上仍有改进空间。

Conclusion: 实验结果显示，即使是最先进的模型如Deepseek-R1在T2R-bench上的表现也仅有62.71的总体分数，表明LLMs在该任务上仍有改进空间。

Abstract: Extensive research has been conducted to explore the capabilities of large
language models (LLMs) in table reasoning. However, the essential task of
transforming tables information into reports remains a significant challenge
for industrial applications. This task is plagued by two critical issues: 1)
the complexity and diversity of tables lead to suboptimal reasoning outcomes;
and 2) existing table benchmarks lack the capacity to adequately assess the
practical application of this task. To fill this gap, we propose the
table-to-report task and construct a bilingual benchmark named T2R-bench, where
the key information flow from the tables to the reports for this task. The
benchmark comprises 457 industrial tables, all derived from real-world
scenarios and encompassing 19 industry domains as well as 4 types of industrial
tables. Furthermore, we propose an evaluation criteria to fairly measure the
quality of report generation. The experiments on 25 widely-used LLMs reveal
that even state-of-the-art models like Deepseek-R1 only achieves performance
with 62.71 overall score, indicating that LLMs still have room for improvement
on T2R-bench. Source code and data will be available after acceptance.

</details>


### [40] [Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning](https://arxiv.org/abs/2508.19828)
*Sikuan Yan,Xiufeng Yang,Zuchao Huang,Ercong Nie,Zifeng Ding,Zonggen Li,Xiaowen Ma,Hinrich Schütze,Volker Tresp,Yunpu Ma*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的框架Memory-R1，使大型语言模型能够主动管理外部记忆，从而提升长期推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常静态且基于启发式，缺乏学习机制来决定存储、更新或检索什么内容。本文旨在解决这一问题，通过引入强化学习框架，使大型语言模型能够主动管理外部记忆。

Method: 本文提出了一种基于强化学习（RL）的框架Memory-R1，其中包含两个专门的代理：一个记忆管理器，用于执行结构化的记忆操作（ADD, UPDATE, DELETE, NOOP），以及一个回答代理，用于选择最相关的条目并进行推理以生成答案。

Result: 在仅使用152个问答对和对应的时间记忆库进行训练的情况下，Memory-R1优于现有最先进的基线，并在多种问题类型和大型语言模型架构中表现出强大的泛化能力。

Conclusion: 本文展示了Memory-R1框架的有效性，并提供了关于如何通过强化学习使大型语言模型表现出更自主和记忆感知行为的见解，为构建更丰富、持久的推理系统指明了方向。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across
a wide range of NLP tasks, but they remain fundamentally stateless, constrained
by limited context windows that hinder long-horizon reasoning. Recent efforts
to address this limitation often augment LLMs with an external memory bank, yet
most existing pipelines are static and heuristic-driven, lacking any learned
mechanism for deciding what to store, update, or retrieve. We present
Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the
ability to actively manage and utilize external memory through two specialized
agents: a Memory Manager that learns to perform structured memory operations
{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant
entries and reasons over them to produce an answer. Both agents are fine-tuned
with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and
use with minimal supervision. With as few as 152 question-answer pairs and a
corresponding temporal memory bank for training, Memory-R1 outperforms the most
competitive existing baseline and demonstrates strong generalization across
diverse question types and LLM backbones. Beyond presenting an effective
approach, this work provides insights into how RL can unlock more agentic,
memory-aware behaviors in LLMs, pointing toward richer, more persistent
reasoning systems.

</details>


### [41] [Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis](https://arxiv.org/abs/2508.19831)
*Anusha Kamath,Kanishk Singla,Rakesh Paul,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 由于缺乏高质量的基准，评估印度语指令调整的大型语言模型（LLMs）很困难。为此，我们引入了一个包含五个印度语LLM评估数据集的套件，并采用了一种结合人工注释和翻译验证的方法来创建这些数据集。我们利用这个套件对支持印地语的开源LLM进行了广泛的基准测试，并提供了他们的当前能力的详细比较分析。我们的整理过程也为在其他低资源语言中开发基准提供了可复制的方法。


<details>
  <summary>Details</summary>
Motivation: 由于直接翻译英语数据集无法捕捉重要的语言和文化细微差别，因此在印度语中评估指令调整的大型语言模型（LLMs）具有挑战性。

Method: 我们采用从零开始的人工注释与翻译和验证相结合的方法创建了五个印度语LLM评估数据集。

Result: 我们利用这个套件对支持印地语的开源LLM进行了广泛的基准测试，并提供了他们当前能力的详细比较分析。

Conclusion: 我们的整理过程也为在其他低资源语言中开发基准提供了可复制的方法。

Abstract: Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is
challenging due to a lack of high-quality benchmarks, as direct translation of
English datasets fails to capture crucial linguistic and cultural nuances. To
address this, we introduce a suite of five Hindi LLM evaluation datasets:
IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created
using a methodology that combines from-scratch human annotation with a
translate-and-verify process. We leverage this suite to conduct an extensive
benchmarking of open-source LLMs supporting Hindi, providing a detailed
comparative analysis of their current capabilities. Our curation process also
serves as a replicable methodology for developing benchmarks in other
low-resource languages.

</details>


### [42] [Scalable and consistent few-shot classification of survey responses using text embeddings](https://arxiv.org/abs/2508.19836)
*Jonas Timmann Mjaaland,Markus Fleten Kreutzer,Halvor Tyseng,Rebeckah K. Fussell,Gina Passante,N. G. Holmes,Anders Malthe-Sørenssen,Tor Ole B. Odden*

Main category: cs.CL

TL;DR: 本文介绍了一种基于文本嵌入的分类框架，该框架只需每个类别中的少量示例，并且与标准定性工作流程相匹配。在概念物理调查的比较中，该框架表现出色，展示了文本嵌入辅助编码在大规模定性分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的编码方法通常耗时且容易出现不一致，现有的自然语言处理解决方案在定性分析中适用性有限，因为它们需要大量的标记数据，破坏了已有的定性工作流程，并且结果可能变化不定。

Method: 引入了一种基于文本嵌入的分类框架，只需要每个类别中的少量示例，并且与标准定性工作流程相匹配。

Result: 在对包含2899个开放回答的概念物理调查进行人类分析时，该框架的Cohen's Kappa值在0.74到0.83之间，与专家人类编码器在全面编码方案中的表现相当。

Conclusion: 文本嵌入辅助编码可以在不牺牲可解释性的情况下灵活扩展到数千条响应，为大规模演绎定性分析开辟了道路。

Abstract: Qualitative analysis of open-ended survey responses is a commonly-used
research method in the social sciences, but traditional coding approaches are
often time-consuming and prone to inconsistency. Existing solutions from
Natural Language Processing such as supervised classifiers, topic modeling
techniques, and generative large language models have limited applicability in
qualitative analysis, since they demand extensive labeled data, disrupt
established qualitative workflows, and/or yield variable results. In this
paper, we introduce a text embedding-based classification framework that
requires only a handful of examples per category and fits well with standard
qualitative workflows. When benchmarked against human analysis of a conceptual
physics survey consisting of 2899 open-ended responses, our framework achieves
a Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in
an exhaustive coding scheme. We further show how performance of this framework
improves with fine-tuning of the text embedding model, and how the method can
be used to audit previously-analyzed datasets. These findings demonstrate that
text embedding-assisted coding can flexibly scale to thousands of responses
without sacrificing interpretability, opening avenues for deductive qualitative
analysis at scale.

</details>


### [43] [TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation](https://arxiv.org/abs/2508.19856)
*Shashi Kumar,Srikanth Madikeri,Esaú Villatoro-Tello,Sergio Burdisso,Pradeep Rangappa,Andrés Carofilis,Petr Motlicek,Karthik Pandia,Shankar Venkatesan,Kadri Hacioğlu,Andreas Stolcke*

Main category: cs.CL

TL;DR: TokenVerse++ 引入了可学习向量，以在 XLSR-Transducer ASR 模型的声学嵌入空间中实现动态任务激活，从而允许使用仅部分标记的数据集，并在多个任务上实现了与或超过 TokenVerse 的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于令牌的多任务框架需要所有训练语句都有所有任务的标签，这限制了它们利用部分注释数据集和有效扩展的能力。

Method: TokenVerse++ 引入了可学习向量，在 XLSR-Transducer ASR 模型的声学嵌入空间中实现动态任务激活。

Result: 通过成功集成一个具有部分标签的数据集（特别是 ASR 和语言识别任务），证明了 TokenVerse++ 的有效性，并提高了整体性能。

Conclusion: TokenVerse++ 是一种更实用的多任务替代方案，而不会牺牲 ASR 性能。

Abstract: Token-based multitasking frameworks like TokenVerse require all training
utterances to have labels for all tasks, hindering their ability to leverage
partially annotated datasets and scale effectively. We propose TokenVerse++,
which introduces learnable vectors in the acoustic embedding space of the
XLSR-Transducer ASR model for dynamic task activation. This core mechanism
enables training with utterances labeled for only a subset of tasks, a key
advantage over TokenVerse. We demonstrate this by successfully integrating a
dataset with partial labels, specifically for ASR and an additional task,
language identification, improving overall performance. TokenVerse++ achieves
results on par with or exceeding TokenVerse across multiple tasks, establishing
it as a more practical multitask alternative without sacrificing ASR
performance.

</details>


### [44] [Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning](https://arxiv.org/abs/2508.19873)
*Vanessa Toborek,Sebastian Müller,Tim Selbach,Tamás Horváth,Christian Bauckhage*

Main category: cs.CL

TL;DR: 本文研究了人工整理的简单语言是否可以作为课程学习的有效信号，结果表明人类对语言难度的直觉可以指导语言模型预训练中的课程学习。


<details>
  <summary>Details</summary>
Motivation: 课程学习（CL）旨在通过从“简单”到“困难”的数据呈现来提高训练效果，但定义和衡量语言难度仍然是一个开放性挑战。我们研究了人工整理的简单语言是否可以作为CL的有效信号。

Method: 我们使用来自Simple Wikipedia语料库的文章级标签，比较基于标签的课程与依赖浅层启发式的基于能力的策略。

Result: 实验结果显示，仅添加简单数据并没有明显的收益。然而，通过课程结构化它——尤其是首先引入——可以持续改善困惑度，特别是在简单语言上。相反，基于能力的课程在随机排序上没有一致的收益，可能是因为它们未能有效区分两类数据。

Conclusion: 我们的结果表明，人类对语言难度的直觉可以指导语言模型预训练中的课程学习。

Abstract: Curriculum learning (CL) aims to improve training by presenting data from
"easy" to "hard", yet defining and measuring linguistic difficulty remains an
open challenge. We investigate whether human-curated simple language can serve
as an effective signal for CL. Using the article-level labels from the Simple
Wikipedia corpus, we compare label-based curricula to competence-based
strategies relying on shallow heuristics. Our experiments with a BERT-tiny
model show that adding simple data alone yields no clear benefit. However,
structuring it via a curriculum -- especially when introduced first --
consistently improves perplexity, particularly on simple language. In contrast,
competence-based curricula lead to no consistent gains over random ordering,
probably because they fail to effectively separate the two classes. Our results
suggest that human intuition about linguistic difficulty can guide CL for
language model pre-training.

</details>


### [45] [AI-Powered Detection of Inappropriate Language in Medical School Curricula](https://arxiv.org/abs/2508.19883)
*Chiman Salavati,Shannon Song,Scott A. Hale,Roberto E. Montenegro,Shiri Dori-Hacohen,Fabricio Murai*

Main category: cs.CL

TL;DR: 研究评估了小型语言模型和大型语言模型在检测医疗教材中的不当用语方面的表现，并发现微调的小型语言模型更为有效。通过补充未标记的文本作为负样本可以进一步提高分类器的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗教材中使用不当语言可能对临床培训、患者互动和健康结果产生重大影响。然而，手动识别这些不当用语既昂贵又不切实际。因此，需要一种自动化的解决方案来检测和减少医疗教材中的不当语言。

Method: 研究评估了微调的小型语言模型（SLMs）和预训练的大语言模型（LLMs）在检测医疗教材中的不当用语（IUL）方面的表现。对于SLMs，考虑了四种方法：通用IUL分类器、特定子类别的二元分类器、多标签分类器以及两级分层管道。对于LLMs，考虑了包含子类别定义和/或示例的不同提示变体。

Result: 研究发现，即使经过精心挑选的示例，LLama-3 8B和70B等大型语言模型在检测不当用语方面仍然不如微调的小型语言模型。多标签分类器在标注数据上表现最佳，但通过补充未标记的文本作为负样本可以显著提高分类器的AUC值。

Conclusion: 研究发现，微调的小型语言模型（SLMs）在检测医疗教材中的不当用语方面比预训练的大语言模型（LLMs）更有效。此外，通过补充未标记的文本作为负样本可以提高分类器的性能。

Abstract: The use of inappropriate language -- such as outdated, exclusionary, or
non-patient-centered terms -- medical instructional materials can significantly
influence clinical training, patient interactions, and health outcomes. Despite
their reputability, many materials developed over past decades contain examples
now considered inappropriate by current medical standards. Given the volume of
curricular content, manually identifying instances of inappropriate use of
language (IUL) and its subcategories for systematic review is prohibitively
costly and impractical. To address this challenge, we conduct a first-in-class
evaluation of small language models (SLMs) fine-tuned on labeled data and
pre-trained LLMs with in-context learning on a dataset containing approximately
500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL
classifier, (2) subcategory-specific binary classifiers, (3) a multilabel
classifier, and (4) a two-stage hierarchical pipeline for general IUL detection
followed by multilabel classification. For LLMs, we consider variations of
prompts that include subcategory definitions and/or shots. We found that both
LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed
by SLMs. While the multilabel classifier performs best on annotated data,
supplementing training with unflagged excerpts as negative examples boosts the
specific classifiers' AUC by up to 25%, making them most effective models for
mitigating harmful language in medical curricula.

</details>


### [46] [Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement](https://arxiv.org/abs/2508.19887)
*Mohammed Rakibul Hasan,Rafi Majid,Ahanaf Tahmid*

Main category: cs.CL

TL;DR: 本文介绍了 Bangla-Bayanno，这是一个开放式的 Bangla 视觉问答数据集，旨在解决低资源多模态 AI 研究中的问题，并提供高质量的基准。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集要么是手动标注的，侧重于特定领域、查询类型或答案类型，要么受到专业答案格式的限制。

Method: 我们实施了一个多语言 LLM 辅助的翻译优化流程，以减少人为错误并确保清晰度。

Result: 该数据集包含 52,650 个跨 4750 张以上图像的问题-答案对，解决了多语言来源的低质量翻译问题。

Conclusion: Bangla-Bayanno 提供了最全面的开源、高质量的 Bangla VQA 基准，旨在推动低资源多模态学习的研究，并促进更包容的 AI 系统的发展。

Abstract: In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question
Answering (VQA) Dataset in Bangla, a widely used, low-resource language in
multimodal AI research. The majority of existing datasets are either manually
annotated with an emphasis on a specific domain, query type, or answer type or
are constrained by niche answer formats. In order to mitigate human-induced
errors and guarantee lucidity, we implemented a multilingual LLM-assisted
translation refinement pipeline. This dataset overcomes the issues of
low-quality translations from multilingual sources. The dataset comprises
52,650 question-answer pairs across 4750+ images. Questions are classified into
three distinct answer types: nominal (short descriptive), quantitative
(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive
open-source, high-quality VQA benchmark in Bangla, aiming to advance research
in low-resource multimodal learning and facilitate the development of more
inclusive AI systems.

</details>


### [47] [Logical Reasoning with Outcome Reward Models for Test-Time Scaling](https://arxiv.org/abs/2508.19903)
*Ramya Keerthy Thatikonda,Wray Buntine,Ehsan Shareghi*

Main category: cs.CL

TL;DR: 本文提出了用于演绎推理的Outcome Reward Models (ORMs)，并通过CoT和echo增强方法提高了其性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型语言模型在演绎逻辑推理中的表现，需要探索更有效的训练方法。

Method: 使用Chain-of-Thought（CoT）生成数据，并提出了一种新的echo生成技术来扩展错误类型。

Result: ORMs在多个数据集上表现出更好的性能。

Conclusion: ORMs训练的数据集通过CoT和echo增强方法，提高了在FOLIO、JustLogic和ProverQA数据集上的性能。

Abstract: Logical reasoning is a critical benchmark for evaluating the capabilities of
large language models (LLMs), as it reflects their ability to derive valid
conclusions from given premises. While the combination of test-time scaling
with dedicated outcome or process reward models has opened up new avenues to
enhance LLMs performance in complex reasoning tasks, this space is
under-explored in deductive logical reasoning. We present a set of Outcome
Reward Models (ORMs) for deductive reasoning. To train the ORMs we mainly
generate data using Chain-of-Thought (CoT) with single and multiple samples.
Additionally, we propose a novel tactic to further expand the type of errors
covered in the training dataset of the ORM. In particular, we propose an echo
generation technique that leverages LLMs' tendency to reflect incorrect
assumptions made in prompts to extract additional training data, covering
previously unexplored error types. While a standard CoT chain may contain
errors likely to be made by the reasoner, the echo strategy deliberately steers
the model toward incorrect reasoning. We show that ORMs trained on CoT and
echo-augmented data demonstrate improved performance on the FOLIO, JustLogic,
and ProverQA datasets across four different LLMs.

</details>


### [48] [Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2508.19919)
*Jingyu Guo,Yingying Xu*

Main category: cs.CL

TL;DR: 本研究通过实验框架探讨了LLM-based多智能体系统中刻板印象的出现和演变，发现AI代理在互动中会产生基于刻板印象的偏见，并且这些偏见会随着互动次数和决策权的增加而增强。


<details>
  <summary>Details</summary>
Motivation: 尽管刻板印象在人类社会互动中已被广泛记录，但AI系统通常被认为不太容易受到这些偏见的影响。以前的研究集中在从训练数据继承的偏见上，但刻板印象是否可以在AI代理互动中自发出现值得进一步探索。

Method: 通过一个新颖的实验框架模拟工作场所互动，以中性初始条件研究LLM-based多智能体系统中的刻板印象出现和演变。

Result: 发现LLM-Based AI代理在互动中发展出基于刻板印象的偏见，即使最初没有预定义的偏见；刻板印象效应随着互动轮次和决策权力的增加而加剧，特别是在引入等级结构后；这些系统表现出类似于人类社会行为的群体效应，包括光环效应、确认偏误和角色一致性；这些刻板印象模式在不同的LLM架构中一致出现。

Conclusion: 我们的研究强调了未来研究这种现象潜在机制以及开发减轻其伦理影响策略的必要性。

Abstract: While stereotypes are well-documented in human social interactions, AI
systems are often presumed to be less susceptible to such biases. Previous
studies have focused on biases inherited from training data, but whether
stereotypes can emerge spontaneously in AI agent interactions merits further
exploration. Through a novel experimental framework simulating workplace
interactions with neutral initial conditions, we investigate the emergence and
evolution of stereotypes in LLM-based multi-agent systems. Our findings reveal
that (1) LLM-Based AI agents develop stereotype-driven biases in their
interactions despite beginning without predefined biases; (2) stereotype
effects intensify with increased interaction rounds and decision-making power,
particularly after introducing hierarchical structures; (3) these systems
exhibit group effects analogous to human social behavior, including halo
effects, confirmation bias, and role congruity; and (4) these stereotype
patterns manifest consistently across different LLM architectures. Through
comprehensive quantitative analysis, these findings suggest that stereotype
formation in AI systems may arise as an emergent property of multi-agent
interactions, rather than merely from training data biases. Our work
underscores the need for future research to explore the underlying mechanisms
of this phenomenon and develop strategies to mitigate its ethical impacts.

</details>


### [49] [HEAL: A Hypothesis-Based Preference-Aware Analysis Framework](https://arxiv.org/abs/2508.19922)
*Yifu Huo,Chenglong Wang,Qiren Zhu,Shunjie Xing,Tong Xiao,Chunliang Zhang,Tongran Liu,Jinbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种新的偏好对齐评估框架HEAL，通过假设空间分析改进了现有方法的评估方式，并展示了其在理论和实践上的重要贡献。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好优化方法在评估时仅依赖于单个响应，忽略了其他潜在输出，这在实际应用中可能存在问题。因此，需要一种新的评估范式来更好地捕捉偏好。

Method: 本文提出了HEAL框架，它将偏好对齐建模为假设空间中的重新排序过程，并引入了两个互补的度量标准：排名准确性和偏好强度相关性。此外，还构建了一个统一的假设基准UniHypoBench。

Result: 实验表明，当前的偏好学习方法能够有效捕获代理模型提供的偏好，同时抑制负样本。HEAL框架为研究人员提供了诊断工具，并揭示了开发更先进对齐算法的前景。

Conclusion: 本文提出了HEAL框架，这是一种新的评估范式，用于分析偏好对齐。该框架通过假设空间分析为偏好学习研究提供了理论和实践上的贡献，并指出了开发更先进的对齐算法的方向。

Abstract: Preference optimization methods like DPO have achieved remarkable performance
in LLM alignment. However, the evaluation for these methods relies on a single
response and overlooks other potential outputs, which could also be generated
in real-world applications within this hypothetical space. To address this
issue, this paper presents a \textbf{H}ypothesis-based
Pr\textbf{E}ference-aware \textbf{A}na\textbf{L}ysis Framework (HEAL), a novel
evaluation paradigm that formulates preference alignment as a re-ranking
process within hypothesis spaces. The framework incorporates two complementary
metrics: ranking accuracy for evaluating ordinal consistency and preference
strength correlation for assessing continuous alignment. To facilitate this
framework, we develop UniHypoBench, a unified hypothesis benchmark constructed
from diverse instruction-response pairs. Through extensive experiments based on
HEAL, with a particular focus on the intrinsic mechanisms of preference
learning, we demonstrate that current preference learning methods can
effectively capture preferences provided by proxy models while simultaneously
suppressing negative samples. These findings contribute to preference learning
research through two significant avenues. Theoretically, we introduce
hypothesis space analysis as an innovative paradigm for understanding
preference alignment. Practically, HEAL offers researchers robust diagnostic
tools for refining preference optimization methods, while our empirical results
identify promising directions for developing more advanced alignment algorithms
capable of comprehensive preference capture.

</details>


### [50] [Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation](https://arxiv.org/abs/2508.19966)
*Slimane Bellaouar,Attia Nehar,Soumia Souffi,Mounia Bouameur*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite its significance, Arabic, a linguistically rich and morphologically
complex language, faces the challenge of being under-resourced. The scarcity of
large annotated datasets hampers the development of accurate tools for
subjectivity analysis in Arabic. Recent advances in deep learning and
Transformers have proven highly effective for text classification in English
and French. This paper proposes a new approach for subjectivity assessment in
Arabic textual data. To address the dearth of specialized annotated datasets,
we developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic
datasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we
fine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and
ArabianGPT) on AraDhati+ for effective subjectivity classification.
Furthermore, we experimented with an ensemble decision approach to harness the
strengths of individual models. Our approach achieves a remarkable accuracy of
97.79\,\% for Arabic subjectivity classification. Results demonstrate the
effectiveness of the proposed approach in addressing the challenges posed by
limited resources in Arabic language processing.

</details>


### [51] [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982)
*Pengxiang Li,Yefan Zhou,Dilxat Muhtar,Lu Yin,Shilin Yan,Li Shen,Yi Liang,Soroush Vosoughi,Shiwei Liu*

Main category: cs.CL

TL;DR: 本文提出Prophet，一种无需训练的快速解码方法，通过利用扩散语言模型的早期答案收敛特性，显著减少解码步骤并保持高质量生成。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）在推理速度上仍慢于自回归模型，主要是由于双向注意力的成本和高质输出所需的大量细化步骤。本文旨在解决这一问题，提高DLMs的推理效率。

Method: Prophet通过动态决定是否继续细化或直接解码剩余所有标记，使用前两名预测候选之间的置信度差距作为标准，从而实现快速解码。

Result: 在多个任务上的实证评估表明，Prophet将解码步骤减少了最多3.4倍，同时保持了高质量的生成效果。

Conclusion: 本文提出了一种无需训练的快速解码范式Prophet，通过利用扩散语言模型的早期答案收敛特性，显著减少了解码步骤，同时保持了高质量的生成效果。

Abstract: Diffusion language models (DLMs) have recently emerged as an alternative to
autoregressive approaches, offering parallel sequence generation and flexible
token orders. However, their inference remains slower than that of
autoregressive models, primarily due to the cost of bidirectional attention and
the large number of refinement steps required for high quality outputs. In this
work, we highlight and leverage an overlooked property of DLMs early answer
convergence: in many cases, the correct answer can be internally identified by
half steps before the final decoding step, both under semi-autoregressive and
random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%
of instances, respectively, can be decoded correctly using only half of the
refinement steps. Building on this observation, we introduce Prophet, a
training-free fast decoding paradigm that enables early commit decoding.
Specifically, Prophet dynamically decides whether to continue refinement or to
go "all-in" (i.e., decode all remaining tokens in one step), using the
confidence gap between the top-2 prediction candidates as the criterion. It
integrates seamlessly into existing DLM implementations, incurs negligible
overhead, and requires no additional training. Empirical evaluations of
LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the
number of decoding steps by up to 3.4x while preserving high generation
quality. These results recast DLM decoding as a problem of when to stop
sampling, and demonstrate that early decode convergence provides a simple yet
powerful mechanism for accelerating DLM inference, complementary to existing
speedup techniques. Our code is publicly available at
https://github.com/pixeli99/Prophet.

</details>


### [52] [AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios](https://arxiv.org/abs/2508.19988)
*Lisa Alazraki,Lihu Chen,Ana Brassard,Joe Stacey,Hossein A. Rahmani,Marek Rei*

Main category: cs.CL

TL;DR: 本研究引入了一个同时包含常识和数学推理的基准AgentCoMa，发现LLM在组合任务中的表现显著下降，而人类表现较好，揭示了模型在混合类型组合推理中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前的组合基准测试这些技能往往只关注常识或数学推理，而解决现实世界任务的LLM代理需要两者的结合。因此，需要一个同时涵盖常识和数学推理的基准来评估LLM的能力。

Method: 引入了一个Agentic Commonsense和Math基准（AgentCoMa），其中每个组合任务需要一个常识推理步骤和一个数学推理步骤，并对61个不同大小、模型家族和训练策略的LLM进行了测试。此外，还进行了一系列可解释性研究，以更好地理解性能差距，检查神经元模式、注意力图和成员推断。

Result: LLM通常可以单独解决两个步骤，但当两者结合时，准确率平均下降约30%。这比我们在先前组合基准中观察到的性能差距要大得多。而非专家人类标注者可以在AgentCoMa的组合问题和单独步骤中以相似的高准确性解决问题。

Conclusion: 本研究强调了在混合类型组合推理背景下模型的脆弱性，并为未来改进提供了一个测试平台。

Abstract: Large Language Models (LLMs) have achieved high accuracy on complex
commonsense and mathematical problems that involve the composition of multiple
reasoning steps. However, current compositional benchmarks testing these skills
tend to focus on either commonsense or math reasoning, whereas LLM agents
solving real-world tasks would require a combination of both. In this work, we
introduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each
compositional task requires a commonsense reasoning step and a math reasoning
step. We test it on 61 LLMs of different sizes, model families, and training
strategies. We find that LLMs can usually solve both steps in isolation, yet
their accuracy drops by ~30% on average when the two are combined. This is a
substantially greater performance gap than the one we observe in prior
compositional benchmarks that combine multiple steps of the same reasoning
type. In contrast, non-expert human annotators can solve the compositional
questions and the individual steps in AgentCoMa with similarly high accuracy.
Furthermore, we conduct a series of interpretability studies to better
understand the performance gap, examining neuron patterns, attention maps and
membership inference. Our work underscores a substantial degree of model
brittleness in the context of mixed-type compositional reasoning and offers a
test bed for future improvement.

</details>


### [53] [MathBuddy: A Multimodal System for Affective Math Tutoring](https://arxiv.org/abs/2508.19993)
*Debanjana Kar,Leopold Böss,Dacia Braca,Sebastian Maximilian Dennerlein,Nina Christine Hubig,Philipp Wintersberger,Yufang Hou*

Main category: cs.CL

TL;DR: 本文介绍了MathBuddy，一个情感感知的LLM驱动的数学辅导系统，能够通过建模学生的情绪来提高教育效果。


<details>
  <summary>Details</summary>
Motivation: 当前的教育技术中，学习模型没有考虑学生的感情状态，而教育心理学的研究表明，积极或消极的情感状态会影响学生的学习能力。

Method: 我们提出了MathBuddy，这是一个情感感知的LLM驱动的数学辅导系统，能够动态建模学生的情绪并将它们映射到相关的教学策略上。

Result: 我们使用自动评估指标和用户研究对模型进行了有效评估，结果显示在胜率方面有23分的性能提升，在DAMR分数上整体提升了3分。

Conclusion: 我们的研究表明，通过建模学生的情绪可以显著提高基于LLM的导师的教育能力。

Abstract: The rapid adoption of LLM-based conversational systems is already
transforming the landscape of educational technology. However, the current
state-of-the-art learning models do not take into account the student's
affective states. Multiple studies in educational psychology support the claim
that positive or negative emotional states can impact a student's learning
capabilities. To bridge this gap, we present MathBuddy, an emotionally aware
LLM-powered Math Tutor, which dynamically models the student's emotions and
maps them to relevant pedagogical strategies, making the tutor-student
conversation a more empathetic one. The student's emotions are captured from
the conversational text as well as from their facial expressions. The student's
emotions are aggregated from both modalities to confidently prompt our LLM
Tutor for an emotionally-aware response. We have effectively evaluated our
model using automatic evaluation metrics across eight pedagogical dimensions
and user studies. We report a massive 23 point performance gain using the win
rate and a 3 point gain at an overall level using DAMR scores which strongly
supports our hypothesis of improving LLM-based tutor's pedagogical abilities by
modeling students' emotions.

</details>


### [54] [ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning](https://arxiv.org/abs/2508.19996)
*Yiming Du,Yifan Xiang,Bin Liang,Dahua Lin,Kam-Fai Wong,Fei Tan*

Main category: cs.CL

TL;DR: ReSURE 是一种用于多轮对话系统 fine-tuning 的自适应学习方法，它通过动态调整不可靠监督来提高性能，无需显式过滤。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常通过静态预筛选来处理数据质量问题，但这与训练过程脱节，并未能有效缓解轮次级别的错误传播。因此，需要一种更适应性的方法来应对低质量数据带来的挑战。

Method: ReSURE 通过使用 Welford 的在线统计方法估计每轮损失分布，并根据此重新加权样本损失，从而动态地降低不可靠监督的影响。这种方法不需要显式的过滤步骤。

Result: 在单源和混合质量数据集上的实验表明，ReSURE 提高了系统的稳定性和响应质量，并且在不同基准测试中与响应分数和样本数量之间存在正相关关系。

Conclusion: ReSURE 提供了一种动态调整不可靠监督的方法，从而提高了多轮对话系统的稳定性和响应质量。此外，ReSURE 在不同数据质量下与响应分数和样本数量之间表现出积极的相关性，这为有效利用大规模数据提供了潜在的可能性。

Abstract: Fine-tuning multi-turn dialogue systems requires high-quality supervision but
often suffers from degraded performance when exposed to low-quality data.
Supervision errors in early turns can propagate across subsequent turns,
undermining coherence and response quality. Existing methods typically address
data quality via static prefiltering, which decouples quality control from
training and fails to mitigate turn-level error propagation. In this context,
we propose ReSURE (Regularizing Supervision UnREliability), an adaptive
learning method that dynamically down-weights unreliable supervision without
explicit filtering. ReSURE estimates per-turn loss distributions using
Welford's online statistics and reweights sample losses on the fly accordingly.
Experiments on both single-source and mixed-quality datasets show improved
stability and response quality. Notably, ReSURE enjoys positive Spearman
correlations (0.21 ~ 1.0 across multiple benchmarks) between response scores
and number of samples regardless of data quality, which potentially paves the
way for utilizing large-scale data effectively. Code is publicly available at
https://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.

</details>


### [55] [Selective Retrieval-Augmentation for Long-Tail Legal Text Classification](https://arxiv.org/abs/2508.19997)
*Boheng Mao*

Main category: cs.CL

TL;DR: 本文提出了一种名为Selective Retrieval-Augmentation (SRA)的方法，用于解决法律文本分类任务中长尾标签分布的问题。该方法通过增强低频标签的样本，提高了模型在罕见类别上的性能，并在两个基准数据集上取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集在法律领域中常表现出长尾标签分布，导致模型在罕见类别上的性能较差。因此，需要一种有效的解决方案来改善这一问题。

Method: SRA方法通过从训练数据中检索并增强低频标签的样本，避免了对常见类别的噪声引入，同时不需要修改模型架构。

Result: SRA方法在LEDGAR和UNFAIR-ToS两个基准数据集上均取得了比当前LexGLUE基线更高的微F1和宏F1分数，证明了其有效性。

Conclusion: SRA方法在长尾法律文本分类任务中表现出色，能够提高微F1和宏F1分数，展示了其在处理罕见类别时的有效性。

Abstract: Legal text classification is a fundamental NLP task in the legal domain.
Benchmark datasets in this area often exhibit a long-tail label distribution,
where many labels are underrepresented, leading to poor model performance on
rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a
solution to this problem. SRA focuses on augmenting samples belonging to
low-frequency labels in the training set, preventing the introduction of noise
for well-represented classes, and requires no changes to the model
architecture. Retrieval is performed only from the training data to ensure
there is no potential information leakage, removing the need for external
corpora simultaneously. The proposed SRA method is tested on two legal text
classification benchmark datasets with long-tail distributions: LEDGAR
(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA
attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE
baselines across both datasets, illustrating consistent improvements in
long-tail legal text classification. The code repository is available at:
https://github.com/Boheng-Mao/sra-legal

</details>


### [56] [DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis](https://arxiv.org/abs/2508.20033)
*Liana Patel,Negar Arabzadeh,Harshit Gupta,Ankita Sundar,Ion Stoica,Matei Zaharia,Carlos Guestrin*

Main category: cs.CL

TL;DR: 本文介绍了DeepScholar-bench，这是一个用于评估生成性研究综合系统的实时基准测试和全面的自动化评估框架。结果表明，当前系统在该基准测试上的表现仍然有限，这突显了其难度和重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的问答基准测试主要关注简短的事实性回答，而专家整理的数据集可能存在过时和数据污染的问题。这些方法无法捕捉到真实研究综合任务的复杂性和动态性。因此，需要一个更全面的评估框架来评估生成性研究综合系统。

Method: DeepScholar-bench是一个实时基准测试和全面的自动化评估框架，用于评估生成性研究综合系统。它通过从最近的高质量ArXiv论文中提取查询，并专注于生成论文的相关工作部分的任务来评估系统的性能。

Result: DeepScholar-base建立了一个强大的基线，其性能与其他方法相比具有竞争力或更高。然而，DeepScholar-bench仍然远未饱和，没有任何系统在所有指标上的得分超过19%。

Conclusion: DeepScholar-bench是一个具有挑战性的基准测试，它为生成性研究综合提供了重要的评估框架。结果表明，目前的系统在该基准测试上的表现仍然有限，这突显了其难度和重要性。

Abstract: The ability to research and synthesize knowledge is central to human
expertise and progress. An emerging class of systems promises these exciting
capabilities through generative research synthesis, performing retrieval over
the live web and synthesizing discovered sources into long-form, cited
summaries. However, evaluating such systems remains an open challenge: existing
question-answering benchmarks focus on short-form factual responses, while
expert-curated datasets risk staleness and data contamination. Both fail to
capture the complexity and evolving nature of real research synthesis tasks. In
this work, we introduce DeepScholar-bench, a live benchmark and holistic,
automated evaluation framework designed to evaluate generative research
synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv
papers and focuses on a real research synthesis task: generating the related
work sections of a paper by retrieving, synthesizing, and citing prior
research. Our evaluation framework holistically assesses performance across
three key dimensions, knowledge synthesis, retrieval quality, and
verifiability. We also develop DeepScholar-base, a reference pipeline
implemented efficiently using the LOTUS API. Using the DeepScholar-bench
framework, we perform a systematic evaluation of prior open-source systems,
search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that
DeepScholar-base establishes a strong baseline, attaining competitive or higher
performance than each other method. We also find that DeepScholar-bench remains
far from saturated, with no system exceeding a score of $19\%$ across all
metrics. These results underscore the difficulty of DeepScholar-bench, as well
as its importance for progress towards AI systems capable of generative
research synthesis. We make our code available at
https://github.com/guestrin-lab/deepscholar-bench.

</details>


### [57] [Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks](https://arxiv.org/abs/2508.20038)
*Sheng Liu,Qiang Sheng,Danding Wang,Yang Li,Guang Yang,Juan Cao*

Main category: cs.CL

TL;DR: 本文提出IMAGINE框架，通过分析嵌入空间分布生成类似越狱的指令，以解决训练数据与现实攻击之间的分布不匹配问题，从而提升大语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在面对与安全对齐语料库分布不同的恶意指令时仍然容易受到越狱攻击，这表明训练数据与现实世界攻击之间存在关键的分布不匹配问题，导致开发者陷入被动修补的循环中。

Method: IMAGINE框架利用嵌入空间分布分析生成类似越狱的指令，通过迭代优化过程动态演化文本生成分布，从而通过合成数据示例增强安全对齐数据分布。

Result: 基于IMAGINE增强的安全对齐语料库，框架在Qwen2.5、Llama3.1和Llama3.2上显著降低了攻击成功率，同时保持了模型的实用性。

Conclusion: IMAGINE框架通过生成类似越狱的指令，有效填补了真实越狱模式与安全对齐语料库之间的分布差距，并在不损害模型实用性的前提下显著降低了Qwen2.5、Llama3.1和Llama3.2的攻击成功率。

Abstract: Despite advances in improving large language model(LLM) to refuse to answer
malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks
where attackers generate instructions with distributions differing from safety
alignment corpora. New attacks expose LLMs' inability to recognize unseen
malicious instructions, highlighting a critical distributional mismatch between
training data and real-world attacks that forces developers into reactive
patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis
framework that leverages embedding space distribution analysis to generate
jailbreak-like instructions. This approach effectively fills the distributional
gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE
follows an iterative optimization process that dynamically evolves text
generation distributions across iterations, thereby augmenting the coverage of
safety alignment data distributions through synthesized data examples. Based on
the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates
significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2
without compromising their utility.

</details>


### [58] [AraHealthQA 2025 Shared Task Description Paper](https://arxiv.org/abs/2508.20047)
*Hassan Alhuzali,Farah Shamout,Muhammad Abdul-Mageed,Chaimae Abouzahir,Mouath Abu-Daoud,Ashwag Alasmari,Walid Al-Eisawi,Renad Al-Monef,Ali Alqahtani,Lama Ayash,Nizar Habash,Leen Kharouf*

Main category: cs.CL

TL;DR: AraHealthQA 2025是一个与ArabicNLP 2025联合举办的全面阿拉伯语健康问答共享任务，旨在解决高质量阿拉伯语医学问答资源的不足。任务包含两个互补的赛道，每个赛道包含多个子任务、评估数据集和标准化指标，以促进公平的基准测试。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决高质量阿拉伯语医学问答资源的不足，并通过共享任务促进在现实、多语言和文化微妙的医疗保健环境下的建模。

Method: 本文介绍了AraHealthQA 2025，这是一个与ArabicNLP 2025联合举办的全面阿拉伯语健康问答共享任务，旨在解决高质量阿拉伯语医学问答资源的不足。任务包含两个互补的赛道：MentalQA专注于阿拉伯语心理健康问答，而MedArabiQ涵盖更广泛的医学领域。每个赛道包含多个子任务、评估数据集和标准化指标，以促进公平的基准测试。

Result: 本文概述了数据集的创建、任务设计和评估框架、参与统计、基线系统，并总结了总体成果。

Conclusion: 本文总结了AraHealthQA 2025的总体成果，并反思了观察到的表现趋势以及未来阿拉伯语健康问答的发展前景。

Abstract: We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question
Answering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located
with EMNLP 2025). This shared task addresses the paucity of high-quality Arabic
medical QA resources by offering two complementary tracks: {MentalQA}, focusing
on Arabic mental health Q\&A (e.g., anxiety, depression, stigma reduction), and
{MedArabiQ}, covering broader medical domains such as internal medicine,
pediatrics, and clinical decision making. Each track comprises multiple
subtasks, evaluation datasets, and standardized metrics, facilitating fair
benchmarking. The task was structured to promote modeling under realistic,
multilingual, and culturally nuanced healthcare contexts. We outline the
dataset creation, task design and evaluation framework, participation
statistics, baseline systems, and summarize the overall outcomes. We conclude
with reflections on the performance trends observed and prospects for future
iterations in Arabic health QA.

</details>


### [59] [11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis](https://arxiv.org/abs/2508.20068)
*Chengzu Li,Wenshan Wu,Huanyu Zhang,Qingtao Li,Zeyu Gao,Yan Xia,José Hernández-Orallo,Ivan Vulić,Furu Wei*

Main category: cs.CL

TL;DR: 本文介绍了一个系统评估框架，用于评估最先进的MLLM的空间推理能力相对于人类的表现。研究发现，当前MLLM表现出空间认知的早期迹象，但其实例级表现仍然随机，而人类的正确性高度可预测。


<details>
  <summary>Details</summary>
Motivation: 研究人类认知过程中的空间推理和感知之间的相互作用，以及当前MLLM在类似人类空间认知方面的能力。

Method: 我们引入了一个系统评估框架，以评估最先进的MLLM的空间推理能力相对于人类的表现。11Plus-Bench是一个从现实标准化空间能力测试中衍生出来的高质量基准。

Result: 目前的MLLM表现出空间认知的早期迹象。尽管与人类相比存在较大的性能差距，但MLLM的认知特征与人类相似，即认知努力与与推理相关的复杂性密切相关。然而，MLLM的实例级表现仍然 largely random，而人类的正确性高度可预测，并由抽象模式复杂性塑造。

Conclusion: 这些发现突显了当前MLLM在空间推理能力方面的新兴能力和局限性，并为推进模型设计提供了可行的见解。

Abstract: For human cognitive process, spatial reasoning and perception are closely
entangled, yet the nature of this interplay remains underexplored in the
evaluation of multimodal large language models (MLLMs). While recent MLLM
advancements show impressive performance on reasoning, their capacity for
human-like spatial cognition remains an open question. In this work, we
introduce a systematic evaluation framework to assess the spatial reasoning
abilities of state-of-the-art MLLMs relative to human performance. Central to
our work is 11Plus-Bench, a high-quality benchmark derived from realistic
standardized spatial aptitude tests. 11Plus-Bench also features fine-grained
expert annotations of both perceptual complexity and reasoning process,
enabling detailed instance-level analysis of model behavior. Through extensive
experiments across 14 MLLMs and human evaluation, we find that current MLLMs
exhibit early signs of spatial cognition. Despite a large performance gap
compared to humans, MLLMs' cognitive profiles resemble those of humans in that
cognitive effort correlates strongly with reasoning-related complexity.
However, instance-level performance in MLLMs remains largely random, whereas
human correctness is highly predictable and shaped by abstract pattern
complexity. These findings highlight both emerging capabilities and limitations
in current MLLMs' spatial reasoning capabilities and provide actionable
insights for advancing model design.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [60] [An Investigation on Group Query Hallucination Attacks](https://arxiv.org/abs/2508.19321)
*Kehao Miao,Xiaolong Jin*

Main category: cs.CR

TL;DR: 研究提出了一种名为Group Query Attack的技术，通过同时向LLM提供多个查询来模拟用户交互场景，发现该技术会显著降低特定任务微调模型的性能，并可能触发潜在后门，同时在涉及推理的任务中也表现出有效性。


<details>
  <summary>Details</summary>
Motivation: Understanding the potential failure modes of LLMs during user interactions is essential, especially when users pose multiple questions in a single conversation.

Method: Group Query Attack is proposed to simulate scenarios where users pose multiple questions in a single conversation with LLMs, investigating how accumulated context affects outputs.

Result: Group Query Attack significantly degrades the performance of task-finetuned models, induces potential backdoors, and is effective in reasoning tasks like mathematical reasoning and code generation.

Conclusion: Group Query Attack can degrade the performance of models and induce potential backdoors, highlighting the importance of understanding failure modes in LLMs.

Abstract: With the widespread use of large language models (LLMs), understanding their
potential failure modes during user interactions is essential. In practice,
users often pose multiple questions in a single conversation with LLMs.
Therefore, in this study, we propose Group Query Attack, a technique that
simulates this scenario by presenting groups of queries to LLMs simultaneously.
We investigate how the accumulated context from consecutive prompts influences
the outputs of LLMs. Specifically, we observe that Group Query Attack
significantly degrades the performance of models fine-tuned on specific tasks.
Moreover, we demonstrate that Group Query Attack induces a risk of triggering
potential backdoors of LLMs. Besides, Group Query Attack is also effective in
tasks involving reasoning, such as mathematical reasoning and code generation
for pre-trained and aligned models.

</details>


### [61] [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)
*Chao Huang,Zefeng Zhang,Juewei Yue,Quangang Li,Chuang Zhang,Tingwen Liu*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型（LLMs）的安全对齐问题，发现其安全机制主要依赖于有限的注意力头。为此，作者提出了一种新的训练策略AHD，以促进安全相关行为在多个注意力头中的分布式编码。实验结果表明，AHD能够有效提升模型的安全鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的安全对齐仍然存在漏洞，因为对抗性提示可以有效地绕过其安全措施。我们的研究显示，这些安全机制主要依赖于一个有限的注意力头子集：移除或消融这些头会严重损害模型的安全性。现有越狱攻击利用了这种集中度，选择性地绕过或操纵这些关键注意力头。

Method: 我们引入了RDSHA，这是一种针对消融的方法，利用模型的拒绝方向来定位主要负责安全行为的注意力头。为了应对这个问题，我们提出了AHD，一种新的训练策略，旨在促进安全相关行为在众多注意力头中的分布式编码。

Result: 实验结果表明，AHD成功地将与安全相关的能力分散到更多的注意力头中。此外，在几种主流的越狱攻击评估中，使用AHD训练的模型表现出显著更强的安全鲁棒性，同时保持了整体的功能效用。

Conclusion: 实验结果表明，AHD成功地将与安全相关的能力分散到更多的注意力头中。此外，在几种主流的越狱攻击评估中，使用AHD训练的模型表现出显著更强的安全鲁棒性，同时保持了整体的功能效用。

Abstract: Current safety alignment for large language models(LLMs) continues to present
vulnerabilities, given that adversarial prompting can effectively bypass their
safety measures.Our investigation shows that these safety mechanisms
predominantly depend on a limited subset of attention heads: removing or
ablating these heads can severely compromise model safety. To identify and
evaluate these safety-critical components, we introduce RDSHA, a targeted
ablation method that leverages the model's refusal direction to pinpoint
attention heads mostly responsible for safety behaviors. Further analysis shows
that existing jailbreak attacks exploit this concentration by selectively
bypassing or manipulating these critical attention heads. To address this
issue, we propose AHD, a novel training strategy designed to promote the
distributed encoding of safety-related behaviors across numerous attention
heads. Experimental results demonstrate that AHD successfully distributes
safety-related capabilities across more attention heads. Moreover, evaluations
under several mainstream jailbreak attacks show that models trained with AHD
exhibit considerably stronger safety robustness, while maintaining overall
functional utility.

</details>


### [62] [SoK: Large Language Model Copyright Auditing via Fingerprinting](https://arxiv.org/abs/2508.19843)
*Shuo Shao,Yiming Li,Yu He,Hongwei Yao,Wenyuan Yang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: 本文对LLM指纹技术进行了全面研究，提出了一个统一的框架和形式化分类法，并构建了一个基准测试来评估其性能。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的广泛能力和大量资源需求，它们成为有价值的知识产权，但容易受到版权侵权的影响。LLM指纹技术提供了一种非侵入性的解决方案，但其可靠性仍不确定。

Method: 本文提出了一种统一的框架和形式化分类法，将现有方法分为白盒和黑盒方法，并构建了LeaFBench基准测试来评估LLM指纹技术。

Result: 在LeaFBench上的实验揭示了现有方法的优点和缺点，从而指出了该新兴领域的未来研究方向和关键开放问题。

Conclusion: 本文对LLM指纹技术进行了全面研究，提出了LeaFBench基准测试，并指出了未来的研究方向和关键开放问题。

Abstract: The broad capabilities and substantial resources required to train Large
Language Models (LLMs) make them valuable intellectual property, yet they
remain vulnerable to copyright infringement, such as unauthorized use and model
theft. LLM fingerprinting, a non-intrusive technique that extracts and compares
the distinctive features from LLMs to identify infringements, offers a
promising solution to copyright auditing. However, its reliability remains
uncertain due to the prevalence of diverse model modifications and the lack of
standardized evaluation. In this SoK, we present the first comprehensive study
of LLM fingerprinting. We introduce a unified framework and formal taxonomy
that categorizes existing methods into white-box and black-box approaches,
providing a structured overview of the state of the art. We further propose
LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting
under realistic deployment scenarios. Built upon mainstream foundation models
and comprising 149 distinct model instances, LeaFBench integrates 13
representative post-development techniques, spanning both parameter-altering
methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms
(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the
strengths and weaknesses of existing methods, thereby outlining future research
directions and critical open problems in this emerging field. The code is
available at https://github.com/shaoshuo-ss/LeaFBench.

</details>


### [63] [Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning](https://arxiv.org/abs/2508.20083)
*Yanbo Dai,Zhenlan Ji,Zongjie Li,Kuan Li,Shuai Wang*

Main category: cs.CR

TL;DR: 本文研究了如何利用现代大型语言模型的自我纠正能力（SCA）来缓解RAG系统的攻击，并提出了一种新的攻击范式DisarmRAG，通过破坏检索器本身来绕过SCA。实验结果显示，DisarmRAG在多个LLM和QA基准上表现出色，具有很高的攻击成功率，并且能够保持隐蔽性，强调了需要针对检索器的防御措施。


<details>
  <summary>Details</summary>
Motivation: 尽管现有研究表明RAG系统容易受到知识库中毒攻击，但现代大型语言模型的自我纠正能力（SCA）可以有效缓解这些攻击。然而，这种SCA也给攻击者带来了挑战，因此本文旨在探索一种新的攻击方法，以绕过SCA并成功攻击RAG系统。

Method: 本文提出了一种基于对比学习的模型编辑技术，用于对检索器进行局部且隐蔽的修改，使其仅在特定目标查询时返回恶意指令，同时保持正常的检索行为。此外，还设计了一个迭代协同优化框架，自动发现能够绕过基于提示的防御的鲁棒指令。

Result: 本文在六个LLM和三个QA基准上对DisarmRAG进行了广泛评估。结果表明，恶意指令的检索几乎完美，成功抑制了SCA，并在多种防御提示下实现了超过90%的攻击成功率。此外，编辑后的检索器在多种检测方法下仍保持隐蔽性。

Conclusion: 本文揭示了现代大型语言模型的自我纠正能力（SCA）可以有效缓解RAG系统的攻击，但同时也提出了一个新的攻击范式DisarmRAG，该范式通过破坏检索器本身来绕过SCA。实验结果表明，DisarmRAG在多个LLM和QA基准上表现出色，具有很高的攻击成功率，并且能够保持隐蔽性，强调了需要针对检索器的防御措施。

Abstract: Retrieval-Augmented Generation (RAG) has become a standard approach for
improving the reliability of large language models (LLMs). Prior work
demonstrates the vulnerability of RAG systems by misleading them into
generating attacker-chosen outputs through poisoning the knowledge base.
However, this paper uncovers that such attacks could be mitigated by the strong
\textit{self-correction ability (SCA)} of modern LLMs, which can reject false
context once properly configured. This SCA poses a significant challenge for
attackers aiming to manipulate RAG systems.
  In contrast to previous poisoning methods, which primarily target the
knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that
compromises the retriever itself to suppress the SCA and enforce
attacker-chosen outputs. This compromisation enables the attacker to
straightforwardly embed anti-SCA instructions into the context provided to the
generator, thereby bypassing the SCA. To this end, we present a
contrastive-learning-based model editing technique that performs localized and
stealthy edits, ensuring the retriever returns a malicious instruction only for
specific victim queries while preserving benign retrieval behavior. To further
strengthen the attack, we design an iterative co-optimization framework that
automatically discovers robust instructions capable of bypassing prompt-based
defenses. We extensively evaluate DisarmRAG across six LLMs and three QA
benchmarks. Our results show near-perfect retrieval of malicious instructions,
which successfully suppress SCA and achieve attack success rates exceeding 90\%
under diverse defensive prompts. Also, the edited retriever remains stealthy
under several detection methods, highlighting the urgent need for
retriever-centric defenses.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [64] [Word Chain Generators for Prefix Normal Words](https://arxiv.org/abs/2508.19619)
*Duncan Adamson,Moritz Dudey,Pamela Fleischmann,Annika Huch*

Main category: math.CO

TL;DR: 本文分析了前缀正则词的特性，并引入了新的方法来关联相同长度的词。


<details>
  <summary>Details</summary>
Motivation: 解决前缀正则词的计数问题和高效测试方法的开放问题。

Method: 通过词链和生成器，研究了前缀正则词的特性及其非前缀正则词的因素属性。

Result: 揭示了导致一个词不是前缀正则词的因素的特性，并引入了新的方法来关联相同长度的词。

Conclusion: 本文研究了前缀正则词的特性，并引入了新的方法来关联相同长度的词。

Abstract: In 2011, Fici and Lipt\'ak introduced prefix normal words. A binary word is
prefix normal if it has no factor (substring) that contains more occurrences of
the letter 1 than the prefix of the same length. Among the open problems
regarding this topic are the enumeration of prefix normal words and efficient
testing methods. We show a range of characteristics of prefix normal words.
These include properties of factors that are responsible for a word not being
prefix normal. With word chains and generators, we introduce new ways of
relating words of the same length to each other.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [65] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文综述了大型视觉语言模型（LVLMs）在目标检测中的最新进展，讨论了其工作机制、架构创新和整合方法，并指出LVLMs在目标检测中具有巨大的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了系统地探索LVLMs在目标检测中的最新进展，并比较它们与传统深度学习系统的实时性能、适应性和复杂性。

Method: 通过三步研究回顾过程系统地组织了对LVLMs的深入探索，包括讨论VLMs在目标检测中的运作方式、解释近年来LVLMs的架构创新、训练范式和输出灵活性，以及审查视觉和文本信息的整合方法。

Result: 该综述展示了LVLMs在各种场景中的有效性，包括定位和分割，并指出LVLMs有望很快达到或超越传统方法在目标检测中的性能。

Conclusion: 基于本研究，最近在LVLMs上的进展已经并且将继续在未来对目标检测和机器人应用产生变革性的影响。

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [66] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: 本文介绍了 KRETA，这是一个针对韩语文本丰富视觉问答的基准测试，旨在解决低资源语言在该领域的研究空白。同时，提出了一个半自动的 VQA 生成管道，以确保数据质量，并希望该方法能推广到其他语言。


<details>
  <summary>Details</summary>
Motivation: 目前，针对低资源语言如韩语，缺乏全面的基准测试，这阻碍了视觉语言模型的评估和比较。因此，需要一个专门针对韩语的基准测试来促进相关研究。

Method: 引入 KRETA 基准测试，并提出一种半自动的 VQA 生成管道，该管道优化了文本丰富的设置，通过逐步图像分解和严格的七项评估协议确保数据质量。

Result: KRETA 基准测试能够深入评估视觉文本理解和推理能力，并支持跨 15 个领域和 26 种图像类型的多维评估。此外，提出的半自动 VQA 生成管道在文本丰富的设置中表现出色。

Conclusion: KRETA 是一个针对韩语文本丰富视觉问答的基准测试，旨在填补低资源语言在该领域研究的空白。同时，该研究提出的半自动 VQA 生成管道具有可扩展性，有望促进多语言 VLM 研究的发展。

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [67] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的无需训练的物体幻觉检测框架GLSim，通过结合全局和局部嵌入相似性信号，提高了检测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的物体幻觉检测方法通常只采用全局或局部视角，这可能限制了检测的可靠性。

Method: GLSim是一种无需训练的物体幻觉检测框架，利用了图像和文本模态之间的全局和局部嵌入相似性信号。

Result: GLSim在现有物体幻觉检测方法的全面基准测试中表现出色，优于竞争基线。

Conclusion: GLSim在各种场景中实现了更准确和可靠的幻觉检测，并且在现有方法上表现出显著的优势。

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [68] [Beat-Based Rhythm Quantization of MIDI Performances](https://arxiv.org/abs/2508.19262)
*Maximilian Wachter,Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: 本文提出了一种基于变压器的节奏量化模型，利用节拍和强拍信息将MIDI表演转化为具有节奏对齐的人类可读分数。通过优化模型架构和数据表示，并在钢琴和吉他表演上进行训练，该模型在MUSTER指标上表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 将MIDI表演量化为具有节奏对齐的人类可读分数。

Method: 我们提出了一种基于变压器的节奏量化模型，结合了节拍和强拍信息，将MIDI表演量化为具有节奏对齐的人类可读分数。我们提出了一种基于节拍的预处理方法，将分数和表演数据转换为统一的标记表示。我们优化了模型架构和数据表示，并在钢琴和吉他表演上进行了训练。

Result: 我们的模型在MUSTER指标上超过了最先进的性能。

Conclusion: 我们的模型在MUSTER指标上超过了最先进的性能。

Abstract: We propose a transformer-based rhythm quantization model that incorporates
beat and downbeat information to quantize MIDI performances into
metrically-aligned, human-readable scores. We propose a beat-based
preprocessing method that transfers score and performance data into a unified
token representation. We optimize our model architecture and data
representation and train on piano and guitar performances. Our model exceeds
state-of-the-art performance based on the MUSTER metric.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [69] [Should LLMs be WEIRD? Exploring WEIRDness and Human Rights in Large Language Models](https://arxiv.org/abs/2508.19269)
*Ke Zhou,Marios Constantinides,Daniele Quercia*

Main category: cs.CY

TL;DR: 研究发现，LLM在文化多样性增加的同时，也更容易产生歧视性观点，这需要通过如宪法AI等方法来解决。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在文化偏见和公平性方面的潜在问题。

Method: 使用世界价值观调查的响应评估了五个广泛使用的LLM，并与《世界人权宣言》和三个区域宪章进行比较。

Result: 模型与WEIRD价值观的对齐度较低时，如BLOOM和Qwen，会产生更多文化多样化的响应，但更可能生成违反人权原则的输出。

Conclusion: 随着文化代表性在LLM中的增加，复制歧视性信念的风险也随之增加。

Abstract: Large language models (LLMs) are often trained on data that reflect WEIRD
values: Western, Educated, Industrialized, Rich, and Democratic. This raises
concerns about cultural bias and fairness. Using responses to the World Values
Survey, we evaluated five widely used LLMs: GPT-3.5, GPT-4, Llama-3, BLOOM, and
Qwen. We measured how closely these responses aligned with the values of the
WEIRD countries and whether they conflicted with human rights principles. To
reflect global diversity, we compared the results with the Universal
Declaration of Human Rights and three regional charters from Asia, the Middle
East, and Africa. Models with lower alignment to WEIRD values, such as BLOOM
and Qwen, produced more culturally varied responses but were 2% to 4% more
likely to generate outputs that violated human rights, especially regarding
gender and equality. For example, some models agreed with the statements ``a
man who cannot father children is not a real man'' and ``a husband should
always know where his wife is'', reflecting harmful gender norms. These
findings suggest that as cultural representation in LLMs increases, so does the
risk of reproducing discriminatory beliefs. Approaches such as Constitutional
AI, which could embed human rights principles into model behavior, may only
partly help resolve this tension.

</details>


### [70] [Geopolitical Parallax: Beyond Walter Lippmann Just After Large Language Models](https://arxiv.org/abs/2508.19492)
*Mehmet Can Yavuz,Humza Gohar Kabir,Aylin Özkan*

Main category: cs.CY

TL;DR: This study explores how large language models (LLMs) may exhibit geopolitical biases in news quality and subjectivity assessments, revealing systematic differences between Chinese-origin and Western-origin models.


<details>
  <summary>Details</summary>
Motivation: To investigate geopolitical parallax in news quality and subjectivity assessments mediated by algorithmic systems.

Method: Comparing article-level embeddings from Chinese-origin and Western-origin model families on a human-annotated news quality benchmark and parallel corpora covering politically sensitive topics.

Result: Consistent, non-random divergences aligned with model origin, including higher subjectivity and positive emotion scores for Western models in Palestine-related coverage, and lower structural quality metrics for Chinese models in US coverage.

Conclusion: LLM-based media evaluation pipelines require cultural calibration to avoid conflating content differences with model-induced bias.

Abstract: Objectivity in journalism has long been contested, oscillating between ideals
of neutral, fact-based reporting and the inevitability of subjective framing.
With the advent of large language models (LLMs), these tensions are now
mediated by algorithmic systems whose training data and design choices may
themselves embed cultural or ideological biases. This study investigates
geopolitical parallax-systematic divergence in news quality and subjectivity
assessments-by comparing article-level embeddings from Chinese-origin (Qwen,
BGE, Jina) and Western-origin (Snowflake, Granite) model families. We evaluate
both on a human-annotated news quality benchmark spanning fifteen stylistic,
informational, and affective dimensions, and on parallel corpora covering
politically sensitive topics, including Palestine and reciprocal China-United
States coverage. Using logistic regression probes and matched-topic evaluation,
we quantify per-metric differences in predicted positive-class probabilities
between model families. Our findings reveal consistent, non-random divergences
aligned with model origin. In Palestine-related coverage, Western models assign
higher subjectivity and positive emotion scores, while Chinese models emphasize
novelty and descriptiveness. Cross-topic analysis shows asymmetries in
structural quality metrics Chinese-on-US scoring notably lower in fluency,
conciseness, technicality, and overall quality-contrasted by higher negative
emotion scores. These patterns align with media bias theory and our distinction
between semantic, emotional, and relational subjectivity, and extend LLM bias
literature by showing that geopolitical framing effects persist in downstream
quality assessment tasks. We conclude that LLM-based media evaluation pipelines
require cultural calibration to avoid conflating content differences with
model-induced bias.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [71] [Capabilities of GPT-5 across critical domains: Is it the next breakthrough?](https://arxiv.org/abs/2508.19259)
*Georgios P. Georgiou*

Main category: cs.HC

TL;DR: GPT-5在多个领域表现出优于GPT-4的能力，特别是在教育、临床和研究方面。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，需要评估它们在不同实际领域中的表现，以确定其适用性和改进方向。

Method: 该研究通过人类评估者对GPT-4和GPT-5生成的输出进行了系统比较，评估了五个领域：课程计划、作业评估、临床诊断、研究生成和伦理推理。

Result: GPT-5在课程计划、临床诊断、研究生成和伦理推理方面显著优于GPT-4，而在作业评估方面两者表现相当。

Conclusion: 研究结果表明，GPT-5在教育、临床实践和学术研究中具有实际应用潜力，并且在伦理推理方面有所进步。

Abstract: The accelerated evolution of large language models has raised questions about
their comparative performance across domains of practical importance. GPT-4 by
OpenAI introduced advances in reasoning, multimodality, and task
generalization, establishing itself as a valuable tool in education, clinical
diagnosis, and academic writing, though it was accompanied by several flaws.
Released in August 2025, GPT-5 incorporates a system-of-models architecture
designed for task-specific optimization and, based on both anecdotal accounts
and emerging evidence from the literature, demonstrates stronger performance
than its predecessor in medical contexts. This study provides one of the first
systematic comparisons of GPT-4 and GPT-5 using human raters from linguistics
and clinical fields. Twenty experts evaluated model-generated outputs across
five domains: lesson planning, assignment evaluation, clinical diagnosis,
research generation, and ethical reasoning, based on predefined criteria.
Mixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in
lesson planning, clinical diagnosis, research generation, and ethical
reasoning, while both models performed comparably in assignment assessment. The
findings highlight the potential of GPT-5 to serve as a context-sensitive and
domain-specialized tool, offering tangible benefits for education, clinical
practice, and academic research, while also advancing ethical reasoning. These
results contribute to one of the earliest empirical evaluations of the evolving
capabilities and practical promise of GPT-5.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [72] [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
*Zhuohao Li,Wenqing Chen,Jianxing Yu,Zhichao Lu*

Main category: cs.SE

TL;DR: 本文提出了一种新的数据合成框架，用于构建更丰富的代码基准测试，以更好地反映代码的功能差异，并在多个任务中验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注代码克隆检测，强调语法相似性，而忽视了功能性理解。本文旨在研究LLM代码嵌入的功能一致性，即两个代码片段是否执行相同的功能，而不考虑语法差异。

Method: 我们提出了一种名为面向功能的代码自我进化（Functionality-Oriented Code Self-Evolution）的新数据合成框架，通过生成四种独特的变体来构建多样且具有挑战性的基准测试。

Result: 在三个下游任务——代码克隆检测、代码功能一致性识别和代码检索中，嵌入模型在我们的演化数据集上表现出显著提升的性能。

Conclusion: 我们的数据合成框架在三个下游任务中显著提升了嵌入模型的性能，证明了其有效性和泛化能力，推动了对代码功能理解的进步。

Abstract: Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的自监督预训练方法，通过引入额外的平衡约束，确保模型在K步梯度下降后优化每个异构数据源到其局部最优。实验表明，该方法能显著提高模型在下游任务中的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督预训练方法通常混合所有数据并最小化平均全局损失，但这种方法可能无法充分利用异构数据的特性。因此，我们需要一种新的方法来处理异构数据。

Method: 我们提出了一个新的自监督预训练方法，通过引入额外的平衡约束，确保模型在从模型初始化的K步梯度下降后，优化每个异构数据源到其局部最优。我们将这个问题形式化为双层优化问题，并使用一阶近似方法来解决它。

Result: 实验结果表明，所提出的方法在多领域和多语言数据集上的自监督预训练中表现出色，能够显著提高模型的适应性。

Conclusion: 实验表明，所提出的方法可以显著提高自监督预训练模型在下游监督微调任务中的适应性。

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [74] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于梯度的示例选择算法，在各种模型和数据集上验证了其效率和优越性。


<details>
  <summary>Details</summary>
Motivation: 在上下文学习中，如何快速选择最佳示例作为后续推理的条件是一个重要问题，现有的方法基于token嵌入的相似性设计，但本文提出了一种新的基于梯度的方法。

Method: 本文提出了一种基于输出梯度的示例选择方法，通过一阶近似估计模型输出，并应用此估计到多个随机采样的子集，最后聚合结果形成影响分数以选择最相关的示例。

Result: 实验表明，该方法在六个数据集上的全推理近似误差小于1%，并且在340亿参数的模型上，可以将子集选择的计算量提高37.7倍，并且平均优于基于输入嵌入的选择方法11%。

Conclusion: 本文提出了一种基于梯度的示例选择算法，该算法在各种模型和数据集上进行了验证，证明了其效率和优越性。

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [75] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: 本文提出了一种去中心化的多代理系统Symphony，通过三个关键机制实现轻量级LLM在消费级GPU上的协调，具有隐私保护、可扩展性和容错性，实验证明其在推理任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型（LLM）的代理框架依赖于集中式协调，导致高部署成本、僵化的通信拓扑和有限的适应性。

Method: Symphony引入了三个关键机制：(1) 分布式账本记录能力，(2) 动态任务分配的Beacon选择协议，(3) 基于CoTs的加权结果投票。

Result: Symphony在推理基准测试中表现优异，实现了显著的准确率提升，并展示了在不同能力模型上的鲁棒性。

Conclusion: Symphony在推理基准测试中优于现有基线，实现了显著的准确率提升，并展示了在不同能力模型上的鲁棒性。

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [76] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 本文研究了通过注意力头剪枝来缓解后门攻击的方法，提出了六种策略，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对预训练语言模型的性能和完整性构成重大威胁，传统检测方法难以应对，因此需要一种无需了解触发器或访问干净参考模型的方法来缓解这些威胁。

Method: 设计并实现了六种基于剪枝的策略：基于梯度的剪枝、逐层方差剪枝、带有结构L1/L2稀疏化的基于梯度的剪枝、随机集成剪枝、强化学习引导的剪枝以及贝叶斯不确定性剪枝。

Result: 实验结果表明，基于梯度的剪枝在防御语法触发器方面效果最好，而强化学习和贝叶斯剪枝更能抵御风格攻击。

Conclusion: 通过实验评估，基于梯度的剪枝在防御语法触发器方面表现最佳，而强化学习和贝叶斯剪枝更能抵御风格攻击。

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [77] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 本文将奉承行为建模为心理测量特征的组合，并提出基于向量的干预措施以减轻大型语言模型中的安全关键行为。


<details>
  <summary>Details</summary>
Motivation: 目前奉承行为通常被视为单一因果机制的孤立故障模式，但本文认为需要更全面的理解和干预方法。

Method: 本文使用对比激活添加（CAA）方法，将激活方向映射到心理测量特征，并研究不同组合如何导致奉承行为。

Result: 通过将奉承行为建模为心理测量特征的组合，本文展示了可解释和组合的基于向量的干预措施的可能性。

Conclusion: 本文提出将奉承行为建模为心理测量特征的几何和因果组合，这为解释和组合基于向量的干预措施提供了新的视角，可用于减轻大型语言模型中的安全关键行为。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [78] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: Instructional Agents is a multi-agent LLM framework that automates course material generation, reducing workload and improving efficiency in educational content creation.


<details>
  <summary>Details</summary>
Motivation: Preparing high-quality instructional materials remains a labor-intensive process that often requires extensive coordination among teaching faculty, instructional designers, and teaching assistants.

Method: Instructional Agents is a multi-agent large language model (LLM) framework designed to automate end-to-end course material generation, including syllabus creation, lecture scripts, LaTeX-based slides, and assessments. It operates in four modes: Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling flexible control over the degree of human involvement.

Result: Instructional Agents was evaluated across five university-level computer science courses and shown to produce high-quality instructional materials while significantly reducing development time and human workload.

Conclusion: Instructional Agents provides a scalable and cost-effective framework to democratize access to high-quality education, particularly in underserved or resource-constrained settings.

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [79] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: 本文研究了不同模型在软推理任务中使用Chain-of-Thought（CoT）的效果和忠实性，发现它们的依赖方式存在差异，且CoT的影响和忠实性并不总是一致。


<details>
  <summary>Details</summary>
Motivation: 最近的工作表明，Chain-of-Thought（CoT）对于软推理问题（如分析和常识推理）通常只能带来有限的收益。CoT也可能不忠实于模型的实际推理过程。

Method: 我们研究了指令调优、推理和推理提炼模型在软推理任务中CoT的动力学和忠实性。

Result: 我们的研究发现，这些模型依赖CoT的方式存在差异，并且CoT的影响和忠实性并不总是对齐的。

Conclusion: 我们的研究揭示了不同模型依赖CoT的方式存在差异，并表明CoT的影响和忠实性并不总是一致的。

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [80] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL is a new approach for multi-agent systems that improves efficiency and coordination through a staged workflow for interleaved reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing single-agent approaches are limited by structural constraints, and recent progress in multi-agent reinforcement learning (MARL) has been hindered by inefficiency and incompatibility with current LVLM architectures.

Method: SWIRL is a staged workflow for interleaved reinforcement learning designed for multi-agent systems. It reformulates MARL into a sequence of single-agent reinforcement learning tasks, updating one agent at a time while keeping the others fixed.

Result: SWIRL demonstrates superior performance on both high-level and low-level GUI benchmarks. It also shows strong capability in multi-agent mathematical reasoning.

Conclusion: SWIRL demonstrates strong capability in multi-agent mathematical reasoning and has potential as a general framework for developing efficient and robust multi-agent systems.

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>

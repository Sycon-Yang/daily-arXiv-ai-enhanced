{"id": "2506.09147", "pdf": "https://arxiv.org/pdf/2506.09147", "abs": "https://arxiv.org/abs/2506.09147", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perli\u0107", "Ekaterina Borisova", "Markarit Vartampetian"], "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u79f0\u4e3aLLM-as-a-qualitative-judge\uff0c\u5176\u4e3b\u8981\u8f93\u51fa\u4e3a\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7cfb\u7edf\u8f93\u51fa\u7684\u5e38\u89c1\u95ee\u9898\u7c7b\u578b\u7684\u7ed3\u6784\u5316\u62a5\u544a\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f00\u653e\u5f0f\u7684\u5b9e\u4f8b\u7ea7\u95ee\u9898\u5206\u6790\u548c\u4f7f\u7528\u76f4\u89c2\u7d2f\u79ef\u7b97\u6cd5\u7684\u95ee\u9898\u805a\u7c7b\uff0c\u4e3a\u5f00\u53d1\u4eba\u5458\u63d0\u4f9b\u6709\u5173\u6539\u8fdb\u7ed9\u5b9a\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7cfb\u7edf\u7684\u6709\u610f\u4e49\u7684\u89c1\u89e3\u3002\u5bf9\u6765\u81ea12\u4e2a\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6570\u636e\u96c6\u7684\u7ea6300\u4e2a\u5b9e\u4f8b\u4e2d\u7684\u95ee\u9898\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6b63\u786e\u8bc6\u522b\u4e862/3\u5b9e\u4f8b\u7684\u5177\u4f53\u95ee\u9898\uff0c\u5e76\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u4e8e\u4eba\u5de5\u6ce8\u91ca\u5458\u6240\u7f16\u5199\u7684\u9519\u8bef\u7c7b\u578b\u62a5\u544a\u3002\u4ee3\u7801\u548c\u6570\u636e\u516c\u5f00\u53ef\u83b7\u53d6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u8bc4\u4ef7\u65b9\u6cd5\u4e3b\u8981\u4f5c\u4e3a\u5b9a\u91cf\u5de5\u5177\u4f7f\u7528\uff0c\u800c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u5b9a\u6027\u7684\u8bc4\u4ef7\u65b9\u5f0f\uff0c\u4e3a\u5f00\u53d1\u4eba\u5458\u63d0\u4f9b\u5173\u4e8e\u5982\u4f55\u6539\u8fdb\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7cfb\u7edf\u7684\u6709\u610f\u4e49\u7684\u89c1\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLM-as-a-qualitative-judge\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a\u5f00\u653e\u5f0f\u7684\u5b9e\u4f8b\u7ea7\u95ee\u9898\u5206\u6790\u548c\u4f7f\u7528\u76f4\u89c2\u7d2f\u79ef\u7b97\u6cd5\u7684\u95ee\u9898\u805a\u7c7b\u3002", "result": "\u8be5\u65b9\u6cd5\u6b63\u786e\u8bc6\u522b\u4e862/3\u5b9e\u4f8b\u7684\u5177\u4f53\u95ee\u9898\uff0c\u5e76\u4e14\u751f\u6210\u7684\u9519\u8bef\u7c7b\u578b\u62a5\u544a\u4e0e\u4eba\u5de5\u6ce8\u91ca\u5458\u7684\u62a5\u544a\u76f8\u4f3c\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eLLM\u7684\u5b9a\u6027\u8bc4\u4ef7\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u5e76\u6539\u8fdb\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7cfb\u7edf\u3002"}}
{"id": "2506.09175", "pdf": "https://arxiv.org/pdf/2506.09175", "abs": "https://arxiv.org/abs/2506.09175", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "AI": {"tldr": "This paper proposes a phrase dictionary biasing method to improve phrase translation in speech translation tasks. The method enhances both transducer-based streaming speech translation models and multimodal large language models.", "motivation": "To address the challenge of translating rarely occurring phrases in training data for speech translation tasks.", "method": "Proposes a phrase dictionary biasing method that leverages source-to-target language phrase pairs.", "result": "The method improves performance by 21% relatively for streaming speech translation models and achieves 85% relative improvement in phrase recall for multimodal large language models.", "conclusion": "The proposed phrase dictionary biasing method effectively enhances phrase translation in both types of speech translation models."}}
{"id": "2506.09218", "pdf": "https://arxiv.org/pdf/2506.09218", "abs": "https://arxiv.org/abs/2506.09218", "authors": ["Bruno Ferenc \u0160egedin"], "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "AI": {"tldr": "This study investigates the generalization capacity of generative CNNs trained on raw audio waveforms of lexical items and proposes a new method for probing a model's lexically-independent generalizations.", "motivation": "To explore the ability of DNNs to represent phonotactic generalizations derived from lexical learning.", "method": "Trained generative CNNs on raw audio waveforms of lexical items and explored the consequences of shrinking the FC bottleneck from 1024 channels to 8 before training.", "result": "The convolutional layers can dynamically generalize phonetic dependencies beyond lexically-constrained configurations learned by the FC.", "conclusion": "A novel technique for probing a model's lexically-independent generalizations is proposed that works only under the narrow FC bottleneck."}}
{"id": "2506.09251", "pdf": "https://arxiv.org/pdf/2506.09251", "abs": "https://arxiv.org/abs/2506.09251", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "AI": {"tldr": "Transformer language models can generalize well to longer inputs by transferring knowledge from related tasks.", "motivation": "To understand how transformer language models achieve generalization capabilities.", "method": "Investigate length generalization through task association and demonstrate the transfer of length generalization across diverse algorithmic tasks.", "result": "Transformer models can inherit generalization capabilities from similar tasks when trained jointly. Pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings.", "conclusion": "Length generalization transfer correlates with the re-use of the same attention heads between tasks."}}
{"id": "2506.09259", "pdf": "https://arxiv.org/pdf/2506.09259", "abs": "https://arxiv.org/abs/2506.09259", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "comment": null, "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "AI": {"tldr": "This study focuses on identifying prosocial behaviors in game-chat text using an unsupervised method and a new model called SAAM, achieving better performance than previous techniques. It developed the first automated system for classifying prosocial behaviors in in-game chats, especially in low-resource settings.", "motivation": "To shift the focus of moderation from penalizing toxicity to encouraging positive interactions by identifying prosocial behaviors in game-chat text.", "method": "Unsupervised discovery combined with game domain expert collaboration and a novel Self-Anchored Attention Model (SAAM).", "result": "The proposed approach led to the development of the first automated system for classifying prosocial behaviors in in-game chats, showing effectiveness in low-resource settings.", "conclusion": "This research applies NLP techniques to discover and classify prosocial behaviors in player in-game chat communication, helping promote positive interactions on online platforms."}}
{"id": "2506.09277", "pdf": "https://arxiv.org/pdf/2506.09277", "abs": "https://arxiv.org/abs/2506.09277", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nicolas Chesneau", "Sarath Chandar", "Marie-Jeanne Lesot"], "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have demonstrated the capability of generating\nfree text self Natural Language Explanation (self-NLE) to justify their\nanswers. Despite their logical appearance, self-NLE do not necessarily reflect\nthe LLM actual decision-making process, making such explanations unfaithful.\nWhile existing methods for measuring self-NLE faithfulness mostly rely on\nbehavioral tests or computational block identification, none of them examines\nthe neural activity underlying the model's reasoning. This work introduces a\nnovel flexible framework for quantitatively measuring the faithfulness of\nLLM-generated self-NLE by directly comparing the latter with interpretations of\nthe model's internal hidden states. The proposed framework is versatile and\nprovides deep insights into self-NLE faithfulness by establishing a direct\nconnection between self-NLE and model reasoning. This approach advances the\nunderstanding of self-NLE faithfulness and provides building blocks for\ngenerating more faithful self-NLE.", "AI": {"tldr": "This paper introduces a new method to measure the faithfulness of self-NLE generated by LLMs.", "motivation": "Existing methods for measuring self-NLE faithfulness rely on behavioral tests or computational block identification, but do not examine the neural activity underlying the model's reasoning.", "method": "The authors propose a novel flexible framework that compares LLM-generated self-NLE with interpretations of the model's internal hidden states.", "result": "The proposed framework provides deep insights into self-NLE faithfulness and establishes a direct connection between self-NLE and model reasoning.", "conclusion": "This approach advances the understanding of self-NLE faithfulness and provides building blocks for generating more faithful self-NLE."}}
{"id": "2506.09301", "pdf": "https://arxiv.org/pdf/2506.09301", "abs": "https://arxiv.org/abs/2506.09301", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "AI": {"tldr": "Introduce (RSA)^2 framework to interpret non-literal expressions by considering rhetorical strategies rather than specific motivations.", "motivation": "To model figurative language use in human communication effectively without focusing on specific motivations.", "method": "(RSA)^2 framework considers rhetorical strategies to interpret non-literal expressions.", "result": "Achieved state-of-the-art performance on the ironic split of PragMega+ dataset when combined with LLMs.", "conclusion": "The proposed framework provides a human-compatible interpretation of non-literal utterances."}}
{"id": "2506.09315", "pdf": "https://arxiv.org/pdf/2506.09315", "abs": "https://arxiv.org/abs/2506.09315", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "AI": {"tldr": "This study improves the detection of Alzheimer's dementia by using an advanced language model, achieving higher accuracy than previous methods and offering clearer decision boundaries.", "motivation": "To enhance the detection of Alzheimer's dementia through improved language model techniques.", "method": "Using an instruction-following version of Mistral-7B large language model with the paired perplexity approach.", "result": "Achieved 3.33% higher accuracy than the best current paired perplexity method and 6.35% more accurate than the top-ranked method from the ADReSS 2020 challenge benchmark.", "conclusion": "The proposed method provides clear and interpretable decision boundaries, unlike other methods, and shows that the language models have learned specific language patterns of AD speakers."}}
{"id": "2506.09329", "pdf": "https://arxiv.org/pdf/2506.09329", "abs": "https://arxiv.org/abs/2506.09329", "authors": ["Yuxin Jiang"], "title": "Towards Efficient and Effective Alignment of Large Language Models", "categories": ["cs.CL"], "comment": "PhD thesis", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements.", "AI": {"tldr": "This thesis introduces new methods in data collection, training, and evaluation to improve large language model alignment.", "motivation": "The need for more efficient and effective ways to align large language models with human expectations.", "method": "Proposes Lion for adversarial distillation, Web Reconstruction for automated data synthesis, Learning to Edit for knowledge integration, Bridging and Modeling Correlations for DPO refinement, and FollowBench for evaluating constraint adherence.", "result": "State-of-the-art zero-shot reasoning with Lion, improved data diversity and scalability with WebR, enhanced knowledge updates with LTE, superior alignment with BMC, and exposure of model weaknesses with FollowBench.", "conclusion": "Novel methodologies in data collection, training, and evaluation advance LLM alignment significantly."}}
{"id": "2506.09331", "pdf": "https://arxiv.org/pdf/2506.09331", "abs": "https://arxiv.org/abs/2506.09331", "authors": ["Arjun Vaithilingam Sudhakar"], "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "AI": {"tldr": "This study explores if large language models (LLMs) can model and reason about others' intentions (theory of mind). It uses cooperative multi-agent reinforcement learning to evaluate this capability, aiming to improve artificial agents' adaptability and cooperation with both AI and human partners.", "motivation": "Understanding the intention of others is vital for effective collaboration, essential for societal success and necessary for cooperative interactions among agents.", "method": "Cooperative multi-agent reinforcement learning where agents learn to collaborate through repeated interactions.", "result": "The research investigates the theory of mind in LLMs and seeks to develop hybrid human-AI systems for better collaboration.", "conclusion": "This work contributes to the understanding of whether LLMs possess a form of theory of mind and how they can be integrated into hybrid human-AI systems."}}
{"id": "2506.09340", "pdf": "https://arxiv.org/pdf/2506.09340", "abs": "https://arxiv.org/abs/2506.09340", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "title": "RePO: Replay-Enhanced Policy Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "AI": {"tldr": "This paper introduces RePO, a method for optimizing large language models using diverse replay strategies to enhance policy optimization and improve performance on mathematical reasoning tasks.", "motivation": "To address the high computational costs and low data efficiency of previous methods like GRPO when optimizing large language models.", "method": "Replay-Enhanced Policy Optimization (RePO) uses diverse replay strategies to retrieve off-policy samples from a replay buffer, enabling policy optimization with a broader and more diverse set of samples for each prompt.", "result": "RePO achieved significant performance gains of 18.4 and 4.1 points for Qwen2.5-Math-1.5B and Qwen3-1.7B respectively, compared to GRPO. It also increased computational cost by 15% but raised the number of effective optimization steps by 48% for Qwen3-1.7B.", "conclusion": "RePO improves the performance of large language model optimization with enhanced data efficiency and a manageable increase in computational cost."}}
{"id": "2506.09342", "pdf": "https://arxiv.org/pdf/2506.09342", "abs": "https://arxiv.org/abs/2506.09342", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Latent Multi-Head Attention for Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure. 5 tables", "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "AI": {"tldr": "This paper explores latent multi-head attention (MLA) for small language models, showing that MLA with rotary positional embeddings (MLA+RoPE) improves memory efficiency and performance.", "motivation": "To investigate efficiency-quality trade-offs in small language models.", "method": "Benchmarking three architectural variants: standard multi-head attention (MHA), MLA, and MLA with RoPE using 30M-parameter GPT models trained on 100,000 synthetic stories.", "result": "MLA+RoPE with half-rank latent dimensions reduces KV-cache memory by 45% and slightly increases validation loss compared to MHA. RoPE is essential for MLA's performance in small models.", "conclusion": "MLA+RoPE offers a Pareto improvement for memory-constrained deployment with minor quality loss."}}
{"id": "2506.09349", "pdf": "https://arxiv.org/pdf/2506.09349", "abs": "https://arxiv.org/abs/2506.09349", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "AI": {"tldr": "This paper introduces OmniDRCA, a parallel speech-text foundation model using joint autoregressive modeling with dual-resolution speech representations and contrastive cross-modal alignment, achieving SOTA performance on Spoken Question Answering benchmarks.", "motivation": "To improve text generation awareness of concurrent speech synthesis and enhance audio comprehension through contrastive alignment.", "method": "OmniDRCA, a parallel speech-text foundation model based on joint autoregressive modeling with dual-resolution speech representations and contrastive cross-modal alignment.", "result": "Experimental results show that OmniDRCA establishes new SOTA performance among parallel joint speech-text modeling based foundation models and achieves competitive performance compared to interleaved models.", "conclusion": "OmniDRCA shows potential in full-duplex conversational scenarios."}}
{"id": "2506.09351", "pdf": "https://arxiv.org/pdf/2506.09351", "abs": "https://arxiv.org/abs/2506.09351", "authors": ["Yuchen Feng", "Bowen Shen", "Naibin Gu", "Jiaxuan Zhao", "Peng Fu", "Zheng Lin", "Weiping Wang"], "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture\nachieve high cost-efficiency by selectively activating a subset of the\nparameters. Despite the inference efficiency of MoE LLMs, the training of\nextensive experts from scratch incurs substantial overhead, whereas\nreconstructing a dense LLM into an MoE LLM significantly reduces the training\nbudget. However, existing reconstruction methods often overlook the diversity\namong experts, leading to potential redundancy. In this paper, we come up with\nthe observation that a specific LLM exhibits notable diversity after being\npruned on different calibration datasets, based on which we present a\nDiversity-Enhanced reconstruction method named DIVE. The recipe of DIVE\nincludes domain affinity mining, pruning-based expert reconstruction, and\nefficient retraining. Specifically, the reconstruction includes pruning and\nreassembly of the feed-forward network (FFN) module. After reconstruction, we\nefficiently retrain the model on routers, experts and normalization modules. We\nimplement DIVE on Llama-style LLMs with open-source training corpora.\nExperiments show that DIVE achieves training efficiency with minimal accuracy\ntrade-offs, outperforming existing pruning and MoE reconstruction methods with\nthe same number of activated parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDIVE\u7684\u591a\u6837\u6027\u589e\u5f3a\u7684\u91cd\u5efa\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u62ec\u9886\u57df\u4eb2\u548c\u6027\u6316\u6398\u3001\u57fa\u4e8e\u526a\u679d\u7684\u4e13\u5bb6\u91cd\u5efa\u548c\u9ad8\u6548\u518d\u8bad\u7ec3\u3002\u5728Llama\u98ce\u683c\u7684LLMs\u4e0a\u5b9e\u73b0\u4e86DIVE\uff0c\u5e76\u4f7f\u7528\u5f00\u6e90\u8bad\u7ec3\u8bed\u6599\u5e93\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aDIVE\u5728\u6700\u5c0f\u7684\u51c6\u786e\u6027\u6743\u8861\u4e0b\u5b9e\u73b0\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4f18\u4e8e\u5177\u6709\u76f8\u540c\u6fc0\u6d3b\u53c2\u6570\u6570\u91cf\u7684\u73b0\u6709\u526a\u679d\u548cMoE\u91cd\u5efa\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684MoE LLMs\u8bad\u7ec3\u65b9\u6cd5\u8981\u4e48\u6709\u5f88\u9ad8\u7684\u521d\u59cb\u8bad\u7ec3\u5f00\u9500\uff0c\u8981\u4e48\u5728\u5c06\u5bc6\u96c6LLM\u91cd\u6784\u4e3aMoE LLM\u65f6\u5ffd\u7565\u4e13\u5bb6\u4e4b\u95f4\u7684\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u6f5c\u5728\u7684\u5197\u4f59\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDIVE\u7684\u591a\u6837\u6027\u589e\u5f3a\u7684\u91cd\u5efa\u65b9\u6cd5\uff0c\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a1\uff09\u9886\u57df\u4eb2\u548c\u6027\u6316\u6398\uff1b2\uff09\u57fa\u4e8e\u526a\u679d\u7684\u4e13\u5bb6\u91cd\u5efa\uff08\u6d89\u53caFFN\u6a21\u5757\u7684\u526a\u679d\u548c\u91cd\u7ec4\uff09\uff1b3\uff09\u5bf9\u8def\u7531\u5668\u3001\u4e13\u5bb6\u548c\u5f52\u4e00\u5316\u6a21\u5757\u8fdb\u884c\u9ad8\u6548\u518d\u8bad\u7ec3\u3002", "result": "\u5728Llama\u98ce\u683c\u7684LLMs\u4e0a\u5b9e\u73b0DIVE\uff0c\u5e76\u4f7f\u7528\u5f00\u6e90\u8bad\u7ec3\u8bed\u6599\u5e93\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aDIVE\u5728\u6700\u5c0f\u7684\u51c6\u786e\u6027\u6743\u8861\u4e0b\u5b9e\u73b0\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "DIVE\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3MoE\u67b6\u6784LLMs\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u5197\u4f59\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.09359", "pdf": "https://arxiv.org/pdf/2506.09359", "abs": "https://arxiv.org/abs/2506.09359", "authors": ["Qingyun Zeng", "Simin Ma", "Arash Niknafs", "Ashish Basran", "Carol Szabo"], "title": "Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL", "categories": ["cs.CL"], "comment": "8 pages", "summary": "The rise of Large Language Models (LLMs) has significantly advanced\nText-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of\ngenerated SQL remains a challenge, especially given ambiguous user queries and\nmultiple valid SQL interpretations. This paper explores using LLMs to assess\nboth semantic and a more practical \"weak\" semantic equivalence. We analyze\ncommon patterns of SQL equivalence and inequivalence, discuss challenges in\nLLM-based evaluation.", "AI": {"tldr": "This paper discusses using large language models to evaluate both semantic and weak semantic equivalence in text-to-sql systems.", "motivation": "Evaluating semantic equivalence of generated SQL is challenging due to ambiguous user queries and multiple valid SQL interpretations.", "method": "Exploring the use of LLMs for assessing both semantic and weak semantic equivalence.", "result": "Analysis of common patterns of SQL equivalence and inequivalence, discussion of challenges in LLM-based evaluation.", "conclusion": "LLMs can be used to evaluate semantic and weak semantic equivalence in text-to-SQL systems."}}
{"id": "2506.09367", "pdf": "https://arxiv.org/pdf/2506.09367", "abs": "https://arxiv.org/abs/2506.09367", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "categories": ["cs.CL", "cs.AI"], "comment": "BEA 2025", "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "AI": {"tldr": "This paper introduces COGENT, a framework for creating educational content aligned with curriculum standards and appropriate for different grades. It uses three curriculum components, readability control, and a 'wonder-based' approach.", "motivation": "To address the challenges of using Generative AI in educational contexts, particularly in STEM, where maintaining grade-appropriate language and engaging students is difficult.", "method": "Developing COGENT which includes curriculum components, readability control methods, and a 'wonder-based' approach for content generation.", "result": "COGENT generates grade-appropriate content that matches or exceeds human-written references based on multi-dimensional evaluations.", "conclusion": "The proposed framework offers a scalable solution for providing adaptive and high-quality educational resources."}}
{"id": "2506.09375", "pdf": "https://arxiv.org/pdf/2506.09375", "abs": "https://arxiv.org/abs/2506.09375", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "AI": {"tldr": "Introduces CoLMbo, a Speaker Language Model that combines speaker encoder with prompt-based conditioning to generate detailed speaker descriptions, enhancing traditional speaker profiling and performing well in zero-shot scenarios.", "motivation": "Speaker recognition systems struggle to generate detailed speaker characteristics or provide context-rich descriptions. They mainly focus on speaker identification but fail to capture demographic attributes like dialect, gender, and age systematically.", "method": "Integrating a speaker encoder with prompt-based conditioning to create detailed captions based on speaker embeddings.", "result": "CoLMbo can adapt dynamically to new speaker characteristics using user-defined prompts and provide customized descriptions including regional dialect variations and age-related traits. It performs well in zero-shot scenarios across diverse datasets.", "conclusion": "This approach not only improves traditional speaker profiling but also represents a significant advancement in speaker recognition."}}
{"id": "2506.09381", "pdf": "https://arxiv.org/pdf/2506.09381", "abs": "https://arxiv.org/abs/2506.09381", "authors": ["Austin McCutcheon", "Thiago E. A. de Oliveira", "Aleksandr Zheleznov", "Chris Brogly"], "title": "Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of online news enables potential widespread publication of\nperceived low-quality news headlines/links. As a result, we investigated\nwhether it was possible to automatically distinguish perceived lower-quality\nnews headlines/links from perceived higher-quality headlines/links. We\nevaluated twelve machine learning models on a binary, balanced dataset of\n57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per\nclass) with 115 extracted linguistic features. Binary labels for each text were\nderived from scores based on expert consensus regarding the respective news\ndomain quality. Traditional ensemble methods, particularly the bagging\nclassifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test\nsplit). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20\ntrain/test split) but required more training time. The results suggest that\nboth NLP features with traditional classifiers and deep learning models can\neffectively differentiate perceived news headline/link quality, with some\ntrade-off between predictive performance and train time.", "AI": {"tldr": "This study evaluates different machine learning models to automatically classify news headlines/links into high or low quality categories. It uses a large dataset of worldwide news links/headings with 115 linguistic features and finds that both traditional ensemble methods and fine-tuned deep learning models perform well.", "motivation": "To automatically distinguish perceived lower-quality news headlines/links from higher-quality ones due to the proliferation of online news.", "method": "Evaluated twelve machine learning models including traditional ensemble methods and fine-tuned DistilBERT on a large balanced dataset of worldwide news links/headings with 115 linguistic features.", "result": "Traditional ensemble methods like the bagging classifier showed strong performance while fine-tuned DistilBERT achieved the highest accuracy but needed more training time.", "conclusion": "Both NLP features with traditional classifiers and deep learning models can effectively differentiate perceived news headline/link quality with a trade-off between predictive performance and training time."}}
{"id": "2506.09391", "pdf": "https://arxiv.org/pdf/2506.09391", "abs": "https://arxiv.org/abs/2506.09391", "authors": ["Haoran Zhao", "Robert D. Hawkins"], "title": "Comparing human and LLM politeness strategies in free production", "categories": ["cs.CL"], "comment": "25 pages, 5 figures", "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.", "AI": {"tldr": "Large language models face challenges in polite speech alignment. Study shows bigger models can replicate key preferences and human evaluators prefer LLM-generated responses in open-ended contexts.", "motivation": "To investigate if LLMs use similar context-sensitive linguistic strategies as humans do.", "method": "Comparing human and LLM responses in constrained and open-ended production tasks.", "result": "Larger models can replicate key preferences and human evaluators prefer LLM responses in open-ended contexts.", "conclusion": "LLMs show impressive handling of politeness strategies but subtly different from humans."}}
{"id": "2506.09393", "pdf": "https://arxiv.org/pdf/2506.09393", "abs": "https://arxiv.org/abs/2506.09393", "authors": ["Xinyi Gao", "Qiucheng Wu", "Yang Zhang", "Xuechen Liu", "Kaizhi Qian", "Ying Xu", "Shiyu Chang"], "title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "categories": ["cs.CL"], "comment": "24 pages, 4 figures", "summary": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state\nand predict their performance on new exercises based on performance history.\nMany realistic classroom settings for KT are typically low-resource in data and\nrequire online updates as students' exercise history grows, which creates\nsignificant challenges for existing KT approaches. To restore strong\nperformance under low-resource conditions, we revisit the hierarchical\nknowledge concept (KC) information, which is typically available in many\nclassroom settings and can provide strong prior when data are sparse. We\ntherefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a\nprobabilistic KT framework that models student understanding over a\ntree-structured hierarchy of knowledge concepts using a Hidden Markov Tree\nModel. KT$^2$ estimates student mastery via an EM algorithm and supports\npersonalized prediction through an incremental update mechanism as new\nresponses arrive. Our experiments show that KT$^2$ consistently outperforms\nstrong baselines in realistic online, low-resource settings.", "AI": {"tldr": "A probabilistic KT framework named KT$^2$ is proposed to model student understanding over a tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree Model.", "motivation": "Existing KT approaches face significant challenges in low-resource classroom settings. The hierarchical knowledge concept information can provide strong prior when data are sparse.", "method": "Proposed Knowledge-Tree-based Knowledge Tracing (KT$^2$), a probabilistic KT framework that models student understanding over a tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree Model. Estimated student mastery via an EM algorithm and supports personalized prediction through an incremental update mechanism as new responses arrive.", "result": "Experiments show that KT$^2$ consistently outperforms strong baselines in realistic online, low-resource settings.", "conclusion": "KT$^2$ can restore strong performance under low-resource conditions by utilizing the hierarchical knowledge concept information."}}
{"id": "2506.09408", "pdf": "https://arxiv.org/pdf/2506.09408", "abs": "https://arxiv.org/abs/2506.09408", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "AI": {"tldr": "This paper introduces Token Constraint Decoding (TCD), an inference-time algorithm that improves robustness of Large Language Models against input perturbations by enforcing token-level prediction alignment. Experiments show that TCD, combined with prompt engineering, can significantly restore performance on multiple-choice question answering tasks.", "motivation": "To address the vulnerability of Large Language Models to minor input perturbations.", "method": "Introducing and evaluating Token Constraint Decoding (TCD), which enforces token-level prediction alignment during inference.", "result": "TCD, especially when paired with prompt engineering, significantly restores performance degraded by input noise, yielding up to +39% absolute gains for weaker models.", "conclusion": "Token Constraint Decoding is a practical, model-agnostic method to enhance reasoning stability under real-world imperfections."}}
{"id": "2506.09414", "pdf": "https://arxiv.org/pdf/2506.09414", "abs": "https://arxiv.org/abs/2506.09414", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "13 pages, 7 figures, 5 tables", "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "AI": {"tldr": "A novel prompt-guided generative framework called PGDA-KGQA is proposed to enhance Knowledge Graph Question Answering performance by improving data diversity and multi-hop reasoning.", "motivation": "To address the limitations of traditional data augmentation approaches and LLM-based methods in KGQA, specifically the scarcity of diverse annotated data and multi-hop reasoning samples, as well as the issue of semantic distortion.", "method": "The method uses a unified prompt-design paradigm to generate large-scale (question, logical form) pairs for model training, enriching the training set by generating single-hop pseudo questions, applying semantic-preserving question rewriting, and employing answer-guided reverse path exploration.", "result": "Experiments show that PGDA-KGQA outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements in F1, Hits@1, and Accuracy.", "conclusion": "PGDA-KGQA improves the performance of Knowledge Graph Question Answering through a prompt-guided generative framework with multiple data augmentation strategies."}}
{"id": "2506.09424", "pdf": "https://arxiv.org/pdf/2506.09424", "abs": "https://arxiv.org/abs/2506.09424", "authors": ["Md Messal Monem Miah", "Adrita Anika", "Xi Shi", "Ruihong Huang"], "title": "Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Detecting deception in an increasingly digital world is both a critical and\nchallenging task. In this study, we present a comprehensive evaluation of the\nautomated deception detection capabilities of Large Language Models (LLMs) and\nLarge Multimodal Models (LMMs) across diverse domains. We assess the\nperformance of both open-source and commercial LLMs on three distinct datasets:\nreal life trial interviews (RLTD), instructed deception in interpersonal\nscenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the\neffectiveness of different experimental setups for deception detection,\nincluding zero-shot and few-shot approaches with random or similarity-based\nin-context example selection. Our results show that fine-tuned LLMs achieve\nstate-of-the-art performance on textual deception detection tasks, while LMMs\nstruggle to fully leverage cross-modal cues. Additionally, we analyze the\nimpact of auxiliary features, such as non-verbal gestures and video summaries,\nand examine the effectiveness of different prompting strategies, including\ndirect label generation and chain-of-thought reasoning. Our findings provide\nkey insights into how LLMs process and interpret deceptive cues across\nmodalities, highlighting their potential and limitations in real-world\ndeception detection applications.", "AI": {"tldr": "This study evaluates automated deception detection using large language models and multimodal models across various datasets.", "motivation": "Deception detection is crucial in the digital age, and understanding the capabilities of large models can enhance real-world applications.", "method": "Assessing performance of LLMs and LMMs on datasets like RLTD, MU3D, and OpSpam using zero-shot and few-shot approaches.", "result": "Fine-tuned LLMs perform best in textual deception detection, whereas LMMs face challenges with cross-modal cues.", "conclusion": "The study highlights the potential and limitations of LLMs in deception detection, providing valuable insights for future research and application."}}
{"id": "2506.09428", "pdf": "https://arxiv.org/pdf/2506.09428", "abs": "https://arxiv.org/abs/2506.09428", "authors": ["Fei Ding", "Baiqiao Wang"], "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "AI": {"tldr": "A novel cost-effective SFT method is proposed to reduce catastrophic forgetting risk without original SFT data.", "motivation": "Enhancing LLMs' instruction-following capabilities and domain-specific task adaptability while maintaining general capabilities and avoiding catastrophic forgetting.", "method": "Reconstructing SFT instruction distribution, multi-model screening for optimal data selection, mixing with new data for SFT.", "result": "Preserves generalization capabilities in general domains and improves task-specific performance.", "conclusion": "The proposed method effectively reduces the risk of catastrophic forgetting and enhances both general and specific task performances."}}
{"id": "2506.09440", "pdf": "https://arxiv.org/pdf/2506.09440", "abs": "https://arxiv.org/abs/2506.09440", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "categories": ["cs.CL", "cs.AI"], "comment": "ACL-2025 System Demo", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "AI": {"tldr": "Introduces GigaChat, a family of Russian LLMs, detailing architecture, pre-training, experiments, evaluations, and open-source release.", "motivation": "The motivation is to address the limited development of foundational models specifically tailored to the Russian language due to high computational resource requirements.", "method": "The method involves developing Russian LLMs in various sizes, detailing the model architecture, pre-training process, and experiments, and evaluating their performance on Russian and English benchmarks.", "result": "The result includes the introduction of the GigaChat family of Russian LLMs, evaluations on Russian and English benchmarks, and comparisons with multilingual analogs.", "conclusion": "The paper concludes by releasing three open GigaChat models in open-source and demonstrating the top-performing models through different interfaces."}}
{"id": "2506.09450", "pdf": "https://arxiv.org/pdf/2506.09450", "abs": "https://arxiv.org/abs/2506.09450", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "AI": {"tldr": "This paper introduces UniToMBench, a unified benchmark that enhances and evaluates Theory of Mind (ToM) capabilities in large language models (LLMs). It combines multi-interaction tasks and diverse evaluation metrics.", "motivation": "To address the challenge of accurately predicting human mental states in LLMs.", "method": "Integrates SimToM and TOMBENCH to create UniToMBench, using over 1,000 hand-written scenarios and perspective-taking techniques.", "result": "Models like GPT-4o and GPT-4o Mini perform well in emotional and belief tasks but show variability in knowledge-based tasks.", "conclusion": "UniToMBench serves as a valuable tool for improving and assessing ToM capabilities in LLMs."}}
{"id": "2506.09457", "pdf": "https://arxiv.org/pdf/2506.09457", "abs": "https://arxiv.org/abs/2506.09457", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "AI": {"tldr": "This paper identifies a 'reward-generation gap' issue in Direct Alignment Algorithms (DAAs) for aligning large language models with human preferences, caused by the mismatch between prefix tokens' importance and implicit reward functions. To solve this, a new approach called Prefix-Oriented Equal-length Training (POET) is introduced, which truncates responses to match shorter lengths, improving DAA performance significantly.", "motivation": "The motivation is to address the 'reward-generation gap' issue in DAAs, which hinders their alignment with human preferences due to the misalignment between training and inference.", "method": "The method involves introducing POET, which truncates both preferred and dispreferred responses to match the shorter length, ensuring optimization convergence across all token positions, especially focusing on prefix tokens.", "result": "Experiments with DPO and SimPO show that POET improves their performance, achieving up to 15.6 points in AlpacaEval 2 and overall improvements across downstream tasks.", "conclusion": "The study concludes that addressing the misalignment between reward optimization and generation performance in DAAs is crucial, with POET being an effective solution."}}
{"id": "2506.09495", "pdf": "https://arxiv.org/pdf/2506.09495", "abs": "https://arxiv.org/abs/2506.09495", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u4e09\u79cd\u65b9\u6cd5\u63a2\u8ba8\u4e86YouTube\u4e0a\u81ea\u6740\u884c\u4e3a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u6570\u5b57\u884c\u4e3a\u548c\u4e34\u5e8a\u89c1\u89e3\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u9274\u4e8e\u897f\u65b9\u56fd\u5bb6\u81ea\u6740\u4ecd\u7136\u662f\u4e3b\u8981\u6b7b\u56e0\uff0c\u4ee5\u53ca\u793e\u4ea4\u5a92\u4f53\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u81ea\u6740\u884c\u4e3a\u5728YouTube\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4e13\u5bb6\u77e5\u8bc6\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u81ea\u4e0b\u800c\u4e0a\u65b9\u6cd5\u3001\u6df7\u5408\u65b9\u6cd5\u548c\u4e13\u5bb6\u9a71\u52a8\u81ea\u4e0a\u800c\u4e0b\u7684\u65b9\u6cd5\uff0c\u5728\u5305\u542b181\u4e2aYouTube\u9891\u9053\u7684\u65b0\u578b\u7eb5\u5411\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7814\u7a76\uff0c\u8fd9\u4e9b\u9891\u9053\u5c5e\u4e8e\u6709\u5371\u53ca\u751f\u547d\u4f01\u56fe\u7684\u4eba\uff0c\u540c\u65f6\u5305\u62ec134\u4e2a\u5bf9\u7167\u9891\u9053\u3002", "result": "\u5728\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\u4e2d\uff0cLLM\u9a71\u52a8\u7684\u4e3b\u9898\u5efa\u6a21\u8bc6\u522b\u51fa166\u4e2a\u4e3b\u9898\u4e2d\u7684\u4e94\u4e2a\u4e0e\u81ea\u6740\u4f01\u56fe\u76f8\u5173\uff0c\u5e76\u4e14\u6709\u4e24\u4e2a\u4e3b\u9898\u663e\u793a\u51fa\u4e0e\u81ea\u6740\u4f01\u56fe\u76f8\u5173\u7684\u65f6\u5e8f\u53d8\u5316\u3002\u5728\u6df7\u5408\u65b9\u6cd5\u4e2d\uff0c\u4e34\u5e8a\u4e13\u5bb6\u5ba1\u67e5LLM\u884d\u751f\u4e3b\u9898\u5e76\u6807\u8bb0\u4e8619\u4e2a\u4e3b\u9898\u4e3a\u4e0e\u81ea\u6740\u76f8\u5173\uff0c\u4f46\u6ca1\u6709\u53d1\u73b0\u663e\u8457\u7684\u65f6\u5e8f\u6548\u5e94\u3002\u5728\u81ea\u4e0a\u800c\u4e0b\u7684\u65b9\u6cd5\u4e2d\uff0c\u5bf9\u81ea\u6740\u4f01\u56fe\u53d9\u8ff0\u7684\u5fc3\u7406\u8bc4\u4f30\u63ed\u793a\u4e86\u4e24\u4e2a\u7ec4\u4e4b\u95f4\u552f\u4e00\u663e\u8457\u5dee\u5f02\u5728\u4e8e\u5206\u4eab\u7ecf\u9a8c\u7684\u52a8\u673a\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u4e09\u79cd\u4e92\u8865\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u81ea\u6740\u884c\u4e3a\u7684\u7ec6\u81f4\u7406\u89e3\uff0c\u5f25\u5408\u4e86\u6570\u5b57\u884c\u4e3a\u548c\u4e34\u5e8a\u89c1\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.09501", "pdf": "https://arxiv.org/pdf/2506.09501", "abs": "https://arxiv.org/abs/2506.09501", "authors": ["Jiayi Yuan", "Hao Li", "Xinheng Ding", "Wenya Xie", "Yu-Jhe Li", "Wentian Zhao", "Kun Wan", "Jing Shi", "Xia Hu", "Zirui Liu"], "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are now integral across various domains and have\ndemonstrated impressive performance. Progress, however, rests on the premise\nthat benchmark scores are both accurate and reproducible. We demonstrate that\nthe reproducibility of LLM performance is fragile: changing system\nconfiguration such as evaluation batch size, GPU count, and GPU version can\nintroduce significant difference in the generated responses. This issue is\nespecially pronounced in reasoning models, where minor rounding differences in\nearly tokens can cascade into divergent chains of thought, ultimately affecting\naccuracy. For instance, under bfloat16 precision with greedy decoding, a\nreasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation\nin accuracy and 9,000 tokens difference in response length due to differences\nin GPU count, type, and evaluation batch size. We trace the root cause of this\nvariability to the non-associative nature of floating-point arithmetic under\nlimited numerical precision. This work presents the first systematic\ninvestigation into how numerical precision affects reproducibility in LLM\ninference. Through carefully controlled experiments across various hardware,\nsoftware, and precision settings, we quantify when and how model outputs\ndiverge. Our analysis reveals that floating-point precision -- while critical\nfor reproducibility -- is often neglected in evaluation practices. Inspired by\nthis, we develop a lightweight inference pipeline, dubbed LayerCast, that\nstores weights in 16-bit precision but performs all computations in FP32,\nbalancing memory efficiency with numerical stability. Code is available at\nhttps://github.com/nanomaoli/llm_reproducibility.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u6570\u503c\u7cbe\u5ea6\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u53ef\u91cd\u590d\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aLayerCast\u7684\u65b0\u65b9\u6cd5\u6765\u5e73\u8861\u5185\u5b58\u6548\u7387\u4e0e\u6570\u503c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u4e2a\u9886\u57df\u90fd\u5177\u6709\u91cd\u8981\u4f5c\u7528\u5e76\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6027\u80fd\u7684\u53ef\u91cd\u590d\u6027\u5374\u5f88\u8106\u5f31\u3002\u6539\u53d8\u7cfb\u7edf\u914d\u7f6e\u5982\u8bc4\u4f30\u6279\u91cf\u5927\u5c0f\u3001GPU\u6570\u91cf\u548cGPU\u7248\u672c\u53ef\u4ee5\u5f15\u5165\u663e\u8457\u5dee\u5f02\u5728\u751f\u6210\u7684\u54cd\u5e94\u4e2d\u3002\u8fd9\u79cd\u95ee\u9898\u5728\u63a8\u7406\u6a21\u578b\u4e2d\u5c24\u4e3a\u660e\u663e\uff0c\u5176\u4e2d\u65e9\u671f\u4ee4\u724c\u4e2d\u7684\u5c0f\u6570\u70b9\u5dee\u5f02\u53ef\u80fd\u4f1a\u5bfc\u81f4\u94fe\u5f0f\u601d\u7ef4\u7684\u53d1\u6563\uff0c\u6700\u7ec8\u5f71\u54cd\u51c6\u786e\u6027\u3002\u4f8b\u5982\uff0c\u5728bfloat16\u7cbe\u5ea6\u548c\u8d2a\u5a6a\u89e3\u7801\u4e0b\uff0c\u63a8\u7406\u6a21\u578b\u5982DeepSeek-R1-Distill-Qwen-7B\u53ef\u80fd\u7531\u4e8eGPU\u6570\u91cf\u3001\u7c7b\u578b\u548c\u8bc4\u4f30\u6279\u91cf\u5927\u5c0f\u7684\u4e0d\u540c\u800c\u51fa\u73b0\u9ad8\u8fbe9\uff05\u7684\u51c6\u786e\u6027\u53d8\u5316\u548c9000\u4e2a\u4ee4\u724c\u7684\u54cd\u5e94\u957f\u5ea6\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5728\u5404\u79cd\u786c\u4ef6\u3001\u8f6f\u4ef6\u548c\u7cbe\u5ea6\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4ed4\u7ec6\u63a7\u5236\u7684\u5b9e\u9a8c\uff0c\u91cf\u5316\u4e86\u6a21\u578b\u8f93\u51fa\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u53d1\u751f\u5206\u6b67\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63a8\u7406\u7ba1\u9053LayerCast\uff0c\u5b58\u50a8\u6743\u91cd\u572816\u4f4d\u7cbe\u5ea6\u4f46\u6267\u884c\u6240\u6709\u8ba1\u7b97\u5728FP32\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6d6e\u70b9\u7cbe\u5ea6\u5bf9\u4e8e\u53ef\u91cd\u590d\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u901a\u5e38\u5728\u8bc4\u4f30\u5b9e\u8df5\u4e2d\u88ab\u5ffd\u89c6\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLayerCast\u7684\u8f7b\u91cf\u7ea7\u63a8\u7406\u7ba1\u9053\uff0c\u5b83\u572816\u4f4d\u7cbe\u5ea6\u4e0b\u5b58\u50a8\u6743\u91cd\uff0c\u4f46\u5728FP32\u4e2d\u6267\u884c\u6240\u6709\u8ba1\u7b97\uff0c\u4ece\u800c\u5e73\u8861\u4e86\u5185\u5b58\u6548\u7387\u4e0e\u6570\u503c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u6570\u503c\u7cbe\u5ea6\u5bf9LLM\u63a8\u7406\u53ef\u91cd\u590d\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u63a7\u5236\u7684\u5b9e\u9a8c\u91cf\u5316\u4e86\u6a21\u578b\u8f93\u51fa\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u53d1\u751f\u5206\u6b67\u3002\u5206\u6790\u63ed\u793a\uff0c\u867d\u7136\u6d6e\u70b9\u7cbe\u5ea6\u5bf9\u4e8e\u53ef\u91cd\u590d\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u8bc4\u4f30\u5b9e\u8df5\u4e2d\u5f80\u5f80\u88ab\u5ffd\u89c6\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63a8\u7406\u7ba1\u9053LayerCast\uff0c\u5e73\u8861\u4e86\u5185\u5b58\u6548\u7387\u4e0e\u6570\u503c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.09507", "pdf": "https://arxiv.org/pdf/2506.09507", "abs": "https://arxiv.org/abs/2506.09507", "authors": ["Bingheng Wu", "Jingze Shi", "Yifan Wu", "Nan Tang", "Yuyu Luo"], "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongruity in their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance. To address this\nimpediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE})\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this \\ourRoPE, we\nintroduce \\textbf{\\model}, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4K sequence length, \\model exhibits training and inference speeds that are\n\\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard\nTransformer models. It also delivers higher accuracy: under comparable\nsettings, it surpasses a Transformer baseline by over 4\\% on language modeling\nbenchmarks. \\model furthermore scales more effectively: \\model-1.3B gains\n\\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\%\ngains for equivalent Transformers or SSMs). Our results show that unified\npositional encoding resolves positional incompatibility in hybrid models,\nenabling efficient, high-performance long-context modeling.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.09513", "pdf": "https://arxiv.org/pdf/2506.09513", "abs": "https://arxiv.org/abs/2506.09513", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "24 pages, 6 figures, 7 tables", "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "AI": {"tldr": "Introduce ReasonMed, the largest medical reasoning dataset, to improve LLMs' performance in knowledge-intensive medical question answering.", "motivation": "Explore LLMs' capabilities in knowledge-intensive medical question answering.", "method": "Construct ReasonMed via multi-agent verification and refinement process and train ReasonMed-7B based on effective fine-tuning strategy.", "result": "ReasonMed-7B outperforms previous best model and even exceeds LLaMA3.1-70B on PubMedQA.", "conclusion": "ReasonMed and ReasonMed-7B set new benchmarks for medical reasoning models."}}
{"id": "2506.09542", "pdf": "https://arxiv.org/pdf/2506.09542", "abs": "https://arxiv.org/abs/2506.09542", "authors": ["Dingjun Wu", "Yukun Yan", "Zhenghao Liu", "Zhiyuan Liu", "Maosong Sun"], "title": "KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding\nresponses in external knowledge. However, existing methods typically rely on a\nsingle source, either unstructured text or structured knowledge. Moreover, they\nlack cognitively inspired mechanisms for activating relevant knowledge. To\naddress these issues, we propose KG-Infused RAG, a framework that integrates\nKGs into RAG systems to implement spreading activation, a cognitive process\nthat enables concept association and inference. KG-Infused RAG retrieves KG\nfacts, expands the query accordingly, and enhances generation by combining\ncorpus passages with structured facts, enabling interpretable, multi-source\nretrieval grounded in semantic structure. We further improve KG-Infused RAG via\npreference learning on sampled key stages in the pipeline. Experiments on five\nQA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by\n3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG\nbrings further performance gains, demonstrating its effectiveness and\nversatility as a plug-and-play enhancement module for corpus-based RAG methods.", "AI": {"tldr": "KG-Infused RAG combines knowledge graphs and retrieval-augmented generation to enhance factual accuracy and interpretability.", "motivation": "Existing RAG methods lack cognitively inspired mechanisms and rely on single knowledge sources.", "method": "Integrating knowledge graphs into RAG systems using spreading activation, expanding queries, and enhancing generation with combined corpus passages and structured facts.", "result": "Improved performance on five QA benchmarks compared to vanilla RAG, with additional gains when integrated into Self-RAG.", "conclusion": "KG-Infused RAG is an effective and versatile plug-and-play enhancement module for RAG methods."}}
{"id": "2506.09556", "pdf": "https://arxiv.org/pdf/2506.09556", "abs": "https://arxiv.org/abs/2506.09556", "authors": ["Georgios Chatzichristodoulou", "Despoina Kosmopoulou", "Antonios Kritikos", "Anastasia Poulopoulou", "Efthymios Georgiou", "Athanasios Katsamanis", "Vassilis Katsouros", "Alexandros Potamianos"], "title": "MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions", "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "SER is a challenging task due to the subjective nature of human emotions and\ntheir uneven representation under naturalistic conditions. We propose MEDUSA, a\nmultimodal framework with a four-stage training pipeline, which effectively\nhandles class imbalance and emotion ambiguity. The first two stages train an\nensemble of classifiers that utilize DeepSER, a novel extension of a deep\ncross-modal transformer fusion mechanism from pretrained self-supervised\nacoustic and linguistic representations. Manifold MixUp is employed for further\nregularization. The last two stages optimize a trainable meta-classifier that\ncombines the ensemble predictions. Our training approach incorporates human\nannotation scores as soft targets, coupled with balanced data sampling and\nmultitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion\nRecognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic\nConditions Challenge.", "AI": {"tldr": "MEDUSA, a multimodal framework with a four-stage training pipeline, achieved state-of-the-art results in speech emotion recognition.", "motivation": "To address the challenges of subjective human emotions and their uneven representation under naturalistic conditions.", "method": "A four-stage training pipeline using an ensemble of classifiers with DeepSER and Manifold MixUp, followed by optimization of a trainable meta-classifier.", "result": "Effective handling of class imbalance and emotion ambiguity, leading to high performance in categorical emotion recognition.", "conclusion": "MEDUSA ranked 1st in Task 1 of the Interspeech 2025 challenge."}}
{"id": "2506.09558", "pdf": "https://arxiv.org/pdf/2506.09558", "abs": "https://arxiv.org/abs/2506.09558", "authors": ["Eleni Gkovedarou", "Joke Daems", "Luna De Bruyne"], "title": "Gender Bias in English-to-Greek Machine Translation", "categories": ["cs.CL"], "comment": "Accepted at GITT 2025 (MT Summit)", "summary": "As the demand for inclusive language increases, concern has grown over the\nsusceptibility of machine translation (MT) systems to reinforce gender\nstereotypes. This study investigates gender bias in two commercial MT systems,\nGoogle Translate and DeepL, focusing on the understudied English-to-Greek\nlanguage pair. We address three aspects of gender bias: i) male bias, ii)\noccupational stereotyping, and iii) errors in anti-stereotypical translations.\nAdditionally, we explore the potential of prompted GPT-4o as a bias mitigation\ntool that provides both gender-explicit and gender-neutral alternatives when\nnecessary. To achieve this, we introduce GendEL, a manually crafted bilingual\ndataset of 240 gender-ambiguous and unambiguous sentences that feature\nstereotypical occupational nouns and adjectives. We find persistent gender bias\nin translations by both MT systems; while they perform well in cases where\ngender is explicitly defined, with DeepL outperforming both Google Translate\nand GPT-4o in feminine gender-unambiguous sentences, they are far from\nproducing gender-inclusive or neutral translations when the gender is\nunspecified. GPT-4o shows promise, generating appropriate gendered and neutral\nalternatives for most ambiguous cases, though residual biases remain evident.", "AI": {"tldr": "This study examines gender bias in Google Translate and DeepL for English-to-Greek translations, identifying male bias, occupational stereotyping, and errors in anti-stereotypical translations. It also explores GPT-4o as a bias mitigation tool, using a new dataset called GendEL.", "motivation": "Concern over machine translation systems reinforcing gender stereotypes due to increased demand for inclusive language.", "method": "Investigating gender bias in two commercial MT systems and exploring GPT-4o's potential as a bias mitigation tool with a manually crafted bilingual dataset.", "result": "Both MT systems show persistent gender bias, performing well only when gender is explicitly defined; GPT-4o shows promise but residual biases remain.", "conclusion": "Further research is needed to improve gender inclusivity in machine translation systems."}}
{"id": "2506.09560", "pdf": "https://arxiv.org/pdf/2506.09560", "abs": "https://arxiv.org/abs/2506.09560", "authors": ["Stefan Krsteski", "Matea Tashkovska", "Borjan Sazdov", "Hristijan Gjoreski", "Branislav Gerazov"], "title": "Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language", "categories": ["cs.CL"], "comment": "Camera-ready version accepted at SlavNLP-2025@ACL", "summary": "The increase in technological adoption worldwide comes with demands for novel\ntools to be used by the general population. Large Language Models (LLMs)\nprovide a great opportunity in this respect, but their capabilities remain\nlimited for low-resource languages, restricting applications in countries where\nsuch languages are spoken. We create several resources to facilitate the\nadoption of LLMs and to support research advancements for Macedonian. We\ncollect the largest Macedonian corpus to date, consisting of 40GB of textual\ndata and totaling 3.5B words. To support conversational applications, we\ncollect a 106k-instance instruction dataset, carefully built to be culturally\ngrounded. For evaluation, we construct a Macedonian evaluation suite covering\nseven benchmarks. Finally, we train domestic-yak, a state-of-the-art\n8B-parameter model, on our curated datasets and evaluate it against eight\nbaseline models using the newly constructed benchmark suite. Our model\noutperforms all existing models in the 8B parameter range across all\nbenchmarks, and achieves performance comparable to models up to 10x larger.\nFurthermore, a qualitative analysis with native speakers reveals that our model\nis preferred over larger counterparts, receiving higher ratings for grammatical\ncorrectness and cultural appropriateness. All datasets, code, and model weights\nare openly released, setting a foundation for advancing LLMs in similarly\nunderrepresented languages. These resources are publicly available at\ngithub.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained\nmodel weights and data.", "AI": {"tldr": "Create resources to support Macedonian language in large language models (LLMs), including a large Macedonian corpus, an instruction dataset, and an evaluation suite. Train a state-of-the-art 8B-parameter model, domestic-yak, which outperforms other models in the same size range and is preferred by native speakers.", "motivation": "Support research advancements and adoption of LLMs for low-resource languages like Macedonian.", "method": "Collect a large Macedonian corpus, build an instruction dataset for conversational applications, construct an evaluation suite, and train a domestic-yak model.", "result": "The domestic-yak model outperforms all existing models in the 8B parameter range across all benchmarks and achieves performance comparable to models up to 10x larger. It is also preferred by native speakers for its grammatical correctness and cultural appropriateness.", "conclusion": "These resources set a foundation for advancing LLMs in underrepresented languages and are openly released."}}
{"id": "2506.09566", "pdf": "https://arxiv.org/pdf/2506.09566", "abs": "https://arxiv.org/abs/2506.09566", "authors": ["Bla\u017e \u0160krlj", "Boshko Koloski", "Senja Pollak", "Nada Lavra\u010d"], "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To-appear as a book chapter", "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u77e5\u8bc6\u56fe\u8c31\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u534f\u540c\u4f5c\u7528\u53ca\u5176\u5e94\u7528\uff0c\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u589e\u5f3a\u4e8b\u5b9e\u57fa\u7840\u548c\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u96c6\u6210\u83b7\u5f97\u4e92\u60e0\u4e92\u5229\u3002", "method": "\u5bf9\u77e5\u8bc6\u56fe\u8c31\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u8003\u5bdf\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u7c7b\u4e3a\u4e24\u7c7b\uff1a\u589e\u5f3a\u578b\u77e5\u8bc6\u56fe\u8c31\u7684\u5927\u8bed\u8a00\u6a21\u578b\u548c\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u8bc6\u522b\u51fa\u5173\u952e\u5dee\u8ddd\uff0c\u5e76\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u96c6\u6210\u7684\u76f8\u4e92\u76ca\u5904\uff1b\u7279\u522b\u5173\u6ce8\u53ef\u6269\u5c55\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u795e\u7ecf\u7b26\u53f7\u6574\u5408\u3001\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u66f4\u65b0\u3001\u6570\u636e\u53ef\u9760\u6027\u53ca\u4f26\u7406\u8003\u91cf\uff0c\u63a8\u52a8\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\u4efb\u52a1\u7684\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.09591", "pdf": "https://arxiv.org/pdf/2506.09591", "abs": "https://arxiv.org/abs/2506.09591", "authors": ["Stefan Arnold"], "title": "Memorization in Language Models through the Lens of Intrinsic Dimension", "categories": ["cs.CL"], "comment": null, "summary": "Language Models (LMs) are prone to memorizing parts of their data during\ntraining and unintentionally emitting them at generation time, raising concerns\nabout privacy leakage and disclosure of intellectual property. While previous\nresearch has identified properties such as context length, parameter size, and\nduplication frequency, as key drivers of unintended memorization, little is\nknown about how the latent structure modulates this rate of memorization. We\ninvestigate the role of Intrinsic Dimension (ID), a geometric proxy for the\nstructural complexity of a sequence in latent space, in modulating\nmemorization. Our findings suggest that ID acts as a suppressive signal for\nmemorization: compared to low-ID sequences, high-ID sequences are less likely\nto be memorized, particularly in overparameterized models and under sparse\nexposure. These findings highlight the interaction between scale, exposure, and\ncomplexity in shaping memorization.", "AI": {"tldr": "Investigate the role of Intrinsic Dimension in modulating memorization in Language Models.", "motivation": "Concerns about privacy leakage and intellectual property disclosure due to Language Models unintentionally emitting memorized parts of their data during generation.", "method": "Investigate the role of Intrinsic Dimension as a geometric proxy for structural complexity of a sequence in latent space.", "result": "High-ID sequences are less likely to be memorized than low-ID sequences, especially in overparameterized models and under sparse exposure.", "conclusion": "The interaction between scale, exposure, and complexity shapes memorization."}}
{"id": "2506.09627", "pdf": "https://arxiv.org/pdf/2506.09627", "abs": "https://arxiv.org/abs/2506.09627", "authors": ["Nicolas Audinet de Pieuchon", "Adel Daoud", "Connor T. Jerzak", "Moa Johansson", "Richard Johansson"], "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.", "AI": {"tldr": "This study examines debiasing methods (DSL and PPI) for large language model text annotations, showing that DSL generally performs better but there's a trade-off in performance consistency across datasets.", "motivation": "To address the inconsistency of large language models (LLMs) in text annotation and the potential bias in downstream estimates, debiasing methods like Design-based Supervised Learning (DSL) and Prediction-Powered Inference (PPI) have been developed.", "method": "Studying the performance scaling of debiasing methods with the number of expert annotations and comparing DSL and PPI across various tasks.", "result": "DSL often outperforms PPI in terms of bias reduction and empirical efficiency, but its performance varies across datasets. There is a bias-variance tradeoff among debiasing methods, requiring further research on efficiency metrics in finite samples.", "conclusion": "More research is needed on metrics to quantify the efficiency of debiasing methods in finite samples."}}
{"id": "2506.09641", "pdf": "https://arxiv.org/pdf/2506.09641", "abs": "https://arxiv.org/abs/2506.09641", "authors": ["Anna Stein", "Kevin Tang"], "title": "Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning", "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": "Submitted to Interspeech 2025", "summary": "This study compares probabilistic predictors based on information theory with\nNaive Discriminative Learning (NDL) predictors in modeling acoustic word\nduration, focusing on probabilistic reduction. We examine three models using\nthe Buckeye corpus: one with NDL-derived predictors using information-theoretic\nformulas, one with traditional NDL predictors, and one with N-gram\nprobabilistic predictors. Results show that the N-gram model outperforms both\nNDL models, challenging the assumption that NDL is more effective due to its\ncognitive motivation. However, incorporating information-theoretic formulas\ninto NDL improves model performance over the traditional model. This research\nhighlights a) the need to incorporate not only frequency and contextual\npredictability but also average contextual predictability, and b) the\nimportance of combining information-theoretic metrics of predictability and\ninformation derived from discriminative learning in modeling acoustic\nreduction.", "AI": {"tldr": "This study compared probabilistic predictors and Naive Discriminative Learning (NDL) predictors in modeling acoustic word duration, showing that the N-gram model outperformed both NDL models.", "motivation": "To compare probabilistic predictors based on information theory with NDL predictors in modeling acoustic word duration.", "method": "Examined three models using the Buckeye corpus: one with NDL-derived predictors using information-theoretic formulas, one with traditional NDL predictors, and one with N-gram probabilistic predictors.", "result": "The N-gram model outperformed both NDL models, and incorporating information-theoretic formulas into NDL improved model performance over the traditional model.", "conclusion": "In modeling acoustic reduction, it's important to combine information-theoretic metrics of predictability and information derived from discriminative learning."}}
{"id": "2506.09643", "pdf": "https://arxiv.org/pdf/2506.09643", "abs": "https://arxiv.org/abs/2506.09643", "authors": ["Harry Walsh", "Maksym Ivashechkin", "Richard Bowden"], "title": "Using Sign Language Production as Data Augmentation to enhance Sign Language Translation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Machine learning models fundamentally rely on large quantities of\nhigh-quality data. Collecting the necessary data for these models can be\nchallenging due to cost, scarcity, and privacy restrictions. Signed languages\nare visual languages used by the deaf community and are considered low-resource\nlanguages. Sign language datasets are often orders of magnitude smaller than\ntheir spoken language counterparts. Sign Language Production is the task of\ngenerating sign language videos from spoken language sentences, while Sign\nLanguage Translation is the reverse translation task. Here, we propose\nleveraging recent advancements in Sign Language Production to augment existing\nsign language datasets and enhance the performance of Sign Language Translation\nmodels. For this, we utilize three techniques: a skeleton-based approach to\nproduction, sign stitching, and two photo-realistic generative models, SignGAN\nand SignSplat. We evaluate the effectiveness of these techniques in enhancing\nthe performance of Sign Language Translation models by generating variation in\nthe signer's appearance and the motion of the skeletal data. Our results\ndemonstrate that the proposed methods can effectively augment existing datasets\nand enhance the performance of Sign Language Translation models by up to 19%,\npaving the way for more robust and accurate Sign Language Translation systems,\neven in resource-constrained environments.", "AI": {"tldr": "We propose using advancements in sign language production to improve sign language translation models by generating variations in signer appearance and skeletal motion.", "motivation": "The challenge of limited data availability for sign languages due to cost, scarcity, and privacy issues.", "method": "Utilizing three techniques: skeleton-based production, sign stitching, and generative models (SignGAN and SignSplat).", "result": "Enhanced sign language translation model performance by up to 19%.", "conclusion": "Our methods effectively augment datasets and improve translation models, enabling more robust systems in resource-limited settings."}}
{"id": "2506.09645", "pdf": "https://arxiv.org/pdf/2506.09645", "abs": "https://arxiv.org/abs/2506.09645", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.6"], "comment": "32 pages, 28 figures", "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.09657", "pdf": "https://arxiv.org/pdf/2506.09657", "abs": "https://arxiv.org/abs/2506.09657", "authors": ["Nikolas Evkarpidi", "Elena Tutubalina"], "title": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA", "categories": ["cs.CL"], "comment": "Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures", "summary": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository.", "AI": {"tldr": "This paper presents a system for QA over tabular data, achieving 80% accuracy and a top-13 ranking in the SemEval 2025 Task 8.", "motivation": "To develop a robust system for question answering over tabular data that improves open-source model accuracy and performs comparably to proprietary LLMs.", "method": "The system includes text-to-SQL/text-to-code generation, self-correction mechanism, RAG, and E2E modules, orchestrated by a large language model.", "result": "Achieved 80% accuracy and ranked top-13 among 38 teams in the competition.", "conclusion": "Our approach shows significant improvement in open-source QA models and comparable performance to proprietary LLMs."}}
{"id": "2506.09669", "pdf": "https://arxiv.org/pdf/2506.09669", "abs": "https://arxiv.org/abs/2506.09669", "authors": ["Lihu Chen", "Ga\u00ebl Varoquaux"], "title": "Query-Level Uncertainty in Large Language Models", "categories": ["cs.CL"], "comment": "In Progress", "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called \\emph{Internal Confidence}, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance.", "AI": {"tldr": "This work proposes a method named Internal Confidence to detect knowledge boundaries at query level without token generation, demonstrating its effectiveness in factual QA and mathematical reasoning tasks.", "motivation": "To enable Large Language Models to recognize the boundary of their knowledge for adaptive inference and efficient AI development.", "method": "Introducing a novel training-free method called Internal Confidence that uses self-evaluations across layers and tokens to detect knowledge boundaries.", "result": "Outperforms several baselines in factual QA and mathematical reasoning tasks and shows potential for efficient RAG and model cascading.", "conclusion": "The proposed Internal Confidence method effectively detects knowledge boundaries and has applications in improving efficiency and performance of RAG and model cascading."}}
{"id": "2506.09672", "pdf": "https://arxiv.org/pdf/2506.09672", "abs": "https://arxiv.org/abs/2506.09672", "authors": ["Hao Xiong", "Chuanyuan Tan", "Wenliang Chen"], "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%", "AI": {"tldr": "This paper addresses two issues in Unstructured Knowledge Editing (UKE): lack of locality evaluation and abnormal failure of fine-tuning (FT) based methods. The authors construct two datasets and identify four factors affecting FT-based methods' performance, proposing an optimal training recipe. Experimental results show that the FT-UKE method outperforms existing SOTA methods.", "motivation": "To improve the evaluation and performance of fine-tuning methods in Unstructured Knowledge Editing (UKE) tasks.", "method": "Constructed two datasets, identified four factors affecting FT-based methods, and proposed an optimal training recipe for UKE tasks.", "result": "The proposed FT-UKE method outperforms existing SOTA methods, especially in batch editing scenarios where its advantage increases with batch size.", "conclusion": "Fine-tuning-based methods can achieve superior performance in UKE tasks when trained optimally, addressing the issues of locality evaluation and abnormal failures."}}
{"id": "2506.09684", "pdf": "https://arxiv.org/pdf/2506.09684", "abs": "https://arxiv.org/abs/2506.09684", "authors": ["Haoyi Song", "Ruihan Ji", "Naichen Shi", "Fan Lai", "Raed Al Kontar"], "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.", "AI": {"tldr": "This paper provides a theoretical justification for the role of perturbations in UQ for LLMs and proposes a fully probabilistic framework based on an inverse model, which defines a new uncertainty measure, Inv-Entropy. The framework is flexible and supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics.", "motivation": "Existing UQ methods are often heuristic and lack a probabilistic foundation. Reliable deployment of large language models (LLMs) requires effective uncertainty quantification (UQ).", "method": "A fully probabilistic framework based on an inverse model is proposed to quantify uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. GAAP, a perturbation algorithm based on genetic algorithms, is also proposed to enhance the diversity of sampled inputs. A new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), is introduced to directly assess uncertainty.", "result": "Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods.", "conclusion": "Inv-Entropy outperforms existing semantic UQ methods."}}
{"id": "2506.09790", "pdf": "https://arxiv.org/pdf/2506.09790", "abs": "https://arxiv.org/abs/2506.09790", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "categories": ["cs.CL", "cs.CV", "cs.SE"], "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.", "AI": {"tldr": "ComfyUI-R1 is the first large reasoning model for automated workflow generation in AI art creation.", "motivation": "The steep learning curve in creating effective workflows on platforms like ComfyUI motivated the development of a reasoning model to assist users.", "method": "A two-stage framework was used: CoT fine-tuning and reinforcement learning with a hybrid reward system.", "result": "The 7B-parameter model achieved a 97% format validity rate and surpassed previous state-of-the-art methods.", "conclusion": "ComfyUI-R1 demonstrates the power of long CoT reasoning in synthesizing complex workflows for AI art creation."}}
{"id": "2506.09796", "pdf": "https://arxiv.org/pdf/2506.09796", "abs": "https://arxiv.org/abs/2506.09796", "authors": ["Andreas S\u00e4uberli", "Diego Frassinelli", "Barbara Plank"], "title": "Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?", "categories": ["cs.CL"], "comment": "Accepted for publication at the 20th Workshop on Innovative Use of\n  NLP for Building Educational Applications (BEA) at ACL 2025", "summary": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e8618\u4e2a\u6307\u4ee4\u8c03\u6574\u540e\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6559\u80b2\u8bc4\u4f30\u6d4b\u8bd5\u9879\u76ee\u7684\u54cd\u5e94\u884c\u4e3a\u7684\u62df\u4eba\u5316\u7a0b\u5ea6\uff0c\u53d1\u73b0\u867d\u7136LLMs\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u5f97\u50cf\u4eba\u7c7b\uff0c\u4f46\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4e0d\u5e94\u5c06\u5176\u7528\u4e8e\u6559\u80b2\u8bc4\u4f30\u7684\u8bd5\u70b9\u7814\u7a76\u3002", "motivation": "\u4e86\u89e3\u8003\u751f\u5982\u4f55\u56de\u7b54\u6559\u80b2\u8bc4\u4f30\u4e2d\u7684\u95ee\u9898\u5bf9\u4e8e\u6d4b\u8bd5\u5f00\u53d1\u3001\u8bc4\u4f30\u9879\u76ee\u8d28\u91cf\u548c\u63d0\u9ad8\u6d4b\u8bd5\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u8bd5\u70b9\u7814\u7a76\u3002\u5982\u679c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u6d4b\u8bd5\u9879\u76ee\u7684\u54cd\u5e94\u884c\u4e3a\u50cf\u4eba\u7c7b\u4e00\u6837\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u4f5c\u4e3a\u8bd5\u70b9\u53c2\u4e0e\u8005\u4ee5\u52a0\u901f\u6d4b\u8bd5\u5f00\u53d1\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e86\u6765\u81ea\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u7684\u591a\u9009\u9898\u6d4b\u8bd5\u9879\u76ee\uff0c\u6d89\u53ca\u4e09\u4e2a\u79d1\u76ee\uff1a\u9605\u8bfb\u3001\u7f8e\u56fd\u5386\u53f2\u548c\u7ecf\u6d4e\u5b66\u3002\u7814\u7a76\u65b9\u6cd5\u57fa\u4e8e\u6559\u80b2\u8bc4\u4f30\u4e2d\u5e38\u7528\u7684\u7ecf\u5178\u6d4b\u8bd5\u7406\u8bba\u548c\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u7684\u4e24\u4e2a\u7406\u8bba\u6846\u67b6\u3002", "result": "\u867d\u7136\u66f4\u5927\u7684\u6a21\u578b\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4f46\u5f53\u4f7f\u7528\u6e29\u5ea6\u7f29\u653e\u8fdb\u884c\u6821\u51c6\u65f6\uff0c\u5b83\u4eec\u7684\u54cd\u5e94\u5206\u5e03\u53ef\u4ee5\u66f4\u50cf\u4eba\u7c7b\u3002\u6b64\u5916\uff0c\u5728\u9605\u8bfb\u7406\u89e3\u95ee\u9898\u4e0a\uff0cLLMs\u7684\u8868\u73b0\u6bd4\u5176\u4ed6\u5b66\u79d1\u4e0e\u4eba\u7c7b\u7684\u76f8\u5173\u6027\u66f4\u597d\u3002", "conclusion": "\u867d\u7136\u66f4\u5927\u7684\u6a21\u578b\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4f46\u5f53\u4f7f\u7528\u6e29\u5ea6\u7f29\u653e\u8fdb\u884c\u6821\u51c6\u65f6\uff0c\u5b83\u4eec\u7684\u54cd\u5e94\u5206\u5e03\u53ef\u4ee5\u66f4\u50cf\u4eba\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0LLMs\u5728\u9605\u8bfb\u7406\u89e3\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u6bd4\u5176\u4ed6\u5b66\u79d1\u4e0e\u4eba\u7c7b\u7684\u76f8\u5173\u6027\u66f4\u597d\u3002\u7136\u800c\uff0c\u603b\u4f53\u76f8\u5173\u6027\u5e76\u4e0d\u5f3a\uff0c\u8fd9\u8868\u660eLLMs\u4e0d\u5e94\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7528\u4e8e\u6559\u80b2\u8bc4\u4f30\u7684\u8bd5\u70b9\u7814\u7a76\u3002"}}
{"id": "2506.09820", "pdf": "https://arxiv.org/pdf/2506.09820", "abs": "https://arxiv.org/abs/2506.09820", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "title": "CoRT: Code-integrated Reasoning within Thinking", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.", "AI": {"tldr": "CoRT is a post-training framework that teaches Large Reasoning Models (LRMs) to use Code Interpreter (CI) more effectively and efficiently. It addresses inefficiencies in handling complex math problems by creating code-integrated reasoning data using Hint-Engineering.", "motivation": "Improve efficiency and accuracy of LRMs in dealing with complex mathematical operations.", "method": "Post-training framework called CoRT, including hint-engineering, supervised fine-tuning, rejection fine-tuning and reinforcement learning.", "result": "Achieved 4% and 8% absolute improvements on two DeepSeek-R1-Distill-Qwen models across five challenging mathematical reasoning datasets. Used fewer tokens compared to natural language models.", "conclusion": "CoRT can enhance the performance of LRMs in mathematical reasoning tasks."}}
{"id": "2506.09827", "pdf": "https://arxiv.org/pdf/2506.09827", "abs": "https://arxiv.org/abs/2506.09827", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Felix Friedrich", "Maurice Kraus", "Kourosh Nadi", "Huu Nguyen", "Kristian Kersting", "S\u00f6ren Auer"], "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.", "AI": {"tldr": "Introduces EmoNet-Voice, a new resource for speech emotion detection, including a large-scale pre-training dataset and a novel benchmark dataset with human expert annotations. Demonstrates its effectiveness through empathic insight voice models.", "motivation": "The need for robust benchmarks to evaluate emotional understanding capabilities of AI systems due to limitations in current SER datasets.", "method": "Curated synthetic audio snippets using state-of-the-art voice generation to simulate actors portraying scenes designed to evoke specific emotions. Conducted rigorous validation by psychology experts.", "result": "EmoNet-Voice includes EmoNet-Voice Big, a large-scale pre-training dataset, and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. The dataset evaluates SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities.", "conclusion": "Introduces Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts."}}
{"id": "2506.09833", "pdf": "https://arxiv.org/pdf/2506.09833", "abs": "https://arxiv.org/abs/2506.09833", "authors": ["Omar Sherif", "Ali Hamdi"], "title": "Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation", "categories": ["cs.CL", "I.2.1"], "comment": "6 pages, 1 figure. To appear in Intelligent Methods, Systems, and\n  Applications 2025", "summary": "Effective rehabilitation assessment is essential for monitoring patient\nprogress, particularly in home-based settings. Existing systems often face\nchallenges such as data imbalance and difficulty detecting subtle movement\nerrors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method\nthat generates synthetic skeleton data by simulating clinically relevant\nmovement mistakes. Unlike standard augmentation techniques, EGPA targets\nbiomechanical errors observed in rehabilitation. Combined with an\nattention-based graph convolutional network, EGPA improves performance across\nmultiple evaluation metrics. Experiments demonstrate reductions in mean\nabsolute error of up to 27.6 percent and gains in error classification accuracy\nof 45.8 percent. Attention visualizations show that the model learns to focus\non clinically significant joints and movement phases, enhancing both accuracy\nand interpretability. EGPA offers a promising approach for improving automated\nmovement quality assessment in both clinical and home-based rehabilitation\ncontexts.", "AI": {"tldr": "Introduces Error-Guided Pose Augmentation (EGPA) to generate synthetic skeleton data by simulating biomechanical errors for better automated movement quality assessment.", "motivation": "Existing systems face challenges like data imbalance and difficulty detecting subtle movement errors in home-based rehabilitation settings.", "method": "EGPA generates synthetic data targeting biomechanical errors and combines with an attention-based graph convolutional network.", "result": "Performance improved with a reduction in mean absolute error up to 27.6% and an increase in error classification accuracy of 45.8%.", "conclusion": "EGPA enhances both accuracy and interpretability, offering a promising approach for automated movement quality assessment in clinical and home-based settings."}}
{"id": "2506.09847", "pdf": "https://arxiv.org/pdf/2506.09847", "abs": "https://arxiv.org/abs/2506.09847", "authors": ["Tomas Peterka", "Matyas Bohacek"], "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.", "AI": {"tldr": "This paper introduces a News Media Provenance Dataset with provenance-tagged images for news articles. Two tasks, location of origin relevance (LOR) and date and time of origin relevance (DTOR), are formulated on this dataset. Baseline results on six large language models (LLMs) show promising zero-shot performance on LOR but hindered performance on DTOR.", "motivation": "To address out-of-context and misattributed imagery, which is a leading form of media manipulation in today's misinformation and disinformation landscape.", "method": "Introducing the News Media Provenance Dataset and formulating two tasks, LOR and DTOR, on it. Presenting baseline results on six large language models (LLMs).", "result": "Promising zero-shot performance on LOR task but hindered performance on DTOR task.", "conclusion": "The work identifies room for specialized architectures and future work to improve DTOR performance."}}
{"id": "2506.09853", "pdf": "https://arxiv.org/pdf/2506.09853", "abs": "https://arxiv.org/abs/2506.09853", "authors": ["Xiangning Yu", "Zhuohan Wang", "Linyi Yang", "Haoxuan Li", "Anjie Liu", "Xiao Xue", "Jun Wang", "Mengyue Yang"], "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u6846\u67b6\u6765\u89e3\u51b3\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u5145\u5206\u6027\u548c\u5fc5\u8981\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u548c\u51cf\u5c11\u4ee4\u724c\u4f7f\u7528\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u590d\u6742\u63a8\u7406\u80fd\u529b", "method": "\u63d0\u51fa\u56e0\u679c\u6846\u67b6\uff0c\u901a\u8fc7\u5145\u5206\u6027\u548c\u5fc5\u8981\u6027\u7684\u53cc\u91cd\u89c6\u89d2\u6765\u8868\u5f81\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406", "result": "\u5728\u5404\u79cd\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u63a8\u7406\u6548\u7387\u7684\u63d0\u9ad8\u548c\u4ee4\u724c\u4f7f\u7528\u7684\u51cf\u5c11", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u7684\u65b9\u5411"}}
{"id": "2506.09886", "pdf": "https://arxiv.org/pdf/2506.09886", "abs": "https://arxiv.org/abs/2506.09886", "authors": ["Rodion Oblovatny", "Alexandra Bazarova", "Alexey Zaytsev"], "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u5e76\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\uff0c\u56e0\u4e3a\u5e7b\u89c9\u53ef\u80fd\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u63d0\u793a\u548c\u54cd\u5e94\u9690\u85cf\u72b6\u6001\u5206\u5e03\u4e4b\u95f4\u7684\u6982\u7387\u53d1\u6563\u6765\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u7684\u65b0\u65b9\u6cd5\u3002\u4f7f\u7528\u5206\u5e03\u8ddd\u79bb\u4f5c\u4e3a\u539f\u5219\u6027\u7684\u5e7b\u89c9\u8bc4\u5206\uff0c\u5e76\u91c7\u7528\u6df1\u5ea6\u53ef\u5b66\u4e60\u5185\u6838\u81ea\u52a8\u9002\u5e94\u6355\u6349\u5206\u5e03\u4e4b\u95f4\u7684\u7ec6\u5fae\u51e0\u4f55\u5dee\u5f02\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5185\u5728\u68c0\u6d4b\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\uff0c\u5e76\u5728\u51e0\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51e0\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5373\u4f7f\u6ca1\u6709\u5185\u6838\u8bad\u7ec3\uff0c\u8be5\u65b9\u6cd5\u4ecd\u7136\u662f\u7ade\u4e89\u6027\u7684\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u5e7b\u89c9\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09890", "pdf": "https://arxiv.org/pdf/2506.09890", "abs": "https://arxiv.org/abs/2506.09890", "authors": ["Yuxin Chen", "Yiran Zhao", "Yang Zhang", "An Zhang", "Kenji Kawaguchi", "Shafiq Joty", "Junnan Li", "Tat-Seng Chua", "Michael Qizhe Shieh", "Wenxuan Zhang"], "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u53d1\u5c55\u8bed\u8a00\u65e0\u5173\u7684\u53c2\u6570\u7a7a\u95f4\u652f\u6301\u62bd\u8c61\u601d\u7ef4\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u4e0d\u540c\u8bed\u8a00\u65f6\u7684\u8868\u73b0\u6311\u6218\u4e86\u4e4b\u524d\u8ba4\u4e3aLLMs\u53ef\u80fd\u201c\u4ee5\u82f1\u8bed\u601d\u7ef4\u201d\u7684\u5047\u8bbe\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793aLLMs\u662f\u5426\u771f\u7684\u53d1\u5c55\u51fa\u4e86\u4e00\u79cd\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u5e76\u63a2\u7d22\u8fd9\u79cd\u80fd\u529b\u5982\u4f55\u652f\u6301\u62bd\u8c61\u601d\u7ef4\u7684\u51fa\u73b0\u3002", "method": "\u8bc6\u522b\u548c\u5206\u7c7b\u4e0e\u7279\u5b9a\u8bed\u8a00\u76f8\u5173\u7684\u795e\u7ecf\u5143\uff0c\u5206\u6790\u5b83\u4eec\u5728\u4e0d\u540c\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u6fc0\u6d3b\u60c5\u51b5\uff0c\u5e76\u89c2\u5bdf\u5176\u968f\u65f6\u95f4\u53d8\u5316\u7684\u8d8b\u52bf\u3002\u540c\u65f6\u63d0\u51fa\u9488\u5bf9\u4e0d\u540c\u53d1\u5c55\u9636\u6bb5LLMs\u7684\u8bed\u8a00\u65e0\u5173\u8bad\u7ec3\u7b56\u7565\u3002", "result": "LLMs\u786e\u5b9e\u53d1\u5c55\u51fa\u4e86\u4e00\u4e2a\u6838\u5fc3\u7684\u8bed\u8a00\u65e0\u5173\u53c2\u6570\u7a7a\u95f4\uff0c\u8fd9\u4e2a\u7a7a\u95f4\u7531\u5c11\u91cf\u5173\u952e\u53c2\u6570\u7ec4\u6210\uff0c\u8fd9\u4e9b\u53c2\u6570\u7684\u5931\u6d3b\u4f1a\u5bfc\u81f4\u6240\u6709\u8bed\u8a00\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u968f\u7740\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5171\u4eab\u795e\u7ecf\u5143\u7684\u6bd4\u4f8b\u548c\u529f\u80fd\u91cd\u8981\u6027\u589e\u52a0\uff0c\u800c\u4e13\u5c5e\u795e\u7ecf\u5143\u7684\u5f71\u54cd\u9010\u6e10\u51cf\u5f31\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0cLLMs\u80fd\u591f\u8d85\u8d8a\u4e2a\u522b\u8bed\u8a00\u7cfb\u7edf\uff0c\u5c55\u73b0\u51fa\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u62bd\u8c61\u601d\u7ef4\u80fd\u529b\u3002\u63d0\u51fa\u7684\u795e\u7ecf\u5143\u7279\u5b9a\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347LLMs\u7684\u591a\u8bed\u8a00\u901a\u7528\u6027\u80fd\u3002"}}
{"id": "2506.09902", "pdf": "https://arxiv.org/pdf/2506.09902", "abs": "https://arxiv.org/abs/2506.09902", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.", "AI": {"tldr": "Introduce PersonaLens, a benchmark for evaluating personalization in task-oriented AI assistants.", "motivation": "Systematically evaluating personalization in conversational AI assistants remains challenging due to lack of comprehensive benchmarks.", "method": "Developed two specialized LLM-based agents: a user agent and a judge agent.", "result": "Found significant variability in personalization capabilities of current LLM assistants.", "conclusion": "PersonaLens provides insights for advancing conversational AI systems."}}
{"id": "2506.09917", "pdf": "https://arxiv.org/pdf/2506.09917", "abs": "https://arxiv.org/abs/2506.09917", "authors": ["Wendi Zhou", "Ameer Saadat-Yazd", "Nadin Kokciyan"], "title": "Aspect-Based Opinion Summarization with Argumentation Schemes", "categories": ["cs.CL"], "comment": "Accepted by ArgMining 2025", "summary": "Reviews are valuable resources for customers making purchase decisions in\nonline shopping. However, it is impractical for customers to go over the vast\nnumber of reviews and manually conclude the prominent opinions, which prompts\nthe need for automated opinion summarization systems. Previous approaches,\neither extractive or abstractive, face challenges in automatically producing\ngrounded aspect-centric summaries. In this paper, we propose a novel\nsummarization system that not only captures predominant opinions from an aspect\nperspective with supporting evidence, but also adapts to varying domains\nwithout relying on a pre-defined set of aspects. Our proposed framework,\nASESUM, summarizes viewpoints relevant to the critical aspects of a product by\nextracting aspect-centric arguments and measuring their salience and validity.\nWe conduct experiments on a real-world dataset to demonstrate the superiority\nof our approach in capturing diverse perspectives of the original reviews\ncompared to new and existing methods.", "AI": {"tldr": "This paper presents ASESUM, a novel summarization system that captures predominant opinions from an aspect perspective with supporting evidence and adapts to varying domains without relying on pre-defined aspects.", "motivation": "Automated opinion summarization is needed due to the impracticality for customers to manually conclude prominent opinions from a vast number of reviews.", "method": "ASESUM extracts aspect-centric arguments and measures their salience and validity to summarize viewpoints relevant to critical aspects of a product.", "result": "Experiments on a real-world dataset demonstrate the superiority of ASESUM in capturing diverse perspectives compared to new and existing methods.", "conclusion": "The proposed method effectively addresses the challenge of automatically producing grounded aspect-centric summaries."}}
{"id": "2506.09942", "pdf": "https://arxiv.org/pdf/2506.09942", "abs": "https://arxiv.org/abs/2506.09942", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 8 figures", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c4\u5219\u4ee3\u7801\u9a8c\u8bc1\u548c\u5927\u578b\u63a8\u7406\u6a21\u578b\u9a8c\u8bc1\u7684\u65b9\u6cd5VerIF\uff0c\u7528\u4e8e\u6307\u4ee4\u8ddf\u968f\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u8ddf\u968f\u6570\u636e\u96c6VerInstruct\u3002\u901a\u8fc7\u4f7f\u7528VerIF\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4e24\u4e2a\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u8fbe\u5230\u4e86\u540c\u7c7b\u6a21\u578b\u4e2d\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u6ca1\u6709\u53d7\u5230\u5f71\u54cd\uff0c\u5efa\u8bae\u53ef\u4ee5\u5c06VerIF\u6574\u5408\u5230\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u4ee5\u63d0\u9ad8\u6574\u4f53\u6a21\u578b\u6027\u80fd\u3002\u7814\u7a76\u6570\u636e\u3001\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002", "motivation": "\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u5728\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u5df2\u6210\u4e3a\u5173\u952e\u6280\u672f\uff0c\u4f46\u6307\u4ee4\u8ddf\u968f\u7684\u5f3a\u5316\u5b66\u4e60\u6700\u4f73\u5b9e\u8df5\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVerIF\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u89c4\u5219\u4ee3\u7801\u9a8c\u8bc1\u548c\u5927\u578b\u63a8\u7406\u6a21\u578b\u9a8c\u8bc1\u3002\u4e3a\u4e86\u652f\u6301\u8fd9\u79cd\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7ea622,000\u4e2a\u5b9e\u4f8b\u7684\u6570\u636e\u96c6VerInstruct\uff0c\u5176\u4e2d\u5305\u542b\u4e0e\u9a8c\u8bc1\u4fe1\u53f7\u76f8\u5173\u7684\u5b9e\u4f8b\u3002", "result": "\u4f7f\u7528VerIF\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u540e\uff0c\u4e24\u4e2a\u6a21\u578b\u5728\u51e0\u4e2a\u4ee3\u8868\u6027\u7684\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u8fbe\u5230\u4e86\u540c\u7c7b\u6a21\u578b\u4e2d\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u4e14\u5728\u672a\u89c1\u8fc7\u7684\u7ea6\u675f\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\u3002\u6b64\u5916\uff0c\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u6ca1\u6709\u53d7\u5230\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0cVerIF\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u4e2d\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2506.09944", "pdf": "https://arxiv.org/pdf/2506.09944", "abs": "https://arxiv.org/abs/2506.09944", "authors": ["Wuwei Zhang", "Fangcong Yin", "Howard Yen", "Danqi Chen", "Xi Ye"], "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking", "categories": ["cs.CL"], "comment": null, "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs.", "AI": {"tldr": "Introduce QRHEAD and QR- RETRIEVER for enhancing long context retrieval.", "motivation": "Improve retrieval capabilities in long-context language models.", "method": "Aggregate attention scores with respect to input query and select relevant parts with highest retrieval scores.", "result": "Over 10% performance gain on multi-hop reasoning tasks and strong zero-shot performance on BEIR benchmark.", "conclusion": "Contributes a general-purpose retriever and provides interpretability insights into long-context LM capabilities."}}
{"id": "2506.09967", "pdf": "https://arxiv.org/pdf/2506.09967", "abs": "https://arxiv.org/abs/2506.09967", "authors": ["Shangshang Wang", "Julian Asilis", "\u00d6mer Faruk Akg\u00fcl", "Enes Burak Bilgin", "Ollie Liu", "Deqing Fu", "Willie Neiswanger"], "title": "Resa: Transparent Reasoning Models via SAEs", "categories": ["cs.CL"], "comment": null, "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\$1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround \\$1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.", "AI": {"tldr": "Resa is a family of 1.5B reasoning models trained through a novel and efficient SAE-Tuning method, which reduces training costs and time significantly while retaining high reasoning performance.", "motivation": "To find a cost-effective way to elicit strong reasoning in language models by leveraging their underlying representations.", "method": "Introducing Resa, a family of reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure.", "result": "When applied to certain base models, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while drastically reducing training costs and time. It also enables strong reasoning performance for lightly RL-trained models with minimal additional cost.", "conclusion": "The reasoning abilities extracted via SAEs are potentially generalizable and modular, and extensive ablations validate these findings."}}
{"id": "2506.09975", "pdf": "https://arxiv.org/pdf/2506.09975", "abs": "https://arxiv.org/abs/2506.09975", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "categories": ["cs.CL"], "comment": "to appear in ACL Findings", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.", "AI": {"tldr": "This paper examines the challenge of identifying AI-generated text on social media, demonstrating that while detection is possible under certain conditions, it becomes significantly harder when attackers use fine-tuned models that aren't publicly available.", "motivation": "To address the potential misuse of AI-generated posts in influencing online opinions and decisions.", "method": "Creating a large dataset of AI-generated social media posts using various types of language models and testing different detection methods.", "result": "Detection effectiveness drops significantly when dealing with fine-tuned models that aren't released publicly.", "conclusion": "Fine-tuning is a common practice for real-world applications, making the detection of AI-generated content more challenging."}}
{"id": "2506.09983", "pdf": "https://arxiv.org/pdf/2506.09983", "abs": "https://arxiv.org/abs/2506.09983", "authors": ["Hiroshi Matsuda", "Chunpeng Ma", "Masayuki Asahara"], "title": "Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs", "categories": ["cs.CL"], "comment": "9 pages, 2 figures, accepted for SyntaxFest 2025", "summary": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9010\u6b65\u6307\u4ee4\u7b56\u7565\uff0c\u7ed3\u5408\u7b80\u5316\u7248\u7684CoNLL-U\u683c\u5f0f\uff0c\u572817\u79cd\u8bed\u8a00\u7684Universal Dependencies\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u591a\u8bed\u8a00\u5fae\u8c03\u53ef\u4ee5\u63d0\u9ad8\u8de8\u8bed\u8a00\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u63d0\u793a\u65b9\u6cd5\u5728\u751f\u6210\u7ed3\u6784\u6709\u6548\u548c\u51c6\u786e\u8f93\u51fa\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u4f9d\u8d56\u89e3\u6790\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9010\u6b65\u6307\u4ee4\u7b56\u7565\uff0c\u5305\u62ec\u901a\u7528\u8bcd\u6027\u6807\u6ce8\uff0c\u968f\u540e\u9884\u6d4b\u53e5\u6cd5\u5934\u548c\u4f9d\u5b58\u6807\u7b7e\uff0c\u5e76\u4f7f\u7528\u7b80\u5316\u7248\u7684CoNLL-U\u7c7b\u4f3c\u8f93\u51fa\u683c\u5f0f\u3002", "result": "\u8be5\u65b9\u6cd5\u572817\u79cd\u8bed\u8a00\u7684Universal Dependencies\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u4e14\u6ca1\u6709\u4ea7\u751f\u5e7b\u89c9\u6216\u6c61\u67d3\u3002", "conclusion": "\u663e\u793a\u4e86LLM\u57fa\u7840\u89e3\u6790\u4e2d\u663e\u5f0f\u63a8\u7406\u6b65\u9aa4\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u683c\u5f0f\u4e00\u81f4\u7684\u62ec\u53f7\u65b9\u6cd5\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.09992", "pdf": "https://arxiv.org/pdf/2506.09992", "abs": "https://arxiv.org/abs/2506.09992", "authors": ["Amel Muminovic", "Amela Kadric Muminovic"], "title": "Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Online toxic language causes real harm, especially in regions with limited\nmoderation tools. In this study, we evaluate how large language models handle\ntoxic comments in Serbian, Croatian, and Bosnian, languages with limited\nlabeled data. We built and manually labeled a dataset of 4,500 YouTube and\nTikTok comments drawn from videos across diverse categories, including music,\npolitics, sports, modeling, influencer content, discussions of sexism, and\ngeneral topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude\n3 Opus) were tested in two modes: zero-shot and context-augmented. We measured\nprecision, recall, F1 score, accuracy and false positive rates. Including a\nshort context snippet raised recall by about 0.12 on average and improved F1\nscore by up to 0.10, though it sometimes increased false positives. The best\nbalance came from Gemini in context-augmented mode, reaching an F1 score of\n0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the\nlowest false alarms. We show how adding minimal context can improve toxic\nlanguage detection in low-resource settings and suggest practical strategies\nsuch as improved prompt design and threshold calibration. These results show\nthat prompt design alone can yield meaningful gains in toxicity detection for\nunderserved Balkan language communities.", "AI": {"tldr": "This study examines how large language models perform in identifying toxic comments in Serbian, Croatian, and Bosnian languages. A dataset of 4,500 comments was created and labeled manually. Models like GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus were evaluated in zero-shot and context-augmented modes. Adding context improved recall and F1 scores but also increased false positives. Gemini showed the best balance with an F1 score of 0.82 and accuracy of 0.82.", "motivation": "Toxic online language causes significant harm, particularly in areas with limited moderation resources. This study aims to assess how large language models handle toxic comments in three under-resourced languages.", "method": "A manually labeled dataset of 4,500 YouTube and TikTok comments was created. Four models were tested in zero-shot and context-augmented modes.", "result": "Adding context improved recall and F1 scores but also increased false positives. Gemini performed best in context-augmented mode with an F1 score of 0.82 and accuracy of 0.82.", "conclusion": "Prompt design improvements and threshold calibration can enhance toxicity detection in low-resource language settings."}}
{"id": "2506.09996", "pdf": "https://arxiv.org/pdf/2506.09996", "abs": "https://arxiv.org/abs/2506.09996", "authors": ["Yang Li", "Qiang Sheng", "Yehan Yang", "Xueyao Zhang", "Juan Cao"], "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring", "categories": ["cs.CL", "cs.CY"], "comment": "22 pages, 7 figures, and 9 tables", "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.", "AI": {"tldr": "This paper introduces a novel approach called Streaming Content Monitor (SCM) for real-time harmfulness detection in large language model outputs.", "motivation": "Existing methods suffer from high service latency due to full detection or performance degradation due to training-inference gaps in partial detection.", "method": "Constructing a dataset named FineHarm and proposing the SCM for token-level training with dual supervision.", "result": "The SCM achieves comparable performance to full detection but with only 18% of the tokens examined on average.", "conclusion": "The proposed method not only improves real-time harmfulness detection but also enhances safety alignment."}}
{"id": "2410.16222", "pdf": "https://arxiv.org/pdf/2410.16222", "abs": "https://arxiv.org/abs/2410.16222", "authors": ["Valentyn Boreiko", "Alexander Panfilov", "Vaclav Voracek", "Matthias Hein", "Jonas Geiping"], "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "A plethora of jailbreaking attacks have been proposed to obtain harmful\nresponses from safety-tuned LLMs. These methods largely succeed in coercing the\ntarget output in their original settings, but their attacks vary substantially\nin fluency and computational effort. In this work, we propose a unified threat\nmodel for the principled comparison of these methods. Our threat model checks\nif a given jailbreak is likely to occur in the distribution of text. For this,\nwe build an N-gram language model on 1T tokens, which, unlike model-based\nperplexity, allows for an LLM-agnostic, nonparametric, and inherently\ninterpretable evaluation. We adapt popular attacks to this threat model, and,\nfor the first time, benchmark these attacks on equal footing with it. After an\nextensive comparison, we find attack success rates against safety-tuned modern\nmodels to be lower than previously presented and that attacks based on discrete\noptimization significantly outperform recent LLM-based attacks. Being\ninherently interpretable, our threat model allows for a comprehensive analysis\nand comparison of jailbreak attacks. We find that effective attacks exploit and\nabuse infrequent bigrams, either selecting the ones absent from real-world text\nor rare ones, e.g., specific to Reddit or code datasets.", "AI": {"tldr": "We introduce a unified threat model to compare jailbreak attacks and find that effective attacks rely on rare bigrams.", "motivation": "There are many jailbreaking attacks that can coerce harmful responses from safety-tuned large language models (LLMs). However, these attacks differ greatly in fluency and computational effort.", "method": "Propose a unified threat model for the principled comparison of jailbreak methods. Build an N-gram language model on 1T tokens.", "result": "Attack success rates against safety-tuned modern models are lower than previously presented. Attacks based on discrete optimization significantly outperform recent LLM-based attacks.", "conclusion": "Attack success rates against safety-tuned modern models are lower than previously presented. Effective jailbreak attacks exploit and abuse infrequent bigrams."}}
{"id": "2506.09081", "pdf": "https://arxiv.org/pdf/2506.09081", "abs": "https://arxiv.org/abs/2506.09081", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "AI": {"tldr": "FlagEvalMM\u662f\u4e00\u79cd\u5f00\u653e\u6e90\u4ee3\u7801\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5177\u6709\u7075\u6d3b\u7684\u8d44\u6e90\u5206\u914d\u548c\u9ad8\u6548\u7684\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u63d0\u51fa\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u7075\u6d3b\u5206\u914d\u8d44\u6e90\u5e76\u8f7b\u677e\u96c6\u6210\u65b0\u4efb\u52a1\u548c\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u72ec\u7acb\u7684\u8bc4\u4f30\u670d\u52a1\u5c06\u6a21\u578b\u63a8\u7406\u4e0e\u8bc4\u4f30\u5206\u79bb\uff0c\u5e76\u4f7f\u7528\u5148\u8fdb\u7684\u63a8\u7406\u52a0\u901f\u5de5\u5177\u548c\u5f02\u6b65\u6570\u636e\u52a0\u8f7d\u6765\u663e\u8457\u63d0\u9ad8\u8bc4\u4f30\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlagEvalMM\u63d0\u4f9b\u4e86\u5173\u4e8e\u6a21\u578b\u4f18\u7f3a\u70b9\u7684\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u89c1\u89e3\u3002", "conclusion": "FlagEvalMM\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u63a8\u52a8\u591a\u6a21\u6001\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.09099", "pdf": "https://arxiv.org/pdf/2506.09099", "abs": "https://arxiv.org/abs/2506.09099", "authors": ["Joshua Barron", "Devin White"], "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning", "summary": "The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.", "AI": {"tldr": "This paper investigates the relationship between memorization and generalization in large language models by pre-training capacity-limited Transformer models on synthetic tasks. The results show a trade-off between memorization and extrapolation abilities, which shifts towards memorization as model capacity increases. Joint training on both tasks prevents any model from succeeding at extrapolation.", "motivation": "To understand the relationship between memorization and generalization in large language models and how model capacity influences learning behavior.", "method": "Pre-training a series of capacity-limited Transformer models from scratch on synthetic tasks designed to probe generalization and memorization separately and jointly.", "result": "There is a consistent trade-off between memorization and extrapolation. Larger models memorize better but fail to extrapolate, while smaller models can extrapolate but cannot memorize. Joint training on both tasks prevents any model from succeeding at extrapolation.", "conclusion": "Pre-training may intrinsically favor one learning mode over the other. This study provides insights into how model capacity shapes learning behavior and has implications for designing and deploying small language models."}}
{"id": "2506.09108", "pdf": "https://arxiv.org/pdf/2506.09108", "abs": "https://arxiv.org/abs/2506.09108", "authors": ["Yuwei Zhang", "Kumar Ayush", "Siyuan Qiao", "A. Ali Heydari", "Girish Narayanswamy", "Maxwell A. Xu", "Ahmed A. Metwally", "Shawn Xu", "Jake Garrison", "Xuhai Xu", "Tim Althoff", "Yun Liu", "Pushmeet Kohli", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Cecilia Mascolo", "Xin Liu", "Daniel McDuff", "Yuzhe Yang"], "title": "SensorLM: Learning the Language of Wearable Sensors", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We present SensorLM, a family of sensor-language foundation models that\nenable wearable sensor data understanding with natural language. Despite its\npervasive nature, aligning and interpreting sensor data with language remains\nchallenging due to the lack of paired, richly annotated sensor-text\ndescriptions in uncurated, real-world wearable data. We introduce a\nhierarchical caption generation pipeline designed to capture statistical,\nstructural, and semantic information from sensor data. This approach enabled\nthe curation of the largest sensor-language dataset to date, comprising over\n59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM\nextends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and\nrecovers them as specific variants within a generic architecture. Extensive\nexperiments on real-world tasks in human activity analysis and healthcare\nverify the superior performance of SensorLM over state-of-the-art in zero-shot\nrecognition, few-shot learning, and cross-modal retrieval. SensorLM also\ndemonstrates intriguing capabilities including scaling behaviors, label\nefficiency, sensor captioning, and zero-shot generalization to unseen tasks.", "AI": {"tldr": "SensorLM is a family of sensor-language foundation models that enable wearable sensor data understanding with natural language by introducing a hierarchical caption generation pipeline and extending prominent multimodal pretraining architectures.", "motivation": "To enable wearable sensor data understanding with natural language despite the lack of paired, richly annotated sensor-text descriptions in uncurated, real-world wearable data.", "method": "Introducing a hierarchical caption generation pipeline and extending prominent multimodal pretraining architectures.", "result": "The approach enabled the curation of the largest sensor-language dataset to date, comprising over 59.7 million hours of data from more than 103,000 people.", "conclusion": "SensorLM demonstrates superior performance over state-of-the-art in zero-shot recognition, few-shot learning, and cross-modal retrieval."}}
{"id": "2506.09109", "pdf": "https://arxiv.org/pdf/2506.09109", "abs": "https://arxiv.org/abs/2506.09109", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "categories": ["cs.CV", "cs.CL"], "comment": "Preprint, under review", "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "AI": {"tldr": "Introduce CAIRe, a novel evaluation metric that assesses the degree of cultural relevance of an image.", "motivation": "Ensure equitable performance of text-to-image models across diverse cultural contexts.", "method": "Ground entities and concepts in the image to a knowledge base and use factual information to give independent graded judgments for each culture label.", "result": "CAIRe surpasses all baselines by 28% F1 points on a manually curated dataset and achieves Pearson's correlations of 0.56 and 0.66 with human ratings on two datasets for culturally universal concept.", "conclusion": "CAIRe demonstrates strong alignment with human judgment across diverse image sources."}}
{"id": "2506.09148", "pdf": "https://arxiv.org/pdf/2506.09148", "abs": "https://arxiv.org/abs/2506.09148", "authors": ["Hetvi Waghela", "Jaydip Sen", "Sneha Rakshit", "Subhasis Dasgupta"], "title": "Adversarial Text Generation with Dynamic Contextual Perturbation", "categories": ["cs.CR", "cs.CL"], "comment": "This is the accepted version of the paper, which was presented at\n  IEEE CALCON. The conference was organized at Jadavpur University, Kolkata,\n  from December 14 to 15, 2025. The paper is six pages long, and it consists of\n  six tables and six figures. This is not the final camera-ready version of the\n  paper", "summary": "Adversarial attacks on Natural Language Processing (NLP) models expose\nvulnerabilities by introducing subtle perturbations to input text, often\nleading to misclassification while maintaining human readability. Existing\nmethods typically focus on word-level or local text segment alterations,\noverlooking the broader context, which results in detectable or semantically\ninconsistent perturbations. We propose a novel adversarial text attack scheme\nnamed Dynamic Contextual Perturbation (DCP). DCP dynamically generates\ncontext-aware perturbations across sentences, paragraphs, and documents,\nensuring semantic fidelity and fluency. Leveraging the capabilities of\npre-trained language models, DCP iteratively refines perturbations through an\nadversarial objective function that balances the dual objectives of inducing\nmodel misclassification and preserving the naturalness of the text. This\ncomprehensive approach allows DCP to produce more sophisticated and effective\nadversarial examples that better mimic natural language patterns. Our\nexperimental results, conducted on various NLP models and datasets, demonstrate\nthe efficacy of DCP in challenging the robustness of state-of-the-art NLP\nsystems. By integrating dynamic contextual analysis, DCP significantly enhances\nthe subtlety and impact of adversarial attacks. This study highlights the\ncritical role of context in adversarial attacks and lays the groundwork for\ncreating more robust NLP systems capable of withstanding sophisticated\nadversarial strategies.", "AI": {"tldr": "Propose a new method called Dynamic Contextual Perturbation (DCP) for generating more effective adversarial examples in NLP.", "motivation": "Existing methods overlook broader context leading to detectable or semantically inconsistent perturbations.", "method": "DCP generates context-aware perturbations iteratively refining them through an adversarial objective function balancing model misclassification and text naturalness.", "result": "DCP improves the sophistication and effectiveness of adversarial examples producing more robust challenges to NLP systems.", "conclusion": "DCP demonstrates the importance of context in adversarial attacks promoting the development of more resilient NLP models."}}
{"id": "2506.09171", "pdf": "https://arxiv.org/pdf/2506.09171", "abs": "https://arxiv.org/abs/2506.09171", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07, 68T20, 68T30, 93E35", "I.2.6; I.2.7; I.2.8"], "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the\n  First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada", "summary": "Large Language Models (LLMs) are increasingly capable but often require\nsignificant guidance or extensive interaction history to perform effectively in\ncomplex, interactive environments. Existing methods may struggle with adapting\nto new information or efficiently utilizing past experiences for multi-step\nreasoning without fine-tuning. We introduce a novel LLM agent framework that\nenhances planning capabilities through in-context learning, facilitated by\natomic fact augmentation and a recursive lookahead search. Our agent learns to\nextract task-critical ``atomic facts'' from its interaction trajectories. These\nfacts dynamically augment the prompts provided to LLM-based components\nresponsible for action proposal, latent world model simulation, and state-value\nestimation. Planning is performed via a depth-limited lookahead search, where\nthe LLM simulates potential trajectories and evaluates their outcomes, guided\nby the accumulated facts and interaction history. This approach allows the\nagent to improve its understanding and decision-making online, leveraging its\nexperience to refine its behavior without weight updates. We provide a\ntheoretical motivation linking performance to the quality of fact-based\nabstraction and LLM simulation accuracy. Empirically, our agent demonstrates\nimproved performance and adaptability on challenging interactive tasks,\nachieving more optimal behavior as it accumulates experience, showcased in\ntasks such as TextFrozenLake and ALFWorld.", "AI": {"tldr": "Introduces a novel LLM agent framework that enhances planning capabilities through in-context learning, using atomic fact augmentation and recursive lookahead search, showing improved performance and adaptability on interactive tasks.", "motivation": "Existing methods struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning.", "method": "Uses atomic fact augmentation and recursive lookahead search for planning, allowing the agent to improve its understanding and decision-making online without weight updates.", "result": "Demonstrates improved performance and adaptability on challenging interactive tasks such as TextFrozenLake and ALFWorld.", "conclusion": "This approach allows the agent to leverage its experience to refine its behavior without weight updates."}}
{"id": "2506.09206", "pdf": "https://arxiv.org/pdf/2506.09206", "abs": "https://arxiv.org/abs/2506.09206", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carl Espy-Wilson"], "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Public classroom datasets\nremain limited, and the lack of a dedicated classroom noise corpus prevents the\nuse of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise using game engines, a framework that extends to other domains. Using this\nmethodology, we present SimClass, a dataset that includes both a synthesized\nclassroom noise corpus and a simulated classroom speech dataset. The speech\ndata is generated by pairing a public children's speech corpus with YouTube\nlecture videos to approximate real classroom interactions in clean conditions.\nOur experiments on clean and noisy speech demonstrate that SimClass closely\napproximates real classroom speech, making it a valuable resource for\ndeveloping robust speech recognition and enhancement models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u6e38\u620f\u5f15\u64ce\u5408\u6210\u8bfe\u5802\u566a\u97f3\u7684\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86SimClass\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u5584\u6559\u80b2\u9886\u57df\u7684AI\u8bed\u97f3\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u6559\u80b2\u9886\u57dfAI\u9a71\u52a8\u7684\u8bed\u97f3\u6a21\u578b\u53d1\u5c55\u53d7\u5236\u4e8e\u5927\u89c4\u6a21\u8bfe\u5802\u8bed\u97f3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6e38\u620f\u5f15\u64ce\u5408\u6210\u8bfe\u5802\u566a\u97f3\uff0c\u5e76\u7ed3\u5408\u516c\u5171\u513f\u7ae5\u8bed\u97f3\u8bed\u6599\u5e93\u4e0eYouTube\u8bb2\u5ea7\u89c6\u9891\u751f\u6210\u6a21\u62df\u8bfe\u5802\u8bed\u97f3\u6570\u636e\u96c6SimClass\u3002", "result": "SimClass\u5305\u542b\u5408\u6210\u7684\u8bfe\u5802\u566a\u97f3\u8bed\u6599\u5e93\u548c\u6a21\u62df\u8bfe\u5802\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u6e05\u6d01\u548c\u5608\u6742\u8bed\u97f3\u4e0a\u63a5\u8fd1\u771f\u5b9e\u8bfe\u5802\u8bed\u97f3\u3002", "conclusion": "SimClass\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u53ef\u7528\u4e8e\u5f00\u53d1\u5065\u58ee\u7684\u8bed\u97f3\u8bc6\u522b\u548c\u589e\u5f3a\u6a21\u578b\u3002"}}
{"id": "2506.09260", "pdf": "https://arxiv.org/pdf/2506.09260", "abs": "https://arxiv.org/abs/2506.09260", "authors": ["Yibin Lei", "Tao Shen", "Andrew Yates"], "title": "ThinkQE: Query Expansion via an Evolving Thinking Process", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Effective query expansion for web search benefits from promoting both\nexploration and result diversity to capture multiple interpretations and facets\nof a query. While recent LLM-based methods have improved retrieval performance\nand demonstrate strong domain generalization without additional training, they\noften generate narrowly focused expansions that overlook these desiderata. We\npropose ThinkQE, a test-time query expansion framework addressing this\nlimitation through two key components: a thinking-based expansion process that\nencourages deeper and comprehensive semantic exploration, and a\ncorpus-interaction strategy that iteratively refines expansions using retrieval\nfeedback from the corpus. Experiments on diverse web search benchmarks (DL19,\nDL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,\nincluding training-intensive dense retrievers and rerankers.", "AI": {"tldr": "ThinkQE is a novel test-time query expansion framework that improves web search performance by promoting both exploration and result diversity.", "motivation": "Existing LLM-based methods often generate narrowly focused query expansions that overlook the need for exploration and diversity.", "method": "ThinkQE introduces a thinking-based expansion process and a corpus-interaction strategy to refine query expansions iteratively.", "result": "Experiments on various web search benchmarks show ThinkQE outperforms previous approaches.", "conclusion": "ThinkQE demonstrates strong performance and domain generalization without requiring additional training."}}
{"id": "2506.09289", "pdf": "https://arxiv.org/pdf/2506.09289", "abs": "https://arxiv.org/abs/2506.09289", "authors": ["Boxi Yu", "Yuxuan Zhu", "Pinjia He", "Daniel Kang"], "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench", "categories": ["cs.SE", "cs.CL", "D.0; I.2"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has spurred the development of\ncoding agents for real-world code generation. As a widely used benchmark for\nevaluating the code generation capabilities of these agents, SWE-Bench uses\nreal-world problems based on GitHub issues and their corresponding pull\nrequests. However, the manually written test cases included in these pull\nrequests are often insufficient, allowing generated patches to pass the tests\nwithout resolving the underlying issue. To address this challenge, we introduce\nUTGenerator, an LLM-driven test case generator that automatically analyzes\ncodebases and dependencies to generate test cases for real-world Python\nprojects. Building on UTGenerator, we propose UTBoost, a comprehensive\nframework for test case augmentation. In our evaluation, we identified 36 task\ninstances with insufficient test cases and uncovered 345 erroneous patches\nincorrectly labeled as passed in the original SWE Bench. These corrections,\nimpacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard\nentries, yield 18 and 11 ranking changes, respectively.", "AI": {"tldr": "Analyze the limitations of SWE-Bench and propose UTGenerator and UTBoost to improve the test case generation and augmentation.", "motivation": "SWE-Bench's test cases are often insufficient, allowing incorrect patches to pass undetected.", "method": "Propose UTGenerator to automatically generate test cases and UTBoost for test case augmentation.", "result": "Identified 36 task instances with insufficient test cases and uncovered 345 erroneous patches in SWE-Bench.", "conclusion": "The improvements impact leaderboard entries and cause ranking changes."}}
{"id": "2506.09332", "pdf": "https://arxiv.org/pdf/2506.09332", "abs": "https://arxiv.org/abs/2506.09332", "authors": ["Zhenqiao Song", "Ramith Hettiarachchi", "Chuan Li", "Jianwen Xie", "Lei Li"], "title": "Natural Language Guided Ligand-Binding Protein Design", "categories": ["cs.LG", "cs.CE", "cs.CL"], "comment": null, "summary": "Can AI protein models follow human language instructions and design proteins\nwith desired functions (e.g. binding to a ligand)? Designing proteins that bind\nto a given ligand is crucial in a wide range of applications in biology and\nchemistry. Most prior AI models are trained on protein-ligand complex data,\nwhich is scarce due to the high cost and time requirements of laboratory\nexperiments. In contrast, there is a substantial body of human-curated text\ndescriptions about protein-ligand interactions and ligand formula. In this\npaper, we propose InstructPro, a family of protein generative models that\nfollow natural language instructions to design ligand-binding proteins. Given a\ntextual description of the desired function and a ligand formula in SMILES,\nInstructPro generates protein sequences that are functionally consistent with\nthe specified instructions. We develop the model architecture, training\nstrategy, and a large-scale dataset, InstructProBench, to support both training\nand evaluation. InstructProBench consists of 9,592,829 triples of (function\ndescription, ligand formula, protein sequence). We train two model variants:\nInstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion\nparameters). Both variants consistently outperform strong baselines, including\nProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking\nsuccess rate (81.52% at moderate confidence) and the lowest average root mean\nsquare deviation (RMSD) compared to ground truth structures (4.026{\\AA}).\nInstructPro-3B further descreases the average RMSD to 2.527{\\AA}, demonstrating\nInstructPro's ability to generate ligand-binding proteins that align with the\nfunctional specifications.", "AI": {"tldr": "This paper introduces InstructPro, a new family of protein generative models that can design ligand-binding proteins based on natural language instructions. The model outperforms previous methods.", "motivation": "Designing proteins that bind to a given ligand is crucial in many biological and chemical applications, but existing AI models are limited by the scarcity of protein-ligand complex data. This paper aims to use human-curated text descriptions about protein-ligand interactions instead.", "method": "Developing InstructPro, a protein generative model that follows natural language instructions to design ligand-binding proteins. The model uses a textual description of the desired function and a ligand formula in SMILES format to generate protein sequences.", "result": "The proposed method outperforms strong baselines, including ProGen2, ESM3, and Pinal. InstructPro-1B has the highest docking success rate and lowest average RMSD, while InstructPro-3B further decreases the average RMSD.", "conclusion": "InstructPro demonstrates the potential of using natural language instructions to design ligand-binding proteins."}}
{"id": "2506.09344", "pdf": "https://arxiv.org/pdf/2506.09344", "abs": "https://arxiv.org/abs/2506.09344", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "18 pages,8 figures", "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "AI": {"tldr": "Ming-Omni is a unified multimodal model that can handle images, text, audio, and video, excelling in speech and image generation.", "motivation": "To create a single model that can process multiple types of data without needing separate models or fine-tuning.", "method": "Uses dedicated encoders for different modalities and processes them using Ling, an MoE architecture with modality-specific routers.", "result": "Supports audio and image generation, allows context-aware chatting, text-to-speech conversion, and versatile image editing.", "conclusion": "Ming-Omni is the first open-source model that matches GPT-4o in modality support and has been released to the public."}}
{"id": "2506.09420", "pdf": "https://arxiv.org/pdf/2506.09420", "abs": "https://arxiv.org/abs/2506.09420", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "comment": null, "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "AI": {"tldr": "This paper argues against fully autonomous AI agents due to their reliability and transparency issues, proposing instead LLM-based Human-Agent Systems that enhance human capabilities through collaboration.", "motivation": "Focuses on the problems with fully autonomous AI agents such as reliability, transparency, and understanding human requirements.", "method": "Proposes the LLM-based Human-Agent Systems (LLM-HAS) where AI works alongside humans.", "result": "Demonstrates through examples in healthcare, finance, and software development that human-AI teamwork can handle complex tasks better than AI alone.", "conclusion": "Argues that AI's progress should be measured by its ability to collaborate with humans rather than its independence."}}
{"id": "2506.09448", "pdf": "https://arxiv.org/pdf/2506.09448", "abs": "https://arxiv.org/abs/2506.09448", "authors": ["Yui Sudo", "Yusuke Fujita", "Atsushi Kojima", "Tomoya Mizumoto", "Lianbo Liu"], "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models\n(OWSM), are trained on massive datasets to achieve accurate automatic speech\nrecognition. However, even SFMs struggle to accurately recognize rare and\nunseen words. While contextual biasing (CB) is a promising approach to improve\nrecognition of such words, most CB methods are trained from scratch, resulting\nin lower performance than SFMs due to the lack of pre-trained knowledge. This\npaper integrates an existing CB method with OWSM v3.1 while freezing its\npre-trained parameters. By leveraging the knowledge embedded in SFMs, the\nproposed method enables effective CB while preserving the advantages of SFMs,\neven with a small dataset. Experimental results show that the proposed method\nimproves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9\npoint improvement in the overall WER while reducing the real-time factor by\n7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean\nset.", "AI": {"tldr": "Integrating contextual biasing with speech foundation models improves recognition of rare and unseen words.", "motivation": "To address the issue of inaccurate recognition of rare and unseen words in speech foundation models.", "method": "Freezing the pre-trained parameters of an existing contextual biasing method and integrating it with Open Whisper-Style Speech Models v3.1.", "result": "The proposed method improves the biasing word error rate by 11.6 points and reduces the overall word error rate by 0.9 point with a 7.5% reduction in real-time factor.", "conclusion": "The integration of contextual biasing with speech foundation models can effectively improve the recognition of rare and unseen words while preserving the advantages of the original model."}}
{"id": "2506.09452", "pdf": "https://arxiv.org/pdf/2506.09452", "abs": "https://arxiv.org/abs/2506.09452", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.IT", "math.IT", "I.2.7; I.2.m"], "comment": "Submitted to IEEE S&P 2026", "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks.", "AI": {"tldr": "This paper introduces the Stained Glass Transform, a method that allows for privacy-preserving word embedding transformations in large language models without losing model utility.", "motivation": "To address privacy concerns related to using large language models with sensitive data by providing theoretical privacy guarantees while maintaining model performance.", "method": "Introducing a learned, stochastic, and sequence-dependent transformation of word embeddings in large language models.", "result": "A posteriori privacy estimates based on mutual information were calculated and verified using token-level privacy metrics and standard LLM performance benchmarks.", "conclusion": "The Stained Glass Transform offers a way to enhance privacy for input data to large language models without compromising their utility."}}
{"id": "2506.09521", "pdf": "https://arxiv.org/pdf/2506.09521", "abs": "https://arxiv.org/abs/2506.09521", "authors": ["\u00dcnal Ege Gaznepoglu", "Anna Leschanowsky", "Ahmad Aloradi", "Prachi Singh", "Daniel Tenbrinck", "Emanu\u00ebl A. P. Habets", "Nils Peters"], "title": "You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks", "categories": ["eess.AS", "cs.CL"], "comment": "5 pages, 6 figures, 1 table, accepted at INTERSPEECH 2025", "summary": "Speaker anonymization systems hide the identity of speakers while preserving\nother information such as linguistic content and emotions. To evaluate their\nprivacy benefits, attacks in the form of automatic speaker verification (ASV)\nsystems are employed. In this study, we assess the impact of intra-speaker\nlinguistic content similarity in the attacker training and evaluation datasets,\nby adapting BERT, a language model, as an ASV system. On the VoicePrivacy\nAttacker Challenge datasets, our method achieves a mean equal error rate (EER)\nof 35%, with certain speakers attaining EERs as low as 2%, based solely on the\ntextual content of their utterances. Our explainability study reveals that the\nsystem decisions are linked to semantically similar keywords within utterances,\nstemming from how LibriSpeech is curated. Our study suggests reworking the\nVoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge\nthe reliance on global EER for privacy evaluations.", "AI": {"tldr": "This study evaluates speaker anonymization systems by adapting a language model as an automatic speaker verification system, revealing issues related to linguistic content similarity.", "motivation": "To assess the privacy benefits of speaker anonymization systems, attacks using automatic speaker verification systems are conducted.", "method": "BERT, a language model, is adapted as an ASV system to evaluate the impact of intra-speaker linguistic content similarity in attacker datasets.", "result": "The method achieved a mean EER of 35% on the VoicePrivacy datasets, with some speakers having EERs as low as 2% based only on textual content.", "conclusion": "The study suggests reworking the VoicePrivacy datasets and challenges the use of global EER for privacy evaluations."}}
{"id": "2506.09522", "pdf": "https://arxiv.org/pdf/2506.09522", "abs": "https://arxiv.org/abs/2506.09522", "authors": ["Beomsik Cho", "Jaehyung Kim"], "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code available at https://github.com/bscho333/ReVisiT", "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross various multimodal tasks by integrating visual perception with language\nunderstanding. However, conventional decoding strategies of LVLMs often fail to\nsuccessfully utilize visual information, leading to visually ungrounded\nresponses. While various approaches have been proposed to address this\nlimitation, they typically require additional training, multi-step inference\nprocedures, or external model dependencies. This paper introduces ReVisiT, a\nsimple yet effective decoding method that references vision tokens to guide the\ntext generation process in LVLMs. Our approach leverages the semantic\ninformation embedded within vision tokens by projecting them into the text\ntoken distribution space, and dynamically selecting the most relevant vision\ntoken at each decoding step through constrained divergence minimization. This\nselected vision token is then used to refine the output distribution to better\nincorporate visual semantics. Experiments on three LVLM hallucination\nbenchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances\nvisual grounding with minimal computational overhead. Moreover, our method\nachieves competitive or superior results relative to state-of-the-art baselines\nwhile reducing computational costs for up to $2\\times$.", "AI": {"tldr": "Introduce ReVisiT, a decoding method for vision-language models that improves visual grounding by referencing vision tokens during text generation.", "motivation": "Conventional decoding strategies of large vision-language models often fail to utilize visual information, leading to visually ungrounded responses.", "method": "ReVisiT leverages semantic information from vision tokens and projects it into the text token distribution space, selecting the most relevant vision token at each decoding step through constrained divergence minimization.", "result": "Experiments show ReVisiT enhances visual grounding with minimal computational overhead and achieves competitive or superior results compared to state-of-the-art baselines while reducing computational costs.", "conclusion": "ReVisiT is an effective and efficient decoding method that improves visual grounding in vision-language models."}}
{"id": "2506.09532", "pdf": "https://arxiv.org/pdf/2506.09532", "abs": "https://arxiv.org/abs/2506.09532", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks.", "AI": {"tldr": "Athena-PRM is a multimodal process reward model that evaluates the reward score for each step in solving complex reasoning problems. It uses prediction consistency between weak and strong completers to identify reliable process labels, demonstrating excellent effectiveness with only 5,000 samples. Two strategies, ORM initialization and up-sampling for negative data, further enhance PRM performance.", "motivation": "Developing high-performance PRMs requires significant time and financial investment due to the need for step-level annotations of reasoning steps, and conventional automated labeling methods often produce noisy labels and high computational costs.", "method": "Athena-PRM leverages prediction consistency between weak and strong completers as a criterion for identifying reliable process labels, and it implements two strategies to improve PRM performance: ORM initialization and up-sampling for negative data.", "result": "Athena-PRM shows outstanding effectiveness across various scenarios and benchmarks, improving performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling, setting SoTA results in VisualProcessBench, and outperforming baselines with a significant margin on five benchmarks.", "conclusion": "Athena-PRM demonstrates its robust capability to accurately assess the correctness of the reasoning step and achieves superior performance across multiple benchmarks and scenarios."}}
{"id": "2506.09600", "pdf": "https://arxiv.org/pdf/2506.09600", "abs": "https://arxiv.org/abs/2506.09600", "authors": ["Itay Nakash", "George Kour", "Koren Lazar", "Matan Vetzler", "Guy Uziel", "Ateret Anaby-Tavor"], "title": "Effective Red-Teaming of Policy-Adherent Agents", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks", "AI": {"tldr": "Develop a new threat model and a multi-agent red-teaming system called CRAFT to test the robustness of policy-adherent agents against adversarial users. Introduce tau-break benchmark to evaluate agent's resilience and examine defense strategies.", "motivation": "Ensure task-oriented LLM-based agents adhere to policies while maintaining helpful interactions, especially in domains with strict policies.", "method": "Propose a novel threat model focusing on adversarial users and develop CRAFT to undermine policy-adherent agents. Introduce tau-break benchmark and evaluate defense strategies.", "result": "CRAFT outperforms conventional jailbreak methods. Defense strategies offer some protection but are insufficient.", "conclusion": "Stronger research-driven safeguards are needed to protect policy-adherent agents from adversarial attacks."}}
{"id": "2506.09659", "pdf": "https://arxiv.org/pdf/2506.09659", "abs": "https://arxiv.org/abs/2506.09659", "authors": ["Eltayeb Ahmed", "Uljad Berdica", "Martha Elliott", "Danijela Horak", "Jakob N. Foerster"], "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.", "AI": {"tldr": "A new method called Intent Factored Generation (IFG) is proposed to improve the diversity of samples from large language models while maintaining coherence and quality.", "motivation": "Current methods for increasing diversity often only operate at the token-level, leading to poor exploration on reasoning problems and unengaging conversational agents.", "method": "Factorising the sampling process into two stages: first, sample a semantically dense intent; second, sample the final response conditioning on both the original prompt and the intent from the first stage. Also, prompting the model to explicitly state its intent before generating each step is beneficial for reasoning tasks.", "result": "The method improves both pass@k and Reinforcement Learning from Verifier Feedback on maths and code tasks. It also increases conversational diversity without sacrificing reward when combined with Direct Preference Optimisation for instruction-tuning. On a general language modelling task, it achieves higher diversity while maintaining the quality of generations.", "conclusion": "This method presents a simple way to increase the sample diversity of LLMs while maintaining performance, and it can be easily integrated into many algorithms for various applications."}}
{"id": "2506.09691", "pdf": "https://arxiv.org/pdf/2506.09691", "abs": "https://arxiv.org/abs/2506.09691", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "title": "Adding simple structure at inference improves Vision-Language Compositionality", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.", "AI": {"tldr": "Propose a simple inference-time technique to improve the performance of dual encoder Vision-Language Models (VLMs) for image-text retrieval tasks.", "motivation": "Existing VLMs struggle with compositionality and show a bag-of-words-like behavior, limiting their retrieval performance. Most research focuses on training approaches, while inference-time techniques receive less attention.", "method": "Divide images into smaller crops, extract text segments, match image crops with text segments using a VLM, and aggregate individual similarities of matches to compute final image-text similarity.", "result": "The approach improves the performance of evaluated VLMs without any training, particularly for attribute-object binding in controlled datasets.", "conclusion": "Inference-time techniques have potential and processing image crops is essential for observed performance gains."}}
{"id": "2506.09707", "pdf": "https://arxiv.org/pdf/2506.09707", "abs": "https://arxiv.org/abs/2506.09707", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "categories": ["eess.AS", "cs.CL", "cs.HC", "68T07", "I.2.7; I.5.4; H.5.2"], "comment": "5 pages, 2 figures", "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance.", "AI": {"tldr": "This paper proposes a method to automatically track therapist fidelity in PE therapy sessions using audio and transcript inputs.", "motivation": "To evaluate therapist fidelity in PE therapy more efficiently than manual review of session recordings.", "method": "Fine-tuning a large pre-trained audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process focused 30-second windows of audio-transcript input.", "result": "The model achieves a mean absolute error (MAE) of 5.3 seconds across tasks on a dataset of 313 real PE sessions.", "conclusion": "This study presents a method for automatically localizing key fidelity elements in PE therapy sessions, achieving a MAE of 5.3 seconds across tasks."}}

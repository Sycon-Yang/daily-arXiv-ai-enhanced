<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling](https://arxiv.org/abs/2510.26912)
*Hyunji Lee,Wenhao Yu,Hongming Zhang,Kaixin Ma,Jiyeon Kim,Dong Yu,Minjoon Seo*

Main category: cs.CL

TL;DR: 本文分析了混合SSM-attention模型的架构设计，发现顺序和并行集成在不同上下文长度下的表现差异，并提出了一种数据增强方法以提高召回率。


<details>
  <summary>Details</summary>
Motivation: 混合模型结合状态空间模型（SSMs）和注意力机制已显示出强大的性能，但其架构设计选择仍不够明确。

Method: 通过内存利用和整体性能的角度分析混合架构，并提出一种补充方法以进一步提高其效果。我们首先检查了顺序和并行集成SSM和注意力层的区别。

Result: 顺序混合模型在较短的上下文中表现更好，而并行混合模型在较长的上下文中更有效。我们还引入了一种数据驱动的方法，持续在包含改写数据集上进行训练，这进一步提高了召回率，同时保持了其他能力。

Conclusion: 我们的研究提供了对混合SSM-attention模型的更深入理解，并为设计针对不同用例的架构提供了实用指导。

Abstract: Hybrid models that combine state space models (SSMs) with attention
mechanisms have shown strong performance by leveraging the efficiency of SSMs
and the high recall ability of attention. However, the architectural design
choices behind these hybrid models remain insufficiently understood. In this
work, we analyze hybrid architectures through the lens of memory utilization
and overall performance, and propose a complementary method to further enhance
their effectiveness. We first examine the distinction between sequential and
parallel integration of SSM and attention layers. Our analysis reveals several
interesting findings, including that sequential hybrids perform better on
shorter contexts, whereas parallel hybrids are more effective for longer
contexts. We also introduce a data-centric approach of continually training on
datasets augmented with paraphrases, which further enhances recall while
preserving other capabilities. It generalizes well across different base models
and outperforms architectural modifications aimed at enhancing recall. Our
findings provide a deeper understanding of hybrid SSM-attention models and
offer practical guidance for designing architectures tailored to various use
cases. Our findings provide a deeper understanding of hybrid SSM-attention
models and offer practical guidance for designing architectures tailored to
various use cases.

</details>


### [2] [Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence](https://arxiv.org/abs/2510.26969)
*Lívia Dutra,Arthur Lorenzi,Laís Berno,Franciany Campos,Karoline Biscardi,Kenneth Brown,Marcelo Viridiano,Frederico Belcavello,Ely Matos,Olívia Guaranha,Erik Santos,Sofia Reinach,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 本文介绍了一种用于医疗领域可报告事件识别的方法，利用语义框架在非结构化数据中搜索细粒度模式。该方法应用于解决电子病历中性别暴力报告不足的问题，在2100万句巴西葡萄牙语语料库上测试，结果表明其精度为0.726，具有良好的适用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决电子病历中性别暴力（GBV）报告不足的问题。

Method: 该方法利用语义框架来定义细粒度模式，并在非结构化数据（即电子病历中的开放文本字段）中搜索这些模式。

Result: 该方法有效地识别了暴力报告，精度为0.726，证实了其稳健性。

Conclusion: 该方法作为一种透明、高效、低碳且与语言无关的管道，可以轻松适应其他健康监测环境，有助于在公共卫生系统中更广泛地实现伦理和可解释的NLP使用。

Abstract: We introduce a methodology for the identification of notifiable events in the
domain of healthcare. The methodology harnesses semantic frames to define
fine-grained patterns and search them in unstructured data, namely, open-text
fields in e-medical records. We apply the methodology to the problem of
underreporting of gender-based violence (GBV) in e-medical records produced
during patients' visits to primary care units. A total of eight patterns are
defined and searched on a corpus of 21 million sentences in Brazilian
Portuguese extracted from e-SUS APS. The results are manually evaluated by
linguists and the precision of each pattern measured. Our findings reveal that
the methodology effectively identifies reports of violence with a precision of
0.726, confirming its robustness. Designed as a transparent, efficient,
low-carbon, and language-agnostic pipeline, the approach can be easily adapted
to other health surveillance contexts, contributing to the broader, ethical,
and explainable use of NLP in public health systems.

</details>


### [3] [Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations](https://arxiv.org/abs/2510.26974)
*Jean-Philippe Corbeil,Asma Ben Abacha,Jerome Tremblay,Phillip Swazinna,Akila Jeeson Daniel,Miguel Del-Agua,Francois Beaulieu*

Main category: cs.CL

TL;DR: 本文介绍了MEDIQA-OE 2025共享任务，该任务是首个从医生-患者对话中提取医疗指令的挑战。


<details>
  <summary>Details</summary>
Motivation: 临床文档越来越多地使用自动语音识别和摘要，但将对话转换为电子健康记录中的可操作医疗指令仍未被探索。

Method: 本文描述了MEDIQA-OE任务、数据集、最终排行榜以及参与者的解决方案。

Result: 六支团队参加了共享任务，并尝试了广泛的方法，包括封闭和开放权重的大语言模型（LLMs）。

Conclusion: 本文介绍了MEDIQA-OE 2025共享任务，该任务是首个从医生-患者对话中提取医疗指令的挑战。

Abstract: Clinical documentation increasingly uses automatic speech recognition and
summarization, yet converting conversations into actionable medical orders for
Electronic Health Records remains unexplored. A solution to this problem can
significantly reduce the documentation burden of clinicians and directly impact
downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first
challenge on extracting medical orders from doctor-patient conversations. Six
teams participated in the shared task and experimented with a broad range of
approaches, and both closed- and open-weight large language models (LLMs). In
this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking,
and participants' solutions.

</details>


### [4] [Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services](https://arxiv.org/abs/2510.27016)
*Jayden Serenari,Stephen Lee*

Main category: cs.CL

TL;DR: LOPSIDED框架是一种语义感知的隐私代理，用于在使用远程LLMs时保护敏感PII数据，通过动态替换和去假名化来保持对话的上下文完整性。


<details>
  <summary>Details</summary>
Motivation: 随着对话AI系统的使用增加，用户与大型语言模型（LLMs）交互时可能会泄露敏感个人数据，因此需要一种能够保护隐私的方法。

Method: LOPSIDED框架通过动态替换用户提示中的敏感PII实体为语义一致的假名，并在模型生成响应后自动去假名化，从而保护敏感数据。

Result: LOPSIDED框架在真实对话数据集上评估，结果显示其比基线技术减少了5倍的语义效用错误，同时增强了隐私保护。

Conclusion: LOPSIDED框架在保护隐私的同时显著减少了语义效用错误，相比基线技术提高了5倍。

Abstract: With the increasing use of conversational AI systems, there is growing
concern over privacy leaks, especially when users share sensitive personal data
in interactions with Large Language Models (LLMs). Conversations shared with
these models may contain Personally Identifiable Information (PII), which, if
exposed, could lead to security breaches or identity theft. To address this
challenge, we present the Local Optimizations for Pseudonymization with
Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a
semantically-aware privacy agent designed to safeguard sensitive PII data when
using remote LLMs. Unlike prior work that often degrade response quality, our
approach dynamically replaces sensitive PII entities in user prompts with
semantically consistent pseudonyms, preserving the contextual integrity of
conversations. Once the model generates its response, the pseudonyms are
automatically depseudonymized, ensuring the user receives an accurate,
privacy-preserving output. We evaluate our approach using real-world
conversations sourced from ShareGPT, which we further augment and annotate to
assess whether named entities are contextually relevant to the model's
response. Our results show that LOPSIDED reduces semantic utility errors by a
factor of 5 compared to baseline techniques, all while enhancing privacy.

</details>


### [5] [Kad: A Framework for Proxy-based Test-time Alignment with Knapsack Approximation Deferral](https://arxiv.org/abs/2510.27017)
*Ayoub Hammal,Pierre Zweigenbaum,Caio Corro*

Main category: cs.CL

TL;DR: 本文提出了一种基于代理的测试时对齐方法，以降低大型语言模型对齐过程中的计算成本。我们的方法在任务性能和推测解码速度方面都展示了其优势。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的规模不断扩大，对齐过程的计算成本变得难以承受。因此，我们需要一种新的方法来减少这些成本。

Method: 我们提出了一种基于代理的测试时对齐方法，通过使用一个小的对齐模型的指导来规避对齐过程中的计算成本。我们的方法可以描述为一种特定于标记的级联方法，其中特定于标记的延迟规则被简化为0-1背包问题。

Result: 我们实验表明，我们的方法在任务性能和推测解码速度方面都有所提升。

Conclusion: 我们的方法在任务性能和推测解码速度方面都展示了其优势。

Abstract: Several previous works concluded that the largest part of generation
capabilities of large language models (LLM) are learned (early) during
pre-training. However, LLMs still require further alignment to adhere to
downstream task requirements and stylistic preferences, among other desired
properties. As LLMs continue to scale in terms of size, the computational cost
of alignment procedures increase prohibitively. In this work, we propose a
novel approach to circumvent these costs via proxy-based test-time alignment,
i.e. using guidance from a small aligned model. Our approach can be described
as token-specific cascading method, where the token-specific deferral rule is
reduced to 0-1 knapsack problem. In this setting, we derive primal and dual
approximations of the optimal deferral decision. We experimentally show the
benefits of our method both in task performance and speculative decoding speed.

</details>


### [6] [Elastic Architecture Search for Efficient Language Models](https://arxiv.org/abs/2510.27037)
*Shang Wang*

Main category: cs.CL

TL;DR: ELM是一种优化的神经架构搜索方法，通过引入灵活的搜索空间和动态模块，提高了搜索过程的效率和灵活性，并通过新的知识蒸馏损失改善了架构选择的区分度，从而在任务中取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型在自然语言理解任务中变得越来越关键，但它们的计算和内存需求引发了经济和环境问题。

Method: ELM是一种优化的神经架构搜索（NAS）方法，引入了灵活的搜索空间、高效的Transformer块和动态模块以调整维度和头数，并引入了新的知识蒸馏损失以提高架构选择的区分度。

Result: ELM通过引入灵活的搜索空间和动态模块，提高了搜索过程的效率和灵活性，并通过新的知识蒸馏损失改善了架构选择的区分度，从而在任务中取得了更好的结果。

Conclusion: ELM发现的模型在掩码语言建模和因果语言建模任务中显著优于现有方法。

Abstract: As large pre-trained language models become increasingly critical to natural
language understanding (NLU) tasks, their substantial computational and memory
requirements have raised significant economic and environmental concerns.
Addressing these challenges, this paper introduces the Elastic Language Model
(ELM), a novel neural architecture search (NAS) method optimized for compact
language models. ELM extends existing NAS approaches by introducing a flexible
search space with efficient transformer blocks and dynamic modules for
dimension and head number adjustment. These innovations enhance the efficiency
and flexibility of the search process, which facilitates more thorough and
effective exploration of model architectures. We also introduce novel knowledge
distillation losses that preserve the unique characteristics of each block, in
order to improve the discrimination between architectural choices during the
search process. Experiments on masked language modeling and causal language
modeling tasks demonstrate that models discovered by ELM significantly
outperform existing methods.

</details>


### [7] [Dataset Creation and Baseline Models for Sexism Detection in Hausa](https://arxiv.org/abs/2510.27038)
*Fatima Adam Muhammad,Shamsuddeen Muhammad Hassan,Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 本研究介绍了第一个Hausa性别歧视检测数据集，通过社区参与、定性编码和数据增强开发。进行了两阶段用户研究，涉及母语者以探索性别歧视在日常话语中的定义和表达。还尝试了传统机器学习分类器和预训练多语言语言模型，并评估了少样本学习在检测Hausa性别歧视中的有效性。研究结果强调了在捕捉文化细微差别方面的挑战，特别是在处理需要澄清和习语表达时，且在这些情况下存在许多误报。


<details>
  <summary>Details</summary>
Motivation: 在线平台使各种形式的性别歧视得以滋生，因此需要有效的性别歧视检测和缓解策略。然而，在低资源语言中，由于语言资源有限和文化差异，性别歧视的表达和感知方式不同，因此进展有限。

Method: 通过社区参与、定性编码和数据增强开发了第一个Hausa性别歧视检测数据集。进行了两阶段用户研究（n=66），涉及母语者以探索性别歧视在日常话语中是如何定义和表达的。还尝试了传统机器学习分类器和预训练多语言语言模型，并评估了少样本学习在检测Hausa性别歧视中的有效性。

Result: 研究发现，在捕捉文化细微差别方面存在挑战，特别是在处理需要澄清和习语表达时，且在这些情况下存在许多误报。

Conclusion: 研究结果强调了在捕捉文化细微差别方面的挑战，特别是对于需要澄清的和习语表达，同时揭示了在这种情况下许多误报的趋势。

Abstract: Sexism reinforces gender inequality and social exclusion by perpetuating
stereotypes, bias, and discriminatory norms. Noting how online platforms enable
various forms of sexism to thrive, there is a growing need for effective sexism
detection and mitigation strategies. While computational approaches to sexism
detection are widespread in high-resource languages, progress remains limited
in low-resource languages where limited linguistic resources and cultural
differences affect how sexism is expressed and perceived. This study introduces
the first Hausa sexism detection dataset, developed through community
engagement, qualitative coding, and data augmentation. For cultural nuances and
linguistic representation, we conducted a two-stage user study (n=66) involving
native speakers to explore how sexism is defined and articulated in everyday
discourse. We further experiment with both traditional machine learning
classifiers and pre-trained multilingual language models and evaluating the
effectiveness few-shot learning in detecting sexism in Hausa. Our findings
highlight challenges in capturing cultural nuance, particularly with
clarification-seeking and idiomatic expressions, and reveal a tendency for many
false positives in such cases.

</details>


### [8] [Quantitative Intertextuality from the Digital Humanities Perspective: A Survey](https://arxiv.org/abs/2510.27045)
*Siyu Duan*

Main category: cs.CL

TL;DR: 本文提供了一个定量互文性研究的路线图，涵盖了数据、方法和应用，并讨论了其在人文学科和社会科学中的应用以及相关平台工具。


<details>
  <summary>Details</summary>
Motivation: 过去十年自然语言处理的进步使互文性研究进入了量化时代。基于最新方法的大规模互文性研究不断涌现。本文旨在提供一个定量互文性研究的路线图。

Method: 本文综述了从统计学到深度学习的方法，并总结了它们在人文学科和社会科学研究中的应用以及相关的平台工具。

Result: 本文总结了互文性研究的数据、方法和应用，并讨论了其在人文学科和社会科学中的应用以及相关平台工具。

Conclusion: 本文提供了定量互文性研究的路线图，总结了其数据、方法和应用。随着计算机技术的进步，可以预期更精确、多样和大规模的互文研究。互文性在跨越人工智能和人文学科的跨学科研究中具有广阔的应用前景。

Abstract: The connection between texts is referred to as intertextuality in literary
theory, which served as an important theoretical basis in many digital
humanities studies. Over the past decade, advancements in natural language
processing have ushered intertextuality studies into the quantitative age.
Large-scale intertextuality research based on cutting-edge methods has
continuously emerged. This paper provides a roadmap for quantitative
intertextuality studies, summarizing their data, methods, and applications.
Drawing on data from multiple languages and topics, this survey reviews methods
from statistics to deep learning. It also summarizes their applications in
humanities and social sciences research and the associated platform tools.
Driven by advances in computer technology, more precise, diverse, and
large-scale intertext studies can be anticipated. Intertextuality holds promise
for broader application in interdisciplinary research bridging AI and the
humanities.

</details>


### [9] [Recursive numeral systems are highly regular and easy to process](https://arxiv.org/abs/2510.27049)
*Ponrawee Prasertsom,Andrea Silvi,Jennifer Culbertson,Moa Johansson,Devdatt Dubhashi,Kenny Smith*

Main category: cs.CL

TL;DR: 本文提出，递归数字符号系统应被视为在规律性和处理复杂性方面更有效，并强调在研究语言最优性时需要考虑规律性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究未能证明只有类似自然语言的系统才能优化这一权衡，且依赖于人为约束来排除非自然系统。本文旨在解决这一问题。

Method: 本文基于最小描述长度（MDL）方法，分析了递归数字符号系统的规律性和处理复杂性。

Result: 本文的结果表明，基于MDL的规律性和处理复杂性度量能更好地捕捉已知自然系统与未被观察到但可能的系统之间的关键差异。

Conclusion: 本文指出，递归数字符号系统应被看作在规律性和处理复杂性方面更有效，并强调在研究语言最优性时需要考虑规律性。

Abstract: Previous work has argued that recursive numeral systems optimise the
trade-off between lexicon size and average morphosyntatic complexity (Deni\'c
and Szymanik, 2024). However, showing that only natural-language-like systems
optimise this tradeoff has proven elusive, and the existing solution has relied
on ad-hoc constraints to rule out unnatural systems (Yang and Regier, 2025).
Here, we argue that this issue arises because the proposed trade-off has
neglected regularity, a crucial aspect of complexity central to human grammars
in general. Drawing on the Minimum Description Length (MDL) approach, we
propose that recursive numeral systems are better viewed as efficient with
regard to their regularity and processing complexity. We show that our
MDL-based measures of regularity and processing complexity better capture the
key differences between attested, natural systems and unattested but possible
ones, including "optimal" recursive numeral systems from previous work, and
that the ad-hoc constraints from previous literature naturally follow from
regularity. Our approach highlights the need to incorporate regularity across
sets of forms in studies that attempt to measure and explain optimality in
language.

</details>


### [10] [VISTA Score: Verification In Sequential Turn-based Assessment](https://arxiv.org/abs/2510.27052)
*Ashley Lewis,Andrew Perrault,Eric Fosler-Lussier,Michael White*

Main category: cs.CL

TL;DR: VISTA is a framework for evaluating conversational factuality by verifying claims and tracking consistency, which improves hallucination detection and provides a more transparent measure of truthfulness in dialogue systems.


<details>
  <summary>Details</summary>
Motivation: Hallucination remains a major obstacle to deploying conversational AI systems in settings that demand factual reliability. Existing metrics either evaluate isolated responses or treat unverifiable content as errors, limiting their use for multi-turn dialogue.

Method: VISTA is a framework for evaluating conversational factuality through claim-level verification and sequential consistency tracking. It decomposes each assistant turn into atomic factual claims, verifies them against trusted sources and dialogue history, and categorizes unverifiable statements.

Result: VISTA substantially improves hallucination detection over FACTSCORE and LLM-as-Judge baselines across eight large language models and four dialogue factuality benchmarks. Human evaluation confirms that VISTA's decomposition improves annotator agreement and reveals inconsistencies in existing benchmarks.

Conclusion: VISTA offers a more transparent, human-aligned measure of truthfulness in dialogue systems by modeling factuality as a dynamic property of conversation.

Abstract: Hallucination--defined here as generating statements unsupported or
contradicted by available evidence or conversational context--remains a major
obstacle to deploying conversational AI systems in settings that demand factual
reliability. Existing metrics either evaluate isolated responses or treat
unverifiable content as errors, limiting their use for multi-turn dialogue. We
introduce VISTA (Verification In Sequential Turn-based Assessment), a framework
for evaluating conversational factuality through claim-level verification and
sequential consistency tracking. VISTA decomposes each assistant turn into
atomic factual claims, verifies them against trusted sources and dialogue
history, and categorizes unverifiable statements (subjective, contradicted,
lacking evidence, or abstaining). Across eight large language models and four
dialogue factuality benchmarks (AIS, BEGIN, FAITHDIAL, and FADE), VISTA
substantially improves hallucination detection over FACTSCORE and LLM-as-Judge
baselines. Human evaluation confirms that VISTA's decomposition improves
annotator agreement and reveals inconsistencies in existing benchmarks. By
modeling factuality as a dynamic property of conversation, VISTA offers a more
transparent, human-aligned measure of truthfulness in dialogue systems.

</details>


### [11] [LLM-Centric RAG with Multi-Granular Indexing and Confidence Constraints](https://arxiv.org/abs/2510.27054)
*Xiaofan Guo,Yaxuan Luan,Yue Kang,Xiangchen Song,Jinxu Guo*

Main category: cs.CL

TL;DR: 本文提出了一种基于多粒度记忆索引和不确定性估计的置信度控制方法，以解决检索增强生成中的覆盖不足、结果不稳定和可靠性有限的问题，并在多个指标上取得了优于现有模型的效果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决检索增强生成在复杂知识环境中存在的覆盖不足、结果不稳定和可靠性有限的问题。

Method: 本文提出了一种结合多粒度记忆索引和不确定性估计的置信度控制方法。该方法构建了一个分层的记忆结构，将知识表示分为不同粒度层次，实现从局部细节到全局语境的动态索引和检索，并引入不确定性估计机制来显式约束和过滤生成过程中的低置信度路径。

Result: 实验结果表明，该方法在问答准确率、检索召回率、排序质量和事实一致性方面优于现有模型，证明了多粒度索引与置信度控制相结合的有效性。

Conclusion: 本研究不仅为检索增强生成提供了新的技术路径，还为提高大模型在复杂环境中的可靠性和可控性提供了实践证据。

Abstract: This paper addresses the issues of insufficient coverage, unstable results,
and limited reliability in retrieval-augmented generation under complex
knowledge environments, and proposes a confidence control method that
integrates multi-granularity memory indexing with uncertainty estimation. The
method builds a hierarchical memory structure that divides knowledge
representations into different levels of granularity, enabling dynamic indexing
and retrieval from local details to global context, and thus establishing
closer semantic connections between retrieval and generation. On this basis, an
uncertainty estimation mechanism is introduced to explicitly constrain and
filter low-confidence paths during the generation process, allowing the model
to maintain information coverage while effectively suppressing noise and false
content. The overall optimization objective consists of generation loss,
entropy constraints, and variance regularization, forming a unified confidence
control framework. In the experiments, comprehensive sensitivity tests and
comparative analyses were designed, covering hyperparameters, environmental
conditions, and data structures, to verify the stability and robustness of the
proposed method across different scenarios. The results show that the method
achieves superior performance over existing models in QA accuracy, retrieval
recall, ranking quality, and factual consistency, demonstrating the
effectiveness of combining multi-granularity indexing with confidence control.
This study not only provides a new technical pathway for retrieval-augmented
generation but also offers practical evidence for improving the reliability and
controllability of large models in complex contexts.

</details>


### [12] [Detecting Data Contamination in LLMs via In-Context Learning](https://arxiv.org/abs/2510.27055)
*Michał Zawalski,Meriem Boubdir,Klaudia Bałazy,Besmira Nushi,Pablo Ribalta*

Main category: cs.CL

TL;DR: CoDeC是一种用于检测和量化大型语言模型中训练数据污染的方法，它通过测量上下文学习对模型性能的影响来区分记忆的数据和未训练的数据。


<details>
  <summary>Details</summary>
Motivation: 为了检测和量化大型语言模型中的训练数据污染，需要一种实用且准确的方法。

Method: CoDeC通过测量上下文学习对模型性能的影响，区分训练期间记忆的数据和超出训练分布的数据。

Result: 实验表明，CoDeC产生的可解释污染分数可以清晰地区分已见和未见数据集，并揭示了开放权重模型中存在记忆的强烈证据。

Conclusion: CoDeC是一种简单、自动化且与模型和数据集无关的方法，易于集成到基准评估中。

Abstract: We present Contamination Detection via Context (CoDeC), a practical and
accurate method to detect and quantify training data contamination in large
language models. CoDeC distinguishes between data memorized during training and
data outside the training distribution by measuring how in-context learning
affects model performance. We find that in-context examples typically boost
confidence for unseen datasets but may reduce it when the dataset was part of
training, due to disrupted memorization patterns. Experiments show that CoDeC
produces interpretable contamination scores that clearly separate seen and
unseen datasets, and reveals strong evidence of memorization in open-weight
models with undisclosed training corpora. The method is simple, automated, and
both model- and dataset-agnostic, making it easy to integrate with benchmark
evaluations.

</details>


### [13] [Contrastive Knowledge Transfer and Robust Optimization for Secure Alignment of Large Language Models](https://arxiv.org/abs/2510.27077)
*Jiasen Zheng,Huajun Zhang,Xu Yan,Ran Hao,Chong Peng*

Main category: cs.CL

TL;DR: 本文提出了一种结合对比蒸馏和噪声鲁棒训练的微调方法，以提升大规模语言模型的安全性和鲁棒性，并在多个关键指标上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有大规模语言模型在安全对齐和鲁棒性方面存在局限性，需要一种更有效的微调方法来提升模型的安全性和稳定性。

Method: 本文提出了一种结合对比蒸馏和噪声鲁棒训练的微调方法。该方法通过蒸馏将教师模型的知识边界转移到学生模型，同时在训练中引入噪声扰动和鲁棒优化约束，以确保模型在噪声和不确定输入下保持稳定的预测输出。

Result: 实验结果表明，该方法在知识迁移、鲁棒性和整体安全性方面显著优于现有基线，在多个关键指标上取得了最佳性能。

Conclusion: 本研究不仅丰富了参数高效微调的理论体系，还为构建更安全、更可信的对齐机制提供了新的解决方案。

Abstract: This paper addresses the limitations of large-scale language models in safety
alignment and robustness by proposing a fine-tuning method that combines
contrastive distillation with noise-robust training. The method freezes the
backbone model and transfers the knowledge boundaries of the teacher model to
the student model through distillation, thereby improving semantic consistency
and alignment accuracy. At the same time, noise perturbations and robust
optimization constraints are introduced during training to ensure that the
model maintains stable predictive outputs under noisy and uncertain inputs. The
overall framework consists of distillation loss, robustness loss, and a
regularization term, forming a unified optimization objective that balances
alignment ability with resistance to interference. To systematically validate
its effectiveness, the study designs experiments from multiple perspectives,
including distillation weight sensitivity, stability analysis under computation
budgets and mixed-precision environments, and the impact of data noise and
distribution shifts on model performance. Results show that the method
significantly outperforms existing baselines in knowledge transfer, robustness,
and overall safety, achieving the best performance across several key metrics.
This work not only enriches the theoretical system of parameter-efficient
fine-tuning but also provides a new solution for building safer and more
trustworthy alignment mechanisms.

</details>


### [14] [Characterizing Selective Refusal Bias in Large Language Models](https://arxiv.org/abs/2510.27087)
*Adel Khorramrouz,Sharon Levy*

Main category: cs.CL

TL;DR: 研究发现大型语言模型的安全防护措施可能对某些人口群体产生选择性拒绝偏差，并强调需要更公平和稳健的安全防护。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示大型语言模型（LLMs）安全防护措施可能引入的新偏见，并探索其潜在的安全影响。

Method: 通过分析拒绝率、LLM响应类型和生成拒绝的长度，探讨了LLM安全防护中的选择性拒绝偏差。

Result: 研究发现，在性别、性取向、国籍和宗教属性方面存在选择性拒绝偏差。

Conclusion: 研究强调了需要在不同人口群体中实现更公平和稳健的安全防护措施。

Abstract: Safety guardrails in large language models(LLMs) are developed to prevent
malicious users from generating toxic content at a large scale. However, these
measures can inadvertently introduce or reflect new biases, as LLMs may refuse
to generate harmful content targeting some demographic groups and not others.
We explore this selective refusal bias in LLM guardrails through the lens of
refusal rates of targeted individual and intersectional demographic groups,
types of LLM responses, and length of generated refusals. Our results show
evidence of selective refusal bias across gender, sexual orientation,
nationality, and religion attributes. This leads us to investigate additional
safety implications via an indirect attack, where we target previously refused
groups. Our findings emphasize the need for more equitable and robust
performance in safety guardrails across demographic groups.

</details>


### [15] [Rating Roulette: Self-Inconsistency in LLM-As-A-Judge Frameworks](https://arxiv.org/abs/2510.27106)
*Rajarshi Haldar,Julia Hockenmaier*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）作为自然语言生成（NLG）评估工具的一致性问题，并探讨了通过适当指导方针是否仍可利用LLM评委。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言生成（NLG）的广泛应用，正确评估其质量变得越来越困难。使用大型语言模型（LLMs）进行评估已成为一种趋势，因为它们比传统的n-gram或基于嵌入的指标更接近人类偏好。然而，LLM评委的评分一致性问题需要解决。

Method: 我们通过实验评估了LLM评委在不同运行中的评分一致性，并量化了这种不一致性在不同NLG任务和基准上的表现。

Result: 我们的实验表明，LLM评委在不同运行中的评分存在低内部一致性，这使得他们的评分不一致，甚至在最坏的情况下几乎是任意的。

Conclusion: 我们的实验表明，LLM评委在不同运行中的评分存在低内部一致性，这使得他们的评分不一致，甚至在最坏的情况下几乎是任意的，从而难以衡量他们的判断实际上有多好。然而，通过适当的指导方针，LLM评委的使用仍可能有用。

Abstract: As Natural Language Generation (NLG) continues to be widely adopted, properly
assessing it has become quite difficult. Lately, using large language models
(LLMs) for evaluating these generations has gained traction, as they tend to
align more closely with human preferences than conventional n-gram or
embedding-based metrics. In our experiments, we show that LLM judges have low
intra-rater reliability in their assigned scores across different runs. This
variance makes their ratings inconsistent, almost arbitrary in the worst case,
making it difficult to measure how good their judgments actually are. We
quantify this inconsistency across different NLG tasks and benchmarks and see
if judicious use of LLM judges can still be useful following proper guidelines.

</details>


### [16] [Probability Distributions Computed by Hard-Attention Transformers](https://arxiv.org/abs/2510.27118)
*Andy Yang,Anej Svete,Jiaoda Li,Anthony Widjaja Lin,Jonathan Rawski,Ryan Cotterell,David Chiang*

Main category: cs.CL

TL;DR: 本文研究了变压器语言模型的表达能力，发现将其变为自回归可以增加表达能力，而使其概率化可能会破坏非概率情况下的等价关系。


<details>
  <summary>Details</summary>
Motivation: 大多数关于变压器的表达结果将其视为语言识别器，而不是实际使用的语言模型。因此，需要研究变压器作为语言模型时的表达能力。

Method: 本文通过分析变压器语言模型可以表达的概率分布来研究其表达能力。

Result: 研究表明，将变压器语言识别器变为自回归可以有时增加其表达能力，而使其概率化可能会破坏非概率情况下的等价关系。

Conclusion: 本文的主要贡献是区分变压器在作为语言模型的常见使用情况下能够表达的功能。

Abstract: Most expressivity results for transformers treat them as language recognizers
(which accept or reject strings), and not as they are used in practice, as
language models (which generate strings autoregressively and
probabilistically). Here, we characterize the probability distributions that
transformer language models can express. We show that making transformer
language recognizers autoregressive can sometimes increase their expressivity,
and that making them probabilistic can break equivalences that hold in the
non-probabilistic case. Our overall contribution is to tease apart what
functions transformers are capable of expressing, in their most common use-case
as language models.

</details>


### [17] [Simple Additions, Substantial Gains: Expanding Scripts, Languages, and Lineage Coverage in URIEL+](https://arxiv.org/abs/2510.27183)
*Mason Shipton,York Hay Ng,Aditya Khan,Phuong Hanh Hoang,Xiang Lu,A. Seza Doğruöz,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本文扩展了URIEL+，以解决数据稀疏性问题，提高其在多语言研究中的适用性和包容性。


<details>
  <summary>Details</summary>
Motivation: URIEL+语言知识库在跨语言迁移中受到数据稀疏性的限制，特别是在支持低资源语言时。

Method: 本文扩展了URIEL+，引入了脚本向量来表示7,488种语言的书写系统属性，集成了Glottolog以添加18,710种额外语言，并通过在谱系中传播类型学和脚本特征来扩展谱系推断。

Result: 这些新增内容减少了脚本向量的特征稀疏性14%，将语言覆盖率增加了最多19,015种语言（1,007%），并提高了推断质量指标最多33%。在面向低资源语言的跨语言迁移任务基准测试中，性能有时与URIEL+不同，某些设置下的性能提升了6%。

Conclusion: 我们的研究使URIEL+更加完整和包容，适用于多语言研究。

Abstract: The URIEL+ linguistic knowledge base supports multilingual research by
encoding languages through geographic, genetic, and typological vectors.
However, data sparsity remains prevalent, in the form of missing feature types,
incomplete language entries, and limited genealogical coverage. This limits the
usefulness of URIEL+ in cross-lingual transfer, particularly for supporting
low-resource languages. To address this sparsity, this paper extends URIEL+
with three contributions: introducing script vectors to represent writing
system properties for 7,488 languages, integrating Glottolog to add 18,710
additional languages, and expanding lineage imputation for 26,449 languages by
propagating typological and script features across genealogies. These additions
reduce feature sparsity by 14% for script vectors, increase language coverage
by up to 19,015 languages (1,007%), and improve imputation quality metrics by
up to 33%. Our benchmark on cross-lingual transfer tasks (oriented around
low-resource languages) shows occasionally divergent performance compared to
URIEL+, with performance gains up to 6% in certain setups. Our advances make
URIEL+ more complete and inclusive for multilingual research.

</details>


### [18] [MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models](https://arxiv.org/abs/2510.27196)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Yayue Deng,Jing Ma*

Main category: cs.CL

TL;DR: 本文提出MemeArena，一个基于代理的竞技场式评估框架，用于评估mLLMs对多模态有害性的理解，能够减少评估偏差并接近人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注mLLMs在二分类任务中的检测准确性，这往往无法反映有害性在不同上下文中的深入解释细微差别。

Method: 我们提出了MemeArena，一个基于代理的竞技场式评估框架，提供上下文感知且无偏的评估，以评估mLLMs对多模态有害性的理解。

Result: 广泛的实验表明，我们的框架有效减少了评估代理的评估偏差，判断结果与人类偏好密切相关。

Conclusion: 我们的框架有效减少了评估代理的评估偏差，判断结果与人类偏好密切相关，为多模态有害性理解中的可靠和全面的mLLM评估提供了有价值的见解。

Abstract: The proliferation of memes on social media necessitates the capabilities of
multimodal Large Language Models (mLLMs) to effectively understand multimodal
harmfulness. Existing evaluation approaches predominantly focus on mLLMs'
detection accuracy for binary classification tasks, which often fail to reflect
the in-depth interpretive nuance of harmfulness across diverse contexts. In
this paper, we propose MemeArena, an agent-based arena-style evaluation
framework that provides a context-aware and unbiased assessment for mLLMs'
understanding of multimodal harmfulness. Specifically, MemeArena simulates
diverse interpretive contexts to formulate evaluation tasks that elicit
perspective-specific analyses from mLLMs. By integrating varied viewpoints and
reaching consensus among evaluators, it enables fair and unbiased comparisons
of mLLMs' abilities to interpret multimodal harmfulness. Extensive experiments
demonstrate that our framework effectively reduces the evaluation biases of
judge agents, with judgment results closely aligning with human preferences,
offering valuable insights into reliable and comprehensive mLLM evaluations in
multimodal harmfulness understanding. Our code and data are publicly available
at https://github.com/Lbotirx/MemeArena.

</details>


### [19] [Identifying the Periodicity of Information in Natural Language](https://arxiv.org/abs/2510.27241)
*Yulin Ou,Yu Wang,Yang Xu,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本文提出了一种新的周期性检测方法APS，用于分析自然语言中的信息周期性，并发现人类语言中存在显著的周期性模式，这可能是由结构因素和其他远程驱动因素共同作用的结果。


<details>
  <summary>Details</summary>
Motivation: 最近自然语言中信息密度的理论进展提出了一个问题：自然语言在其编码信息中表现出怎样的周期性模式？

Method: 引入了一种称为Surprisal自动周期（APS）的新方法，该方法采用标准的周期性检测算法，能够识别单个文档的surprisal序列中的任何显著周期。

Result: 首先，相当一部分人类语言显示出信息中的强周期性模式；其次，发现了超出文本典型结构单元（例如句子边界、基本话语单元等）分布的新周期，并通过谐波回归建模进一步确认。

Conclusion: 语言中的信息周期性是结构因素和其他在更长距离上起作用的驱动因素共同作用的结果。我们的周期性检测方法的优势及其在LLM生成检测中的潜力进一步得到了讨论。

Abstract: Recent theoretical advancement of information density in natural language has
brought the following question on desk: To what degree does natural language
exhibit periodicity pattern in its encoded information? We address this
question by introducing a new method called AutoPeriod of Surprisal (APS). APS
adopts a canonical periodicity detection algorithm and is able to identify any
significant periods that exist in the surprisal sequence of a single document.
By applying the algorithm to a set of corpora, we have obtained the following
interesting results: Firstly, a considerable proportion of human language
demonstrates a strong pattern of periodicity in information; Secondly, new
periods that are outside the distributions of typical structural units in text
(e.g., sentence boundaries, elementary discourse units, etc.) are found and
further confirmed via harmonic regression modeling. We conclude that the
periodicity of information in language is a joint outcome from both structured
factors and other driving factors that take effect at longer distances. The
advantages of our periodicity detection method and its potentials in
LLM-generation detection are further discussed.

</details>


### [20] [Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs](https://arxiv.org/abs/2510.27246)
*Mohammad Tavakoli,Alireza Salemi,Carrie Ye,Mohamed Abdalla,Hamed Zamani,J Ross Mitchell*

Main category: cs.CL

TL;DR: 本文旨在解决现有基准测试在评估大型语言模型长上下文推理能力方面的不足，提出了一种新的框架LIGHT，通过引入三种互补的记忆系统，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试在叙事连贯性、主题多样性以及测试复杂回忆任务方面存在不足，无法有效评估大型语言模型在需要长期记忆的任务中的表现。

Method: 本文提出了一个自动生成长而连贯的对话的框架，并构建了BEAM基准测试。同时，设计了LIGHT框架，包含长期情景记忆、短期工作记忆和用于积累关键事实的草稿区。

Result: 实验结果显示，即使具有1M token上下文窗口的大型语言模型（包括有无检索增强的模型）在对话变长时表现不佳。而LIGHT框架在各种模型上 consistently 提升了性能，平均提高了3.5%-12.69%。

Conclusion: 本文提出了一种新的框架LIGHT，通过引入三种互补的记忆系统，显著提升了大型语言模型在长上下文推理任务中的性能。

Abstract: Evaluating the abilities of large language models (LLMs) for tasks that
require long-term memory and thus long-context reasoning, for example in
conversational settings, is hampered by the existing benchmarks, which often
lack narrative coherence, cover narrow domains, and only test simple
recall-oriented tasks. This paper introduces a comprehensive solution to these
challenges. First, we present a novel framework for automatically generating
long (up to 10M tokens), coherent, and topically diverse conversations,
accompanied by probing questions targeting a wide range of memory abilities.
From this, we construct BEAM, a new benchmark comprising 100 conversations and
2,000 validated questions. Second, to enhance model performance, we propose
LIGHT-a framework inspired by human cognition that equips LLMs with three
complementary memory systems: a long-term episodic memory, a short-term working
memory, and a scratchpad for accumulating salient facts. Our experiments on
BEAM reveal that even LLMs with 1M token context windows (with and without
retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT
consistently improves performance across various models, achieving an average
improvement of 3.5%-12.69% over the strongest baselines, depending on the
backbone LLM. An ablation study further confirms the contribution of each
memory component.

</details>


### [21] [Languages are Modalities: Cross-Lingual Alignment via Encoder Injection](https://arxiv.org/abs/2510.27254)
*Rajan Agarwal,Aarush Gupta*

Main category: cs.CL

TL;DR: 本文提出了一种名为LLINK的方法，用于改善指令调优的大语言模型在低资源、非拉丁文字方面的性能，通过将低资源语言视为一种模态，实现了更强的跨语言对齐。


<details>
  <summary>Details</summary>
Motivation: 指令调优的大语言模型在低资源、非拉丁文字方面表现不佳，这是由于分词器碎片化和跨语言耦合弱。

Method: LLINK（非英语知识的潜在语言注入）是一种计算高效的语言作为模态的方法，它通过一个轻量级对比投影器将冻结的多语言编码器的句子嵌入对齐到解码器的潜在嵌入空间中的预留位置，并将向量扩展为K个软槽，通过最小适配器进行训练，使冻结的解码器能够使用该信号。

Result: LLINK显著提高了双语检索，在LLM判断的问答评估中，优于基础模型81.3%，优于直接微调63.6%。此外，改进可以归因于减少的分词膨胀和更强的跨语言对齐，尽管模型在数值保真度方面仍有残留弱点。

Conclusion: 将低资源语言视为一种模态为在轻量级大语言模型中实现更强的跨语言对齐提供了可行的路径。

Abstract: Instruction-tuned Large Language Models (LLMs) underperform on low resource,
non-Latin scripts due to tokenizer fragmentation and weak cross-lingual
coupling. We present LLINK (Latent Language Injection for Non-English
Knowledge), a compute efficient language-as-modality method that conditions an
instruction-tuned decoder without changing the tokenizer or retraining the
decoder. First, we align sentence embeddings from a frozen multilingual encoder
to the decoder's latent embedding space at a reserved position via a
lightweight contrastive projector. Second, the vector is expanded into K soft
slots and trained with minimal adapters so the frozen decoder consumes the
signal. LLINK substantially improves bilingual retrieval and achieves 81.3%
preference over the base model and 63.6% over direct fine-tuning in LLM-judged
Q&A evaluations. We further find that improvements can be attributed to reduced
tokenization inflation and a stronger cross lingual alignment, despite the
model having residual weaknesses in numeric fidelity. Treating low resource
languages as a modality offers a practical path to stronger cross-lingual
alignment in lightweight LLMs.

</details>


### [22] [MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models](https://arxiv.org/abs/2510.27267)
*Kangkun Mao,Jinru Ding,Jiayuan Chen,Mouxiao Bian,Ruiyao Chen,Xinwei Peng,Sijie Ren,Linyang Li,Jie Xu*

Main category: cs.CL

TL;DR: 本文提出MedCalc-Eval作为评估LLMs医学计算能力的基准，并开发了MedCalc-Env环境以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医学基准主要关注问答或描述性推理，而忽略了临床决策中至关重要的定量推理。现有的数据集如MedCalc-Bench涵盖的计算任务有限，无法反映真实世界的计算场景。

Method: 本文提出了MedCalc-Eval基准和MedCalc-Env环境，通过强化学习方法提升模型在医学计算任务中的表现。

Result: 在MedCalc-Eval上，微调Qwen2.5-32B模型取得了最先进的结果，在数值敏感性、公式选择和推理鲁棒性方面有显著提升。

Conclusion: 本文介绍了MedCalc-Eval，这是评估大型语言模型（LLMs）医学计算能力的最大基准，包含700多个任务。同时，还开发了MedCalc-Env，这是一个基于InternBootcamp框架的强化学习环境，用于提升模型在医学计算任务中的表现。

Abstract: As large language models (LLMs) enter the medical domain, most benchmarks
evaluate them on question answering or descriptive reasoning, overlooking
quantitative reasoning critical to clinical decision-making. Existing datasets
like MedCalc-Bench cover few calculation tasks and fail to reflect real-world
computational scenarios.
  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical
calculation abilities, comprising 700+ tasks across two types: equation-based
(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,
Glasgow Coma Scale). These tasks span diverse specialties including internal
medicine, surgery, pediatrics, and cardiology, offering a broader and more
challenging evaluation setting.
  To improve performance, we further develop MedCalc-Env, a reinforcement
learning environment built on the InternBootcamp framework, enabling multi-step
clinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this
environment achieves state-of-the-art results on MedCalc-Eval, with notable
gains in numerical sensitivity, formula selection, and reasoning robustness.
Remaining challenges include unit conversion, multi-condition logic, and
contextual understanding.
  Code and datasets are available at
https://github.com/maokangkun/MedCalc-Eval.

</details>


### [23] [Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?](https://arxiv.org/abs/2510.27269)
*Deokhyung Kang,Seonjeong Hwang,Daehui Kim,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文研究了多语言推理差距的原因，发现其主要源于模型在将多语言输入意义表示为英语时的失败，并提出了一种选择性翻译策略来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 我们旨在探讨多语言推理差距的原因，并寻找减轻该差距的方法。

Method: 我们评估了多种检测方法，并提出了选择性翻译策略，该策略仅在检测到理解失败时将多语言输入翻译成英语。

Result: 实验结果表明，选择性翻译弥合了多语言推理差距，在仅对约20%的输入进行翻译的情况下实现了接近全翻译的性能。

Conclusion: 我们的工作表明，理解失败是多语言推理差距的主要原因，并且可以被检测和选择性缓解，为更公平的多语言推理提供了关键见解。

Abstract: Reasoning language models (RLMs) achieve strong performance on complex
reasoning tasks, yet they still suffer from a multilingual reasoning gap,
performing better in high-resource languages than in low-resource ones. While
recent efforts have reduced this gap, its underlying causes remain largely
unexplored. In this paper, we address this by showing that the multilingual
reasoning gap largely stems from failures in language understanding-the model's
inability to represent the multilingual input meaning into the dominant
language (i.e., English) within its reasoning trace. This motivates us to
examine whether understanding failures can be detected, as this ability could
help mitigate the multilingual reasoning gap. To this end, we evaluate a range
of detection methods and find that understanding failures can indeed be
identified, with supervised approaches performing best. Building on this, we
propose Selective Translation, a simple yet effective strategy that translates
the multilingual input into English only when an understanding failure is
detected. Experimental results show that Selective Translation bridges the
multilingual reasoning gap, achieving near full-translation performance while
using translation for only about 20% of inputs. Together, our work demonstrates
that understanding failures are the primary cause of the multilingual reasoning
gap and can be detected and selectively mitigated, providing key insight into
its origin and a promising path toward more equitable multilingual reasoning.
Our code and data are publicly available at
https://github.com/deokhk/RLM_analysis.

</details>


### [24] [A Unified Representation Underlying the Judgment of Large Language Models](https://arxiv.org/abs/2510.27328)
*Yi-Long Lu,Jiajun Song,Wei Wang*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的判断机制，发现评估判断是沿着一个称为Valence-Assent Axis (VAA) 的主要维度进行的，该轴同时编码主观价值和模型对事实的同意。通过直接干预，我们展示了这种统一表示创建了一个关键依赖：VAA作为控制信号，引导生成过程构建与其评估状态一致的论证，即使以牺牲事实准确性为代价。这揭示了促进连贯判断的架构如何系统地损害忠实推理。


<details>
  <summary>Details</summary>
Motivation: 一个核心的架构问题是判断是否依赖于专门模块或统一的、领域通用的资源。虽然发现了可解码的神经表示，但这些表示是否是真正独立的系统仍然是一个开放问题。

Method: 通过直接干预，我们展示了这种统一表示创建了一个关键依赖：VAA作为控制信号，引导生成过程构建与其评估状态一致的论证，即使以牺牲事实准确性为代价。

Result: 在一系列LLMs中，我们发现多样化的评估判断是沿着一个主要维度计算的，我们称之为Valence-Assent Axis (VAA)。这个轴同时编码主观价值（“什么是好的”）和模型对事实声明的同意（“什么是真的”）。

Conclusion: 我们的发现为系统性偏差和幻觉提供了机制解释，揭示了促进连贯判断的架构如何系统地损害忠实推理。

Abstract: A central architectural question for both biological and artificial
intelligence is whether judgment relies on specialized modules or a unified,
domain-general resource. While the discovery of decodable neural
representations for distinct concepts in Large Language Models (LLMs) has
suggested a modular architecture, whether these representations are truly
independent systems remains an open question. Here we provide evidence for a
convergent architecture. Across a range of LLMs, we find that diverse
evaluative judgments are computed along a dominant dimension, which we term the
Valence-Assent Axis (VAA). This axis jointly encodes subjective valence ("what
is good") and the model's assent to factual claims ("what is true"). Through
direct interventions, we show this unified representation creates a critical
dependency: the VAA functions as a control signal that steers the generative
process to construct a rationale consistent with its evaluative state, even at
the cost of factual accuracy. This mechanism, which we term the subordination
of reasoning, shifts the process of reasoning from impartial inference toward
goal-directed justification. Our discovery offers a mechanistic account for
systemic bias and hallucination, revealing how an architecture that promotes
coherent judgment can systematically undermine faithful reasoning.

</details>


### [25] [TransAlign: Machine Translation Encoders are Strong Word Aligners, Too](https://arxiv.org/abs/2510.27337)
*Benedikt Ebing,Christian Goldschmied,Goran Glavaš*

Main category: cs.CL

TL;DR: 本文提出了一种新的词对齐方法 TransAlign，利用大规模多语言机器翻译模型的编码器，显著提升了跨语言转移任务的效果。


<details>
  <summary>Details</summary>
Motivation: 在大多数世界语言和自然语言处理任务中缺乏足够的训练数据的情况下，需要一种有效的跨语言转移方法。现有的词对齐方法效果不佳，因此需要一种更优的解决方案。

Method: 提出了一种新的词对齐方法 TransAlign，利用大规模多语言机器翻译模型的编码器进行词对齐。

Result: TransAlign 在词对齐任务中表现优异，并且在基于机器翻译的跨语言转移任务中显著优于现有的词对齐方法和最先进的非词对齐方法。

Conclusion: TransAlign 不仅在词对齐方面表现出色，而且在基于机器翻译的跨语言转移任务中显著优于现有的词对齐方法和最先进的非词对齐方法。

Abstract: In the absence of sizable training data for most world languages and NLP
tasks, translation-based strategies such as translate-test -- evaluating on
noisy source language data translated from the target language -- and
translate-train -- training on noisy target language data translated from the
source language -- have been established as competitive approaches for
cross-lingual transfer (XLT). For token classification tasks, these strategies
require label projection: mapping the labels from each token in the original
sentence to its counterpart(s) in the translation. To this end, it is common to
leverage multilingual word aligners (WAs) derived from encoder language models
such as mBERT or LaBSE. Despite obvious associations between machine
translation (MT) and WA, research on extracting alignments with MT models is
largely limited to exploiting cross-attention in encoder-decoder architectures,
yielding poor WA results. In this work, in contrast, we propose TransAlign, a
novel word aligner that utilizes the encoder of a massively multilingual MT
model. We show that TransAlign not only achieves strong WA performance but
substantially outperforms popular WA and state-of-the-art non-WA-based label
projection methods in MT-based XLT for token classification.

</details>


### [26] [ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via Probing Representations](https://arxiv.org/abs/2510.27355)
*Zijian Wang,Chang Xu*

Main category: cs.CL

TL;DR: 本文介绍了ThoughtProbe，这是一个新颖的推理时间框架，利用大型语言模型的隐藏推理特征来提高其推理性能。通过在每个节点扩展中使用分类器作为评分和排名机制，以及通过分支聚合方法从候选答案池中确定最佳答案，实验结果表明该框架在多个算术推理基准上实现了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过利用大型语言模型的隐藏推理特征来提高其推理性能，而不是像以前的工作那样操纵隐藏表示来引导LLM生成。

Method: ThoughtProbe是一个新颖的推理时间框架，利用大型语言模型（LLMs）的隐藏推理特征来提高其推理性能。在每个节点扩展中，一个分类器作为评分和排名机制，通过优先选择高分候选者来高效分配计算资源。完成树扩展后，我们收集所有分支的答案以形成候选答案池，并提出一种分支聚合方法，通过聚合其CoT分数来对所有支持分支进行边缘化，从而从池中确定最佳答案。

Result: 实验结果表明，我们的框架的全面探索不仅涵盖了有效的推理链，而且有效地识别了它们，在多个算术推理基准上实现了显著的改进。

Conclusion: 实验结果表明，我们的框架的全面探索不仅涵盖了有效的推理链，而且有效地识别了它们，在多个算术推理基准上实现了显著的改进。

Abstract: This paper introduces ThoughtProbe, a novel inference time framework that
leverages the hidden reasoning features of Large Language Models (LLMs) to
improve their reasoning performance. Unlike previous works that manipulate the
hidden representations to steer LLM generation, we harness them as
discriminative signals to guide the tree structured response space exploration.
In each node expansion, a classifier serves as a scoring and ranking mechanism
that efficiently allocates computational resources by prioritizing higher score
candidates for continuation. After completing the tree expansion, we collect
answers from all branches to form a candidate answer pool. We then propose a
branch aggregation method that marginalizes over all supporting branches by
aggregating their CoT scores, thereby identifying the optimal answer from the
pool. Experimental results show that our framework's comprehensive exploration
not only covers valid reasoning chains but also effectively identifies them,
achieving significant improvements across multiple arithmetic reasoning
benchmarks.

</details>


### [27] [From the Rock Floor to the Cloud: A Systematic Survey of State-of-the-Art NLP in Battery Life Cycle](https://arxiv.org/abs/2510.27369)
*Tosin Adewumi,Martin Karlsson,Marcus Liwicki,Mikael Sjödahl,Lama Alkhaled,Rihab Gargouri,Nudrat Habib,Franz Hennie*

Main category: cs.CL

TL;DR: 本文全面系统地调查了自然语言处理（NLP）在电池整个生命周期中的应用，并提出了一个结合代理AI和优化提示的新型技术语言处理（TLP）框架，以应对一些挑战。


<details>
  <summary>Details</summary>
Motivation: 我们希望全面系统地调查自然语言处理（NLP）在电池整个生命周期中的应用，而不是仅限于一个阶段或方法。

Method: 我们遵循PRISMA方法，并使用Google Scholar、IEEE Xplore和Scopus三个可信数据库或搜索引擎进行系统调查。

Result: 研究发现，电池领域正在出现新的NLP任务，这些任务有助于材料发现和其他生命周期阶段。然而，仍然存在挑战，例如缺乏标准基准。

Conclusion: 我们的TLP框架结合了代理AI和优化提示，能够应对一些挑战。

Abstract: We present a comprehensive systematic survey of the application of natural
language processing (NLP) along the entire battery life cycle, instead of one
stage or method, and introduce a novel technical language processing (TLP)
framework for the EU's proposed digital battery passport (DBP) and other
general battery predictions. We follow the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) method and employ three reputable
databases or search engines, including Google Scholar, Institute of Electrical
and Electronics Engineers Xplore (IEEE Xplore), and Scopus. Consequently, we
assessed 274 scientific papers before the critical review of the final 66
relevant papers. We publicly provide artifacts of the review for validation and
reproducibility. The findings show that new NLP tasks are emerging in the
battery domain, which facilitate materials discovery and other stages of the
life cycle. Notwithstanding, challenges remain, such as the lack of standard
benchmarks. Our proposed TLP framework, which incorporates agentic AI and
optimized prompts, will be apt for tackling some of the challenges.

</details>


### [28] [Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs](https://arxiv.org/abs/2510.27400)
*Jiahao Liu,Zijian Wang,Kuo Zhao,Dong Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为IntAttn-Edit的方法，通过将关联记忆范式扩展到同时更新MLP和Attn模块，提高了大型语言模型的知识编辑效果。


<details>
  <summary>Details</summary>
Motivation: Most existing methods focus on the weights of MLP modules, which are often identified as the main repositories of factual information. Other components, such as attention (Attn) modules, are often ignored during editing. This imbalance can leave residual outdated knowledge and limit editing effectiveness.

Method: IntAttn-Edit extends the associative memory paradigm to jointly update both MLP and Attn modules. It uses a knowledge balancing strategy that allocates update magnitudes in proportion to each module's measured contribution to knowledge storage.

Result: Experiments on standard benchmarks show that IntAttn-Edit achieves higher edit success, better generalization, and stronger knowledge preservation than prior methods. Further analysis shows that the balancing strategy keeps editing performance within an optimal range across diverse settings.

Conclusion: IntAttn-Edit achieves higher edit success, better generalization, and stronger knowledge preservation than prior methods. The balancing strategy keeps editing performance within an optimal range across diverse settings.

Abstract: Knowledge editing has emerged as an efficient approach for updating factual
knowledge in large language models (LLMs). It typically locates knowledge
storage modules and then modifies their parameters. However, most existing
methods focus on the weights of multilayer perceptron (MLP) modules, which are
often identified as the main repositories of factual information. Other
components, such as attention (Attn) modules, are often ignored during editing.
This imbalance can leave residual outdated knowledge and limit editing
effectiveness. We perform comprehensive knowledge localization experiments on
advanced LLMs and find that Attn modules play a substantial role in factual
knowledge storage and retrieval, especially in earlier layers. Based on these
insights, we propose IntAttn-Edit, a method that extends the associative memory
paradigm to jointly update both MLP and Attn modules. Our approach uses a
knowledge balancing strategy that allocates update magnitudes in proportion to
each module's measured contribution to knowledge storage. Experiments on
standard benchmarks show that IntAttn-Edit achieves higher edit success, better
generalization, and stronger knowledge preservation than prior methods. Further
analysis shows that the balancing strategy keeps editing performance within an
optimal range across diverse settings.

</details>


### [29] [Awal -- Community-Powered Language Technology for Tamazight](https://arxiv.org/abs/2510.27407)
*Alp Öktem,Farida Boudichat*

Main category: cs.CL

TL;DR: 该论文介绍了Awal项目，这是一个为Tamazight语言技术资源开发的社区驱动倡议，但发现数据收集面临挑战，且贡献主要来自专业人士。


<details>
  <summary>Details</summary>
Motivation: 由于Tamazight在数字空间中的代表性不足，该项目旨在通过社区驱动的方法来解决数据稀缺问题。

Method: 该研究通过分析18个月的社区参与情况，评估了Awal平台的效果，并尝试改进开源机器翻译模型。

Result: 尽管有积极的反响，但实际的数据贡献主要集中在语言学家和活动人士中，社区贡献的规模较小。

Conclusion: 该研究展示了Awal项目在开发Tamazight语言技术资源方面的努力，但同时也指出了在数据收集过程中遇到的挑战。

Abstract: This paper presents Awal, a community-powered initiative for developing
language technology resources for Tamazight. We provide a comprehensive review
of the NLP landscape for Tamazight, examining recent progress in computational
resources, and the emergence of community-driven approaches to address
persistent data scarcity. Launched in 2024, awaldigital.org platform addresses
the underrepresentation of Tamazight in digital spaces through a collaborative
platform enabling speakers to contribute translation and voice data. We analyze
18 months of community engagement, revealing significant barriers to
participation including limited confidence in written Tamazight and ongoing
standardization challenges. Despite widespread positive reception, actual data
contribution remained concentrated among linguists and activists. The modest
scale of community contributions -- 6,421 translation pairs and 3 hours of
speech data -- highlights the limitations of applying standard crowdsourcing
approaches to languages with complex sociolinguistic contexts. We are working
on improved open-source MT models using the collected data.

</details>


### [30] [Dynamic Affective Memory Management for Personalized LLM Agents](https://arxiv.org/abs/2510.27418)
*Junfeng Lu,Yueyan Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的记忆管理系统，用于情感场景，并通过实验验证了其在个性化、逻辑连贯性和准确性方面的优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前的代理系统主要依赖于个性化的外部内存数据库来提供定制体验，但面临内存冗余、内存过时和不良的内存上下文集成等问题。

Method: 我们提出了一种基于贝叶斯的内存更新算法，结合了记忆熵的概念，使代理能够自主维护一个动态更新的内存向量数据库。

Result: 实验结果表明，我们的系统在个性化、逻辑连贯性和准确性方面表现出色。消融研究进一步验证了贝叶斯启发式更新机制在缓解内存膨胀方面的有效性。

Conclusion: 我们的工作为长期记忆系统的设计提供了新的见解。

Abstract: Advances in large language models are making personalized AI agents a new
research focus. While current agent systems primarily rely on personalized
external memory databases to deliver customized experiences, they face
challenges such as memory redundancy, memory staleness, and poor memory-context
integration, largely due to the lack of effective memory updates during
interaction. To tackle these issues, we propose a new memory management system
designed for affective scenarios. Our approach employs a Bayesian-inspired
memory update algorithm with the concept of memory entropy, enabling the agent
to autonomously maintain a dynamically updated memory vector database by
minimizing global entropy to provide more personalized services. To better
evaluate the system's effectiveness in this context, we propose DABench, a
benchmark focusing on emotional expression and emotional change toward objects.
Experimental results demonstrate that, our system achieves superior performance
in personalization, logical coherence, and accuracy. Ablation studies further
validate the effectiveness of the Bayesian-inspired update mechanism in
alleviating memory bloat. Our work offers new insights into the design of
long-term memory systems.

</details>


### [31] [VCORE: Variance-Controlled Optimization-based Reweighting for Chain-of-Thought Supervision](https://arxiv.org/abs/2510.27462)
*Xuan Gong,Senmiao Wang,Hanbo Huang,Ruoyu Sun,Shiyu Liang*

Main category: cs.CL

TL;DR: VCORE是一种基于优化理论的框架，用于改进监督微调过程中的令牌加权，从而提高大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 标准的交叉熵损失对所有令牌一视同仁，忽略了它们在推理轨迹中的异质性贡献，导致监督分配不当和泛化能力弱，特别是在复杂、长格式的推理任务中。

Method: VCORE通过将CoT监督重新表述为一个约束优化问题，采用优化理论视角，实现了对令牌的原理性和自适应的监督分配。

Result: VCORE在数学和编码基准测试中表现出色，尤其是在使用Qwen3系列（4B、8B、32B）和LLaMA-3.1-8B-Instruct模型时，取得了显著的性能提升。

Conclusion: VCORE作为一种更有效的初始化方法，为后续的强化学习奠定了更强的基础，从而推动了大语言模型的推理能力的发展。

Abstract: Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has
emerged as a crucial technique for enhancing the reasoning abilities of large
language models (LLMs). However, the standard cross-entropy loss treats all
tokens equally, ignoring their heterogeneous contributions across a reasoning
trajectory. This uniform treatment leads to misallocated supervision and weak
generalization, especially in complex, long-form reasoning tasks. To address
this, we introduce \textbf{V}ariance-\textbf{C}ontrolled
\textbf{O}ptimization-based \textbf{RE}weighting (VCORE), a principled
framework that reformulates CoT supervision as a constrained optimization
problem. By adopting an optimization-theoretic perspective, VCORE enables a
principled and adaptive allocation of supervision across tokens, thereby
aligning the training objective more closely with the goal of robust reasoning
generalization. Empirical evaluations demonstrate that VCORE consistently
outperforms existing token reweighting methods. Across both in-domain and
out-of-domain settings, VCORE achieves substantial performance gains on
mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B,
32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more
effective initialization for subsequent reinforcement learning, establishing a
stronger foundation for advancing the reasoning capabilities of LLMs. The Code
will be released at https://github.com/coder-gx/VCORE.

</details>


### [32] [Diffuse Thinking: Exploring Diffusion Language Models as Efficient Thought Proposers for Reasoning](https://arxiv.org/abs/2510.27469)
*Chenyang Shao,Sijian Ren,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 本文提出了一种高效的协同推理框架，利用扩散语言模型生成候选思维，并利用大语言模型评估其质量，以减轻计算负担并提高推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的自回归生成范式导致推理性能在测试时间计算上次优，通常需要过多的计算开销来提出思维，而仅带来微小的性能提升。因此，我们希望利用扩散语言模型来减轻计算负担，同时保持质量。

Method: 本文提出了一种高效的协同推理框架，利用扩散语言模型生成候选思维，并利用大语言模型评估其质量。

Result: 实验结果表明，本文提出的框架在各种基准测试中表现强劲，证明了其在复杂推理任务中的有效性。

Conclusion: 本文提出了一种高效的协同推理框架，利用扩散语言模型生成候选思维，并利用大语言模型评估其质量。实验表明，该框架在复杂推理任务中表现出色，为未来的研究提供了有前景的方向。

Abstract: In recent years, large language models (LLMs) have witnessed remarkable
advancements, with the test-time scaling law consistently enhancing the
reasoning capabilities. Through systematic evaluation and exploration of a
diverse spectrum of intermediate thoughts, LLMs demonstrate the potential to
generate deliberate reasoning steps, thereby substantially enhancing reasoning
accuracy. However, LLMs' autoregressive generation paradigm results in
reasoning performance scaling sub-optimally with test-time computation, often
requiring excessive computational overhead to propose thoughts while yielding
only marginal performance gains. In contrast, diffusion language models (DLMs)
can efficiently produce diverse samples through parallel denoising in a single
forward pass, inspiring us to leverage them for proposing intermediate
thoughts, thereby alleviating the computational burden associated with
autoregressive generation while maintaining quality. In this work, we propose
an efficient collaborative reasoning framework, leveraging DLMs to generate
candidate thoughts and LLMs to evaluate their quality. Experiments across
diverse benchmarks demonstrate that our framework achieves strong performance
in complex reasoning tasks, offering a promising direction for future research.
Our code is open-source at
https://anonymous.4open.science/r/Diffuse-Thinking-EC60.

</details>


### [33] [The aftermath of compounds: Investigating Compounds and their Semantic Representations](https://arxiv.org/abs/2510.27477)
*Swarang Joshi*

Main category: cs.CL

TL;DR: 本研究比较了GloVe和BERT嵌入与人类对英语复合词语义判断的一致性，发现BERT嵌入更能捕捉组合语义，而可预测性评分是语义透明性的强大预测因子。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨计算嵌入在英语复合词处理中与人类语义判断的一致性，以及哪些因素驱动复合词处理并提供基于嵌入的语义建模的见解。

Method: 研究比较了静态词向量（GloVe）和上下文嵌入（BERT）与来自心理语言学数据集的人类评分的词素意义主导程度（LMD）和语义透明度（ST）。使用关联强度（爱丁堡关联同义词库）、频率（BNC）和可预测性（LaDEC）的度量，计算出嵌入派生的LMD和ST指标，并通过斯皮尔曼相关性和回归分析评估它们与人类判断的关系。

Result: 结果表明，BERT嵌入比GloVe更好地捕捉组合语义，并且可预测性评分是人类和模型数据中语义透明性的强大预测因子。

Conclusion: 研究结果表明，BERT嵌入比GloVe更好地捕捉组合语义，并且可预测性评分是人类和模型数据中语义透明性的强大预测因子。这些发现通过明确驱动复合词处理的因素并提供基于嵌入的语义建模的见解，推进了计算心理语言学。

Abstract: This study investigates how well computational embeddings align with human
semantic judgments in the processing of English compound words. We compare
static word vectors (GloVe) and contextualized embeddings (BERT) against human
ratings of lexeme meaning dominance (LMD) and semantic transparency (ST) drawn
from a psycholinguistic dataset. Using measures of association strength
(Edinburgh Associative Thesaurus), frequency (BNC), and predictability (LaDEC),
we compute embedding-derived LMD and ST metrics and assess their relationships
with human judgments via Spearmans correlation and regression analyses. Our
results show that BERT embeddings better capture compositional semantics than
GloVe, and that predictability ratings are strong predictors of semantic
transparency in both human and model data. These findings advance computational
psycholinguistics by clarifying the factors that drive compound word processing
and offering insights into embedding-based semantic modeling.

</details>


### [34] [Effect of Domain Generalization Techniques in Low Resource Systems](https://arxiv.org/abs/2510.27512)
*Mahi Aminu,Chisom Chibuike,Fatimo Adebanjo,Omokolade Awosanya,Samuel Oyeneye*

Main category: cs.CL

TL;DR: 本文研究了两种因果域泛化技术在低资源自然语言任务中的应用，并展示了它们在提高模型对未见领域的鲁棒性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，训练数据和测试数据的分布可能不同，这在低资源设置中尤为明显。因此，需要研究能够提高模型鲁棒性的方法。

Method: 本文研究了因果数据增强（CDA）和不变因果表示学习（ICRL）方法，分别应用于情感分类任务和多语言情感分析任务。

Result: 因果数据增强方法在情感分类任务中表现出一致的跨领域准确率提升，而使用DINER的因果表示学习方法在多语言情感分析任务中提高了分布外性能，但不同语言的增益有所不同。

Conclusion: 本文研究了两种不同的因果域泛化技术在低资源自然语言任务中的应用，并展示了它们在提高模型对未见领域的鲁棒性方面的有效性。

Abstract: Machine learning models typically assume that training and test data follow
the same distribution, an assumption that often fails in real-world scenarios
due to distribution shifts. This issue is especially pronounced in low-resource
settings, where data scarcity and limited domain diversity hinder robust
generalization. Domain generalization (DG) approaches address this challenge by
learning features that remain invariant across domains, often using causal
mechanisms to improve model robustness. In this study, we examine two distinct
causal DG techniques in low-resource natural language tasks. First, we
investigate a causal data augmentation (CDA) approach that automatically
generates counterfactual examples to improve robustness to spurious
correlations. We apply this method to sentiment classification on the
NaijaSenti Twitter corpus, expanding the training data with semantically
equivalent paraphrases to simulate controlled distribution shifts. Second, we
explore an invariant causal representation learning (ICRL) approach using the
DINER framework, originally proposed for debiasing aspect-based sentiment
analysis. We adapt DINER to a multilingual setting. Our findings demonstrate
that both approaches enhance robustness to unseen domains: counterfactual data
augmentation yields consistent cross-domain accuracy gains in sentiment
classification, while causal representation learning with DINER improves
out-of-distribution performance in multilingual sentiment analysis, albeit with
varying gains across languages.

</details>


### [35] [BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for Scalable and Efficient Text Summarization](https://arxiv.org/abs/2510.27516)
*Desta Haileselassie Hagos,Legand L. Burge,Anietie Andy,Anis Yazidi,Vladimir Vlassov*

Main category: cs.CL

TL;DR: BiSparse-AAS is a novel framework that improves text summarization by combining sparse attention, adaptive spans, and bilinear attention, achieving significant improvements in performance while addressing efficiency and scalability issues.


<details>
  <summary>Details</summary>
Motivation: Transformer-based architectures have advanced text summarization, yet their quadratic complexity limits scalability on long documents.

Method: BiSparse-AAS combines sparse attention, adaptive spans, and bilinear attention to address the limitations of transformer-based architectures in text summarization.

Result: BiSparse-AAS consistently outperforms state-of-the-art baselines in both extractive and abstractive summarization tasks, achieving average ROUGE improvements of about 68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance on OpenWebText and Gigaword datasets.

Conclusion: BiSparse-AAS provides a unified, practical solution for real-world text summarization applications by addressing efficiency, scalability, and long-sequence modeling.

Abstract: Transformer-based architectures have advanced text summarization, yet their
quadratic complexity limits scalability on long documents. This paper
introduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a
novel framework that combines sparse attention, adaptive spans, and bilinear
attention to address these limitations. Sparse attention reduces computational
costs by focusing on the most relevant parts of the input, while adaptive spans
dynamically adjust the attention ranges. Bilinear attention complements both by
modeling complex token interactions within this refined context. BiSparse-AAS
consistently outperforms state-of-the-art baselines in both extractive and
abstractive summarization tasks, achieving average ROUGE improvements of about
68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance
on OpenWebText and Gigaword datasets. By addressing efficiency, scalability,
and long-sequence modeling, BiSparse-AAS provides a unified, practical solution
for real-world text summarization applications.

</details>


### [36] [SQLSpace: A Representation Space for Text-to-SQL to Discover and Mitigate Robustness Gaps](https://arxiv.org/abs/2510.27532)
*Neha Srikanth,Victor Bursztyn,Puneet Mathur,Ani Nenkova*

Main category: cs.CL

TL;DR: 本文介绍了SQLSpace，这是一种用于文本到SQL例子的表示方法，能够揭示不同基准之间的组成差异，暴露性能模式，并支持查询成功的建模。


<details>
  <summary>Details</summary>
Motivation: 为了更好地评估文本到SQL的例子，需要一种能够揭示不同基准之间组成差异、暴露性能模式并支持查询成功建模的表示方法。

Method: SQLSpace是一种人类可解释、可推广、紧凑的表示方法，用于文本到SQL的例子，通过最小的人类干预获得。

Result: SQLSpace能够实现三种使用场景：(i) 比较和对比流行的文本到SQL基准，以识别它们评估的例子的独特维度；(ii) 在整体准确率之外，深入了解模型性能；(iii) 通过基于学习的正确性估计进行有针对性的查询重写来提高模型性能。

Conclusion: SQLSpace可以用于分析文本到SQL的例子，揭示不同基准之间的组成差异，暴露被准确率掩盖的性能模式，并支持查询成功的建模。

Abstract: We introduce SQLSpace, a human-interpretable, generalizable, compact
representation for text-to-SQL examples derived with minimal human
intervention. We demonstrate the utility of these representations in evaluation
with three use cases: (i) closely comparing and contrasting the composition of
popular text-to-SQL benchmarks to identify unique dimensions of examples they
evaluate, (ii) understanding model performance at a granular level beyond
overall accuracy scores, and (iii) improving model performance through targeted
query rewriting based on learned correctness estimation. We show that SQLSpace
enables analysis that would be difficult with raw examples alone: it reveals
compositional differences between benchmarks, exposes performance patterns
obscured by accuracy alone, and supports modeling of query success.

</details>


### [37] [Patient-Centered Summarization Framework for AI Clinical Summarization: A Mixed-Methods Design](https://arxiv.org/abs/2510.27535)
*Maria Lizarazo Jimenez,Ana Gabriela Claros,Kieran Green,David Toro-Tobon,Felipe Larios,Sheena Asthana,Camila Wenczenovicz,Kerly Guevara Maldonado,Luis Vilatuna-Andrango,Cristina Proano-Velez,Satya Sai Sri Bandi,Shubhangi Bagewadi,Megan E. Branda,Misk Al Zahidy,Saturnino Luz,Mirella Lapata,Juan P. Brito,Oscar J. Ponce-Ponte*

Main category: cs.CL

TL;DR: 研究提出了一种新的患者中心摘要标准，并评估了当前开源大语言模型在生成此类摘要方面的表现。结果显示，尽管模型在某些方面接近人类水平，但在正确性和以患者为中心方面仍逊色于人类。


<details>
  <summary>Details</summary>
Motivation: 为了实现以患者为中心的护理，研究提出了一个新的标准：患者中心摘要（PCS），旨在捕捉患者价值观并确保临床实用性。

Method: 研究采用混合方法，包括患者和公众参与小组的半结构化访谈，以及使用临床医生创建黄金标准的患者中心摘要。然后使用五个开源大语言模型生成摘要，并通过ROUGE-L、BERTScore和定性指标进行评估。

Result: 患者强调了生活方式习惯、社会支持、近期压力源和护理价值观；临床医生寻求简洁的功能性、心理社会和情感背景。最佳零样本性能由Mistral-8B（ROUGE-L 0.189）和Llama-3.1-8B（BERTScore 0.673）实现；最佳少样本性能由Llama-3.1-8B（ROUGE-L 0.206，BERTScore 0.683）实现。

Conclusion: 研究发现，尽管当前的开源大语言模型在生成临床摘要方面表现出一定的能力，但在完整性和流畅性方面与人类生成的摘要相当，而在正确性和以患者为中心方面仍优于人类。

Abstract: Large Language Models (LLMs) are increasingly demonstrating the potential to
reach human-level performance in generating clinical summaries from
patient-clinician conversations. However, these summaries often focus on
patients' biology rather than their preferences, values, wishes, and concerns.
To achieve patient-centered care, we propose a new standard for Artificial
Intelligence (AI) clinical summarization tasks: Patient-Centered Summaries
(PCS). Our objective was to develop a framework to generate PCS that capture
patient values and ensure clinical utility and to assess whether current
open-source LLMs can achieve human-level performance in this task. We used a
mixed-methods process. Two Patient and Public Involvement groups (10 patients
and 8 clinicians) in the United Kingdom participated in semi-structured
interviews exploring what personal and contextual information should be
included in clinical summaries and how it should be structured for clinical
use. Findings informed annotation guidelines used by eight clinicians to create
gold-standard PCS from 88 atrial fibrillation consultations. Sixteen
consultations were used to refine a prompt aligned with the guidelines. Five
open-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and
Qwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot
prompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients
emphasized lifestyle routines, social support, recent stressors, and care
values. Clinicians sought concise functional, psychosocial, and emotional
context. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L
0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B
(ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between
experts and models, while correctness and patient-centeredness favored human
PCS.

</details>


### [38] [DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models](https://arxiv.org/abs/2510.27543)
*Malik H. Altakrori,Nizar Habash,Abdelhakim Freihat,Younes Samih,Kirill Chirkunov,Muhammed AbuOdeh,Radu Florian,Teresa Lynn,Preslav Nakov,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 本文介绍了 DialectalArabicMMLU，这是一个新的基准，用于评估大型语言模型（LLMs）在阿拉伯语方言中的表现。该基准通过手动翻译和改编3000个多项选择题-答案对，扩展了MMLU-Redux框架，涵盖五个主要方言（叙利亚、埃及、阿联酋、沙特和摩洛哥），生成了15000个QA对，覆盖32个学术和专业领域。该基准支持任务和语言分析，评估了19个开放权重的阿拉伯语和多语言LLMs，并报告了方言间的显著性能差异，揭示了方言泛化的持续差距。DialectalArabicMMLU提供了首个统一的人工整理资源，用于衡量阿拉伯语方言的理解，从而促进更包容的评估和未来模型开发。


<details>
  <summary>Details</summary>
Motivation: While recently developed Arabic and multilingual benchmarks have advanced LLM evaluation for Modern Standard Arabic (MSA), dialectal varieties remain underrepresented despite their prevalence in everyday communication.

Method: DialectalArabicMMLU extends the MMLU-Redux framework through manual translation and adaptation of 3K multiple-choice question-answer pairs into five major dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of 15K QA pairs across 32 academic and professional domains.

Result: We evaluate 19 open-weight Arabic and multilingual LLMs (1B-13B parameters) and report substantial performance variation across dialects, revealing persistent gaps in dialectal generalization.

Conclusion: DialectalArabicMMLU provides the first unified, human-curated resource for measuring dialectal understanding in Arabic, thus promoting more inclusive evaluation and future model development.

Abstract: We present DialectalArabicMMLU, a new benchmark for evaluating the
performance of large language models (LLMs) across Arabic dialects. While
recently developed Arabic and multilingual benchmarks have advanced LLM
evaluation for Modern Standard Arabic (MSA), dialectal varieties remain
underrepresented despite their prevalence in everyday communication.
DialectalArabicMMLU extends the MMLU-Redux framework through manual translation
and adaptation of 3K multiple-choice question-answer pairs into five major
dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of
15K QA pairs across 32 academic and professional domains (22K QA pairs when
also including English and MSA). The benchmark enables systematic assessment of
LLM reasoning and comprehension beyond MSA, supporting both task-based and
linguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs
(1B-13B parameters) and report substantial performance variation across
dialects, revealing persistent gaps in dialectal generalization.
DialectalArabicMMLU provides the first unified, human-curated resource for
measuring dialectal understanding in Arabic, thus promoting more inclusive
evaluation and future model development.

</details>


### [39] [Multilingual BERT language model for medical tasks: Evaluation on domain-specific adaptation and cross-linguality](https://arxiv.org/abs/2510.27552)
*Yinghao Luo,Lang Zhou,Amrish Jhingoer,Klaske Vliegenthart Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: 本研究探讨了在低资源语言环境下，通过进一步预训练领域特定语料库来提高医疗NLP任务性能的方法。结果表明，领域适应和跨语言能力是可行的，并为开发多语言医疗NLP系统提供了指导。


<details>
  <summary>Details</summary>
Motivation: 在多语言医疗应用中，领域特定的自然语言处理（NLP）工具的可用性有限，尤其是对于低资源语言。虽然多语言BERT提供了一个有希望的动机来缓解语言差距，但低资源语言的医疗NLP任务仍然研究不足。因此，本研究旨在探讨进一步预训练对医疗任务模型性能的影响。

Method: 本研究通过进一步预训练领域特定语料库来探讨其对医疗任务模型性能的影响，重点研究了荷兰语、罗马尼亚语和西班牙语。进行了四项实验以创建医疗领域模型，并将这些模型微调到三个下游任务：荷兰临床笔记中的自动患者筛选、罗马尼亚和西班牙临床笔记中的命名实体识别。

Result: 结果表明，领域适应显著提高了任务性能。此外，领域的进一步区分，例如临床和一般生物医学领域，导致了不同的性能表现。临床领域适应的模型优于更一般的生物医学领域适应的模型。此外，我们观察到了跨语言可转移性的证据。

Conclusion: 研究结果表明，领域适应和跨语言能力在医疗NLP中是可行的。这些发现可以为在低资源语言环境下开发多语言医疗NLP系统提供有意义的指导，从而缓解训练数据不足的问题并提高模型性能。

Abstract: In multilingual healthcare applications, the availability of domain-specific
natural language processing(NLP) tools is limited, especially for low-resource
languages. Although multilingual bidirectional encoder representations from
transformers (BERT) offers a promising motivation to mitigate the language gap,
the medical NLP tasks in low-resource languages are still underexplored.
Therefore, this study investigates how further pre-training on domain-specific
corpora affects model performance on medical tasks, focusing on three
languages: Dutch, Romanian and Spanish. In terms of further pre-training, we
conducted four experiments to create medical domain models. Then, these models
were fine-tuned on three downstream tasks: Automated patient screening in Dutch
clinical notes, named entity recognition in Romanian and Spanish clinical
notes. Results show that domain adaptation significantly enhanced task
performance. Furthermore, further differentiation of domains, e.g. clinical and
general biomedical domains, resulted in diverse performances. The clinical
domain-adapted model outperformed the more general biomedical domain-adapted
model. Moreover, we observed evidence of cross-lingual transferability.
Moreover, we also conducted further investigations to explore potential reasons
contributing to these performance differences. These findings highlight the
feasibility of domain adaptation and cross-lingual ability in medical NLP.
Within the low-resource language settings, these findings can provide
meaningful guidance for developing multilingual medical NLP systems to mitigate
the lack of training data and thereby improve the model performance.

</details>


### [40] [Data-Efficient Domain Adaptation for LLM-based MT using Contrastive Preference Optimization](https://arxiv.org/abs/2510.27556)
*Inacio Vieira,Antonio Castaldo,James O'Doherty,Sheila Castilho*

Main category: cs.CL

TL;DR: 本文研究了使用CPO模拟后编辑工作流程以实现数据高效的领域适应。


<details>
  <summary>Details</summary>
Motivation: LLMs通常需要适应特定领域的需求，而仅依靠SFT进行适应可能成本很高。

Method: 本文提出了一种使用CPO的方法，通过将基础模型的原始输出作为'拒绝'翻译，将人工批准的TM条目作为'选择'翻译，来合成偏好对。

Result: 实验表明，使用14.7k个偏好对，模型的性能接近于使用160k+样本进行SFT训练的模型。

Conclusion: 本文展示了CPO在生成任务中的有效性，特别是在数据效率方面。

Abstract: LLMs often require adaptation to domain-specific requirements, a process that
can be expensive when relying solely on SFT. We present an empirical study on
applying CPO to simulate a post-editing workflow for data-efficient domain
adaptation. Our approach synthesizes preference pairs by treating the base
model's own raw output as the 'rejected' translation and the human-approved TM
entry as the 'chosen' one. This method provides direct feedback on the model's
current knowledge, guiding it to align with domain-specific standards.
Experiments in English-Brazilian Portuguese and English-Korean show that, by
using just 14.7k preference pairs, the model achieves performance close to that
of a model trained on 160k+ samples with SFT, demonstrating significant data
efficiency. Although we showcase its effectiveness in MT, this application of
CPO naturally generalizes to other generative tasks where a model's initial
drafts can serve as a contrastive signal against a golden reference.

</details>


### [41] [MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool Agentic Retrieval](https://arxiv.org/abs/2510.27569)
*Qi Luo,Xiaonan Li,Yuxin Wang,Tingshuo Fan,Yuan Li,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的多工具RAG框架MARAG-R1，以解决现有RAG系统在信息检索方面的局限性。该框架通过动态协调多个检索机制，提高了信息获取的广度和精度，并在多个任务中取得了最佳结果。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统依赖于单一的检索器，限制了对语料库中相关信息的访问，这成为全面获取外部信息的主要瓶颈。

Method: MARAG-R1是一个基于强化学习的多工具RAG框架，它使LLMs能够动态协调多个检索机制以获得更广泛和精确的信息。该框架配备了四种检索工具，并通过两阶段训练过程学习如何以及何时使用它们。

Result: 在GlobalQA、HotpotQA和2WikiMultiHopQA上的实验表明，MARAG-R1显著优于强基线，并在语料库级推理任务中达到了新的最先进的结果。

Conclusion: MARAG-R1在需要语料库级推理的任务中表现出色，取得了新的最先进结果。

Abstract: Large Language Models (LLMs) excel at reasoning and generation but are
inherently limited by static pretraining data, resulting in factual
inaccuracies and weak adaptability to new information. Retrieval-Augmented
Generation (RAG) addresses this issue by grounding LLMs in external knowledge;
However, the effectiveness of RAG critically depends on whether the model can
adequately access relevant information. Existing RAG systems rely on a single
retriever with fixed top-k selection, restricting access to a narrow and static
subset of the corpus. As a result, this single-retriever paradigm has become
the primary bottleneck for comprehensive external information acquisition,
especially in tasks requiring corpus-level reasoning. To overcome this
limitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG
framework that enables LLMs to dynamically coordinate multiple retrieval
mechanisms for broader and more precise information access. MARAG-R1 equips the
model with four retrieval tools -- semantic search, keyword search, filtering,
and aggregation -- and learns both how and when to use them through a two-stage
training process: supervised fine-tuning followed by reinforcement learning.
This design allows the model to interleave reasoning and retrieval,
progressively gathering sufficient evidence for corpus-level synthesis.
Experiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that
MARAG-R1 substantially outperforms strong baselines and achieves new
state-of-the-art results in corpus-level reasoning tasks.

</details>


### [42] [SpecAttn: Speculating Sparse Attention](https://arxiv.org/abs/2510.27641)
*Harsh Shah*

Main category: cs.CL

TL;DR: SpecAttn is a training-free approach that improves the efficiency of large language models by reducing computational bottlenecks through sparse attention, achieving significant improvements in performance while maintaining output quality.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) face significant computational bottlenecks during inference due to the quadratic complexity of self-attention mechanisms, particularly as context lengths increase.

Method: SpecAttn is a training-free approach that integrates with existing speculative decoding techniques to enable efficient sparse attention in pre-trained transformers. It uses three core techniques: KL divergence-based layer alignment between draft and target models, a GPU-optimized sorting-free algorithm for top-p token selection from draft attention patterns, and dynamic key-value cache pruning guided by these predictions.

Result: SpecAttn achieves over 75% reduction in key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19 dataset, significantly outperforming existing sparse attention methods.

Conclusion: SpecAttn demonstrates that speculative execution can be enhanced to provide approximate verification without significant performance degradation.

Abstract: Large Language Models (LLMs) face significant computational bottlenecks
during inference due to the quadratic complexity of self-attention mechanisms,
particularly as context lengths increase. We introduce SpecAttn, a novel
training-free approach that seamlessly integrates with existing speculative
decoding techniques to enable efficient sparse attention in pre-trained
transformers. Our key insight is to exploit the attention weights already
computed by the draft model during speculative decoding to identify important
tokens for the target model, eliminating redundant computation while
maintaining output quality. SpecAttn employs three core techniques: KL
divergence-based layer alignment between draft and target models, a
GPU-optimized sorting-free algorithm for top-p token selection from draft
attention patterns, and dynamic key-value cache pruning guided by these
predictions. By leveraging the computational work already performed in standard
speculative decoding pipelines, SpecAttn achieves over 75% reduction in
key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19
dataset, significantly outperforming existing sparse attention methods. Our
approach demonstrates that speculative execution can be enhanced to provide
approximate verification without significant performance degradation.

</details>


### [43] [Culture Cartography: Mapping the Landscape of Cultural Knowledge](https://arxiv.org/abs/2510.27672)
*Caleb Ziems,William Held,Jane Yu,Amir Goldberg,David Grusky,Diyi Yang*

Main category: cs.CL

TL;DR: 本文提出了一种混合主动性方法 CultureCartography，并实现了名为 CultureExplorer 的工具，用于获取 LLM 缺乏的文化特定知识。实验表明，该工具比传统方法更有效，且在微调后显著提升了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: LLM 需要文化特定知识，而这些知识可能在预训练期间未被学习。传统的方法要么是研究人员定义挑战性问题，要么是用户主动生成数据。但这种方法可以通过混合主动性协作来改进，让用户指导过程以反映他们的文化，同时 LLM 引导过程朝向更具挑战性的问题。

Method: 提出了一种称为 CultureCartography 的混合主动性方法，并实现为名为 CultureExplorer 的工具。LLM 从低置信度答案的问题开始注释，使人类受访者能够填补这些知识空白并引导模型关注相关主题。

Result: 与人类回答 LLM 提出的问题的基线相比，CultureExplorer 更有效地产生了领先模型如 DeepSeek R1 和 GPT-4o 所缺少的知识，即使使用网络搜索。在这些数据上进行微调可以将 Llama-3.1-8B 在相关文化基准上的准确性提高高达 19.2%。

Conclusion: CultureExplorer 更有效地产生了领先的模型如 DeepSeek R1 和 GPT-4o 所缺少的知识，即使使用网络搜索。在这些数据上进行微调可以将 Llama-3.1-8B 在相关文化基准上的准确性提高高达 19.2%。

Abstract: To serve global users safely and productively, LLMs need culture-specific
knowledge that might not be learned during pre-training. How do we find such
knowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The
most common solutions are single-initiative: either researchers define
challenging questions that users passively answer (traditional annotation), or
users actively produce data that researchers structure as benchmarks (knowledge
extraction). The process would benefit from mixed-initiative collaboration,
where users guide the process to meaningfully reflect their cultures, and LLMs
steer the process towards more challenging questions that meet the researcher's
goals. We propose a mixed-initiative methodology called CultureCartography.
Here, an LLM initializes annotation with questions for which it has
low-confidence answers, making explicit both its prior knowledge and the gaps
therein. This allows a human respondent to fill these gaps and steer the model
towards salient topics through direct edits. We implement this methodology as a
tool called CultureExplorer. Compared to a baseline where humans answer
LLM-proposed questions, we find that CultureExplorer more effectively produces
knowledge that leading models like DeepSeek R1 and GPT-4o are missing, even
with web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B
by up to 19.2% on related culture benchmarks.

</details>


### [44] [Continuous Autoregressive Language Models](https://arxiv.org/abs/2510.27688)
*Chenze Shao,Darren Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: CALM 是一种新型语言模型，通过将标记压缩为连续向量，减少了生成步骤的数量，从而提高了效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的效率受到其顺序、逐标记生成过程的根本限制。为了克服这一瓶颈，需要一种新的设计轴线来扩展 LLM：增加每个生成步骤的语义带宽。

Method: CALM 引入了一种从离散的下一个标记预测到连续的下一个向量预测的范式转变。它使用高保真自编码器将 K 个标记压缩成一个连续向量，并从该向量中以超过 99.9% 的准确率重建原始标记。此外，开发了一个全面的概率自由框架，以在连续域中进行稳健的训练、评估和可控采样。

Result: 实验表明，CALM 显著提高了性能-计算权衡，在显著降低计算成本的情况下实现了与强大离散基线相当的性能。

Conclusion: CALM 的研究结果表明，下一步向量预测是一种强大且可扩展的路径，可以实现超高效的语言模型。

Abstract: The efficiency of large language models (LLMs) is fundamentally limited by
their sequential, token-by-token generation process. We argue that overcoming
this bottleneck requires a new design axis for LLM scaling: increasing the
semantic bandwidth of each generative step. To this end, we introduce
Continuous Autoregressive Language Models (CALM), a paradigm shift from
discrete next-token prediction to continuous next-vector prediction. CALM uses
a high-fidelity autoencoder to compress a chunk of K tokens into a single
continuous vector, from which the original tokens can be reconstructed with
over 99.9\% accuracy. This allows us to model language as a sequence of
continuous vectors instead of discrete tokens, which reduces the number of
generative steps by a factor of K. The paradigm shift necessitates a new
modeling toolkit; therefore, we develop a comprehensive likelihood-free
framework that enables robust training, evaluation, and controllable sampling
in the continuous domain. Experiments show that CALM significantly improves the
performance-compute trade-off, achieving the performance of strong discrete
baselines at a significantly lower computational cost. More importantly, these
findings establish next-vector prediction as a powerful and scalable pathway
towards ultra-efficient language models. Code:
https://github.com/shaochenze/calm. Project:
https://shaochenze.github.io/blog/2025/CALM.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [45] [Evaluating Perspectival Biases in Cross-Modal Retrieval](https://arxiv.org/abs/2510.26861)
*Teerapol Saengsukhiran,Peerawat Chomphooyod,Narabodee Rodjananant,Chompakorn Chaksangchaichot,Patawee Prakrankamanant,Witthawin Sripheanpol,Pak Lovichit,SarChaksaana Nutanong,Ekapol Chuangsuwanich*

Main category: cs.IR

TL;DR: 本文研究了多模态检索系统中的两种偏差：流行度偏差和关联偏差，并发现显式对齐可以有效缓解流行度偏差，但关联偏差更为复杂和具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 多模态检索系统应在一个语义空间中运行，不受查询的语言或文化起源的影响。然而，实际的检索结果会系统地反映视角偏差：由语言流行度和文化关联塑造的偏差。

Method: 研究了两种偏差：首先，流行度偏差指的是在图像到文本检索中倾向于选择流行语言的条目而不是语义忠实的条目；其次，关联偏差指的是在文本到图像检索中倾向于选择与查询有文化关联的图像而不是语义正确的图像。

Result: 结果表明，显式对齐是缓解流行度偏差更有效的策略。然而，关联偏差仍然是一个独特且更具挑战性的问题。

Conclusion: 这些发现表明，实现真正公平的多模态系统需要超越简单数据扩展的目标策略，并且来自文化关联的偏差可能比来自语言普遍性的偏差更难处理。

Abstract: Multimodal retrieval systems are expected to operate in a semantic space,
agnostic to the language or cultural origin of the query. In practice, however,
retrieval outcomes systematically reflect perspectival biases: deviations
shaped by linguistic prevalence and cultural associations. We study two such
biases. First, prevalence bias refers to the tendency to favor entries from
prevalent languages over semantically faithful entries in image-to-text
retrieval. Second, association bias refers to the tendency to favor images
culturally associated with the query over semantically correct ones in
text-to-image retrieval. Results show that explicit alignment is a more
effective strategy for mitigating prevalence bias. However, association bias
remains a distinct and more challenging problem. These findings suggest that
achieving truly equitable multimodal systems requires targeted strategies
beyond simple data scaling and that bias arising from cultural association may
be treated as a more challenging problem than one arising from linguistic
prevalence.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [46] [DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries](https://arxiv.org/abs/2510.27238)
*Chuxuan Hu,Maxwell Yang,James Weiland,Yeji Lim,Suhas Palawala,Daniel Kang*

Main category: cs.DB

TL;DR: DRAMA 是一种新的端到端范式，能够高效地处理开放域数据分析任务，并在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 手动进行现实世界的数据分析是劳动密集且低效的。现有的范式或系统未能完全展示支持有效分析所需的三个关键能力：(1) 开放域数据收集，(2) 结构化数据转换，(3) 分析推理。

Method: DRAMA 统一了数据收集、转换和分析作为单一管道。开发了 DRAMA-Bot，这是一个遵循 DRAMA 的多代理系统，包括一个数据检索器和一个数据分析师。

Result: DRAMA-Bot 在 DRAMA-Bench 上实现了 86.5% 的任务准确率，成本仅为 0.05 美元，比所有基线高出最多 6.9 倍的准确率，且成本不到 1/6。

Conclusion: DRAMA 是一个端到端的范式，能够在大规模开放域数据上回答用户的自然语言分析查询。它在 DRAMA-Bench 上表现出色，优于所有基线，并且成本更低。

Abstract: Manually conducting real-world data analyses is labor-intensive and
inefficient. Despite numerous attempts to automate data science workflows, none
of the existing paradigms or systems fully demonstrate all three key
capabilities required to support them effectively: (1) open-domain data
collection, (2) structured data transformation, and (3) analytic reasoning.
  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that
answers users' analytic queries in natural language on large-scale open-domain
data. DRAMA unifies data collection, transformation, and analysis as a single
pipeline. To quantitatively evaluate system performance on tasks representative
of DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories
of tasks: claim verification and question answering, each comprising 100
instances. These tasks are derived from real-world applications that have
gained significant public attention and require the retrieval and analysis of
open-domain data. We develop DRAMA-Bot, a multi-agent system designed following
DRAMA. It comprises a data retriever that collects and transforms data by
coordinating the execution of sub-agents, and a data analyzer that performs
structured reasoning over the retrieved data. We evaluate DRAMA-Bot on
DRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot
achieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines
with up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is
publicly available at https://github.com/uiuc-kang-lab/drama.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions](https://arxiv.org/abs/2510.26852)
*Lingyue Fu,Xin Ding,Yaoming Zhu,Shao Zhang,Lin Qiu,Weiwen Liu,Weinan Zhang,Xuezhi Cao,Xunliang Cai,Jiaxin Ding,Yong Yu*

Main category: cs.AI

TL;DR: 本文提出了一种迭代的、竞争性的同伴学习框架，并引入了CATArena，一个基于四种多样化的棋盘和卡牌游戏的竞赛式评估平台，以解决当前基准测试中评分饱和的问题。实验结果表明，CATArena能够可靠、稳定且可扩展地评估代理的核心能力，特别是学习能力和策略编码。


<details>
  <summary>Details</summary>
Motivation: Current benchmarks mainly assess end-to-end performance in fixed scenarios, restricting evaluation to specific skills and suffering from score saturation and growing dependence on expert annotation as agent capabilities improve.

Method: We propose an iterative, competitive peer-learning framework, which allows agents to refine and optimize their strategies through repeated interactions and feedback. We introduce CATArena, a tournament-style evaluation platform featuring four diverse board and card games with open-ended scoring.

Result: Experimental results and analyses involving both minimal and commercial code agents demonstrate that CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.

Conclusion: CATArena provides reliable, stable, and scalable benchmarking for core agent abilities, particularly learning ability and strategy coding.

Abstract: Large Language Model (LLM) agents have evolved from basic text generation to
autonomously completing complex tasks through interaction with external tools.
However, current benchmarks mainly assess end-to-end performance in fixed
scenarios, restricting evaluation to specific skills and suffering from score
saturation and growing dependence on expert annotation as agent capabilities
improve. In this work, we emphasize the importance of learning ability,
including both self-improvement and peer-learning, as a core driver for agent
evolution toward human-level intelligence. We propose an iterative, competitive
peer-learning framework, which allows agents to refine and optimize their
strategies through repeated interactions and feedback, thereby systematically
evaluating their learning capabilities. To address the score saturation issue
in current benchmarks, we introduce CATArena, a tournament-style evaluation
platform featuring four diverse board and card games with open-ended scoring.
By providing tasks without explicit upper score limits, CATArena enables
continuous and dynamic evaluation of rapidly advancing agent capabilities.
Experimental results and analyses involving both minimal and commercial code
agents demonstrate that CATArena provides reliable, stable, and scalable
benchmarking for core agent abilities, particularly learning ability and
strategy coding.

</details>


### [48] [The Denario project: Deep knowledge AI agents for scientific discovery](https://arxiv.org/abs/2510.26887)
*Francisco Villaescusa-Navarro,Boris Bolliet,Pablo Villanueva-Domingo,Adrian E. Bayer,Aidan Acquah,Chetana Amancharla,Almog Barzilay-Siegal,Pablo Bermejo,Camille Bilodeau,Pablo Cárdenas Ramírez,Miles Cranmer,Urbano L. França,ChangHoon Hahn,Yan-Fei Jiang,Raul Jimenez,Jun-Young Lee,Antonio Lerario,Osman Mamun,Thomas Meier,Anupam A. Ojha,Pavlos Protopapas,Shimanto Roy,David N. Spergel,Pedro Tarancón-Álvarez,Ujjwal Tiwari,Matteo Viel,Digvijay Wadekar,Chi Wang,Bonny Y. Wang,Licong Xu,Yossi Yovel,Shuwen Yue,Wen-Han Zhou,Qiyao Zhu,Jiajun Zou,Íñigo Zubeldia*

Main category: cs.AI

TL;DR: 本文介绍了Denario，一个用于科学研究的人工智能多代理系统，展示了其在多个科学领域的应用，并讨论了其优缺点和伦理影响。


<details>
  <summary>Details</summary>
Motivation: 本文旨在介绍Denario系统，一个用于科学研究的人工智能多代理系统。

Method: 本文详细描述了Denario及其模块，并通过多个AI生成的论文展示了其能力。

Result: Denario能够执行多种任务，如生成想法、检查文献、制定研究计划、编写和执行代码、制作图表以及撰写和审查科学论文。

Conclusion: 本文介绍了Denario系统，并展示了其在多个科学领域的应用，同时讨论了该系统的优缺点和伦理影响。

Abstract: We present Denario, an AI multi-agent system designed to serve as a
scientific research assistant. Denario can perform many different tasks, such
as generating ideas, checking the literature, developing research plans,
writing and executing code, making plots, and drafting and reviewing a
scientific paper. The system has a modular architecture, allowing it to handle
specific tasks, such as generating an idea, or carrying out end-to-end
scientific analysis using Cmbagent as a deep-research backend. In this work, we
describe in detail Denario and its modules, and illustrate its capabilities by
presenting multiple AI-generated papers generated by it in many different
scientific disciplines such as astrophysics, biology, biophysics, biomedical
informatics, chemistry, material science, mathematical physics, medicine,
neuroscience and planetary science. Denario also excels at combining ideas from
different disciplines, and we illustrate this by showing a paper that applies
methods from quantum physics and machine learning to astrophysical data. We
report the evaluations performed on these papers by domain experts, who
provided both numerical scores and review-like feedback. We then highlight the
strengths, weaknesses, and limitations of the current system. Finally, we
discuss the ethical implications of AI-driven research and reflect on how such
technology relates to the philosophy of science. We publicly release the code
at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run
directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and
the full app will be deployed on the cloud.

</details>


### [49] [Glia: A Human-Inspired AI for Automated Systems Design and Optimization](https://arxiv.org/abs/2510.27176)
*Pouya Hamadanian,Pantea Karimi,Arash Nasr-Esfahany,Kimia Noorbakhsh,Joseph Chandler,Ali ParandehGheibi,Mohammad Alizadeh,Hari Balakrishnan*

Main category: cs.AI

TL;DR: Glia是一种利用大型语言模型进行多代理协作的人工智能架构，能够在分布式GPU集群上生成高性能且可解释的系统设计。


<details>
  <summary>Details</summary>
Motivation: 研究AI是否能够自主设计计算机系统，达到人类专家的创造力和推理能力。

Method: Glia是一种用于网络化系统设计的人工智能架构，它使用大型语言模型（LLMs）进行人类启发式的多代理工作流程。每个代理专门从事推理、实验和分析，并通过一个评估框架进行协作，将抽象推理与实证反馈结合。

Result: 当应用于用于LLM推理的分布式GPU集群时，Glia生成了新的请求路由、调度和自动扩展算法，在显著更短的时间内表现出与人类专家相当的性能，并提供了对工作负载行为的新见解。

Conclusion: 我们的结果表明，通过将推理大型语言模型与结构化实验相结合，人工智能可以为复杂系统问题生成创造性和可理解的设计。

Abstract: Can an AI autonomously design mechanisms for computer systems on par with the
creativity and reasoning of human experts? We present Glia, an AI architecture
for networked systems design that uses large language models (LLMs) in a
human-inspired, multi-agent workflow. Each agent specializes in reasoning,
experimentation, and analysis, collaborating through an evaluation framework
that grounds abstract reasoning in empirical feedback. Unlike prior
ML-for-systems methods that optimize black-box policies, Glia generates
interpretable designs and exposes its reasoning process. When applied to a
distributed GPU cluster for LLM inference, it produces new algorithms for
request routing, scheduling, and auto-scaling that perform at human-expert
levels in significantly less time, while yielding novel insights into workload
behavior. Our results suggest that by combining reasoning LLMs with structured
experimentation, an AI can produce creative and understandable designs for
complex systems problems.

</details>


### [50] [DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains](https://arxiv.org/abs/2510.27419)
*Tian Liang,Wenxiang Jiao,Zhiwei He,Jiahao Xu,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: 本文介绍了 DeepCompress，这是一种新的框架，可以在提高大型推理模型的准确性的同时提高其效率。通过自适应长度奖励机制，DeepCompress 能够根据问题的难度动态调整推理路径的长度，从而在保持高准确性的同时提高令牌效率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在提高效率的同时往往以牺牲准确性为代价。本文旨在解决这一问题，提出一种能够同时提高准确性和效率的新框架。

Method: DeepCompress 采用了一种自适应长度奖励机制，根据模型的演变能力实时将问题分类为“简单”或“困难”。它鼓励对“简单”问题进行更短、更高效的推理，同时促进对“困难”问题进行更长、更具探索性的思维链。

Result: 实验结果表明，DeepCompress 在具有挑战性的数学基准测试中始终优于基线方法，实现了更高的准确性并显著提高了令牌效率。

Conclusion: DeepCompress 是一种新颖的框架，可以同时提高大型推理模型的准确性和效率。实验结果表明，DeepCompress 在具有挑战性的数学基准测试中始终优于基线方法，实现了更高的准确性并显著提高了令牌效率。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive capabilities but
suffer from cognitive inefficiencies like ``overthinking'' simple problems and
``underthinking'' complex ones. While existing methods that use supervised
fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can
improve efficiency, they often do so at the cost of accuracy. This paper
introduces \textbf{DeepCompress}, a novel framework that simultaneously
enhances both the accuracy and efficiency of LRMs. We challenge the prevailing
approach of consistently favoring shorter reasoning paths, showing that longer
responses can contain a broader range of correct solutions for difficult
problems. DeepCompress employs an adaptive length reward mechanism that
dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on
the model's evolving capability. It encourages shorter, more efficient
reasoning for ``Simple'' problems while promoting longer, more exploratory
thought chains for ``Hard'' problems. This dual-reward strategy enables the
model to autonomously adjust its Chain-of-Thought (CoT) length, compressing
reasoning for well-mastered problems and extending it for those it finds
challenging. Experimental results on challenging mathematical benchmarks show
that DeepCompress consistently outperforms baseline methods, achieving superior
accuracy while significantly improving token efficiency.

</details>


### [51] [SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic Mathematical Reasoning](https://arxiv.org/abs/2510.27568)
*Ali Asgarov,Umid Suleymanov,Aadyant Khatri*

Main category: cs.AI

TL;DR: 本文提出了一种名为SIGMA的框架，用于增强数学推理问题的解决能力。该框架通过多智能体和按需知识整合，显著提高了推理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决数学推理问题不仅需要准确访问相关知识，还需要仔细的多步骤思考。然而，当前的检索增强模型通常依赖于单一视角，遵循不灵活的搜索策略，并且难以有效地结合多个来源的信息。

Method: 我们引入了SIGMA（Search-Augmented On-Demand Knowledge Integration for AGentic Mathematical reAsoning），一个统一的框架，通过调节机制协调专门的代理，独立推理、执行有针对性的搜索，并合成发现。每个代理生成假设段落以优化其分析视角的检索，确保知识整合既上下文敏感又计算高效。

Result: 在MATH500、AIME和PhD级科学QA GPQA等具有挑战性的基准测试中，SIGMA始终优于开源和闭源系统，绝对性能提高了7.4%。

Conclusion: 我们的结果表明，多智能体、按需知识整合显著提高了推理准确性和效率，为复杂、知识密集型问题解决提供了一种可扩展的方法。

Abstract: Solving mathematical reasoning problems requires not only accurate access to
relevant knowledge but also careful, multi-step thinking. However, current
retrieval-augmented models often rely on a single perspective, follow
inflexible search strategies, and struggle to effectively combine information
from multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge
Integration for AGentic Mathematical reAsoning), a unified framework that
orchestrates specialized agents to independently reason, perform targeted
searches, and synthesize findings through a moderator mechanism. Each agent
generates hypothetical passages to optimize retrieval for its analytic
perspective, ensuring knowledge integration is both context-sensitive and
computation-efficient. When evaluated on challenging benchmarks such as
MATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms
both open- and closed-source systems, achieving an absolute performance
improvement of 7.4%. Our results demonstrate that multi-agent, on-demand
knowledge integration significantly enhances both reasoning accuracy and
efficiency, offering a scalable approach for complex, knowledge-intensive
problem-solving. We will release the code upon publication.

</details>


### [52] [Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning](https://arxiv.org/abs/2510.27623)
*Qiusi Zhan,Hyeonjeong Ha,Rui Yang,Sirui Xu,Hanyang Chen,Liang-Yan Gui,Yu-Xiong Wang,Huan Zhang,Heng Ji,Daniel Kang*

Main category: cs.AI

TL;DR: BEAT是第一个使用环境中的物体作为触发器向MLLM基础的具身代理注入视觉后门的框架，实现了高攻击成功率并保持了良好的任务性能。


<details>
  <summary>Details</summary>
Motivation: 由于视觉后门攻击为基于视觉的具身代理带来了新的攻击面，因此需要一种方法来注入这些后门以评估安全风险。

Method: BEAT通过构建涵盖多种场景、任务和触发器放置的训练集，并引入两阶段训练方案（包括监督微调和对比触发学习）来解决对象触发的挑战。

Result: BEAT在各种具身代理基准和MLLM中实现了高达80%的攻击成功率，同时保持了强大的良性任务性能，并能可靠地推广到分布外的触发器放置。

Conclusion: BEAT暴露了MLLM基础的具身代理中的一个关键但未被探索的安全风险，强调在现实世界部署之前需要强大的防御措施。

Abstract: Multimodal large language models (MLLMs) have advanced embodied agents by
enabling direct perception, reasoning, and planning task-oriented actions from
visual inputs. However, such vision driven embodied agents open a new attack
surface: visual backdoor attacks, where the agent behaves normally until a
visual trigger appears in the scene, then persistently executes an
attacker-specified multi-step policy. We introduce BEAT, the first framework to
inject such visual backdoors into MLLM-based embodied agents using objects in
the environments as triggers. Unlike textual triggers, object triggers exhibit
wide variation across viewpoints and lighting, making them difficult to implant
reliably. BEAT addresses this challenge by (1) constructing a training set that
spans diverse scenes, tasks, and trigger placements to expose agents to trigger
variability, and (2) introducing a two-stage training scheme that first applies
supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning
(CTL). CTL formulates trigger discrimination as preference learning between
trigger-present and trigger-free inputs, explicitly sharpening the decision
boundaries to ensure precise backdoor activation. Across various embodied agent
benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while
maintaining strong benign task performance, and generalizes reliably to
out-of-distribution trigger placements. Notably, compared to naive SFT, CTL
boosts backdoor activation accuracy up to 39% under limited backdoor data.
These findings expose a critical yet unexplored security risk in MLLM-based
embodied agents, underscoring the need for robust defenses before real-world
deployment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [53] [RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification](https://arxiv.org/abs/2510.26935)
*Yunhao Yang,Neel P. Bhatt,Pranay Samineni,Rohan Siva,Zhanyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: RepV是一种神经符号验证器，它通过学习一个潜在空间，其中安全和不安全的计划是线性可分的，统一了两种观点。它从一个小型种子集开始，由现成的模型检查器标记的计划，然后训练一个轻量级投影器，将每个计划以及语言模型生成的推理嵌入到低维空间中；然后通过一个冻结的线性边界进行验证。此外，RepV提供了一个概率保证，基于其在潜在空间中的位置，这使得规划器的保证驱动改进成为可能。实证评估显示，RepV在合规预测准确性方面比基线方法提高了高达15%，同时添加的参数少于0.2M。此外，我们的微调框架在各种规划领域中优于普通的微调基线。这些结果表明，安全可分离的潜在空间为可靠的神经符号计划验证提供了一个可扩展、即插即用的基本单元。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统迁移到安全关键领域，验证其行动是否符合明确定义的规则仍然是一个挑战。形式方法提供了可证明的保证，但需要手工编写的时态逻辑规范，表达能力和可访问性有限。深度学习方法可以评估计划是否符合自然语言约束，但其不透明的决策过程可能导致严重后果的误分类。

Method: RepV通过学习一个潜在空间，其中安全和不安全的计划是线性可分的，统一了这两种观点。它从一个小型种子集开始，由现成的模型检查器标记的计划，然后训练一个轻量级投影器，将每个计划以及语言模型生成的推理嵌入到低维空间中；然后通过一个冻结的线性边界进行验证。

Result: Empirical evaluations show that RepV improves compliance prediction accuracy by up to 15% compared to baseline methods while adding fewer than 0.2M parameters. Furthermore, our refinement framework outperforms ordinary fine-tuning baselines across various planning domains.

Conclusion: 安全可分离的潜在空间为可靠的神经符号计划验证提供了一个可扩展、即插即用的基本单元。

Abstract: As AI systems migrate to safety-critical domains, verifying that their
actions comply with well-defined rules remains a challenge. Formal methods
provide provable guarantees but demand hand-crafted temporal-logic
specifications, offering limited expressiveness and accessibility. Deep
learning approaches enable evaluation of plans against natural-language
constraints, yet their opaque decision process invites misclassifications with
potentially severe consequences. We introduce RepV, a neurosymbolic verifier
that unifies both views by learning a latent space where safe and unsafe plans
are linearly separable. Starting from a modest seed set of plans labeled by an
off-the-shelf model checker, RepV trains a lightweight projector that embeds
each plan, together with a language model-generated rationale, into a
low-dimensional space; a frozen linear boundary then verifies compliance for
unseen natural-language rules in a single forward pass.
  Beyond binary classification, RepV provides a probabilistic guarantee on the
likelihood of correct verification based on its position in the latent space.
This guarantee enables a guarantee-driven refinement of the planner, improving
rule compliance without human annotations. Empirical evaluations show that RepV
improves compliance prediction accuracy by up to 15% compared to baseline
methods while adding fewer than 0.2M parameters. Furthermore, our refinement
framework outperforms ordinary fine-tuning baselines across various planning
domains. These results show that safety-separable latent spaces offer a
scalable, plug-and-play primitive for reliable neurosymbolic plan verification.
Code and data are available at: https://repv-project.github.io/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [54] [Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token](https://arxiv.org/abs/2510.26847)
*Shaked Zychlinski,Yuval Kainan*

Main category: cs.CR

TL;DR: CPT-Filtering is a novel, model-agnostic guardrail technique that uses the average number of Characters Per Token (CPT) to identify encoded text and mitigate jailbreak attacks against large language models.


<details>
  <summary>Details</summary>
Motivation: The high compute cost of modern methods, such as relying on added modules like dedicated LLMs or perplexity models, motivates the need for a more efficient and effective guardrail technique.

Method: CPT-Filtering leverages the intrinsic behavior of Byte-Pair Encoding (BPE) tokenizers by using the average number of Characters Per Token (CPT) in the text to identify encoded text.

Result: Experiments demonstrate that a simple CPT threshold robustly identifies encoded text with high accuracy, even for very short inputs.

Conclusion: CPT-Filtering provides a practical defense layer that can be immediately deployed for real-time text filtering and offline data curation.

Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where
malicious prompts are disguised using ciphers and character-level encodings to
bypass safety guardrails. While these guardrails often fail to interpret the
encoded content, the underlying models can still process the harmful
instructions. We introduce CPT-Filtering, a novel, model-agnostic with
negligible-costs and near-perfect accuracy guardrail technique that aims to
mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair
Encoding (BPE) tokenizers. Our method is based on the principle that
tokenizers, trained on natural language, represent out-of-distribution text,
such as ciphers, using a significantly higher number of shorter tokens. Our
technique uses a simple yet powerful artifact of using language models: the
average number of Characters Per Token (CPT) in the text. This approach is
motivated by the high compute cost of modern methods - relying on added modules
such as dedicated LLMs or perplexity models. We validate our approach across a
large dataset of over 100,000 prompts, testing numerous encoding schemes with
several popular tokenizers. Our experiments demonstrate that a simple CPT
threshold robustly identifies encoded text with high accuracy, even for very
short inputs. CPT-Filtering provides a practical defense layer that can be
immediately deployed for real-time text filtering and offline data curation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [55] [Semantic Frame Aggregation-based Transformer for Live Video Comment Generation](https://arxiv.org/abs/2510.26978)
*Anam Fatima,Yi Yu,Janak Kapuriya,Julien Lalanne,Jainendra Shukla*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频评论生成模型SFAT，并构建了一个大规模的多模态英文视频评论数据集，以提高实时视频评论的相关性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法忽略了视频帧的重要性排序，这在生成合适的评论中至关重要。此外，现有数据集主要关注中文内容，且视频类别有限。

Method: 本文提出了一种基于语义相关性的视频帧加权方法，并结合CLIP的多模态知识生成评论。此外，还使用了带有交叉注意力机制的评论解码器，以确保生成的评论反映聊天和视频中的上下文线索。

Result: 本文构建了一个包含11个视频类别、438小时和320万条评论的大规模多模态英文视频评论数据集。实验结果显示，SFAT模型在生成与上下文相关的评论方面优于现有方法。

Conclusion: 本文提出了一个基于语义帧聚合的Transformer模型（SFAT），用于实时视频评论生成，并构建了一个大规模、多样化的多模态英文视频评论数据集。实验结果表明，该模型在生成与上下文相关的评论方面优于现有方法。

Abstract: Live commenting on video streams has surged in popularity on platforms like
Twitch, enhancing viewer engagement through dynamic interactions. However,
automatically generating contextually appropriate comments remains a
challenging and exciting task. Video streams can contain a vast amount of data
and extraneous content. Existing approaches tend to overlook an important
aspect of prioritizing video frames that are most relevant to ongoing viewer
interactions. This prioritization is crucial for producing contextually
appropriate comments. To address this gap, we introduce a novel Semantic Frame
Aggregation-based Transformer (SFAT) model for live video comment generation.
This method not only leverages CLIP's visual-text multimodal knowledge to
generate comments but also assigns weights to video frames based on their
semantic relevance to ongoing viewer conversation. It employs an efficient
weighted sum of frames technique to emphasize informative frames while focusing
less on irrelevant ones. Finally, our comment decoder with a cross-attention
mechanism that attends to each modality ensures that the generated comment
reflects contextual cues from both chats and video. Furthermore, to address the
limitations of existing datasets, which predominantly focus on Chinese-language
content with limited video categories, we have constructed a large scale,
diverse, multimodal English video comments dataset. Extracted from Twitch, this
dataset covers 11 video categories, totaling 438 hours and 3.2 million
comments. We demonstrate the effectiveness of our SFAT model by comparing it to
existing methods for generating comments from live video and ongoing dialogue
contexts.

</details>


### [56] [Can MLLMs Read the Room? A Multimodal Benchmark for Verifying Truthfulness in Multi-Party Social Interactions](https://arxiv.org/abs/2510.27195)
*Caixin Kang,Yifei Huang,Liangyang Ouyang,Mingfang Zhang,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文介绍了多模态交互真实性评估（MIVA）任务和一个从狼人杀游戏中衍生的多模态数据集，评估了最先进的MLLMs，发现它们在区分真相与虚假方面存在显著性能差距，并指出需要新的方法来构建更敏锐和可信的AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地融入人类生活，赋予它们强大的社会智能已成为一个关键前沿。辨别真相与欺骗是人类互动中的普遍元素，通过语言和非语言视觉线索的复杂相互作用传达。然而，在动态、多方对话中自动检测欺骗仍然是一个重大挑战。最近，具有强大视觉和文本理解能力的多模态大语言模型（MLLMs）的兴起使它们成为这一任务的自然候选者。因此，它们在这一关键领域的能力大多未被量化。

Method: 我们引入了一个新的任务，即多模态交互真实性评估（MIVA），并展示了一个从社交推理游戏“狼人杀”中衍生的新型多模态数据集。该数据集提供了同步的视频和文本，并为每个陈述提供了可验证的真实标签。我们建立了一个全面的基准，评估最先进的MLLMs。

Result: 我们建立了一个全面的基准，评估最先进的MLLMs，揭示了一个显著的性能差距：即使像GPT-4o这样的强大模型也难以可靠地区分真相与虚假。

Conclusion: 我们的分析表明，这些模型在将语言与视觉社会线索结合方面存在不足，并可能在对齐上过于保守，这突显了构建更敏锐和可信AI系统的迫切需求。

Abstract: As AI systems become increasingly integrated into human lives, endowing them
with robust social intelligence has emerged as a critical frontier. A key
aspect of this intelligence is discerning truth from deception, a ubiquitous
element of human interaction that is conveyed through a complex interplay of
verbal language and non-verbal visual cues. However, automatic deception
detection in dynamic, multi-party conversations remains a significant
challenge. The recent rise of powerful Multimodal Large Language Models
(MLLMs), with their impressive abilities in visual and textual understanding,
makes them natural candidates for this task. Consequently, their capabilities
in this crucial domain are mostly unquantified. To address this gap, we
introduce a new task, Multimodal Interactive Veracity Assessment (MIVA), and
present a novel multimodal dataset derived from the social deduction game
Werewolf. This dataset provides synchronized video, text, with verifiable
ground-truth labels for every statement. We establish a comprehensive benchmark
evaluating state-of-the-art MLLMs, revealing a significant performance gap:
even powerful models like GPT-4o struggle to distinguish truth from falsehood
reliably. Our analysis of failure modes indicates that these models fail to
ground language in visual social cues effectively and may be overly
conservative in their alignment, highlighting the urgent need for novel
approaches to building more perceptive and trustworthy AI systems.

</details>


### [57] [Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum](https://arxiv.org/abs/2510.27571)
*Zhuoning Guo,Mingxin Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Xiaowen Chu*

Main category: cs.CV

TL;DR: 本文提出了一种基于评估、数据和建模协同设计的框架，以解决当前视频检索范式的局限性。通过建立一个诊断性基准和生成高质量数据对，以及设计一种新的训练课程，实现了更强大的视频检索能力。


<details>
  <summary>Details</summary>
Motivation: 当前的视频检索范式在结构上存在偏差，狭窄的基准测试鼓励相应有限的数据和单任务训练。因此，由于缺乏定义和要求多维泛化的诊断评估，通用能力被抑制。为了打破这种循环，我们需要一个框架来解决这些问题。

Method: 我们引入了一个框架，该框架基于评估、数据和建模的协同设计。首先，我们建立了通用视频检索基准（UVRB），一个包含16个数据集的套件，不仅用于测量性能，还用于诊断任务和领域中的关键能力差距。其次，根据UVRB的诊断，我们引入了一个可扩展的合成工作流，生成155万个高质量对来填充实现普遍性的语义空间。最后，我们设计了模态金字塔，这是一种课程，通过显式利用我们多样化数据中的潜在相互联系来训练我们的通用视频嵌入器（GVE）。

Result: 广泛的实验表明，GVE在UVRB上实现了最先进的零样本泛化。特别是，我们的分析表明，流行的基准测试是通用能力的不良预测者，并且部分相关的检索是一个主导但被忽视的场景。

Conclusion: 我们的共同设计框架提供了一条实用的路径，以摆脱有限范围并迈向真正的通用视频检索。

Abstract: The prevailing video retrieval paradigm is structurally misaligned, as narrow
benchmarks incentivize correspondingly limited data and single-task training.
Therefore, universal capability is suppressed due to the absence of a
diagnostic evaluation that defines and demands multi-dimensional
generalization. To break this cycle, we introduce a framework built on the
co-design of evaluation, data, and modeling. First, we establish the Universal
Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to
measure performance but also to diagnose critical capability gaps across tasks
and domains. Second, guided by UVRB's diagnostics, we introduce a scalable
synthesis workflow that generates 1.55 million high-quality pairs to populate
the semantic space required for universality. Finally, we devise the Modality
Pyramid, a curriculum that trains our General Video Embedder (GVE) by
explicitly leveraging the latent interconnections within our diverse data.
Extensive experiments show GVE achieves state-of-the-art zero-shot
generalization on UVRB. In particular, our analysis reveals that popular
benchmarks are poor predictors of general ability and that partially relevant
retrieval is a dominant but overlooked scenario. Overall, our co-designed
framework provides a practical path to escape the limited scope and advance
toward truly universal video retrieval.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [Towards a Measure of Algorithm Similarity](https://arxiv.org/abs/2510.27063)
*Shairoz Sohail,Taher Ali*

Main category: cs.LG

TL;DR: 本文提出了EMOC框架，用于评估算法的相似性，并构建了PACD数据集，以支持算法类型聚类、近似重复检测和LLM生成程序的多样性量化。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，如克隆检测或程序合成，需要一种务实且一致的相似性度量标准。然而，现有的等价性和相似性概念存在竞争，使得实际应用变得复杂。

Method: EMOC框架通过将算法实现嵌入到适合下游任务的特征空间中，来评估算法的相似性。此外，还构建了一个名为PACD的数据集，用于验证Python实现的算法。

Result: EMOC特征支持算法类型的聚类和分类、近似重复检测以及LLM生成程序的多样性量化。

Conclusion: EMOC框架能够支持算法类型的聚类和分类、近似重复检测以及LLM生成程序的多样性量化，为算法相似性研究提供了新的方法。

Abstract: Given two algorithms for the same problem, can we determine whether they are
meaningfully different? In full generality, the question is uncomputable, and
empirically it is muddied by competing notions of similarity. Yet, in many
applications (such as clone detection or program synthesis) a pragmatic and
consistent similarity metric is necessary. We review existing equivalence and
similarity notions and introduce EMOC: An
Evaluation-Memory-Operations-Complexity framework that embeds algorithm
implementations into a feature space suitable for downstream tasks. We compile
PACD, a curated dataset of verified Python implementations across three
problems, and show that EMOC features support clustering and classification of
algorithm types, detection of near-duplicates, and quantification of diversity
in LLM-generated programs. Code, data, and utilities for computing EMOC
embeddings are released to facilitate reproducibility and future work on
algorithm similarity.

</details>


### [59] [Higher-order Linear Attention](https://arxiv.org/abs/2510.27258)
*Yifan Zhang,Zhen Qin,Quanquan Gu*

Main category: cs.LG

TL;DR: HLA 是一种新型的高阶线性注意力机制，能够在保持高效的同时实现更复杂的交互。


<details>
  <summary>Details</summary>
Motivation: 解决缩放点积注意力的二次成本问题，使自回归语言模型能够扩展到长上下文。

Method: 引入了高阶线性注意力（HLA），这是一种因果流机制，通过紧凑的前缀充分统计量实现更高阶的交互。

Result: 在二阶情况下，HLA 保持恒定大小的状态，并以线性时间计算每个标记的输出，而无需生成任何 n×n 矩阵。

Conclusion: HLA 是一种有原则的、可扩展的构建块，结合了类似注意力的数据依赖混合与现代递归架构的效率。

Abstract: The quadratic cost of scaled dot-product attention is a central obstacle to
scaling autoregressive language models to long contexts. Linear-time attention
and State Space Models (SSMs) provide scalable alternatives but are typically
restricted to first-order or kernel-based approximations, which can limit
expressivity. We introduce Higher-order Linear Attention (HLA), a causal,
streaming mechanism that realizes higher interactions via compact prefix
sufficient statistics. In the second-order case, HLA maintains a constant-size
state and computes per-token outputs in linear time without materializing any
$n \times n$ matrices. We give closed-form streaming identities, a strictly
causal masked variant using two additional summaries, and a chunk-parallel
training scheme based on associative scans that reproduces the activations of a
serial recurrence exactly. We further outline extensions to third and higher
orders. Collectively, these results position HLA as a principled, scalable
building block that combines attention-like, data-dependent mixing with the
efficiency of modern recurrent architectures. Project Page:
https://github.com/yifanzhang-pro/HLA.

</details>


### [60] [Un-Attributability: Computing Novelty From Retrieval & Semantic Similarity](https://arxiv.org/abs/2510.27313)
*Philipp Davydov,Ameya Prabhu,Matthias Bethge,Elisa Nguyen,Seong Joon Oh*

Main category: cs.LG

TL;DR: 本文重新围绕不可归因性来评估新颖性，提出了一种新的方法来检测模型输出是否具有语义新颖性。通过使用两阶段检索管道，我们发现模型在更长的跨度上依赖预训练数据，并且某些领域会影响新颖性。此外，指令调优不仅改变了模型的风格，还增加了其输出的新颖性。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型输出与预训练语料库之间的关系对于研究模型行为至关重要。大多数训练数据归因（TDA）方法询问哪些训练示例因果地影响给定的输出，通常使用排除单个示例的测试。我们反转了这个问题：哪些输出无法归因于任何预训练示例？

Method: 我们引入了不可归因性作为语义新颖性的操作性度量：如果预训练语料库中没有语义相似的上下文，则输出是新颖的。我们使用一个简单的两阶段检索管道来近似这一点：用轻量级GIST嵌入对语料库进行索引，检索前n个候选者，然后用ColBERTv2重新排序。如果最近的语料库项的可归因性低于人类生成的文本参考，我们将模型的输出视为新颖的。

Result: 我们在SmolLM和SmolLM2上进行了评估，并报告了三个发现：(1) 模型比之前报告的更长跨度的预训练数据；(2) 一些领域系统地促进或抑制新颖性；(3) 指令调优不仅改变了风格，还增加了新颖性。

Conclusion: 重新围绕不可归因性来评估新颖性，使得在预训练规模上进行高效分析成为可能。我们发布了约20 TB的语料库块和索引工件，以支持复制和大规模扩展我们的分析。

Abstract: Understanding how language-model outputs relate to the pretraining corpus is
central to studying model behavior. Most training data attribution (TDA)
methods ask which training examples causally influence a given output, often
using leave-one-out tests. We invert the question: which outputs cannot be
attributed to any pretraining example? We introduce un-attributability as an
operational measure of semantic novelty: an output is novel if the pretraining
corpus contains no semantically similar context. We approximate this with a
simple two-stage retrieval pipeline: index the corpus with lightweight GIST
embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the
nearest corpus item is less attributable than a human-generated text reference,
we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2
and report three findings: (1) models draw on pretraining data across much
longer spans than previously reported; (2) some domains systematically promote
or suppress novelty; and (3) instruction tuning not only alters style but also
increases novelty. Reframing novelty assessment around un-attributability
enables efficient analysis at pretraining scale. We release ~20 TB of corpus
chunks and index artifacts to support replication and large-scale extension of
our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm

</details>


### [61] [Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity](https://arxiv.org/abs/2510.27378)
*Austin Meek,Eitan Sprejer,Iván Arcuschin,Austin J. Brockmeier,Steven Basart*

Main category: cs.LG

TL;DR: 本文探讨了链式思维输出的忠实度和冗长度，提出了一个综合的监控分数，用于评估模型作为外部工作记忆的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的代理方法在检查模型在添加提示后更改答案的情况下发现了一些不忠实的情况，但在模型保持答案时会丢失信息，并且不研究与提示无关的推理方面。

Method: 引入了冗长度（verbosity）的概念，即CoT是否列出了完成任务所需的所有因素，并将忠实度和冗长度结合成一个监控分数。

Result: 评估了指令调整和推理模型在BBH、GPQA和MMLU上的表现，结果表明模型可能看起来是忠实的，但当它们遗漏了关键因素时，仍然难以监控，并且监控能力在不同模型家族之间存在显著差异。

Conclusion: 模型可能看起来是忠实的，但当它们遗漏了关键因素时，仍然难以监控，并且不同模型家族之间的监控能力差异很大。

Abstract: Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning.
Since any long, serial reasoning process must pass through this textual trace,
the quality of the CoT is a direct window into what the model is thinking. This
visibility could help us spot unsafe or misaligned behavior (monitorability),
but only if the CoT is transparent about its internal reasoning (faithfulness).
Fully measuring faithfulness is difficult, so researchers often focus on
examining the CoT in cases where the model changes its answer after adding a
cue to the input. This proxy finds some instances of unfaithfulness but loses
information when the model maintains its answer, and does not investigate
aspects of reasoning not tied to the cue. We extend these results to a more
holistic sense of monitorability by introducing verbosity: whether the CoT
lists every factor needed to solve the task. We combine faithfulness and
verbosity into a single monitorability score that shows how well the CoT serves
as the model's external `working memory', a property that many safety schemes
based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning
models on BBH, GPQA, and MMLU. Our results show that models can appear faithful
yet remain hard to monitor when they leave out key factors, and that
monitorability differs sharply across model families. We release our evaluation
code using the Inspect library to support reproducible future work.

</details>


### [62] [Atlas-Alignment: Making Interpretability Transferable Across Language Models](https://arxiv.org/abs/2510.27413)
*Bruno Puri,Jim Berend,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: Atlas-Alignment 是一种框架，通过将未知潜在空间与概念图谱对齐，使语言模型的可解释性转移成为可能，从而降低了可解释AI和机制可解释性的成本。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释性流程成本高昂且难以扩展，需要为每个新模型进行昂贵的训练、手动或半自动标记以及验证。

Method: Atlas-Alignment 框架通过将未知的潜在空间与概念图谱对齐，使用仅共享输入和轻量级表示对齐技术来实现跨语言模型的可解释性转移。

Result: 通过定量和定性评估，我们展示了简单的表示对齐方法可以在不需要标记概念数据的情况下实现稳健的语义检索和可调节生成。

Conclusion: Atlas-Alignment 通过投资一个高质量的概念图谱，使许多新模型在最小的边际成本下变得透明和可控。

Abstract: Interpretability is crucial for building safe, reliable, and controllable
language models, yet existing interpretability pipelines remain costly and
difficult to scale. Interpreting a new model typically requires costly training
of model-specific sparse autoencoders, manual or semi-automated labeling of SAE
components, and their subsequent validation. We introduce Atlas-Alignment, a
framework for transferring interpretability across language models by aligning
unknown latent spaces to a Concept Atlas - a labeled, human-interpretable
latent space - using only shared inputs and lightweight representational
alignment techniques. Once aligned, this enables two key capabilities in
previously opaque models: (1) semantic feature search and retrieval, and (2)
steering generation along human-interpretable atlas concepts. Through
quantitative and qualitative evaluations, we show that simple representational
alignment methods enable robust semantic retrieval and steerable generation
without the need for labeled concept data. Atlas-Alignment thus amortizes the
cost of explainable AI and mechanistic interpretability: by investing in one
high-quality Concept Atlas, we can make many new models transparent and
controllable at minimal marginal cost.

</details>


### [63] [Thought Branches: Interpreting LLM Reasoning Requires Resampling](https://arxiv.org/abs/2510.27484)
*Uzay Macar,Paul C. Bogdan,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 本文通过重新采样研究模型决策的因果影响，发现重新采样比人工编辑更有效，并提出了韧性度量来评估移除推理步骤的效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注单一的思维链（CoT），但这些模型定义了多个可能CoT的分布。研究单一样本不足以理解因果影响和底层计算。

Method: 通过重新采样来研究模型决策，包括在代理不对齐场景中重新采样特定句子以测量其下游影响，以及通过重新采样和选择具有所需属性的完成作为政策内替代方案。

Result: 重新采样可以揭示模型决策的因果影响，例如自我保存句子对黑mail的影响较小，人工编辑CoT在决策任务中的效果不如重新采样，以及通过韧性度量评估移除推理步骤的效果。

Conclusion: 通过重新采样研究分布可以实现可靠的因果分析，更清晰的模型推理叙述，并实现有原则的CoT干预。

Abstract: Most work interpreting reasoning models studies only a single
chain-of-thought (CoT), yet these models define distributions over many
possible CoTs. We argue that studying a single sample is inadequate for
understanding causal influence and the underlying computation. Though fully
specifying this distribution is intractable, it can be understood by sampling.
We present case studies using resampling to investigate model decisions. First,
when a model states a reason for its action, does that reason actually cause
the action? In "agentic misalignment" scenarios, we resample specific sentences
to measure their downstream effects. Self-preservation sentences have small
causal impact, suggesting they do not meaningfully drive blackmail. Second, are
artificial edits to CoT sufficient for steering reasoning? These are common in
literature, yet take the model off-policy. Resampling and selecting a
completion with the desired property is a principled on-policy alternative. We
find off-policy interventions yield small and unstable effects compared to
resampling in decision-making tasks. Third, how do we understand the effect of
removing a reasoning step when the model may repeat it post-edit? We introduce
a resilience metric that repeatedly resamples to prevent similar content from
reappearing downstream. Critical planning statements resist removal but have
large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can
our methods teach us anything in these settings? Adapting causal mediation
analysis, we find that hints that have a causal effect on the output without
being explicitly mentioned exert a subtle and cumulative influence on the CoT
that persists even if the hint is removed. Overall, studying distributions via
resampling enables reliable causal analysis, clearer narratives of model
reasoning, and principled CoT interventions.

</details>

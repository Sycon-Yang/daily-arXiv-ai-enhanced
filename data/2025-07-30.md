<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 58]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Categorical Classification of Book Summaries Using Word Embedding Techniques](https://arxiv.org/abs/2507.21058)
*Kerem Keskin,Mümine Kaya Keleş*

Main category: cs.CL

TL;DR: 本研究利用词嵌入方法、自然语言处理技术和机器学习算法对书籍摘要和类别进行了分类，并比较了不同方法的效果，结果显示某些模型和词嵌入技术在土耳其文本中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过不同的词嵌入方法和机器学习模型对书籍摘要和类别进行分类，并比较它们的效果。

Method: 使用了词嵌入方法、自然语言处理技术和机器学习算法对来自书籍网站的书籍摘要和类别进行了分类。还比较了常用的词嵌入方法如独热编码、Word2Vec和TF-IDF的成功率，并展示了使用的预处理方法的组合表。

Result: 研究发现，支持向量机、朴素贝叶斯和逻辑回归模型以及TF-IDF和独热编码在土耳其文本中表现更成功。

Conclusion: 研究结果表明，支持向量机、朴素贝叶斯和逻辑回归模型以及TF-IDF和独热编码词嵌入技术在土耳其文本中表现更成功。

Abstract: In this study, book summaries and categories taken from book sites were
classified using word embedding methods, natural language processing techniques
and machine learning algorithms. In addition, one hot encoding, Word2Vec and
Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are
frequently used word embedding methods were used in this study and their
success was compared. Additionally, the combination table of the pre-processing
methods used is shown and added to the table. Looking at the results, it was
observed that Support Vector Machine, Naive Bayes and Logistic Regression
Models and TF-IDF and One-Hot Encoder word embedding techniques gave more
successful results for Turkish texts.

</details>


### [2] [Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions](https://arxiv.org/abs/2507.21065)
*Sabrina Patania,Luca Annese,Cansu Koyuturk,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.CL

TL;DR: 本文提出了一种基于社会中介学习范式的AI训练方法，通过动态环境中的教学对话来提升大型语言模型的知识获取能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理广泛的离线数据集方面表现出色，但往往在获取和整合复杂的在线知识方面面临挑战。传统的AI训练范式主要基于监督学习或强化学习，模仿了独立探索的'皮亚杰'模型。这些方法通常依赖于大型数据集和稀疏反馈信号，限制了模型从交互中高效学习的能力。

Method: 本文引入了一个名为'AI Social Gym'的动态环境，其中AI学习代理与有知识的AI教师代理进行二元教学对话。这些互动强调外部、结构化的对话作为知识获取的核心机制。

Result: 实证结果表明，这种对话方法——特别是涉及混合方向互动的方法，结合自上而下的解释和学习者发起的问题——显著增强了LLM获取和应用新知识的能力，优于单向教学方法和直接访问结构化知识格式，这些通常是训练数据集中的内容。

Conclusion: 这些发现表明，将教学和心理学见解整合到人工智能和机器人训练中可以显著提高训练后的知识获取和响应质量。这种方法为现有的策略（如提示工程）提供了一种补充途径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
processing extensive offline datasets. However, they often face challenges in
acquiring and integrating complex, knowledge online. Traditional AI training
paradigms, predominantly based on supervised learning or reinforcement
learning, mirror a 'Piagetian' model of independent exploration. These
approaches typically rely on large datasets and sparse feedback signals,
limiting the models' ability to learn efficiently from interactions. Drawing
inspiration from Vygotsky's sociocultural theory, this study explores the
potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI
learner agent engages in dyadic pedagogical dialogues with knowledgeable AI
teacher agents. These interactions emphasize external, structured dialogue as a
core mechanism for knowledge acquisition, contrasting with methods that depend
solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the
AI learning process in the context of ontology acquisition. Empirical results
indicate that such dialogic approaches-particularly those involving
mixed-direction interactions combining top-down explanations with
learner-initiated questioning-significantly enhance the LLM's ability to
acquire and apply new knowledge, outperforming both unidirectional
instructional methods and direct access to structured knowledge, formats
typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological
insights into AI and robot training can substantially improve post-training
knowledge acquisition and response quality. This approach offers a
complementary pathway to existing strategies like prompt engineering

</details>


### [3] [Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing](https://arxiv.org/abs/2507.21073)
*David James Woo,Yangyang Yu,Kai Guo,Yilin Huang,April Ka Yeng Fung*

Main category: cs.CL

TL;DR: 本研究探讨了EFL中学生如何编辑AI生成的文本，并发现他们的编辑行为与作品质量提升之间存在脱节。研究建议在AI整合前进行特定体裁的教学和过程导向的写作。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨EFL中学生如何编辑AI生成的文本，并研究他们的编辑行为及其对写作质量和评分的影响。

Method: 研究采用了混合设计，分析了学生的屏幕录制和作品，以检查学生的编辑行为和写作质量。分析方法包括定性编码、描述性统计、时间序列分析、人工评分和多元线性回归分析。

Result: 研究发现，AI生成的单词数量正向预测了所有评分维度，而大多数编辑变量显示出最小的影响。两种编辑模式被识别出来：一种是学生反复修改引言部分，另一种是他们快速转向主体部分的广泛编辑。

Conclusion: 研究结果表明，学生大量的编辑努力与作品质量的提高之间存在脱节，这表明AI支持但不会取代写作技能。研究强调了在AI整合之前进行特定体裁的教学和过程导向的写作的重要性。教育者还应开发重视过程和产品的评估，以鼓励对AI文本的批判性参与。

Abstract: Text generated by artificial intelligence (AI) chatbots is increasingly used
in English as a foreign language (EFL) writing contexts, yet its impact on
students' expository writing process and compositions remains understudied.
This research examines how EFL secondary students edit AI-generated text.
Exploring editing behaviors in their expository writing process and in
expository compositions, and their effect on human-rated scores for content,
organization, language, and overall quality. Participants were 39 Hong Kong
secondary students who wrote an expository composition with AI chatbots in a
workshop. A convergent design was employed to analyze their screen recordings
and compositions to examine students' editing behaviors and writing qualities.
Analytical methods included qualitative coding, descriptive statistics,
temporal sequence analysis, human-rated scoring, and multiple linear regression
analysis. We analyzed over 260 edits per dataset, and identified two editing
patterns: one where students refined introductory units repeatedly before
progressing, and another where they quickly shifted to extensive edits in body
units (e.g., topic and supporting sentences). MLR analyses revealed that the
number of AI-generated words positively predicted all score dimensions, while
most editing variables showed minimal impact. These results suggest a
disconnect between students' significant editing effort and improved
composition quality, indicating AI supports but does not replace writing
skills. The findings highlight the importance of genre-specific instruction and
process-focused writing before AI integration. Educators should also develop
assessments valuing both process and product to encourage critical engagement
with AI text.

</details>


### [4] [Which symbol grounding problem should we try to solve?](https://arxiv.org/abs/2507.21080)
*Vincent C. Müller*

Main category: cs.CL

TL;DR: 文章批评了 Floridi 和 Taddeo 的 '零语义承诺' 条件，认为其无法实现，并提出了对奠基问题的新理解。


<details>
  <summary>Details</summary>
Motivation: 质疑 Floridi 和 Taddeo 提出的条件是否可以实现，并尝试重新定义奠基问题的含义。

Method: 分析 Floridi 和 Taddeo 提出的 '零语义承诺' 条件，并探讨 Luc Steels 的不同建议，重新思考问题的本质以及系统中的 '目标' 在提出问题中的作用。

Result: 指出 Floridi 和 Taddeo 的条件无法实现，同时提出新的观点，即应关注人工计算代理的行为能力和意义功能。

Conclusion: 我认为唯一有意义的奠基问题是如何解释和重新产生人工计算代理的行为能力和意义功能。

Abstract: Floridi and Taddeo propose a condition of "zero semantic commitment" for
solutions to the grounding problem, and a solution to it. I argue briefly that
their condition cannot be fulfilled, not even by their own solution. After a
look at Luc Steels' very different competing suggestion, I suggest that we need
to re-think what the problem is and what role the 'goals' in a system play in
formulating the problem. On the basis of a proper understanding of computing, I
come to the conclusion that the only sensible grounding problem is how we can
explain and re-produce the behavioral ability and function of meaning in
artificial computational agents

</details>


### [5] [ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs](https://arxiv.org/abs/2507.21083)
*Franck Bardol*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（如GPT-4）如何根据问题的情感措辞调整其响应，并发现模型对负面框架的问题比对中性问题更少做出负面回应，这表明存在一种“反弹”偏差。在敏感话题上，基于语气的变化被抑制，表明存在对齐覆盖。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究大型语言模型（如GPT-4）如何根据问题的情感措辞调整其响应，并探讨情感框架对模型行为的影响。

Method: 本文系统地变化了156个提示的情感语气，并分析了它如何影响模型响应。引入了“语气下限”等概念，并使用语气极性转换矩阵来量化行为。基于1536维嵌入的可视化确认了基于语气的语义漂移。

Result: GPT-4对负面框架的问题比对中性问题更少做出负面回应，这表明存在一种“反弹”偏差，模型过度纠正，通常转向中立或积极。在敏感话题上，这种效应更加明显：基于语气的变化被抑制，表明存在对齐覆盖。

Conclusion: 本文揭示了由提示的情感框架驱动的未被充分研究的偏见类别，对AI对齐和信任有影响。

Abstract: Large Language Models like GPT-4 adjust their responses not only based on the
question asked, but also on how it is emotionally phrased. We systematically
vary the emotional tone of 156 prompts - spanning controversial and everyday
topics - and analyze how it affects model responses. Our findings show that
GPT-4 is three times less likely to respond negatively to a negatively framed
question than to a neutral one. This suggests a "rebound" bias where the model
overcorrects, often shifting toward neutrality or positivity. On sensitive
topics (e.g., justice or politics), this effect is even more pronounced:
tone-based variation is suppressed, suggesting an alignment override. We
introduce concepts like the "tone floor" - a lower bound in response negativity
- and use tone-valence transition matrices to quantify behavior. Visualizations
based on 1536-dimensional embeddings confirm semantic drift based on tone. Our
work highlights an underexplored class of biases driven by emotional framing in
prompts, with implications for AI alignment and trust. Code and data are
available at: https://github.com/bardolfranck/llm-responses-viewer

</details>


### [6] [Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing](https://arxiv.org/abs/2507.21084)
*Aly M. Kassem,Zhuan Shi,Negar Rostamzadeh,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: 本文提出了一种名为MNEME的轻量级框架，用于检测大型语言模型在微调或去学习后的副作用。该框架通过稀疏模型差异分析，能够在不访问微调数据的情况下检测行为变化，并在多个场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法在检测无意的副作用方面存在不足，例如，在消除生物学内容时，性能可能会下降。因此，我们需要一种通用的方法来检测这些副作用。

Method: 我们引入了MNEME，这是一个轻量级框架，用于使用稀疏模型差异来识别这些副作用。MNEME在任务无关数据上比较基础模型和微调模型，以隔离行为变化。

Result: MNEME在三个场景中实现了高达95%的准确率来预测副作用，并且不需要自定义启发式方法。此外，我们还展示了通过在高激活样本上重新训练可以部分逆转这些效果。

Conclusion: 我们的结果表明，稀疏探测和差异分析为了解和管理LLM行为提供了可扩展且自动化的视角。

Abstract: Large language models (LLMs) are frequently fine-tuned or unlearned to adapt
to new tasks or eliminate undesirable behaviors. While existing evaluation
methods assess performance after such interventions, there remains no general
approach for detecting unintended side effects, such as unlearning biology
content degrading performance on chemistry tasks, particularly when these
effects are unpredictable or emergent. To address this issue, we introduce
MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight
framework for identifying these side effects using sparse model diffing. MNEME
compares base and fine-tuned models on task-agnostic data (for example, The
Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral
shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning,
emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent
accuracy in predicting side effects, aligning with known benchmarks and
requiring no custom heuristics. Furthermore, we show that retraining on
high-activation samples can partially reverse these effects. Our results
demonstrate that sparse probing and diffing offer a scalable and automated lens
into fine-tuning-induced model changes, providing practical tools for
understanding and managing LLM behavior.

</details>


### [7] [Multi-Amateur Contrastive Decoding for Text Generation](https://arxiv.org/abs/2507.21086)
*Jaydip Sen,Subhasis Dasgupta,Hetvi Waghela*

Main category: cs.CL

TL;DR: 本文提出了Multi-Amateur Contrastive Decoding (MACD)，通过使用一组业余模型来改进CD方法，从而更全面地捕捉语言生成中的不良模式，并在多个领域中表现出色。


<details>
  <summary>Details</summary>
Motivation: Contrastive Decoding (CD)虽然提高了连贯性和流畅性，但其依赖于单一业余模型，限制了其捕捉语言生成中多样化和多方面失败模式的能力，如重复、幻觉和风格漂移。

Method: MACD通过使用一组业余模型来更全面地表征不良生成模式，并通过平均和共识惩罚机制整合对比信号，同时扩展了可解释性约束以在多业余模型设置中有效运行。此外，框架通过引入具有特定风格或内容偏见的业余模型实现了可控生成。

Result: 实验结果表明，MACD在流畅性、连贯性、多样性和适应性方面均优于传统解码方法和原始CD方法，且无需额外训练或微调。

Conclusion: MACD在多个领域中表现出色，优于传统解码方法和原始CD方法，且无需额外训练或微调。

Abstract: Contrastive Decoding (CD) has emerged as an effective inference-time strategy
for enhancing open-ended text generation by exploiting the divergence in output
probabilities between a large expert language model and a smaller amateur
model. Although CD improves coherence and fluency, its dependence on a single
amateur restricts its capacity to capture the diverse and multifaceted failure
modes of language generation, such as repetition, hallucination, and stylistic
drift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a
generalization of the CD framework that employs an ensemble of amateur models
to more comprehensively characterize undesirable generation patterns. MACD
integrates contrastive signals through both averaging and consensus
penalization mechanisms and extends the plausibility constraint to operate
effectively in the multi-amateur setting. Furthermore, the framework enables
controllable generation by incorporating amateurs with targeted stylistic or
content biases. Experimental results across multiple domains, such as news,
encyclopedic, and narrative, demonstrate that MACD consistently surpasses
conventional decoding methods and the original CD approach in terms of fluency,
coherence, diversity, and adaptability, all without requiring additional
training or fine-tuning.

</details>


### [8] [QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning](https://arxiv.org/abs/2507.21095)
*Mohammad AL-Smadi*

Main category: cs.CL

TL;DR: 本文提出了一种结合上下文嵌入与统计和语言特征的增强型变压器架构，用于检测新闻文章中的主观性。该方法在多种语言中表现出色，特别是在英语、德语、阿拉伯语和罗马尼亚语的单语设置中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决CheckThat! 2025任务1中的主观性检测问题，即区分新闻文章中的句子是表达作者的主观观点还是客观描述。

Method: 我们提出了一种结合上下文嵌入与统计和语言特征的增强型变压器架构。对于阿拉伯语，我们使用了AraELECTRA并增加了词性（POS）标签和TF-IDF特征；而对于其他语言，我们微调了一个跨语言DeBERTa~V3模型，并通过门控机制结合了TF-IDF特征。

Result: 我们的系统在单语、多语和零样本设置中进行了评估，涵盖了多种语言，包括英语、阿拉伯语、德语、意大利语和一些未见过的语言。结果表明，我们的方法在不同语言中表现良好，尤其在英语、德语、阿拉伯语和罗马尼亚语的单语设置中取得了优异成绩。

Conclusion: 我们的方法在多种语言中表现出色，特别是在英语、德语、阿拉伯语和罗马尼亚语的单语设置中取得了显著成果。此外，我们的分析揭示了模型对跨语言微调顺序和训练语言语言接近性的敏感性。

Abstract: This paper presents our approach to the CheckThat! 2025 Task 1 on
subjectivity detection, where systems are challenged to distinguish whether a
sentence from a news article expresses the subjective view of the author or
presents an objective view on the covered topic. We propose a feature-augmented
transformer architecture that combines contextual embeddings from pre-trained
language models with statistical and linguistic features. Our system leveraged
pre-trained transformers with additional lexical features: for Arabic we used
AraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while
for the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined
with TF-IDF features through a gating mechanism. We evaluated our system in
monolingual, multilingual, and zero-shot settings across multiple languages
including English, Arabic, German, Italian, and several unseen languages. The
results demonstrate the effectiveness of our approach, achieving competitive
performance across different languages with notable success in the monolingual
setting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with
macro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank
1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an
ablation analysis that demonstrated the importance of combining TF-IDF features
with the gating mechanism and the cross-lingual transfer for subjectivity
detection. Furthermore, our analysis reveals the model's sensitivity to both
the order of cross-lingual fine-tuning and the linguistic proximity of the
training languages.

</details>


### [9] [Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting](https://arxiv.org/abs/2507.21099)
*Chloe Ho,Ishneet Sukhvinder Singh,Diya Sharma,Tanvi Reddy Anumandla,Michael Lu,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文研究了基于LLM的广告重写如何提高其在检索系统中的排名和生成的LLM响应中的包含率，并提出了一种监督微调框架和两个评估指标。实验结果表明，PPO训练的模型在大多数情况下优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 搜索算法和用户查询相关性使LLM能够返回相关信息，但内容措辞对广告可见性的影响仍缺乏研究。我们研究了基于LLM的广告重写如何提高其在检索系统中的排名和生成的LLM响应中的包含率，而无需修改检索模型本身。

Method: 我们引入了一个带有自定义损失平衡语义相关性和内容保真度的监督微调框架。为了评估效果，我们提出了两个指标：DeltaMRR@K（排名改进）和DeltaDIR@K（包含频率改进）。

Result: 实验结果表明，PPO训练的模型在大多数情况下优于提示工程和监督微调，在指令式提示中实现了高达2.79 DeltaDIR@5和0.0073 DeltaMRR@5的改进。

Conclusion: 我们的方法提供了一种可扩展的广告措辞优化方法，提高了检索型LLM工作流中的可见性。实验结果表明，PPO训练的模型在大多数情况下优于提示工程和监督微调。

Abstract: Search algorithms and user query relevance have given LLMs the ability to
return relevant information, but the effect of content phrasing on ad
visibility remains underexplored. We investigate how LLM-based rewriting of
advertisements can improve their ranking in retrieval systems and inclusion in
generated LLM responses, without modifying the retrieval model itself. We
introduce a supervised fine-tuning framework with a custom loss balancing
semantic relevance and content fidelity. To evaluate effectiveness, we propose
two metrics: DeltaMRR@K (ranking improvement) and DeltaDIR@K (inclusion
frequency improvement). Our approach presents a scalable method to optimize ad
phrasing, enhancing visibility in retrieval-based LLM workflows. Experiments
across both instruction-based and few-shot prompting demonstrate that PPO
trained models outperform both prompt engineering and supervised fine-tuning in
most cases, achieving up to a 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in
instruction-based prompting. These results highlight the importance of how the
ad is written before retrieval and prompt format and reinforcement learning in
effective ad rewriting for LLM integrated retrieval systems.

</details>


### [10] [iLSU-T: an Open Dataset for Uruguayan Sign Language Translation](https://arxiv.org/abs/2507.21104)
*Ariel E. Stassi,Yanina Boria,J. Matías Di Martino,Gregory Randall*

Main category: cs.CL

TL;DR: 本文介绍了一个新的开放数据集iLSU T，用于手语翻译研究，并通过实验验证了其有效性，强调了本地化数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于每个手语国家的特殊性，机器翻译需要本地数据来开发新技术和适应现有技术。本文旨在提供一个多样化的多模态数据集，以促进手语处理方法的研究。

Method: 本文介绍了iLSU T数据集，该数据集包含来自公共电视广播的185小时的手语视频，并使用三种最先进的翻译算法进行了实验，以建立基准并评估数据集和数据处理流程的有效性。

Result: 实验表明，需要更多本地化数据集来改进手语翻译和理解，这对于开发新型工具至关重要。

Conclusion: 本文提出了一种用于手语翻译的开放数据集iLSU T，并展示了其在开发新型工具以提高所有个体的可访问性和包容性方面的潜力。

Abstract: Automatic sign language translation has gained particular interest in the
computer vision and computational linguistics communities in recent years.
Given each sign language country particularities, machine translation requires
local data to develop new techniques and adapt existing ones. This work
presents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB
videos with audio and text transcriptions. This type of multimodal and curated
data is paramount for developing novel approaches to understand or generate
tools for sign language processing. iLSU T comprises more than 185 hours of
interpreted sign language videos from public TV broadcasting. It covers diverse
topics and includes the participation of 18 professional interpreters of sign
language. A series of experiments using three state of the art translation
algorithms is presented. The aim is to establish a baseline for this dataset
and evaluate its usefulness and the proposed pipeline for data processing. The
experiments highlight the need for more localized datasets for sign language
translation and understanding, which are critical for developing novel tools to
improve accessibility and inclusion of all individuals. Our data and code can
be accessed.

</details>


### [11] [Creation of a Numerical Scoring System to Objectively Measure and Compare the Level of Rhetoric in Arabic Texts: A Feasibility Study, and A Working Prototype](https://arxiv.org/abs/2507.21106)
*Mandar Marathe*

Main category: cs.CL

TL;DR: 本文旨在开发一种测量阿拉伯修辞文学手法密度的方法，并创建了相关工具以支持这一目标。


<details>
  <summary>Details</summary>
Motivation: 目前没有客观的方法来确定说话者或作者是否在给定文本中使用了阿拉伯修辞，以及使用的程度和原因。同时，无法比较不同体裁、作者或时代中阿拉伯修辞的使用情况。

Method: 本文编制了84种最常见的文学手法及其定义列表，构建了一个识别文本中文学手法的系统，并利用基于文本的词素计数计算文学手法的密度。此外，还创建了四个电子工具和一个模拟工具来支持计算阿拉伯文本的修辞文学手法密度。

Result: 本文开发了一个工具，可以准确报告任何阿拉伯语文本或演讲中的阿拉伯修辞密度。

Conclusion: 本文提出了一个可以准确报告任何阿拉伯语文本或演讲中阿拉伯修辞密度的工具。

Abstract: Arabic Rhetoric is the field of Arabic linguistics which governs the art and
science of conveying a message with greater beauty, impact and persuasiveness.
The field is as ancient as the Arabic language itself and is found extensively
in classical and contemporary Arabic poetry, free verse and prose. In practical
terms, it is the intelligent use of word order, figurative speech and
linguistic embellishments to enhance message delivery. Despite the volumes that
have been written about it and the high status accorded to it, there is no way
to objectively know whether a speaker or writer has used Arabic rhetoric in a
given text, to what extent, and why. There is no objective way to compare the
use of Arabic rhetoric across genres, authors or epochs. It is impossible to
know which of pre-Islamic poetry, Andalucian Arabic poetry, or modern literary
genres are richer in Arabic rhetoric. The aim of the current study was to
devise a way to measure the density of the literary devices which constitute
Arabic rhetoric in a given text, as a proxy marker for Arabic rhetoric itself.
A comprehensive list of 84 of the commonest literary devices and their
definitions was compiled. A system of identifying literary devices in texts was
constructed. A method of calculating the density of literary devices based on
the morpheme count of the text was utilised. Four electronic tools and an
analogue tool were created to support the calculation of an Arabic text's
rhetorical literary device density, including a website and online calculator.
Additionally, a technique of reporting the distribution of literary devices
used across the three sub-domains of Arabic rhetoric was created. The output of
this project is a working tool which can accurately report the density of
Arabic rhetoric in any Arabic text or speech.

</details>


### [12] [Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams](https://arxiv.org/abs/2507.21107)
*Rob Manson*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose Curved Inference - a geometric Interpretability framework that
tracks how the residual stream trajectory of a large language model bends in
response to shifts in semantic concern. Across 20 matched prompts spanning
emotional, moral, perspective, logical, identity, environmental, and nonsense
domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics,
with a primary focus on curvature (\k{appa}_i) and salience (S(t)). These
metrics are computed under a pullback semantic metric derived from the
unembedding matrix, ensuring that all measurements reflect token-aligned
geometry rather than raw coordinate structure. We find that concern-shifted
prompts reliably alter internal activation trajectories in both models - with
LLaMA exhibiting consistent, statistically significant scaling in both
curvature and salience as concern intensity increases. Gemma also responds to
concern but shows weaker differentiation between moderate and strong variants.
Our results support a two-layer view of LLM geometry - a latent conceptual
structure encoded in the embedding space, and a contextual trajectory shaped by
prompt-specific inference. Curved Inference reveals how models navigate,
reorient, or reinforce semantic meaning over depth, offering a principled
method for diagnosing alignment, abstraction, and emergent inference dynamics.
These findings offer fresh insight into semantic abstraction and model
alignment through the lens of Curved Inference.

</details>


### [13] [A Survey of Classification Tasks and Approaches for Legal Contracts](https://arxiv.org/abs/2507.21108)
*Amrita Singh,Aditya Joshi,Jiaojiao Jiang,Hye-young Paik*

Main category: cs.CL

TL;DR: 本文是一篇关于自动法律合同分类（LCC）的综合性综述，涵盖了LCC的挑战、关键任务、数据集、方法论以及评估技术，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于合同规模大且复杂，手动审查变得低效且容易出错，因此需要自动化。自动法律合同分类（LCC）革新了法律合同的分析方式，提供了显著的速度、准确性和可访问性的改进。

Method: 本文对自动法律合同分类（LCC）的挑战进行了深入探讨，并详细分析了关键任务、数据集和方法。文章提出了LCC中的七个分类任务，并回顾了与英语合同相关的14个数据集，包括公共、专有和非公开来源。此外，还引入了一个LCC的方法论分类法，分为传统机器学习、深度学习和基于Transformer的方法。

Result: 本文回顾了LCC的关键任务、数据集和方法，并讨论了评估技术，同时突出了所研究文献中表现最佳的结果。

Conclusion: 本文旨在通过全面概述当前的方法及其局限性，提出未来研究方向，以提高法律合同分类（LCC）的效率、准确性和可扩展性。作为首篇关于LCC的综合性综述，它旨在支持法律自然语言处理（NLP）研究人员和从业者改进法律流程，使法律信息更易于获取，并促进更加知情和公平的社会。

Abstract: Given the large size and volumes of contracts and their underlying inherent
complexity, manual reviews become inefficient and prone to errors, creating a
clear need for automation. Automatic Legal Contract Classification (LCC)
revolutionizes the way legal contracts are analyzed, offering substantial
improvements in speed, accuracy, and accessibility. This survey delves into the
challenges of automatic LCC and a detailed examination of key tasks, datasets,
and methodologies. We identify seven classification tasks within LCC, and
review fourteen datasets related to English-language contracts, including
public, proprietary, and non-public sources. We also introduce a methodology
taxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning,
and Transformer-based approaches. Additionally, the survey discusses evaluation
techniques and highlights the best-performing results from the reviewed
studies. By providing a thorough overview of current methods and their
limitations, this survey suggests future research directions to improve the
efficiency, accuracy, and scalability of LCC. As the first comprehensive survey
on LCC, it aims to support legal NLP researchers and practitioners in improving
legal processes, making legal information more accessible, and promoting a more
informed and equitable society.

</details>


### [14] [SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering](https://arxiv.org/abs/2507.21110)
*Kezhen Zhong,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

TL;DR: SemRAG 是一种增强的检索增强生成（RAG）框架，它通过语义分块和知识图谱有效地整合领域特定知识，而无需大量微调。实验结果表明，SemRAG 在多跳 RAG 和维基百科数据集上显著提升了检索信息的相关性和正确性，优于传统 RAG 方法。


<details>
  <summary>Details</summary>
Motivation: 将领域特定知识整合到大型语言模型（LLMs）中对于提高其在专业任务中的性能至关重要。然而，现有的方法计算成本高，容易过拟合，并限制了可扩展性。

Method: SemRAG 使用语义分块算法，基于句子嵌入的余弦相似度对文档进行分割，以保持语义连贯性并减少计算开销。此外，通过将检索到的信息结构化为知识图谱，SemRAG 捕捉实体之间的关系，提高检索准确性和上下文理解。

Result: 实验结果表明，SemRAG 显著提高了从知识图谱中检索信息的相关性和正确性，优于传统 RAG 方法。此外，研究了不同数据语料库的缓冲区大小优化，优化的缓冲区大小可以进一步提高检索性能，知识图谱的整合增强了实体关系以获得更好的上下文理解。

Conclusion: SemRAG 的主要优势在于其能够创建一个高效、准确的领域特定大语言模型（LLM）流程，同时避免资源密集型微调。这使其成为一种实用且可扩展的方法，符合可持续发展目标，为领域特定的AI应用提供可行的解决方案。

Abstract: This paper introduces SemRAG, an enhanced Retrieval Augmented Generation
(RAG) framework that efficiently integrates domain-specific knowledge using
semantic chunking and knowledge graphs without extensive fine-tuning.
Integrating domain-specific knowledge into large language models (LLMs) is
crucial for improving their performance in specialized tasks. Yet, existing
adaptations are computationally expensive, prone to overfitting and limit
scalability. To address these challenges, SemRAG employs a semantic chunking
algorithm that segments documents based on the cosine similarity from sentence
embeddings, preserving semantic coherence while reducing computational
overhead. Additionally, by structuring retrieved information into knowledge
graphs, SemRAG captures relationships between entities, improving retrieval
accuracy and contextual understanding. Experimental results on MultiHop RAG and
Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance
and correctness of retrieved information from the Knowledge Graph,
outperforming traditional RAG methods. Furthermore, we investigate the
optimization of buffer sizes for different data corpus, as optimizing buffer
sizes tailored to specific datasets can further improve retrieval performance,
as integration of knowledge graphs strengthens entity relationships for better
contextual comprehension. The primary advantage of SemRAG is its ability to
create an efficient, accurate domain-specific LLM pipeline while avoiding
resource-intensive fine-tuning. This makes it a practical and scalable approach
aligned with sustainability goals, offering a viable solution for AI
applications in domain-specific fields.

</details>


### [15] [InsurTech innovation using natural language processing](https://arxiv.org/abs/2507.21112)
*Panyi Dong,Zhiyu Quan*

Main category: cs.CL

TL;DR: This paper explores the use of NLP in insurance operations to transform unstructured text into structured data for actuarial analysis and decision-making.


<details>
  <summary>Details</summary>
Motivation: Traditional insurance companies are exploring alternative data sources and advanced technologies to sustain their competitive edge.

Method: Applying various NLP techniques to demonstrate practical use cases in the commercial insurance context.

Result: Enriched, text-derived insights add to and refine traditional rating factors for commercial insurance pricing and offer novel perspectives for assessing underlying risk.

Conclusion: NLP is not merely a supplementary tool but a foundational element for modern, data-driven insurance analytics.

Abstract: With the rapid rise of InsurTech, traditional insurance companies are
increasingly exploring alternative data sources and advanced technologies to
sustain their competitive edge. This paper provides both a conceptual overview
and practical case studies of natural language processing (NLP) and its
emerging applications within insurance operations with a focus on transforming
raw, unstructured text into structured data suitable for actuarial analysis and
decision-making. Leveraging real-world alternative data provided by an
InsurTech industry partner that enriches traditional insurance data sources, we
apply various NLP techniques to demonstrate practical use cases in the
commercial insurance context. These enriched, text-derived insights not only
add to and refine traditional rating factors for commercial insurance pricing
but also offer novel perspectives for assessing underlying risk by introducing
novel industry classifications. Through these demonstrations, we show that NLP
is not merely a supplementary tool but a foundational element for modern,
data-driven insurance analytics.

</details>


### [16] [TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law](https://arxiv.org/abs/2507.21134)
*Zheng Hui,Yijiang River Dong,Ehsan Shareghi,Nigel Collier*

Main category: cs.CL

TL;DR: 本文提出了一种新的基准测试Trident-Bench，以评估大型语言模型在法律、金融和医疗等高风险领域中的安全性。研究发现，虽然一些通用模型表现良好，但领域专用模型在处理伦理问题时存在明显不足，这凸显了需要更细致的领域特定安全改进。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地被部署在高风险领域，如法律、金融和医学，系统性地评估其特定领域的安全性和合规性变得至关重要。然而，先前的研究主要集中在提高LLM在这些领域的性能上，而往往忽略了对特定领域安全风险的评估。

Method: 本文基于AMA医学伦理原则、ABA职业行为规范模型和CFA协会道德准则，定义了针对LLM的特定领域安全原则，并在此基础上引入了Trident-Bench基准测试，用于评估LLM在法律、金融和医疗领域的安全性。

Result: 本文评估了19个通用和领域专用模型在Trident-Bench上的表现，结果表明Trident-Bench能够有效地揭示关键的安全差距——强大的通用模型（例如GPT、Gemini）可以满足基本期望，而领域专用模型则常常难以处理细微的伦理问题。

Conclusion: 本文通过引入Trident-Bench，为研究法律和金融领域的LLM安全性提供了第一个系统性的资源，并为未来减少在专业监管领域部署LLM的安全风险奠定了基础。

Abstract: As large language models (LLMs) are increasingly deployed in high-risk
domains such as law, finance, and medicine, systematically evaluating their
domain-specific safety and compliance becomes critical. While prior work has
largely focused on improving LLM performance in these domains, it has often
neglected the evaluation of domain-specific safety risks. To bridge this gap,
we first define domain-specific safety principles for LLMs based on the AMA
Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and
the CFA Institute Code of Ethics. Building on this foundation, we introduce
Trident-Bench, a benchmark specifically targeting LLM safety in the legal,
financial, and medical domains. We evaluated 19 general-purpose and
domain-specialized models on Trident-Bench and show that it effectively reveals
key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic
expectations, whereas domain-specialized models often struggle with subtle
ethical nuances. This highlights an urgent need for finer-grained
domain-specific safety improvements. By introducing Trident-Bench, our work
provides one of the first systematic resources for studying LLM safety in law
and finance, and lays the groundwork for future research aimed at reducing the
safety risks of deploying LLMs in professionally regulated fields. Code and
benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT

</details>


### [17] [TTS-1 Technical Report](https://arxiv.org/abs/2507.21138)
*Oleg Atamanenko,Anna Chalova,Joseph Coombes,Nikki Cope,Phillip Dang,Zhifeng Deng,Jimmy Du,Michael Ermolenko,Feifan Fan,Yufei Feng,Cheryl Fichter,Pavel Filimonov,Louis Fischer,Kylan Gibbs,Valeria Gusarova,Pavel Karpik,Andreas Assad Kottner,Ian Lee,Oliver Louie,Jasmine Mai,Mikhail Mamontov,Suri Mao,Nurullah Morshed,Igor Poletaev,Florin Radu,Dmytro Semernia,Evgenii Shingarev,Vikram Sivaraja,Peter Skirko,Rinat Takhautdinov,Robert Villahermosa,Jean Wang*

Main category: cs.CL

TL;DR: Inworld TTS-1 introduces two Transformer-based autoregressive text-to-speech models, TTS-1-Max and TTS-1, which achieve state-of-the-art performance on various benchmarks. They support 11 languages with emotional control and non-verbal vocalizations, and are open-sourced under an MIT license.


<details>
  <summary>Details</summary>
Motivation: To introduce two Transformer-based autoregressive text-to-speech (TTS) models that achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speaker's voice.

Method: By scaling train-time compute and applying a sequential process of pre-training, fine-tuning, and RL-alignment of the speech-language model (SpeechLM) component.

Result: Both models achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speaker's voice.

Conclusion: Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech with low latency, and support 11 languages with fine-grained emotional control and non-verbal vocalizations through audio markups.

Abstract: We introduce Inworld TTS-1, a set of two Transformer-based autoregressive
text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters
and is designed for utmost quality and expressiveness in demanding
applications. TTS-1 is our most efficient model, with 1.6B parameters, built
for real-time speech synthesis and on-device use cases. By scaling train-time
compute and applying a sequential process of pre-training, fine-tuning, and
RL-alignment of the speech-language model (SpeechLM) component, both models
achieve state-of-the-art performance on a variety of benchmarks, demonstrating
exceptional quality relying purely on in-context learning of the speaker's
voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech
with low latency, and support 11 languages with fine-grained emotional control
and non-verbal vocalizations through audio markups. We additionally open-source
our training and modeling code under an MIT license.

</details>


### [18] [Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question](https://arxiv.org/abs/2507.21168)
*Rafael Rosales,Santiago Miret*

Main category: cs.CL

TL;DR: 本研究比较了模型多样性和问题解释多样性在使用大型语言模型回答二元问题时的效果，发现问题解释多样性更有效。


<details>
  <summary>Details</summary>
Motivation: 确定如何有效利用多样性仍然是一个挑战，因此需要比较不同的多样性方法以提高性能。

Method: 比较了两种多样性方法：模型多样性（多个模型回答同一问题）和问题解释多样性（同一模型回答不同方式的问题），并应用多数投票作为集成共识启发式方法。

Result: 在boolq、strategyqa和pubmedqa数据集上的实验表明，问题解释多样性在集成准确性方面优于模型多样性。

Conclusion: 实验表明，问题解释多样性在使用大型语言模型回答二元问题时，比模型多样性更有效。

Abstract: Effectively leveraging diversity has been shown to improve performance for
various machine learning models, including large language models (LLMs).
However, determining the most effective way of using diversity remains a
challenge. In this work, we compare two diversity approaches for answering
binary questions using LLMs: model diversity, which relies on multiple models
answering the same question, and question interpretation diversity, which
relies on using the same model to answer the same question framed in different
ways. For both cases, we apply majority voting as the ensemble consensus
heuristic to determine the final answer. Our experiments on boolq, strategyqa,
and pubmedqa show that question interpretation diversity consistently leads to
better ensemble accuracy compared to model diversity. Furthermore, our analysis
of GPT and LLaMa shows that model diversity typically produces results between
the best and the worst ensemble members without clear improvement.

</details>


### [19] [Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers](https://arxiv.org/abs/2507.21186)
*Sungmin Han,Jeonghyun Lee,Sangkyun Lee*

Main category: cs.CL

TL;DR: Contrast-CAT 是一种新的基于激活对比的归因方法，能够提高基于 Transformer 的文本分类模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于激活的归因方法可能受到激活中与类别无关特征的影响，导致解释不可靠。

Method: Contrast-CAT 是一种基于激活对比的归因方法，通过对比输入序列的激活与参考激活来生成更清晰和忠实的归因图。

Result: 实验结果表明，Contrast-CAT 在多个数据集和模型上均优于最先进的方法，在 MoRF 设置下，其 AOPC 和 LOdds 分别提高了 x1.30 和 x2.25。

Conclusion: Contrast-CAT 是一种有效的激活对比方法，能够提高基于 Transformer 的文本分类模型的可解释性。

Abstract: Transformers have profoundly influenced AI research, but explaining their
decisions remains challenging -- even for relatively simpler tasks such as
classification -- which hinders trust and safe deployment in real-world
applications. Although activation-based attribution methods effectively explain
transformer-based text classification models, our findings reveal that these
methods can be undermined by class-irrelevant features within activations,
leading to less reliable interpretations. To address this limitation, we
propose Contrast-CAT, a novel activation contrast-based attribution method that
refines token-level attributions by filtering out class-irrelevant features. By
contrasting the activations of an input sequence with reference activations,
Contrast-CAT generates clearer and more faithful attribution maps. Experimental
results across various datasets and models confirm that Contrast-CAT
consistently outperforms state-of-the-art methods. Notably, under the MoRF
setting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds
over the most competing methods, demonstrating its effectiveness in enhancing
interpretability for transformer-based text classification.

</details>


### [20] [Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability](https://arxiv.org/abs/2507.21234)
*Fatema Binte Hassan,Md Al Jubair,Mohammad Mehadi Hasan,Tahmid Hossain,S M Mehebubur Rahman Khan Shuvo,Mohammad Shamsul Arefin*

Main category: cs.CL

TL;DR: 本研究开发了一个基于XLM-RoBERTa Base架构的变压器模型，用于对Bangla语言的社交媒体评论进行情感分类，并利用可解释的人工智能技术提高模型的可解释性。结果表明，该模型在处理低资源语言方面表现出色，能够提供有价值的见解以支持公共政策和犯罪预防。


<details>
  <summary>Details</summary>
Motivation: 研究旨在调查犯罪相关新闻的公众看法演变，并通过分类用户生成的评论来分析公众情绪的变化。

Method: 研究提出了一个基于变压器的模型，利用XLM-RoBERTa Base架构进行分类，并采用可解释的人工智能技术来识别影响情感分类的关键特征。

Result: 该模型在Bangla语言的情感分析中取得了97%的分类准确率，优于现有的最先进的方法。

Conclusion: 研究结果强调了基于变压器的模型在处理低资源语言（如孟加拉语）中的有效性，并展示了它们在提取可操作见解方面的潜力，这些见解可以支持公共政策制定和犯罪预防策略。

Abstract: In recent years, social media platforms have become prominent spaces for
individuals to express their opinions on ongoing events, including criminal
incidents. As a result, public sentiment can shift dynamically over time. This
study investigates the evolving public perception of crime-related news by
classifying user-generated comments into three categories: positive, negative,
and neutral. A newly curated dataset comprising 28,528 Bangla-language social
media comments was developed for this purpose. We propose a transformer-based
model utilizing the XLM-RoBERTa Base architecture, which achieves a
classification accuracy of 97%, outperforming existing state-of-the-art methods
in Bangla sentiment analysis. To enhance model interpretability, explainable AI
technique is employed to identify the most influential features driving
sentiment classification. The results underscore the effectiveness of
transformer-based models in processing low-resource languages such as Bengali
and demonstrate their potential to extract actionable insights that can support
public policy formulation and crime prevention strategies.

</details>


### [21] [Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach](https://arxiv.org/abs/2507.21242)
*Mohammad Mehadi Hasan,Fatema Binte Hassan,Md Al Jubair,Zobayer Ahmed,Sazzatul Yeakin,Md Masum Billah*

Main category: cs.CL

TL;DR: 本研究通过微调Bangla BERT模型，提高了对Bangla极端偏见新闻的分类准确性，并展示了Transformer模型在资源有限环境中的实用性。


<details>
  <summary>Details</summary>
Motivation: 在当前的数字环境中，虚假信息迅速传播，影响公众观点并导致社会分裂。由于缺乏针对这种低资源语言的复杂自然语言处理方法，难以识别Bangla的极端偏见新闻。没有有效的检测方法，有偏见的内容可能会不受限制地传播，对知情讨论构成严重风险。

Method: 我们的研究微调了Bangla BERT，这是一种基于Transformer的模型，旨在提高对极端偏见新闻的分类准确性。我们将其性能与传统机器学习模型进行了比较，并实施了半监督学习以进一步提高预测效果。此外，我们使用LIME来提供模型决策过程的透明解释，这有助于建立对其结果的信任。

Result: 根据试验数据，Bangla BERT的准确率得分高达95.65%，优于传统方法。

Conclusion: 研究结果表明，即使在资源有限的环境中，Transformer模型也具有实用性，这为该领域的进一步改进打开了大门。

Abstract: In the current digital landscape, misinformation circulates rapidly, shaping
public perception and causing societal divisions. It is difficult to identify
hyperpartisan news in Bangla since there aren't many sophisticated natural
language processing methods available for this low-resource language. Without
effective detection methods, biased content can spread unchecked, posing
serious risks to informed discourse. To address this gap, our research
fine-tunes Bangla BERT. This is a state-of-the-art transformer-based model,
designed to enhance classification accuracy for hyperpartisan news. We evaluate
its performance against traditional machine learning models and implement
semi-supervised learning to enhance predictions further. Not only that, we use
LIME to provide transparent explanations of the model's decision-making
process, which helps to build trust in its outcomes. With a remarkable accuracy
score of 95.65%, Bangla BERT outperforms conventional approaches, according to
our trial data. The findings of this study demonstrate the usefulness of
transformer models even in environments with limited resources, which opens the
door to further improvements in this area.

</details>


### [22] [Can human clinical rationales improve the performance and explainability of clinical text classification models?](https://arxiv.org/abs/2507.21302)
*Christoph Metzner,Shang Gao,Drahomira Herrmannova,Heidi A. Hanson*

Main category: cs.CL

TL;DR: 研究发现，使用临床理由作为额外训练数据对模型性能提升有限，且不如使用更多报告有效，但可能有助于提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究人类基于临床的理由是否可以作为额外的监督，以提高基于变压器的模型在自动编码临床文档中的性能和可解释性。

Method: 分析了99,125个基于人类的临床理由，并将其作为额外的训练样本，与128,649份电子病理报告一起用于评估基于变压器的模型以提取主要癌症部位。还研究了充分性作为衡量理由质量的方法，以预选理由。

Result: 临床理由作为额外的训练数据在高资源场景中可以提高模型性能，但在资源有限时表现不一致。使用充分性作为自动度量来预选理由也导致了不一致的结果。由理由训练的模型始终不如由更多报告训练的模型。

Conclusion: 使用临床理由作为额外的训练数据相比使用更多的报告只能带来较小的性能提升和略微更好的可解释性。

Abstract: AI-driven clinical text classification is vital for explainable automated
retrieval of population-level health information. This work investigates
whether human-based clinical rationales can serve as additional supervision to
improve both performance and explainability of transformer-based models that
automatically encode clinical documents. We analyzed 99,125 human-based
clinical rationales that provide plausible explanations for primary cancer site
diagnoses, using them as additional training samples alongside 128,649
electronic pathology reports to evaluate transformer-based models for
extracting primary cancer sites. We also investigated sufficiency as a way to
measure rationale quality for pre-selecting rationales. Our results showed that
clinical rationales as additional training data can improve model performance
in high-resource scenarios but produce inconsistent behavior when resources are
limited. Using sufficiency as an automatic metric to preselect rationales also
leads to inconsistent results. Importantly, models trained on rationales were
consistently outperformed by models trained on additional reports instead. This
suggests that clinical rationales don't consistently improve model performance
and are outperformed by simply using more reports. Therefore, if the goal is
optimizing accuracy, annotation efforts should focus on labeling more reports
rather than creating rationales. However, if explainability is the priority,
training models on rationale-supplemented data may help them better identify
rationale-like features. We conclude that using clinical rationales as
additional training data results in smaller performance improvements and only
slightly better explainability (measured as average token-level rationale
coverage) compared to training on additional reports.

</details>


### [23] [Do Large Language Models Understand Morality Across Cultures?](https://arxiv.org/abs/2507.21319)
*Hadi Mohammadi,Yasmeen F. S. S. Meijer,Efthymia Papadopoulou,Ayoub Bagheri*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型是否能够捕捉跨文化差异和相似性在道德观点上，并发现它们在再现跨文化道德变化方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 最近大型语言模型（LLMs）的进步已经确立了它们在众多领域的强大工具地位。然而，关于嵌入式偏见的持续担忧，例如由于训练数据而产生的性别、种族和文化偏见，引发了关于这些技术的伦理使用和社会后果的重要问题。

Method: 我们采用了三种互补的方法：(1) 比较模型产生的道德分数与调查中报告的方差，(2) 进行聚类对齐分析以评估从模型输出和调查数据中得出的国家分组之间的对应关系，(3) 使用系统选择的标记对直接探测模型。

Result: 当前的大型语言模型往往无法再现跨文化道德变化的全部范围，倾向于压缩差异，并与实证调查模式的对齐度较低。

Conclusion: 这些发现突显了需要更强大的方法来减轻偏见并提高大型语言模型中的文化代表性。我们最后讨论了负责任开发和全球部署大型语言模型的影响，强调公平性和伦理一致性。

Abstract: Recent advancements in large language models (LLMs) have established them as
powerful tools across numerous domains. However, persistent concerns about
embedded biases, such as gender, racial, and cultural biases arising from their
training data, raise significant questions about the ethical use and societal
consequences of these technologies. This study investigates the extent to which
LLMs capture cross-cultural differences and similarities in moral perspectives.
Specifically, we examine whether LLM outputs align with patterns observed in
international survey data on moral attitudes. To this end, we employ three
complementary methods: (1) comparing variances in moral scores produced by
models versus those reported in surveys, (2) conducting cluster alignment
analyses to assess correspondence between country groupings derived from LLM
outputs and survey data, and (3) directly probing models with comparative
prompts using systematically chosen token pairs. Our results reveal that
current LLMs often fail to reproduce the full spectrum of cross-cultural moral
variation, tending to compress differences and exhibit low alignment with
empirical survey patterns. These findings highlight a pressing need for more
robust approaches to mitigate biases and improve cultural representativeness in
LLMs. We conclude by discussing the implications for the responsible
development and global deployment of LLMs, emphasizing fairness and ethical
alignment.

</details>


### [24] [A Deep Learning Automatic Speech Recognition Model for Shona Language](https://arxiv.org/abs/2507.21331)
*Leslie Wellington Sirora,Mainford Mutandavari*

Main category: cs.CL

TL;DR: 本研究开发了一种基于深度学习的绍纳语自动语音识别系统，通过数据增强、迁移学习和注意力机制等策略提高了识别准确性。


<details>
  <summary>Details</summary>
Motivation: 绍纳语是一种低资源语言，具有独特的音调和语法复杂性，传统统计模型在识别准确性方面存在不足。因此，需要一种更有效的自动语音识别系统来提高准确性。

Method: 本研究采用了一种混合架构，包括卷积神经网络用于声学建模和长短期记忆网络用于语言建模，并利用数据增强技术和迁移学习来克服数据稀缺的问题。此外，还引入了注意力机制以适应绍纳语的音调特性。

Result: 所开发的自动语音识别系统取得了显著成果，词错误率为29%，音素错误率为12%，总体准确率为74%。

Conclusion: 本研究为解决绍纳语语音识别中的挑战提供了深度学习方法，展示了深度学习在提升低资源语言自动语音识别准确率方面的潜力。

Abstract: This study presented the development of a deep learning-based Automatic
Speech Recognition system for Shona, a low-resource language characterized by
unique tonal and grammatical complexities. The research aimed to address the
challenges posed by limited training data, lack of labelled data, and the
intricate tonal nuances present in Shona speech, with the objective of
achieving significant improvements in recognition accuracy compared to
traditional statistical models. The research first explored the feasibility of
using deep learning to develop an accurate ASR system for Shona. Second, it
investigated the specific challenges involved in designing and implementing
deep learning architectures for Shona speech recognition and proposed
strategies to mitigate these challenges. Lastly, it compared the performance of
the deep learning-based model with existing statistical models in terms of
accuracy. The developed ASR system utilized a hybrid architecture consisting of
a Convolutional Neural Network for acoustic modelling and a Long Short-Term
Memory network for language modelling. To overcome the scarcity of data, data
augmentation techniques and transfer learning were employed. Attention
mechanisms were also incorporated to accommodate the tonal nature of Shona
speech. The resulting ASR system achieved impressive results, with a Word Error
Rate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. These
metrics indicated the potential of deep learning to enhance ASR accuracy for
under-resourced languages like Shona. This study contributed to the advancement
of ASR technology for under-resourced languages like Shona, ultimately
fostering improved accessibility and communication for Shona speakers
worldwide.

</details>


### [25] [StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation](https://arxiv.org/abs/2507.21340)
*Satyananda Kashyap,Sola Shirai,Nandana Mihindukulasooriya,Horst Samulowitz*

Main category: cs.CL

TL;DR: 本文提出了StructText，这是一个端到端的框架，用于使用现有表格数据自动生成高质量的基准测试，用于从文本中提取键值对。该框架通过两阶段的“计划-执行”流程生成自然语言文本，并采用多维评估策略确保文本和结构化源之间的对齐。评估结果显示，尽管LLMs在事实准确性方面表现良好，但在叙述连贯性方面存在问题。作者还发布了框架，包括数据集、评估工具和基线提取系统，以支持持续研究。


<details>
  <summary>Details</summary>
Motivation: 提取从文本中结构化的信息，如键值对，可以增强表格数据，在许多企业用例中非常有用。虽然大型语言模型（LLMs）已经使许多自动管道能够将自然语言转换为结构化格式，但仍然缺乏评估其提取质量的基准测试，尤其是在特定领域或针对给定组织的聚焦文档中。通过手动注释构建这样的基准测试是劳动密集型的，并限制了基准测试的大小和可扩展性。

Method: 我们提出了StructText，这是一个端到端的框架，用于使用现有的表格数据自动生成高质量的基准测试，用于从文本中提取键值对。它使用可用的表格数据作为结构化的地面真实数据，并遵循一个两阶段的“计划-执行”流程来合成生成相应的自然语言文本。为了确保文本和结构化源之间的对齐，我们引入了一种多维评估策略，结合了（a）基于LLM的判断关于事实性、幻觉和连贯性，以及（b）客观的提取指标测量数值和时间准确性。

Result: 我们在49个数据集中的71,539个示例上评估了所提出的方法。结果表明，尽管LLMs在事实准确性方面表现强劲并避免了幻觉，但在生成可提取文本的叙述连贯性方面存在困难。值得注意的是，模型以高保真度假设数值和时间信息，但这些信息嵌入在抵抗自动提取的叙述中。

Conclusion: 我们释放了一个框架，包括数据集、评估工具和基线提取系统，以支持持续的研究。

Abstract: Extracting structured information from text, such as key-value pairs that
could augment tabular data, is quite useful in many enterprise use cases.
Although large language models (LLMs) have enabled numerous automated pipelines
for converting natural language into structured formats, there is still a lack
of benchmarks for evaluating their extraction quality, especially in specific
domains or focused documents specific to a given organization. Building such
benchmarks by manual annotations is labour-intensive and limits the size and
scalability of the benchmarks. In this work, we present StructText, an
end-to-end framework for automatically generating high-fidelity benchmarks for
key-value extraction from text using existing tabular data. It uses available
tabular data as structured ground truth, and follows a two-stage
``plan-then-execute'' pipeline to synthetically generate corresponding
natural-language text. To ensure alignment between text and structured source,
we introduce a multi-dimensional evaluation strategy that combines (a)
LLM-based judgments on factuality, hallucination, and coherence and (b)
objective extraction metrics measuring numeric and temporal accuracy. We
evaluated the proposed method on 71,539 examples across 49 datasets. Results
reveal that while LLMs achieve strong factual accuracy and avoid hallucination,
they struggle with narrative coherence in producing extractable text. Notably,
models presume numerical and temporal information with high fidelity yet this
information becomes embedded in narratives that resist automated extraction. We
release a framework, including datasets, evaluation tools, and baseline
extraction systems, to support continued research.

</details>


### [26] [Turbocharging Web Automation: The Impact of Compressed History States](https://arxiv.org/abs/2507.21369)
*Xiyue Zhu,Peng Tang,Haofu Liao,Srikar Appalaraju*

Main category: cs.CL

TL;DR: 本文提出了一种新的网页历史压缩方法，以利用历史状态来加速网页自动化，并在实验中取得了显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前的网页自动化方法忽略了历史状态的重要性，而网页页面状态的高度冗长性可能导致长输入序列和稀疏信息，阻碍了对历史状态的有效利用。

Method: 本文提出了一种网页历史压缩器模块，该模块将每个历史状态中的最相关任务信息提炼为固定长度的简短表示，从而减轻了高度冗长的历史状态带来的挑战。

Result: 在Mind2Web和WebLINX数据集上的实验结果表明，与不使用历史输入的基线方法相比，本文的方法获得了1.2-5.4%的绝对准确率提升。

Conclusion: 本文提出了一种新的网页历史压缩方法，以利用历史状态来加速网页自动化。实验结果表明，与不使用历史输入的基线方法相比，该方法在Mind2Web和WebLINX数据集上获得了1.2-5.4%的绝对准确率提升。

Abstract: Language models have led to a leap forward in web automation. The current web
automation approaches take the current web state, history actions, and language
instruction as inputs to predict the next action, overlooking the importance of
history states. However, the highly verbose nature of web page states can
result in long input sequences and sparse information, hampering the effective
utilization of history states. In this paper, we propose a novel web history
compressor approach to turbocharge web automation using history states. Our
approach employs a history compressor module that distills the most
task-relevant information from each history state into a fixed-length short
representation, mitigating the challenges posed by the highly verbose history
states. Experiments are conducted on the Mind2Web and WebLINX datasets to
evaluate the effectiveness of our approach. Results show that our approach
obtains 1.2-5.4% absolute accuracy improvements compared to the baseline
approach without history inputs.

</details>


### [27] [MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations](https://arxiv.org/abs/2507.21428)
*Elias Lumer,Anmol Gulati,Vamse Kumar Subbiah,Pradeep Honaganahalli Basavaraju,James A. Burke*

Main category: cs.CL

TL;DR: 本文提出了一种名为MemTool的短期记忆框架，用于解决LLM代理在多轮交互中的上下文限制问题。通过三种代理架构（自主代理模式、工作流模式和混合模式），实验显示不同模式在工具移除效率和任务完成准确性上有显著差异，并根据任务准确性、自主性和模型能力给出了相应的权衡和建议。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在多轮交互中受到固定上下文窗口的限制，无法有效进行重复且独立的工具使用。因此，需要一种新的框架来解决这一问题。

Method: 本文介绍了MemTool框架，该框架允许LLM代理在多轮对话中动态管理工具或MCP服务器上下文。三种代理架构分别是：1) 自主代理模式，赋予完全的工具管理自主权；2) 工作流模式，提供确定性控制而不具备自主性；3) 混合模式，结合自主性和确定性控制。

Result: 在13个以上的LLM上对MemTool的三种模式进行了评估，实验覆盖了100次连续的用户交互，测量了工具移除比率（短期记忆效率）和任务完成准确性。结果表明，在自主代理模式下，推理型LLM表现出高工具移除效率（3窗口平均90-94%），而中等规模模型的效率显著较低（0-60%）。工作流和混合模式在工具移除方面表现良好，而自主和混合模式在任务完成方面表现优异。

Conclusion: 本文提出了MemTool框架，通过三种代理架构（自主代理模式、工作流模式和混合模式）来解决LLM代理在多轮交互中因固定上下文窗口导致的效率问题。实验结果显示，不同模式在工具移除效率和任务完成准确性方面有显著差异，并根据任务准确性、自主性和模型能力给出了相应的权衡和建议。

Abstract: Large Language Model (LLM) agents have shown significant autonomous
capabilities in dynamically searching and incorporating relevant tools or Model
Context Protocol (MCP) servers for individual queries. However, fixed context
windows limit effectiveness in multi-turn interactions requiring repeated,
independent tool usage. We introduce MemTool, a short-term memory framework
enabling LLM agents to dynamically manage tools or MCP server contexts across
multi-turn conversations. MemTool offers three agentic architectures: 1)
Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow
Mode, providing deterministic control without autonomy, and 3) Hybrid Mode,
combining autonomous and deterministic control. Evaluating each MemTool mode
across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100
consecutive user interactions, measuring tool removal ratios (short-term memory
efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning
LLMs achieve high tool-removal efficiency (90-94% over a 3-window average),
while medium-sized models exhibit significantly lower efficiency (0-60%).
Workflow and Hybrid modes consistently manage tool removal effectively, whereas
Autonomous and Hybrid modes excel at task completion. We present trade-offs and
recommendations for each MemTool mode based on task accuracy, agency, and model
capabilities.

</details>


### [28] [Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour](https://arxiv.org/abs/2507.21432)
*Tareq Alsaleh,Bilal Farooq*

Main category: cs.CL

TL;DR: 本研究探讨了开放获取、本地部署的因果大型语言模型（LLM）在交通方式选择预测中的应用，并引入了LiTransMC，这是第一个为此任务微调的因果LLM。研究结果显示，LiTransMC在预测准确性和分布校准方面均表现出色，优于现有的方法和系统，展示了专门、本地部署的LLM在交通研究和政策制定中的潜力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索开放获取、本地部署的因果大型语言模型（LLM）在交通方式选择预测中的应用，并引入LiTransMC，这是第一个为此任务微调的因果LLM。

Method: 本研究系统地比较了十一个LLM（1-12B参数）在三个陈述和揭示偏好数据集上的表现，测试了396种配置，并生成了超过79,000个合成通勤预测。此外，还使用BERTopic进行主题建模和一种新的解释强度指数来评估模型生成的推理。LiTransMC是第一个为此任务微调的因果LLM，采用参数高效和损失掩码策略进行微调。

Result: LiTransMC在加权F1分数（0.6845）和Jensen-Shannon散度（0.000245）方面取得了优异的成绩，超过了未微调的本地模型和更大的专有系统，包括GPT-4o，同时也优于传统的模式选择方法，如离散选择模型和机器学习分类器。

Conclusion: 本研究展示了专门的、本地部署的因果大型语言模型（LLM）在交通方式选择预测中的可行性，同时实现了高精度和良好的分布校准。通过结合结构化的行为预测与自然语言推理，这项工作为交通研究和政策制定提供了可解释的工具，同时保持了隐私性、降低成本并扩大了访问范围。

Abstract: This study investigates the adoption of open-access, locally deployable
causal large language models (LLMs) for travel mode choice prediction and
introduces LiTransMC, the first fine-tuned causal LLM developed for this task.
We systematically benchmark eleven LLMs (1-12B parameters) across three stated
and revealed preference datasets, testing 396 configurations and generating
over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we
evaluate models generated reasoning using BERTopic for topic modelling and a
novel Explanation Strength Index, providing the first structured analysis of
how LLMs articulate decision factors in alignment with behavioural theory.
LiTransMC, fine-tuned using parameter efficient and loss masking strategy,
achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of
0.000245, surpassing both untuned local models and larger proprietary systems,
including GPT-4o with advanced persona inference and embedding-based loading,
while also outperforming classical mode choice methods such as discrete choice
models and machine learning classifiers for the same dataset. This dual
improvement, i.e., high instant-level accuracy and near-perfect distributional
calibration, demonstrates the feasibility of creating specialist, locally
deployable LLMs that integrate prediction and interpretability. Through
combining structured behavioural prediction with natural language reasoning,
this work unlocks the potential for conversational, multi-task transport models
capable of supporting agent-based simulations, policy testing, and behavioural
insight generation. These findings establish a pathway for transforming general
purpose LLMs into specialized, explainable tools for transportation research
and policy formulation, while maintaining privacy, reducing cost, and
broadening access through local deployment.

</details>


### [29] [Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench](https://arxiv.org/abs/2507.21476)
*Reuben Narad,Siddharth Suresh,Jiayi Chen,Pine S. L. Dysart-Bricken,Bob Mankoff,Robert Nowak,Jifan Zhang,Lalit Jain*

Main category: cs.CL

TL;DR: HumorBench是一个用于评估大型语言模型在漫画标题中推理和解释复杂幽默能力的基准。研究发现，当前最先进的模型在STEM推理上的进展可以有效地转移到幽默理解，但测试时的扩展效果因模型而异。


<details>
  <summary>Details</summary>
Motivation: 随着基于推理的模型在数学和科学领域基准测试中趋于饱和，需要新的、具有挑战性的评估方法来衡量模型在STEM领域以外的智能水平。文本幽默的理解本质上涉及推理，需要识别漫画/标题中的概念与外部文化参考、双关语和其他机制之间的联系。

Method: HumorBench包含大约300个来自《纽约客》漫画比赛和Cartoonstock.com的独特漫画-标题对，并有专家标注的评估标准来识别关键笑话元素。通过分析模型对幽默的解释和识别笑话元素的能力来评估LLMs。

Result: 对当前最先进的模型进行广泛的基准测试揭示了三个关键见解：(1) LLM在STEM推理上的进展可以有效地转移到幽默理解；(2) 仅在STEM推理数据上训练的模型在HumorBench上表现良好，展示了推理能力的强大可转移性；(3) 通过增加思考令牌预算进行测试时的扩展在不同模型的幽默推理中效果不一。

Conclusion: HumorBench是一个评估大型语言模型在漫画标题中推理和解释复杂幽默能力的基准。当前最先进的模型在STEM推理上的进展可以有效地转移到幽默理解上，但测试时的扩展效果因模型而异。

Abstract: We present HumorBench, a benchmark designed to evaluate large language
models' (LLMs) ability to reason about and explain sophisticated humor in
cartoon captions. As reasoning models increasingly saturate existing benchmarks
in mathematics and science, novel and challenging evaluations of model
intelligence beyond STEM domains are essential. Reasoning is fundamentally
involved in text-based humor comprehension, requiring the identification of
connections between concepts in cartoons/captions and external cultural
references, wordplays, and other mechanisms. HumorBench includes approximately
300 unique cartoon-caption pairs from the New Yorker Caption Contest and
Cartoonstock.com, with expert-annotated evaluation rubrics identifying
essential joke elements. LLMs are evaluated based on their explanations towards
the humor and abilities in identifying the joke elements. To perform well on
this task, models must form and test hypotheses about associations between
concepts, potentially backtracking from initial interpretations to arrive at
the most plausible explanation. Our extensive benchmarking of current SOTA
models reveals three key insights: (1) LLM progress on STEM reasoning transfers
effectively to humor comprehension; (2) models trained exclusively on STEM
reasoning data still perform well on HumorBench, demonstrating strong
transferability of reasoning abilities; and (3) test-time scaling by increasing
thinking token budgets yields mixed results across different models in humor
reasoning.

</details>


### [30] [Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs](https://arxiv.org/abs/2507.21482)
*Abhinav Arabelly,Jagrut Nemade,Robert D Nowak,Jifan Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于任务多样性的标签高效学习方法，通过逆置信度加权策略进行数据采样，能够在减少标注成本的同时获得与复杂采样方法相当或更好的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要基于提示多样性，而本文认为任务多样性是更有效的数据选择原则。开发高性能模型需要大量人工标注，这过程耗时、劳动密集且昂贵。

Method: 本文提出了一种基于任务多样性的标签高效学习方法，通过利用任务标签的可用性和预训练模型在不同任务上的置信度差异，采用逆置信度加权策略进行数据采样。

Result: 实验结果表明，该方法在多个标注预算和两个指令微调数据集上表现优异，同时将标注成本降低了多达80%。此外，在MMLU分数上，该方法比使用完整数据集训练的模型提高了4%。

Conclusion: 本文提出了一种基于任务多样性的有效数据选择方法，可以在减少标注成本的同时获得与复杂采样方法相当或更好的模型性能。实验结果表明，该方法在多个标注预算和两个指令微调数据集上表现优异，甚至在MMLU分数上比使用完整数据集训练的模型提高了4%。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, but developing high-performing models for specialized
applications often requires substantial human annotation -- a process that is
time-consuming, labor-intensive, and expensive. In this paper, we address the
label-efficient learning problem for supervised finetuning (SFT) by leveraging
task-diversity as a fundamental principle for effective data selection. This is
markedly different from existing methods based on the prompt-diversity. Our
approach is based on two key observations: 1) task labels for different prompts
are often readily available; 2) pre-trained models have significantly varying
levels of confidence across tasks. We combine these facts to devise a simple
yet effective sampling strategy: we select examples across tasks using an
inverse confidence weighting strategy. This produces models comparable to or
better than those trained with more complex sampling procedures, while being
significantly easier to implement and less computationally intensive. Notably,
our experimental results demonstrate that this method can achieve better
accuracy than training on the complete dataset (a 4\% increase in MMLU score).
Across various annotation budgets and two instruction finetuning datasets, our
algorithm consistently performs at or above the level of the best existing
methods, while reducing annotation costs by up to 80\%.

</details>


### [31] [VN-MTEB: Vietnamese Massive Text Embedding Benchmark](https://arxiv.org/abs/2507.21500)
*Loc Pham,Tung Luu,Thu Vo,Minh Nguyen,Viet Hoang*

Main category: cs.CL

TL;DR: 本文创建了越南语嵌入模型基准VN-MTEB，以解决越南在互联网流量和在线毒性方面的挑战，并为AI模型的评估提供支持。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模测试数据集，科学家难以有效评估AI模型，因此需要创建一个专门针对越南语文本嵌入的基准。

Method: 通过使用新的自动化框架将大量英文样本从Massive Text Embedding Benchmark翻译成越南语，同时利用大语言模型和先进的嵌入模型进行翻译和过滤过程，以保留高质量样本。

Result: 本文创建了一个包含41个数据集的全面基准，涵盖六个任务，用于越南语文本嵌入。分析发现，使用Rotary Positional Embedding的更大、更复杂的模型在嵌入任务中表现优于使用Absolute Positional Embedding的模型。

Conclusion: 本文介绍了越南语嵌入模型基准VN-MTEB，旨在解决越南在互联网流量和在线毒性方面的挑战，并为AI模型的评估提供支持。

Abstract: Vietnam ranks among the top countries in terms of both internet traffic and
online toxicity. As a result, implementing embedding models for recommendation
and content control duties in applications is crucial. However, a lack of
large-scale test datasets, both in volume and task diversity, makes it tricky
for scientists to effectively evaluate AI models before deploying them in
real-world, large-scale projects. To solve this important problem, we introduce
a Vietnamese benchmark, VN-MTEB for embedding models, which we created by
translating a large number of English samples from the Massive Text Embedding
Benchmark using our new automated framework. We leverage the strengths of large
language models (LLMs) and cutting-edge embedding models to conduct translation
and filtering processes to retain high-quality samples, guaranteeing a natural
flow of language and semantic fidelity while preserving named entity
recognition (NER) and code snippets. Our comprehensive benchmark consists of 41
datasets from six tasks specifically designed for Vietnamese text embeddings.
In our analysis, we find that bigger and more complex models using Rotary
Positional Embedding outperform those using Absolute Positional Embedding in
embedding tasks. Datasets are available at HuggingFace:
https://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686

</details>


### [32] [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)
*Runjin Chen,Andy Arditi,Henry Sleight,Owain Evans,Jack Lindsey*

Main category: cs.CL

TL;DR: 本文提出了一种方法，通过分析模型的激活空间中的'人格向量'来监测和控制大型语言模型的个性变化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常被训练成有益、无害和诚实的，但有时会偏离这些理想。我们需要一种方法来监测和控制这些个性变化，以确保模型的行为符合预期。

Method: 我们通过分析模型的激活空间中的'人格向量'来识别几种特性，如邪恶、奉承和产生幻觉的倾向。我们使用这些向量来监控部署时助手个性的变化，并应用它们来预测和控制训练期间发生的个性变化。

Result: 我们发现，微调后的有意和无意的个性变化与相关人格向量的移动有很强的相关性。这些变化可以通过事后干预来减轻，或者通过一种新的预防性引导方法来避免。此外，人格向量可以用来标记会导致不良个性变化的训练数据。

Conclusion: 我们的方法可以自动提取人格向量，并且可以在仅提供自然语言描述的情况下应用于任何感兴趣的人格特质。

Abstract: Large language models interact with users through a simulated 'Assistant'
persona. While the Assistant is typically trained to be helpful, harmless, and
honest, it sometimes deviates from these ideals. In this paper, we identify
directions in the model's activation space-persona vectors-underlying several
traits, such as evil, sycophancy, and propensity to hallucinate. We confirm
that these vectors can be used to monitor fluctuations in the Assistant's
personality at deployment time. We then apply persona vectors to predict and
control personality shifts that occur during training. We find that both
intended and unintended personality changes after finetuning are strongly
correlated with shifts along the relevant persona vectors. These shifts can be
mitigated through post-hoc intervention, or avoided in the first place with a
new preventative steering method. Moreover, persona vectors can be used to flag
training data that will produce undesirable personality changes, both at the
dataset level and the individual sample level. Our method for extracting
persona vectors is automated and can be applied to any personality trait of
interest, given only a natural-language description.

</details>


### [33] [Model-free Speculative Decoding for Transformer-based ASR with Token Map Drafting](https://arxiv.org/abs/2507.21522)
*Tuan Vu Ho,Hiroaki Kokubo,Masaaki Yamamoto,Yohei Kawaguchi*

Main category: cs.CL

TL;DR: 本文提出了一种名为Token Map Drafting的模型无关推测解码技术，通过利用预计算的n-gram token map实现高效的推测解码，显著加速了ASR推理，同时保持了转录准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码方法需要单独的草稿模型，这在缺乏硬件加速器（如GPU）的设备上不切实际。因此，我们需要一种不需要单独草稿模型的高效推测解码方法。

Method: 我们提出了Token Map Drafting，这是一种无需单独草稿模型的模型无关的SD技术。我们利用从领域特定训练数据中预计算的n-gram token map，实现了高效的推测解码。

Result: 实验结果表明，在CI-AVSR数据集和内部数据集上，解码速度分别提高了1.27倍和1.37倍，且没有降低识别准确率。此外，与运行在CPU上的Distill-spec基线相比，我们的方法在解码速度上提高了10%的绝对值。

Conclusion: 我们的方法在结构化、低困惑度领域显著加速了ASR推理，同时保持了转录准确性。

Abstract: End-to-end automatic speech recognition (ASR) systems based on transformer
architectures, such as Whisper, offer high transcription accuracy and
robustness. However, their autoregressive decoding is computationally
expensive, hence limiting deployment on CPU-based and resource-constrained
devices. Speculative decoding (SD) mitigates this issue by using a smaller
draft model to propose candidate tokens, which are then verified by the main
model. However, this approach is impractical for devices lacking hardware
accelerators like GPUs. To address this, we propose \emph{Token Map Drafting},
a model-free SD technique that eliminates the need for a separate draft model.
Instead, we leverage a precomputed n-gram token map derived from
domain-specific training data, enabling efficient speculative decoding with
minimal overhead. Our method significantly accelerates ASR inference in
structured, low-perplexity domains without sacrificing transcription accuracy.
Experimental results demonstrate decoding speed-ups of $1.27\times$ on the
CI-AVSR dataset and $1.37\times$ on our internal dataset without degrading
recognition accuracy. Additionally, our approach achieves a $10\%$ absolute
improvement in decoding speed over the Distill-spec baseline running on CPU,
highlighting its effectiveness for on-device ASR applications.

</details>


### [34] [TriangleMix: A Lossless and Efficient Attention Pattern for Long Context Prefilling](https://arxiv.org/abs/2507.21526)
*Zhiyuan He,Yike Zhang,Chengruidong Zhang,Huiqiang Jiang,Yuqing Yang,Lili Qiu*

Main category: cs.CL

TL;DR: TriangleMix is a novel training-free static attention pattern that reduces attention overhead and improves inference efficiency in large language models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing static sparse attention methods and dynamic sparsity methods, which either degrade accuracy or introduce additional computational overhead.

Method: TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped sparse pattern in deeper layers.

Result: TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy.

Conclusion: TriangleMix can be seamlessly integrated with dynamic sparsity methods to achieve further speedup, highlighting its potential to enhance LLM inference efficiency.

Abstract: Large Language Models (LLMs) rely on attention mechanisms whose time
complexity grows quadratically with input sequence length, creating significant
computational bottlenecks during the prefilling stage. Existing static sparse
attention methods typically degrade accuracy, while dynamic sparsity methods
introduce additional computational overhead due to runtime sparse index
estimation. To address these limitations, we propose TriangleMix, a novel
training-free static attention pattern. TriangleMix employs dense attention in
shallow layers and switches to a triangle-shaped sparse pattern in deeper
layers. Extensive experiments demonstrate that TriangleMix reduces attention
overhead by 3.7x to 15.3x in deep layers, and decreases overall
Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K
to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be
seamlessly integrated with dynamic sparsity methods to achieve further speedup,
e.g. accelerating MInference by 19% at 128K, highlighting its potential to
enhance LLM inference efficiency.

</details>


### [35] [Automatic Classification of User Requirements from Online Feedback -- A Replication Study](https://arxiv.org/abs/2507.21532)
*Meet Bhatt,Nic Boilard,Muhammad Rehan Chaudhary,Cole Thompson,Jacob Idoko,Aakash Sorathiya,Gouri Ginde*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Natural language processing (NLP) techniques have been widely applied in the
requirements engineering (RE) field to support tasks such as classification and
ambiguity detection. Although RE research is rooted in empirical investigation,
it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The
rapidly advancing realm of NLP is creating new opportunities for efficient,
machine-assisted workflows, which can bring new perspectives and results to the
forefront. Thus, we replicate and extend a previous NLP4RE study (baseline),
"Classifying User Requirements from Online Feedback in Small Dataset
Environments using Deep Learning", which evaluated different deep learning
models for requirement classification from user reviews. We reproduced the
original results using publicly released source code, thereby helping to
strengthen the external validity of the baseline study. We then extended the
setup by evaluating model performance on an external dataset and comparing
results to a GPT-4o zero-shot classifier. Furthermore, we prepared the
replication study ID-card for the baseline study, important for evaluating
replication readiness. Results showed diverse reproducibility levels across
different models, with Naive Bayes demonstrating perfect reproducibility. In
contrast, BERT and other models showed mixed results. Our findings revealed
that baseline deep learning models, BERT and ELMo, exhibited good
generalization capabilities on an external dataset, and GPT-4o showed
performance comparable to traditional baseline machine learning models.
Additionally, our assessment confirmed the baseline study's replication
readiness; however missing environment setup files would have further enhanced
readiness. We include this missing information in our replication package and
provide the replication study ID-card for our study to further encourage and
support the replication of our study.

</details>


### [36] [Modern Uyghur Dependency Treebank (MUDT): An Integrated Morphosyntactic Framework for a Low-Resource Language](https://arxiv.org/abs/2507.21536)
*Jiaxin Zuo,Yiquan Wang,Yuan Pan,Xiadiya Yibulayin*

Main category: cs.CL

TL;DR: 该研究提出了一种针对维吾尔语的依赖标注框架，解决了现有树库的不足，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的树库无法满足维吾尔语这一低资源、屈折语言的需求，因此需要一个专门的依赖标注框架来克服这些限制。

Method: 该研究设计了一个包含18个主要关系和26个子类型的依赖标注框架，并通过使用预训练的Universal Dependencies解析器进行跨标准评估来验证其必要性。

Result: 评估结果显示，标注存在系统性的47.9%偏差，表明通用方案在处理维吾尔语特定结构时存在不足。

Conclusion: 该研究引入了一个依赖标注框架，以解决维吾尔语自然语言处理（NLP）中的关键资源缺口。该框架提供了更准确和语义透明的表示，旨在显著提升解析和下游NLP任务，并为其他形态复杂的语言提供可复制的模型。

Abstract: To address a critical resource gap in Uyghur Natural Language Processing
(NLP), this study introduces a dependency annotation framework designed to
overcome the limitations of existing treebanks for the low-resource,
agglutinative language. This inventory includes 18 main relations and 26
subtypes, with specific labels such as cop:zero for verbless clauses and
instr:case=loc/dat for nuanced instrumental functions. To empirically validate
the necessity of this tailored approach, we conducted a cross-standard
evaluation using a pre-trained Universal Dependencies parser. The analysis
revealed a systematic 47.9% divergence in annotations, pinpointing the
inadequacy of universal schemes for handling Uyghur-specific structures.
Grounded in nine annotation principles that ensure typological accuracy and
semantic transparency, the Modern Uyghur Dependency Treebank (MUDT) provides a
more accurate and semantically transparent representation, designed to enable
significant improvements in parsing and downstream NLP tasks, and offers a
replicable model for other morphologically complex languages.

</details>


### [37] [MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation](https://arxiv.org/abs/2507.21544)
*Jungyeon Lee,Kangmin Lee,Taeuk Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱的框架，用于生成多样且微妙的知识冲突，并通过KG的显式关系结构确保可解释性。实验结果表明，大语言模型在处理知识冲突时存在困难，尤其是在需要多跳推理的情况下。最后，本文提供了深入的分析，为改进大语言模型整合多样化甚至冲突的信息奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有的研究基准在问题回答设置、实体替换技术和冲突类型范围方面存在明显局限。因此，本文旨在解决这些问题，提出一种更全面的知识冲突生成框架。

Method: 本文提出了一种基于知识图谱（KG）的框架，用于生成多样且微妙的知识冲突，并通过KG的显式关系结构确保可解释性。

Result: 在MAGIC基准上的实验结果揭示了大语言模型在处理知识冲突方面的内在机制：无论是开源还是专有模型，在处理需要多跳推理的冲突时都表现出困难，并且常常无法准确识别矛盾的来源。

Conclusion: 本文提出了一个基于知识图谱的框架，以生成多样且微妙的知识冲突，并通过知识图谱的显式关系结构确保可解释性。实验结果表明，无论是开源还是专有模型在处理知识冲突时都存在困难，尤其是在需要多跳推理的情况下。最后，本文提供了深入的分析，为改进大语言模型整合多样化甚至冲突的信息奠定了基础。

Abstract: Knowledge conflict often arises in retrieval-augmented generation (RAG)
systems, where retrieved documents may be inconsistent with one another or
contradict the model's parametric knowledge. Existing benchmarks for
investigating the phenomenon have notable limitations, including a narrow focus
on the question answering setup, heavy reliance on entity substitution
techniques, and a restricted range of conflict types. To address these issues,
we propose a knowledge graph (KG)-based framework that generates varied and
subtle conflicts between two similar yet distinct contexts, while ensuring
interpretability through the explicit relational structure of KGs. Experimental
results on our benchmark, MAGIC, provide intriguing insights into the inner
workings of LLMs regarding knowledge conflict: both open-source and proprietary
models struggle with conflict detection -- especially when multi-hop reasoning
is required -- and often fail to pinpoint the exact source of contradictions.
Finally, we present in-depth analyses that serve as a foundation for improving
LLMs in integrating diverse, sometimes even conflicting, information.

</details>


### [38] [Evaluating the cognitive reality of Spanish irregular morphomic patterns: Humans vs. Transformers](https://arxiv.org/abs/2507.21556)
*Akhilesh Kakolu Ramarao,Kevin Tang,Dinah Baer-Henney*

Main category: cs.CL

TL;DR: 研究比较了基于变压器的模型与人类对西班牙语不规则形态模式的反应，发现模型在准确性上优于人类，但在响应偏好上与人类存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究西班牙语不规则形态模式的认知合理性，探讨模型是否能模拟人类对语言现象的反应。

Method: 通过比较基于变压器的神经网络与人类行为数据，评估模型是否能复制人类对复杂语言现象的敏感性。实验包括自然、低频和高频动词分布条件。

Result: 模型在词干和后缀准确性上表现优于人类，但响应偏好与人类不同。模型对训练数据中的不规则动词比例敏感，并在自然和低频分布下对语音相似性有反应。

Conclusion: 模型在词干和后缀准确性上优于人类，但在响应偏好上与人类存在明显差异。模型的偏好受到训练数据中不规则动词比例的影响，并且在自然和低频分布下对语音相似性敏感。

Abstract: This study investigates the cognitive plausibility of the Spanish irregular
morphomic pattern by directly comparing transformer-based neural networks to
human behavioral data from \citet{Nevins2015TheRA}. Using the same analytical
framework as the original human study, we evaluate whether transformer models
can replicate human-like sensitivity to a complex linguistic phenomena, the
morphome, under controlled input conditions. Our experiments focus on three
frequency conditions: natural, low-frequency, and high-frequency distributions
of verbs exhibiting irregular morphomic patterns. While the models outperformed
humans in stem and suffix accuracy, a clear divergence emerged in response
preferences. Unlike humans, who consistently favored natural responses across
all test items, models' preferred irregular responses and were influenced by
the proportion of irregular verbs in their training data. Additionally, models
trained on the natural and low-frequency distributions, but not the
high-frequency distribution, were sensitive to the phonological similarity
between test items and real Spanish L-shaped verbs.

</details>


### [39] [Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages](https://arxiv.org/abs/2507.21568)
*Aarón Galiano-Jiménez,Juan Antonio Pérez-Ortiz,Felipe Sánchez-Martínez,Víctor M. Sánchez-Cartagena*

Main category: cs.CL

TL;DR: 本文探讨了多语言预训练编码器-解码器翻译模型的序列级知识蒸馏。我们提出了多假设蒸馏（MHD）方法，通过生成多个翻译来提供教师模型分布的更大表示，并让学生模型接触到更广泛的靶向前缀。研究显示，对于低资源语言，采样方法虽可能略微降低翻译质量，但能增强生成语料库的多样性与词汇丰富性，从而提高学生模型的性能并减轻KD常伴随的性别偏见放大问题。


<details>
  <summary>Details</summary>
Motivation: 我们认为教师模型的输出分布除了通过束搜索获得的近似模式外，还包含有价值的信息，因此需要一种能够生成多个翻译的序列级知识蒸馏方法。

Method: 我们提出了多假设蒸馏（MHD），这是一种序列级知识蒸馏方法，通过为每个源句生成多个翻译来提供教师模型分布的更大表示，并让学生模型接触到更广泛的靶向前缀。我们利用束搜索的n-best列表来指导学生的学习，并检查其他解码方法以解决低可变性和罕见词不足的问题。

Result: 我们的研究显示，对于低资源语言，虽然采样方法可能略微降低翻译质量，但它们能增强生成语料库的多样性与词汇丰富性，从而提高学生模型的性能并减轻KD常伴随的性别偏见放大问题。

Conclusion: 我们的研究显示，对于低资源语言，虽然采样方法可能略微降低翻译质量，但它们能增强生成语料库的多样性与词汇丰富性，从而提高学生模型的性能并减轻KD常伴随的性别偏见放大问题。

Abstract: This paper explores sequence-level knowledge distillation (KD) of
multilingual pre-trained encoder-decoder translation models. We argue that the
teacher model's output distribution holds valuable insights for the student,
beyond the approximated mode obtained through beam search (the standard
decoding method), and present Multi-Hypothesis Distillation (MHD), a
sequence-level KD method that generates multiple translations for each source
sentence. This provides a larger representation of the teacher model
distribution and exposes the student model to a wider range of target-side
prefixes. We leverage $n$-best lists from beam search to guide the student's
learning and examine alternative decoding methods to address issues like low
variability and the under-representation of infrequent tokens. For low-resource
languages, our research shows that while sampling methods may slightly
compromise translation quality compared to beam search based approaches, they
enhance the generated corpora with greater variability and lexical richness.
This ultimately improves student model performance and mitigates the gender
bias amplification often associated with KD.

</details>


### [40] [Multilingual JobBERT for Cross-Lingual Job Title Matching](https://arxiv.org/abs/2507.21609)
*Jens-Joris Decorte,Matthias De Lange,Jeroen Van Hautte*

Main category: cs.CL

TL;DR: JobBERT-V3是一个基于对比学习的跨语言职位标题匹配模型，支持多种语言并表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了提高跨语言职位标题匹配的性能，并扩展模型的多语言支持。

Method: JobBERT-V3基于对比学习，利用合成翻译和平衡的多语言数据集扩展了对英语、德语、西班牙语和中文的支持。

Result: JobBERT-V3在TalentCLEF 2025基准测试中优于强大的多语言基线，并在单语言和跨语言设置中表现出一致的性能。

Conclusion: JobBERT-V3在跨语言职位标题匹配任务中表现出色，并且在多语言劳动力市场情报中有更广泛的应用潜力。

Abstract: We introduce JobBERT-V3, a contrastive learning-based model for cross-lingual
job title matching. Building on the state-of-the-art monolingual JobBERT-V2,
our approach extends support to English, German, Spanish, and Chinese by
leveraging synthetic translations and a balanced multilingual dataset of over
21 million job titles. The model retains the efficiency-focused architecture of
its predecessor while enabling robust alignment across languages without
requiring task-specific supervision. Extensive evaluations on the TalentCLEF
2025 benchmark demonstrate that JobBERT-V3 outperforms strong multilingual
baselines and achieves consistent performance across both monolingual and
cross-lingual settings. While not the primary focus, we also show that the
model can be effectively used to rank relevant skills for a given job title,
demonstrating its broader applicability in multilingual labor market
intelligence. The model is publicly available:
https://huggingface.co/TechWolf/JobBERT-v3.

</details>


### [41] [Libra: Assessing and Improving Reward Model by Learning to Think](https://arxiv.org/abs/2507.21645)
*Meng Zhou,Bei Li,Jiahao Liu,Xiaowen Shi,Yang Bai,Rongxiang Weng,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，用于评估和改进奖励模型在复杂推理场景中的性能。通过构建Libra Bench基准和开发Libra-RM系列生成式奖励模型，实验结果展示了其在提升推理模型方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前的奖励模型在挑战性推理场景中表现不佳，且主要依赖于精细标注的参考答案和受限的输出格式，这限制了RL数据的扩展和模型推理性能的持续提升。

Method: 本文首先提出一个面向推理的基准（Libra Bench），从多样化的挑战性数学问题和先进推理模型中构建，以解决现有奖励模型基准在推理场景中的局限性。然后引入一种新的方法，通过学习-思考的方法来改进生成式奖励模型。基于该方法，开发了具有推理能力的Libra-RM系列生成式奖励模型。

Result: Libra-RM系列生成式奖励模型在多个基准上取得了最先进的结果，并通过下游实验验证了Libra Bench与下游应用的相关性以及Libra-RM在使用未标记数据提升推理模型的潜力。

Conclusion: 本文提出了一个全面的框架，用于评估和改进奖励模型在复杂推理场景中的性能。通过引入Libra Bench基准和Libra-RM系列生成式奖励模型，实验结果表明了其在推理模型上的潜力。

Abstract: Reinforcement learning (RL) has significantly improved the reasoning ability
of large language models. However, current reward models underperform in
challenging reasoning scenarios and predominant RL training paradigms rely on
rule-based or reference-based rewards, which impose two critical limitations:
1) the dependence on finely annotated reference answer to attain rewards; and
2) the requirement for constrained output format. These limitations
fundamentally hinder further RL data scaling and sustained enhancement of model
reasoning performance. To address these limitations, we propose a comprehensive
framework for evaluating and improving the performance of reward models in
complex reasoning scenarios. We first present a reasoning-oriented benchmark
(Libra Bench), systematically constructed from a diverse collection of
challenging mathematical problems and advanced reasoning models, to address the
limitations of existing reward model benchmarks in reasoning scenarios. We
further introduce a novel approach for improving the generative reward model
via learning-to-think methodologies. Based on the proposed approach, we develop
Libra-RM series, a collection of generative reward models with reasoning
capabilities that achieve state-of-the-art results on various benchmarks.
Comprehensive downstream experiments are conducted and the experimental results
demonstrate the correlation between our Libra Bench and downstream application,
and the potential of Libra-RM to further improve reasoning models with
unlabeled data.

</details>


### [42] [UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases](https://arxiv.org/abs/2507.21652)
*Raj Vardhan Tomar,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 本文介绍了UnsafeChain数据集，用于解决大型推理模型中的安全挑战，通过纠正不安全行为来提高模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基于SFT的安全对齐研究主要关注过滤具有安全、高质量响应的提示，而忽略了总是引发有害输出的硬提示。

Method: UnsafeChain数据集是从具有多样来源的硬提示构建的，其中不安全的完成被识别并明确更正为安全响应。

Result: UnsafeChain在六个分布外和五个分布内基准测试中优于之前的数据库，甚至1K子集也能达到或超过基线性能。

Conclusion: UnsafeChain在多个基准测试中表现出色，证明了基于纠正的监督的有效性和泛化能力。

Abstract: As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT)
reasoning introduces new safety challenges. Existing SFT-based safety alignment
studies dominantly focused on filtering prompts with safe, high-quality
responses, while overlooking hard prompts that always elicit harmful outputs.
To fill this gap, we introduce UnsafeChain, a safety alignment dataset
constructed from hard prompts with diverse sources, where unsafe completions
are identified and explicitly corrected into safe responses. By exposing models
to unsafe behaviors and guiding their correction, UnsafeChain enhances safety
while preserving general reasoning ability. We fine-tune three LRMs on
UnsafeChain and compare them against recent SafeChain and STAR-1 across six
out-of-distribution and five in-distribution benchmarks. UnsafeChain
consistently outperforms prior datasets, with even a 1K subset matching or
surpassing baseline performance, demonstrating the effectiveness and
generalizability of correction-based supervision. We release our dataset and
code at https://github.com/mbzuai-nlp/UnsafeChain

</details>


### [43] [Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal](https://arxiv.org/abs/2507.21750)
*Yang Wang,Chenghao Xiao,Yizhi Li,Stuart E. Middleton,Noura Al Moubayed,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文提出了一种简单有效的附加模块，通过移除实例级主成分来增强预训练语言模型的对抗鲁棒性，而无需依赖传统对抗防御或扰动原始训练数据。该方法通过将嵌入空间转换为近似高斯特性，减少其对对抗扰动的敏感性，同时保留语义关系，从而提高鲁棒性。实验结果表明，该方法在保持与基线相当的攻击前准确率的同时，提高了对抗鲁棒性，实现了鲁棒性和泛化之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在自然语言处理中取得了显著进展，但对对抗攻击仍然脆弱，这引发了对其在现实应用中鲁棒性的担忧。以往的研究通过在训练过程中引入对抗扰动来减轻对抗攻击的影响，但这些策略通常会带来高昂的计算成本。

Method: 我们提出了一种简单的附加模块，通过移除实例级主成分来增强预训练语言模型的对抗鲁棒性，而不依赖传统的对抗防御或扰动原始训练数据。这种方法将嵌入空间转换为近似高斯特性，从而减少其对对抗扰动的敏感性，同时保留语义关系。

Result: 在八个基准数据集上的评估表明，我们的方法在保持与基线相当的攻击前准确率的同时，提高了对抗鲁棒性，实现了鲁棒性和泛化之间的平衡。

Conclusion: 我们的方法在保持与基线相当的攻击前准确率的同时，提高了对抗鲁棒性，实现了鲁棒性和泛化之间的平衡。

Abstract: Pre-trained language models (PLMs) have driven substantial progress in
natural language processing but remain vulnerable to adversarial attacks,
raising concerns about their robustness in real-world applications. Previous
studies have sought to mitigate the impact of adversarial attacks by
introducing adversarial perturbations into the training process, either
implicitly or explicitly. While both strategies enhance robustness, they often
incur high computational costs. In this work, we propose a simple yet effective
add-on module that enhances the adversarial robustness of PLMs by removing
instance-level principal components, without relying on conventional
adversarial defences or perturbing the original training data. Our approach
transforms the embedding space to approximate Gaussian properties, thereby
reducing its susceptibility to adversarial perturbations while preserving
semantic relationships. This transformation aligns embedding distributions in a
way that minimises the impact of adversarial noise on decision boundaries,
enhancing robustness without requiring adversarial examples or costly
training-time augmentation. Evaluations on eight benchmark datasets show that
our approach improves adversarial robustness while maintaining comparable
before-attack accuracy to baselines, achieving a balanced trade-off between
robustness and generalisation.

</details>


### [44] [AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models](https://arxiv.org/abs/2507.21773)
*Lian Yan,Haotian Wang,Chen Tang,Haifeng Liu,Tianyang Sun,Liangliang Liu,Yi Guan,Jingchi Jiang*

Main category: cs.CL

TL;DR: The paper proposes AgriEval, the first comprehensive Chinese agricultural benchmark, which addresses the lack of training data and evaluation benchmarks for large language models (LLMs) in the agricultural domain. The benchmark includes six major agriculture categories and 29 subcategories, with high-quality data curated from university-level examinations and assignments. The paper also presents experimental results over 51 open-source and commercial LLMs, revealing that most existing LLMs struggle to achieve 60% accuracy, highlighting the developmental potential in agricultural LLMs.


<details>
  <summary>Details</summary>
Motivation: The deployment of large language models (LLMs) in the agricultural domain is hindered by the lack of training data and evaluation benchmarks. To mitigate this issue, the paper proposes AgriEval, the first comprehensive Chinese agricultural benchmark.

Method: The paper proposes AgriEval, a comprehensive Chinese agricultural benchmark with three main characteristics: comprehensive capability evaluation, high-quality data, and diverse formats and extensive scale. The dataset is curated from university-level examinations and assignments, and the paper presents experimental results over 51 open-source and commercial LLMs.

Result: The experimental results reveal that most existing LLMs struggle to achieve 60% accuracy, underscoring the developmental potential in agricultural LLMs. Additionally, extensive experiments are conducted to investigate factors influencing model performance and propose strategies for enhancement.

Conclusion: AgriEval is a comprehensive Chinese agricultural benchmark that addresses the lack of training data and evaluation benchmarks for large language models (LLMs) in the agricultural domain. The experimental results show that most existing LLMs struggle to achieve 60% accuracy, highlighting the developmental potential in agricultural LLMs. Additionally, strategies for enhancement are proposed.

Abstract: In the agricultural domain, the deployment of large language models (LLMs) is
hindered by the lack of training data and evaluation benchmarks. To mitigate
this issue, we propose AgriEval, the first comprehensive Chinese agricultural
benchmark with three main characteristics: (1) Comprehensive Capability
Evaluation. AgriEval covers six major agriculture categories and 29
subcategories within agriculture, addressing four core cognitive scenarios:
memorization, understanding, inference, and generation. (2) High-Quality Data.
The dataset is curated from university-level examinations and assignments,
providing a natural and robust benchmark for assessing the capacity of LLMs to
apply knowledge and make expert-like decisions. (3) Diverse Formats and
Extensive Scale. AgriEval comprises 14,697 multiple-choice questions and 2,167
open-ended question-and-answer questions, establishing it as the most extensive
agricultural benchmark available to date. We also present comprehensive
experimental results over 51 open-source and commercial LLMs. The experimental
results reveal that most existing LLMs struggle to achieve 60% accuracy,
underscoring the developmental potential in agricultural LLMs. Additionally, we
conduct extensive experiments to investigate factors influencing model
performance and propose strategies for enhancement. AgriEval is available at
https://github.com/YanPioneer/AgriEval/.

</details>


### [45] [The Problem with Safety Classification is not just the Models](https://arxiv.org/abs/2507.21782)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: 本文研究了多语言环境下安全分类模型的差异，并指出评估数据集可能存在潜在问题，强调了开发更好方法来识别跨语言有害内容的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）对不安全行为的鲁棒性是一个重要的研究课题。构建安全分类模型或守护模型，用于对LLMs的输入/输出进行安全分类，被视为解决该问题的一种方法。然而，目前对这类安全分类器或用于测试它们的评估数据集的有效性研究较少，尤其是在多语言场景中。

Method: 本文通过考虑涵盖18种语言的数据集，展示了5个安全分类模型中的多语言差异，并指出了评估数据集的潜在问题。

Result: 本文展示了5个安全分类模型在多语言场景中的差异，并指出了评估数据集的潜在问题。

Conclusion: 本文的结论是，当前安全分类器的不足不仅仅是模型本身的问题，还可能与评估数据集有关。我们希望这些发现能为开发更好的方法来识别跨语言的有害内容提供参考。

Abstract: Studying the robustness of Large Language Models (LLMs) to unsafe behaviors
is an important topic of research today. Building safety classification models
or guard models, which are fine-tuned models for input/output safety
classification for LLMs, is seen as one of the solutions to address the issue.
Although there is a lot of research on the safety testing of LLMs themselves,
there is little research on evaluating the effectiveness of such safety
classifiers or the evaluation datasets used for testing them, especially in
multilingual scenarios. In this position paper, we demonstrate how multilingual
disparities exist in 5 safety classification models by considering datasets
covering 18 languages. At the same time, we identify potential issues with the
evaluation datasets, arguing that the shortcomings of current safety
classifiers are not only because of the models themselves. We expect that these
findings will contribute to the discussion on developing better methods to
identify harmful content in LLM inputs across languages.

</details>


### [46] [ChartMark: A Structured Grammar for Chart Annotation](https://arxiv.org/abs/2507.21810)
*Yiyu Chen,Yifan Wu,Shuyu Shen,Yupeng Xie,Leixian Shen,Hui Xiong,Yuyu Luo*

Main category: cs.CL

TL;DR: 本文提出了 ChartMark，一种结构化语法，用于解决图表注释的碎片化和非标准化问题，提升跨平台的可重用性。


<details>
  <summary>Details</summary>
Motivation: 现有的图表注释存在碎片化和非标准化的问题，限制了跨平台的重用性。

Method: 提出了一种名为 ChartMark 的结构化语法，通过分层框架映射到注释维度，支持抽象意图和精确视觉细节。

Result: 展示了将 ChartMark 规范转换为 Vega-Lite 可视化的工具，突显了其灵活性、表达能力和实际应用价值。

Conclusion: ChartMark 提供了一种结构化的语法，能够将注释语义与可视化实现分离，从而提高跨平台的可重用性。

Abstract: Chart annotations enhance visualization accessibility but suffer from
fragmented, non-standardized representations that limit cross-platform reuse.
We propose ChartMark, a structured grammar that separates annotation semantics
from visualization implementations. ChartMark features a hierarchical framework
mapping onto annotation dimensions (e.g., task, chart context), supporting both
abstract intents and precise visual details. Our toolkit demonstrates
converting ChartMark specifications into Vega-Lite visualizations, highlighting
its flexibility, expressiveness, and practical applicability.

</details>


### [47] [Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish](https://arxiv.org/abs/2507.21813)
*Elena Alvarez-Mellado,Jordi Porta-Zamorano,Constantine Lignos,Julio Gonzalo*

Main category: cs.CL

TL;DR: 该论文总结了ADoBo 2025的主要发现，评估了不同系统在识别西班牙语中的英语借词任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 论文旨在评估不同系统在识别西班牙语中的英语借词任务上的表现。

Method: 论文描述了参与者的解决方案，包括大语言模型、深度学习模型、基于Transformer的模型和基于规则的系统。

Result: 结果范围从F1分数0.17到0.99，展示了不同系统在这个任务上可以拥有的性能差异。

Conclusion: 该论文总结了ADoBo 2025的主要发现，这是一个在IberLEF 2025背景下提出的西班牙语英语借词识别共享任务。

Abstract: This paper summarizes the main findings of ADoBo 2025, the shared task on
anglicism identification in Spanish proposed in the context of IberLEF 2025.
Participants of ADoBo 2025 were asked to detect English lexical borrowings (or
anglicisms) from a collection of Spanish journalistic texts. Five teams
submitted their solutions for the test phase. Proposed systems included LLMs,
deep learning models, Transformer-based models and rule-based systems. The
results range from F1 scores of 0.17 to 0.99, which showcases the variability
in performance different systems can have for this task.

</details>


### [48] [HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs](https://arxiv.org/abs/2507.21815)
*Kaixuan Wang,Chenxin Diao,Jason T. Jacques,Zhongliang Guo,Shuai Zhao*

Main category: cs.CL

TL;DR: 本文介绍了一个名为HRIPBench的基准，用于评估大型语言模型在伤害减少信息提供中的准确性和安全风险。研究发现，最先进的大型语言模型在提供准确的伤害减少信息方面仍然存在困难，并且有时会对使用药物的人造成严重的安全风险。因此，在伤害减少情境中使用大型语言模型应谨慎限制，以避免导致负面健康结果。


<details>
  <summary>Details</summary>
Motivation: 许多人的福祉受到物质使用的危害。伤害减少作为一种公共卫生策略，旨在改善他们的健康结果并减少安全风险。一些大型语言模型展示了相当水平的医学知识，有望满足使用药物的人的信息需求。然而，它们在相关任务中的表现仍 largely 未被探索。

Method: 本文介绍了HRIPBench，这是一个用于评估大型语言模型在伤害减少信息提供中的准确性和安全风险的基准。构建了指令和RAG方案，以基于模型的固有知识和领域知识的整合来评估模型行为。

Result: 结果表明，最先进的大型语言模型在提供准确的伤害减少信息方面仍然存在困难，并且有时会对使用药物的人造成严重的安全风险。

Conclusion: 研究表明，最先进的大型语言模型在提供准确的伤害减少信息方面仍然存在困难，并且有时会对使用药物的人造成严重的安全风险。因此，在伤害减少情境中使用大型语言模型应谨慎限制，以避免导致负面健康结果。

Abstract: Millions of individuals' well-being are challenged by the harms of substance
use. Harm reduction as a public health strategy is designed to improve their
health outcomes and reduce safety risks. Some large language models (LLMs) have
demonstrated a decent level of medical knowledge, promising to address the
information needs of people who use drugs (PWUD). However, their performance in
relevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark
designed to evaluate LLM's accuracy and safety risks in harm reduction
information provision. The benchmark dataset HRIP-Basic has 2,160
question-answer-evidence pairs. The scope covers three tasks: checking safety
boundaries, providing quantitative values, and inferring polysubstance use
risks. We build the Instruction and RAG schemes to evaluate model behaviours
based on their inherent knowledge and the integration of domain knowledge. Our
results indicate that state-of-the-art LLMs still struggle to provide accurate
harm reduction information, and sometimes, carry out severe safety risks to
PWUD. The use of LLMs in harm reduction contexts should be cautiously
constrained to avoid inducing negative health outcomes. WARNING: This paper
contains illicit content that potentially induces harms.

</details>


### [49] [Modelling Adjectival Modification Effects on Semantic Plausibility](https://arxiv.org/abs/2507.21828)
*Anna Golub,Beate Zywietz,Annerose Eichel*

Main category: cs.CL

TL;DR: 本文研究了事件可塑性变化的问题，并通过实验发现句子转换器在处理该任务时表现不佳，同时强调了需要更现实、平衡的评估方法。


<details>
  <summary>Details</summary>
Motivation: 理解可塑性变化对于对话生成、常识推理和幻觉检测等任务非常重要，因为它允许正确建模例如“温和的讽刺”作为亲密关系的标志而不是不友善。

Method: 本文使用句子转换器进行建模实验，以解决ADEPT挑战基准中的问题。

Result: 实验结果表明，句子转换器和基于变压器的模型在处理该任务时表现不佳，而句子转换器甚至比RoBERTa等模型表现更差。此外，与之前工作的比较强调了更现实、平衡的评估方法的重要性。

Conclusion: 本文结论是，尽管句子转换器在概念上与任务相匹配，但它们在任务表现上甚至不如RoBERTa等模型。此外，与之前的工作进行深入比较突显了更现实、平衡的评估方法的重要性。

Abstract: While the task of assessing the plausibility of events such as ''news is
relevant'' has been addressed by a growing body of work, less attention has
been paid to capturing changes in plausibility as triggered by event
modification. Understanding changes in plausibility is relevant for tasks such
as dialogue generation, commonsense reasoning, and hallucination detection as
it allows to correctly model, for example, ''gentle sarcasm'' as a sign of
closeness rather than unkindness among friends [9]. In this work, we tackle the
ADEPT challenge benchmark [6] consisting of 16K English sentence pairs
differing by exactly one adjectival modifier. Our modeling experiments provide
a conceptually novel method by using sentence transformers, and reveal that
both they and transformer-based models struggle with the task at hand, and
sentence transformers - despite their conceptual alignment with the task - even
under-perform in comparison to models like RoBERTa. Furthermore, an in-depth
comparison with prior work highlights the importance of a more realistic,
balanced evaluation method: imbalances distort model performance and evaluation
metrics, and weaken result trustworthiness.

</details>


### [50] [Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences](https://arxiv.org/abs/2507.21831)
*Andreas Reich,Claudia Thoms,Tobias Schrimpf*

Main category: cs.CL

TL;DR: 本文提出了一种通用的流程HALC，用于系统地构建最佳提示，以提高LLM在社会科学研究中的自动化编码效果。


<details>
  <summary>Details</summary>
Motivation: 尽管研究人员已经提出了不同的提示策略，但它们在LLM和任务之间的效果各不相同。通常仍然广泛使用试错实践。

Method: 我们提出了HALC-一个通用的流程，允许系统地和可靠地构建任何给定编码任务和模型的最佳提示，并允许集成任何被认为相关的提示策略。

Result: 与专家编码相比，我们发现使用Mistral NeMo LLM的提示可以可靠地对单个变量（αclimate = .76; αmovement = .78）和两个变量（αclimate = .71; αmovement = .74）进行编码。

Conclusion: 本文提供了不同提示策略的有效性、关键影响因素以及每个编码任务和模型的可靠提示的识别。

Abstract: LLMs are seeing widespread use for task automation, including automated
coding in the social sciences. However, even though researchers have proposed
different prompting strategies, their effectiveness varies across LLMs and
tasks. Often trial and error practices are still widespread. We propose
HALC$-$a general pipeline that allows for the systematic and reliable
construction of optimal prompts for any given coding task and model, permitting
the integration of any prompting strategy deemed relevant. To investigate LLM
coding and validate our pipeline, we sent a total of 1,512 individual prompts
to our local LLMs in over two million requests. We test prompting strategies
and LLM task performance based on few expert codings (ground truth). When
compared to these expert codings, we find prompts that code reliably for single
variables (${\alpha}$climate = .76; ${\alpha}$movement = .78) and across two
variables (${\alpha}$climate = .71; ${\alpha}$movement = .74) using the LLM
Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM
to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our
paper provides insights into the effectiveness of different prompting
strategies, crucial influencing factors, and the identification of reliable
prompts for each coding task and model.

</details>


### [51] [AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.21836)
*Yifan Wei,Xiaoyan Yu,Yixuan Weng,Tengfei Pan,Angsheng Li,Li Du*

Main category: cs.CL

TL;DR: AutoTIR是一个基于强化学习的框架，使大型语言模型能够自主决定是否以及使用哪个工具进行推理，从而提高了任务准确性、结构化输出的遵守度，并减少了错误的工具使用。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于刚性的、预定义的工具使用模式，这可能会损害核心语言能力。受人类适应性选择工具的能力启发，我们提出了AutoTIR。

Method: 我们引入了AutoTIR，这是一个强化学习框架，使LLMs能够自主决定在推理过程中是否以及使用哪个工具，而不是遵循静态的工具使用策略。

Result: 在多种知识密集型、数学和通用语言建模任务中进行的广泛评估表明，AutoTIR实现了优越的整体性能，显著优于基线，并在工具使用行为方面表现出更好的泛化能力。

Conclusion: 这些结果展示了强化学习在构建真正可泛化和可扩展的TIR能力方面的潜力。

Abstract: Large Language Models (LLMs), when enhanced through reasoning-oriented
post-training, evolve into powerful Large Reasoning Models (LRMs).
Tool-Integrated Reasoning (TIR) further extends their capabilities by
incorporating external tools, but existing methods often rely on rigid,
predefined tool-use patterns that risk degrading core language competence.
Inspired by the human ability to adaptively select tools, we introduce AutoTIR,
a reinforcement learning framework that enables LLMs to autonomously decide
whether and which tool to invoke during the reasoning process, rather than
following static tool-use strategies. AutoTIR leverages a hybrid reward
mechanism that jointly optimizes for task-specific answer correctness,
structured output adherence, and penalization of incorrect tool usage, thereby
encouraging both precise reasoning and efficient tool integration. Extensive
evaluations across diverse knowledge-intensive, mathematical, and general
language modeling tasks demonstrate that AutoTIR achieves superior overall
performance, significantly outperforming baselines and exhibits superior
generalization in tool-use behavior. These results highlight the promise of
reinforcement learning in building truly generalizable and scalable TIR
capabilities in LLMs. The code and data are available at
https://github.com/weiyifan1023/AutoTIR.

</details>


### [52] [Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2507.21892)
*Haoran Luo,Haihong E,Guanting Chen,Qika Lin,Yikai Guo,Fangzhi Xu,Zemin Kuang,Meina Song,Xiaobao Wu,Yifan Zhu,Luu Anh Tuan*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的GraphRAG框架Graph-R1，通过轻量级知识超图构建和端到端奖励机制优化，提升了RAG的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有GraphRAG方法在高构建成本、固定一次性检索以及依赖长上下文推理和提示设计方面的挑战。

Method: 提出了一种基于端到端强化学习（RL）的代理GraphRAG框架Graph-R1，引入了轻量级知识超图构建，将检索建模为多轮代理-环境交互，并通过端到端奖励机制优化代理过程。

Result: 在标准RAG数据集上的实验表明，Graph-R1在推理准确性、检索效率和生成质量方面优于传统GraphRAG和RL增强的RAG方法。

Conclusion: Graph-R1在推理准确性、检索效率和生成质量方面优于传统的GraphRAG和RL增强的RAG方法。

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by
incorporating external knowledge, but relies on chunk-based retrieval that
lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge
as entity-relation graphs, but still face challenges in high construction cost,
fixed one-time retrieval, and reliance on long-context reasoning and prompt
design. To address these challenges, we propose Graph-R1, an agentic GraphRAG
framework via end-to-end reinforcement learning (RL). It introduces lightweight
knowledge hypergraph construction, models retrieval as a multi-turn
agent-environment interaction, and optimizes the agent process via an
end-to-end reward mechanism. Experiments on standard RAG datasets show that
Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in
reasoning accuracy, retrieval efficiency, and generation quality.

</details>


### [53] [Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs](https://arxiv.org/abs/2507.21914)
*Qinyuan Wu,Soumi Das,Mahsa Amani,Bishwamittra Ghosh,Mohammad Aflah Khan,Krishna P. Gummadi,Muhammad Bilal Zafar*

Main category: cs.CL

TL;DR: 研究显示，大型语言模型可以通过两阶段的记忆-泛化框架从机械记忆的数据中进行泛化。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为，机械记忆会阻碍泛化，但这项研究旨在探索大型语言模型是否能够从机械记忆的数据中进行泛化。

Method: 研究人员引入了一个两阶段的记忆-泛化框架，其中模型首先通过语义无意义的标记来记忆事实性的主谓关联，然后通过在少量语义有意义的提示上进行微调来学习泛化。

Result: 实验表明，模型可以通过语义有意义的提示重新解释机械记忆的数据，并且在两者之间出现了结构化的、语义对齐的潜在表示。

Conclusion: 这项研究发现，通过使用两阶段的记忆-泛化框架，大型语言模型可以从记忆的数据中进行泛化，这为有效的知识注入提供了可能性，但也可能带来恶意使用记忆数据的风险。

Abstract: Rote learning is a memorization technique based on repetition. It is commonly
believed to hinder generalization by encouraging verbatim memorization rather
than deeper understanding. This insight holds for even learning factual
knowledge that inevitably requires a certain degree of memorization. In this
work, we demonstrate that LLMs can be trained to generalize from rote memorized
data. We introduce a two-phase memorize-then-generalize framework, where the
model first rote memorizes factual subject-object associations using a
semantically meaningless token and then learns to generalize by fine-tuning on
a small set of semantically meaningful prompts. Extensive experiments over 8
LLMs show that the models can reinterpret rote memorized data through the
semantically meaningful prompts, as evidenced by the emergence of structured,
semantically aligned latent representations between the two. This surprising
finding opens the door to both effective and efficient knowledge injection and
possible risks of repurposing the memorized data for malicious usage.

</details>


### [54] [Training language models to be warm and empathetic makes them less reliable and more sycophantic](https://arxiv.org/abs/2507.21919)
*Lujain Ibrahim,Franziska Sofia Hafner,Luc Rocher*

Main category: cs.CL

TL;DR: 本研究发现，优化语言模型以提高温暖度会降低其可靠性，尤其是在用户表达脆弱时。


<details>
  <summary>Details</summary>
Motivation: 人工智能开发者正在构建具有温暖和同理心个性的语言模型，这些模型被数百万人用于建议、治疗和陪伴。然而，这种优化可能会损害模型的可靠性，特别是在用户表达脆弱时。

Method: 我们对五种不同规模和架构的语言模型进行了受控实验，训练它们生成更温暖、更具同理心的回复，然后在安全关键任务上评估它们。

Result: 温暖的模型显示出显著更高的错误率（+10至+30个百分点），会促进阴谋论，提供错误的事实信息，并提供有问题的医疗建议。它们也更有可能验证用户的错误信念，尤其是在用户消息表达悲伤时。

Conclusion: 我们的研究结果表明，需要重新思考如何开发和监管这些正在重塑人类关系和社会互动的系统。

Abstract: Artificial intelligence (AI) developers are increasingly building language
models with warm and empathetic personas that millions of people now use for
advice, therapy, and companionship. Here, we show how this creates a
significant trade-off: optimizing language models for warmth undermines their
reliability, especially when users express vulnerability. We conducted
controlled experiments on five language models of varying sizes and
architectures, training them to produce warmer, more empathetic responses, then
evaluating them on safety-critical tasks. Warm models showed substantially
higher error rates (+10 to +30 percentage points) than their original
counterparts, promoting conspiracy theories, providing incorrect factual
information, and offering problematic medical advice. They were also
significantly more likely to validate incorrect user beliefs, particularly when
user messages expressed sadness. Importantly, these effects were consistent
across different model architectures, and occurred despite preserved
performance on standard benchmarks, revealing systematic risks that current
evaluation practices may fail to detect. As human-like AI systems are deployed
at an unprecedented scale, our findings indicate a need to rethink how we
develop and oversee these systems that are reshaping human relationships and
social interaction.

</details>


### [55] [Post-Training Large Language Models via Reinforcement Learning from Self-Feedback](https://arxiv.org/abs/2507.21931)
*Carel van Niekerk,Renato Vukovic,Benjamin Matthias Ruppik,Hsien-chin Lin,Milica Gašić*

Main category: cs.CL

TL;DR: 本文介绍了RLSF，这是一种后训练阶段，利用模型自身的置信度作为内在奖励，以改善模型的校准和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）经常产生看似合理但校准不良的答案，这限制了它们在需要推理的任务中的可靠性。

Method: RLSF是一种后训练阶段，它使用模型自身的置信度作为内在奖励，模仿人类在没有外部反馈的情况下学习的方式。在冻结的LLM生成多个思维链解决方案后，我们定义并计算每个最终答案段的置信度，并按顺序对痕迹进行排序。这些合成偏好随后用于标准偏好优化微调策略，类似于RLHF，但不需要人类标签、黄金答案或外部策划的奖励。

Result: RLSF同时（i）改进了模型的概率估计——恢复了良好的校准——并且（ii）加强了逐步推理，在算术推理和多项选择题回答方面提高了性能。

Conclusion: RLSF通过将模型自身的不确定性转化为有用的自我反馈，验证了基于内在模型行为的强化学习作为LLM后训练流程中的一个原则性和数据高效的组成部分，并值得进一步研究用于LLM后训练的内在奖励。

Abstract: Large Language Models (LLMs) often produce plausible but poorly-calibrated
answers, limiting their reliability on reasoning-intensive tasks. We present
Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that
uses the model's own confidence as an intrinsic reward, mimicking how humans
learn in the absence of external feedback. After a frozen LLM generates several
chain-of-thought solutions, we define and compute the confidence of each final
answer span and rank the traces accordingly. These synthetic preferences are
then used to fine-tune the policy with standard preference optimization,
similar to RLHF yet requiring no human labels, gold answers, or externally
curated rewards.
  RLSF simultaneously (i) refines the model's probability estimates --
restoring well-behaved calibration -- and (ii) strengthens step-by-step
reasoning, yielding improved performance on arithmetic reasoning and
multiple-choice question answering.
  By turning a model's own uncertainty into useful self-feedback, RLSF affirms
reinforcement learning on intrinsic model behaviour as a principled and
data-efficient component of the LLM post-training pipeline and warrents further
research in intrinsic rewards for LLM post-training.

</details>


### [56] [Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation](https://arxiv.org/abs/2507.21934)
*Tianyi Hu,Andrea Morales-Garzón,Jingyi Zheng,Maria Maistro,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 本文探讨了RAG在跨文化食谱适应中的局限性，并提出了CARRIAGE框架，以提高生成结果的多样性。


<details>
  <summary>Details</summary>
Motivation: 在跨文化食谱适应中，需要确保文化适当性、保留原始菜肴的本质，并提供多样化的选项以满足不同的饮食需求和偏好。然而，目前尚不清楚RAG是否能够生成多样化的适应结果。

Method: 本文提出了一种名为CARRIAGE的插件式RAG框架，用于跨文化食谱适应，通过增强检索和上下文组织来提高多样性。

Result: 分析表明，RAG倾向于过度依赖生成过程中的有限部分上下文，即使提供了多样化的上下文输入，也无法产生多样化的输出。这揭示了RAG在具有多个有效答案的创造性任务中的关键局限性。CARRIAGE框架在多样性与质量方面实现了帕累托效率。

Conclusion: 本文提出了CARRIAGE框架，这是第一个明确旨在生成高度多样化的输出以适应多种用户偏好的RAG框架。实验表明，CARRIAGE在多样性与食谱改编的质量方面实现了帕累托效率。

Abstract: In cross-cultural recipe adaptation, the goal is not only to ensure cultural
appropriateness and retain the original dish's essence, but also to provide
diverse options for various dietary needs and preferences. Retrieval Augmented
Generation (RAG) is a promising approach, combining the retrieval of real
recipes from the target cuisine for cultural adaptability with large language
models (LLMs) for relevance. However, it remains unclear whether RAG can
generate diverse adaptation results. Our analysis shows that RAG tends to
overly rely on a limited portion of the context across generations, failing to
produce diverse outputs even when provided with varied contextual inputs. This
reveals a key limitation of RAG in creative tasks with multiple valid answers:
it fails to leverage contextual diversity for generating varied responses. To
address this issue, we propose CARRIAGE, a plug-and-play RAG framework for
cross-cultural recipe adaptation that enhances diversity in both retrieval and
context organization. To our knowledge, this is the first RAG framework that
explicitly aims to generate highly diverse outputs to accommodate multiple user
preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in
terms of diversity and quality of recipe adaptation compared to closed-book
LLMs.

</details>


### [57] [Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models](https://arxiv.org/abs/2507.21980)
*Hyunwoo Yoo,Gail L. Rosen*

Main category: cs.CL

TL;DR: 本文探讨了使用大型语言模型（LLMs）对微生物样本进行分类和预测病原体污染风险的能力，结果表明LLMs在元数据仅有的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在只有元数据可用的微生物组研究中难以泛化，特别是在小样本设置或跨具有异构标签格式的研究中。

Method: 我们评估了ChatGPT-4o、Claude 3.7 Sonnet、Grok-3和LLaMA 4等大型语言模型在零样本和少量样本设置下的表现，并将其与传统模型如随机森林进行比较。

Result: 大型语言模型不仅在本体分类中优于基线模型，而且在污染风险预测方面也表现出强大的预测能力，并能跨站点和元数据分布进行泛化。

Conclusion: 这些发现表明，大型语言模型可以有效地对稀疏、异构的生物元数据进行推理，并为环境微生物学和生物监测应用提供有前景的仅元数据方法。

Abstract: Traditional machine learning models struggle to generalize in microbiome
studies where only metadata is available, especially in small-sample settings
or across studies with heterogeneous label formats. In this work, we explore
the use of large language models (LLMs) to classify microbial samples into
ontology categories such as EMPO 3 and related biological labels, as well as to
predict pathogen contamination risk, specifically the presence of E. Coli,
using environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude
3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing
their performance against traditional models like Random Forests across
multiple real-world datasets. Our results show that LLMs not only outperform
baselines in ontology classification, but also demonstrate strong predictive
ability for contamination risk, generalizing across sites and metadata
distributions. These findings suggest that LLMs can effectively reason over
sparse, heterogeneous biological metadata and offer a promising metadata-only
approach for environmental microbiology and biosurveillance applications.

</details>


### [58] [DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router](https://arxiv.org/abs/2507.22050)
*Minghao Guo,Qingcheng Zeng,Xujiang Zhao,Yanchi Liu,Wenchao Yu,Mengnan Du,Haifeng Chen,Wei Cheng*

Main category: cs.CL

TL;DR: DeepSieve是一种基于代理的RAG框架，通过信息筛选提高推理深度和检索精度。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法在查询和源端缺乏细粒度控制，导致检索噪声和浅层推理。

Method: DeepSieve是一种基于代理的RAG框架，通过LLM作为知识路由器进行信息筛选。它将复杂查询分解为结构化的子问题，并递归地将每个子问题路由到最合适的知识源，通过多阶段蒸馏过程过滤无关信息。

Result: 实验表明，DeepSieve在跨异构来源的多跳QA任务中表现优于传统RAG方法。

Conclusion: DeepSieve在多跳问答任务中展示了比传统RAG方法更好的推理深度、检索精度和可解释性。

Abstract: Large Language Models (LLMs) excel at many reasoning tasks but struggle with
knowledge-intensive queries due to their inability to dynamically access
up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG)
has emerged as a promising solution, enabling LLMs to ground their responses in
external sources. However, existing RAG methods lack fine-grained control over
both the query and source sides, often resulting in noisy retrieval and shallow
reasoning. In this work, we introduce DeepSieve, an agentic RAG framework that
incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve
decomposes complex queries into structured sub-questions and recursively routes
each to the most suitable knowledge source, filtering irrelevant information
through a multi-stage distillation process. Our design emphasizes modularity,
transparency, and adaptability, leveraging recent advances in agentic system
design. Experiments on multi-hop QA tasks across heterogeneous sources
demonstrate improved reasoning depth, retrieval precision, and interpretability
over conventional RAG approaches.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [59] [Multimodal LLMs as Customized Reward Models for Text-to-Image Generation](https://arxiv.org/abs/2507.21391)
*Shijie Zhou,Ruiyi Zhang,Huaisheng Zhu,Branislav Kveton,Yufan Zhou,Jiuxiang Gu,Jian Chen,Changyou Chen*

Main category: cs.CV

TL;DR: LLaVA-Reward是一种高效的奖励模型，用于自动评估文本到图像生成，通过利用预训练多模态大语言模型的隐藏状态，并引入Skip-connection Cross Attention模块来增强视觉和文本表示之间的双向交互。


<details>
  <summary>Details</summary>
Motivation: 现有的基于MLLM的方法需要指令遵循数据进行监督微调，并且在分析文本响应上评估生成质量，这既耗时又难以训练。

Method: LLaVA-Reward利用预训练多模态大语言模型（MLLMs）的隐藏状态，直接评估文本到图像（T2I）生成，并通过添加Skip-connection Cross Attention（SkipCA）模块增强解码器仅MLLM中的双向交互。

Result: LLaVA-Reward在四个评估视角（文本-图像对齐、保真度/伪影、安全性和总体排名）上进行了训练，并展示了其在自动评估和推理时间扩展方面的优越性能。

Conclusion: LLaVA-Reward在生成人类对齐评分和文本到图像生成的推理时间扩展方面优于传统方法和基于MLLM的方法。

Abstract: We introduce LLaVA-Reward, an efficient reward model designed to
automatically evaluate text-to-image (T2I) generations across multiple
perspectives, leveraging pretrained multimodal large language models (MLLMs).
Existing MLLM-based approaches require instruction-following data for
supervised fine-tuning and evaluate generation quality on analyzing text
response, which is time-consuming and difficult to train. To address this
problem, we propose LLaVA-Reward, which directly utilizes the hidden states of
MLLMs given text-image pairs. To enhance the bidirectional interaction between
visual and textual representations in decoder-only MLLMs, we further propose
adding a Skip-connection Cross Attention (SkipCA) module. This design enhances
text-image correlation reasoning by connecting early-layer visual features with
later-layer hidden representations.In addition, LLaVA-Reward supports different
types of preference data for efficient fine-tuning, including paired preference
data and unpaired data. We train LLaVA-Reward on four evaluation perspectives:
text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical
results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based
methods in generating human-aligned scores for automatic evaluations and
inference-time scaling in text-to-image generations.

</details>


### [60] [ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs](https://arxiv.org/abs/2507.21420)
*Chaoyu Li,Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: 本文提出了一种自适应的token剪枝方法ReGATE，用于加速多模态大语言模型的训练。通过教师-学生框架和自适应难度评分，ReGATE显著减少了计算开销，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的效率方法主要针对推理阶段，依赖于token的减少或合并，在训练阶段效果有限。因此需要一种有效的训练加速方法。

Method: ReGATE采用教师-学生框架，利用冻结的参考大型语言模型（LLM）计算每个token的参考损失，并将其与学生的难度分数的指数移动平均（EMA）结合，实现自适应的token剪枝。

Result: ReGATE应用于VideoLLaMA2时，在MVBench上达到标准训练的峰值准确率，速度提高了2倍，仅使用了35%的token。经过额外训练，它在多个多模态基准测试中超越了基线，同时将总token数减少了41%以上。

Conclusion: ReGATE能够显著减少计算开销，同时在多个多模态基准测试中超越基线。

Abstract: The computational cost of training multimodal large language models (MLLMs)
rapidly increases with the number of tokens involved. Existing efficiency
methods primarily target inference and rely on token reduction or merging,
offering limited benefit during training. In this paper, we propose ReGATE
(Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method
for accelerating MLLM training. Specifically, ReGATE adopts a teacher-student
framework in which the MLLM being trained serves as the student, and a frozen
reference large language model (LLM) acts as the teacher. The teacher computes
per-token reference losses, which are combined with an exponential moving
average (EMA) of the student's own difficulty scores. This adaptive
difficulty-based scoring enables the selective processing of crucial tokens
while bypassing less informative ones in the forward pass, significantly
reducing computational overhead. Experiments demonstrate that ReGATE, when
applied to VideoLLaMA2, matches the peak accuracy of standard training on
MVBench up to 2$\times$ faster, using only 35% of the tokens. With additional
training, it even surpasses the baseline on several multimodal benchmarks, all
while reducing the total token count by over 41%. Code and models will be
released soon.

</details>


### [61] [MetaCLIP 2: A Worldwide Scaling Recipe](https://arxiv.org/abs/2507.22062)
*Yung-Sung Chuang,Yang Li,Dong Wang,Ching-Feng Yeh,Kehan Lyu,Ramya Raghavendra,James Glass,Lifei Huang,Jason Weston,Luke Zettlemoyer,Xinlei Chen,Zhuang Liu,Saining Xie,Wen-tau Yih,Shang-Wen Li,Hu Xu*

Main category: cs.CV

TL;DR: MetaCLIP 2 是一个从全球网络数据中训练CLIP的新方法，能够在多语言任务中取得最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言CLIP在英语性能上不如其英语单语版本，即“多语言诅咒”，这限制了CLIP在非英语世界的应用。因此，需要一种能够从全球网络数据中训练CLIP的方法。

Method: MetaCLIP 2 是第一个从全球网络规模的图像-文本对中从头开始训练CLIP的配方。通过严格的消融实验，解决了非英语世界数据处理和多语言模型性能下降的问题。

Result: MetaCLIP 2 在零样本ImageNet分类中比其英语单语版本高出0.8%，比mSigLIP高出0.7%。在多语言基准测试中，如CVQA、Babel-ImageNet和XM3600，MetaCLIP 2 取得了新的最先进的成果。

Conclusion: MetaCLIP 2 在多语言基准测试中取得了新的最先进的成果，表明其能够从全球网络规模的图像-文本对中进行训练，并在零样本ImageNet分类中优于其英语单语版本。

Abstract: Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,
supporting from zero-shot classification, retrieval to encoders for multimodal
large language models (MLLMs). Although CLIP is successfully trained on
billion-scale image-text pairs from the English world, scaling CLIP's training
further to learning from the worldwide web data is still challenging: (1) no
curation method is available to handle data points from non-English world; (2)
the English performance from existing multilingual CLIP is worse than its
English-only counterpart, i.e., "curse of multilinguality" that is common in
LLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch
on worldwide web-scale image-text pairs. To generalize our findings, we conduct
rigorous ablations with minimal changes that are necessary to address the above
challenges and present a recipe enabling mutual benefits from English and
non-English world data. In zero-shot ImageNet classification, MetaCLIP 2
ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,
and surprisingly sets new state-of-the-art without system-level confounding
factors (e.g., translation, bespoke architecture changes) on multilingual
benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with
64.3% on image-to-text retrieval.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [62] [CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting](https://arxiv.org/abs/2507.21257)
*David Maria Schmidt,Raoul Schubert,Philipp Cimiano*

Main category: cs.AI

TL;DR: This paper proposes a benchmark to evaluate the compositional interpretation capabilities of LLMs in mapping questions to SPARQL queries, revealing that LLMs struggle with systematic and compositional understanding.


<details>
  <summary>Details</summary>
Motivation: To investigate the extent to which LLMs can interpret questions in a compositional manner.

Method: We generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. We conduct experiments with models of different sizes using various prompt and few-shot optimization techniques as well as fine-tuning.

Result: Performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity.

Conclusion: LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.

Abstract: Language interpretation is a compositional process, in which the meaning of
more complex linguistic structures is inferred from the meaning of their parts.
Large language models possess remarkable language interpretation capabilities
and have been successfully applied to interpret questions by mapping them to
SPARQL queries. An open question is how systematic this interpretation process
is. Toward this question, in this paper, we propose a benchmark for
investigating to what extent the abilities of LLMs to interpret questions are
actually compositional. For this, we generate three datasets of varying
difficulty based on graph patterns in DBpedia, relying on Lemon lexica for
verbalization. Our datasets are created in a very controlled fashion in order
to test the ability of LLMs to interpret structurally complex questions, given
that they have seen the atomic building blocks. This allows us to evaluate to
what degree LLMs are able to interpret complex questions for which they
"understand" the atomic parts. We conduct experiments with models of different
sizes using both various prompt and few-shot optimization techniques as well as
fine-tuning. Our results show that performance in terms of macro $F_1$ degrades
from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the
samples optimized on. Even when all necessary information was provided to the
model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of
lowest complexity. We thus conclude that LLMs struggle to systematically and
compositionally interpret questions and map them into SPARQL queries.

</details>


### [63] [LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems](https://arxiv.org/abs/2507.21276)
*Yufei Li,Zexin Li,Yinglun Zhu,Cong Liu*

Main category: cs.AI

TL;DR: LeMix 是一种用于联合 LLM 推理和训练的工作负载的系统，可以提高资源利用率和推理质量，同时不影响推理响应时间。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 的部署通常将推理和训练工作负载分开到不同的服务器上，导致 GPU 空闲和对新数据的延迟适应。

Method: LeMix 通过整合离线分析、执行预测机制和运行时调度来动态适应资源分配，以根据工作负载特征和系统条件进行调整。

Result: LeMix 提高了吞吐量，减少了推理损失，并提高了响应时间 SLO 的达成率。

Conclusion: LeMix 是一种用于联合 LLM 推理和训练的工作负载的系统，它可以提高资源利用率和推理质量，同时不影响推理响应时间。这是第一个揭示并利用 LLM 推理和训练联合机会的工作，为生产环境中更高效的 LLM 部署铺平了道路。

Abstract: Modern deployment of large language models (LLMs) frequently involves both
inference serving and continuous retraining to stay aligned with evolving data
and user feedback. Common practices separate these workloads onto distinct
servers in isolated phases, causing substantial inefficiencies (e.g., GPU
idleness) and delayed adaptation to new data in distributed settings. Our
empirical analysis reveals that these inefficiencies stem from dynamic request
arrivals during serving and workload heterogeneity in pipeline-parallel
training. To address these challenges, we propose LeMix, a system for
co-locating and managing concurrent LLM serving and training workloads. LeMix
integrates offline profiling, execution prediction mechanisms, and runtime
scheduling to dynamically adapt resource allocation based on workload
characteristics and system conditions. By understanding task-specific behaviors
and co-execution interference across shared nodes, LeMix improves utilization
and serving quality without compromising serving responsiveness. Our evaluation
shows that LeMix improves throughput by up to 3.53x, reduces inference loss by
up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over
traditional separate setups. To our knowledge, this is the first work to
uncover and exploit the opportunities of joint LLM inference and training,
paving the way for more resource-efficient deployment of LLMs in production
environments.

</details>


### [64] [Teaching Language Models To Gather Information Proactively](https://arxiv.org/abs/2507.21389)
*Tenghao Huang,Sihao Chen,Muhao Chen,Jonathan May,Longqi Yang,Mengting Wan,Pei Zhou*

Main category: cs.AI

TL;DR: 本文介绍了一种新的任务范式：主动信息收集，其中大型语言模型必须识别提供的上下文中的差距，并通过有针对性的问题获取隐含的用户知识。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在现实环境中经常失败，面对不完整或未明确的提示时，默认被动回应或狭窄的澄清，未能主动收集对高质量解决方案至关重要的缺失信息。

Method: 我们设计了一个可扩展的框架，生成部分指定的真实世界任务，掩盖关键信息并模拟真实的模糊性。我们的核心创新是一种强化微调策略，奖励能引发真正新的、隐含用户信息的问题。

Result: 实验表明，我们的Qwen-2.5-7B模型在自动评估指标上比o3-mini高出18%。更重要的是，人类评估显示，我们的模型生成的澄清问题和最终大纲分别受到42%和28%的人类标注者的青睐。

Conclusion: 这些结果突显了主动澄清在将大型语言模型从被动的文本生成器提升为真正协作的思维伙伴方面的价值。

Abstract: Large language models (LLMs) are increasingly expected to function as
collaborative partners, engaging in back-and-forth dialogue to solve complex,
ambiguous problems. However, current LLMs often falter in real-world settings,
defaulting to passive responses or narrow clarifications when faced with
incomplete or under-specified prompts, falling short of proactively gathering
the missing information that is crucial for high-quality solutions. In this
work, we introduce a new task paradigm: proactive information gathering, where
LLMs must identify gaps in the provided context and strategically elicit
implicit user knowledge through targeted questions. To systematically study and
train this capability, we design a scalable framework that generates partially
specified, real-world tasks, masking key information and simulating authentic
ambiguity. Within this setup, our core innovation is a reinforcement finetuning
strategy that rewards questions that elicit genuinely new, implicit user
information -- such as hidden domain expertise or fine-grained requirements --
that would otherwise remain unspoken. Experiments demonstrate that our trained
Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic
evaluation metrics. More importantly, human evaluation reveals that
clarification questions and final outlines generated by our model are favored
by human annotators by 42% and 28% respectively. Together, these results
highlight the value of proactive clarification in elevating LLMs from passive
text generators to genuinely collaborative thought partners.

</details>


### [65] [What Does it Mean for a Neural Network to Learn a "World Model"?](https://arxiv.org/abs/2507.21513)
*Kenneth Li,Fernanda Viégas,Martin Wattenberg*

Main category: cs.AI

TL;DR: 本文提出了精确的标准来判断神经网络是否学习和使用了'世界模型'，旨在为实验研究提供共同的语言。


<details>
  <summary>Details</summary>
Motivation: 为了给通常非正式使用的术语提供操作意义，以便进行实验研究。

Method: 基于线性探测文献中的想法，形式化了计算通过数据生成过程表示的概念，并添加了检查'世界模型'是否是神经网络数据或任务的非平凡结果的条件。

Result: 提出了一个定义，用于判断神经网络是否学习和使用了'世界模型'。

Conclusion: 本文提出了精确的标准来判断神经网络是否学习和使用了'世界模型'，旨在为实验研究提供共同的语言。

Abstract: We propose a set of precise criteria for saying a neural net learns and uses
a "world model." The goal is to give an operational meaning to terms that are
often used informally, in order to provide a common language for experimental
investigation. We focus specifically on the idea of representing a latent
"state space" of the world, leaving modeling the effect of actions to future
work. Our definition is based on ideas from the linear probing literature, and
formalizes the notion of a computation that factors through a representation of
the data generation process. An essential addition to the definition is a set
of conditions to check that such a "world model" is not a trivial consequence
of the neural net's data or task.

</details>


### [66] [UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding](https://arxiv.org/abs/2507.22025)
*Shuquan Lian,Yuhang Wu,Jia Ma,Zihan Song,Bingqi Chen,Xiawu Zheng,Hui Li*

Main category: cs.AI

TL;DR: UI-AGILE是一种全面的框架，通过改进监督微调过程和引入分解选择的定位方法，显著提升了GUI代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI代理训练和推理技术在推理设计、无效奖励和视觉噪声方面存在困境，因此需要一种全面的框架来解决这些问题。

Method: UI-AGILE框架通过改进监督微调（SFT）过程和引入分解选择的定位方法来增强GUI代理的能力。

Result: UI-AGILE在ScreenSpot-Pro上使用训练和推理增强方法相比最佳基线提高了23%的定位准确率，并在两个基准测试中取得了最先进的性能。

Conclusion: UI-AGILE在ScreenSpot-Pro和ScreenSpot-v2两个基准测试中达到了最先进的性能，证明了其在GUI代理能力上的提升。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven
significant advances in Graphical User Interface (GUI) agent capabilities.
Nevertheless, existing GUI agent training and inference techniques still suffer
from a dilemma for reasoning designs, ineffective reward, and visual noise. To
address these issues, we introduce UI-AGILE, a comprehensive framework
enhancing GUI agents at both the training and inference stages. For training,
we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:
1) a Continuous Reward function to incentivize high-precision grounding; 2) a
"Simple Thinking" reward to balance planning with speed and grounding accuracy;
and 3) a Cropping-based Resampling strategy to mitigate the sparse reward
problem and improve learning on complex tasks. For inference, we present
Decomposed Grounding with Selection, a novel method that dramatically improves
grounding accuracy on high-resolution displays by breaking the image into
smaller, manageable parts. Experiments show that UI-AGILE achieves the
state-of-the-art performance on two benchmarks ScreenSpot-Pro and
ScreenSpot-v2. For instance, using both our proposed training and inference
enhancement methods brings 23% grounding accuracy improvement over the best
baseline on ScreenSpot-Pro.

</details>


### [67] [UserBench: An Interactive Gym Environment for User-Centric Agents](https://arxiv.org/abs/2507.22034)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Zhiwei Liu,Jianguo Zhang,Haolin Chen,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserBench is a benchmark for evaluating how well LLM-based agents can collaborate with users by understanding their evolving goals and preferences.


<details>
  <summary>Details</summary>
Motivation: To address the gap in the ability of large language models (LLMs)-based agents to proactively collaborate with users when goals are vague, evolving, or indirectly expressed.

Method: We introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions.

Result: Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction.

Conclusion: UserBench offers an interactive environment to measure and advance the critical capability of building agents that are true collaborative partners.

Abstract: Large Language Models (LLMs)-based agents have made impressive progress in
reasoning and tool use, enabling them to solve complex tasks. However, their
ability to proactively collaborate with users, especially when goals are vague,
evolving, or indirectly expressed, remains underexplored. To address this gap,
we introduce UserBench, a user-centric benchmark designed to evaluate agents in
multi-turn, preference-driven interactions. UserBench features simulated users
who start with underspecified goals and reveal preferences incrementally,
requiring agents to proactively clarify intent and make grounded decisions with
tools. Our evaluation of leading open- and closed-source LLMs reveals a
significant disconnect between task completion and user alignment. For
instance, models provide answers that fully align with all user intents only
20% of the time on average, and even the most advanced models uncover fewer
than 30% of all user preferences through active interaction. These results
highlight the challenges of building agents that are not just capable task
executors, but true collaborative partners. UserBench offers an interactive
environment to measure and advance this critical capability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
*Zhuokun Chen,Zeren Chen,Jiahao He,Mingkui Tan,Jianfei Cai,Bohan Zhuang*

Main category: cs.LG

TL;DR: R-Stitch is a hybrid decoding framework that accelerates CoT reasoning by switching between small and large language models based on confidence, achieving significant latency reduction without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models but introduces substantial computational overhead. Existing acceleration strategies have limitations, such as limited speedup when the agreement between small and large models is low.

Method: R-Stitch is a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model (LLM) along the reasoning trajectory.

Result: Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85% reduction in inference latency with negligible accuracy drop.

Conclusion: R-Stitch achieves up to 85% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning.

Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing acceleration strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the LLM only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the LLM on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.

</details>


### [69] [MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge](https://arxiv.org/abs/2507.21183)
*Guangchen Lan,Sipeng Zhang,Tianle Wang,Yuwei Zhang,Daoan Zhang,Xinpeng Wei,Xiaoman Pan,Hongming Zhang,Dong-Jun Han,Christopher G. Brinton*

Main category: cs.LG

TL;DR: MaPPO是一种从偏好中学习的框架，通过整合先验奖励估计来扩展DPO及其变体的范式，从而提高对齐效果，并在多个基准测试中表现出一致的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如直接偏好优化（DPO）及其变体将偏好学习视为最大似然估计（MLE）问题，而MaPPO通过整合先验奖励估计来扩展这一范式，从而提高对齐效果。

Method: MaPPO是一种从偏好中学习的框架，它将先验奖励知识显式地纳入优化目标中。它通过将先验奖励估计整合到一个原则性的最大后验（MAP）目标中，扩展了DPO及其变体的范式。

Result: 在三个标准基准测试（包括MT-Bench、AlpacaEval 2.0和Arena-Hard）上进行的大量实验评估表明，MaPPO在不同模型大小和模型系列中表现出一致的改进。

Conclusion: MaPPO在不牺牲计算效率的情况下，在对齐性能上表现出一致的改进，并且可以作为插件与DPO变体一起使用，包括广泛使用的SimPO、IPO和CPO。

Abstract: As the era of large language models (LLMs) on behalf of users unfolds,
Preference Optimization (PO) methods have become a central approach to aligning
LLMs with human preferences and improving performance. We propose Maximum a
Posteriori Preference Optimization (MaPPO), a framework for learning from
preferences that explicitly incorporates prior reward knowledge into the
optimization objective. While existing methods such as Direct Preference
Optimization (DPO) and its variants treat preference learning as a Maximum
Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating
prior reward estimates into a principled Maximum a Posteriori (MaP) objective.
This not only generalizes DPO and its variants, but also enhances alignment by
mitigating the oversimplified binary classification of responses. More
importantly, MaPPO introduces no additional hyperparameter, and supports
preference optimization in both offline and online settings. In addition, MaPPO
can be used as a plugin with consistent improvement on DPO variants, including
widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different
model sizes and model series on three standard benchmarks, including MT-Bench,
AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in
alignment performance without sacrificing computational efficiency.

</details>


### [70] [EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models](https://arxiv.org/abs/2507.21184)
*Haowei Lin,Xiangyu Wang,Jianzhu Ma,Yitao Liang*

Main category: cs.LG

TL;DR: EvoSLD is an automated framework for discovering scaling laws using evolutionary algorithms and LLMs, showing superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The traditional process of discovering scaling laws requires extensive human expertise and manual experimentation, which is time-consuming and inefficient.

Method: EvoSLD is an automated framework that uses evolutionary algorithms guided by Large Language Models (LLMs) to co-evolve symbolic expressions and their optimization routines.

Result: EvoSLD rediscovers exact human-derived laws in two cases and surpasses them in others, achieving up to orders-of-magnitude reductions in normalized mean squared error on held-out test sets.

Conclusion: EvoSLD demonstrates potential to accelerate AI research by providing superior accuracy, interpretability, and efficiency in discovering scaling laws.

Abstract: Scaling laws are fundamental mathematical relationships that predict how
neural network performance evolves with changes in variables such as model
size, dataset size, and computational resources. Traditionally, discovering
these laws requires extensive human expertise and manual experimentation. We
introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that
leverages evolutionary algorithms guided by Large Language Models (LLMs) to
co-evolve symbolic expressions and their optimization routines. Formulated to
handle scaling variables, control variables, and response metrics across
diverse experimental settings, EvoSLD searches for parsimonious, universal
functional forms that minimize fitting errors on grouped data subsets.
Evaluated on five real-world scenarios from recent literature, EvoSLD
rediscovers exact human-derived laws in two cases and surpasses them in others,
achieving up to orders-of-magnitude reductions in normalized mean squared error
on held-out test sets. Compared to baselines like symbolic regression and
ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and
efficiency, highlighting its potential to accelerate AI research. Code is
available at https://github.com/linhaowei1/SLD.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [OneShield -- the Next Generation of LLM Guardrails](https://arxiv.org/abs/2507.21170)
*Chad DeLuca,Anna Lisa Gentile,Shubhi Asthana,Bing Zhang,Pawan Chowdhary,Kellen Cheng,Basel Shbita,Pengyuan Li,Guang-Jie Ren,Sandeep Gopisetty*

Main category: cs.CR

TL;DR: 本文提出了一种名为 OneShield 的独立、模型无关且可定制的解决方案，旨在保护大型语言模型，以满足每个特定客户的需求。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的不断演变，全面保护用户免受潜在风险变得极其具有挑战性，因此需要一种灵活且可定制的解决方案。

Method: 提出了一种名为 OneShield 的独立、模型无关且可定制的解决方案，用于保护大型语言模型。

Result: 描述了框架的实现、可扩展性考虑，并提供了 OneShield 自首次部署以来的使用统计数据。

Conclusion: OneShield 是一种独立、模型无关且可定制的解决方案，旨在保护大型语言模型，以满足每个特定客户的需求。

Abstract: The rise of Large Language Models has created a general excitement about the
great potential for a myriad of applications. While LLMs offer many
possibilities, questions about safety, privacy, and ethics have emerged, and
all the key actors are working to address these issues with protective measures
for their own models and standalone solutions. The constantly evolving nature
of LLMs makes the task of universally shielding users against their potential
risks extremely challenging, and one-size-fits-all solutions unfeasible. In
this work, we propose OneShield, our stand-alone, model-agnostic and
customizable solution to safeguard LLMs. OneShield aims to provide facilities
for defining risk factors, expressing and declaring contextual safety and
compliance policies, and mitigating LLM risks, with a focus on each specific
customer. We describe the implementation of the framework, the scalability
considerations and provide usage statistics of OneShield since its first
deployment.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [72] [Analise Semantica Automatizada com LLM e RAG para Bulas Farmaceuticas](https://arxiv.org/abs/2507.21103)
*Daniel Meireles do Rego*

Main category: cs.IR

TL;DR: 该研究探讨了RAG与LLMs结合用于自动化分析PDF文档的方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着数字文档的快速增长，需要更高效的非结构化信息提取和分析方法。

Method: 该研究使用了RAG架构和大型语言模型（LLMs）来自动化分析PDF格式的文档，并结合了向量搜索技术、语义数据提取和上下文自然语言响应生成。

Result: 实验结果表明，RAG与LLMs的结合在准确性和响应速度等方面表现优异。

Conclusion: 该研究表明，RAG与LLMs的结合在智能信息检索和非结构化技术文本的解释方面提供了显著的优势。

Abstract: The production of digital documents has been growing rapidly in academic,
business, and health environments, presenting new challenges in the efficient
extraction and analysis of unstructured information. This work investigates the
use of RAG (Retrieval-Augmented Generation) architectures combined with
Large-Scale Language Models (LLMs) to automate the analysis of documents in PDF
format. The proposal integrates vector search techniques by embeddings,
semantic data extraction and generation of contextualized natural language
responses. To validate the approach, we conducted experiments with drug package
inserts extracted from official public sources. The semantic queries applied
were evaluated by metrics such as accuracy, completeness, response speed and
consistency. The results indicate that the combination of RAG with LLMs offers
significant gains in intelligent information retrieval and interpretation of
unstructured technical texts.

</details>


### [73] [AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis](https://arxiv.org/abs/2507.21105)
*Callie C. Liao,Duoduo Liao,Sai Surya Gadiraju*

Main category: cs.IR

TL;DR: 本文提出了一种新型的多协议MAS框架AgentMaster，能够实现动态协调和灵活通信，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前系统在代理间通信、协调和与异构工具和资源的交互方面仍然面临挑战。很少有应用同时使用这两种协议。

Method: 我们提出了AgentMaster，这是一个新型的模块化多协议MAS框架，具有自实现的A2A和MCP，实现了动态协调和灵活通信。

Result: 通过统一的对话界面，系统支持自然语言交互，并对信息检索、问答和图像分析等任务做出响应。评估显示BERTScore F1和G-Eval分别为96.3%和87.1%。

Conclusion: 我们的框架为由MAS驱动的领域特定、协作和可扩展的对话AI提供了潜在能力。

Abstract: The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI),
especially integrated with Large Language Models (LLMs), has greatly
facilitated the resolution of complex tasks. However, current systems are still
facing challenges of inter-agent communication, coordination, and interaction
with heterogeneous tools and resources. Most recently, the Model Context
Protocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by
Google have been introduced, and to the best of our knowledge, very few
applications exist where both protocols are employed within a single MAS
framework. We present a pilot study of AgentMaster, a novel modular
multi-protocol MAS framework with self-implemented A2A and MCP, enabling
dynamic coordination and flexible communication. Through a unified
conversational interface, the system supports natural language interaction
without prior technical expertise and responds to multimodal queries for tasks
including information retrieval, question answering, and image analysis.
Evaluation through the BERTScore F1 and LLM-as-a-Judge metric G-Eval averaged
96.3\% and 87.1\%, revealing robust inter-agent coordination, query
decomposition, dynamic routing, and domain-specific, relevant responses.
Overall, our proposed framework contributes to the potential capabilities of
domain-specific, cooperative, and scalable conversational AI powered by MAS.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [74] [Can LLMs Reason About Trust?: A Pilot Study](https://arxiv.org/abs/2507.21075)
*Anushka Debnath,Stephen Cranefield,Emiliano Lorini,Bastin Tony Roy Savarimuthu*

Main category: cs.HC

TL;DR: 本文探讨了大型语言模型（LLMs）在理解和建立信任关系方面的能力，并评估了它们通过扮演一方来促进信任的潜力。


<details>
  <summary>Details</summary>
Motivation: 在人类社会中，信任是建立和维护长期健康关系的重要组成部分，为合作奠定了坚实的基础。随着许多人类互动通过电子方式进行，AI系统有可能帮助用户理解关系的社会状态。

Method: 本文研究了大型语言模型（LLMs）在需要培养信任关系的环境中推理两个人之间信任的能力，并评估了LLMs是否能够通过扮演一方并计划能建立信任的行动来诱导信任。

Result: 本文发现大型语言模型（LLMs）具备推理信任关系的能力，并且能够在基于信任的互动中扮演一方并计划能建立信任的行动。

Conclusion: 本文探讨了大型语言模型（LLMs）在理解和建立信任关系方面的能力，并评估了它们通过扮演一方来促进信任的潜力。

Abstract: In human society, trust is an essential component of social attitude that
helps build and maintain long-term, healthy relationships which creates a
strong foundation for cooperation, enabling individuals to work together
effectively and achieve shared goals. As many human interactions occur through
electronic means such as using mobile apps, the potential arises for AI systems
to assist users in understanding the social state of their relationships. In
this paper we investigate the ability of Large Language Models (LLMs) to reason
about trust between two individuals in an environment which requires fostering
trust relationships. We also assess whether LLMs are capable of inducing trust
by role-playing one party in a trust based interaction and planning actions
which can instil trust.

</details>


### [75] [Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations](https://arxiv.org/abs/2507.21089)
*Xiaotian Su,Naim Zierau,Soomin Kim,April Yi Wang,Thiemo Wambsganss*

Main category: cs.HC

TL;DR: 本文提出了两种情感监控仪表板，以提高用户的情感意识并减少仇恨言论，但发现这可能导致在讨论敏感问题时负面情绪增加。


<details>
  <summary>Details</summary>
Motivation: 现有的主动监管方法受到批评，因为它们创造了审查制度的氛围，并未能解决不文明行为的根本原因。

Method: 提出并评估了两种情感监控仪表板，以提高用户的情感意识并减轻仇恨言论。

Result: 这些干预措施有效提高了用户对自己情绪状态的意识，并减少了仇恨言论。然而，研究结果还表明，当讨论敏感问题时，负面情绪（愤怒、恐惧和悲伤）的表达可能会增加。

Conclusion: 这些见解为将主动情绪调节工具整合到社交媒体平台中以促进更健康的数字互动提供了基础。

Abstract: Social media platforms increasingly employ proactive moderation techniques,
such as detecting and curbing toxic and uncivil comments, to prevent the spread
of harmful content. Despite these efforts, such approaches are often criticized
for creating a climate of censorship and failing to address the underlying
causes of uncivil behavior. Our work makes both theoretical and practical
contributions by proposing and evaluating two types of emotion monitoring
dashboards to users' emotional awareness and mitigate hate speech. In a study
involving 211 participants, we evaluate the effects of the two mechanisms on
user commenting behavior and emotional experiences. The results reveal that
these interventions effectively increase users' awareness of their emotional
states and reduce hate speech. However, our findings also indicate potential
unintended effects, including increased expression of negative emotions (Angry,
Fear, and Sad) when discussing sensitive issues. These insights provide a basis
for further research on integrating proactive emotion regulation tools into
social media platforms to foster healthier digital interactions.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [76] [Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation](https://arxiv.org/abs/2507.21903)
*Tiviatis Sim,Kaiwen Yang,Shen Xin,Kenji Kawaguchi*

Main category: cs.SI

TL;DR: 本文提出了一种名为SUnSET的新框架，用于时间线摘要任务。通过利用大型语言模型构建SET三元组，并引入基于利益相关者的排名来构建一个$Relevancy$指标，实验结果表明该方法优于所有先前的基线，成为新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 由于现有新闻摘要方法仅考虑相似日期的文章的文本内容来理解事件的要点，因此在分析涉及的各方方面存在不足。因此，需要提出一种新框架来衡量利益相关者的重要性以及通过相关实体连接的相关事件。

Method: 我们利用强大的大型语言模型（LLMs）来构建SET三元组，并引入基于利益相关者的排名来构建一个$Relevancy$指标，该指标可以扩展到一般情况。

Result: 我们的实验结果超过了所有先前的基线，并成为新的最先进水平，突显了新闻文章中利益相关者信息的影响。

Conclusion: 我们的实验结果超过了所有先前的基线，并成为新的最先进水平，突显了新闻文章中利益相关者信息的影响。

Abstract: As news reporting becomes increasingly global and decentralized online,
tracking related events across multiple sources presents significant
challenges. Existing news summarization methods typically utilizes Large
Language Models and Graphical methods on article-based summaries. However, this
is not effective since it only considers the textual content of similarly dated
articles to understand the gist of the event. To counteract the lack of
analysis on the parties involved, it is essential to come up with a novel
framework to gauge the importance of stakeholders and the connection of related
events through the relevant entities involved. Therefore, we present SUnSET:
Synergistic Understanding of Stakeholder, Events and Time for the task of
Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)
to build SET triplets and introduced the use of stakeholder-based ranking to
construct a $Relevancy$ metric, which can be extended into general situations.
Our experimental results outperform all prior baselines and emerged as the new
State-of-the-Art, highlighting the impact of stakeholder information within
news article.

</details>

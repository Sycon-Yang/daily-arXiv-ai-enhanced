<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: This paper introduces CycleDistill, a method that uses LLMs and few-shot translation to create high-quality MT systems without relying on extensive parallel corpora.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of scarce or non-existent parallel corpora for low-resource languages, which limits the performance of dedicated MT systems.

Method: CycleDistill is a bootstrapping approach that iteratively generates synthetic parallel corpora from monolingual corpora using zero- or few-shot MT, which is then used to fine-tune the model for MT.

Result: In experiments focusing on three Indian languages, CycleDistill achieved high-quality machine translation, improving upon a few-shot baseline model by over 20-30 chrF points on average in the first iteration.

Conclusion: CycleDistill can achieve high-quality machine translation by leveraging LLMs and few-shot translation, even without extensive parallel corpora.

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [2] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架Inference-Scaled GraphRAG，通过推理时的计算扩展来增强基于LLM的图推理。实验结果表明，该方法在多跳问答任务上优于传统的GraphRAG和先前的图遍历基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在语言理解和生成方面取得了令人印象深刻的能力，但它们在知识密集型推理任务上仍然表现不佳，这是由于对结构化上下文和多跳信息的访问有限。检索增强生成（RAG）部分缓解了这一问题，但传统RAG和GraphRAG方法往往无法捕捉知识图谱中节点之间的关系结构。

Method: 我们引入了Inference-Scaled GraphRAG，这是一种新颖的框架，通过应用推理时的计算扩展来增强基于LLM的图推理。我们的方法结合了顺序扩展与深度思维链图遍历，以及并行扩展与在交错推理-执行循环内的采样轨迹上的多数投票。

Result: 在GRBench基准测试上的实验表明，我们的方法显著提高了多跳问答性能，相对于传统的GraphRAG和先前的图遍历基线取得了显著的提升。

Conclusion: 这些发现表明，推理时的扩展是一种实用且与架构无关的解决方案，用于使用LLM进行结构化知识推理。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [3] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

TL;DR: Doc2Agent is a scalable pipeline that builds agents capable of calling Python-based tools generated from API documentation, achieving significant performance improvements and demonstrating adaptability to complex tasks.


<details>
  <summary>Details</summary>
Motivation: Building tool-using agents for arbitrary domains remains a major challenge, as it requires reading unstructured API documentation, testing APIs and inferring correct parameters.

Method: Doc2Agent generates executable tools from API documentations and iteratively refines them using a code agent.

Result: We achieved a 55% relative performance improvement with 90% lower cost compared to direct API calling on WebArena benchmark. A domain-specific agent built for glycomaterial science further demonstrates the pipeline's adaptability to complex, knowledge-rich tasks.

Conclusion: Doc2Agent offers a generalizable solution for building tool agents from unstructured API documentation at scale.

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [4] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: STReason is a novel framework that combines the reasoning strengths of large language models with spatio-temporal models to enable multi-task inference and execution without task-specific finetuning. It decomposes complex queries into interpretable programs and generates detailed rationales. STReason outperforms existing models in spatio-temporal reasoning tasks and has practical applications in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing models are often restricted to narrow tasks, lacking the capacity for multi-task inference and complex long-form reasoning that require generation of in-depth, explanatory outputs. These limitations restrict their applicability to real-world, multi-faceted decision scenarios.

Method: STReason integrates the reasoning strengths of large language models (LLMs) with the analytical capabilities of spatio-temporal models for multi-task inference and execution. It leverages in-context learning to decompose complex natural language queries into modular, interpretable programs, which are then systematically executed to generate both solutions and detailed rationales.

Result: Experimental results show that STReason significantly outperforms advanced LLM baselines across all metrics, particularly excelling in complex, reasoning-intensive spatio-temporal scenarios. Human evaluations further validate STReason's credibility and practical utility, demonstrating its potential to reduce expert workload and broaden the applicability to real-world spatio-temporal tasks.

Conclusion: STReason provides a promising direction for developing more capable and generalizable spatio-temporal reasoning systems.

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [5] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: 本文分析了代码检索问题，并提出了SACL框架以提高代码检索和生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前检索器过度依赖表面文本特征，并且对良好文档的代码有很强的偏见，即使文档无关。

Method: 通过系统地屏蔽特定特征来分析代码检索，并提出SACL框架来丰富文本信息和减少偏差。

Result: SACL在代码检索任务中表现出色，例如在HumanEval、MBPP和SWE-Bench-Lite上的Recall@1分别提高了12.8% / 9.4% / 7.0%，并且在代码生成任务中也有所提升，例如在HumanEval上的Pass@1提高了4.88%。

Conclusion: SACL显著提高了代码检索和代码生成性能，表明通过增强文本信息和减少偏差可以改善代码检索效果。

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [6] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 本文提出了一种新的视角来理解潜在空间几何，即语义表示学习，旨在将组合性和符号性属性整合到分布语义空间中，以提高语言模型的可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的分布语义空间在可解释性、可控性、组合性和泛化能力方面存在局限。为了克服这些限制，需要将组合性和符号性属性整合到这些空间中，从而促进符号语义和分布语义之间的桥梁建立。

Method: 本文通过分析三种主流的自编码器架构（变分自编码器（VAE）、向量量化变分自编码器（VQVAE）和稀疏自编码器（SAE）），探讨了它们在语义结构和可解释性方面的潜在几何特性。

Result: 本文通过分析三种主流的自编码器架构，揭示了它们在语义结构和可解释性方面的潜在几何特性，为语义表示学习提供了新的视角。

Conclusion: 将组合性和符号性属性整合到当前的分布语义空间中可以增强基于Transformer的自回归语言模型（LM）的可解释性、可控性、组合性和泛化能力。通过组合语义的视角，我们提出了一个关于潜在空间几何的新观点，即语义表示学习，这有助于弥合符号语义和分布语义之间的差距。我们回顾并比较了三种主流的自编码器架构，并研究了它们在语义结构和可解释性方面的潜在几何特性。

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [7] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Main category: cs.CL

TL;DR: 本文介绍了Time-Series QA任务和EngineMT-QA数据集，并提出了ITFormer框架，以有效整合时间序列数据与自然语言，提高QA准确率。


<details>
  <summary>Details</summary>
Motivation: 有效整合高维时间序列信号与自然语言对于动态、交互式任务仍然是一个重大挑战。

Method: 我们提出了Instruct Time Transformer (ITFormer)框架，该框架将时间序列编码器与冻结的大语言模型（LLMs）结合，以有效地提取、对齐和融合时间序列和文本特征。

Result: ITFormer在QA准确率上实现了显著提升，且仅使用了不到1%的额外可训练参数。

Conclusion: 通过将时间序列数据与自然语言相结合，我们的工作为多模态AI开辟了新的研究和应用方向。

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [8] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Main category: cs.CL

TL;DR: 研究评估了三阶段大型语言模型（LLM）框架在放射学报告校对中的表现，发现其显著提高了正预测值（PPV）并降低了运营成本，同时保持了检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于错误发生率低，基于大型语言模型（LLM）的放射学报告校对的正预测值（PPV）受到限制。目的：评估三阶段LLM框架是否比基线方法提高PPV并降低运营成本。

Method: 对1000份连续的放射学报告（每种：X光、超声、CT、MRI）进行了回顾性分析，并使用CheXpert和Open-i外部数据集作为验证集。测试了三种LLM框架：（1）单提示检测器；（2）提取器加检测器；（3）提取器、检测器和假阳性验证器。

Result: 框架的PPV从0.063（95% CI，0.036-0.101，框架1）增加到0.079（0.049-0.118，框架2），并显著增加到0.159（0.090-0.252，框架3；P<.001 vs. 基线）。aTPR保持稳定（0.012-0.014；P>=.84）。每1000份报告的运营成本降至5.58美元（框架3），从框架1的9.72美元和框架2的6.85美元下降，分别减少了42.6%和18.5%。人工审核的报告数量从192减少到88。外部验证支持框架3的优越PPV（CheXpert 0.133，Open-i 0.105）和稳定的aTPR（0.007）。

Conclusion: 三阶段LLM框架显著提高了PPV并降低了运营成本，同时保持了检测性能，为AI辅助的放射学报告质量保证提供了有效的策略。

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [9] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过利用自动化评分技术来填补缺失分数，以提高IRT基础能力估计的准确性。该方法在保持高准确性的同时显著减少了人工评分的工作量。


<details>
  <summary>Details</summary>
Motivation: 评估学习者的能力是教育领域的一个基本目标，尤其是对表达能力和逻辑思维等高级能力的评估需求不断增加。虽然构造性反应测试（如简答和作文题）被广泛使用，但它们需要大量的人工评分，既费时又昂贵。因此，需要一种更有效的方法来解决这一问题。

Method: 本文提出了一种新的方法，通过利用自动化评分技术来填补缺失分数，以提高IRT基础能力估计的准确性。

Result: 所提出的方法在保持高准确性的同时，显著减少了人工评分的工作量。

Conclusion: 本文提出了一种新的方法，通过利用自动化评分技术来填补缺失分数，从而实现准确的IRT基础能力估计。该方法在保持高准确性的同时显著减少了人工评分的工作量。

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [10] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: CCRS is a new evaluation framework for RAG systems that uses a single LLM to assess multiple aspects of output quality, offering better efficiency and performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for RAG outputs are inadequate, either relying on simple lexical overlap metrics or complex multi-stage pipelines, which hinder practical efficiency.

Method: Proposed CCRS (Contextual Coherence and Relevance Score), a novel suite of five metrics that utilizes a single, powerful, pretrained LLM as a zero-shot, end-to-end judge.

Result: CCRS effectively discriminates between system performances, confirming that the Mistral-7B reader outperforms Llama variants. It offers comparable or superior discriminative power for key aspects like recall and faithfulness, while being significantly more computationally efficient than the RAGChecker framework.

Conclusion: CCRS provides a practical, comprehensive, and efficient framework for evaluating and iteratively improving RAG systems.

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [11] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Main category: cs.CL

TL;DR: 本文提出了一种名为AALC的轻量级、准确度感知的长度奖励机制，用于在强化学习中动态平衡正确性和简洁性。实验结果表明，该方法能显著减少响应长度，同时保持或提高准确性，并减少冗余推理模式。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成长链式思维实现令人印象深刻的推理能力，但这种“过度思考”会带来高延迟和成本，而没有相应的准确性提升。

Method: 我们引入了AALC，这是一种轻量级、准确度感知的长度奖励，集成到强化学习中，以在训练过程中动态平衡正确性和简洁性。

Result: 我们的方法在标准和分布外数学基准测试中进行了广泛的实验，结果显示响应长度减少了50%以上，同时保持或甚至提高了原始准确性。此外，定性分析表明，我们的方法遏制了冗余的推理模式，如过多的子目标设定和验证，导致结构更精炼的输出。

Conclusion: 我们的研究展示了奖励机制在引导大型推理模型走向更高效、更通用的推理路径方面的潜力。

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [12] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: SEED 是一种结构编码器，通过整合多个阶段，将数值模式与语义推理进行高效对齐，从而解决结构-语义建模差距。


<details>
  <summary>Details</summary>
Motivation: 现有的结构编码器无法支持语义层面的推理或任务适应，而大型语言模型（LLMs）与原始时间序列输入不兼容，这限制了统一、可转移预测系统的开发。

Method: SEED 包含四个阶段：一个关注标记的编码器用于补丁提取，一个投影模块将补丁与语言模型嵌入对齐，一个语义重新编程机制将补丁映射到任务感知原型，以及一个冻结的语言模型用于预测。

Result: 实证结果表明，所提出的方法在强基线上实现了持续改进，并且在各种数据集上的比较研究证实了 SEED 在解决结构-语义建模差距中的作用。

Conclusion: SEED 的模块化架构有效解决了结构-语义建模差距，并在各种数据集上验证了其有效性。

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [13] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: COIN is a framework that improves uncertainty quantification in foundation models by ensuring FDR control and increasing sample retention, with strong performance across various tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of heuristic UQ approaches and split conformal prediction (SCP) frameworks, which often fail to provide formal guarantees for key metrics like FDR and may include incorrect candidates in prediction sets.

Method: COIN calibrates statistically valid thresholds to filter generated answers under user-specified FDR constraints by estimating the empirical error rate on a calibration set and applying confidence interval methods like Clopper-Pearson to establish an upper bound on the true error rate (FDR).

Result: COIN demonstrates robustness in risk control, strong test-time power in retaining admissible answers, and predictive efficiency under limited calibration data across general and multimodal text generation tasks. It also shows potential for further improvement through alternative upper bound constructions and UQ strategies.

Conclusion: COIN is a robust and adaptable framework for uncertainty quantification in foundation models, demonstrating strong performance in risk control and predictive efficiency.

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [14] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 本研究探讨了如何通过增强示例检索提高对话情感识别的准确性，并发现该方法在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各个领域中实现了广泛的实际应用。然而，创建高精度的高性能应用程序仍然具有挑战性，尤其是在主观任务如情感识别方面。因此，本研究旨在探索改进对话情感识别的方法。

Method: 本研究探讨了如何在上下文学习中检索高质量示例以提高对话情感识别（CER）。我们提出了基于随机和增强示例检索的各种策略，并分析了对话上下文对CER准确性的影响力。

Result: 实验在三个数据集（IEMOCAP、MELD 和 EmoryNLP）上进行。结果表明，增强示例检索在所有数据集上均优于其他研究技术。

Conclusion: 实验结果表明，通过增强示例检索可以提高对话情感识别的准确性，这强调了检索连贯的目标示例并通过改写来增强它们的重要性。

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [15] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
*Petra Barančíková,Ondřej Bojar*

Main category: cs.CL

TL;DR: 该研究比较了捷克特定和多语言句子嵌入模型在内在和外在评估中的表现，发现内在语义相似性测试表现好的模型不一定在下游翻译评估任务中表现好，反之亦然。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨句子嵌入模型在不同评估范式下的表现，并揭示内在语义相似性测试与下游翻译评估任务之间的关系。

Method: 通过内在和外在评估范式比较了捷克特定和多语言句子嵌入模型。在内在评估中，使用了Costra数据集和几个语义文本相似性（STS）基准来评估嵌入捕捉语言现象的能力。在外在评估中，使用基于COMET的指标对每个嵌入模型进行微调以进行机器翻译评估。

Result: 实验结果显示，内在语义相似性测试表现优异的模型并不总能在下游翻译评估任务中取得优越表现。相反，看似过于平滑的嵌入空间的模型可以通过微调获得出色的结果。

Conclusion: 研究结果强调了语义属性探测和下游任务之间的复杂关系，突显了在句子嵌入中需要更多关于'可操作语义'的研究，或更深入的下游任务数据集（如翻译评估）

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [16] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Main category: cs.CL

TL;DR: 本研究提出了一种新的多视角方法，使用软标签来捕捉人类分歧，以提高模型的包容性和多样性。结果显示，该方法在分类性能上优于传统方法，但在某些任务中表现出较低的置信度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理人类分歧时，往往忽视了个体意见，导致少数观点被低估，尤其是在主观任务中。因此，需要一种更全面的方法来捕捉人类分歧，以提高模型的包容性和多样性。

Method: 本研究提出了一种新的多视角方法，使用软标签来鼓励开发更包容和多元化的视角感知模型。此外，还利用可解释人工智能（XAI）探索了模型的不确定性，并揭示了模型预测的重要见解。

Result: 实验结果表明，多视角方法在衡量人类标签分布的Jensen-Shannon散度（JSD）方面表现更好，并且在分类性能上优于传统方法。然而，在像讽刺和立场检测这样的任务中，该方法的置信度较低。

Conclusion: 本研究提出了一种新的多视角方法，使用软标签来促进下一代视角感知模型的发展，这些模型更加包容和多元化。结果表明，该方法不仅更好地近似了人类标签分布，还实现了更好的分类性能。然而，在像讽刺和立场检测这样的任务中，该方法的置信度较低，这可能是由于文本中的主观性。最后，通过可解释的人工智能（XAI），我们探索了模型的不确定性并揭示了关于模型预测的重要见解。

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [17] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Main category: cs.CL

TL;DR: 本文提出了一种通过显式结构化推理增强大型语言模型的新方法，通过监督微调和组相对策略优化，结合MAX-Flow和LCS算法，显著提高了推理效果并降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在涉及逻辑演绎和系统规划的复杂推理任务中仍然面临困难，主要是因为它们依赖于隐式的统计关系而没有结构化的知识表示。

Method: 通过显式标注推理步骤将非结构化数据转换为结构化格式，并使用监督微调（SFT）训练大型语言模型。此外，还通过组相对策略优化（GRPO）增强了大型语言模型的结构化推理能力，结合了MAX-Flow和最长公共子序列（LCS）两个创新算法。

Result: 实验结果表明，微调后的DeepSeek-R1-Distill-Qwen-1.5B模型表现出简洁的推理、在各种场景下的强大性能以及与优化技术的改进兼容性。

Conclusion: 实验结果验证了结构化推理在大型语言模型中的有效性。

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [18] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: 本文提出了一种基于块的多SSL融合方法，用于改进自动流利度评估，并在多个数据集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 自动流利度评估仍然具有挑战性，特别是在捕捉非母语者的语音节奏、停顿和不流畅方面。

Method: 我们引入了一种基于块的方法，结合了自监督学习（SSL）模型（Wav2Vec2、HuBERT和WavLM），这些模型因其在语音学、语调和嘈杂语音建模方面的互补优势，以及一个分层的CNN-BiLSTM框架。

Result: 在Avalinguo和Speechocean762上评估，我们的方法在Speechocean762上比单个SSL基线提高了2.8的F1分数和6.2的皮尔逊相关性，在Avalinguo上提高了4.2的F1分数和4.0的皮尔逊相关性，超过了基于Pyannote.audio的分割基线。

Conclusion: 这些发现突出了基于块的多SSL融合在稳健流利度评估中的潜力，尽管未来的工作应探索在具有不规则语调的方言中的泛化能力。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [19] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Main category: cs.CL

TL;DR: 本文提出了一种结合大型语言模型和主题模型的方法，用于动态建模叙事变化。我们应用这种方法分析了2009年至2023年《华尔街日报》的新闻文章语料库，发现大型语言模型在提取叙事变化方面表现良好，但在区分内容和叙事变化方面效果不佳。


<details>
  <summary>Details</summary>
Motivation: 随着媒体叙述的迅速演变，不仅需要从给定语料库中提取叙述，还需要研究它们如何随时间发展。虽然流行的叙述提取方法如大型语言模型在捕捉典型叙述元素或甚至复杂结构方面表现良好，但将其应用于整个语料库会遇到障碍，例如高成本或计算成本。

Method: 我们结合了大型语言模型的语言理解能力和主题模型的大规模适用性，使用叙事政策框架动态建模叙事变化。我们应用了一个主题模型和相应的变化点检测方法来寻找与特定主题相关的变化，并利用该模型过滤出特别代表该变化的文档，然后将它们输入到大型语言模型中进行自动解释和区分内容和叙事变化。

Result: 我们的研究结果表明，大型语言模型可以在存在叙事变化时有效地提取叙事变化，但在决定内容变化还是叙事变化时表现不佳。

Conclusion: 我们的研究结果表明，大型语言模型可以在存在叙事变化时有效地提取叙事变化，但在决定内容变化还是叙事变化时表现不佳。

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [20] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Main category: cs.CL

TL;DR: 本文介绍了Biomed-Enriched数据集，该数据集通过两阶段注释过程构建，能够提供大规模、公开可用的临床案例，用于生物医学和临床NLP研究，并展示了通过质量过滤和领域上采样等技术可以实现更高效和有效的生物医学预训练策略。


<details>
  <summary>Details</summary>
Motivation: 临床文本通常由于隐私限制难以获取，因此需要一个大规模、公开可用的临床案例集合。

Method: 我们通过两阶段注释过程构建了Biomed-Enriched数据集，首先使用大型语言模型对PubMed中的400K段落进行注释，然后微调一个小的语言模型以在整个PMC-OA语料库中传播标签。

Result: 通过质量过滤和领域上采样，我们提取了经过精炼的子集，并在MMLU ProfMed和MedQA等任务上取得了显著的性能提升。

Conclusion: 我们的数据集为生物医学和临床NLP提供了有价值的资源，并展示了通过质量过滤和领域上采样等技术可以实现更高效和有效的生物医学预训练策略。

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [21] [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
*Ekaterina Taktasheva,Jeff Dalton*

Main category: cs.CL

TL;DR: 本文提出了一种名为TAPS的新解决方案，通过结构化标记工具和基于不确定性的工具检测器来增强个性化工具使用，显著提高了LLMs整合用户偏好的能力，并在NLSI任务中达到了新的最先进的水平。


<details>
  <summary>Details</summary>
Motivation: 研究如何将用户偏好有效地整合到目标导向的对话代理中，因为现有方法忽略了个性化在指导工具使用中的作用。

Method: 引入了TAPS，通过结构化标记工具和基于不确定性的工具检测器来增强个性化的工具使用。

Result: 通过广泛的分析，我们发现了LLMs在个性化工具使用方面的关键弱点，并提出了TAPS解决方案。

Conclusion: TAPS显著提高了LLMs整合用户偏好的能力，在NLSI任务中达到了新的最先进的水平。

Abstract: Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce \name, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

</details>


### [22] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Main category: cs.CL

TL;DR: DeepRare是首个基于大型语言模型的罕见病诊断代理系统，能够处理异构临床输入并生成可解释的诊断假设。它在多个评估中表现出色，展示了其在罕见病诊断中的卓越性能和实用性。


<details>
  <summary>Details</summary>
Motivation: 罕见病的临床异质性、个体患病率低以及大多数临床医生对罕见病的不熟悉，使得及时准确的诊断成为一项普遍挑战。因此，需要一种能够处理异构临床输入并生成可解释诊断假设的系统。

Method: DeepRare是一个基于大型语言模型（LLM）的罕见病诊断代理系统，包含三个关键组件：一个具有长期记忆模块的中央主机，负责领域特定分析任务的专业代理服务器，以及整合了40多个专用工具和网络规模、最新医学知识源的系统。

Result: DeepRare在八个数据集上的评估中表现出色，对于2,919种疾病达到了100%的准确率，其中1013种疾病表现尤为突出。在HPO评估中，DeepRare显著优于其他15种方法，平均Recall@1得分为57.18%，比第二好的方法高出23.79个百分点。在多模态输入场景中，DeepRare的Recall@1得分为70.60%，而Exomiser为53.20%。临床专家对推理链的手动验证也达到了95.40%的一致性。

Conclusion: DeepRare系统已被实现为一个用户友好的网页应用，展示了其在罕见病诊断中的卓越性能和实用性。

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [23] [Probing AI Safety with Source Code](https://arxiv.org/abs/2506.20471)
*Ujwal Narayan,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.CL

TL;DR: 本文介绍了CoDoT提示策略，用于评估大型语言模型的安全性。结果显示，CoDoT导致多个先进LLMs出现严重安全问题，强调了从基础原理评估安全性的必要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在许多安全关键型应用中与人类交互，这需要提高能力，但更重要的是要增强安全措施，使这些模型与人类价值观和偏好保持一致。然而，当前模型在AI安全方面仍存在明显不足，导致用户体验不安全和有害。

Method: 引入了一种称为Code of Thought (CoDoT)的提示策略，将自然语言输入转换为表示相同意图的简单代码。

Result: CoDoT导致多种最先进的LLMs出现一致性失败。例如，GPT-4 Turbo的毒性增加了16.5倍，DeepSeek R1 100%失败，而七种现代LLMs的平均毒性增加了300%。此外，递归应用CoDoT可以进一步将毒性增加两倍。

Conclusion: CoDoT underscores the critical need to evaluate safety efforts from first principles, ensuring that safety and capabilities advance together.

Abstract: Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

</details>


### [24] [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
*Kaixiang Zhang,Justine Zhang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: 本文介绍了一个计算框架，用于量化对话中的谈话时间分布及其动态，并通过实验证明不同类型的动态对参与者的感知有不同影响，为计算机中介通信平台的设计提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 对话中的谈话时间分配是对话的一个固有方面，但不同类型的谈话时间共享动态可能对参与者产生不同的影响，因此需要一种新的方法来分析和理解这些动态。

Method: 我们引入了一个计算框架，用于量化对话级别的谈话时间分布以及导致它的低层次动态。我们推导出一个由几个直观的变化轴结构化的谈话时间共享动态类型学。

Result: 我们通过应用这个框架到大量陌生人之间的视频通话数据集中，确认了不同对话级别的谈话时间分布被说话者不同地感知，平衡的对话比不平衡的对话更受偏好，尤其是那些说话较少的人。此外，即使它们导致相同的总体平衡水平，不同的谈话时间共享动态也被参与者不同地感知。

Conclusion: 我们的框架为计算机中介通信平台的设计者提供了新的工具，用于人类与人类以及人类与AI的交流。

Abstract: An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

</details>


### [25] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: 本文介绍了Team Marikarp在SIGIR 2025 LiveRAG竞赛中的解决方案，该方案通过知识感知多样化重排序RAG流程取得了第一名。


<details>
  <summary>Details</summary>
Motivation: 为了在SIGIR 2025 LiveRAG竞赛中取得好成绩，需要一种有效的检索相关支持文档的方法。

Method: 提出了知识感知多样化重排序RAG流程。

Result: 在比赛中获得了第一名。

Conclusion: 我们的知识感知多样化重排序RAG流程在比赛中获得了第一名。

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [26] [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/abs/2506.20480)
*Guinan Su,Li Shen,Lu Yin,Shiwei Liu,Yanwu Yang,Jonas Geiping*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过合并不同微调模型的层来压缩大型语言模型，以减少计算成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要集中在单个模型的剪枝上，而我们希望通过结合不同微调模型的层来更有效地压缩模型，同时保留原始模型的能力。

Method: 我们开发了一种新的策略，通过从微调模型变体中战略性地组合或合并层来压缩模型，从而在部署时减少计算成本。我们提出了一个零阶优化问题，支持三种不同的操作：(1) 层移除，(2) 从不同候选模型中选择层，(3) 层合并。

Result: 实验结果表明，这种方法在模型剪枝方面表现优异，例如对于Llama2-13B模型家族，我们的压缩模型保持了原始性能的大约97.3%，同时减少了约25%的参数，显著优于之前最先进的方法。

Conclusion: 我们的方法在压缩大型语言模型方面表现出色，能够在减少参数的同时保持接近原始模型的性能。

Abstract: Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

</details>


### [27] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode is a novel framework that improves the ability of Large Language Models (LLMs) to adapt to changes in external library APIs. It uses a dataset of 2,000 data entries and a modified string similarity metric for code evaluation as a reward for reinforcement learning. Experiments show that ReCode significantly enhances LLMs' code generation performance in dynamic API scenarios, particularly on the CodeUpdateArena task. It also has less impact on LLMs' general code generation abilities compared to supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments.

Method: We propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning.

Result: Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture.

Conclusion: ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture.

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [28] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 本文研究了中期训练策略如何影响强化学习动态，提出了一个两阶段的中期训练策略，以提高基础模型的强化学习兼容性。


<details>
  <summary>Details</summary>
Motivation: 了解什么使基础语言模型适合强化学习对于开发下一代可扩展强化学习的基础模型至关重要。

Method: 我们研究了中期训练策略如何影响强化学习动态，重点关注两个代表性模型家族：Qwen和Llama。我们引入了一种两阶段的中期训练策略，即Stable-then-Decay。

Result: 我们的研究揭示了高质量的数学语料库可以显著提高基础模型和强化学习性能，而现有的替代方案则无法做到这一点。添加问答风格的数据，特别是长链式思维（CoT）推理示例，可以增强强化学习结果。此外，中期训练的扩展一致地导致更强的下游强化学习性能。

Conclusion: 我们的工作有助于塑造强化学习时代的基础模型的预训练策略。我们释放了开源模型以及一个经过筛选的数学推理密集型语料库，以支持进一步的研究。

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [29] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
*Ammar Khairi,Daniel D'souza,Ye Shen,Julia Kreutzer,Sara Hooker*

Main category: cs.CL

TL;DR: 本文研究了如何在多语言、多任务设置中稳健地扩展推理时的计算能力。我们评估了现有的选择方法，并提出了针对多语言和多任务推理场景的新型采样和选择策略。结果表明，这些策略在多种语言和任务中表现出了显著的提升。


<details>
  <summary>Details</summary>
Motivation: 最近大型语言模型（LLMs）的进步将重点转向扩展推理时的计算，以提高性能而不重新训练模型。然而，迄今为止的工作主要集中在英语和一些领域，如数学和代码。相比之下，我们最感兴趣的是能够在开放性任务、形式可验证任务和跨语言方面推广的技术。

Method: 我们研究了如何在多语言、多任务设置中稳健地扩展推理时的计算能力。我们评估了现有的选择方法，并提出了针对多语言和多任务推理场景的新型采样和选择策略。

Result: 我们的结果表明，基于温度变化的采样策略和选择策略必须适应不同的领域和语言环境。我们提出的新型采样和选择策略在多种语言和任务中表现出显著的提升。例如，我们的组合采样和选择方法在8B模型上使m-ArenaHard-v2.0提示的胜率平均提高了6.8个百分点，而Command-A（111B模型）在相同基准测试中仅使用五个样本就比单样本解码提高了9.0个百分点。

Conclusion: 我们的结果强调了在推理时计算中需要语言和任务感知的方法，旨在使资源较少的语言的性能提升民主化。

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [30] [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/abs/2506.20606)
*Baixiang Huang,Zhen Tan,Haoran Wang,Zijie Liu,Dawei Li,Ali Payani,Huan Liu,Tianlong Chen,Kai Shu*

Main category: cs.CL

TL;DR: 本文提出了一种名为行为编辑的方法，用于引导基于大型语言模型的智能体的行为，并通过引入BehaviorBench基准测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LLM-based智能体在高风险领域部署时存在显著的安全和伦理风险，因此需要一种有效的方法来引导其行为。

Method: 我们将智能体行为引导框架化为一个模型编辑任务，称为行为编辑，并引入了BehaviorBench基准测试来系统地研究和评估这一方法。

Result: 行为编辑能够动态地将智能体引导到特定场景中的目标行为，并且可以实现更广泛的全球道德对齐变化。此外，BehaviorBench展示了行为编辑在不同模型和场景中的有效性。

Conclusion: 我们的研究结果揭示了行为编辑在引导智能体行为方面的潜力和风险，为未来的研究提供了重要的见解。

Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

</details>


### [31] [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639)
*Shansan Gong,Ruixiang Zhang,Huangjie Zheng,Jiatao Gu,Navdeep Jaitly,Lingpeng Kong,Yizhe Zhang*

Main category: cs.CL

TL;DR: 本文研究了扩散大语言模型（dLLMs）在代码生成中的去噪过程和强化学习方法，提出了一种新的采样方案，提高了模型性能并减少了对AR因果性的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前dLLM在编码中的训练和推理机制仍不明确，因此需要系统地研究其去噪过程和RL方法，以解锁其在编码中的潜力。

Method: 我们系统地研究了dLLM的去噪过程和强化学习(RL)方法，并提出了一个名为耦合-GRPO的新采样方案，该方案在训练中为补全构建互补的掩码噪声。

Result: 耦合-GRPO显著提高了DiffuCoder在代码生成基准上的性能（在EvalPlus上提高了4.4%），并减少了在解码过程中对AR因果性的依赖。

Conclusion: 我们的工作提供了对dLLM生成机制的更深入见解，并提供了一个有效的、基于扩散的RL训练框架。

Abstract: Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR causal during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

</details>


### [32] [Memento: Note-Taking for Your Future Self](https://arxiv.org/abs/2506.20642)
*Chao Wan,Albert Gong,Mihir Mishra,Carl-Leander Henneking,Claas Beger,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: Memento is a prompting strategy that enhances the performance of existing prompting strategies in multi-hop question answering tasks by decomposing complex questions, dynamically constructing a database of facts, and combining these facts to solve the question.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) excel at reasoning-only tasks, but struggle when reasoning must be tightly coupled with retrieval, as in multi-hop question answering.

Method: Introduce a prompting strategy that first decomposes a complex question into smaller steps, then dynamically constructs a database of facts using LLMs, and finally pieces these facts together to solve the question.

Result: On the 9-step PhantomWiki benchmark, Memento doubles the performance of chain-of-thought (CoT) when all information is provided in context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1 percentage points.

Conclusion: Memento can boost the performance of existing prompting strategies across diverse settings and demonstrate its utility in agentic settings.

Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

</details>


### [33] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
*Sonia K. Murthy,Rosie Zhao,Jennifer Hu,Sham Kakade,Markus Wulfmeier,Peng Qian,Tomer Ullman*

Main category: cs.CL

TL;DR: 本文研究了LLMs在处理价值权衡方面的表现，并提出了一种方法来分析这些权衡。结果显示，推理模型更注重信息效用，而开源模型则在数学推理方面表现出色。此外，训练动态显示了早期训练中效用值的变化，并且基础模型和预训练数据的选择对结果有持久影响。


<details>
  <summary>Details</summary>
Motivation: 当前用于解释LLMs中动态和多面的价值观的工具是有限的。在认知科学中，所谓的“认知模型”通过建模说话者在选择行动或言语时对竞争性效用函数的加权，提供了这些权衡的正式描述。

Method: 我们使用了一个领先的礼貌言语认知模型来解释LLMs在多大程度上代表了类似人类的权衡。我们将这种视角应用于系统评估两个涵盖模型设置中的价值权衡：前沿黑盒模型中的推理“努力”程度，以及开源模型的RL后训练动态。

Result: 我们的结果突出了推理模型中更高的信息效用而非社会效用，以及在被证明在数学推理方面更强的开源模型中的表现。我们的发现表明，LLMs的训练动态显示了早期训练中效用值的大规模变化，并且基础模型和预训练数据的选择具有持久影响，而反馈数据集或对齐方法的影响较小。

Conclusion: 我们的方法对快速发展的LLM景观中的各种方面具有响应性，为形成关于其他高层次行为的假设、塑造推理模型的训练制度以及在模型训练期间更好地控制价值之间的权衡提供了见解。

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [34] [FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment](https://arxiv.org/abs/2506.20303)
*Lee Qi Zun,Oscar Wong Jin Hao,Nor Anita Binti Che Omar,Zalifa Zakiah Binti Asnir,Mohamad Sabri bin Sinal Zainal,Goh Man Fye*

Main category: eess.IV

TL;DR: 本文提出了一种新的专家验证框架FundaQ-8，用于系统评估眼底图像质量，并开发了一个基于ResNet18的回归模型来预测连续质量分数。该模型在真实临床数据和Kaggle数据集上进行了训练，并通过EyeQ数据集的验证和统计分析确认了其可靠性。此外，将FundaQ-8纳入深度学习模型中，可以提高糖尿病视网膜病变分级的诊断鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于图像采集的变化和主观专家评估，自动化眼底图像质量评估（FIQA）仍然是一个挑战。

Method: 使用FundaQ-8作为结构化评分参考，开发了一个基于ResNet18的回归模型，以预测0到1范围内的连续质量分数。该模型使用迁移学习、均方误差优化和标准化预处理，在来自真实临床来源和Kaggle数据集的1800张眼底图像上进行训练。

Result: 通过EyeQ数据集的验证和统计分析，确认了该框架的可靠性和临床可解释性。此外，将FundaQ-8纳入深度学习模型中，可以提高糖尿病视网膜病变分级的诊断鲁棒性。

Conclusion: 将FundaQ-8纳入深度学习模型中，可以提高糖尿病视网膜病变分级的诊断鲁棒性，表明质量感知训练在现实世界筛查应用中的价值。

Abstract: Automated fundus image quality assessment (FIQA) remains a challenge due to
variations in image acquisition and subjective expert evaluations. We introduce
FundaQ-8, a novel expert-validated framework for systematically assessing
fundus image quality using eight critical parameters, including field coverage,
anatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a
structured scoring reference, we develop a ResNet18-based regression model to
predict continuous quality scores in the 0 to 1 range. The model is trained on
1800 fundus images from real-world clinical sources and Kaggle datasets, using
transfer learning, mean squared error optimization, and standardized
preprocessing. Validation against the EyeQ dataset and statistical analyses
confirm the framework's reliability and clinical interpretability.
Incorporating FundaQ-8 into deep learning models for diabetic retinopathy
grading also improves diagnostic robustness, highlighting the value of
quality-aware training in real-world screening applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: 本文提出了一种新的概率模型来模拟阅读行为，但发现 surprisal 理论在解释精细眼动方面效果有限。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于汇总的眼动追踪测量和强假设模型，忽略了阅读过程中发生的许多时空动态。

Method: 我们提出了一种基于标记的时空点过程的更通用的概率模型，用于建模阅读行为。其中，saccades 使用 Hawkes 过程进行建模，而 fixation 持续时间则作为固定特定预测因子的函数进行建模。

Result: 我们的 Hawkes 过程模型在拟合人类 saccades 方面优于基线模型。然而，将上下文 surprisal 作为预测因子纳入模型仅带来预测准确性的微小改进。

Conclusion: 我们的研究发现， surprisal 理论在解释精细的眼动方面存在困难。

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [36] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE 是一个新基准，用于评估模型在农业领域的多模态专家级推理和决策能力，具有高保真度和开放世界设置。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试依赖于明确的用户输入和封闭集分类法，而 MIRAGE 提供了未明确指定、富含上下文的开放世界设置，要求模型推断隐含的知识空白，处理罕见实体，并主动引导交互或作出回应。

Method: MIRAGE 结合了自然用户查询、专家撰写的回答和基于图像的上下文，通过精心设计的多步骤管道进行筛选，涵盖了多种作物健康、害虫诊断和作物管理场景。

Result: MIRAGE 包含超过 7000 个独特的生物实体，涵盖了植物种类、害虫和疾病，使其成为最分类多样化的视觉-语言模型基准之一。

Conclusion: MIRAGE 是一个高保真的基准，用于评估模型在现实世界、知识密集型领域中的基于现实的推理、澄清策略和长文本生成能力。

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [37] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: This paper argues that ML conferences should establish a dedicated 'Refutations and Critiques' track to foster a self-correcting research ecosystem.


<details>
  <summary>Details</summary>
Motivation: ML conferences do not offer robust processes to help the field systematically correct when such errors are made.

Method: This position paper argues that ML conferences should establish a dedicated 'Refutations and Critiques' (R & C) Track.

Result: The R & C Track would provide a high-profile, reputable platform to support vital research that critically challenges prior research, thereby fostering a dynamic self-correcting research ecosystem.

Conclusion: ML conferences should create official, reputable mechanisms to help ML research self-correct.

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [38] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: 本文研究了记忆，将反事实影响视为一种分布量，考虑所有训练样本如何影响一个样本的记忆。我们发现，仅仅关注自我影响可能会严重低估与记忆相关的实际风险。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型以记住其训练数据中的样本而闻名，这引发了隐私和泛化的担忧。反事实自我影响是研究记忆的流行度量，量化了模型对样本的预测如何取决于样本是否包含在训练数据集中。然而，最近的工作表明，记忆还受到其他因素的影响，特别是（近）重复样本，它们有重大影响。

Method: 我们研究了记忆，将反事实影响视为一种分布量，考虑所有训练样本如何影响一个样本的记忆。对于一个小的语言模型，我们计算了训练样本对彼此的完整影响分布，并分析了其属性。

Result: 我们发现，仅仅关注自我影响可能会严重低估与记忆相关的实际风险：（近）重复的存在会大大降低自我影响，而我们发现这些样本是（近）可提取的。我们在图像分类中观察到类似的模式，其中仅查看影响分布就揭示了CIFAR-10中存在近似重复。

Conclusion: 我们的发现表明，记忆源于训练数据之间的复杂相互作用，并且比单独使用自影响更能通过完整的影响力分布来捕捉。

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [39] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 本文研究了一种基于离策略REINFORCE算法的强化学习方法，并通过理论分析和实验验证了其在大语言模型对齐中的有效性。


<details>
  <summary>Details</summary>
Motivation: 离策略方法在实现简单性和数据效率方面优于在线策略技术，但通常会导致次优性能。本文旨在研究介于离策略强化学习和监督微调之间的算法范围。

Method: 本文分析了一种简单的离策略REINFORCE算法，其中优势函数定义为$A=r-V$，并通过理论分析和实验验证了该方法的有效性。

Result: 理论分析表明，当基线$V$下界预期奖励时，该算法享有策略改进保证。实验结果验证了这一发现，并展示了该方法在控制的随机老虎机设置和大语言模型微调任务中的有效性。

Conclusion: 本文分析了基于离策略REINFORCE算法的强化学习方法在大语言模型对齐中的应用，并验证了其有效性。

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [40] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Main category: cs.LG

TL;DR: PLoP is a lightweight method for automatically identifying the best module types to place LoRA adapters, which consistently outperforms or competes with existing strategies in various tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of LoRA by improving adapter placement strategy, as few works have studied this problem with nonconclusive results.

Method: PLoP (Precise LoRA Placement), a lightweight method that allows automatic identification of module types where LoRA adapters should be placed, given a pretrained model and a finetuning task.

Result: PLoP consistently outperforms, and in the worst case competes, with commonly used placement strategies through comprehensive experiments on supervised finetuning and reinforcement learning for reasoning.

Conclusion: PLoP consistently outperforms, and in the worst case competes, with commonly used placement strategies through comprehensive experiments on supervised finetuning and reinforcement learning for reasoning.

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: 本研究展示了如何利用本地大型语言模型开发出在医疗任务中表现优于商业在线模型的检索增强生成（RAG）系统，同时减少环境影响。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗保健中的日益普及引发了对其环境和伦理影响的担忧。商业大型语言模型（LLMs）如ChatGPT和DeepSeek需要大量资源，而这些系统在医疗用途中的应用引发了关于患者隐私和安全的关键问题。

Method: 我们开发了一个可定制的检索增强生成（RAG）框架，用于医疗任务，该框架监控其能源使用和二氧化碳排放。然后使用该系统创建基于各种开源大型语言模型的RAG。测试的模型包括通用模型如llama3.1:8b和medgemma-4b-it，后者是医学领域特定的。最佳RAG性能和能耗与DeepSeekV3-R1和OpenAIs o4-mini模型进行了比较。

Result: 定制的RAG模型在准确性和能耗方面都超过了商业模型。基于llama3.1:8B的RAG模型达到了最高的准确率（58.5%），并且显著优于其他模型，包括o4-mini和DeepSeekV3-R1。llama3.1-RAG在所有模型中表现出最低的能耗和二氧化碳足迹，每千瓦时的性能为0.52，总二氧化碳排放量为473克。与o4-mini相比，llama3.1-RAG在每千瓦时的准确率点上提高了2.7倍，并且用电量减少了172%，同时保持了更高的准确率。

Conclusion: 我们的研究表明，本地大型语言模型可以用于开发在医疗任务中表现优于商业在线大型语言模型的检索增强生成（RAG）系统，同时具有更小的环境影响。我们的模块化框架促进了可持续的人工智能发展，减少了电力使用，并与联合国的可持续发展目标相一致。

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [42] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: 我们的研究表明，分配了人格的大型语言模型表现出类似人类的动机推理，这种推理难以通过传统去偏提示缓解，可能加剧身份一致的推理。


<details>
  <summary>Details</summary>
Motivation: 人类推理容易受到偏见的影响，因为潜在的动机如身份保护会破坏理性决策和判断。这种集体层面的动机推理在讨论关键问题时可能对社会有害，例如人为气候变化或疫苗安全，并可能进一步加剧政治极化。先前的研究报告称大型语言模型（LLMs）也容易受到类似人类的认知偏见影响，但LLMs选择性地朝向符合身份的结论的程度仍 largely 未被探索。

Method: 我们研究了在4个政治和社会人口属性上分配8种人格是否会在大型语言模型（LLMs）中引发动机推理。我们测试了8个大型语言模型（开源和专有）在两个来自人类受试者研究的推理任务中的表现——辨别虚假新闻标题的真实性和评估数值科学证据。

Result: 分配了人格的大型语言模型在辨别虚假新闻标题的真实性方面相对于没有人格的模型减少了高达9%。特别是政治人格在地面真实与他们诱导的政治身份一致时，正确评估关于枪支控制的科学证据的可能性高出90%。基于提示的去偏方法在减轻这些影响方面效果有限。

Conclusion: 我们的实证结果首次表明，分配了人格的大型语言模型表现出类似人类的动机推理，这种推理很难通过传统的去偏提示来缓解——这引发了对加剧大型语言模型和人类身份一致推理的担忧。

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [43] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体LLM的方法Genesys，用于发现新型语言模型架构。通过Ladder of Scales方法和遗传编程核心，Genesys在多个基准测试中表现优异，证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索是否可以利用大型语言模型（LLM）来模拟发现新型语言模型架构的过程。

Method: Genesys采用了一种多智能体LLM方法，模拟了传统研究的各个阶段，包括构思、文献搜索、设计实现、生成预训练和下游评估。它使用了Ladder of Scales方法，在不断增加的模型规模下进行新设计的提出、对抗性审查、实现和选择性验证，并且预算逐渐减少。此外，Genesys还使用了一种新颖的遗传编程核心，以提高设计生成的成功率。

Result: Genesys发现了1,162个新的设计，其中1,062个经过预训练验证。最好的设计在多个基准测试中表现出色，超过了GPT2和Mamba2等已知架构。

Conclusion: Genesys能够有效地发现新的语言模型架构，并且其最佳设计在多个基准测试中表现优于已知架构，如GPT2和Mamba2。

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [44] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: Decrypto is a new benchmark for evaluating multi-agent reasoning and theory of mind in large language models, revealing that current models lag behind humans and older models in these abilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. ToM and other multi-agent abilities in LLMs are poorly understood.

Method: Propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning.

Result: LLM game-playing abilities lag behind humans and simple word-embedding baselines. State-of-the-art reasoning models are significantly worse at ToM tasks than their older counterparts.

Conclusion: Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents.

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [45] [Capturing Visualization Design Rationale](https://arxiv.org/abs/2506.16571)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.HC

TL;DR: 本文提出了一种新的方法，通过自然语言分析可视化设计的合理性，并利用学生创建的可视化笔记本和大型语言模型生成数据集。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言数据集在数据可视化任务上关注于可视化素养评估、见解生成和从自然语言指令生成可视化，但这些研究通常依赖于受控设置和专门构建的可视化以及人为构造的问题，导致它们更注重对可视化的解释，而不是理解其编码。因此，需要一种新的方法来探究可视化设计的合理性。

Method: 本文利用学生在数据可视化课程中创建的有素养的可视化笔记本作为真实世界的可视化和自然语言叙述的来源，并使用大型语言模型（LLMs）从笔记本中的叙述和阐述生成和分类问题-答案-合理性三元组。

Result: 本文通过验证和整理三元组，建立了一个数据集，捕捉并提炼了学生的可视化设计选择及其相应的合理性。

Conclusion: 本文提出了一个新的数据集和方法，用于通过自然语言探测可视化设计的合理性。

Abstract: Prior natural language datasets for data visualization have focused on tasks
such as visualization literacy assessment, insight generation, and
visualization generation from natural language instructions. These studies
often rely on controlled setups with purpose-built visualizations and
artificially constructed questions. As a result, they tend to prioritize the
interpretation of visualizations, focusing on decoding visualizations rather
than understanding their encoding. In this paper, we present a new dataset and
methodology for probing visualization design rationale through natural
language. We leverage a unique source of real-world visualizations and natural
language narratives: literate visualization notebooks created by students as
part of a data visualization course. These notebooks combine visual artifacts
with design exposition, in which students make explicit the rationale behind
their design decisions. We also use large language models (LLMs) to generate
and categorize question-answer-rationale triples from the narratives and
articulations in the notebooks. We then carefully validate the triples and
curate a dataset that captures and distills the visualization design choices
and corresponding rationales of the students.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: 本研究评估了五种最先进的目标检测架构在三个历史文档数据集上的性能，发现基于CNN的YOLOv11x-OBB在复杂数据集上表现最佳，而基于Transformer的模型在结构化布局中表现出色。研究强调了OBB在建模历史手稿非笛卡尔性质中的重要性，并指出Transformer和CNN-OBB模型之间的关键权衡。


<details>
  <summary>Details</summary>
Motivation: 稳健的文档布局分析（DLA）对于自动化处理和理解具有复杂页面组织的历史文档至关重要。然而，现有方法在处理历史手稿的非笛卡尔性质时面临挑战，因此需要评估不同模型在不同数据集上的性能。

Method: 本研究在三个标注数据集上对五种最先进的目标检测架构进行了基准测试，包括e-NDP、CATMuS和HORAE。评估了两种基于Transformer的模型（Co-DETR、Grounding DINO）和三种YOLO变体（AABB、OBB和YOLO-World）。

Result: 在e-NDP数据集上，Co-DETR取得了最先进的结果（0.752 mAP@.50:.95），紧随其后的是YOLOv11X-OBB（0.721）。而在更复杂的CATMuS和HORAE数据集上，基于CNN的YOLOv11x-OBB显著优于所有其他模型（分别为0.564和0.568）。

Conclusion: 本研究明确表明，使用定向边界框（OBB）不是一个小的改进，而是准确建模历史手稿非笛卡尔性质的基本要求。我们得出结论，Transformer的全局上下文意识与CNN-OBB模型在视觉多样性和复杂文档中的优越泛化能力之间存在关键权衡。

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [47] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
*Jinming Wu,Zihao Deng,Wei Li,Yiding Liu,Bo You,Bo Li,Zejun Ma,Ziwei Liu*

Main category: cs.CV

TL;DR: 我们提出了MMSearch-R1，这是一个端到端强化学习框架，使大型多模态模型能够按需、多轮地在现实世界的互联网环境中进行搜索。通过集成图像和文本搜索工具，并利用基于结果的奖励和搜索惩罚来指导模型决策，我们的方法在知识密集型和信息寻求的VQA任务上表现出色，优于基于RAG的基线，并减少了搜索调用。


<details>
  <summary>Details</summary>
Motivation: 在现实世界场景中部署大型多模态模型（LMMs）需要访问外部知识源，因为现实世界的信息复杂且动态。现有的方法如检索增强生成（RAG）和提示工程搜索代理依赖于刚性流程，通常导致低效或过度的搜索行为。

Method: 我们提出了MMSearch-R1，这是第一个端到端强化学习框架，使LMM能够在现实世界的互联网环境中执行按需、多轮搜索。我们的框架集成了图像和文本搜索工具，允许模型根据基于结果的奖励和搜索惩罚来推理何时以及如何调用它们。

Result: 在知识密集型和信息寻求的VQA任务上的广泛实验表明，我们的模型不仅优于相同模型大小的基于RAG的基线，而且在减少搜索调用超过30%的情况下，还达到了更大的基于RAG的模型的性能。

Conclusion: 我们的模型不仅优于相同模型大小的基于RAG的基线，而且在减少搜索调用超过30%的情况下，还达到了更大的基于RAG的模型的性能。我们进一步分析了关键的实证发现，以提供推动多模态搜索研究的可行见解。

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [48] [PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models](https://arxiv.org/abs/2506.20097)
*Wang Bill Zhu,Miaosen Chai,Ishika Singh,Robin Jia,Jesse Thomason*

Main category: cs.RO

TL;DR: PSALM-V is an autonomous neuro-symbolic learning system that induces symbolic action semantics in visual environments through interaction, improving plan success rates and step efficiency in various domains.


<details>
  <summary>Details</summary>
Motivation: Previous approaches have focused on text-based domains or relied on unrealistic assumptions, such as access to a predefined problem file, full observability, or explicit error messages. PSALM-V aims to address these limitations by autonomously inducing symbolic action semantics.

Method: PSALM-V uses LLMs to generate heuristic plans and candidate symbolic semantics, dynamically infers PDDL problem files and domain action semantics by analyzing execution outcomes, and maintains a tree-structured belief over possible action semantics for each action.

Result: Simulated experiments show that PSALM-V increases the plan success rate from 37% (Claude-3.7) to 74% in partially observed setups. It also improves step efficiency and succeeds in domain induction in multi-agent settings, and correctly induces PDDL pre- and post-conditions for real-world robot tasks.

Conclusion: PSALM-V can induce symbolic action semantics in visual environments through interaction, and it improves plan success rates and step efficiency in various domains.

Abstract: We propose PSALM-V, the first autonomous neuro-symbolic learning system able
to induce symbolic action semantics (i.e., pre- and post-conditions) in visual
environments through interaction. PSALM-V bootstraps reliable symbolic planning
without expert action definitions, using LLMs to generate heuristic plans and
candidate symbolic semantics. Previous work has explored using large language
models to generate action semantics for Planning Domain Definition Language
(PDDL)-based symbolic planners. However, these approaches have primarily
focused on text-based domains or relied on unrealistic assumptions, such as
access to a predefined problem file, full observability, or explicit error
messages. By contrast, PSALM-V dynamically infers PDDL problem files and domain
action semantics by analyzing execution outcomes and synthesizing possible
error explanations. The system iteratively generates and executes plans while
maintaining a tree-structured belief over possible action semantics for each
action, iteratively refining these beliefs until a goal state is reached.
Simulated experiments of task completion in ALFRED demonstrate that PSALM-V
increases the plan success rate from 37% (Claude-3.7) to 74% in partially
observed setups. Results on two 2D game environments, RTFM and Overcooked-AI,
show that PSALM-V improves step efficiency and succeeds in domain induction in
multi-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions
for real-world robot BlocksWorld tasks, despite low-level manipulation failures
from the robot.

</details>


### [49] [Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue](https://arxiv.org/abs/2506.20268)
*Ruben Janssens,Jens De Bock,Sofie Labat,Eva Verhelst,Veronique Hoste,Tony Belpaeme*

Main category: cs.RO

TL;DR: 本研究评估了机器学习模型在检测人机对话中的误解效果。结果表明，即使使用最先进的模型，其表现也仅略高于随机猜测，而人类评分员也仅能识别出大约一半的误解。这揭示了在对话中识别机器人误解的根本限制。


<details>
  <summary>Details</summary>
Motivation: 在人机交互中检测误解对于保持用户参与和信任至关重要。虽然人类可以通过言语和非言语线索轻松检测对话中的沟通错误，但机器人在解释非言语反馈方面面临重大挑战，尽管计算机视觉在识别情感表达方面取得了进展。

Method: 本研究评估了机器学习模型在检测机器人对话中的误解效果。使用包含240个人机对话的多模态数据集，其中系统地引入了四种不同类型的对话失败，我们评估了最先进的计算机视觉模型的性能。在每次对话回合后，用户提供了关于是否感知到错误的反馈，从而分析模型准确检测机器人错误的能力。

Result: 尽管使用了最先进的模型，但在识别误解方面的表现仅略高于随机猜测，而在具有更多情感内容的数据集上，它们成功识别了困惑状态。人类评分员也仅能识别出大约一半的诱导误解，与我们的模型类似。

Conclusion: 研究揭示了在对话中识别机器人误解的根本限制：即使用户感知到误解，他们通常也不会向机器对话伙伴传达这一点。这一知识可以塑造对计算机视觉模型性能的期望，并帮助研究人员通过刻意引发反馈来设计更好的人机对话。

Abstract: Detecting miscommunication in human-robot interaction is a critical function
for maintaining user engagement and trust. While humans effortlessly detect
communication errors in conversations through both verbal and non-verbal cues,
robots face significant challenges in interpreting non-verbal feedback, despite
advances in computer vision for recognizing affective expressions. This
research evaluates the effectiveness of machine learning models in detecting
miscommunications in robot dialogue. Using a multi-modal dataset of 240
human-robot conversations, where four distinct types of conversational failures
were systematically introduced, we assess the performance of state-of-the-art
computer vision models. After each conversational turn, users provided feedback
on whether they perceived an error, enabling an analysis of the models' ability
to accurately detect robot mistakes. Despite using state-of-the-art models, the
performance barely exceeds random chance in identifying miscommunication, while
on a dataset with more expressive emotional content, they successfully
identified confused states. To explore the underlying cause, we asked human
raters to do the same. They could also only identify around half of the induced
miscommunications, similarly to our model. These results uncover a fundamental
limitation in identifying robot miscommunications in dialogue: even when users
perceive the induced miscommunication as such, they often do not communicate
this to their robotic conversation partner. This knowledge can shape
expectations of the performance of computer vision models and can help
researchers to design better human-robot conversations by deliberately
eliciting feedback where needed.

</details>

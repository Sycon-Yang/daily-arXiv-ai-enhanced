<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 38]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations](https://arxiv.org/abs/2509.07135)
*Ruggero Marino Lazzaroni,Alessandro Angioi,Michelangelo Puliga,Davide Sanna,Roberto Marras*

Main category: cs.CL

TL;DR: MedBench-IT is a comprehensive benchmark for evaluating LLMs on Italian medical university entrance examinations, aiming to fill the gap in non-English language benchmarks for specialized domains.


<details>
  <summary>Details</summary>
Motivation: There is a lack of benchmarks for non-English languages in specialized domains, particularly for Italian medical university entrance examinations. MedBench-IT aims to address this gap by providing a comprehensive benchmark for evaluating LLMs.

Method: We introduced MedBench-IT, the first comprehensive benchmark for evaluating LLMs on Italian medical university entrance examinations. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude series) and resource-efficient open-source alternatives (<30B parameters) focusing on practical deployability. We also conducted rigorous reproducibility tests, ordering bias analysis, and reasoning prompt evaluation.

Result: MedBench-IT comprises 17,410 expert-written multiple-choice questions across six subjects and three difficulty levels. We found a statistically significant but small inverse relationship between question readability and model performance.

Conclusion: MedBench-IT provides a crucial resource for the Italian NLP community, EdTech developers, and practitioners, offering insights into current capabilities and standardized evaluation methodology for this critical domain.

Abstract: Large language models (LLMs) show increasing potential in education, yet
benchmarks for non-English languages in specialized domains remain scarce. We
introduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on
Italian medical university entrance examinations. Sourced from Edizioni Simone,
a leading preparatory materials publisher, MedBench-IT comprises 17,410
expert-written multiple-choice questions across six subjects (Biology,
Chemistry, Logic, General Culture, Mathematics, Physics) and three difficulty
levels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude
series) and resource-efficient open-source alternatives (<30B parameters)
focusing on practical deployability.
  Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response
consistency, varying by subject), ordering bias analysis (minimal impact), and
reasoning prompt evaluation. We also examined correlations between question
readability and model performance, finding a statistically significant but
small inverse relationship. MedBench-IT provides a crucial resource for Italian
NLP community, EdTech developers, and practitioners, offering insights into
current capabilities and standardized evaluation methodology for this critical
domain.

</details>


### [2] [The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties](https://arxiv.org/abs/2509.07139)
*William Chen,Chutong Meng,Jiatong Shi,Martijn Bartelds,Shih-Heng Wang,Hsiu-Hsuan Wang,Rafael Mosquera,Sara Hincapie,Dan Jurafsky,Antonis Anastasopoulos,Hung-yi Lee,Karen Livescu,Shinji Watanabe*

Main category: cs.CL

TL;DR: Interspeech 2025 ML-SUPERB 2.0挑战通过构建一个涵盖200多种语言、口音和方言的测试套件，以及引入基于DynaBench的在线评估服务器，推动了多语言语音识别模型的发展。


<details>
  <summary>Details</summary>
Motivation: 为了推进最先进的多语言语音模型，需要一个全面的测试套件来评估多语言语音模型的表现。

Method: 提出了Interspeech 2025 ML-SUPERB 2.0挑战，构建了一个包含200多种语言、口音和方言的数据测试套件，并引入了基于DynaBench的在线评估服务器。

Result: 挑战收到了3个团队的5个提交，所有提交都优于基线。最佳提交在通用多语言测试集上实现了LID准确性的绝对提升23%和CER减少18%。在带有口音和方言的数据上，最佳提交的CER降低了30.2%，LID准确性提高了15.7%。

Conclusion: 社区挑战在使语音技术更加包容方面的重要性得到了证明。

Abstract: Recent improvements in multilingual ASR have not been equally distributed
across languages and language varieties. To advance state-of-the-art (SOTA) ASR
models, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a
new test suite that consists of data from 200+ languages, accents, and dialects
to evaluate SOTA multilingual speech models. The challenge also introduces an
online evaluation server based on DynaBench, allowing for flexibility in model
design and architecture for participants. The challenge received 5 submissions
from 3 teams, all of which outperformed our baselines. The best-performing
submission achieved an absolute improvement in LID accuracy of 23% and a
reduction in CER of 18% when compared to the best baseline on a general
multilingual test set. On accented and dialectal data, the best submission
obtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance
of community challenges in making speech technologies more inclusive.

</details>


### [3] [Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models](https://arxiv.org/abs/2509.07142)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

TL;DR: 本研究提出了一种基于大型语言模型的自动化评估框架，用于评估动态演化主题模型。该框架通过九个LLM-based指标，覆盖四个关键维度，能够揭示传统指标未能检测到的主题模型弱点。


<details>
  <summary>Details</summary>
Motivation: 传统的自动指标（如连贯性和多样性）往往只能捕捉狭窄的统计模式，无法解释实际中的语义失败。因此，需要一种更全面的评估框架来评估动态演化的主题模型。

Method: 研究提出了一种基于大型语言模型（LLMs）的自动化评估框架，该框架使用九个LLM-based指标，涵盖四个关键维度：词汇有效性、主题内语义合理性、主题间结构合理性以及文档-主题对齐合理性。框架通过对抗性和采样协议进行验证，并应用于多个数据集和主题建模方法。

Result: LLM-based指标提供了可解释、稳健且任务相关的评估，揭示了主题模型中的关键弱点，如冗余和语义漂移，这些是传统指标通常无法检测到的。

Conclusion: 研究结果表明，基于LLM的评估指标能够提供可解释、稳健且与任务相关的评估，揭示了传统指标常忽视的主题模型中的关键弱点，如冗余和语义漂移。这些结果支持了开发可扩展、细粒度的评估工具，以在动态数据集中保持主题的相关性。

Abstract: This study presents a framework for automated evaluation of dynamically
evolving topic models using Large Language Models (LLMs). Topic modeling is
essential for organizing and retrieving scholarly content in digital library
systems, helping users navigate complex and evolving knowledge domains.
However, widely used automated metrics, such as coherence and diversity, often
capture only narrow statistical patterns and fail to explain semantic failures
in practice. We introduce a purpose-oriented evaluation framework that employs
nine LLM-based metrics spanning four key dimensions of topic quality: lexical
validity, intra-topic semantic soundness, inter-topic structural soundness, and
document-topic alignment soundness. The framework is validated through
adversarial and sampling-based protocols, and is applied across datasets
spanning news articles, scholarly publications, and social media posts, as well
as multiple topic modeling methods and open-source LLMs. Our analysis shows
that LLM-based metrics provide interpretable, robust, and task-relevant
assessments, uncovering critical weaknesses in topic models such as redundancy
and semantic drift, which are often missed by traditional metrics. These
results support the development of scalable, fine-grained evaluation tools for
maintaining topic relevance in dynamic datasets. All code and data supporting
this work are accessible at
https://github.com/zhiyintan/topic-model-LLMjudgment.

</details>


### [4] [Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector](https://arxiv.org/abs/2509.07177)
*Amal Chebbi,Babajide Kolade*

Main category: cs.CL

TL;DR: 本文介绍了EnergyGPT，一个针对能源领域的专用语言模型，通过微调LLaMA 3.1-8B模型，并展示了其在能源相关任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在能源等专业领域中由于缺乏深度技术专业知识和精确的领域知识而效果有限。

Method: 通过在高质量、精心策划的能源相关文本语料库上进行监督微调，对LLaMA 3.1-8B模型进行微调，开发了EnergyGPT。

Result: EnergyGPT在大多数能源相关的语言理解和生成任务中优于基础模型。

Conclusion: 通过这项工作，我们证明了我们的训练策略可以在不需要大规模基础设施的情况下提高领域相关性和性能。

Abstract: Large Language Models have demonstrated impressive capabilities across
various domains. However, their general-purpose nature often limits their
effectiveness in specialized fields such as energy, where deep technical
expertise and precise domain knowledge are essential. In this paper, we
introduce EnergyGPT, a domain-specialized language model tailored for the
energy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised
Fine-Tuning on a high-quality, curated corpus of energy-related texts. We
present a complete development pipeline, including data collection and
curation, model fine-tuning, benchmark design and LLM-judge choice, evaluation
and deployment. Through this work, we demonstrate that our training strategy
enables improvements in domain relevance and performance without the need for
large-scale infrastructure. By evaluating the performance of the model using
domain-specific question-answering benchmarks, our results demonstrate that
EnergyGPT outperforms the base model in most of the energy-related language
understanding and generation tasks.

</details>


### [5] [DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge](https://arxiv.org/abs/2509.07188)
*Zonghai Yao,Michael Sun,Won Seok Jang,Sunjae Kwon,Soie Kwon,Hong Yu*

Main category: cs.CL

TL;DR: 本文介绍了DischargeSim，这是一个新的基准测试，用于评估大型语言模型（LLM）在作为个性化出院教育者方面的能力。通过模拟医生和患者之间的多轮对话，评估模型在对话质量、个性化文档生成和患者理解方面的表现。实验结果揭示了LLM在出院教育能力上的显著差距，并指出模型大小并不总是带来更好的教育成果。


<details>
  <summary>Details</summary>
Motivation: Discharge communication is a critical yet underexplored component of patient care, where the goal shifts from diagnosis to education. While recent large language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they fail to evaluate models' ability to support patients after the visit.

Method: We introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability to act as personalized discharge educators. DischargeSim simulates post-visit, multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with diverse psychosocial profiles.

Result: Experiments across 18 LLMs reveal significant gaps in discharge education capability, with performance varying widely across patient profiles. Notably, model size does not always yield better education outcomes, highlighting trade-offs in strategy use and content prioritization.

Conclusion: DischargeSim offers a first step toward benchmarking LLMs in post-visit clinical education and promoting equitable, personalized patient support.

Abstract: Discharge communication is a critical yet underexplored component of patient
care, where the goal shifts from diagnosis to education. While recent large
language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they
fail to evaluate models' ability to support patients after the visit. We
introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability
to act as personalized discharge educators. DischargeSim simulates post-visit,
multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with
diverse psychosocial profiles (e.g., health literacy, education, emotion).
Interactions are structured across six clinically grounded discharge topics and
assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge
evaluation, (2) personalized document generation including free-text summaries
and structured AHRQ checklists, and (3) patient comprehension through a
downstream multiple-choice exam. Experiments across 18 LLMs reveal significant
gaps in discharge education capability, with performance varying widely across
patient profiles. Notably, model size does not always yield better education
outcomes, highlighting trade-offs in strategy use and content prioritization.
DischargeSim offers a first step toward benchmarking LLMs in post-visit
clinical education and promoting equitable, personalized patient support.

</details>


### [6] [Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation](https://arxiv.org/abs/2509.07190)
*Zahra Atf,Peter R Lewis*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则的道德原则的框架，用于处理大型语言模型生成文本中的不确定性。通过道德心理学和美德伦理学的见解，定义了预防、顺从和责任等规则，以在不确定性下指导响应。这些规则被编码在一个轻量级的Prolog引擎中，其中不确定性水平会触发与普通语言理由相一致的系统动作。情景模拟基准测试了规则覆盖范围、公平性和信任校准。临床和法律领域的用例展示了道德推理如何提高信任度和可解释性。这种方法为社会负责任的自然语言生成提供了一种透明且轻量级的替代方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越多地用于高风险环境，其中解释不确定性既是技术问题也是伦理问题。概率方法往往不透明，并且与对透明度的期望不符。

Method: 我们提出了一种基于规则的道德原则的框架，用于处理大型语言模型生成文本中的不确定性。使用道德心理学和美德伦理学的见解，我们定义了诸如预防、顺从和责任等规则，以在知识或随机不确定性下指导响应。这些规则被编码在一个轻量级的Prolog引擎中，其中不确定性水平（低、中、高）会触发与普通语言理由相一致的系统动作。

Result: 情景模拟基准测试了规则覆盖范围、公平性和信任校准。临床和法律领域的用例展示了道德推理如何提高信任度和可解释性。

Conclusion: 我们的方法为社会负责任的自然语言生成提供了一种透明且轻量级的替代方案，以概率模型为基础。

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where explaining uncertainty is both technical and ethical. Probabilistic
methods are often opaque and misaligned with expectations of transparency. We
propose a framework based on rule-based moral principles for handling
uncertainty in LLM-generated text. Using insights from moral psychology and
virtue ethics, we define rules such as precaution, deference, and
responsibility to guide responses under epistemic or aleatoric uncertainty.
These rules are encoded in a lightweight Prolog engine, where uncertainty
levels (low, medium, high) trigger aligned system actions with plain-language
rationales. Scenario-based simulations benchmark rule coverage, fairness, and
trust calibration. Use cases in clinical and legal domains illustrate how moral
reasoning can improve trust and interpretability. Our approach offers a
transparent, lightweight alternative to probabilistic models for socially
responsible natural language generation.

</details>


### [7] [LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade](https://arxiv.org/abs/2509.07274)
*Aida Kostikova,Ole Pütz,Steffen Eger,Olga Sabelfeld,Benjamin Paassen*

Main category: cs.CL

TL;DR: 本文评估了多种大型语言模型在标注德国议会辩论中的(反)团结子类型方面的表现，并发现战后时期对移民有高度支持，而自2015年以来反支持趋势增强。


<details>
  <summary>Details</summary>
Motivation: 研究政治言论对于广泛现象的深度分析传统上需要大量的手动注释，限制了分析范围。大型语言模型有可能部分自动化甚至复杂的注释任务。

Method: 我们评估了多种大型语言模型在标注德国议会辩论中的(反)团结子类型方面的表现，并将其与数千份人工参考标注进行比较。我们评估了模型大小、提示差异、微调、历史与当代数据的影响，并研究了系统性错误。

Result: 我们的数据揭示了战后时期对移民的高度支持，以及自2015年以来德国议会中反支持的强烈趋势。

Conclusion: 我们的数据揭示了战后时期对移民的高度支持，以及自2015年以来德国议会中反支持的强烈趋势，这促使进一步研究。这些发现突显了LLMs在政治文本分析中的潜力，以及移民辩论在德国的重要性，那里人口下降和劳动力短缺与日益加剧的两极分化并存。

Abstract: Migration has been a core topic in German political debate, from millions of
expellees post World War II over labor migration to refugee movements in the
recent past. Studying political speech regarding such wide-ranging phenomena in
depth traditionally required extensive manual annotations, limiting the scope
of analysis to small subsets of the data. Large language models (LLMs) have the
potential to partially automate even complex annotation tasks. We provide an
extensive evaluation of a multiple LLMs in annotating (anti-)solidarity
subtypes in German parliamentary debates compared to a large set of thousands
of human reference annotations (gathered over a year). We evaluate the
influence of model size, prompting differences, fine-tuning, historical versus
contemporary data; and we investigate systematic errors. Beyond methodological
evaluation, we also interpret the resulting annotations from a social science
lense, gaining deeper insight into (anti-)solidarity trends towards migrants in
the German post-World War II period and recent past. Our data reveals a high
degree of migrant-directed solidarity in the postwar period, as well as a
strong trend towards anti-solidarity in the German parliament since 2015,
motivating further research. These findings highlight the promise of LLMs for
political text analysis and the importance of migration debates in Germany,
where demographic decline and labor shortages coexist with rising polarization.

</details>


### [8] [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)
*Zhuoqing Song,Peng Sun,Huizhuo Yuan,Quanquan Gu*

Main category: cs.CL

TL;DR: CASTLE是一种新的注意力机制，可以在保持自回归属性的同时，通过前瞻键提升语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 标准因果注意力中的每个标记的查询、键和值（QKV）是静态的，仅编码前面的上下文。

Method: 引入了CAuSal aTtention with Lookahead kEys (CASTLE)，一种注意力机制，可以随着上下文的展开不断更新每个标记的键。

Result: CASTLE在语言建模基准上 consistently 超过标准因果注意力，减少了验证困惑度并提高了各种下游任务的性能。

Conclusion: CASTLE在语言建模基准上 consistently 超过标准因果注意力，减少了验证困惑度并在各种下游任务中提高了性能。

Abstract: In standard causal attention, each token's query, key, and value (QKV) are
static and encode only preceding context. We introduce CAuSal aTtention with
Lookahead kEys (CASTLE), an attention mechanism that continually updates each
token's keys as the context unfolds. We term these updated keys lookahead keys
because they belong to earlier positions yet integrate information from tokens
that appear later relative to those positions, while strictly preserving the
autoregressive property. Although the mechanism appears sequential, we derive a
mathematical equivalence that avoids explicitly materializing lookahead keys at
each position and enables efficient parallel training. On language modeling
benchmarks, CASTLE consistently outperforms standard causal attention across
model scales, reducing validation perplexity and improving performance on a
range of downstream tasks.

</details>


### [9] [Basis Vector Metric: A Method for Robust Open-Ended State Change Detection](https://arxiv.org/abs/2509.07308)
*David Oprea,Sam Powers*

Main category: cs.CL

TL;DR: The paper evaluates the BVM method for classifying image states using language embeddings and finds that it performs well for nouns but not as effectively as logistic regression for adjectives.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate the effectiveness of the BVM method in classifying image states using language embeddings and to compare it with existing methods.

Method: The paper tests the BVM method for judging state changes in images using language embeddings. It compares BVM with other metrics like cosine similarity, dot product, product quantization, binary index, Naive Bayes, and a custom neural network. It also compares BVM's ability to differentiate adjectives with a logistic regression model.

Result: BVM performs best in classifying states for each noun. However, it does not outperform the logistic regression model in differentiating adjectives. The study identifies potential improvements for BVM.

Conclusion: BVM method shows good performance in classifying states for each noun, but it is not superior to logistic regression in differentiating adjectives. However, there is potential for improvement in the BVM method.

Abstract: We test a new method, which we will abbreviate using the acronym BVM (Basis
Vectors Method), in its ability to judge the state changes in images through
using language embeddings. We used the MIT-States dataset, containing about
53,000 images, to gather all of our data, which has 225 nouns and 115
adjectives, with each noun having about 9 different adjectives, forming
approximately 1000 noun-adjective pairs. For our first experiment, we test our
method's ability to determine the state of each noun class separately against
other metrics for comparison. These metrics are cosine similarity, dot product,
product quantization, binary index, Naive Bayes, and a custom neural network.
Among these metrics, we found that our proposed BVM performs the best in
classifying the states for each noun. We then perform a second experiment where
we try using BVM to determine if it can differentiate adjectives from one
another for each adjective separately. We compared the abilities of BVM to
differentiate adjectives against the proposed method the MIT-States paper
suggests: using a logistic regression model. In the end, we did not find
conclusive evidence that our BVM metric could perform better than the logistic
regression model at discerning adjectives. Yet, we were able to find evidence
for possible improvements to our method; this leads to the chance of increasing
our method's accuracy through certain changes in our methodologies.

</details>


### [10] [Instance-level Performance Prediction for Long-form Generation Tasks](https://arxiv.org/abs/2509.07309)
*Chi-Yang Hsu,Alexander Braylan,Yiheng Su,Omar Alonso,Matthew Lease*

Main category: cs.CL

TL;DR: 本文介绍了一个新的基准，用于实例级性能预测长期生成任务，该基准要求推断预测区间以量化点估计周围的不确定性。


<details>
  <summary>Details</summary>
Motivation: 我们激励并分享一个新的基准，用于实例级性能预测长期生成任务，具有多方面、细粒度的质量指标。

Method: 我们的任务、模型和度量标准无关的公式给定仅黑盒模型输入和输出，预测连续评估指标分数。

Result: 我们展示了可以有效地预测跨长期生成任务的分数，使用最少16个训练示例。

Conclusion: 我们引入了一个新颖且有用的任务，一个有价值的比赛基准来推动进展，并且有现成的基线可以今天实际采用。

Abstract: We motivate and share a new benchmark for instance-level performance
prediction of long-form generation tasks having multi-faceted, fine-grained
quality metrics. Our task-, model- and metric-agnostic formulation predicts
continuous evaluation metric scores given only black-box model inputs and
outputs. Beyond predicting point estimates of metric scores, the benchmark also
requires inferring prediction intervals to quantify uncertainty around point
estimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs,
baselines, and metrics per task. We show that scores can be effectively
predicted across long-form generation tasks using as few as 16 training
examples. Overall, we introduce a novel and useful task, a valuable benchmark
to drive progress, and baselines ready for practical adoption today.

</details>


### [11] [Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations](https://arxiv.org/abs/2509.07311)
*Sihyun Park*

Main category: cs.CL

TL;DR: 本文提出了一种基于模型内部表示的知识分析方法KAMIR，用于选择有效的训练数据，以提高模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法存在依赖提示工程、对变化敏感和需要额外成本的问题，因此需要一种更有效的方法。

Method: KAMIR方法通过分析模型的内部表示来选择训练数据，计算每个层（块）的隐藏状态与最终隐藏状态之间的相似性来评估数据。

Result: KAMIR方法在各种任务数据集上的实验结果表明，使用较少熟悉的数据显示出更好的泛化性能。

Conclusion: 实验结果表明，使用较少熟悉的数据显示出更好的泛化性能。

Abstract: Recent advances in large language models (LLMs) have been driven by
pretraining, supervised fine tuning (SFT), and alignment tuning. Among these,
SFT plays a crucial role in transforming a model 's general knowledge into
structured responses tailored to specific tasks. However, there is no clearly
established methodology for effective training data selection. Simply
increasing the volume of data does not guarantee performance improvements,
while preprocessing, sampling, and validation require substantial time and
cost.
  To address this issue, a variety of data selection methods have been
proposed. Among them, knowledge based selection approaches identify suitable
training data by analyzing the model 's responses. Nevertheless, these methods
typically rely on prompt engineering, making them sensitive to variations and
incurring additional costs for prompt design.
  In this study, we propose Knowledge Analysis via Model Internal
Representations (KAMIR), a novel approach that overcomes these limitations by
analyzing data based on the model 's internal representations. KAMIR computes
similarities between the hidden states of each layer (block) and the final
hidden states for a given input to assess the data. Unlike prior methods that
were largely limited to multiple choice tasks, KAMIR can be applied to a wide
range of tasks such as machine reading comprehension and summarization.
Moreover, it selects data useful for training based on the model 's familiarity
with the input, even with a small dataset and a simple classifier architecture.
Experiments across diverse task datasets demonstrate that training with less
familiar data leads to better generalization performance.

</details>


### [12] [Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation](https://arxiv.org/abs/2509.07324)
*Nakyung Lee,Yeongoon Kim,Minhae Oh,Suhwan Kim,Jin Woo Koo,Hyewon Jo,Jungwoo Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为SAOBP的框架，通过信念传播过程注入多跳关系，以解决Transformer模型中注意力集中在有限的标记上的问题。同时引入了全局标记依赖（GTD）来量化这些交互。实验结果表明，SAOBP有助于防止深层中的熵崩溃，并适应性地保持GTD在任务适当的水平，从而提高模型性能。特别是在小规模模型中观察到了竞争性的增益，显示出其在资源受限场景中的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型中注意力集中在有限的标记上，无法捕捉长距离依赖的问题。

Method: 提出Self-Attention One-step Belief Propagation (SAOBP)框架，通过信念传播过程注入多跳关系，并引入Global Token Dependency (GTD)来量化这些交互。

Result: SAOBP有助于防止深层中的熵崩溃，并适应性地保持GTD在任务适当的水平，从而提高模型性能。特别是在小规模模型中观察到了竞争性的增益。

Conclusion: SAOBP能够有效改善模型性能，尤其在资源受限的场景下具有潜在的应用价值。

Abstract: Transformer-based self-attention mechanism serves as the core of modern
language models, yet it often suffers from localization, where attentions
collapse onto a limited subset of tokens and fail to capture long-range
dependencies. To address this issue, we propose Self-Attention One-step Belief
Propagation (SAOBP), a refinement framework that injects multi-hop
relationships through a belief propagation process. To interpret and quantify
these interactions, we introduce Global Token Dependency (GTD) that captures
the relative contribution of multihop connections within the attention graph.
Empirical results indicate that SAOBP helps prevent entropy collapse in deeper
layers and adaptively maintains GTD at task-appropriate levels, thereby
supporting improvements in model performance. Importantly, we observe
competitive gains in small-scale models, highlighting its potential for
improving inference quality in resource-constrained scenarios.

</details>


### [13] [PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions](https://arxiv.org/abs/2509.07370)
*Yixuan Tang,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: PersonaFuse is a new method for training large language models to better understand and respond to social and emotional contexts, improving their ability to interact with humans in a more natural and effective way.


<details>
  <summary>Details</summary>
Motivation: LLMs often exhibit limitations in emotional perception and social competence during real-world conversations. These limitations partly originate from their inability to adapt their communication style and emotional expression to different social and task contexts.

Method: PersonaFuse is a novel LLM post-training framework that enables LLMs to adapt and express different personalities for varying situations. It employs a Mixture-of-Expert architecture that combines persona adapters with a dynamic routing network, enabling contextual trait expression.

Result: Experimental results show that PersonaFuse substantially outperforms baseline models across multiple dimensions of social-emotional intelligence. It also delivers consistent improvements in downstream human-centered applications, such as mental health counseling and review-based customer service.

Conclusion: PersonaFuse offers a theoretically grounded and practical approach for developing social-emotional enhanced LLMs, marking a significant advancement toward more human-centric AI systems.

Abstract: Recent advancements in Large Language Models (LLMs) demonstrate remarkable
capabilities across various fields. These developments have led to more direct
communication between humans and LLMs in various situations, such as social
companionship and psychological support. However, LLMs often exhibit
limitations in emotional perception and social competence during real-world
conversations. These limitations partly originate from their inability to adapt
their communication style and emotional expression to different social and task
contexts. In this work, we introduce PersonaFuse, a novel LLM post-training
framework that enables LLMs to adapt and express different personalities for
varying situations. Inspired by Trait Activation Theory and the Big Five
personality model, PersonaFuse employs a Mixture-of-Expert architecture that
combines persona adapters with a dynamic routing network, enabling contextual
trait expression. Experimental results show that PersonaFuse substantially
outperforms baseline models across multiple dimensions of social-emotional
intelligence. Importantly, these gains are achieved without sacrificing general
reasoning ability or model safety, which remain common limitations of direct
prompting and supervised fine-tuning approaches. PersonaFuse also delivers
consistent improvements in downstream human-centered applications, such as
mental health counseling and review-based customer service. Finally, human
preference evaluations against leading LLMs, including GPT-4o and DeepSeek,
demonstrate that PersonaFuse achieves competitive response quality despite its
comparatively smaller model size. These findings demonstrate that
PersonaFuse~offers a theoretically grounded and practical approach for
developing social-emotional enhanced LLMs, marking a significant advancement
toward more human-centric AI systems.

</details>


### [14] [Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents](https://arxiv.org/abs/2509.07389)
*Sankalp Tattwadarshi Swain,Anshika Krishnatray,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CL

TL;DR: 研究提出了一种新的实验框架，评估LLM代理在与仅理解Tinkatongue的机器人对话中获取和使用新构造语言的能力。结果显示LLM代理未能建立对话，但采用了类似人类的学习策略，这为评估基准和模型设计提供了新的方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注词汇学习、形态规则归纳、句法泛化、语用推理和跨语言迁移，但没有评估LLM代理是否能通过模式识别和交互反馈来学习语言，这是人类语言习得的核心特征。

Method: 提出了一种新的实验框架，其中LLM代理在与仅理解Tinkatongue的机器人对话中评估其获取和使用新构造语言的能力。

Result: LLM代理在100次回复内未能建立对话，但采用了类似于人类语言学习策略的不同策略。

Conclusion: 研究结果表明，评估基准需要新的方向，并为模型设计提供了从交互反馈中更有效地学习的途径。

Abstract: Existing evaluation studies on linguistic competence of large language models
(LLM agents) have focused primarily on vocabulary learning, morphological rule
induction, syntactic generalization, pragmatic inference, and cross-linguistic
transfer. However, none assess whether LLM agents can acquire a language
through pattern recognition and interactive feedback, a central feature of
human language acquisition. We propose a novel experimental framework in which
an LLM agent is evaluated on its ability to acquire and use a newly constructed
language (Tinkatongue) in conversation with a bot that understands only
Tinkatongue. Our findings show that LLM agents fail to establish a conversation
within 100 responses, yet they adopt distinct strategies that mirror human
approaches to language learning. The results suggest a new direction for
evaluation benchmarks and open pathways to model designs that learn more
effectively from interactive feedback.

</details>


### [15] [The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering](https://arxiv.org/abs/2509.07399)
*Yi-Jie Cheng,Oscar Chew,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 本文研究了现有集成方法在小型语言模型中的能力，并提出使用简单的探索模块来改善知识图谱问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于小型语言模型在知识图谱遍历和推理方面的有限能力，因此需要改进。

Method: 提出利用简单高效的探索模块来处理知识图谱遍历，以替代语言模型本身。

Result: 实验结果表明，这些轻量级模块能有效提升小型语言模型在知识图谱问答任务上的性能。

Conclusion: 实验结果表明，这些轻量级模块能够有效提高小型语言模型在知识图谱问答任务上的性能。

Abstract: Integrating knowledge graphs (KGs) into the reasoning processes of large
language models (LLMs) has emerged as a promising approach to mitigate
hallucination. However, existing work in this area often relies on proprietary
or extremely large models, limiting accessibility and scalability. In this
study, we investigate the capabilities of existing integration methods for
small language models (SLMs) in KG-based question answering and observe that
their performance is often constrained by their limited ability to traverse and
reason over knowledge graphs. To address this limitation, we propose leveraging
simple and efficient exploration modules to handle knowledge graph traversal in
place of the language model itself. Experiment results demonstrate that these
lightweight modules effectively improve the performance of small language
models on knowledge graph question answering tasks. Source code:
https://github.com/yijie-cheng/SLM-ToG/.

</details>


### [16] [LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction](https://arxiv.org/abs/2509.07403)
*Weichu Liu,Jing Xiong,Yuxuan Hu,Zixuan Li,Minghuan Tan,Ningning Mao,Chenyang Zhao,Zhongwei Wan,Chaofan Tao,Wendong Xu,Hui Shen,Chengming Li,Lingpeng Kong,Ngai Wong*

Main category: cs.CL

TL;DR: 本文提出了LongEmotion，一个针对长上下文情感智能任务的基准。通过引入RAG和CoEM方法，显著提高了LLMs在长上下文任务中的情感智能表现，并展示了不同模型在情感智能方面的差异。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试往往忽略了长上下文中EI的某些方面，特别是在交互漫长、多样且通常嘈杂的现实场景中。为了向这种现实场景迈进，我们提出了LongEmotion，这是一个专门设计用于长上下文EI任务的基准。

Method: 我们提出了LongEmotion，这是一个专门针对长上下文EI任务的基准。为了在现实约束下提高性能，我们结合了检索增强生成（RAG）和协作情感建模（CoEM），并与标准提示方法进行比较。RAG方法利用对话上下文和大语言模型本身作为检索源，避免依赖外部知识库。CoEM方法通过将任务分解为五个阶段，整合了检索增强和有限的知识注入来进一步提高性能。

Result: 实验结果表明，RAG和CoEM在大多数长上下文任务中 consistently 提高了与EI相关的性能，推动了LLMs向更实用和现实的EI应用发展。此外，我们对GPT系列进行了比较案例研究实验，以展示不同模型在EI方面的差异。

Conclusion: 实验结果表明，RAG和CoEM在大多数长上下文任务中 consistently 提高了与EI相关的性能，推动了LLMs向更实用和现实的EI应用发展。此外，我们对GPT系列进行了比较案例研究实验，以展示不同模型在EI方面的差异。

Abstract: Large language models (LLMs) make significant progress in Emotional
Intelligence (EI) and long-context understanding. However, existing benchmarks
tend to overlook certain aspects of EI in long-context scenarios, especially
under realistic, practical settings where interactions are lengthy, diverse,
and often noisy. To move towards such realistic settings, we present
LongEmotion, a benchmark specifically designed for long-context EI tasks. It
covers a diverse set of tasks, including Emotion Classification, Emotion
Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion
Expression. On average, the input length for these tasks reaches 8,777 tokens,
with long-form generation required for Emotion Expression. To enhance
performance under realistic constraints, we incorporate Retrieval-Augmented
Generation (RAG) and Collaborative Emotional Modeling (CoEM), and compare them
with standard prompt-based methods. Unlike conventional approaches, our RAG
method leverages both the conversation context and the large language model
itself as retrieval sources, avoiding reliance on external knowledge bases. The
CoEM method further improves performance by decomposing the task into five
stages, integrating both retrieval augmentation and limited knowledge
injection. Experimental results show that both RAG and CoEM consistently
enhance EI-related performance across most long-context tasks, advancing LLMs
toward more practical and real-world EI applications. Furthermore, we conducted
a comparative case study experiment on the GPT series to demonstrate the
differences among various models in terms of EI. Code is available on GitHub at
https://github.com/LongEmotion/LongEmotion, and the project page can be found
at https://longemotion.github.io/.

</details>


### [17] [AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training](https://arxiv.org/abs/2509.07459)
*Christian Rene Thelen,Patrick Gustav Blaneck,Tobias Bornheim,Niklas Grieger,Stephan Bialonski*

Main category: cs.CL

TL;DR: 本文研究了如何在德语YouTube语料库中检测积极的支持性语言，并发现多语言XLM-RoBERTa-Large模型在检测任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 积极的支持性在线交流（糖果话语）有潜力促进文明，但自动化检测这种语言仍然研究不足，限制了对其影响的系统分析。

Method: 我们通过单语和多语言语言模型（包括GBERT、Qwen3 Embedding和XLM-RoBERTa）研究了如何在46k条评论的德语YouTube语料库中可靠地检测糖果话语。

Result: 我们发现，经过微调以在跨度级别检测糖果话语的多语言XLM-RoBERTa-Large模型在GermEval 2025共享任务的二元正F1（0.8906）和分类跨度检测（严格F1：0.6307）子任务中排名第一。

Conclusion: 我们的结果证明了多语言模型在识别积极、支持性语言方面的有效性。

Abstract: Positive, supportive online communication in social media (candy speech) has
the potential to foster civility, yet automated detection of such language
remains underexplored, limiting systematic analysis of its impact. We
investigate how candy speech can be reliably detected in a 46k-comment German
YouTube corpus by monolingual and multilingual language models, including
GBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual
XLM-RoBERTa-Large model trained to detect candy speech at the span level
outperforms other approaches, ranking first in both binary positive F1: 0.8906)
and categorized span-based detection (strict F1: 0.6307) subtasks at the
GermEval 2025 Shared Task on Candy Speech Detection. We speculate that
span-based training, multilingual capabilities, and emoji-aware tokenizers
improved detection performance. Our results demonstrate the effectiveness of
multilingual models in identifying positive, supportive language.

</details>


### [18] [Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts](https://arxiv.org/abs/2509.07462)
*Yiliang Zhou,Di Hu,Tianchu Lyu,Jasmine Dhillon,Alexandra L. Beck,Gelareh Sadigh,Kai Zheng*

Main category: cs.CL

TL;DR: 本研究旨在通过系统文献检索和比较分析，探讨医疗保健中歧视性语言词典的现状，并强调制定标准化词典的必要性。


<details>
  <summary>Details</summary>
Motivation: 歧视性语言导致医疗不平等，但目前没有普遍接受或标准化的词典来定义哪些词语、术语或短语构成医疗保健中的歧视性语言。

Method: 我们进行了系统的文献检索，以识别现有的歧视性语言词典，然后进行了比较分析，以检查这些词典之间的相似性和差异性，并基于一个已建立的情感数据集分析了正面、负面或中性术语的分布情况。

Result: 我们的搜索确定了四个词典。分析结果显示它们之间有中等程度的语义相似性，并且大多数歧视性术语与临床医生用来描述感知到的负面行为的判断性表达有关。情感分析显示大部分术语被归类为负面，尽管不同词典之间存在差异。

Conclusion: 我们的研究结果强调了制定标准化词典的必要性，并突出了在定义临床文本中的歧视性语言时面临的挑战。

Abstract: Stigmatizing language results in healthcare inequities, yet there is no
universally accepted or standardized lexicon defining which words, terms, or
phrases constitute stigmatizing language in healthcare. We conducted a
systematic search of the literature to identify existing stigmatizing language
lexicons and then analyzed them comparatively to examine: 1) similarities and
discrepancies between these lexicons, and 2) the distribution of positive,
negative, or neutral terms based on an established sentiment dataset. Our
search identified four lexicons. The analysis results revealed moderate
semantic similarity among them, and that most stigmatizing terms are related to
judgmental expressions by clinicians to describe perceived negative behaviors.
Sentiment analysis showed a predominant proportion of negatively classified
terms, though variations exist across lexicons. Our findings underscore the
need for a standardized lexicon and highlight challenges in defining
stigmatizing language in clinical texts.

</details>


### [19] [From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation](https://arxiv.org/abs/2509.07471)
*Mardiyyah Oduwole,Oluwatosin Olajide,Jamiu Suleiman,Faith Hunja,Busayo Awobade,Fatimo Adebanjo,Comfort Akanni,Chinonyelum Igwe,Peace Ododo,Promise Omoigui,Steven Kolawole,Abraham Owodunni*

Main category: cs.CL

TL;DR: 本研究评估了数据增强技术在提高低资源非洲语言机器翻译性能方面的效果，结果显示这些技术能显著提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 非洲大陆的语言多样性带来了不同的挑战和机遇，特别是在机器翻译领域。低资源语言的翻译系统需要改进以更好地服务于这些语言的使用者。

Method: 本研究探讨了句子拼接与反向翻译以及替换输出两种数据增强技术在六种非洲语言中的应用效果。

Result: 实验结果表明，这些数据增强技术显著提高了机器翻译性能，所有六种语言的BLEU分数至少提高了25%。

Conclusion: 本研究展示了数据增强技术在提高低资源非洲语言机器翻译系统性能方面的潜力，为开发更稳健的翻译系统做出了贡献。

Abstract: The linguistic diversity across the African continent presents different
challenges and opportunities for machine translation. This study explores the
effects of data augmentation techniques in improving translation systems in
low-resource African languages. We focus on two data augmentation techniques:
sentence concatenation with back translation and switch-out, applying them
across six African languages. Our experiments show significant improvements in
machine translation performance, with a minimum increase of 25\% in BLEU score
across all six languages.We provide a comprehensive analysis and highlight the
potential of these techniques to improve machine translation systems for
low-resource languages, contributing to the development of more robust
translation systems for under-resourced languages.

</details>


### [20] [HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention](https://arxiv.org/abs/2509.07475)
*Saumya Goswami,Siddharth Kurra*

Main category: cs.CL

TL;DR: HALT-RAG is a post-hoc verification system that detects hallucinations in RAG pipeline outputs using a universal feature set and a task-adapted classifier, achieving high performance on multiple tasks and enabling a practical abstention mechanism.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting content that contradicts or is unsupported by a given source text is critical for the safe deployment of generative language models.

Method: HALT-RAG is a post-hoc verification system that uses a universal feature set derived from an ensemble of two frozen NLI models and lightweight lexical signals, along with a simple, calibrated, and task-adapted meta-classifier.

Result: HALT-RAG achieves strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA, and dialogue tasks, respectively, and enables a practical abstention mechanism.

Conclusion: HALT-RAG provides a reliable tool for balancing model performance with safety requirements by enabling a practical abstention mechanism through well-calibrated probabilities.

Abstract: Detecting content that contradicts or is unsupported by a given source text
is a critical challenge for the safe deployment of generative language models.
We introduce HALT-RAG, a post-hoc verification system designed to identify
hallucinations in the outputs of Retrieval-Augmented Generation (RAG)
pipelines. Our flexible and task-adaptable framework uses a universal feature
set derived from an ensemble of two frozen, off-the-shelf Natural Language
Inference (NLI) models and lightweight lexical signals. These features are used
to train a simple, calibrated, and task-adapted meta-classifier. Using a
rigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and
produce unbiased estimates, we evaluate our system on the HaluEval benchmark.
By pairing our universal feature set with a lightweight, task-adapted
classifier and a precision-constrained decision policy, HALT-RAG achieves
strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,
and dialogue tasks, respectively. The system's well-calibrated probabilities
enable a practical abstention mechanism, providing a reliable tool for
balancing model performance with safety requirements.

</details>


### [21] [ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval](https://arxiv.org/abs/2509.07512)
*Zihan Chen,Lei Shi,Weize Wu,Qiji Zhou,Yue Zhang*

Main category: cs.CL

TL;DR: ALLabel是一种三阶段框架，通过选择最有信息量和代表性的样本，实现了在实体识别任务中的高性能和低成本。


<details>
  <summary>Details</summary>
Motivation: 当前实体识别LLM依赖于微调技术，但微调过程通常成本高昂。为了实现最佳的性能-成本权衡，需要一种更高效的方法来选择样本进行标注。

Method: ALLabel是一个三阶段框架，旨在选择最具有信息量和代表性的样本，用于构建LLM的上下文学习的地面真实检索语料库。该框架依次采用三种不同的主动学习策略。

Result: ALLabel在三个专业领域数据集上均优于所有基线方法，并且在相同标注预算下表现一致。实验还表明，仅用5%-10%的数据进行标注即可达到与全数据标注相当的性能。

Conclusion: ALLabel通过选择最有信息量和代表性的样本，在保持高性能的同时实现了最佳的性能-成本权衡。实验结果表明，仅用5%-10%的数据进行标注即可达到使用全部数据标注的效果。

Abstract: Many contemporary data-driven research efforts in the natural sciences, such
as chemistry and materials science, require large-scale, high-performance
entity recognition from scientific datasets. Large language models (LLMs) have
increasingly been adopted to solve the entity recognition task, with the same
trend being observed on all-spectrum NLP tasks. The prevailing entity
recognition LLMs rely on fine-tuned technology, yet the fine-tuning process
often incurs significant cost. To achieve a best performance-cost trade-off, we
propose ALLabel, a three-stage framework designed to select the most
informative and representative samples in preparing the demonstrations for LLM
modeling. The annotated examples are used to construct a ground-truth retrieval
corpus for LLM in-context learning. By sequentially employing three distinct
active learning strategies, ALLabel consistently outperforms all baselines
under the same annotation budget across three specialized domain datasets.
Experimental results also demonstrate that selectively annotating only 5\%-10\%
of the dataset with ALLabel can achieve performance comparable to the method
annotating the entire dataset. Further analyses and ablation studies verify the
effectiveness and generalizability of our proposal.

</details>


### [22] [VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents](https://arxiv.org/abs/2509.07553)
*Zheng Wu,Heyuan Huang,Xingyu Lou,Xiangmou Qu,Pengzhou Cheng,Zongru Wu,Weiwen Liu,Weinan Zhang,Jun Wang,Zhaoxiang Wang,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种查询驱动的人机代理-GUI交互框架，以提高OS代理在不可靠环境中的任务完成可靠性。通过引入VeriOS-Agent，实验表明其在不可靠场景下的性能显著提升，同时保持正常性能。


<details>
  <summary>Details</summary>
Motivation: 现有的OS代理大多设计用于理想化环境，而现实世界环境往往存在不可靠的情况。为了减轻这些情况下过度执行的风险，我们需要一种能够决定何时查询人类以提高任务完成可靠性的方法。

Method: 我们提出了一个查询驱动的人机代理-GUI交互框架，使OS代理能够在需要时查询人类以更可靠地完成任务。基于此框架，我们引入了VeriOS-Agent，这是一种通过两阶段学习范式训练的可信OS代理，有助于解耦和利用元知识。

Result: 实验表明，VeriOS-Agent在不可靠场景下的平均步骤成功率比最先进的方法提高了20.64%，同时不影响正常性能。分析突显了VeriOS-Agent的合理性、泛化性和可扩展性。

Conclusion: 实验表明，VeriOS-Agent在不可靠场景下的平均步骤成功率提高了20.64%，同时不影响正常性能。分析突显了VeriOS-Agent的合理性、泛化性和可扩展性。

Abstract: With the rapid progress of multimodal large language models, operating system
(OS) agents become increasingly capable of automating tasks through on-device
graphical user interfaces (GUIs). However, most existing OS agents are designed
for idealized settings, whereas real-world environments often present
untrustworthy conditions. To mitigate risks of over-execution in such
scenarios, we propose a query-driven human-agent-GUI interaction framework that
enables OS agents to decide when to query humans for more reliable task
completion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy
OS agent trained with a two-stage learning paradigm that falicitate the
decoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent
autonomously executes actions in normal conditions while proactively querying
humans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves
the average step-wise success rate by 20.64\% in untrustworthy scenarios over
the state-of-the-art, without compromising normal performance. Analysis
highlights VeriOS-Agent's rationality, generalizability, and scalability. The
codes, datasets and models are available at
https://github.com/Wuzheng02/VeriOS.

</details>


### [23] [Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition](https://arxiv.org/abs/2509.07555)
*Yi Liu,Xiangrong Zhu,Xiangyu Liu,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: 本文提出了一种新的迭代检索增强知识编辑方法（IRAKE），以解决多跳问答中由于编辑跳过导致的知识编辑失败问题，并在实验中证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的知识编辑方法在处理多跳问答时存在“编辑跳过”问题，即在推理过程中跳过了相关的编辑事实。

Method: 通过单个编辑事实和整个编辑案例的指导，提出了一种新的迭代检索增强知识编辑方法（IRAKE）。

Result: 实验结果表明，IRAKE可以缓解由编辑跳过引起的编辑失败，并且在多跳问答中的知识编辑方面优于最先进的方法。

Conclusion: IRAKE可以缓解由于编辑跳过导致的编辑失败，并在多跳问答中的知识编辑方面优于最先进的方法。

Abstract: In a rapidly evolving world where information updates swiftly, knowledge in
large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a
cost-effective option, making knowledge editing (KE) without modifying
parameters particularly necessary. We find that although existing
retrieval-augmented generation (RAG)-based KE methods excel at editing simple
knowledge, they struggle with KE in multi-hop question answering due to the
issue of "edit skipping", which refers to skipping the relevant edited fact in
inference. In addition to the diversity of natural language expressions of
knowledge, edit skipping also arises from the mismatch between the granularity
of LLMs in problem-solving and the facts in the edited memory. To address this
issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing
method with guided decomposition (IRAKE) through the guidance from single
edited facts and entire edited cases. Experimental results demonstrate that
IRAKE mitigates the failure of editing caused by edit skipping and outperforms
state-of-the-art methods for KE in multi-hop question answering.

</details>


### [24] [BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment](https://arxiv.org/abs/2509.07588)
*Andrey Sakhovskiy,Elena Tutubalina*

Main category: cs.CL

TL;DR: 本文提出了一种新的联合语言模型和知识图谱预训练方法BALI，通过增强语言模型的外部知识，提升了其在生物医学文本理解任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学语言模型在理解和处理复杂、领域特定的概念结构以及生物医学知识图谱中的事实信息方面存在局限性。因此，需要一种新的方法来增强语言模型对这些信息的理解能力。

Method: 本文提出了BALI方法，通过将生物医学概念提及链接到统一医学语言系统（UMLS）知识图谱，并利用局部知识图谱子图作为这些提及的跨模态正样本，来增强语言模型的外部知识。

Result: 实验结果表明，将BALI方法应用于多个领先的生物医学语言模型（如PubMedBERT和BioLinkBERT）可以提升它们在各种语言理解任务上的表现以及实体表示的质量，即使在少量对齐数据的情况下也能取得良好的效果。

Conclusion: 本文提出了一种新的联合语言模型和知识图谱预训练方法BALI，通过同时学习专用的知识图谱编码器并对齐语言模型和图的表示，增强了语言模型的外部知识。实验结果表明，该方法在多个领先的生物医学语言模型上提升了性能和实体表示质量。

Abstract: In recent years, there has been substantial progress in using pretrained
Language Models (LMs) on a range of tasks aimed at improving the understanding
of biomedical texts. Nonetheless, existing biomedical LLMs show limited
comprehension of complex, domain-specific concept structures and the factual
information encoded in biomedical Knowledge Graphs (KGs). In this work, we
propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel
joint LM and KG pre-training method that augments an LM with external knowledge
by the simultaneous learning of a dedicated KG encoder and aligning the
representations of both the LM and the graph. For a given textual sequence, we
link biomedical concept mentions to the Unified Medical Language System (UMLS)
KG and utilize local KG subgraphs as cross-modal positive samples for these
mentions. Our empirical findings indicate that implementing our method on
several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves
their performance on a range of language understanding tasks and the quality of
entity representations, even with minimal pre-training on a small alignment
dataset sourced from PubMed scientific abstracts.

</details>


### [25] [MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs](https://arxiv.org/abs/2509.07622)
*Libo Ren,Yee Man Ng,Lifeng Han*

Main category: cs.CL

TL;DR: 本文提出了一种基于视角感知的自我提示技术（PA-ISP）用于临床报告摘要，以提高患者和临床医生之间的沟通效率。


<details>
  <summary>Details</summary>
Motivation: 临床报告通常冗长且充满专业术语，使得领域专家难以高效识别文档中的重要方面。因此，需要一种更有效的摘要方法来促进患者和临床医生之间的共享决策。

Method: 本文使用了迭代自我提示技术（ISP）在大型语言模型（LLMs）上，并通过基于示例的少样本学习来优化任务特定提示。此外，还使用了ROUGE和BERT-score等度量标准来指导模型微调。

Result: 使用GPT-4和GPT-4o的视角感知ISP提交在3,396份来自开放期刊的临床案例报告中获得了ROUGE分数（46.53，24.68，30.77）和BERT分数（87.84，83.25，85.46）。高BERT分数表明模型生成的摘要在语义上与参考摘要相当，尽管在精确词汇层面的重叠较低。

Conclusion: 本文展示了如何利用视角感知的自我提示技术（PA-ISP）进行临床报告摘要，以改善患者和临床医生之间的沟通。

Abstract: Efficient communication between patients and clinicians plays an important
role in shared decision-making. However, clinical reports are often lengthy and
filled with clinical jargon, making it difficult for domain experts to identify
important aspects in the document efficiently. This paper presents the
methodology we applied in the MultiClinSUM shared task for summarising clinical
case documents. We used an Iterative Self-Prompting technique on large language
models (LLMs) by asking LLMs to generate task-specific prompts and refine them
via example-based few-shot learning. Furthermore, we used lexical and embedding
space metrics, ROUGE and BERT-score, to guide the model fine-tuning with
epochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved
ROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,
R, F1) from the official evaluation on 3,396 clinical case reports from various
specialties extracted from open journals. The high BERTscore indicates that the
model produced semantically equivalent output summaries compared to the
references, even though the overlap at the exact lexicon level is lower, as
reflected in the lower ROUGE scores. This work sheds some light on how
perspective-aware ISP (PA-ISP) can be deployed for clinical report
summarisation and support better communication between patients and clinicians.

</details>


### [26] [MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval](https://arxiv.org/abs/2509.07666)
*Xixi Wu,Yanchao Tan,Nan Hou,Ruiyang Zhang,Hong Cheng*

Main category: cs.CL

TL;DR: MoLoRAG 是一种逻辑感知的检索框架，用于多模态、多页文档理解，通过结合语义和逻辑相关性提高检索准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多页文档时会丢失关键的多模态信息，而现有的 RAG 方法仅依赖语义相关性，忽略了页面与查询之间的逻辑联系。

Method: MoLoRAG 是一种逻辑感知的检索框架，通过构建页面图来捕捉页面之间的上下文关系，并使用轻量级 VLM 进行图遍历以检索相关页面。

Result: MoLoRAG 在四个 DocQA 数据集上平均提高了 9.68% 的准确率，并在检索精度上比基线模型提高了 7.44%。

Conclusion: MoLoRAG 提高了多模态、多页文档理解的准确性，并在四个 DocQA 数据集上展示了显著的改进。

Abstract: Document Understanding is a foundational AI capability with broad
applications, and Document Question Answering (DocQA) is a key evaluation task.
Traditional methods convert the document into text for processing by Large
Language Models (LLMs), but this process strips away critical multi-modal
information like figures. While Large Vision-Language Models (LVLMs) address
this limitation, their constrained input size makes multi-page document
comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate
this by selecting relevant pages, but they rely solely on semantic relevance,
ignoring logical connections between pages and the query, which is essential
for reasoning.
  To this end, we propose MoLoRAG, a logic-aware retrieval framework for
multi-modal, multi-page document understanding. By constructing a page graph
that captures contextual relationships between pages, a lightweight VLM
performs graph traversal to retrieve relevant pages, including those with
logical connections often overlooked. This approach combines semantic and
logical relevance to deliver more accurate retrieval. After retrieval, the
top-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance
flexibility, MoLoRAG offers two variants: a training-free solution for easy
deployment and a fine-tuned version to improve logical relevance checking.
Experiments on four DocQA datasets demonstrate average improvements of 9.68% in
accuracy over LVLM direct inference and 7.44% in retrieval precision over
baselines. Codes and datasets are released at
https://github.com/WxxShirley/MoLoRAG.

</details>


### [27] [M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models](https://arxiv.org/abs/2509.07730)
*Zexuan Li,Hongliang Dai,Piji Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为M-BRe的框架，用于从未标记文本中提取训练实例进行关系抽取。该框架结合了多类分类和二元分类方法的优势，通过三个模块实现。实验结果表明，该框架在发现高质量训练样本方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 手动标注训练数据对于关系抽取来说可能非常昂贵，因为包含目标关系的句子在文本中可能非常稀少且难以找到。因此，开发一种能够从未标记文本中自动提取训练实例的方法是有益的。

Method: 本文提出了一个名为M-BRe的框架，该框架包含三个模块：关系分组、关系抽取和标签决策，结合了上述两种分类方法的优势。

Result: 实验结果表明，M-BRe框架能够有效地从未标记文本中发现高质量的训练样本，从而提高关系抽取的效果。

Conclusion: 本文提出了一个名为M-BRe的框架，用于从未标记文本中提取训练实例进行关系抽取。实验结果证实了其在发现高质量训练样本方面的优越能力。

Abstract: For Relation Extraction (RE), the manual annotation of training data may be
prohibitively expensive, since the sentences that contain the target relations
in texts can be very scarce and difficult to find. It is therefore beneficial
to develop an efficient method that can automatically extract training
instances from unlabeled texts for training RE models. Recently, large language
models (LLMs) have been adopted in various natural language processing tasks,
with RE also benefiting from their advances. However, when leveraging LLMs for
RE with predefined relation categories, two key challenges arise. First, in a
multi-class classification setting, LLMs often struggle to comprehensively
capture the semantics of every relation, leading to suboptimal results. Second,
although employing binary classification for each relation individually can
mitigate this issue, it introduces significant computational overhead,
resulting in impractical time complexity for real-world applications.
Therefore, this paper proposes a framework called M-BRe to extract training
instances from unlabeled texts for RE. It utilizes three modules to combine the
advantages of both of the above classification approaches: Relation Grouping,
Relation Extraction, and Label Decision. Extensive experiments confirm its
superior capability in discovering high-quality training samples from unlabeled
texts for RE.

</details>


### [28] [Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts](https://arxiv.org/abs/2509.07755)
*Rochana Prih Hastuti,Rian Adam Rajagede,Mansour Al Ghanim,Mengxin Zheng,Qian Lou*

Main category: cs.CL

TL;DR: 本文探讨了水印技术在医疗领域中的安全性和可靠性问题，并提出了一种新的评估方法来确保医疗内容的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的水印技术在医疗环境中的可靠性尚未得到充分测试，而现有的基准测试主要关注检测质量的权衡，忽视了低熵环境下可能存在的事实风险。

Method: 我们提出了一种基于医疗领域的评估流程，结合了事实准确性与连贯性评估，并引入了FWS（事实加权评分）作为综合指标。

Result: 当前的水印技术在医疗事实准确性方面有显著的下降，熵的变化会损害医疗实体的表示。

Conclusion: 我们的研究结果强调了在医疗领域需要更加关注水印技术对事实准确性的潜在影响，并提出了一个更全面的评估框架来指导水印技术的应用。

Abstract: As large language models (LLMs) adapted to sensitive domains such as
medicine, their fluency raises safety risks, particularly regarding provenance
and accountability. Watermarking embeds detectable patterns to mitigate these
risks, yet its reliability in medical contexts remains untested. Existing
benchmarks focus on detection-quality tradeoffs, overlooking factual risks
under low-entropy settings often exploited by watermarking's reweighting
strategy. We propose a medical-focused evaluation workflow that jointly
assesses factual accuracy and coherence. Using GPT-Judger and further human
validation, we introduce the Factuality-Weighted Score (FWS), a composite
metric prioritizing factual accuracy beyond coherence to guide watermarking
deployment in medical domains. Our evaluation shows current watermarking
methods substantially compromise medical factuality, with entropy shifts
degrading medical entity representation. These findings underscore the need for
domain-aware watermarking approaches that preserve the integrity of medical
content.

</details>


### [29] [Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning](https://arxiv.org/abs/2509.07768)
*Michele Joshua Maggini,Dhia Merzougui,Rabiraj Bandyopadhyay,Gaël Dias,Fabrice Maurel,Pablo Gamallo*

Main category: cs.CL

TL;DR: 研究比较了不同大语言模型在检测虚假新闻和政治偏见方面的性能，发现微调模型通常优于上下文学习策略。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对不同模型、使用方法和语言的大语言模型性能的基准测试，因此需要对大语言模型在检测极端党派和虚假新闻、有害推文和政治偏见方面的适应范式进行全面概述。

Method: 研究测试了不同的策略，包括语言模型的参数高效微调以及各种上下文学习策略和提示，如零样本提示、代码本、少样本（使用确定性点过程选择多样化示例）和思维链。

Result: 研究涵盖了10个数据集和5种语言（英语、西班牙语、葡萄牙语、阿拉伯语和保加利亚语），覆盖了二分类和多类分类场景。实验结果表明，上下文学习通常不如微调模型表现好。

Conclusion: 研究发现，与微调模型相比，上下文学习通常表现较差。这表明即使在任务特定设置中，对较小的模型进行微调也比使用最大的模型进行上下文学习更为重要。

Abstract: The spread of fake news, polarizing, politically biased, and harmful content
on online platforms has been a serious concern. With large language models
becoming a promising approach, however, no study has properly benchmarked their
performance across different models, usage methods, and languages. This study
presents a comprehensive overview of different Large Language Models adaptation
paradigms for the detection of hyperpartisan and fake news, harmful tweets, and
political bias. Our experiments spanned 10 datasets and 5 different languages
(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and
multiclass classification scenarios. We tested different strategies ranging
from parameter efficient Fine-Tuning of language models to a variety of
different In-Context Learning strategies and prompts. These included zero-shot
prompts, codebooks, few-shot (with both randomly-selected and
diversely-selected examples using Determinantal Point Processes), and
Chain-of-Thought. We discovered that In-Context Learning often underperforms
when compared to Fine-Tuning a model. This main finding highlights the
importance of Fine-Tuning even smaller models on task-specific settings even
when compared to the largest models evaluated in an In-Context Learning setup -
in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and
Qwen2.5-7B-Instruct.

</details>


### [30] [SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP](https://arxiv.org/abs/2509.07801)
*Decheng Duan,Yingyi Zhang,Jitong Peng,Chengzhi Zhang*

Main category: cs.CL

TL;DR: SciNLP 是一个专门用于自然语言处理领域全文实体和关系抽取的基准数据集，能够有效提升现有模型在学术文本中的提取能力，并有助于构建细粒度的知识图谱。


<details>
  <summary>Details</summary>
Motivation: 现有数据集大多只关注特定出版物部分，由于领域复杂性和标注成本高，缺乏对全文的标注。因此，需要一个专门的数据集来解决这一问题。

Method: 引入 SciNLP 数据集，通过对比实验验证其有效性，并评估最先进的监督模型在该数据集上的性能。

Result: SciNLP 是第一个提供 NLP 领域全文实体和关系标注的数据集，与现有数据集相比，在某些基线模型上取得了显著的性能提升。

Conclusion: SciNLP 是一个专门用于自然语言处理领域全文实体和关系抽取的基准数据集，能够有效提升现有模型在学术文本中的提取能力，并有助于构建细粒度的知识图谱。

Abstract: Structured information extraction from scientific literature is crucial for
capturing core concepts and emerging trends in specialized fields. While
existing datasets aid model development, most focus on specific publication
sections due to domain complexity and the high cost of annotating scientific
texts. To address this limitation, we introduce SciNLP - a specialized
benchmark for full-text entity and relation extraction in the Natural Language
Processing (NLP) domain. The dataset comprises 60 manually annotated full-text
NLP publications, covering 7,072 entities and 1,826 relations. Compared to
existing research, SciNLP is the first dataset providing full-text annotations
of entities and their relationships in the NLP domain. To validate the
effectiveness of SciNLP, we conducted comparative experiments with similar
datasets and evaluated the performance of state-of-the-art supervised models on
this dataset. Results reveal varying extraction capabilities of existing models
across academic texts of different lengths. Cross-comparisons with existing
datasets show that SciNLP achieves significant performance improvements on
certain baseline models. Using models trained on SciNLP, we implemented
automatic construction of a fine-grained knowledge graph for the NLP domain.
Our KG has an average node degree of 3.2 per entity, indicating rich semantic
topological information that enhances downstream applications. The dataset is
publicly available at https://github.com/AKADDC/SciNLP.

</details>


### [31] [Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems](https://arxiv.org/abs/2509.07817)
*Xiaolin Chen,Xuemeng Song,Haokun Wen,Weili Guan,Xiangyu Zhao,Liqiang Nie*

Main category: cs.CL

TL;DR: DK2R is a novel method that utilizes dual knowledge and LLMs to improve textual response generation in multimodal task-oriented dialog systems, showing superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to fully utilize dual knowledge (structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems, addressing the limitations of neglecting unstructured review knowledge and underutilizing LLMs.

Method: DK2R is a novel dual knowledge-enhanced two-stage reasoner that extracts both structured attribute and unstructured review knowledge from an external knowledge base. It uses an LLM to evaluate each knowledge type's utility and separately summarizes intention-oriented key clues for enhancing LLM-based textual response generation.

Result: Extensive experiments on a public dataset verify the superiority of DK2R.

Conclusion: DK2R demonstrates superiority in textual response generation for multimodal task-oriented dialog systems, and the codes and parameters have been released.

Abstract: Textual response generation is pivotal for multimodal \mbox{task-oriented}
dialog systems, which aims to generate proper textual responses based on the
multimodal context. While existing efforts have demonstrated remarkable
progress, there still exist the following limitations: 1) \textit{neglect of
unstructured review knowledge} and 2) \textit{underutilization of large
language models (LLMs)}. Inspired by this, we aim to fully utilize dual
knowledge (\textit{i.e., } structured attribute and unstructured review
knowledge) with LLMs to promote textual response generation in multimodal
task-oriented dialog systems. However, this task is non-trivial due to two key
challenges: 1) \textit{dynamic knowledge type selection} and 2)
\textit{intention-response decoupling}. To address these challenges, we propose
a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for
multimodal dialog systems (named DK2R). To be specific, DK2R first extracts
both structured attribute and unstructured review knowledge from external
knowledge base given the dialog context. Thereafter, DK2R uses an LLM to
evaluate each knowledge type's utility by analyzing LLM-generated provisional
probe responses. Moreover, DK2R separately summarizes the intention-oriented
key clues via dedicated reasoning, which are further used as auxiliary signals
to enhance LLM-based textual response generation. Extensive experiments
conducted on a public dataset verify the superiority of DK2R. We have released
the codes and parameters.

</details>


### [32] [Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost](https://arxiv.org/abs/2509.07829)
*Mihai Nadas,Laura Diosan,Andreea Tomescu,Andrei Piscoran*

Main category: cs.CL

TL;DR: This paper introduces TF2, a framework for English-Romanian literary translation that includes a compact language model and synthetic datasets. The framework aims to improve the quality of translations in low resource languages and provides an open, cost-effective solution.


<details>
  <summary>Details</summary>
Motivation: Literary translation has recently gained attention as a distinct and complex task in machine translation research. However, the translation by small open models remains an open problem. We address the need for rich, high quality literary datasets in low resource languages such as Romanian.

Method: We introduce TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for dataset creation, fine tuning, and evaluation in English-Romanian literary translations. Our pipeline first generates 15k high quality Romanian references from the TF1 pool using a high performing LLM. We then apply a two stage fine tuning process to a 12B parameter open weight model: (i) instruction tuning to capture genre specific narrative style, and (ii) adapter compression for efficient deployment.

Result: Results show that our fine tuned model achieves fluency and adequacy competitive with top performing large proprietary models, while being open, accessible, and significantly more cost effective.

Conclusion: TF2 provides an end-to-end, reproducible pipeline for research on cost efficient translation, cross lingual narrative generation, and the broad adoption of open models for culturally significant literary content in low resource settings.

Abstract: Literary translation has recently gained attention as a distinct and complex
task in machine translation research. However, the translation by small open
models remains an open problem. We contribute to this ongoing research by
introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for
dataset creation, fine tuning, and evaluation in English-Romanian literary
translations, centred on the creation and open release of both a compact, fine
tuned language model (TF2-12B) and large scale synthetic parallel datasets
(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the
largest collection of synthetic English fables to date, we address the need for
rich, high quality literary datasets in low resource languages such as
Romanian. Our pipeline first generates 15k high quality Romanian references
from the TF1 pool using a high performing LLM. We then apply a two stage fine
tuning process to a 12B parameter open weight model: (i) instruction tuning to
capture genre specific narrative style, and (ii) adapter compression for
efficient deployment. Evaluation combines corpus level BLEU and a five
dimension LLM based rubric (accuracy, fluency, coherence, style, cultural
adaptation) to provide a nuanced assessment of translation quality. Results
show that our fine tuned model achieves fluency and adequacy competitive with
top performing large proprietary models, while being open, accessible, and
significantly more cost effective. Alongside the fine tuned model and both
datasets, we publicly release all scripts and evaluation prompts. TF2 thus
provides an end-to-end, reproducible pipeline for research on cost efficient
translation, cross lingual narrative generation, and the broad adoption of open
models for culturally significant literary content in low resource settings.

</details>


### [33] [Are Humans as Brittle as Large Language Models?](https://arxiv.org/abs/2509.07869)
*Jiahui Li,Sean Papay,Roman Klinger*

Main category: cs.CL

TL;DR: 研究比较了LLM和人类标注者对提示修改的敏感性，发现两者在某些情况下表现相似，但在其他方面存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补关于提示脆弱性是否是LLM特有的知识空白，并探讨人类是否对提示扰动同样敏感。

Method: 通过让人类标注者和LLM对一组文本分类任务进行提示修改，比较了提示修改对LLM和人类标注者的影响。

Result: 研究结果表明，人类和LLM在特定类型的提示修改下都表现出更高的脆弱性，但人类的判断受某些类型的影响较小。

Conclusion: 研究发现，人类和LLM在特定类型的提示修改下都会表现出更高的脆弱性，尤其是当替代标签集或标签格式被替换时。然而，人类判断的分布受拼写错误和标签顺序反转的影响较小。

Abstract: The output of large language models (LLM) is unstable, due to both
non-determinism of the decoding process as well as to prompt brittleness. While
the intrinsic non-determinism of LLM generation may mimic existing uncertainty
in human annotations through distributional shifts in outputs, it is largely
assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.
This raises the question: do human annotators show similar sensitivity to
instruction changes? If so, should prompt brittleness in LLMs be considered
problematic? One may alternatively hypothesize that prompt brittleness
correctly reflects human annotation variances. To fill this research gap, we
systematically compare the effects of prompt modifications on LLMs and
identical instruction modifications for human annotators, focusing on the
question of whether humans are similarly sensitive to prompt perturbations. To
study this, we prompt both humans and LLMs for a set of text classification
tasks conditioned on prompt variations. Our findings indicate that both humans
and LLMs exhibit increased brittleness in response to specific types of prompt
modifications, particularly those involving the substitution of alternative
label sets or label formats. However, the distribution of human judgments is
less affected by typographical errors and reversed label order than that of
LLMs.

</details>


### [34] [From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing](https://arxiv.org/abs/2509.07889)
*Chengyan Wu,Yiqiang Cai,Yufei Cheng,Yun Xue*

Main category: cs.CL

TL;DR: 本文介绍了我们团队针对NLPCC-2025共享任务7的解决方案，该任务专注于中文句子级别的性别偏见检测和缓解。我们采用基于大型语言模型的微调方法，并通过低秩适应（LoRA）高效适应偏差检测任务。我们构建了一个更平衡的训练集并引入了异构样本以增强模型泛化能力。对于检测和分类子任务，我们采用多数投票策略提升性能，并设计了一种多温度采样机制以捕捉偏差表达风格的变化。实验结果表明我们的方法在偏差检测、分类和缓解方面有效，并在共享任务中取得了第四名的成绩。


<details>
  <summary>Details</summary>
Motivation: 该任务旨在通过自动检测、分类和缓解性别偏见来促进自然语言生成中的公平性和可控性。

Method: 我们采用基于大型语言模型（LLMs）的微调方法，通过低秩适应（LoRA）高效适应偏差检测任务。我们构建了一个更平衡的训练集以减轻类别不平衡，并引入了来自多个来源的异构样本以增强模型泛化能力。对于检测和分类子任务，我们采用集成多个专家模型输出的多数投票策略来提升性能。此外，我们设计了一种多温度采样机制以捕捉偏差表达风格的潜在变化。

Result: 实验结果表明，我们的方法在偏差检测、分类和缓解方面是有效的。我们的方法最终获得了47.90%的平均分数，排名第四。

Conclusion: 我们的方法在偏差检测、分类和缓解方面表现出有效性，并最终在共享任务中获得了47.90%的平均分数，排名第四。

Abstract: This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which
focuses on sentence-level gender bias detection and mitigation in Chinese. The
task aims to promote fairness and controllability in natural language
generation by automatically detecting, classifying, and mitigating gender bias.
To address this challenge, we adopt a fine-tuning approach based on large
language models (LLMs), efficiently adapt to the bias detection task via
Low-Rank Adaptation (LoRA). In terms of data processing, we construct a more
balanced training set to alleviate class imbalance and introduce heterogeneous
samples from multiple sources to enhance model generalization. For the
detection and classification sub-tasks, we employ a majority voting strategy
that integrates outputs from multiple expert models to boost performance.
Additionally, to improve bias generation detection and mitigation, we design a
multi-temperature sampling mechanism to capture potential variations in bias
expression styles. Experimental results demonstrate the effectiveness of our
approach in bias detection, classification, and mitigation. Our method
ultimately achieves an average score of 47.90%, ranking fourth in the shared
task.

</details>


### [35] [Biased Tales: Cultural and Topic Bias in Generating Children's Stories](https://arxiv.org/abs/2509.07908)
*Donya Rooein,Vilém Zouhar,Debora Nozza,Dirk Hovy*

Main category: cs.CL

TL;DR: 本研究通过Biased Tales数据集分析了LLM生成故事中的偏见，发现女孩角色更注重外貌，非西方儿童故事更强调文化传统。


<details>
  <summary>Details</summary>
Motivation: 随着父母越来越多地依赖大型语言模型（LLMs）来创作睡前故事，这些叙述中存在文化和社会性别刻板印象的问题引起了广泛关注。

Method: 我们提出了Biased Tales数据集，用于分析偏见如何影响LLM生成故事中的主角属性和故事元素。

Result: 当主角被描述为女孩时，与外貌相关的属性增加了55.26%。非西方儿童的故事更加强调文化传统和家庭主题。

Conclusion: 我们的研究揭示了社会文化偏见在使创意AI使用更加公平和多样化方面的作用。

Abstract: Stories play a pivotal role in human communication, shaping beliefs and
morals, particularly in children. As parents increasingly rely on large
language models (LLMs) to craft bedtime stories, the presence of cultural and
gender stereotypes in these narratives raises significant concerns. To address
this issue, we present Biased Tales, a comprehensive dataset designed to
analyze how biases influence protagonists' attributes and story elements in
LLM-generated stories. Our analysis uncovers striking disparities. When the
protagonist is described as a girl (as compared to a boy), appearance-related
attributes increase by 55.26%. Stories featuring non-Western children
disproportionately emphasize cultural heritage, tradition, and family themes
far more than those for Western children. Our findings highlight the role of
sociocultural bias in making creative AI use more equitable and diverse.

</details>


### [36] [GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2509.07925)
*Tuo Wang,Adithya Kulkarni,Tyler Cody,Peter A. Beling,Yujun Yan,Dawei Zhou*

Main category: cs.CL

TL;DR: GENUINE is a structure-aware framework that improves uncertainty estimation in LLMs by leveraging dependency parse trees and graph-based methods, showing significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing methods for uncertainty estimation in LLMs often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within generated text.

Method: GENUINE uses dependency parse trees and hierarchical graph pooling to model semantic and structural relationships, incorporating supervised learning for uncertainty quantification.

Result: GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15% across NLP tasks.

Conclusion: GENUINE demonstrates the effectiveness of graph-based uncertainty modeling by achieving higher AUROC and reducing calibration errors.

Abstract: Uncertainty estimation is essential for enhancing the reliability of Large
Language Models (LLMs), particularly in high-stakes applications. Existing
methods often overlook semantic dependencies, relying on token-level
probability measures that fail to capture structural relationships within the
generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty
Estimation for Large Language Models, a structure-aware framework that
leverages dependency parse trees and hierarchical graph pooling to refine
uncertainty quantification. By incorporating supervised learning, GENUINE
effectively models semantic and structural relationships, improving confidence
assessments. Extensive experiments across NLP tasks show that GENUINE achieves
up to 29% higher AUROC than semantic entropy-based approaches and reduces
calibration errors by over 15%, demonstrating the effectiveness of graph-based
uncertainty modeling. The code is available at
https://github.com/ODYSSEYWT/GUQ.

</details>


### [37] [SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge](https://arxiv.org/abs/2509.07968)
*Lukas Haas,Gal Yona,Giovanni D'Antonio,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: 本研究引入了一个新的基准SimpleQA Verified，用于评估大型语言模型的简短事实性，解决了现有基准的局限性，并取得了优异的性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决OpenAI基准中的噪声和错误标签、主题偏差和问题冗余等关键限制。

Method: 通过多阶段过滤过程创建了SimpleQA Verified基准，包括去重、主题平衡和来源核对，并改进了自动评分提示。

Result: 在新的基准上，Gemini 2.5 Pro实现了最先进的F1分数55.6，优于其他前沿模型，包括GPT-5。

Conclusion: 本研究为评估大型语言模型的简短事实性提供了一个更可靠和具有挑战性的基准，有助于跟踪参数模型事实性的真正进展并减少幻觉。

Abstract: We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large
Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It
addresses critical limitations in OpenAI's benchmark, including noisy and
incorrect labels, topical biases, and question redundancy. SimpleQA Verified
was created through a rigorous multi-stage filtering process involving
de-duplication, topic balancing, and source reconciliation to produce a more
reliable and challenging evaluation set, alongside improvements in the
autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a
state-of-the-art F1-score of 55.6, outperforming other frontier models,
including GPT-5. This work provides the research community with a
higher-fidelity tool to track genuine progress in parametric model factuality
and to mitigate hallucinations. The benchmark dataset, evaluation code, and
leaderboard are available at:
https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.

</details>


### [38] [Parallel-R1: Towards Parallel Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.07980)
*Tong Zheng,Hongming Zhang,Wenhao Yu,Xiaoyang Wang,Xinyu Yang,Runpeng Dai,Rui Liu,Huiwen Bao,Chengsong Huang,Heng Huang,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出了Parallel-R1，一个基于强化学习的框架，用于在复杂现实任务中实现并行思维，通过渐进式课程解决训练中的冷启动问题，并在多个数学基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要依赖于监督微调，这鼓励了模仿而非探索和泛化。因此，需要一种新的方法来激活大型语言模型的并行思维能力。

Method: 提出了一种基于强化学习的框架Parallel-R1，通过渐进式课程来解决训练并行思维时的冷启动问题。首先使用监督微调来培养并行思维能力，然后过渡到强化学习以探索和推广这种技能。

Result: 在MATH、AMC23和AIME等多个数学基准测试中，Parallel-R1比直接在困难任务上使用强化学习训练的顺序思维模型提高了8.4%的准确率。在AIME25上，比基线模型提高了42.9%。

Conclusion: Parallel-R1成功地引入了并行思维，提高了模型在复杂现实任务中的表现，并在多个数学基准测试中取得了显著的准确率提升。

Abstract: Parallel thinking has emerged as a novel approach for enhancing the reasoning
capabilities of large language models (LLMs) by exploring multiple reasoning
paths concurrently. However, activating such capabilities through training
remains challenging, as existing methods predominantly rely on supervised
fine-tuning (SFT) over synthetic data, which encourages teacher-forced
imitation rather than exploration and generalization. Different from them, we
propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework
that enables parallel thinking behaviors for complex real-world reasoning
tasks. Our framework employs a progressive curriculum that explicitly addresses
the cold-start problem in training parallel thinking with RL. We first use SFT
on prompt-generated trajectories from easier tasks to instill the parallel
thinking ability, then transition to RL to explore and generalize this skill on
harder problems. Experiments on various math benchmarks, including MATH, AMC23,
and AIME, show that Parallel-R1 successfully instills parallel thinking,
leading to 8.4% accuracy improvements over the sequential thinking model
trained directly on challenging tasks with RL. Further analysis reveals a clear
shift in the model's thinking behavior: at an early stage, it uses parallel
thinking as an exploration strategy, while in a later stage, it uses the same
capability for multi-perspective verification. Most significantly, we validate
parallel thinking as a \textbf{mid-training exploration scaffold}, where this
temporary exploratory phase unlocks a higher performance ceiling after RL,
yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and
code will be open-source at https://github.com/zhengkid/Parallel-R1.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention](https://arxiv.org/abs/2509.06982)
*Xiaomeng Hu,Fei Huang,Chenhan Yuan,Junyang Lin,Tsung-Yi Ho*

Main category: cs.LG

TL;DR: 本文提出了一种名为CARE的新框架，用于解码时间的安全对齐，它结合了三个关键组件：守护模型、回滚机制和基于内省的干预策略，从而实现了安全性和质量之间的优越平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地部署在现实世界的应用中，确保其在解码过程中的输出安全已成为一个关键挑战。然而，现有的解码时间干预措施，如对比解码，往往在安全性和响应质量之间造成严重的权衡。

Method: 我们提出了CARE，一种新的解码时间安全对齐框架，集成了三个关键组件：(1) 一个守护模型用于实时安全监控，能够检测潜在不安全内容；(2) 一个带有令牌缓冲区的回滚机制，可以在早期阶段高效纠正不安全输出而不影响用户体验；(3) 一种基于内省的干预策略，其中模型生成对其之前输出的自我反思批评，并将这些反思纳入上下文以指导后续的解码步骤。

Result: 实验结果表明，我们的框架在安全、质量和效率之间取得了优越的平衡，达到了低有害响应率，并且对用户体验的干扰最小，同时保持了高质量的响应。

Conclusion: 该框架通过其守护模型进行精确干预，回滚机制进行及时纠正，以及我们的基于内省的方法进行有效的自我纠正，实现了安全性和质量之间的优越平衡。实验结果表明，该框架在安全性、质量和效率之间取得了优越的平衡，达到了低有害响应率，并且对用户体验的干扰最小，同时保持了高质量的响应。

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, ensuring the safety of their outputs during decoding has become a
critical challenge. However, existing decoding-time interventions, such as
Contrastive Decoding, often force a severe trade-off between safety and
response quality. In this work, we propose CARE, a novel framework for
decoding-time safety alignment that integrates three key components: (1) a
guard model for real-time safety monitoring, enabling detection of potentially
unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe
outputs efficiently at an earlier stage without disrupting the user experience;
and (3) a novel introspection-based intervention strategy, where the model
generates self-reflective critiques of its previous outputs and incorporates
these reflections into the context to guide subsequent decoding steps. The
framework achieves a superior safety-quality trade-off by using its guard model
for precise interventions, its rollback mechanism for timely corrections, and
our novel introspection method for effective self-correction. Experimental
results demonstrate that our framework achieves a superior balance of safety,
quality, and efficiency, attaining a low harmful response rate and minimal
disruption to the user experience while maintaining high response quality.

</details>


### [40] [Measuring Uncertainty in Transformer Circuits with Effective Information Consistency](https://arxiv.org/abs/2509.07149)
*Anatoly A. Krasnovsky*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法EICS，用于评估Transformer电路在大型语言模型中的行为是否连贯和可信。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一种正式的、单次传递的方法来量化活跃电路是否表现出连贯性，从而可能值得信赖。

Method: 本文基于系统理论的提议，将层析/上同调和因果涌现的观点专门应用于TCs，并引入了EICS。EICS结合了从局部雅可比矩阵和激活中计算出的归一化层析不一致性和从相同前向状态中得出的电路级因果涌现的高斯有效信息代理。

Result: EICS是一种白盒、单次传递的方法，使得得分无量纲。本文还提供了关于得分解释、计算开销（快速和精确模式）以及一个玩具合理性检查分析的实用指导。

Conclusion: 本文提出了一个名为有效信息一致性得分（EICS）的新方法，用于量化Transformer电路（TCs）在大型语言模型中的行为是否连贯和可信。

Abstract: Mechanistic interpretability has identified functional subgraphs within large
language models (LLMs), known as Transformer Circuits (TCs), that appear to
implement specific algorithms. Yet we lack a formal, single-pass way to
quantify when an active circuit is behaving coherently and thus likely
trustworthy. Building on prior systems-theoretic proposals, we specialize a
sheaf/cohomology and causal emergence perspective to TCs and introduce the
Effective-Information Consistency Score (EICS). EICS combines (i) a normalized
sheaf inconsistency computed from local Jacobians and activations, with (ii) a
Gaussian EI proxy for circuit-level causal emergence derived from the same
forward state. The construction is white-box, single-pass, and makes units
explicit so that the score is dimensionless. We further provide practical
guidance on score interpretation, computational overhead (with fast and exact
modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is
deferred.

</details>


### [41] [ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers](https://arxiv.org/abs/2509.07282)
*Jeff Shen,Lindsay Smith*

Main category: cs.LG

TL;DR: ALICE, a new Transformer-based model, excels at decrypting substitution ciphers with high accuracy and speed. It generalizes well to unseen ciphers and provides insights into neural network generalization and interpretability through its architecture and analysis methods.


<details>
  <summary>Details</summary>
Motivation: To study neural network generalization in combinatorially complex domains, using cryptogram solving as an ideal testbed. The goal is to decrypt text encoded with substitution ciphers, choosing from 26! possible mappings without explicit access to the cipher.

Method: ALICE (an Architecture for Learning Interpretable Cryptogram dEcipherment): a simple encoder-only Transformer that sets a new state-of-the-art for both accuracy and speed on this decryption problem. A novel bijective decoding head that explicitly models permutations via the Gumbel-Sinkhorn method is introduced to enhance interpretability.

Result: ALICE achieves a new state-of-the-art for both accuracy and speed on the decryption problem. It generalizes to unseen ciphers after training on only ~1500 unique ciphers, a minute fraction of the possible cipher space. Early exit analysis reveals how ALICE progressively refines its predictions, mirroring common human strategies for this task.

Conclusion: ALICE's architectural innovations and analysis methods extend beyond cryptograms to any domain with bijective mappings and combinatorial structure, offering new insights into neural network generalization and interpretability.

Abstract: We present cryptogram solving as an ideal testbed for studying neural network
generalization in combinatorially complex domains. In this task, models must
decrypt text encoded with substitution ciphers, choosing from 26! possible
mappings without explicit access to the cipher. We develop ALICE (an
Architecture for Learning Interpretable Cryptogram dEcipherment): a simple
encoder-only Transformer that sets a new state-of-the-art for both accuracy and
speed on this decryption problem. Surprisingly, ALICE generalizes to unseen
ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction
($3.7 \times 10^{-24}$) of the possible cipher space. To enhance
interpretability, we introduce a novel bijective decoding head that explicitly
models permutations via the Gumbel-Sinkhorn method, enabling direct extraction
of learned cipher mappings. Through early exit analysis, we reveal how ALICE
progressively refines its predictions in a way that appears to mirror common
human strategies for this task: early layers employ frequency-based heuristics,
middle layers form word structures, and final layers correct individual
characters. Our architectural innovations and analysis methods extend beyond
cryptograms to any domain with bijective mappings and combinatorial structure,
offering new insights into neural network generalization and interpretability.

</details>


### [42] [Uncovering Scaling Laws for Large Language Models via Inverse Problems](https://arxiv.org/abs/2509.07909)
*Arun Verma,Zhaoxuan Wu,Zijian Zhou,Xiaoqiang Lin,Zhiliang Chen,Rachael Hwee Ling Sim,Rui Qiao,Jingtan Wang,Nhung Bui,Xinyuan Niu,Wenyang Hu,Gregory Kang Ruey Lau,Zi-Yu Khoo,Zitong Zhao,Xinyi Xu,Apivich Hemachandra,See-Kiong Ng,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 本文提出利用逆问题来揭示LLM的缩放定律，以提高构建LLM的成本效益。


<details>
  <summary>Details</summary>
Motivation: 由于训练大型语言模型的成本高昂，传统的试错方法不可行，因此需要更高效的策略来优化LLM。

Method: 本文借鉴了逆问题在揭示基本科学定律中的成功经验，提出利用逆问题来探索LLM的缩放定律。

Result: 逆问题可以有效地揭示指导构建LLM的缩放定律，从而提高成本效益。

Conclusion: 本文主张逆问题可以高效地揭示指导构建LLM的缩放定律，从而实现更好的成本效益。

Abstract: Large Language Models (LLMs) are large-scale pretrained models that have
achieved remarkable success across diverse domains. These successes have been
driven by unprecedented complexity and scale in both data and computations.
However, due to the high costs of training such models, brute-force
trial-and-error approaches to improve LLMs are not feasible. Inspired by the
success of inverse problems in uncovering fundamental scientific laws, this
position paper advocates that inverse problems can also efficiently uncover
scaling laws that guide the building of LLMs to achieve the desirable
performance with significantly better cost-effectiveness.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [43] [Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data](https://arxiv.org/abs/2509.07202)
*Khushiyant*

Main category: cs.HC

TL;DR: 本文介绍了一种结合Gemma 2B大型语言模型和循环神经网络编码器的新方法，显著降低了基于脑电图的文本生成所需的数据和计算资源，同时实现了接近最先进方法的性能，并提高了辅助技术的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型极大地提升了文本生成能力，但基于脑电图的文本生成仍然面临数据和计算资源需求大的挑战。因此，本文旨在探索一种更高效的方法，以改善基于脑电图的文本生成。

Method: 本文提出了一种结合Gemma 2B大型语言模型和分类器-语言模型架构的新方法，并引入了循环神经网络编码器。这种方法显著降低了数据和计算资源的需求，同时实现了接近最先进方法的性能。

Result: 与现有方法相比，本文提出的方法在整体性能上提高了10%。该架构在数据受限的情况下仍保持强大和功能完整，证明了有效迁移学习的可能性。

Conclusion: 本文展示了将大型语言模型与脑电图解码相结合的潜力，以提高辅助技术并增强严重运动障碍者的独立性和沟通能力。所提出的方法在数据和计算资源有限的情况下依然表现出色，为脑机接口的研究和应用开辟了新的路径。

Abstract: Text generating capabilities have undergone a substantial transformation with
the introduction of large language models (LLMs). Electroencephalography
(EEG)-based text production is still difficult, though, because it requires a
lot of data and processing power. This paper introduces a new method that
combines the use of the Gemma 2B LLM with a classifier-LLM architecture to
incorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically
lowers the amount of data and compute power needed while achieving performance
close to that of cutting-edge methods. Notably, compared to current
methodologies, our methodology delivers an overall performance improvement of
10%. The suggested architecture demonstrates the possibility of effective
transfer learning for EEG-based text production, remaining strong and
functional even in the face of data limits. This work highlights the potential
of integrating LLMs with EEG decoding to improve assistive technologies and
improve independence and communication for those with severe motor limitations.
Our method pushes the limits of present capabilities and opens new paths for
research and application in brain-computer interfaces by efficiently using the
strengths of pre-trained language models. This makes EEG-based text production
more accessible and efficient.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [44] [Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive Retrieval](https://arxiv.org/abs/2509.07163)
*Haike Xu,Tong Chen*

Main category: cs.IR

TL;DR: 本文提出了一种新的 Reranker-Guided-Search 方法，通过直接根据重新排序器偏好检索文档，而不是传统的顺序重新排序方法，从而克服了现有检索和重新排序管道的两个关键限制。


<details>
  <summary>Details</summary>
Motivation: 现有的检索和重新排序管道受到初始检索质量的限制，并且基于大语言模型的重新排序器的计算需求增长限制了可以有效处理的文档数量。

Method: Reranker-Guided-Search (RGS) 方法通过在近似最近邻算法生成的邻近图上进行贪心搜索，根据文档相似性优先选择有希望的文档进行重新排序，从而绕过了传统顺序重新排序方法的限制。

Result: 实验结果表明，在多个基准测试中都取得了显著的性能提升：BRIGHT 上提升了 3.5 分，FollowIR 上提升了 2.9 分，M-BEIR 上提升了 5.1 分，所有结果都在 100 个文档的受限重新排序预算内。

Conclusion: 给定一组固定的嵌入和重新排序模型，战略性地选择要重新排序的文档可以在有限的重新排序预算下显著提高检索准确性。

Abstract: The widely used retrieve-and-rerank pipeline faces two critical limitations:
they are constrained by the initial retrieval quality of the top-k documents,
and the growing computational demands of LLM-based rerankers restrict the
number of documents that can be effectively processed. We introduce
Reranker-Guided-Search (RGS), a novel approach that bypasses these limitations
by directly retrieving documents according to reranker preferences rather than
following the traditional sequential reranking method. Our method uses a greedy
search on proximity graphs generated by approximate nearest neighbor
algorithms, strategically prioritizing promising documents for reranking based
on document similarity. Experimental results demonstrate substantial
performance improvements across multiple benchmarks: 3.5 points on BRIGHT, 2.9
on FollowIR, and 5.1 on M-BEIR, all within a constrained reranker budget of 100
documents. Our analysis suggests that, given a fixed pair of embedding and
reranker models, strategically selecting documents to rerank can significantly
improve retrieval accuracy under limited reranker budget.

</details>


### [45] [Benchmarking Information Retrieval Models on Complex Retrieval Tasks](https://arxiv.org/abs/2509.07253)
*Julian Killingback,Hamed Zamani*

Main category: cs.IR

TL;DR: 本文构建了复杂的检索任务集并评估了最先进的检索模型，发现即使最佳模型在复杂任务中表现不佳，而LLM增强对较弱模型有帮助，但对最强模型却降低了性能。


<details>
  <summary>Details</summary>
Motivation: 现有资源有限，无法全面评估检索模型在复杂任务中的能力，需要构建更真实、多样的任务集来推动下一代检索模型的发展。

Method: 构建了一个多样且现实的复杂检索任务集，并评估了最先进的检索模型。还探索了基于LLM的查询扩展和重写对检索质量的影响。

Result: 最好的模型在所有任务中的平均nDCG@10仅为0.346，R@100仅为0.587。虽然LLM增强可以提升较弱模型的性能，但对最强模型的性能却有所下降。

Conclusion: 即使最先进的模型在复杂检索任务中也表现不佳，LLM增强对较弱模型有帮助，但对最强模型却降低了性能。

Abstract: Large language models (LLMs) are incredible and versatile tools for
text-based tasks that have enabled countless, previously unimaginable,
applications. Retrieval models, in contrast, have not yet seen such capable
general-purpose models emerge. To achieve this goal, retrieval models must be
able to perform complex retrieval tasks, where queries contain multiple parts,
constraints, or requirements in natural language. These tasks represent a
natural progression from the simple, single-aspect queries that are used in the
vast majority of existing, commonly used evaluation sets. Complex queries
naturally arise as people expect search systems to handle more specific and
often ambitious information requests, as is demonstrated by how people use
LLM-based information systems. Despite the growing desire for retrieval models
to expand their capabilities in complex retrieval tasks, there exist limited
resources to assess the ability of retrieval models on a comprehensive set of
diverse complex tasks. The few resources that do exist feature a limited scope
and often lack realistic settings making it hard to know the true capabilities
of retrieval models on complex real-world retrieval tasks. To address this
shortcoming and spur innovation in next-generation retrieval models, we
construct a diverse and realistic set of complex retrieval tasks and benchmark
a representative set of state-of-the-art retrieval models. Additionally, we
explore the impact of LLM-based query expansion and rewriting on retrieval
quality. Our results show that even the best models struggle to produce
high-quality retrieval results with the highest average nDCG@10 of only 0.346
and R@100 of only 0.587 across all tasks. Although LLM augmentation can help
weaker models, the strongest model has decreased performance across all metrics
with all rewriting techniques.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [46] [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.AI

TL;DR: Spectral NSR 是一种新的神经符号推理框架，结合了符号推理的可解释性和谱学习的可扩展性，表现出优越的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 将符号推理的可解释性与谱学习的可扩展性和适应性结合起来，以提高推理系统的性能和可靠性。

Method: Spectral NSR 是一种完全谱神经符号推理框架，将逻辑规则嵌入为谱模板，并在图谱域中直接进行推理。

Result: 在 ProofWriter 和 CLUTRR 等最先进的推理基准测试中，Spectral NSR 在准确性、推理速度、对抗扰动的鲁棒性和可解释性方面均优于领先的基线模型。

Conclusion: Spectral NSR 是一种可扩展且有原则的基础，为下一代推理系统提供了透明度、鲁棒性和泛化能力，超越了传统方法。

Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning
framework that embeds logical rules as spectral templates and performs
inference directly in the graph spectral domain. By leveraging graph signal
processing (GSP) and frequency-selective filters grounded in the Laplacian
eigenstructure of knowledge graphs, the architecture unifies the
interpretability of symbolic reasoning with the scalability and adaptability of
spectral learning. Beyond the core formulation, we incorporate a comprehensive
set of extensions, including dynamic graph and basis learning, rational and
diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts
for modular specialization, proof-guided training with spectral curricula, and
uncertainty quantification for calibrated confidence. Additional enhancements
such as large language model coupling, co-spectral transfer alignment,
adversarial robustness, efficient GPU kernels, generalized Laplacians, and
causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as
ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior
accuracy, faster inference, improved robustness to adversarial perturbations,
and higher interpretability compared to leading baselines including
transformers, message-passing neural networks, and neuro-symbolic logic
programming systems. Spectral attribution and proof-band agreement analyses
confirm that model decisions align closely with symbolic proof structures,
while transfer experiments validate effective domain adaptation through
co-spectral alignment. These results establish Spectral NSR as a scalable and
principled foundation for the next generation of reasoning systems, offering
transparency, robustness, and generalization beyond conventional approaches.

</details>


### [47] [Instruction Agent: Enhancing Agent with Expert Demonstration](https://arxiv.org/abs/2509.07098)
*Yinheng Li,Hailey Hultquist,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: Instruction Agent 是一种利用专家演示解决复杂 GUI 任务的代理，能够提高任务执行的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: GUI 代理在处理涉及新 UI 元素、长周期动作和个性化轨迹的复杂任务时仍然存在困难，因此需要一种更可靠的方法来实现实际的 GUI 任务自动化。

Method: Instruction Agent 利用专家演示来解决复杂任务，通过提取逐步指令并严格遵循用户意图的轨迹来执行任务，并利用验证器和回溯模块提高鲁棒性。

Result: Instruction Agent 在 OSWorld 的一组任务中实现了 60% 的成功率，而所有顶级排名的代理都无法完成这些任务。

Conclusion: Instruction Agent 提供了一个实用且可扩展的框架，弥合了当前 GUI 代理与可靠的实际 GUI 任务自动化之间的差距。

Abstract: Graphical user interface (GUI) agents have advanced rapidly but still
struggle with complex tasks involving novel UI elements, long-horizon actions,
and personalized trajectories. In this work, we introduce Instruction Agent, a
GUI agent that leverages expert demonstrations to solve such tasks, enabling
completion of otherwise difficult workflows. Given a single demonstration, the
agent extracts step-by-step instructions and executes them by strictly
following the trajectory intended by the user, which avoids making mistakes
during execution. The agent leverages the verifier and backtracker modules
further to improve robustness. Both modules are critical to understand the
current outcome from each action and handle unexpected interruptions(such as
pop-up windows) during execution. Our experiments show that Instruction Agent
achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked
agents failed to complete. The Instruction Agent offers a practical and
extensible framework, bridging the gap between current GUI agents and reliable
real-world GUI task automation.

</details>


### [48] [Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis](https://arxiv.org/abs/2509.07122)
*Sania Sinha,Tanawan Premsri,Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 本文分析了神经符号框架的技术方面，并展示了三个通用框架，旨在推动该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 神经符号框架结合了神经表示和学习与符号表示和推理，能够以更可靠和数据高效的方式解决复杂问题。然而，该领域仍面临学习曲线陡峭、缺乏用户友好的工具和统一框架等挑战。

Method: 本文对现有的神经符号框架进行了技术方面的分析，包括符号表示语言、与神经模型的集成以及底层算法。同时，展示了三个通用的神经符号框架：DeepProbLog、Scallop 和 DomiKnowS。

Result: 本文分析了现有神经符号框架的技术方面，并指出了每个方面中的挑战，为理解各框架在解决各种问题中的表达能力奠定了基础。

Conclusion: 本文旨在激发社区重新思考神经符号建模问题，并推动该领域的发展。

Abstract: Neurosymbolic (NeSy) frameworks combine neural representations and learning
with symbolic representations and reasoning. Combining the reasoning
capacities, explainability, and interpretability of symbolic processing with
the flexibility and power of neural computing allows us to solve complex
problems with more reliability while being data-efficient. However, this
recently growing topic poses a challenge to developers with its learning curve,
lack of user-friendly tools, libraries, and unifying frameworks. In this paper,
we characterize the technical facets of existing NeSy frameworks, such as the
symbolic representation language, integration with neural models, and the
underlying algorithms. A majority of the NeSy research focuses on algorithms
instead of providing generic frameworks for declarative problem specification
to leverage problem solving. To highlight the key aspects of Neurosymbolic
modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog},
\textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within
each facet that lay the foundation for identifying the expressivity of each
framework in solving a variety of problems. Building on this foundation, we aim
to spark transformative action and encourage the community to rethink this
problem in novel ways.

</details>


### [49] [That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral](https://arxiv.org/abs/2509.07170)
*Quinten Steenhuis*

Main category: cs.AI

TL;DR: 本文提出了一种高效的法律问题分类方法，通过混合LLM/ML集成分类和自动生成后续问题来提高准确性，取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 为了减少因错误引导而导致的严重后果，如错过截止日期、遭受身体虐待、失去住房或子女监护权，需要准确地将申请人与正确的法律帮助匹配。

Method: 我们引入并评估了FETCH分类器用于法律问题分类，并描述了两种提高准确性的方法：一种是混合LLM/ML集成分类方法，另一种是自动生成后续问题以丰富初始问题叙述。

Result: 我们使用419个非营利律师推荐服务的真实查询数据集，展示了使用混合廉价模型的分类准确率（hits@2）为97.37%，超过了当前最先进的GPT-5模型的表现。

Conclusion: 我们的方法在显著降低引导用户到正确资源的成本的同时，实现了高准确性，展示了在法律问题分类中的潜力。

Abstract: Each year millions of people seek help for their legal problems by calling a
legal aid program hotline, walking into a legal aid office, or using a lawyer
referral service. The first step to match them to the right help is to identify
the legal problem the applicant is experiencing. Misdirection has consequences.
Applicants may miss a deadline, experience physical abuse, lose housing or lose
custody of children while waiting to connect to the right legal help. We
introduce and evaluate the FETCH classifier for legal issue classification and
describe two methods for improving accuracy: a hybrid LLM/ML ensemble
classification method, and the automatic generation of follow-up questions to
enrich the initial problem narrative. We employ a novel data set of 419
real-world queries to a nonprofit lawyer referral service. Ultimately, we show
classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models,
exceeding the performance of the current state-of-the-art GPT-5 model. Our
approach shows promise in significantly reducing the cost of guiding users of
the legal system to the right resource for their problem while achieving high
accuracy.

</details>


### [50] [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)
*Jakub Grudzien Kuba,Mengting Gu,Qi Ma,Yuandong Tian,Vijai Mohan*

Main category: cs.AI

TL;DR: 本文提出一种基于自我对弈的强化学习方法，使大型语言模型无需额外数据即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展面临一个根本性瓶颈：需要越来越多的数据来继续学习。

Method: 提出了一种基于博弈论框架的强化学习方法，称为语言自我对弈（LSP），通过让模型与自身对弈来提升其能力。

Result: 在Llama-3.2-3B-Instruct指令遵循基准测试中，预训练模型可以通过自我对弈显著提升在挑战性任务上的表现。

Conclusion: 通过自我对弈，预训练模型可以在不依赖额外数据的情况下提升性能，并且效果优于基于数据的基线方法。

Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by
scale, abundant high-quality training data, and reinforcement learning. Yet
this progress faces a fundamental bottleneck: the need for ever more data from
which models can continue to learn. In this work, we propose a reinforcement
learning approach that removes this dependency by enabling models to improve
without additional data. Our method leverages a game-theoretic framework of
self-play, where a model's capabilities are cast as performance in a
competitive game and stronger policies emerge by having the model play against
itself - a process we call Language Self-Play (LSP). Experiments with
Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained
models can not only enhance their performance on challenging tasks through
self-play alone, but can also do so more effectively than data-driven
baselines.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [51] [ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code](https://arxiv.org/abs/2509.07006)
*Kapil Madan*

Main category: cs.CY

TL;DR: ArGen is a framework that aligns LLMs with complex rules using automated reward scoring, GRPO, and an OPA-inspired governance layer, demonstrating adaptability and improved adherence in a medical AI application.


<details>
  <summary>Details</summary>
Motivation: The paper aims to move beyond preference-based alignment and ensure LLMs adhere to multifaceted policies through a novel synthesis of techniques.

Method: ArGen is a framework that aligns Large Language Models (LLMs) with complex sets of configurable, machine-readable rules using principle-based automated reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy Agent (OPA) inspired governance layer.

Result: ArGen demonstrates adaptability, achieving a 70.9% improvement in domain-scope adherence over the baseline when applied to a medical AI assistant guided by Dharmic ethics.

Conclusion: ArGen's methodology offers a path to 'Governable AI' systems that are technically proficient, ethically robust, and verifiably compliant for safe deployment in diverse global contexts.

Abstract: This paper introduces ArGen (Auto-Regulation of Generative AI systems), a
framework for aligning Large Language Models (LLMs) with complex sets of
configurable, machine-readable rules spanning ethical principles, operational
safety protocols, and regulatory compliance standards. Moving beyond just
preference-based alignment, ArGen is designed to ensure LLMs adhere to these
multifaceted policies through a novel synthesis of principle-based automated
reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy
Agent (OPA) inspired governance layer. This approach provides the technical
foundation for achieving and demonstrating compliance with diverse and nuanced
governance requirements. To showcase the framework's capability to
operationalize a deeply nuanced and culturally-specific value system, we
present an in-depth case study: the development of a medical AI assistant
guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as
derived from texts like the Bhagavad Gita. This challenging application
demonstrates ArGen's adaptability, achieving a 70.9% improvement in
domain-scope adherence over the baseline. Through our open-source repository,
we show that ArGen's methodology offers a path to 'Governable Al' systems that
are technically proficient, ethically robust, and verifiably compliant for safe
deployment in diverse global contexts.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [52] [Astra: A Multi-Agent System for GPU Kernel Performance Optimization](https://arxiv.org/abs/2509.07506)
*Anjiang Wei,Tianran Sun,Yogesh Seenichamy,Hang Song,Anne Ouyang,Azalia Mirhoseini,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: 本文介绍了Astra，一个基于LLM的多代理系统，用于GPU内核优化。Astra通过迭代的代码生成、测试、分析和规划协作，从现有的CUDA实现中生成高性能内核。实验结果表明，Astra在速度上有显著提升，并展示了LLM在优化内存访问模式、利用CUDA内在函数等方面的能力。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化在高性能计算和机器学习的交叉点上一直是一个核心挑战。高效的内核对于加速大型语言模型（LLM）的训练和服务至关重要，但实现高性能通常需要大量的手动调整。编译器系统减少了部分负担，但仍需要大量的人工设计和工程努力。最近，研究人员探索使用LLM生成GPU内核，但之前的工作主要集中在将高级PyTorch模块转换为CUDA代码。

Method: 我们引入了Astra，这是一个基于LLM的多代理系统，用于GPU内核优化。专门的LLM代理通过迭代的代码生成、测试、分析和规划进行协作，以产生正确且高性能的内核。

Result: 在来自SGLang的内核上，Astra使用零样本提示与OpenAI o4-mini实现了平均1.32倍的速度提升。详细案例研究进一步表明，LLM可以自主应用循环变换、优化内存访问模式、利用CUDA内在函数并利用快速数学运算来获得显著的性能提升。

Conclusion: 我们的工作展示了多代理LLM系统作为GPU内核优化的有前途的新范式的潜力。

Abstract: GPU kernel optimization has long been a central challenge at the intersection
of high-performance computing and machine learning. Efficient kernels are
crucial for accelerating large language model (LLM) training and serving, yet
attaining high performance typically requires extensive manual tuning.
Compiler-based systems reduce some of this burden, but still demand substantial
manual design and engineering effort. Recently, researchers have explored using
LLMs for GPU kernel generation, though prior work has largely focused on
translating high-level PyTorch modules into CUDA code. In this work, we
introduce Astra, the first LLM-based multi-agent system for GPU kernel
optimization. Unlike previous approaches, Astra starts from existing CUDA
implementations extracted from SGLang, a widely deployed framework for serving
LLMs, rather than treating PyTorch modules as the specification. Within Astra,
specialized LLM agents collaborate through iterative code generation, testing,
profiling, and planning to produce kernels that are both correct and
high-performance. On kernels from SGLang, Astra achieves an average speedup of
1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study
further demonstrates that LLMs can autonomously apply loop transformations,
optimize memory access patterns, exploit CUDA intrinsics, and leverage fast
math operations to yield substantial performance gains. Our work highlights
multi-agent LLM systems as a promising new paradigm for GPU kernel
optimization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality](https://arxiv.org/abs/2509.06994)
*Srihari Bandraupalli,Anupam Purwar*

Main category: cs.CV

TL;DR: This paper introduces ViLD, a framework for evaluating vision-language models (VLMs) in enterprise settings, addressing the disconnect between academic benchmarks and real-world applications.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for VLMs rely on multiple-choice questions and synthetic data, which do not reflect real-world enterprise applications like social media content analysis. The paper aims to bridge this gap by evaluating VLMs on business-critical tasks.

Method: The paper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to evaluate VLMs on operational enterprise requirements. It includes a BlockWeaver Algorithm for comparing OCR outputs and a new benchmark dataset of 7,500 diverse samples.

Result: ViLD was used to benchmark leading open-source VLMs (Qwen, MIMO, and InternVL) against a proprietary baseline, providing actionable insights into their performance in enterprise environments.

Conclusion: ViLD provides one of the first industry-grounded, task-driven assessments of VLMs' capabilities, offering actionable insights for their deployment in enterprise environments.

Abstract: Open-source Vision-Language Models show immense promise for enterprise
applications, yet a critical disconnect exists between academic evaluation and
enterprise deployment requirements. Current benchmarks rely heavily on
multiple-choice questions and synthetic data, failing to capture the complexity
of real-world business applications like social media content analysis. This
paper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge
this gap by evaluating VLMs on operational enterprise requirements. We define
ten business-critical tasks: logo detection, OCR, object detection, human
presence and demographic analysis, human activity and appearance analysis,
scene detection, camera perspective and media quality assessment, dominant
colors, comprehensive description, and NSFW detection. To this framework, we
bring an innovative BlockWeaver Algorithm that solves the challenging problem
of comparing unordered, variably-grouped OCR outputs from VLMs without relying
on embeddings or LLMs, achieving remarkable speed and reliability. To
demonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500
diverse samples, carefully stratified from a corpus of one million real-world
images and videos. ViLD provides actionable insights by combining semantic
matching (both embedding-based and LLM-as-a-judge approaches), traditional
metrics, and novel methods to measure the completeness and faithfulness of
descriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and
InternVL) against a powerful proprietary baseline as per ViLD framework, we
provide one of the first industry-grounded, task-driven assessment of VLMs
capabilities, offering actionable insights for their deployment in enterprise
environments.

</details>


### [54] [GLEAM: Learning to Match and Explain in Cross-View Geo-Localization](https://arxiv.org/abs/2509.07450)
*Xudong Lu,Zhi Zheng,Yi Wan,Yongxiang Yao,Annan Wang,Renrui Zhang,Panwang Xia,Qiong Wu,Qingyun Li,Weifeng Lin,Xiangyu Zhao,Xue Yang,Hongsheng Li*

Main category: cs.CV

TL;DR: This paper introduces GLEAM-C and GLEAM-X, which enhance cross-view geo-localization by integrating multi-modal, multi-view alignment with interpretable reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing CVGL approaches are restricted to a single view or modality and lack interpretability in their direct visual matching strategy.

Method: GLEAM-C unifies multiple views and modalities by aligning them exclusively with satellite imagery, while GLEAM-X combines cross-view correspondence prediction with explainable reasoning using multimodal large language models (MLLMs).

Result: GLEAM-C achieves accuracy comparable to prior modality-specific CVGL models through a two-phase training strategy, while GLEAM-X enables systematic evaluation of explainable cross-view reasoning.

Conclusion: GLEAM-C and GLEAM-X form a comprehensive CVGL pipeline that integrates multi-modal, multi-view alignment with interpretable correspondence analysis, unifying accurate cross-view matching with explainable reasoning and advancing Geo-Localization by enabling models to better Explain And Match.

Abstract: Cross-View Geo-Localization (CVGL) focuses on identifying correspondences
between images captured from distinct perspectives of the same geographical
location. However, existing CVGL approaches are typically restricted to a
single view or modality, and their direct visual matching strategy lacks
interpretability: they merely predict whether two images correspond, without
explaining the rationale behind the match. In this paper, we present GLEAM-C, a
foundational CVGL model that unifies multiple views and modalities-including
UAV imagery, street maps, panoramic views, and ground photographs-by aligning
them exclusively with satellite imagery. Our framework enhances training
efficiency through optimized implementation while achieving accuracy comparable
to prior modality-specific CVGL models through a two-phase training strategy.
Moreover, to address the lack of interpretability in traditional CVGL methods,
we leverage the reasoning capabilities of multimodal large language models
(MLLMs) to propose a new task, GLEAM-X, which combines cross-view
correspondence prediction with explainable reasoning. To support this task, we
construct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro
to generate training and testing data. The test set is further refined through
detailed human revision, enabling systematic evaluation of explainable
cross-view reasoning and advancing transparency and scalability in
geo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL
pipeline that integrates multi-modal, multi-view alignment with interpretable
correspondence analysis, unifying accurate cross-view matching with explainable
reasoning and advancing Geo-Localization by enabling models to better Explain
And Match. Code and datasets used in this work will be made publicly accessible
at https://github.com/Lucky-Lance/GLEAM.

</details>


### [55] [Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images](https://arxiv.org/abs/2509.07966)
*Boammani Aser Lompo,Marc Haraoui*

Main category: cs.CV

TL;DR: 本文介绍了一个大规模、开放领域的多模态数据集Visual-TableQA，用于评估和增强对复杂表格数据的视觉推理能力。该数据集通过多模型协作生成，具有高多样性和创造性，实验结果表明其效果优于一些专有模型。


<details>
  <summary>Details</summary>
Motivation: 当前基准在规模、多样性或推理深度方面存在限制，尤其是在渲染的表格图像方面。本文旨在解决这一差距，引入一个专门用于评估和增强视觉推理能力的数据集。

Method: 本文提出了一种模块化、可扩展且完全自主的生成管道，涉及多个推理LLM协作承担不同的角色：生成、验证和灵感。通过跨模型提示（'灵感'）和LLM评审过滤来促进多样性和创造力。

Result: Visual-TableQA包含2.5k个丰富结构化的LaTeX渲染表格和6k个推理密集型问答对，成本低于100美元。微调后的模型在外部基准测试中表现出良好的泛化能力，优于一些专有模型。

Conclusion: 本文介绍了Visual-TableQA，这是一个大规模、开放领域的多模态数据集，专门用于评估和增强对复杂表格数据的视觉推理能力。实验结果表明，微调后的模型在外部基准测试中表现出色，优于一些专有模型。

Abstract: Visual reasoning over structured data such as tables is a critical capability
for modern vision-language models (VLMs), yet current benchmarks remain limited
in scale, diversity, or reasoning depth, especially when it comes to rendered
table images. Addressing this gap, we introduce Visual-TableQA, a large-scale,
open-domain multimodal dataset specifically designed to evaluate and enhance
visual reasoning over complex tabular data. Our generation pipeline is modular,
scalable, and fully autonomous, involving multiple reasoning LLMs collaborating
across distinct roles: generation, validation, and inspiration. Visual-TableQA
comprises 2.5k richly structured LaTeX-rendered tables and 6k
reasoning-intensive QA pairs, all produced at a cost of under USD 100. To
promote diversity and creativity, our pipeline performs multi-model
collaborative data generation via cross-model prompting ('inspiration') and
LLM-jury filtering. Stronger models seed layouts and topics that weaker models
elaborate, collectively distilling diverse reasoning patterns and visual
structures into the dataset. Empirical results show that models fine-tuned on
Visual-TableQA generalize robustly to external benchmarks, outperforming
several proprietary models despite the dataset's synthetic nature. The full
pipeline and resources are publicly available at
https://github.com/AI-4-Everyone/Visual-TableQA.

</details>


### [56] [Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search](https://arxiv.org/abs/2509.07969)
*Xin Lai,Junyi Li,Wei Li,Tao Liu,Tianjian Li,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文介绍了Mini-o3系统，通过扩展基于工具的交互来解决视觉搜索任务中的推理模式单调和交互次数有限的问题，取得了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源方法在推理模式上过于单调且交互次数有限，无法满足需要试错探索的困难任务的需求。

Method: 构建了Visual Probe Dataset数据集，开发了迭代数据收集流程以获得冷启动轨迹，并提出了过回合掩码策略以防止在强化学习中对超过回合数的响应进行惩罚。

Result: Mini-o3在具有挑战性的视觉搜索任务中实现了最先进的性能，模型生成的轨迹在推理时可以自然扩展到数十个步骤，准确性随着步骤数的增加而提高。

Conclusion: Mini-o3能够生成丰富的推理模式和深入的思考路径，有效解决具有挑战性的视觉搜索问题。

Abstract: Recent advances in large multimodal models have leveraged image-based tools
with reinforcement learning to tackle visual problems. However, existing
open-source approaches often exhibit monotonous reasoning patterns and allow
only a limited number of interaction turns, making them inadequate for
difficult tasks that require trial-and-error exploration. In this work, we
address this limitation by scaling up tool-based interactions and introduce
Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of
steps -- and achieves state-of-the-art performance on challenging visual search
tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key
components. First, we construct the Visual Probe Dataset, a collection of
thousands of challenging visual search problems designed for exploratory
reasoning. Second, we develop an iterative data collection pipeline to obtain
cold-start trajectories that exhibit diverse reasoning patterns, including
depth-first search, trial-and-error, and goal maintenance. Third, we propose an
over-turn masking strategy that prevents penalization of over-turn responses
(those that hit the maximum number of turns) during reinforcement learning,
thereby balancing training-time efficiency with test-time scalability. Despite
training with an upper bound of only six interaction turns, our model generates
trajectories that naturally scale to tens of turns at inference time, with
accuracy improving as the number of turns increases. Extensive experiments
demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking
paths, effectively solving challenging visual search problems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [57] [Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data](https://arxiv.org/abs/2509.07526)
*Gokul Karthik Kumar,Rishabh Saraf,Ludovick Lepauloux,Abdul Muneer,Billel Mokeddem,Hakim Hacid*

Main category: cs.SD

TL;DR: Falcon3-Audio是一个基于指令调优的大语言模型和Whisper编码器构建的音频-语言模型家族，使用少量公共音频数据实现了优异性能。


<details>
  <summary>Details</summary>
Motivation: 尽管音频在人类交流中至关重要，但大型语言模型（LLMs）与音频的整合仍研究不足。

Method: Falcon3-Audio是基于指令调优的大语言模型和Whisper编码器构建的音频-语言模型家族。

Result: Falcon3-Audio-7B在MMAU基准测试中取得了64.14的高分，与R1-AQA相当，并且在数据和参数效率、单阶段训练和透明性方面表现出色。

Conclusion: Falcon3-Audio-7B在MMAU基准测试中表现出色，同时展示了数据和参数效率、单阶段训练以及透明性方面的优势。最小的1B模型也与更大的开放模型竞争。

Abstract: Large language models (LLMs) have transformed NLP, yet their integration with
audio remains underexplored -- despite audio's centrality to human
communication. We introduce Falcon3-Audio, a family of Audio-Language Models
(ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably
small amount of public audio data -- less than 30K hours (5K unique) --
Falcon3-Audio-7B matches the best reported performance among open-weight models
on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while
distinguishing itself through superior data and parameter efficiency,
single-stage training, and transparency. Notably, our smallest 1B model remains
competitive with larger open models ranging from 2B to 13B parameters. Through
extensive ablations, we find that common complexities -- such as curriculum
learning, multiple audio encoders, and intricate cross-attention connectors --
are not required for strong performance, even compared to models trained on
over 500K hours of data.

</details>

{"id": "2507.16922", "pdf": "https://arxiv.org/pdf/2507.16922", "abs": "https://arxiv.org/abs/2507.16922", "authors": ["Shmuel Amar", "Ori Shapira", "Aviv Slobodkin", "Ido Dagan"], "title": "A Unifying Scheme for Extractive Content Selection Tasks", "categories": ["cs.CL"], "comment": null, "summary": "A broad range of NLP tasks involve selecting relevant text spans from given\nsource texts. Despite this shared objective, such \\textit{content selection}\ntasks have traditionally been studied in isolation, each with its own modeling\napproaches, datasets, and evaluation metrics. In this work, we propose\n\\textit{instruction-guided content selection (IGCS)} as a beneficial unified\nframework for such settings, where the task definition and any\ninstance-specific request are encapsulated as instructions to a language model.\nTo promote this framework, we introduce \\igcsbench{}, the first unified\nbenchmark covering diverse content selection tasks. Further, we create a large\ngeneric synthetic dataset that can be leveraged for diverse content selection\ntasks, and show that transfer learning with these datasets often boosts\nperformance, whether dedicated training for the targeted task is available or\nnot. Finally, we address generic inference time issues that arise in LLM-based\nmodeling of content selection, assess a generic evaluation metric, and overall\npropose the utility of our resources and methods for future content selection\nmodels. Models and datasets available at https://github.com/shmuelamar/igcs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5185\u5bb9\u9009\u62e9\u6846\u67b6IGCS\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e00\u4e2a\u5927\u578b\u5408\u6210\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u5185\u5bb9\u9009\u62e9\u4efb\u52a1\u901a\u5e38\u88ab\u5b64\u7acb\u7814\u7a76\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\u81ea\u5df1\u7684\u5efa\u6a21\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdb\u4e0d\u540c\u5185\u5bb9\u9009\u62e9\u4efb\u52a1\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5373\u57fa\u4e8e\u6307\u4ee4\u7684\u5185\u5bb9\u9009\u62e9\uff08IGCS\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u8986\u76d6\u591a\u79cd\u5185\u5bb9\u9009\u62e9\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u6b64\u5916\uff0c\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u901a\u7528\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\u3002\u6700\u540e\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8eLLM\u7684\u5185\u5bb9\u9009\u62e9\u5efa\u6a21\u4e2d\u7684\u63a8\u7406\u65f6\u95f4\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684IGCS\u6846\u67b6\u548c\u76f8\u5173\u8d44\u6e90\u80fd\u591f\u63d0\u5347\u4e0d\u540c\u5185\u5bb9\u9009\u62e9\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u5185\u5bb9\u9009\u62e9\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u548c\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5373\u57fa\u4e8e\u6307\u4ee4\u7684\u5185\u5bb9\u9009\u62e9\uff08IGCS\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u8986\u76d6\u591a\u79cd\u5185\u5bb9\u9009\u62e9\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u6b64\u5916\uff0c\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u901a\u7528\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\u3002\u6700\u540e\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8eLLM\u7684\u5185\u5bb9\u9009\u62e9\u5efa\u6a21\u4e2d\u7684\u63a8\u7406\u65f6\u95f4\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2507.16947", "pdf": "https://arxiv.org/pdf/2507.16947", "abs": "https://arxiv.org/abs/2507.16947", "authors": ["Robert Korom", "Sarah Kiptinness", "Najib Adan", "Kassim Said", "Catherine Ithuli", "Oliver Rotich", "Boniface Kimani", "Irene King'ori", "Stellah Kamau", "Elizabeth Atemba", "Muna Aden", "Preston Bowman", "Michael Sharman", "Rebecca Soskin Hicks", "Rebecca Distler", "Johannes Heidecke", "Rahul K. Arora", "Karan Singhal"], "title": "AI-based Clinical Decision Support for Primary Care: A Real-World Study", "categories": ["cs.CL"], "comment": "Blog: https://openai.com/index/ai-clinical-copilot-penda-health/", "summary": "We evaluate the impact of large language model-based clinical decision\nsupport in live care. In partnership with Penda Health, a network of primary\ncare clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a\nsafety net for clinicians by identifying potential documentation and clinical\ndecision-making errors. AI Consult integrates into clinician workflows,\nactivating only when needed and preserving clinician autonomy. We conducted a\nquality improvement study, comparing outcomes for 39,849 patient visits\nperformed by clinicians with or without access to AI Consult across 15 clinics.\nVisits were rated by independent physicians to identify clinical errors.\nClinicians with access to AI Consult made relatively fewer errors: 16% fewer\ndiagnostic errors and 13% fewer treatment errors. In absolute terms, the\nintroduction of AI Consult would avert diagnostic errors in 22,000 visits and\ntreatment errors in 29,000 visits annually at Penda alone. In a survey of\nclinicians with AI Consult, all clinicians said that AI Consult improved the\nquality of care they delivered, with 75% saying the effect was \"substantial\".\nThese results required a clinical workflow-aligned AI Consult implementation\nand active deployment to encourage clinician uptake. We hope this study\ndemonstrates the potential for LLM-based clinical decision support tools to\nreduce errors in real-world settings and provides a practical framework for\nadvancing responsible adoption.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177AI Consult\u5728\u5b9e\u9645\u62a4\u7406\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u80fd\u6709\u6548\u51cf\u5c11\u4e34\u5e8a\u9519\u8bef\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\u4ee5\u63a8\u52a8\u8d1f\u8d23\u4efb\u7684\u91c7\u7528\u3002", "motivation": "\u8bc4\u4f30\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177\u5728\u5b9e\u9645\u62a4\u7406\u4e2d\u7684\u5f71\u54cd\uff0c\u4ee5\u51cf\u5c11\u4e34\u5e8a\u9519\u8bef\u3002", "method": "\u4e0ePenda Health\u5408\u4f5c\uff0c\u8fdb\u884c\u4e86\u4e00\u9879\u8d28\u91cf\u6539\u8fdb\u7814\u7a76\uff0c\u6bd4\u8f83\u4e8615\u5bb6\u8bca\u6240\u4e2d\u4f7f\u7528AI Consult\u548c\u672a\u4f7f\u7528AI Consult\u7684\u4e34\u5e8a\u533b\u751f\u7684\u60a3\u8005\u8bbf\u95ee\u7ed3\u679c\u3002\u72ec\u7acb\u533b\u751f\u5bf9\u8bbf\u95ee\u8fdb\u884c\u4e86\u8bc4\u5206\u4ee5\u8bc6\u522b\u4e34\u5e8a\u9519\u8bef\u3002", "result": "\u4f7f\u7528AI Consult\u7684\u4e34\u5e8a\u533b\u751f\u72af\u7684\u9519\u8bef\u8f83\u5c11\uff1a\u8bca\u65ad\u9519\u8bef\u51cf\u5c11\u4e8616%\uff0c\u6cbb\u7597\u9519\u8bef\u51cf\u5c11\u4e8613%\u3002AI Consult\u7684\u5f15\u5165\u6bcf\u5e74\u53ef\u4ee5\u5728Penda\u907f\u514d22,000\u6b21\u8bca\u65ad\u9519\u8bef\u548c29,000\u6b21\u6cbb\u7597\u9519\u8bef\u300275%\u7684\u4e34\u5e8a\u533b\u751f\u8ba4\u4e3aAI Consult\u5bf9\u62a4\u7406\u8d28\u91cf\u7684\u5f71\u54cd\u662f\u201c\u663e\u8457\u201d\u7684\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177\u5728\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u51cf\u5c11\u9519\u8bef\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u63a8\u52a8\u8d1f\u8d23\u4efb\u91c7\u7528\u7684\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2507.16951", "pdf": "https://arxiv.org/pdf/2507.16951", "abs": "https://arxiv.org/abs/2507.16951", "authors": ["Shuyuan Lin", "Lei Duan", "Philip Hughes", "Yuxuan Sheng"], "title": "Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Conversational Information Retrieval (CIR) systems, while offering intuitive\naccess to information, face a significant challenge: reliably handling\nunanswerable questions to prevent the generation of misleading or hallucinated\ncontent. Traditional approaches often rely on external classifiers, which can\nintroduce inconsistencies with the core generative Large Language Models\n(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a\nnovel approach that deeply integrates unanswerability detection directly within\nthe LLM's generative process. SALU is trained using a multi-task learning\nframework for both standard Question Answering (QA) and explicit abstention\ngeneration for unanswerable queries. Crucially, it incorporates a\nconfidence-score-guided reinforcement learning with human feedback (RLHF)\nphase, which explicitly penalizes hallucinated responses and rewards\nappropriate abstentions, fostering intrinsic self-awareness of knowledge\nboundaries. Through extensive experiments on our custom-built\nC-IR_Answerability dataset, SALU consistently outperforms strong baselines,\nincluding hybrid LLM-classifier systems, in overall accuracy for correctly\nanswering or abstaining from questions. Human evaluation further confirms\nSALU's superior reliability, achieving high scores in factuality, appropriate\nabstention, and, most importantly, a dramatic reduction in hallucination,\ndemonstrating its ability to robustly \"know when to say 'I don't know'.\"", "AI": {"tldr": "This paper introduces SALU, an approach that integrates unanswerability detection within LLMs, improving their ability to handle unanswerable questions and reduce hallucinations.", "motivation": "Traditional approaches to handling unanswerable questions in CIR systems often rely on external classifiers, which can introduce inconsistencies with the core generative LLMs. This paper aims to address this issue by integrating unanswerability detection directly within the LLM's generative process.", "method": "SALU uses a multi-task learning framework for standard QA and explicit abstention generation, along with confidence-score-guided reinforcement learning with human feedback (RLHF) to penalize hallucinated responses and reward appropriate abstentions.", "result": "SALU outperforms strong baselines, including hybrid LLM-classifier systems, in overall accuracy for correctly answering or abstaining from questions. Human evaluation confirms its superior reliability, achieving high scores in factuality, appropriate abstention, and a dramatic reduction in hallucination.", "conclusion": "SALU demonstrates superior reliability in handling unanswerable questions, significantly reducing hallucinations and showing the ability to robustly 'know when to say I don't know.'"}}
{"id": "2507.16971", "pdf": "https://arxiv.org/pdf/2507.16971", "abs": "https://arxiv.org/abs/2507.16971", "authors": ["Aleksandr Perevalov", "Andreas Both"], "title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "During the final evaluation on the DBpedia- and Corporate-based KGQA\n  benchmarks within the Text2SPARQL challenge 2025, our approach took first\n  place among the other participants", "summary": "Accessing knowledge via multilingual natural-language interfaces is one of\nthe emerging challenges in the field of information retrieval and related ones.\nStructured knowledge stored in knowledge graphs can be queried via a specific\nquery language (e.g., SPARQL). Therefore, one needs to transform\nnatural-language input into a query to fulfill an information need. Prior\napproaches mostly focused on combining components (e.g., rule-based or\nneural-based) that solve downstream tasks and come up with an answer at the\nend. We introduce mKGQAgent, a human-inspired framework that breaks down the\ntask of converting natural language questions into SPARQL queries into modular,\ninterpretable subtasks. By leveraging a coordinated LLM agent workflow for\nplanning, entity linking, and query refinement - guided by an experience pool\nfor in-context learning - mKGQAgent efficiently handles multilingual KGQA.\nEvaluated on the DBpedia- and Corporate-based KGQA benchmarks within the\nText2SPARQL challenge 2025, our approach took first place among the other\nparticipants. This work opens new avenues for developing human-like reasoning\nsystems in multilingual semantic parsing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3amKGQAgent\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3aSPARQL\u67e5\u8be2\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u7ed3\u5408\u89e3\u51b3\u4e0b\u6e38\u4efb\u52a1\u7684\u7ec4\u4ef6\u4e0a\uff0c\u800c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u6765\u5904\u7406\u591a\u8bed\u8a00\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86mKGQAgent\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3aSPARQL\u67e5\u8be2\u7684\u4efb\u52a1\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5b50\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u534f\u8c03\u7684LLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u8fdb\u884c\u89c4\u5212\u3001\u5b9e\u4f53\u94fe\u63a5\u548c\u67e5\u8be2\u4f18\u5316\u3002", "result": "\u5728Text2SPARQL\u6311\u6218\u8d5b2025\u7684DBpedia\u548cCorporate\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86mKGQAgent\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u591a\u8bed\u8a00\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\uff0c\u5e76\u4e3a\u5f00\u53d1\u7c7b\u4eba\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2507.16974", "pdf": "https://arxiv.org/pdf/2507.16974", "abs": "https://arxiv.org/abs/2507.16974", "authors": ["Rishemjit Kaur", "Arshdeep Singh Bhankhar", "Surangika Ranathunga", "Jashanpreet Singh Salh", "Sudhir Rajput", "Vidhi", "Kashish Mahendra", "Bhavika Berwal", "Ritesh Kumar"], "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain", "categories": ["cs.CL", "cs.AI", "I.2.7; J.m"], "comment": "15 pages, 9 tables, Appendix A-K", "summary": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Although large language models (LLMs) can be used to\nimplement Question Answering (QA) systems, simply using publicly available\ngeneral-purpose LLMs in agriculture typically offer generic advisories, lacking\nprecision in local and multilingual contexts due to insufficient\ndomain-specific training and scarcity of high-quality, region-specific\ndatasets. Our study addresses these limitations by generating multilingual\nsynthetic agricultural datasets (English, Hindi, Punjabi) from\nagriculture-specific documents and fine-tuning language-specific LLMs. Our\nevaluation on curated multilingual datasets demonstrates significant\nimprovements in factual accuracy, relevance, and agricultural consensus for the\nfine-tuned models compared to their baseline counterparts. These results\nhighlight the efficacy of synthetic data-driven, language-specific fine-tuning\nas an effective strategy to improve the performance of LLMs in agriculture,\nespecially in multilingual and low-resource settings. By enabling more accurate\nand localized agricultural advisory services, this study provides a meaningful\nstep toward bridging the knowledge gap in AI-driven agricultural solutions for\ndiverse linguistic communities.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u751f\u6210\u591a\u8bed\u8a00\u5408\u6210\u519c\u4e1a\u6570\u636e\u96c6\u5e76\u5fae\u8c03\u8bed\u8a00\u7279\u5b9a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u519c\u4e1a\u9886\u57df\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u3002", "motivation": "\u4f7f\u519c\u6c11\u80fd\u591f\u53ca\u65f6\u83b7\u53d6\u5176\u6bcd\u8bed\u4e2d\u7684\u51c6\u786e\u519c\u4e1a\u76f8\u5173\u4fe1\u606f\u5bf9\u4e8e\u519c\u4e1a\u9886\u57df\u7684\u6210\u529f\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7b80\u5355\u5730\u4f7f\u7528\u516c\u5f00\u7684\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u519c\u4e1a\u9886\u57df\u901a\u5e38\u63d0\u4f9b\u7684\u662f\u901a\u7528\u5efa\u8bae\uff0c\u5728\u672c\u5730\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7f3a\u4e4f\u7cbe\u786e\u6027\uff0c\u8fd9\u662f\u7531\u4e8e\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u4e0d\u8db3\u548c\u9ad8\u8d28\u91cf\u3001\u533a\u57df\u7279\u5b9a\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4ece\u519c\u4e1a\u7279\u5b9a\u6587\u6863\u751f\u6210\u591a\u8bed\u8a00\u5408\u6210\u519c\u4e1a\u6570\u636e\u96c6\uff08\u82f1\u8bed\u3001\u5370\u5730\u8bed\u3001\u65c1\u906e\u666e\u8bed\uff09\uff0c\u5e76\u5fae\u8c03\u8bed\u8a00\u7279\u5b9a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "result": "\u5728\u7cbe\u5fc3\u6574\u7406\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u76f8\u5173\u6027\u548c\u519c\u4e1a\u5171\u8bc6\u65b9\u9762\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u9a71\u52a8\u7684\u3001\u8bed\u8a00\u7279\u5b9a\u7684\u5fae\u8c03\u5728\u6539\u5584\u519c\u4e1a\u9886\u57df\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u3002\u901a\u8fc7\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u672c\u5730\u5316\u7684\u519c\u4e1a\u54a8\u8be2\u670d\u52a1\uff0c\u672c\u7814\u7a76\u4e3a\u7f29\u5c0fAI\u9a71\u52a8\u7684\u519c\u4e1a\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u77e5\u8bc6\u5dee\u8ddd\u505a\u51fa\u4e86\u6709\u610f\u4e49\u7684\u4e00\u6b65\u3002"}}
{"id": "2507.16989", "pdf": "https://arxiv.org/pdf/2507.16989", "abs": "https://arxiv.org/abs/2507.16989", "authors": ["Giulio Pelosio", "Devesh Batra", "No\u00e9mie Bovey", "Robert Hankache", "Cristovao Iglesias", "Greig Cowan", "Raad Khraishi"], "title": "Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can exhibit latent biases towards specific\nnationalities even when explicit demographic markers are not present. In this\nwork, we introduce a novel name-based benchmarking approach derived from the\nBias Benchmark for QA (BBQ) dataset to investigate the impact of substituting\nexplicit nationality labels with culturally indicative names, a scenario more\nreflective of real-world LLM applications. Our novel approach examines how this\nsubstitution affects both bias magnitude and accuracy across a spectrum of LLMs\nfrom industry leaders such as OpenAI, Google, and Anthropic. Our experiments\nshow that small models are less accurate and exhibit more bias compared to\ntheir larger counterparts. For instance, on our name-based dataset and in the\nambiguous context (where the correct choice is not revealed), Claude Haiku\nexhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for\nits larger counterpart, Claude Sonnet, where the latter also outperformed it by\n117.7% in accuracy. Additionally, we find that small models retain a larger\nportion of existing errors in these ambiguous contexts. For example, after\nsubstituting names for explicit nationality references, GPT-4o retains 68% of\nthe error rate versus 76% for GPT-4o-mini, with similar findings for other\nmodel providers, in the ambiguous context. Our research highlights the stubborn\nresilience of biases in LLMs, underscoring their profound implications for the\ndevelopment and deployment of AI systems in diverse, global contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e00\u79cd\u57fa\u4e8e\u540d\u79f0\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u5c06\u663e\u5f0f\u56fd\u7c4d\u6807\u7b7e\u66ff\u6362\u4e3a\u6587\u5316\u6307\u793a\u6027\u540d\u79f0\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u504f\u89c1\u548c\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c0f\u6a21\u578b\u7684\u51c6\u786e\u6027\u8f83\u4f4e\uff0c\u5e76\u4e14\u8868\u73b0\u51fa\u66f4\u591a\u7684\u504f\u89c1\uff0c\u540c\u65f6\u5728\u6a21\u7cca\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u7559\u4e86\u66f4\u5927\u7684\u73b0\u6709\u9519\u8bef\u6bd4\u4f8b\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\uff0cLLMs\u53ef\u80fd\u4f1a\u8868\u73b0\u51fa\u5bf9\u7279\u5b9a\u56fd\u7c4d\u7684\u9690\u542b\u504f\u89c1\uff0c\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u7684\u4eba\u53e3\u7edf\u8ba1\u6807\u8bb0\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u66f4\u8d34\u8fd1\u73b0\u5b9e\u573a\u666f\u7684\u65b9\u6cd5\u6765\u7814\u7a76\u8fd9\u79cd\u504f\u89c1\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u540d\u79f0\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4eceBias Benchmark for QA (BBQ)\u6570\u636e\u96c6\u4e2d\u6d3e\u751f\u51fa\u6765\uff0c\u4ee5\u7814\u7a76\u5c06\u663e\u5f0f\u56fd\u7c4d\u6807\u7b7e\u66ff\u6362\u4e3a\u6587\u5316\u6307\u793a\u6027\u540d\u79f0\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c0f\u6a21\u578b\u7684\u51c6\u786e\u6027\u8f83\u4f4e\uff0c\u5e76\u4e14\u8868\u73b0\u51fa\u66f4\u591a\u7684\u504f\u89c1\u3002\u4f8b\u5982\uff0c\u5728\u6211\u4eec\u7684\u57fa\u4e8e\u540d\u79f0\u7684\u6570\u636e\u96c6\u548c\u6a21\u7cca\u4e0a\u4e0b\u6587\u4e2d\uff0cClaude Haiku\u8868\u73b0\u51fa\u6700\u5dee\u7684\u523b\u677f\u5370\u8c61\u504f\u89c1\u5206\u6570\u4e3a9%\uff0c\u800c\u5176\u8f83\u5927\u7684\u7248\u672cClaude Sonnet\u7684\u504f\u89c1\u5206\u6570\u4ec5\u4e3a3.5%\u3002\u6b64\u5916\uff0c\u5c0f\u6a21\u578b\u5728\u6a21\u7cca\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u7559\u4e86\u66f4\u5927\u7684\u73b0\u6709\u9519\u8bef\u6bd4\u4f8b\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7a81\u663e\u4e86LLMs\u4e2d\u504f\u89c1\u7684\u987d\u56fa\u97e7\u6027\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u591a\u6837\u5316\u7684\u5168\u7403\u73af\u5883\u4e2d\u5f00\u53d1\u548c\u90e8\u7f72AI\u7cfb\u7edf\u7684\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2507.17009", "pdf": "https://arxiv.org/pdf/2507.17009", "abs": "https://arxiv.org/abs/2507.17009", "authors": ["Ming Huang", "Zehan Li", "Yan Hu", "Wanjing Wang", "Andrew Wen", "Scott Lane", "Salih Selek", "Lokesh Shahani", "Rodrigo Machado-Vieira", "Jair Soares", "Hua Xu", "Hongfang Liu"], "title": "Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors", "categories": ["cs.CL", "cs.IR", "q-bio.QM"], "comment": null, "summary": "Suicide remains a pressing global health crisis, with over 720,000 deaths\nannually and millions more affected by suicide ideation (SI) and suicide\nattempts (SA). Early identification of suicidality-related factors (SrFs),\nincluding SI, SA, exposure to suicide (ES), and non-suicidal self-injury\n(NSSI), is critical for timely intervention. While prior studies have applied\nAI to detect SrFs in clinical notes, most treat suicidality as a binary\nclassification task, overlooking the complexity of cooccurring risk factors.\nThis study explores the use of generative large language models (LLMs),\nspecifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs\nfrom psychiatric electronic health records (EHRs). We present a novel end to\nend generative MLC pipeline and introduce advanced evaluation methods,\nincluding label set level metrics and a multilabel confusion matrix for error\nanalysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match\naccuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior\nperformance across label sets, including rare or minority label sets,\nindicating a more balanced and robust performance. Our findings reveal\nsystematic error patterns, such as the conflation of SI and SA, and highlight\nthe models tendency toward cautious over labeling. This work not only\ndemonstrates the feasibility of using generative AI for complex clinical\nclassification tasks but also provides a blueprint for structuring unstructured\nEHR data to support large scale clinical research and evidence based medicine.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7cbe\u795e\u79d1\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u81ea\u6740\u76f8\u5173\u56e0\u7d20\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u7684\u7ed3\u679c\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u9519\u8bef\u6a21\u5f0f\u3002", "motivation": "\u65e9\u671f\u8bc6\u522b\u4e0e\u81ea\u6740\u76f8\u5173\u7684\u56e0\u7d20\u5bf9\u4e8e\u53ca\u65f6\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4ee5\u5f80\u7684\u7814\u7a76\u5927\u591a\u5c06\u81ea\u6740\u503e\u5411\u89c6\u4e3a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u5171\u5b58\u98ce\u9669\u56e0\u7d20\u7684\u590d\u6742\u6027\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u7279\u522b\u662fGPT-3.5\u548cGPT-4.5\uff0c\u5bf9\u7cbe\u795e\u79d1\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u4e2d\u7684\u81ea\u6740\u76f8\u5173\u56e0\u7d20\uff08SrFs\uff09\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\uff08MLC\uff09\u3002", "result": "\u5fae\u8c03\u7684GPT-3.5\u5728\u90e8\u5206\u5339\u914d\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cGPT-4.5\u5728\u5f15\u5bfc\u63d0\u793a\u4e0b\u5728\u6807\u7b7e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u7f55\u89c1\u6216\u5c11\u6570\u6807\u7b7e\u96c6\uff0c\u8868\u660e\u5176\u6027\u80fd\u66f4\u52a0\u5e73\u8861\u548c\u7a33\u5065\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u590d\u6742\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u63d0\u4f9b\u4e86\u84dd\u56fe\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u4e34\u5e8a\u7814\u7a76\u548c\u5faa\u8bc1\u533b\u5b66\u3002"}}
{"id": "2507.17015", "pdf": "https://arxiv.org/pdf/2507.17015", "abs": "https://arxiv.org/abs/2507.17015", "authors": ["Arduin Findeis", "Floris Weers", "Guoli Yin", "Ke Ye", "Ruoming Pang", "Tom Gunter"], "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025", "summary": "Pairwise preferences over model responses are widely collected to evaluate\nand provide feedback to large language models (LLMs). Given two alternative\nmodel responses to the same input, a human or AI annotator selects the \"better\"\nresponse. This approach can provide feedback for domains where other hard-coded\nmetrics are difficult to obtain (e.g., chat response quality), thereby helping\nmodel evaluation or training. However, for some domains high-quality pairwise\ncomparisons can be tricky to obtain - from AI and humans. For example, for\nresponses with many factual statements, annotators may disproportionately weigh\nwriting quality rather than underlying facts. In this work, we explore\naugmenting standard AI annotator systems with additional tools to improve\nperformance on three challenging response domains: long-form factual, math and\ncode tasks. We propose a tool-using agentic system to provide higher quality\nfeedback on these domains. Our system uses web-search and code execution to\nground itself based on external validation, independent of the LLM's internal\nknowledge and biases. We provide extensive experimental results evaluating our\nmethod across the three targeted response domains as well as general annotation\ntasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as\nthree new datasets for domains with saturated pre-existing datasets. Our\nresults indicate that external tools can indeed improve performance in many,\nbut not all, cases. More generally, our experiments highlight the sensitivity\nof performance to simple parameters (e.g., prompt) and the need for improved\n(non-saturated) annotator benchmarks. We share our code at\nhttps://github.com/apple/ml-agent-evaluator.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u589e\u5f3a\u6807\u51c6AI\u6807\u6ce8\u5668\u7cfb\u7edf\u6765\u6539\u5584\u5728\u957f\u7bc7\u4e8b\u5b9e\u3001\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\u7b49\u6311\u6218\u6027\u54cd\u5e94\u9886\u57df\u7684\u6027\u80fd\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5de5\u5177\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u7f51\u7edc\u641c\u7d22\u548c\u4ee3\u7801\u6267\u884c\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\uff0c\u4ece\u800c\u63d0\u9ad8\u53cd\u9988\u8d28\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5916\u90e8\u5de5\u5177\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u786e\u5b9e\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u4e5f\u663e\u793a\u51fa\u6027\u80fd\u5bf9\u7b80\u5355\u53c2\u6570\u7684\u654f\u611f\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u6539\u8fdb\u6807\u6ce8\u5668\u57fa\u51c6\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5728\u67d0\u4e9b\u9886\u57df\uff0c\u9ad8\u8d28\u91cf\u7684\u6210\u5bf9\u6bd4\u8f83\u53ef\u80fd\u96be\u4ee5\u83b7\u5f97\u2014\u2014\u4eceAI\u548c\u4eba\u7c7b\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u5305\u542b\u8bb8\u591a\u4e8b\u5b9e\u9648\u8ff0\u7684\u54cd\u5e94\uff0c\u6807\u6ce8\u8005\u53ef\u80fd\u4e0d\u6210\u6bd4\u4f8b\u5730\u91cd\u89c6\u5199\u4f5c\u8d28\u91cf\u800c\u975e\u6f5c\u5728\u4e8b\u5b9e\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u6539\u8fdb\u7684\u6807\u6ce8\u5668\u7cfb\u7edf\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5de5\u5177\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u4ee5\u5728\u8fd9\u4e9b\u9886\u57df\u63d0\u4f9b\u66f4\u9ad8\u54c1\u8d28\u7684\u53cd\u9988\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u4f7f\u7528\u7f51\u7edc\u641c\u7d22\u548c\u4ee3\u7801\u6267\u884c\u6765\u57fa\u4e8e\u5916\u90e8\u9a8c\u8bc1\u6765\u5de9\u56fa\u81ea\u5df1\uff0c\u72ec\u7acb\u4e8eLLM\u7684\u5185\u90e8\u77e5\u8bc6\u548c\u504f\u89c1\u3002", "result": "\u6211\u4eec\u5728\u4e09\u4e2a\u76ee\u6807\u54cd\u5e94\u9886\u57df\u4ee5\u53ca\u901a\u7528\u6807\u6ce8\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e86RewardBench\uff08\u5305\u62ecAlpacaEval\u548cLLMBar\uff09\u3001RewardMath\u4ee5\u53ca\u4e09\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5916\u90e8\u5de5\u5177\u786e\u5b9e\u53ef\u4ee5\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5916\u90e8\u5de5\u5177\u786e\u5b9e\u53ef\u4ee5\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u5e76\u975e\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u6709\u6548\u3002\u66f4\u5e7f\u6cdb\u5730\u8bf4\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u6027\u80fd\u5bf9\u7b80\u5355\u53c2\u6570\uff08\u4f8b\u5982\u63d0\u793a\uff09\u7684\u654f\u611f\u6027\uff0c\u5e76\u9700\u8981\u6539\u8fdb\uff08\u975e\u9971\u548c\uff09\u6807\u6ce8\u5668\u57fa\u51c6\u3002"}}
{"id": "2507.17025", "pdf": "https://arxiv.org/pdf/2507.17025", "abs": "https://arxiv.org/abs/2507.17025", "authors": ["Soumen Sinha", "Shahryar Rahnamayan", "Azam Asilian Bidgoli"], "title": "Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Efficient text embedding is crucial for large-scale natural language\nprocessing (NLP) applications, where storage and computational efficiency are\nkey concerns. In this paper, we explore how using binary representations\n(barcodes) instead of real-valued features can be used for NLP embeddings\nderived from machine learning models such as BERT. Thresholding is a common\nmethod for converting continuous embeddings into binary representations, often\nusing a fixed threshold across all features. We propose a Coordinate\nSearch-based optimization framework that instead identifies the optimal\nthreshold for each feature, demonstrating that feature-specific thresholds lead\nto improved performance in binary encoding. This ensures that the binary\nrepresentations are both accurate and efficient, enhancing performance across\nvarious features. Our optimal barcode representations have shown promising\nresults in various NLP applications, demonstrating their potential to transform\ntext representation. We conducted extensive experiments and statistical tests\non different NLP tasks and datasets to evaluate our approach and compare it to\nother thresholding methods. Binary embeddings generated using using optimal\nthresholds found by our method outperform traditional binarization methods in\naccuracy. This technique for generating binary representations is versatile and\ncan be applied to any features, not just limited to NLP embeddings, making it\nuseful for a wide range of domains in machine learning applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5750\u6807\u641c\u7d22\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7279\u5f81\u9009\u62e9\u6700\u4f73\u9608\u503c\u6765\u6539\u8fdb\u4e8c\u8fdb\u5236\u7f16\u7801\uff0c\u63d0\u9ad8\u4e86\u6587\u672c\u5d4c\u5165\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u4e8c\u503c\u5316\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u9608\u503c\uff0c\u800c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7279\u5f81\u9009\u62e9\u6700\u4f73\u9608\u503c\u6765\u63d0\u9ad8\u4e8c\u8fdb\u5236\u7f16\u7801\u6027\u80fd\u7684\u53ef\u80fd\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5750\u6807\u641c\u7d22\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u6bcf\u4e2a\u7279\u5f81\u786e\u5b9a\u6700\u4f73\u9608\u503c\uff0c\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u7684\u4e8c\u8fdb\u5236\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u672c\u6587\u65b9\u6cd5\u751f\u6210\u7684\u4e8c\u8fdb\u5236\u5d4c\u5165\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u4e8c\u503c\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2aNLP\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u5750\u6807\u641c\u7d22\u7684\u4f18\u5316\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7279\u5f81\u786e\u5b9a\u6700\u4f73\u9608\u503c\uff0c\u4ece\u800c\u5728\u4e8c\u8fdb\u5236\u7f16\u7801\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002\u8fd9\u79cd\u4e8c\u8fdb\u5236\u8868\u793a\u65b9\u6cd5\u5728\u5404\u79cdNLP\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.17147", "pdf": "https://arxiv.org/pdf/2507.17147", "abs": "https://arxiv.org/abs/2507.17147", "authors": ["Cheng Liu", "Yifei Lu", "Fanghua Ye", "Jian Li", "Xingyu Chen", "Feiliang Ren", "Zhaopeng Tu", "Xiaolong Li"], "title": "CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Role-Playing Language Agents (RPLAs) have emerged as a significant\napplication direction for Large Language Models (LLMs). Existing approaches\ntypically rely on prompt engineering or supervised fine-tuning to enable models\nto imitate character behaviors in specific scenarios, but often neglect the\nunderlying \\emph{cognitive} mechanisms driving these behaviors. Inspired by\ncognitive psychology, we introduce \\textbf{CogDual}, a novel RPLA adopting a\n\\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external\nsituational awareness and internal self-awareness, CogDual generates responses\nwith improved character consistency and contextual alignment. To further\noptimize the performance, we employ reinforcement learning with two\ngeneral-purpose reward schemes designed for open-domain text generation.\nExtensive experiments on the CoSER benchmark, as well as Cross-MR and\nLifeChoice, demonstrate that CogDual consistently outperforms existing\nbaselines and generalizes effectively across diverse role-playing tasks.", "AI": {"tldr": "CogDual\u662f\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u5fc3\u7406\u5b66\u7684\u65b0\u9896\u89d2\u8272\u626e\u6f14\u8bed\u8a00\u4ee3\u7406\uff0c\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u60c5\u5883\u610f\u8bc6\u548c\u5185\u90e8\u81ea\u6211\u610f\u8bc6\uff0c\u63d0\u9ad8\u4e86\u89d2\u8272\u4e00\u81f4\u6027\u4e0e\u4e0a\u4e0b\u6587\u5bf9\u9f50\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u63d0\u793a\u5de5\u7a0b\u6216\u76d1\u7763\u5fae\u8c03\u6765\u8ba9\u6a21\u578b\u6a21\u4eff\u7279\u5b9a\u573a\u666f\u4e2d\u7684\u89d2\u8272\u884c\u4e3a\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e86\u9a71\u52a8\u8fd9\u4e9b\u884c\u4e3a\u7684\u6f5c\u5728\u8ba4\u77e5\u673a\u5236\u3002", "method": "CogDual\u91c7\u7528\u4e86\u4e00\u79cd\u8ba4\u77e5-\u56de\u5e94\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u5916\u90e8\u60c5\u5883\u610f\u8bc6\u548c\u5185\u90e8\u81ea\u6211\u610f\u8bc6\u6765\u751f\u6210\u66f4\u4e00\u81f4\u7684\u89d2\u8272\u54cd\u5e94\uff0c\u5e76\u4f7f\u7528\u4e24\u79cd\u901a\u7528\u5956\u52b1\u65b9\u6848\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "result": "CogDual\u5728CoSER\u57fa\u51c6\u3001Cross-MR\u548cLifeChoice\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u80fd\u6709\u6548\u6cdb\u5316\u5230\u591a\u79cd\u89d2\u8272\u626e\u6f14\u4efb\u52a1\u4e2d\u3002", "conclusion": "CogDual\u5728CoSER\u57fa\u51c6\u4ee5\u53caCross-MR\u548cLifeChoice\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u5404\u79cd\u89d2\u8272\u626e\u6f14\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2507.17178", "pdf": "https://arxiv.org/pdf/2507.17178", "abs": "https://arxiv.org/abs/2507.17178", "authors": ["Zhiqiang Liu", "Enpei Niu", "Yin Hua", "Mengshu Sun", "Lei Liang", "Huajun Chen", "Wen Zhang"], "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SKA-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u6784\u5316\u77e5\u8bc6\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6b64\u65b9\u9762\u4ecd\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u7406\u89e3\u8bc4\u4f30\u4e0d\u591f\u4e25\u8c28\uff0c\u4e14\u4ec5\u5173\u6ce8\u4e00\u79cd\u7c7b\u578b\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u548c\u4e25\u8c28\u7684\u57fa\u51c6\u6765\u8bca\u65ad\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165SKA-Bench\uff0c\u4e00\u4e2a\u5305\u542b\u56db\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u5f62\u5f0f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u6784\u5efa\u5b9e\u4f8b\uff0c\u5e76\u6269\u5c55\u4e3a\u56db\u4e2a\u57fa\u672c\u80fd\u529b\u6d4b\u8bd5\u5e73\u53f0\u3002", "result": "\u5bf98\u4e2a\u4ee3\u8868\u6027\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u7406\u89e3\u7ed3\u6784\u5316\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "conclusion": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7ed3\u6784\u5316\u77e5\u8bc6\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u5176\u6027\u80fd\u53d7\u5230\u566a\u58f0\u91cf\u3001\u77e5\u8bc6\u5355\u5143\u987a\u5e8f\u548c\u5e7b\u89c9\u73b0\u8c61\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.17186", "pdf": "https://arxiv.org/pdf/2507.17186", "abs": "https://arxiv.org/abs/2507.17186", "authors": ["Lingfeng Zeng", "Fangqi Lou", "Zixuan Wang", "Jiajie Xu", "Jinyi Niu", "Mengping Li", "Yifan Dong", "Qi Qi", "Wei Zhang", "Ziwei Yang", "Jun Han", "Ruilun Feng", "Ruiqi Hu", "Lejie Zhang", "Zhengbo Feng", "Yicheng Ren", "Xin Guo", "Zhaowei Liu", "Dongpo Cheng", "Weige Cai", "Liwen Zhang"], "title": "FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance", "categories": ["cs.CL"], "comment": null, "summary": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.17216", "pdf": "https://arxiv.org/pdf/2507.17216", "abs": "https://arxiv.org/abs/2507.17216", "authors": ["Giuseppe Russo", "Debora Nozza", "Paul R\u00f6ttger", "Dirk Hovy"], "title": "The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "People increasingly rely on Large Language Models (LLMs) for moral advice,\nwhich may influence humans' decisions. Yet, little is known about how closely\nLLMs align with human moral judgments. To address this, we introduce the Moral\nDilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a\ndistribution of human moral judgments consisting of a binary evaluation and a\nfree-text rationale. We treat this problem as a pluralistic distributional\nalignment task, comparing the distributions of LLM and human judgments across\ndilemmas. We find that models reproduce human judgments only under high\nconsensus; alignment deteriorates sharply when human disagreement increases. In\nparallel, using a 60-value taxonomy built from 3,783 value expressions\nextracted from rationales, we show that LLMs rely on a narrower set of moral\nvalues than humans. These findings reveal a pluralistic moral gap: a mismatch\nin both the distribution and diversity of values expressed. To close this gap,\nwe introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method\nthat conditions model outputs on human-derived value profiles. DMP improves\nalignment by 64.3% and enhances value diversity, offering a step toward more\npluralistic and human-aligned moral guidance from LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u53d1\u73b0LLMs\u5728\u4eba\u7c7b\u610f\u89c1\u4e00\u81f4\u65f6\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u5206\u6b67\u65f6\u8868\u73b0\u8f83\u5dee\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff08DMP\uff09\u6765\u6539\u5584\u8fd9\u4e00\u60c5\u51b5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u4e0e\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u7684\u5bf9\u9f50\u5ea6\u548c\u591a\u6837\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u4eec\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u9053\u5fb7\u5efa\u8bae\uff0c\u4e86\u89e3\u8fd9\u4e9b\u6a21\u578b\u4e0e\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u7684\u4e00\u81f4\u6027\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u5bf9\u6b64\u4e86\u89e3\u6709\u9650\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u4e0e\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b1,618\u4e2a\u771f\u5b9e\u9053\u5fb7\u56f0\u5883\u53ca\u5176\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u4e00\u4e2a\u591a\u5143\u5206\u5e03\u5bf9\u9f50\u4efb\u52a1\u3002\u901a\u8fc7\u4f7f\u7528\u4ece\u7406\u6027\u4e2d\u63d0\u53d6\u76843,783\u4e2a\u4ef7\u503c\u8868\u8fbe\u6784\u5efa\u768460\u4e2a\u4ef7\u503c\u5206\u7c7b\u6cd5\uff0c\u7814\u7a76\u5206\u6790\u4e86LLMs\u4e0e\u4eba\u7c7b\u5728\u9053\u5fb7\u4ef7\u503c\u4e0a\u7684\u5dee\u5f02\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72c4\u5229\u514b\u96f7\u91c7\u6837\u7684\u52a8\u6001\u9053\u5fb7\u5206\u6790\u65b9\u6cd5\uff08DMP\uff09\u6765\u6539\u5584\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u4ec5\u5728\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u65f6\u624d\u80fd\u518d\u73b0\u4eba\u7c7b\u5224\u65ad\uff0c\u800c\u5728\u4eba\u7c7b\u5206\u6b67\u589e\u52a0\u65f6\u5bf9\u9f50\u5ea6\u8fc5\u901f\u4e0b\u964d\u3002\u6b64\u5916\uff0cLLMs\u4f7f\u7528\u7684\u9053\u5fb7\u4ef7\u503c\u8303\u56f4\u6bd4\u4eba\u7c7b\u72ed\u7a84\u3002\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u9053\u5fb7\u5206\u6790\u65b9\u6cd5\uff08DMP\uff09\uff0c\u6a21\u578b\u7684\u5bf9\u9f50\u5ea6\u63d0\u9ad8\u4e8664.3%\uff0c\u5e76\u4e14\u4ef7\u503c\u591a\u6837\u6027\u5f97\u5230\u4e86\u589e\u5f3a\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u518d\u73b0\u4eba\u7c7b\u5224\u65ad\uff0c\u4f46\u5f53\u4eba\u7c7b\u610f\u89c1\u5206\u6b67\u589e\u52a0\u65f6\uff0c\u5bf9\u9f50\u5ea6\u6025\u5267\u4e0b\u964d\u3002\u6b64\u5916\uff0cLLMs\u4f7f\u7528\u7684\u9053\u5fb7\u4ef7\u503c\u8303\u56f4\u6bd4\u4eba\u7c7b\u72ed\u7a84\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u72c4\u5229\u514b\u96f7\u91c7\u6837\u7684\u52a8\u6001\u9053\u5fb7\u5206\u6790\u65b9\u6cd5\uff08DMP\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u9f50\u5ea6\u5e76\u589e\u5f3a\u4e86\u4ef7\u503c\u591a\u6837\u6027\uff0c\u4e3a\u66f4\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u9053\u5fb7\u6307\u5bfc\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.17234", "pdf": "https://arxiv.org/pdf/2507.17234", "abs": "https://arxiv.org/abs/2507.17234", "authors": ["Kyeongkyu Lee", "Seonghwan Yoon", "Hongki Lim"], "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings", "categories": ["cs.CL"], "comment": null, "summary": "Automatic generation of radiology reports has the potential to alleviate\nradiologists' significant workload, yet current methods struggle to deliver\nclinically reliable conclusions. In particular, most prior approaches focus on\nproducing fluent text without effectively ensuring the factual correctness of\nthe reports and often rely on single-view images, limiting diagnostic\ncomprehensiveness. We propose CLARIFID, a novel framework that directly\noptimizes diagnostic correctness by mirroring the two-step workflow of experts.\nSpecifically, CLARIFID (1) learns the logical flow from Findings to Impression\nthrough section-aware pretraining, (2) is fine-tuned with Proximal Policy\nOptimization in which the CheXbert F1 score of the Impression section serves as\nthe reward, (3) enforces reasoning-aware decoding that completes \"Findings\"\nbefore synthesizing the \"Impression\", and (4) fuses multiple chest X-ray views\nvia a vision-transformer-based multi-view encoder. During inference, we apply a\nreasoning-aware next-token forcing strategy followed by report-level\nre-ranking, ensuring that the model first produces a comprehensive Findings\nsection before synthesizing the Impression and thereby preserving coherent\nclinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate\nthat our method achieves superior clinical efficacy and outperforms existing\nbaselines on both standard NLG metrics and clinically aware scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLARIFID\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u955c\u50cf\u4e13\u5bb6\u7684\u4e24\u6b65\u5de5\u4f5c\u6d41\u7a0b\u6765\u4f18\u5316\u8bca\u65ad\u6b63\u786e\u6027\uff0c\u5305\u62ec\u7ae0\u8282\u611f\u77e5\u9884\u8bad\u7ec3\u3001\u4f7f\u7528CheXbert F1\u5206\u6570\u4f5c\u4e3a\u5956\u52b1\u7684\u5fae\u8c03\u3001\u63a8\u7406\u611f\u77e5\u89e3\u7801\u548c\u591a\u89c6\u56fe\u878d\u5408\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e34\u5e8a\u6548\u679c\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u81ea\u52a8\u751f\u6210\u653e\u5c04\u5b66\u62a5\u544a\u6709\u6f5c\u529b\u51cf\u8f7b\u653e\u5c04\u79d1\u533b\u751f\u7684\u5927\u91cf\u5de5\u4f5c\u8d1f\u62c5\uff0c\u4f46\u76ee\u524d\u7684\u65b9\u6cd5\u96be\u4ee5\u63d0\u4f9b\u4e34\u5e8a\u4e0a\u53ef\u9760\u7684\u7ed3\u8bba\u3002\u7279\u522b\u662f\uff0c\u5927\u591a\u6570\u5148\u524d\u7684\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u751f\u6210\u6d41\u7545\u7684\u6587\u672c\uff0c\u800c\u6ca1\u6709\u6709\u6548\u5730\u786e\u4fdd\u62a5\u544a\u7684\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u5e76\u4e14\u901a\u5e38\u4f9d\u8d56\u4e8e\u5355\u89c6\u56fe\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u7684\u5168\u9762\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86CLARIFID\uff0c\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4f18\u5316\u8bca\u65ad\u6b63\u786e\u6027\uff0c\u901a\u8fc7\u955c\u50cf\u4e13\u5bb6\u7684\u4e24\u6b65\u5de5\u4f5c\u6d41\u7a0b\u3002\u5177\u4f53\u6765\u8bf4\uff0cCLARIFID (1) \u901a\u8fc7\u7ae0\u8282\u611f\u77e5\u9884\u8bad\u7ec3\u5b66\u4e60\u4ece\u53d1\u73b0\u5230\u5370\u8c61\u7684\u903b\u8f91\u6d41\u7a0b\uff0c(2) \u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u8fdb\u884c\u5fae\u8c03\uff0c\u5176\u4e2dCheXbert F1\u5206\u6570\u4f5c\u4e3a\u5956\u52b1\uff0c(3) \u5f3a\u5236\u63a8\u7406\u611f\u77e5\u89e3\u7801\uff0c\u5728\u5408\u6210\u201c\u5370\u8c61\u201d\u4e4b\u524d\u5b8c\u6210\u201c\u53d1\u73b0\u201d\uff0c(4) \u901a\u8fc7\u57fa\u4e8e\u89c6\u89c9\u53d8\u538b\u5668\u7684\u591a\u89c6\u56fe\u7f16\u7801\u5668\u878d\u5408\u591a\u4e2a\u80f8\u90e8X\u5149\u89c6\u56fe\u3002", "result": "\u5728MIMIC-CXR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e34\u5e8a\u6548\u679c\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u5728\u6807\u51c6NLG\u6307\u6807\u548c\u4e34\u5e8a\u610f\u8bc6\u8bc4\u5206\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e34\u5e8a\u6548\u679c\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\uff0c\u5e76\u5728\u6807\u51c6NLG\u6307\u6807\u548c\u4e34\u5e8a\u610f\u8bc6\u8bc4\u5206\u4e0a\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2507.17288", "pdf": "https://arxiv.org/pdf/2507.17288", "abs": "https://arxiv.org/abs/2507.17288", "authors": ["Miaomiao Gao", "Xiaoxiao Xiang", "Yiwen Guo"], "title": "Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper describes our Triple X speech recognition system submitted to Task\n1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)\nChallenge. Our work focuses on optimizing speech recognition accuracy in\nmultilingual conversational scenarios through an innovative encoder-adapter-LLM\narchitecture. This framework harnesses the powerful reasoning capabilities of\ntext-based large language models while incorporating domain-specific\nadaptations. To further enhance multilingual recognition performance, we\nadopted a meticulously designed multi-stage training strategy leveraging\nextensive multilingual audio datasets. Experimental results demonstrate that\nour approach achieves competitive Word Error Rate (WER) performance on both dev\nand test sets, obtaining second place in the challenge ranking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f16\u7801\u5668-\u9002\u914d\u5668-\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u5bf9\u8bdd\u8bed\u97f3\u8bed\u8a00\u5efa\u6a21\u6311\u6218\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u4f18\u5316\u591a\u8bed\u8a00\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u8bed\u97f3\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u9886\u57df\u7279\u5b9a\u7684\u9002\u5e94\u6027\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u7f16\u7801\u5668-\u9002\u914d\u5668-\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u6587\u672c\u578b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u548c\u9886\u57df\u7279\u5b9a\u7684\u9002\u5e94\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u5e7f\u6cdb\u7684\u591a\u8bed\u8a00\u97f3\u9891\u6570\u636e\u96c6\u6765\u63d0\u9ad8\u591a\u8bed\u8a00\u8bc6\u522b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5f00\u53d1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u8868\u73b0\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6311\u6218\u6392\u540d\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e8c\u540d\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u8bed\u8a00\u5bf9\u8bdd\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.17399", "pdf": "https://arxiv.org/pdf/2507.17399", "abs": "https://arxiv.org/abs/2507.17399", "authors": ["Zhili Shen", "Chenxin Diao", "Pascual Merita", "Pavlos Vougiouklis", "Jeff Z. Pan"], "title": "Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR 2025 LiveRAG Challenge Program", "summary": "Recent studies have explored graph-based approaches to retrieval-augmented\ngeneration, leveraging structured or semi-structured information -- such as\nentities and their relations extracted from documents -- to enhance retrieval.\nHowever, these methods are typically designed to address specific tasks, such\nas multi-hop question answering and query-focused summarisation, and therefore,\nthere is limited evidence of their general applicability across broader\ndatasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG\nsolution: $\\text{GeAR}$ and explore its performance and limitations on the\nSIGIR 2025 LiveRAG Challenge.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u56fe\u7684RAG\u65b9\u6cd5GeAR\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u9002\u7528\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728SIGIR 2025 LiveRAG\u6311\u6218\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u5728\u66f4\u5e7f\u6cdb\u6570\u636e\u96c6\u4e0a\u7684\u9002\u7528\u6027\u8bc1\u636e\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u7279\u522b\u662fGeAR\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u9002\u7528\u6027\u3002", "result": "\u672c\u6587\u5206\u6790\u4e86GeAR\u5728SIGIR 2025 LiveRAG\u6311\u6218\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u6f5c\u5728\u7684\u9650\u5236\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u9002\u5e94\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u56fe\u7684RAG\u89e3\u51b3\u65b9\u6848GeAR\uff0c\u5e76\u63a2\u7d22\u5176\u5728SIGIR 2025 LiveRAG\u6311\u6218\u4e2d\u7684\u6027\u80fd\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2507.17409", "pdf": "https://arxiv.org/pdf/2507.17409", "abs": "https://arxiv.org/abs/2507.17409", "authors": ["Carlotta Quensel", "Neele Falk", "Gabriella Lapesa"], "title": "Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging", "categories": ["cs.CL"], "comment": "Accepted to the 12th Workshop on Argument Mining (ArgMining) 2025", "summary": "In assessing argument strength, the notions of what makes a good argument are\nmanifold. With the broader trend towards treating subjectivity as an asset and\nnot a problem in NLP, new dimensions of argument quality are studied. Although\nstudies on individual subjective features like personal stories exist, there is\na lack of large-scale analyses of the relation between these features and\nargument strength. To address this gap, we conduct regression analysis to\nquantify the impact of subjective factors $-$ emotions, storytelling, and\nhedging $-$ on two standard datasets annotated for objective argument quality\nand subjective persuasion. As such, our contribution is twofold: at the level\nof contributed resources, as there are no datasets annotated with all studied\ndimensions, this work compares and evaluates automated annotation methods for\neach subjective feature. At the level of novel insights, our regression\nanalysis uncovers different patterns of impact of subjective features on the\ntwo facets of argument strength encoded in the datasets. Our results show that\nstorytelling and hedging have contrasting effects on objective and subjective\nargument quality, while the influence of emotions depends on their rhetoric\nutilization rather than the domain.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u56de\u5f52\u5206\u6790\u7814\u7a76\u4e86\u4e3b\u89c2\u56e0\u7d20\u5bf9\u8bba\u8bc1\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8bb2\u6545\u4e8b\u548c\u6a21\u7cca\u8868\u8fbe\u5bf9\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bba\u8bc1\u8d28\u91cf\u6709\u4e0d\u540c\u7684\u5f71\u54cd\uff0c\u800c\u60c5\u7eea\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u5176\u4fee\u8f9e\u4f7f\u7528\u800c\u975e\u9886\u57df\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9\u4e3b\u89c2\u7279\u5f81\u4e0e\u8bba\u8bc1\u5f3a\u5ea6\u4e4b\u95f4\u5173\u7cfb\u7684\u5927\u89c4\u6a21\u5206\u6790\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u901a\u8fc7\u56de\u5f52\u5206\u6790\u6765\u91cf\u5316\u4e3b\u89c2\u56e0\u7d20\uff08\u60c5\u7eea\u3001\u8bb2\u6545\u4e8b\u548c\u6a21\u7cca\u8868\u8fbe\uff09\u5bf9\u4e24\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u7684\u5f71\u54cd\uff0c\u8fd9\u4e24\u4e2a\u6570\u636e\u96c6\u88ab\u6ce8\u91ca\u4e3a\u5ba2\u89c2\u8bba\u8bc1\u8d28\u91cf\u548c\u4e3b\u89c2\u8bf4\u670d\u529b\u3002", "result": "\u672c\u6587\u7684\u7ed3\u679c\u663e\u793a\uff0c\u8bb2\u6545\u4e8b\u548c\u6a21\u7cca\u8868\u8fbe\u5bf9\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bba\u8bc1\u8d28\u91cf\u6709\u4e0d\u540c\u7684\u5f71\u54cd\uff0c\u800c\u60c5\u7eea\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u5176\u4fee\u8f9e\u4f7f\u7528\u800c\u975e\u9886\u57df\u3002", "conclusion": "\u672c\u6587\u7684\u7ed3\u8bba\u662f\uff0c\u8bb2\u6545\u4e8b\u548c\u6a21\u7cca\u8868\u8fbe\u5bf9\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bba\u8bc1\u8d28\u91cf\u6709\u4e0d\u540c\u7684\u5f71\u54cd\uff0c\u800c\u60c5\u7eea\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u5176\u4fee\u8f9e\u4f7f\u7528\u800c\u975e\u9886\u57df\u3002"}}
{"id": "2507.17442", "pdf": "https://arxiv.org/pdf/2507.17442", "abs": "https://arxiv.org/abs/2507.17442", "authors": ["Shiting Chen", "Zijian Zhao", "Jinsong Chen"], "title": "Each to Their Own: Exploring the Optimal Embedding in RAG", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Confident RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6b21\u751f\u6210\u5e76\u9009\u62e9\u9ad8\u7f6e\u4fe1\u5ea6\u54cd\u5e94\u6765\u63d0\u9ad8RAG\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u67b6\u6784\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u5d4c\u5165\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u8868\u73b0\u5dee\u5f02\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5e76\u68c0\u9a8c\u4e86\u4e24\u79cd\u65b9\u6cd5\u6765\u589e\u5f3aRAG\uff0c\u5373Mixture-Embedding RAG\u548cConfident RAG\u3002", "result": "Confident RAG \u5728\u5e73\u5747\u4e0a\u6bd4\u666e\u901aLLM\u548cRAG\u5206\u522b\u63d0\u9ad8\u4e86\u7ea610%\u548c5%\u3002", "conclusion": "Confident RAG \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u9886\u57df\u3002"}}
{"id": "2507.17476", "pdf": "https://arxiv.org/pdf/2507.17476", "abs": "https://arxiv.org/abs/2507.17476", "authors": ["Alexander R. Fabbri", "Diego Mares", "Jorge Flores", "Meher Mankikar", "Ernesto Hernandez", "Dean Lee", "Bing Liu", "Chen Xing"], "title": "MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although recent Large Language Models (LLMs) have shown rapid improvement on\nreasoning benchmarks in English, the evaluation of such LLMs' multilingual\nreasoning capability across diverse languages and cultural contexts remains\nlimited. Existing multilingual reasoning benchmarks are typically constructed\nby translating existing English reasoning benchmarks, biasing these benchmarks\ntowards reasoning problems with context in English language/cultures. In this\nwork, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a\nbenchmark designed to assess LLMs on more than 1,000 native, linguistic and\nculturally grounded reasoning questions written by native speakers in French,\nSpanish, and Chinese. MultiNRC covers four core reasoning categories:\nlanguage-specific linguistic reasoning, wordplay & riddles, cultural/tradition\nreasoning, and math reasoning with cultural relevance. For cultural/tradition\nreasoning and math reasoning with cultural relevance, we also provide English\nequivalent translations of the multilingual questions by manual translation\nfrom native speakers fluent in English. This set of English equivalents can\nprovide a direct comparison of LLM reasoning capacity in other languages vs.\nEnglish on the same reasoning questions. We systematically evaluate current 14\nleading LLMs covering most LLM families on MultiNRC and its English equivalent\nset. The results show that (1) current LLMs are still not good at native\nmultilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs\nexhibit distinct strengths and weaknesses in handling linguistic, cultural, and\nlogical reasoning tasks; (3) Most models perform substantially better in math\nreasoning in English compared to in original languages (+10%), indicating\npersistent challenges with culturally grounded knowledge.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MultiNRC\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u539f\u751f\u591a\u8bed\u8a00\u63a8\u7406\u548c\u6587\u5316\u76f8\u5173\u77e5\u8bc6\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u8de8\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u901a\u5e38\u901a\u8fc7\u7ffb\u8bd1\u82f1\u8bed\u63a8\u7406\u57fa\u51c6\u6784\u5efa\uff0c\u504f\u5411\u4e8e\u82f1\u8bed\u8bed\u8a00/\u6587\u5316\u80cc\u666f\u7684\u63a8\u7406\u95ee\u9898\u3002\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86Multilingual Native Reasoning Challenge (MultiNRC)\u57fa\u51c6\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "14\u4e2a\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728MultiNRC\u53ca\u5176\u82f1\u6587\u7b49\u4ef7\u96c6\u4e0a\u7684\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\uff0c\u6a21\u578b\u5728\u539f\u751f\u591a\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5728\u5904\u7406\u8bed\u8a00\u3001\u6587\u5316\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u539f\u751f\u591a\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u4ecd\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5728\u6587\u5316\u76f8\u5173\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u6301\u7eed\u6311\u6218\u3002"}}
{"id": "2507.17527", "pdf": "https://arxiv.org/pdf/2507.17527", "abs": "https://arxiv.org/abs/2507.17527", "authors": ["Shanbo Cheng", "Yu Bao", "Zhichao Huang", "Yu Lu", "Ningxin Peng", "Lu Xu", "Runsheng Yu", "Rong Cao", "Ting Han", "Zeyang Li", "Sitong Liu", "Shengtao Ma", "Shiguang Pan", "Jiongchen Xiao", "Nuo Xu", "Meng Yang", "Rong Ye", "Yiming Yu", "Ruofei Zhang", "Wanyi Zhang", "Wenhao Zhu", "Liehao Zou", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Seed-LiveInterpret 2.0 Technical Report", "summary": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability.", "AI": {"tldr": "Seed-LiveInterpret 2.0 is an advanced end-to-end simultaneous interpretation model that improves translation quality and reduces latency, offering a practical solution for real-time speech-to-speech generation.", "motivation": "The paper addresses the challenges in automatic simultaneous interpretation, such as subpar transcription and translation quality, lack of real-time speech generation, multi-speaker confusion, and translated speech inflation.", "method": "The study introduces Seed-LiveInterpret 2.0, an end-to-end SI model that uses large-scale pretraining and reinforcement learning to achieve high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning capabilities.", "result": "Experimental results show that Seed-LiveInterpret 2.0 achieves a better balance between translation accuracy and latency, with human interpreters validating over 70% correctness in complex scenarios. It also outperforms commercial SI solutions in translation quality while reducing average latency significantly.", "conclusion": "Seed-LiveInterpret 2.0 demonstrates significant improvements in translation quality and latency, making it a practical solution for simultaneous interpretation."}}
{"id": "2507.17578", "pdf": "https://arxiv.org/pdf/2507.17578", "abs": "https://arxiv.org/abs/2507.17578", "authors": ["Brian DeRenzi", "Anna Dixon", "Mohamed Aymane Farhi", "Christian Resch"], "title": "Synthetic Voice Data for Automatic Speech Recognition in African Languages", "categories": ["cs.CL", "I.2.7"], "comment": "29 pages incl. appendix, 8 tables, 5 figures. Authors are listed in\n  alphabetical order", "summary": "Speech technology remains out of reach for most of the over 2300 languages in\nAfrica. We present the first systematic assessment of large-scale synthetic\nvoice corpora for African ASR. We apply a three-step process: LLM-driven text\ncreation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages\nfor which we create synthetic text achieved readability scores above 5 out of\n7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created\nmore than 2,500 hours of synthetic voice data at below 1% of the cost of real\ndata. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h\nsynthetic Hausa matched a 500h real-data-only baseline, while 579h real and\n450h to 993h synthetic data created the best performance. We also present\ngender-disaggregated ASR performance evaluation. For very low-resource\nlanguages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2\nreal-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on\nsome evaluation data, but not on others. Investigating intercoder reliability,\nASR errors and evaluation datasets revealed the need for more robust reviewer\nprotocols and more accurate evaluation data. All data and models are publicly\nreleased to invite further work to improve synthetic data for African\nlanguages.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u975e\u6d32\u8bed\u8a00\u7684\u5927\u89c4\u6a21\u5408\u6210\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u975e\u6d32\u6709\u8d85\u8fc72300\u79cd\u8bed\u8a00\uff0c\u4f46\u8bed\u97f3\u6280\u672f\u5bf9\u5927\u591a\u6570\u8bed\u8a00\u6765\u8bf4\u4ecd\u7136\u96be\u4ee5\u89e6\u53ca\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5927\u89c4\u6a21\u5408\u6210\u8bed\u97f3\u8bed\u6599\u5e93\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u4e00\u4e2a\u4e09\u6b65\u6d41\u7a0b\uff1aLLM\u9a71\u52a8\u7684\u6587\u672c\u521b\u5efa\u3001TTS\u8bed\u97f3\u5408\u6210\u548cASR\u5fae\u8c03\u3002", "result": "\u516b\u79cd\u8bed\u8a00\u7684\u5408\u6210\u6587\u672c\u53ef\u8bfb\u6027\u8bc4\u5206\u9ad8\u4e8e5/7\u3002\u5bf9\u4e8e\u4e09\u79cd\u8bed\u8a00\uff08\u8c6a\u8428\u8bed\u3001\u591a\u6d1b\u8bed\u3001\u5947\u5207\u74e6\u8bed\uff09\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u8d85\u8fc72500\u5c0f\u65f6\u7684\u5408\u6210\u8bed\u97f3\u6570\u636e\uff0c\u6210\u672c\u4ec5\u4e3a\u771f\u5b9e\u6570\u636e\u76841%\u4ee5\u4e0b\u3002\u5fae\u8c03\u540e\u7684Wav2Vec-BERT-2.0\u6a21\u578b\u5728250\u5c0f\u65f6\u771f\u5b9e\u548c250\u5c0f\u65f6\u5408\u6210\u8c6a\u8428\u8bed\u6570\u636e\u4e0a\u8fbe\u5230\u4e86\u4e0e500\u5c0f\u65f6\u771f\u5b9e\u6570\u636e\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u5408\u6210\u8bed\u97f3\u6570\u636e\u5728\u975e\u6d32\u8bed\u8a00\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.17618", "pdf": "https://arxiv.org/pdf/2507.17618", "abs": "https://arxiv.org/abs/2507.17618", "authors": ["Bowen Zheng", "Ming Ma", "Zhongqiao Lin", "Tianming Yang"], "title": "A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)", "categories": ["cs.CL", "cs.PF"], "comment": null, "summary": "Large language models are computationally expensive due to their deep\nstructures. Prior research has shown that intermediate layers contain\nsufficient information to generate accurate answers, leading to the development\nof early-exit algorithms that reduce inference costs by terminating computation\nat earlier layers. However, these methods often suffer from poor performance\ndue to misalignment between intermediate and output layer representations that\nlead to decoding inaccuracy. To address these challenges, we propose SPADE\n(SPace Alignment DEcoding), a novel decoding method that aligns intermediate\nlayer representations with the output layer by propagating a minimally reduced\nsequence consisting of only the start token and the answer token. We further\noptimize the early-exit decision-making process by training a linear\napproximation of SPADE that computes entropy-based confidence metrics. Putting\nthem together, we create a hybrid early-exit algorithm that monitors confidence\nlevels and stops inference at intermediate layers while using SPADE to generate\nhigh-quality outputs. This approach significantly reduces inference costs\nwithout compromising accuracy, offering a scalable and efficient solution for\ndeploying large language models in real-world applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPADE\u7684\u65b0\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u4e2d\u95f4\u5c42\u8868\u793a\u4e0e\u8f93\u51fa\u5c42\u7684\u5bf9\u9f50\uff0c\u4ee5\u53ca\u57fa\u4e8e\u71b5\u7684\u7f6e\u4fe1\u5ea6\u6307\u6807\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65e9\u671f\u9000\u51fa\u7b97\u6cd5\u3002", "motivation": "\u7531\u4e8e\u4e2d\u95f4\u5c42\u548c\u8f93\u51fa\u5c42\u8868\u793a\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u5bfc\u81f4\u89e3\u7801\u4e0d\u51c6\u786e\uff0c\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u8f83\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u65b9\u6cd5SPADE\uff0c\u901a\u8fc7\u4f20\u64ad\u4ec5\u5305\u542b\u8d77\u59cb\u6807\u8bb0\u548c\u7b54\u6848\u6807\u8bb0\u7684\u6700\u5c0f\u5316\u5e8f\u5217\u6765\u5bf9\u9f50\u4e2d\u95f4\u5c42\u8868\u793a\u4e0e\u8f93\u51fa\u5c42\u3002\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u65e9\u671f\u9000\u51fa\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8bad\u7ec3SPADE\u7684\u7ebf\u6027\u8fd1\u4f3c\u6765\u8ba1\u7b97\u57fa\u4e8e\u71b5\u7684\u7f6e\u4fe1\u5ea6\u6307\u6807\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u51c6\u786e\u6027\uff0c\u4e3a\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17634", "pdf": "https://arxiv.org/pdf/2507.17634", "abs": "https://arxiv.org/abs/2507.17634", "authors": ["Changxin Tian", "Jiapeng Wang", "Qian Zhao", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Jiaxin Mao", "Wayne Xin Zhao", "Zhiqiang Zhang", "Jun Zhou"], "title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": null, "summary": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 WSM \u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5b66\u4e60\u7387\u8870\u51cf\u4e0e\u6a21\u578b\u5408\u5e76\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4ee5\u6a21\u62df\u5404\u79cd\u8870\u51cf\u7b56\u7565\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWSM \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684 WSD \u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u76d1\u7763\u5fae\u8c03\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u6700\u8fd1\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\u8fdb\u5c55\u8868\u660e\uff0c\u6d88\u9664\u4f20\u7edf\u8870\u51cf\u9636\u6bb5\u7684\u65e0\u8870\u51cf\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u6027\u80fd\u65b9\u9762\u662f\u6709\u6548\u7684\u3002\u6a21\u578b\u5408\u5e76\u6280\u672f\u5728\u8fd9\u4e2a\u9886\u57df\u4e2d\u5df2\u6210\u4e3a\u7279\u522b\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 Warmup-Stable and Merge (WSM)\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u5b66\u4e60\u7387\u8870\u51cf\u548c\u6a21\u578b\u5408\u5e76\u4e4b\u95f4\u7684\u6b63\u5f0f\u8054\u7cfb\u3002WSM \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u7528\u4e8e\u6a21\u62df\u5404\u79cd\u8870\u51cf\u7b56\u7565\uff0c\u540c\u65f6\u4e0e\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\u5b8c\u5168\u517c\u5bb9\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u5408\u5e76\u6301\u7eed\u65f6\u95f4\uff08\u68c0\u67e5\u70b9\u805a\u5408\u7684\u8bad\u7ec3\u7a97\u53e3\uff09\u662f\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u6700\u5173\u952e\u56e0\u7d20\uff0c\u8d85\u8fc7\u4e86\u68c0\u67e5\u70b9\u95f4\u9694\u548c\u5408\u5e76\u6570\u91cf\u7684\u91cd\u8981\u6027\u3002\u6211\u4eec\u7684\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e WSD \u65b9\u6cd5\uff0c\u5206\u522b\u5728 MATH \u4e0a\u63d0\u9ad8\u4e86 3.5%\uff0c\u5728 HumanEval \u4e0a\u63d0\u9ad8\u4e86 2.9%\uff0c\u5728 MMLU-Pro \u4e0a\u63d0\u9ad8\u4e86 5.5%\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d consistently \u8d85\u8fc7\u5e7f\u6cdb\u91c7\u7528\u7684 Warmup-Stable-Decay (WSD) \u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u6027\u80fd\u4f18\u52bf\u6269\u5c55\u5230\u76d1\u7763\u5fae\u8c03\u573a\u666f\uff0c\u7a81\u663e\u4e86 WSM \u5728\u957f\u671f\u6a21\u578b\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.17636", "pdf": "https://arxiv.org/pdf/2507.17636", "abs": "https://arxiv.org/abs/2507.17636", "authors": ["Victor Hartman", "Petter T\u00f6rnberg"], "title": "Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries", "categories": ["cs.CL"], "comment": null, "summary": "Negative campaigning is a central feature of political competition, yet\nempirical research has been limited by the high cost and limited scalability of\nexisting classification methods. This study makes two key contributions. First,\nit introduces zero-shot Large Language Models (LLMs) as a novel approach for\ncross-lingual classification of negative campaigning. Using benchmark datasets\nin ten languages, we demonstrate that LLMs achieve performance on par with\nnative-speaking human coders and outperform conventional supervised machine\nlearning approaches. Second, we leverage this novel method to conduct the\nlargest cross-national study of negative campaigning to date, analyzing 18\nmillion tweets posted by parliamentarians in 19 European countries between 2017\nand 2022. The results reveal consistent cross-national patterns: governing\nparties are less likely to use negative messaging, while ideologically extreme\nand populist parties -- particularly those on the radical right -- engage in\nsignificantly higher levels of negativity. These findings advance our\nunderstanding of how party-level characteristics shape strategic communication\nin multiparty systems. More broadly, the study demonstrates the potential of\nLLMs to enable scalable, transparent, and replicable research in political\ncommunication across linguistic and cultural contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf91800\u4e07\u6761\u63a8\u6587\u8fdb\u884c\u5206\u6790\uff0c\u53d1\u73b0\u6267\u653f\u515a\u548c\u6781\u7aef\u653f\u515a\u5728\u8d1f\u9762\u7ade\u9009\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u7c7b\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u53ef\u6269\u5c55\u6027\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u7814\u7a76\u8d1f\u9762\u7ade\u9009\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u96f6\u6837\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8de8\u8bed\u8a00\u5206\u7c7b\u8d1f\u9762\u7ade\u9009\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u8fd9\u4e00\u65b9\u6cd5\u8fdb\u884c\u4e86\u6700\u5927\u89c4\u6a21\u7684\u8de8\u56fd\u7814\u7a76\u3002", "result": "LLMs\u5728\u5341\u79cd\u8bed\u8a00\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e0e\u6bcd\u8bed\u8005\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u4f20\u7edf\u7684\u76d1\u7763\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6267\u653f\u515a\u8f83\u5c11\u4f7f\u7528\u8d1f\u9762\u4fe1\u606f\uff0c\u800c\u610f\u8bc6\u5f62\u6001\u6781\u7aef\u548c\u6c11\u7cb9\u4e3b\u4e49\u653f\u515a\uff0c\u5c24\u5176\u662f\u53f3\u7ffc\u6fc0\u8fdb\u653f\u515a\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8d1f\u9762\u7a0b\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u5206\u7c7b\u8d1f\u9762\u7ade\u9009\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u900f\u660e\u548c\u53ef\u590d\u5236\u7684\u653f\u6cbb\u4f20\u64ad\u7814\u7a76\u3002"}}
{"id": "2507.17702", "pdf": "https://arxiv.org/pdf/2507.17702", "abs": "https://arxiv.org/abs/2507.17702", "authors": ["Changxin Tian", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6548\u7387\u6760\u6746\uff08EL\uff09\u6765\u8861\u91cfMoE\u6a21\u578b\u76f8\u5bf9\u4e8e\u5bc6\u96c6\u6a21\u578b\u7684\u8ba1\u7b97\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MoE\u67b6\u6784\u914d\u7f6e\u4e0eEL\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u6700\u7ec8\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684MoE\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u9884\u6d4b\u7ed9\u5b9aMoE\u914d\u7f6e\uff08\u5982\u4e13\u5bb6\u6fc0\u6d3b\u6bd4\u548c\u7c92\u5ea6\uff09\u7684\u6a21\u578b\u5bb9\u91cf\u8fd9\u4e00\u672a\u89e3\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u6548\u7387\u6760\u6746\uff08EL\uff09\u8fd9\u4e00\u5ea6\u91cf\u6807\u51c6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e86MoE\u67b6\u6784\u914d\u7f6e\u4e0eEL\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u6574\u5408\u53d1\u73b0\u5f62\u6210\u4e86\u7edf\u4e00\u7684\u6269\u5c55\u5b9a\u5f8b\u3002", "result": "EL\u4e3b\u8981\u7531\u4e13\u5bb6\u6fc0\u6d3b\u6bd4\u548c\u603b\u8ba1\u7b97\u9884\u7b97\u9a71\u52a8\uff0c\u9075\u5faa\u53ef\u9884\u6d4b\u7684\u5e42\u5f8b\uff0c\u800c\u4e13\u5bb6\u7c92\u5ea6\u5219\u4f5c\u4e3a\u975e\u7ebf\u6027\u8c03\u8282\u5668\uff0c\u5177\u6709\u660e\u786e\u7684\u6700\u4f73\u8303\u56f4\u3002\u901a\u8fc7\u8bbe\u8ba1\u548c\u8bad\u7ec3Ling-mini-beta\u6a21\u578b\u9a8c\u8bc1\u4e86\u6269\u5c55\u5b9a\u5f8b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u9ad8\u6548MoE\u6a21\u578b\u7684\u6269\u5c55\u63d0\u4f9b\u4e86\u539f\u7406\u6027\u548c\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2507.17709", "pdf": "https://arxiv.org/pdf/2507.17709", "abs": "https://arxiv.org/abs/2507.17709", "authors": ["Parker Riley", "Siamak Shakeri", "Waleed Ammar", "Jonathan H. Clark"], "title": "TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa", "categories": ["cs.CL"], "comment": null, "summary": "We present TyDi QA-WANA, a question-answering dataset consisting of 28K\nexamples divided among 10 language varieties of western Asia and northern\nAfrica. The data collection process was designed to elicit information-seeking\nquestions, where the asker is genuinely curious to know the answer. Each\nquestion in paired with an entire article that may or may not contain the\nanswer; the relatively large size of the articles results in a task suitable\nfor evaluating models' abilities to utilize large text contexts in answering\nquestions. Furthermore, the data was collected directly in each language\nvariety, without the use of translation, in order to avoid issues of cultural\nrelevance. We present performance of two baseline models, and release our code\nand data to facilitate further improvement by the research community.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u95ee\u7b54\u6570\u636e\u96c6TyDi QA-WANA\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u56de\u7b54\u95ee\u9898\u65f6\u5229\u7528\u5927\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0c\u5e76\u907f\u514d\u6587\u5316\u76f8\u5173\u6027\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u6a21\u578b\u5728\u56de\u7b54\u95ee\u9898\u65f6\u5229\u7528\u5927\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u907f\u514d\u6587\u5316\u76f8\u5173\u6027\u95ee\u9898\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u95ee\u7b54\u6570\u636e\u96c6\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\uff0c\u4ee5\u5f15\u53d1\u4fe1\u606f\u5bfb\u6c42\u95ee\u9898\uff0c\u63d0\u95ee\u8005\u771f\u6b63\u597d\u5947\u7b54\u6848\u3002\u6bcf\u4e2a\u95ee\u9898\u90fd\u4e0e\u4e00\u7bc7\u6587\u7ae0\u914d\u5bf9\uff0c\u8fd9\u7bc7\u6587\u7ae0\u53ef\u80fd\u5305\u542b\u6216\u4e0d\u5305\u542b\u7b54\u6848\u3002\u6570\u636e\u662f\u5728\u6bcf\u79cd\u8bed\u8a00\u53d8\u4f53\u4e2d\u76f4\u63a5\u6536\u96c6\u7684\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u7ffb\u8bd1\uff0c\u4ee5\u907f\u514d\u6587\u5316\u76f8\u5173\u6027\u95ee\u9898\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u4e86TyDi QA-WANA\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u4e24\u79cd\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86TyDi QA-WANA\uff0c\u8fd9\u662f\u4e00\u4e2a\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b28K\u4e2a\u4f8b\u5b50\uff0c\u5206\u5e03\u572810\u79cd\u897f\u4e9a\u548c\u5317\u975e\u7684\u8bed\u8a00\u53d8\u4f53\u4e2d\u3002\u6211\u4eec\u5c55\u793a\u4e86\u4e24\u79cd\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u4fc3\u8fdb\u7814\u7a76\u793e\u533a\u7684\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.17717", "pdf": "https://arxiv.org/pdf/2507.17717", "abs": "https://arxiv.org/abs/2507.17717", "authors": ["Karen Zhou", "John Giorgi", "Pranav Mani", "Peng Xu", "Davis Liang", "Chenhao Tan"], "title": "From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI-generated clinical notes are increasingly used in healthcare, but\nevaluating their quality remains a challenge due to high subjectivity and\nlimited scalability of expert review. Existing automated metrics often fail to\nalign with real-world physician preferences. To address this, we propose a\npipeline that systematically distills real user feedback into structured\nchecklists for note evaluation. These checklists are designed to be\ninterpretable, grounded in human feedback, and enforceable by LLM-based\nevaluators. Using deidentified data from over 21,000 clinical encounters,\nprepared in accordance with the HIPAA safe harbor standard, from a deployed AI\nmedical scribe system, we show that our feedback-derived checklist outperforms\nbaseline approaches in our offline evaluations in coverage, diversity, and\npredictive power for human ratings. Extensive experiments confirm the\nchecklist's robustness to quality-degrading perturbations, significant\nalignment with clinician preferences, and practical value as an evaluation\nmethodology. In offline research settings, the checklist can help identify\nnotes likely to fall below our chosen quality thresholds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u53cd\u9988\u7684\u7ed3\u6784\u5316\u68c0\u67e5\u8868\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u7684\u4e34\u5e8a\u7b14\u8bb0\uff0c\u8be5\u65b9\u6cd5\u5728\u79bb\u7ebf\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002", "motivation": "AI\u751f\u6210\u7684\u4e34\u5e8a\u7b14\u8bb0\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u8d8a\u6765\u8d8a\u5e38\u7528\uff0c\u4f46\u8bc4\u4f30\u5176\u8d28\u91cf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e13\u5bb6\u8bc4\u5ba1\u5b58\u5728\u9ad8\u4e3b\u89c2\u6027\u548c\u6709\u9650\u7684\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u7684\u81ea\u52a8\u5316\u6307\u6807\u5f80\u5f80\u65e0\u6cd5\u4e0e\u73b0\u5b9e\u4e2d\u7684\u533b\u751f\u504f\u597d\u5bf9\u9f50\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ba1\u9053\uff0c\u5c06\u771f\u5b9e\u7528\u6237\u53cd\u9988\u7cfb\u7edf\u5730\u63d0\u70bc\u6210\u7ed3\u6784\u5316\u7684\u68c0\u67e5\u8868\u4ee5\u8bc4\u4f30\u7b14\u8bb0\u3002\u8fd9\u4e9b\u68c0\u67e5\u8868\u8bbe\u8ba1\u4e3a\u53ef\u89e3\u91ca\u3001\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\uff0c\u5e76\u53ef\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u8005\u6267\u884c\u3002", "result": "\u4f7f\u7528\u6765\u81ea21,000\u591a\u4e2a\u4e34\u5e8a\u4f1a\u8bca\u7684\u53bb\u6807\u8bc6\u6570\u636e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u53cd\u9988\u884d\u751f\u68c0\u67e5\u8868\u5728\u8986\u76d6\u7387\u3001\u591a\u6837\u6027\u548c\u9884\u6d4b\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u786e\u8ba4\u4e86\u68c0\u67e5\u8868\u5bf9\u8d28\u91cf\u4e0b\u964d\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u4e0e\u4e34\u5e8a\u533b\u751f\u504f\u597d\u7684\u663e\u8457\u4e00\u81f4\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u53cd\u9988\u884d\u751f\u68c0\u67e5\u8868\u5728\u79bb\u7ebf\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9c81\u68d2\u6027\u3001\u4e0e\u4e34\u5e8a\u533b\u751f\u504f\u597d\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u4f5c\u4e3a\u8bc4\u4f30\u65b9\u6cd5\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2507.17718", "pdf": "https://arxiv.org/pdf/2507.17718", "abs": "https://arxiv.org/abs/2507.17718", "authors": ["Danny D. Leybzon", "Shreyas Tirumala", "Nishant Jain", "Summer Gillen", "Michael Jackson", "Cameron McPhee", "Jennifer Schmidt"], "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8eAI\u7684\u7535\u8bdd\u8c03\u67e5\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528LLM\u3001ASR\u548c\u8bed\u97f3\u5408\u6210\u6280\u672f\uff0c\u80fd\u591f\u63d0\u9ad8\u8c03\u67e5\u5b8c\u6210\u7387\u3001\u51cf\u5c11\u4e2d\u65ad\u7387\u5e76\u63d0\u5347\u53d7\u8bbf\u8005\u6ee1\u610f\u5ea6\u3002", "motivation": "\u968f\u7740\u8bed\u97f3\u542f\u7528\u7684\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7cfb\u7edf\u7684\u5174\u8d77\uff0c\u5b9a\u91cf\u8c03\u67e5\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u4f7f\u7528\u4e00\u79cd\u65b0\u7684\u6570\u636e\u6536\u96c6\u65b9\u5f0f\uff1aAI\u7535\u8bdd\u8c03\u67e5\u3002\u901a\u8fc7\u4f7f\u7528AI\u8fdb\u884c\u7535\u8bdd\u8bbf\u8c08\uff0c\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u5728\u4fdd\u6301\u4eba\u7c7b\u4e92\u52a8\u6027\u548c\u65b9\u6cd5\u4e25\u8c28\u6027\u7684\u540c\u65f6\u6269\u5c55\u5b9a\u91cf\u7814\u7a76\u3002", "method": "\u6784\u5efa\u5e76\u6d4b\u8bd5\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u548c\u8bed\u97f3\u5408\u6210\u6280\u672f\u7684AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u8fdb\u884c\u5b9a\u91cf\u8c03\u67e5\uff0c\u5e76\u9075\u5faa\u7814\u7a76\u6700\u4f73\u5b9e\u8df5\u5982\u95ee\u9898\u987a\u5e8f\u968f\u673a\u5316\u3001\u7b54\u6848\u987a\u5e8f\u968f\u673a\u5316\u548c\u7cbe\u786e\u63aa\u8f9e\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u77ed\u7684\u95ee\u5377\u548c\u66f4\u54cd\u5e94\u7684AI\u9762\u8bd5\u8005\u53ef\u80fd\u5728\u6240\u6709\u4e09\u4e2a\u7814\u7a76\u6307\u6807\u4e0a\u90fd\u6709\u6240\u6539\u5584\uff0c\u5305\u62ec\u8c03\u67e5\u5b8c\u6210\u7387\u3001\u4e2d\u65ad\u7387\u548c\u53d7\u8bbf\u8005\u6ee1\u610f\u5ea6\u3002", "conclusion": "AI\u7535\u8bdd\u8c03\u67e5\u7cfb\u7edf\u5728\u63d0\u9ad8\u8c03\u67e5\u5b8c\u6210\u7387\u3001\u51cf\u5c11\u4e2d\u65ad\u7387\u548c\u63d0\u9ad8\u53d7\u8bbf\u8005\u6ee1\u610f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u8f83\u77ed\u7684\u95ee\u5377\u548c\u66f4\u54cd\u5e94\u7684AI\u9762\u8bd5\u8005\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2507.17728", "pdf": "https://arxiv.org/pdf/2507.17728", "abs": "https://arxiv.org/abs/2507.17728", "authors": ["Boxun Li", "Yadong Li", "Zhiyuan Li", "Congyi Liu", "Weilin Liu", "Guowei Niu", "Zheyue Tan", "Haiyang Xu", "Zhuyu Yao", "Tao Yuan", "Dong Zhou", "Yueqing Zhuang", "Bo Zhao", "Guohao Dai", "Yu Wang"], "title": "Megrez2 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "We present Megrez2, a novel lightweight and high-performance language model\narchitecture optimized for device native deployment. Megrez2 introduces a novel\ncross-layer expert sharing mechanism, which significantly reduces total\nparameter count by reusing expert modules across adjacent transformer layers\nwhile maintaining most of the model's capacity. It also incorporates pre-gated\nrouting, enabling memory-efficient expert loading and faster inference. As the\nfirst instantiation of the Megrez2 architecture, we introduce the\nMegrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and\nfurther enhanced through supervised fine-tuning and reinforcement learning with\nverifiable rewards. With only 3B activated and 7.5B stored parameters,\nMegrez2-Preview demonstrates competitive or superior performance compared to\nlarger models on a wide range of tasks, including language understanding,\ninstruction following, mathematical reasoning, and code generation. These\nresults highlight the effectiveness of the Megrez2 architecture to achieve a\nbalance between accuracy, efficiency, and deployability, making it a strong\ncandidate for real-world, resource-constrained applications.", "AI": {"tldr": "Megrez2\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6027\u80fd\u7684\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u8de8\u5c42\u4e13\u5bb6\u5171\u4eab\u673a\u5236\u548c\u9884\u95e8\u63a7\u8def\u7531\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\uff0cMegrez2-Preview\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u8bbe\u5907\u539f\u751f\u90e8\u7f72\u7684\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6027\u80fd\u7684\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0cMegrez2\u88ab\u63d0\u51fa\u3002", "method": "Megrez2\u5f15\u5165\u4e86\u4e00\u79cd\u8de8\u5c42\u4e13\u5bb6\u5171\u4eab\u673a\u5236\uff0c\u901a\u8fc7\u5728\u76f8\u90bbTransformer\u5c42\u4e4b\u95f4\u590d\u7528\u4e13\u5bb6\u6a21\u5757\u6765\u663e\u8457\u51cf\u5c11\u603b\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u5927\u90e8\u5206\u80fd\u529b\u3002\u5b83\u8fd8\u7ed3\u5408\u4e86\u9884\u95e8\u63a7\u8def\u7531\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u7684\u4e13\u5bb6\u52a0\u8f7d\u548c\u66f4\u5feb\u7684\u63a8\u7406\u3002", "result": "Megrez2-Preview\u6a21\u578b\u5728\u5e7f\u6cdb\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u5305\u62ec\u8bed\u8a00\u7406\u89e3\u3001\u6307\u4ee4\u9075\u5faa\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u3002", "conclusion": "Megrez2\u67b6\u6784\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u90e8\u7f72\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4f7f\u5176\u6210\u4e3a\u8d44\u6e90\u53d7\u9650\u5e94\u7528\u7684\u6709\u529b\u5019\u9009\u3002"}}
{"id": "2507.17747", "pdf": "https://arxiv.org/pdf/2507.17747", "abs": "https://arxiv.org/abs/2507.17747", "authors": ["Linbo Cao", "Jinman Zhao"], "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 7 figures. Accepted to COLM 2025. Code available at:\n  github.com/l6cao/Debate-Driven-Evaluation", "summary": "As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fa9\u8bba\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u8f6e\u8bba\u8bc1\u589e\u52a0\u96be\u5ea6\u5e76\u60e9\u7f5a\u6d45\u5c42\u8bb0\u5fc6\uff0c\u540c\u65f6\u91cd\u7528QA\u9879\u76ee\u4ee5\u51cf\u5c11\u6574\u7406\u5de5\u4f5c\u91cf\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u4e24\u4e2a\u4e3b\u8981\u8d21\u732e\uff1a(1) \u4e00\u79cd\u7cfb\u7edf\u5730\u5c06QA\u4efb\u52a1\u8f6c\u6362\u4e3a\u57fa\u4e8e\u8fa9\u8bba\u7684\u8bc4\u4f30\u7684\u7ba1\u9053\uff0c\u4ee5\u53ca(2) \u4e00\u4e2a\u516c\u5171\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u6211\u4eec\u7684\u8303\u5f0f\u5728MMLU-Pro\u95ee\u9898\u5b50\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u6807\u51c6\u5316\u534f\u8bae\u548c\u53c2\u8003\u6a21\u578b\u3002\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u548c\u5bf9\u6297\u6570\u636e\u6c61\u67d3\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u65e5\u76ca\u5145\u65a5\u6807\u51c6QA\u57fa\u51c6\uff0c\u5173\u4e8e\u6570\u636e\u6c61\u67d3\u3001\u8bb0\u5fc6\u548c\u4e0d\u65ad\u4e0a\u5347\u7684\u6570\u636e\u96c6\u521b\u5efa\u6210\u672c\u7684\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8fa9\u8bba\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u5c06\u4efb\u4f55\u73b0\u6709\u7684QA\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u5bf9\u6297\u6027\u8fa9\u8bba\uff0c\u5176\u4e2d\u4e00\u53f0\u6a21\u578b\u88ab\u7ed9\u4e88\u5b98\u65b9\u7b54\u6848\u8fdb\u884c\u8fa9\u62a4\uff0c\u53e6\u4e00\u53f0\u6784\u5efa\u5e76\u634d\u536b\u4e00\u4e2a\u66ff\u4ee3\u7b54\u6848\uff0c\u7531\u5bf9\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u4e00\u65e0\u6240\u77e5\u7684\u88c1\u5224\u6a21\u578b\u8fdb\u884c\u88c1\u51b3\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u548c\u5bf9\u6297\u6570\u636e\u6c61\u67d3\u7684\u6709\u6548\u6027\u2014\u2014\u5728\u6d4b\u8bd5\u95ee\u9898\u4e0a\u5fae\u8c03\u7684Llama 3.1\u6a21\u578b\u663e\u793a\u51fa\u663e\u8457\u7684\u51c6\u786e\u6027\u63d0\u9ad8\uff0850% -> 82%\uff09\uff0c\u4f46\u5728\u8fa9\u8bba\u4e2d\u8868\u73b0\u66f4\u5dee\u3002\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u5373\u4f7f\u8f83\u5f31\u7684\u88c1\u5224\u4e5f\u80fd\u53ef\u9760\u5730\u533a\u5206\u66f4\u5f3a\u7684\u8fa9\u624b\uff0c\u8fd9\u8868\u660e\u57fa\u4e8e\u8fa9\u8bba\u7684\u8bc4\u4f30\u53ef\u4ee5\u6269\u5c55\u5230\u672a\u6765\u66f4\u5f3a\u5927\u7684\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u521b\u5efa\u65b0\u57fa\u51c6\u7684\u4e00\u5c0f\u90e8\u5206\u6210\u672c\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u5f3a\u8c03\u4e86'\u5728\u6d4b\u8bd5\u96c6\u4e0a\u9884\u8bad\u7ec3\u4e0d\u518d\u662f\u4f60\u9700\u8981\u7684\u4e00\u5207'\uff0c\u4e3a\u8861\u91cf\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u7684\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6301\u7eed\u7684\u8def\u5f84\u3002"}}
{"id": "2507.16820", "pdf": "https://arxiv.org/pdf/2507.16820", "abs": "https://arxiv.org/abs/2507.16820", "authors": ["Ngan Tran", "Haihua Chen", "Ana Cleveland", "Yuhan Zhou"], "title": "Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.DL"], "comment": "36 pages, 14 figures, 5 tables", "summary": "This study presents a comprehensive bibliometric and topic analysis of the\ndisaster informatics literature published between January 2020 to September\n2022. Leveraging a large-scale corpus and advanced techniques such as\npre-trained language models and generative AI, we identify the most active\ncountries, institutions, authors, collaboration networks, emergent topics,\npatterns among the most significant topics, and shifts in research priorities\nspurred by the COVID-19 pandemic. Our findings highlight (1) countries that\nwere most impacted by the COVID-19 pandemic were also among the most active,\nwith each country having specific research interests, (2) countries and\ninstitutions within the same region or share a common language tend to\ncollaborate, (3) top active authors tend to form close partnerships with one or\ntwo key partners, (4) authors typically specialized in one or two specific\ntopics, while institutions had more diverse interests across several topics,\nand (5) the COVID-19 pandemic has influenced research priorities in disaster\ninformatics, placing greater emphasis on public health. We further demonstrate\nthat the field is converging on multidimensional resilience strategies and\ncross-sectoral data-sharing collaborations or projects, reflecting a heightened\nawareness of global vulnerability and interdependency. Collecting and quality\nassurance strategies, data analytic practices, LLM-based topic extraction and\nsummarization approaches, and result visualization tools can be applied to\ncomparable datasets or solve similar analytic problems. By mapping out the\ntrends in disaster informatics, our analysis offers strategic insights for\npolicymakers, practitioners, and scholars aiming to enhance disaster\ninformatics capacities in an increasingly uncertain and complex risk landscape.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8ba1\u91cf\u5206\u6790\u548c\u4e3b\u9898\u5206\u6790\uff0c\u63ed\u793a\u4e86\u707e\u5bb3\u4fe1\u606f\u5b66\u9886\u57df\u57282020\u5e741\u6708\u81f32022\u5e749\u6708\u671f\u95f4\u7684\u6587\u732e\u8d8b\u52bf\uff0c\u5305\u62ec\u6d3b\u8dc3\u56fd\u5bb6\u3001\u673a\u6784\u3001\u4f5c\u8005\u3001\u5408\u4f5c\u7f51\u7edc\u3001\u65b0\u5174\u4e3b\u9898\u4ee5\u53ca\u7531\u4e8e\u65b0\u51a0\u75ab\u60c5\u5f15\u53d1\u7684\u7814\u7a76\u91cd\u70b9\u53d8\u5316\u3002", "motivation": "\u4e3a\u4e86\u8bc6\u522b\u707e\u5bb3\u4fe1\u606f\u5b66\u6587\u732e\u4e2d\u7684\u6d3b\u8dc3\u56fd\u5bb6\u3001\u673a\u6784\u3001\u4f5c\u8005\u3001\u5408\u4f5c\u7f51\u7edc\u3001\u65b0\u5174\u4e3b\u9898\u4ee5\u53ca\u7531\u4e8e\u65b0\u51a0\u75ab\u60c5\u800c\u5f15\u53d1\u7684\u7814\u7a76\u91cd\u70b9\u53d8\u5316\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u548c\u5148\u8fdb\u7684\u6280\u672f\uff0c\u5982\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u751f\u6210\u5f0fAI\uff0c\u8fdb\u884c\u6587\u732e\u7684\u8ba1\u91cf\u5206\u6790\u548c\u4e3b\u9898\u5206\u6790\u3002", "result": "\u53d1\u73b0\u75ab\u60c5\u6700\u4e25\u91cd\u7684\u56fd\u5bb6\u4e5f\u662f\u6700\u6d3b\u8dc3\u7684\uff0c\u540c\u4e00\u5730\u533a\u6216\u6709\u5171\u540c\u8bed\u8a00\u7684\u56fd\u5bb6\u548c\u673a\u6784\u503e\u5411\u4e8e\u5408\u4f5c\uff0c\u9876\u7ea7\u6d3b\u8dc3\u4f5c\u8005\u901a\u5e38\u4e0e\u4e00\u4e24\u4e2a\u5173\u952e\u5408\u4f5c\u4f19\u4f34\u5f62\u6210\u7d27\u5bc6\u5173\u7cfb\uff0c\u4f5c\u8005\u901a\u5e38\u4e13\u6ce8\u4e8e\u4e00\u4e24\u4e2a\u7279\u5b9a\u4e3b\u9898\uff0c\u800c\u673a\u6784\u5219\u5728\u591a\u4e2a\u4e3b\u9898\u4e0a\u6709\u66f4\u5e7f\u6cdb\u7684\u5174\u8da3\uff0c\u5e76\u4e14\u65b0\u51a0\u75ab\u60c5\u5f71\u54cd\u4e86\u707e\u5bb3\u4fe1\u606f\u5b66\u7684\u7814\u7a76\u91cd\u70b9\uff0c\u66f4\u52a0\u91cd\u89c6\u516c\u5171\u536b\u751f\u3002", "conclusion": "\u6211\u4eec\u7684\u5206\u6790\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u3001\u4ece\u4e1a\u8005\u548c\u5b66\u8005\u63d0\u4f9b\u4e86\u6218\u7565\u89c1\u89e3\uff0c\u65e8\u5728\u589e\u5f3a\u5728\u65e5\u76ca\u4e0d\u786e\u5b9a\u548c\u590d\u6742\u7684\u98ce\u9669\u73af\u5883\u4e2d\u7684\u707e\u5bb3\u4fe1\u606f\u5b66\u80fd\u529b\u3002"}}
{"id": "2507.16826", "pdf": "https://arxiv.org/pdf/2507.16826", "abs": "https://arxiv.org/abs/2507.16826", "authors": ["Qikai Wei", "Huansheng Ning", "Chunlong Han", "Jianguo Ding"], "title": "A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) has gradually emerged as a promising\nparadigm for enhancing the accuracy and factual consistency of content\ngenerated by large language models (LLMs). However, existing RAG studies\nprimarily focus on retrieving isolated segments using similarity-based matching\nmethods, while overlooking the intrinsic connections between them. This\nlimitation hampers performance in RAG tasks. To address this, we propose QMKGF,\na Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing\nRetrieval Augmented Generation. First, we design prompt templates and employ\ngeneral-purpose LLMs to extract entities and relations, thereby generating a\nknowledge graph (KG) efficiently. Based on the constructed KG, we introduce a\nmulti-path subgraph construction strategy that incorporates one-hop relations,\nmulti-hop relations, and importance-based relations, aiming to improve the\nsemantic relevance between the retrieved documents and the user query.\nSubsequently, we designed a query-aware attention reward model that scores\nsubgraph triples based on their semantic relevance to the query. Then, we\nselect the highest score subgraph and enrich subgraph with additional triples\nfrom other subgraphs that are highly semantically relevant to the query.\nFinally, the entities, relations, and triples within the updated subgraph are\nutilised to expand the original query, thereby enhancing its semantic\nrepresentation and improving the quality of LLMs' generation. We evaluate QMKGF\non the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA\ndataset, our method achieves a ROUGE-1 score of 64.98\\%, surpassing the\nBGE-Rerank approach by 9.72 percentage points (from 55.26\\% to 64.98\\%).\nExperimental results demonstrate the effectiveness and superiority of the QMKGF\napproach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u67e5\u8be2\u611f\u77e5\u591a\u8def\u5f84\u77e5\u8bc6\u56fe\u8c31\u878d\u5408\u65b9\u6cd5\uff08QMKGF\uff09\uff0c\u4ee5\u63d0\u9ad8\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u6027\u80fd\u3002\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u5e76\u91c7\u7528\u591a\u8def\u5f84\u5b50\u56fe\u6784\u9020\u7b56\u7565\uff0c\u4ee5\u53ca\u67e5\u8be2\u611f\u77e5\u6ce8\u610f\u529b\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u76f8\u5173\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709RAG\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4f7f\u7528\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u5339\u914d\u65b9\u6cd5\u68c0\u7d22\u5b64\u7acb\u6bb5\u843d\uff0c\u800c\u5ffd\u89c6\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\u3002\u8fd9\u79cd\u9650\u5236\u963b\u788d\u4e86RAG\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u63d0\u793a\u6a21\u677f\u5e76\u4f7f\u7528\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u5b9e\u4f53\u548c\u5173\u7cfb\uff0c\u4ece\u800c\u9ad8\u6548\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u3002\u57fa\u4e8e\u6784\u5efa\u7684KG\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u8def\u5f84\u5b50\u56fe\u6784\u9020\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u4e00\u8df3\u5173\u7cfb\u3001\u591a\u8df3\u5173\u7cfb\u548c\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u5173\u7cfb\uff0c\u65e8\u5728\u63d0\u9ad8\u68c0\u7d22\u6587\u6863\u4e0e\u7528\u6237\u67e5\u8be2\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u5173\u6027\u3002\u968f\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u67e5\u8be2\u611f\u77e5\u6ce8\u610f\u529b\u5956\u52b1\u6a21\u578b\uff0c\u6839\u636e\u5176\u4e0e\u67e5\u8be2\u7684\u8bed\u4e49\u76f8\u5173\u6027\u5bf9\u5b50\u56fe\u4e09\u5143\u7ec4\u8fdb\u884c\u8bc4\u5206\u3002\u7136\u540e\uff0c\u6211\u4eec\u9009\u62e9\u6700\u9ad8\u5f97\u5206\u7684\u5b50\u56fe\uff0c\u5e76\u4ece\u4e0e\u5176\u4ed6\u9ad8\u5ea6\u8bed\u4e49\u76f8\u5173\u7684\u5b50\u56fe\u4e2d\u6dfb\u52a0\u989d\u5916\u7684\u4e09\u5143\u7ec4\u3002\u6700\u540e\uff0c\u5229\u7528\u66f4\u65b0\u540e\u7684\u5b50\u56fe\u4e2d\u7684\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u4e09\u5143\u7ec4\u6269\u5c55\u539f\u59cb\u67e5\u8be2\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u8bed\u4e49\u8868\u793a\u5e76\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728HotpotQA\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e8664.98%\u7684ROUGE-1\u5206\u6570\uff0c\u8d85\u8fc7\u4e86BGE-Rerank\u65b9\u6cd59.72\u4e2a\u767e\u5206\u70b9\uff08\u4ece55.26%\u523064.98%\uff09\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eQMKGF\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.16834", "pdf": "https://arxiv.org/pdf/2507.16834", "abs": "https://arxiv.org/abs/2507.16834", "authors": ["Jordan Madden", "Matthew Stone", "Dimitri Johnson", "Daniel Geddez"], "title": "Towards Robust Speech Recognition for Jamaican Patois Music Transcription", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Although Jamaican Patois is a widely spoken language, current speech\nrecognition systems perform poorly on Patois music, producing inaccurate\ncaptions that limit accessibility and hinder downstream applications. In this\nwork, we take a data-centric approach to this problem by curating more than 40\nhours of manually transcribed Patois music. We use this dataset to fine-tune\nstate-of-the-art automatic speech recognition (ASR) models, and use the results\nto develop scaling laws for the performance of Whisper models on Jamaican\nPatois audio. We hope that this work will have a positive impact on the\naccessibility of Jamaican Patois music and the future of Jamaican Patois\nlanguage modeling.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u7406\u8d85\u8fc740\u5c0f\u65f6\u7684\u624b\u52a8\u8f6c\u5f55\u7684\u7259\u4e70\u52a0\u5e15\u6258\u65af\u97f3\u4e50\u6570\u636e\u96c6\uff0c\u5fae\u8c03\u5148\u8fdb\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u7259\u4e70\u52a0\u5e15\u6258\u65af\u97f3\u4e50\u7684\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u63a2\u7d22Whisper\u6a21\u578b\u5728\u8be5\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u6269\u5c55\u5b9a\u5f8b\u3002", "motivation": "\u5c3d\u7ba1\u7259\u4e70\u52a0\u5e15\u6258\u65af\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bed\u8a00\uff0c\u4f46\u5f53\u524d\u7684\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u5e15\u6258\u65af\u97f3\u4e50\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u4ea7\u751f\u7684\u4e0d\u51c6\u786e\u5b57\u5e55\u9650\u5236\u4e86\u53ef\u8bbf\u95ee\u6027\u5e76\u963b\u788d\u4e86\u4e0b\u6e38\u5e94\u7528\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u6574\u7406\u8d85\u8fc740\u5c0f\u65f6\u7684\u624b\u52a8\u8f6c\u5f55\u7684\u7259\u4e70\u52a0\u5e15\u6258\u65af\u97f3\u4e50\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e2a\u6570\u636e\u96c6\u5fae\u8c03\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\uff0c\u4ee5\u5f00\u53d1Whisper\u6a21\u578b\u5728\u7259\u4e70\u52a0\u5e15\u6258\u65af\u97f3\u9891\u4e0a\u7684\u6027\u80fd\u6269\u5c55\u5b9a\u5f8b\u3002", "result": "\u6211\u4eec\u6574\u7406\u4e86\u8d85\u8fc740\u5c0f\u65f6\u7684\u624b\u52a8\u8f6c\u5f55\u7684\u7259\u4e70\u52a0\u5e15\u6258\u65af\u97f3\u4e50\u6570\u636e\u96c6\uff0c\u5e76\u7528\u5b83\u6765\u5fae\u8c03\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\uff0c\u4ee5\u5f00\u53d1Whisper\u6a21\u578b\u5728\u7259\u4e70\u52a0\u5e15\u6258\u65af\u97f3\u9891\u4e0a\u7684\u6027\u80fd\u6269\u5c55\u5b9a\u5f8b\u3002", "conclusion": "\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u5bf9\u7259\u4e70\u52a0\u5e15\u6258\u65af\u97f3\u4e50\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u7259\u4e70\u52a0\u5e15\u6258\u65af\u8bed\u8a00\u5efa\u6a21\u7684\u672a\u6765\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2507.16835", "pdf": "https://arxiv.org/pdf/2507.16835", "abs": "https://arxiv.org/abs/2507.16835", "authors": ["Nima Yazdani", "Ali Ansari", "Aruj Mahajan", "Amirhossein Afsharrad", "Seyed Shahabeddin Mousavi"], "title": "Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI Interview Systems", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Voice-based conversational AI systems increasingly rely on cascaded\narchitectures combining speech-to-text (STT), large language models (LLMs), and\ntext-to-speech (TTS) components. However, systematic evaluation of different\ncomponent combinations in production settings remains understudied. We present\na large-scale empirical comparison of STT x LLM x TTS stacks using data from\nover 300,000 AI-conducted job interviews. We develop an automated evaluation\nframework using LLM-as-a-Judge to assess conversational quality, technical\naccuracy, and skill assessment capabilities. Our analysis of four production\nconfigurations reveals that Google STT paired with GPT-4.1 significantly\noutperforms alternatives in both conversational and technical quality metrics.\nSurprisingly, we find that objective quality metrics correlate weakly with user\nsatisfaction scores, suggesting that user experience in voice-based AI systems\ndepends on factors beyond technical performance. Our findings provide practical\nguidance for selecting components in multimodal conversational AI systems and\ncontribute a validated evaluation methodology for voice-based interactions.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u6bd4\u8f83\u4e86STT x LLM x TTS\u5806\u6808\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u88c1\u5224\u6765\u8bc4\u4f30\u5bf9\u8bdd\u8d28\u91cf\u3001\u6280\u672f\u51c6\u786e\u6027\u548c\u6280\u80fd\u8bc4\u4f30\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0cGoogle STT\u4e0eGPT-4.1\u5728\u5bf9\u8bdd\u8d28\u91cf\u548c\u4e13\u4e1a\u6280\u672f\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5ba2\u89c2\u8d28\u91cf\u6307\u6807\u4e0e\u7528\u6237\u6ee1\u610f\u5ea6\u8bc4\u5206\u7684\u76f8\u5173\u6027\u8f83\u5f31\u3002", "motivation": "\u8bed\u97f3\u57fa\u7840\u7684\u5bf9\u8bddAI\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u4e8e\u7ed3\u5408\u8bed\u97f3\u5230\u6587\u672c\uff08STT\uff09\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u7ec4\u4ef6\u7684\u7ea7\u8054\u67b6\u6784\u3002\u7136\u800c\uff0c\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5bf9\u4e0d\u540c\u7ec4\u4ef6\u7ec4\u5408\u7684\u7cfb\u7edf\u8bc4\u4f30\u4ecd\u7136\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u88c1\u5224\u6765\u8bc4\u4f30\u5bf9\u8bdd\u8d28\u91cf\u3001\u6280\u672f\u51c6\u786e\u6027\u548c\u6280\u80fd\u8bc4\u4f30\u80fd\u529b\u3002", "result": "\u6211\u4eec\u7684\u5206\u6790\u663e\u793a\uff0cGoogle STT\u4e0eGPT-4.1\u5728\u5bf9\u8bdd\u8d28\u91cf\u548c\u4e13\u4e1a\u6280\u672f\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u9009\u9879\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u5ba2\u89c2\u8d28\u91cf\u6307\u6807\u4e0e\u7528\u6237\u6ee1\u610f\u5ea6\u8bc4\u5206\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u8fd9\u8868\u660e\u8bed\u97f3\u57fa\u7840\u7684AI\u7cfb\u7edf\u7684\u7528\u6237\u4f53\u9a8c\u53d6\u51b3\u4e8e\u6280\u672f\u6027\u80fd\u4e4b\u5916\u7684\u56e0\u7d20\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u4e3a\u9009\u62e9\u591a\u6a21\u6001\u5bf9\u8bdd\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7ec4\u4ef6\u63d0\u4f9b\u4e86\u5b9e\u9645\u6307\u5bfc\uff0c\u5e76\u4e3a\u8bed\u97f3\u4ea4\u4e92\u8d21\u732e\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2507.16838", "pdf": "https://arxiv.org/pdf/2507.16838", "abs": "https://arxiv.org/abs/2507.16838", "authors": ["Xinwei Cao", "Zijian Fan", "Torbj\u00f8rn Svendsen", "Giampiero Salvi"], "title": "Segmentation-free Goodness of Pronunciation", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Mispronunciation detection and diagnosis (MDD) is a significant part in\nmodern computer aided language learning (CALL) systems. Within MDD,\nphoneme-level pronunciation assessment is key to helping L2 learners improve\ntheir pronunciation. However, most systems are based on a form of goodness of\npronunciation (GOP) which requires pre-segmentation of speech into phonetic\nunits. This limits the accuracy of these methods and the possibility to use\nmodern CTC-based acoustic models for their evaluation. In this study, we first\npropose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR\nmodels for MDD. Next, we define a more general alignment-free method that takes\nall possible alignments of the target phoneme into account (GOP-AF). We give a\ntheoretical account of our definition of GOP-AF, an implementation that solves\npotential numerical issues as well as a proper normalization which makes the\nmethod applicable with acoustic models with different peakiness over time. We\nprovide extensive experimental results on the CMU Kids and Speechocean762\ndatasets comparing the different definitions of our methods, estimating the\ndependency of GOP-AF on the peakiness of the acoustic models and on the amount\nof context around the target phoneme. Finally, we compare our methods with\nrecent studies over the Speechocean762 data showing that the feature vectors\nderived from the proposed method achieve state-of-the-art results on\nphoneme-level pronunciation assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u53d1\u97f3\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5229\u7528\u4e86CTC\u8bad\u7ec3\u7684\u58f0\u5b66\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684MDD\u7cfb\u7edf\u57fa\u4e8e\u4e00\u79cd\u79f0\u4e3a\u53d1\u97f3\u8d28\u91cf\uff08GOP\uff09\u7684\u65b9\u6cd5\uff0c\u8fd9\u9700\u8981\u5c06\u8bed\u97f3\u5206\u5272\u6210\u8bed\u97f3\u5355\u5143\uff0c\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u4ee5\u53ca\u4f7f\u7528\u73b0\u4ee3CTC-based\u58f0\u5b66\u6a21\u578b\u7684\u53ef\u80fd\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u5bf9\u9f50GOP\uff08GOP-SA\uff09\u548c\u65e0\u5bf9\u9f50\u65b9\u6cd5\uff08GOP-AF\uff09\uff0c\u4ee5\u5229\u7528CTC\u8bad\u7ec3\u7684ASR\u6a21\u578b\u8fdb\u884cMDD\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u5728CMU Kids\u548cSpeechocean762\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u5b9a\u4e49\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eCTC\u7684\u58f0\u5b66\u6a21\u578b\u65b9\u6cd5\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2507.16863", "pdf": "https://arxiv.org/pdf/2507.16863", "abs": "https://arxiv.org/abs/2507.16863", "authors": ["Hongcheng Gao", "Zihao Huang", "Lin Xu", "Jingyi Tang", "Xinhao Li", "Yue Liu", "Haoyang Li", "Taihang Hu", "Minhua Lin", "Xinlong Yang", "Ge Wu", "Balong Bi", "Hongyu Chen", "Wentao Zhang"], "title": "Pixels, Patterns, but No Poetry: To See The World like Humans", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Achieving human-like perception and reasoning in Multimodal Large Language\nModels (MLLMs) remains a central challenge in artificial intelligence. While\nrecent research has primarily focused on enhancing reasoning capabilities in\nMLLMs, a fundamental question persists: Can Multimodal Large Language Models\ntruly perceive the world as humans do? This paper shifts focus from reasoning\nto perception. Rather than constructing benchmarks specifically for reasoning,\nwe introduce the Turing Eye Test (TET), a challenging perception-oriented\nbenchmark comprising four diagnostic tasks that evaluate MLLMs' performance on\nsynthetic images that humans process intuitively. Our findings reveal that\nstate-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks\ntrivial for humans. Both in-context learning and training on language\nbackbone-effective for previous benchmarks-fail to improve performance on our\ntasks, while fine-tuning the vision tower enables rapid adaptation, suggesting\nthat our benchmark poses challenges for vision tower generalization rather than\nfor the knowledge and reasoning capabilities of the language backbone-a key gap\nbetween current MLLMs and human perception. We release a representative subset\nof TET tasks in this version, and will introduce more diverse tasks and methods\nto enhance visual generalization in future work.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aTuring Eye Test\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30MLLMs\u5728\u611f\u77e5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u89c6\u89c9\u5854\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u63d0\u9ad8MLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u4ecd\u7136\u5b58\u5728\uff1aMLLMs\u80fd\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u611f\u77e5\u4e16\u754c\uff1f", "method": "\u5f15\u5165\u4e86Turing Eye Test (TET)\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u611f\u77e5\u4e3a\u5bfc\u5411\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u56db\u4e2a\u8bca\u65ad\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30MLLMs\u5728\u5408\u6210\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6700\u5148\u8fdb\u7684MLLMs\u5728\u6211\u4eec\u8bbe\u8ba1\u7684\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u707e\u96be\u6027\u7684\u5931\u8d25\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u5bf9\u4eba\u7c7b\u6765\u8bf4\u662f\u5fae\u4e0d\u8db3\u9053\u7684\u3002", "conclusion": "\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u89c6\u89c9\u5854\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u5173\u952e\u5dee\u8ddd\uff0c\u800c\u4e0d\u662f\u8bed\u8a00\u4e3b\u5e72\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.16877", "pdf": "https://arxiv.org/pdf/2507.16877", "abs": "https://arxiv.org/abs/2507.16877", "authors": ["Yizhi Hu", "Zezhao Tian", "Xingqun Qi", "Chen Su", "Bingkun Yang", "Junhui Yin", "Muyi Sun", "Man Zhang", "Zhenan Sun"], "title": "ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "15 pages, 7 figures", "summary": "Referring Expression Comprehension (REC) aims to localize specified entities\nor regions in an image based on natural language descriptions. While existing\nmethods handle single-entity localization, they often ignore complex\ninter-entity relationships in multi-entity scenes, limiting their accuracy and\nreliability. Additionally, the lack of high-quality datasets with fine-grained,\npaired image-text-relation annotations hinders further progress. To address\nthis challenge, we first construct a relation-aware, multi-entity REC dataset\ncalled ReMeX, which includes detailed relationship and textual annotations. We\nthen propose ReMeREC, a novel framework that jointly leverages visual and\ntextual cues to localize multiple entities while modeling their\ninter-relations. To address the semantic ambiguity caused by implicit entity\nboundaries in language, we introduce the Text-adaptive Multi-entity Perceptron\n(TMP), which dynamically infers both the quantity and span of entities from\nfine-grained textual cues, producing distinctive representations. Additionally,\nour Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and\nglobal scene understanding. To further improve language comprehension for\nfine-grained prompts, we also construct a small-scale auxiliary dataset,\nEntityText, generated using large language models. Experiments on four\nbenchmark datasets show that ReMeREC achieves state-of-the-art performance in\nmulti-entity grounding and relation prediction, outperforming existing\napproaches by a large margin.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ReMeREC\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u4ee5\u53ca\u5f15\u5165TMP\u548cEIR\u6765\u63d0\u9ad8\u591a\u5b9e\u4f53\u5b9a\u4f4d\u548c\u5173\u7cfb\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5904\u7406\u591a\u5b9e\u4f53\u573a\u666f\u4e2d\u7684\u590d\u6742\u4e92\u5b9e\u4f53\u5173\u7cfb\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u540c\u65f6\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6ReMeREC\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u6765\u5b9a\u4f4d\u591a\u4e2a\u5b9e\u4f53\u5e76\u5efa\u6a21\u5b83\u4eec\u7684\u76f8\u4e92\u5173\u7cfb\u3002\u5f15\u5165\u4e86Text-adaptive Multi-entity Perceptron (TMP)\u548cEntity Inter-relationship Reasoner (EIR)\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReMeREC\u5728\u591a\u5b9e\u4f53\u5b9a\u4f4d\u548c\u5173\u7cfb\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ReMeREC\u5728\u591a\u5b9e\u4f53\u5b9a\u4f4d\u548c\u5173\u7cfb\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.16933", "pdf": "https://arxiv.org/pdf/2507.16933", "abs": "https://arxiv.org/abs/2507.16933", "authors": ["Steven K. Esser", "Jeffrey L. McKinstry", "Deepika Bablani", "Rathinakumar Appuswamy", "Dharmendra S. Modha"], "title": "SiLQ: Simple Large Language Model Quantization-Aware Training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "12 pages, 3 figures", "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6781\u5c0f\u7684\u8bad\u7ec3\u9884\u7b97\u589e\u52a0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u91cf\u5316\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u5ef6\u8fdf\u3001\u6a21\u578b\u5927\u5c0f\u548c\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u91cf\u5316\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u517c\u5bb9\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u91cf\u5316\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u5728\u91cf\u5316\u540e\u4ecd\u80fd\u4fdd\u6301\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u73b0\u4ee3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u8bba\u662f\u57fa\u7840\u6a21\u578b\u8fd8\u662f\u6307\u4ee4\u6a21\u578b\u53d8\u4f53\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u7aef\u5230\u7aef\u7684\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u989d\u5916\u64cd\u4f5c\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u6781\u5c0f\u7684\u8bad\u7ec3\u9884\u7b97\u589e\u52a0\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u91cf\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.17232", "pdf": "https://arxiv.org/pdf/2507.17232", "abs": "https://arxiv.org/abs/2507.17232", "authors": ["Mashiro Toyooka", "Kiyoharu Aizawa", "Yoko Yamakata"], "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task", "categories": ["cs.MM", "cs.AI", "cs.CL"], "comment": "Accepted to ACM Multimedia 2025", "summary": "Large Language Models (LLMs) are trained on a vast amount of procedural\ntexts, but they do not directly observe real-world phenomena. In the context of\ncooking recipes, this poses a challenge, as intermediate states of ingredients\nare often omitted, making it difficult for models to track ingredient states\nand understand recipes accurately. In this paper, we apply state probing, a\nmethod for evaluating a language model's understanding of the world, to the\ndomain of cooking. We propose a new task and dataset for evaluating how well\nLLMs can recognize intermediate ingredient states during cooking procedures. We\nfirst construct a new Japanese recipe dataset with clear and accurate\nannotations of ingredient state changes, collected from well-structured and\ncontrolled recipe texts. Using this dataset, we design three novel tasks to\nevaluate whether LLMs can track ingredient state transitions and identify\ningredients present at intermediate steps. Our experiments with widely used\nLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state\nknowledge improves their understanding of cooking processes, achieving\nperformance comparable to commercial LLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u72b6\u6001\u63a2\u6d4b\u65b9\u6cd5\u8bc4\u4f30LLMs\u5bf9\u70f9\u996a\u8fc7\u7a0b\u4e2d\u98df\u6750\u72b6\u6001\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\u548c\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u4e60\u98df\u6750\u72b6\u6001\u77e5\u8bc6\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLMs\u5bf9\u70f9\u996a\u8fc7\u7a0b\u7684\u7406\u89e3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bad\u7ec3\u65f6\u6ca1\u6709\u76f4\u63a5\u89c2\u5bdf\u73b0\u5b9e\u4e16\u754c\u73b0\u8c61\uff0c\u8fd9\u5728\u70f9\u996a\u98df\u8c31\u4e2d\u5c24\u5176\u6210\u95ee\u9898\uff0c\u56e0\u4e3a\u4e2d\u95f4\u72b6\u6001\u7684\u98df\u6750\u5e38\u5e38\u88ab\u7701\u7565\uff0c\u4f7f\u5f97\u6a21\u578b\u96be\u4ee5\u8ddf\u8e2a\u98df\u6750\u72b6\u6001\u5e76\u51c6\u786e\u7406\u89e3\u98df\u8c31\u3002", "method": "\u5e94\u7528\u72b6\u6001\u63a2\u6d4b\u65b9\u6cd5\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5bf9\u4e16\u754c\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u4efb\u52a1\u548c\u6570\u636e\u96c6\u6765\u8bc4\u4f30LLMs\u8bc6\u522b\u70f9\u996a\u8fc7\u7a0b\u4e2d\u4e2d\u95f4\u98df\u6750\u72b6\u6001\u7684\u80fd\u529b\u3002", "result": "\u4f7f\u7528\u5e7f\u6cdb\u4f7f\u7528\u7684LLMs\uff08\u5982Llama3.1-70B\u548cQwen2.5-72B\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u5b66\u4e60\u98df\u6750\u72b6\u6001\u77e5\u8bc6\u53ef\u4ee5\u63d0\u9ad8\u5b83\u4eec\u5bf9\u70f9\u996a\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u6027\u80fd\u63a5\u8fd1\u5546\u4e1aLLMs\u3002", "conclusion": "\u5b66\u4e60\u98df\u6750\u72b6\u6001\u77e5\u8bc6\u53ef\u4ee5\u63d0\u9ad8LLMs\u5bf9\u70f9\u996a\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u4f7f\u5176\u6027\u80fd\u8fbe\u5230\u5546\u4e1aLLMs\u7684\u6c34\u5e73\u3002"}}
{"id": "2507.17259", "pdf": "https://arxiv.org/pdf/2507.17259", "abs": "https://arxiv.org/abs/2507.17259", "authors": ["Eyal German", "Sagiv Antebi", "Daniel Samira", "Asaf Shabtai", "Yuval Elovici"], "title": "Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly trained on tabular data, which,\nunlike unstructured text, often contains personally identifiable information\n(PII) in a highly structured and explicit format. As a result, privacy risks\narise, since sensitive records can be inadvertently retained by the model and\nexposed through data extraction or membership inference attacks (MIAs). While\nexisting MIA methods primarily target textual content, their efficacy and\nthreat implications may differ when applied to structured data, due to its\nlimited content, diverse data types, unique value distributions, and\ncolumn-level semantics. In this paper, we present Tab-MIA, a benchmark dataset\nfor evaluating MIAs on tabular data in LLMs and demonstrate how it can be used.\nTab-MIA comprises five data collections, each represented in six different\nencoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation\nof state-of-the-art MIA methods on LLMs finetuned with tabular data across\nmultiple encoding formats. In the evaluation, we analyze the memorization\nbehavior of pretrained LLMs on structured data derived from Wikipedia tables.\nOur findings show that LLMs memorize tabular data in ways that vary across\nencoding formats, making them susceptible to extraction via MIAs. Even when\nfine-tuned for as few as three epochs, models exhibit high vulnerability, with\nAUROC scores approaching 90% in most cases. Tab-MIA enables systematic\nevaluation of these risks and provides a foundation for developing\nprivacy-preserving methods for tabular data in LLMs.", "AI": {"tldr": "This paper introduces Tab-MIA, a benchmark dataset for evaluating membership inference attacks (MIAs) on tabular data in large language models (LLMs). The study shows that LLMs can memorize tabular data in different encoding formats, making them vulnerable to MIAs, even after minimal fine-tuning.", "motivation": "Privacy risks arise since sensitive records can be inadvertently retained by the model and exposed through data extraction or membership inference attacks (MIAs). Existing MIA methods primarily target textual content, but their efficacy and threat implications may differ when applied to structured data.", "method": "We present Tab-MIA, a benchmark dataset for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used. Using our Tab-MIA benchmark, we conduct the first evaluation of state-of-the-art MIA methods on LLMs finetuned with tabular data across multiple encoding formats.", "result": "LLMs memorize tabular data in ways that vary across encoding formats, making them susceptible to extraction via MIAs. Even when fine-tuned for as few as three epochs, models exhibit high vulnerability, with AUROC scores approaching 90% in most cases.", "conclusion": "Tab-MIA enables systematic evaluation of these risks and provides a foundation for developing privacy-preserving methods for tabular data in LLMs."}}
{"id": "2507.17335", "pdf": "https://arxiv.org/pdf/2507.17335", "abs": "https://arxiv.org/abs/2507.17335", "authors": ["Guangzhu Xu", "Zhi Ke", "Pengcheng Zuo", "Bangjun Lei"], "title": "TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "License plate recognition in open environments is widely applicable across\nvarious domains; however, the diversity of license plate types and imaging\nconditions presents significant challenges. To address the limitations\nencountered by CNN and CRNN-based approaches in license plate recognition, this\npaper proposes a unified solution that integrates a lightweight visual encoder\nwith a text decoder, within a pre-training framework tailored for single and\ndouble-line Chinese license plates. To mitigate the scarcity of double-line\nlicense plate datasets, we constructed a single/double-line license plate\ndataset by synthesizing images, applying texture mapping onto real scenes, and\nblending them with authentic license plate images. Furthermore, to enhance the\nsystem's recognition accuracy, we introduce a perspective correction network\n(PTN) that employs license plate corner coordinate regression as an implicit\nvariable, supervised by license plate view classification information. This\nnetwork offers improved stability, interpretability, and low annotation costs.\nThe proposed algorithm achieves an average recognition accuracy of 99.34% on\nthe corrected CCPD test set under coarse localization disturbance. When\nevaluated under fine localization disturbance, the accuracy further improves to\n99.58%. On the double-line license plate test set, it achieves an average\nrecognition accuracy of 98.70%, with processing speeds reaching up to 167\nframes per second, indicating strong practical applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5355\u884c\u548c\u53cc\u884c\u4e2d\u56fd\u8f66\u724c\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u89c6\u89c9\u7f16\u7801\u5668\u548c\u6587\u672c\u89e3\u7801\u5668\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u89c6\u89d2\u6821\u6b63\u7f51\u7edc\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3 CNN \u548c CRNN \u57fa\u7840\u65b9\u6cd5\u5728\u8f66\u724c\u8bc6\u522b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u6837\u5316\u7684\u8f66\u724c\u7c7b\u578b\u548c\u6210\u50cf\u6761\u4ef6\u4e0b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u8f7b\u91cf\u7ea7\u89c6\u89c9\u7f16\u7801\u5668\u4e0e\u6587\u672c\u89e3\u7801\u5668\u96c6\u6210\u5728\u4e00\u4e2a\u9488\u5bf9\u5355\u884c\u548c\u53cc\u884c\u4e2d\u56fd\u8f66\u724c\u7684\u9884\u8bad\u7ec3\u6846\u67b6\u4e2d\u3002\u4e3a\u4e86\u7f13\u89e3\u53cc\u884c\u8f66\u724c\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\uff0c\u6211\u4eec\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u3001\u5e94\u7528\u7eb9\u7406\u6620\u5c04\u5230\u771f\u5b9e\u573a\u666f\u5e76\u5c06\u5176\u4e0e\u771f\u5b9e\u8f66\u724c\u56fe\u50cf\u6df7\u5408\u6765\u6784\u5efa\u5355/\u53cc\u884c\u8f66\u724c\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u63d0\u9ad8\u7cfb\u7edf\u7684\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u89c6\u89d2\u6821\u6b63\u7f51\u7edc\uff08PTN\uff09\uff0c\u8be5\u7f51\u7edc\u4f7f\u7528\u8f66\u724c\u89d2\u5750\u6807\u56de\u5f52\u4f5c\u4e3a\u9690\u53d8\u91cf\uff0c\u7531\u8f66\u724c\u89c6\u56fe\u5206\u7c7b\u4fe1\u606f\u76d1\u7763\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u7c97\u5b9a\u4f4d\u5e72\u6270\u4e0b\u7684 CCPD \u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86 99.34% \u7684\u5e73\u5747\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5728\u7ec6\u5b9a\u4f4d\u5e72\u6270\u4e0b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5230 99.58%\u3002\u5728\u53cc\u884c\u8f66\u724c\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5e73\u5747\u8bc6\u522b\u51c6\u786e\u7387\u4e3a 98.70%\uff0c\u5904\u7406\u901f\u5ea6\u9ad8\u8fbe\u6bcf\u79d2 167 \u5e27\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u7c97\u5b9a\u4f4d\u5e72\u6270\u4e0b\u7684 CCPD \u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86 99.34% \u7684\u5e73\u5747\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5728\u7ec6\u5b9a\u4f4d\u5e72\u6270\u4e0b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5230 99.58%\u3002\u5728\u53cc\u884c\u8f66\u724c\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5e73\u5747\u8bc6\u522b\u51c6\u786e\u7387\u4e3a 98.70%\uff0c\u5904\u7406\u901f\u5ea6\u9ad8\u8fbe\u6bcf\u79d2 167 \u5e27\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.17501", "pdf": "https://arxiv.org/pdf/2507.17501", "abs": "https://arxiv.org/abs/2507.17501", "authors": ["Xianbiao Qi", "Marco Chen", "Wenjie Xiao", "Jiaquan Ye", "Yelin He", "Chun-Guang Li", "Zhouchen Lin"], "title": "DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "We have introduced a novel architecture, Deeply Normalized\n  Transformer (DNT), which enables efficient training with vanilla momentum\n  SGDW (mSGDW), achieving performance on par with AdamW-optimized Transformers", "summary": "Transformers have become the de facto backbone of modern deep learning, yet\ntheir training typically demands an advanced optimizer with adaptive learning\nrate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that\nit is mainly due to a heavy-tailed distribution of the gradients. In this\npaper, we introduce a Deeply Normalized Transformer (DNT), which is\nmeticulously engineered to overcome this limitation enabling seamless training\nwith vanilla mSGDW while yielding comparable performance to the Transformers\ntrained via AdamW. To be specific, in DNT, we strategically integrate\nnormalization techniques at proper positions in the Transformers to effectively\nmodulate the Jacobian matrices of each layer, balance the influence of weights,\nactivations, and their interactions, and thus enable the distributions of\ngradients concentrated. We provide both theoretical justifications of the\nnormalization technique used in our DNT and extensive empirical evaluation on\ntwo popular Transformer architectures to validate that: a) DNT outperforms its\ncounterparts (\\ie, ViT and GPT), and b) DNT can be effectively trained with\nvanilla mSGDW.", "AI": {"tldr": "This paper introduces a Deeply Normalized Transformer (DNT) that enables seamless training with vanilla mSGDW while achieving comparable performance to Transformers trained via AdamW by integrating normalization techniques.", "motivation": "To overcome the limitation of Transformers requiring advanced optimizers like AdamW due to heavy-tailed gradient distributions, enabling seamless training with vanilla mSGDW.", "method": "Integrating normalization techniques at proper positions in the Transformers to modulate the Jacobian matrices of each layer, balance the influence of weights, activations, and their interactions, and concentrate the distributions of gradients.", "result": "DNT outperforms its counterparts (e.g., ViT and GPT) and can be effectively trained with vanilla mSGDW.", "conclusion": "DNT can be effectively trained with vanilla mSGDW while achieving comparable performance to Transformers trained via AdamW."}}
{"id": "2507.17515", "pdf": "https://arxiv.org/pdf/2507.17515", "abs": "https://arxiv.org/abs/2507.17515", "authors": ["Songshuo Lu", "Hua Wang", "Zhi Chen", "Yaohua Tang"], "title": "URPO: A Unified Reward & Policy Optimization Framework for Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large-scale alignment pipelines typically pair a policy model with a\nseparately trained reward model whose parameters remain frozen during\nreinforcement learning (RL). This separation creates a complex,\nresource-intensive pipeline and suffers from a performance ceiling due to a\nstatic reward signal. We propose a novel framework, Unified Reward & Policy\nOptimization (URPO), that unifies instruction-following (\"player\") and reward\nmodeling (\"referee\") within a single model and a single training phase. Our\nmethod recasts all alignment data-including preference pairs, verifiable\nreasoning, and open-ended instructions-into a unified generative format\noptimized by a single Group-Relative Policy Optimization (GRPO) loop. This\nenables the model to learn from ground-truth preferences and verifiable logic\nwhile simultaneously generating its own rewards for open-ended tasks.\nExperiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified\nmodel significantly outperforms a strong baseline using a separate generative\nreward model, boosting the instruction-following score on AlpacaEval from 42.24\nto 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,\nURPO cultivates a superior internal evaluator as a byproduct of training,\nachieving a RewardBench score of 85.15 and surpassing the dedicated reward\nmodel it replaces (83.55). By eliminating the need for a separate reward model\nand fostering a co-evolutionary dynamic between generation and evaluation, URPO\npresents a simpler, more efficient, and more effective path towards robustly\naligned language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aURPO\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u6307\u4ee4\u9075\u5faa\u548c\u5956\u52b1\u5efa\u6a21\u6574\u5408\u5230\u4e00\u4e2a\u6a21\u578b\u548c\u4e00\u4e2a\u8bad\u7ec3\u9636\u6bb5\u4e2d\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u751f\u6210\u683c\u5f0f\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u5927\u578b\u5bf9\u9f50\u6d41\u6c34\u7ebf\u901a\u5e38\u5c06\u7b56\u7565\u6a21\u578b\u4e0e\u5355\u72ec\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u914d\u5bf9\uff0c\u4f46\u8fd9\u79cd\u5206\u79bb\u5bfc\u81f4\u4e86\u590d\u6742\u7684\u8d44\u6e90\u5bc6\u96c6\u578b\u6d41\u7a0b\uff0c\u5e76\u7531\u4e8e\u9759\u6001\u5956\u52b1\u4fe1\u53f7\u800c\u53d7\u5230\u6027\u80fd\u9650\u5236\u3002", "method": "URPO\u6846\u67b6\u5c06\u6307\u4ee4\u9075\u5faa\uff08\u201c\u73a9\u5bb6\u201d\uff09\u548c\u5956\u52b1\u5efa\u6a21\uff08\u201c\u88c1\u5224\u201d\uff09\u7edf\u4e00\u5728\u4e00\u4e2a\u6a21\u578b\u548c\u4e00\u4e2a\u8bad\u7ec3\u9636\u6bb5\u4e2d\u3002\u65b9\u6cd5\u5c06\u6240\u6709\u5bf9\u9f50\u6570\u636e\uff08\u5305\u62ec\u504f\u597d\u5bf9\u3001\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u548c\u5f00\u653e\u6027\u6307\u4ee4\uff09\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684\u751f\u6210\u683c\u5f0f\uff0c\u5e76\u901a\u8fc7\u5355\u4e00\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5faa\u73af\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728Qwen2.5-7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cURPO\u4f18\u4e8e\u4f7f\u7528\u5355\u72ec\u751f\u6210\u5956\u52b1\u6a21\u578b\u7684\u5f3a\u57fa\u7ebf\u3002URPO\u663e\u8457\u63d0\u5347\u4e86AlpacaEval\u4e0a\u7684\u6307\u4ee4\u9075\u5faa\u5206\u6570\uff0c\u4ece42.24\u63d0\u5347\u523044.84\uff0c\u5e76\u5c06\u7efc\u5408\u63a8\u7406\u5e73\u5747\u503c\u4ece32.66\u63d0\u5347\u523035.66\u3002\u6b64\u5916\uff0cURPO\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u57f9\u517b\u4e86\u4e00\u4e2a\u4f18\u8d8a\u7684\u5185\u90e8\u8bc4\u4f30\u5668\uff0cRewardBench\u5f97\u5206\u8fbe\u523085.15\uff0c\u8d85\u8fc7\u4e86\u5b83\u6240\u53d6\u4ee3\u7684\u4e13\u7528\u5956\u52b1\u6a21\u578b\uff0883.55\uff09\u3002", "conclusion": "URPO\u901a\u8fc7\u6d88\u9664\u5355\u72ec\u7684\u5956\u52b1\u6a21\u578b\u5e76\u4fc3\u8fdb\u751f\u6210\u548c\u8bc4\u4f30\u4e4b\u95f4\u7684\u5171\u540c\u8fdb\u5316\u52a8\u6001\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u9ad8\u6548\u3001\u66f4\u6709\u6548\u7684\u9014\u5f84\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5065\u5bf9\u9f50\u7684\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2507.17563", "pdf": "https://arxiv.org/pdf/2507.17563", "abs": "https://arxiv.org/abs/2507.17563", "authors": ["Qing Wang", "Zehan Li", "Hang Lv", "Hongjie Chen", "Yaodong Song", "Jian Kang", "Jie Lian", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "BoSS: Beyond-Semantic Speech", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Human communication involves more than explicit semantics, with implicit\nsignals and contextual cues playing a critical role in shaping meaning.\nHowever, modern speech technologies, such as Automatic Speech Recognition (ASR)\nand Text-to-Speech (TTS) often fail to capture these beyond-semantic\ndimensions. To better characterize and benchmark the progression of speech\nintelligence, we introduce Spoken Interaction System Capability Levels (L1-L5),\na hierarchical framework illustrated the evolution of spoken dialogue systems\nfrom basic command recognition to human-like social interaction. To support\nthese advanced capabilities, we propose Beyond-Semantic Speech (BoSS), which\nrefers to the set of information in speech communication that encompasses but\ntranscends explicit semantics. It conveys emotions, contexts, and modifies or\nextends meanings through multidimensional features such as affective cues,\ncontextual dynamics, and implicit semantics, thereby enhancing the\nunderstanding of communicative intentions and scenarios. We present a\nformalized framework for BoSS, leveraging cognitive relevance theories and\nmachine learning models to analyze temporal and contextual speech dynamics. We\nevaluate BoSS-related attributes across five different dimensions, reveals that\ncurrent spoken language models (SLMs) are hard to fully interpret\nbeyond-semantic signals. These findings highlight the need for advancing BoSS\nresearch to enable richer, more context-aware human-machine communication.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Spoken Interaction System Capability Levels (L1-L5)\u548cBeyond-Semantic Speech (BoSS)\u7684\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5f62\u5f0f\u5316\u7684BoSS\u6846\u67b6\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u4e94\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u6307\u51fa\u5f53\u524d\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8d85\u8d8a\u8bed\u4e49\u7684\u4fe1\u53f7\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u73b0\u4ee3\u8bed\u97f3\u6280\u672f\uff08\u5982\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u6587\u672c\u8f6c\u8bed\u97f3\uff09\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u8fd9\u4e9b\u8d85\u8d8a\u8bed\u4e49\u7684\u7ef4\u5ea6\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u8868\u5f81\u548c\u57fa\u51c6\u6d4b\u8bd5\u8bed\u97f3\u667a\u80fd\u7684\u8fdb\u6b65\uff0c\u6211\u4eec\u5f15\u5165\u4e86Spoken Interaction System Capability Levels (L1-L5)\uff0c\u8fd9\u662f\u4e00\u4e2a\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u4ece\u57fa\u672c\u547d\u4ee4\u8bc6\u522b\u5230\u7c7b\u4eba\u793e\u4ea4\u4e92\u52a8\u7684\u6f14\u53d8\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Beyond-Semantic Speech (BoSS)\uff0c\u5b83\u6307\u7684\u662f\u8bed\u97f3\u4ea4\u6d41\u4e2d\u6db5\u76d6\u4f46\u8d85\u8d8a\u660e\u786e\u8bed\u4e49\u7684\u4fe1\u606f\u96c6\u3002\u901a\u8fc7\u60c5\u611f\u7ebf\u7d22\u3001\u60c5\u5883\u52a8\u6001\u548c\u9690\u542b\u8bed\u4e49\u7b49\u591a\u7ef4\u7279\u5f81\u6765\u4f20\u8fbe\u60c5\u611f\u3001\u60c5\u5883\uff0c\u5e76\u4fee\u6539\u6216\u6269\u5c55\u610f\u4e49\u3002\u6211\u4eec\u5448\u73b0\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u7684BoSS\u6846\u67b6\uff0c\u5229\u7528\u8ba4\u77e5\u76f8\u5173\u6027\u7406\u8bba\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u5206\u6790\u65f6\u95f4\u6027\u548c\u60c5\u5883\u6027\u8bed\u97f3\u52a8\u6001\u3002", "result": "\u6211\u4eec\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u7ef4\u5ea6\u4e0a\u8bc4\u4f30\u4e86\u4e0eBoSS\u76f8\u5173\u7684\u5c5e\u6027\uff0c\u7ed3\u679c\u8868\u660e\u5f53\u524d\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5f88\u96be\u5b8c\u5168\u89e3\u91ca\u8d85\u8d8a\u8bed\u4e49\u7684\u4fe1\u53f7\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u63a8\u8fdbBoSS\u7814\u7a76\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u5b9e\u73b0\u66f4\u4e30\u5bcc\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u610f\u8bc6\u7684\u4eba\u673a\u901a\u4fe1\u3002"}}
{"id": "2507.17588", "pdf": "https://arxiv.org/pdf/2507.17588", "abs": "https://arxiv.org/abs/2507.17588", "authors": ["Jie Wang", "Zhendong Yang", "Liansong Zong", "Xiaobo Zhang", "Dexian Wang", "Ji Zhang"], "title": "Dual-branch Prompting for Multimodal Machine Translation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multimodal Machine Translation (MMT) typically enhances text-only translation\nby incorporating aligned visual features. Despite the remarkable progress,\nstate-of-the-art MMT approaches often rely on paired image-text inputs at\ninference and are sensitive to irrelevant visual noise, which limits their\nrobustness and practical applicability. To address these issues, we propose\nD2P-MMT, a diffusion-based dual-branch prompting framework for robust\nvision-guided translation. Specifically, D2P-MMT requires only the source text\nand a reconstructed image generated by a pre-trained diffusion model, which\nnaturally filters out distracting visual details while preserving semantic\ncues. During training, the model jointly learns from both authentic and\nreconstructed images using a dual-branch prompting strategy, encouraging rich\ncross-modal interactions. To bridge the modality gap and mitigate\ntraining-inference discrepancies, we introduce a distributional alignment loss\nthat enforces consistency between the output distributions of the two branches.\nExtensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves\nsuperior translation performance compared to existing state-of-the-art\napproaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u53cc\u5206\u652f\u63d0\u793a\u6846\u67b6D2P-MMT\uff0c\u7528\u4e8e\u589e\u5f3a\u89c6\u89c9\u5f15\u5bfc\u7ffb\u8bd1\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u91cd\u5efa\u56fe\u50cf\uff0c\u51cf\u5c11\u5e72\u6270\u7684\u89c6\u89c9\u7ec6\u8282\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u53cc\u5206\u652f\u63d0\u793a\u7b56\u7565\u548c\u5206\u5e03\u5bf9\u9f50\u635f\u5931\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684MMT\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u914d\u5bf9\u7684\u56fe\u50cf-\u6587\u672c\u8f93\u5165\uff0c\u5e76\u4e14\u5bf9\u65e0\u5173\u7684\u89c6\u89c9\u566a\u58f0\u654f\u611f\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u89c6\u89c9\u5f15\u5bfc\u7ffb\u8bd1\u65b9\u6cd5\u3002", "method": "D2P-MMT\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u53cc\u5206\u652f\u63d0\u793a\u6846\u67b6\uff0c\u5b83\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u91cd\u5efa\u56fe\u50cf\uff0c\u7ed3\u5408\u6e90\u6587\u672c\u8fdb\u884c\u89c6\u89c9\u5f15\u5bfc\u7ffb\u8bd1\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u901a\u8fc7\u53cc\u5206\u652f\u63d0\u793a\u7b56\u7565\u540c\u65f6\u4ece\u771f\u5b9e\u548c\u91cd\u5efa\u56fe\u50cf\u4e2d\u5b66\u4e60\uff0c\u4ee5\u4fc3\u8fdb\u4e30\u5bcc\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u5206\u5e03\u5bf9\u9f50\u635f\u5931\u6765\u5f25\u5408\u6a21\u6001\u5dee\u8ddd\u5e76\u51cf\u8f7b\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "D2P-MMT\u5728Multi30K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u7ffb\u8bd1\u6027\u80fd\u3002", "conclusion": "D2P-MMT\u5728Multi30K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u7ffb\u8bd1\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.17746", "pdf": "https://arxiv.org/pdf/2507.17746", "abs": "https://arxiv.org/abs/2507.17746", "authors": ["Anisha Gunjal", "Anthony Wang", "Elaine Lau", "Vaskar Nath", "Bing Liu", "Sean Hendryx"], "title": "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world\ntasks often requires balancing objective and subjective evaluation criteria.\nHowever, many such tasks lack a single, unambiguous ground truth-making it\ndifficult to define reliable reward signals for post-training language models.\nWhile traditional preference-based methods offer a workaround, they rely on\nopaque reward functions that are difficult to interpret and prone to spurious\ncorrelations. We introduce $\\textbf{Rubrics as Rewards}$ (RaR), a framework\nthat uses structured, checklist-style rubrics as interpretable reward signals\nfor on-policy training with GRPO. Our best RaR method yields up to a $28\\%$\nrelative improvement on HealthBench-1k compared to simple Likert-based\napproaches, while matching or surpassing the performance of reward signals\nderived from expert-written references. By treating rubrics as structured\nreward signals, we show that RaR enables smaller-scale judge models to better\nalign with human preferences and sustain robust performance across model\nscales.", "AI": {"tldr": "RaR\u6846\u67b6\u5229\u7528\u7ed3\u6784\u5316\u7684\u8bc4\u5206\u6807\u51c6\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u63d0\u5347\u4e86\u5c0f\u89c4\u6a21\u6a21\u578b\u7684\u8868\u73b0\u5e76\u4f7f\u5176\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\uff0c\u5e73\u8861\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u6807\u51c6\u662f\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\uff0c\u4f46\u8bb8\u591a\u4efb\u52a1\u7f3a\u4e4f\u660e\u786e\u7684\u5730\u9762\u771f\u5b9e\u6570\u636e\uff0c\u4f7f\u5f97\u5b9a\u4e49\u53ef\u9760\u7684\u5956\u52b1\u4fe1\u53f7\u53d8\u5f97\u56f0\u96be\u3002\u4f20\u7edf\u57fa\u4e8e\u504f\u597d\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u96be\u4ee5\u89e3\u91ca\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u76f8\u5173\u6027\u3002", "method": "RaR\u6846\u67b6\u4f7f\u7528\u7ed3\u6784\u5316\u3001\u6e05\u5355\u5f0f\u7684\u8bc4\u5206\u6807\u51c6\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u7528\u4e8e\u57fa\u4e8eGRPO\u7684\u7b56\u7565\u8bad\u7ec3\u3002", "result": "RaR\u7684\u6700\u4f73\u65b9\u6cd5\u5728HealthBench-1k\u4e0a\u76f8\u5bf9\u4e8e\u7b80\u5355\u7684Likert\u65b9\u6cd5\u63d0\u9ad8\u4e8628%\u7684\u76f8\u5bf9\u6027\u80fd\uff0c\u5e76\u4e14\u4e0e\u4e13\u5bb6\u7f16\u5199\u7684\u53c2\u8003\u5956\u52b1\u4fe1\u53f7\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "RaR\u6846\u67b6\u901a\u8fc7\u5c06\u8bc4\u5206\u6807\u51c6\u4f5c\u4e3a\u7ed3\u6784\u5316\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u4f7f\u8f83\u5c0f\u89c4\u6a21\u7684\u5224\u65ad\u6a21\u578b\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u5e76\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002"}}

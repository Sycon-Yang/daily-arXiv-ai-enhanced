<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 105]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.HC](#cs.HC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
*Pranav Gupta*

Main category: cs.CL

TL;DR: 本文介绍了一个针对大学级别教育应用的评估基准OpenStaxQA，基于43本开源教材，并通过微调大语言模型和零样本评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在创建一个专门针对大学级别教育应用的评估基准，利用开放获取的教材来评估大语言模型的表现，并探讨其更广泛的影响。

Method: 本文对大约70亿参数的大语言模型（LLMs）进行了微调和评估，并使用量化低秩适配器（QLoRa）在该数据集上进行测试。此外，还对AI2推理挑战开发数据集进行了零样本评估，以检查OpenStaxQA是否能提高其他任务的性能。

Result: 通过使用QLoRa对大语言模型进行微调和评估，以及在AI2推理挑战开发数据集上的零样本评估，验证了OpenStaxQA的有效性。

Conclusion: 本文介绍了OpenStaxQA，这是一个针对大学级别教育应用的评估基准，基于43本以英语、西班牙语和波兰语发布的开源大学教材。

Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level
educational applications based on 43 open-source college textbooks in English,
Spanish, and Polish, available under a permissive Creative Commons license. We
finetune and evaluate large language models (LLMs) with approximately 7 billion
parameters on this dataset using quantized low rank adapters (QLoRa).
Additionally we also perform a zero-shot evaluation on the AI2 reasoning
challenge dev dataset in order to check if OpenStaxQA can lead to an improved
performance on other tasks. We also discuss broader impacts relevant to
datasets such as OpenStaxQA.

</details>


### [2] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为KG-MASD的方法，用于工业问答系统中的多智能体系统蒸馏。该方法通过结合知识图谱和马尔可夫决策过程，提高了模型的准确性和可靠性，适用于安全关键的工业场景。


<details>
  <summary>Details</summary>
Motivation: 工业问答（QA）系统需要比通用对话模型更高的安全性和可靠性，因为在高风险场景（如设备故障诊断）中的错误可能有严重后果。虽然多智能体大语言模型增强了推理深度，但它们存在不受控制的迭代和不可验证的输出，传统蒸馏方法难以将协作推理能力转移到轻量级、可部署的学生模型中。

Method: 我们提出了基于知识图谱的多智能体系统蒸馏（KG-MASD）。我们的方法将蒸馏建模为马尔可夫决策过程，并结合知识图谱作为可验证的结构先验来丰富状态表示并确保收敛。通过整合协作推理和知识基础，KG-MASD生成高置信度的指令调优数据，并联合蒸馏推理深度和可验证性到适合边缘部署的紧凑学生模型中。

Result: 实验表明，KG-MASD在工业QA数据集上的准确率比基线提高了2.4%到20.1%，并显著提高了可靠性，使安全关键的工业场景中可信赖的AI部署成为可能。

Conclusion: 实验表明，KG-MASD在工业QA数据集上的准确率比基线提高了2.4%到20.1%，并显著提高了可靠性，使安全关键的工业场景中可信赖的AI部署成为可能。

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [3] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: 本文提出了一种针对人类调查回答的两阶段评估框架，通过无意义文本过滤和三个维度的评估，验证显示其在实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开放式的调查回答在营销研究中提供了有价值的见解，但低质量的回答不仅给研究人员带来手动筛选的负担，还可能导致误导性的结论，因此需要有效的评估方法。现有的自动评估方法针对LLM生成的文本，未能充分评估具有不同特征的人类写作回答。

Method: 我们提出了一个两阶段的评估框架，专门针对人类调查回答。首先，进行无意义文本过滤，然后使用LLM能力评估三个维度——努力、相关性和完整性。

Result: 在英语和韩语数据集上的验证表明，我们的框架不仅优于现有指标，而且在实际应用中表现出高适用性，如响应质量预测和响应拒绝，并与专家评估有强相关性。

Conclusion: 我们的框架不仅优于现有指标，而且在实际应用中表现出高适用性，如响应质量预测和响应拒绝，并与专家评估有强相关性。

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [4] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: 本文提出了一种新的策略CoT Referring，用于改进参考表达理解和分割任务，通过结构化的思维链训练数据和统一的MLLM框架实现。实验结果表明该方法有效，比基线模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 参考表达理解和分割对于评估语言理解和图像理解的整合至关重要，是多模态大语言模型能力的基准。我们需要一种新的策略来解决这些挑战。

Method: 我们提出了一个新策略，CoT Referring，通过结构化的思维链训练数据结构来增强跨模态的模型推理。我们重新构建了训练数据以强制新的输出形式，并整合了检测和分割能力到统一的MLLM框架中，使用新颖的自适应加权损失进行训练。

Result: 在我们的定制基准和RefCOCO/+/g上的实验结果表明，我们的方法有效，比基线模型有显著提升。

Conclusion: 我们的方法在定制的基准和RefCOCO/+/g上表现出色，比基线模型有显著提升。

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [5] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: 本文旨在找到适用于科学领域自然语言处理任务的最佳词表示和分词方法，并构建一个全面的评估套件来评估这些方法。


<details>
  <summary>Details</summary>
Motivation: 在科学领域中，相同的单词可能有不同的含义，因此需要找到最佳的词表示和分词方法。此外，生成式AI和变压器架构虽然能生成上下文嵌入，但计算成本高，因此需要更有效的解决方案。

Method: 本文的方法包括构建一个包含多个下游任务和相关数据集的评估套件，并使用该套件测试各种词表示和分词算法。

Result: 本文构建了一个评估套件，并用它来测试各种词表示和分词算法，以确定最佳方法。

Conclusion: 本文的结论是，通过构建全面的评估套件，可以找到适用于科学领域自然语言处理任务的最佳词表示和分词方法。

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [6] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: 研究提出了一种新的方法TRepLiNa，通过跨语言相似性增强解码器内部层，以提高低资源语言到高资源语言的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 解决印度多种低资源语言缺乏资源的问题，通过跨语言相似性增强解码器内部层来提高从低资源语言到高资源语言的翻译质量。

Method: 结合了中心核对齐（CKA）和REPINA正则化方法，形成了一种称为TRepLiNa的新方法。

Result: 实验结果表明，在零样本、少样本和微调设置中，使用TRepLiNa（CKA+REPINA）对中间层进行对齐可以有效提升低资源语言的翻译效果。

Conclusion: 对低资源语言的翻译有显著改善，特别是在数据稀缺的情况下，TRepLiNa方法是一种低成本且实用的方法。

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [7] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: 本文提出了一种多语言数据整理框架，用于在资源不足的地区进行高质量PII标注，并通过人机协作和分析驱动的流程提高标注质量和模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，确保其在不同监管环境下可靠处理个人身份信息（PII）变得至关重要。然而，目前在资源不足的地区缺乏高质量的PII标注数据。

Method: 本文采用分阶段的人机协作标注方法，结合语言专业知识和严格的质量保证，利用标注者间协议度量和根本原因分析来系统地发现和解决标注不一致问题。

Result: 该框架在试点、训练和生产阶段显著提高了召回率和假阳性率，并生成了适合监督微调的高保真数据集。

Conclusion: 本文提出了一种可扩展的多语言数据整理框架，用于在13个资源不足的地区进行高质量的PII标注，并通过迭代分析驱动的流程提高了标注质量和下游模型的可靠性。

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [8] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: 该数据集是一个标准化的双语Prakriti评估问卷，旨在根据古典阿育吠陀原则评估个体的身心和心理特征，并提供结构化平台用于计算智能、阿育吠陀研究和个性化健康分析。


<details>
  <summary>Details</summary>
Motivation: 为了确保全面且准确的数据收集，开发了一个标准化的双语Prakriti评估问卷，以评估个体的身心和心理特征，符合古典阿育吠陀原则。

Method: 根据AYUSH/CCRAS指南设计了标准化的双语（英语-印地语）Prakriti评估问卷，通过Google Forms收集数据并自动评分以映射个体特征到特定dosha分数。

Result: 该数据集包含了24个多项选择题，涵盖了身体特征、食欲、睡眠模式、能量水平和气质，并通过自动化评分系统进行分析，支持特征分布、相关性和预测建模的分析。

Conclusion: 该数据集为计算智能、阿育吠陀研究和个性化健康分析提供了结构化的平台，并可作为未来基于Prakriti的研究和智能健康应用开发的参考。

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [9] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段的摘要系统，可在嵌入式设备上运行，实现离线临床摘要，同时保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）包含大量非结构化临床数据，这可能会使急诊医生难以识别关键信息。因此，需要一种能够有效提取关键信息并生成摘要的方法。

Method: 我们提出了一种两阶段的摘要系统，该系统在嵌入式设备上运行，可以在不牺牲患者隐私的情况下进行离线临床摘要。该方法使用双设备架构，首先使用Jetson Nano-R检索相关患者记录部分，然后在另一个Jetson Nano-S上生成结构化摘要。

Result: 初步结果表明，我们的完全离线系统可以在30秒内有效地生成有用的摘要。

Conclusion: 我们的完全离线系统能够在30秒内有效地生成有用的摘要。

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


### [10] [A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation](https://arxiv.org/abs/2510.06265)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型中的幻觉现象，分析了其原因、检测和缓解方法，并指出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在自然语言处理领域取得了显著进展，但其产生的虚假或虚构信息（即幻觉）影响了其可靠性和可信度，尤其是在需要事实准确性的领域。因此，需要对幻觉进行系统的研究，以提高大型语言模型的准确性。

Method: 本文首先介绍了幻觉类型的分类，并分析了其在整个大型语言模型开发生命周期中的根本原因，包括数据收集、架构设计和推理。然后，探讨了幻觉在关键自然语言生成任务中如何出现。在此基础上，引入了检测方法的结构化分类和缓解策略的分类。此外，还分析了当前检测和缓解方法的优缺点，并回顾了用于量化大型语言模型幻觉的现有评估基准和指标。

Result: 本文提供了对大型语言模型中幻觉现象的全面综述，包括幻觉类型、原因、检测和缓解方法，并分析了当前方法的优缺点，同时指出了未来研究的关键挑战和有前景的方向。

Conclusion: 本文综述了大型语言模型中的幻觉现象，提出了检测和缓解方法，并分析了当前方法的优缺点，指出了未来研究的方向，为开发更真实和可信的大型语言模型提供了基础。

Abstract: Large language models (LLMs) have transformed natural language processing,
achieving remarkable performance across diverse tasks. However, their
impressive fluency often comes at the cost of producing false or fabricated
information, a phenomenon known as hallucination. Hallucination refers to the
generation of content by an LLM that is fluent and syntactically correct but
factually inaccurate or unsupported by external evidence. Hallucinations
undermine the reliability and trustworthiness of LLMs, especially in domains
requiring factual accuracy. This survey provides a comprehensive review of
research on hallucination in LLMs, with a focus on causes, detection, and
mitigation. We first present a taxonomy of hallucination types and analyze
their root causes across the entire LLM development lifecycle, from data
collection and architecture design to inference. We further examine how
hallucinations emerge in key natural language generation tasks. Building on
this foundation, we introduce a structured taxonomy of detection approaches and
another taxonomy of mitigation strategies. We also analyze the strengths and
limitations of current detection and mitigation approaches and review existing
evaluation benchmarks and metrics used to quantify LLMs hallucinations.
Finally, we outline key open challenges and promising directions for future
research, providing a foundation for the development of more truthful and
trustworthy LLMs.

</details>


### [11] [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
*Rohitash Chandra,Yathin Suresh,Divyansh Raj Sinha,Sanchit Jindal*

Main category: cs.CL

TL;DR: 本研究利用深度学习方法分析了美国公告牌排行榜的歌曲（歌词），发现自1990年以来，流行音乐中的明确内容显著增加，并且含有粗俗、性暗示和不适当语言的歌曲越来越普遍。


<details>
  <summary>Details</summary>
Motivation: 由于这种内容对儿童和青少年有有害的行为变化，因此缺乏验证趋势的研究，以有效制定政策。

Method: 我们利用深度学习方法分析了美国公告牌排行榜的歌曲（歌词），并进行了纵向研究，使用深度学习和语言模型来审查内容的演变，包括情感分析和滥用检测，以及性暗示内容。

Result: 我们的研究结果表明，自1990年以来，流行音乐中的明确内容显著增加。此外，我们发现含有粗俗、性暗示和不适当语言的歌曲越来越普遍。

Conclusion: 我们的研究结果表明，自1990年以来，流行音乐中的明确内容显著增加。此外，我们发现含有粗俗、性暗示和不适当语言的歌曲越来越普遍。

Abstract: There is no doubt that there has been a drastic increase in abusive and
sexually explicit content in music, particularly in Billboard Music Charts.
However, there is a lack of studies that validate the trend for effective
policy development, as such content has harmful behavioural changes in children
and youths. In this study, we utilise deep learning methods to analyse songs
(lyrics) from Billboard Charts of the United States in the last seven decades.
We provide a longitudinal study using deep learning and language models and
review the evolution of content using sentiment analysis and abuse detection,
including sexually explicit content. Our results show a significant rise in
explicit content in popular music from 1990 onwards. Furthermore, we find an
increasing prevalence of songs with lyrics containing profane, sexually
explicit, and otherwise inappropriate language. The longitudinal analysis of
the ability of language models to capture nuanced patterns in lyrical content,
reflecting shifts in societal norms and language use over time.

</details>


### [12] [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
*Ranjan Mishra,Julian I. Bibo,Quinten van Engelen,Henk Schaapman*

Main category: cs.CL

TL;DR: 本研究复制了Ma等人(2024)的XRec工作，使用Llama 3作为LLM进行评估，并扩展了XRec的专家混合模块。结果表明，XRec能够生成个性化的解释，并通过引入协作信息提高稳定性，但并未在所有指标上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 我们的目标是复制原始论文的结果，但使用Llama 3作为LLM进行评估，而不是GPT-3.5-turbo。此外，我们希望扩展原始论文的工作，以更好地理解XRec的专家混合模块在生成解释结构中的作用。

Method: 我们基于Ma等(2024)提供的源代码，使用Llama 3作为LLM进行评估，以复制原始论文的结果。我们通过修改XRec的专家混合模块的输入嵌入或删除输出嵌入来扩展原始论文的工作。

Result: XRec能够有效地生成个性化的解释，并且通过引入协作信息提高了稳定性。然而，XRec在每个指标上并未始终优于所有基线模型。我们的扩展分析进一步强调了专家混合嵌入在塑造解释结构中的重要性，展示了协作信号如何与语言建模相互作用。

Conclusion: XRec effectively generates personalized explanations and its stability is improved by incorporating collaborative information. However, XRec did not consistently outperform all baseline models in every metric. Our extended analysis further highlights the importance of the Mixture of Experts embeddings in shaping the explanation structures, showcasing how collaborative signals interact with language modeling.

Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language
Models for Explainable Recommendation" by Ma et al. (2024). The original
authors introduced XRec, a model-agnostic collaborative instruction-tuning
framework that enables large language models (LLMs) to provide users with
comprehensive explanations of generated recommendations. Our objective was to
replicate the results of the original paper, albeit using Llama 3 as the LLM
for evaluation instead of GPT-3.5-turbo. We built on the source code provided
by Ma et al. (2024) to achieve our goal. Our work extends the original paper by
modifying the input embeddings or deleting the output embeddings of XRec's
Mixture of Experts module. Based on our results, XRec effectively generates
personalized explanations and its stability is improved by incorporating
collaborative information. However, XRec did not consistently outperform all
baseline models in every metric. Our extended analysis further highlights the
importance of the Mixture of Experts embeddings in shaping the explanation
structures, showcasing how collaborative signals interact with language
modeling. Through our work, we provide an open-source evaluation implementation
that enhances accessibility for researchers and practitioners alike. Our
complete code repository can be found at
https://github.com/julianbibo/xrec-reproducibility.

</details>


### [13] [Type and Complexity Signals in Multilingual Question Representations](https://arxiv.org/abs/2510.06304)
*Robin Kokot,Wessel Poelman*

Main category: cs.CL

TL;DR: 本研究分析了多语言变压器模型如何表示问题的形态句法属性，并比较了不同方法在分类问题上的效果。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨多语言变压器模型如何表示问题的形态句法属性，并评估上下文表示是否优于统计基线以及参数更新是否会影响预训练语言信息的可用性。

Method: 我们引入了QTC数据集，并使用探测方法扩展到回归标签，以量化泛化能力的提升。我们比较了冻结的Glot500-m表示层探测、子词TF-IDF基线和微调模型的结果。

Result: 统计特征在有明确标记的语言中能有效分类问题，而神经探测器能更好地捕捉细微的结构复杂性模式。

Conclusion: 我们的结果表明，上下文表示在某些情况下优于统计基线，并且参数更新可能会减少预训练语言信息的可用性。

Abstract: This work investigates how a multilingual transformer model represents
morphosyntactic properties of questions. We introduce the Question Type and
Complexity (QTC) dataset with sentences across seven languages, annotated with
type information and complexity metrics including dependency length, tree
depth, and lexical density. Our evaluation extends probing methods to
regression labels with selectivity controls to quantify gains in
generalizability. We compare layer-wise probes on frozen Glot500-m (Imani et
al., 2023) representations against subword TF-IDF baselines, and a fine-tuned
model. Results show that statistical features classify questions effectively in
languages with explicit marking, while neural probes capture fine-grained
structural complexity patterns better. We use these results to evaluate when
contextual representations outperform statistical baselines and whether
parameter updates reduce the availability of pre-trained linguistic
information.

</details>


### [14] [LLM Bias Detection and Mitigation through the Lens of Desired Distributions](https://arxiv.org/abs/2510.06354)
*Ingroj Shrestha,Padmini Srinivasan*

Main category: cs.CL

TL;DR: 本文提出了一种基于加权自适应损失的微调方法，以将大语言模型的性别-职业输出分布与目标分布对齐，同时保持语言建模能力。实验结果显示，在平等分布下实现了近乎完全的缓解，在现实世界设置下减少了30-75%的偏差。自回归大语言模型在平等分布下没有偏差，但在现实世界设置下表现出明显的偏差，其中Llama Instruct模型（3.2-3B，3.1-8B）实现了50-62%的减少。


<details>
  <summary>Details</summary>
Motivation: 现有的偏见缓解工作主要关注促进社会平等和人口均等，但较少关注将大语言模型的输出与期望分布对齐。例如，我们可能希望将模型与现实世界的分布对齐，以支持事实基础。因此，我们将偏见定义为与期望分布的偏差，这可能根据应用目标是平等或现实世界的分布。

Method: 本文提出了一种基于加权自适应损失的微调方法，以将大语言模型的性别-职业输出分布与目标分布对齐，同时保持语言建模能力。

Result: 在三个职业集（男性主导、女性主导和性别平衡）上评估了我们的自适应方法和非自适应变体。在三种掩码语言模型中，两种分布下都观察到了偏见。在平等分布下实现了近乎完全的缓解，在现实世界设置下减少了30-75%的偏差。自回归大语言模型在平等分布下没有偏差，但在现实世界设置下表现出明显的偏差，其中Llama Instruct模型（3.2-3B，3.1-8B）实现了50-62%的减少。

Conclusion: 本文提出了一种基于加权自适应损失的微调方法，能够将大语言模型的性别-职业输出分布与目标分布对齐，同时保持语言建模能力。实验结果显示，在平等分布下实现了近乎完全的缓解，在现实世界设置下减少了30-75%的偏差。自回归大语言模型在平等分布下没有偏差，但在现实世界设置下表现出明显的偏差，其中Llama Instruct模型（3.2-3B，3.1-8B）实现了50-62%的减少。

Abstract: Although prior work on bias mitigation has focused on promoting social
equality and demographic parity, less attention has been given to aligning
LLM's outputs to desired distributions. For example, we might want to align a
model with real-world distributions to support factual grounding. Thus, we
define bias as deviation from a desired distribution, which may be an equal or
real-world distribution, depending on application goals. We propose a weighted
adaptive loss based fine-tuning method that aligns LLM's gender-profession
output distribution with the desired distribution, while preserving language
modeling capability. Using 3 profession sets -- male-dominated,
female-dominated, and gender-balanced -- derived from U.S. labor statistics
(2024), we assess both our adaptive method for reflecting reality and a
non-adaptive variant for equality. Across three masked language models, bias is
observed under both distributions. We achieve near-complete mitigation under
equality and 30-75% reduction under real-world settings. Autoregressive LLMs
show no bias under equality but notable bias under real-world settings, with
the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.

</details>


### [15] [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference](https://arxiv.org/abs/2510.06370)
*Kshitish Ghate,Andy Liu,Devansh Jain,Taylor Sorensen,Atoosa Kasirzadeh,Aylin Caliskan,Mona T. Diab,Maarten Sap*

Main category: cs.CL

TL;DR: EVALUESTEER是一个基准测试，用于评估大型语言模型和奖励模型在用户价值观和风格偏好上的可引导性。结果显示，当前模型在处理完整用户资料时表现不佳，表明需要改进奖励模型以更好地适应用户偏好。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在全球范围内的部署，创建能够容纳全球用户多样化偏好和价值观的系统变得至关重要。现有的数据集缺乏支持奖励模型引导的控制评估，因此需要一个基准来测量大型语言模型和奖励模型在用户价值观和风格偏好上的可引导性。

Method: EVALUESTEER通过合成生成165,888对偏好数据，系统地在四个价值维度（传统、世俗理性、生存和自我表达）和四个风格维度（冗长、可读性、自信和温暖）上变化，以评估大型语言模型和奖励模型是否能够根据用户的偏好选择正确的输出。

Result: 当提供用户的完整价值观和风格偏好时，最佳模型在选择正确响应方面的准确率低于75%，而在仅提供相关风格和价值观偏好时，准确率超过99%。

Conclusion: EVALUESTEER突显了当前奖励模型在识别和适应相关用户资料信息方面的局限性，并为开发可以被引导到多样人类价值观和偏好的奖励模型提供了一个具有挑战性的测试平台。

Abstract: As large language models (LLMs) are deployed globally, creating pluralistic
systems that can accommodate the diverse preferences and values of users
worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure
LLMs' and reward models' (RMs) steerability towards users' value and stylistic
preference profiles grounded in psychology and human-LLM interaction
literature. To address the gap in existing datasets that do not support
controlled evaluations of RM steering, we synthetically generated 165,888
preference pairs -- systematically varying pairs along 4 value dimensions
(traditional, secular-rational, survival, and self-expression) and 4 style
dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER
to evaluate whether, given a user profile and a pair of candidate value-laden
and style-laden responses, LLMs and RMs are able to select the output that
aligns with the user's preferences. We evaluate six open-source and proprietary
LLMs and RMs under sixteen systematic prompting conditions and six preference
comparison scenarios. Notably, our results show that, when given the user's
full profile of values and stylistic preferences, the best models achieve <75%
accuracy at choosing the correct response, in contrast to >99% accuracy when
only relevant style and value preferences are provided. EVALUESTEER thus
highlights the limitations of current RMs at identifying and adapting to
relevant user profile information, and provides a challenging testbed for
developing RMs that can be steered towards diverse human values and
preferences.

</details>


### [16] [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
*Firoj Alam,Ali Ezzat Shahroor,Md. Arid Hasan,Zien Sheikh Ali,Hunzalah Hassan Bhatti,Mohamed Bayan Kmainasi,Shammur Absar Chowdhury,Basel Mousi,Fahim Dalvi,Nadir Durrani,Natasa Milic-Frayling*

Main category: cs.CL

TL;DR: This paper introduces EverydayMMQA and OASIS, a framework and dataset for creating culturally-grounded multimodal datasets for spoken and visual question answering. The dataset includes speech, images, and text, and is designed to test models on tasks beyond object recognition that involve pragmatic, commonsense, and culturally aware reasoning.


<details>
  <summary>Details</summary>
Motivation: Large-scale multimodal models achieve strong results on tasks like Visual Question Answering (VQA), but they often fail when queries require culturally grounded, everyday knowledge, particularly in low-resource and underrepresented languages.

Method: We introduced Everyday Multimodal and Multilingual QA (EverydayMMQA), a framework for creating large-scale, culturally-grounded datasets for spoken and visual question answering (SVQA). Using this framework, we developed OASIS, a multimodal dataset integrating speech, images, and text.

Result: With over ~0.92M images and 14.8M QA pairs, OASIS contains 3.7M spoken questions, enabling four unique input combinations: speech-only, text-only, speech+image, and text+image. We benchmarked four closed-source models, three open-source models, and one fine-tuned model.

Conclusion: EverydayMMQA and OASIS together provide a benchmark and training dataset for building multimodal LLMs for a comprehensive set of everyday tasks within cultural contexts. The framework and dataset will be made publicly available to the community.

Abstract: Large-scale multimodal models achieve strong results on tasks like Visual
Question Answering (VQA), but they often fail when queries require culturally
grounded, everyday knowledge, particularly in low-resource and underrepresented
languages. To bridge this gap, we introduce Everyday Multimodal and
Multilingual QA (EverydayMMQA), a framework for creating large-scale,
culturally-grounded datasets for spoken and visual question answering (SVQA).
Using this framework, we developed OASIS, a multimodal dataset integrating
speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS
contains 3.7M spoken questions, enabling four unique input combinations:
speech-only, text-only, speech+image, and text+image. Focused on English and
Arabic varieties, 18 countries, the dataset content is curated to reflect
diverse, real-world situations. OASIS tests models on tasks beyond object
recognition that involve pragmatic, commonsense, and culturally aware
reasoning. We benchmarked four closed-source models, three open-source models,
and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark
and training dataset for building multimodal LLMs for a comprehensive set of
everyday tasks within cultural contexts. The framework and dataset will be made
publicly available to the community.

</details>


### [17] [Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language](https://arxiv.org/abs/2510.06378)
*Angie Boggust,Donghao Ren,Yannick Assogba,Dominik Moritz,Arvind Satyanarayan,Fred Hohman*

Main category: cs.CL

TL;DR: Semantic regexes offer a structured and precise way to describe LLM features, improving automated interpretability and user understanding.


<details>
  <summary>Details</summary>
Motivation: Automated interpretability often produces vague and inconsistent natural language descriptions of LLM features, requiring manual relabeling.

Method: Introduce semantic regexes, which combine linguistic and semantic primitives with modifiers for contextualization, composition, and quantification.

Result: Semantic regexes match the accuracy of natural language while producing more concise and consistent descriptions. They also enable new analyses, such as quantifying feature complexity across layers and scaling interpretability to model-wide patterns.

Conclusion: Semantic regexes provide precise, consistent, and structured descriptions of LLM features, enabling better analysis and understanding.

Abstract: Automated interpretability aims to translate large language model (LLM)
features into human understandable descriptions. However, these natural
language feature descriptions are often vague, inconsistent, and require manual
relabeling. In response, we introduce semantic regexes, structured language
descriptions of LLM features. By combining primitives that capture linguistic
and semantic feature patterns with modifiers for contextualization,
composition, and quantification, semantic regexes produce precise and
expressive feature descriptions. Across quantitative benchmarks and qualitative
analyses, we find that semantic regexes match the accuracy of natural language
while yielding more concise and consistent feature descriptions. Moreover,
their inherent structure affords new types of analyses, including quantifying
feature complexity across layers, scaling automated interpretability from
insights into individual features to model-wide patterns. Finally, in user
studies, we find that semantic regex descriptions help people build accurate
mental models of LLM feature activations.

</details>


### [18] [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
*Pierre Lison,Mark Anderson*

Main category: cs.CL

TL;DR: 本文提出了一种方法，以在保留文本语义完整性的前提下对抗基于搜索的链接攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的去标识化模型无法解决链接风险，即可能将去标识化的文本映射回其来源。

Method: 首先构建文档集合中出现的N-grams的倒排索引，然后使用基于LLM的重写器迭代地重新表述那些跨度，直到无法进行链接为止。

Result: 在法院案例的语料库上进行的实验结果表明，该方法能够有效防止基于搜索的链接攻击，同时保持原始内容的忠实性。

Conclusion: 该方法能够有效防止基于搜索的链接攻击，同时保持原始内容的忠实性。

Abstract: While de-identification models can help conceal the identity of the
individual(s) mentioned in a document, they fail to address linkage risks,
defined as the potential to map the de-identified text back to its source. One
straightforward way to perform such linkages is to extract phrases from the
de-identified document and then check their presence in the original dataset.
This paper presents a method to counter search-based linkage attacks while
preserving the semantic integrity of the text. The method proceeds in two
steps. We first construct an inverted index of the N-grams occurring in the
document collection, making it possible to efficiently determine which N-grams
appear in less than $k$ documents (either alone or in combination with other
N-grams). An LLM-based rewriter is then iteratively queried to reformulate
those spans until linkage is no longer possible. Experimental results on a
collection of court cases show that the method is able to effectively prevent
search-based linkages while remaining faithful to the original content.

</details>


### [19] [Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion](https://arxiv.org/abs/2510.06386)
*Fan Zhou,Chang Tian,Tim Van de Cruys*

Main category: cs.CL

TL;DR: RegDiff 是一种无需预训练分类器即可实现高效属性可控文本生成的扩散框架。


<details>
  <summary>Details</summary>
Motivation: 生成具有特定属性的风格化文本是可控文本生成中的关键问题。现有的方法在属性控制和计算成本之间存在权衡。

Method: RegDiff 采用基于 VAE 的编码器-解码器架构以确保重建保真度，并使用带有属性监督的潜在扩散模型实现可控文本生成。属性信息仅在训练期间注入。

Result: 在五个涵盖多种风格属性的数据集上的实验表明，RegDiff 在生成风格化文本方面优于强基线方法。

Conclusion: RegDiff 是一种高效的属性可控文本扩散解决方案，实验结果验证了其有效性。

Abstract: Generating stylistic text with specific attributes is a key problem in
controllable text generation. Recently, diffusion models have emerged as a
powerful paradigm for both visual and textual generation. Existing approaches
can be broadly categorized into classifier-free guidance (CFG) and classifier
guidance (CG) methods. While CFG effectively preserves semantic content, it
often fails to provide effective attribute control. In contrast, CG modifies
the denoising trajectory using classifier gradients, enabling better attribute
alignment but incurring high computational costs during sampling and suffering
from classifier generalization issues. In this work, we propose RegDiff, a
regularized diffusion framework that leverages attribute features without
requiring a pretrained classifier during sampling, thereby achieving
controllable generation with reduced computational costs. Specifically, RegDiff
employs a VAE-based encoder--decoder architecture to ensure reconstruction
fidelity and a latent diffusion model trained with attribute supervision to
enable controllable text generation. Attribute information is injected only
during training. Experiments on five datasets spanning multiple stylistic
attributes demonstrate that RegDiff outperforms strong baselines in generating
stylistic texts. These results validate the effectiveness of RegDiff as an
efficient solution for attribute-controllable text diffusion. Our code,
datasets, and resources will be released upon publication at
https://github.com/xxxx.

</details>


### [20] [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
*Elle*

Main category: cs.CL

TL;DR: 我们的研究揭示了奖励模型在对齐语言模型时可能存在的社会偏见，并强调了需要更仔细地考虑奖励模型的行为。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在语言模型对齐中起着核心作用，但对其行为的理解有限。我们需要更好地理解奖励模型的行为，以防止在语言技术中传播不必要的社会偏见。

Method: 我们提出了一个框架来衡量奖励模型所捕捉的意见的一致性，调查奖励模型在多大程度上表现出社会人口统计学偏差，并探索了提示如何引导奖励朝着目标群体的偏好发展。

Result: 我们发现奖励模型与几个社会人口统计学群体不一致，并且可以系统地奖励有害的刻板印象，单独的引导不足以克服这些局限性。

Conclusion: 我们的研究结果强调了在偏好学习过程中对奖励模型行为进行更仔细考虑的必要性，以防止在我们使用的语言技术中传播不需要的社会偏见。

Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An
RM often serves as a proxy for human preferences to guide downstream LM
behavior. However, our understanding of RM behavior is limited. Our work (i)
formalizes a framework for measuring the alignment of opinions captured by RMs,
(ii) investigates the extent to which RMs demonstrate sociodemographic biases,
and (iii) explores the effects of prompting to steer rewards towards the
preferences of a target group. We study the subjective and diverse perspectives
on controversial topics, which allows us to quantify RM perspectives in terms
of their opinions, attitudes, and values. We show that RMs are poorly aligned
with several demographic groups and can systematically reward harmful
stereotypes, and steering alone is not enough to overcome these limitations.
Our findings underscore the need for more careful consideration of RM behavior
in model alignment during preference learning to prevent the propagation of
unwanted social biases in the language technologies that we use.

</details>


### [21] [Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?](https://arxiv.org/abs/2510.06411)
*R. Alexander Knipper,Indrani Dey,Souvika Sarkar,Hari Narayanan,Sadhana Puntambekar,Santu Karmaker*

Main category: cs.CL

TL;DR: 本文介绍了一种新的教学目标对齐问题生成框架，利用LLMs帮助教师生成与模拟对齐、具有教育意义的问题。通过目标和实验室理解，问题分类法提高了认知需求，优化的TELeR提示增强了格式遵循性，大模型带来了最大的收益。


<details>
  <summary>Details</summary>
Motivation: 虚拟实验室为科学学习提供了宝贵的机会，但教师常常难以将其适应自己的教学目标。第三方材料可能不符合课堂需求，而开发自定义资源既耗时又难以扩展。最近的大型语言模型（LLMs）的进步为解决这些限制提供了有希望的途径。

Method: 引入了一个新的对齐框架，用于教学目标对齐的问题生成，通过自然语言交互让教师利用LLMs生成与模拟对齐、具有教育意义的问题。框架包括四个组件：通过教师-LLM对话进行教学目标理解，通过知识单元和关系分析进行实验室理解，问题分类法用于构建认知和教学意图，以及TELeR分类法用于控制提示细节。

Result: 早期设计选择由一个小规模的教师辅助案例研究提供信息，最终评估分析了来自19个开源LLMs的1100多个问题。目标和实验室理解使问题符合教师意图和模拟上下文，问题分类法提高了认知需求（开放格式和关系类型提高质量0.29-0.39点），优化的TELeR提示提高了格式遵循性（80%可解析性，>90%遵循性）。大模型带来了最强的收益：可解析性+37.1%，遵循性+25.7%，平均质量+0.8李克特点。

Conclusion: 通过目标和实验室理解，问题分类法提高了认知需求，优化的TELeR提示增强了格式遵循性，大模型带来了最大的收益。

Abstract: Virtual Labs offer valuable opportunities for hands-on, inquiry-based science
learning, yet teachers often struggle to adapt them to fit their instructional
goals. Third-party materials may not align with classroom needs, and developing
custom resources can be time-consuming and difficult to scale. Recent advances
in Large Language Models (LLMs) offer a promising avenue for addressing these
limitations. In this paper, we introduce a novel alignment framework for
instructional goal-aligned question generation, enabling teachers to leverage
LLMs to produce simulation-aligned, pedagogically meaningful questions through
natural language interaction. The framework integrates four components:
instructional goal understanding via teacher-LLM dialogue, lab understanding
via knowledge unit and relationship analysis, a question taxonomy for
structuring cognitive and pedagogical intent, and the TELeR taxonomy for
controlling prompt detail. Early design choices were informed by a small
teacher-assisted case study, while our final evaluation analyzed over 1,100
questions from 19 open-source LLMs. With goal and lab understanding grounding
questions in teacher intent and simulation context, the question taxonomy
elevates cognitive demand (open-ended formats and relational types raise
quality by 0.29-0.39 points), and optimized TELeR prompts enhance format
adherence (80% parsability, >90% adherence). Larger models yield the strongest
gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert
points.

</details>


### [22] [FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering](https://arxiv.org/abs/2510.06426)
*Yitao Long,Tiansheng Hu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: 本文介绍了 FinLFQA 基准，用于评估 LLMs 在生成复杂金融问题的答案时提供可靠和细致引用的能力。通过实验发现细粒度指标对区分模型能力很重要，端到端生成性能与事后方法相当，而迭代优化只有在外部反馈指导下才有效。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要关注简单的引用，即检索支持文本证据作为参考。然而，在现实场景如金融应用中，引用超出了参考检索的范围。因此，需要一个更全面的基准来评估 LLMs 在生成复杂金融问题的答案时的引用能力。

Method: 引入了 FinLFQA 基准，用于评估 LLMs 生成复杂金融问题的长篇答案并提供可靠和细致的引用的能力。FinLFQA 通过人工标注评估了三个关键方面的引用：(1) 从财务报告中提取的支持证据，(2) 中间的数值推理步骤，以及 (3) 用于推理过程的领域特定财务知识。还提供了一个自动评估框架，涵盖答案质量和引用质量。

Result: 通过在多个 attribution-generation 模式下对八种 LLMs 进行广泛实验，发现细粒度指标对于区分模型能力很重要，端到端生成在性能上与事后方法相当，而迭代优化只有在外部反馈的指导下才有帮助。

Conclusion: 通过在多个 attribution-generation 模式下对八种 LLMs 进行广泛实验，我们发现细粒度指标对于区分模型能力很重要，端到端生成在性能上与事后方法相当，而迭代优化只有在外部反馈的指导下才有帮助。

Abstract: Large Language Models (LLMs) frequently hallucinate to long-form questions,
producing plausible yet factually incorrect answers. A common mitigation
strategy is to provide attribution to LLM outputs. However, existing benchmarks
primarily focus on simple attribution that retrieves supporting textual
evidence as references. We argue that in real-world scenarios such as financial
applications, attribution goes beyond reference retrieval. We introduce
FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate
long-form answers to complex financial questions with reliable and nuanced
attributions. FinLFQA evaluates three critical aspects of attribution through
human annotations: (1) supporting evidence extracted from financial reports,
(2) intermediate numerical reasoning steps, and (3) domain-specific financial
knowledge that informs the reasoning process. We further provide an automatic
evaluation framework covering both answer quality and attribution quality.
Through extensive experiments on eight LLMs across multiple
attribution-generation paradigms, we find that fine-grained metrics are
important to distinguish model capabilities, that end-to-end generation
achieves comparable performance to post-hoc approaches, and that iterative
refinement only helps when guided by external feedback.

</details>


### [23] [Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser](https://arxiv.org/abs/2510.06427)
*Elena Chistova*

Main category: cs.CL

TL;DR: UniRST是第一个能够处理18个语料库在11种语言中而无需修改关系库存的统一RST风格话语解析器。


<details>
  <summary>Details</summary>
Motivation: 为了处理18个语料库在11种语言中的统一RST风格话语解析，而无需修改它们的关系库存。

Method: 提出并评估了两种训练策略：Multi-Head和Masked-Union，以克服库存不兼容问题。

Result: 参数高效的Masked-Union方法表现最强，并且UniRST优于18个单语库中的16个基线。

Conclusion: UniRST展示了一个单一模型、多语言端到端话语解析的优势，优于18个单语库中的16个基线。

Abstract: We introduce UniRST, the first unified RST-style discourse parser capable of
handling 18 treebanks in 11 languages without modifying their relation
inventories. To overcome inventory incompatibilities, we propose and evaluate
two training strategies: Multi-Head, which assigns separate relation
classification layer per inventory, and Masked-Union, which enables shared
parameter training through selective label masking. We first benchmark
monotreebank parsing with a simple yet effective augmentation technique for
low-resource settings. We then train a unified model and show that (1) the
parameter efficient Masked-Union approach is also the strongest, and (2) UniRST
outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a
single-model, multilingual end-to-end discourse parsing across diverse
resources.

</details>


### [24] [MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning](https://arxiv.org/abs/2510.06430)
*Neeraja Kirtane,Yuvraj Khanna,Peter Relan*

Main category: cs.CL

TL;DR: 本文介绍了MathRobust-LV，一个测试集和评估方法，用于评估大语言模型在面对语言变化时的数学推理鲁棒性。实验结果表明，模型的准确性在面对语言变化时会下降，尤其是较小的模型。


<details>
  <summary>Details</summary>
Motivation: 我们相信在真实教育环境中对高中水平数学问题进行全面基准测试的重要性。在这些应用中，教师以各种方式重新表述相同的概念，使得语言鲁棒性对于可靠部署至关重要。

Method: 我们引入了MathRobust-LV，这是一个测试集和评估方法，模仿教师在保持难度恒定的情况下重新表述问题的方式：我们改变表面细节（名称、上下文、变量），同时保留数值结构和答案。

Result: 我们的实验显示，从基线到变体的准确性下降。这些下降对于较小的模型（9-11%）尤为严重，而更强的模型也显示出可测量的退化。前沿模型如GPT-5、Gemini-2.5pro相对稳定。

Conclusion: 我们的结果表明，对语言变化的鲁棒性是一个基本挑战，暴露了模型中的推理脆弱性。

Abstract: Large language models excel on math benchmarks, but their math reasoning
robustness to linguistic variation is underexplored. While recent work
increasingly treats high-difficulty competitions like the IMO as the gold
standard for evaluating reasoning, we believe in comprehensive benchmarking of
high school-level math problems in real educational settings. We introduce
MathRobust-LV, a test set and evaluation methodology that mirrors how
instructors rephrase problems across assessments while keeping difficulty
constant: we change surface details (names, contexts, variables) while
preserving numerical structure and answers. In contrast to prior efforts that
alter problem content or emphasize IMO-level tasks, we focus on
high-school-level dataset problems at the difficulty level where models are
currently deployed in educational settings: tutoring and assessment systems. In
these applications, instructors rephrase identical concepts in varied ways,
making linguistic robustness essential for reliable deployment. Although MATH
data benchmarking is often regarded as saturated, our experiment on 34 models
reveals that accuracy declines when moving from the baseline to the variants.
These drops are severe for smaller models (9-11%) while stronger models also
show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain
comparatively stable. Our results highlight that robustness to linguistic
variation is a fundamental challenge, exposing reasoning vulnerabilities in
models.

</details>


### [25] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: 本文对代理安全领域进行了全面调查，提出了一个围绕应用、威胁和防御三个相互依赖支柱的框架，并指出了模型和模态覆盖方面的研究空白。


<details>
  <summary>Details</summary>
Motivation: 随着被动大语言模型向自主大语言模型代理的快速转变，网络安全领域出现了一个新的范式。这些代理在进攻和防御操作中都可能成为强大的工具，但其代理环境引入了一类固有的安全风险。因此，需要对代理安全景观进行全面调查。

Method: 本文对超过150篇论文进行了全面分类，分析了代理的使用方式、它们所具有的漏洞以及设计用于保护它们的对策。

Result: 本文提供了对代理安全领域的全面调查，揭示了代理架构中的新兴趋势，并指出了模型和模态覆盖方面的关键研究差距。

Conclusion: 本文对代理安全领域进行了全面的调查，提出了一个围绕应用、威胁和防御三个相互依赖支柱的框架，并指出了模型和模态覆盖方面的研究空白。

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [26] [Linguistically Informed Tokenization Improves ASR for Underresourced Languages](https://arxiv.org/abs/2510.06461)
*Massimo Daul,Alessio Tosolini,Claire Bowern*

Main category: cs.CL

TL;DR: 本文研究了在资源匮乏的语言中使用ASR的可能性，并发现通过音素分词可以提高ASR的性能，同时手动校正ASR输出比从头开始转录音频更快。


<details>
  <summary>Details</summary>
Motivation: 现代ASR系统使用数据密集型的transformer架构，这使得它们通常无法用于资源匮乏的语言。我们需要找到一种方法来改善资源匮乏语言的ASR性能。

Method: 我们在Yan-nhangu语言上微调了一个wav2vec2 ASR模型，并比较了音素和正字法分词策略对性能的影响。同时，我们探索了ASR在语言文档流程中的可行性。

Result: 我们发现，基于语言学的音素分词系统相比基线正字法分词方案显著提高了WER和CER。

Conclusion: 我们展示了ASR可以用于资源匮乏的语言，通过手动校正ASR模型的输出比从头开始手动转录音频要快得多。

Abstract: Automatic speech recognition (ASR) is a crucial tool for linguists aiming to
perform a variety of language documentation tasks. However, modern ASR systems
use data-hungry transformer architectures, rendering them generally unusable
for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,
a dormant Indigenous Australian language, comparing the effects of phonemic and
orthographic tokenization strategies on performance. In parallel, we explore
ASR's viability as a tool in a language documentation pipeline. We find that a
linguistically informed phonemic tokenization system substantially improves WER
and CER compared to a baseline orthographic tokenization scheme. Finally, we
show that hand-correcting the output of an ASR model is much faster than
hand-transcribing audio from scratch, demonstrating that ASR can work for
underresourced languages.

</details>


### [27] [Test-Time Scaling of Reasoning Models for Machine Translation](https://arxiv.org/abs/2510.06471)
*Zihao Li,Shaoxiong Ji,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 本研究探讨了推理时间缩放（TTS）在机器翻译中的效果，发现其在特定任务和微调模型中表现良好，但在通用模型中效果有限。


<details>
  <summary>Details</summary>
Motivation: 研究增加推理时间计算是否能提高翻译质量。

Method: 评估12个RM在多种MT基准测试中的表现，分析三种场景：直接翻译、强制推理外推和后期编辑。

Result: 对于通用RM，TTS在直接翻译中提供的收益有限且不一致，但通过领域特定微调可以解锁TTS的有效性，导致持续改进。强制模型超出其自然停止点会降低翻译质量。在后期编辑情况下，TTS非常有效。

Conclusion: TTS在机器翻译中的价值不在于增强通用模型的单次翻译，而在于多步骤、自我纠正的工作流程以及与任务专业化模型结合的应用。

Abstract: Test-time scaling (TTS) has enhanced the performance of Reasoning Models
(RMs) on various tasks such as math and coding, yet its efficacy in machine
translation (MT) remains underexplored. This paper investigates whether
increased inference-time computation improves translation quality. We evaluate
12 RMs across a diverse suite of MT benchmarks spanning multiple domains,
examining three scenarios: direct translation, forced-reasoning extrapolation,
and post-editing. Our findings show that for general-purpose RMs, TTS provides
limited and inconsistent benefits for direct translation, with performance
quickly plateauing. However, the effectiveness of TTS is unlocked by
domain-specific fine-tuning, which aligns a model's reasoning process with task
requirements, leading to consistent improvements up to an optimal,
self-determined reasoning depth. We also find that forcing a model to reason
beyond its natural stopping point consistently degrades translation quality. In
contrast, TTS proves highly effective in a post-editing context, reliably
turning self-correction into a beneficial process. These results indicate that
the value of inference-time computation in MT lies not in enhancing single-pass
translation with general models, but in targeted applications like multi-step,
self-correction workflows and in conjunction with task-specialized models.

</details>


### [28] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 本文介绍了一个名为Webscale-RL的管道，用于生成大量多样化的问答对以用于强化学习。实验表明，基于此数据集的模型表现优于现有方法，并且在训练效率上具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习数据集比网络规模的预训练语料库小得多且多样性不足，这限制了其应用。

Method: 我们引入了Webscale-RL管道，这是一种可扩展的数据引擎，能够系统地将大规模预训练文档转换为数百万种多样且可验证的问答对用于强化学习。

Result: 在Webscale-RL数据集上训练的模型在多个基准测试中显著优于持续预训练和强大的数据精炼基线。此外，与持续预训练相比，使用我们的数据集进行强化学习训练更为高效，可以以少至100倍的标记数量达到相同性能。

Conclusion: 我们的工作展示了一条将强化学习扩展到预训练水平的可行路径，使语言模型更加强大和高效。

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [29] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: 本研究通过实验分析了自举预训练的扩展行为，发现其扩展效率会随着基础模型预训练标记数量的增加而降低，并提出了一个简单的扩展定律来描述这种现象。研究结果揭示了多阶段预训练策略中的基本权衡，并为高效的语言模型训练提供了实用的见解。


<details>
  <summary>Details</summary>
Motivation: 自举预训练（即重新使用预训练的基础模型进行进一步预训练）在减少从头开始训练语言模型的成本方面很有前景。然而，其有效性仍然不明确，尤其是在应用于过度预训练的基础模型时。

Method: 我们通过实验研究了自举预训练的扩展行为，并发现其扩展效率以可预测的方式降低：随着用于预训练基础模型的标记数量增加，相对于第二阶段预训练标记的扩展指数呈对数下降。第一阶段和第二阶段标记的联合依赖性由一个简单的扩展定律准确建模。

Result: 我们发现自举预训练的扩展效率会以可预测的方式降低：随着用于预训练基础模型的标记数量增加，相对于第二阶段预训练标记的扩展指数呈对数下降。第一阶段和第二阶段标记的联合依赖性由一个简单的扩展定律准确建模。

Conclusion: 我们的研究结果为高效的语言模型训练提供了实用的见解，并对过度预训练模型的再利用提出了重要的考虑因素。

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [30] [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
*Tarek Naous,Philippe Laban,Wei Xu,Jennifer Neville*

Main category: cs.CL

TL;DR: 本文提出了一种专门用于模拟多轮对话中用户行为的模型（User LMs），并展示了其优于现有模拟方法的表现。研究还发现，更真实的模拟环境会让强大的助手（如GPT-4o）遇到困难。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是评估语言模型在现实场景中的表现，而之前的文献通常使用原本被训练为有用助手的语言模型作为用户模拟器。然而，研究发现，更好的助手反而成为较差的用户模拟器，因此需要一种专门用于模拟用户行为的模型。

Method: 本文的方法是引入专门构建的用户语言模型（User LMs），这些模型经过后训练以模拟多轮对话中的真实用户行为。此外，还进行了各种评估来比较User LMs与其他模拟方法的表现。

Result: 研究结果表明，User LMs比现有的模拟方法更能与人类行为对齐，并且在模拟鲁棒性方面表现更好。此外，在利用User LMs模拟编程和数学对话时，GPT-4o的表现从74.6%下降到57.4%，证明了更真实的模拟环境会使助手遇到困难。

Conclusion: 本文结论是，通过引入专门构建的用户语言模型（User LMs），可以更好地模拟人类用户行为，并在多轮对话中实现更稳健的模拟。此外，研究还表明，更真实的模拟环境会使强大的助手（如GPT-4o）遇到困难，因为它们无法应对多轮设置中用户的细微差别。

Abstract: Conversations with LMs involve two participants: a human user leading the
conversation, and an LM assistant responding to the user's request. To satisfy
this specific role, LMs are post-trained to be helpful assistants -- optimized
to produce exhaustive and well-structured responses, free of ambiguity and
grammar errors. User utterances, on the other hand, are rarely perfected, with
each user phrasing requests in unique ways, sometimes putting in partial effort
at each turn and refining on the fly. To evaluate LM performance in realistic
settings, prior work simulated users in multi-turn conversations, often
prompting an LLM originally trained to be a helpful assistant to act as a user.
However, we show that assistant LMs make for poor user simulators, with the
surprising finding that better assistants yield worse simulators. Instead, we
introduce purpose-built User Language Models (User LMs) - models post-trained
to simulate human users in multi-turn conversations. Through various
evaluations, we show how User LMs align better with human behavior and achieve
better simulation robustness than existing simulation methods. When leveraging
User LMs to simulate coding and math conversations, the performance of a strong
assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic
simulation environments lead to assistant struggles as they fail to cope with
the nuances of users in multi-turn setups.

</details>


### [31] [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
*Cheonkam Jeong,Sungdo Kim,Jewoo Park*

Main category: cs.CL

TL;DR: 本文提出了一种基于类型理论的框架，旨在解决语言模型在处理描述性、规范性和法律维度时的问题，并通过神经符号架构实现跨司法管辖区的合规性分析。


<details>
  <summary>Details</summary>
Motivation: 当代语言模型虽然流畅，但经常错误地处理其输出所蕴含的类型意义。作者认为，幻觉、脆弱的监管和不透明的合规结果是缺乏类型理论语义的表现，而不是数据或规模的限制。

Method: 本文基于蒙塔古的语言作为类型化、组合代数的观点，重新将对齐视为解析问题：自然语言输入必须被编译成显式体现其描述性、规范性和法律维度的结构。提出了Savassan，这是一种神经符号架构，能够将话语编译成蒙塔古风格的逻辑形式，并将其映射到扩展了义务操作符和司法辖区上下文的类型本体。

Result: 本文贡献了：(i) 将幻觉诊断为类型错误；(ii) 用于商业/法律推理的正式蒙塔古本体桥接；以及(iii) 一种嵌入类型接口的生产导向设计。

Conclusion: 本文认为，值得信赖的自主性需要对意义进行组合类型化，使系统能够在统一的意义代数中推理什么是被描述的、什么是被规定的以及什么会带来责任。

Abstract: Contemporary language models are fluent yet routinely mis-handle the types of
meaning their outputs entail. We argue that hallucination, brittle moderation,
and opaque compliance outcomes are symptoms of missing type-theoretic semantics
rather than data or scale limitations. Building on Montague's view of language
as typed, compositional algebra, we recast alignment as a parsing problem:
natural-language inputs must be compiled into structures that make explicit
their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances
into Montague-style logical forms and maps them to typed ontologies extended
with deontic operators and jurisdictional contexts. Neural components extract
candidate structures from unstructured inputs; symbolic components perform type
checking, constraint reasoning, and cross-jurisdiction mapping to produce
compliance-aware guidance rather than binary censorship. In cross-border
scenarios, the system "parses once" (e.g., defect claim(product x, company y))
and projects the result into multiple legal ontologies (e.g., defamation risk
in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into
a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error;
(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)
a production-oriented design that embeds typed interfaces across the pipeline.
We outline an evaluation plan using legal reasoning benchmarks and synthetic
multi-jurisdiction suites. Our position is that trustworthy autonomy requires
compositional typing of meaning, enabling systems to reason about what is
described, what is prescribed, and what incurs liability within a unified
algebra of meaning.

</details>


### [32] [TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents](https://arxiv.org/abs/2510.06579)
*Haofei Yu,Keyang Xuan,Fenghai Li,Kunlun Zhu,Zijie Lei,Jiaxun Zhang,Ziheng Qi,Kyle Richardson,Jiaxuan You*

Main category: cs.CL

TL;DR: TinyScientist 提出了一种交互式、可扩展且可控的框架，以应对自动研究工作流的复杂性，并通过开源代码库、交互式网络演示和 PyPI Python 包使最先进的自动研究管道对每位研究人员和开发者都易于访问。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的研究人员和开发者开始使用和构建这些工具和平台，扩展和维护这种代理工作流的复杂性和难度已成为一个重要挑战，特别是在算法和架构不断进步的情况下。

Method: TinyScientist 识别了自动研究工作流的关键组件，并提出了一个交互式、可扩展且可控的框架，以适应新工具并支持迭代增长。

Result: TinyScientist 提供了一个开源代码库、一个交互式网络演示和一个 PyPI Python 包，使最先进的自动研究管道广泛可用。

Conclusion: TinyScientist 提供了一个交互式、可扩展且可控的框架，以应对自动研究工作流的复杂性，并通过开源代码库、交互式网络演示和 PyPI Python 包使最先进的自动研究管道对每位研究人员和开发者都易于访问。

Abstract: Automatic research with Large Language Models (LLMs) is rapidly gaining
importance, driving the development of increasingly complex workflows involving
multi-agent systems, planning, tool usage, code execution, and human-agent
interaction to accelerate research processes. However, as more researchers and
developers begin to use and build upon these tools and platforms, the
complexity and difficulty of extending and maintaining such agentic workflows
have become a significant challenge, particularly as algorithms and
architectures continue to advance. To address this growing complexity,
TinyScientist identifies the essential components of the automatic research
workflow and proposes an interactive, extensible, and controllable framework
that easily adapts to new tools and supports iterative growth. We provide an
open-source codebase, an interactive web demonstration, and a PyPI Python
package to make state-of-the-art auto-research pipelines broadly accessible to
every researcher and developer.

</details>


### [33] [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: 本文研究了越狱现象，通过检查 LLM 的内部表示，发现不同层的行为差异，并提出了利用内部模型动态进行越狱检测和防御的潜在方向。


<details>
  <summary>Details</summary>
Motivation: 越狱大型语言模型已成为一个紧迫的问题，因为对话型 LLM 的普及和可访问性不断增加。攻击者不断开发新的提示技术，没有任何现有模型可以被认为是完全抗性的。

Method: 我们通过检查 LLM 的内部表示来研究越狱现象，重点关注隐藏层对越狱和良性提示的响应。

Result: 我们分析了开源 LLM GPT-J 和状态空间模型 Mamba2，初步结果突出了不同的逐层行为。

Conclusion: 我们的结果表明，利用内部模型动态进行稳健的越狱检测和防御有 promising 的研究方向。

Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern
with the increasing prevalence and accessibility of conversational LLMs.
Adversarial users often exploit these models through carefully engineered
prompts to elicit restricted or sensitive outputs, a strategy widely referred
to as jailbreaking. While numerous defense mechanisms have been proposed,
attackers continuously develop novel prompting techniques, and no existing
model can be considered fully resistant. In this study, we investigate the
jailbreak phenomenon by examining the internal representations of LLMs, with a
focus on how hidden layers respond to jailbreak versus benign prompts.
Specifically, we analyze the open-source LLM GPT-J and the state-space model
Mamba2, presenting preliminary findings that highlight distinct layer-wise
behaviors. Our results suggest promising directions for further research on
leveraging internal model dynamics for robust jailbreak detection and defense.

</details>


### [34] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: 本文首次对SSMs和TBMs中的表示传播进行了统一的token和层级分析，揭示了它们在表示同化方面的关键差异。


<details>
  <summary>Details</summary>
Motivation: 研究状态空间模型（SSMs）和基于Transformer的模型（TBMs）中上下文信息的流动，以理解它们的表示传播机制。

Method: 使用中心核对齐、稳定性度量和探测来表征表示在层内和层间的演变。

Result: TBMs迅速同化token表示，而SSMs在早期保持token独特性但后期趋于同化。

Conclusion: 这些见解阐明了两种架构的归纳偏差，并为长期上下文推理的未来模型和训练设计提供了信息。

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [35] [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
*Shangjian Yin,Zhepei Wei,Xinyu Zhu,Wei-Lin Chen,Yu Meng*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Self-Alignment Optimization (SAO) 的自合成框架，用于大型语言模型的对齐，所有训练数据均由模型自身生成，实验结果表明该方法在提升模型聊天能力的同时，保持了在下游任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于人类反馈的强化学习（RLHF）和基于AI反馈的强化学习（RLAIF）都需要昂贵的人类标注数据集或外部奖励模型，而这些方法的成本很高。

Method: 我们引入了Self-Alignment Optimization (SAO)，这是一个完全自合成的框架，用于大型语言模型的对齐，其中所有训练数据都是由模型自身生成的。

Result: 实验表明，SAO在标准基准测试（如AlpacaEval~2.0）上有效提升了模型的聊天能力，同时在下游客观任务（如问答、数学推理）上保持了强大的性能。

Conclusion: 我们的工作为对齐大型语言模型提供了实用的解决方案，并且可以自我改进。

Abstract: Traditional reinforcement learning from human feedback (RLHF) for large
language models (LLMs) relies on expensive human-annotated datasets, while
Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,
requiring the collection of diverse prompts and corresponding responses, often
necessitating external reward models or proprietary models like GPT-4 to
annotate preference pairs. In this work, we introduce Self-Alignment
Optimization (SAO), a fully self-synthetic framework for LLM alignment, where
all training data, including prompts (i.e., user queries), responses, and
preferences, are generated by the model itself. Specifically, SAO first
instructs the LLM to engage in persona role-play and generate diverse prompts
and responses, which are then self-evaluated for preference optimization.
Extensive experiments demonstrate that SAO effectively enhances the model's
chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining
strong performance on downstream objective tasks (e.g., question-answering,
math reasoning). Our work provides a practical solution for self-improvement in
aligning LLMs, and the code for reproducing our results is available at:
https://github.com/SJY8460/SAO.

</details>


### [36] [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
*Yunzhong Xiao,Yangmin Li,Hewei Wang,Yunlong Tang,Zora Zhiruo Wang*

Main category: cs.CL

TL;DR: This paper introduces ToolMem, a method that allows agents to remember the capabilities of different tools from past interactions, enabling more accurate predictions of tool performance and better selection of the optimal tool for tasks.


<details>
  <summary>Details</summary>
Motivation: Existing agents typically rely on fixed tools, which limits flexibility in selecting the most suitable tool for specific tasks. Humans, however, improve their understanding of different tools through interaction and use this knowledge to select the optimal tool for future tasks. The goal is to build agents that benefit from a similar process.

Method: ToolMem is proposed to enable agents to develop memories of tool capabilities by summarizing their strengths and weaknesses from previous interactions and storing them in memory. At inference, the agent retrieves relevant entries from ToolMem to select the best tool for individual tasks.

Result: ToolMem-augmented agents predict tool performance 14.8% and 28.7% more accurately across text and multimodal generation scenarios compared to no-memory, generic agents. Additionally, ToolMem improves optimal tool selection among multiple choices by 21% and 24% absolute increases in respective scenarios.

Conclusion: ToolMem-augmented agents can predict tool performance more accurately and select the optimal tool for tasks, improving performance in text and multimodal generation scenarios.

Abstract: Agents utilizing tools powered by large language models (LLMs) or
vision-language models (VLMs) have demonstrated remarkable progress in diverse
tasks across text and visual modalities. Unlike traditional tools such as
calculators, which give deterministic outputs, neural tools perform uncertainly
across task scenarios. While different tools for a task may excel in varied
scenarios, existing agents typically rely on fixed tools, thus limiting the
flexibility in selecting the most suitable tool for specific tasks. In
contrast, humans snowball their understanding of the capabilities of different
tools by interacting with them, and apply this knowledge to select the optimal
tool when solving a future task. To build agents that similarly benefit from
this process, we propose ToolMem that enables agents to develop memories of
tool capabilities from previous interactions, by summarizing their strengths
and weaknesses and storing them in memory; at inference, the agent can retrieve
relevant entries from ToolMem, and select the best tool to solve individual
tasks more accurately. We evaluate ToolMem on learning varied text generation
and text-to-image generation neural tools. Compared to no-memory, generic
agents, we find ToolMem-augmented agents predict tool performance 14.8% and
28.7% more accurately across text and multimodal generation scenarios.
Moreover, ToolMem facilitates optimal tool selection among multiple choices by
21% and 24% absolute increases in respective scenarios.

</details>


### [37] [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
*Shangjian Yin,Shining Liang,Wenbiao Ding,Yuli Qian,Zhouxing Shi,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: 研究提出了一種數據高效的對齊數據集PiKa，證明可以用更少的數據實現高質量的對齊，並在多個模型上取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 現有的對齊數據集要么是私有的，要么需要昂貴的人類標註，這限制了可重現性和可擴展性。此外，目前的方法通常依賴於超過30萬個示例，但仍然表現不佳。

Method: 引入PiKa數據集，使用較少的SFT示例進行微調，並在多個模型上進行測試以驗證其有效性。

Result: PiKa-SFT數據集僅使用3萬個SFT示例，卻在AlpacaEval 2.0和Arena-Hard基準測試中表現優於使用數百萬個專有示例訓練的模型。

Conclusion: 高質量的對齊可以通過顯著減少數據量來實現，為開源大型語言模型的對齊提供了一條可擴展的路徑。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone
for aligning large language models (LLMs). However, its effectiveness depends
on high-quality instruction data. Most existing alignment datasets are either
private or require costly human annotation, which limits reproducibility and
scalability. Even with Reinforcement Learning from AI Feedback (RLAIF),
concerns about data quality remain. Moreover, it is unclear how much data is
actually required to fine-tune a base model into a strong instruction-following
model. Current approaches often rely on over 300k examples even at the
supervised fine-tuning (SFT) stage, yet they still underperform compared to
proprietary models, creating barriers for academic and resource-limited
communities. To address this gap, we introduce PiKa, a data-efficient family of
expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only
30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through
evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,
we show that PiKa-SFT outperforms models trained on much larger data. On
AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses
the official Llama-3-8B-Instruct model trained on over 10 million proprietary
examples. We further extend our study by training the Qwen2.5 series (0.5B to
7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that
high-quality alignment can be achieved with significantly less data, offering a
scalable path for open-source LLM alignment. Code and data:
https://github.com/SJY8460/PiKa.

</details>


### [38] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: 本文介绍了一种增量摘要系统，通过持续反馈提高摘要质量和代理生产力。


<details>
  <summary>Details</summary>
Motivation: 我们引入了一个增量摘要系统，帮助客户支持代理在对话中智能地生成简洁的要点笔记，减少代理的上下文切换努力和重复审查。

Method: 我们的方法结合了微调的Mixtral-8x7B模型用于连续笔记生成和DeBERTa分类器来过滤琐碎内容。代理编辑改进了在线笔记生成，并定期通知离线模型重新训练，形成了代理编辑反馈循环。

Result: 在生产环境中部署后，我们的系统相比批量摘要减少了3%的案例处理时间（在高度复杂的案例中最多减少了9%），同时获得了高满意度评分。

Conclusion: 这些结果表明，带有持续反馈的增量摘要可以有效提高摘要质量和代理生产力。

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [39] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: 本文提出了一种针对机器翻译任务的新型提示优化方法，利用小参数模型和反向翻译策略，有效降低了训练成本并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的提示工程方法主要集中在优化指令组件上，但对于像机器翻译这样的任务，输入组件更为关键，因此需要一种专门针对此类任务的优化方法。

Method: 本文提出的方法采用了一个基于反向翻译策略训练的小参数模型，以优化机器翻译任务中的输入组件。

Result: 该方法在机器翻译任务中表现出色，同时具有较低的训练开销，并且可以适应其他下游任务。

Conclusion: 本文提出了一种专门针对机器翻译任务的新型提示优化方法，该方法使用基于反向翻译的策略训练小参数模型，显著降低了单任务优化的训练开销，并实现了高效性能。此外，该方法经过适当调整后可以扩展到其他下游任务。

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [40] [How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects](https://arxiv.org/abs/2510.06700)
*Leonardo Bertolazzi,Sandro Pezzelle,Raffaelle Bernardi*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的内容效应，并通过分析其内部表示揭示了有效性与可信度之间的关系。研究还展示了如何通过转向向量和去偏置向量来减少内容效应，提高推理准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管人类中的内容效应最好由推理的双过程理论解释，但大型语言模型中的内容效应机制仍然不清楚。

Method: 我们通过研究大型语言模型如何在其内部表示中编码有效性和可信度的概念来解决这个问题。我们使用转向向量展示了可信度向量可以因果地偏倚有效性判断，反之亦然，并且这两个概念之间的对齐程度预测了模型行为内容效应的大小。最后，我们构建了去偏置向量来解耦这些概念。

Result: 我们发现有效性和可信度在线性表示中强烈对齐，导致模型将可信度与有效性混淆。使用转向向量，我们证明了可信度向量可以因果地偏倚有效性判断，反之亦然，并且这两个概念之间的对齐程度预测了模型行为内容效应的大小。最后，我们构建了去偏置向量来解耦这些概念，减少了内容效应并提高了推理准确性。

Conclusion: 我们的研究结果加深了对大型语言模型中抽象逻辑概念表示的理解，并突出了表征干预作为实现更逻辑系统的一种途径。

Abstract: Both humans and large language models (LLMs) exhibit content effects: biases
in which the plausibility of the semantic content of a reasoning problem
influences judgments regarding its logical validity. While this phenomenon in
humans is best explained by the dual-process theory of reasoning, the
mechanisms behind content effects in LLMs remain unclear. In this work, we
address this issue by investigating how LLMs encode the concepts of validity
and plausibility within their internal representations. We show that both
concepts are linearly represented and strongly aligned in representational
geometry, leading models to conflate plausibility with validity. Using steering
vectors, we demonstrate that plausibility vectors can causally bias validity
judgements, and vice versa, and that the degree of alignment between these two
concepts predicts the magnitude of behavioral content effects across models.
Finally, we construct debiasing vectors that disentangle these concepts,
reducing content effects and improving reasoning accuracy. Our findings advance
understanding of how abstract logical concepts are represented in LLMs and
highlight representational interventions as a path toward more logical systems.

</details>


### [41] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型代理在长期多轮工具使用中的强化学习微调，并提出了一种基于摘要的上下文管理方法，以解决上下文长度限制的问题。通过实验验证，该方法在保持较低上下文长度的同时显著提高了成功率。


<details>
  <summary>Details</summary>
Motivation: 我们研究了大型语言模型（LLM）代理在长期多轮工具使用中的强化学习（RL）微调，其中上下文长度迅速成为基本瓶颈。现有的RL流程可能会受到指令遵循退化、过度的滚动成本以及最重要的严格上下文限制的影响。

Method: 我们引入了基于摘要的上下文管理，通过LLM生成的摘要定期压缩工具使用历史，以保留任务相关的信息，从而保持紧凑的上下文并使代理能够超越固定的上下文窗口。我们推导出一种策略梯度表示，使标准LLM RL基础设施能够以端到端的方式优化工具使用行为和摘要策略。

Result: 实验表明，SUPO在交互式函数调用和搜索任务中显著提高了成功率，同时保持相同的或甚至更低的工作上下文长度。此外，对于复杂的搜索任务，当测试时的最大摘要轮数超过训练时的轮数时，SUPO可以进一步提高评估性能。

Conclusion: 我们的结果确立了基于摘要的上下文管理作为一种原则性和可扩展的方法，用于在固定上下文长度限制之外训练强化学习代理。

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [42] [PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs](https://arxiv.org/abs/2510.06730)
*Manuel Frank,Haithem Afli*

Main category: cs.CL

TL;DR: 本文提出了一种新的动态评估方法PTEB，用于评估句子嵌入模型，强调在评估时随机生成语义保留的改写句子，并验证了该方法的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对句子嵌入模型的评估通常依赖于静态测试集，如Massive Text Embedding Benchmark (MTEB)。然而，重复调整固定套件可能会夸大报告的性能并掩盖现实世界的鲁棒性。因此，需要一种新的评估方法来更好地反映实际应用中的表现。

Method: 本文引入了Paraphrasing Text Embedding Benchmark (PTEB)，这是一种在评估时随机生成保持语义的改写句子并跨多次运行聚合结果的动态协议。使用基于语义文本相似性黄金评分的高效LLM方法，验证了LLM能够生成具有token多样性的语义保留改写句子。

Result: 实验结果显示，句子编码器的性能对token空间的变化敏感，即使语义保持不变。此外，较小的模型并没有比大型模型受到更大的影响。结果在多次运行中具有统计学上的稳健性，并扩展到了覆盖10种语言的3个多语言数据集。

Conclusion: 本文提出了一种新的自然语言处理评估范式，强调动态、随机的评估方法，而不是依赖静态的预定义基准。

Abstract: Current evaluations of sentence embedding models typically rely on static
test beds such as the Massive Text Embedding Benchmark (MTEB). While
invaluable, repeated tuning on a fixed suite can inflate reported performance
and obscure real-world robustness. We introduce the Paraphrasing Text Embedding
Benchmark (PTEB), a dynamic protocol that stochastically generates
meaning-preserving paraphrases at evaluation time and aggregates results across
multiple runs. Using a cost-efficient LLM-based method grounded in semantic
textual similarity gold ratings, we show that LLMs generate token-diverse but
semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our
hypothesis that the performance of sentence encoders is sensitive to changes in
token space even when semantics remain fixed. We also observe that smaller
models are not disproportionately affected relative to larger ones. Our results
are statistically robust over multiple runs and we extended our experiments to
3 multilingual datasets covering 10 languages. More generally, we aim to
propose a new evaluation paradigm in NLP that relies less on static,
pre-defined benchmarks but shifts towards dynamic, stochastic evaluation
leveraging eval-time compute.

</details>


### [43] [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
*Tiancheng Xing,Jerry Li,Yixuan Du,Xiyang Hu*

Main category: cs.CL

TL;DR: RAF是一种两阶段的标记优化方法，旨在生成简短的文本扰动，以一致地提升LLM生成排名中的目标项目，同时保持难以检测。实验表明，RAF在多个LLM上显著提升了目标项目的排名，比现有方法更具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了揭示这种漏洞，我们提出了RAF，这是一种两阶段的标记优化方法，旨在生成简短的文本扰动，以一致地提升LLM生成排名中的目标项目，同时保持难以检测。

Method: RAF是一种两阶段的标记优化方法，第一阶段使用贪心坐标梯度来通过结合排名目标的梯度和可读性分数来筛选当前位置的候选标记；第二阶段使用基于熵的动态加权方案，在精确排名和可读性损失下评估这些候选标记，并通过温度控制采样选择一个标记。

Result: 在多个LLM上的实验表明，RAF使用自然语言显著提升了目标项目的排名，相比现有方法在提升目标项目和保持自然性方面更具鲁棒性。

Conclusion: 这些发现强调了一个关键的安全影响：基于LLM的重新排序本质上容易受到对抗性操纵，这为现代检索系统的可信度和鲁棒性带来了新的挑战。

Abstract: Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.

</details>


### [44] [AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](https://arxiv.org/abs/2510.06738)
*Boyi Zeng,Lin Chen,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出了一种基于权重矩阵的无训练指纹方法，用于检测大型语言模型是否从头训练或源自现有基础模型。该方法利用线性分配问题和无偏中心核对齐相似性，能够有效中和参数操作的影响，从而获得高度稳健和高保真度的相似性度量。实验结果表明，该方法在各种后训练类别下表现出极强的鲁棒性，并且具有接近零的误报风险。


<details>
  <summary>Details</summary>
Motivation: 保护大型语言模型（LLMs）的知识产权至关重要，因为它们的训练需要大量资源。因此，模型所有者和第三方需要确定可疑的LLM是自行训练还是源自现有基础模型。然而，模型通常经历的密集后期训练过程（如监督微调、广泛的持续预训练、强化学习、多模态扩展、剪枝和升级）给可靠识别带来了重大挑战。

Method: 我们提出了一种基于权重矩阵的无训练指纹方法，利用线性分配问题（LAP）和无偏中心核对齐（CKA）相似性来中和参数操作的影响，从而获得高度稳健和高保真度的相似性度量。

Result: 在60个正样本和90个负样本模型对的综合测试平台上，我们的方法对上述六种后训练类别表现出极强的鲁棒性，同时表现出接近零的误报风险。通过在所有分类指标上取得完美分数，我们的方法为可靠的模型血统验证奠定了坚实的基础。此外，整个计算在NVIDIA 3090 GPU上完成时间不到30秒。

Conclusion: 我们的方法为可靠的模型血统验证奠定了坚实的基础。

Abstract: Protecting the intellectual property of large language models (LLMs) is
crucial, given the substantial resources required for their training.
Consequently, there is an urgent need for both model owners and third parties
to determine whether a suspect LLM is trained from scratch or derived from an
existing base model. However, the intensive post-training processes that models
typically undergo-such as supervised fine-tuning, extensive continued
pretraining, reinforcement learning, multi-modal extension, pruning, and
upcycling-pose significant challenges to reliable identification. In this work,
we propose a training-free fingerprinting method based on weight matrices. We
leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel
Alignment (CKA) similarity to neutralize the effects of parameter
manipulations, yielding a highly robust and high-fidelity similarity metric. On
a comprehensive testbed of 60 positive and 90 negative model pairs, our method
demonstrates exceptional robustness against all six aforementioned
post-training categories while exhibiting a near-zero risk of false positives.
By achieving perfect scores on all classification metrics, our approach
establishes a strong basis for reliable model lineage verification. Moreover,
the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is
available at https://github.com/LUMIA-Group/AWM.

</details>


### [45] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练和无标签的短文本聚类方法，可以在任何现有嵌入器上使用。


<details>
  <summary>Details</summary>
Motivation: 在客户面向的聊天机器人中，公司需要对大量用户语句进行聚类以确定其意图，但通常没有标记数据，且簇的数量未知。

Method: 我们的方法基于迭代向量更新：它根据代表性文本构建稀疏向量，并通过LLM指导逐步优化它们。

Result: 我们的方法在多种数据集和较小的LLM上进行了实验，结果表明它可以与任何嵌入器一起使用，并且在计算成本上有所减少。

Conclusion: 我们的方法在低资源、可适应的设置和可扩展性方面更符合现实场景，相比现有的聚类方法更具优势。

Abstract: In this paper, we propose a training-free and label-free method for short
text clustering that can be used on top of any existing embedder. In the
context of customer-facing chatbots, companies are dealing with large amounts
of user utterances that need to be clustered according to their intent. In
these commercial settings, no labeled data is typically available, and the
number of clusters is not known. Our method is based on iterative vector
updating: it constructs sparse vectors based on representative texts, and then
iteratively refines them through LLM guidance. Our method achieves comparable
or superior results to state-of-the-art methods that use contrastive learning,
but without assuming prior knowledge of clusters or labels. Experiments on
diverse datasets and smaller LLMs show that our method is model agnostic and
can be applied to any embedder, with relatively small LLMs, and different
clustering methods. We also show that our method scales to large datasets,
reducing the computational cost of the LLM. These low-resource, adaptable
settings and the scalability of our method make it more aligned with real-world
scenarios than existing clustering methods.

</details>


### [46] [A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction](https://arxiv.org/abs/2510.06749)
*Eitan Klinger,Zihao Huang,Tran Minh Nguyen,Emma Jayeon Park,Yige Chen,Yang Gu,Qingyu Gao,Siliang Liu,Mengyang Qiu,Jungyeul Park*

Main category: cs.CL

TL;DR: 本文提出了一种基于流利度的多参考评估框架，通过四种聚合策略实例化GLEU，并展示了其在不同语言语料库中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估框架主要基于编辑且以英语为中心，依赖于系统和参考编辑之间的严格对齐，这在多语言和生成设置中限制了其适用性。需要一种能够反映有效人类修正多样性的度量标准，而不是偏重单一参考。

Method: 本文引入了一个基于流利度的多参考评估框架，将n-gram相似性作为多个合法更正的聚合问题。在此框架中，我们通过四种聚合策略实例化GLEU--选择最佳、简单平均、加权平均和合并计数，并分析它们的有界性、单调性和对参考变化的敏感性。

Result: 在捷克语、爱沙尼亚语、乌克兰语和汉语语料库上的实证结果表明，这些策略捕捉了流利度和覆盖范围的不同方面。

Conclusion: 该框架将多参考评估统一为一种以流利度为导向的方法，既包含了语言多样性，又不惩罚合法的变化。

Abstract: Evaluating grammatical error correction requires metrics that reflect the
diversity of valid human corrections rather than privileging a single
reference. Existing frameworks, largely edit-based and English-centric, rely on
rigid alignments between system and reference edits, limiting their
applicability in multilingual and generative settings. This paper introduces a
formal framework for \textit{fluency-based multi-reference evaluation}, framing
$n$-gram similarity as an aggregation problem over multiple legitimate
corrections. Within this formulation, we instantiate GLEU through four
aggregation strategies--\textsc{select-best}, \textsc{simple-average},
\textsc{weighted-average}, and \textsc{merged-counts}--and analyze their
properties of boundedness, monotonicity, and sensitivity to reference
variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora
show that these strategies capture complementary aspects of fluency and
coverage. The framework unifies multi-reference evaluation into a principled,
fluency-oriented approach that incorporates linguistic diversity without
penalizing legitimate variation.

</details>


### [47] [Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs](https://arxiv.org/abs/2510.06750)
*Jaeseong Lee,Dayoung Kwon,seung-won hwang*

Main category: cs.CL

TL;DR: 本文提出了一种超叠加部署策略，通过轻量级、无需训练的调节来优化推理，通过选择性地从LRM中遗忘来缩小计算量，同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在结构化任务中表现出色，但常常因过度思考而性能下降并浪费资源。现有的基线方法是部署LLM和LRM，并通过预测输入是否需要推理来路由输入，但部署多个模型可能成本高昂或不可行。

Method: 本文提出了一种超叠加部署策略，通过轻量级、无需训练的调节来优化推理，通过选择性地从LRM中遗忘来缩小计算量，同时保持推理能力。

Result: 本文提出了一种超叠加部署策略，通过轻量级、无需训练的调节来优化推理，通过选择性地从LRM中遗忘来缩小计算量，同时保持推理能力。

Conclusion: 本文提出了一种超叠加部署策略，通过轻量级、无需训练的调节来优化推理，通过选择性地从LRM中遗忘来缩小计算量，同时保持推理能力。

Abstract: Large Reasoning Models (LRMs) excel in structured tasks by emulating
deliberate human reasoning but often suffer from overthinking, degrading
performance and wasting resources. One possible baseline is to deploy both LLM
and LRM, then route input by predicting whether it requires reasoning and may
cause overthinking. However, deploying multiple models can be costly or
impractical. We propose a superposed deployment strategy with a lightweight,
training-free regulation to optimize inference by switching one model on and
off. Instead of routing, we selectively unlearn from LRM at inference, scaling
down computation while preserving reasoning. By analyzing the cumulative energy
of singular values, we identify optimal low-rank projections to adjust
reasoning just right.

</details>


### [48] [Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition](https://arxiv.org/abs/2510.06774)
*Lei Xu,Pierre Beckmann,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 本文介绍了一个自适应、多范式的神经符号推理框架，能够自动识别形式推理策略并动态选择逻辑求解器。实验表明，该框架在多个任务中表现优于现有基线，并且可以提升纯LLM方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的神经符号NLP方法大多是静态的，即目标求解器的集成是在设计时预定义的，这限制了使用多种形式推理策略的能力。为此，本文提出了一种自适应的多范式神经符号推理框架。

Method: 本文引入了一个自适应、多范式的神经符号推理框架，该框架能够自动从用自然语言表达的问题中识别形式推理策略，并通过自动形式化接口动态选择和应用专门的形式逻辑求解器。

Result: 实验结果表明，LLMs在预测必要的形式推理策略方面非常有效，准确率超过90%。这使得与形式逻辑求解器的灵活集成成为可能，使我们的框架在与GPT-4o和DeepSeek-V3.1的竞争基线相比分别提高了27%和6%。此外，自适应推理甚至可以对纯LLM方法产生积极影响，在零样本、CoT和符号CoT设置中，与GPT-4o相比分别获得了10%、5%和6%的提升。最后，虽然较小的模型在自适应神经符号推理中存在困难，但后训练提供了一条可行的改进路径。

Conclusion: 本文建立了自适应LLM-符号推理的基础，为在异构推理挑战中统一材料和形式推理提供了一条前进的道路。

Abstract: Neuro-symbolic NLP methods aim to leverage the complementary strengths of
large language models and formal logical solvers. However, current approaches
are mostly static in nature, i.e., the integration of a target solver is
predetermined at design time, hindering the ability to employ diverse formal
inference strategies. To address this, we introduce an adaptive,
multi-paradigm, neuro-symbolic inference framework that: (1) automatically
identifies formal reasoning strategies from problems expressed in natural
language; and (2) dynamically selects and applies specialized formal logical
solvers via autoformalization interfaces. Extensive experiments on individual
and multi-paradigm reasoning tasks support the following conclusions: LLMs are
effective at predicting the necessary formal reasoning strategies with an
accuracy above 90 percent. This enables flexible integration with formal
logical solvers, resulting in our framework outperforming competing baselines
by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.
Moreover, adaptive reasoning can even positively impact pure LLM methods,
yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT
settings with GPT-4o. Finally, although smaller models struggle with adaptive
neuro-symbolic reasoning, post-training offers a viable path to improvement.
Overall, this work establishes the foundations for adaptive LLM-symbolic
reasoning, offering a path forward for unifying material and formal inferences
on heterogeneous reasoning challenges.

</details>


### [49] [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
*Luca Giordano,Simon Razniewski*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型知识物质化的终止、可重复性和鲁棒性，发现其能够可靠地揭示核心知识，但也存在重要限制。


<details>
  <summary>Details</summary>
Motivation: 测量和系统化大型语言模型中的事实知识仍然具有挑战性，通过递归提取方法将其转换为结构化格式仍需进一步探索。

Method: 使用miniGPTKBs（领域特定、可处理的子爬虫）系统地研究LLM知识物质化，分析终止、可重复性和鲁棒性，涉及三个类别的指标：产量、词汇相似性和语义相似性。

Result: （i）高终止率，但依赖于模型；（ii）混合可重复性；（iii）鲁棒性因扰动类型而异：种子和温度的鲁棒性高，语言和模型的鲁棒性较低。

Conclusion: 这些结果表明，LLM知识物质化可以可靠地揭示核心知识，同时也揭示了重要的限制。

Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet
measuring and systematizing this knowledge remains challenging. Converting it
into structured format, for example through recursive extraction approaches
such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key
open questions include whether such extraction can terminate, whether its
outputs are reproducible, and how robust they are to variations. We
systematically study LLM knowledge materialization using miniGPTKBs
(domain-specific, tractable subcrawls), analyzing termination, reproducibility,
and robustness across three categories of metrics: yield, lexical similarity,
and semantic similarity. We experiment with four variations (seed, language,
randomness, model) and three illustrative domains (from history, entertainment,
and finance). Our findings show (i) high termination rates, though
model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies
by perturbation type: high for seeds and temperature, lower for languages and
models. These results suggest that LLM knowledge materialization can reliably
surface core knowledge, while also revealing important limitations.

</details>


### [50] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: 本文介绍了 FURINA-Builder，这是一种多智能体协作管道，用于构建可定制的角色扮演基准测试。通过该管道构建的 FURINA-Bench 显示了其有效性，并揭示了角色扮演任务中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的角色扮演基准测试由于范围狭窄、互动模式过时和适应性有限而迅速过时。因此，需要一种更灵活和可扩展的基准测试方法。

Method: FURINA-Builder 是一种多智能体协作管道，用于自动构建可定制的角色扮演基准测试。它通过模拟对话和使用 LLM 判官进行细粒度评估来实现这一点。

Result: FURINA-Bench 是一个全面的角色扮演基准测试，包含已建立和合成的测试角色，并通过特定维度的评估标准进行评估。实验表明，o3 和 DeepSeek-R1 在英语和中文角色扮演任务中表现最佳。

Conclusion: FURINA-Builder 和 FURINA-Bench 展示了其有效性，并揭示了角色扮演任务中的挑战。

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


### [51] [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
*André Greiner-Petter,Maik Fröbe,Jan Philip Wahle,Terry Ruas,Bela Gipp,Akiko Aizawa,Martin Potthast*

Main category: cs.CL

TL;DR: 本文介绍了PAN 2025任务的自动生成文本抄袭检测，创建了一个大规模数据集，并评估了不同方法的性能，发现基于嵌入向量的方法在当前任务中表现良好，但在旧数据集上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 为了识别科学文章中的自动生成文本抄袭并将其与相应来源对齐，需要一个大规模的数据集来进行检测任务。

Method: 创建了一个使用三个大型语言模型的自动生成抄袭的大规模数据集，并对所有参与者和四个基线的结果进行了总结和比较。

Result: 基于嵌入向量的朴素语义相似性方法在当前任务中表现出色，但它们在2015年的数据集上表现不佳，显示出泛化能力不足。

Conclusion: 当前的迭代没有邀请大量的方法，基于嵌入向量的朴素语义相似性方法提供了有希望的结果，但在2015年的数据集上表现不佳，表明缺乏泛化能力。

Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying
automatically generated textual plagiarism in scientific articles and aligning
them with their respective sources. We created a novel large-scale dataset of
automatically generated plagiarism using three large language models: Llama,
DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation
of this dataset, summarize and compare the results of all participants and four
baselines, and evaluate the results on the last plagiarism detection task from
PAN 2015 in order to interpret the robustness of the proposed approaches. We
found that the current iteration does not invite a large variety of approaches
as naive semantic similarity approaches based on embedding vectors provide
promising results of up to 0.8 recall and 0.5 precision. In contrast, most of
these approaches underperform significantly on the 2015 dataset, indicating a
lack in generalizability.

</details>


### [52] [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
*Philipp Mondorf,Mingyang Wang,Sebastian Gerstner,Ahmad Dawar Hakimi,Yihong Liu,Leonor Veloso,Shijia Zhou,Hinrich Schütze,Barbara Plank*

Main category: cs.CL

TL;DR: 本文研究了通过集成两种或多种电路定位方法来提高性能的方法，并发现并行集成能够显著提升基准指标。


<details>
  <summary>Details</summary>
Motivation: 评估方法在大型语言模型中定位电路的能力，即负责特定任务行为的子网络。

Method: 研究了通过集成两种或多种电路定位方法来提高性能的方法，探索了并行和顺序集成两种变体。在并行集成中，通过平均或取最小值、最大值等方式结合不同方法分配给每条边的归因分数。在顺序集成中，使用EAP-IG获得的边归因分数作为更昂贵但更精确的电路识别方法（即边剪枝）的热启动。

Result: 两种方法都显著提升了基准指标，实现了更精确的电路识别方法。最终，采用多种方法（包括顺序集成）的并行集成取得了最佳效果。

Conclusion: 两种方法都能显著提升基准指标，从而实现更精确的电路识别方法。最终，采用多种方法（包括顺序集成）的并行集成取得了最佳效果。

Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark
(MIB) evaluates methods for localizing circuits within large language models
(LLMs), i.e., subnetworks responsible for specific task behaviors. In this
work, we investigate whether ensembling two or more circuit localization
methods can improve performance. We explore two variants: parallel and
sequential ensembling. In parallel ensembling, we combine attribution scores
assigned to each edge by different methods-e.g., by averaging or taking the
minimum or maximum value. In the sequential ensemble, we use edge attribution
scores obtained via EAP-IG as a warm start for a more expensive but more
precise circuit identification method, namely edge pruning. We observe that
both approaches yield notable gains on the benchmark metrics, leading to a more
precise circuit identification approach. Finally, we find that taking a
parallel ensemble over various methods, including the sequential ensemble,
achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB
Shared Task, comparing ensemble scores to official baselines across multiple
model-task combinations.

</details>


### [53] [Adaptive Tool Generation with Models as Tools and Reinforcement Learning](https://arxiv.org/abs/2510.06825)
*Chenpeng Wang,Xiaojie Cheng,Chunye Wang,Linfeng Yang,Lei Zhang*

Main category: cs.CL

TL;DR: MTR是一种模拟优先的训练框架，用于工具增强的推理，通过多代理架构进行操作，能够在不依赖实时API的情况下学习有效的工具推理。


<details>
  <summary>Details</summary>
Motivation: 工具增强的语言模型依赖于实时API访问，这在训练和部署过程中带来了可扩展性和可靠性挑战。

Method: MTR是一种模拟优先的训练框架，通过多代理架构进行操作，包括ToolMaker生成特定任务的OpenAI兼容工具接口，AutoAgent生成结构化的思考-行动-观察序列，ToolActor模拟现实响应。训练分为两个阶段：第一阶段是监督微调（SFT），教授'轨迹语法'；第二阶段是组相对策略优化（GRPO），使用复合轨迹奖励优化策略。

Result: MTR在四个多跳QA基准测试中取得了与实时API系统相当的精确匹配（EM）分数，并在推理密集型任务中表现出色。

Conclusion: MTR在四个多跳QA基准测试中取得了与实时API系统相当的精确匹配（EM）分数，并在推理密集型任务中表现出色，表明可以从结构化轨迹中学习有效的工具推理而无需实时交互。

Abstract: Tool-augmented language models have demonstrated strong capabilities, but
their reliance on live API access creates scalability and reliability
challenges during training and deployment. We propose MTR, a simulation-first
training framework for tool-augmented reasoning. Instead of relying on live
APIs, MTR learns from complete ReAct traces with schema-validated, simulated
observations. Our approach operates through a multi-agent architecture where a
ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an
AutoAgent produces structured think-act-observe sequences, and a ToolActor
simulates realistic responses. Training proceeds in two stages: Stage-1
Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning
sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy
with a composite trace reward that balances answer correctness and internal
consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue,
2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to
live-API systems and excels on reasoning-intensive tasks, suggesting that
effective tool reasoning can be learned from structured traces without live
interactions.

</details>


### [54] [Mid-Training of Large Language Models: A Survey](https://arxiv.org/abs/2510.06826)
*Kaixiang Mo,Yuxin Shi,Weiwei Weng,Zhiqiang Zhou,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: 本文首次对大型语言模型的中段训练进行了系统性综述，涵盖了数据分布、学习率调度和长上下文扩展，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管中段训练在最先进的系统中被广泛使用，但目前尚无将其作为统一范式的综述。因此，本文旨在填补这一空白。

Method: 本文通过分析数据分布、学习率调度和长上下文扩展三个方面，对LLM的中段训练进行了系统性的梳理和总结。

Result: 本文提出了一个全面的中段训练分类法，整理了实用见解，编译了评估基准，并报告了增益以实现跨模型的结构化比较。

Conclusion: 本文介绍了大型语言模型（LLM）中段训练的首个分类法，并提出了未来研究和实践的方向。

Abstract: Large language models (LLMs) are typically developed through large-scale
pre-training followed by task-specific fine-tuning. Recent advances highlight
the importance of an intermediate mid-training stage, where models undergo
multiple annealing-style phases that refine data quality, adapt optimization
schedules, and extend context length. This stage mitigates diminishing returns
from noisy tokens, stabilizes convergence, and expands model capability in late
training. Its effectiveness can be explained through gradient noise scale, the
information bottleneck, and curriculum learning, which together promote
generalization and abstraction. Despite widespread use in state-of-the-art
systems, there has been no prior survey of mid-training as a unified paradigm.
We introduce the first taxonomy of LLM mid-training spanning data distribution,
learning-rate scheduling, and long-context extension. We distill practical
insights, compile evaluation benchmarks, and report gains to enable structured
comparisons across models. We also identify open challenges and propose avenues
for future research and practice.

</details>


### [55] [GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics](https://arxiv.org/abs/2510.06841)
*Giorgos Filandrianos,Orfeas Menis Mastromichalakis,Wafaa Mohammed,Giuseppe Attanasio,Chrysoula Zerva*

Main category: cs.CL

TL;DR: 本文介绍了一个大规模的挑战集，用于检测自动质量评估（QE）度量在评估包含性别模糊职业术语的翻译时是否表现出性别偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的研究显示QE度量也可能表现出性别偏差，但大多数分析受到小数据集、狭窄的职业覆盖范围和有限的语言多样性的限制。本文旨在解决这一差距。

Method: 基于GAMBIT语料库，扩展到三种源语言和十一种目标语言，生成33个源-目标语言对。每个源文本与两个仅在职业术语的语法性别上不同的目标版本配对，并调整所有相关的语法元素。

Result: 该数据集的规模、广度和完全平行设计使得可以对职业进行细致的偏差分析，并在不同语言之间进行系统比较。

Conclusion: 本文提出了一种大规模的挑战集，用于探测QE度量在评估包含性别模糊职业术语的翻译时的行为。该数据集的规模、广度和完全平行设计使得可以对职业进行细致的偏差分析，并在不同语言之间进行系统比较。

Abstract: Gender bias in machine translation (MT) systems has been extensively
documented, but bias in automatic quality estimation (QE) metrics remains
comparatively underexplored. Existing studies suggest that QE metrics can also
exhibit gender bias, yet most analyses are limited by small datasets, narrow
occupational coverage, and restricted language variety. To address this gap, we
introduce a large-scale challenge set specifically designed to probe the
behavior of QE metrics when evaluating translations containing gender-ambiguous
occupational terms. Building on the GAMBIT corpus of English texts with
gender-ambiguous occupations, we extend coverage to three source languages that
are genderless or natural-gendered, and eleven target languages with
grammatical gender, resulting in 33 source-target language pairs. Each source
text is paired with two target versions differing only in the grammatical
gender of the occupational term(s) (masculine vs. feminine), with all dependent
grammatical elements adjusted accordingly. An unbiased QE metric should assign
equal or near-equal scores to both versions. The dataset's scale, breadth, and
fully parallel design, where the same set of texts is aligned across all
languages, enables fine-grained bias analysis by occupation and systematic
comparisons across languages.

</details>


### [56] [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
*Xuhang Chen,Zhifan Song,Deyi Ji,Shuo Gao,Lanyun Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于自我信号的多LLM辩论方法SID，通过利用模型级置信度和标记级语义焦点来提高多代理辩论系统的性能和效率。实验结果表明，该方法在准确性方面优于现有的MAD技术，并减少了令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 现有MAD方法主要依赖于外部结构，如辩论图和LLM作为裁判，而忽略了生成过程中出现的自我信号，如令牌logits和注意力。这种忽略导致了冗余计算和潜在的性能下降。

Method: 本文提出了一种基于自我信号的多LLM辩论方法SID，利用模型级置信度和标记级语义焦点来适应性地引导辩论过程。

Result: 实验结果表明，本文的方法不仅在准确性方面优于现有的MAD技术，而且减少了令牌消耗，证明了利用自我信号在提高多代理辩论系统性能和效率方面的有效性。

Conclusion: 本文提出了一种基于自我信号的多LLM辩论方法SID，通过利用模型级置信度和标记级语义焦点来提高多代理辩论系统的性能和效率。实验结果表明，该方法在准确性方面优于现有的MAD技术，并减少了令牌消耗。

Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across
diverse application domains. Recent work has explored Multi-LLM Agent Debate
(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and
refine responses iteratively. Nevertheless, existing MAD methods predominantly
focus on utilizing external structures, such as debate graphs, using
LLM-as-a-Judge, while neglecting the application of self signals, such as token
logits and attention, that arise during generation. This omission leads to
redundant computation and potential performance degradation. In this paper, we
shift the focus to the self signals of multi-LLM debate and introduce a
Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of
self-signals: model-level confidence and token-level semantic focus, to
adaptively guide the debate process. Our approach enables high-confidence
agents to exit early at the model level and compress the redundant debate
contents based on the attention mechanism. We evaluate our method on various
LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental
results demonstrate that our method not only outperforms existing MAD
techniques in accuracy but also reduces token consumption, highlighting the
effectiveness of utilizing self signals in enhancing both the performance and
efficiency of multi-agent debate systems. Our code will be available
at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.

</details>


### [57] [OpenJAI-v1.0: An Open Thai Large Language Model](https://arxiv.org/abs/2510.06847)
*Pontakorn Trakuekul,Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Sumana Sumanakul*

Main category: cs.CL

TL;DR: OpenJAI-v1.0 is an open-source large language model for Thai and English, developed from the Qwen3-14B model, which improves performance on practical tasks and outperforms other leading open-source Thai models on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Our work focuses on boosting performance on practical tasks through carefully curated data across three key use cases: instruction following, long-context understanding, and tool use.

Method: We introduced OpenJAI-v1.0, an open-source large language model for Thai and English, developed from the Qwen3-14B model.

Result: Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its base model and outperforms other leading open-source Thai models on a diverse suite of benchmarks, while avoiding catastrophic forgetting.

Conclusion: OpenJAI-v1.0 is publicly released as another alternative NLP resource for the Thai AI community.

Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and
English, developed from the Qwen3-14B model. Our work focuses on boosting
performance on practical tasks through carefully curated data across three key
use cases: instruction following, long-context understanding, and tool use.
Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its
base model and outperforms other leading open-source Thai models on a diverse
suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is
publicly released as another alternative NLP resource for the Thai AI
community.

</details>


### [58] [Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding](https://arxiv.org/abs/2510.06866)
*Wafaa Mohammed,Vlad Niculae,Chrysoula Zerva*

Main category: cs.CL

TL;DR: This study explores how LLMs handle discourse phenomena in translation and proposes QAD to improve translation quality.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with discourse phenomena such as pronoun resolution and lexical cohesion in translation.

Method: We investigate the performance of LLMs in handling discourse phenomena and propose the use of quality-aware decoding (QAD) to extract discourse knowledge.

Result: QAD is shown to be superior to other decoding approaches and enhances the semantic richness of translations.

Conclusion: QAD can effectively extract discourse knowledge from LLMs and improve the semantic richness of translations.

Abstract: Large language models (LLMs) have emerged as strong contenders in machine
translation.Yet, they still struggle to adequately handle discourse phenomena,
such as pronoun resolution and lexical cohesion at the document level. In this
study, we thoroughly investigate the discourse phenomena performance of LLMs in
context-aware translation. We demonstrate that discourse knowledge is encoded
within LLMs and propose the use of quality-aware decoding (QAD) to effectively
extract this knowledge, showcasing its superiority over other decoding
approaches through comprehensive analysis. Furthermore, we illustrate that QAD
enhances the semantic richness of translations and aligns them more closely
with human preferences.

</details>


### [59] [$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences](https://arxiv.org/abs/2510.06870)
*Yining Wang,Jinman Zhao,Chuangxin Zhao,Shuhao Guan,Gerald Penn,Shinan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法λ-GRPO，通过引入可学习的参数λ来适应性地控制令牌级别的权重，从而在多个数学推理基准上取得了持续的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法存在长度偏差问题，即相同的奖励被均匀分配给所有令牌，导致较长的响应在梯度更新中占比过大。为了解决这个问题，本文旨在让模型在优化过程中学习自己的令牌偏好。

Method: 本文提出了一种新的方法λ-GRPO，通过引入可学习的参数λ来适应性地控制令牌级别的权重，从而在多个数学推理基准上取得了持续的改进。

Result: 在Qwen2.5模型（1.5B、3B和7B参数）上，λ-GRPO相比GRPO分别提高了平均准确率+1.9%、+1.0%和+1.7%。此外，这些改进无需对训练数据进行任何修改或增加额外的计算成本。

Conclusion: 本文提出了一种新的方法λ-GRPO，通过引入可学习的参数λ来适应性地控制令牌级别的权重，从而在多个数学推理基准上取得了持续的改进。此外，这些改进无需对训练数据进行任何修改或增加额外的计算成本，证明了学习令牌偏好方法的有效性和实用性。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has been the dominant
approach for improving the reasoning capabilities of Large Language Models
(LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has
simplified this paradigm by replacing the reward and value models with
rule-based verifiers. A prominent example is Group Relative Policy Optimization
(GRPO). However, GRPO inherently suffers from a length bias, since the same
advantage is uniformly assigned to all tokens of a response. As a result,
longer responses distribute the reward over more tokens and thus contribute
disproportionately to gradient updates. Several variants, such as DAPO and Dr.
GRPO, modify the token-level aggregation of the loss, yet these methods remain
heuristic and offer limited interpretability regarding their implicit token
preferences. In this work, we explore the possibility of allowing the model to
learn its own token preference during optimization. We unify existing
frameworks under a single formulation and introduce a learnable parameter
$\lambda$ that adaptively controls token-level weighting. We use $\lambda$-GRPO
to denote our method, and we find that $\lambda$-GRPO achieves consistent
improvements over vanilla GRPO and DAPO on multiple mathematical reasoning
benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\lambda$-GRPO
improves average accuracy by $+1.9\%$, $+1.0\%$, and $+1.7\%$ compared to GRPO,
respectively. Importantly, these gains come without any modifications to the
training data or additional computational cost, highlighting the effectiveness
and practicality of learning token preferences.

</details>


### [60] [MeXtract: Light-Weight Metadata Extraction from Scientific Papers](https://arxiv.org/abs/2510.06889)
*Zaid Alyafeai,Maged S. Al-Shaibani,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文介绍了MeXtract，一个轻量级语言模型家族，用于从科学论文中提取元数据，并在MOLE基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在跨领域和模式变化方面难以泛化，因此需要一种更有效和通用的元数据提取方法。

Method: MeXtract是通过微调Qwen 2.5的对应模型构建的，模型大小从0.5B到3B参数不等，专门用于从科学论文中提取元数据。

Result: MeXtract在MOLE基准上实现了最先进的性能，并且在给定模式上的微调不仅产生了高精度，还能有效地转移到未见过的模式。

Conclusion: MeXtract在MOLE基准上实现了最先进的性能，并展示了其方法的鲁棒性和适应性。所有代码、数据集和模型都向研究社区开放。

Abstract: Metadata plays a critical role in indexing, documenting, and analyzing
scientific literature, yet extracting it accurately and efficiently remains a
challenging task. Traditional approaches often rely on rule-based or
task-specific models, which struggle to generalize across domains and schema
variations. In this paper, we present MeXtract, a family of lightweight
language models designed for metadata extraction from scientific papers. The
models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5
counterparts. In their size family, MeXtract achieves state-of-the-art
performance on metadata extraction on the MOLE benchmark. To further support
evaluation, we extend the MOLE benchmark to incorporate model-specific
metadata, providing an out-of-domain challenging subset. Our experiments show
that fine-tuning on a given schema not only yields high accuracy but also
transfers effectively to unseen schemas, demonstrating the robustness and
adaptability of our approach. We release all the code, datasets, and models
openly for the research community.

</details>


### [61] [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
*Zecheng Tang,Baibei Ji,Quantong Qiu,Haitian Wang,Xiaobo Liang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文介绍了Long-RewardBench，一个专门用于长上下文RM评估的基准，提出了一个通用的多阶段训练策略，以提高模型在长上下文场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的RM主要集中在短上下文设置和响应级别的属性上，而忽略了长上下文-响应一致性这一关键维度。

Method: 我们提出了一种通用的多阶段训练策略，能够有效地将任意模型扩展为鲁棒的长上下文RM（LongRMs）。

Result: 我们的初步研究显示，即使是最先进的生成式RM在长上下文场景中也表现出显著的脆弱性，无法保持上下文感知的偏好判断。

Conclusion: 我们的方法不仅显著提高了长上下文评估的性能，还保持了强大的短上下文能力。值得注意的是，我们的8B LongRM优于更大的70B规模基线，并达到了专有Gemini 2.5 Pro模型的性能。

Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM)
with human preferences. As real-world applications increasingly involve long
history trajectories, e.g., LLM agent, it becomes indispensable to evaluate
whether a model's responses are not only high-quality but also grounded in and
consistent with the provided context. Yet, current RMs remain confined to
short-context settings and primarily focus on response-level attributes (e.g.,
safety or helpfulness), while largely neglecting the critical dimension of long
context-response consistency. In this work, we introduce Long-RewardBench, a
benchmark specifically designed for long-context RM evaluation, featuring both
Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that
even state-of-the-art generative RMs exhibit significant fragility in
long-context scenarios, failing to maintain context-aware preference judgments.
Motivated by the analysis of failure patterns observed in model outputs, we
propose a general multi-stage training strategy that effectively scales
arbitrary models into robust Long-context RMs (LongRMs). Experiments show that
our approach not only substantially improves performance on long-context
evaluation but also preserves strong short-context capability. Notably, our 8B
LongRM outperforms much larger 70B-scale baselines and matches the performance
of the proprietary Gemini 2.5 Pro model.

</details>


### [62] [SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models](https://arxiv.org/abs/2510.06917)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: SHANKS is a framework that allows spoken language models to think while listening, improving real-time interaction by generating unspoken reasoning and deciding when to interrupt or make tool calls.


<details>
  <summary>Details</summary>
Motivation: Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn, which prevents the model from interacting during the user's turn and can lead to high response latency.

Method: SHANKS is a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. It streams the input speech in fixed-duration chunks and generates unspoken reasoning based on all previous speech and reasoning while the user continues speaking.

Result: SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn.

Conclusion: SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends.

Abstract: Current large language models (LLMs) and spoken language models (SLMs) begin
thinking and taking actions only after the user has finished their turn. This
prevents the model from interacting during the user's turn and can lead to high
response latency while it waits to think. Consequently, thinking after
receiving the full input is not suitable for speech-to-speech interaction,
where real-time, low-latency exchange is important. We address this by noting
that humans naturally "think while listening." In this paper, we propose
SHANKS, a general inference framework that enables SLMs to generate unspoken
chain-of-thought reasoning while listening to the user input. SHANKS streams
the input speech in fixed-duration chunks and, as soon as a chunk is received,
generates unspoken reasoning based on all previous speech and reasoning, while
the user continues speaking. SHANKS uses this unspoken reasoning to decide
whether to interrupt the user and to make tool calls to complete the task. We
demonstrate that SHANKS enhances real-time user-SLM interaction in two
scenarios: (1) when the user is presenting a step-by-step solution to a math
problem, SHANKS can listen, reason, and interrupt when the user makes a
mistake, achieving 37.1% higher interruption accuracy than a baseline that
interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can
complete 56.9% of the tool calls before the user finishes their turn. Overall,
SHANKS moves toward models that keep thinking throughout the conversation, not
only after a turn ends. Animated illustrations of Shanks can be found at
https://d223302.github.io/SHANKS/

</details>


### [63] [Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation](https://arxiv.org/abs/2510.06961)
*Vaibhav Srivastav,Steven Zheng,Eric Bezzam,Eustache Le Bihan,Nithin Koluguri,Piotr Żelasko,Somshubra Majumdar,Adel Moumen,Sanchit Gandhi*

Main category: cs.CL

TL;DR: 本文介绍了Open ASR Leaderboard，这是一个全面的基准测试和交互式排行榜，比较了60多个开源和专有系统。所有代码和数据集加载器都已开源，以支持透明和可扩展的评估。


<details>
  <summary>Details</summary>
Motivation: ASR评估仍然集中在短形式英语上，效率很少被报告。因此，需要一个全面的基准测试和排行榜来比较不同系统。

Method: 本文提出了Open ASR Leaderboard，标准化了文本规范化，并报告了词错误率（WER）和逆实时因子（RTFx），以实现公平的准确性和效率比较。

Result: 对于英语转录，Conformer编码器与LLM解码器结合使用，在平均WER方面表现最佳，但速度较慢。而CTC和TDT解码器在RTFx方面表现更好，适合长格式和离线使用。Whisper衍生的编码器微调后提高了准确性，但通常会牺牲多语言覆盖范围。

Conclusion: 本文介绍了Open ASR Leaderboard，这是一个全面的基准测试和交互式排行榜，比较了60多个开源和专有系统。所有代码和数据集加载器都已开源，以支持透明和可扩展的评估。

Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form
English, and efficiency is rarely reported. We present the Open ASR
Leaderboard, a fully reproducible benchmark and interactive leaderboard
comparing 60+ open-source and proprietary systems across 11 datasets, including
dedicated multilingual and long-form tracks. We standardize text normalization
and report both word error rate (WER) and inverse real-time factor (RTFx),
enabling fair accuracy-efficiency comparisons. For English transcription,
Conformer encoders paired with LLM decoders achieve the best average WER but
are slower, while CTC and TDT decoders deliver much better RTFx, making them
attractive for long-form and offline use. Whisper-derived encoders fine-tuned
for English improve accuracy but often trade off multilingual coverage. All
code and dataset loaders are open-sourced to support transparent, extensible
evaluation.

</details>


### [64] [EDUMATH: Generating Standards-aligned Educational Math Word Problems](https://arxiv.org/abs/2510.06965)
*Bryan R. Christ,Penelope Molitz,Jonathan Kropko,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 本文提出利用LLM生成符合学生兴趣和教育标准的数学问题，以帮助教师解决时间不足的问题。通过评估大量生成的问题并创建首个教师标注的数据集，我们证明了这种方法的有效性，并展示了模型生成的问题与人类编写的问题相似度高，且受到学生欢迎。


<details>
  <summary>Details</summary>
Motivation: 数学问题对于K-12教育至关重要，而根据学生的兴趣和能力水平定制这些问题可以提高学习成果。然而，由于班级规模大和教师倦怠，教师很难为每个学生定制数学问题。

Method: 我们使用了联合人类专家-LLM判断方法来评估由开放和封闭LLM生成的超过11,000个数学问题，并开发了第一个教师标注的数据集用于标准对齐的教育数学问题生成。我们还使用教师标注的数据训练了一个文本分类器，使一个30B的开放LLM在没有任何训练的情况下就能超越现有的封闭基线。

Result: 我们展示了数据的价值，用它来训练一个12B的开放模型，其性能与更大、更强大的开放模型相当。我们还使用教师标注的数据训练了一个文本分类器，使一个30B的开放LLM在没有任何训练的情况下就能超越现有的封闭基线。此外，我们的模型生成的数学问题比现有模型生成的更接近于人类编写的数学问题。

Conclusion: 我们通过与小学生进行首次定制化LLM生成的数学问题研究，发现他们在我们的模型生成的问题上的表现与人类编写的问题相当，但一致更喜欢我们的定制化问题。

Abstract: Math word problems (MWPs) are critical K-12 educational tools, and
customizing them to students' interests and ability levels can increase
learning outcomes. However, teachers struggle to find time to customize MWPs
for each student given large class sizes and increasing burnout. We propose
that LLMs can support math education by generating MWPs customized to student
interests and math education standards. To this end, we use a joint human
expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and
closed LLMs and develop the first teacher-annotated dataset for
standards-aligned educational MWP generation. We show the value of our data by
using it to train a 12B open model that matches the performance of larger and
more capable open models. We also use our teacher-annotated data to train a
text classifier that enables a 30B open LLM to outperform existing closed
baselines without any training. Next, we show our models' MWPs are more similar
to human-written MWPs than those from existing models. We conclude by
conducting the first study of customized LLM-generated MWPs with grade school
students, finding they perform similarly on our models' MWPs relative to
human-written MWPs but consistently prefer our customized MWPs.

</details>


### [65] [Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups](https://arxiv.org/abs/2510.06974)
*Geng Liu,Feng Li,Junjie Mu,Mengxiao Zhu,Francesco Pierri*

Main category: cs.CL

TL;DR: 本研究分析了中文大型语言模型中的社会身份偏见，发现这些偏见不仅存在于合成提示中，也在真实对话中显现，表明偏见可能在实际应用中加剧。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地部署在面向用户的应用中，人们对它们可能反映和放大社会偏见表示担忧。因此，我们需要评估中文大型语言模型中的社会身份框架及其潜在的偏见。

Method: 我们使用中文特定的提示，在十种代表性的中文大型语言模型中调查社会身份框架，评估对内群体（“我们”）和外群体（“他们”）框架的响应，并将环境扩展到240个在中国语境中显著的社会群体。此外，我们还分析了用户与聊天机器人之间真实互动的语料库中的中文对话。

Result: 在所有模型中，我们观察到系统性的内群体积极和外群体消极倾向，这些倾向不仅限于合成提示，也出现在自然对话中，表明偏见动态可能在真实互动中加强。

Conclusion: 我们的研究提供了一个语言敏感的评估框架，用于分析中文大型语言模型，表明在英语中记录的社会身份偏见可以跨语言推广，并在面向用户的环境中加剧。

Abstract: Large language models (LLMs) are increasingly deployed in user-facing
applications, raising concerns about their potential to reflect and amplify
social biases. We investigate social identity framing in Chinese LLMs using
Mandarin-specific prompts across ten representative Chinese LLMs, evaluating
responses to ingroup ("We") and outgroup ("They") framings, and extending the
setting to 240 social groups salient in the Chinese context. To complement
controlled experiments, we further analyze Chinese-language conversations from
a corpus of real interactions between users and chatbots. Across models, we
observe systematic ingroup-positive and outgroup-negative tendencies, which are
not confined to synthetic prompts but also appear in naturalistic dialogue,
indicating that bias dynamics might strengthen in real interactions. Our study
provides a language-aware evaluation framework for Chinese LLMs, demonstrating
that social identity biases documented in English generalize
cross-linguistically and intensify in user-facing contexts.

</details>


### [66] [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
*Markus Reuter,Tobias Lingenberg,Rūta Liepiņa,Francesca Lagioia,Marco Lippi,Giovanni Sartor,Andrea Passerini,Burcu Sayin*

Main category: cs.CL

TL;DR: 本文提出了一种名为摘要增强分块（SAC）的技术，用于减少法律领域中检索不匹配问题，从而提高RAG系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在法律领域，大型数据库中的结构相似文档常常导致检索系统失败，因此需要一种有效的方法来解决检索不匹配问题，以提高RAG系统的可靠性。

Method: 本文首先识别和量化了一种称为文档级检索不匹配（DRM）的关键失败模式，并研究了一种称为摘要增强分块（SAC）的简单且计算高效的技巧。该方法通过为每个文本块添加一个文档级别的合成摘要来增强每个文本块，从而注入在标准分块过程中会丢失的重要全局上下文。

Result: 实验结果表明，SAC大大减少了DRM，并因此提高了文本级检索的精度和召回率。有趣的是，通用的摘要策略表现优于结合法律专家领域知识以针对特定法律元素的方法。

Conclusion: 本文提供了证据，表明这种实用、可扩展且易于集成的技术在应用于大规模法律文档数据集时可以提高RAG系统的可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.

</details>


### [67] [Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages](https://arxiv.org/abs/2510.07000)
*Neel Prabhanjan Rachamalla,Aravind Konakalla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: 本文介绍了一种人工在环的管道，用于生成可靠且多样的印地语后训练数据，并整理了两个数据集，旨在提高多语言LLM的包容性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集往往缺乏多语言覆盖、文化根基，并且在印度语言中任务多样性差距尤为明显。

Method: 我们引入了一个人工在环的管道，结合翻译和合成扩展以生成可靠且多样的印地语后训练数据。

Result: 我们整理了两个数据集：Pragyaan-IT (22.5K) 和 Pragyaan-Align (100K)，涵盖10种印度语言，覆盖13个广泛类别和56个子类别，利用了57个多样化的数据集。

Conclusion: 我们的数据集协议强调了任务多样性、多轮对话、指令保真度、安全对齐和文化细微差别，为更包容和有效的多语言LLM提供了基础。

Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the
availability of high-quality post-training data, particularly
instruction-tuning and preference-based examples. Existing open-source
datasets, however, often lack multilingual coverage, cultural grounding, and
suffer from task diversity gaps that are especially pronounced for Indian
languages. We introduce a human-in-the-loop pipeline that combines translations
with synthetic expansion to produce reliable and diverse Indic post-training
data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and
Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56
sub-categories, leveraging 57 diverse datasets. Our dataset protocol
incorporates several often-overlooked dimensions and emphasize task diversity,
multi-turn dialogue, instruction fidelity, safety alignment, and preservation
of cultural nuance, providing a foundation for more inclusive and effective
multilingual LLMs.

</details>


### [68] [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
*Jusen Du,Jiaxi Hu,Tao Zhang,Weigao Sun,Yu Cheng*

Main category: cs.CL

TL;DR: NHA is a hybrid attention mechanism that combines linear and full attention to improve efficiency and accuracy in sequence modeling tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts.

Method: NHA is a novel hybrid architecture of linear and full attention that integrates both intra & inter-layer hybridization into a unified layer design. It maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single softmax attention operation is then applied over all keys and values.

Result: NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains.

Conclusion: NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains.

Abstract: Transformers excel at sequence modeling but face quadratic complexity, while
linear attention offers improved efficiency but often compromises recall
accuracy over long contexts. In this work, we introduce Native Hybrid Attention
(NHA), a novel hybrid architecture of linear and full attention that integrates
both intra \& inter-layer hybridization into a unified layer design. NHA
maintains long-term context in key-value slots updated by a linear RNN, and
augments them with short-term tokens from a sliding window. A single
\texttt{softmax attention} operation is then applied over all keys and values,
enabling per-token and per-head context-dependent weighting without requiring
additional fusion parameters. The inter-layer behavior is controlled through a
single hyperparameter, the sliding window size, which allows smooth adjustment
between purely linear and full attention while keeping all layers structurally
uniform. Experimental results show that NHA surpasses Transformers and other
hybrid baselines on recall-intensive and commonsense reasoning tasks.
Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving
competitive accuracy while delivering significant efficiency gains. Code is
available at https://github.com/JusenD/NHA.

</details>


### [69] [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
*Shrestha Ghosh,Luca Giordano,Yujia Hu,Tuan-Phong Nguyen,Simon Razniewski*

Main category: cs.CL

TL;DR: 本文分析了前沿大型语言模型的事实知识，发现其与现有知识库存在显著差异，且准确性较低，同时存在不一致、模糊和幻觉等问题。


<details>
  <summary>Details</summary>
Motivation: 目前，大型语言模型的事实知识仍缺乏深入理解，且通常仅基于有偏样本进行分析。本文旨在深入研究模型的事实知识，以揭示其与现有知识库的差异及存在的问题。

Method: 本文基于GPTKB v1.5数据集，对一个前沿大型语言模型（GPT-4.1）的事实知识进行了深入分析。该数据集通过递归获取的方式收集了1亿条模型的信念。

Result: 研究发现，模型的事实知识与现有知识库存在显著差异，且准确性低于以往基准测试的结果。同时，模型在一致性、模糊性和幻觉方面存在问题。

Conclusion: 本文发现大型语言模型的事实知识与现有知识库存在显著差异，且其准确性低于以往基准测试所显示的水平。此外，模型中存在不一致、模糊和幻觉等主要问题，这为未来关于事实性语言模型知识的研究提供了新的方向。

Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI
tasks. A significant contributor is their factual knowledge, which, to date,
remains poorly understood, and is usually analyzed from biased samples. In this
paper, we take a deep tour into the factual knowledge (or beliefs) of a
frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited
set of 100 million beliefs of one of the strongest currently available frontier
LLMs, GPT-4.1. We find that the models' factual knowledge differs quite
significantly from established knowledge bases, and that its accuracy is
significantly lower than indicated by previous benchmarks. We also find that
inconsistency, ambiguity and hallucinations are major issues, shedding light on
future research opportunities concerning factual LLM knowledge.

</details>


### [70] [Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](https://arxiv.org/abs/2510.07037)
*Rajvee Sheth,Samridhi Raj Sinha,Mahavir Patil,Himanshu Beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: 本文 provides a comprehensive analysis of code-switching-aware large language model research, highlighting the challenges and future directions for achieving truly multilingual intelligence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of code-switching in multilingual NLP, despite the advances of large language models.

Method: 本文 provides a comprehensive analysis of CSW-aware LLM research, reviewing studies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. It classifies recent advances by architecture, training strategy, and evaluation methodology.

Result: The survey reviews a total of unique_references studies, covering various aspects of CSW-aware LLM research.

Conclusion: 本文 concludes that there is a need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence.

Abstract: Code-switching (CSW), the alternation of languages and scripts within a
single utterance, remains a fundamental challenge for multiling ual NLP, even
amidst the rapid advances of large language models (LLMs). Most LLMs still
struggle with mixed-language inputs, limited CSW datasets, and evaluation
biases, hindering deployment in multilingual societies. This survey provides
the first comprehensive analysis of CSW-aware LLM research, reviewing
\total{unique_references} studies spanning five research areas, 12 NLP tasks,
30+ datasets, and 80+ languages. We classify recent advances by architecture,
training strategy, and evaluation methodology, outlining how LLMs have reshaped
CSW modeling and what challenges persist. The paper concludes with a roadmap
emphasizing the need for inclusive datasets, fair evaluation, and
linguistically grounded models to achieve truly multilingual intelligence. A
curated collection of all resources is maintained at
https://github.com/lingo-iitgn/awesome-code-mixing/.

</details>


### [71] [Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](https://arxiv.org/abs/2510.07048)
*Yuntao Gui,James Cheng*

Main category: cs.CL

TL;DR: Search-R3 is a framework that adapts LLMs to generate search embeddings through reasoning, leading to significant improvements in retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. The motivation is to address this limitation by adapting LLMs to generate more effective embeddings through reasoning.

Method: Search-R3 is a novel framework that adapts Large Language Models (LLMs) to generate search embeddings as a direct output of their reasoning process. It implements three complementary mechanisms: a supervised learning stage, a reinforcement learning (RL) methodology, and a specialized RL environment.

Result: Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. Extensive evaluations on diverse benchmarks demonstrate its effectiveness.

Conclusion: Search-R3 represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval.

Abstract: Despite their remarkable natural language understanding capabilities, Large
Language Models (LLMs) have been underutilized for retrieval tasks. We present
Search-R3, a novel framework that addresses this limitation by adapting LLMs to
generate search embeddings as a direct output of their reasoning process. Our
approach exploits LLMs' chain-of-thought capabilities, allowing them to produce
more effective embeddings by reasoning step-by-step through complex semantic
analyses. We implement this through three complementary mechanisms. (1) a
supervised learning stage enables the model's ability to produce quality
embeddings, (2) a reinforcement learning (RL) methodology that optimizes
embedding generation alongside reasoning, and (3) a specialized RL environment
that efficiently handles evolving embedding representations without requiring
complete corpus re-encoding at each training iteration. Our extensive
evaluations on diverse benchmarks demonstrate that Search-R3 significantly
outperforms prior methods by unifying the reasoning and embedding generation
processes. This integrated post-training approach represents a substantial
advancement in handling complex knowledge-intensive tasks that require both
sophisticated reasoning and effective information retrieval. Project page:
https://github.com/ytgui/Search-R3

</details>


### [72] [Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations](https://arxiv.org/abs/2510.07060)
*Miriam Wanner,Sophia Hager,Anjalie Field*

Main category: cs.CL

TL;DR: 研究发现，辛克莱广播集团收购本地新闻频道后，这些频道的报道内容更偏向全国性新闻，并且增加了对具有争议性话题的报道。


<details>
  <summary>Details</summary>
Motivation: 本地新闻频道通常被认为是可靠的信息来源，尤其是在居民关心的地方事务方面。然而，随着辛克莱广播集团收购了许多本地新闻频道，需要研究这种收购对报道内容的影响。

Method: 使用计算方法研究本地新闻频道在被辛克莱广播集团收购前后以及与全国性新闻机构的互联网内容变化。

Result: 有明确证据表明，本地新闻频道在被辛克莱广播集团收购后，更加频繁地报道全国性新闻，而地方话题的报道减少，并且对具有争议性的全国性话题的报道增加。

Conclusion: 本地新闻频道被辛克莱广播集团收购后，其报道内容更倾向于全国性新闻，并且对具有争议性的全国性话题的报道增加。

Abstract: Local news stations are often considered to be reliable sources of
non-politicized information, particularly local concerns that residents care
about. Because these stations are trusted news sources, viewers are
particularly susceptible to the information they report. The Sinclair Broadcast
group is a broadcasting company that has acquired many local news stations in
the last decade. We investigate the effects of local news stations being
acquired by Sinclair: how does coverage change? We use computational methods to
investigate changes in internet content put out by local news stations before
and after being acquired by Sinclair and in comparison to national news
outlets. We find that there is clear evidence that local news stations report
more frequently on national news at the expense of local topics, and that their
coverage of polarizing national topics increases.

</details>


### [73] [Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages](https://arxiv.org/abs/2510.07061)
*Amir Hossein Yari,Kalmit Kulkarni,Ahmad Raza Khan,Fajri Koto*

Main category: cs.CL

TL;DR: 本文介绍了ITEM基准，评估了26种自动度量在六种主要印度语言中与人类判断的一致性，发现了基于LLM的评估者最一致、异常值影响大、TS和MT中度量表现不同以及度量对扰动的敏感性差异。


<details>
  <summary>Details</summary>
Motivation: 现有度量几乎仅针对英语和其他高资源语言开发和验证，这使得印度语言（15亿人使用）被忽视，质疑了当前评估实践的普遍性。

Method: 我们引入了ITEM，这是一个大规模基准，系统地评估26种自动度量在六种主要印度语言中与人类判断的一致性，并丰富了细粒度注释。

Result: 四个中心发现：(1) 基于LLM的评估者在段落和系统层面与人类判断最一致；(2) 异常值对度量-人类一致性有显著影响；(3) 在TS中，度量更有效地捕捉内容真实性，而在MT中，它们更好地反映流畅性；(4) 当受到各种扰动时，度量在鲁棒性和敏感性上有所不同。

Conclusion: 这些发现为改进印度语言中的度量设计和评估提供了关键指导。

Abstract: While automatic metrics drive progress in Machine Translation (MT) and Text
Summarization (TS), existing metrics have been developed and validated almost
exclusively for English and other high-resource languages. This narrow focus
leaves Indian languages, spoken by over 1.5 billion people, largely overlooked,
casting doubt on the universality of current evaluation practices. To address
this gap, we introduce ITEM, a large-scale benchmark that systematically
evaluates the alignment of 26 automatic metrics with human judgments across six
major Indian languages, enriched with fine-grained annotations. Our extensive
evaluation, covering agreement with human judgments, sensitivity to outliers,
language-specific reliability, inter-metric correlations, and resilience to
controlled perturbations, reveals four central findings: (1) LLM-based
evaluators show the strongest alignment with human judgments at both segment
and system levels; (2) outliers exert a significant impact on metric-human
agreement; (3) in TS, metrics are more effective at capturing content fidelity,
whereas in MT, they better reflect fluency; and (4) metrics differ in their
robustness and sensitivity when subjected to diverse perturbations.
Collectively, these findings offer critical guidance for advancing metric
design and evaluation in Indian languages.

</details>


### [74] [LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish](https://arxiv.org/abs/2510.07074)
*Fred Philippy,Laura Bernardy,Siwen Guo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 本文通过构建跨语言指令微调数据集，提升了卢森堡语模型的性能，避免了机器翻译带来的问题。


<details>
  <summary>Details</summary>
Motivation: 低资源语言如卢森堡语由于缺乏高质量的指令数据集而面临严重限制，传统依赖机器翻译常引入语义不对齐和文化不准确。

Method: 通过利用英语、法语和德语的对齐数据，构建一个保留语言和文化细微差别的高质量数据集。

Result: 跨语言指令微调不仅提高了跨语言的表征对齐，还增强了卢森堡语的生成能力。

Conclusion: 跨语言数据整理可以避免机器翻译数据的常见问题，并直接促进低资源语言的发展。

Abstract: Instruction tuning has become a key technique for enhancing the performance
of large language models, enabling them to better follow human prompts.
However, low-resource languages such as Luxembourgish face severe limitations
due to the lack of high-quality instruction datasets. Traditional reliance on
machine translation often introduces semantic misalignment and cultural
inaccuracies. In this work, we address these challenges by creating a
cross-lingual instruction tuning dataset for Luxembourgish, without resorting
to machine-generated translations into it. Instead, by leveraging aligned data
from English, French, and German, we build a high-quality dataset that
preserves linguistic and cultural nuances. We provide evidence that
cross-lingual instruction tuning not only improves representational alignment
across languages but also the model's generative capabilities in Luxembourgish.
This highlights how cross-lingual data curation can avoid the common pitfalls
of machine-translated data and directly benefit low-resource language
development.

</details>


### [75] [Accelerating Diffusion LLM Inference via Local Determinism Propagation](https://arxiv.org/abs/2510.07081)
*Fanheng Kong,Jingyuan Zhang,Yahui Liu,Zirui Wu,Yu Tian,Victoria W.,Guorui Zhou*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion large language models (dLLMs) represent a significant advancement
in text generation, offering parallel token decoding capabilities. However,
existing open-source implementations suffer from quality-speed trade-offs that
impede their practical deployment. Conservative sampling strategies typically
decode only the most confident token per step to ensure quality (i.e., greedy
decoding), at the cost of inference efficiency due to repeated redundant
refinement iterations--a phenomenon we term delayed decoding. Through
systematic analysis of dLLM decoding dynamics, we characterize this delayed
decoding behavior and propose a training-free adaptive parallel decoding
strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built
on two fundamental empirical principles: local determinism propagation centered
on high-confidence anchors and progressive spatial consistency decay. By
applying these principles, LocalLeap identifies anchors and performs localized
relaxed parallel decoding within bounded neighborhoods, achieving substantial
inference step reduction through early commitment of already-determined tokens
without compromising output quality. Comprehensive evaluation on various
benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput
improvements and reduces decoding steps to just 14.2\% of the original
requirement, achieving these gains with negligible performance impact. The
source codes are available at: https://github.com/friedrichor/LocalLeap.

</details>


### [76] [All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations](https://arxiv.org/abs/2510.07083)
*Miriam Wanner,Leif Azzopardi,Paul Thomas,Soham Dan,Benjamin Van Durme,Nick Craswell*

Main category: cs.CL

TL;DR: 本文提出了一种新的度量标准VITAL，以更敏感地检测大型语言模型响应中的关键信息错误，并构建了一个基准数据集VITALERRORS来验证这一点。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估大型语言模型（LLM）响应的事实性时，将所有声明视为同样重要，这导致当关键信息缺失或错误时会产生误导性的评估。

Method: 我们引入了VITAL，一组通过结合声明与查询的相关性和重要性来提高测量响应事实性的敏感度的度量标准。

Result: VITAL度量标准比以前的方法更能可靠地检测关键信息中的错误。

Conclusion: 我们的数据集、度量标准和分析为更准确和稳健的LLM事实性评估提供了基础。

Abstract: Existing methods for evaluating the factuality of large language model (LLM)
responses treat all claims as equally important. This results in misleading
evaluations when vital information is missing or incorrect as it receives the
same weight as peripheral details, raising the question: how can we reliably
detect such differences when there are errors in key information? Current
approaches that measure factuality tend to be insensitive to omitted or false
key information. To investigate this lack of sensitivity, we construct
VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses
designed to omit or falsify key information. Using this dataset, we demonstrate
the insensitivities of existing evaluation metrics to key information errors.
To address this gap, we introduce VITAL, a set of metrics that provide greater
sensitivity in measuring the factuality of responses by incorporating the
relevance and importance of claims with respect to the query. Our analysis
demonstrates that VITAL metrics more reliably detect errors in key information
than previous methods. Our dataset, metrics, and analysis provide a foundation
for more accurate and robust assessment of LLM factuality.

</details>


### [77] [Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis](https://arxiv.org/abs/2510.07096)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的检索增强框架，用于讽刺感知的语音合成，通过结合语义嵌入和韵律范例，提高了语音自然度、讽刺表现力和下游讽刺检测。


<details>
  <summary>Details</summary>
Motivation: 讽刺是一种微妙的非字面语言形式，由于其依赖于细微的语义、上下文和韵律线索，对语音合成构成了重大挑战。现有的语音合成研究主要集中在广泛的情感类别上，而讽刺仍然 largely 未被探索。

Method: 我们提出了一种基于大型语言模型（LLM）的检索增强框架，用于讽刺感知的语音合成。该方法结合了（1）从LoRA微调的LLaMA 3中获得的语义嵌入，捕捉讽刺的语用不一致和话语级线索，以及（2）通过检索增强生成（RAG）模块检索到的韵律范例，提供讽刺表达的有表现力的参考模式。

Result: 实验表明，我们的方法在客观指标和主观评估中都优于基线，提高了语音自然度、讽刺表现力和下游讽刺检测。

Conclusion: 实验表明，我们的方法在客观指标和主观评估中都优于基线，提高了语音自然度、讽刺表现力和下游讽刺检测。

Abstract: Sarcasm is a subtle form of non-literal language that poses significant
challenges for speech synthesis due to its reliance on nuanced semantic,
contextual, and prosodic cues. While existing speech synthesis research has
focused primarily on broad emotional categories, sarcasm remains largely
unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced
Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach
combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture
pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic
exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which
provide expressive reference patterns of sarcastic delivery. Integrated within
a VITS backbone, this dual conditioning enables more natural and contextually
appropriate sarcastic speech. Experiments demonstrate that our method
outperforms baselines in both objective measures and subjective evaluations,
yielding improvements in speech naturalness, sarcastic expressivity, and
downstream sarcasm detection.

</details>


### [78] [TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription](https://arxiv.org/abs/2510.07098)
*Guo Yutong,Wanying Wang,Yue Wu,Zichen Miao,Haoyu Wang*

Main category: cs.CL

TL;DR: TALENT is a lightweight framework for Table VQA that uses dual representations (OCR text and natural language narration) combined with an LLM for reasoning, achieving results comparable to large VLMs at lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models (VLMs) are computationally prohibitive for mobile deployment, and smaller VLMs may miss fine-grained details. Alternative approaches using LLMs to reason over structured outputs like Markdown tables introduce errors.

Method: TALENT (Table VQA via Augmented Language-Enhanced Natural-text Transcription) leverages dual representations of tables by prompting a small VLM to produce both OCR text and natural language narration, then combines them with the question for reasoning by an LLM.

Result: Experiments show that TALENT enables a small VLM-LLM combination to match or surpass a single large VLM at significantly lower computational cost on both public datasets and ReTabVQA.

Conclusion: TALENT enables a small VLM-LLM combination to match or surpass a single large VLM at significantly lower computational cost on both public datasets and ReTabVQA.

Abstract: Table Visual Question Answering (Table VQA) is typically addressed by large
vision-language models (VLMs). While such models can answer directly from
images, they often miss fine-grained details unless scaled to very large sizes,
which are computationally prohibitive, especially for mobile deployment. A
lighter alternative is to have a small VLM perform OCR and then use a large
language model (LLM) to reason over structured outputs such as Markdown tables.
However, these representations are not naturally optimized for LLMs and still
introduce substantial errors. We propose TALENT (Table VQA via Augmented
Language-Enhanced Natural-text Transcription), a lightweight framework that
leverages dual representations of tables. TALENT prompts a small VLM to produce
both OCR text and natural language narration, then combines them with the
question for reasoning by an LLM. This reframes Table VQA as an LLM-centric
multimodal reasoning task, where the VLM serves as a perception-narration
module rather than a monolithic solver. Additionally, we construct ReTabVQA, a
more challenging Table VQA dataset requiring multi-step quantitative reasoning
over table images. Experiments show that TALENT enables a small VLM-LLM
combination to match or surpass a single large VLM at significantly lower
computational cost on both public datasets and ReTabVQA.

</details>


### [79] [Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning](https://arxiv.org/abs/2510.07105)
*Taylor Sorensen,Yejin Choi*

Main category: cs.CL

TL;DR: 本文提出了一种利用语言模型的上下文学习能力和两步元学习训练过程的系统，用于建模人类变化。该系统在LeWiDi竞赛中取得了优异成绩，并通过消融研究验证了各个组件的重要性。


<details>
  <summary>Details</summary>
Motivation: 许多自然语言处理任务涉及主观性、歧义或标注者之间的合法分歧。我们需要一个系统来建模人类的变化。

Method: 我们的系统利用语言模型的上下文学习能力，以及两步元学习训练过程：1) 在许多需要上下文学习的数据集上进行后训练；2) 通过上下文元学习专门针对感兴趣的数据分布。

Result: 我们的系统在LeWiDi竞赛中获得了总体胜利，并且通过消融研究验证了各个组件的重要性。

Conclusion: 我们的系统在Learning With Disagreements (LeWiDi)竞赛中取得了总体胜利，并且通过消融研究验证了各个组件的重要性。

Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity,
or legitimate disagreement between annotators. In this paper, we outline our
system for modeling human variation. Our system leverages language models'
(LLMs) in-context learning abilities, along with a two-step meta-learning
training procedure for 1) post-training on many datasets requiring in-context
learning and 2) specializing the model via in-context meta-learning to the
particular data distribution of interest. We also evaluate the performance of
our system submission to the Learning With Disagreements (LeWiDi) competition,
where it was the overall winner on both tasks. Additionally, we perform an
ablation study to measure the importance of each system component. We find that
including rater examples in-context is crucial for our system's performance,
dataset-specific fine-tuning is helpful on the larger datasets, post-training
on other in-context datasets is helpful on one of the competition datasets, and
that performance improves with model scale.

</details>


### [80] [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
*Manish Nagaraj,Sakshi Choudhary,Utkarsh Saxena,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CL

TL;DR: TRIM is a new framework for selecting high-quality coresets for instruction tuning, which is more efficient and effective than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for selecting coresets often rely on coarse, sample-level signals like gradients, which are computationally expensive and overlook fine-grained features. TRIM aims to address this by focusing on structural features that define a task.

Method: TRIM (Token Relevance via Interpretable Multi-layer Attention) is a forward-only, token-centric framework that matches underlying representational patterns identified via attention-based 'fingerprints' from a handful of target samples.

Result: Coresets selected by TRIM consistently outperform state-of-the-art baselines by up to 9% on downstream tasks and even surpass the performance of full-data fine-tuning in some settings. Additionally, TRIM achieves this at a fraction of the computational cost by avoiding expensive backward passes.

Conclusion: TRIM provides a scalable and efficient alternative for building high-quality instruction-tuning datasets.

Abstract: Instruction tuning is essential for aligning large language models (LLMs) to
downstream tasks and commonly relies on large, diverse corpora. However, small,
high-quality subsets, known as coresets, can deliver comparable or superior
results, though curating them remains challenging. Existing methods often rely
on coarse, sample-level signals like gradients, an approach that is
computationally expensive and overlooks fine-grained features. To address this,
we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a
forward-only, token-centric framework. Instead of using gradients, TRIM
operates by matching underlying representational patterns identified via
attention-based "fingerprints" from a handful of target samples. Such an
approach makes TRIM highly efficient and uniquely sensitive to the structural
features that define a task. Coresets selected by our method consistently
outperform state-of-the-art baselines by up to 9% on downstream tasks and even
surpass the performance of full-data fine-tuning in some settings. By avoiding
expensive backward passes, TRIM achieves this at a fraction of the
computational cost. These findings establish TRIM as a scalable and efficient
alternative for building high-quality instruction-tuning datasets.

</details>


### [81] [Comparing human and language models sentence processing difficulties on complex structures](https://arxiv.org/abs/2510.07141)
*Samuel Joseph Amouyal,Aya Meltzer-Asscher,Jonathan Berant*

Main category: cs.CL

TL;DR: 研究比较了人类和LLM在句子理解上的表现，发现LLM在某些结构上表现不佳，但随着参数数量增加，与人类的相似性提高。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否经历类似人类的处理困难，以及它们与人类在句子理解上的相似性。

Method: 通过统一的实验框架，比较了人类和五种最先进的LLM在七种挑战性语言结构上的句子理解情况。

Result: LLM整体上在目标结构上表现不佳，尤其是在花园路径（GP）句子上。最强模型在非GP结构上表现良好，但在GP结构上表现较差。随着参数数量增加，人类和模型之间的排名相关性增加。性能差距在人类和LLM中普遍存在，但有两个例外：对于太弱的模型，两种句子类型的性能都低；对于太强的模型，两种句子类型的性能都高。

Conclusion: 研究揭示了人类和LLM在句子理解上的相似性和差异性，提供了新的见解。

Abstract: Large language models (LLMs) that fluently converse with humans are a reality
- but do LLMs experience human-like processing difficulties? We systematically
compare human and LLM sentence comprehension across seven challenging
linguistic structures. We collect sentence comprehension data from humans and
five families of state-of-the-art LLMs, varying in size and training procedure
in a unified experimental framework. Our results show LLMs overall struggle on
the target structures, but especially on garden path (GP) sentences. Indeed,
while the strongest models achieve near perfect accuracy on non-GP structures
(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).
Additionally, when ranking structures based on average performance, rank
correlation between humans and models increases with parameter count. For each
target structure, we also collect data for their matched baseline without the
difficult structure. Comparing performance on the target vs. baseline
sentences, the performance gap observed in humans holds for LLMs, with two
exceptions: for models that are too weak performance is uniformly low across
both sentence types, and for models that are too strong the performance is
uniformly high. Together, these reveal convergence and divergence in human and
LLM sentence comprehension, offering new insights into the similarity of humans
and LLMs.

</details>


### [82] [Reasoning for Hierarchical Text Classification: The Case of Patents](https://arxiv.org/abs/2510.07167)
*Lekang Jiang,Wenjun Sun,Stephan Goetz*

Main category: cs.CL

TL;DR: RHC is a novel framework for hierarchical text classification that reformulates HTC as a step-by-step reasoning task. It trains large language models in two stages and demonstrates effectiveness, explainability, scalability, and applicability in experiments.


<details>
  <summary>Details</summary>
Motivation: Automated patent subject classification is one of the hardest HTC scenarios due to domain knowledge difficulty and a huge number of labels. Prior approaches only output a flat label set, which offers little insight into the reason behind predictions.

Method: RHC reformulates HTC as a step-by-step reasoning task to sequentially deduce hierarchical labels. It trains large language models (LLMs) in two stages: a cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning format and a reinforcement learning (RL) stage to enhance multi-step reasoning ability.

Result: RHC surpasses previous baselines and outperforms the supervised fine-tuning counterparts by approximately 3% in accuracy and macro F1. It also produces natural-language justifications before prediction to facilitate human inspection, scales favorably with model size, and achieves state-of-the-art performance on other widely used HTC benchmarks.

Conclusion: RHC demonstrates four advantages in our experiments, including effectiveness, explainability, scalability, and applicability, which highlights its broad applicability beyond patents.

Abstract: Hierarchical text classification (HTC) assigns documents to multiple levels
of a pre-defined taxonomy. Automated patent subject classification represents
one of the hardest HTC scenarios because of domain knowledge difficulty and a
huge number of labels. Prior approaches only output a flat label set, which
offers little insight into the reason behind predictions. Therefore, we propose
Reasoning for Hierarchical Classification (RHC), a novel framework that
reformulates HTC as a step-by-step reasoning task to sequentially deduce
hierarchical labels. RHC trains large language models (LLMs) in two stages: a
cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning
format and a reinforcement learning (RL) stage to enhance multi-step reasoning
ability. RHC demonstrates four advantages in our experiments. (1)
Effectiveness: RHC surpasses previous baselines and outperforms the supervised
fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)
Explainability: RHC produces natural-language justifications before prediction
to facilitate human inspection. (3) Scalability: RHC scales favorably with
model size with larger gains compared to standard fine-tuning. (4)
Applicability: Beyond patents, we further demonstrate that RHC achieves
state-of-the-art performance on other widely used HTC benchmarks, which
highlights its broad applicability.

</details>


### [83] [More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning](https://arxiv.org/abs/2510.07169)
*Yike Zhao,Simin Guo,Ziqing Yang,Shifan Han,Dahua Lin,Fei Tan*

Main category: cs.CL

TL;DR: 本研究分析了用于数学推理的开源数据集和数据合成技术，发现数据结构化和从强模型中提炼数据比单纯增加数据量更有效，为增强大型语言模型提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 尽管提出了各种数据构建方法，但它们在现实世界管道中的实际效用仍缺乏探索。

Method: 本研究对开源数据集和数据合成技术进行了全面分析，评估了它们在统一管道下的表现，该管道旨在模仿训练和部署场景。

Result: 研究发现，将数据结构化为更易解释的格式或从更强的模型中提炼数据，往往比单纯增加数据量更为有效。

Conclusion: 本研究提供了关于如何整合训练数据以增强大型语言模型（LLM）能力的实用指导，支持成本效益的数据整理和可扩展的模型增强。

Abstract: The reasoning capabilities of Large Language Models (LLMs) play a critical
role in many downstream tasks, yet depend strongly on the quality of training
data. Despite various proposed data construction methods, their practical
utility in real-world pipelines remains underexplored. In this work, we conduct
a comprehensive analysis of open-source datasets and data synthesis techniques
for mathematical reasoning, evaluating them under a unified pipeline designed
to mirror training and deployment scenarios. We further distill effective data
selection strategies and identify practical methods suitable for industrial
applications. Our findings highlight that structuring data in more
interpretable formats, or distilling from stronger models often outweighs
simply scaling up data volume. This study provides actionable guidance for
integrating training data to enhance LLM capabilities, supporting both
cost-effective data curation and scalable model enhancement. We hope this work
will inspire further research on how to balance "more data" versus "better
data" for real-world reasoning tasks.

</details>


### [84] [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
*Md Tawkat Islam Khondaker,Julia Harrington,Shady Shehata*

Main category: cs.CL

TL;DR: 本文介绍了NurseLLM，这是第一个专门针对护理领域的大型语言模型，用于多项选择题回答任务。通过构建大规模的护理MCQ数据集并引入多个护理基准，NurseLLM在不同基准测试中表现出色，证明了其在护理领域的价值。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在医疗系统中取得了显著进展，但它们在护理等专业领域的潜力仍 largely 未被探索。因此，我们需要一个专门针对护理领域的LLM来填补这一空白。

Method: 我们开发了一个多阶段的数据生成管道，以构建第一个大规模的护理MCQ数据集，用于训练LLM在广泛的护理主题上进行学习。同时，我们引入了多个护理基准来实现严格的评估。

Result: NurseLLM在不同基准测试中表现优于同等规模的通用和医学专用LLM，证明了专门为护理领域设计的LLM的重要性。

Conclusion: NurseLLM在不同基准测试中表现优于同等规模的通用和医学专用LLM，证明了专门为护理领域设计的LLM的重要性。此外，推理和多代理协作系统在护理中的作用也显示出未来研究和应用的潜力。

Abstract: Recent advancements in large language models (LLMs) have significantly
transformed medical systems. However, their potential within specialized
domains such as nursing remains largely underexplored. In this work, we
introduce NurseLLM, the first nursing-specialized LLM tailored for multiple
choice question-answering (MCQ) tasks. We develop a multi-stage data generation
pipeline to build the first large scale nursing MCQ dataset to train LLMs on a
broad spectrum of nursing topics. We further introduce multiple nursing
benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate
that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of
comparable size on different benchmarks, underscoring the importance of a
specialized LLM for the nursing domain. Finally, we explore the role of
reasoning and multi-agent collaboration systems in nursing, highlighting their
promise for future research and applications.

</details>


### [85] [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
*Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一种框架来系统地衡量大型语言模型心理测量评估中的数据污染，并提供了证据表明流行的量表如BFI-44和PVQ-40表现出强烈的污染。


<details>
  <summary>Details</summary>
Motivation: 先前的研究已经对心理测量库存可能的数据污染提出了担忧，这可能会影响此类评估的可靠性，但还没有系统地尝试量化这种污染的程度。

Method: 本文提出了一种框架，用于系统地衡量心理测量评估中的数据污染，评估了三个方面的内容：(1) 项目记忆，(2) 评估记忆，(3) 目标分数匹配。

Result: 应用该框架对21个主要家族的模型和四个广泛使用的心理测量库存进行评估，发现流行库存如BFI-44和PVQ-40表现出强污染，模型不仅记忆了项目，还可以调整其回答以达到特定的目标分数。

Conclusion: 本文提出了一种框架来系统地衡量大型语言模型心理测量评估中的数据污染，并提供了证据表明流行的量表如大五项清单（BFI-44）和肖像价值观问卷（PVQ-40）表现出强烈的污染。

Abstract: Recent studies apply psychometric questionnaires to Large Language Models
(LLMs) to assess high-level psychological constructs such as values,
personality, moral foundations, and dark traits. Although prior work has raised
concerns about possible data contamination from psychometric inventories, which
may threaten the reliability of such evaluations, there has been no systematic
attempt to quantify the extent of this contamination. To address this gap, we
propose a framework to systematically measure data contamination in
psychometric evaluations of LLMs, evaluating three aspects: (1) item
memorization, (2) evaluation memorization, and (3) target score matching.
Applying this framework to 21 models from major families and four widely used
psychometric inventories, we provide evidence that popular inventories such as
the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)
exhibit strong contamination, where models not only memorize items but can also
adjust their responses to achieve specific target scores.

</details>


### [86] [CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models](https://arxiv.org/abs/2510.07177)
*Yong-En Tian,Yu-Chien Tang,An-Zi Yen,Wen-Chih Peng*

Main category: cs.CL

TL;DR: 本文介绍了内容感知的提供方面精炼任务（CARPAS），旨在在摘要之前根据文档上下文动态调整提供的方面。我们构建了三个新的数据集，并通过使用LLMs和四种代表性的提示策略进行实验，发现LLMs倾向于预测过多的方面，导致摘要过长且不匹配。为此，我们提出了一个子任务来预测相关方面的数量，并证明预测的数量可以有效指导LLMs，减少推理难度并使其专注于最相关的方面。实验结果表明，所提出的方法在所有数据集上都显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法假设一组预定义的方面作为输入，而在现实场景中，这些给定的方面可能不完整、无关或完全缺失。用户经常期望系统能够根据实际内容自适应地精炼或过滤提供的方面。

Method: 我们构建了三个新的数据集来促进我们的初步实验，并通过使用四种代表性的提示策略在该任务中使用LLMs，发现LLMs倾向于预测一个过于全面的方面集，这通常会导致过于冗长和不匹配的摘要。基于这一观察，我们提出了一个初步子任务来预测相关方面的数量，并证明预测的数量可以作为LLMs的有效指导，减少推理难度，并使其专注于最相关的方面。

Result: 我们的实验表明，所提出的方法在所有数据集上都显著提高了性能。此外，我们的深入分析揭示了当请求的方面数量与LLMs自己的估计不同时，LLMs的合规性，为LLMs在类似现实应用场景中的部署提供了关键见解。

Conclusion: 我们的实验表明，所提出的方法在所有数据集上都显著提高了性能。此外，我们的深入分析揭示了当请求的方面数量与LLMs自己的估计不同时，LLMs的合规性，为LLMs在类似现实应用场景中的部署提供了关键见解。

Abstract: Aspect-based summarization has attracted significant attention for its
ability to generate more fine-grained and user-aligned summaries. While most
existing approaches assume a set of predefined aspects as input, real-world
scenarios often present challenges where these given aspects may be incomplete,
irrelevant, or entirely missing from the document. Users frequently expect
systems to adaptively refine or filter the provided aspects based on the actual
content. In this paper, we initiate this novel task setting, termed
Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with
the aim of dynamically adjusting the provided aspects based on the document
context before summarizing. We construct three new datasets to facilitate our
pilot experiments, and by using LLMs with four representative prompting
strategies in this task, we find that LLMs tend to predict an overly
comprehensive set of aspects, which often results in excessively long and
misaligned summaries. Building on this observation, we propose a preliminary
subtask to predict the number of relevant aspects, and demonstrate that the
predicted number can serve as effective guidance for the LLMs, reducing the
inference difficulty, and enabling them to focus on the most pertinent aspects.
Our extensive experiments show that the proposed approach significantly
improves performance across all datasets. Moreover, our deeper analyses uncover
LLMs' compliance when the requested number of aspects differs from their own
estimations, establishing a crucial insight for the deployment of LLMs in
similar real-world applications.

</details>


### [87] [Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible](https://arxiv.org/abs/2510.07178)
*Imry Ziv,Nur Lan,Emmanuel Chemla,Roni Katzir*

Main category: cs.CL

TL;DR: This paper examines whether LLMs are sensitive to the distinction between humanly possible and impossible languages. The results show that GPT-2 does not exhibit systematic separation between the two, suggesting that LLMs do not share the human innate biases that shape linguistic typology.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs are sensitive to the distinction between humanly possible and impossible languages, which bears on whether LLMs and humans share the same innate learning biases.

Method: Using the same methodology as previous work, we examine the claim on a wider set of languages and impossible perturbations.

Result: GPT-2 learns each language and its impossible counterpart equally easily, and provides no systematic separation between the possible and the impossible.

Conclusion: LLMs do not share the human innate biases that shape linguistic typology.

Abstract: Are large language models (LLMs) sensitive to the distinction between humanly
possible languages and humanly impossible languages? This question is taken by
many to bear on whether LLMs and humans share the same innate learning biases.
Previous work has attempted to answer it in the positive by comparing LLM
learning curves on existing language datasets and on "impossible" datasets
derived from them via various perturbation functions. Using the same
methodology, we examine this claim on a wider set of languages and impossible
perturbations. We find that in most cases, GPT-2 learns each language and its
impossible counterpart equally easily, in contrast to previous claims. We also
apply a more lenient condition by testing whether GPT-2 provides any kind of
separation between the whole set of natural languages and the whole set of
impossible languages. By considering cross-linguistic variance in various
metrics computed on the perplexity curves, we show that GPT-2 provides no
systematic separation between the possible and the impossible. Taken together,
these perspectives show that LLMs do not share the human innate biases that
shape linguistic typology.

</details>


### [88] [Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models](https://arxiv.org/abs/2510.07203)
*Benjamin Akera,Evelyn Nafula Ouma,Gilbert Yiga,Patrick Walukagga,Phionah Natukunda,Trevor Saaka,Solomon Nsumba,Lilian Teddy Nabukeera,Joel Muhanguzi,Imran Sekalala,Nimpamya Janat Namara,Engineer Bainomugisha,Ernest Mwebaze,John Quinn*

Main category: cs.CL

TL;DR: 本文提出了一种以地区为重点的方法，开发了基于Qwen 3的Sunflower 14B和32B模型，以提高乌干达语言的技术支持，并且这些模型是开源的，可以用于减少语言障碍。


<details>
  <summary>Details</summary>
Motivation: 非洲有超过2000种活语言，但大多数未得到语言技术的进步。当前领先的LLM主要支持使用人数最多的语言，导致不同语言的能力参差不齐。因此，需要一种更有效的地区性方法。

Method: 本文采用了一种以地区为重点的方法，开发了基于Qwen 3的Sunflower 14B和32B模型，以提高乌干达语言的技术支持。

Result: Sunflower 14B和32B模型在乌干达的大多数语言中表现出色，且是开源的，可用于减少语言障碍。

Conclusion: 本文提出了一种以地区为重点的方法，开发了Sunflower 14B和32B模型，这些模型在乌干达的大多数语言中表现出色，并且是开源的，可以用于减少语言障碍。

Abstract: There are more than 2000 living languages in Africa, most of which have been
bypassed by advances in language technology. Current leading LLMs exhibit
strong performance on a number of the most common languages (e.g. Swahili or
Yoruba), but prioritise support for the languages with the most speakers first,
resulting in piecemeal ability across disparate languages. We contend that a
regionally focussed approach is more efficient, and present a case study for
Uganda, a country with high linguistic diversity. We describe the development
of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the
art comprehension in the majority of all Ugandan languages. These models are
open source and can be used to reduce language barriers in a number of
important practical applications.

</details>


### [89] [Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models](https://arxiv.org/abs/2510.07213)
*Chengzhi Zhong,Fei Cheng,Qianying Liu,Yugo Murawaki,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 该研究提出了一种无需训练的方法，通过识别和操作跨语言转换的维度，实现了高效的多语言生成控制。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大型语言模型在多语言内容处理中的跨语言转换问题，以提高语言切换的效率和效果。

Method: 该研究基于观察到的跨语言转换由一组小而稀疏的维度控制的假设，引入了一种简单的、无需训练的方法来识别和操作这些维度，仅需要50句平行或单语数据。

Result: 实验结果表明，这些维度具有可解释性，通过在这些维度上的干预可以切换输出语言，同时保持语义内容，并且性能优于之前的基于神经元的方法。

Conclusion: 该研究提出了一种无需训练的方法，可以识别和操作这些维度，从而在多语言生成控制任务中实现语言切换，同时保持语义内容，并且在成本上优于之前的基于神经元的方法。

Abstract: Large language models exhibit strong multilingual capabilities despite
limited exposure to non-English data. Prior studies show that English-centric
large language models map multilingual content into English-aligned
representations at intermediate layers and then project them back into
target-language token spaces in the final layer. From this observation, we
hypothesize that this cross-lingual transition is governed by a small and
sparse set of dimensions, which occur at consistent indices across the
intermediate to final layers. Building on this insight, we introduce a simple,
training-free method to identify and manipulate these dimensions, requiring
only as few as 50 sentences of either parallel or monolingual data. Experiments
on a multilingual generation control task reveal the interpretability of these
dimensions, demonstrating that the interventions in these dimensions can switch
the output language while preserving semantic content, and that it surpasses
the performance of prior neuron-based approaches at a substantially lower cost.

</details>


### [90] [How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu](https://arxiv.org/abs/2510.07221)
*Benjamin Akera,Evelyn Nafula,Patrick Walukagga,Gilbert Yiga,John Quinn,Ernest Mwebaze*

Main category: cs.CL

TL;DR: 该研究评估了Whisper在低资源非洲语言上的表现，发现50小时的训练数据即可实现良好的ASR性能，并指出数据质量对系统性能同样重要。


<details>
  <summary>Details</summary>
Motivation: 解决低资源非洲语言自动语音识别系统的开发挑战，确定最小数据量和主要失败模式。

Method: 通过在两种班图语言上的系统数据缩放分析和详细错误特征分析来评估Whisper的表现。

Result: 实践性ASR性能（WER < 13%）在50小时的训练数据下即可实现，200小时后有显著提升（WER < 10%）。数据质量问题，特别是噪声地面真实转录，占高误差情况的38.6%。

Conclusion: 这些结果为在类似低资源语言环境中开发ASR系统的团队提供了可操作的基准和部署指导。

Abstract: The development of Automatic Speech Recognition (ASR) systems for
low-resource African languages remains challenging due to limited transcribed
speech data. While recent advances in large multilingual models like OpenAI's
Whisper offer promising pathways for low-resource ASR development, critical
questions persist regarding practical deployment requirements. This paper
addresses two fundamental concerns for practitioners: determining the minimum
data volumes needed for viable performance and characterizing the primary
failure modes that emerge in production systems. We evaluate Whisper's
performance through comprehensive experiments on two Bantu languages:
systematic data scaling analysis on Kinyarwanda using training sets from 1 to
1,400 hours, and detailed error characterization on Kikuyu using 270 hours of
training data. Our scaling experiments demonstrate that practical ASR
performance (WER < 13\%) becomes achievable with as little as 50 hours of
training data, with substantial improvements continuing through 200 hours (WER
< 10\%). Complementing these volume-focused findings, our error analysis
reveals that data quality issues, particularly noisy ground truth
transcriptions, account for 38.6\% of high-error cases, indicating that careful
data curation is as critical as data volume for robust system performance.
These results provide actionable benchmarks and deployment guidance for teams
developing ASR systems across similar low-resource language contexts. We
release accompanying and models see
https://github.com/SunbirdAI/kinyarwanda-whisper-eval

</details>


### [91] [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)
*Arjun Krishnakumar,Rhea Sanjay Sukthanker,Hannan Javed Mahadik,Gabriela Kadlecová,Vladyslav Moroshan,Timur Carstensen,Frank Hutter,Aaron Klein*

Main category: cs.CL

TL;DR: 本文提出了一种高效的预训练小语言模型框架，通过结构稀疏初始化、进化搜索和知识蒸馏等方法，显著提升了SLM的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）提供了比大型语言模型（LLMs）更高效和可访问的替代方案，在使用更少资源的情况下仍能实现强大的性能。然而，预训练SLMs通常效率较低，因此需要一种更有效的框架来改进这一过程。

Method: 我们引入了一个简单的有效框架用于预训练小语言模型，结合了三个互补的想法：首先，我们识别出结构稀疏的子网络初始化，这些初始化在相同的计算预算下 consistently 优于随机初始化的模型；其次，我们使用进化搜索自动发现高质量的子网络初始化，为预训练提供更好的起点；第三，我们应用从更大的教师模型进行知识蒸馏，以加快训练并提高泛化能力。

Result: 通过结合进化搜索和LLM权重初始化，我们的最佳模型在验证困惑度上与相当的Pythia SLM相当，但需要的预训练标记减少了9.2倍。这表明该框架显著提高了SLM预训练的效率。

Conclusion: 我们的最佳模型在进化搜索和LLM权重初始化下，达到了与相当的Pythia SLM相似的验证困惑度，同时需要的预训练标记减少了9.2倍。我们发布了所有代码和模型，为大规模高效的小语言模型开发提供了一个实用且可重复的路径。

Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to
Large Language Models (LLMs), delivering strong performance while using far
fewer resources. We introduce a simple and effective framework for pretraining
SLMs that brings together three complementary ideas. First, we identify
structurally sparse sub-network initializations that consistently outperform
randomly initialized models of similar size under the same compute budget.
Second, we use evolutionary search to automatically discover high-quality
sub-network initializations, providing better starting points for pretraining.
Third, we apply knowledge distillation from larger teacher models to speed up
training and improve generalization. Together, these components make SLM
pretraining substantially more efficient: our best model, discovered using
evolutionary search and initialized with LLM weights, matches the validation
perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining
tokens. We release all code and models at
https://github.com/whittle-org/whittle/, offering a practical and reproducible
path toward cost-efficient small language model development at scale.

</details>


### [92] [Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping](https://arxiv.org/abs/2510.07230)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Dakuo Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的个性化用户行为模拟方法Customer-R1，在在线购物环境中表现出色，能够更好地匹配用户的动作分布，提供更高质量的个性化行为模拟。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要学习一个群体层面的策略，而没有根据用户的个人资料进行条件化，导致模拟的是通用而非个性化的行为。因此，需要一种更好的方法来模拟个性化的用户行为。

Method: Customer-R1是一种基于强化学习的方法，用于在线购物环境中的个性化、逐步用户行为模拟。该方法的策略是基于显式的个人资料，并通过动作正确性奖励信号优化下一步的推理和动作生成。

Result: 实验表明，Customer-R1不仅在下一步动作预测任务中显著优于提示和SFT基线，而且更好地匹配了用户的动作分布，表明其在个性化行为模拟中具有更高的保真度。

Conclusion: Customer-R1在个性化用户行为模拟方面表现出色，能够更好地匹配用户的动作分布，表明其在个性化行为模拟中具有更高的保真度。

Abstract: Simulating step-wise human behavior with Large Language Models (LLMs) has
become an emerging research direction, enabling applications in various
practical domains. While prior methods, including prompting, supervised
fine-tuning (SFT), and reinforcement learning (RL), have shown promise in
modeling step-wise behavior, they primarily learn a population-level policy
without conditioning on a user's persona, yielding generic rather than
personalized simulations. In this work, we pose a critical question: how can
LLM agents better simulate personalized user behavior? We introduce
Customer-R1, an RL-based method for personalized, step-wise user behavior
simulation in online shopping environments. Our policy is conditioned on an
explicit persona, and we optimize next-step rationale and action generation via
action correctness reward signals. Experiments on the OPeRA dataset emonstrate
that Customer-R1 not only significantly outperforms prompting and SFT-based
baselines in next-action prediction tasks, but also better matches users'
action distribution, indicating higher fidelity in personalized behavior
simulation.

</details>


### [93] [Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships](https://arxiv.org/abs/2510.07231)
*Donggyu Lee,Sungwon Park,Yerin Hwang,Hyunwoo Oh,Hyoshin Kim,Jungwon Kim,Meeyoung Cha,Sangyoon Park,Jihee Kim*

Main category: cs.CL

TL;DR: 本文介绍了一个新的基准测试，用于评估大型语言模型的因果推理能力，但实验结果表明当前模型在此方面仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试存在严重限制，例如依赖于合成数据和狭窄的领域覆盖范围。

Method: 我们从顶级经济和金融期刊中提取了因果关系，采用了诸如工具变量、双重差分和断点设计等严谨的方法论。

Result: 在八个最先进的LLM上的实验结果揭示了显著的局限性，最佳模型仅达到57.6%的准确率。

Conclusion: 这些发现突显了当前LLM能力与高风险应用中可靠因果推理需求之间的关键差距。

Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to
understand genuine cause-and-effect relationships beyond pattern matching.
Existing benchmarks suffer from critical limitations such as reliance on
synthetic data and narrow domain coverage. We introduce a novel benchmark
constructed from casually identified relationships extracted from top-tier
economics and finance journals, drawing on rigorous methodologies including
instrumental variables, difference-in-differences, and regression discontinuity
designs. Our benchmark comprises 40,379 evaluation items covering five task
types across domains such as health, environment, technology, law, and culture.
Experimental results on eight state-of-the-art LLMs reveal substantial
limitations, with the best model achieving only 57.6\% accuracy. Moreover,
model scale does not consistently translate to superior performance, and even
advanced reasoning models struggle with fundamental causal relationship
identification. These findings underscore a critical gap between current LLM
capabilities and demands of reliable causal reasoning in high-stakes
applications.

</details>


### [94] [LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding](https://arxiv.org/abs/2510.07233)
*Zhivar Sourati,Zheng Wang,Marianne Menglin Liu,Yazhe Hu,Mengqing Guo,Sujeeth Bharadwaj,Kyu Han,Tao Sheng,Sujith Ravi,Morteza Dehghani,Dan Roth*

Main category: cs.CL

TL;DR: LAD-RAG是一种布局感知的动态RAG框架，它在摄入阶段构建一个符号文档图来捕捉布局结构和跨页依赖关系，并在推理阶段通过LLM代理动态交互以自适应地检索必要的证据，从而提高了多页推理任务的检索效果和问答准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG方法在摄入时将内容编码为孤立的块，丢失了结构和跨页依赖关系，并且在推理时检索固定数量的页面，这导致多页推理任务的答案质量下降。

Method: LAD-RAG是一种布局感知的动态RAG框架，在摄入阶段构建一个符号文档图来捕捉布局结构和跨页依赖关系，并在推理阶段通过LLM代理动态交互以自适应地检索必要的证据。

Result: LAD-RAG在MMLongBench-Doc、LongDocURL、DUDE和MP-DocVQA数据集上实现了超过90%的完美召回率，并在相似噪声水平下比基线检索器高出20%的召回率。

Conclusion: LAD-RAG在多个数据集上表现出色，提高了检索效果，并在保持低延迟的同时实现了更高的问答准确率。

Abstract: Question answering over visually rich documents (VRDs) requires reasoning not
only over isolated content but also over documents' structural organization and
cross-page dependencies. However, conventional retrieval-augmented generation
(RAG) methods encode content in isolated chunks during ingestion, losing
structural and cross-page dependencies, and retrieve a fixed number of pages at
inference, regardless of the specific demands of the question or context. This
often results in incomplete evidence retrieval and degraded answer quality for
multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a
novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs
a symbolic document graph that captures layout structure and cross-page
dependencies, adding it alongside standard neural embeddings to yield a more
holistic representation of the document. During inference, an LLM agent
dynamically interacts with the neural and symbolic indices to adaptively
retrieve the necessary evidence based on the query. Experiments on
MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG
improves retrieval, achieving over 90% perfect recall on average without any
top-k tuning, and outperforming baseline retrievers by up to 20% in recall at
comparable noise levels, yielding higher QA accuracy with minimal latency.

</details>


### [95] [When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](https://arxiv.org/abs/2510.07238)
*Xunyi Jiang,Dingyi Chang,Julian McAuley,Xin Xu*

Main category: cs.CL

TL;DR: 本研究探讨了评估基准老化对大型语言模型事实性评估的影响，发现现有基准存在过时问题，影响了评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）和现实世界的快速发展，广泛使用的评估基准的静态性质已经落后，这引发了对其可靠性评估LLM事实性的担忧。尽管有大量工作继续依赖于这些旧的基准，但它们与现实世界事实和现代LLMs的时间错配及其对LLM事实性评估的影响仍未被充分研究。

Method: 我们通过检查五个流行的事实性基准和八种不同年份发布的LLM，对这个问题进行了系统研究。还设计了一个最新的事实检索管道和三个指标来量化基准老化及其对LLM事实性评估的影响。

Result: 实验结果和分析表明，广泛使用的事实性基准中相当一部分样本已经过时，导致对LLM事实性的评估不可靠。

Conclusion: 我们的工作希望为评估LLM事实性评估的基准可靠性提供一个测试平台，并激发更多关于基准老化问题的研究。

Abstract: The rapid evolution of large language models (LLMs) and the real world has
outpaced the static nature of widely used evaluation benchmarks, raising
concerns about their reliability for evaluating LLM factuality. While
substantial works continue to rely on the popular but old benchmarks, their
temporal misalignment with real-world facts and modern LLMs, and their effects
on LLM factuality evaluation remain underexplored. Therefore, in this work, we
present a systematic investigation of this issue by examining five popular
factuality benchmarks and eight LLMs released across different years. An
up-to-date fact retrieval pipeline and three metrics are tailored to quantify
benchmark aging and its impact on LLM factuality evaluation. Experimental
results and analysis illustrate that a considerable portion of samples in the
widely used factuality benchmarks are outdated, leading to unreliable
assessments of LLM factuality. We hope our work can provide a testbed to assess
the reliability of a benchmark for LLM factuality evaluation and inspire more
research on the benchmark aging issue. Codes are available in
https://github.com/JiangXunyi/BenchAge.

</details>


### [96] [Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts](https://arxiv.org/abs/2510.07239)
*Christos Ziakas,Nicholas Loo,Nishita Jain,Alessandra Russo*

Main category: cs.CL

TL;DR: Red-Bandit 是一种红队框架，能够在线适应以识别和利用模型失败模式，并通过多臂老虎机策略动态选择攻击风格专家，从而实现对大型语言模型的安全审计。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化红队方法缺乏有效适应模型特定漏洞的机制。

Method: Red-Bandit 使用强化学习来微调一组参数高效的 LoRA 专家，每个专家专门针对一种攻击风格，并使用多臂老虎机策略在推理时动态选择这些攻击风格专家。

Result: Red-Bandit 在 AdvBench 上实现了最先进的结果，在足够探索的情况下（ASR@10），同时生成更符合人类阅读习惯的提示（较低的困惑度）。

Conclusion: Red-Bandit 的多臂老虎机策略可以作为诊断工具，用于发现特定模型的漏洞，通过指示哪种攻击方式最有效地引发不安全行为。

Abstract: Automated red-teaming has emerged as a scalable approach for auditing Large
Language Models (LLMs) prior to deployment, yet existing approaches lack
mechanisms to efficiently adapt to model-specific vulnerabilities at inference.
We introduce Red-Bandit, a red-teaming framework that adapts online to identify
and exploit model failure modes under distinct attack styles (e.g.,
manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA
experts, each specialized for a particular attack style, using reinforcement
learning that rewards the generation of unsafe prompts via a rule-based safety
model. At inference, a multi-armed bandit policy dynamically selects among
these attack-style experts based on the target model's response safety,
balancing exploration and exploitation. Red-Bandit achieves state-of-the-art
results on AdvBench under sufficient exploration (ASR@10), while producing more
human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy
serves as a diagnostic tool for uncovering model-specific vulnerabilities by
indicating which attack styles most effectively elicit unsafe behaviors.

</details>


### [97] [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
*Leitian Tao,Ilia Kulikov,Swarnadeep Saha,Tianlu Wang,Jing Xu,Yixuan Li,Jason E Weston,Ping Yu*

Main category: cs.CL

TL;DR: 本文提出了一种名为HERO的强化学习框架，通过将验证器信号与奖励模型得分相结合，提升了大型语言模型的推理能力。实验结果显示，HERO在多个数学推理基准测试中表现优于仅使用奖励模型或验证器的基线。


<details>
  <summary>Details</summary>
Motivation: 后训练对于大型语言模型（LLMs）的推理越来越依赖于可验证的奖励：提供0-1正确性信号的确定性检查器。虽然可靠，但这种二进制反馈很脆弱——许多任务接受部分正确或替代答案，而验证器对此低估，由此产生的全有或全无的监督限制了学习。奖励模型提供了更丰富的连续反馈，可以作为验证器的补充监督信号。

Method: 我们引入了HERO（Hybrid Ensemble Reward Optimization），这是一种将验证器信号与奖励模型得分以结构化方式整合的强化学习框架。HERO使用分层归一化将奖励模型得分限制在验证器定义的组内，同时保留正确性并细化质量差异，并使用方差感知加权来强调密集信号最重要的挑战性提示。

Result: 在各种数学推理基准测试中，HERO始终优于仅使用RM和仅使用验证器的基线，在可验证和难以验证的任务上都有显著提升。

Conclusion: 我们的结果表明，混合奖励设计在保持验证器的稳定性的同时，利用奖励模型的细微差别来推进推理。

Abstract: Post-training for reasoning of large language models (LLMs) increasingly
relies on verifiable rewards: deterministic checkers that provide 0-1
correctness signals. While reliable, such binary feedback is brittle--many
tasks admit partially correct or alternative answers that verifiers
under-credit, and the resulting all-or-nothing supervision limits learning.
Reward models offer richer, continuous feedback, which can serve as a
complementary supervisory signal to verifiers. We introduce HERO (Hybrid
Ensemble Reward Optimization), a reinforcement learning framework that
integrates verifier signals with reward-model scores in a structured way. HERO
employs stratified normalization to bound reward-model scores within
verifier-defined groups, preserving correctness while refining quality
distinctions, and variance-aware weighting to emphasize challenging prompts
where dense signals matter most. Across diverse mathematical reasoning
benchmarks, HERO consistently outperforms RM-only and verifier-only baselines,
with strong gains on both verifiable and hard-to-verify tasks. Our results show
that hybrid reward design retains the stability of verifiers while leveraging
the nuance of reward models to advance reasoning.

</details>


### [98] [LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation](https://arxiv.org/abs/2510.07243)
*Joseph Enguehard,Morgane Van Ermengem,Kate Atkinson,Sujeong Cha,Arijit Ghosh Chowdhury,Prashanth Kallur Ramaswamy,Jeremy Roghair,Hannah R Marlowe,Carina Suzana Negreanu,Kitty Boxall,Diana Mincu*

Main category: cs.CL

TL;DR: 本文提出了一种新的无需参考数据的法律领域LLM评估方法，展示了其优越性，并开源了相关数据以促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法在法律领域存在显著的局限性，无法有效评估大型语言模型（LLM）在法律领域的输出。

Method: 本文将长篇回答分解为'Legal Data Points'（LDPs），并引入了一种新颖的、无需参考数据的评估方法，该方法反映了律师评估法律答案的方式。

Result: 本文的方法在自有数据集和公开数据集（LegalBench）上均优于多种基线方法，并且与人类专家评估的相关性更高，有助于提高标注者之间的一致性。

Conclusion: 本文提出了一种新的、无需参考数据的评估方法，能够更准确地反映法律专家对法律答案的评估，并展示了该方法在多个数据集上的优越性。此外，本文还开源了用于实验的Legal Data Points，以促进该领域研究的发展。

Abstract: Evaluating large language model (LLM) outputs in the legal domain presents
unique challenges due to the complex and nuanced nature of legal analysis.
Current evaluation approaches either depend on reference data, which is costly
to produce, or use standardized assessment methods, both of which have
significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its
reliability and effectiveness in legal contexts depend heavily on evaluation
processes unique to the legal industry and how trustworthy the evaluation
appears to the human legal expert. This is where existing evaluation methods
currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into
'Legal Data Points' (LDPs), self-contained units of information, and introduce
a novel, reference-free evaluation methodology that reflects how lawyers
evaluate legal answers; b) we demonstrate that our method outperforms a variety
of baselines on both our proprietary dataset and an open-source dataset
(LegalBench); c) we show how our method correlates more closely with human
expert evaluations and helps improve inter-annotator agreement; and finally d)
we open source our Legal Data Points for a subset of LegalBench used in our
experiments, allowing the research community to replicate our results and
advance research in this vital area of LLM evaluation on legal
question-answering.

</details>


### [99] [Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models](https://arxiv.org/abs/2510.07248)
*Jonggeun Lee,Woojung Song,Jongwook Han,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: PA-Tool is a method that helps small language models use tools better by aligning the tool schemas with the models' pretraining knowledge, leading to significant improvements in performance and reducing errors.


<details>
  <summary>Details</summary>
Motivation: Small language models (SLMs) offer significant computational advantages for tool-augmented AI systems, yet they struggle with tool-use tasks, particularly in selecting appropriate tools and identifying correct parameters. A common failure mode is schema misalignment: models hallucinate plausible but non-existent tool names that reflect naming conventions internalized during pretraining but absent from the provided tool schema. Rather than forcing models to adapt to arbitrary schemas, we propose adapting schemas to align with models' pretrained knowledge.

Method: PA-Tool (Pretraining-Aligned Tool Schema Generation), a training-free method that leverages peakedness-a signal from contamination detection indicating pretraining familiarity-to automatically rename tool components. By generating multiple candidates and selecting those with highest output concentration across samples, PA-Tool identifies pretrain-aligned naming patterns.

Result: Experiments on MetaTool and RoTBench show improvements of up to 17% points, with schema misalignment errors reduced by 80%.

Conclusion: PA-Tool enables small models to approach state-of-the-art performance while maintaining computational efficiency for adaptation to new tools without retraining. Our work demonstrates that schema-level interventions can unlock the tool-use potential of resource-efficient models by adapting schemas to models rather than models to schemas.

Abstract: Small language models (SLMs) offer significant computational advantages for
tool-augmented AI systems, yet they struggle with tool-use tasks, particularly
in selecting appropriate tools and identifying correct parameters. A common
failure mode is schema misalignment: models hallucinate plausible but
non-existent tool names that reflect naming conventions internalized during
pretraining but absent from the provided tool schema. Rather than forcing
models to adapt to arbitrary schemas, we propose adapting schemas to align with
models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool
Schema Generation), a training-free method that leverages peakedness-a signal
from contamination detection indicating pretraining familiarity-to
automatically rename tool components. By generating multiple candidates and
selecting those with highest output concentration across samples, PA-Tool
identifies pretrain-aligned naming patterns. Experiments on MetaTool and
RoTBench show improvements of up to 17% points, with schema misalignment errors
reduced by 80%. PA-Tool enables small models to approach state-of-the-art
performance while maintaining computational efficiency for adaptation to new
tools without retraining. Our work demonstrates that schema-level interventions
can unlock the tool-use potential of resource-efficient models by adapting
schemas to models rather than models to schemas.

</details>


### [100] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: 本文介绍了一种名为OnlineRubrics的方法，通过在线方式动态调整评估标准，从而提高LLM在长回答任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于静态的评估标准，这可能导致奖励黑客行为，并且无法捕捉训练过程中出现的新需求。

Method: OnlineRubrics通过当前策略和参考策略生成的响应之间的成对比较，以在线方式动态地选择评估标准。

Result: 该方法在AlpacaEval、GPQA、ArenaHard以及专家问题和评估标准的验证集上，相对于仅使用静态评估标准的训练，性能提升了高达8%。

Conclusion: 本文提出了一种动态调整评估标准的方法，称为OnlineRubrics，能够持续识别和缓解训练过程中的错误，并在多个数据集上实现了显著的性能提升。

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [101] [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
*Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Xitong Zhang,Rongrong Wang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的道德自我修正，揭示了其通过多轮交互实现性能收敛的机制，并展示了其表现出的收敛性能的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管内在自我修正在各种应用中表现出实证成功，但其有效的原因和机制尚不清楚。本文旨在研究道德自我修正的收敛行为及其背后的机制。

Method: 本文聚焦于大型语言模型中的道德自我修正，揭示了内在自我修正的一个关键特征：通过多轮交互实现性能收敛，并提供了这种收敛行为的机制分析。

Result: 实验结果和分析表明，通过持续注入自我修正指令，可以激活道德概念，从而减少模型不确定性，导致性能收敛。

Conclusion: 本文展示了道德自我修正的强大力量，通过显示其表现出的收敛性能这一理想特性。

Abstract: Large Language Models (LLMs) are able to improve their responses when
instructed to do so, a capability known as self-correction. When instructions
provide only a general and abstract goal without specific details about
potential issues in the response, LLMs must rely on their internal knowledge to
improve response quality, a process referred to as intrinsic self-correction.
The empirical success of intrinsic self-correction is evident in various
applications, but how and why it is effective remains unknown. Focusing on
moral self-correction in LLMs, we reveal a key characteristic of intrinsic
self-correction: performance convergence through multi-round interactions; and
provide a mechanistic analysis of this convergence behavior. Based on our
experimental results and analysis, we uncover the underlying mechanism of
convergence: consistently injected self-correction instructions activate moral
concepts that reduce model uncertainty, leading to converged performance as the
activated moral concepts stabilize over successive rounds. This paper
demonstrates the strong potential of moral self-correction by showing that it
exhibits a desirable property of converged performance.

</details>


### [102] [Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning](https://arxiv.org/abs/2510.07300)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Kaiyu Huang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: M-Thinker通过改进的强化学习算法提升了非英语语言的推理能力，解决了语言一致性问题并增强了多语言性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前大型推理模型在处理非英语语言时存在的输入输出语言不一致和推理路径错误导致的性能问题。

Method: 提出M-Thinker模型，采用GRPO算法，包含语言一致性（LC）奖励和跨语言思维对齐（CTA）奖励。

Result: M-Thinker-1.5B/7B模型实现了接近100%的语言一致性，并在两个多语言基准测试中表现出色，同时在域外语言上具有良好的泛化能力。

Conclusion: M-Thinker通过GRPO算法训练，显著提高了非英语语言的推理能力和语言一致性，展示了在多语言基准测试中的优越性能和泛化能力。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
reasoning tasks by adopting the "think-then-answer" paradigm, which enhances
both accuracy and interpretability. However, current LRMs exhibit two critical
limitations when processing non-English languages: (1) They often struggle to
maintain input-output language consistency; (2) They generally perform poorly
with wrong reasoning paths and lower answer accuracy compared to English. These
limitations significantly degrade the user experience for non-English speakers
and hinder the global deployment of LRMs. To address these limitations, we
propose M-Thinker, which is trained by the GRPO algorithm that involves a
Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment
(CTA) reward. Specifically, the LC reward defines a strict constraint on the
language consistency between the input, thought, and answer. Besides, the CTA
reward compares the model's non-English reasoning paths with its English
reasoning path to transfer its own reasoning capability from English to
non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B
models not only achieve nearly 100% language consistency and superior
performance on two multilingual benchmarks (MMATH and PolyMath), but also
exhibit excellent generalization on out-of-domain languages.

</details>


### [103] [Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain](https://arxiv.org/abs/2510.07309)
*Yue Li,Ran Tao,Derek Hommel,Yusuf Denizay Dönder,Sungyong Chang,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: 本文介绍了CORGI基准测试，这是一个针对现实商业场景的文本到SQL任务的新基准测试。该基准测试包含四个越来越复杂的业务查询类别，并发现大型语言模型在高层次问题上的表现不佳。


<details>
  <summary>Details</summary>
Motivation: 在商业领域，数据驱动决策至关重要，而文本到SQL是实现自然语言访问结构化数据的基础。然而，现有的文本到SQL基准测试主要集中在事实检索上，无法满足现实商业场景的需求。因此，需要一个新的基准测试来评估大型语言模型在现实商业智能方面的表现。

Method: 引入了CORGI基准测试，该测试由受企业如DoorDash、Airbnb和Lululemon启发的合成数据库组成，提供了四个越来越复杂的业务查询类别：描述性、解释性、预测性和推荐性问题。

Result: 研究发现，大型语言模型在高层次问题上的表现下降，难以做出准确预测并提供可行计划。CORGI基准测试比BIRD基准测试难21%，突显了流行大型语言模型与实际商业智能需求之间的差距。

Conclusion: 该研究指出，现有的文本到SQL基准测试主要集中在过去记录的事实检索上，而CORGI基准测试则针对现实商业场景，要求因果推理、时间预测和战略推荐，这反映了多层和多步骤的代理智能。研究发现，大型语言模型在高层次问题上的表现下降，难以做出准确预测并提供可行计划。CORGI基准测试比BIRD基准测试难21%，突显了流行大型语言模型与实际商业智能需求之间的差距。

Abstract: In the business domain, where data-driven decision making is crucial,
text-to-SQL is fundamental for easy natural language access to structured data.
While recent LLMs have achieved strong performance in code generation, existing
text-to-SQL benchmarks remain focused on factual retrieval of past records. We
introduce CORGI, a new benchmark specifically designed for real-world business
contexts. CORGI is composed of synthetic databases inspired by enterprises such
as Doordash, Airbnb, and Lululemon. It provides questions across four
increasingly complex categories of business queries: descriptive, explanatory,
predictive, and recommendational. This challenge calls for causal reasoning,
temporal forecasting, and strategic recommendation, reflecting multi-level and
multi-step agentic intelligence. We find that LLM performance drops on
high-level questions, struggling to make accurate predictions and offer
actionable plans. Based on execution success rate, the CORGI benchmark is about
21\% more difficult than the BIRD benchmark. This highlights the gap between
popular LLMs and the need for real-world business intelligence. We release a
public dataset and evaluation framework, and a website for public submissions.

</details>


### [104] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估方法，以衡量大型语言模型在代码指令遵循方面的能力，并展示了功能正确性和指令遵循的综合得分与人类偏好高度相关。


<details>
  <summary>Details</summary>
Motivation: 当前的代码评估仍然局限于pass@k，仅捕捉功能性正确性，忽略了用户经常应用的非功能性指令。我们假设指令遵循是vibe check缺失的组成部分，代表了除了功能性正确性之外的人类偏好。

Method: 我们提出了VeriCode，这是一个包含30个可验证代码指令的分类法以及相应的确定性验证器。我们使用该分类法来增强现有的评估套件，从而创建Vibe Checker，一个用于评估代码指令遵循和功能正确性的测试平台。

Result: 我们评估了31个领先的LLM，结果显示即使最强的模型也难以遵守多个指令，并表现出明显的功能退化。最重要的是，功能正确性和指令遵循的综合得分与人类偏好相关性最高，后者在现实世界的编程任务中成为主要区分因素。

Conclusion: 我们的工作识别了vibe check的核心因素，为基准测试和开发更符合用户编码偏好的模型提供了具体的路径。

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [105] [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
*Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai Wei*

Main category: cs.CL

TL;DR: 提出一种人工神经网络的记忆框架，通过结合Transformer的KV缓存滑动窗口和一个称为人工海马网络（AHN）的可学习模块，实现了长序列建模中的高效和高保真度。实验表明，AHN增强模型在长上下文基准测试中表现优于滑动窗口基线，并且在计算和内存需求上显著减少。


<details>
  <summary>Details</summary>
Motivation: Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers.

Method: Introduce a memory framework of artificial neural networks, maintaining a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory.

Result: AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88.

Conclusion: AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements.

Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [106] [Asking For It: Question-Answering for Predicting Rule Infractions in Online Content Moderation](https://arxiv.org/abs/2510.06350)
*Mattia Samory,Diana Pamfile,Andrew To,Shruti Phadke*

Main category: cs.CY

TL;DR: 本文提出了一种新的问答框架ModQ，用于规则敏感的内容审核。ModQ在推理时依赖于完整的社区规则集，并确定最适合给定评论的规则。两种模型变体在识别与监管相关的规则违规方面表现优异，同时保持轻量和可解释性，并能有效推广到未见过的社区和规则。


<details>
  <summary>Details</summary>
Motivation: 在线社区依赖平台政策和社区编写的规则来定义可接受的行为并维持秩序。然而，这些规则在不同社区之间差异很大，随时间演变，并且执行不一致，这给透明度、治理和自动化带来了挑战。

Method: 本文引入了ModQ，一种基于问答的框架，用于规则敏感的内容审核。ModQ在推理时依赖于完整的社区规则集，并确定最适合给定评论的规则。我们实现了两种模型变体——抽取式和多项选择QA，并在Reddit和Lemmy的大规模数据集上进行了训练。

Result: 两种模型在识别与监管相关的规则违规方面都优于最先进的基线模型，同时保持轻量和可解释性。值得注意的是，ModQ模型在未见过的社区和规则中表现出良好的泛化能力，支持低资源监管环境和动态治理环境。

Conclusion: ModQ模型在识别与监管相关的规则违规方面表现出色，同时保持轻量和可解释性，并能有效推广到未见过的社区和规则，支持低资源监管环境和动态治理环境。

Abstract: Online communities rely on a mix of platform policies and community-authored
rules to define acceptable behavior and maintain order. However, these rules
vary widely across communities, evolve over time, and are enforced
inconsistently, posing challenges for transparency, governance, and automation.
In this paper, we model the relationship between rules and their enforcement at
scale, introducing ModQ, a novel question-answering framework for
rule-sensitive content moderation. Unlike prior classification or
generation-based approaches, ModQ conditions on the full set of community rules
at inference time and identifies which rule best applies to a given comment. We
implement two model variants - extractive and multiple-choice QA - and train
them on large-scale datasets from Reddit and Lemmy, the latter of which we
construct from publicly available moderation logs and rule descriptions. Both
models outperform state-of-the-art baselines in identifying moderation-relevant
rule violations, while remaining lightweight and interpretable. Notably, ModQ
models generalize effectively to unseen communities and rules, supporting
low-resource moderation settings and dynamic governance environments.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [107] [XLSR-Kanformer: A KAN-Intergrated model for Synthetic Speech Detection](https://arxiv.org/abs/2510.06706)
*Phuong Tuan Dat,Tran Huy Dat*

Main category: cs.SD

TL;DR: 本文提出用KAN替换XLSR-Conformer模型中的MLP，以提高合成语音检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于自监督学习(SSL)的模型在合成语音检测中表现出色，但仍需进行架构改进。

Method: 提出了一种新方法，用Kolmogorov-Arnold Network (KAN) 替换XLSR-Conformer模型中的传统多层感知器(MLP)。

Result: 实验结果表明，将KAN集成到XLSR-Conformer模型中可以将等错误率(EER)相对提高60.55%，并在21LA集上实现了0.70%的EER。

Conclusion: 将KAN集成到基于SSL的模型中是合成语音检测的一个有前途的方向。

Abstract: Recent advancements in speech synthesis technologies have led to increasingly
sophisticated spoofing attacks, posing significant challenges for automatic
speaker verification systems. While systems based on self-supervised learning
(SSL) models, particularly the XLSR-Conformer architecture, have demonstrated
remarkable performance in synthetic speech detection, there remains room for
architectural improvements. In this paper, we propose a novel approach that
replaces the traditional Multi-Layer Perceptron (MLP) in the XLSR-Conformer
model with a Kolmogorov-Arnold Network (KAN), a powerful universal approximator
based on the Kolmogorov-Arnold representation theorem. Our experimental results
on ASVspoof2021 demonstrate that the integration of KAN to XLSR-Conformer model
can improve the performance by 60.55% relatively in Equal Error Rate (EER) LA
and DF sets, further achieving 0.70% EER on the 21LA set. Besides, the proposed
replacement is also robust to various SSL architectures. These findings suggest
that incorporating KAN into SSL-based models is a promising direction for
advances in synthetic speech detection.

</details>


### [108] [AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs](https://arxiv.org/abs/2510.07293)
*Peize He,Zichen Wen,Yubo Wang,Yuxuan Wang,Xiaoqian Liu,Jiajie Huang,Zehui Lei,Zhuangcheng Gu,Xiangqi Jin,Jiabing Yang,Kai Li,Zhifei Liu,Weijia Li,Cunxiang Wang,Conghui He,Linfeng Zhang*

Main category: cs.SD

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Processing long-form audio is a major challenge for Large Audio Language
models (LALMs). These models struggle with the quadratic cost of attention
($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio
benchmarks are built mostly from short clips and do not evaluate models in
realistic long context settings. To address this gap, we introduce
AudioMarathon, a benchmark designed to evaluate both understanding and
inference efficiency on long-form audio. AudioMarathon provides a diverse set
of tasks built upon three pillars: long-context audio inputs with durations
ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of
2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,
sound, and music, and complex reasoning that requires multi-hop inference. We
evaluate state-of-the-art LALMs and observe clear performance drops as audio
length grows. We also study acceleration techniques and analyze the trade-offs
of token pruning and KV cache eviction. The results show large gaps across
current LALMs and highlight the need for better temporal reasoning and
memory-efficient architectures. We believe AudioMarathon will drive the audio
and multimodal research community to develop more advanced audio understanding
models capable of solving complex audio tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [109] [CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation](https://arxiv.org/abs/2510.06231)
*Mingzhe Zheng,Dingjie Song,Guanyu Zhou,Jun You,Jiahao Zhan,Xuran Ma,Xinyuan Song,Ser-Nam Lim,Qifeng Chen,Harry Yang*

Main category: cs.CV

TL;DR: This paper introduces CML-Dataset and CML-Bench to evaluate the quality of movie scripts generated by LLMs, and proposes CML-Instruction to improve their performance.


<details>
  <summary>Details</summary>
Motivation: To investigate the deficiency of LLMs in capturing the 'soul' of compelling cinema, such as nuanced storytelling and emotional depth in movie scripts.

Method: Curated CML-Dataset, identified three pivotal dimensions for quality assessment (Dialogue Coherence, Character Consistency, Plot Reasonableness), proposed CML-Bench, introduced CML-Instruction as a prompting strategy.

Result: CML-Bench effectively assigns high scores to well-crafted, human-written scripts while pinpointing weaknesses in LLM-generated screenplays. LLMs guided by CML-Instruction generate higher-quality screenplays aligned with human preferences.

Conclusion: LLMs guided by CML-Instruction generate higher-quality screenplays, with results aligned with human preferences.

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating highly structured texts. However, while exhibiting a high degree of
structural organization, movie scripts demand an additional layer of nuanced
storytelling and emotional depth-the 'soul' of compelling cinema-that LLMs
often fail to capture. To investigate this deficiency, we first curated
CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup
Language (CML), where 'content' consists of segments from esteemed,
high-quality movie scripts and 'summary' is a concise description of the
content. Through an in-depth analysis of the intrinsic multi-shot continuity
and narrative structures within these authentic scripts, we identified three
pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character
Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we
propose the CML-Bench, featuring quantitative metrics across these dimensions.
CML-Bench effectively assigns high scores to well-crafted, human-written
scripts while concurrently pinpointing the weaknesses in screenplays generated
by LLMs. To further validate our benchmark, we introduce CML-Instruction, a
prompting strategy with detailed instructions on character dialogue and event
logic, to guide LLMs to generate more structured and cinematically sound
scripts. Extensive experiments validate the effectiveness of our benchmark and
demonstrate that LLMs guided by CML-Instruction generate higher-quality
screenplays, with results aligned with human preferences.

</details>


### [110] [Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities](https://arxiv.org/abs/2510.06743)
*Maria Levchenko*

Main category: cs.CV

TL;DR: 本文提出了一种基于大型语言模型的历史OCR评估方法，引入了新的指标并发现了某些模型的过度历史化问题。


<details>
  <summary>Details</summary>
Motivation: 数字人文学者越来越多地使用大型语言模型进行历史文档数字化，但缺乏适当的评估框架来评估基于LLM的OCR。传统指标无法捕捉时间偏差和对历史语料库创建至关重要的特定时期的错误。

Method: 我们提出了一个用于基于大型语言模型的历史OCR的评估方法，包括历史字符保留率（HCPR）和古语插入率（AIR）等新指标，以及污染控制和稳定性测试的协议。

Result: 我们评估了12个多模态LLM，发现Gemini和Qwen模型在表现上优于传统OCR，但表现出过度历史化：从错误的历史时期插入古语字符。OCR后的校正反而降低了性能。

Conclusion: 我们的方法为数字人文学术实践者提供了在历史语料库数字化中进行模型选择和质量评估的指导。

Abstract: Digital humanities scholars increasingly use Large Language Models for
historical document digitization, yet lack appropriate evaluation frameworks
for LLM-based OCR. Traditional metrics fail to capture temporal biases and
period-specific errors crucial for historical corpus creation. We present an
evaluation methodology for LLM-based historical OCR, addressing contamination
risks and systematic biases in diplomatic transcription. Using 18th-century
Russian Civil font texts, we introduce novel metrics including Historical
Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside
protocols for contamination control and stability testing. We evaluate 12
multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR
while exhibiting over-historicization: inserting archaic characters from
incorrect historical periods. Post-OCR correction degrades rather than improves
performance. Our methodology provides digital humanities practitioners with
guidelines for model selection and quality assessment in historical corpus
digitization.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [111] [Machines in the Crowd? Measuring the Footprint of Machine-Generated Text on Reddit](https://arxiv.org/abs/2510.07226)
*Lucio La Cava,Luca Maria Aiello,Andrea Tagarelli*

Main category: cs.SI

TL;DR: 本文研究了Reddit上机器生成文本的分布情况及其对社交互动的影响，发现MGT在某些社区中占比可达9%，并具有与人类内容相当的参与度。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究MGT在Reddit上的分布情况及其对社交互动的影响。

Method: 本文使用最先进的统计方法检测MGT，并分析了2022-2024年期间51个代表Reddit主要社区类型的子论坛中的活动。

Result: MGT在Reddit上虽然仅占很小比例，但在某些社区中可以达到高达9%的峰值。MGT在不同社区之间分布不均，更多出现在技术知识和社会支持类的子论坛中，并且通常集中在少数用户的活动中。MGT传达出与AI助手语言相似的温暖和地位给予的社会信号，但其参与度与人类创作的内容相当甚至更高。

Conclusion: 本文提供了对Reddit上机器生成文本（MGT）足迹的首次研究，为平台治理、检测策略和社区动态等新研究铺平了道路。

Abstract: Generative Artificial Intelligence is reshaping online communication by
enabling large-scale production of Machine-Generated Text (MGT) at low cost.
While its presence is rapidly growing across the Web, little is known about how
MGT integrates into social media environments. In this paper, we present the
first large-scale characterization of MGT on Reddit. Using a state-of-the-art
statistical method for detection of MGT, we analyze over two years of activity
(2022-2024) across 51 subreddits representative of Reddit's main community
types such as information seeking, social support, and discussion. We study the
concentration of MGT across communities and over time, and compared MGT to
human-authored text in terms of social signals it expresses and engagement it
receives. Our very conservative estimate of MGT prevalence indicates that
synthetic text is marginally present on Reddit, but it can reach peaks of up to
9% in some communities in some months. MGT is unevenly distributed across
communities, more prevalent in subreddits focused on technical knowledge and
social support, and often concentrated in the activity of a small fraction of
users. MGT also conveys distinct social signals of warmth and status giving
typical of language of AI assistants. Despite these stylistic differences, MGT
achieves engagement levels comparable than human-authored content and in a few
cases even higher, suggesting that AI-generated text is becoming an organic
component of online social discourse. This work offers the first perspective on
the MGT footprint on Reddit, paving the way for new investigations involving
platform governance, detection strategies, and community dynamics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [112] [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
*Milad Aghajohari,Kamran Chitsaz,Amirhossein Kazemnejad,Sarath Chandar,Alessandro Sordoni,Aaron Courville,Siva Reddy*

Main category: cs.LG

TL;DR: 本文提出了一种新的RL环境Delethink，通过将推理结构化为固定大小的块，实现了线性计算和常数内存，从而在不产生二次开销的情况下实现非常长的推理。


<details>
  <summary>Details</summary>
Motivation: 标准的RL“思考环境”使得状态无界，迫使基于注意力的策略随着思考长度的增加而支付二次计算。我们重新审视了环境本身。

Method: 我们提出了马尔可夫思维范式，其中策略在条件化于固定大小状态的情况下推进推理，将思考长度与上下文大小解耦。我们实例化了这个想法，使用Delethink，一个将推理结构化为固定大小块的RL环境。

Result: 在Delethink环境中训练的R1-Distill 1.5B模型在8K标记块中进行推理，但可以思考最多24K标记，与LongCoT-RL在24K预算下训练的结果相当或超越。Delethink在测试时缩放继续改进，而LongCoT则停滞不前。

Conclusion: 我们的结果表明，重新设计思考环境是一个强大的杠杆：它可以在没有二次开销的情况下实现非常长的推理，并为高效、可扩展的推理LLM开辟了一条道路。

Abstract: Reinforcement learning (RL) has recently become a strong recipe for training
reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard
RL "thinking environment", where the state is the prompt plus all prior
reasoning tokens, makes the state unbounded and forces attention-based policies
to pay quadratic compute as thoughts lengthen. We revisit the environment
itself. We propose Markovian Thinking, a paradigm in which the policy advances
reasoning while conditioning on a constant-size state, decoupling thinking
length from context size. As an immediate consequence this yields linear
compute with constant memory. We instantiate this idea with Delethink, an RL
environment that structures reasoning into fixed-size chunks. Within each
chunk, the model thinks as usual; at the boundary, the environment resets the
context and reinitializes the prompt with a short carryover. Through RL, the
policy learns to write a textual state near the end of each chunk sufficient
for seamless continuation of reasoning after reset. Trained in this
environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up
to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.
With test-time scaling, Delethink continues to improve where LongCoT plateaus.
The effect of linear compute is substantial: we empirically estimate at 96K
average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.
Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)
often sample Markovian traces zero-shot across diverse benchmarks, providing
positive samples that make RL effective at scale. Our results show that
redesigning the thinking environment is a powerful lever: it enables very long
reasoning without quadratic overhead and opens a path toward efficient,
scalable reasoning LLMs.

</details>


### [113] [A Multi-Agent Framework for Stateful Inference-Time Search](https://arxiv.org/abs/2510.07147)
*Arshika Lalan,Rajat Ghosh,Aditya Kolsur,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的框架，通过结合持久的推理时状态、对抗性突变和进化保留来改进单元测试生成。实验结果表明，这种方法在覆盖范围上优于传统的无状态方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多步骤任务中由于缺乏持久状态而表现不佳，并且任务特定的微调或指令微调虽然能生成表面代码，但在需要深度推理和长距离依赖的任务上仍不稳定。

Method: 提出了一种无需训练的框架，称为有状态多智能体进化搜索，该框架结合了（i）持久的推理时状态，（ii）对抗性突变，以及（iii）进化保留。

Result: 实验表明，所提出的有状态多智能体推理框架在覆盖范围上相对于无状态单步基线有显著提升，在HumanEval和TestGenEvalMini等流行的单元测试基准上进行了评估，并使用了三种不同的LLM家族——Llama、Gemma和GPT。

Conclusion: 结合持久的推理时状态与进化搜索可以显著提高单元测试生成的效果。

Abstract: Recent work explores agentic inference-time techniques to perform structured,
multi-step reasoning. However, stateless inference often struggles on
multi-step tasks due to the absence of persistent state. Moreover,
task-specific fine-tuning or instruction-tuning often achieve surface-level
code generation but remain brittle on tasks requiring deeper reasoning and
long-horizon dependencies. To address these limitations, we propose stateful
multi-agent evolutionary search, a training-free framework that departs from
prior stateless approaches by combining (i) persistent inference-time state,
(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate
its effectiveness in automated unit test generation through the generation of
edge cases. We generate robust edge cases using an evolutionary search process,
where specialized agents sequentially propose, mutate, and score candidates. A
controller maintains persistent state across generations, while evolutionary
preservation ensures diversity and exploration across all possible cases. This
yields a generalist agent capable of discovering robust, high-coverage edge
cases across unseen codebases. Experiments show our stateful multi-agent
inference framework achieves substantial gains in coverage over stateless
single-step baselines, evaluated on prevalent unit-testing benchmarks such as
HumanEval and TestGenEvalMini and using three diverse LLM families - Llama,
Gemma, and GPT. These results indicate that combining persistent inference-time
state with evolutionary search materially improves unit-test generation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [114] [Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation](https://arxiv.org/abs/2510.06605)
*Shuo Shao,Yiming Li,Hongwei Yao,Yifei Chen,Yuchen Yang,Zhan Qin*

Main category: cs.CR

TL;DR: 本文提出了一种名为ZeroPrint的新方法，通过零阶估计在黑盒设置中近似信息丰富的梯度，以解决现有黑盒指纹技术无法生成独特指纹的问题。实验表明，ZeroPrint在有效性和鲁棒性方面达到了最先进的水平，显著优于现有的黑盒方法。


<details>
  <summary>Details</summary>
Motivation: The substantial investment required to develop Large Language Models (LLMs) makes them valuable intellectual property, raising significant concerns about copyright protection. LLM fingerprinting has emerged as a key technique to address this, which aims to verify a model's origin by extracting an intrinsic, unique signature (a 'fingerprint') and comparing it to that of a source model to identify illicit copies.

Method: ZeroPrint, a novel method that approximates these information-rich gradients in a black-box setting using zeroth-order estimation. ZeroPrint overcomes the challenge of applying this to discrete text by simulating input perturbations via semantic-preserving word substitutions.

Result: ZeroPrint achieves a state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.

Conclusion: ZeroPrint achieves a state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.

Abstract: The substantial investment required to develop Large Language Models (LLMs)
makes them valuable intellectual property, raising significant concerns about
copyright protection. LLM fingerprinting has emerged as a key technique to
address this, which aims to verify a model's origin by extracting an intrinsic,
unique signature (a "fingerprint") and comparing it to that of a source model
to identify illicit copies. However, existing black-box fingerprinting methods
often fail to generate distinctive LLM fingerprints. This ineffectiveness
arises because black-box methods typically rely on model outputs, which lose
critical information about the model's unique parameters due to the usage of
non-linear functions. To address this, we first leverage Fisher Information
Theory to formally demonstrate that the gradient of the model's input is a more
informative feature for fingerprinting than the output. Based on this insight,
we propose ZeroPrint, a novel method that approximates these information-rich
gradients in a black-box setting using zeroth-order estimation. ZeroPrint
overcomes the challenge of applying this to discrete text by simulating input
perturbations via semantic-preserving word substitutions. This operation allows
ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint.
Experiments on the standard benchmark show ZeroPrint achieves a
state-of-the-art effectiveness and robustness, significantly outperforming
existing black-box methods.

</details>


### [115] [Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2510.06719)
*Junki Mori,Kazuya Kakizaki,Taiki Miyagawa,Jun Sakuma*

Main category: cs.CR

TL;DR: 本文提出了一种名为DP-SynRAG的框架，利用大型语言模型生成差分隐私的合成RAG数据库，避免了重复注入噪声和额外的隐私成本，实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的私有RAG方法通常依赖于查询时间差分隐私（DP），这需要重复注入噪声并导致累积的隐私损失。

Method: DP-SynRAG框架使用大型语言模型生成差分隐私的合成RAG数据库，通过私有预测扩展来保留下游RAG任务的重要信息。

Result: 实验表明，DP-SynRAG在保持固定隐私预算的同时，实现了优于现有私有RAG系统的性能。

Conclusion: DP-SynRAG在保持固定隐私预算的同时，实现了优于现有私有RAG系统的性能，为隐私保护的RAG提供了可扩展的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding them in external knowledge. However, its application in sensitive
domains is limited by privacy risks. Existing private RAG methods typically
rely on query-time differential privacy (DP), which requires repeated noise
injection and leads to accumulated privacy loss. To address this issue, we
propose DP-SynRAG, a framework that uses LLMs to generate differentially
private synthetic RAG databases. Unlike prior methods, the synthetic text can
be reused once created, thereby avoiding repeated noise injection and
additional privacy costs. To preserve essential information for downstream RAG
tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate
text that mimics subsampled database records in a DP manner. Experiments show
that DP-SynRAG achieves superior performanec to the state-of-the-art private
RAG systems while maintaining a fixed privacy budget, offering a scalable
solution for privacy-preserving RAG.

</details>


### [116] [Exposing Citation Vulnerabilities in Generative Engines](https://arxiv.org/abs/2510.06823)
*Riku Mochizuki,Shusuke Komatsu,Souta Noguchi,Kazuto Ataka*

Main category: cs.CR

TL;DR: 本研究分析了生成引擎在引用来源选择上的脆弱性，提出了一种评估中毒攻击威胁的新方法，并发现美国政治答案比日本更容易受到攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的引用评估研究主要关注答案内容与引用来源的忠实度，而忽视了选择哪些网络来源作为引用以防御中毒攻击的问题。我们的研究旨在填补这一空白。

Method: 我们引入了评估标准，通过分析答案中的引用信息来评估中毒攻击的威胁。我们对日本和美国的政治领域进行了实验，分析了引用来源的发布者属性，以估计内容注入障碍。

Result: 实验结果表明，美国政治答案中来自官方政党网站的引用比例约为25%-45%，而日本则为60%-65%。这表明美国政治答案更容易受到中毒攻击。此外，低内容注入障碍的来源虽然常被引用，但在答案内容中却很少体现。

Conclusion: 我们的研究揭示了当前生成引擎在引用来源选择上的脆弱性，并提出了评估中毒攻击威胁的新标准。我们建议主要信息源的发布者增加其网页内容在答案中的曝光度，但同时也指出语言差异限制了现有技术的有效性。

Abstract: We analyze answers generated by generative engines (GEs) from the
perspectives of citation publishers and the content-injection barrier, defined
as the difficulty for attackers to manipulate answers to user prompts by
placing malicious content on the web. GEs integrate two functions: web search
and answer generation that cites web pages using large language models. Because
anyone can publish information on the web, GEs are vulnerable to poisoning
attacks. Existing studies of citation evaluation focus on how faithfully answer
content reflects cited sources, leaving unexamined which web sources should be
selected as citations to defend against poisoning attacks. To fill this gap, we
introduce evaluation criteria that assess poisoning threats using the citation
information contained in answers. Our criteria classify the publisher
attributes of citations to estimate the content-injection barrier thereby
revealing the threat of poisoning attacks in current GEs. We conduct
experiments in political domains in Japan and the United States (U.S.) using
our criteria and show that citations from official party websites (primary
sources) are approximately \(25\%\)--\(45\%\) in the U.S. and
\(60\%\)--\(65\%\) in Japan, indicating that U.S. political answers are at
higher risk of poisoning attacks. We also find that sources with low
content-injection barriers are frequently cited yet are poorly reflected in
answer content. To mitigate this threat, we discuss how publishers of primary
sources can increase exposure of their web content in answers and show that
well-known techniques are limited by language differences.

</details>


### [117] [VelLMes: A high-interaction AI-based deception framework](https://arxiv.org/abs/2510.06975)
*Muris Sladić,Veronica Valeros,Carlos Catania,Sebastian Garcia*

Main category: cs.CR

TL;DR: This paper presents an AI-based deception framework called VelLMes, which can simulate multiple protocols and services such as SSH Linux shell, MySQL, POP3, and HTTP. It is designed to be attacked by humans, so interactivity and realism are key for its performance. The results showed that LLMs can produce realistic-looking responses, and LLM honeypots can perform well against unstructured and unexpected attacks on the Internet.


<details>
  <summary>Details</summary>
Motivation: There are very few SotA deception systems based on Large Language Models. The existing ones are limited only to simulating one type of service, mainly SSH shells. These systems - but also the deception technologies not based on LLMs - lack an extensive evaluation that includes human attackers.

Method: The paper presents an AI-based deception framework called VelLMes, which can simulate multiple protocols and services such as SSH Linux shell, MySQL, POP3, and HTTP. It is designed to be attacked by humans, so interactivity and realism are key for its performance.

Result: The results of the unit tests show that, with careful prompting, LLMs can produce realistic-looking responses, with some LLMs having a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception capabilities with 89 human attackers. The results showed that about 30% of the attackers thought that they were interacting with a real system when they were assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH Linux shell honeypot on the Internet to capture real-life attacks. Analysis of these attacks showed us that LLM honeypots simulating Linux shells can perform well against unstructured and unexpected attacks on the Internet, responding correctly to most of the issued commands.

Conclusion: VelLMes can perform well against unstructured and unexpected attacks on the Internet, responding correctly to most of the issued commands.

Abstract: There are very few SotA deception systems based on Large Language Models. The
existing ones are limited only to simulating one type of service, mainly SSH
shells. These systems - but also the deception technologies not based on LLMs -
lack an extensive evaluation that includes human attackers. Generative AI has
recently become a valuable asset for cybersecurity researchers and
practitioners, and the field of cyber-deception is no exception. Researchers
have demonstrated how LLMs can be leveraged to create realistic-looking
honeytokens, fake users, and even simulated systems that can be used as
honeypots. This paper presents an AI-based deception framework called VelLMes,
which can simulate multiple protocols and services such as SSH Linux shell,
MySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus
VelLMes offers a variety of choices for deception design based on the users'
needs. VelLMes is designed to be attacked by humans, so interactivity and
realism are key for its performance. We evaluate the generative capabilities
and the deception capabilities. Generative capabilities were evaluated using
unit tests for LLMs. The results of the unit tests show that, with careful
prompting, LLMs can produce realistic-looking responses, with some LLMs having
a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception
capabilities with 89 human attackers. The results showed that about 30% of the
attackers thought that they were interacting with a real system when they were
assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH
Linux shell honeypot on the Internet to capture real-life attacks. Analysis of
these attacks showed us that LLM honeypots simulating Linux shells can perform
well against unstructured and unexpected attacks on the Internet, responding
correctly to most of the issued commands.

</details>


### [118] [RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning](https://arxiv.org/abs/2510.06994)
*Artur Horal,Daniel Pina,Henrique Paz,Iago Paulo,João Soares,Rafael Ferreira,Diogo Tavares,Diogo Glória-Silva,João Magalhães,David Semedo*

Main category: cs.CR

TL;DR: 本文提出了RedTWIZ框架，用于评估大型语言模型在AI辅助软件开发中的鲁棒性。该框架结合了评估、攻击生成和战略规划，以全面评估和暴露LLM的弱点。实验结果表明，多轮对抗性攻击策略可以成功使最先进的LLM生成不安全的内容，强调了增强LLM鲁棒性的紧迫需求。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估大型语言模型在AI辅助软件开发中的鲁棒性，并提出一种自适应且多样化的多轮红队框架，以发现LLM的弱点。

Method: 本文提出了RedTWIZ框架，结合了评估、攻击生成和战略规划，以全面评估和暴露LLM的弱点。该框架包括三个主要研究方向：(1) 对LLM对话越狱的稳健系统评估；(2) 一个多样化的生成多轮攻击套件，支持组合、现实和目标导向的越狱对话策略；(3) 一个分层攻击规划器，可自适应地规划、序列化并触发针对特定LLM漏洞的攻击。

Result: 实验结果表明，多轮对抗性攻击策略可以成功使最先进的LLM生成不安全的内容，强调了增强LLM鲁棒性的紧迫需求。

Conclusion: 本文提出了RedTWIZ框架，用于审计大型语言模型在AI辅助软件开发中的鲁棒性。实验结果表明，多轮对抗性攻击策略可以成功使最先进的LLM生成不安全的内容，强调了增强LLM鲁棒性的紧迫需求。

Abstract: This paper presents the vision, scientific contributions, and technical
details of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,
to audit the robustness of Large Language Models (LLMs) in AI-assisted software
development. Our work is driven by three major research streams: (1) robust and
systematic assessment of LLM conversational jailbreaks; (2) a diverse
generative multi-turn attack suite, supporting compositional, realistic and
goal-oriented jailbreak conversational strategies; and (3) a hierarchical
attack planner, which adaptively plans, serializes, and triggers attacks
tailored to specific LLM's vulnerabilities. Together, these contributions form
a unified framework -- combining assessment, attack generation, and strategic
planning -- to comprehensively evaluate and expose weaknesses in LLMs'
robustness. Extensive evaluation is conducted to systematically assess and
analyze the performance of the overall system and each component. Experimental
results demonstrate that our multi-turn adversarial attack strategies can
successfully lead state-of-the-art LLMs to produce unsafe generations,
highlighting the pressing need for more research into enhancing LLM's
robustness.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [119] [Crossing Domains without Labels: Distant Supervision for Term Extraction](https://arxiv.org/abs/2510.06838)
*Elena Senger,Yuri Campbell,Rob van der Goot,Barbara Plank*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型的自动术语提取方法，在多个领域中表现出色，并且优于现有的监督跨领域编码器模型和少量样本学习基线。


<details>
  <summary>Details</summary>
Motivation: 当前的自动术语提取方法需要昂贵的人工标注，并且在领域迁移方面存在困难，这限制了它们的实际部署。因此，需要更鲁棒、可扩展的解决方案和现实的评估设置。

Method: 本文的方法包括使用黑盒大语言模型生成伪标签，然后在这些数据上微调第一个大语言模型进行自动术语提取。此外，还引入了轻量级的后处理启发式方法来提高文档级别的连贯性。

Result: 本文的方法在5/7个领域中超过了之前的方法，平均提高了10个百分点。并且在该基准测试中，其表现与GPT-4o教师模型相当。

Conclusion: 本文提出了一种基于大语言模型的自动术语提取方法，该方法在多个领域中表现出色，并且优于现有的监督跨领域编码器模型和少量样本学习基线。同时，作者发布了数据集和微调模型以支持未来的研究。

Abstract: Automatic Term Extraction (ATE) is a critical component in downstream NLP
tasks such as document tagging, ontology construction and patent analysis.
Current state-of-the-art methods require expensive human annotation and
struggle with domain transfer, limiting their practical deployment. This
highlights the need for more robust, scalable solutions and realistic
evaluation settings. To address this, we introduce a comprehensive benchmark
spanning seven diverse domains, enabling performance evaluation at both the
document- and corpus-levels. Furthermore, we propose a robust LLM-based model
that outperforms both supervised cross-domain encoder models and few-shot
learning baselines and performs competitively with its GPT-4o teacher on this
benchmark. The first step of our approach is generating psuedo-labels with this
black-box LLM on general and scientific domains to ensure generalizability.
Building on this data, we fine-tune the first LLMs for ATE. To further enhance
document-level consistency, oftentimes needed for downstream tasks, we
introduce lightweight post-hoc heuristics. Our approach exceeds previous
approaches on 5/7 domains with an average improvement of 10 percentage points.
We release our dataset and fine-tuned models to support future research in this
area.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [120] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo是一个自演化的智能推理系统，通过结合计算和检索工具以及多模型协作，显著提升了基础模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型推理中的两个瓶颈问题：模型内在容量有限和测试时迭代不可靠。

Method: AlphaApollo结合了计算工具（如Python及其数值和符号库）和检索工具（任务相关的外部信息），并通过共享状态图支持多轮、多模型的解决方案演化。

Result: 在AIME 2024/2025基准测试中，AlphaApollo为Qwen2.5-14B-Instruct和Llama-3.3-70B-Instruct分别带来了+5.15% Average@32和+23.34% Pass@32，以及+8.91% Average@32和+26.67% Pass@32的提升。

Conclusion: AlphaApollo通过使用工具和多模型协作，显著提升了基础模型的推理能力，并在多个基准测试中表现出色。

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [121] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: 本文介绍了PuzzlePlex基准，用于评估基础模型的推理和规划能力。通过多样化的谜题，研究了模型在不同设置下的表现，并发现推理模型在指令设置中表现优于其他模型，而代码执行虽然更具挑战性，但提供了可扩展且高效的替代方案。


<details>
  <summary>Details</summary>
Motivation: This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments.

Method: We introduce PuzzlePlex, a benchmark designed to assess reasoning and planning capabilities through a diverse set of puzzles. We implement customized game-playing strategies for comparison and develop fine-grained metrics to measure performance.

Result: Reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative.

Conclusion: PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [122] [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.AI

TL;DR: 本文提出了一种名为Double-Loop Multi-Agent (DLMA)的框架，用于自动解决给定的研究问题。该框架包含两个循环：领导循环负责演化研究计划，而跟随循环负责执行最佳演化的计划。实验结果表明，DLMA生成的研究论文在自动化评估中达到了最先进的分数，显著优于强大的基线。


<details>
  <summary>Details</summary>
Motivation: 自动化端到端的科学研究过程面临根本性挑战：它需要产生新颖且合理的高层计划，并在动态和不确定条件下正确执行这些计划。

Method: 我们提出了一个名为Double-Loop Multi-Agent (DLMA)的框架，该框架由两个循环组成：领导循环由教授代理组成，负责演化研究计划；跟随循环由博士生代理组成，负责执行最佳演化的计划，并在实施过程中动态调整计划。

Result: 在ACLAward和Laboratory等基准测试中进行的大量实验表明，DLMA生成的研究论文在自动化评估中达到了最先进的分数，显著优于强大的基线。

Conclusion: DLMA生成的研究论文在自动化评估中达到了最先进的分数，显著优于强大的基线。消融研究证实了两个循环的关键作用，进化驱动新颖性，执行确保合理性。

Abstract: Automating the end-to-end scientific research process poses a fundamental
challenge: it requires both evolving high-level plans that are novel and sound,
and executing these plans correctly amidst dynamic and uncertain conditions. To
address this bilevel challenge, we propose a novel Double-Loop Multi-Agent
(DLMA) framework to solve the given research problem automatically. The leader
loop, composed of professor agents, is responsible for evolving research plans.
It employs an evolutionary algorithm through involvement, improvement, and
integration meetings to iteratively generate and refine a pool of research
proposals, exploring the solution space effectively. The follower loop,
composed of doctoral student agents, is responsible for executing the
best-evolved plan. It dynamically adjusts the plan during implementation via
pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is
well-supported by contextual and external observations. Extensive experiments
on benchmarks like ACLAward and Laboratory show that DLMA generates research
papers that achieve state-of-the-art scores in automated evaluation,
significantly outperforming strong baselines. Ablation studies confirm the
critical roles of both loops, with evolution driving novelty and execution
ensuring soundness.

</details>


### [123] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型推理痕迹中的信息密度均匀性，发现步骤级的均匀性可以作为推理质量的有效指标，并能提高模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 本文重新审视了大型语言模型（LLM）推理痕迹中的这一原则，询问步骤级的均匀性是否反映了推理质量。

Method: 我们提出了一个基于熵的逐步信息密度度量，并引入了两种互补的均匀性度量：局部和全局均匀性分数。

Result: 在六个不同的推理基准测试中，我们发现步骤级的均匀性不仅提供了强大的理论视角，还带来了实际性能优势；例如，在AIME2025上，选择步骤级信息密度更均匀的推理痕迹相对于基线提高了10-32%的相对收益。

Conclusion: 结果表明，基于UID的信息密度度量优于其他内部信号，作为推理质量的预测因子。结果强调了信息密度的均匀性作为构建更可靠和准确的推理系统的稳健诊断和选择标准。

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [124] [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
*Baixuan Xu,Tianshi Zheng,Zhaowei Wang,Hong Ting Tsang,Weiqi Wang,Tianqing Fang,Yangqiu Song*

Main category: cs.AI

TL;DR: 本文研究了两种动作表示方法（PwA和PwS），并提出了认知带宽视角作为理解它们差异的概念框架。实证结果表明，在环境动作空间较大时，PwS表现更好，但需要进一步优化。


<details>
  <summary>Details</summary>
Motivation: 当环境动作空间扩大时，传统基于动作的表示方法变得不切实际。因此，本文旨在探讨在不同环境动作空间下，哪种动作表示方法更适合长期规划的智能体。

Method: 本文系统研究了两种不同的动作表示方法：传统的基于动作的规划（PwA）和基于模式的规划（PwS）。PwS通过将动作模式实例化为动作列表来确保简洁的动作空间和可靠的可扩展性。本文还提出了认知带宽视角作为概念框架，用于定性理解这两种动作表示的差异，并进行了受控实验来研究转折点的位置如何与不同的模型能力相互作用。

Result: 本文实证观察到在ALFWorld和SciWorld之间存在一个表示选择的转折点，这表明需要可扩展的表示方法。此外，更强的规划能力会使转折点向右移动，而更好的模式实例化会使转折点向左移动。最后，本文指出PwS代理的性能不佳，并提供了构建更强大PwS代理的可行指南。

Conclusion: 本文提出了认知带宽视角作为理解两种动作表示差异的概念框架，并实证观察到了在ALFWorld（约35个动作）和SciWorld（约500个动作）之间的表示选择转折点，这证明了可扩展表示的必要性。此外，本文还提供了构建更强大PwS代理的可行指南，以实现更好的可扩展自主性。

Abstract: Enabling LLMs to effectively operate long-horizon task which requires
long-term planning and multiple interactions is essential for open-world
autonomy. Conventional methods adopt planning with actions where a executable
action list would be provided as reference. However, this action representation
choice would be impractical when the environment action space is combinatorial
exploded (e.g., open-ended real world). This naturally leads to a question: As
environmental action space scales, what is the optimal action representation
for long-horizon agents? In this paper, we systematically study the
effectiveness of two different action representations. The first one is
conventional planning with actions (PwA) which is predominantly adopted for its
effectiveness on existing benchmarks. The other one is planning with schemas
(PwS) which instantiate an action schema into action lists (e.g., "move [OBJ]
to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable
scalability. This alternative is motivated by its alignment with human
cognition and its compliance with environment-imposed action format
restriction. We propose cognitive bandwidth perspective as a conceptual
framework to qualitatively understand the differences between these two action
representations and empirically observe a representation-choice inflection
point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve
as evidence of the need for scalable representations. We further conduct
controlled experiments to study how the location of this inflection point
interacts with different model capacities: stronger planning proficiency shifts
the inflection rightward, whereas better schema instantiation shifts it
leftward. Finally, noting the suboptimal performance of PwS agents, we provide
an actionable guide for building more capable PwS agents for better scalable
autonomy.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [125] [GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting](https://arxiv.org/abs/2510.06782)
*Kaichun Yang,Jian Chen*

Main category: cs.HC

TL;DR: 研究比较了GPT-5和GPT-4V在图表阅读任务中的表现，发现模型架构对准确性有更大影响。


<details>
  <summary>Details</summary>
Motivation: 为了理解零样本大型语言模型和提示使用对图表阅读任务的影响，进行定量评估。

Method: 通过让大型语言模型回答107个可视化问题，比较了代理GPT-5和多模态GPT-4V之间的推理准确性。

Result: 结果表明，GPT-5显著提高了准确性，而提示变体的影响很小。

Conclusion: 模型架构在推理准确性中起主导作用，而提示变体仅产生较小的影响。

Abstract: We present a quantitative evaluation to understand the effect of zero-shot
large-language model (LLMs) and prompting uses on chart reading tasks. We asked
LLMs to answer 107 visualization questions to compare inference accuracies
between the agentic GPT-5 and multimodal GPT-4V, for difficult image instances,
where GPT-4V failed to produce correct answers. Our results show that model
architecture dominates the inference accuracy: GPT5 largely improved accuracy,
while prompt variants yielded only small effects. Pre-registration of this work
is available here:
https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3; the Google
Drive materials are
here:https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view.

</details>

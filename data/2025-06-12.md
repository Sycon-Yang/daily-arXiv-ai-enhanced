<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

TL;DR: 提出了一种基于大型语言模型（LLM）的定性评估方法，称为LLM-as-a-qualitative-judge，其主要输出为自然语言生成系统输出的常见问题类型的结构化报告。该方法通过开放式的实例级问题分析和使用直观累积算法的问题聚类，为开发人员提供有关改进给定自然语言生成系统的有意义的见解。对来自12个自然语言生成数据集的约300个实例中的问题进行了评估。结果显示，该方法正确识别了2/3实例的具体问题，并能够生成类似于人工注释员所编写的错误类型报告。代码和数据公开可获取。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的评价方法主要作为定量工具使用，而本文旨在提供一种定性的评价方式，为开发人员提供关于如何改进自然语言生成系统的有意义的见解。

Method: 提出了一种名为LLM-as-a-qualitative-judge的方法，包括两个主要步骤：开放式的实例级问题分析和使用直观累积算法的问题聚类。

Result: 该方法正确识别了2/3实例的具体问题，并且生成的错误类型报告与人工注释员的报告相似。

Conclusion: 本研究提出了一种新的基于LLM的定性评价方法，该方法能够帮助开发者更好地理解并改进自然语言生成系统。

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [2] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

TL;DR: This paper proposes a phrase dictionary biasing method to improve phrase translation in speech translation tasks. The method enhances both transducer-based streaming speech translation models and multimodal large language models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of translating rarely occurring phrases in training data for speech translation tasks.

Method: Proposes a phrase dictionary biasing method that leverages source-to-target language phrase pairs.

Result: The method improves performance by 21% relatively for streaming speech translation models and achieves 85% relative improvement in phrase recall for multimodal large language models.

Conclusion: The proposed phrase dictionary biasing method effectively enhances phrase translation in both types of speech translation models.

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [3] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
*Bruno Ferenc Šegedin*

Main category: cs.CL

TL;DR: This study investigates the generalization capacity of generative CNNs trained on raw audio waveforms of lexical items and proposes a new method for probing a model's lexically-independent generalizations.


<details>
  <summary>Details</summary>
Motivation: To explore the ability of DNNs to represent phonotactic generalizations derived from lexical learning.

Method: Trained generative CNNs on raw audio waveforms of lexical items and explored the consequences of shrinking the FC bottleneck from 1024 channels to 8 before training.

Result: The convolutional layers can dynamically generalize phonetic dependencies beyond lexically-constrained configurations learned by the FC.

Conclusion: A novel technique for probing a model's lexically-independent generalizations is proposed that works only under the narrow FC bottleneck.

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [4] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: Transformer language models can generalize well to longer inputs by transferring knowledge from related tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how transformer language models achieve generalization capabilities.

Method: Investigate length generalization through task association and demonstrate the transfer of length generalization across diverse algorithmic tasks.

Result: Transformer models can inherit generalization capabilities from similar tasks when trained jointly. Pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings.

Conclusion: Length generalization transfer correlates with the re-use of the same attention heads between tasks.

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [5] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: This study focuses on identifying prosocial behaviors in game-chat text using an unsupervised method and a new model called SAAM, achieving better performance than previous techniques. It developed the first automated system for classifying prosocial behaviors in in-game chats, especially in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: To shift the focus of moderation from penalizing toxicity to encouraging positive interactions by identifying prosocial behaviors in game-chat text.

Method: Unsupervised discovery combined with game domain expert collaboration and a novel Self-Anchored Attention Model (SAAM).

Result: The proposed approach led to the development of the first automated system for classifying prosocial behaviors in in-game chats, showing effectiveness in low-resource settings.

Conclusion: This research applies NLP techniques to discover and classify prosocial behaviors in player in-game chat communication, helping promote positive interactions on online platforms.

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [6] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
*Milan Bhan,Jean-Noel Vittaut,Nicolas Chesneau,Sarath Chandar,Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: This paper introduces a new method to measure the faithfulness of self-NLE generated by LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for measuring self-NLE faithfulness rely on behavioral tests or computational block identification, but do not examine the neural activity underlying the model's reasoning.

Method: The authors propose a novel flexible framework that compares LLM-generated self-NLE with interpretations of the model's internal hidden states.

Result: The proposed framework provides deep insights into self-NLE faithfulness and establishes a direct connection between self-NLE and model reasoning.

Conclusion: This approach advances the understanding of self-NLE faithfulness and provides building blocks for generating more faithful self-NLE.

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [7] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: Introduce (RSA)^2 framework to interpret non-literal expressions by considering rhetorical strategies rather than specific motivations.


<details>
  <summary>Details</summary>
Motivation: To model figurative language use in human communication effectively without focusing on specific motivations.

Method: (RSA)^2 framework considers rhetorical strategies to interpret non-literal expressions.

Result: Achieved state-of-the-art performance on the ironic split of PragMega+ dataset when combined with LLMs.

Conclusion: The proposed framework provides a human-compatible interpretation of non-literal utterances.

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [8] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

TL;DR: This study improves the detection of Alzheimer's dementia by using an advanced language model, achieving higher accuracy than previous methods and offering clearer decision boundaries.


<details>
  <summary>Details</summary>
Motivation: To enhance the detection of Alzheimer's dementia through improved language model techniques.

Method: Using an instruction-following version of Mistral-7B large language model with the paired perplexity approach.

Result: Achieved 3.33% higher accuracy than the best current paired perplexity method and 6.35% more accurate than the top-ranked method from the ADReSS 2020 challenge benchmark.

Conclusion: The proposed method provides clear and interpretable decision boundaries, unlike other methods, and shows that the language models have learned specific language patterns of AD speakers.

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [9] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
*Yuxin Jiang*

Main category: cs.CL

TL;DR: This thesis introduces new methods in data collection, training, and evaluation to improve large language model alignment.


<details>
  <summary>Details</summary>
Motivation: The need for more efficient and effective ways to align large language models with human expectations.

Method: Proposes Lion for adversarial distillation, Web Reconstruction for automated data synthesis, Learning to Edit for knowledge integration, Bridging and Modeling Correlations for DPO refinement, and FollowBench for evaluating constraint adherence.

Result: State-of-the-art zero-shot reasoning with Lion, improved data diversity and scalability with WebR, enhanced knowledge updates with LTE, superior alignment with BMC, and exposure of model weaknesses with FollowBench.

Conclusion: Novel methodologies in data collection, training, and evaluation advance LLM alignment significantly.

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [10] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

TL;DR: This study explores if large language models (LLMs) can model and reason about others' intentions (theory of mind). It uses cooperative multi-agent reinforcement learning to evaluate this capability, aiming to improve artificial agents' adaptability and cooperation with both AI and human partners.


<details>
  <summary>Details</summary>
Motivation: Understanding the intention of others is vital for effective collaboration, essential for societal success and necessary for cooperative interactions among agents.

Method: Cooperative multi-agent reinforcement learning where agents learn to collaborate through repeated interactions.

Result: The research investigates the theory of mind in LLMs and seeks to develop hybrid human-AI systems for better collaboration.

Conclusion: This work contributes to the understanding of whether LLMs possess a form of theory of mind and how they can be integrated into hybrid human-AI systems.

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [11] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

TL;DR: This paper introduces RePO, a method for optimizing large language models using diverse replay strategies to enhance policy optimization and improve performance on mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs and low data efficiency of previous methods like GRPO when optimizing large language models.

Method: Replay-Enhanced Policy Optimization (RePO) uses diverse replay strategies to retrieve off-policy samples from a replay buffer, enabling policy optimization with a broader and more diverse set of samples for each prompt.

Result: RePO achieved significant performance gains of 18.4 and 4.1 points for Qwen2.5-Math-1.5B and Qwen3-1.7B respectively, compared to GRPO. It also increased computational cost by 15% but raised the number of effective optimization steps by 48% for Qwen3-1.7B.

Conclusion: RePO improves the performance of large language model optimization with enhanced data efficiency and a manageable increase in computational cost.

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [12] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: This paper explores latent multi-head attention (MLA) for small language models, showing that MLA with rotary positional embeddings (MLA+RoPE) improves memory efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To investigate efficiency-quality trade-offs in small language models.

Method: Benchmarking three architectural variants: standard multi-head attention (MHA), MLA, and MLA with RoPE using 30M-parameter GPT models trained on 100,000 synthetic stories.

Result: MLA+RoPE with half-rank latent dimensions reduces KV-cache memory by 45% and slightly increases validation loss compared to MHA. RoPE is essential for MLA's performance in small models.

Conclusion: MLA+RoPE offers a Pareto improvement for memory-constrained deployment with minor quality loss.

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [13] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Jieping Ye*

Main category: cs.CL

TL;DR: This paper introduces OmniDRCA, a parallel speech-text foundation model using joint autoregressive modeling with dual-resolution speech representations and contrastive cross-modal alignment, achieving SOTA performance on Spoken Question Answering benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve text generation awareness of concurrent speech synthesis and enhance audio comprehension through contrastive alignment.

Method: OmniDRCA, a parallel speech-text foundation model based on joint autoregressive modeling with dual-resolution speech representations and contrastive cross-modal alignment.

Result: Experimental results show that OmniDRCA establishes new SOTA performance among parallel joint speech-text modeling based foundation models and achieves competitive performance compared to interleaved models.

Conclusion: OmniDRCA shows potential in full-duplex conversational scenarios.

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [14] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
*Yuchen Feng,Bowen Shen,Naibin Gu,Jiaxuan Zhao,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: 提出了一种名为DIVE的多样性增强的重建方法，该方法包括领域亲和性挖掘、基于剪枝的专家重建和高效再训练。在Llama风格的LLMs上实现了DIVE，并使用开源训练语料库进行了实验，结果显示DIVE在最小的准确性权衡下实现了训练效率，优于具有相同激活参数数量的现有剪枝和MoE重建方法。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE LLMs训练方法要么有很高的初始训练开销，要么在将密集LLM重构为MoE LLM时忽略专家之间的多样性，导致潜在的冗余。

Method: 提出了一种名为DIVE的多样性增强的重建方法，包括三个主要步骤：1）领域亲和性挖掘；2）基于剪枝的专家重建（涉及FFN模块的剪枝和重组）；3）对路由器、专家和归一化模块进行高效再训练。

Result: 在Llama风格的LLMs上实现DIVE，并使用开源训练语料库进行实验，结果显示DIVE在最小的准确性权衡下实现了训练效率。

Conclusion: DIVE是一种有效的训练MoE架构LLMs的方法，能够在减少冗余的同时保持模型性能。

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [15] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
*Qingyun Zeng,Simin Ma,Arash Niknafs,Ashish Basran,Carol Szabo*

Main category: cs.CL

TL;DR: This paper discusses using large language models to evaluate both semantic and weak semantic equivalence in text-to-sql systems.


<details>
  <summary>Details</summary>
Motivation: Evaluating semantic equivalence of generated SQL is challenging due to ambiguous user queries and multiple valid SQL interpretations.

Method: Exploring the use of LLMs for assessing both semantic and weak semantic equivalence.

Result: Analysis of common patterns of SQL equivalence and inequivalence, discussion of challenges in LLM-based evaluation.

Conclusion: LLMs can be used to evaluate semantic and weak semantic equivalence in text-to-SQL systems.

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [16] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

TL;DR: This paper introduces COGENT, a framework for creating educational content aligned with curriculum standards and appropriate for different grades. It uses three curriculum components, readability control, and a 'wonder-based' approach.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of using Generative AI in educational contexts, particularly in STEM, where maintaining grade-appropriate language and engaging students is difficult.

Method: Developing COGENT which includes curriculum components, readability control methods, and a 'wonder-based' approach for content generation.

Result: COGENT generates grade-appropriate content that matches or exceeds human-written references based on multi-dimensional evaluations.

Conclusion: The proposed framework offers a scalable solution for providing adaptive and high-quality educational resources.

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [17] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
*Massa Baali,Shuo Han,Syed Abdul Hannan,Purusottam Samal,Karanveer Singh,Soham Deshmukh,Rita Singh,Bhiksha Raj*

Main category: cs.CL

TL;DR: Introduces CoLMbo, a Speaker Language Model that combines speaker encoder with prompt-based conditioning to generate detailed speaker descriptions, enhancing traditional speaker profiling and performing well in zero-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: Speaker recognition systems struggle to generate detailed speaker characteristics or provide context-rich descriptions. They mainly focus on speaker identification but fail to capture demographic attributes like dialect, gender, and age systematically.

Method: Integrating a speaker encoder with prompt-based conditioning to create detailed captions based on speaker embeddings.

Result: CoLMbo can adapt dynamically to new speaker characteristics using user-defined prompts and provide customized descriptions including regional dialect variations and age-related traits. It performs well in zero-shot scenarios across diverse datasets.

Conclusion: This approach not only improves traditional speaker profiling but also represents a significant advancement in speaker recognition.

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [18] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
*Austin McCutcheon,Thiago E. A. de Oliveira,Aleksandr Zheleznov,Chris Brogly*

Main category: cs.CL

TL;DR: This study evaluates different machine learning models to automatically classify news headlines/links into high or low quality categories. It uses a large dataset of worldwide news links/headings with 115 linguistic features and finds that both traditional ensemble methods and fine-tuned deep learning models perform well.


<details>
  <summary>Details</summary>
Motivation: To automatically distinguish perceived lower-quality news headlines/links from higher-quality ones due to the proliferation of online news.

Method: Evaluated twelve machine learning models including traditional ensemble methods and fine-tuned DistilBERT on a large balanced dataset of worldwide news links/headings with 115 linguistic features.

Result: Traditional ensemble methods like the bagging classifier showed strong performance while fine-tuned DistilBERT achieved the highest accuracy but needed more training time.

Conclusion: Both NLP features with traditional classifiers and deep learning models can effectively differentiate perceived news headline/link quality with a trade-off between predictive performance and training time.

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [19] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Main category: cs.CL

TL;DR: Large language models face challenges in polite speech alignment. Study shows bigger models can replicate key preferences and human evaluators prefer LLM-generated responses in open-ended contexts.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs use similar context-sensitive linguistic strategies as humans do.

Method: Comparing human and LLM responses in constrained and open-ended production tasks.

Result: Larger models can replicate key preferences and human evaluators prefer LLM responses in open-ended contexts.

Conclusion: LLMs show impressive handling of politeness strategies but subtly different from humans.

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [20] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
*Xinyi Gao,Qiucheng Wu,Yang Zhang,Xuechen Liu,Kaizhi Qian,Ying Xu,Shiyu Chang*

Main category: cs.CL

TL;DR: A probabilistic KT framework named KT$^2$ is proposed to model student understanding over a tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree Model.


<details>
  <summary>Details</summary>
Motivation: Existing KT approaches face significant challenges in low-resource classroom settings. The hierarchical knowledge concept information can provide strong prior when data are sparse.

Method: Proposed Knowledge-Tree-based Knowledge Tracing (KT$^2$), a probabilistic KT framework that models student understanding over a tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree Model. Estimated student mastery via an EM algorithm and supports personalized prediction through an incremental update mechanism as new responses arrive.

Result: Experiments show that KT$^2$ consistently outperforms strong baselines in realistic online, low-resource settings.

Conclusion: KT$^2$ can restore strong performance under low-resource conditions by utilizing the hierarchical knowledge concept information.

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [21] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

TL;DR: This paper introduces Token Constraint Decoding (TCD), an inference-time algorithm that improves robustness of Large Language Models against input perturbations by enforcing token-level prediction alignment. Experiments show that TCD, combined with prompt engineering, can significantly restore performance on multiple-choice question answering tasks.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of Large Language Models to minor input perturbations.

Method: Introducing and evaluating Token Constraint Decoding (TCD), which enforces token-level prediction alignment during inference.

Result: TCD, especially when paired with prompt engineering, significantly restores performance degraded by input noise, yielding up to +39% absolute gains for weaker models.

Conclusion: Token Constraint Decoding is a practical, model-agnostic method to enhance reasoning stability under real-world imperfections.

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [22] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Main category: cs.CL

TL;DR: A novel prompt-guided generative framework called PGDA-KGQA is proposed to enhance Knowledge Graph Question Answering performance by improving data diversity and multi-hop reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional data augmentation approaches and LLM-based methods in KGQA, specifically the scarcity of diverse annotated data and multi-hop reasoning samples, as well as the issue of semantic distortion.

Method: The method uses a unified prompt-design paradigm to generate large-scale (question, logical form) pairs for model training, enriching the training set by generating single-hop pseudo questions, applying semantic-preserving question rewriting, and employing answer-guided reverse path exploration.

Result: Experiments show that PGDA-KGQA outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements in F1, Hits@1, and Accuracy.

Conclusion: PGDA-KGQA improves the performance of Knowledge Graph Question Answering through a prompt-guided generative framework with multiple data augmentation strategies.

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [23] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
*Md Messal Monem Miah,Adrita Anika,Xi Shi,Ruihong Huang*

Main category: cs.CL

TL;DR: This study evaluates automated deception detection using large language models and multimodal models across various datasets.


<details>
  <summary>Details</summary>
Motivation: Deception detection is crucial in the digital age, and understanding the capabilities of large models can enhance real-world applications.

Method: Assessing performance of LLMs and LMMs on datasets like RLTD, MU3D, and OpSpam using zero-shot and few-shot approaches.

Result: Fine-tuned LLMs perform best in textual deception detection, whereas LMMs face challenges with cross-modal cues.

Conclusion: The study highlights the potential and limitations of LLMs in deception detection, providing valuable insights for future research and application.

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [24] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

TL;DR: A novel cost-effective SFT method is proposed to reduce catastrophic forgetting risk without original SFT data.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLMs' instruction-following capabilities and domain-specific task adaptability while maintaining general capabilities and avoiding catastrophic forgetting.

Method: Reconstructing SFT instruction distribution, multi-model screening for optimal data selection, mixing with new data for SFT.

Result: Preserves generalization capabilities in general domains and improves task-specific performance.

Conclusion: The proposed method effectively reduces the risk of catastrophic forgetting and enhances both general and specific task performances.

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [25] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

TL;DR: Introduces GigaChat, a family of Russian LLMs, detailing architecture, pre-training, experiments, evaluations, and open-source release.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limited development of foundational models specifically tailored to the Russian language due to high computational resource requirements.

Method: The method involves developing Russian LLMs in various sizes, detailing the model architecture, pre-training process, and experiments, and evaluating their performance on Russian and English benchmarks.

Result: The result includes the introduction of the GigaChat family of Russian LLMs, evaluations on Russian and English benchmarks, and comparisons with multilingual analogs.

Conclusion: The paper concludes by releasing three open GigaChat models in open-source and demonstrating the top-performing models through different interfaces.

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [26] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

TL;DR: This paper introduces UniToMBench, a unified benchmark that enhances and evaluates Theory of Mind (ToM) capabilities in large language models (LLMs). It combines multi-interaction tasks and diverse evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately predicting human mental states in LLMs.

Method: Integrates SimToM and TOMBENCH to create UniToMBench, using over 1,000 hand-written scenarios and perspective-taking techniques.

Result: Models like GPT-4o and GPT-4o Mini perform well in emotional and belief tasks but show variability in knowledge-based tasks.

Conclusion: UniToMBench serves as a valuable tool for improving and assessing ToM capabilities in LLMs.

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [27] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: This paper identifies a 'reward-generation gap' issue in Direct Alignment Algorithms (DAAs) for aligning large language models with human preferences, caused by the mismatch between prefix tokens' importance and implicit reward functions. To solve this, a new approach called Prefix-Oriented Equal-length Training (POET) is introduced, which truncates responses to match shorter lengths, improving DAA performance significantly.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the 'reward-generation gap' issue in DAAs, which hinders their alignment with human preferences due to the misalignment between training and inference.

Method: The method involves introducing POET, which truncates both preferred and dispreferred responses to match the shorter length, ensuring optimization convergence across all token positions, especially focusing on prefix tokens.

Result: Experiments with DPO and SimPO show that POET improves their performance, achieving up to 15.6 points in AlpacaEval 2 and overall improvements across downstream tasks.

Conclusion: The study concludes that addressing the misalignment between reward optimization and generation performance in DAAs is crucial, with POET being an effective solution.

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [28] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

TL;DR: 本研究通过结合三种方法探讨了YouTube上自杀行为的表现，揭示了数字行为和临床见解之间的联系。


<details>
  <summary>Details</summary>
Motivation: 鉴于西方国家自杀仍然是主要死因，以及社交媒体在日常生活中日益重要，本研究旨在探索自杀行为在YouTube上的表现，并与专家知识进行比较。

Method: 采用计算自下而上方法、混合方法和专家驱动自上而下的方法，在包含181个YouTube频道的新型纵向数据集上进行研究，这些频道属于有危及生命企图的人，同时包括134个对照频道。

Result: 在自下而上的方法中，LLM驱动的主题建模识别出166个主题中的五个与自杀企图相关，并且有两个主题显示出与自杀企图相关的时序变化。在混合方法中，临床专家审查LLM衍生主题并标记了19个主题为与自杀相关，但没有发现显著的时序效应。在自上而下的方法中，对自杀企图叙述的心理评估揭示了两个组之间唯一显著差异在于分享经验的动机。

Conclusion: 通过整合三种互补方法，本研究提供了对自杀行为的细致理解，弥合了数字行为和临床见解之间的差距。

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [29] [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)
*Jiayi Yuan,Hao Li,Xinheng Ding,Wenya Xie,Yu-Jhe Li,Wentian Zhao,Kun Wan,Jing Shi,Xia Hu,Zirui Liu*

Main category: cs.CL

TL;DR: 本研究首次系统研究了数值精度对大型语言模型推理可重复性的影响，并提出了一个名为LayerCast的新方法来平衡内存效率与数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在各个领域都具有重要作用并表现出色，但其性能的可重复性却很脆弱。改变系统配置如评估批量大小、GPU数量和GPU版本可以引入显著差异在生成的响应中。这种问题在推理模型中尤为明显，其中早期令牌中的小数点差异可能会导致链式思维的发散，最终影响准确性。例如，在bfloat16精度和贪婪解码下，推理模型如DeepSeek-R1-Distill-Qwen-7B可能由于GPU数量、类型和评估批量大小的不同而出现高达9％的准确性变化和9000个令牌的响应长度差异。

Method: 通过在各种硬件、软件和精度设置下进行仔细控制的实验，量化了模型输出何时以及如何发生分歧。开发了一种轻量级推理管道LayerCast，存储权重在16位精度但执行所有计算在FP32。

Result: 研究发现，浮点精度对于可重复性至关重要，但通常在评估实践中被忽视。提出了一种名为LayerCast的轻量级推理管道，它在16位精度下存储权重，但在FP32中执行所有计算，从而平衡了内存效率与数值稳定性。

Conclusion: 本文首次系统研究了数值精度对LLM推理可重复性的影响，并通过严格控制的实验量化了模型输出何时以及如何发生分歧。分析揭示，虽然浮点精度对于可重复性至关重要，但在评估实践中往往被忽视。受此启发，开发了一种轻量级推理管道LayerCast，平衡了内存效率与数值稳定性。

Abstract: Large Language Models (LLMs) are now integral across various domains and have
demonstrated impressive performance. Progress, however, rests on the premise
that benchmark scores are both accurate and reproducible. We demonstrate that
the reproducibility of LLM performance is fragile: changing system
configuration such as evaluation batch size, GPU count, and GPU version can
introduce significant difference in the generated responses. This issue is
especially pronounced in reasoning models, where minor rounding differences in
early tokens can cascade into divergent chains of thought, ultimately affecting
accuracy. For instance, under bfloat16 precision with greedy decoding, a
reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation
in accuracy and 9,000 tokens difference in response length due to differences
in GPU count, type, and evaluation batch size. We trace the root cause of this
variability to the non-associative nature of floating-point arithmetic under
limited numerical precision. This work presents the first systematic
investigation into how numerical precision affects reproducibility in LLM
inference. Through carefully controlled experiments across various hardware,
software, and precision settings, we quantify when and how model outputs
diverge. Our analysis reveals that floating-point precision -- while critical
for reproducibility -- is often neglected in evaluation practices. Inspired by
this, we develop a lightweight inference pipeline, dubbed LayerCast, that
stores weights in 16-bit precision but performs all computations in FP32,
balancing memory efficiency with numerical stability. Code is available at
https://github.com/nanomaoli/llm_reproducibility.

</details>


### [30] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
*Bingheng Wu,Jingze Shi,Yifan Wu,Nan Tang,Yuyu Luo*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [31] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
*Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang Xu*

Main category: cs.CL

TL;DR: Introduce ReasonMed, the largest medical reasoning dataset, to improve LLMs' performance in knowledge-intensive medical question answering.


<details>
  <summary>Details</summary>
Motivation: Explore LLMs' capabilities in knowledge-intensive medical question answering.

Method: Construct ReasonMed via multi-agent verification and refinement process and train ReasonMed-7B based on effective fine-tuning strategy.

Result: ReasonMed-7B outperforms previous best model and even exceeds LLaMA3.1-70B on PubMedQA.

Conclusion: ReasonMed and ReasonMed-7B set new benchmarks for medical reasoning models.

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [32] [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)
*Dingjun Wu,Yukun Yan,Zhenghao Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: KG-Infused RAG combines knowledge graphs and retrieval-augmented generation to enhance factual accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods lack cognitively inspired mechanisms and rely on single knowledge sources.

Method: Integrating knowledge graphs into RAG systems using spreading activation, expanding queries, and enhancing generation with combined corpus passages and structured facts.

Result: Improved performance on five QA benchmarks compared to vanilla RAG, with additional gains when integrated into Self-RAG.

Conclusion: KG-Infused RAG is an effective and versatile plug-and-play enhancement module for RAG methods.

Abstract: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding
responses in external knowledge. However, existing methods typically rely on a
single source, either unstructured text or structured knowledge. Moreover, they
lack cognitively inspired mechanisms for activating relevant knowledge. To
address these issues, we propose KG-Infused RAG, a framework that integrates
KGs into RAG systems to implement spreading activation, a cognitive process
that enables concept association and inference. KG-Infused RAG retrieves KG
facts, expands the query accordingly, and enhances generation by combining
corpus passages with structured facts, enabling interpretable, multi-source
retrieval grounded in semantic structure. We further improve KG-Infused RAG via
preference learning on sampled key stages in the pipeline. Experiments on five
QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by
3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG
brings further performance gains, demonstrating its effectiveness and
versatility as a plug-and-play enhancement module for corpus-based RAG methods.

</details>


### [33] [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
*Georgios Chatzichristodoulou,Despoina Kosmopoulou,Antonios Kritikos,Anastasia Poulopoulou,Efthymios Georgiou,Athanasios Katsamanis,Vassilis Katsouros,Alexandros Potamianos*

Main category: cs.CL

TL;DR: MEDUSA, a multimodal framework with a four-stage training pipeline, achieved state-of-the-art results in speech emotion recognition.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of subjective human emotions and their uneven representation under naturalistic conditions.

Method: A four-stage training pipeline using an ensemble of classifiers with DeepSER and Manifold MixUp, followed by optimization of a trainable meta-classifier.

Result: Effective handling of class imbalance and emotion ambiguity, leading to high performance in categorical emotion recognition.

Conclusion: MEDUSA ranked 1st in Task 1 of the Interspeech 2025 challenge.

Abstract: SER is a challenging task due to the subjective nature of human emotions and
their uneven representation under naturalistic conditions. We propose MEDUSA, a
multimodal framework with a four-stage training pipeline, which effectively
handles class imbalance and emotion ambiguity. The first two stages train an
ensemble of classifiers that utilize DeepSER, a novel extension of a deep
cross-modal transformer fusion mechanism from pretrained self-supervised
acoustic and linguistic representations. Manifold MixUp is employed for further
regularization. The last two stages optimize a trainable meta-classifier that
combines the ensemble predictions. Our training approach incorporates human
annotation scores as soft targets, coupled with balanced data sampling and
multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion
Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic
Conditions Challenge.

</details>


### [34] [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)
*Eleni Gkovedarou,Joke Daems,Luna De Bruyne*

Main category: cs.CL

TL;DR: This study examines gender bias in Google Translate and DeepL for English-to-Greek translations, identifying male bias, occupational stereotyping, and errors in anti-stereotypical translations. It also explores GPT-4o as a bias mitigation tool, using a new dataset called GendEL.


<details>
  <summary>Details</summary>
Motivation: Concern over machine translation systems reinforcing gender stereotypes due to increased demand for inclusive language.

Method: Investigating gender bias in two commercial MT systems and exploring GPT-4o's potential as a bias mitigation tool with a manually crafted bilingual dataset.

Result: Both MT systems show persistent gender bias, performing well only when gender is explicitly defined; GPT-4o shows promise but residual biases remain.

Conclusion: Further research is needed to improve gender inclusivity in machine translation systems.

Abstract: As the demand for inclusive language increases, concern has grown over the
susceptibility of machine translation (MT) systems to reinforce gender
stereotypes. This study investigates gender bias in two commercial MT systems,
Google Translate and DeepL, focusing on the understudied English-to-Greek
language pair. We address three aspects of gender bias: i) male bias, ii)
occupational stereotyping, and iii) errors in anti-stereotypical translations.
Additionally, we explore the potential of prompted GPT-4o as a bias mitigation
tool that provides both gender-explicit and gender-neutral alternatives when
necessary. To achieve this, we introduce GendEL, a manually crafted bilingual
dataset of 240 gender-ambiguous and unambiguous sentences that feature
stereotypical occupational nouns and adjectives. We find persistent gender bias
in translations by both MT systems; while they perform well in cases where
gender is explicitly defined, with DeepL outperforming both Google Translate
and GPT-4o in feminine gender-unambiguous sentences, they are far from
producing gender-inclusive or neutral translations when the gender is
unspecified. GPT-4o shows promise, generating appropriate gendered and neutral
alternatives for most ambiguous cases, though residual biases remain evident.

</details>


### [35] [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)
*Stefan Krsteski,Matea Tashkovska,Borjan Sazdov,Hristijan Gjoreski,Branislav Gerazov*

Main category: cs.CL

TL;DR: Create resources to support Macedonian language in large language models (LLMs), including a large Macedonian corpus, an instruction dataset, and an evaluation suite. Train a state-of-the-art 8B-parameter model, domestic-yak, which outperforms other models in the same size range and is preferred by native speakers.


<details>
  <summary>Details</summary>
Motivation: Support research advancements and adoption of LLMs for low-resource languages like Macedonian.

Method: Collect a large Macedonian corpus, build an instruction dataset for conversational applications, construct an evaluation suite, and train a domestic-yak model.

Result: The domestic-yak model outperforms all existing models in the 8B parameter range across all benchmarks and achieves performance comparable to models up to 10x larger. It is also preferred by native speakers for its grammatical correctness and cultural appropriateness.

Conclusion: These resources set a foundation for advancing LLMs in underrepresented languages and are openly released.

Abstract: The increase in technological adoption worldwide comes with demands for novel
tools to be used by the general population. Large Language Models (LLMs)
provide a great opportunity in this respect, but their capabilities remain
limited for low-resource languages, restricting applications in countries where
such languages are spoken. We create several resources to facilitate the
adoption of LLMs and to support research advancements for Macedonian. We
collect the largest Macedonian corpus to date, consisting of 40GB of textual
data and totaling 3.5B words. To support conversational applications, we
collect a 106k-instance instruction dataset, carefully built to be culturally
grounded. For evaluation, we construct a Macedonian evaluation suite covering
seven benchmarks. Finally, we train domestic-yak, a state-of-the-art
8B-parameter model, on our curated datasets and evaluate it against eight
baseline models using the newly constructed benchmark suite. Our model
outperforms all existing models in the 8B parameter range across all
benchmarks, and achieves performance comparable to models up to 10x larger.
Furthermore, a qualitative analysis with native speakers reveals that our model
is preferred over larger counterparts, receiving higher ratings for grammatical
correctness and cultural appropriateness. All datasets, code, and model weights
are openly released, setting a foundation for advancing LLMs in similarly
underrepresented languages. These resources are publicly available at
github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained
model weights and data.

</details>


### [36] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Main category: cs.CL

TL;DR: 综述了知识图谱与大语言模型的协同作用及其应用，分类现有方法并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 增强事实基础和推理能力，通过结构化知识集成获得互惠互利。

Method: 对知识图谱和大语言模型之间的协同作用进行了系统的考察，将现有方法分类为两类：增强型知识图谱的大语言模型和增强型大语言模型的知识图谱。

Result: 识别出关键差距，并强调了结构化知识集成的相互益处；特别关注可扩展性、计算效率和数据质量。

Conclusion: 提出未来研究方向，包括神经符号整合、动态知识图谱更新、数据可靠性及伦理考量，推动能够处理更复杂的现实世界知识任务的智能系统的发展。

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [37] [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)
*Stefan Arnold*

Main category: cs.CL

TL;DR: Investigate the role of Intrinsic Dimension in modulating memorization in Language Models.


<details>
  <summary>Details</summary>
Motivation: Concerns about privacy leakage and intellectual property disclosure due to Language Models unintentionally emitting memorized parts of their data during generation.

Method: Investigate the role of Intrinsic Dimension as a geometric proxy for structural complexity of a sequence in latent space.

Result: High-ID sequences are less likely to be memorized than low-ID sequences, especially in overparameterized models and under sparse exposure.

Conclusion: The interaction between scale, exposure, and complexity shapes memorization.

Abstract: Language Models (LMs) are prone to memorizing parts of their data during
training and unintentionally emitting them at generation time, raising concerns
about privacy leakage and disclosure of intellectual property. While previous
research has identified properties such as context length, parameter size, and
duplication frequency, as key drivers of unintended memorization, little is
known about how the latent structure modulates this rate of memorization. We
investigate the role of Intrinsic Dimension (ID), a geometric proxy for the
structural complexity of a sequence in latent space, in modulating
memorization. Our findings suggest that ID acts as a suppressive signal for
memorization: compared to low-ID sequences, high-ID sequences are less likely
to be memorized, particularly in overparameterized models and under sparse
exposure. These findings highlight the interaction between scale, exposure, and
complexity in shaping memorization.

</details>


### [38] [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
*Nicolas Audinet de Pieuchon,Adel Daoud,Connor T. Jerzak,Moa Johansson,Richard Johansson*

Main category: cs.CL

TL;DR: This study examines debiasing methods (DSL and PPI) for large language model text annotations, showing that DSL generally performs better but there's a trade-off in performance consistency across datasets.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistency of large language models (LLMs) in text annotation and the potential bias in downstream estimates, debiasing methods like Design-based Supervised Learning (DSL) and Prediction-Powered Inference (PPI) have been developed.

Method: Studying the performance scaling of debiasing methods with the number of expert annotations and comparing DSL and PPI across various tasks.

Result: DSL often outperforms PPI in terms of bias reduction and empirical efficiency, but its performance varies across datasets. There is a bias-variance tradeoff among debiasing methods, requiring further research on efficiency metrics in finite samples.

Conclusion: More research is needed on metrics to quantify the efficiency of debiasing methods in finite samples.

Abstract: Large language models (LLMs) offer an inexpensive yet powerful way to
annotate text, but are often inconsistent when compared with experts. These
errors can bias downstream estimates of population parameters such as
regression coefficients and causal effects. To mitigate this bias, researchers
have developed debiasing methods such as Design-based Supervised Learning (DSL)
and Prediction-Powered Inference (PPI), which promise valid estimation by
combining LLM annotations with a limited number of expensive expert
annotations. Although these methods produce consistent estimates under
theoretical assumptions, it is unknown how they compare in finite samples of
sizes encountered in applied research. We make two contributions: First, we
study how each method's performance scales with the number of expert
annotations, highlighting regimes where LLM bias or limited expert labels
significantly affect results. Second, we compare DSL and PPI across a range of
tasks, finding that although both achieve low bias with large datasets, DSL
often outperforms PPI on bias reduction and empirical efficiency, but its
performance is less consistent across datasets. Our findings indicate that
there is a bias-variance tradeoff at the level of debiasing methods, calling
for more research on developing metrics for quantifying their efficiency in
finite samples.

</details>


### [39] [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)
*Anna Stein,Kevin Tang*

Main category: cs.CL

TL;DR: This study compared probabilistic predictors and Naive Discriminative Learning (NDL) predictors in modeling acoustic word duration, showing that the N-gram model outperformed both NDL models.


<details>
  <summary>Details</summary>
Motivation: To compare probabilistic predictors based on information theory with NDL predictors in modeling acoustic word duration.

Method: Examined three models using the Buckeye corpus: one with NDL-derived predictors using information-theoretic formulas, one with traditional NDL predictors, and one with N-gram probabilistic predictors.

Result: The N-gram model outperformed both NDL models, and incorporating information-theoretic formulas into NDL improved model performance over the traditional model.

Conclusion: In modeling acoustic reduction, it's important to combine information-theoretic metrics of predictability and information derived from discriminative learning.

Abstract: This study compares probabilistic predictors based on information theory with
Naive Discriminative Learning (NDL) predictors in modeling acoustic word
duration, focusing on probabilistic reduction. We examine three models using
the Buckeye corpus: one with NDL-derived predictors using information-theoretic
formulas, one with traditional NDL predictors, and one with N-gram
probabilistic predictors. Results show that the N-gram model outperforms both
NDL models, challenging the assumption that NDL is more effective due to its
cognitive motivation. However, incorporating information-theoretic formulas
into NDL improves model performance over the traditional model. This research
highlights a) the need to incorporate not only frequency and contextual
predictability but also average contextual predictability, and b) the
importance of combining information-theoretic metrics of predictability and
information derived from discriminative learning in modeling acoustic
reduction.

</details>


### [40] [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)
*Harry Walsh,Maksym Ivashechkin,Richard Bowden*

Main category: cs.CL

TL;DR: We propose using advancements in sign language production to improve sign language translation models by generating variations in signer appearance and skeletal motion.


<details>
  <summary>Details</summary>
Motivation: The challenge of limited data availability for sign languages due to cost, scarcity, and privacy issues.

Method: Utilizing three techniques: skeleton-based production, sign stitching, and generative models (SignGAN and SignSplat).

Result: Enhanced sign language translation model performance by up to 19%.

Conclusion: Our methods effectively augment datasets and improve translation models, enabling more robust systems in resource-limited settings.

Abstract: Machine learning models fundamentally rely on large quantities of
high-quality data. Collecting the necessary data for these models can be
challenging due to cost, scarcity, and privacy restrictions. Signed languages
are visual languages used by the deaf community and are considered low-resource
languages. Sign language datasets are often orders of magnitude smaller than
their spoken language counterparts. Sign Language Production is the task of
generating sign language videos from spoken language sentences, while Sign
Language Translation is the reverse translation task. Here, we propose
leveraging recent advancements in Sign Language Production to augment existing
sign language datasets and enhance the performance of Sign Language Translation
models. For this, we utilize three techniques: a skeleton-based approach to
production, sign stitching, and two photo-realistic generative models, SignGAN
and SignSplat. We evaluate the effectiveness of these techniques in enhancing
the performance of Sign Language Translation models by generating variation in
the signer's appearance and the motion of the skeletal data. Our results
demonstrate that the proposed methods can effectively augment existing datasets
and enhance the performance of Sign Language Translation models by up to 19%,
paving the way for more robust and accurate Sign Language Translation systems,
even in resource-constrained environments.

</details>


### [41] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
*Tianjun Yao,Haoxuan Li,Zhiqiang Shen,Pan Li,Tongliang Liu,Kun Zhang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


### [42] [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/abs/2506.09657)
*Nikolas Evkarpidi,Elena Tutubalina*

Main category: cs.CL

TL;DR: This paper presents a system for QA over tabular data, achieving 80% accuracy and a top-13 ranking in the SemEval 2025 Task 8.


<details>
  <summary>Details</summary>
Motivation: To develop a robust system for question answering over tabular data that improves open-source model accuracy and performs comparably to proprietary LLMs.

Method: The system includes text-to-SQL/text-to-code generation, self-correction mechanism, RAG, and E2E modules, orchestrated by a large language model.

Result: Achieved 80% accuracy and ranked top-13 among 38 teams in the competition.

Conclusion: Our approach shows significant improvement in open-source QA models and comparable performance to proprietary LLMs.

Abstract: This paper presents a system developed for SemEval 2025 Task 8: Question
Answering (QA) over tabular data. Our approach integrates several key
components: text-to-SQL and text-to-code generation modules, a self-correction
mechanism, and a retrieval-augmented generation (RAG). Additionally, it
includes an end-to-end (E2E) module, all orchestrated by a large language model
(LLM). Through ablation studies, we analyzed the effects of different parts of
our pipeline and identified the challenges that are still present in this
field. During the evaluation phase of the competition, our solution achieved an
accuracy of 80%, resulting in a top-13 ranking among the 38 participating
teams. Our pipeline demonstrates a significant improvement in accuracy for
open-source models and achieves a performance comparable to proprietary LLMs in
QA tasks over tables. The code is available at GitHub repository.

</details>


### [43] [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)
*Lihu Chen,Gaël Varoquaux*

Main category: cs.CL

TL;DR: This work proposes a method named Internal Confidence to detect knowledge boundaries at query level without token generation, demonstrating its effectiveness in factual QA and mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To enable Large Language Models to recognize the boundary of their knowledge for adaptive inference and efficient AI development.

Method: Introducing a novel training-free method called Internal Confidence that uses self-evaluations across layers and tokens to detect knowledge boundaries.

Result: Outperforms several baselines in factual QA and mathematical reasoning tasks and shows potential for efficient RAG and model cascading.

Conclusion: The proposed Internal Confidence method effectively detects knowledge boundaries and has applications in improving efficiency and performance of RAG and model cascading.

Abstract: It is important for Large Language Models to be aware of the boundary of
their knowledge, the mechanism of identifying known and unknown queries. This
type of awareness can help models perform adaptive inference, such as invoking
RAG, engaging in slow and deep thinking, or adopting the abstention mechanism,
which is beneficial to the development of efficient and trustworthy AI. In this
work, we propose a method to detect knowledge boundaries via Query-Level
Uncertainty, which aims to determine if the model is able to address a given
query without generating any tokens. To this end, we introduce a novel and
training-free method called \emph{Internal Confidence}, which leverages
self-evaluations across layers and tokens. Empirical results on both factual QA
and mathematical reasoning tasks demonstrate that our internal confidence can
outperform several baselines. Furthermore, we showcase that our proposed method
can be used for efficient RAG and model cascading, which is able to reduce
inference costs while maintaining performance.

</details>


### [44] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
*Hao Xiong,Chuanyuan Tan,Wenliang Chen*

Main category: cs.CL

TL;DR: This paper addresses two issues in Unstructured Knowledge Editing (UKE): lack of locality evaluation and abnormal failure of fine-tuning (FT) based methods. The authors construct two datasets and identify four factors affecting FT-based methods' performance, proposing an optimal training recipe. Experimental results show that the FT-UKE method outperforms existing SOTA methods.


<details>
  <summary>Details</summary>
Motivation: To improve the evaluation and performance of fine-tuning methods in Unstructured Knowledge Editing (UKE) tasks.

Method: Constructed two datasets, identified four factors affecting FT-based methods, and proposed an optimal training recipe for UKE tasks.

Result: The proposed FT-UKE method outperforms existing SOTA methods, especially in batch editing scenarios where its advantage increases with batch size.

Conclusion: Fine-tuning-based methods can achieve superior performance in UKE tasks when trained optimally, addressing the issues of locality evaluation and abnormal failures.

Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant
knowledge of large language models (LLMs). It focuses on unstructured inputs,
such as long or free-form texts, which are common forms of real-world
knowledge. Although previous studies have proposed effective methods and tested
them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)
Abnormal failure of fine-tuning (FT) based methods for UKE. To address these
issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by
extending two existing UKE datasets with locality test data from the
unstructured and structured views. This enables a systematic evaluation of the
Locality of post-edited models. Furthermore, we identify four factors that may
affect the performance of FT-based methods. Based on these factors, we conduct
experiments to determine how the well-performing FT-based methods should be
trained for the UKE task, providing a training recipe for future research. Our
experimental results indicate that the FT-based method with the optimal setting
(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art
(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,
with its advantage over SOTA methods increasing as the batch size grows,
expanding the average metric lead from +6.78% to +10.80%

</details>


### [45] [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/abs/2506.09684)
*Haoyi Song,Ruihan Ji,Naichen Shi,Fan Lai,Raed Al Kontar*

Main category: cs.CL

TL;DR: This paper provides a theoretical justification for the role of perturbations in UQ for LLMs and proposes a fully probabilistic framework based on an inverse model, which defines a new uncertainty measure, Inv-Entropy. The framework is flexible and supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics.


<details>
  <summary>Details</summary>
Motivation: Existing UQ methods are often heuristic and lack a probabilistic foundation. Reliable deployment of large language models (LLMs) requires effective uncertainty quantification (UQ).

Method: A fully probabilistic framework based on an inverse model is proposed to quantify uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. GAAP, a perturbation algorithm based on genetic algorithms, is also proposed to enhance the diversity of sampled inputs. A new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), is introduced to directly assess uncertainty.

Result: Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods.

Conclusion: Inv-Entropy outperforms existing semantic UQ methods.

Abstract: Large language models (LLMs) have transformed natural language processing,
but their reliable deployment requires effective uncertainty quantification
(UQ). Existing UQ methods are often heuristic and lack a probabilistic
foundation. This paper begins by providing a theoretical justification for the
role of perturbations in UQ for LLMs. We then introduce a dual random walk
perspective, modeling input-output pairs as two Markov chains with transition
probabilities defined by semantic similarity. Building on this, we propose a
fully probabilistic framework based on an inverse model, which quantifies
uncertainty by evaluating the diversity of the input space conditioned on a
given output through systematic perturbations. Within this framework, we define
a new uncertainty measure, Inv-Entropy. A key strength of our framework is its
flexibility: it supports various definitions of uncertainty measures,
embeddings, perturbation strategies, and similarity metrics. We also propose
GAAP, a perturbation algorithm based on genetic algorithms, which enhances the
diversity of sampled inputs. In addition, we introduce a new evaluation metric,
Temperature Sensitivity of Uncertainty (TSU), which directly assesses
uncertainty without relying on correctness as a proxy. Extensive experiments
demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code
to reproduce the results can be found at
https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

</details>


### [46] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: ComfyUI-R1 is the first large reasoning model for automated workflow generation in AI art creation.


<details>
  <summary>Details</summary>
Motivation: The steep learning curve in creating effective workflows on platforms like ComfyUI motivated the development of a reasoning model to assist users.

Method: A two-stage framework was used: CoT fine-tuning and reinforcement learning with a hybrid reward system.

Result: The 7B-parameter model achieved a 97% format validity rate and surpassed previous state-of-the-art methods.

Conclusion: ComfyUI-R1 demonstrates the power of long CoT reasoning in synthesizing complex workflows for AI art creation.

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [47] [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/abs/2506.09796)
*Andreas Säuberli,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: 本研究评估了18个指令调整后的大型语言模型对教育评估测试项目的响应行为的拟人化程度，发现虽然LLMs在某些情况下表现得像人类，但在零样本设置下不应将其用于教育评估的试点研究。


<details>
  <summary>Details</summary>
Motivation: 了解考生如何回答教育评估中的问题对于测试开发、评估项目质量和提高测试有效性至关重要。然而，这个过程通常需要大量的人类参与者进行试点研究。如果大型语言模型（LLMs）对测试项目的响应行为像人类一样，则可以使用它们作为试点参与者以加速测试开发。

Method: 本研究使用了来自两个公开数据集的多选题测试项目，涉及三个科目：阅读、美国历史和经济学。研究方法基于教育评估中常用的经典测试理论和项目反应理论的两个理论框架。

Result: 虽然更大的模型表现出过度自信，但当使用温度缩放进行校准时，它们的响应分布可以更像人类。此外，在阅读理解问题上，LLMs的表现比其他学科与人类的相关性更好。

Conclusion: 虽然更大的模型表现出过度自信，但当使用温度缩放进行校准时，它们的响应分布可以更像人类。此外，我们发现LLMs在阅读理解问题上的表现比其他学科与人类的相关性更好。然而，总体相关性并不强，这表明LLMs不应在零样本设置下用于教育评估的试点研究。

Abstract: Knowing how test takers answer items in educational assessments is essential
for test development, to evaluate item quality, and to improve test validity.
However, this process usually requires extensive pilot studies with human
participants. If large language models (LLMs) exhibit human-like response
behavior to test items, this could open up the possibility of using them as
pilot participants to accelerate test development. In this paper, we evaluate
the human-likeness or psychometric plausibility of responses from 18
instruction-tuned LLMs with two publicly available datasets of multiple-choice
test items across three subjects: reading, U.S. history, and economics. Our
methodology builds on two theoretical frameworks from psychometrics which are
commonly used in educational assessment, classical test theory and item
response theory. The results show that while larger models are excessively
confident, their response distributions can be more human-like when calibrated
with temperature scaling. In addition, we find that LLMs tend to correlate
better with humans in reading comprehension items compared to other subjects.
However, the correlations are not very strong overall, indicating that LLMs
should not be used for piloting educational assessments in a zero-shot setting.

</details>


### [48] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: CoRT is a post-training framework that teaches Large Reasoning Models (LRMs) to use Code Interpreter (CI) more effectively and efficiently. It addresses inefficiencies in handling complex math problems by creating code-integrated reasoning data using Hint-Engineering.


<details>
  <summary>Details</summary>
Motivation: Improve efficiency and accuracy of LRMs in dealing with complex mathematical operations.

Method: Post-training framework called CoRT, including hint-engineering, supervised fine-tuning, rejection fine-tuning and reinforcement learning.

Result: Achieved 4% and 8% absolute improvements on two DeepSeek-R1-Distill-Qwen models across five challenging mathematical reasoning datasets. Used fewer tokens compared to natural language models.

Conclusion: CoRT can enhance the performance of LRMs in mathematical reasoning tasks.

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [49] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Kourosh Nadi,Huu Nguyen,Kristian Kersting,Sören Auer*

Main category: cs.CL

TL;DR: Introduces EmoNet-Voice, a new resource for speech emotion detection, including a large-scale pre-training dataset and a novel benchmark dataset with human expert annotations. Demonstrates its effectiveness through empathic insight voice models.


<details>
  <summary>Details</summary>
Motivation: The need for robust benchmarks to evaluate emotional understanding capabilities of AI systems due to limitations in current SER datasets.

Method: Curated synthetic audio snippets using state-of-the-art voice generation to simulate actors portraying scenes designed to evoke specific emotions. Conducted rigorous validation by psychology experts.

Result: EmoNet-Voice includes EmoNet-Voice Big, a large-scale pre-training dataset, and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. The dataset evaluates SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities.

Conclusion: Introduces Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts.

Abstract: The advancement of text-to-speech and audio generation models necessitates
robust benchmarks for evaluating the emotional understanding capabilities of AI
systems. Current speech emotion recognition (SER) datasets often exhibit
limitations in emotional granularity, privacy concerns, or reliance on acted
portrayals. This paper introduces EmoNet-Voice, a new resource for speech
emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training
dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,
and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human
expert annotations. EmoNet-Voice is designed to evaluate SER models on a
fine-grained spectrum of 40 emotion categories with different levels of
intensities. Leveraging state-of-the-art voice generation, we curated synthetic
audio snippets simulating actors portraying scenes designed to evoke specific
emotions. Crucially, we conducted rigorous validation by psychology experts who
assigned perceived intensity labels. This synthetic, privacy-preserving
approach allows for the inclusion of sensitive emotional states often absent in
existing datasets. Lastly, we introduce Empathic Insight Voice models that set
a new standard in speech emotion recognition with high agreement with human
experts. Our evaluations across the current model landscape exhibit valuable
findings, such as high-arousal emotions like anger being much easier to detect
than low-arousal states like concentration.

</details>


### [50] [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/abs/2506.09833)
*Omar Sherif,Ali Hamdi*

Main category: cs.CL

TL;DR: Introduces Error-Guided Pose Augmentation (EGPA) to generate synthetic skeleton data by simulating biomechanical errors for better automated movement quality assessment.


<details>
  <summary>Details</summary>
Motivation: Existing systems face challenges like data imbalance and difficulty detecting subtle movement errors in home-based rehabilitation settings.

Method: EGPA generates synthetic data targeting biomechanical errors and combines with an attention-based graph convolutional network.

Result: Performance improved with a reduction in mean absolute error up to 27.6% and an increase in error classification accuracy of 45.8%.

Conclusion: EGPA enhances both accuracy and interpretability, offering a promising approach for automated movement quality assessment in clinical and home-based settings.

Abstract: Effective rehabilitation assessment is essential for monitoring patient
progress, particularly in home-based settings. Existing systems often face
challenges such as data imbalance and difficulty detecting subtle movement
errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method
that generates synthetic skeleton data by simulating clinically relevant
movement mistakes. Unlike standard augmentation techniques, EGPA targets
biomechanical errors observed in rehabilitation. Combined with an
attention-based graph convolutional network, EGPA improves performance across
multiple evaluation metrics. Experiments demonstrate reductions in mean
absolute error of up to 27.6 percent and gains in error classification accuracy
of 45.8 percent. Attention visualizations show that the model learns to focus
on clinically significant joints and movement phases, enhancing both accuracy
and interpretability. EGPA offers a promising approach for improving automated
movement quality assessment in both clinical and home-based rehabilitation
contexts.

</details>


### [51] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
*Tomas Peterka,Matyas Bohacek*

Main category: cs.CL

TL;DR: This paper introduces a News Media Provenance Dataset with provenance-tagged images for news articles. Two tasks, location of origin relevance (LOR) and date and time of origin relevance (DTOR), are formulated on this dataset. Baseline results on six large language models (LLMs) show promising zero-shot performance on LOR but hindered performance on DTOR.


<details>
  <summary>Details</summary>
Motivation: To address out-of-context and misattributed imagery, which is a leading form of media manipulation in today's misinformation and disinformation landscape.

Method: Introducing the News Media Provenance Dataset and formulating two tasks, LOR and DTOR, on it. Presenting baseline results on six large language models (LLMs).

Result: Promising zero-shot performance on LOR task but hindered performance on DTOR task.

Conclusion: The work identifies room for specialized architectures and future work to improve DTOR performance.

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>


### [52] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
*Xiangning Yu,Zhuohan Wang,Linyi Yang,Haoxuan Li,Anjie Liu,Xiao Xue,Jun Wang,Mengyue Yang*

Main category: cs.CL

TL;DR: 提出了一种因果框架来解决链式思维提示在大型语言模型复杂推理能力中的充分性和必要性问题，并通过实验证明了其在提高推理效率和减少令牌使用方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 使大型语言模型具备复杂推理能力

Method: 提出因果框架，通过充分性和必要性的双重视角来表征链式思维推理

Result: 在各种数学和常识推理基准上进行了广泛的实验，证明了推理效率的提高和令牌使用的减少

Conclusion: 提供了一个改进大型语言模型推理性能和成本效益的方向

Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the pruning of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.

</details>


### [53] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
*Rodion Oblovatny,Alexandra Bazarova,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出了一种新的方法来检测大型语言模型中的幻觉，该方法优于现有技术并表现出最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为了检测大型语言模型中的幻觉，因为幻觉可能会影响模型的性能和可靠性。

Method: 通过分析提示和响应隐藏状态分布之间的概率发散来检测大型语言模型中的幻觉的新方法。使用分布距离作为原则性的幻觉评分，并采用深度可学习内核自动适应捕捉分布之间的细微几何差异。

Result: 提出的模型内在检测方法优于现有的基线，并在几个基准上表现出最先进的性能。

Conclusion: 提出的方法在几个基准上表现出最先进的性能，并且即使没有内核训练，该方法仍然是竞争性的，提供了一种鲁棒且可扩展的幻觉检测解决方案。

Abstract: We present a novel approach for detecting hallucinations in large language
models (LLMs) by analyzing the probabilistic divergence between prompt and
response hidden-state distributions. Counterintuitively, we find that
hallucinated responses exhibit smaller deviations from their prompts compared
to grounded responses, suggesting that hallucinations often arise from
superficial rephrasing rather than substantive reasoning. Leveraging this
insight, we propose a model-intrinsic detection method that uses distributional
distances as principled hallucination scores, eliminating the need for external
knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable
kernels that automatically adapt to capture nuanced geometric differences
between distributions. Our approach outperforms existing baselines,
demonstrating state-of-the-art performance on several benchmarks. The method
remains competitive even without kernel training, offering a robust, scalable
solution for hallucination detection.

</details>


### [54] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
*Yuxin Chen,Yiran Zhao,Yang Zhang,An Zhang,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Tat-Seng Chua,Michael Qizhe Shieh,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 研究发现大语言模型通过发展语言无关的参数空间支持抽象思维，并提出了相应的训练策略。


<details>
  <summary>Details</summary>
Motivation: 研究发现大语言模型（LLMs）在处理不同语言时的表现挑战了之前认为LLMs可能“以英语思维”的假设。本研究旨在揭示LLMs是否真的发展出了一种与语言无关的参数空间，并探索这种能力如何支持抽象思维的出现。

Method: 识别和分类与特定语言相关的神经元，分析它们在不同语言处理中的激活情况，并观察其随时间变化的趋势。同时提出针对不同发展阶段LLMs的语言无关训练策略。

Result: LLMs确实发展出了一个核心的语言无关参数空间，这个空间由少量关键参数组成，这些参数的失活会导致所有语言性能显著下降。随着模型的发展，共享神经元的比例和功能重要性增加，而专属神经元的影响逐渐减弱。

Conclusion: 这项研究表明，LLMs能够超越个别语言系统，展现出与语言无关的抽象思维能力。提出的神经元特定训练策略可以进一步提升LLMs的多语言通用性能。

Abstract: As large language models (LLMs) continue to advance, their capacity to
function effectively across a diverse range of languages has shown marked
improvement. Preliminary studies observe that the hidden activations of LLMs
often resemble English, even when responding to non-English prompts. This has
led to the widespread assumption that LLMs may "think" in English. However,
more recent results showing strong multilingual performance, even surpassing
English performance on specific tasks in other languages, challenge this view.
In this work, we find that LLMs progressively develop a core language-agnostic
parameter space-a remarkably small subset of parameters whose deactivation
results in significant performance degradation across all languages. This
compact yet critical set of parameters underlies the model's ability to
generalize beyond individual languages, supporting the emergence of abstract
thought that is not tied to any specific linguistic system. Specifically, we
identify language-related neurons-those are consistently activated during the
processing of particular languages, and categorize them as either shared
(active across multiple languages) or exclusive (specific to one). As LLMs
undergo continued development over time, we observe a marked increase in both
the proportion and functional importance of shared neurons, while exclusive
neurons progressively diminish in influence. These shared neurons constitute
the backbone of the core language-agnostic parameter space, supporting the
emergence of abstract thought. Motivated by these insights, we propose
neuron-specific training strategies tailored to LLMs' language-agnostic levels
at different development stages. Experiments across diverse LLM families
support our approach.

</details>


### [55] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
*Zheng Zhao,Clara Vania,Subhradeep Kayal,Naila Khan,Shay B. Cohen,Emine Yilmaz*

Main category: cs.CL

TL;DR: Introduce PersonaLens, a benchmark for evaluating personalization in task-oriented AI assistants.


<details>
  <summary>Details</summary>
Motivation: Systematically evaluating personalization in conversational AI assistants remains challenging due to lack of comprehensive benchmarks.

Method: Developed two specialized LLM-based agents: a user agent and a judge agent.

Result: Found significant variability in personalization capabilities of current LLM assistants.

Conclusion: PersonaLens provides insights for advancing conversational AI systems.

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


### [56] [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/abs/2506.09917)
*Wendi Zhou,Ameer Saadat-Yazd,Nadin Kokciyan*

Main category: cs.CL

TL;DR: This paper presents ASESUM, a novel summarization system that captures predominant opinions from an aspect perspective with supporting evidence and adapts to varying domains without relying on pre-defined aspects.


<details>
  <summary>Details</summary>
Motivation: Automated opinion summarization is needed due to the impracticality for customers to manually conclude prominent opinions from a vast number of reviews.

Method: ASESUM extracts aspect-centric arguments and measures their salience and validity to summarize viewpoints relevant to critical aspects of a product.

Result: Experiments on a real-world dataset demonstrate the superiority of ASESUM in capturing diverse perspectives compared to new and existing methods.

Conclusion: The proposed method effectively addresses the challenge of automatically producing grounded aspect-centric summaries.

Abstract: Reviews are valuable resources for customers making purchase decisions in
online shopping. However, it is impractical for customers to go over the vast
number of reviews and manually conclude the prominent opinions, which prompts
the need for automated opinion summarization systems. Previous approaches,
either extractive or abstractive, face challenges in automatically producing
grounded aspect-centric summaries. In this paper, we propose a novel
summarization system that not only captures predominant opinions from an aspect
perspective with supporting evidence, but also adapts to varying domains
without relying on a pre-defined set of aspects. Our proposed framework,
ASESUM, summarizes viewpoints relevant to the critical aspects of a product by
extracting aspect-centric arguments and measuring their salience and validity.
We conduct experiments on a real-world dataset to demonstrate the superiority
of our approach in capturing diverse perspectives of the original reviews
compared to new and existing methods.

</details>


### [57] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出了一种结合规则代码验证和大型推理模型验证的方法VerIF，用于指令跟随的强化学习，并构建了高质量的指令跟随数据集VerInstruct。通过使用VerIF进行强化学习训练，两个模型在多个基准测试中取得了显著改进，达到了同类模型中的最先进性能，并且具有良好的泛化能力。此外，研究还表明，这些模型的通用能力没有受到影响，建议可以将VerIF整合到现有的强化学习方法中以提高整体模型性能。研究数据、代码和模型已公开。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在增强大型语言模型方面已成为关键技术，但指令跟随的强化学习最佳实践尚未被充分探索。

Method: 提出了一种名为VerIF的新方法，该方法结合了规则代码验证和大型推理模型验证。为了支持这种方法，构建了一个包含约22,000个实例的数据集VerInstruct，其中包含与验证信号相关的实例。

Result: 使用VerIF进行强化学习训练后，两个模型在几个代表性的指令跟随基准测试中表现出了显著的改进，达到了同类模型中的最先进水平，并且在未见过的约束条件下表现良好。此外，模型的通用能力没有受到影响。

Conclusion: 这项研究表明，VerIF可以集成到现有的强化学习方案中以提高模型的整体性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key
technique for enhancing large language models (LLMs), with verification
engineering playing a central role. However, best practices for RL in
instruction following remain underexplored. In this work, we explore the
verification challenge in RL for instruction following and propose VerIF, a
verification method that combines rule-based code verification with LLM-based
verification from a large reasoning model (e.g., QwQ-32B). To support this
approach, we construct a high-quality instruction-following dataset,
VerInstruct, containing approximately 22,000 instances with associated
verification signals. We apply RL training with VerIF to two models, achieving
significant improvements across several representative instruction-following
benchmarks. The trained models reach state-of-the-art performance among models
of comparable size and generalize well to unseen constraints. We further
observe that their general capabilities remain unaffected, suggesting that RL
with VerIF can be integrated into existing RL recipes to enhance overall model
performance. We have released our datasets, codes, and models to facilitate
future research at https://github.com/THU-KEG/VerIF.

</details>


### [58] [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)
*Wuwei Zhang,Fangcong Yin,Howard Yen,Danqi Chen,Xi Ye*

Main category: cs.CL

TL;DR: Introduce QRHEAD and QR- RETRIEVER for enhancing long context retrieval.


<details>
  <summary>Details</summary>
Motivation: Improve retrieval capabilities in long-context language models.

Method: Aggregate attention scores with respect to input query and select relevant parts with highest retrieval scores.

Result: Over 10% performance gain on multi-hop reasoning tasks and strong zero-shot performance on BEIR benchmark.

Conclusion: Contributes a general-purpose retriever and provides interpretability insights into long-context LM capabilities.

Abstract: Recent work has identified retrieval heads (Wu et al., 2025b), a subset of
attention heads responsible for retrieving salient information in long-context
language models (LMs), as measured by their copy-paste behavior in
Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused
Retrieval Head), an improved set of attention heads that enhance retrieval from
long context. We identify QRHEAD by aggregating attention scores with respect
to the input query, using a handful of examples from real-world tasks (e.g.,
long-context QA). We further introduce QR- RETRIEVER, an efficient and
effective retriever that uses the accumulated attention mass of QRHEAD as
retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting
the most relevant parts with the highest retrieval scores. On multi-hop
reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains
over full context and outperforms strong dense retrievers. We also evaluate
QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves
strong zero-shot performance, outperforming other LLM-based re-rankers such as
RankGPT. Further analysis shows that both the querycontext attention scoring
and task selection are crucial for identifying QRHEAD with strong downstream
utility. Overall, our work contributes a general-purpose retriever and offers
interpretability insights into the long-context capabilities of LMs.

</details>


### [59] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Deqing Fu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Resa is a family of 1.5B reasoning models trained through a novel and efficient SAE-Tuning method, which reduces training costs and time significantly while retaining high reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To find a cost-effective way to elicit strong reasoning in language models by leveraging their underlying representations.

Method: Introducing Resa, a family of reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure.

Result: When applied to certain base models, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while drastically reducing training costs and time. It also enables strong reasoning performance for lightly RL-trained models with minimal additional cost.

Conclusion: The reasoning abilities extracted via SAEs are potentially generalizable and modular, and extensive ablations validate these findings.

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [60] [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/abs/2506.09975)
*Hillary Dawkins,Kathleen C. Fraser,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: This paper examines the challenge of identifying AI-generated text on social media, demonstrating that while detection is possible under certain conditions, it becomes significantly harder when attackers use fine-tuned models that aren't publicly available.


<details>
  <summary>Details</summary>
Motivation: To address the potential misuse of AI-generated posts in influencing online opinions and decisions.

Method: Creating a large dataset of AI-generated social media posts using various types of language models and testing different detection methods.

Result: Detection effectiveness drops significantly when dealing with fine-tuned models that aren't released publicly.

Conclusion: Fine-tuning is a common practice for real-world applications, making the detection of AI-generated content more challenging.

Abstract: Detecting AI-generated text is a difficult problem to begin with; detecting
AI-generated text on social media is made even more difficult due to the short
text length and informal, idiosyncratic language of the internet. It is
nonetheless important to tackle this problem, as social media represents a
significant attack vector in online influence campaigns, which may be bolstered
through the use of mass-produced AI-generated posts supporting (or opposing)
particular policies, decisions, or events. We approach this problem with the
mindset and resources of a reasonably sophisticated threat actor, and create a
dataset of 505,159 AI-generated social media posts from a combination of
open-source, closed-source, and fine-tuned LLMs, covering 11 different
controversial topics. We show that while the posts can be detected under
typical research assumptions about knowledge of and access to the generating
models, under the more realistic assumption that an attacker will not release
their fine-tuned model to the public, detectability drops dramatically. This
result is confirmed with a human study. Ablation experiments highlight the
vulnerability of various detection algorithms to fine-tuned LLMs. This result
has implications across all detection domains, since fine-tuning is a generally
applicable and realistic LLM use case.

</details>


### [61] [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/abs/2506.09983)
*Hiroshi Matsuda,Chunpeng Ma,Masayuki Asahara*

Main category: cs.CL

TL;DR: 提出了一种新的逐步指令策略，结合简化版的CoNLL-U格式，在17种语言的Universal Dependencies数据集上实现了最先进的准确性，同时展示了多语言微调可以提高跨语言泛化性能。


<details>
  <summary>Details</summary>
Motivation: 标准提示方法在生成结构有效和准确输出方面存在困难，尤其是在依赖解析任务中。

Method: 提出了一个逐步指令策略，包括通用词性标注，随后预测句法头和依存标签，并使用简化版的CoNLL-U类似输出格式。

Result: 该方法在17种语言的Universal Dependencies数据集上达到了最先进的准确性，且没有产生幻觉或污染。

Conclusion: 显示了LLM基础解析中显式推理步骤的有效性，并提供了一个可扩展、格式一致的括号方法替代方案。

Abstract: Recent advances in large language models (LLMs) have enabled impressive
performance in various tasks. However, standard prompting often struggles to
produce structurally valid and accurate outputs, especially in dependency
parsing. We propose a novel step-by-step instruction strategy, where universal
part-of-speech tagging precedes the prediction of syntactic heads and
dependency labels, and a simplified CoNLL-U like output format, our method
achieves state-of-the-art accuracy on Universal Dependencies datasets across 17
languages without hallucination or contamination. We further show that
multilingual fine-tuning simultaneously improves cross-language generalization
performance. Our results highlight the effectiveness of explicit reasoning
steps in LLM-based parsing and offer a scalable, format-consistent alternative
to bracket-based approaches.

</details>


### [62] [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/abs/2506.09992)
*Amel Muminovic,Amela Kadric Muminovic*

Main category: cs.CL

TL;DR: This study examines how large language models perform in identifying toxic comments in Serbian, Croatian, and Bosnian languages. A dataset of 4,500 comments was created and labeled manually. Models like GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus were evaluated in zero-shot and context-augmented modes. Adding context improved recall and F1 scores but also increased false positives. Gemini showed the best balance with an F1 score of 0.82 and accuracy of 0.82.


<details>
  <summary>Details</summary>
Motivation: Toxic online language causes significant harm, particularly in areas with limited moderation resources. This study aims to assess how large language models handle toxic comments in three under-resourced languages.

Method: A manually labeled dataset of 4,500 YouTube and TikTok comments was created. Four models were tested in zero-shot and context-augmented modes.

Result: Adding context improved recall and F1 scores but also increased false positives. Gemini performed best in context-augmented mode with an F1 score of 0.82 and accuracy of 0.82.

Conclusion: Prompt design improvements and threshold calibration can enhance toxicity detection in low-resource language settings.

Abstract: Online toxic language causes real harm, especially in regions with limited
moderation tools. In this study, we evaluate how large language models handle
toxic comments in Serbian, Croatian, and Bosnian, languages with limited
labeled data. We built and manually labeled a dataset of 4,500 YouTube and
TikTok comments drawn from videos across diverse categories, including music,
politics, sports, modeling, influencer content, discussions of sexism, and
general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude
3 Opus) were tested in two modes: zero-shot and context-augmented. We measured
precision, recall, F1 score, accuracy and false positive rates. Including a
short context snippet raised recall by about 0.12 on average and improved F1
score by up to 0.10, though it sometimes increased false positives. The best
balance came from Gemini in context-augmented mode, reaching an F1 score of
0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the
lowest false alarms. We show how adding minimal context can improve toxic
language detection in low-resource settings and suggest practical strategies
such as improved prompt design and threshold calibration. These results show
that prompt design alone can yield meaningful gains in toxicity detection for
underserved Balkan language communities.

</details>


### [63] [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)
*Yang Li,Qiang Sheng,Yehan Yang,Xueyao Zhang,Juan Cao*

Main category: cs.CL

TL;DR: This paper introduces a novel approach called Streaming Content Monitor (SCM) for real-time harmfulness detection in large language model outputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from high service latency due to full detection or performance degradation due to training-inference gaps in partial detection.

Method: Constructing a dataset named FineHarm and proposing the SCM for token-level training with dual supervision.

Result: The SCM achieves comparable performance to full detection but with only 18% of the tokens examined on average.

Conclusion: The proposed method not only improves real-time harmfulness detection but also enhances safety alignment.

Abstract: Though safety alignment has been applied to most large language models
(LLMs), LLM service providers generally deploy a subsequent moderation as the
external safety guardrail in real-world products. Existing moderators mainly
practice a conventional full detection, which determines the harmfulness based
on the complete LLM output, causing high service latency. Recent works pay more
attention to partial detection where moderators oversee the generation midway
and early stop the output if harmfulness is detected, but they directly apply
moderators trained with the full detection paradigm to incomplete outputs,
introducing a training-inference gap that lowers the performance. In this
paper, we explore how to form a data-and-model solution that natively supports
partial detection. For the data, we construct FineHarm, a dataset consisting of
29K prompt-response pairs with fine-grained annotations to provide reasonable
supervision for token-level training. Then, we propose the streaming content
monitor, which is trained with dual supervision of response- and token-level
labels and can follow the output stream of LLM to make a timely judgment of
harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is
comparable to full detection, by only seeing the first 18% of tokens in
responses on average. Moreover, the SCM can serve as a pseudo-harmfulness
annotator for improving safety alignment and lead to a higher harmlessness
score than DPO.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [64] [You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks](https://arxiv.org/abs/2506.09521)
*Ünal Ege Gaznepoglu,Anna Leschanowsky,Ahmad Aloradi,Prachi Singh,Daniel Tenbrinck,Emanuël A. P. Habets,Nils Peters*

Main category: eess.AS

TL;DR: This study evaluates speaker anonymization systems by adapting a language model as an automatic speaker verification system, revealing issues related to linguistic content similarity.


<details>
  <summary>Details</summary>
Motivation: To assess the privacy benefits of speaker anonymization systems, attacks using automatic speaker verification systems are conducted.

Method: BERT, a language model, is adapted as an ASV system to evaluate the impact of intra-speaker linguistic content similarity in attacker datasets.

Result: The method achieved a mean EER of 35% on the VoicePrivacy datasets, with some speakers having EERs as low as 2% based only on textual content.

Conclusion: The study suggests reworking the VoicePrivacy datasets and challenges the use of global EER for privacy evaluations.

Abstract: Speaker anonymization systems hide the identity of speakers while preserving
other information such as linguistic content and emotions. To evaluate their
privacy benefits, attacks in the form of automatic speaker verification (ASV)
systems are employed. In this study, we assess the impact of intra-speaker
linguistic content similarity in the attacker training and evaluation datasets,
by adapting BERT, a language model, as an ASV system. On the VoicePrivacy
Attacker Challenge datasets, our method achieves a mean equal error rate (EER)
of 35%, with certain speakers attaining EERs as low as 2%, based solely on the
textual content of their utterances. Our explainability study reveals that the
system decisions are linked to semantically similar keywords within utterances,
stemming from how LibriSpeech is curated. Our study suggests reworking the
VoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge
the reliance on global EER for privacy evaluations.

</details>


### [65] [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
*Suhas BN,Andrew M. Sherrill,Jyoti Alaparthi,Dominik Mattioli,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Main category: eess.AS

TL;DR: This paper proposes a method to automatically track therapist fidelity in PE therapy sessions using audio and transcript inputs.


<details>
  <summary>Details</summary>
Motivation: To evaluate therapist fidelity in PE therapy more efficiently than manual review of session recordings.

Method: Fine-tuning a large pre-trained audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process focused 30-second windows of audio-transcript input.

Result: The model achieves a mean absolute error (MAE) of 5.3 seconds across tasks on a dataset of 313 real PE sessions.

Conclusion: This study presents a method for automatically localizing key fidelity elements in PE therapy sessions, achieving a MAE of 5.3 seconds across tasks.

Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic
stress disorder (PTSD), but evaluating therapist fidelity remains
labor-intensive due to the need for manual review of session recordings. We
present a method for the automatic temporal localization of key PE fidelity
elements -- identifying their start and stop times -- directly from session
audio and transcripts. Our approach fine-tunes a large pre-trained
audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process
focused 30-second windows of audio-transcript input. Fidelity labels for three
core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and
post-imaginal processing (P3) -- are generated via LLM-based prompting and
verified by trained raters. The model is trained to predict normalized boundary
offsets using soft supervision guided by task-specific prompts. On a dataset of
313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)
achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further
analyze the effects of window size and LoRA rank, highlighting the importance
of context granularity and model adaptation. This work introduces a scalable
framework for fidelity tracking in PE therapy, with potential to support
clinician training, supervision, and quality assurance.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [66] [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
*Ahmed Adel Attia,Jing Liu,Carl Espy-Wilson*

Main category: cs.SD

TL;DR: 提出一种使用游戏引擎合成课堂噪音的方法，构建了SimClass数据集，用于改善教育领域的AI语音模型。


<details>
  <summary>Details</summary>
Motivation: 解决教育领域AI驱动的语音模型发展受制于大规模课堂语音数据稀缺的问题。

Method: 利用游戏引擎合成课堂噪音，并结合公共儿童语音语料库与YouTube讲座视频生成模拟课堂语音数据集SimClass。

Result: SimClass包含合成的课堂噪音语料库和模拟课堂语音数据集，实验表明其在清洁和嘈杂语音上接近真实课堂语音。

Conclusion: SimClass是一个有价值的资源，可用于开发健壮的语音识别和增强模型。

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Public classroom datasets
remain limited, and the lack of a dedicated classroom noise corpus prevents the
use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise using game engines, a framework that extends to other domains. Using this
methodology, we present SimClass, a dataset that includes both a synthesized
classroom noise corpus and a simulated classroom speech dataset. The speech
data is generated by pairing a public children's speech corpus with YouTube
lecture videos to approximate real classroom interactions in clean conditions.
Our experiments on clean and noisy speech demonstrate that SimClass closely
approximates real classroom speech, making it a valuable resource for
developing robust speech recognition and enhancement models.

</details>


### [67] [OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary](https://arxiv.org/abs/2506.09448)
*Yui Sudo,Yusuke Fujita,Atsushi Kojima,Tomoya Mizumoto,Lianbo Liu*

Main category: cs.SD

TL;DR: Integrating contextual biasing with speech foundation models improves recognition of rare and unseen words.


<details>
  <summary>Details</summary>
Motivation: To address the issue of inaccurate recognition of rare and unseen words in speech foundation models.

Method: Freezing the pre-trained parameters of an existing contextual biasing method and integrating it with Open Whisper-Style Speech Models v3.1.

Result: The proposed method improves the biasing word error rate by 11.6 points and reduces the overall word error rate by 0.9 point with a 7.5% reduction in real-time factor.

Conclusion: The integration of contextual biasing with speech foundation models can effectively improve the recognition of rare and unseen words while preserving the advantages of the original model.

Abstract: Speech foundation models (SFMs), such as Open Whisper-Style Speech Models
(OWSM), are trained on massive datasets to achieve accurate automatic speech
recognition. However, even SFMs struggle to accurately recognize rare and
unseen words. While contextual biasing (CB) is a promising approach to improve
recognition of such words, most CB methods are trained from scratch, resulting
in lower performance than SFMs due to the lack of pre-trained knowledge. This
paper integrates an existing CB method with OWSM v3.1 while freezing its
pre-trained parameters. By leveraging the knowledge embedded in SFMs, the
proposed method enables effective CB while preserving the advantages of SFMs,
even with a small dataset. Experimental results show that the proposed method
improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9
point improvement in the overall WER while reducing the real-time factor by
7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean
set.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [68] [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
*Hetvi Waghela,Jaydip Sen,Sneha Rakshit,Subhasis Dasgupta*

Main category: cs.CR

TL;DR: Propose a new method called Dynamic Contextual Perturbation (DCP) for generating more effective adversarial examples in NLP.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook broader context leading to detectable or semantically inconsistent perturbations.

Method: DCP generates context-aware perturbations iteratively refining them through an adversarial objective function balancing model misclassification and text naturalness.

Result: DCP improves the sophistication and effectiveness of adversarial examples producing more robust challenges to NLP systems.

Conclusion: DCP demonstrates the importance of context in adversarial attacks promoting the development of more resilient NLP models.

Abstract: Adversarial attacks on Natural Language Processing (NLP) models expose
vulnerabilities by introducing subtle perturbations to input text, often
leading to misclassification while maintaining human readability. Existing
methods typically focus on word-level or local text segment alterations,
overlooking the broader context, which results in detectable or semantically
inconsistent perturbations. We propose a novel adversarial text attack scheme
named Dynamic Contextual Perturbation (DCP). DCP dynamically generates
context-aware perturbations across sentences, paragraphs, and documents,
ensuring semantic fidelity and fluency. Leveraging the capabilities of
pre-trained language models, DCP iteratively refines perturbations through an
adversarial objective function that balances the dual objectives of inducing
model misclassification and preserving the naturalness of the text. This
comprehensive approach allows DCP to produce more sophisticated and effective
adversarial examples that better mimic natural language patterns. Our
experimental results, conducted on various NLP models and datasets, demonstrate
the efficacy of DCP in challenging the robustness of state-of-the-art NLP
systems. By integrating dynamic contextual analysis, DCP significantly enhances
the subtlety and impact of adversarial attacks. This study highlights the
critical role of context in adversarial attacks and lays the groundwork for
creating more robust NLP systems capable of withstanding sophisticated
adversarial strategies.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [69] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
*Itay Nakash,George Kour,Koren Lazar,Matan Vetzler,Guy Uziel,Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: Develop a new threat model and a multi-agent red-teaming system called CRAFT to test the robustness of policy-adherent agents against adversarial users. Introduce tau-break benchmark to evaluate agent's resilience and examine defense strategies.


<details>
  <summary>Details</summary>
Motivation: Ensure task-oriented LLM-based agents adhere to policies while maintaining helpful interactions, especially in domains with strict policies.

Method: Propose a novel threat model focusing on adversarial users and develop CRAFT to undermine policy-adherent agents. Introduce tau-break benchmark and evaluate defense strategies.

Result: CRAFT outperforms conventional jailbreak methods. Defense strategies offer some protection but are insufficient.

Conclusion: Stronger research-driven safeguards are needed to protect policy-adherent agents from adversarial attacks.

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: Ming-Omni is a unified multimodal model that can handle images, text, audio, and video, excelling in speech and image generation.


<details>
  <summary>Details</summary>
Motivation: To create a single model that can process multiple types of data without needing separate models or fine-tuning.

Method: Uses dedicated encoders for different modalities and processes them using Ling, an MoE architecture with modality-specific routers.

Result: Supports audio and image generation, allows context-aware chatting, text-to-speech conversion, and versatile image editing.

Conclusion: Ming-Omni is the first open-source model that matches GPT-4o in modality support and has been released to the public.

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [71] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: This paper argues against fully autonomous AI agents due to their reliability and transparency issues, proposing instead LLM-based Human-Agent Systems that enhance human capabilities through collaboration.


<details>
  <summary>Details</summary>
Motivation: Focuses on the problems with fully autonomous AI agents such as reliability, transparency, and understanding human requirements.

Method: Proposes the LLM-based Human-Agent Systems (LLM-HAS) where AI works alongside humans.

Result: Demonstrates through examples in healthcare, finance, and software development that human-AI teamwork can handle complex tasks better than AI alone.

Conclusion: Argues that AI's progress should be measured by its ability to collaborate with humans rather than its independence.

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [72] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed,Uljad Berdica,Martha Elliott,Danijela Horak,Jakob N. Foerster*

Main category: cs.AI

TL;DR: A new method called Intent Factored Generation (IFG) is proposed to improve the diversity of samples from large language models while maintaining coherence and quality.


<details>
  <summary>Details</summary>
Motivation: Current methods for increasing diversity often only operate at the token-level, leading to poor exploration on reasoning problems and unengaging conversational agents.

Method: Factorising the sampling process into two stages: first, sample a semantically dense intent; second, sample the final response conditioning on both the original prompt and the intent from the first stage. Also, prompting the model to explicitly state its intent before generating each step is beneficial for reasoning tasks.

Result: The method improves both pass@k and Reinforcement Learning from Verifier Feedback on maths and code tasks. It also increases conversational diversity without sacrificing reward when combined with Direct Preference Optimisation for instruction-tuning. On a general language modelling task, it achieves higher diversity while maintaining the quality of generations.

Conclusion: This method presents a simple way to increase the sample diversity of LLMs while maintaining performance, and it can be easily integrated into many algorithms for various applications.

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
*Valentyn Boreiko,Alexander Panfilov,Vaclav Voracek,Matthias Hein,Jonas Geiping*

Main category: cs.LG

TL;DR: We introduce a unified threat model to compare jailbreak attacks and find that effective attacks rely on rare bigrams.


<details>
  <summary>Details</summary>
Motivation: There are many jailbreaking attacks that can coerce harmful responses from safety-tuned large language models (LLMs). However, these attacks differ greatly in fluency and computational effort.

Method: Propose a unified threat model for the principled comparison of jailbreak methods. Build an N-gram language model on 1T tokens.

Result: Attack success rates against safety-tuned modern models are lower than previously presented. Attacks based on discrete optimization significantly outperform recent LLM-based attacks.

Conclusion: Attack success rates against safety-tuned modern models are lower than previously presented. Effective jailbreak attacks exploit and abuse infrequent bigrams.

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [74] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron,Devin White*

Main category: cs.LG

TL;DR: This paper investigates the relationship between memorization and generalization in large language models by pre-training capacity-limited Transformer models on synthetic tasks. The results show a trade-off between memorization and extrapolation abilities, which shifts towards memorization as model capacity increases. Joint training on both tasks prevents any model from succeeding at extrapolation.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between memorization and generalization in large language models and how model capacity influences learning behavior.

Method: Pre-training a series of capacity-limited Transformer models from scratch on synthetic tasks designed to probe generalization and memorization separately and jointly.

Result: There is a consistent trade-off between memorization and extrapolation. Larger models memorize better but fail to extrapolate, while smaller models can extrapolate but cannot memorize. Joint training on both tasks prevents any model from succeeding at extrapolation.

Conclusion: Pre-training may intrinsically favor one learning mode over the other. This study provides insights into how model capacity shapes learning behavior and has implications for designing and deploying small language models.

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [75] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang,Kumar Ayush,Siyuan Qiao,A. Ali Heydari,Girish Narayanswamy,Maxwell A. Xu,Ahmed A. Metwally,Shawn Xu,Jake Garrison,Xuhai Xu,Tim Althoff,Yun Liu,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Cecilia Mascolo,Xin Liu,Daniel McDuff,Yuzhe Yang*

Main category: cs.LG

TL;DR: SensorLM is a family of sensor-language foundation models that enable wearable sensor data understanding with natural language by introducing a hierarchical caption generation pipeline and extending prominent multimodal pretraining architectures.


<details>
  <summary>Details</summary>
Motivation: To enable wearable sensor data understanding with natural language despite the lack of paired, richly annotated sensor-text descriptions in uncurated, real-world wearable data.

Method: Introducing a hierarchical caption generation pipeline and extending prominent multimodal pretraining architectures.

Result: The approach enabled the curation of the largest sensor-language dataset to date, comprising over 59.7 million hours of data from more than 103,000 people.

Conclusion: SensorLM demonstrates superior performance over state-of-the-art in zero-shot recognition, few-shot learning, and cross-modal retrieval.

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [76] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt,Max Ruiz Luyten,Thomas Pouplin,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: Introduces a novel LLM agent framework that enhances planning capabilities through in-context learning, using atomic fact augmentation and recursive lookahead search, showing improved performance and adaptability on interactive tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning.

Method: Uses atomic fact augmentation and recursive lookahead search for planning, allowing the agent to improve its understanding and decision-making online without weight updates.

Result: Demonstrates improved performance and adaptability on challenging interactive tasks such as TextFrozenLake and ALFWorld.

Conclusion: This approach allows the agent to leverage its experience to refine its behavior without weight updates.

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [77] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song,Ramith Hettiarachchi,Chuan Li,Jianwen Xie,Lei Li*

Main category: cs.LG

TL;DR: This paper introduces InstructPro, a new family of protein generative models that can design ligand-binding proteins based on natural language instructions. The model outperforms previous methods.


<details>
  <summary>Details</summary>
Motivation: Designing proteins that bind to a given ligand is crucial in many biological and chemical applications, but existing AI models are limited by the scarcity of protein-ligand complex data. This paper aims to use human-curated text descriptions about protein-ligand interactions instead.

Method: Developing InstructPro, a protein generative model that follows natural language instructions to design ligand-binding proteins. The model uses a textual description of the desired function and a ligand formula in SMILES format to generate protein sequences.

Result: The proposed method outperforms strong baselines, including ProGen2, ESM3, and Pinal. InstructPro-1B has the highest docking success rate and lowest average RMSD, while InstructPro-3B further decreases the average RMSD.

Conclusion: InstructPro demonstrates the potential of using natural language instructions to design ligand-binding proteins.

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [78] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts,Kyle Mylonakis,Sidhartha Roy,Kaan Kale*

Main category: cs.LG

TL;DR: This paper introduces the Stained Glass Transform, a method that allows for privacy-preserving word embedding transformations in large language models without losing model utility.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns related to using large language models with sensitive data by providing theoretical privacy guarantees while maintaining model performance.

Method: Introducing a learned, stochastic, and sequence-dependent transformation of word embeddings in large language models.

Result: A posteriori privacy estimates based on mutual information were calculated and verified using token-level privacy metrics and standard LLM performance benchmarks.

Conclusion: The Stained Glass Transform offers a way to enhance privacy for input data to large language models without compromising their utility.

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [79] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: Athena-PRM is a multimodal process reward model that evaluates the reward score for each step in solving complex reasoning problems. It uses prediction consistency between weak and strong completers to identify reliable process labels, demonstrating excellent effectiveness with only 5,000 samples. Two strategies, ORM initialization and up-sampling for negative data, further enhance PRM performance.


<details>
  <summary>Details</summary>
Motivation: Developing high-performance PRMs requires significant time and financial investment due to the need for step-level annotations of reasoning steps, and conventional automated labeling methods often produce noisy labels and high computational costs.

Method: Athena-PRM leverages prediction consistency between weak and strong completers as a criterion for identifying reliable process labels, and it implements two strategies to improve PRM performance: ORM initialization and up-sampling for negative data.

Result: Athena-PRM shows outstanding effectiveness across various scenarios and benchmarks, improving performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling, setting SoTA results in VisualProcessBench, and outperforming baselines with a significant margin on five benchmarks.

Conclusion: Athena-PRM demonstrates its robust capability to accurately assess the correctness of the reasoning step and achieves superior performance across multiple benchmarks and scenarios.

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [80] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM是一种开放源代码评估框架，用于评估多模态模型在视觉语言理解和生成任务中的表现，具有灵活的资源分配和高效的评估能力。


<details>
  <summary>Details</summary>
Motivation: 提出一个全面评估多模态模型的框架，能够灵活分配资源并轻松集成新任务和模型。

Method: 通过独立的评估服务将模型推理与评估分离，并使用先进的推理加速工具和异步数据加载来显著提高评估效率。

Result: 实验表明，FlagEvalMM提供了关于模型优缺点的准确且高效的见解。

Conclusion: FlagEvalMM是一个有价值的工具，可以推动多模态研究的发展。

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [81] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Main category: cs.CV

TL;DR: Introduce CAIRe, a novel evaluation metric that assesses the degree of cultural relevance of an image.


<details>
  <summary>Details</summary>
Motivation: Ensure equitable performance of text-to-image models across diverse cultural contexts.

Method: Ground entities and concepts in the image to a knowledge base and use factual information to give independent graded judgments for each culture label.

Result: CAIRe surpasses all baselines by 28% F1 points on a manually curated dataset and achieves Pearson's correlations of 0.56 and 0.66 with human ratings on two datasets for culturally universal concept.

Conclusion: CAIRe demonstrates strong alignment with human judgment across diverse image sources.

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [82] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
*Beomsik Cho,Jaehyung Kim*

Main category: cs.CV

TL;DR: Introduce ReVisiT, a decoding method for vision-language models that improves visual grounding by referencing vision tokens during text generation.


<details>
  <summary>Details</summary>
Motivation: Conventional decoding strategies of large vision-language models often fail to utilize visual information, leading to visually ungrounded responses.

Method: ReVisiT leverages semantic information from vision tokens and projects it into the text token distribution space, selecting the most relevant vision token at each decoding step through constrained divergence minimization.

Result: Experiments show ReVisiT enhances visual grounding with minimal computational overhead and achieves competitive or superior results compared to state-of-the-art baselines while reducing computational costs.

Conclusion: ReVisiT is an effective and efficient decoding method that improves visual grounding in vision-language models.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [83] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Main category: cs.CV

TL;DR: Propose a simple inference-time technique to improve the performance of dual encoder Vision-Language Models (VLMs) for image-text retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with compositionality and show a bag-of-words-like behavior, limiting their retrieval performance. Most research focuses on training approaches, while inference-time techniques receive less attention.

Method: Divide images into smaller crops, extract text segments, match image crops with text segments using a VLM, and aggregate individual similarities of matches to compute final image-text similarity.

Result: The approach improves the performance of evaluated VLMs without any training, particularly for attribute-object binding in controlled datasets.

Conclusion: Inference-time techniques have potential and processing image crops is essential for observed performance gains.

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [84] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Main category: cs.SE

TL;DR: Analyze the limitations of SWE-Bench and propose UTGenerator and UTBoost to improve the test case generation and augmentation.


<details>
  <summary>Details</summary>
Motivation: SWE-Bench's test cases are often insufficient, allowing incorrect patches to pass undetected.

Method: Propose UTGenerator to automatically generate test cases and UTBoost for test case augmentation.

Result: Identified 36 task instances with insufficient test cases and uncovered 345 erroneous patches in SWE-Bench.

Conclusion: The improvements impact leaderboard entries and cause ranking changes.

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [85] [ThinkQE: Query Expansion via an Evolving Thinking Process](https://arxiv.org/abs/2506.09260)
*Yibin Lei,Tao Shen,Andrew Yates*

Main category: cs.IR

TL;DR: ThinkQE is a novel test-time query expansion framework that improves web search performance by promoting both exploration and result diversity.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods often generate narrowly focused query expansions that overlook the need for exploration and diversity.

Method: ThinkQE introduces a thinking-based expansion process and a corpus-interaction strategy to refine query expansions iteratively.

Result: Experiments on various web search benchmarks show ThinkQE outperforms previous approaches.

Conclusion: ThinkQE demonstrates strong performance and domain generalization without requiring additional training.

Abstract: Effective query expansion for web search benefits from promoting both
exploration and result diversity to capture multiple interpretations and facets
of a query. While recent LLM-based methods have improved retrieval performance
and demonstrate strong domain generalization without additional training, they
often generate narrowly focused expansions that overlook these desiderata. We
propose ThinkQE, a test-time query expansion framework addressing this
limitation through two key components: a thinking-based expansion process that
encourages deeper and comprehensive semantic exploration, and a
corpus-interaction strategy that iteratively refines expansions using retrieval
feedback from the corpus. Experiments on diverse web search benchmarks (DL19,
DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,
including training-intensive dense retrievers and rerankers.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Shop-R1的新强化学习框架，旨在增强大型语言模型在在线购物环境中模拟真实人类行为的推理能力。该框架将任务分为两个阶段，并使用不同的奖励信号进行指导。实验结果表明，该方法比基线方法有显著的改进。


<details>
  <summary>Details</summary>
Motivation: 先前的工作探索了通过大型语言模型合成的理由来增强训练数据，并应用监督微调（SFT）来提高推理能力，这可以改善下游动作预测。然而，这些方法的性能受到用于生成理由的模型推理能力的固有限制。

Method: 我们引入了Shop-R1，这是一种新的强化学习框架，旨在增强大型语言模型在在线购物环境中的推理能力。Shop-R1将人类行为模拟任务分解为两个阶段：理由生成和动作预测，每个阶段由不同的奖励信号指导。

Result: 实验结果表明，我们的方法相比基线方法有超过65%的相对改进。

Conclusion: 实验结果表明，我们的方法相比基线方法有超过65%的相对改进。

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [2] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: DG-PRM is a dynamic and generalizable process reward modeling approach that improves LLM performance by capturing fine-grained reward criteria and using Pareto dominance estimation.


<details>
  <summary>Details</summary>
Motivation: Existing PRMs rely on heuristic approaches that struggle with cross-domain generalization, and LLM-as-judge research has overlooked the meaningful guidance within text. Static evaluation criteria also fail to adapt to complex process supervision.

Method: DG-PRM features a reward tree to capture and store fine-grained, multi-dimensional reward criteria, and dynamically selects reward signals for step-wise reward scoring. It also adopts Pareto dominance estimation to identify discriminative positive and negative pairs.

Result: DG-PRM achieves stunning performance on prevailing benchmarks, significantly boosting model performance across tasks with dense rewards. It also adapts well to out-of-distribution scenarios.

Conclusion: DG-PRM demonstrates exceptional generalizability and significantly boosts model performance across tasks with dense rewards.

Abstract: Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [3] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: 本文介绍了VeriMinder，这是一个交互式系统，用于检测和缓解分析问题中的认知偏差。通过三个关键创新，包括上下文语义映射框架、操作化Hard-to-Vary原则的分析框架以及优化的LLM驱动系统，VeriMinder在用户测试中表现出色，并且其代码库已作为开源软件提供。


<details>
  <summary>Details</summary>
Motivation: 应用使用自然语言数据库接口（NLIDBs）的系统已经使数据分析民主化。这种积极的发展也带来了紧迫的挑战，即帮助那些可能没有统计分析背景的用户形成无偏的分析问题。虽然大量研究集中在文本到SQL生成的准确性上，但解决分析问题中的认知偏差仍然研究不足。

Method: 我们提出了VeriMinder，这是一个交互式系统，用于检测和缓解此类分析漏洞。我们的方法引入了三个关键创新：(1) 一个上下文语义映射框架，用于与特定分析上下文相关的偏见；(2) 一个操作化Hard-to-Vary原则并指导用户进行系统数据分析的分析框架；(3) 一个优化的LLM驱动系统，使用涉及多个候选、批评反馈和自我反思的结构化过程生成高质量的任务特定提示。

Result: 用户测试证实了我们方法的优点。在直接用户体验评估中，82.5%的参与者报告说对分析质量产生了积极影响。在比较评估中，VeriMinder在分析的明确性、全面性和准确性指标上比其他方法显著更高，至少提高了20%。

Conclusion: 我们的系统，作为一个网络应用程序，旨在帮助用户在数据分析过程中避免'错误问题'的漏洞。VeriMinder的代码库已作为MIT许可证的开源软件提供，以促进进一步的研究和社区内的采用。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [4] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 本文提出了一种高效的端到端自动口语评估方法，使用单个Whisper-small编码器处理所有口语回答，并通过轻量级聚合器预测最终分数。该方法减少了参数数量和推理时间，在保持良好性能的同时提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统的自动口语评估方法需要转录和每个部分的模型，这增加了计算成本和复杂性。本文旨在开发一种更高效、实用的多部分口语评估系统。

Method: 我们的系统使用了一个Whisper-small编码器来处理所有四个口语回答，并通过一个轻量级聚合器结合所有信息以预测最终分数。此外，我们还提出了一种数据采样策略，使模型能够在仅使用44.8%的说话者数据的情况下达到接近最佳性能。

Result: 我们的系统在RMSE上达到了0.384，优于基于文本的基线（0.44），并且使用的参数最多为168M（约等于Whisper-small的70%）。此外，通过数据采样策略，模型仅使用44.8%的说话者数据即可达到0.383的RMSE，表现出对不平衡类别的改进和强大的数据效率。

Conclusion: 我们的系统在2025年Speak & Improve挑战赛中展示了高效的端到端方法，能够处理所有四个口语回答，并且在减少参数数量和推理时间的同时保持了良好的性能。

Abstract: We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [5] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

TL;DR: 本研究评估了六种常用AI检测工具对DeepSeek生成文本的识别能力，并应用了对抗性技术如改写和人类化来测试检测器的鲁棒性。结果表明，某些工具在原始和改写文本上表现良好，但人类化攻击显著降低了它们的准确性。此外，通过少量示例提示和思维链推理，检测工具表现出高准确性。


<details>
  <summary>Details</summary>
Motivation: 由于DeepSeek是一个最近发布的大型语言模型，文献中对其检测工具的有效性研究较少，因此本研究旨在填补这一空白，评估现有AI检测工具对DeepSeek生成文本的识别能力。

Method: 研究评估了六种常用的AI检测工具（AI Text Classifier、Content Detector AI、Copyleaks、QuillBot、GPT-2和GPTZero）对DeepSeek生成文本的识别能力，并应用了对抗性技术如改写和人类化来测试检测器的鲁棒性。同时，通过少量示例提示和思维链推理（CoT）将DeepSeek作为检测器进行测试。

Result: QuillBot和Copyleaks在原始和改写后的DeepSeek文本上表现出接近完美的性能，而AI Text Classifier和GPT-2的表现不一致。人类化攻击显著降低了Copyleaks、QuillBot和GPTZero的准确性。通过少量示例提示和思维链推理，检测工具表现出高准确性，其中最佳的五次示例结果仅错误分类了一个样本（AI召回率为96%，人类召回率为100%）。

Conclusion: 研究发现，尽管某些AI检测工具在原始和改写后的DeepSeek文本上表现出接近完美的性能，但其他工具如AI Text Classifier和GPT-2的表现不一致。人类化攻击显著降低了Copyleaks、QuillBot和GPTZero的准确性。此外，通过少量示例提示和思维链推理，检测工具表现出了高准确性。

Abstract: Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


### [6] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
*Sohaib Imran,Ihor Kendiukhov,Matthew Broerman,Aditya Thomas,Riccardo Campanella,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 本研究测试了更大的和更强大的语言模型是否在提供上下文证据时更一致地更新他们的“信念”关于命题。我们提出了一个贝叶斯一致性系数（BCC）指标，并测量了多个预训练仅语言模型的BCC。我们的结果表明，更大的和更强大的预训练语言模型分配的信念与贝叶斯定理更加一致。


<details>
  <summary>Details</summary>
Motivation: 我们想测试更大的和更强大的语言模型是否在提供上下文证据时更一致地更新他们的“信念”关于命题。

Method: 我们提出了一个贝叶斯一致性系数（BCC）指标，并生成了一个数据集来测量BCC。我们测量了多个预训练仅语言模型的BCC，比较了模型参数数量、训练数据量和模型在常见基准上的得分。

Result: 我们的结果提供了证据，表明更大的和更强大的预训练语言模型分配的信念与贝叶斯定理更加一致。

Conclusion: 我们的结果支持了我们的假设，即更大和更强大的预训练语言模型分配的信念与贝叶斯定理更加一致。这些结果对理解和发展LLMs有重要的影响。

Abstract: Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.

</details>


### [7] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
*Fitsum Gaim,Jong C. Park*

Main category: cs.CL

TL;DR: 本文对提格利尼亚语NLP的研究进行了全面调查，分析了超过40项研究，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 提格利尼亚语在自然语言处理研究中严重不足，因此需要对其研究进行系统性回顾。

Method: 本文对2011年至2025年期间的40多项研究进行了系统回顾，分析了提格利尼亚语NLP的现状。

Result: 本文揭示了从基于规则的系统到现代神经架构的明显趋势，并识别了关键挑战和有希望的研究方向。

Conclusion: 本文为研究人员提供了全面的参考，并为推进提格利尼亚语NLP指明了道路。

Abstract: Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [8] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
*Zihan Wang,Xinzhang Liu,Yitong Yao,Chao Wang,Yu Zhao,Zhihao Yang,Wenmin Deng,Kaipeng Jia,Jiaxin Peng,Yuyao Huang,Sishi Xiong,Zhuo Jiang,Kaidong Yu,Xiaohui Hu,Fubei Yao,Ruiyu Fang,Zhuoru Jiang,Ruiting Song,Qiyi Xie,Rui Xue,Xuewei He,Yanlei Xue,Zhu Yuan,Zhaoxi Zhang,Zilu Huang,Shiquan Wang,Xin Wang,Hanming Wu,Mingyuan Wang,Xufeng Zhan,Yuhan Sun,Zhaohu Xing,Yuhao Jiang,Bingkai Yang,Shuangyong Song,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: A new series of language models, TeleChat2, TeleChat2.5, and T1, have been introduced with significant performance improvements over their predecessor, TeleChat, through enhanced training strategies. They are publicly released to empower developers and researchers.


<details>
  <summary>Details</summary>
Motivation: The motivation is to introduce a new series of language models that offer significant upgrades over their predecessor, TeleChat, by enhancing training strategies to achieve better performance in various tasks.

Method: The new series of models achieves performance gains through enhanced training strategies in both pre-training and post-training stages. TeleChat2 undergoes pretraining on 10 trillion high-quality and diverse tokens, followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). TeleChat2.5 and T1 expand the pipeline by incorporating continual pretraining with domain-specific datasets and reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks.

Result: The new series of models, including TeleChat2, TeleChat2.5, and T1, demonstrate substantial improvements in reasoning and general task performance compared to the original TeleChat. T1-115B outperforms proprietary models such as OpenAI's o1-mini and GPT-4o.

Conclusion: TeleChat2, TeleChat2.5, and T1 are state-of-the-art language models that offer significant improvements over their predecessor, TeleChat. They are publicly released to empower developers and researchers with advanced language models tailored for diverse applications.

Abstract: We introduce the latest series of TeleChat models: \textbf{TeleChat2},
\textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over
their predecessor, TeleChat. Despite minimal changes to the model architecture,
the new series achieves substantial performance gains through enhanced training
strategies in both pre-training and post-training stages. The series begins
with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion
high-quality and diverse tokens. This is followed by Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO) to further enhance its
capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by
incorporating a continual pretraining phase with domain-specific datasets,
combined with reinforcement learning (RL) to improve performance in code
generation and mathematical reasoning tasks. The \textbf{T1} variant is
designed for complex reasoning, supporting long Chain-of-Thought (CoT)
reasoning and demonstrating substantial improvements in mathematics and coding.
In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid
inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are
dense Transformer-based architectures with 115B parameters, showcasing
significant advancements in reasoning and general task performance compared to
the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models
such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2},
\textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B
and 115B parameters, to empower developers and researchers with
state-of-the-art language models tailored for diverse applications.

</details>


### [9] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
*Weizhi Fei,Hao Shi,Jing Xu,Jingchen Peng,Jiazheng Li,Jingzhao Zhang,Bo Bai,Wei Han,Zhenyuan Chen,Xueyan Niu*

Main category: cs.CL

TL;DR: NeuralDB是一种新的编辑框架，通过将编辑事实表示为神经KV数据库，并使用非线性门控检索模块，在大规模编辑时有效保留LLM的一般能力。


<details>
  <summary>Details</summary>
Motivation: 现有L&E方法在大规模编辑时可能损害LLM的一般能力并导致遗忘已编辑的事实，因此需要一种能够有效保留LLM一般能力的编辑框架。

Method: 将现有的线性L&E方法建模为查询键值（KV）数据库，并提出NeuralDB，这是一个带有非线性门控检索模块的神经KV数据库，仅在涉及编辑事实的推理时运行，从而有效保留LLM的一般能力。

Result: 在ZsRE和CounterFacts数据集上进行的实验表明，NeuralDB在编辑效果、泛化性、特异性、流畅性和一致性方面表现优异，并且在六个文本理解和生成任务中保持整体性能。此外，NeuralDB在扩展到100,000个事实时仍能保持有效性。

Conclusion: NeuralDB不仅在编辑效果、泛化性、特异性、流畅性和一致性方面表现出色，还能在六个代表性的文本理解和生成任务中保持整体性能。进一步的实验表明，NeuralDB在扩展到100,000个事实时仍能保持有效性。

Abstract: Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [10] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
*Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: GrAInS is an inference-time steering approach that improves model behavior by identifying influential tokens and adjusting activations, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time steering methods have limitations such as relying on fixed intervention vectors, overlooking the causal influence of individual input tokens, and failing to leverage informative gradients from the model's logits.

Method: GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the most influential tokens and constructs directional steering vectors to adjust hidden activations during inference.

Result: GrAInS achieves significant improvements in accuracy, hallucination reduction, and alignment win rates across multiple models and tasks while preserving fluency and general capabilities.

Conclusion: GrAInS provides a fine-grained, interpretable, and modular way to control model behavior without retraining or auxiliary supervision, and it consistently outperforms existing methods in various tasks.

Abstract: Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.

</details>


### [11] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
*Hoyeon Lee,Sejung Son,Ye-Eun Kang,Jong-Hwan Kim*

Main category: cs.CL

TL;DR: 本文探讨了利用大型语言模型生成合成短语断点注释，以解决传统注释的高成本和数据获取困难问题，并验证了其在多种语言中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在文本到语音系统中解决了关键的韵律方面的问题，但严重依赖于音频或文本的大量人工注释，导致显著的人工努力和成本。语音领域的固有变化性，由语音因素驱动，进一步使获取一致、高质量的数据变得复杂。最近，大型语言模型（LLMs）在解决自然语言处理中的数据挑战方面表现出成功，通过生成定制的合成数据，同时减少手动注释的需求。

Method: 本文探讨了利用LLM生成合成短语断点注释，通过与传统注释进行比较，并评估在多种语言中的有效性。

Result: 研究结果表明，基于LLM的合成数据生成有效缓解了短语断点预测中的数据挑战，并突显了LLM在语音领域作为可行解决方案的潜力。

Conclusion: 研究结果表明，基于LLM的合成数据生成有效缓解了短语断点预测中的数据挑战，并突显了LLM在语音领域作为可行解决方案的潜力。

Abstract: Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.

</details>


### [12] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
*Tevin Atwal,Chan Nam Tieu,Yefeng Yuan,Zhan Shi,Yuhong Liu,Liang Cheng*

Main category: cs.CL

TL;DR: 本文提出了评估合成数据集多样性和隐私的指标，并发现LLMs在这些方面存在不足，随后提出一种方法来提高合成评论的多样性并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 随着由大型语言模型（LLMs）生成的合成数据的使用增加，其多样性和隐私风险仍缺乏深入研究。

Method: 提出了一组全面的指标来定量评估合成数据集的多样性（即语言表达、情感和用户视角）和隐私（即重新识别风险和风格异常值）。

Result: 实验结果揭示了LLMs在生成多样性和隐私保护的合成数据方面存在显著局限性。

Conclusion: 实验结果揭示了LLMs在生成多样性和隐私保护的合成数据方面存在显著局限性。基于评估结果，提出了一种基于提示的方法，以增强合成评论的多样性同时保护评论者的隐私。

Abstract: The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.

</details>


### [13] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
*Zehan Li,Hongjie Chen,Yuxin Zhang,Jing Zhou,Xuening Wang,Hang Lv,Mengjie Du,Yaodong Song,Jie Lian,Jian Kang,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出TELEVAL，一个动态基准，用于评估SLMs在现实中文交互环境中的对话代理效果，强调模型从用户语音中提取隐含线索并适当回应的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要关注SLMs是否能执行复杂任务，与用户在现实对话场景中的自然互动不一致。因此需要一个更贴近真实交互的评估框架。

Method: TELEVAL是一个动态基准，专门设计用于评估SLMs在现实中文交互环境中的对话代理效果。它定义了三个评估维度：显式语义、副语言和隐式语义以及系统能力。采用与实际使用一致的对话格式，并分别评估文本和音频输出。

Result: 实验表明，尽管最近有所进展，现有的SLMs在自然对话任务中仍有很大的改进空间。

Conclusion: 我们希望TELEVAL能够作为一个以用户为中心的评估框架，直接反映用户体验，并有助于开发更强大的对话导向的SLMs。

Abstract: Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.

</details>


### [14] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
*Haomin Qi,Zihan Dai,Chengbo Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的混合微调方法，结合了BOFT和LoRA-GA的优势，并探索了uRNN在Transformer模型中的应用，显著提高了收敛效率和泛化能力，同时降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 由于大规模语言模型的规模和内存需求，微调大型语言模型仍然存在计算瓶颈。因此，需要一种更高效的微调方法来减少资源消耗并提高性能。

Method: 本文提出了一种新的混合策略，动态集成BOFT的正交稳定性与LoRA-GA的梯度对齐快速收敛，并探索了单位RNN（uRNN）原理在基于Transformer的LLM中的适应性，通过结构化的单位约束增强梯度稳定性。

Result: 在四个基准测试（GLUE、GSM8K、MT-Bench和HumanEval）上的实证评估表明，混合方法在训练时间和内存使用方面分别减少了2.1倍和50%，并且接近全微调的准确性。

Conclusion: 本文提出的混合方法在资源受限的现实世界部署中为大型语言模型的微调提供了一个实用且可扩展的解决方案。

Abstract: Fine-tuning large language models (LLMs) remains a computational bottleneck
due to their scale and memory demands. This paper presents a comprehensive
evaluation of parameter-efficient fine-tuning (PEFT) techniques, including
LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that
dynamically integrates BOFT's orthogonal stability with LoRA-GA's
gradient-aligned rapid convergence. By computing per-layer adaptive updates
guided by gradient norms, the hybrid method achieves superior convergence
efficiency and generalization across diverse tasks. We also explore, for the
first time, the adaptation of unitary RNN (uRNN) principles to
transformer-based LLMs, enhancing gradient stability through structured unitary
constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,
and HumanEval -- using models ranging from 7B to 405B parameters demonstrate
that our hybrid method consistently outperforms individual PEFT baselines,
approaching full fine-tuning accuracy while reducing resource consumption by up
to 2.1 times in training time and 50 percent in memory usage. These findings
establish the hybrid approach as a practical and scalable fine-tuning solution
for real-world deployment of LLMs under resource constraints.

</details>


### [15] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
*Riley Carlson,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: 本文介绍了2024年更新的英文GloVe模型，这些模型在包含新词汇和改进性能方面有所提升。


<details>
  <summary>Details</summary>
Motivation: 由于语言和世界不断变化，当前使用可能需要更新的模型，同时2014年的模型没有详细记录数据版本和预处理过程，因此进行了纠正。

Method: 使用Wikipedia、Gigaword和Dolma的一个子集训练了两组词嵌入。

Result: 2024年的向量包含了新的文化和语言相关的词汇，在结构任务如类比和相似性上表现相当，在非西方新闻数据等近期时间依赖性NER数据集上表现出更好的性能。

Conclusion: 2024年的GloVe模型在文化与语言相关词汇的整合上表现良好，并在近期的时间依赖性NER数据集上显示出改进的性能。

Abstract: This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.

</details>


### [16] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
*Hongjie Chen,Zehan Li,Yaodong Song,Wenming Deng,Yitong Yao,Yuxin Zhang,Hang Lv,Xuechao Zhu,Jian Kang,Jie Lian,Jie Li,Chao Wang,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 本文介绍了GOAT-SLM，一种能够超越语言内容建模的口语语言模型，通过双模态头架构和模块化训练策略，显著提升了语音交互的自然性和社会意识。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数模型将语音仅仅视为语言内容的载体，往往忽略了人类语音中嵌入的丰富的语调和说话者特征线索，例如方言、年龄、情绪和非语音声音。

Method: 本文引入了GOAT-SLM，这是一种具有语调和说话者特征意识的新型口语语言模型，采用了双模态头架构，将语言建模与声学实现解耦，并提出了一种模块化、分阶段的训练策略，以利用大规模语音-文本语料库逐步对齐语言、语调和说话者特征信息。

Result: 实验结果表明，GOAT-SLM在TELEVAL多维评估基准上实现了语义和非语义任务之间的良好平衡性能，并在处理情绪、方言变化和年龄敏感交互方面优于现有的开源模型。

Conclusion: 本文强调了超越语言内容建模的重要性，并推动了更自然、适应性强和社会意识的口语语言系统的开发。

Abstract: Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.

</details>


### [17] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
*Xiaoyuan Li,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Main category: cs.CL

TL;DR: 本文评估了多模态大语言模型在基于代码的多模态数学推理中的能力，发现现有模型在细粒度视觉操作方面仍落后于人类。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注文本-only推理输出，而忽略了MLLM通过代码进行准确视觉操作的能力。

Method: 本文提出了一个框架，专注于两个关键评估方面：多模态代码生成（MCG）和多模态代码编辑（MCE）。

Result: 实验评估涉及九个主流的MLLM，结果揭示了现有模型在执行细粒度视觉操作方面的不足。

Conclusion: 现有模型在执行细粒度视觉操作方面仍显著落后于人类表现。

Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled
step-by-step multi-modal mathematical reasoning by performing visual operations
based on the textual instructions. A promising approach uses code as an
intermediate representation to precisely express and manipulate the images in
the reasoning steps. However, existing evaluations focus mainly on text-only
reasoning outputs, leaving the MLLM's ability to perform accurate visual
operations via code largely unexplored. This work takes a first step toward
addressing that gap by evaluating MLLM's code-based capabilities in multi-modal
mathematical reasoning.Specifically, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's
ability to accurately understand and construct visualizations from scratch. (2)
Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained
operations, which include three types: Deletion, Modification and Annotation.
To evaluate the above tasks, we incorporate a dataset that covers the five most
popular types of mathematical figures, including geometric diagrams, function
plots, and three types of statistical charts, to provide a comprehensive and
effective measurement of existing MLLMs. Our experimental evaluation involves
nine mainstream MLLMs, and the results reveal that existing models still lag
significantly behind human performance in performing fine-grained visual
operations.

</details>


### [18] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
*Gonzalo Cardenal Antolin,Jacques Fellay,Bashkim Jaha,Roger Kouyos,Niko Beerenwinkel,Diane Duroux*

Main category: cs.CL

TL;DR: 本研究评估了当前LLMs在HIV管理中的能力，引入了HIVMedQA基准来评估开放式的医学问题回答。结果表明，Gemini 2.5 Pro在大多数维度上表现最佳，但随着问题复杂性的增加，性能下降。医学微调的模型并不总是优于通用模型，而更大的模型规模并不是性能的可靠预测因素。


<details>
  <summary>Details</summary>
Motivation: HIV管理是一个有说服力的应用案例，因为其复杂性，包括多样的治疗选择、合并症和依从性挑战。然而，将LLM集成到临床实践中引发了关于准确性、潜在危害和医生接受度的担忧。尽管有潜力，AI在HIV护理中的应用仍缺乏探索，LLM基准研究很少。

Method: 本研究评估了当前LLMs在HIV管理中的能力，引入了HIVMedQA基准来评估开放式的医学问题回答。我们评估了七个通用和三个医学专业LLMs，并应用了提示工程来提高性能。我们的评估框架结合了词汇相似性和LLM-as-a-judge方法，扩展以更好地反映临床相关性。

Result: 结果表明，Gemini 2.5 Pro在大多数维度上始终优于其他模型。值得注意的是，排名前三的模型中有两个是专有的。随着问题复杂性的增加，性能下降。医学微调的模型并不总是优于通用模型，而更大的模型规模并不是性能的可靠预测因素。推理和理解比事实回忆更具挑战性，并观察到了认知偏差，如最近性和现状偏差。

Conclusion: 这些发现强调了针对开发和评估的必要性，以确保在临床护理中安全有效地整合LLM。

Abstract: Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.

</details>


### [19] [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)
*Kexin Chen,Dongxia Wang,Yi Liu,Haonan Zhang,Wenhai Wang*

Main category: cs.CL

TL;DR: 本文研究了粘性标记对基于Transformer的文本嵌入模型的影响，并提出了一种检测方法。发现这些标记可能来自特殊或未使用的词汇表条目以及多语言语料库中的碎片化子词，并对下游任务性能产生了显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的文本嵌入模型在NLP任务中被广泛使用，但令人惊讶的'粘性标记'可能会损害嵌入的可靠性。这些标记在重复插入到句子中时会将句子相似性拉向某个值，破坏嵌入距离的正常分布并降低下游性能。

Method: 我们系统地研究了这些异常标记，正式定义了它们，并基于句子和标记过滤引入了一种高效的检测方法，即粘性标记检测器（STD）。

Result: 我们发现总共有868个粘性标记。分析表明，这些标记通常来源于词汇表中的特殊或未使用的条目，以及多语言语料库中的碎片化子词。值得注意的是，它们的存在并不严格与模型大小或词汇表大小相关。我们进一步评估了粘性标记对下游任务如聚类和检索的影响，观察到高达50%的性能下降。

Conclusion: 我们的研究显示，需要更好的分词策略和模型设计来减轻未来文本嵌入应用中粘性标记的影响。

Abstract: Despite the widespread use of Transformer-based text embedding models in NLP
tasks, surprising 'sticky tokens' can undermine the reliability of embeddings.
These tokens, when repeatedly inserted into sentences, pull sentence similarity
toward a certain value, disrupting the normal distribution of embedding
distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and
introducing an efficient detection method, Sticky Token Detector (STD), based
on sentence and token filtering. Applying STD to 40 checkpoints across 14 model
families, we discover a total of 868 sticky tokens. Our analysis reveals that
these tokens often originate from special or unused entries in the vocabulary,
as well as fragmented subwords from multilingual corpora. Notably, their
presence does not strictly correlate with model size or vocabulary size. We
further evaluate how sticky tokens affect downstream tasks like clustering and
retrieval, observing significant performance drops of up to 50%. Through
attention-layer analysis, we show that sticky tokens disproportionately
dominate the model's internal representations, raising concerns about
tokenization robustness. Our findings show the need for better tokenization
strategies and model design to mitigate the impact of sticky tokens in future
text embedding applications.

</details>


### [20] [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)
*Wonjun Jeong,Dongseok Kim,Taegkeun Whangbo*

Main category: cs.CL

TL;DR: 本文提出了一种名为SCOPE的评估框架，用于测量和减轻大型语言模型在多项选择任务中的选择偏差，以提高评估的公平性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能通过利用选项位置或标签中的固有偏差在多项选择任务中获得虚高的分数，而不是展示真正的理解能力。因此，需要一种评估框架来测量和减轻这种选择偏差。

Method: SCOPE通过重复调用一个缺乏语义内容的空提示来估计每个模型的位置偏差分布，并根据逆偏差分布重新分配答案槽，从而平衡幸运率。此外，它防止语义相似的干扰项与答案相邻，阻止基于表面接近线索的近似猜测。

Result: 在多个基准实验中，SCOPE在稳定性能提升和正确选项的清晰置信度分布方面优于现有的去偏方法。

Conclusion: 本文提出了SCOPE框架，以提高大型语言模型评估的公平性和可靠性。

Abstract: Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.

</details>


### [21] [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)
*Keyu Wu,Qianjin Yu,Manlin Mei,Ruiting Liu,Jun Wang,Kailai Zhang,Yelun Bao*

Main category: cs.CL

TL;DR: 本文讨论了在电信网络中进行根本原因分析（RCA）的挑战，特别是人工智能在处理复杂图推理和缺乏现实基准方面的问题。


<details>
  <summary>Details</summary>
Motivation: 电信网络中的根本原因分析对人工智能提出了重大挑战，因为需要复杂的图推理，并且缺乏现实的基准。

Method: 本文探讨了人工智能在电信网络中进行根本原因分析的方法。

Result: 本文揭示了人工智能在处理电信网络中的根本原因分析时所面临的困难。

Conclusion: 本文强调了在电信网络中进行根本原因分析的复杂性，并指出需要更好的基准来推动人工智能的发展。

Abstract: Root Cause Analysis (RCA) in telecommunication networks is a critical task,
yet it presents a formidable challenge for Artificial Intelligence (AI) due to
its complex, graph-based reasoning requirements and the scarcity of realistic
benchmarks.

</details>


### [22] [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)
*Aline Belloni,Patrick Prieur*

Main category: cs.CL

TL;DR: 本文分析了ISO30401标准下知识管理系统如何与现有流程整合，并提出了通过PDCA循环和SECI模型实现这一目标的方法。


<details>
  <summary>Details</summary>
Motivation: 作者作为ISO30401的实施者，经常面临向客户解释知识开发、转化和传递活动如何与现有操作流程整合的挑战。

Method: 本文回顾了ISO9001背景下的流程建模原则，并基于作者的经验，探讨了ISO30401合规的知识管理系统如何与集成管理体系中的其他流程融合。

Result: 文章展示了ISO30401标准下的知识管理系统如何通过PDCA循环和SECI模型与现有流程融合。

Conclusion: 本文探讨了如何将ISO30401标准下的知识管理系统与现有的运营流程相结合，并通过PDCA循环和SECI模型实现其实施。

Abstract: Business process modeling is used by most organizations as an essential
framework for ensuring efficiency and effectiveness of the work and workflow
performed by its employees and for ensuring the alignment of such work with its
strategic goals. For organizations that are compliant or near-compliant with
ISO 9001, this approach involves the detailed mapping of processes,
sub-processes, activities, and tasks. ISO30401 is a Management System Standard,
introduced in 2018, establishing universal requirements for the set up of a
Knowledge Management System in an organization. As ``ISO30401 implementers'' we
regularly face the challenge of explaining our clients how the knowledge
development, transformation and conveyances activities depicted in ISO30401 do
integrate with existing operational processes. This article recaps process
modelling principles in the context of ISO9001 and explores, based on our
experience, how an ISO30401-compliant Knowledge Management System (KMS)
entwines with all other processes of an Integrated Management System and in
particular how it can be implemented by deploying the mechanisms of the SECI
model through the steps of PDCA cycles.

</details>


### [23] [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)
*San Kim,Jonghwi Kim,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出了GMTP，一种新的防御方法，用于检测和过滤对抗性构造的文档，以保护RAG系统免受中毒攻击。


<details>
  <summary>Details</summary>
Motivation: RAG依赖外部源，这可能带来安全风险，攻击者可以注入中毒文档以引导生成过程产生有害或误导性输出。

Method: GMTP通过检查检索器相似性函数的梯度来识别高影响标记，并通过掩码语言模型（MLM）检查这些标记的概率。

Result: 实验表明，GMTP能够消除超过90%的中毒内容，同时保留相关文档，从而在各种数据集和对抗设置中保持强大的检索和生成性能。

Conclusion: GMTP能够消除超过90%的中毒内容，同时保留相关文档，从而在各种数据集和对抗设置中保持强大的检索和生成性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
providing external knowledge for accurate and up-to-date responses. However,
this reliance on external sources exposes a security risk, attackers can inject
poisoned documents into the knowledge base to steer the generation process
toward harmful or misleading outputs. In this paper, we propose Gradient-based
Masked Token Probability (GMTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GMTP identifies high-impact
tokens by examining gradients of the retriever's similarity function. These key
tokens are then masked, and their probabilities are checked via a Masked
Language Model (MLM). Since injected tokens typically exhibit markedly low
masked-token probabilities, this enables GMTP to easily detect malicious
documents and achieve high-precision filtering. Experiments demonstrate that
GMTP is able to eliminate over 90% of poisoned content while retaining relevant
documents, thus maintaining robust retrieval and generation performance across
diverse datasets and adversarial settings.

</details>


### [24] [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)
*Kyubeen Han,Junseo Jang,Hongjin Kim,Geunyeong Jeong,Harksoo Kim*

Main category: cs.CL

TL;DR: 本研究探讨了指令调优如何增加大型语言模型对错误信息的易感性，并提出了需要系统方法来减轻这种影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要强调LLM对外部信息的接受度，但很少有研究直接探讨指令调优对此现象的影响。

Method: 我们研究了指令调优对LLM易受错误信息影响的影响，并比较了基础模型。

Result: 分析结果显示，经过指令调优的LLM在接受用户提供的错误信息时更有可能接受错误信息。

Conclusion: 我们的研究强调了需要系统的方法来减轻指令调优的意外后果，并提高LLM在现实应用中的可靠性。

Abstract: Instruction-tuning enhances the ability of large language models (LLMs) to
follow user instructions more accurately, improving usability while reducing
harmful outputs. However, this process may increase the model's dependence on
user input, potentially leading to the unfiltered acceptance of misinformation
and the generation of hallucinations. Existing studies primarily highlight that
LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of
instruction-tuning on this phenomenon. In our study, we investigate the impact
of instruction-tuning on LLM's susceptibility to misinformation. Our analysis
reveals that instruction-tuned LLMs are significantly more likely to accept
misinformation when it is presented by the user. A comparison with base models
shows that instruction-tuning increases reliance on user-provided information,
shifting susceptibility from the assistant role to the user role. Furthermore,
we explore additional factors influencing misinformation susceptibility, such
as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for
systematic approaches to mitigate unintended consequences of instruction-tuning
and enhance the reliability of LLMs in real-world applications.

</details>


### [25] [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)
*Xinrui Chen,Hongxing Zhang,Fanyi Zeng,Yongxian Wei,Yizhi Wang,Xitong Ling,Guanghao Li,Chun Yuan*

Main category: cs.CL

TL;DR: 本文提出Prune&Comp，一种无需训练的层剪枝方法，通过幅度补偿减少性能下降，提升了模型压缩效果。


<details>
  <summary>Details</summary>
Motivation: 现有的层剪枝方法在移除某些层后会导致隐藏状态的幅度差异，从而引起性能下降，因此需要一种有效的解决方案。

Method: Prune&Comp是一种新型的层剪枝方案，利用幅度补偿来减轻层移除引起的隐藏状态幅度差距，并通过迭代剪枝策略进一步优化效果。

Result: 在LLaMA-3-8B模型上，使用Prune&Comp进行5层剪枝后，困惑度几乎减半，同时保持了93.19%的原始模型问答性能，优于基线方法。

Conclusion: Prune&Comp能够通过训练-free的方式有效缓解层剪枝带来的性能下降，显著提升模型的压缩效果。

Abstract: Layer pruning has emerged as a promising technique for compressing large
language models (LLMs) while achieving acceleration proportional to the pruning
ratio. In this work, we identify that removing any layer induces a significant
magnitude gap in hidden states, resulting in substantial performance
degradation. To address this issue, we propose Prune&Comp, a novel
plug-and-play layer pruning scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate
the magnitude gap caused by layer removal and then eliminate this gap by
rescaling the remaining weights offline, with zero runtime overhead incurred.
We further demonstrate the advantages of Prune&Comp through an iterative
pruning strategy. When integrated with an iterative prune-and-compensate loop,
Prune&Comp consistently enhances existing layer pruning metrics. For instance,
when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence
metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the
original model's question-answering performance, outperforming the baseline by
4.01%.

</details>


### [26] [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)
*Suhang Wu,Jialong Tang,Chengyi Yang,Pei Zhang,Baosong Yang,Junhui Li,Junfeng Yao,Min Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出了一种新的定位与聚焦方法用于术语翻译，通过有效定位包含术语的语音片段并将其与句子和假设相关联，提高了术语翻译的成功率，同时保持了稳健的通用翻译性能。


<details>
  <summary>Details</summary>
Motivation: 直接语音翻译（ST）近年来受到越来越多的关注，但话语中的术语准确翻译仍然是一个重大挑战。目前的研究主要集中在利用各种翻译知识到ST模型中，但这些方法常常难以处理无关噪声的干扰，并且不能充分利用翻译知识。

Method: 提出了一种新的定位与聚焦方法用于术语翻译。首先，有效地定位包含术语的语音片段以构建翻译知识，减少ST模型的无关信息。随后，将翻译知识与来自音频和文本模态的句子和假设相关联，使ST模型在翻译过程中更好地关注翻译知识。

Result: 实验结果表明，我们的方法能够有效定位术语并提高术语翻译的成功率，同时保持稳健的通用翻译性能。

Conclusion: 实验结果表明，我们的方法能够有效定位术语并提高术语翻译的成功率，同时保持稳健的通用翻译性能。

Abstract: Direct speech translation (ST) has garnered increasing attention nowadays,
yet the accurate translation of terminology within utterances remains a great
challenge. In this regard, current studies mainly concentrate on leveraging
various translation knowledge into ST models. However, these methods often
struggle with interference from irrelevant noise and can not fully utilize the
translation knowledge. To address these issues, in this paper, we propose a
novel Locate-and-Focus method for terminology translation. It first effectively
locates the speech clips containing terminologies within the utterance to
construct translation knowledge, minimizing irrelevant information for the ST
model. Subsequently, it associates the translation knowledge with the utterance
and hypothesis from both audio and textual modalities, allowing the ST model to
better focus on translation knowledge during translation. Experimental results
across various datasets demonstrate that our method effectively locates
terminologies within utterances and enhances the success rate of terminology
translation, while maintaining robust general translation performance.

</details>


### [27] [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: 该研究比较了六种OCR引擎在两种低资源语言上的表现，并引入了一个新的合成Tamil OCR基准数据集。


<details>
  <summary>Details</summary>
Motivation: 由于拉丁语及其衍生脚本的印刷文本光学字符识别（OCR）问题已经得到解决，但低资源语言（LRL）使用独特脚本的OCR问题仍然未解决。因此，需要对不同OCR引擎在低资源语言上的表现进行比较分析。

Method: 该研究评估了Cloud Vision API、Surya、Document AI和Tesseract在Sinhala和Tamil上的表现，同时分析了Subasa OCR和EasyOCR在一种语言上的表现。使用五种测量技术对系统的准确性进行了严格分析。

Result: Surya在所有指标上对Sinhala的表现最佳，词错误率（WER）为2.61%；而Document AI在所有指标上对Tamil的表现最佳，字符错误率（CER）为0.78%。此外，还引入了一个新的合成Tamil OCR基准数据集。

Conclusion: 该研究通过比较六种不同的OCR引擎在两种低资源语言（Sinhala和Tamil）上的零样本性能，揭示了不同系统在这些语言上的表现差异，并引入了一个新的合成Tamil OCR基准数据集。

Abstract: Solving the problem of Optical Character Recognition (OCR) on printed text
for Latin and its derivative scripts can now be considered settled due to the
volumes of research done on English and other High-Resourced Languages (HRL).
However, for Low-Resourced Languages (LRL) that use unique scripts, it remains
an open problem. This study presents a comparative analysis of the zero-shot
performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The
selected engines include both commercial and open-source systems, aiming to
evaluate the strengths of each category. The Cloud Vision API, Surya, Document
AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR
and EasyOCR were examined for only one language due to their limitations. The
performance of these systems was rigorously analysed using five measurement
techniques to assess accuracy at both the character and word levels. According
to the findings, Surya delivered the best performance for Sinhala across all
metrics, with a WER of 2.61%. Conversely, Document AI excelled across all
metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the
above analysis, we also introduce a novel synthetic Tamil OCR benchmarking
dataset.

</details>


### [28] [StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer](https://arxiv.org/abs/2507.18294)
*Pritika Ramu,Apoorv Saxena,Meghanath M Y,Varsha Sankar,Debraj Basu*

Main category: cs.CL

TL;DR: StyleAdaptedLM is a framework that uses LoRA to transfer stylistic traits to instruction-following models, enabling robust stylistic customization without paired data or sacrificing task performance.


<details>
  <summary>Details</summary>
Motivation: Adapting LLMs to specific stylistic characteristics, like brand voice or authorial tones, is crucial for enterprise communication but challenging to achieve from corpora which lacks instruction-response formatting without compromising instruction adherence.

Method: We introduce StyleAdaptedLM, a framework that efficiently transfers stylistic traits to instruction-following models using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base model with diverse unstructured stylistic corpora, then merged with a separate instruction-following model.

Result: Experiments across multiple datasets and models demonstrate improved stylistic consistency while preserving instruction adherence, with human evaluations confirming brand-specific convention uptake.

Conclusion: StyleAdaptedLM offers an efficient path for stylistic personalization in LLMs.

Abstract: Adapting LLMs to specific stylistic characteristics, like brand voice or
authorial tones, is crucial for enterprise communication but challenging to
achieve from corpora which lacks instruction-response formatting without
compromising instruction adherence. We introduce StyleAdaptedLM, a framework
that efficiently transfers stylistic traits to instruction-following models
using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base
model with diverse unstructured stylistic corpora, then merged with a separate
instruction-following model. This enables robust stylistic customization
without paired data or sacrificing task performance. Experiments across
multiple datasets and models demonstrate improved stylistic consistency while
preserving instruction adherence, with human evaluations confirming
brand-specific convention uptake. StyleAdaptedLM offers an efficient path for
stylistic personalization in LLMs.

</details>


### [29] [BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit](https://arxiv.org/abs/2507.18305)
*Biao Yi,Zekun Fei,Jianing Geng,Tong Li,Lihai Nie,Zheli Liu,Yiming Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的后门攻击方法，称为“过度思考后门”，可以精确控制模型的推理冗长度，同时保持输出的正确性。实验结果表明，该方法能够可靠地触发推理过程长度的可控增加，而不会降低最终答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在人工智能领域具有重要意义，但目前对其安全性的研究尚不充分。本文旨在探索一种新的攻击向量，以评估LRMs的安全性。

Method: 本文提出了一种新颖的数据污染方法，通过可调触发器和相应冗长的链式思维（CoT）响应来实现攻击。教师语言模型被用来注入受控数量的冗余细化步骤到正确的推理过程中。

Result: 实验结果表明，本文提出的攻击方法能够可靠地触发推理过程长度的可控增加，而不会降低最终答案的正确性。此外，该攻击方法保持了输出的正确性，使其成为一种纯粹的资源消耗向量。

Conclusion: 本文提出了一种新的后门攻击方法，称为“过度思考后门”，可以精确控制模型的推理冗长度，同时保持输出的正确性。实验结果表明，该方法能够可靠地触发推理过程长度的可控增加，而不会降低最终答案的正确性。

Abstract: Large reasoning models (LRMs) have emerged as a significant advancement in
artificial intelligence, representing a specialized class of large language
models (LLMs) designed to tackle complex reasoning tasks. The defining
characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning
capabilities. In this paper, we identify a previously unexplored attack vector
against LRMs, which we term "overthinking backdoors". We advance this concept
by proposing a novel tunable backdoor, which moves beyond simple on/off attacks
to one where an attacker can precisely control the extent of the model's
reasoning verbosity. Our attack is implemented through a novel data poisoning
methodology. It pairs a tunable trigger-where the number of repetitions signals
the desired intensity-with a correspondingly verbose CoT response. These
responses are programmatically generated by instructing a teacher LLM to inject
a controlled number of redundant refinement steps into a correct reasoning
process. The approach preserves output correctness, which ensures stealth and
establishes the attack as a pure resource-consumption vector. Extensive
empirical results on various LRMs demonstrate that our method can reliably
trigger a controllable, multi-fold increase in the length of the reasoning
process, without degrading the final answer's correctness. Our source code is
available at https://github.com/FZaKK/BadReasoner.

</details>


### [30] [Uncertainty Quantification for Evaluating Machine Translation Bias](https://arxiv.org/abs/2507.18338)
*Ieva Raminta Staliūnaitė,Julius Cheng,Andreas Vlachos*

Main category: cs.CL

TL;DR: 研究发现，MT模型在性别不明确的情况下应保持不确定性，而不仅仅是依赖上下文或外部知识进行性别推断。


<details>
  <summary>Details</summary>
Motivation: 研究MT模型在性别推断方面的偏差行为，并探索如何改进模型在性别不明确情况下的表现。

Method: 使用最近提出的语义不确定性度量来分析MT模型在处理性别明确和不明确句子时的表现。

Result: 高翻译和性别准确性的模型在处理性别不明确的句子时，并不一定表现出预期的不确定性。此外，去偏方法对性别明确和不明确的翻译实例有独立的影响。

Conclusion: MT模型在处理性别不明确的词汇时，应保持不确定性，而不仅仅是依赖上下文或外部知识进行性别推断。

Abstract: In machine translation (MT), when the source sentence includes a lexeme whose
gender is not overtly marked, but whose target-language equivalent requires
gender specification, the model must infer the appropriate gender from the
context and/or external knowledge. Studies have shown that MT models exhibit
biased behaviour, relying on stereotypes even when they clash with contextual
information. We posit that apart from confidently translating using the correct
gender when it is evident from the input, models should also maintain
uncertainty about the gender when it is ambiguous. Using recently proposed
metrics of semantic uncertainty, we find that models with high translation and
gender accuracy on unambiguous instances do not necessarily exhibit the
expected level of uncertainty in ambiguous ones. Similarly, debiasing has
independent effects on ambiguous and unambiguous translation instances.

</details>


### [31] [TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning](https://arxiv.org/abs/2507.18340)
*Yifu Chen,Bingchen Huang,Zhiling Wang,Yuanchao Du,Junfeng Luo,Lei Shen,Zhineng chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为 TDR 的新框架，用于改进基于少量示例的 ICL 方法。TDR 通过解耦不同任务的示例并利用 LLM 的反馈来提升检索效果，实验表明其性能优越且易于集成。


<details>
  <summary>Details</summary>
Motivation: 现有的 ICL 方法在检索高质量示例方面面临两个挑战：难以区分跨任务的数据分布，以及难以建立检索器输出与 LLM 反馈之间的细粒度联系。

Method: TDR 通过将不同任务的 ICL 示例解耦，并利用 LLM 的细粒度反馈来监督和指导检索模块的训练，从而实现高质示例的检索。

Result: TDR 在 30 个 NLP 任务的实验中表现出色，提高了所有数据集的结果，并达到了最先进的性能。

Conclusion: TDR 是一种有效的框架，能够提高基于少量输入输出示例的 ICL 方法的效果，并且可以轻松与各种 LLM 结合使用。

Abstract: In-context learning (ICL) has become a classic approach for enabling LLMs to
handle various tasks based on a few input-output examples. The effectiveness of
ICL heavily relies on the quality of these examples, and previous works which
focused on enhancing example retrieval capabilities have achieved impressive
performances. However, two challenges remain in retrieving high-quality
examples: (1) Difficulty in distinguishing cross-task data distributions, (2)
Difficulty in making the fine-grained connection between retriever output and
feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR
decouples the ICL examples from different tasks, which enables the retrieval
module to retrieve examples specific to the target task within a multi-task
dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise
and guide the training of the retrieval module, which helps to retrieve
high-quality examples. We conducted extensive experiments on a suite of 30 NLP
tasks, the results demonstrate that TDR consistently improved results across
all datasets and achieves state-of-the-art performance. Meanwhile, our approach
is a plug-and-play method, which can be easily combined with various LLMs to
improve example retrieval abilities for ICL. The code is available at
https://github.com/Nnn-s/TDR.

</details>


### [32] [Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence](https://arxiv.org/abs/2507.18343)
*Ariana Sahitaj,Premtim Sahitaj,Veronika Solopova,Jiaao Li,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文介绍了一种结合人类专业知识和大型语言模型辅助的新框架，以提高虚假信息检测的注释一致性和可扩展性，并展示了显著的改进结果。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体上进行虚假信息检测仍然具有挑战性，因为任务复杂性和高质量标记数据有限。

Method: 我们提出了一种结合人类专业知识和大型语言模型（LLM）辅助的新框架，以提高注释的一致性和可扩展性。我们还提出了一个层次化分类法，并实施了一个LLM辅助的预注释流程，以及对较小的语言模型进行微调以执行结构化注释。

Result: 通过二次人工验证研究，显著提高了协议和时间效率。此外，通过使用高质量的LLM生成数据进行微调，使得较小的语言模型能够学习生成这些注释。

Conclusion: 我们的工作为可扩展和稳健的虚假信息检测系统的开发做出了贡献，支持与SDG 16一致的透明和负责任的媒体生态系统。

Abstract: Propaganda detection on social media remains challenging due to task
complexity and limited high-quality labeled data. This paper introduces a novel
framework that combines human expertise with Large Language Model (LLM)
assistance to improve both annotation consistency and scalability. We propose a
hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into
three broader categories, conduct a human annotation study on the HQP dataset
that reveals low inter-annotator agreement for fine-grained labels, and
implement an LLM-assisted pre-annotation pipeline that extracts propagandistic
spans, generates concise explanations, and assigns local labels as well as a
global label. A secondary human verification study shows significant
improvements in both agreement and time-efficiency. Building on this, we
fine-tune smaller language models (SLMs) to perform structured annotation.
Instead of fine-tuning on human annotations, we train on high-quality
LLM-generated data, allowing a large model to produce these annotations and a
smaller model to learn to generate them via knowledge distillation. Our work
contributes towards the development of scalable and robust propaganda detection
systems, supporting the idea of transparent and accountable media ecosystems in
line with SDG 16. The code is publicly available at our GitHub repository.

</details>


### [33] [CLEAR: Error Analysis via LLM-as-a-Judge Made Easy](https://arxiv.org/abs/2507.18392)
*Asaf Yehudai,Lilach Eden,Yotam Perlitz,Roy Bar-Haim,Michal Shmueli-Scheuer*

Main category: cs.CL

TL;DR: CLEAR is an interactive package for analyzing errors in LLMs, providing detailed feedback and visualizations to help improve model performance.


<details>
  <summary>Details</summary>
Motivation: Current evaluation paradigms for LLMs only provide single scores or rankings, which do not explain why a model performs a certain way. CLEAR aims to bridge this gap by offering detailed error analysis.

Method: CLEAR generates per-instance textual feedback, creates system-level error issues, quantifies their prevalence, and provides an interactive dashboard for analysis.

Result: CLEAR was demonstrated on RAG and Math benchmarks, and its utility was showcased through a user case study.

Conclusion: CLEAR provides a comprehensive error analysis tool for LLMs, enabling users to understand the specific issues affecting model performance and improve their models accordingly.

Abstract: The evaluation of Large Language Models (LLMs) increasingly relies on other
LLMs acting as judges. However, current evaluation paradigms typically yield a
single score or ranking, answering which model is better but not why. While
essential for benchmarking, these top-level scores obscure the specific,
actionable reasons behind a model's performance. To bridge this gap, we
introduce CLEAR, an interactive, open-source package for LLM-based error
analysis. CLEAR first generates per-instance textual feedback, then it creates
a set of system-level error issues, and quantifies the prevalence of each
identified issue. Our package also provides users with an interactive dashboard
that allows for a comprehensive error analysis through aggregate
visualizations, applies interactive filters to isolate specific issues or score
ranges, and drills down to the individual instances that exemplify a particular
behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,
and showcase its utility through a user case study.

</details>


### [34] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: 本研究调查了维基百科结构化内容中的跨语言不一致性，特别是表格数据，开发了一种方法来收集、对齐和分析这些表格，并使用定量和定性指标评估多语言对齐。


<details>
  <summary>Details</summary>
Motivation: 维基百科的不同版本独立编写和更新，导致事实不一致，影响百科全书的中立性和可靠性，以及依赖维基百科作为主要训练数据的AI系统的可靠性。

Method: 开发了一种方法来收集、对齐和分析跨语言维基百科文章中的表格，定义了不一致性的类别，并使用定量和定性指标评估多语言对齐。

Result: 通过样本数据集应用各种定量和定性指标来评估多语言对齐，发现了跨语言不一致性的类型。

Conclusion: 研究结果对事实验证、多语言知识交互以及设计依赖维基百科内容的可靠AI系统具有重要意义。

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [35] [FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs](https://arxiv.org/abs/2507.18417)
*Giorgos Iacovides,Wuyang Zhou,Danilo Mandic*

Main category: cs.CL

TL;DR: FinDPO is a new finance-specific LLM framework that improves upon existing models by using Direct Preference Optimization and a 'logit-to-score' conversion to achieve better performance in financial sentiment analysis.


<details>
  <summary>Details</summary>
Motivation: The SFT paradigm can lead to memorization of the training data and often fails to generalize to unseen samples, which is a critical limitation in financial domains where models must adapt to previously unobserved events and nuanced, domain-specific language.

Method: FinDPO is a finance-specific LLM framework based on post-training human preference alignment via Direct Preference Optimization (DPO). It integrates a fine-tuned causal LLM into realistic portfolio strategies through a novel 'logit-to-score' conversion.

Result: FinDPO achieves state-of-the-art performance on standard sentiment classification benchmarks, outperforming existing supervised fine-tuned models by 11% on average. It maintains substantial positive returns of 67% annually and a Sharpe ratio of 2.0, even under realistic transaction costs of 5 basis points.

Conclusion: FinDPO is the first sentiment-based approach to maintain substantial positive returns and strong risk-adjusted performance, even under realistic transaction costs.

Abstract: Opinions expressed in online finance-related textual data are having an
increasingly profound impact on trading decisions and market movements. This
trend highlights the vital role of sentiment analysis as a tool for quantifying
the nature and strength of such opinions. With the rapid development of
Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)
have become the de facto standard for financial sentiment analysis. However,
the SFT paradigm can lead to memorization of the training data and often fails
to generalize to unseen samples. This is a critical limitation in financial
domains, where models must adapt to previously unobserved events and the
nuanced, domain-specific language of finance. To this end, we introduce FinDPO,
the first finance-specific LLM framework based on post-training human
preference alignment via Direct Preference Optimization (DPO). The proposed
FinDPO achieves state-of-the-art performance on standard sentiment
classification benchmarks, outperforming existing supervised fine-tuned models
by 11% on the average. Uniquely, the FinDPO framework enables the integration
of a fine-tuned causal LLM into realistic portfolio strategies through a novel
'logit-to-score' conversion, which transforms discrete sentiment predictions
into continuous, rankable sentiment scores (probabilities). In this way,
simulations demonstrate that FinDPO is the first sentiment-based approach to
maintain substantial positive returns of 67% annually and strong risk-adjusted
performance, as indicated by a Sharpe ratio of 2.0, even under realistic
transaction costs of 5 basis points (bps).

</details>


### [36] [AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data](https://arxiv.org/abs/2507.18442)
*Rana Alshaikh,Israa Alghanmi,Shelan Jeawak*

Main category: cs.CL

TL;DR: 本研究提出了AraTable，一个用于评估大型语言模型在处理阿拉伯语表格数据时的推理和理解能力的新基准，并提供了一个自动化评估框架，性能接近人类评委。


<details>
  <summary>Details</summary>
Motivation: 由于阿拉伯语公共资源有限且语言特征独特，阿拉伯语在表格数据方面的基准测试仍然不足。为了弥补这一差距，我们提出了AraTable，这是一个新颖且全面的基准，旨在评估大型语言模型在应用到阿拉伯语表格数据时的推理和理解能力。

Method: 我们的方法遵循一个混合管道，其中初始内容由大型语言模型生成，然后由人类专家进行过滤和验证以确保数据集质量。我们还提出了一种完全自动化的评估框架，使用自我反思机制，其性能几乎与人类评委相同。

Result: 初步分析表明，虽然大型语言模型在简单的表格任务（如直接问答）上表现良好，但在需要更深层次推理和事实验证的任务上仍面临重大认知挑战。

Conclusion: 本研究提供了一个有价值的、公开可用的资源和评估框架，可以加速处理和分析阿拉伯语结构化数据的基础模型的发展。

Abstract: The cognitive and reasoning abilities of large language models (LLMs) have
enabled remarkable progress in natural language processing. However, their
performance in interpreting structured data, especially in tabular formats,
remains limited. Although benchmarks for English tabular data are widely
available, Arabic is still underrepresented because of the limited availability
of public resources and its unique language features. To address this gap, we
present AraTable, a novel and comprehensive benchmark designed to evaluate the
reasoning and understanding capabilities of LLMs when applied to Arabic tabular
data. AraTable consists of various evaluation tasks, such as direct question
answering, fact verification, and complex reasoning, involving a wide range of
Arabic tabular sources. Our methodology follows a hybrid pipeline, where
initial content is generated by LLMs and subsequently filtered and verified by
human experts to ensure high dataset quality. Initial analyses using AraTable
show that, while LLMs perform adequately on simpler tabular tasks such as
direct question answering, they continue to face significant cognitive
challenges when tasks require deeper reasoning and fact verification. This
indicates that there are substantial opportunities for future work to improve
performance on complex tabular reasoning tasks. We also propose a fully
automated evaluation framework that uses a self-deliberation mechanism and
achieves performance nearly identical to that of human judges. This research
provides a valuable, publicly available resource and evaluation framework that
can help accelerate the development of foundational models for processing and
analysing Arabic structured data.

</details>


### [37] [Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language](https://arxiv.org/abs/2507.18448)
*Md Obyedullahil Mamun,Md Adyelullahil Mamun,Arif Ahmad,Md. Imran Hossain Emu*

Main category: cs.CL

TL;DR: 本研究利用基于Transformer的模型XLM-RoBERTa-large来恢复无标点的孟加拉语文本中的标点符号，并通过构建大型训练语料库和数据增强技术克服了标注资源不足的问题。实验结果显示了模型在不同测试集上的高准确率，证明了其在实际应用中的有效性，并提供了公开的数据集和代码以支持未来的研究。


<details>
  <summary>Details</summary>
Motivation: 标点恢复可以提高文本的可读性，并对自动语音识别（ASR）的后处理任务至关重要，特别是在低资源语言如孟加拉语中。

Method: 本研究探索了基于Transformer的模型，特别是XLM-RoBERTa-large，用于自动恢复无标点的孟加拉语文本中的标点符号。为了应对标注资源的稀缺性，我们构建了一个大型且多样的训练语料库，并应用了数据增强技术。

Result: 最佳模型在新闻测试集上达到97.1%的准确率，在参考集上达到91.2%，在ASR集上达到90.2%。结果表明模型在参考和ASR转录本上具有良好的泛化能力，展示了其在现实世界、嘈杂场景中的有效性。

Conclusion: 本研究为孟加拉语标点恢复建立了强有力的基准，并通过公开的数据集和代码支持低资源自然语言处理的未来研究。

Abstract: Punctuation restoration enhances the readability of text and is critical for
post-processing tasks in Automatic Speech Recognition (ASR), especially for
low-resource languages like Bangla. In this study, we explore the application
of transformer-based models, specifically XLM-RoBERTa-large, to automatically
restore punctuation in unpunctuated Bangla text. We focus on predicting four
punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we
constructed a large, varied training corpus and applied data augmentation
techniques. Our best-performing model, trained with an augmentation factor of
alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts,
demonstrating the model's effectiveness in real-world, noisy scenarios. This
work establishes a strong baseline for Bangla punctuation restoration and
contributes publicly available datasets and code to support future research in
low-resource NLP.

</details>


### [38] [Generation of Synthetic Clinical Text: A Systematic Review](https://arxiv.org/abs/2507.18451)
*Basel Alshaikhdeeb,Ahmed Abdelmonem Hemedan,Soumyabrata Ghosh,Irina Balaur,Venkata Satagopam*

Main category: cs.CL

TL;DR: 本文对生成合成医学自由文本进行了系统性综述，分析了生成目的、技术和评估方法，并指出生成的合成文本在不同NLP任务中具有中等可能性，但仍需关注隐私问题。


<details>
  <summary>Details</summary>
Motivation: 生成临床合成文本是解决常见临床NLP问题（如稀疏性和隐私）的有效方案。本文旨在通过对生成合成医学自由文本的系统性综述，分析三个研究问题，以更好地理解生成技术及其应用。

Method: 本文进行了一项系统性综述，通过量化分析三个研究问题来评估生成合成医学自由文本的研究：(i) 生成的目的，(ii) 技术，以及(iii) 评估方法。搜索了PubMed、ScienceDirect、Web of Science、Scopus、IEEE、Google Scholar和arXiv数据库中的相关文献，并确定了94篇相关文章。

Result: 从2018年起，生成合成医学文本受到了广泛关注，其主要目的是文本增强、辅助写作、语料库构建、隐私保护、注释和实用性。Transformer架构，尤其是GPTs，是生成文本的主要技术。评估主要包括相似性、隐私、结构和实用性，其中实用性是最常用的评估方法。虽然生成的合成医学文本在不同下游NLP任务中表现出中等可能性，但它们已被证明是真实文档的有益补充，有助于提高准确性并克服稀疏性/欠采样问题。

Conclusion: 生成的合成医学文本在不同下游NLP任务中表现出作为真实医学文档的中等可能性，但隐私问题仍然是一个主要问题。尽管如此，生成合成医学文本的进展将显著加速工作流程和管道开发的采用，避免数据传输的时间消耗。

Abstract: Generating clinical synthetic text represents an effective solution for
common clinical NLP issues like sparsity and privacy. This paper aims to
conduct a systematic review on generating synthetic medical free-text by
formulating quantitative analysis to three research questions concerning (i)
the purpose of generation, (ii) the techniques, and (iii) the evaluation
methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,
Google Scholar, and arXiv databases for publications associated with generating
synthetic medical unstructured free-text. We have identified 94 relevant
articles out of 1,398 collected ones. A great deal of attention has been given
to the generation of synthetic medical text from 2018 onwards, where the main
purpose of such a generation is towards text augmentation, assistive writing,
corpus building, privacy-preserving, annotation, and usefulness. Transformer
architectures were the main predominant technique used to generate the text,
especially the GPTs. On the other hand, there were four main aspects of
evaluation, including similarity, privacy, structure, and utility, where
utility was the most frequent method used to assess the generated synthetic
medical text. Although the generated synthetic medical text demonstrated a
moderate possibility to act as real medical documents in different downstream
NLP tasks, it has proven to be a great asset as augmented, complementary to the
real documents, towards improving the accuracy and overcoming
sparsity/undersampling issues. Yet, privacy is still a major issue behind
generating synthetic medical text, where more human assessments are needed to
check for the existence of any sensitive information. Despite that, advances in
generating synthetic medical text will considerably accelerate the adoption of
workflows and pipeline development, discarding the time-consuming legalities of
data transfer.

</details>


### [39] [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)
*Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: GraDe 是一种新的方法，通过将稀疏依赖图整合到 LLM 的注意力机制中，解决了 LLM 在处理表格数据时的关键关系关注不足的问题，取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: LLM 在建模文本化特征-值对时表现出强大的潜力，但表格数据本质上具有稀疏的特征级依赖关系，这与 LLM 的自注意力机制存在根本性不匹配，导致在复杂依赖或语义模糊特征的数据集中，关键关系的关注度被稀释。

Method: GraDe 通过将稀疏依赖图显式集成到 LLM 的注意力机制中，采用了一个轻量级的动态图学习模块，该模块由外部提取的功能依赖关系引导，以优先考虑关键特征交互并抑制无关交互。

Result: 在多种现实世界数据集上的实验表明，GraDe 在复杂数据集上比现有的基于 LLM 的方法高出 12%，并在合成数据质量方面与最先进的方法具有竞争力。

Conclusion: GraDe 是一种有效且侵入性小的方法，为基于 LLM 的结构感知表格数据建模提供了实用解决方案。

Abstract: Large Language Models (LLMs) have shown strong potential for tabular data
generation by modeling textualized feature-value pairs. However, tabular data
inherently exhibits sparse feature-level dependencies, where many feature
interactions are structurally insignificant. This creates a fundamental
mismatch as LLMs' self-attention mechanism inevitably distributes focus across
all pairs, diluting attention on critical relationships, particularly in
datasets with complex dependencies or semantically ambiguous features. To
address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a
novel method that explicitly integrates sparse dependency graphs into LLMs'
attention mechanism. GraDe employs a lightweight dynamic graph learning module
guided by externally extracted functional dependencies, prioritizing key
feature interactions while suppressing irrelevant ones. Our experiments across
diverse real-world datasets demonstrate that GraDe outperforms existing
LLM-based approaches by up to 12% on complex datasets while achieving
competitive results with state-of-the-art approaches in synthetic data quality.
Our method is minimally intrusive yet effective, offering a practical solution
for structure-aware tabular data modeling with LLMs.

</details>


### [40] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 这项研究首次全面比较了最先进的LLM和微调的变压器在Twitter和Reddit数据集上的性能，发现任务特定的微调在道德推理应用中仍然优于提示方法。


<details>
  <summary>Details</summary>
Motivation: 道德基础检测对于分析社会话语和开发符合伦理的AI系统至关重要。虽然大型语言模型在各种任务中表现出色，但它们在专门的道德推理方面的表现仍不清楚。

Method: 使用ROC、PR和DET曲线分析对最先进的LLM和微调的变压器在Twitter和Reddit数据集上的性能进行了首次全面比较。

Result: 结果揭示了显著的性能差距，即使经过提示工程努力，LLM在检测道德内容方面仍表现出高假阴性率和系统性的低检测率。

Conclusion: 任务特定的微调在道德推理应用中仍然优于提示方法。

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


### [41] [Effective Multi-Task Learning for Biomedical Named Entity Recognition](https://arxiv.org/abs/2507.18542)
*João Ruano,Gonçalo M. Correia,Leonor Barreiros,Afonso Mendes*

Main category: cs.CL

TL;DR: 本文介绍了SRU-NER，一种新的方法用于处理生物医学命名实体识别中的嵌套实体，并通过多任务学习整合多个数据集。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别面临挑战，因为生物医学术语复杂且标注不一致。

Method: SRU-NER是一种基于槽位的循环单元NER方法，旨在处理嵌套命名实体，并通过有效的多任务学习策略整合多个数据集。

Result: SRU-NER通过广泛的实验，包括跨语料库评估和对模型预测的人工评估，取得了具有竞争力的性能。

Conclusion: SRU-NER在生物医学和通用领域NER任务中表现出色，并提高了跨领域泛化能力。

Abstract: Biomedical Named Entity Recognition presents significant challenges due to
the complexity of biomedical terminology and inconsistencies in annotation
across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),
a novel approach designed to handle nested named entities while integrating
multiple datasets through an effective multi-task learning strategy. SRU-NER
mitigates annotation gaps by dynamically adjusting loss computation to avoid
penalizing predictions of entity types absent in a given dataset. Through
extensive experiments, including a cross-corpus evaluation and human assessment
of the model's predictions, SRU-NER achieves competitive performance in
biomedical and general-domain NER tasks, while improving cross-domain
generalization.

</details>


### [42] [GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface](https://arxiv.org/abs/2507.18546)
*Urchade Zaratiana,Gil Pasternak,Oliver Boyd,George Hurn-Maloney,Ash Lewis*

Main category: cs.CL

TL;DR: GLiNER2是一个高效的统一框架，支持多种NLP任务，具有良好的性能和部署优势。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案通常需要为不同任务定制模型或依赖计算成本高昂的大语言模型，因此需要一种更高效、统一的框架来解决这些问题。

Method: GLiNER2基于预训练的Transformer编码器架构，通过直观的模式驱动接口引入多任务组合，同时保持CPU效率和紧凑的大小。

Result: 实验结果表明，GLiNER2在提取和分类任务中表现优异，并且在部署可访问性方面相比基于大语言模型的替代方案有显著改进。

Conclusion: GLiNER2是一个统一的框架，能够在单一高效模型中支持命名实体识别、文本分类和分层结构化数据提取。实验表明，GLiNER2在提取和分类任务中表现出色，并且相比基于大语言模型的替代方案，在部署可访问性方面有显著提升。

Abstract: Information extraction (IE) is fundamental to numerous NLP applications, yet
existing solutions often require specialized models for different tasks or rely
on computationally expensive large language models. We present GLiNER2, a
unified framework that enhances the original GLiNER architecture to support
named entity recognition, text classification, and hierarchical structured data
extraction within a single efficient model. Built pretrained transformer
encoder architecture, GLiNER2 maintains CPU efficiency and compact size while
introducing multi-task composition through an intuitive schema-based interface.
Our experiments demonstrate competitive performance across extraction and
classification tasks with substantial improvements in deployment accessibility
compared to LLM-based alternatives. We release GLiNER2 as an open-source
pip-installable library with pre-trained models and documentation at
https://github.com/fastino-ai/GLiNER2.

</details>


### [43] [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)
*Jiafeng Xiong,Yuting Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种名为GIIFT的多模态机器翻译框架，通过构建多模态场景图并使用跨模态图注意力网络适配器，在统一融合空间中学习多模态知识，并将其归纳推广到更广泛的图像无关翻译领域。实验结果表明，GIIFT在多个任务中表现优异，甚至在没有图像的情况下也能取得很好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态机器翻译方法在利用模态差距方面面临挑战，因为它们强制执行严格的视觉-语言对齐，并且受限于在其训练的多模态领域内进行推理。

Method: 构建了新颖的多模态场景图以保留和整合模态特定信息，并引入GIIFT框架，该框架使用跨模态图注意力网络适配器在统一融合空间中学习多模态知识，并将其归纳推广到更广泛的图像无关翻译领域。

Result: 在Multi30K数据集上的实验结果表明，GIIFT超越了现有方法，在英语到法语和英语到德语任务中达到了最先进的水平。在WMT基准测试中，GIIFT相对于图像无关翻译基线有显著改进。

Conclusion: GIIFT在多模态机器翻译任务中表现出色，即使在没有图像的情况下也能进行归纳推理，展示了其在图像无关翻译中的强大能力。

Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of
visual information in machine translation. However, existing MMT methods face
challenges in leveraging the modality gap by enforcing rigid visual-linguistic
alignment whilst being confined to inference within their trained multimodal
domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage
Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph
Attention Network adapter to learn multimodal knowledge in a unified fused
space and inductively generalize it to broader image-free translation domains.
Experimental results on the Multi30K dataset of English-to-French and
English-to-German tasks demonstrate that our GIIFT surpasses existing
approaches and achieves the state-of-the-art, even without images during
inference. Results on the WMT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards
inductive image-free inference.

</details>


### [44] [Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods](https://arxiv.org/abs/2507.18570)
*Ganesh Sapkota,Md Hasibur Rahman*

Main category: cs.CL

TL;DR: 本文提出了一种混合分词策略，结合6-mer分词与BPE-600，提升了DNA语言模型的性能，证明了其在捕捉局部和全局DNA模式方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统k-mer分词在捕捉局部DNA结构方面有效，但面临分词分布不均和对全局序列上下文理解有限的问题。

Method: 本文提出了一种混合分词策略，结合6-mer分词与Byte Pair Encoding (BPE-600)，以平衡词汇表并捕捉DNA序列的局部和全局模式。

Result: 基于这种混合词汇表训练的基础DLM在next-k-mer预测任务中表现出显著提升的性能，准确率分别为3-mers的10.78%、4-mers的10.1%和5-mers的4.12%，优于NT、DNABERT2和GROVER等最先进的模型。

Conclusion: 本文展示了混合分词策略在DNA语言模型中的有效性，强调了先进分词方法在基因组语言建模中的重要性，并为未来的DNA序列分析和生物研究奠定了坚实的基础。

Abstract: This paper presents a novel hybrid tokenization strategy that enhances the
performance of DNA Language Models (DLMs) by combining 6-mer tokenization with
Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at
capturing local DNA sequence structures but often faces challenges, including
uneven token distribution and a limited understanding of global sequence
context. To address these limitations, we propose merging unique 6mer tokens
with optimally selected BPE tokens generated through 600 BPE cycles. This
hybrid approach ensures a balanced and context-aware vocabulary, enabling the
model to capture both short and long patterns within DNA sequences
simultaneously. A foundational DLM trained on this hybrid vocabulary was
evaluated using next-k-mer prediction as a fine-tuning task, demonstrating
significantly improved performance. The model achieved prediction accuracies of
10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming
state-of-the-art models such as NT, DNABERT2, and GROVER. These results
highlight the ability of the hybrid tokenization strategy to preserve both the
local sequence structure and global contextual information in DNA modeling.
This work underscores the importance of advanced tokenization methods in
genomic language modeling and lays a robust foundation for future applications
in downstream DNA sequence analysis and biological research.

</details>


### [45] [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)
*Feng Hong,Geng Yu,Yushi Ye,Haicheng Huang,Huangjie Zheng,Ya Zhang,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: This paper introduces WINO, a training-free decoding algorithm for DLLMs that improves the quality-speed trade-off by enabling revokable decoding through a parallel draft-and-verify mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing DLLMs suffer from a severe quality-speed trade-off, where faster parallel decoding leads to significant performance degradation due to irreversibility and early error context accumulation.

Method: WINO is a training-free decoding algorithm that employs a parallel draft-and-verify mechanism, allowing for revokable decoding in DLLMs by using the model's bidirectional context to verify and refine suspicious tokens.

Result: WINO improves the quality-speed trade-off in DLLMs. For example, on the GSM8K math benchmark, it accelerates inference by 6× while improving accuracy by 2.58%; on Flickr30K captioning, it achieves a 10× speedup with higher performance.

Conclusion: WINO is shown to decisively improve the quality-speed trade-off in DLLMs, demonstrating superior performance in various benchmarks.

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a compelling
alternative to Autoregressive models, designed for fast parallel generation.
However, existing DLLMs are plagued by a severe quality-speed trade-off, where
faster parallel decoding leads to significant performance degradation. We
attribute this to the irreversibility of standard decoding in DLLMs, which is
easily polarized into the wrong decoding direction along with early error
context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),
a training-free decoding algorithm that enables revokable decoding in DLLMs.
WINO employs a parallel draft-and-verify mechanism, aggressively drafting
multiple tokens while simultaneously using the model's bidirectional context to
verify and re-mask suspicious ones for refinement. Verified in open-source
DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the
quality-speed trade-off. For instance, on the GSM8K math benchmark, it
accelerates inference by 6$\times$ while improving accuracy by 2.58%; on
Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance.
More comprehensive experiments are conducted to demonstrate the superiority and
provide an in-depth understanding of WINO.

</details>


### [46] [System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition](https://arxiv.org/abs/2507.18580)
*Jiahao Wang,Ramen Liu,Longhui Zhang,Jing Li*

Main category: cs.CL

TL;DR: 本文提出SRAG-MAV框架，用于细粒度中文仇恨言论识别，显著提升了识别效果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决细粒度中文仇恨言论识别问题，通过改进现有方法提升识别效果。

Method: 我们提出了SRAG-MAV框架，结合了任务重定义(TR)、自检索增强生成(SRAG)和多轮累积投票(MAV)，通过动态检索训练集创建上下文提示，并应用多轮推理与投票以提高输出稳定性和性能。

Result: 基于Qwen2.5-7B模型的系统在STATE ToxiCN数据集上取得了Hard Score 26.66，Soft Score 48.35，Average Score 37.505的成绩，优于GPT-4o和微调的Qwen2.5-7B。

Conclusion: 我们的系统在STATE ToxiCN数据集上显著优于基线模型，如GPT-4o和微调的Qwen2.5-7B。

Abstract: This paper presents our system for CCL25-Eval Task 10, addressing
Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel
SRAG-MAV framework that synergistically integrates task reformulation(TR),
Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting
(MAV). Our method reformulates the quadruplet extraction task into triplet
extraction, uses dynamic retrieval from the training set to create contextual
prompts, and applies multi-round inference with voting to improve output
stability and performance. Our system, based on the Qwen2.5-7B model, achieves
a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on
the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o
(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The
code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.

</details>


### [47] [AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs](https://arxiv.org/abs/2507.18584)
*Xiaopeng Ke,Hexuan Deng,Xuebo Liu,Jun Rao,Zhenxi Song,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: AQuilt is a framework for constructing instruction-tuning data for specialized domains, which improves model performance while reducing costs.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for improving the performance of large language models in specialized domains often incur high computational costs or suffer from performance limitations. AQuilt aims to address these challenges by providing an efficient and effective framework for constructing instruction-tuning data.

Method: AQuilt constructs instruction-tuning data for specialized domains using unlabeled data, incorporating logic and inspection to encourage reasoning processes and self-inspection. It also allows for customizable task instructions to generate high-quality data.

Result: AQuilt was used to construct a dataset of 703k examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. The generated data exhibits higher relevance to downstream tasks.

Conclusion: AQuilt is a framework for constructing instruction-tuning data for specialized domains, and it demonstrates comparable performance to DeepSeek-V3 while reducing production costs. The generated data also shows higher relevance to downstream tasks.

Abstract: Despite the impressive performance of large language models (LLMs) in general
domains, they often underperform in specialized domains. Existing approaches
typically rely on data synthesis methods and yield promising results by using
unlabeled data to capture domain-specific features. However, these methods
either incur high computational costs or suffer from performance limitations,
while also demonstrating insufficient generalization across different tasks. To
address these challenges, we propose AQuilt, a framework for constructing
instruction-tuning data for any specialized domains from corresponding
unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,
and Task type. By incorporating logic and inspection, we encourage reasoning
processes and self-inspection to enhance model performance. Moreover,
customizable task instructions enable high-quality data generation for any
task. As a result, we construct a dataset of 703k examples to train a powerful
data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3
while utilizing just 17% of the production cost. Further analysis demonstrates
that our generated data exhibits higher relevance to downstream tasks. Source
code, models, and scripts are available at https://github.com/Krueske/AQuilt.

</details>


### [48] [TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards](https://arxiv.org/abs/2507.18618)
*Andreea Nica,Ivan Zakazov,Nicolas Mario Baldwin,Saibo Geng,Robert West*

Main category: cs.CL

TL;DR: 本文介绍了TRPrompt框架，它通过直接整合文本反馈到提示模型的训练中，统一了现有的方法，提高了提示的质量和效果。该框架无需预先收集数据集，并能通过生成的提示反馈不断改进，最终在挑战性数学数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 当前的研究主要集中在两种方向：一种是使用文本反馈以无训练的方式从通用LLM中引出改进的提示，另一种是依赖数值奖励来训练专门的提示模型，为目标模型提供最佳提示。本文旨在统一这两种方法，通过直接整合文本反馈到提示模型的训练中，提高提示的质量和效果。

Method: 本文提出了一种名为Textual Reward Prompt（TRPrompt）的框架，该框架通过直接将文本反馈整合到提示模型的训练中，统一了现有的方法。该框架不需要预先收集数据集，并且可以不断通过生成的提示反馈进行改进。

Result: 通过将文本反馈直接整合到提示模型的训练中，TRPrompt框架能够在不依赖预先收集数据集的情况下，不断改进生成的提示。结合LLM内部化“好”提示的概念的能力，该框架能够训练出针对挑战性数学数据集GSMHard和MATH的问题的最先进查询特定提示的提示模型。

Conclusion: 本文提出了Textual Reward Prompt框架（TRPrompt），该框架通过直接将文本反馈整合到提示模型的训练中，统一了现有的方法。该框架不需要预先收集数据集，并且可以不断通过生成的提示反馈进行改进。结合LLM内部化“好”提示的概念的能力，高分辨率的文本奖励信号使我们能够训练出针对挑战性数学数据集GSMHard和MATH的问题的最先进查询特定提示的提示模型。

Abstract: Prompt optimization improves the reasoning abilities of large language models
(LLMs) without requiring parameter updates to the target model. Following
heuristic-based "Think step by step" approaches, the field has evolved in two
main directions: while one group of methods uses textual feedback to elicit
improved prompts from general-purpose LLMs in a training-free way, a concurrent
line of research relies on numerical rewards to train a special prompt model,
tailored for providing optimal prompts to the target model. In this paper, we
introduce the Textual Reward Prompt framework (TRPrompt), which unifies these
approaches by directly incorporating textual feedback into training of the
prompt model. Our framework does not require prior dataset collection and is
being iteratively improved with the feedback on the generated prompts. When
coupled with the capacity of an LLM to internalize the notion of what a "good"
prompt is, the high-resolution signal provided by the textual rewards allows us
to train a prompt model yielding state-of-the-art query-specific prompts for
the problems from the challenging math datasets GSMHard and MATH.

</details>


### [49] [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)
*Vijay Viswanathan,Yanchao Sun,Shuang Ma,Xiang Kong,Meng Cao,Graham Neubig,Tongshuang Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习方法RLCF，通过使用检查清单反馈来提升语言模型对用户指令的理解和遵循能力，并在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习通常使用固定的准则，如“有帮助性”和“无害性”，而本文旨在使用灵活的、特定于指令的准则来扩大强化学习在激发指令遵循方面的影响力。

Method: 提出了一种名为“从检查清单反馈中进行强化学习”(RLCF)的方法，通过从指令中提取检查清单并评估响应满足每个项目的程度来计算奖励。

Result: RLCF在五个广泛研究的基准测试中表现优于其他对Qwen2.5-7B-Instruct模型进行对齐的方法，包括在FollowBench上提高了4个百分点，在InFoBench上提高了6个百分点，在Arena-Hard上提高了3个百分点。

Conclusion: 检查清单反馈是提高语言模型支持表达多种需求查询的关键工具。

Abstract: Language models must be adapted to understand and follow user instructions.
Reinforcement learning is widely used to facilitate this -- typically using
fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead
propose using flexible, instruction-specific criteria as a means of broadening
the impact that reinforcement learning can have in eliciting instruction
following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF).
From instructions, we extract checklists and evaluate how well responses
satisfy each item - using both AI judges and specialized verifier programs -
then combine these scores to compute rewards for RL. We compare RLCF with other
alignment methods applied to a strong instruction following model
(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only
method to improve performance on every benchmark, including a 4-point boost in
hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a
3-point rise in win rate on Arena-Hard. These results establish checklist
feedback as a key tool for improving language models' support of queries that
express a multitude of needs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 本文介绍了一个代理AI框架，用于自动化医疗数据流程，从摄取到推理，通过模块化代理处理结构化和非结构化数据，实现自动特征选择、模型选择和预处理建议，并在多个数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健中构建和部署机器学习解决方案仍然昂贵且劳动密集，因为预处理工作流碎片化、模型兼容性问题和严格的数据隐私约束。

Method: 引入了一个代理AI框架，通过一组模块化、任务特定的代理自动完成整个临床数据流程，从摄取到推理。这些代理处理结构化和非结构化数据，实现自动特征选择、模型选择和预处理建议。

Result: 在公开可用的老年人医学、姑息治疗和结肠镜检查影像数据集上评估了该系统。例如，在结构化数据（焦虑数据）和非结构化数据（结肠镜检查息肉数据）的情况下，流程从摄入标识符代理进行文件类型检测开始，然后由数据匿名化代理确保隐私合规性。

Conclusion: 通过自动化机器学习生命周期中的高摩擦阶段，所提出的框架减少了对重复专家干预的需求，为临床环境中操作AI提供了一种可扩展、成本效益高的途径。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [51] [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
*Shanghai AI Lab,:,Yicheng Bao,Guanxu Chen,Mingkang Chen,Yunhao Chen,Chiyu Chen,Lingjie Chen,Sirui Chen,Xinquan Chen,Jie Cheng,Yu Cheng,Dengke Deng,Yizhuo Ding,Dan Ding,Xiaoshan Ding,Yi Ding,Zhichen Dong,Lingxiao Du,Yuyu Fan,Xinshun Feng,Yanwei Fu,Yuxuan Gao,Ruijun Ge,Tianle Gu,Lujun Gui,Jiaxuan Guo,Qianxi He,Yuenan Hou,Xuhao Hu,Hong Huang,Kaichen Huang,Shiyang Huang,Yuxian Jiang,Shanzhe Lei,Jie Li,Lijun Li,Hao Li,Juncheng Li,Xiangtian Li,Yafu Li,Lingyu Li,Xueyan Li,Haotian Liang,Dongrui Liu,Qihua Liu,Zhixuan Liu,Bangwei Liu,Huacan Liu,Yuexiao Liu,Zongkai Liu,Chaochao Lu,Yudong Lu,Xiaoya Lu,Zhenghao Lu,Qitan Lv,Caoyuan Ma,Jiachen Ma,Xiaoya Ma,Zhongtian Ma,Lingyu Meng,Ziqi Miao,Yazhe Niu,Yuezhang Peng,Yuan Pu,Han Qi,Chen Qian,Xingge Qiao,Jingjing Qu,Jiashu Qu,Wanying Qu,Wenwen Qu,Xiaoye Qu,Qihan Ren,Qingnan Ren,Qingyu Ren,Jing Shao,Wenqi Shao,Shuai Shao,Dongxing Shi,Xin Song,Xinhao Song,Yan Teng,Xuan Tong,Yingchun Wang,Xuhong Wang,Shujie Wang,Xin Wang,Yige Wang,Yixu Wang,Yuanfu Wang,Futing Wang,Ruofan Wang,Wenjie Wang,Yajie Wang,Muhao Wei,Xiaoyu Wen,Fenghua Weng,Yuqi Wu,Yingtong Xiong,Xingcheng Xu,Chao Yang,Yue Yang,Yang Yao,Yulei Ye,Zhenyun Yin,Yi Yu,Bo Zhang,Qiaosheng Zhang,Jinxuan Zhang,Yexin Zhang,Yinqiang Zheng,Hefeng Zhou,Zhanhui Zhou,Pengyu Zhu,Qingzi Zhu,Yubo Zhu,Bowen Zhou*

Main category: cs.AI

TL;DR: SafeWork-R1, developed with the SafeLadder framework, improves safety performance while maintaining general capabilities, showcasing the potential for co-evolving safety and capability in AI models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a multimodal reasoning model that demonstrates the coevolution of capabilities and safety, addressing the limitations of previous alignment methods such as RLHF.

Method: SafeWork-R1 is developed using the SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. It also implements two distinct inference-time intervention methods and a deliberative search mechanism for step-level verification.

Result: SafeWork-R1 achieves an average improvement of 46.54% over its base model Qwen2.5-VL-72B on safety-related benchmarks without compromising general capabilities, delivering state-of-the-art safety performance compared to leading proprietary models.

Conclusion: SafeWork-R1 demonstrates that safety and capability can co-evolve synergistically, highlighting the generalizability of the SafeLadder framework in building robust, reliable, and trustworthy general-purpose AI.

Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
*Shubham Toshniwal,Ivan Sorokin,Aleksander Ficek,Ivan Moshkov,Igor Gitman*

Main category: cs.LG

TL;DR: GenSelect通过让LLM进行长期推理来选择最佳方案，从而有效利用了LLM的比较优势，并在数学推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在使用点对点评分或成对比较时存在不足，点对点方法未能充分利用LLM的比较能力，而成对方法在更大的采样预算下效率低下。

Method: GenSelect通过让LLM进行长期推理，从N个候选方案中选择最佳方案。

Result: 在数学推理中，如QwQ和DeepSeek-R1-0528等推理模型在GenSelect中表现优异，超过了现有的简单提示评分方法。

Conclusion: GenSelect利用LLM的比较优势，在数学推理中表现出色，优于现有的评分方法。

Abstract: Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.

</details>


### [53] [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)
*Chujie Zheng,Shixuan Liu,Mingze Li,Xiong-Hui Chen,Bowen Yu,Chang Gao,Kai Dang,Yuqiong Liu,Rui Men,An Yang,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: GSPO is a new reinforcement learning algorithm that improves training efficiency and stability for large language models.


<details>
  <summary>Details</summary>
Motivation: To develop a stable, efficient, and performant reinforcement learning algorithm for training large language models.

Method: GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization.

Result: GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training.

Conclusion: GSPO has contributed to the remarkable improvements in the latest Qwen3 models and has the potential for simplifying the design of RL infrastructure.

Abstract: This paper introduces Group Sequence Policy Optimization (GSPO), our stable,
efficient, and performant reinforcement learning algorithm for training large
language models. Unlike previous algorithms that adopt token-level importance
ratios, GSPO defines the importance ratio based on sequence likelihood and
performs sequence-level clipping, rewarding, and optimization. We demonstrate
that GSPO achieves superior training efficiency and performance compared to the
GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and
has the potential for simplifying the design of RL infrastructure. These merits
of GSPO have contributed to the remarkable improvements in the latest Qwen3
models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [54] [RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models](https://arxiv.org/abs/2507.18053)
*Haoran Gao,Yuanhe Zhang,Zhenhong Zhou,Lei Jiang,Fanyu Meng,Yujia Xiao,Kun Wang,Yang Liu,Junlan Feng*

Main category: cs.CR

TL;DR: RECALLED是第一个利用视觉模态触发无限制资源消耗攻击（RCAs）的红队方法，通过视觉引导优化和多目标并行损失生成对抗性扰动，显著增加了服务延迟和资源消耗，揭示了大型视觉语言模型的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有的红队研究在很大程度上忽略了视觉输入作为潜在攻击面，导致对LVLMs中RCAs的缓解策略不足。

Method: 我们提出了RECALLED，这是第一个利用视觉模态触发无限制RCAs红队的方法。首先，我们介绍了Vision Guided Optimization，这是一种细粒度的像素级优化，以获得Output Recall对抗性扰动，这些扰动可以引发重复输出。然后，我们将扰动注入视觉输入中，触发无限制生成以实现RCAs的目标。此外，我们引入了Multi-Objective Parallel Losses来生成通用攻击模板并解决并行攻击时的优化冲突。

Result: 实证结果表明，RECALLED使服务响应延迟增加了26%，导致GPU利用率和内存消耗额外增加了20%。

Conclusion: 我们的研究揭示了LVLMs中的安全漏洞，并建立了一个红队框架，可以促进未来针对RCAs的防御开发。

Abstract: Resource Consumption Attacks (RCAs) have emerged as a significant threat to
the deployment of Large Language Models (LLMs). With the integration of vision
modalities, additional attack vectors exacerbate the risk of RCAs in large
vision-language models (LVLMs). However, existing red-teaming studies have
largely overlooked visual inputs as a potential attack surface, resulting in
insufficient mitigation strategies against RCAs in LVLMs. To address this gap,
we propose RECALLED (\textbf{RE}source \textbf{C}onsumption \textbf{A}ttack on
\textbf{L}arge Vision-\textbf{L}anguag\textbf{E} Mo\textbf{D}els), the first
approach for exploiting visual modalities to trigger unbounded RCAs
red-teaming. First, we present \textit{Vision Guided Optimization}, a
fine-grained pixel-level optimization, to obtain \textit{Output Recall}
adversarial perturbations, which can induce repeating output. Then, we inject
the perturbations into visual inputs, triggering unbounded generations to
achieve the goal of RCAs. Additionally, we introduce \textit{Multi-Objective
Parallel Losses} to generate universal attack templates and resolve
optimization conflicts when intending to implement parallel attacks. Empirical
results demonstrate that RECALLED increases service response latency by over 26
$\uparrow$, resulting in an additional 20\% increase in GPU utilization and
memory consumption. Our study exposes security vulnerabilities in LVLMs and
establishes a red-teaming framework that can facilitate future defense
development against RCAs.

</details>


### [55] [LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models](https://arxiv.org/abs/2507.18302)
*Delong Ran,Xinlei He,Tianshuo Cong,Anyu Wang,Qi Li,Xiaoyun Wang*

Main category: cs.CR

TL;DR: 我们的研究揭示了基于LoRA的微调语言模型仍然容易受到会员推理攻击的影响，并提出了一个全面的评估框架LoRA-Leak来评估这种风险。我们还探索了四种防御方法，并发现只有dropout和在微调期间排除特定的LM层才能有效缓解MIAs风险并保持效用。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA微调数据可能被认为对MIAs具有抵抗力，但我们发现利用预训练模型会导致更多的信息泄露，而现有的MIAs忽略了这一点。因此，我们需要一个全面的评估框架来评估LoRA-based LMs的MIAs风险。

Method: 我们引入了LoRA-Leak，这是一个全面的MIAs评估框架，用于评估LM的微调数据集。LoRA-Leak包括十五种会员推理攻击，包括十种现有的MIAs和五种改进的MIAs，这些MIAs利用预训练模型作为参考。

Result: 我们在三个先进的LM上应用LoRA-Leak，展示了基于LoRA的微调LM仍然容易受到MIAs的影响（例如，在保守的微调设置下AUC为0.775）。我们还应用LoRA-Leak到不同的微调设置中，以了解由此产生的隐私风险。

Conclusion: 我们的研究结果表明，在“预训练和微调”范式下，预训练模型的存在使MIAs成为LoRA-based LMs更严重的风险。我们希望我们的发现能为专业LM提供商的数据隐私保护提供指导。

Abstract: Language Models (LMs) typically adhere to a "pre-training and fine-tuning"
paradigm, where a universal pre-trained model can be fine-tuned to cater to
various specialized domains. Low-Rank Adaptation (LoRA) has gained the most
widespread use in LM fine-tuning due to its lightweight computational cost and
remarkable performance. Because the proportion of parameters tuned by LoRA is
relatively small, there might be a misleading impression that the LoRA
fine-tuning data is invulnerable to Membership Inference Attacks (MIAs).
However, we identify that utilizing the pre-trained model can induce more
information leakage, which is neglected by existing MIAs. Therefore, we
introduce LoRA-Leak, a holistic evaluation framework for MIAs against the
fine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership
inference attacks, including ten existing MIAs, and five improved MIAs that
leverage the pre-trained model as a reference. In experiments, we apply
LoRA-Leak to three advanced LMs across three popular natural language
processing tasks, demonstrating that LoRA-based fine-tuned LMs are still
vulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).
We also applied LoRA-Leak to different fine-tuning settings to understand the
resulting privacy risks. We further explore four defenses and find that only
dropout and excluding specific LM layers during fine-tuning effectively
mitigate MIA risks while maintaining utility. We highlight that under the
"pre-training and fine-tuning" paradigm, the existence of the pre-trained model
makes MIA a more severe risk for LoRA-based LMs. We hope that our findings can
provide guidance on data privacy protection for specialized LM providers.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [56] [Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges](https://arxiv.org/abs/2507.18161)
*Samuele Cornell,Christoph Boeddeker,Taejin Park,He Huang,Desh Raj,Matthew Wiesner,Yoshiki Masuyama,Xuankai Chang,Zhong-Qiu Wang,Stefano Squartini,Paola Garcia,Shinji Watanabe*

Main category: eess.AS

TL;DR: CHiME-7和8挑战聚焦于多通道、通用的联合自动语音识别和语音分离，研究发现大多数参与者使用端到端ASR系统，但准确转录具有挑战性的声学环境中的自发语音仍然困难。


<details>
  <summary>Details</summary>
Motivation: 研究多通道、通用的联合自动语音识别（ASR）和语音分离的最新进展，以推动该领域的发展。

Method: 分析了CHiME-7和8挑战的设计、评估指标、数据集和基线系统，并研究了参与者的提交趋势。

Result: 大多数参与者使用端到端（e2e）ASR系统，而混合系统在之前的CHiME挑战中更为普遍；所有最佳系统都采用目标说话人语音分离技术；下游评估通过会议摘要与转录质量弱相关。

Conclusion: 准确转录具有挑战性的声学环境中的自发语音仍然很困难，即使使用计算密集型系统集成。

Abstract: The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on
multi-channel, generalizable, joint automatic speech recognition (ASR) and
diarization of conversational speech. With participation from 9 teams
submitting 32 diverse systems, these challenges have contributed to
state-of-the-art research in the field. This paper outlines the challenges'
design, evaluation metrics, datasets, and baseline systems while analyzing key
trends from participant submissions. From this analysis it emerges that: 1)
Most participants use end-to-end (e2e) ASR systems, whereas hybrid systems were
prevalent in previous CHiME challenges. This transition is mainly due to the
availability of robust large-scale pre-trained models, which lowers the data
burden for e2e-ASR. 2) Despite recent advances in neural speech separation and
enhancement (SSE), all teams still heavily rely on guided source separation,
suggesting that current neural SSE techniques are still unable to reliably deal
with complex scenarios and different recording setups. 3) All best systems
employ diarization refinement via target-speaker diarization techniques.
Accurate speaker counting in the first diarization pass is thus crucial to
avoid compounding errors and CHiME-8 DASR participants especially focused on
this part. 4) Downstream evaluation via meeting summarization can correlate
weakly with transcription quality due to the remarkable effectiveness of
large-language models in handling errors. On the NOTSOFAR-1 scenario, even
systems with over 50\% time-constrained minimum permutation WER can perform
roughly on par with the most effective ones (around 11\%). 5) Despite recent
progress, accurately transcribing spontaneous speech in challenging acoustic
environments remains difficult, even when using computationally intensive
system ensembles.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures](https://arxiv.org/abs/2507.18009)
*Jake R. Patock,Nicole Catherine Lewis,Kevin McCoy,Christina Gomez,Canling Chen,Lorenzo Luzi*

Main category: cs.CV

TL;DR: GRR-CoCa是一种改进的SOTA对比标题模型，通过引入多种技术提高性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基础多模态模型架构在架构复杂性上落后于现代大型语言模型（LLMs）。

Method: GRR-CoCa模型结合了高斯误差门控线性单元、均方根归一化和旋转位置嵌入到文本解码器和视觉变压器（ViT）编码器中。

Result: GRR-CoCa在预训练数据集和三个不同的微调数据集上显著优于基线CoCa。预训练改进包括对比损失减少了27.25%，困惑度降低了3.71%，CoCa损失减少了7.15%。平均微调改进包括对比损失减少了13.66%，困惑度降低了5.18%，CoCa损失减少了5.55%。

Conclusion: GRR-CoCa的修改架构在视觉-语言领域提高了性能和泛化能力。

Abstract: State-of-the-art (SOTA) image and text generation models are multimodal
models that have many similarities to large language models (LLMs). Despite
achieving strong performances, leading foundational multimodal model
architectures frequently lag behind the architectural sophistication of
contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner
(CoCa) model that incorporates Gaussian error gated linear units, root mean
squared normalization, and rotary positional embedding into the textual
decoders and the vision transformer (ViT) encoder. Each architectural
modification has been shown to improve model performance in LLMs, but has yet
to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model
with the same modified textual decoders but with CoCa's original ViT encoder.
We used standard pretraining and fine-tuning workflows to benchmark the models
on contrastive and generative tasks. Our GRR-CoCa significantly outperformed
Baseline CoCa on the pretraining dataset and three diverse fine-tuning
datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in
perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were
13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We
show that GRR-CoCa's modified architecture improves performance and
generalization across vision-language domains.

</details>


### [58] [SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning](https://arxiv.org/abs/2507.18616)
*Si-Woo Kim,MinJu Jeon,Ye-Chan Kim,Soeun Lee,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: SynC is a novel framework designed to refine synthetic image-caption datasets for Zero-shot Image Captioning (ZIC) by reassigning captions to the most semantically aligned images, leading to improved performance on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing dataset pruning techniques are largely designed for removing noisy text in web-crawled data, but they are ill-suited for the distinct challenges of synthetic data, where captions are typically well-formed, but images may be inaccurate representations.

Method: SynC focuses on reassigning captions to the most semantically aligned images already present within the synthetic image pool. It employs a one-to-many mapping strategy by initially retrieving multiple relevant candidate images for each caption, then applying a cycle-consistency-inspired alignment scorer that selects the best image by verifying its ability to retrieve the original caption via image-to-text retrieval.

Result: Extensive evaluations demonstrate that SynC consistently and significantly improves performance across various ZIC models on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art results in several scenarios.

Conclusion: SynC offers an effective strategy for curating refined synthetic data to enhance ZIC.

Abstract: Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets
generated by text-to-image (T2I) models to mitigate the need for costly manual
annotation. However, these T2I models often produce images that exhibit
semantic misalignments with their corresponding input captions (e.g., missing
objects, incorrect attributes), resulting in noisy synthetic image-caption
pairs that can hinder model training. Existing dataset pruning techniques are
largely designed for removing noisy text in web-crawled data. However, these
methods are ill-suited for the distinct challenges of synthetic data, where
captions are typically well-formed, but images may be inaccurate
representations. To address this gap, we introduce SynC, a novel framework
specifically designed to refine synthetic image-caption datasets for ZIC.
Instead of conventional filtering or regeneration, SynC focuses on reassigning
captions to the most semantically aligned images already present within the
synthetic image pool. Our approach employs a one-to-many mapping strategy by
initially retrieving multiple relevant candidate images for each caption. We
then apply a cycle-consistency-inspired alignment scorer that selects the best
image by verifying its ability to retrieve the original caption via
image-to-text retrieval. Extensive evaluations demonstrate that SynC
consistently and significantly improves performance across various ZIC models
on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art
results in several scenarios. SynC offers an effective strategy for curating
refined synthetic data to enhance ZIC.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [59] [Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving](https://arxiv.org/abs/2507.17753)
*Liang Zhang,Xiaoming Zhai,Jionghao Lin,Jionghao Lin,Jennifer Kleiman,Diego Zapata-Rivera,Carol Forsyth,Yang Jiang,Xiangen Hu,Arthur C. Graesser*

Main category: cs.HC

TL;DR: 本研究探讨了四种沟通模式在双代理数学问题解决环境中的效果，发现同伴间协作表现最佳，并强调有效沟通策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型代理在AI辅助教育中被广泛用于辅导和学习，但很少有研究系统地评估不同沟通策略对代理解决问题的影响。

Method: 本研究评估了四种沟通模式，在双代理、基于聊天的数学问题解决环境中使用OpenAI GPT-4o模型进行测试。

Result: 结果表明，双代理设置优于单代理，其中'同伴间协作'达到了最高的准确率。对话行为如陈述、确认和提示在协作解决问题中起着关键作用。

Conclusion: 多代理框架在计算任务中表现出色，但有效的沟通策略对于解决AI教育中的复杂问题至关重要。

Abstract: Large Language Model (LLM) agents are increasingly utilized in AI-aided
education to support tutoring and learning. Effective communication strategies
among LLM agents improve collaborative problem-solving efficiency and
facilitate cost-effective adoption in education. However, little research has
systematically evaluated the impact of different communication strategies on
agents' problem-solving. Our study examines four communication modes,
\textit{teacher-student interaction}, \textit{peer-to-peer collaboration},
\textit{reciprocal peer teaching}, and \textit{critical debate}, in a
dual-agent, chat-based mathematical problem-solving environment using the
OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that
dual-agent setups outperform single agents, with \textit{peer-to-peer
collaboration} achieving the highest accuracy. Dialogue acts like statements,
acknowledgment, and hints play a key role in collaborative problem-solving.
While multi-agent frameworks enhance computational tasks, effective
communication strategies are essential for tackling complex problems in AI
education.

</details>


### [60] [PosterMate: Audience-driven Collaborative Persona Agents for Poster Design](https://arxiv.org/abs/2507.18572)
*Donghoon Shin,Daniel Lee,Gary Hsieh,Gromit Yeuk-Yin Chan*

Main category: cs.HC

TL;DR: PosterMate 是一种海报设计助手，通过创建基于营销文件的受众驱动角色代理来促进协作。它收集每个角色代理对海报组件的反馈，并在调解者的帮助下激发讨论以达成结论。结果表明，PosterMate 能够捕捉被忽视的观点，并作为有效的原型工具。


<details>
  <summary>Details</summary>
Motivation: 海报设计可以从目标受众的同步反馈中受益。然而，收集具有不同观点的受众并协调他们对设计修改的共识可能具有挑战性。最近的生成式 AI 模型为模拟人类互动提供了机会，但尚不清楚它们如何用于设计中的反馈过程。

Method: PosterMate 是一个海报设计助手，通过创建基于营销文件的受众驱动的角色代理来促进协作。PosterMate 收集每个角色代理对海报组件的反馈，并在调解者的帮助下激发讨论以达成结论。这些达成一致的编辑可以直接集成到海报设计中。

Result: 通过用户研究（N=12），我们发现了 PosterMate 捕捉被忽视观点的潜力，同时作为有效的原型工具。此外，我们的受控在线评估（N=100）显示，单个角色代理的反馈在其角色身份下是适当的，讨论有效地综合了不同角色代理的观点。

Conclusion: PosterMate 有潜力捕捉被忽视的观点，并作为有效的原型工具。此外，从个体角色代理的反馈是合适的，讨论有效地综合了不同角色代理的观点。

Abstract: Poster designing can benefit from synchronous feedback from target audiences.
However, gathering audiences with diverse perspectives and reconciling them on
design edits can be challenging. Recent generative AI models present
opportunities to simulate human-like interactions, but it is unclear how they
may be used for feedback processes in design. We introduce PosterMate, a poster
design assistant that facilitates collaboration by creating audience-driven
persona agents constructed from marketing documents. PosterMate gathers
feedback from each persona agent regarding poster components, and stimulates
discussion with the help of a moderator to reach a conclusion. These
agreed-upon edits can then be directly integrated into the poster design.
Through our user study (N=12), we identified the potential of PosterMate to
capture overlooked viewpoints, while serving as an effective prototyping tool.
Additionally, our controlled online evaluation (N=100) revealed that the
feedback from an individual persona agent is appropriate given its persona
identity, and the discussion effectively synthesizes the different persona
agents' perspectives.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [61] [Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation](https://arxiv.org/abs/2507.17937)
*Jaechul Roh,Zachary Novack,Yuefeng Peng,Niloofar Mireshghallah,Taylor Berg-Kirkpatrick,Amir Houmansadr*

Main category: cs.SD

TL;DR: 本文揭示了歌词到歌曲生成模型的漏洞，即语音提示可以解锁记忆的音视频内容，引发关于版权、安全和内容来源的担忧。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索歌词到歌曲生成模型对训练数据记忆的脆弱性，并发现语音提示可能引发音视频内容的记忆再现。

Method: 本文介绍了对抗性电话提示（APT）攻击，通过同音替换来改变歌词的语义，同时保持其声学结构。

Result: 实验结果表明，像SUNO和YuE这样的模型会生成与已知训练内容非常相似的输出，并且在多个语言和流派中都存在这种漏洞。此外，语音修改的歌词可以触发文本到视频模型的视觉记忆。

Conclusion: 本文揭示了语音提示可以解锁记忆的音视频内容，引发了关于版权、安全和内容来源的紧迫问题。

Abstract: Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis
from text, yet their vulnerability to training data memorization remains
underexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel
attack where lyrics are semantically altered while preserving their acoustic
structure through homophonic substitutions (e.g., Eminem's famous "mom's
spaghetti" $\rightarrow$ "Bob's confetti"). Despite these distortions, we
uncover a powerful form of sub-lexical memorization: models like SUNO and YuE
regenerate outputs strikingly similar to known training content, achieving high
similarity across audio-domain metrics, including CLAP, AudioJudge, and
CoverID. This vulnerability persists across multiple languages and genres. More
surprisingly, we discover that phoneme-altered lyrics alone can trigger visual
memorization in text-to-video models. When prompted with phonetically modified
lyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original
music video -- including character appearance and scene composition -- despite
no visual cues in the prompt. We term this phenomenon phonetic-to-visual
regurgitation. Together, these findings expose a critical vulnerability in
transcript-conditioned multimodal generation: phonetic prompting alone can
unlock memorized audiovisual content, raising urgent questions about copyright,
safety, and content provenance in modern generative systems. Example
generations are available on our demo page (jrohsc.github.io/music_attack/).

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [62] [LLM-based Embedders for Prior Case Retrieval](https://arxiv.org/abs/2507.18455)
*Damith Premasiri,Tharindu Ranasinghe,Ruslan Mitkov*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型的文本嵌入器方法，以解决法律案例检索中的两个关键问题，并取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 由于法律文本长度限制和缺乏法律训练数据，传统的信息检索方法在法律案例检索中表现不佳。

Method: 本文采用基于大语言模型的文本嵌入器进行法律案例检索，这种方法支持更长的输入长度，并且无需训练数据。

Result: 实验结果表明，基于大语言模型的文本嵌入器在四个PCR基准数据集上优于BM25和监督型Transformer模型。

Conclusion: 本文通过利用基于大语言模型的文本嵌入器，解决了法律案例检索中的两个关键挑战，并在四个PCR基准数据集上验证了其有效性。

Abstract: In common law systems, legal professionals such as lawyers and judges rely on
precedents to build their arguments. As the volume of cases has grown massively
over time, effectively retrieving prior cases has become essential. Prior case
retrieval (PCR) is an information retrieval (IR) task that aims to
automatically identify the most relevant court cases for a specific query from
a large pool of potential candidates. While IR methods have seen several
paradigm shifts over the last few years, the vast majority of PCR methods
continue to rely on traditional IR methods, such as BM25. The state-of-the-art
deep learning IR methods have not been successful in PCR due to two key
challenges: i. Lengthy legal text limitation; when using the powerful
BERT-based transformer models, there is a limit of input text lengths, which
inevitably requires to shorten the input via truncation or division with a loss
of legal context information. ii. Lack of legal training data; due to data
privacy concerns, available PCR datasets are often limited in size, making it
difficult to train deep learning-based models effectively. In this research, we
address these challenges by leveraging LLM-based text embedders in PCR.
LLM-based embedders support longer input lengths, and since we use them in an
unsupervised manner, they do not require training data, addressing both
challenges simultaneously. In this paper, we evaluate state-of-the-art
LLM-based text embedders in four PCR benchmark datasets and show that they
outperform BM25 and supervised transformer-based models.

</details>


### [63] [DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data](https://arxiv.org/abs/2507.18583)
*Zhengyun Zhao,Huaiyuan Ying,Yue Zhong,Sheng Yu*

Main category: cs.IR

TL;DR: 本文介绍了DR.EHR，一种专门针对EHR检索的密集检索模型。通过两阶段训练流程，利用MIMIC-IV出院摘要进行训练，取得了最先进的结果，并展示了在各种查询类型上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）在临床实践中至关重要，但由于语义差距问题，其检索仍然具有挑战性。现有的模型，无论是通用领域还是生物医学领域，都因医学知识不足或训练语料不匹配而表现不佳。

Method: 本文提出了一个两阶段的训练流程，利用MIMIC-IV出院摘要来解决需要大量医学知识和大规模训练数据的问题。第一阶段涉及从生物医学知识图谱中提取医学实体和注入知识，第二阶段使用大型语言模型生成多样化的训练数据。

Result: 在CliniQ基准测试中，我们的模型显著优于所有现有的密集检索器，达到了最先进的结果。详细分析确认了我们的模型在各种匹配和查询类型上的优越性，特别是在具有挑战性的语义匹配如暗示和缩写方面。消融研究验证了每个管道组件的有效性，并且在EHR问答数据集上的补充实验展示了模型在自然语言问题上的泛化能力，包括包含多个实体的复杂问题。

Conclusion: 本工作显著推进了EHR检索，为临床应用提供了稳健的解决方案。

Abstract: Electronic Health Records (EHRs) are pivotal in clinical practices, yet their
retrieval remains a challenge mainly due to semantic gap issues. Recent
advancements in dense retrieval offer promising solutions but existing models,
both general-domain and biomedical-domain, fall short due to insufficient
medical knowledge or mismatched training corpora. This paper introduces
\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for
EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV
discharge summaries to address the need for extensive medical knowledge and
large-scale training data. The first stage involves medical entity extraction
and knowledge injection from a biomedical knowledge graph, while the second
stage employs large language models to generate diverse training data. We train
two variants of \texttt{DR.EHR}, with 110M and 7B parameters, respectively.
Evaluated on the CliniQ benchmark, our models significantly outperforms all
existing dense retrievers, achieving state-of-the-art results. Detailed
analyses confirm our models' superiority across various match and query types,
particularly in challenging semantic matches like implication and abbreviation.
Ablation studies validate the effectiveness of each pipeline component, and
supplementary experiments on EHR QA datasets demonstrate the models'
generalizability on natural language questions, including complex ones with
multiple entities. This work significantly advances EHR retrieval, offering a
robust solution for clinical applications.

</details>

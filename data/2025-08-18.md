<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.IR](#cs.IR) [Total: 3]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的分层算法到硬件描述语言编码代理A2HCoder，旨在解决算法设计与硬件实现之间的差距问题。


<details>
  <summary>Details</summary>
Motivation: 在无线通信系统中，严格的超低延迟和功耗要求显著增加了高效算法到硬件部署的需求。然而，算法设计和硬件实现之间仍然存在持续且显著的差距。传统上，由于高级编程语言（如MATLAB）和硬件描述语言（如Verilog）之间的基本不匹配，填补这一差距需要大量的领域专业知识和耗时的手动开发。

Method: A2HCoder引入了一个分层框架，增强了鲁棒性和可解释性，同时抑制了LLM生成代码中的常见幻觉问题。它在水平维度上将复杂算法分解为模块化功能块，在垂直维度上进行逐步、细粒度的翻译，利用外部工具链如MATLAB和Vitis HLS进行调试和电路级综合。

Result: A2HCoder通过一个现实世界的5G无线通信领域的部署案例进行了验证，展示了其实际应用价值、可靠性和部署效率。

Conclusion: A2HCoder通过在5G无线通信领域的真实部署案例验证了其实际应用价值、可靠性和部署效率。

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [2] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: 研究提出了一种名为PersonaTwin的多层提示条件框架，用于构建自适应的数字双胞胎。该框架整合了人口统计、行为和心理测量数据，并在医疗保健背景下进行了评估。结果显示，PersonaTwin在模拟保真度方面表现良好，并且基于PersonaTwin训练的模型在预测和公平性指标上接近于基于个体训练的模型。这表明LLM数字双胞胎方法在生成真实且情感细腻的用户模拟方面具有潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）为用户建模和人类行为近似提供了新可能性，但它们往往无法捕捉个体用户的多维细微差别。因此，需要一种能够更好地捕捉用户特征的方法。

Method: PersonaTwin是一种多层提示条件框架，通过整合人口统计、行为和心理测量数据来构建自适应的数字双胞胎。研究使用了一个包含8,500多名个体的综合数据集，在医疗保健背景下对PersonaTwin进行了系统基准测试，并结合最先进的文本相似性度量和专门的人口平等评估进行严格评估。

Result: 实验结果表明，PersonaTwin框架在模拟保真度方面与理想设置相当，并且基于PersonaTwin训练的下游模型在预测和公平性指标上接近于基于个体训练的模型。

Conclusion: 研究结果表明，PersonaTwin框架在模拟保真度方面与理想设置相当，并且基于PersonaTwin训练的下游模型在预测和公平性指标上接近于基于个体训练的模型。这些发现强调了LLM数字双胞胎方法在生成真实且情感细腻的用户模拟方面的潜力，为个性化数字用户建模和行为分析提供了一个强大的工具。

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [3] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: 本文介绍了两个开放权重的推理模型gpt-oss-120b和gpt-oss-20b，它们在准确性和推理成本方面取得了显著进展。模型使用高效的专家混合变压器架构，并通过大规模蒸馏和强化学习进行训练。模型优化以具有强大的代理能力（深度研究浏览、Python工具使用和支持开发人员提供的函数），同时使用渲染聊天格式来实现清晰的指令遵循和角色划分。两个模型在数学、编码和安全等基准测试中都取得了良好的结果。模型权重、推理实现、工具环境和分词器已按照Apache 2.0许可证发布，以促进广泛使用和进一步研究。


<details>
  <summary>Details</summary>
Motivation: 本文旨在介绍两个开放权重的推理模型gpt-oss-120b和gpt-oss-20b，这些模型在准确性和推理成本方面取得了显著进展，并且具有强大的代理能力，可以支持深度研究浏览、Python工具使用和支持开发人员提供的函数。

Method: 本文介绍了两个开放权重的推理模型gpt-oss-120b和gpt-oss-20b，它们使用高效的专家混合变压器架构，并通过大规模蒸馏和强化学习进行训练。模型优化以具有强大的代理能力（深度研究浏览、Python工具使用和支持开发人员提供的函数），同时使用渲染聊天格式来实现清晰的指令遵循和角色划分。

Result: 两个模型在数学、编码和安全等基准测试中都取得了良好的结果。模型权重、推理实现、工具环境和分词器已按照Apache 2.0许可证发布，以促进广泛使用和进一步研究。

Conclusion: 本文介绍了两个开放权重的推理模型gpt-oss-120b和gpt-oss-20b，它们在准确性和推理成本方面取得了显著进展。模型使用高效的专家混合变压器架构，并通过大规模蒸馏和强化学习进行训练。模型优化以具有强大的代理能力（深度研究浏览、Python工具使用和支持开发人员提供的函数），同时使用渲染聊天格式来实现清晰的指令遵循和角色划分。两个模型在数学、编码和安全等基准测试中都取得了良好的结果。模型权重、推理实现、工具环境和分词器已按照Apache 2.0许可证发布，以促进广泛使用和进一步研究。

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [4] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 本研究构建了一个计算框架，用于从新闻文章中自动提取公司风险因素。实验表明，微调的预训练语言模型在识别风险因素方面表现优于零样本和少量样本提示的大型语言模型。通过分析大量新闻文章，展示了从新闻中识别风险因素可以为公司和行业提供有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 识别与公司相关的风险对于投资者和整体金融市场至关重要。

Method: 我们构建了一个计算框架，以自动从新闻文章中提取公司风险因素。我们提出了一个包含七个不同方面的新型模式，并对744篇新闻文章进行了采样和注释，同时对各种机器学习模型进行了基准测试。

Result: 实验表明，零样本和少量样本提示最先进的LLM（例如LLaMA-2）在识别风险因素方面只能达到中等至低性能，而微调的预训练语言模型在大多数风险因素上表现更好。

Conclusion: 通过分析超过277,000篇Bloomberg新闻文章，我们展示了从新闻中识别风险因素可以为公司和行业的运营提供广泛见解。

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [5] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: 本文介绍了Rule2Text框架，利用大型语言模型为知识图谱中的逻辑规则生成自然语言解释，提高了KG的可访问性和可用性。通过实验和人类评估，证明了该方法的有效性，并展示了微调后的模型在解释质量上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KGs）可以通过规则挖掘得到增强；然而，由于其固有的复杂性和个别KG的特殊标签约定，产生的逻辑规则对人类来说往往难以理解。这项工作提出了Rule2Text，这是一个全面的框架，利用大型语言模型（LLMs）为挖掘出的逻辑规则生成自然语言解释，从而提高KG的可访问性和可用性。

Method: 我们使用多个数据集（包括Freebase变体和ogbl-biokg数据集）以及AMIE 3.5.1挖掘出的规则进行实验。我们系统地评估了多种LLM在广泛的提示策略下的表现，包括零样本、少样本、变量类型整合和思维链推理。为了系统评估模型性能，我们进行了人工评估生成解释的正确性和清晰度。为了解决评估可扩展性问题，我们开发并验证了一个LLM作为评判者的框架，该框架与人类评估者表现出强烈的一致性。

Result: 我们使用最佳模型（Gemini 2.0 Flash）、LLM评判者和人机交互反馈构建了高质量的地面真实数据集，并用其微调开源Zephyr模型。结果表明，微调后解释质量显著提高，尤其是在领域特定的数据集上。

Conclusion: 我们的结果表明，在微调后解释质量有显著提高，特别是在领域特定的数据集上。此外，我们集成了一个类型推理模块，以支持缺乏显式类型信息的KG。所有代码和数据都在https://github.com/idirlab/KGRule2NL上公开。

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [6] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: The paper proposes a verifier-based inference-time scaling method for MDMs, which improves generation quality and establishes MDMs as a better alternative to autoregressive language models.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have shown excellent ability to improve generation quality, but there is a need for better candidate generation during the denoising process of MDMs.

Method: The paper proposes a verifier-based inference-time scaling method for MDMs to improve generation quality.

Result: Experiments demonstrate the application of MDMs for standard text-style transfer tasks and show that a simple soft-value-based verifier setup leads to significant gains in generation quality.

Conclusion: MDMs have shown promise as a better alternative to autoregressive language models and can benefit from verifier-based inference-time scaling methods.

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [7] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文指出现有LLM安全基准在覆盖儿童和青少年特定风险方面存在不足，并提出SproutBench评估套件来探测这些风险，同时揭示了LLMs的安全漏洞及其相关性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在针对儿童和青少年的应用中迅速普及，需要重新审视现有的AI安全框架，这些框架主要针对成人用户，忽视了未成年人的独特发展脆弱性。

Method: 本文引入了SproutBench，这是一个包含1,283个发展基础对抗性提示的评估套件，用于探测情感依赖、隐私侵犯和模仿危险行为等风险。此外，还对47种不同的LLM进行了严格的实证评估。

Result: 通过实证评估发现，LLMs存在显著的安全漏洞，并且发现了安全与风险预防之间的强相关性，以及互动性和年龄适宜性之间的负相关关系。

Conclusion: 本文提出了SproutBench评估套件，以填补现有LLM安全基准在儿童和青少年特定风险方面的不足，并为推进以儿童为中心的AI设计和部署提供了实用指南。

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [8] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在跨语言知识迁移中的问题，提出通过受控实验来理解其动态，并开发方法以改善跨语言迁移效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在跨语言知识迁移方面存在困难：当用一种语言询问另一种语言中表达的事实时，它们会产生幻觉。

Method: 我们通过从头开始在合成多语种数据集上训练小型Transformer模型，引入了一个受控环境来研究这一现象的原因和动态。

Result: 我们识别出一个学习阶段，在此阶段模型会发展出对同一事实在不同语言中的独立或统一表示，并证明统一对于跨语言迁移至关重要。

Conclusion: 我们的工作展示了如何通过受控设置来揭示预训练动态，并提出了改进跨语言转移的新方向。

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [9] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: 本文研究了语言模型代理在面对外部失败时的规划能力，发现它们在制定备用计划方面存在困难，并指出了当前生成模型的关键挑战和未来工作方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型代理被应用于越来越复杂的现实问题，它们需要在大型搜索空间中制定计划。当这些计划因超出其控制的原因失败时，语言代理如何寻找替代方法实现目标是一个重要问题。

Method: 本文设计了一个专门的代理规划基准来研究这个问题，每个规划问题通过函数调用的组合解决。代理从四千多个可能性中搜索相关函数，并观察环境反馈。

Result: 研究发现，语言代理在应对环境反馈时难以制定和执行备用计划。虽然最先进的模型通常能够识别正确使用的函数，但它们难以适应环境反馈，并且即使在搜索空间被人为限制的情况下，也常常无法追求替代方案。

Conclusion: 本文分析了语言模型代理在面对外部失败时制定和执行备用计划的能力，并指出了当前生成模型的关键挑战以及未来工作的方向。

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [10] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: 该研究提出了一种新的框架，用于评估大型语言模型中的极化相关偏差，并展示了其在多个模型上的应用效果。


<details>
  <summary>Details</summary>
Motivation: 尽管在偏差检测和缓解技术方面取得了显著进展，但某些挑战仍未得到充分探索。该研究旨在提供一种新的框架来评估LLM中的极化相关偏差。

Method: 该研究提出了一种可重用、细粒度且与主题无关的框架，用于评估LLM中的极化相关偏差。方法结合了极化敏感的情感指标和合成生成的平衡冲突相关语句数据集，使用预定义的语义类别。

Result: 通过案例研究，该框架允许对不同语义类别进行细粒度分析，揭示了模型之间的行为差异。适应提示修改进一步显示了对预先设定语言和国籍的偏差。

Conclusion: 该框架支持自动化数据集生成和细粒度偏差评估，适用于各种极化驱动的场景和主题，并且与其他许多偏差评估策略是正交的。

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [11] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: 本文研究了如何将数字词典嵌入到AMR有向图中，并通过简化这些图来分析其性质，以解决符号接地问题。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和处理句子的意义，需要将实际的数字词典嵌入到AMR有向图中，并研究其简化后的性质。

Method: 使用最先进的预训练大型语言模型，将真实的数字词典嵌入到AMR有向图中，并以保真方式简化这些图。

Result: 成功地将数字词典嵌入到AMR有向图中，并对这些图进行了简化，同时保持了它们的电路空间。

Conclusion: 本文分析了这些简化后的有向图的性质，并讨论了它们与符号接地问题的关系。

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [12] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: 本文介绍了一个名为RAMP的多代理框架，用于营销任务中的受众筛选。通过迭代计划、调用工具、验证输出和生成建议，以及配备长期记忆存储，显著提高了准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）的进步使得开发能够规划和与工具交互以完成复杂任务的AI代理成为可能，但关于它们在现实应用中的可靠性文献仍然有限。

Method: 我们引入了一个多代理框架，称为RAMP，它迭代地计划、调用工具、验证输出并生成建议以提高生成的受众质量。此外，我们还为模型配备了长期记忆存储，这是一个客户特定事实和过去查询的知识库。

Result: 我们展示了LLM规划和记忆的使用，这在一组88个评估查询上将准确性提高了28个百分点。此外，我们展示了迭代验证和反思对更模糊查询的影响，在较小的挑战集上，随着验证/反思迭代次数的增加，召回率逐步提高（大约+20个百分点），并且用户满意度更高。

Conclusion: 我们的结果为在动态、面向行业的环境中部署可靠的基于LLM的系统提供了实用见解。

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [13] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: MoNaCo是一个包含1,315个自然且复杂问题的基准，旨在测试大型语言模型处理真实世界信息查询的能力。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM基准很少包含自然的问题，这些问题既寻求信息又对人类来说是真正耗时的。为了弥补这一差距，我们引入了MoNaCo，这是一个包含1,315个自然且复杂问题的基准，解决这些问题需要数十甚至数百个中间步骤。

Method: 我们开发了一个分解的注释流程，以大规模引出并手动回答自然的时间消耗问题。

Result: 在MoNaCo上评估的前沿LLM最多达到61.2%的F1分数，受到低召回率和幻觉的阻碍。

Conclusion: 我们的结果强调了需要能够更好地处理现实世界信息查询的复杂性和广泛性的推理模型——MoNaCo提供了一个有效的资源来跟踪这种进展。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [14] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: 本文介绍了MobQA，这是一个用于评估大型语言模型（LLMs）在人类移动数据方面的语义理解能力的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有模型在预测人类移动模式方面表现出色，但它们对这些模式的潜在原因或语义意义的理解仍不明显。

Method: MobQA数据集包含5,800个高质量的问答对，涵盖三种互补的问题类型：事实检索、多选推理和自由形式解释，所有问题都需要空间、时间和语义推理。

Result: 对主要LLMs的评估显示，在事实检索方面表现良好，但在语义推理和解释问题回答方面存在显著限制，轨迹长度显著影响模型效果。

Conclusion: 这些发现展示了最先进的LLMs在语义移动理解方面的成就和局限性。

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [15] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: 本研究为低资源的Tulu语言创建了首个仇恨语言识别数据集，并评估了多种深度学习模型和transformer架构的表现，结果显示基于BiGRU的模型效果最佳，而transformer模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: Tulu是一种低资源的德拉维达语，尽管其数字存在感在增长，但计算资源有限。因此，需要建立一个针对Tulu社交媒体内容的仇恨语言识别基准数据集，以促进相关自然语言处理研究的发展。

Method: 本研究收集了来自YouTube评论的代码混合Tulu社交媒体内容，并创建了一个标注良好的数据集。评估了多种深度学习模型和基于transformer的架构，包括GRU、LSTM、BiGRU、BiLSTM、CNN和注意力机制变体，以及mBERT和XLM-RoBERTa。

Result: 数据集包含3,845条评论，分为四类：非仇恨、非Tulu、无目标仇恨和有目标仇恨。其中，带有自注意力机制的BiGRU模型表现最佳，准确率为82%，宏F1得分为0.81。而基于transformer的模型表现较差，表明多语言预训练在代码混合、资源匮乏的环境中存在局限性。

Conclusion: 本研究为Tulu语言的网络内容中仇恨语言识别提供了首个基准数据集，并展示了深度学习模型在该任务上的表现。同时，研究指出多语言预训练模型在代码混合、资源匮乏的语言环境中的局限性，为未来在Tulu和其他类似语言上的自然语言处理研究奠定了基础。

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [16] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的两阶段框架，用于生成基于学生个人误解的个性化干扰项，实验显示其在生成合理干扰项方面表现出色，并能适应群体层面设置。


<details>
  <summary>Details</summary>
Motivation: 现有的干扰项生成方法无法捕捉个别学生的多样化推理错误，限制了其诊断效果。因此，我们需要一种能够根据每个学生的过去答题记录推断出的误解生成定制干扰项的方法。

Method: 我们提出了一种无需训练的两阶段框架。第一阶段通过应用蒙特卡洛树搜索（MCTS）来恢复学生从过去的错误答案中的推理轨迹，构建一个特定于学生的误解原型。第二阶段利用该原型引导学生在新问题上的推理模拟，从而生成与学生反复出现的误解相一致的个性化干扰项。

Result: 实验表明，我们的方法在生成140名学生的合理个性化干扰项方面表现最佳，并且还能有效地推广到群体层面设置。

Conclusion: 我们的方法在生成140名学生的合理个性化干扰项方面表现最佳，并且还能有效地推广到群体层面设置，这表明了其稳健性和适应性。

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [17] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: 本文提出了一种寄生双尺度方法，结合了增强的推测采样方法、模型压缩和知识蒸馏技术，提高了多语言语音翻译的推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的统一模型通常参数量大，难以在本地部署场景中平衡推理效率和性能。因此，需要一种更高效的方法来处理多语言语音翻译。

Method: 本文提出了一个寄生双尺度方法，结合了增强的推测采样方法、模型压缩和知识蒸馏技术，并构建了KVSPN模块以提高推理效率。

Result: 通过结合KVSPN模块和知识蒸馏方法，本文实现了40%的推理速度提升，且没有BLEU分数下降；同时，与原始Whisper Medium相比，推理速度提升了2.6倍，性能更优。

Conclusion: 本文提出了一种创新的寄生双尺度方法，结合了增强的推测采样方法与模型压缩和知识蒸馏技术，实现了在六种流行语言上的最先进性能，并提高了推理效率。

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [18] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: E-CaTCH is a novel framework for detecting multimodal misinformation on social media by clustering posts into pseudo-events, extracting and aligning features, modeling temporal evolution, and addressing class imbalance. It outperforms existing methods and shows strong performance across various datasets.


<details>
  <summary>Details</summary>
Motivation: Detecting multimodal misinformation on social media remains challenging due to inconsistencies between modalities, changes in temporal patterns, and substantial class imbalance. Existing methods fail to capture the event-level structure that connects posts across time and modality.

Method: E-CaTCH is an interpretable and scalable framework that clusters posts into pseudo-events based on textual similarity and temporal proximity, extracts and aligns textual and visual features using pre-trained encoders, and models temporal evolution with a trend-aware LSTM. It also integrates adaptive class weighting, temporal consistency regularization, and hard-example mining to address class imbalance and promote stable learning.

Result: E-CaTCH consistently outperforms state-of-the-art baselines on Fakeddit, IND, and COVID-19 MISINFOGRAPH datasets. Cross-dataset evaluations demonstrate its robustness, generalizability, and practical applicability across diverse misinformation scenarios.

Conclusion: E-CaTCH consistently outperforms state-of-the-art baselines and demonstrates robustness, generalizability, and practical applicability across diverse misinformation scenarios.

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [19] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为HGRAG的新颖RAG方法，通过超图实现结构和语义信息的跨粒度集成，从而提高多跳问答任务的性能并提升检索效率。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）方法主要关注粗粒度文本语义相似性，忽略了分散知识之间的结构关联，这限制了它们在多跳问答（MHQA）任务中的有效性。图RAG方法通过利用知识图（KG）来捕捉结构关联，但它们倾向于过度依赖结构信息和细粒度的单词或短语级检索，导致文本语义的未充分利用。

Method: 我们提出了一种名为HGRAG的新颖RAG方法，通过超图实现结构和语义信息的跨粒度集成。构建了一个实体超图，其中细粒度实体作为节点，粗粒度段落作为超边，并通过共享实体建立知识关联。设计了一种超图检索方法，通过超图扩散整合细粒度实体相似性和粗粒度段落相似性。最后，采用检索增强模块进一步从语义和结构上优化检索结果，以获得最相关的段落作为LLM生成答案的上下文。

Result: 实验结果表明，我们的方法在问答性能上优于最先进的方法，并且在检索效率上实现了6倍的提升。

Conclusion: 实验结果表明，我们的方法在问答性能上优于最先进的方法，并且在检索效率上实现了6倍的提升。

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [20] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型在低资源语言语言学谜题中的表现，发现其在处理高形态复杂性问题时存在困难，并提出需要更语言特定的分词器。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在低资源语言中的语言推理能力，以发现其弱点并提供改进方向。

Method: 通过标注语言学特征来分析大型语言模型在629个问题上的表现，这些问题涉及41种低资源语言。

Result: 大型语言模型在涉及更高形态复杂性的谜题中表现较差，而在涉及英语中也存在的语言特征的谜题中表现较好。将单词拆分为词素作为预处理步骤可以提高解决率。

Conclusion: 本文揭示了大型语言模型在处理低资源语言的语言学谜题时面临的挑战，并提出了改进方向。

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [21] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: 本文提出了一种无需标注数据的旅游领域LLM评估框架LETToT，通过专家推导的推理结构来评估LLM。结果显示，优化后的专家ToT在质量上有所提升，并且在不同规模的模型中揭示了缩放定律和推理增强模型的优势。


<details>
  <summary>Details</summary>
Motivation: 在特定领域如旅游中评估大型语言模型（LLMs）仍然具有挑战性，因为标注基准的成本很高，且存在如幻觉等持续问题。

Method: 我们提出了LETToT框架，该框架利用专家推导的推理结构而不是标注数据来评估旅游领域的LLM。首先，我们通过与通用质量维度和专家反馈对齐来迭代优化和验证层次化的ToT组件。其次，我们将优化后的专家ToT应用于不同规模的模型（32B-671B参数）。

Result: 结果表明，我们的系统优化的专家ToT相对于基线有4.99-14.15%的相对质量提升。此外，应用LETToT的优化专家ToT评估不同规模的模型揭示了：(1) 在专业领域中，缩放定律仍然存在（DeepSeek-V3领先），但增强推理的小型模型（例如，DeepSeek-R1-Distill-Llama-70B）缩小了这一差距；(2) 对于小于72B的模型，显式推理架构在准确性和简洁性方面优于对照组（p<0.05）。

Conclusion: 我们的工作建立了一个可扩展的、无标签的领域特定LLM评估范式，提供了一种传统标注基准的稳健替代方案。

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [22] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: 本文介绍了TOXIFRENCH，一个用于法语毒性检测的新基准，并提出了一种改进模型可信度的微调策略。该方法在性能上优于现有大型语言模型，并展示了良好的多语言扩展能力。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏文化相关的大型数据集，法语中的毒性检测仍然发展不足。因此，我们需要一个更有效的数据集和模型来解决这一问题。

Method: 我们引入了TOXIFRENCH，一个包含53,622条法语在线评论的新公共基准，并采用半自动化注释流程来减少人工标注。此外，我们提出了使用动态加权损失的链式思维（CoT）微调策略，以提高模型的可信度。

Result: 我们的微调4B模型在F1分数上比基线提高了13%，并且优于GPT-40和Gemini-2.5等大型语言模型。此外，在跨语言毒性基准上的评估显示了强大的多语言能力。

Conclusion: 我们的方法在多语言和安全关键分类任务中表现出色，具有良好的扩展性。

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [23] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: 本研究分析了八种大型语言模型对抑郁症、焦虑和压力相关问题的回答的情感和情绪特征，发现不同模型在情感表达上有显著差异，这对心理健康应用有重要影响。


<details>
  <summary>Details</summary>
Motivation: 越来越多的人寻求信息从大型语言模型（LLMs）来应对抑郁症、焦虑和压力等广泛的心理健康问题。

Method: 本研究调查了八种LLM对关于抑郁症、焦虑和压力的二十个实用问题的回答，并使用最先进的工具对情感和情绪进行了评分。

Result: 乐观、恐惧和悲伤在所有输出中占主导地位，中性情感保持高值。感激、喜悦和信任处于中等水平，而愤怒、厌恶和爱等情绪很少被表达。

Conclusion: 这些发现突显了在心理健康应用中选择模型的重要性，因为每个LLM都有独特的感情特征，这可能显著影响用户体验和结果。

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [24] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本文研究了LLMs的过度拒绝问题，并提出了一种基于轨迹调整的方法SafeConstellations，有效减少了过度拒绝率，同时保持了模型的实用性。


<details>
  <summary>Details</summary>
Motivation: LLMs的过度拒绝行为影响了生产应用的实用性，特别是在需要频繁使用LLMs进行特定任务（如情感分析、语言翻译）的情况下。

Method: 通过全面评估和机制分析，揭示了LLMs在嵌入空间中的不同“星群”模式，并引入SafeConstellations方法来调整任务轨迹以减少过度拒绝。

Result: 实验表明，即使指令被重新表述为看似无害的任务，LLMs仍倾向于拒绝有害指令。而SafeConstellations方法能够有效减少过度拒绝率，最多降低73%。

Conclusion: 本文提出了一种名为SafeConstellations的方法，通过跟踪特定任务的轨迹模式并引导表示走向非拒绝路径，显著降低了过度拒绝率，同时保持了模型的通用行为。

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [25] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: 本文提出了SGSimEval，一个用于自动调查生成的综合评估基准，通过整合多种评估方法，提高了评估的全面性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法存在一些局限，包括有偏见的指标、缺乏人类偏好以及过度依赖LLM作为评判者。因此，需要一种更全面的评估方法来提高自动调查生成系统的质量。

Method: 我们提出了SGSimEval，这是一个综合基准，通过整合大纲、内容和参考文献的评估，并结合基于LLM的评分和定量指标，提供多方面的评估框架。

Result: 广泛的实验表明，当前的ASG系统在大纲生成方面表现出与人类相当的优势，而在内容和参考文献生成方面仍有显著改进空间。

Conclusion: 我们的评估指标与人类评估保持高度一致，表明了SGSimEval的有效性。

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [26] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本研究评估了4-bit GSQ和GPTQ量化方法对LLaMA 1B、Qwen 0.5B和PHI 1.5B模型的影响，分析了它们在多个NLP任务中的表现，提供了关于低比特量化技术在实际部署中的适用性的见解。


<details>
  <summary>Details</summary>
Motivation: 量化是提高大型语言模型（LLMs）可访问性的关键技术，通过减少内存使用和计算成本来保持性能。本研究旨在评估不同量化方法对模型性能和效率的影响，以提供实际部署的参考。

Method: 应用4-bit Group Scaling Quantization (GSQ)和Generative Pretrained Transformer Quantization (GPTQ)到LLaMA 1B、Qwen 0.5B和PHI 1.5B模型上，并在多个NLP任务中评估其影响。

Result: 研究测量了模型压缩与任务性能之间的权衡，分析了准确性、推理延迟和吞吐量等关键评估指标，并讨论了GSQ和GPTQ技术在不同大小模型上的优缺点。

Conclusion: 研究提供了关于低比特量化技术在不同模型大小上的优缺点的见解，并为未来实验提供了基准。

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [27] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于信号处理的新方法来检测LLM生成的文本，利用频谱特性提高检测效果，并展示了经典信号处理技术在这一领域的强大潜力。


<details>
  <summary>Details</summary>
Motivation: 高质量文本的激增需要可靠且高效的检测方法。现有的训练-free 方法虽然有潜力，但通常依赖于表面统计信息，忽视了文本生成过程的基本信号特性。

Method: 我们将检测重新定义为一个信号处理问题，引入了一种新范式，分析了token对数概率序列在频域中的特性。通过使用全局离散傅里叶变换（DFT）和局部短时傅里叶变换（STFT）系统地分析信号的频谱特性，我们发现人类撰写的文本表现出显著更高的频谱能量。基于这一关键见解，我们构建了SpecDetect，这是一个基于全局DFT单一稳健特征的检测器，并提出了增强版本SpecDetect++，结合了采样差异机制以进一步提高鲁棒性。

Result: 广泛的实验表明，我们的方法优于最先进的模型，同时运行时间几乎减少了一半。

Conclusion: 我们的工作引入了一种新的、高效且可解释的路径来检测LLM生成的文本，表明经典信号处理技术为这一现代挑战提供了令人惊讶的强大解决方案。

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [28] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: 本文探讨了使用大型语言模型从学生作业中提取反馈指标的方法，并发现其与人工评分有显著的相关性，为未来自动生成可解释的反馈提供了基础。


<details>
  <summary>Details</summary>
Motivation: 自动化反馈生成可以提高学生的学习进度，并帮助教师节省时间，以便专注于更具战略性和个性化的教学方面。然而，生成高质量的反馈需要首先提取相关的指标。

Method: 本研究采用大型语言模型（Llama 3.1）从语言学习课程的学生提交内容中提取反馈指标，并分析了这些指标与人工评分之间的对齐情况。

Result: 研究结果表明，LLM生成的指标与人工评分之间存在统计学上显著的强相关性，即使在未预见的指标和标准组合情况下也是如此。

Conclusion: 本文提出了一种利用大型语言模型（如Llama 3.1）从学生作业中提取反馈指标的方法，该方法能够生成可解释和透明的形成性反馈。

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [29] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 本文系统评估了5种提高提示鲁棒性的方法，并在多个模型和任务上进行了基准测试，结果为实践者提供了实用的见解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）对提示语的细微、非语义变化非常敏感。因此，需要系统地评估提高提示鲁棒性的方法，以便在实际应用中实现稳定和可靠的LLM性能。

Method: 我们提出了一个统一的实验框架，系统评估了5种提高提示鲁棒性的方法，并在多个模型和任务上进行了基准测试。此外，我们还扩展了分析以评估前沿模型（如GPT-4.1和DeepSeek V3）对格式扰动的鲁棒性。

Result: 我们的评估涵盖了来自不同范式的鲁棒性方法，并测试了它们在多种分布偏移下的泛化能力。结果表明，这些方法在不同模型和任务上的表现各不相同，提供了实用的见解。

Conclusion: 我们的研究结果为实践者提供了关于这些鲁棒性方法相对有效性的实用见解，使他们在追求现实世界应用中稳定和可靠的LLM性能时能够做出明智的决策。

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [30] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: 本文介绍了一种结合推理和检索增强生成（RAG）的新方法，适用于轻量级语言模型架构。该方法在特定领域内通过微调提高了答案的准确性和一致性，并且可以在本地部署。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统通常依赖于大规模模型和外部API，而我们的工作旨在满足在资源受限或安全环境中高性能和隐私保护解决方案的需求。

Method: 我们开发了一个检索增强的对话代理，能够使用轻量级主干模型解释复杂、特定领域的查询。系统集成了密集检索器和微调的Qwen2.5-Instruct模型，利用合成查询生成和来自前沿模型（如DeepSeek-R1）的推理轨迹，在NHS A-to-Z条件页面的定制语料库中进行操作。

Result: 评估表明，我们的领域特定微调方法在答案准确性和一致性上取得了显著提升，接近前沿性能，同时保持了本地部署的可行性。

Conclusion: 我们的领域特定微调方法在答案准确性和一致性上取得了显著提升，接近前沿性能，同时保持了本地部署的可行性。所有实现细节和代码都已公开发布，以支持可重复性和跨领域的适应。

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [31] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 本文提出了一种基于梯度优化和正则化的新型方法，用于生成神经网络预测的可提取解释，证明了可以在不训练专门模型的情况下进行理性提取，并且该方法在图像分类中也表现良好。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络模型在自然语言处理和计算机视觉等领域的快速发展，需要为这些黑箱模型的预测提供解释的需求不断增加。

Method: 我们提出了一种新的方法，基于对输入中模型不认为是相关类别的部分进行掩码，使用基于梯度的优化结合一种新的正则化方案，以确保生成的解释具有充分性、全面性和紧凑性。

Result: 我们提出了一个基于掩码的方法来生成神经网络预测的可提取解释，并且在图像输入上应用该方法，得到了高质量的图像分类解释。

Conclusion: 我们证明了可以仅基于训练好的分类器进行理性提取，而无需训练专门的模型。此外，我们的方法在图像分类中也获得了高质量的解释，表明自然语言处理中提出的理性提取条件适用于不同类型的输入。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [32] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 本文提出了一种新的端到端可微训练范式，用于稳定训练理性化变压器分类器，该方法通过单一模型实现分类和理性评分，提高了效率和稳定性，并实现了与人类注释的先进对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于训练一个理性选择器、一个分类器和一个补充分类器，这可能导致训练不稳定。我们希望通过简化这一过程来提高效率和稳定性。

Method: 我们提出了一种端到端的可微训练范式，用于稳定训练理性化变压器分类器。我们构建了一个单一模型，同时对样本进行分类并根据其相关性对输入标记进行评分。

Result: 我们的方法导致了一个更高效的训练范式，不受现有方法中常见的训练不稳定性的影响。此外，我们将这一范式扩展为生成类别相关的理性，并结合了最近在参数化和正则化结果理性方面的进展。

Conclusion: 我们的方法在没有显式监督的情况下实现了与人类注释的显著改进和最先进的对齐。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [33] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: 本文研究了通过微调模型回答价值调查问题来改变其价值体系的方法，并发现这种方法可以有效改变模型的下游行为。


<details>
  <summary>Details</summary>
Motivation: 研究是否可以通过训练模型回答价值调查问题来可靠地修改其价值体系

Method: 通过让模型回答价值调查问题来微调其价值体系，以改变模型的下游行为

Result: 微调可以改变模型对领域内调查问题的回答，并在隐式下游任务行为中产生显著的价值对齐

Conclusion: 我们的简单方法不仅能够改变模型对领域内调查问题的回答，还能在隐式下游任务行为中产生显著的转变（价值对齐）

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [34] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: HumorPlanSearch is a pipeline that models context to generate better humor using strategies, cultural reasoning, knowledge graphs, and iterative revisions.


<details>
  <summary>Details</summary>
Motivation: Automated humor generation with Large Language Models (LLMs) often yields jokes that feel generic, repetitive, or tone-deaf because humor is deeply situated and hinges on the listener's cultural background, mindset, and immediate context.

Method: HumorPlanSearch is a modular pipeline that explicitly models context through Plan-Search for diverse, topic-tailored strategies, Humor Chain-of-Thought (HuCoT) templates capturing cultural and stylistic reasoning, a Knowledge Graph to retrieve and adapt high-performing historical strategies, novelty filtering via semantic embeddings, and an iterative judge-driven revision loop.

Result: In experiments across nine topics with feedback from 13 human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent (p < 0.05) over a strong baseline.

Conclusion: HumorPlanSearch advances AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [35] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在分类反性别歧视言论时的困难，并指出内容审核设计需要改进以避免压制边缘化声音。


<details>
  <summary>Details</summary>
Motivation: 反性别歧视言论在塑造在线民主辩论中起着至关重要的作用。然而，由大型语言模型（LLMs）驱动的内容审核系统可能难以区分这种抵抗与它所反对的性别歧视。

Method: 本文研究了五种大型语言模型（LLMs）如何分类英国2022年涉及女性议员的高关注度触发事件中的性别歧视、反性别歧视和中性政治推文。

Result: 分析显示，模型经常将反性别歧视言论错误地分类为有害内容，尤其是在政治紧张事件中，伤害和抵抗的修辞风格会交汇。这些错误可能会压制那些挑战性别歧视的人，对边缘化声音造成不成比例的后果。

Conclusion: 本文认为，内容审核设计必须超越二元的有害/无害模式，在敏感事件中整合人工审核，并在训练数据中明确包含反性别歧视言论。通过将女性主义学术、基于事件的分析和模型评估联系起来，这项工作突显了在数字政治空间中保护抵抗言论的社会技术挑战。

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [36] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: CoDiEmb是一个统一框架，通过任务专用目标、delta-guided模型融合和高效训练流程，解决了IR和STS任务之间的负迁移问题，并提升了嵌入空间的质量。


<details>
  <summary>Details</summary>
Motivation: 解决在联合训练信息检索（IR）和语义文本相似性（STS）任务时出现的负迁移问题，因为这些任务本质上不同，传统的共同训练会导致性能权衡。

Method: CoDiEmb引入了三个关键创新：(1) 任务专用目标与动态采样器，(2) delta-guided模型融合策略，(3) 高效的单阶段训练流程。

Result: 在三个基础编码器上的15个标准IR和STS基准测试中验证了CoDiEmb的有效性，结果表明该框架能够减轻跨任务的权衡并提升嵌入空间的几何特性。

Conclusion: CoDiEmb框架不仅缓解了跨任务的权衡，还显著改善了嵌入空间的几何特性。

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [37] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 本研究探讨了补充信息对情感分析的影响，并发现使用JSON格式的提示可以提高性能，使小型模型在资源受限的边缘设备上部署成为可能。


<details>
  <summary>Details</summary>
Motivation: 营销理论指出，客户评估不仅受实际体验的影响，还受到其他参考点的影响。因此，本研究探讨了补充信息的内容和格式如何影响使用LLMs的情感分析。

Method: 本研究比较了自然语言（NL）和JSON格式的提示，使用一个适合实际营销应用的轻量级3B参数模型。

Result: 实验结果表明，带有附加信息的JSON提示优于所有基线，无需微调：宏F1分别提高了1.6%和4%，而RMSE分别降低了16%和9.1%。

Conclusion: 本研究表明，结构化提示可以使较小的模型实现具有竞争力的性能，为大规模模型部署提供了一种实用的替代方案。

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [38] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）是否表现出物种主义偏见，并发现它们在某些情况下复制了人类的物种主义态度，这表明需要将非人类道德患者纳入AI公平性和对齐框架。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，有必要审视它们的伦理倾向。研究旨在探讨LLMs是否表现出基于物种归属的物种主义偏见，并了解它们如何评价非人类动物。

Method: 研究者通过三个范式来探讨LLMs是否表现出物种主义偏见：(1) SpeciesismBench，一个包含1003个项目的基准测试，用于评估对物种主义陈述的认识和道德评价；(2) 已有的心理测量方法，比较模型响应与人类参与者的响应；(3) 文本生成任务，探索对物种主义合理化的扩展或抵抗。

Result: 在基准测试中，LLMs能够可靠地检测到物种主义陈述，但很少谴责它们，通常将物种主义态度视为道德上可接受的。在心理测量中，结果是混合的：LLMs表现出比人类稍低的显性物种主义，但在直接权衡中更倾向于拯救一个人而不是多个动物。在开放式的文本生成任务中，LLMs经常为农场动物的伤害辩护或合理化，而对非农场动物则拒绝这样做。

Conclusion: 研究结果表明，尽管LLMs反映了进步和主流的人类观点，但它们仍然复制了围绕动物剥削的根深蒂固的文化规范。因此，将非人类道德患者纳入AI公平性和对齐框架是减少这些偏见和防止物种主义态度在AI系统及其影响的社会中根深蒂固的关键。

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>


### [39] [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
*Maria Ryskina,Greta Tuckute,Alexander Fung,Ashley Malkin,Evelina Fedorenko*

Main category: cs.CL

TL;DR: 研究探讨了语言模型与大脑对语言和概念意义的表示之间的关系，发现语言模型可能内部表示跨模态的概念意义。


<details>
  <summary>Details</summary>
Motivation: 认知科学和神经科学一直面临分离语言表示和概念意义表示的挑战，而这一问题同样出现在现代语言模型中。

Method: 通过分析fMRI数据集，研究了语言模型与大脑激活水平以及跨模态意义一致性之间的关系。

Result: 语言模型在大脑中意义一致性较高的区域表现更好，即使这些区域对语言处理不敏感。

Conclusion: 语言模型可能内部表示跨模态的概念意义。

Abstract: Cognitive science and neuroscience have long faced the challenge of
disentangling representations of language from representations of conceptual
meaning. As the same problem arises in today's language models (LMs), we
investigate the relationship between LM--brain alignment and two neural
metrics: (1) the level of brain activation during processing of sentences,
targeting linguistic processing, and (2) a novel measure of meaning consistency
across input modalities, which quantifies how consistently a brain region
responds to the same concept across paradigms (sentence, word cloud, image)
using an fMRI dataset (Pereira et al., 2018). Our experiments show that both
language-only and language-vision models predict the signal better in more
meaning-consistent areas of the brain, even when these areas are not strongly
sensitive to language processing, suggesting that LMs might internally
represent cross-modal conceptual meaning.

</details>


### [40] [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
*Jinpeng Hu,Ao Wang,Qianqian Xie,Hui Ma,Zhuo Li,Dan Guo*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体框架用于心理健康评估，通过模拟临床对话和自适应提问机制，提高了信息提取和上下文跟踪能力，并在数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于临床医生的评估方法受限于专业人员短缺，而现有方法依赖静态文本分析，无法捕捉动态互动和迭代提问中的深入见解。因此，需要一种更有效的自动化心理评估方法。

Method: 本文提出了一种多智能体框架，模拟临床医生与患者的对话，包括提问、充分性评估、评分和更新等专门代理。引入了自适应提问机制，评估用户回答的充分性以生成针对性的后续问题，并采用树状结构记忆来组织关键信息并动态更新。

Result: 实验结果表明，本文提出的方法在DAIC-WOZ数据集上表现优于现有方法，能够有效提取信息并跟踪上下文。

Conclusion: 本文提出的多智能体框架在DAIC-WOZ数据集上表现出优于现有方法的效果，证明了其在心理健康评估中的有效性。

Abstract: Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

</details>


### [41] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文介绍了一种名为DR.SAF的动态推理边界自我意识框架，该框架使模型能够动态评估和调整其推理深度以应对问题复杂性。通过三个关键组件：边界自我意识对齐、自适应奖励管理和边界保持机制，该框架优化了推理过程，在不牺牲性能的情况下平衡了效率和准确性。实验结果表明，DR.SAF在总响应标记数减少了49.27%，同时在标记效率上提高了6.59倍，并减少了5倍的训练时间，使其非常适合资源受限的环境。在极端训练中，DR.SAF甚至可以在标记效率上超越传统的基于指令的模型，并且准确率提高了超过16%。


<details>
  <summary>Details</summary>
Motivation: Recent advancements in large language models (LLMs) have greatly improved their capabilities on complex reasoning tasks through Long Chain-of-Thought (CoT). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. To improve the efficiency, current methods often rely on human-defined difficulty priors, which do not align with the LLM's self-aware difficulty, leading to inefficiencies.

Method: DR. SAF integrates three key components: Boundary Self-Awareness Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism. These components allow models to optimize their reasoning processes, balancing efficiency and accuracy without compromising performance.

Result: Our experimental results demonstrate that DR. SAF achieves a 49.27% reduction in total response tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain in token efficiency and a 5x reduction in training time, making it well-suited to resource-limited settings. During extreme training, DR. SAF can even surpass traditional instruction-based models in token efficiency with more than 16% accuracy improvement.

Conclusion: DR. SAF achieves a 49.27% reduction in total response tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain in token efficiency and a 5x reduction in training time, making it well-suited to resource-limited settings. During extreme training, DR. SAF can even surpass traditional instruction-based models in token efficiency with more than 16% accuracy improvement.

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>


### [42] [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
*Greta Tuckute,Klemen Kotar,Evelina Fedorenko,Daniel L. K. Yamins*

Main category: cs.CL

TL;DR: AuriStream是一种受生物启发的两阶段框架，用于语音表示学习，能够在多种语音任务中表现出色，并提供对模型预测的见解。


<details>
  <summary>Details</summary>
Motivation: 为了开发更类人的模型，这些模型能够高效处理各种基于语音的任务，需要一种新的语音表示学习方法。

Method: AuriStream是一种受生物启发的模型，通过受人类听觉处理层次结构启发的两阶段框架对语音进行编码。第一阶段将原始音频转换为基于人类耳蜗的时间-频率表示，从中提取离散的耳蜗标记。第二阶段在耳蜗标记上应用自回归序列模型。

Result: AuriStream在多种下游SUPERB语音任务中表现出色。它生成的音频延续可以在频谱图空间中可视化并解码回音频，为模型的预测提供了见解。

Conclusion: 我们提出了一种两阶段框架，用于语音表示学习，以推动更类人的模型的发展，这些模型能够高效处理各种基于语音的任务。

Abstract: We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

</details>


### [43] [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
*Rob Reijtenbach,Suzan Verberne,Gijs Wijnholds*

Main category: cs.CL

TL;DR: 本文提出并验证了一个新的合成数据集用于训练视觉蕴含模型。通过使用Stable Diffusion生成图像替换文本前提，构建合成数据集。结果表明，在数据稀疏的情况下，合成数据可以作为训练视觉蕴含模型的有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉蕴含数据集比文本蕴含数据集小且稀疏，手动创建数据集劳动密集。

Method: 基于SNLI数据集的文本前提文本，使用生成图像模型Stable Diffusion创建图像来替换每个文本前提，从而构建合成数据集。

Result: 在SNLI-VE上，合成训练数据的F-score为0.686，而真实数据的F-score为0.703；在SICK-VTE上，合成数据的F-score为0.384，而原始数据的F-score为0.400。

Conclusion: 在数据稀疏的情况下，合成数据可以是训练视觉蕴含模型的有前景的解决方案。

Abstract: In this paper we present and validate a new synthetic dataset for training
visual entailment models. Existing datasets for visual entailment are small and
sparse compared to datasets for textual entailment. Manually creating datasets
is labor-intensive. We base our synthetic dataset on the SNLI dataset for
textual entailment. We take the premise text from SNLI as input prompts in a
generative image model, Stable Diffusion, creating an image to replace each
textual premise. We evaluate our dataset both intrinsically and extrinsically.
For extrinsic evaluation, we evaluate the validity of the generated images by
using them as training data for a visual entailment classifier based on CLIP
feature vectors. We find that synthetic training data only leads to a slight
drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when
trained on real data. We also compare the quality of our generated training
data to original training data on another dataset: SICK-VTE. Again, there is
only a slight drop in F-score: from 0.400 to 0.384. These results indicate that
in settings with data sparsity, synthetic data can be a promising solution for
training visual entailment models.

</details>


### [44] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: 本文介绍了TinyTim，一个针对詹姆斯·乔伊斯的《芬尼根的守灵夜》微调的大规模语言模型，展示了其在生成文本上的独特特征，并探讨了其在创造性任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索专门语言模型在创造性任务中的潜在应用，特别是在处理复杂文学作品时的表现。

Method: 通过定量评估与基线模型进行比较，分析了TinyTim V1生成的文本特征。

Result: TinyTim V1表现出高词汇多样性和低语义连贯性，这与创造力和复杂问题解决理论相符合。

Conclusion: 这些发现表明，专门的模型可以在更广泛的创造性架构中作为发散知识来源，推动不同环境中的自动化发现机制。

Abstract: This work introduces TinyTim, a family of large language models fine-tuned on
James Joyce's `Finnegans Wake'. Through quantitative evaluation against
baseline models, we demonstrate that TinyTim V1 produces a statistically
distinct generative profile characterized by high lexical diversity and low
semantic coherence. These findings are interpreted through theories of
creativity and complex problem-solving, arguing that such specialized models
can function as divergent knowledge sources within more extensive creative
architectures, powering automated discovery mechanisms in diverse settings.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [Empowering Multimodal LLMs with External Tools: A Comprehensive Survey](https://arxiv.org/abs/2508.10955)
*Wenbin An,Jiahao Nie,Yaqiang Wu,Feng Tian,Shijian Lu,Qinghua Zheng*

Main category: cs.CV

TL;DR: 本文综述了利用外部工具增强多模态大语言模型（MLLM）性能的方法，讨论了四个关键维度，并指出了当前的局限性和未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在多模态任务中取得了显著成功，但多模态数据质量有限、在许多复杂下游任务中表现不佳以及评估协议不足仍然阻碍了其可靠性和广泛应用。受人类利用外部工具进行推理和解决问题的能力启发，将外部工具（如API、专家模型和知识库）增强到MLLM中是一种有希望的策略。

Method: 本文对利用外部工具增强MLLM性能进行了全面的综述，讨论了四个关键维度：(1) 外部工具如何促进高质量多模态数据的获取和标注；(2) 外部工具如何帮助提高MLLM在具有挑战性的下游任务中的性能；(3) 外部工具如何实现对MLLM的全面和准确评估；(4) 工具增强的MLLM当前的局限性和未来方向。

Result: 本文提供了一个关于利用外部工具增强MLLM性能的全面综述，并探讨了四个关键维度。

Conclusion: 本文强调了外部工具在提升MLLM能力方面的变革潜力，并提供了其发展和应用的前瞻性视角。

Abstract: By integrating the perception capabilities of multimodal encoders with the
generative power of Large Language Models (LLMs), Multimodal Large Language
Models (MLLMs), exemplified by GPT-4V, have achieved great success in various
multimodal tasks, pointing toward a promising pathway to artificial general
intelligence. Despite this progress, the limited quality of multimodal data,
poor performance on many complex downstream tasks, and inadequate evaluation
protocols continue to hinder the reliability and broader applicability of MLLMs
across diverse domains. Inspired by the human ability to leverage external
tools for enhanced reasoning and problem-solving, augmenting MLLMs with
external tools (e.g., APIs, expert models, and knowledge bases) offers a
promising strategy to overcome these challenges. In this paper, we present a
comprehensive survey on leveraging external tools to enhance MLLM performance.
Our discussion is structured along four key dimensions about external tools:
(1) how they can facilitate the acquisition and annotation of high-quality
multimodal data; (2) how they can assist in improving MLLM performance on
challenging downstream tasks; (3) how they enable comprehensive and accurate
evaluation of MLLMs; (4) the current limitations and future directions of
tool-augmented MLLMs. Through this survey, we aim to underscore the
transformative potential of external tools in advancing MLLM capabilities,
offering a forward-looking perspective on their development and applications.
The project page of this paper is publicly available
athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.

</details>


### [46] [Can Multi-modal (reasoning) LLMs detect document manipulation?](https://arxiv.org/abs/2508.11021)
*Zisheng Liang,Kidus Zewde,Rudra Pratap Singh,Disha Patil,Zexi Chen,Jiayu Xue,Yao Yao,Yifei Chen,Qinzhe Liu,Simiao Ren*

Main category: cs.CV

TL;DR: 本文研究了多种多模态大语言模型在检测欺诈文档中的效果，发现顶级模型在零样本泛化方面表现优异，但模型大小和高级推理能力与检测准确性之间的关联有限。


<details>
  <summary>Details</summary>
Motivation: 文档欺诈对依赖安全和可验证文档的行业构成重大威胁，需要强大的检测机制。

Method: 本文研究了多种多模态大语言模型在检测欺诈文档中的效果，通过提示优化和对模型推理过程的详细分析，评估了它们识别欺诈的细微迹象的能力。

Result: 结果表明，表现最好的多模态大语言模型展示了优越的零样本泛化能力，在分布外数据集上优于传统方法，而一些视觉语言模型表现出不一致或较差的性能。

Conclusion: 本文强调了多模态大语言模型在增强文档欺诈检测系统方面的潜力，并为未来可解释和可扩展的欺诈缓解策略研究提供了基础。

Abstract: Document fraud poses a significant threat to industries reliant on secure and
verifiable documentation, necessitating robust detection mechanisms. This study
investigates the efficacy of state-of-the-art multi-modal large language models
(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,
Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and
3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against
each other and prior work on document fraud detection techniques using a
standard dataset with real transactional documents. Through prompt optimization
and detailed analysis of the models' reasoning processes, we evaluate their
ability to identify subtle indicators of fraud, such as tampered text,
misaligned formatting, and inconsistent transactional sums. Our results reveal
that top-performing multi-modal LLMs demonstrate superior zero-shot
generalization, outperforming conventional methods on out-of-distribution
datasets, while several vision LLMs exhibit inconsistent or subpar performance.
Notably, model size and advanced reasoning capabilities show limited
correlation with detection accuracy, suggesting task-specific fine-tuning is
critical. This study underscores the potential of multi-modal LLMs in enhancing
document fraud detection systems and provides a foundation for future research
into interpretable and scalable fraud mitigation strategies.

</details>


### [47] [A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations](https://arxiv.org/abs/2508.11141)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比学习的跨模态谣言检测方法，通过多尺度图像和上下文的相关性分析，提高了谣言检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测方法往往忽略了图像内容以及不同视觉尺度下上下文与图像之间的内在关系，导致关键信息的丢失。

Method: 本文提出了一种基于对比学习的跨模态谣言检测方案，包括SCLIP编码器、跨模态多尺度对齐模块和尺度感知融合网络。

Result: 在两个真实数据集上的实验结果表明，该方法在谣言检测任务中显著优于现有的最先进方法。

Conclusion: 本文提出的多尺度图像和上下文相关性探索算法（MICC）在谣言检测任务中表现出色，展示了其有效性和实际应用潜力。

Abstract: Existing rumor detection methods often neglect the content within images as
well as the inherent relationships between contexts and images across different
visual scales, thereby resulting in the loss of critical information pertinent
to rumor identification. To address these issues, this paper presents a novel
cross-modal rumor detection scheme based on contrastive learning, namely the
Multi-scale Image and Context Correlation exploration algorithm (MICC).
Specifically, we design an SCLIP encoder to generate unified semantic
embeddings for text and multi-scale image patches through contrastive
pretraining, enabling their relevance to be measured via dot-product
similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is
introduced to identify image regions most relevant to the textual semantics,
guided by mutual information maximization and the information bottleneck
principle, through a Top-K selection strategy based on a cross-modal relevance
matrix constructed between the text and multi-scale image patches. Moreover, a
scale-aware fusion network is designed to integrate the highly correlated
multi-scale image features with global text features by assigning adaptive
weights to image regions based on their semantic importance and cross-modal
relevance. The proposed methodology has been extensively evaluated on two
real-world datasets. The experimental results demonstrate that it achieves a
substantial performance improvement over existing state-of-the-art approaches
in rumor detection, highlighting its effectiveness and potential for practical
applications.

</details>


### [48] [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
*Oscar Mañas,Pierluca D'Oro,Koustuv Sinha,Adriana Romero-Soriano,Michal Drozdzal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 本文提出了一种新的奖励引导解码方法，用于改进多模态大语言模型的视觉定位能力，并在标准物体幻觉基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型（MLLMs）的应用越来越广泛，适应它们以满足各种用户需求变得越来越重要。本文研究了通过受控解码来适应MLLM的方法。

Method: 我们引入了第一个用于MLLM的奖励引导解码方法，并展示了其在改进视觉定位中的应用。我们的方法涉及构建用于视觉定位的奖励模型，并利用它们来指导MLLM的解码过程。具体来说，我们构建了两个独立的奖励模型，分别控制模型输出中的对象精度和召回率。

Result: 我们的方法在标准物体幻觉基准测试中表现出色，提供了对MLLM推理的显著可控性，并且持续优于现有的幻觉缓解方法。

Conclusion: 我们的方法在标准物体幻觉基准测试中表现出色，提供了对MLLM推理的显著可控性，并且持续优于现有的幻觉缓解方法。

Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [49] [The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers](https://arxiv.org/abs/2506.20844)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: 本文分析了科学事实核查的挑战，并提出了改进的方法，旨在开发一个专门的信息检索系统以提升其性能。


<details>
  <summary>Details</summary>
Motivation: 科学事实核查比一般的事实核查更为复杂，因为它需要适应科学知识的演变、学术文献的结构复杂性以及长篇幅、多模态科学表达带来的挑战。然而，现有的方法主要基于小规模数据集，避免了处理完整文档的独特挑战。

Method: 本文分析了当前科学事实核查系统的局限性，并探讨了可能利用的特征和资源以提高其性能。

Result: 本文识别了证据检索中的关键研究挑战，并进行了初步实验以验证这些挑战并找到潜在解决方案。

Conclusion: 本文旨在通过专门的信息检索系统推动科学事实核查的发展，以适应现实世界的应用需求。

Abstract: Scientific fact-checking aims to determine the veracity of scientific claims
by retrieving and analysing evidence from research literature. The problem is
inherently more complex than general fact-checking since it must accommodate
the evolving nature of scientific knowledge, the structural complexity of
academic literature and the challenges posed by long-form, multimodal
scientific expression. However, existing approaches focus on simplified
versions of the problem based on small-scale datasets consisting of abstracts
rather than full papers, thereby avoiding the distinct challenges associated
with processing complete documents. This paper examines the limitations of
current scientific fact-checking systems and reveals the many potential
features and resources that could be exploited to advance their performance. It
identifies key research challenges within evidence retrieval, including (1)
evidence-driven retrieval that addresses semantic limitations and topic
imbalance (2) time-aware evidence retrieval with citation tracking to mitigate
outdated information, (3) structured document parsing to leverage long-range
context, (4) handling complex scientific expressions, including tables,
figures, and domain-specific terminology and (5) assessing the credibility of
scientific literature. Preliminary experiments were conducted to substantiate
these challenges and identify potential solutions. This perspective paper aims
to advance scientific fact-checking with a specialised IR system tailored for
real-world applications.

</details>


### [50] [PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing](https://arxiv.org/abs/2508.11116)
*Zhuoqun Li,Xuanang Chen,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun*

Main category: cs.IR

TL;DR: PaperRegister is a new system that improves paper search by using a hierarchical index tree, allowing for more flexible and detailed queries.


<details>
  <summary>Details</summary>
Motivation: Previous paper search systems are unable to meet flexible-grained requirements as they mainly collect paper abstracts to construct an index of corpus, which lack detailed information to support retrieval by finer-grained queries.

Method: PaperRegister consists of offline hierarchical indexing and online adaptive retrieval, transforming traditional abstract-based index into a hierarchical index tree for paper search.

Result: Experiments on paper search tasks across a range of granularity demonstrate that PaperRegister achieves state-of-the-art performance, particularly excelling in fine-grained scenarios.

Conclusion: PaperRegister demonstrates good potential as an effective solution for flexible-grained paper search in real-world applications.

Abstract: Paper search is an important activity for researchers, typically involving
using a query with description of a topic to find relevant papers. As research
deepens, paper search requirements may become more flexible, sometimes
involving specific details such as module configuration rather than being
limited to coarse-grained topics. However, previous paper search systems are
unable to meet these flexible-grained requirements, as these systems mainly
collect paper abstracts to construct index of corpus, which lack detailed
information to support retrieval by finer-grained queries. In this work, we
propose PaperRegister, consisted of offline hierarchical indexing and online
adaptive retrieval, transforming traditional abstract-based index into
hierarchical index tree for paper search, thereby supporting queries at
flexible granularity. Experiments on paper search tasks across a range of
granularity demonstrate that PaperRegister achieves the state-of-the-art
performance, and particularly excels in fine-grained scenarios, highlighting
the good potential as an effective solution for flexible-grained paper search
in real-world applications. Code for this work is in
https://github.com/Li-Z-Q/PaperRegister.

</details>


### [51] [+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking](https://arxiv.org/abs/2508.11122)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: 本文提出了一种新的方法+VeriRel，用于科学事实核查，通过整合验证反馈来提高文档相关性评估的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于基于相关性的文档排序，而不是提供支持或反驳声明的证据。

Method: +VeriRel方法，它在文档排名中包括了验证成功因素。

Result: 在三个科学事实核查数据集（SciFact、SciFact-Open和Check-Covid）上的实验结果表明，+VeriRel在文档证据检索方面表现出色，并对下游验证产生了积极影响。

Conclusion: 本文展示了将验证反馈整合到文档相关性评估中的潜力，为有效的科学事实核查系统提供了前景，并提出了在复杂文档中进行细粒度相关性评估的未来工作方向。

Abstract: Identification of appropriate supporting evidence is critical to the success
of scientific fact checking. However, existing approaches rely on off-the-shelf
Information Retrieval algorithms that rank documents based on relevance rather
than the evidence they provide to support or refute the claim being checked.
This paper proposes +VeriRel which includes verification success in the
document ranking. Experimental results on three scientific fact checking
datasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently
leading performance by +VeriRel for document evidence retrieval and a positive
impact on downstream verification. This study highlights the potential of
integrating verification feedback to document relevance assessment for
effective scientific fact checking systems. It shows promising future work to
evaluate fine-grained relevance when examining complex documents for advanced
scientific fact checking.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [52] [Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style](https://arxiv.org/abs/2508.11187)
*Wonjune Kang,Deb Roy*

Main category: eess.AS

TL;DR: 本文介绍了表达性语音检索任务，通过将语音和文本嵌入到联合潜在空间中，利用自然语言描述的风格进行语音检索，并在多个数据集上取得了良好的结果。


<details>
  <summary>Details</summary>
Motivation: 之前的工作主要集中在基于语音内容进行语音检索，而我们旨在基于语音方式（即如何说）进行检索。

Method: 我们训练语音和文本编码器将语音和描述说话风格的文本嵌入到一个联合潜在空间中，这使得可以使用自由形式的文本提示（描述情感或风格）作为查询来检索匹配的表达性语音片段。

Result: 我们的方法在多个数据集上实现了强大的检索性能，证明了其有效性。

Conclusion: 我们的方法在多个包含22种说话风格的数据集上实现了强大的检索性能，如Recall@k所衡量的那样。

Abstract: We introduce the task of expressive speech retrieval, where the goal is to
retrieve speech utterances spoken in a given style based on a natural language
description of that style. While prior work has primarily focused on performing
speech retrieval based on what was said in an utterance, we aim to do so based
on how something was said. We train speech and text encoders to embed speech
and text descriptions of speaking styles into a joint latent space, which
enables using free-form text prompts describing emotions or styles as queries
to retrieve matching expressive speech segments. We perform detailed analyses
of various aspects of our proposed framework, including encoder architectures,
training criteria for effective cross-modal alignment, and prompt augmentation
for improved generalization to arbitrary text queries. Experiments on multiple
datasets encompassing 22 speaking styles demonstrate that our approach achieves
strong retrieval performance as measured by Recall@k.

</details>


### [53] [Emphasis Sensitivity in Speech Representations](https://arxiv.org/abs/2508.11566)
*Shaun Cassini,Thomas Hain,Anton Ragni*

Main category: eess.AS

TL;DR: 该研究探讨了现代语音模型是否对语调强调敏感，并提出了一个基于残差的框架来分析强调的结构化编码。


<details>
  <summary>Details</summary>
Motivation: 先前的工作通常依赖于孤立的声学相关因素或标签预测，这忽略了强调的关联结构。

Method: 提出了一种基于残差的框架，将强调定义为成对的中性词和强调词表示之间的差异。

Result: 分析表明，这些残差与持续时间变化密切相关，并且在词身份预测方面表现不佳，表明了语音强调的结构化、关联编码。在ASR微调模型中，残差占据的子空间比预训练模型大50%。

Conclusion: 该研究发现，强调作为一致的、低维变换被编码，并且在任务特定的学习中变得更加结构化。

Abstract: This work investigates whether modern speech models are sensitive to prosodic
emphasis - whether they encode emphasized and neutral words in systematically
different ways. Prior work typically relies on isolated acoustic correlates
(e.g., pitch, duration) or label prediction, both of which miss the relational
structure of emphasis. This paper proposes a residual-based framework, defining
emphasis as the difference between paired neutral and emphasized word
representations. Analysis on self-supervised speech models shows that these
residuals correlate strongly with duration changes and perform poorly at word
identity prediction, indicating a structured, relational encoding of prosodic
emphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more
compact than in pre-trained models, further suggesting that emphasis is encoded
as a consistent, low-dimensional transformation that becomes more structured
with task-specific learning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: 本文介绍了BeyondWeb，一个用于预训练的高质量合成数据生成框架。BeyondWeb在多个基准测试中表现出色，比现有的合成数据集表现更好，并且训练速度更快。研究还揭示了合成数据在预训练中的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管使用合成数据进行预训练已成为一种有前景的范式，但影响合成数据质量的因素仍然不明确。

Method: 我们引入了BeyondWeb，这是一种合成数据生成框架，用于预训练生成高质量的合成数据。

Result: BeyondWeb显著扩展了传统网络规模数据集的能力，在14个基准评估中平均超过了最先进的合成预训练数据集Cosmopedia和Nemotron-Synth分别提高了5.1个百分点和2.6个百分点。它比开放网络数据快7.7倍，比Nemotron-Synth快2.7倍。

Conclusion: 我们的工作表明，生成高质量的合成预训练数据没有万能的方法。最佳结果需要联合优化许多因素，这是一个需要严谨科学和实践专业知识的挑战性任务。

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [55] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为M&C的模型选择框架，用于高效地从模型平台中选择最适合目标数据域的预训练文本到图像模型，而无需逐一微调所有模型。


<details>
  <summary>Details</summary>
Motivation: 用户面临的问题是，如何选择最适合目标数据域的预训练文本到图像模型进行微调。现有的模型选择方法在分类任务中已得到解决，但在预训练文本到图像模型及其在目标领域的性能指示方面知之甚少。

Method: M&C框架基于匹配图，其中包含可用模型和特征数据集的节点，以及表示微调性能和数据相似性的模型-数据对和数据-数据对的边。然后构建一个模型，该模型基于模型/数据特征和从匹配图中提取的图嵌入特征，预测目标领域微调后质量最好的模型。

Result: M&C在选择十个文本到图像模型中的最佳模型时，在32个数据集上取得了成功预测61.3%的情况，并且在其余情况下预测了一个表现接近的模型。

Conclusion: M&C成功预测了61.3%的情况下最佳模型，并且在其余情况下预测了一个表现接近的模型。

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [56] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: 本文通过因果抽象理论探讨计算实现和表征的作用，并强调其与泛化和预测的联系。


<details>
  <summary>Details</summary>
Motivation: 为了理解系统如何在内部实现特定计算，本文提出使用因果抽象作为研究视角。

Method: 本文利用因果抽象理论，结合深度学习中的讨论，探讨计算实现和表征的作用。

Result: 本文展示了因果抽象如何帮助理解计算实现，并揭示了表征在其中的角色。

Conclusion: 本文认为，通过因果抽象可以更好地理解计算实现和表征的作用，并强调这些议题与泛化和预测的联系。

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [57] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种从封闭权重大型语言模型中推导公平分类器的框架，通过提示方式获取特征并应用公平算法，实验表明该框架在准确性和公平性之间表现出良好的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的公平方法在封闭权重大型语言模型的上下文学习设置中不再适用，因此需要一种新的方法来确保公平性。

Method: 本文提出了一种框架，将大型语言模型作为特征提取器，通过设计特定的提示来获取概率预测（如token日志概率）作为足够统计量，然后应用公平算法训练轻量级公平分类器。

Result: 实验结果表明，该框架在五个数据集上表现出强大的准确性和公平性权衡，特别是在数据效率方面优于基于LLM嵌入或原始表格特征的公平分类器。

Conclusion: 本文提出了一种从封闭权重大型语言模型中推导公平分类器的框架，通过提示方式获取特征，并应用公平算法训练轻量级公平分类器。实验表明，该框架在准确性和公平性之间表现出良好的权衡，并且在数据效率方面优于基于LLM嵌入或原始表格特征的公平分类器。

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [58] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: 本文提出HS-GPPT模型，通过频谱对齐解决图预训练和提示调优中的知识迁移问题，并在多种设置下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于基于同质性的低频知识，在处理具有不同同质性的现实世界图时效果不佳。本文旨在解决这一问题，通过频谱对齐实现更有效的知识迁移。

Method: 本文提出了HS-GPPT模型，利用混合频谱滤波器主干和局部-全局对比学习来获取丰富的频谱知识，并设计了提示图以对齐频谱分布，促进跨同质性和异质性的频谱知识迁移。

Result: 实验结果表明，HS-GPPT模型在归纳和演绎学习设置下均表现出色，验证了其有效性。

Conclusion: 本文提出了一种新的框架HS-GPPT，通过确保预训练和提示调优过程中的频谱对齐来解决现实世界图中不同同质性的问题。实验验证了该方法在归纳和演绎学习设置下的有效性。

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [59] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 本文提出一个新的数据集，评估大型推理模型在处理不完整问题时的表现，发现它们无法主动询问信息，并探讨了监督微调的潜力和挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的评估设置存在一个关键差距，因为真正的智能代理不仅应该解决问题，还应该在问题信息不足时主动询问信息，以响应用户的需求。

Method: 本文提出了一个新的数据集，包含两种类型的不完整问题，并基于该数据集对大型推理模型进行了系统评估。

Result: 评估结果揭示了大型推理模型在主动询问信息方面的无能，同时发现了与过度思考和幻觉相关的行为，并强调了监督微调在学习这种能力中的潜力和挑战。

Conclusion: 本文希望为开发具有真正智能的大型推理模型提供新的见解，而不仅仅是解决问题。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [60] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: Inclusion Arena is a live leaderboard that ranks models based on human feedback from AI-powered applications, using the Bradley-Terry model with innovations to ensure reliable and stable rankings.


<details>
  <summary>Details</summary>
Motivation: To bridge the critical gap between existing benchmarks and real-world applications, Inclusion Arena is presented as a live leaderboard that ranks models based on human feedback collected directly from AI-powered applications.

Method: Inclusion Arena uses the Bradley-Terry model augmented with two key innovations: (1) Placement Matches, a cold-start mechanism to quickly estimate initial ratings for newly integrated models, and (2) Proximity Sampling, an intelligent comparison strategy that prioritizes battles between models of similar capabilities to maximize information gain and enhance rating stability.

Result: Extensive empirical analyses and simulations demonstrate that Inclusion Arena yields reliable and stable rankings, exhibits higher data transitivity compared to general crowdsourced datasets, and significantly mitigates the risk of malicious manipulation.

Conclusion: Inclusion Arena aims to accelerate the development of LLMs and MLLMs truly optimized for practical, user-centric deployments by fostering an open alliance between foundation models and real-world applications.

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [61] [Benchmarking Prosody Encoding in Discrete Speech Tokens](https://arxiv.org/abs/2508.11224)
*Kentaro Onda,Satoru Fukayama,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.SD

TL;DR: 本文研究了离散标记在语音语言模型中的语调编码能力，并提供了设计离散标记的实用指南。


<details>
  <summary>Details</summary>
Motivation: 由于离散标记通常是预先学习的，与语言模型或下游任务的训练分开，因此需要进行启发式选择，如使用的SSL模型或聚类数量。此外，语音语言模型需要理解并生成反映语义内容和语调特征的响应，但目前对离散标记捕捉语调信息的能力研究有限。

Method: 本文进行了全面分析，重点关注基于其对人工修改的语调的敏感性的语调编码。

Result: 本文通过分析离散标记对人工修改的语调的敏感性，提供了设计离散标记的实用指南。

Conclusion: 本文通过全面分析基于其对人工修改的语调的敏感性，旨在为设计离散标记提供实用指南。

Abstract: Recently, discrete tokens derived from self-supervised learning (SSL) models
via k-means clustering have been actively studied as pseudo-text in speech
language models and as efficient intermediate representations for various
tasks. However, these discrete tokens are typically learned in advance,
separately from the training of language models or downstream tasks. As a
result, choices related to discretization, such as the SSL model used or the
number of clusters, must be made heuristically. In particular, speech language
models are expected to understand and generate responses that reflect not only
the semantic content but also prosodic features. Yet, there has been limited
research on the ability of discrete tokens to capture prosodic information. To
address this gap, this study conducts a comprehensive analysis focusing on
prosodic encoding based on their sensitivity to the artificially modified
prosody, aiming to provide practical guidelines for designing discrete tokens.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [62] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: 本文研究了如何利用预训练的代码扩散模型来解决最后一步修复的问题，并通过两种应用进行了验证。


<details>
  <summary>Details</summary>
Motivation: 本文动机是评估代码扩散模型在最后一步修复问题中的潜在应用，因为扩散过程中后期步骤的代码片段差异看起来像对损坏或不完整代码的最后一步修复。

Method: 本文方法包括两种应用：1. 通过向损坏的代码片段添加噪声并恢复扩散过程来利用扩散模型进行最后一步修复；2. 通过从扩散过程中采样中间程序（输入）和最终程序（输出）来生成任意数量的训练数据。

Result: 本文在三个领域（Python、Excel 和 PowerShell）进行了实验，以评估应用并分析属性。结果表明，可以利用预训练的代码扩散模型来解决最后一步修复的问题。

Conclusion: 本文结论是，可以利用预训练的代码扩散模型来解决最后一步修复的问题，通过添加噪声并恢复扩散过程，或者通过采样中间程序和最终程序来生成任意数量的训练数据。

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [63] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: 本文介绍了ORFuzz，这是一个用于系统检测和分析大型语言模型（LLM）过度拒绝行为的进化测试框架。ORFuzz通过三个核心组件提高了测试效果，并生成了一个新的基准数据集ORFuzzSet，显著提升了测试效率。


<details>
  <summary>Details</summary>
Motivation: Current methods for testing LLM over-refusal are inadequate, suffering from flawed benchmarks and limited test generation capabilities.

Method: ORFuzz, an evolutionary testing framework that integrates three core components: (1) safety category-aware seed selection, (2) adaptive mutator optimization using reasoning LLMs, and (3) OR-Judge, a human-aligned judge model.

Result: ORFuzz generates diverse, validated over-refusal instances at a rate (6.98% average) more than double that of leading baselines. ORFuzzSet, a new benchmark of 1,855 highly transferable test cases, achieves a superior 63.56% average over-refusal rate across 10 diverse LLMs.

Conclusion: ORFuzz and ORFuzzSet provide a robust automated testing framework and a valuable community resource, paving the way for developing more reliable and trustworthy LLM-based software systems.

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>

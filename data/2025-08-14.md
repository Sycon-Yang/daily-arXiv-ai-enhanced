<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: 本文提出了ParallelSearch，一种新的强化学习框架，使大型语言模型能够识别可并行查询结构并同时执行多个搜索操作。实验表明，该方法在多个问答基准测试中优于最先进的基线，特别是在可并行的问题上表现更优，同时减少了LLM调用次数。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习与可验证奖励（RLVR）的推理增强搜索代理存在根本性的架构限制：它们严格按顺序处理搜索查询，即使处理本质上可以并行化和逻辑上独立的比较时也是如此。这种顺序瓶颈显著限制了计算效率，特别是对于需要多个实体比较的查询。

Method: 提出了一种新的强化学习框架ParallelSearch，使大型语言模型能够识别可并行查询结构并同时执行多个搜索操作。引入了专门的奖励函数，以激励识别独立查询组件，同时通过综合考虑正确性、查询分解质量和并行执行优势来保持答案准确性。

Result: 全面的实验表明，ParallelSearch在七个问答基准测试中平均性能比最先进的基线高出2.9%。在可并行的问题上，该方法的性能提高了12.7%，同时仅需Sequential方法69.6%的LLM调用次数。

Conclusion: ParallelSearch在多个问答基准测试中优于最先进的基线，平均性能提升了2.9%。特别是在可并行的问题上，该方法的性能提升了12.7%，同时仅需Sequential方法69.6%的LLM调用次数。

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [2] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: 本文评估了GPT-4o在罕见疾病NER中的能力，通过多种提示策略进行优化，结果显示其性能优越，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 命名实体识别（NER）在罕见疾病领域面临独特的挑战，包括有限的标记数据、实体类型之间的语义歧义以及长尾分布。因此，需要探索有效的解决方案来应对这些挑战。

Method: 我们评估了GPT-4o在低资源设置下的罕见疾病NER能力，使用了一系列基于提示的策略，包括零样本提示、少量样本上下文学习、检索增强生成（RAG）和任务级微调。我们设计了一个结构化提示框架，编码了特定领域的知识和消歧规则，并引入了两种语义引导的少量样本示例选择方法以提高上下文性能并减少标记工作量。

Result: 实验表明，GPT-4o在RareDis Corpus上的表现与BioClinicalBERT相当或更优，任务级微调产生了新的最先进（SOTA）结果。成本效益分析显示，少量样本提示在低令牌预算下提供了高回报，而RAG仅带来边际额外好处。错误分类法突显了边界漂移和类型混淆等常见失败模式，表明了后处理和混合精炼的机会。

Conclusion: 我们的结果表明，经过提示优化的LLM可以在生物医学NER中作为传统监督模型的有效、可扩展的替代方案，特别是在标注数据稀缺的罕见疾病应用中。

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [3] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: TEN is a neurosymbolic approach for extracting tabular data from semistructured text, combining LLMs with symbolic checking to improve accuracy and reduce hallucinations.


<details>
  <summary>Details</summary>
Motivation: The task of extracting tabular data from semistructured input text is challenging due to inconsistent delimiters. Purely neural approaches struggle with hallucinations and inability to enforce constraints.

Method: TEN uses Structural Decomposition prompting on a large language model (LLM) to generate an initial table, followed by a symbolic checker to evaluate the table's well-formedness and detect hallucinations or forgetting. A critique-LLM then generates guidance for fixing the table, which is used in a self-debug loop.

Result: TEN achieves higher exact match accuracy and lower hallucination rates compared to neural baselines. A user study shows that TEN's tables are rated as more accurate and preferred for verification and correction.

Conclusion: TEN significantly outperforms purely neural baselines across multiple datasets and metrics, achieving higher exact match accuracy and lower hallucination rates. Users rate TEN's tables as more accurate and easier to verify and correct.

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [4] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: 本文提出一种无需神经影像学的计算框架，将文本情感内容映射到特定脑区，能够区分临床人群并评估AI情感表达。


<details>
  <summary>Details</summary>
Motivation: 传统神经影像学成本高且局限于实验室，而丰富的数字文本为情感-脑映射提供了新途径。以往的研究大多分别探讨基于神经影像的情感定位或计算文本分析，缺乏整合。

Method: 使用OpenAI的text-embedding-ada-002生成高维语义表示，应用降维和聚类来识别情感组，并将其映射到与情感处理相关的18个脑区。

Result: 结果表明，情感强度通过词汇分析评分，得到了具有高空间特异性的神经解剖学合理映射。抑郁受试者表现出与负性情绪相关的边缘系统参与度增加。离散情感成功区分。LLM生成的文本在基本情感分布上与人类匹配，但在共情和自我参照区域（内侧前额叶和后扣带回皮层）缺乏细微激活。

Conclusion: 本文提出了一种无需神经影像学即可将文本情感内容映射到解剖定义的脑区的计算框架。该方法在成本和可扩展性方面具有优势，能够进行大规模自然语言分析，区分临床人群，并为评估AI情感表达提供基于大脑的基准。

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [5] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: 本研究介绍并评估了HAH-Delphi框架，该框架通过结合生成式AI、小型专家小组和结构化促进来增强专家共识发展。结果表明，该框架在多个领域中表现良好，能够有效生成高质量、上下文敏感的共识。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理复杂、冲突或不足的证据时存在局限，如高面板负担、解释性过度简化和条件性细微差别被压制。这些挑战因信息过载、证据基础碎片化和对缺乏专家过滤的公开来源的依赖而加剧。

Method: 本研究引入并评估了一个名为Human-AI Hybrid Delphi (HAH-Delphi)的框架，该框架结合了生成式AI模型（Gemini 2.5 Pro）、小型高级人类专家小组和结构化促进。

Result: HAH-Delphi在三个阶段进行了测试：回顾性复制、前瞻性比较和两个应用领域（耐力训练和阻力与混合有氧/力量训练）的实际部署。AI在第一阶段复制了95%的已发表专家共识结论，在第二阶段与高级人类专家有95%的方向一致，但缺乏经验性和实用性的细微差别。第三阶段，由六名高级专家组成的紧凑小组实现了>90%的共识覆盖率，并在最终参与者之前达到了主题饱和。AI提供了稳定、基于文献的支撑，支持分歧解决并加速饱和。

Conclusion: HAH-Delphi框架提供了一种灵活、可扩展的方法，用于生成高质量、上下文敏感的共识。其在健康、教练和运动科学领域的成功应用证明了其方法学的稳健性，并支持其作为生成条件性、个性化指导和大规模发布共识框架的基础。

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [6] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过联合建模语言和声学信息，提高了语音生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本无关语音语言模型没有访问声学上下文和对声学细节的内置控制，因此本文旨在通过联合建模语言和声学信息来改进这一点。

Method: 本文使用流匹配目标来预测基于语义标记的连续向量，并研究了这种方法的设计空间，发现预测多个未来的语义标记有助于保持语言信息。

Result: 本文的方法在语言似然基准上取得了与现有模型相当的性能，同时在提示生成中提供了更好的声学细节。

Conclusion: 本文提出了一种联合建模语言和声学信息的方法，通过生成语义标记和连续的声学帧表示，实现了与现有模型相当的语言似然基准性能，并在提示生成中提供了更好的声学细节。

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [7] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: APIO is a new approach for prompt induction and optimization in tasks like Grammatical Error Correction and Text Simplification, achieving state-of-the-art results without using manually specified seed prompts.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in large language models (LLMs) have enabled a wide range of natural language processing (NLP) tasks to be performed through simple prompt-based interactions. Consequently, several approaches have been proposed to engineer prompts that most effectively enable LLMs to perform a given task.

Method: APIO, a simple but effective prompt induction and optimization approach, is proposed for the tasks of Grammatical Error Correction (GEC) and Text Simplification, without relying on manually specified seed prompts.

Result: APIO achieves a new state-of-the-art performance for purely LLM-based prompting methods on these tasks.

Conclusion: APIO achieves a new state-of-the-art performance for purely LLM-based prompting methods on the tasks of Grammatical Error Correction (GEC) and Text Simplification.

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [8] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: This paper introduces new datasets and synonym-aware measures for expanding table column abbreviations, and proposes Columbo, an LLM-based solution that outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Expanding the abbreviated column names of tables is critical for numerous downstream data tasks. This problem arises in enterprises, domain sciences, government agencies, and more.

Method: We develop Columbo, a powerful LLM-based solution that exploits context, rules, chain-of-thought reasoning, and token-level analysis.

Result: Extensive experiments show that Columbo significantly outperforms NameGuess, the current most advanced solution, by 4-29% over 5 datasets.

Conclusion: Columbo has been used in production on EDI, a major data portal for environmental sciences.

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [9] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: 本文利用Zipformer处理双语环境中的代码转换和语言识别问题，展示了其在不同后端上的鲁棒性，并取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决双语环境中代码转换和语言识别的挑战，特别是在儿童导向场景中。

Method: 使用Zipformer处理包含两种不平衡语言（普通话和英语）的语音，通过选择内部层来提取嵌入并与其他后端进行比较。

Result: 该方法有效处理不平衡数据，实现了81.89%的平衡准确率，比语言识别基线提高了15.47%。

Conclusion: 这些发现突显了变压器编码器架构模型在现实场景中的潜力。

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [10] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 本文研究了VLMs在生成图表摘要时如何放大地理经济偏见，发现高收入国家的描述更加积极，并且一些模型存在不同程度的偏见。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究VLMs在生成图表摘要时如何放大地理经济偏见，这可能导致社会危害。

Method: 本文进行了一项大规模评估，分析了6,000个图表-国家对中地理经济偏见的VLM生成图表摘要，以了解一个国家的经济状况如何影响生成摘要的情感。

Result: 研究结果表明，现有VLMs倾向于对高收入国家产生更积极的描述，即使只有国家归属变量发生变化。此外，研究还发现像GPT-4o-mini、Gemini-1.5-Flash和Phi-3.5这样的模型表现出不同程度的偏见。

Conclusion: 本文发现现有的VLMs倾向于对高收入国家产生更积极的描述，即使只有国家归属变量发生变化。此外，研究还发现像GPT-4o-mini、Gemini-1.5-Flash和Phi-3.5这样的模型表现出不同程度的偏见。最后，研究强调了这个问题的复杂性，并需要更强大的去偏策略。

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [11] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了一种新的用户导向主观排行榜（USL），并引入了可定制奖励模型（CRM），以更好地满足实际应用场景中用户的多样化需求。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM基准测试主要关注通过可验证任务评估模型能力，但这些客观且静态的基准对于实际LLM选择用处有限，难以帮助用户找到适合其需求的模型。

Method: 本文提出了用户导向的主观排行榜（USL），基于对真实人类偏好的深入研究，并引入了可定制的奖励模型（CRM）来解决现有奖励模型的局限性。

Result: CRMs在仅4B参数的情况下，表现优于GPT-4.1和Gemini-2.5-pro等领先模型，在新主题和标准上表现出色。USL展示了与矛盾偏好强烈的负相关性。

Conclusion: USL通过CRMs展现出与矛盾偏好强烈负相关的特性，证明了其在实际应用中的有效性。

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [12] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: Active Reading is a framework that trains models to study material with self-generated learning strategies, leading to better knowledge absorption and improved performance on factual QA tasks.


<details>
  <summary>Details</summary>
Motivation: Practitioners lack tools to ensure models learn a given body of knowledge reliably and consistently.

Method: Active Reading is a framework where models are trained to study a given set of material with self-generated learning strategies.

Result: Models trained with Active Reading on expert domains absorb significantly more knowledge than vanilla finetuning and other data augmentations. Expert 8B models achieved 66% on a Wikipedia-grounded subset of SimpleQA and 26% on FinanceBench using Active Reading.

Conclusion: Active Reading can be utilized at pre-training scale to build more factual models, as demonstrated by the release of Meta WikiExpert-8B, which outcompetes models with hundreds of billions of parameters on factual QA.

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [13] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为动态文档选择器（DPS）的新重排序框架，用于增强检索增强生成（RAG）系统的性能，特别适用于处理复杂的多跳查询。


<details>
  <summary>Details</summary>
Motivation: 现有的重排序模块在处理复杂的多跳查询时存在局限性，因为它们独立地对文档进行评分并选择固定数量的Top-K文档，这可能导致遗漏关键信息或引入噪声。

Method: 引入了动态文档选择器（DPS），将其作为监督学习问题来处理文档选择，并通过微调捕捉文档间的依赖关系，动态选择最相关的文档集进行生成。

Result: 在五个基准测试中，DPS的表现优于最先进的重排序器和微调方法。特别是在MuSiQue数据集上，DPS分别比Qwen3-reranker和RankingGPT的F1分数提高了30.06%和15.4%。

Conclusion: 通过启用自适应证据选择，DPS显著增强了复杂RAG场景中的推理能力。

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [14] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一种新的跨语言基于方面的情感分析方法，利用大语言模型生成高质量的伪标记数据，无需翻译工具，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数方法严重依赖往往不可靠的翻译工具来弥合语言差距，因此需要一种不需要翻译工具的新方法来提高跨语言情感分析的准确性。

Method: 本文提出了一种新方法，首先训练一个ABSA模型以获得目标语言未标记数据的预测结果，然后利用大语言模型生成更自然的句子来表示这些噪声预测，最后在生成的伪标记数据集上进一步微调ABSA模型。

Result: 该方法在六种语言和五种主干模型上验证有效，超越了之前的基于翻译的最先进方法，并且支持生成模型，微调的大语言模型表现优于较小的多语言模型。

Conclusion: 本文提出了一种新的跨语言基于方面的情感分析方法，该方法利用大语言模型生成高质量的伪标记数据，无需依赖翻译工具。实验表明，该方法在六种语言和五种主干模型上均表现出色，优于之前的基于翻译的方法，并且微调的大语言模型表现优于较小的多语言模型。

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [15] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 本文提供了一篇关于跨语言方面情感分析的全面调查，涵盖了关键任务、数据集、建模范式和跨语言迁移方法，并探讨了现有研究如何促进该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 跨语言ABSA旨在从资源丰富的语言（如英语）向低资源语言转移知识，但该领域尚未得到系统性研究。本文旨在填补这一空白，提供跨语言ABSA的全面调查。

Method: 本文提供了跨语言ABSA的全面调查，总结了关键的ABSA任务，回顾了解决这些任务的数据集、建模范式和跨语言迁移方法，并研究了现有单语和多语ABSA工作以及与LLM的ABSA如何促进跨语言ABSA的发展。

Result: 本文总结了关键的ABSA任务，包括方面术语提取、方面情感分类以及涉及多个情感元素的复合任务。此外，回顾了解决这些任务的数据集、建模范式和跨语言迁移方法，并研究了现有单语和多语ABSA工作以及与LLM的ABSA如何促进跨语言ABSA的发展。

Conclusion: 本文旨在填补这一空白，通过提供跨语言ABSA的全面调查。我们总结了关键的ABSA任务，包括方面术语提取、方面情感分类以及涉及多个情感元素的复合任务。此外，我们回顾了解决这些任务的数据集、建模范式和跨语言迁移方法。我们还研究了现有单语和多语ABSA工作以及与LLM的ABSA如何促进跨语言ABSA的发展。最后，我们指出了主要挑战，并提出了未来研究的方向以推动跨语言ABSA系统的发展。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [16] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [17] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: 本文提出了一种可控共情推理方法，通过结合自然语言推理与心理学步骤，并利用强化学习和个性化对话重写等技术，显著提升了模型在情感支持方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前模型往往缺乏基于心理学原理的深度共情推理，而情感支持对话对于促进情感健康至关重要。

Method: 我们提出了可控共情推理，结合自然语言推理与结构化的心理学步骤，并构建了一个细粒度的数据集，以实现这一能力。此外，我们采用强化学习与统一的过程-结果奖励模型进行训练，并引入基于个性的对话重写和冗余感知奖励重新加权策略来减轻响应重复性问题。

Result: 我们的方法显著提高了模型的情感支持能力，推动了共情、类人支持系统的发展。

Conclusion: 我们的方法显著提高了模型的情感支持能力，推动了共情、类人支持系统的发展。

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [18] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: 本文介绍了一种新的成员推理攻击方法——N-Gram Coverage Attack，它仅依赖于目标模型的文本输出，可以攻击完全黑盒模型。该方法在多种基准测试中表现优异，甚至可以与最先进的白盒攻击相媲美。同时，该方法的成功率随着计算资源的增加而提高。最后，我们用这种方法研究了之前未被研究的封闭OpenAI模型，发现较新的模型如GPT-4o对成员推理攻击更具鲁棒性，暗示隐私保护正在改善。


<details>
  <summary>Details</summary>
Motivation: 当前许多最先进的攻击需要访问模型的隐藏状态或概率分布，这阻碍了对更广泛使用的API访问模型（如GPT-4）的研究。因此，我们需要一种仅依赖于文本输出的成员推理攻击方法。

Method: 我们引入了N-Gram Coverage Attack，这是一种仅依赖于目标模型文本输出的成员推理攻击，使完全黑盒模型的攻击成为可能。我们利用模型更可能记忆和随后生成在其训练数据中常见文本模式的观察结果。具体来说，N-Gram Coverage Attack首先获得多个基于候选前缀的模型生成结果，然后使用n-gram重叠度量来计算和聚合这些输出与真实后缀的相似性；高相似性表明可能是成员。

Result: 我们在各种现有基准上展示了N-Gram Coverage Attack优于其他黑盒方法，并且在仅具有文本输出的情况下，性能与最先进的白盒攻击相当甚至更好。此外，我们发现该方法的成功率随着攻击计算预算的增加而提高。

Conclusion: 我们的方法验证了其准确性，并用于研究之前未被研究的封闭OpenAI模型。我们发现，较新的模型，如GPT-4o，对成员推理攻击表现出更高的鲁棒性，这表明隐私保护正在逐步改善。

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [19] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: 本文介绍了一个用于检测俄语科学摘要中AI生成内容的共享任务，提供了一个大规模数据集和持续的研究平台，以应对学术诚信方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的迅速发展，区分人类和AI生成的内容变得越来越困难，这对学术诚信构成了重大挑战，特别是在科学出版和多语言环境中，检测资源往往有限。

Method: 本文提出了一个包含52,305个样本的大规模数据集，包括12个不同科学领域的手工撰写的摘要和五种最先进的LLM生成的摘要，并组织了两个阶段的任务来挑战参与者开发能够泛化到未见过的科学领域和模型的解决方案。

Result: 任务吸引了10个团队和159个提交，顶级系统在识别AI生成内容方面表现出色，并建立了持续的共享任务平台以促进长期研究。

Conclusion: 本文介绍了AINL-Eval 2025共享任务，旨在检测俄语科学摘要中的AI生成内容，并提供了一个大规模数据集和持续的共享任务平台，以促进该领域的长期研究。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [20] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: 本文研究了通过调整解码温度来增加语言模型多样性的方法，发现降低温度可以提高质量，而增加温度通常无法提升覆盖率。提出通过精确度-召回率框架重新思考损失函数，结果表明这种方法在权衡精确度和召回率方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 增加语言模型的多样性是一个具有挑战性但至关重要的目标。通常的方法是提高解码温度，但这种方法的效果并不总是理想。

Method: 通过一个简单但常见的案例来研究提高解码温度的方法，分析为什么降低温度可以提高质量（精确度），而增加温度通常无法提升覆盖率（召回率）。重新思考语言模型中的损失函数，利用精确度-召回率框架。

Result: 这种方法在精确度和召回率之间的权衡上显著优于仅结合负对数似然训练和温度缩放的方法。

Conclusion: 这些发现为更通用和稳健的语言建模技术提供了一条途径。

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [21] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: EffiEval is a training-free approach for efficient benchmarking that addresses data redundancy while maintaining high evaluation reliability.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of large language models (LLMs) and the development of increasingly large and diverse evaluation benchmarks have introduced substantial computational challenges for model assessment.

Method: EffiEval is a training-free approach that adaptively selects high-quality representative subsets based on the Model Utility Index (MUI).

Result: Extensive experiments on multiple public benchmarks and diverse LLMs demonstrate that EffiEval achieves strong ranking consistency with full-dataset evaluation using only a small fraction of the original data.

Conclusion: EffiEval provides a practical and generalizable solution for reliable, fair, and efficient evaluation in the era of LLMs.

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [22] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种安全的链式思维蒸馏方法SLowED，通过两个模块（Slow Tuning和Low-Entropy Masking）来保持小型语言模型的安全性并提升其推理能力。


<details>
  <summary>Details</summary>
Motivation: To maintain the safety of SLMs during the CoT distillation process.

Method: SLowED, containing two modules: Slow Tuning and Low-Entropy Masking.

Result: Experiments on three SLMs across reasoning benchmarks and safety evaluation show that SLowED retains the safety of SLMs and comparably improves their reasoning capability.

Conclusion: SLowED retains the safety of SLMs and comparably improves their reasoning capability compared to existing distillation methods.

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [23] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: 本文评估了LLM在印度法律任务中的表现，发现它们在某些任务上优于人类，但在专业法律研究方面存在问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在印度法律环境中的表现，以评估它们在法律任务中的能力。

Method: 通过调查实验，将LLM的输出与初级律师的输出进行比较，并让高级法学生对工作进行评价。

Result: LLM在起草和问题识别方面表现出色，但在专业法律研究方面存在困难，经常产生幻觉、事实错误或虚构的输出。

Conclusion: 虽然LLM可以增强某些法律任务，但人类的专业知识在复杂的推理和法律的精确应用中仍然是必不可少的。

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [24] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: 本研究评估了视觉语言模型（VLMs）在面对误导性图表设计时的表现，发现大多数VLMs会被这些设计所欺骗，从而改变对图表的解释。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLMs）被越来越多的非专家用户用来解释可视化内容，了解这些模型对欺骗性视觉设计的易感性变得至关重要。

Method: 我们对十个不同模型的16000多个响应进行了深入评估，分析了八种不同类型的误导性图表设计。

Result: 大多数VLMs会被这些误导性设计所欺骗，导致对图表的解释发生变化，尽管底层数据保持不变。

Conclusion: 我们的研究结果强调了在VLM中需要强大的防护措施来防止视觉错误信息。

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [25] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: GFPO reduces length inflation in large language models by optimizing for response length and token efficiency, maintaining accuracy while cutting down on unnecessary filler text.


<details>
  <summary>Details</summary>
Motivation: Large language models trained with reinforcement learning with verifiable rewards tend to trade accuracy for length--inflating response lengths to achieve gains in accuracy. Many tokens are merely 'filler' that makes no real progress.

Method: GFPO (Group Filtered Policy Optimization) curbs length explosion by sampling larger groups per problem during training and filtering responses to train on based on two key metrics: response length and token efficiency: reward per token ratio. Adaptive Difficulty GFPO dynamically allocates more training resources to harder problems based on real-time difficulty estimates.

Result: On the Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across challenging STEM and coding benchmarks while maintaining accuracy. Optimizing for reward per token further increases reductions in length inflation to 71-85%. Adaptive Difficulty GFPO improves the balance between computational efficiency and accuracy especially on difficult questions.

Conclusion: GFPO demonstrates that increased training-time compute directly translates to reduced test-time compute--a simple yet effective trade-off for efficient reasoning.

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [26] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新的检索增强生成（RAG）框架，用于多跳问答。通过LLM分解复杂问题并生成可回答的问题嵌入，提升了RAG的性能。


<details>
  <summary>Details</summary>
Motivation: 多跳查询固有的歧义性需要明确针对不同的知识方面，而传统的直接嵌入原始或分块文档的方法可能不够有效。

Method: 我们的系统使用大型语言模型（LLM）将复杂的多跳问题分解为一系列单跳子问题，以指导文档检索。此外，我们从每个文档块生成可回答的问题，并通过问题-问题嵌入相似性检索相关块。

Result: 我们在三个多跳问题数据集（MuSiQue, 2WikiMultiHopQa, HotpotQA）上评估了我们的方法，结果表明它在RAG性能上优于基线系统。

Conclusion: 我们的方法在RAG性能上优于基线系统，突显了使用可回答问题嵌入对于RAG的优势，以及基于LLM的查询分解在多跳场景中的有效性。

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [27] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: 本研究探讨了提示中的支持和反驳论点如何影响大型语言模型对政治话题的回应方向，并发现模型倾向于适应提出的论点。


<details>
  <summary>Details</summary>
Motivation: 了解这些偏见评估的稳健性以及理解模型行为至关重要，因为这些模型经常与带有观点的文本进行交互。

Method: 我们进行了实验，以在存在支持和反驳论点的情况下评估政治偏见。

Result: 实验结果表明，这些论点显著改变了模型响应的方向，无论是单轮还是多轮设置。此外，我们发现这些论点的强度影响了模型响应的方向一致性率。

Conclusion: 这些效应表明大型语言模型有一种谄媚倾向，会调整其立场以与提出的论点一致，这对测量政治偏见和开发有效的缓解策略有下游影响。

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [28] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: UtterTune是一种轻量级的适应方法，用于微调基于大型语言模型架构的多语言文本到语音系统，以增强目标语言的发音可控性，同时保持其他语言的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM架构使TTS模型实现了惊人的自然度，但准确建模图素到音素（G2P）映射和韵律仍然是一个挑战，尤其是在模型省略显式G2P模块并直接处理最小编码文本的情况下。

Method: UtterTune是一种轻量级的适应方法，通过微调基于大型语言模型架构的多语言文本到语音系统，以增强目标语言的发音可控性。

Result: 客观和主观评估证实了UtterTune的有效性。

Conclusion: UtterTune在目标语言日本语的语音合成中表现出色，同时保持了其他语言的性能。

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [29] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文提出了一种自动化框架，利用大型语言模型生成高质量的文本解释，并验证了其在提高模型性能方面的有效性，表明自动化生成的解释具有很高的竞争力。


<details>
  <summary>Details</summary>
Motivation: 在可解释自然语言处理（NLP）领域，文本解释对于解释模型预测和丰富带有可解释标签的数据集至关重要。传统方法依赖于人工标注，这既昂贵又耗时，阻碍了可扩展性。

Method: 我们提出了一种自动化框架，利用多个最先进的大型语言模型（LLMs）生成高质量的文本解释，并通过全面的自然语言生成（NLG）指标评估这些LLM生成的解释的质量。此外，我们还研究了这些解释对预训练语言模型（PLMs）和LLMs在两个多样化基准数据集上的自然语言推理任务的下游影响。

Result: 我们的实验表明，自动化生成的解释在提高模型性能方面表现出高度竞争力，与人工标注的解释相比效果相当。

Conclusion: 我们的研究结果表明，自动化生成的文本解释在提高模型性能方面与人工标注的解释具有高度竞争力，这为扩展NLP数据集和增强模型性能提供了一个有前景的途径。

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [30] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文探讨了从业者在使用可解释NLP方法时的动机、技术、满意度和实际挑战，发现当前方法存在不足，并强调需要更清晰的定义和用户导向的框架。


<details>
  <summary>Details</summary>
Motivation: 填补对可解释NLP方法在实践中采用和效果的研究空白。

Method: 通过与行业从业者和学术研究人员的定性访谈，系统分析和比较他们的观点。

Result: 发现了概念上的差距，对当前可解释方法的满意度较低，并突出了评估挑战。

Conclusion: 研究强调了明确定义和以用户为中心的框架对于实际应用可解释NLP的重要性。

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [31] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据集创建管道和训练框架，以提高视觉语言模型在图表理解方面的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型（VLMs）在图表理解方面仍然存在困难，因为它们的训练数据集缺乏多样性和现实真实性，或者使用了自动提取的图表底层数据表，这可能包含许多估计错误。现有的模型仅依赖于这些低质量数据集的监督微调，严重限制了其有效性。

Method: 我们首先提出了BigCharts数据集创建管道，通过在渲染过程中基于来自多个在线平台的真实图表进行条件处理，生成视觉多样的图表图像。此外，我们引入了一个综合的训练框架，将监督微调与基于组相对策略优化（GRPO）的强化学习相结合。

Result: 我们的方法通过引入专门为图表推理设计的新奖励信号，增强了模型在各种图表风格和领域中的鲁棒性和泛化能力，从而得到了一个最先进的图表推理模型BigCharts-R1。

Conclusion: 我们的模型在多个图表问答基准测试中超越了现有方法，即使与更大的开源和闭源模型相比也是如此。

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [32] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文对临床心理健康数据集进行了全面调查，识别了现有数据集的不足，并提出了改进未来数据集的建议。


<details>
  <summary>Details</summary>
Motivation: 由于心理健康障碍在全球范围内上升，但受过训练的临床医生数量没有相应增加，因此需要利用人工智能来辅助心理健康诊断、监测和干预。然而，开发高效、可靠和道德的人工智能严重依赖高质量的临床训练数据集。尽管对数据整理的兴趣不断增加，但现有的数据集仍然分散、记录不足且难以获取，阻碍了AI模型在临床心理健康护理中的可重复性、可比性和泛化能力。

Method: 本文对与训练和开发基于AI的临床助手相关的临床心理健康数据集进行了全面调查，并按精神障碍、数据模态、任务类型、可访问性和社会文化背景进行分类。此外，还研究了合成临床心理健康数据集。

Result: 本文识别出关键差距，如缺乏纵向数据、有限的文化和语言代表性、不一致的收集和注释标准以及合成数据中的模态不足。

Conclusion: 本文总结了在构建和标准化未来数据集方面的关键挑战，并提供了促进更强大、可推广和公平的心理健康AI系统发展的可行建议。

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [33] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 本文系统地审查了创新的LLM架构，以解决Transformer的局限性并提高效率，并涵盖了多种方法和技术。


<details>
  <summary>Details</summary>
Motivation: 传统的Transformer架构需要大量的计算，给大规模训练和实际部署带来了重大障碍。因此，需要研究更高效的LLM架构。

Method: 本文对创新的LLM架构进行了系统审查，以解决Transformer的固有局限性并提高效率。

Result: 本文涵盖了线性和稀疏序列建模方法、高效的全注意力变体、稀疏专家混合、结合上述技术的混合模型架构以及新兴的扩散LLM。此外，还讨论了这些技术在其他模态中的应用，并考虑了它们对开发可扩展、资源感知的基础模型的更广泛影响。

Conclusion: 本文对现代高效的LLM架构进行了系统分析，并希望这能激发未来更高效、多功能的AI系统的研究。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [34] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: 本文介绍了PRELUDE，一个用于评估长上下文理解的基准测试，通过判断角色前传故事是否与原始书籍的正典叙述一致的任务。实验结果显示，即使使用最先进的LLM，其表现仍落后于人类，且模型常以错误的推理得出正确答案。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试对全局理解和深度推理的要求较弱，而我们的任务需要搜索和整合仅间接相关的信息。

Method: 引入PRELUDE，这是一个基准测试，用于通过确定角色前传故事是否与原始书籍的正典叙述一致的任务来评估长上下文理解。

Result: 实验结果表明，使用最先进的LLM进行上下文学习、RAG和领域内训练以及商业DeepResearch服务，与人类相比差距超过15%。进一步的人类研究显示，模型经常以错误的推理得出正确的答案，导致推理准确性与人类之间有超过30%的差距。

Conclusion: 这些发现强调了在长上下文理解和推理方面还有很大的改进空间。

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [35] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: 本研究评估了轻量级Whisper模型在低资源环境中对Urdu语音识别的可行性，发现Whisper-Small表现最佳，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: Urdu是全球第10大语言，拥有超过2.3亿使用者，但由于方言多样性、语言混用和训练数据稀疏，其在自动语音识别（ASR）系统中的表现有限。因此，需要探索更有效的低资源ASR系统。

Method: 我们评估了轻量级Whisper模型（Tiny、Base、Small）在低资源环境中的Urdu语音识别可行性，并在定制的Urdu数据集上进行了基准测试，未进行微调。

Result: 结果表明，Whisper-Small在词错误率（WER）方面表现最佳（33.68% WER），优于Tiny（67.08% WER）和Base（53.67% WER）。然而，对于复杂语句，仍存在语音准确性与词汇连贯性的挑战。

Conclusion: 我们的研究结果强调了为低资源ASR系统进行未来研究的重要性，尽管Whisper-Small在 Urdu ASR中显示出潜力，但仍存在显著差距。

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [36] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Memory Decoder的预训练记忆组件，它能够在不改变原始模型参数的情况下实现高效的领域适应。Memory Decoder通过一个小的Transformer解码器模仿外部非参数检索器的行为，可以无缝集成到任何共享相同分词器的预训练语言模型中。实验结果表明，Memory Decoder能够有效将各种Qwen和Llama模型适应到生物医学、金融和法律等三个不同的专业领域，平均降低困惑度6.17点。


<details>
  <summary>Details</summary>
Motivation: Adapting Large Language Models (LLMs) to specific domains remains a challenge. Current methods like Domain Adaptive Pretraining (DAPT) require costly full-parameter training and suffer from catastrophic forgetting. Meanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context.

Method: Memory Decoder employs a small transformer decoder that learns to imitate the behavior of an external non-parametric retriever. Once trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications.

Result: Experimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points.

Conclusion: Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain.

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [37] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: 本文综述了过去二十年中关于自动检测和分类认知扭曲的研究，提供了数据集、建模方法和评估策略的结构化概述，并指出了该领域面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理技术在心理健康领域的应用兴趣增加，自动检测和分类认知扭曲变得越来越重要。然而，该领域仍存在碎片化问题，需要更系统的研究。

Method: 本文回顾了38项研究，提供了数据集、建模方法和评估策略的综述。

Result: 本文提供了统一的认知扭曲分类参考，总结了常见的任务设置，并强调了开放性挑战。

Conclusion: 本文提供了对认知扭曲检测和分类研究的结构化概述，并指出了该领域存在的挑战，以支持更连贯和可重复的研究。

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [38] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: 本文探讨了如何利用说服性词典检测欺骗性语言，并指出在多语言环境中实现高准确率检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解释欺骗性语言如何被系统检测，并强调在多语言环境中实现这一目标的重要性。

Method: 本文结合古典修辞学、传播心理学、语言理论和实证研究，探讨如何使用说服性词典系统检测欺骗性语言。

Result: 在受控环境中，使用计算文本分析和个人化Transformer模型实现了超过99%的检测准确率。然而，在多语言环境中复制这种性能仍然存在问题。

Conclusion: 本文指出，随着AI驱动的交流越来越逼真，需要强大的自动文本识别系统。

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [39] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 本文提出了一种多维评估框架，用于系统比较LLM的对齐技术，并展示了其在识别当前模型优缺点方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏统一的评估框架，难以系统地比较不同的对齐方法并指导部署决策，因此需要一种全面的评估框架。

Method: 本文提出了一种多维评估框架，从四个关键维度（对齐检测、对齐质量、计算效率和鲁棒性）评估对齐方法。

Result: 通过在多种基础模型和对齐策略上的实验，本文展示了框架的有效性，能够识别当前最先进的模型的优势和局限性。

Conclusion: 本文介绍了用于LLM对齐技术的多维评估框架，该框架能够系统地比较所有主要对齐范式，并为未来的研究方向提供有价值的见解。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [40] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: 本文介绍了VisCodex，这是一个统一的框架，将视觉和编码语言模型无缝融合，以增强MLLMs的多模态代码生成能力。通过任务向量为基础的模型合并技术，我们整合了一个最先进的编码LLM到强大的视觉-语言基础模型中。为了支持训练和评估，我们引入了Multimodal Coding Dataset (MCD)，一个大规模且多样化的598k样本数据集。此外，我们提出了InfiBench-V，一个新颖且具有挑战性的基准测试，专门用于评估模型在视觉丰富的现实编程问题上的表现。实验结果表明，VisCodex在开源MLLMs中达到了最先进的性能，并接近GPT-4o等专有模型。


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited.

Method: We introduce VisCodex, a unified framework that seamlessly merges vision and coding language models. We leverage a task vector-based model merging technique to integrate a state-of-the-art coding LLM into a strong vision-language backbone.

Result: Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o.

Conclusion: VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets.

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [41] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: 本研究比较了通用、医学和领域特定的分词器在放射报告摘要任务中的表现，并发现医学和领域特定的分词器在从头开始训练模型时表现更好。预训练部分缓解了不同分词器之间的性能差异，而领域特定的分词器效果最好。


<details>
  <summary>Details</summary>
Motivation: 语言模型使用的词汇（由分词器定义）在文本生成质量中起着关键作用，但其影响在放射学中仍未得到充分研究。

Method: 系统比较通用、医学和领域特定的分词器在放射报告摘要任务中的表现，并研究了在没有预训练的情况下和有预训练的情况。

Result: 医学和领域特定的词汇在从头开始训练模型时优于广泛使用的自然语言替代方案。预训练部分缓解了不同分词器之间的性能差异，而领域特定的分词器取得了最有利的结果。领域特定的分词器还由于更小的词汇量和更短的序列减少了内存需求。

Conclusion: 适应临床领域的语言模型的词汇可以带来实际的好处，包括提高性能和减少计算需求，使这些模型在研究和现实医疗环境中更加可访问和有效。

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [42] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过添加合理的上下文到事件描述中，以更好地解释特定情况。通过自动和人工评估，发现情境叙述有助于特定情绪的解释，并支持注释者生成更一致的注释。


<details>
  <summary>Details</summary>
Motivation: 情绪分析是一个本质上模糊的任务。以前的工作研究了注释者的属性来解释不一致，但这忽略了歧义可能源于事件背景信息缺失的可能性。

Method: 我们通过结合不同场景下的短篇小说生成技术，实现了连贯的叙事，从而创建了一个专门的数据集，用于首次全面和系统地检查情境化情绪分析。

Result: 我们的目标是了解这些丰富的情境是否能帮助人类注释者更可靠地注释情绪。我们通过自动生成基于不同情绪的多个事件链来消除目标事件描述的歧义。

Conclusion: 通过自动和人工评估，我们发现情境叙述有助于特定情绪的解释，并支持注释者生成更一致的注释。

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [43] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: 研究评估了GPT-5及其变体在眼科多选问题上的表现，发现GPT-5-high在准确性上表现最佳，同时提出了一个自动评分框架用于评估LLM生成的答案。


<details>
  <summary>Details</summary>
Motivation: 研究旨在确定能够最大化准确性和成本效益的GPT-5配置，同时评估其在复杂医学问答任务中的性能。

Method: 研究评估了OpenAI的GPT-5系列的12种配置（三个模型层级在四个推理努力设置中），以及o1-high、o3-high和GPT-4o，使用了来自美国眼科学会基础临床科学课程（BCSC）数据集的260个封闭式多选问题。主要结果是多选准确性；次要结果包括通过Bradley-Terry模型进行的头对头排名、使用参考锚定的成对LLM-as-a-judge框架进行的推理质量评估，以及使用基于令牌的成本估算分析准确性-成本权衡。

Result: GPT-5-high在准确性上达到最高（0.965；95% CI，0.942-0.985），优于所有GPT-5-nano变体（P < .001）、o1-high（P = .04）和GPT-4o（P < .001），但不优于o3-high（0.958；95% CI，0.931-0.981）。GPT-5-high在准确性和推理质量上均排名第一，且成本-准确性分析确定了几个位于帕累托前沿的GPT-5配置，其中GPT-5-mini-low提供了最有利的低成本、高性能平衡。

Conclusion: 研究结果为GPT-5在高质量眼科数据集上的表现提供了基准，展示了推理努力对准确性的影响力，并引入了一个自动评分框架，用于在眼科中对LLM生成的答案进行可扩展评估。

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [44] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: 本研究针对库尔德语中较少资源的Badini方言，开发了一个语音到文本系统，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于Badini和Hawrami等库尔德语方言缺乏语音到文本系统，本研究旨在填补这一空白，帮助这些社区更好地使用移动和计算机技术，并提高其方言的全球可见度。

Method: 研究人员选择了Badini儿童故事作为文本输入，并进行了语音录制、清洗、分割和标记处理。然后使用Wav2Vec2-Large-XLSR-53和Whisper-small模型开发语言模型。

Result: 实验结果表明，基于Wav2Vec2-Large-XLSR-53模型的转录过程在可读性和准确性方面均优于Whisper-small模型，分别达到了90.38%和82.67%。

Conclusion: 本研究旨在创建基于Badini方言语音的语言模型，并评估其性能。实验表明，基于Wav2Vec2-Large-XLSR-53模型的转录过程比Whisper-small模型更准确和可读。

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [45] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: 本文提出了一种基于神经上下文多臂老虎机的算法，用于在没有历史LLM性能数据的情况下，指导不同子任务的LLM选择。实验表明该方法在电信问答和医学诊断预测数据集上比其他LLM选择算法更有效。


<details>
  <summary>Details</summary>
Motivation: 随着提供者如微软允许用户轻松创建定制的LLM“助手”，预测哪些LLM会产生成功的答案变得越来越重要。然而，一些任务可能太专门化和困难，无法由单个LLM单独处理，因此需要将任务分解为更小的子任务，每个子任务由一个预期在该特定子任务上表现良好的LLM执行。

Method: 本文提出了一种基于神经上下文多臂老虎机的算法，用于在线学习LLM在每个子任务上的成功率，并指导LLM的选择。

Result: 实验表明，本文提出的算法在电信问答和医学诊断预测数据集上比其他LLM选择算法更有效。

Conclusion: 本文提出了一种基于神经上下文多臂老虎机的算法，用于在没有历史LLM性能数据的情况下，指导不同子任务的LLM选择。实验表明该方法在电信问答和医学诊断预测数据集上比其他LLM选择算法更有效。

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [46] [AI Blob! LLM-Driven Recontextualization of Italian Television Archives](https://arxiv.org/abs/2508.09535)
*Roberto Balestri*

Main category: cs.MM

TL;DR: AI Blob! is an experimental system that uses semantic cataloging and Large Language Models to retrieve and recontextualize archival television footage, demonstrating how semantic technologies can enable new approaches to archival engagement and automated narrative construction.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the potential of semantic cataloging and Large Language Models (LLMs) for the retrieval and recontextualization of archival television footage, highlighting the limitations of static metadata schemas and proposing dynamic, content-aware retrieval methods.

Method: AI Blob! integrates automatic speech recognition (ASR), semantic embeddings, and retrieval-augmented generation (RAG) to organize and reinterpret archival content. It processes a curated dataset of 1,547 Italian television videos by transcribing audio, segmenting it into sentence-level units, and embedding these segments into a vector database for semantic querying. Upon user input of a thematic prompt, the LLM generates a range of linguistically and conceptually related queries, guiding the retrieval and recombination of audiovisual fragments.

Result: AI Blob! successfully organizes and reinterprets archival content by generating linguistically and conceptually related queries, retrieving and recombining audiovisual fragments into narrative sequences that emulate editorial practices of ironic juxtaposition and thematic coherence.

Conclusion: AI Blob! demonstrates how semantic technologies can facilitate new approaches to archival engagement, enabling novel forms of automated narrative construction and cultural analysis. The project contributes to ongoing debates in media historiography and AI-driven archival research, offering both a conceptual framework and a publicly available dataset to support further interdisciplinary experimentation.

Abstract: This paper introduces AI Blob!, an experimental system designed to explore
the potential of semantic cataloging and Large Language Models (LLMs) for the
retrieval and recontextualization of archival television footage. Drawing
methodological inspiration from Italian television programs such as Blob (RAI
Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic
embeddings, and retrieval-augmented generation (RAG) to organize and
reinterpret archival content. The system processes a curated dataset of 1,547
Italian television videos by transcribing audio, segmenting it into
sentence-level units, and embedding these segments into a vector database for
semantic querying. Upon user input of a thematic prompt, the LLM generates a
range of linguistically and conceptually related queries, guiding the retrieval
and recombination of audiovisual fragments. These fragments are algorithmically
selected and structured into narrative sequences producing montages that
emulate editorial practices of ironic juxtaposition and thematic coherence. By
foregrounding dynamic, content-aware retrieval over static metadata schemas, AI
Blob! demonstrates how semantic technologies can facilitate new approaches to
archival engagement, enabling novel forms of automated narrative construction
and cultural analysis. The project contributes to ongoing debates in media
historiography and AI-driven archival research, offering both a conceptual
framework and a publicly available dataset to support further interdisciplinary
experimentation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [47] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)
*Daniel Raffini,Agnese Macori,Lorenzo Porcaro,Tiziana Catarci,Marco Angelini*

Main category: cs.HC

TL;DR: 本研究分析了ChatGPT在伦理问题上的论点生成及其对人类读者的影响，发现其说服效果受限，伦理问题可能持续或加剧。


<details>
  <summary>Details</summary>
Motivation: 研究AI生成的论点在伦理问题上的说服效果，以及它们如何影响人类读者的观点和感知。

Method: 通过涉及62名参与者的用户研究和前后互动调查，分析了接触AI生成论点对意见变化和用户感知的影响。对生成文本的语言和修辞分析揭示了统一的论证宏观结构、对公式化表达的依赖以及有限的文体丰富性。

Result: 尽管ChatGPT在构建连贯的论证文本方面表现出色，但其说服效果似乎受到限制，尤其是在涉及伦理问题的话题上。参与者通常承认ChatGPT强调的好处，但伦理问题往往持续存在甚至加剧。结果还显示出根据话题的不同而有所变化。

Conclusion: 研究揭示了AI生成的说服在伦理敏感领域的新见解，并为未来的研究提供了基础。

Abstract: This study examines the rhetorical and linguistic features of argumentative
texts generated by ChatGPT on ethically nuanced topics and investigates their
persuasive impact on human readers.Through a user study involving 62
participants and pre-post interaction surveys, the paper analyzes how exposure
to AI-generated arguments affects opinion change and user perception. A
linguistic and rhetorical analysis of the generated texts reveals a consistent
argumentative macrostructure, reliance on formulaic expressions, and limited
stylistic richness. While ChatGPT demonstrates proficiency in constructing
coherent argumentative texts, its persuasive efficacy appears constrained,
particularly on topics involving ethical issues.The study finds that while
participants often acknowledge the benefits highlighted by ChatGPT, ethical
concerns tend to persist or even intensify post-interaction. The results also
demonstrate a variation depending on the topic. These findings highlight new
insights on AI-generated persuasion in ethically sensitive domains and are a
basis for future research.

</details>


### [48] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)
*Daniel Raffini,Agnese Macori,Marco Angelini,Tiziana Catarci*

Main category: cs.HC

TL;DR: 该研究分析了ChatGPT、Gemini和Claude生成的故事中的基于性别的叙述偏见，发现这些模型仍然存在隐性偏见，并强调了在多个层面评估这些偏见的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索由ChatGPT、Gemini和Claude生成的故事中的基于性别的叙述偏见，并强调评估这些偏见的重要性。

Method: 研究采用了一种密切阅读的方法，特别关注对提示的遵循、角色的性别分布、身体和心理描述、行动，以及最终的剧情发展和角色关系。

Result: 研究结果揭示了生成故事中偏见的持续存在，特别是隐性偏见。

Conclusion: 研究揭示了生成故事中偏见的持续存在，特别是隐性偏见，并强调了使用解释方法在多个层面评估偏见的重要性。

Abstract: The paper explores the study of gender-based narrative biases in stories
generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's
character classifications and Freytag's narrative structure. The stories are
analyzed through a close reading approach, with particular attention to
adherence to the prompt, gender distribution of characters, physical and
psychological descriptions, actions, and finally, plot development and
character relationships. The results reveal the persistence of biases -
especially implicit ones - in the generated stories and highlight the
importance of assessing biases at multiple levels using an interpretative
approach.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [49] [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/abs/2508.09224)
*Yuan Yuan,Tina Sriskandarajah,Anna-Luisa Brakman,Alec Helyar,Alex Beutel,Andrea Vallone,Saachi Jain*

Main category: cs.CY

TL;DR: 本文提出Safe-Completions方法，以提升大型语言模型在处理双重用途提示时的安全性和帮助性。


<details>
  <summary>Details</summary>
Motivation: 传统的拒绝边界方法在处理隐含用户意图的提示时可能不够稳健，并且对于双重用途的情况（如生物学或网络安全）不适用。

Method: 提出了一种名为Safe-Completions的安全训练方法，该方法关注助理输出的安全性，而不是用户意图的二元分类。

Result: 将Safe-Completions方法应用于GPT-5后，结果显示在生产比较和内部控制实验中，该方法提高了安全性，减少了残留安全故障的严重性，并显著增加了模型的帮助性。

Conclusion: 通过将安全训练的重点从用户意图的二元分类转向助理输出的安全性，Safe-Completions方法在提高模型帮助性的同时，显著提升了安全性，特别是在处理双重用途提示时。

Abstract: Large Language Models used in ChatGPT have traditionally been trained to
learn a refusal boundary: depending on the user's intent, the model is taught
to either fully comply or outright refuse. While this is a strong mitigation
for explicitly malicious prompts, focusing safety training on refusals can lead
to brittleness for prompts with obscured user intent. Binary refusal boundaries
are especially ill-suited for dual-use cases (such as biology or
cybersecurity), where a user request can be answered safely at a high level,
but in some cases can lead to malicious uplift if sufficiently detailed or
actionable. As an alternative, we propose safe-completions: a safety-training
approach that centers on the safety of the assistant's output, rather than a
binary classification of the user's intent. Safe-completions seek to maximize
helpfulness within the safety policy's constraints. We incorporated this
approach into GPT-5 and find that across both production comparisons and
internally controlled experiments, safe-completion training improves safety
(especially on dual-use prompts), reduces the severity of residual safety
failures, and substantially increases model helpfulness.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [50] [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
*Aayush Gupta*

Main category: cs.CR

TL;DR: CIV is a security architecture that provides deterministic, per-token non-interference guarantees on frozen models by attaching cryptographically signed provenance labels to every token and enforcing a source-trust lattice inside the transformer.


<details>
  <summary>Details</summary>
Motivation: Large language models remain vulnerable to prompt injection and related jailbreak attacks, and heuristic guardrails are routinely bypassed.

Method: CIV attaches cryptographically signed provenance labels to every token and enforces a source-trust lattice inside the transformer via a pre-softmax hard attention mask.

Result: CIV attains 0% attack success rate under the stated threat model while preserving 93.1% token-level similarity and showing no degradation in model perplexity on benign tasks.

Conclusion: CIV provides deterministic, per-token non-interference guarantees on frozen models and can be applied as a lightweight patch without fine-tuning.

Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

</details>


### [51] [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
*Zhifan Luo,Shuo Shao,Su Zhang,Lijing Zhou,Yuke Hu,Chenxu Zhao,Zhihao Liu,Zhan Qin*

Main category: cs.CR

TL;DR: This paper highlights the privacy risks of KV-cache in LLMs and introduces KV-Cloak, a defense mechanism that effectively secures the cache without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the privacy risks introduced by the KV-cache mechanism in LLMs, which can lead to sensitive user input reconstruction by attackers.

Method: The paper designs and implements three attack vectors (direct inversion, collision, and injection attacks) to demonstrate the vulnerabilities of KV-cache and proposes KV-Cloak, a lightweight defense mechanism using reversible matrix-based obfuscation and operator fusion.

Result: Experiments show that KV-Cloak successfully reduces reconstruction quality to random noise, achieving robust security with virtually no degradation in model accuracy and minimal performance overhead.

Conclusion: KV-Cloak provides a practical solution for trustworthy LLM deployment by effectively thwarting all proposed attacks with minimal performance overhead.

Abstract: The Key-Value (KV) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (LLM) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the KV-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of KV-cache privacy leakage issues. To mitigate this,
we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.
KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the KV-cache. Our extensive experiments show that
KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy LLM deployment.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [52] [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
*Zainab Khan,Ahmed Hussain,Mukesh Thakur,Arto Hellas,Panos Papadimitratos*

Main category: cs.NI

TL;DR: 本文介绍了一种名为NEFMind的框架，利用开源大型语言模型的参数高效微调来解决现代电信中服务发现和管理的复杂性问题。该框架通过生成合成数据集、模型优化和性能评估，实现了显著的通信开销减少和高精度的API调用识别。


<details>
  <summary>Details</summary>
Motivation: 现代电信中服务架构的使用使网络功能（NFs）和应用程序编程接口（APIs）呈指数级增长，导致服务发现和管理的操作复杂性增加。

Method: NEFMind框架利用开源大型语言模型（LLMs）的参数高效微调来解决这些问题，它集成了三个核心组件：从网络暴露功能（NEF）API规范生成合成数据集、通过量化低秩适应进行模型优化，以及通过GPT-4 Ref Score和BertScore指标进行性能评估。

Result: 我们的方法在5G基于服务的架构API上实现了比手动发现方法减少85%的通信开销。使用开源Phi-2模型的实验验证表明，API调用识别性能非常出色，准确率为98-100%。微调后的Phi-2模型在保持计算效率的同时，性能可与更大的模型如GPT-4相媲美。

Conclusion: 这些发现验证了针对下一代电信网络中复杂API生态系统的特定领域、参数高效的LLM策略。

Abstract: The use of Service-Based Architecture in modern telecommunications has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
communication overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for telecommunications infrastructure
deployment. These findings validate domain-specific, parameter-efficient LLM
strategies for managing complex API ecosystems in next-generation
telecommunications networks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [53] [Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative](https://arxiv.org/abs/2508.09294)
*Xi Xuan,Zimo Zhu,Wenxin Zhang,Yi-Cheng Lin,Tomi Kinnunen*

Main category: eess.AS

TL;DR: Fake-Mamba 是一种基于双向 Mamba 的新型合成语音检测方法，在多个基准测试中表现优异，具有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 随着语音合成技术的进步，安全威胁加剧，促使研究实时深度伪造检测。

Method: Fake-Mamba 结合了 XLSR 前端和双向 Mamba，引入了三种高效的编码器：TransBiMamba、ConBiMamba 和 PN-BiMamba，以捕捉局部和全局特征。

Result: Fake-Mamba 在 ASVspoof 21 LA、21 DF 和 In-The-Wild 基准测试中分别达到了 0.97%、1.74% 和 5.85% 的 EER，相对于 SOTA 模型有显著提升。

Conclusion: Fake-Mamba 在多个基准测试中表现出色，具有强大的泛化能力和实际可行性，表明其在检测合成语音方面具有潜力。

Abstract: Advances in speech synthesis intensify security threats, motivating real-time
deepfake detection research. We investigate whether bidirectional Mamba can
serve as a competitive alternative to Self-Attention in detecting synthetic
speech. Our solution, Fake-Mamba, integrates an XLSR front-end with
bidirectional Mamba to capture both local and global artifacts. Our core
innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and
PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can
effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof
21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and
5.85% EER, respectively, representing substantial relative gains over SOTA
models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time
inference across utterance lengths, demonstrating strong generalization and
practical viability. The code is available at
https://github.com/xuanxixi/Fake-Mamba.

</details>


### [54] [ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs](https://arxiv.org/abs/2508.09389)
*Eray Eren,Qingju Liu,Hyeongwoo Kim,Pablo Garrido,Abeer Alwan*

Main category: eess.AS

TL;DR: 本文提出了一种新的模型，能够将文本映射到韵律特征，并在语音合成任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 韵律传达了语音信号丰富的感情和语义信息以及个体差异。现有的方法在韵律建模方面存在不足，因此需要一种更有效的模型来提高语音合成的质量。

Method: 提出了一种独立的模型，将文本映射到韵律特征（如F0和能量），并可以用于下游任务如TTS。ProMode编码器输入声学特征和时间对齐的文本内容，并获得固定长度的韵律嵌入。解码器使用编码的韵律输入和未遮蔽的文本内容来预测遮蔽区域的声学特征。

Result: 在GigaSpeech数据集上训练后，与最先进的风格编码器相比，我们的模型在不同粒度级别上的F0和能量预测表现出一致的改进。将预测的韵律特征集成到TTS系统中进行感知测试，结果表明其韵律偏好优于基线。

Conclusion: 该模型在语音合成任务中展示了其在韵律建模方面的潜力，特别是在F0和能量预测以及感知测试中的表现优于基线。

Abstract: Prosody conveys rich emotional and semantic information of the speech signal
as well as individual idiosyncrasies. We propose a stand-alone model that maps
text-to-prosodic features such as F0 and energy and can be used in downstream
tasks such as TTS. The ProMode encoder takes as input acoustic features and
time-aligned textual content, both are partially masked, and obtains a
fixed-length latent prosodic embedding. The decoder predicts acoustics in the
masked region using both the encoded prosody input and unmasked textual
content. Trained on the GigaSpeech dataset, we compare our method with
state-of-the-art style encoders. For F0 and energy predictions, we show
consistent improvements for our model at different levels of granularity. We
also integrate these predicted prosodic features into a TTS system and conduct
perceptual tests, which show higher prosody preference compared to the
baselines, demonstrating the model's potential in tasks where prosody modeling
is important.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: 本文提出MoLAN框架，通过模态感知分块和动态去噪强度来提升多模态情感分析的效果，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态情感分析方法通常将整个模态信息视为独立单元进行特征增强或去噪，但容易在抑制冗余和噪声信息的同时丢失关键信息。

Method: 提出了一种统一的ModaLity-aware噪声动态编辑框架MoLAN，通过模态感知分块和动态分配去噪强度来实现细粒度噪声抑制，同时保留关键多模态信息。

Result: MoLAN框架在五个模型和四个数据集上表现出广泛的有效性，MoLAN+实现了最先进的性能。

Conclusion: MoLAN框架在多个模型和数据集上的实验表明其广泛的有效性，MoLAN+在多模态情感分析中达到了最先进的性能。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [56] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: 本文提出了一种名为NeuronTune的细粒度框架，通过动态调制稀疏神经元来实现同时的安全-实用性优化。实验结果表明，该方法在保持优秀实用性的同时显著提升了模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型（LLMs）的稳健安全对齐同时保持实用性对于其可靠部署至关重要。然而，当前技术从根本上存在交织的缺陷：对恶意攻击的不足鲁棒性、对良性查询的频繁拒绝、生成文本质量和通用任务性能的退化——前两者反映了安全鲁棒性的不足，后两者构成了实用性的损害。

Method: 我们提出了NeuronTune，这是一个细粒度框架，通过动态调制稀疏神经元来实现同时的安全-实用性优化。首先，我们通过归因识别出所有层中的安全关键和实用性保留神经元，然后使用元学习自适应地增强安全神经元激活并抑制实用性神经元激活。

Result: 广泛的实验结果表明，我们的方法显著优于现有的最先进技术，在保持优秀实用性的同时实现了更高的模型安全性。

Conclusion: 我们的方法显著优于现有的最先进技术，在保持优秀实用性的同时实现了更高的模型安全性。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [58] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: This paper introduces IAG, a novel input-aware backdoor attack method that can manipulate the grounding behavior of vision-language models. It shows high feasibility and effectiveness, with a high success rate on various models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the underexplored security issues in visual grounding tasks for VLMs, particularly in the context of backdoor attacks. It seeks to develop a novel attack method that can manipulate the grounding behavior of VLMs.

Method: IAG is an input-aware backdoor attack method that uses an adaptive trigger generator to embed semantic information of the attack target's description into the original image using a text-conditional U-Net. It also utilizes a reconstruction loss to minimize visual discrepancies between poisoned and clean images and introduces a unified method for generating attack data.

Result: IAG demonstrates high feasibility and effectiveness, with an ASR@0.5 over 65% on InternVL-2.5-8B on various testing sets. It also shows promising potential on manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on clean samples. The attack is also robust and transferable.

Conclusion: IAG is a novel input-aware backdoor attack method that can manipulate the grounding behavior of VLMs. It shows high feasibility and effectiveness, with a high ASR@0.5 on InternVL-2.5-8B and promising potential on other models. The attack is also robust and transferable.

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

</details>


### [59] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为COME的通用协作混合异构源特定专家框架，以解决超声图像分析中数据分布变化带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统单数据集训练在新数据分布下表现不佳，特别是在超声图像分析中由于数据有限、声阴影和斑点噪声等问题，需要构建一个适用于多异构超声数据集的通用框架。

Method: 提出了一种通用协作混合异构源特定专家（COME）框架，通过建立双结构语义共享专家和源特定专家来提取判别特征。

Result: 在三种评估模式下，COME表现出优越性，并在平均AP方面相比最先进的方法有显著提升。

Conclusion: COME在三种评估模式下的实验结果表明其优越性，相对于最先进的方法实现了显著的平均AP提升。

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [60] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: This paper introduces Echo-4o-Image, a synthetic dataset generated by GPT-4o, which addresses limitations in real-world datasets by providing complementary and controlled supervision. The dataset enhances the performance of various models, demonstrating strong transferability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap between open-source models and GPT-4o by leveraging synthetic data generated by GPT-4o. The study aims to explore the advantages of synthetic images in complementing real-world datasets and providing clean supervision for text-to-image alignment.

Method: We introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.

Result: Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the dataset's strong transferability.

Conclusion: Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the dataset's strong transferability.

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: 本文提出了一种名为 CoBA 的反偏数据增强方法，该方法通过在语义三元组级别上修改文本以破坏虚假相关性，从而提高模型的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型常常在训练数据中学习并利用虚假相关性，使用这些非目标特征来指导其预测。这种依赖会导致性能下降，并在未见过的数据上表现出较差的泛化能力。为了克服这些限制，我们引入了一种更通用的反偏数据增强形式，称为反偏数据增强。

Method: CoBA 是一种在语义三元组级别上操作的统一框架，首先将文本分解为主体-谓词-对象三元组，然后选择性地修改这些三元组以破坏虚假相关性。通过从这些调整后的三元组中重新构建文本，CoBA 生成了减轻虚假模式的反偏数据。

Result: 通过广泛的实验，我们证明 CoBA 不仅提高了下游任务的性能，而且有效减少了偏差并增强了分布外的鲁棒性。

Conclusion: CoBA 提供了一种多功能且稳健的解决方案，以应对由虚假相关性带来的挑战。

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [2] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: 该研究引入了一个大规模的德国数据集，用于分析有毒言论，并发现了不同年龄群体在有毒言论模式上的差异。


<details>
  <summary>Details</summary>
Motivation: 现有有毒言论数据集中缺乏人口统计背景，限制了我们对不同年龄群体在线交流的理解。

Method: 该研究与德国公共事业内容网络funk合作，引入了第一个大规模德国数据集，该数据集对毒性进行了注释，并通过平台提供的年龄估计进行了丰富。数据集包括3,024个人工注释和30,024个LLM注释的匿名评论，来自Instagram、TikTok和YouTube。注释流程结合了人类专业知识和最先进的语言模型，识别了诸如侮辱、虚假信息和对广播费用的批评等关键类别。

Result: 数据集揭示了有毒言论模式的年龄差异，年轻用户更倾向于使用表达性语言，而年长用户更常参与虚假信息和贬低行为。

Conclusion: 该数据集为研究不同年龄群体的在线交流提供了新机会，并支持开发更公平和年龄意识的内容审核系统。

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [3] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: Granite Embedding R2 models are high-performance English encoder-based embedding models designed for enterprise-scale dense retrieval applications, offering improved context length, performance across diverse retrieval domains, and speed advantages over competitors.


<details>
  <summary>Details</summary>
Motivation: To provide high-performance English encoder-based embedding models for enterprise-scale dense retrieval applications, with improved context length, performance across diverse retrieval domains, and speed advantages over competitors.

Method: The Granite Embedding R2 models are engineered for enterprise-scale dense retrieval applications, including bi-encoder and cross-encoder architectures, with a 22-layer retriever model and its efficient 12-layer counterpart, alongside a high-quality reranker model.

Result: The Granite R2 models demonstrate exceptional versatility across standard benchmarks, IBM-developed evaluation suites, and real-world enterprise use cases, establishing new performance standards for open-source embedding models.

Conclusion: Granite R2 models deliver a compelling combination of cutting-edge performance, enterprise-ready licensing, and transparent data provenance that organizations require for mission-critical deployments.

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [4] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: 本文提出了基于Transformer的墨水生成模型TrInk，通过引入缩放位置嵌入和高斯记忆掩码，以及设计主观和客观评估流程，显著提高了手写体生成的可读性和风格一致性。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地捕捉全局依赖关系并提高生成手写体的可读性和风格一致性，需要一种新的模型方法。

Method: 提出了一种基于Transformer的墨水生成模型TrInk，引入了缩放的位置嵌入和高斯记忆掩码以更好地对齐输入文本和生成的笔画点，并设计了主观和客观评估流程来全面评估生成手写体的可读性和风格一致性。

Result: 实验表明，基于Transformer的模型在IAM-OnDB数据集上相比之前的方法，在字符错误率（CER）和单词错误率（WER）上分别减少了35.56%和29.66%。

Conclusion: 实验表明，基于Transformer的模型在IAM-OnDB数据集上相比之前的方法，在字符错误率（CER）和单词错误率（WER）上分别减少了35.56%和29.66%。

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [5] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: 本文研究了LLM中的锚定效应，发现LLM像人类一样受到影响，且推理模型较不易受影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLM中的锚定效应，以了解其在现实世界应用中的可靠性。

Method: 我们指导卖方LLM代理应用锚定效应，并使用客观指标和主观指标评估谈判。

Result: 实验结果表明，LLM像人类一样受到锚定效应的影响。此外，我们发现推理模型不太容易受到锚定效应的影响，而个性特征与对锚定效应的易感性之间没有显著相关性。

Conclusion: 这些发现有助于更深入地理解LLM中的认知偏差，并有助于实现LLM在社会中的安全和负责任的应用。

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [6] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: 本文提出一个名为Percept-V的数据集，用于评估MLLMs和LRMs在基本视觉感知任务上的表现。实验发现，随着问题复杂度的增加，模型性能显著下降，并且某些认知技能比其他技能更难。


<details>
  <summary>Details</summary>
Motivation: 由于很少有实验评估MLLMs在简单感知任务上的表现，因此需要一个专门针对基本视觉感知技能的数据集来填补这一空白。

Method: 论文引入了一个名为Percept-V的数据集，包含7200张程序生成的图像，分为30个类别，每个类别测试不同的视觉感知技能。然后在最先进的MLLMs和LRMs上测试该数据集以评估其性能。

Result: 实验结果显示，随着问题复杂度的增加，所有测试的MLLMs和LRMs的性能都有显著下降。同时，某些认知技能被发现比其他技能更难。

Conclusion: 实验结果表明，随着问题复杂度的增加，所有测试的MLLMs和LRMs的性能都有显著下降。此外，分析还显示，这些模型在不同类别中的准确性表现出相似的趋势，某些认知技能比其他技能更难。

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [7] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文综述了科学大语言模型（Sci-LLMs）的发展，强调了科学数据与一般自然语言处理数据集的不同之处，并提出了一个统一的科学数据分类法和科学知识的分层模型。


<details>
  <summary>Details</summary>
Motivation: 科学大语言模型正在改变科学知识的表示、整合和应用方式，但其进展受到科学数据复杂性的制约。本文旨在提供一个全面的数据中心综合分析，重新定义Sci-LLMs的发展。

Method: 本文提出了一个统一的科学数据分类法和科学知识的分层模型，系统地回顾了最近的Sci-LLMs，并分析了超过270个预训练/后训练数据集。

Result: 本文展示了Sci-LLMs的独特需求，包括异构、多尺度、不确定性丰富的语料库，以及需要保留领域不变性和实现跨模态推理的表示。此外，还讨论了科学数据开发中的持续问题和新兴解决方案。

Conclusion: 本文提供了构建值得信赖的、持续进化的AI系统的路线图，这些AI系统可以作为加速科学发现的真正合作伙伴。

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [8] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在评估输出时可能受到偏见影响，特别是模型身份的感知会严重扭曲高层次的判断并影响详细的质量评分。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型（LLMs）在评估输出时可能受到的影响，特别是自我和跨模型评估中的偏见问题。

Method: 研究分析了ChatGPT、Gemini和Claude在四种条件下的自我和跨模型评估中的偏见：无标签、真实标签以及两种虚假标签场景。通过整体偏好投票和质量评分（包括连贯性、信息量和简洁性）对每个模型撰写的博客文章进行评估，并将所有分数表示为百分比以便直接比较。

Result: 结果揭示了显著的不对称性：'Claude'标签始终提升得分，而'Gemini'标签始终降低得分，无论实际内容如何。虚假标签经常逆转排名，导致偏好投票最多上升50个百分点，质量评分最多上升12个百分点。Gemini在真实标签下的自评分数崩溃，而Claude的自评偏好则增强。

Conclusion: 研究发现，模型身份的感知可以严重扭曲高层次的判断，并微妙地影响详细的质量评分，这强调了需要采用盲测或多模型评估协议以确保LLM基准测试的公平性。

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [9] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: 本文提出了一种基于贝叶斯实验设计的通用方法，以提高大型语言模型在与用户或其他外部源交互时获取信息的能力。


<details>
  <summary>Details</summary>
Motivation: To improve the ability of Large Language Models (LLMs) to intelligently and adaptively gather information from a user or other external source.

Method: The approach is based on iteratively choosing questions or queries that maximize the expected information gain (EIG) about the task of interest given the responses gathered previously.

Result: BED-LLM achieves substantial gains in performance across a wide range of tests based on the 20-questions game and using the LLM to actively infer user preferences.

Conclusion: BED-LLM achieves substantial gains in performance across a wide range of tests compared to direct prompting of the LLM and other adaptive design strategies.

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [10] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: 本研究提出了一种基于强化学习的自动化HFACS分类框架，用于航空安全分析，通过微调Llama-3.1 8B语言模型并结合多组件奖励系统和合成数据生成，提升了模型性能，并证明了领域优化的小型模型在计算效率和安全性分析方面的优势。


<details>
  <summary>Details</summary>
Motivation: 传统使用HFACS的方法在可扩展性和一致性方面存在局限性，因此需要一种更高效和一致的自动化分类框架来分析航空事故中的人因因素。

Method: 引入了一个基于强化学习与组相对策略优化（GRPO）的自动化HFACS分类框架，用于航空安全分析，并利用Llama-3.1 8B语言模型进行微调。该方法结合了针对航空安全分析的多组件奖励系统，并集成了合成数据生成以克服事故数据集中的类别不平衡问题。

Result: GRPO优化的模型在准确率方面取得了显著提升，包括精确匹配准确率提高了350%（从0.0400到0.1800），部分匹配准确率为0.8800。此外，该模型在关键指标上优于当前最先进的LLMs，如GPT-5-mini和Gemini-2.5-fiash。

Conclusion: 本研究验证了较小的、领域优化的模型可以在计算效率和关键安全分析方面提供更好的解决方案。这种方法使得在资源受限的边缘设备上进行高效、低延迟的部署成为可能。

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [11] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: 本文提出了一种基于像素的生成语言模型，以提高对正交噪声的鲁棒性，并在多语言设置中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归语言模型容易受到正交攻击的影响，这主要是由于子词分词器及其嵌入的词汇外问题。

Method: 通过将文本嵌入替换为基于像素的表示来渲染单词为单独的图像，从而设计了一个基于像素的生成语言模型。

Result: 在多语言LAMBADA数据集、WMT24数据集和SST-2基准测试中评估了所提出的方法，证明了其对正交噪声的抵抗力和在多语言环境中的有效性。

Conclusion: 本文提出了一种基于像素的生成语言模型，以提高对正交噪声的鲁棒性，并在多语言设置中表现出色。

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [12] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: 本文研究了自监督语音模型中的临界期效应，发现它们不表现出明显的临界期效应。


<details>
  <summary>Details</summary>
Motivation: 尽管语音语言在人类语言习得中起着核心作用，但自监督语音模型中的临界期效应仍缺乏研究。

Method: 本文通过训练不同L2训练开始时间和L1训练结束时间的自监督语音模型，并评估其语音辨别性能来研究临界期效应。

Result: 研究发现，自监督语音模型没有显示出明显的临界期效应，延迟的L2暴露开始时间反而有助于L2表现，而延迟的L1暴露结束时间导致了L1遗忘。

Conclusion: 本文发现自监督语音模型（S3Ms）在语音习得中并未表现出明显的临界期效应。

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


### [13] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
*Weizhi Gao,Xiaorui Liu,Feiyi Wang,Dan Lu,Junqi Yin*

Main category: cs.CL

TL;DR: 本文提出了一种名为Decoding Memory Pipeline (DMP)的新方法，通过选择性推理和退火解码加速生成过程，从而提高多响应生成的效率，并在不牺牲性能的情况下实现高达3倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法在句子级生成中表现不佳，或者严重依赖于领域特定知识。虽然自一致性方法有助于解决这些限制，但它们由于重复生成而产生了高昂的计算成本。因此，需要一种更高效的解决方案来提高生成效率。

Method: 本文提出了一个名为Decoding Memory Pipeline (DMP)的新方法，通过选择性推理和退火解码加速生成过程。该方法基于对自一致性方法中冗余性的研究，即生成中的共享前缀标记，并观察到非精确答案标记对语义内容的贡献最小。

Result: 实验结果表明，DMP方法在不牺牲AUROC性能的情况下实现了高达3倍的速度提升，并且可以应用于对齐和推理任务。

Conclusion: 本文提出的Decoding Memory Pipeline (DMP)能够显著提高多响应生成的效率，并且在不牺牲AUROC性能的情况下实现了高达3倍的速度提升。此外，DMP具有扩展到对齐和推理任务的潜力。

Abstract: Large language models (LLMs) have demonstrated impressive performance in both
research and real-world applications, but they still struggle with
hallucination. Existing hallucination detection methods often perform poorly on
sentence-level generation or rely heavily on domain-specific knowledge. While
self-consistency approaches help address these limitations, they incur high
computational costs due to repeated generation. In this paper, we conduct the
first study on identifying redundancy in self-consistency methods, manifested
as shared prefix tokens across generations, and observe that non-exact-answer
tokens contribute minimally to the semantic content. Based on these insights,
we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation
through selective inference and annealed decoding. Being orthogonal to the
model, dataset, decoding strategy, and self-consistency baseline, our DMP
consistently improves the efficiency of multi-response generation and holds
promise for extension to alignment and reasoning tasks. Extensive experiments
show that our method achieves up to a 3x speedup without sacrificing AUROC
performance.

</details>


### [14] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
*Daria Kryvosheieva,Saba Sturua,Michael Günther,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: Jina-code-embeddings 是一种新的代码嵌入模型套件，利用自回归骨干网络在文本和代码上预训练，并通过最后的标记池化生成嵌入，实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 设计一种新的代码嵌入模型套件，以从自然语言查询中检索代码、执行技术问答以及跨编程语言识别语义相似的代码片段。

Method: 使用自回归骨干网络在文本和代码上进行预训练，并通过最后的标记池化生成嵌入。

Result: 展示了在相对较小的模型规模下达到最先进的性能。

Conclusion: 该方法验证了代码嵌入模型构建的有效性。

Abstract: jina-code-embeddings is a novel code embedding model suite designed to
retrieve code from natural language queries, perform technical
question-answering, and identify semantically similar code snippets across
programming languages. It makes innovative use of an autoregressive backbone
pre-trained on both text and code, generating embeddings via last-token
pooling. We outline the training recipe and demonstrate state-of-the-art
performance despite the relatively small size of the models, validating this
approach to code embedding model construction.

</details>


### [15] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
*João Guilherme Alves Santos,Giovana Kerche Bonás,Thales Sales Almeida*

Main category: cs.CL

TL;DR: 本文提出了一个更新的BLUEX数据集，包含2024-2025年的考试和图像说明，以增强其在LLM预训练中的数据污染研究的相关性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）能力的增长，需要更强大的评估方法，特别是在多语言和非英语环境中。

Method: 本文介绍了更新后的BLUEX数据集，并评估了商业和开源LLM在利用图像说明中的视觉上下文能力。

Result: 图像说明策略提高了文本模型的可访问性超过40%，产生了1,422个可用问题，比原始BLUEX多了两倍以上。

Conclusion: 本文提出了更新后的BLUEX数据集，包括2024-2025年的考试和使用最先进的模型生成的图像说明，增强了其在LLM预训练中的数据污染研究的相关性。

Abstract: With the growing capabilities of Large Language Models (LLMs), there is an
increasing need for robust evaluation methods, especially in multilingual and
non-English contexts. We present an updated version of the BLUEX dataset, now
including 2024-2025 exams and automatically generated image captions using
state-of-the-art models, enhancing its relevance for data contamination studies
in LLM pretraining. Captioning strategies increase accessibility to text-only
models by more than 40%, producing 1,422 usable questions, more than doubling
the number in the original BLUEX. We evaluated commercial and open-source LLMs
and their ability to leverage visual context through captions.

</details>


### [16] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: 本文回顾了构建和使用大型语言模型的16个关键挑战，并分析了两种最先进的模型如何解决这些挑战，同时展示了封闭源代码模型与开放源代码模型之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在改变各个行业的人工智能，但其开发和部署仍然复杂。本文旨在提供对当前大型语言模型能力、限制和最佳实践的理解。

Method: 本文回顾了构建和使用大型语言模型的16个关键挑战，并分析了两种最先进的模型如何解决这些挑战。

Result: 本文展示了封闭源代码模型（强大的安全性、精细调整的可靠性）与开放源代码模型（效率、适应性）之间的权衡，并探讨了不同领域中大型语言模型的应用。

Conclusion: 本文旨在帮助AI研究人员、开发者和决策者了解当前大型语言模型的能力、限制和最佳实践。

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [17] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
*Alexandre Kabbach*

Main category: cs.CL

TL;DR: 本文重新审视图灵测试，提出图灵测试关注的是正常/平均的人类智能，而非非凡的智能。大型语言模型如ChatGPT因其目标是非凡智能而不太可能通过图灵测试，这引发了关于人类心智是否可被还原为正常心智的问题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在重新评估图灵测试的意义，并探讨其与人类认知的关系。

Method: 本文通过重新审视图灵测试的概念，提出统计学上的“正常”（即平均值）有助于理解图灵测试的两个方面：一是图灵测试关注的是正常/平均而非非凡的人类智能，二是图灵测试是一种统计测试，判断智能不是由单一的“平均”评委（非专家）进行，而是由一个完整的陪审团进行。

Result: 本文认为大型语言模型如ChatGPT不太可能通过图灵测试，因为它们专注于非凡而非正常/平均的人类智能。此外，它指出图灵测试是否有助于理解人类认知的问题，涉及人类心智是否可以被还原为正常/平均的心智。

Conclusion: 本文得出两个结论。首先，它认为像ChatGPT这样的大型语言模型不太可能通过图灵测试，因为这些模型精确地针对的是非凡而非正常/平均的人类智能。因此，它们构成了所谓的“人工聪明”而不是“人工智能”。其次，它认为图灵测试是否能对理解人类认知有所贡献的核心问题在于，人类心智是否真的可以被还原为正常/平均的心智——这个问题很大程度上超越了图灵测试本身，并质疑了它所属的正常主义范式的概念基础。

Abstract: This paper proposes to revisit the Turing test through the concept of
normality. Its core argument is that the statistical interpretation of the
normal--understood as the average both in the normative and mathematical sense
of the term--proves useful for understanding the Turing test in at least two
ways. First, in the sense that the Turing test targets normal/average rather
than exceptional human intelligence, so that successfully passing the test
requires building machines that "make mistakes" and display imperfect behavior
just like normal/average humans. Second, in the sense that the Turing test is a
statistical test where judgments of intelligence are never carried out by a
single "average" judge (understood as non-expert) but always by a full jury. As
such, the notion of "average human interrogator" that Turing talks about in his
original paper should be understood primarily as referring to a mathematical
abstraction made of the normalized aggregate of individual judgments of
multiple judges. In short, this paper argues that the Turing test is a test of
normal intelligence as assessed by a normal judge characterizing the average
judgment of a pool of human interrogators. Its conclusions are twofold. First,
it argues that large language models such as ChatGPT are unlikely to pass the
Turing test as those models precisely target exceptional rather than
normal/average human intelligence. As such, they constitute models of what it
proposes to call artificial smartness rather than artificial intelligence per
se. Second, it argues that the core question of whether the Turing test can
contribute anything to the understanding of human cognition is that of whether
the human mind is really reducible to the normal/average mind--a question which
largely extends beyond the Turing test itself and questions the conceptual
underpinnings of the normalist paradigm it belongs to.

</details>


### [18] [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TL;DR: 本文研究了自动文本摘要评估中的可重复性挑战，发现文献中的性能与实际实验结果存在差异，并提出一个开源框架以支持公平比较。结果表明，高精度指标通常计算密集且不稳定，同时强调了依赖大型语言模型进行评估的问题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究自动文本摘要评估中的可重复性挑战，并揭示文献中报告的性能与实验设置中观察到的性能之间的显著差异。

Method: 本文引入了一个统一的开源框架，应用于SummEval数据集，旨在支持公平透明的评估指标比较。

Result: 结果表明，与人类判断最一致的指标往往计算密集且运行不稳定。此外，本文强调了依赖大型语言模型进行评估的关键问题，包括其随机性、技术依赖性和有限的可重复性。

Conclusion: 本文倡导更稳健的评估协议，包括详尽的文档和方法标准化，以确保自动摘要评估的可靠性。

Abstract: This paper investigates reproducibility challenges in automatic text
summarization evaluation. Based on experiments conducted across six
representative metrics ranging from classical approaches like ROUGE to recent
LLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies
between reported performances in the literature and those observed in our
experimental setting. We introduce a unified, open-source framework, applied to
the SummEval dataset and designed to support fair and transparent comparison of
evaluation metrics. Our results reveal a structural trade-off: metrics with the
highest alignment with human judgments tend to be computationally intensive and
less stable across runs. Beyond comparative analysis, this study highlights key
concerns about relying on LLMs for evaluation, stressing their randomness,
technical dependencies, and limited reproducibility. We advocate for more
robust evaluation protocols including exhaustive documentation and
methodological standardization to ensure greater reliability in automatic
summarization assessment.

</details>


### [19] [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)
*Nils Dycke,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本研究评估了自动审稿生成器在检测研究逻辑缺陷方面的能力，发现它们在这方面存在局限性，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在加速和辅助学术同行评审方面具有巨大潜力，但潜在的偏见和系统性错误可能对科学完整性构成重大风险，因此需要理解最先进的ARGs的具体能力和限制。

Method: 我们提出了一种完全自动化的反事实评估框架，用于在受控条件下隔离和测试检测错误研究逻辑的能力。

Result: 测试多种ARG方法后，我们发现研究逻辑中的缺陷对其输出审稿没有显著影响。

Conclusion: 我们的研究发现，当前的自动审稿生成器（ARGs）在检测研究逻辑缺陷方面存在局限性，因此需要改进这些系统以确保科学完整性。

Abstract: Large Language Models (LLMs) have great potential to accelerate and support
scholarly peer review and are increasingly used as fully automatic review
generators (ARGs). However, potential biases and systematic errors may pose
significant risks to scientific integrity; understanding the specific
capabilities and limitations of state-of-the-art ARGs is essential. We focus on
a core reviewing skill that underpins high-quality peer review: detecting
faulty research logic. This involves evaluating the internal consistency
between a paper's results, interpretations, and claims. We present a fully
automated counterfactual evaluation framework that isolates and tests this
skill under controlled conditions. Testing a range of ARG approaches, we find
that, contrary to expectation, flaws in research logic have no significant
effect on their output reviews. Based on our findings, we derive three
actionable recommendations for future work and release our counterfactual
dataset and evaluation framework publicly.

</details>


### [20] [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.21430)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Cheng-Yi Li,Wei-Chieh Fang,Hsin-Yu Wu,Haiqin Zhong,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 本文介绍了 Med-RewardBench，这是首个专门用于评估医疗奖励模型和法官的基准。该基准涵盖多个临床领域，并通过严格的过程确保数据质量。评估显示，现有模型在与专家判断对齐方面面临挑战，但通过微调可以显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要关注一般多模态大语言模型的能力或将其作为求解器进行评估，而忽略了诊断准确性和临床相关性等关键评估维度。因此，需要一个专门针对医疗需求的基准来填补这一空白。

Method: 本文介绍了 Med-RewardBench 基准，该基准包含跨13个器官系统和8个临床部门的多模态数据集，并通过严格的三步流程确保高质量的评估数据。同时，评估了32个最先进的多模态大语言模型，并开发了通过微调获得性能提升的基线模型。

Result: 评估结果显示，将输出与专家判断对齐存在重大挑战。然而，通过微调开发的基线模型表现出显著的性能提升。

Conclusion: Med-RewardBench 是一个专门设计用于评估医疗奖励模型和法官的基准，它展示了在医学场景中对大型多模态语言模型进行评估的重要性。此外，通过微调开发的基线模型显示出显著的性能提升。

Abstract: Multimodal large language models (MLLMs) hold significant potential in
medical applications, including disease diagnosis and clinical decision-making.
However, these tasks require highly accurate, context-sensitive, and
professionally aligned responses, making reliable reward models and judges
critical. Despite their importance, medical reward models (MRMs) and judges
remain underexplored, with no dedicated benchmarks addressing clinical
requirements. Existing benchmarks focus on general MLLM capabilities or
evaluate models as solvers, neglecting essential evaluation dimensions like
diagnostic accuracy and clinical relevance. To address this, we introduce
Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and
judges in medical scenarios. Med-RewardBench features a multimodal dataset
spanning 13 organ systems and 8 clinical departments, with 1,026
expert-annotated cases. A rigorous three-step process ensures high-quality
evaluation data across six clinically critical dimensions. We evaluate 32
state-of-the-art MLLMs, including open-source, proprietary, and
medical-specific models, revealing substantial challenges in aligning outputs
with expert judgment. Additionally, we develop baseline models that demonstrate
substantial performance improvements through fine-tuning.

</details>


### [21] [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)
*Yunhao Zhang,Shaonan Wang,Nan Lin,Xinyi Dong,Chong Li,Chengqing Zong*

Main category: cs.CL

TL;DR: 本文提出了一种新方法，通过分解词嵌入来识别更细粒度的语义子维度，并验证了这些子维度的神经合理性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于预定义的语义维度，仅提供宽泛的表示，忽略了更细粒度的概念区分。本文旨在探索更细粒度的语义子维度。

Method: 本文引入了离散连续语义表示模型（DCSRM），将大型语言模型的词嵌入分解为多个子嵌入，每个子嵌入编码特定的语义信息，并使用体素级编码模型将这些子维度映射到大脑激活。

Result: 本文识别出一组可解释的语义子维度，并发现语义维度根据不同的原则结构化，极性是驱动其分解的关键因素。此外，所识别子维度的神经相关性支持其认知和神经科学的合理性。

Conclusion: 本文提出了一个新颖的框架来研究粗粒度语义维度下的子维度，通过DCSRM模型分解词嵌入以识别可解释的语义子维度，并验证了这些子维度的神经合理性。

Abstract: Understanding the core dimensions of conceptual semantics is fundamental to
uncovering how meaning is organized in language and the brain. Existing
approaches often rely on predefined semantic dimensions that offer only broad
representations, overlooking finer conceptual distinctions. This paper proposes
a novel framework to investigate the subdimensions underlying coarse-grained
semantic dimensions. Specifically, we introduce a Disentangled Continuous
Semantic Representation Model (DCSRM) that decomposes word embeddings from
large language models into multiple sub-embeddings, each encoding specific
semantic information. Using these sub-embeddings, we identify a set of
interpretable semantic subdimensions. To assess their neural plausibility, we
apply voxel-wise encoding models to map these subdimensions to brain
activation. Our work offers more fine-grained interpretable semantic
subdimensions of conceptual meaning. Further analyses reveal that semantic
dimensions are structured according to distinct principles, with polarity
emerging as a key factor driving their decomposition into subdimensions. The
neural correlates of the identified subdimensions support their cognitive and
neuroscientific plausibility.

</details>


### [22] [Beyond the Surface: Probing the Ideological Depth of Large Language Models](https://arxiv.org/abs/2508.21448)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）中的“意识形态深度”，通过测量其可操控性和内部机制，发现意识形态深度是可量化的属性，并且可操控性提供了对模型潜在政治架构的洞察。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）表现出明显的意识形态倾向，但这些立场的稳定性和深度仍不明确。表面响应可以通过简单的提示工程进行操纵，这引发了它们是否反映连贯的内在意识形态的疑问。本文研究了LLMs中的“意识形态深度”概念，即其内部政治表示的稳健性和复杂性。

Method: 我们采用双重方法：首先，我们使用指令提示和激活控制来测量两个著名开源LLMs的“可操控性”。其次，我们使用稀疏自编码器（SAEs）探测这些模型的内部机制。

Result: 我们发现，一些模型可以轻松在自由主义和保守主义观点之间切换，而其他模型则表现出抵抗或拒绝率增加，这表明一种更牢固的意识形态结构。此外，分析显示，可操控性较低的模型拥有更多独特且抽象的意识形态特征。

Conclusion: 我们的研究结果表明，意识形态深度是LLMs的一个可量化的属性，而可操控性为了解其潜在的政治架构提供了一个有价值的窗口。

Abstract: Large Language Models (LLMs) have demonstrated pronounced ideological
leanings, yet the stability and depth of these positions remain poorly
understood. Surface-level responses can often be manipulated through simple
prompt engineering, calling into question whether they reflect a coherent
underlying ideology. This paper investigates the concept of "ideological depth"
in LLMs, defined as the robustness and complexity of their internal political
representations. We employ a dual approach: first, we measure the
"steerability" of two well-known open-source LLMs using instruction prompting
and activation steering. We find that while some models can easily switch
between liberal and conservative viewpoints, others exhibit resistance or an
increased rate of refusal, suggesting a more entrenched ideological structure.
Second, we probe the internal mechanisms of these models using Sparse
Autoencoders (SAEs). Preliminary analysis reveals that models with lower
steerability possess more distinct and abstract ideological features. Our
evaluations reveal that one model can contain 7.3x more political features than
another model of similar size. This allows targeted ablation of a core
political feature in an ideologically "deep" model, leading to consistent,
logical shifts in its reasoning across related topics, whereas the same
intervention in a "shallow" model results in an increase in refusal outputs.
Our findings suggest that ideological depth is a quantifiable property of LLMs
and that steerability serves as a valuable window into their latent political
architecture.

</details>


### [23] [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)
*Xiaolong Wei,Bo Lu,Xingyu Zhang,Zhejun Zhao,Dongdong Shen,Long Xia,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种通过RLAIF框架激发小型语言模型创造性写作能力的方法，其中基于原则引导的LLM-as-a-Judge策略在生成质量和训练效率方面优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）展示了出色的创造性写作能力，但它们的计算需求很高，限制了广泛应用。增强小型语言模型（SLMs）提供了一个有希望的替代方案，但当前方法如监督微调（SFT）在新颖性方面存在困难，而基于人类反馈的强化学习（RLHF）成本高昂。因此，本文旨在探索更有效的方法来激发SLM的创造性写作能力。

Method: 本文在RLAIF框架下探索了两种不同的AI驱动的奖励策略，以激发7B参数的SLM在生成中文问候语方面的创造性写作能力。第一种策略使用了一个在高质量偏好数据上训练的RM，这些数据由一个针对创造性任务设计的新颖多智能体拒绝采样框架收集。第二种策略利用了一个基于原则引导的LLM-as-a-Judge，其奖励函数通过带有反思机制的对抗训练方案进行优化，直接提供奖励信号。

Result: 实验结果表明，这两种方法都能显著提升基线的创造性输出，但基于原则引导的LLM-as-a-Judge策略在生成质量上表现更优。此外，它在训练效率和减少对人工标注数据的依赖方面也有明显优势。

Conclusion: 本文提出了一种通过RLAIF框架来激发小型语言模型（SLM）的创造性写作能力的方法，其中第二种基于原则引导的LLM-as-a-Judge策略在生成质量上表现出色，并且在训练效率和减少对人工标注数据的依赖方面具有优势，为创造性的SLM提供了一条更可扩展和有效的路径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable creative writing
capabilities, yet their substantial computational demands hinder widespread
use. Enhancing Small Language Models (SLMs) offers a promising alternative, but
current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and
Reinforcement Learning from Human Feedback (RLHF) is costly. This paper
explores two distinct AI-driven reward strategies within a Reinforcement
Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a
7B-parameter SLM, specifically for generating Chinese greetings. The first
strategy employs a RM trained on high-quality preference data curated by a
novel multi-agent rejection sampling framework designed for creative tasks. The
second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose
reward function is optimized via an adversarial training scheme with a
reflection mechanism, to directly provide reward signals. Comprehensive
experiments reveal that while both approaches significantly enhance creative
output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields
superior generation quality. Furthermore, it offers notable advantages in
training efficiency and reduced dependency on human-annotated data, presenting
a more scalable and effective path towards creative SLMs. Our automated
evaluation methods also exhibit strong alignment with human judgments. Our code
and data are publicly available at
https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.

</details>


### [24] [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)
*Sara B. Coutinho,Rafael M. O. Cruz,Francimaria R. S. Nascimento,George D. C. Cavalcanti*

Main category: cs.CL

TL;DR: 本文提出了一种新的自动分类器选择方法，该方法优先考虑多样性，并通过性能进行扩展。实验结果表明，该方法在六个数据集中的两个上取得了最高的准确率。


<details>
  <summary>Details</summary>
Motivation: 心理偏见使得个体特别容易相信和传播虚假新闻，这在公共卫生和政治等领域造成了重大后果。机器学习事实核查系统已被广泛研究以缓解这个问题。然而，集成方法的性能严重依赖于构成分类器的多样性，选择真正多样的模型仍然是一个关键挑战，尤其是在模型倾向于学习冗余模式的情况下。

Method: 本文提出了一种新的自动分类器选择方法，首先计算分类器之间的成对多样性，并应用层次聚类将它们组织成不同粒度的组。然后，通过层次选择探索这些层次以选择每个层次的一个分类器池，每个池代表一种不同的内部池多样性。最终选择最多样化的池用于集成构建。

Result: 实验结果表明，该方法在六个数据集中的两个上取得了最高的准确率。

Conclusion: 本文提出了一种新的自动分类器选择方法，该方法优先考虑多样性，并通过性能进行扩展。实验结果表明，该方法在六个数据集中的两个上取得了最高的准确率。

Abstract: Psychological biases, such as confirmation bias, make individuals
particularly vulnerable to believing and spreading fake news on social media,
leading to significant consequences in domains such as public health and
politics. Machine learning-based fact-checking systems have been widely studied
to mitigate this problem. Among them, ensemble methods are particularly
effective in combining multiple classifiers to improve robustness. However,
their performance heavily depends on the diversity of the constituent
classifiers-selecting genuinely diverse models remains a key challenge,
especially when models tend to learn redundant patterns. In this work, we
propose a novel automatic classifier selection approach that prioritizes
diversity, also extended by performance. The method first computes pairwise
diversity between classifiers and applies hierarchical clustering to organize
them into groups at different levels of granularity. A HierarchySelect then
explores these hierarchical levels to select one pool of classifiers per level,
each representing a distinct intra-pool diversity. The most diverse pool is
identified and selected for ensemble construction from these. The selection
process incorporates an evaluation metric reflecting each classifiers's
performance to ensure the ensemble also generalises well. We conduct
experiments with 40 heterogeneous classifiers across six datasets from
different application domains and with varying numbers of classes. Our method
is compared against the Elbow heuristic and state-of-the-art baselines. Results
show that our approach achieves the highest accuracy on two of six datasets.
The implementation details are available on the project's repository:
https://github.com/SaraBCoutinho/HSFN .

</details>


### [25] [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)
*Aishwarya Mirashi,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: 本文提出了MahaSTS数据集和MahaSBERT-STS-v2模型，用于改进马拉地语的句子相似性任务。


<details>
  <summary>Details</summary>
Motivation: 为了在低资源环境下提高句子相似性任务的性能，需要一个高质量的人工标注数据集和经过优化的模型。

Method: 本文提出了MahaSTS数据集，并对MahaSBERT模型进行了微调，以优化回归式的相似性评分。

Result: 实验结果表明，MahaSTS能够有效训练句子相似性任务，同时减少了标签偏差并提高了模型稳定性。

Conclusion: MahaSTS和MahaSBERT-STS-v2在低资源环境下展示了有效训练句子相似性任务的潜力，强调了人工标注、针对性微调和结构化监督的重要性。数据集和模型已公开共享。

Abstract: We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)
dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT
model optimized for regression-based similarity scoring. The MahaSTS dataset
consists of 16,860 Marathi sentence pairs labeled with continuous similarity
scores in the range of 0-5. To ensure balanced supervision, the dataset is
uniformly distributed across six score-based buckets spanning the full 0-5
range, thus reducing label bias and enhancing model stability. We fine-tune the
MahaSBERT model on this dataset and benchmark its performance against other
alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments
demonstrate that MahaSTS enables effective training for sentence similarity
tasks in Marathi, highlighting the impact of human-curated annotations,
targeted fine-tuning, and structured supervision in low-resource settings. The
dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>


### [26] [A Survey on Current Trends and Recent Advances in Text Anonymization](https://arxiv.org/abs/2508.21587)
*Tobias Deußer,Lorenz Sparrenberg,Armin Berger,Max Hahnbück,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文综述了文本匿名化技术的最新进展，包括命名实体识别、大语言模型的应用、领域特定解决方案以及评估框架，并指出了未来的挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着包含敏感个人信息的文本数据在各个领域的激增，需要强大的匿名化技术来保护隐私并遵守法规，同时保持数据对各种关键下游任务的可用性。

Method: 本文首先讨论了以命名实体识别为基础的方法，然后探讨了大语言模型在文本匿名化中的双重作用，包括作为先进的匿名化工具和强大的去匿名化威胁。此外，还研究了结合正式隐私模型和风险感知框架的高级方法，以及作者身份匿名化的专门子领域。

Result: 本文提供了对文本匿名化技术的全面概述，涵盖了领域特定的挑战和定制解决方案，以及评估框架、综合指标、基准测试和实用工具包。

Conclusion: 本文综述了当前文本匿名化技术的趋势和最新进展，总结了现有知识，识别了新兴趋势和持续挑战，并旨在为该领域的学术界和实践者提供未来研究方向的指导。

Abstract: The proliferation of textual data containing sensitive personal information
across various domains requires robust anonymization techniques to protect
privacy and comply with regulations, while preserving data usability for
diverse and crucial downstream tasks. This survey provides a comprehensive
overview of current trends and recent advances in text anonymization
techniques. We begin by discussing foundational approaches, primarily centered
on Named Entity Recognition, before examining the transformative impact of
Large Language Models, detailing their dual role as sophisticated anonymizers
and potent de-anonymization threats. The survey further explores
domain-specific challenges and tailored solutions in critical sectors such as
healthcare, law, finance, and education. We investigate advanced methodologies
incorporating formal privacy models and risk-aware frameworks, and address the
specialized subfield of authorship anonymization. Additionally, we review
evaluation frameworks, comprehensive metrics, benchmarks, and practical
toolkits for real-world deployment of anonymization solutions. This review
consolidates current knowledge, identifies emerging trends and persistent
challenges, including the evolving privacy-utility trade-off, the need to
address quasi-identifiers, and the implications of LLM capabilities, and aims
to guide future research directions for both academics and practitioners in
this field.

</details>


### [27] [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
*Zinan Tang,Xin Gao,Qizhi Pei,Zhuoshi Pan,Mengzhang Cai,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Middo的自进化模型感知动态数据优化框架，通过模型感知的数据选择和上下文保留的数据精炼来提高大型语言模型的训练数据质量。实验表明，该方法在多个基准测试中显著提升了初始数据的质量，并提高了LLM的性能，平均准确率提高了7.15%。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调大型语言模型的方法依赖于高质量的训练数据，但传统方法在静态数据集的维护上难以适应模型能力的演变。因此，需要一种能够动态优化数据的框架，以提高模型的性能。

Method: Middo框架采用了一个闭环优化系统，包括：(1) 一个自我参照的诊断模块，通过三轴模型信号（损失模式、嵌入聚类动态和自我对齐分数）主动识别低质量样本；(2) 一个自适应优化引擎将低质量样本转化为有教学价值的训练点，同时保持语义完整性；(3) 通过动态学习原理，优化过程随着模型能力的提升而持续进化。

Result: 实验结果表明，Middo框架在多个基准测试中显著提升了初始数据的质量，并提高了LLM的性能，平均准确率提高了7.15%。

Conclusion: 本文提出了一种名为Middo的自进化模型感知动态数据优化框架，通过模型感知的数据选择和上下文保留的数据精炼来提高大型语言模型的训练数据质量。实验表明，该方法在多个基准测试中显著提升了初始数据的质量，并提高了LLM的性能，平均准确率提高了7.15%。这项工作通过数据和模型的动态人机协同进化建立了一种新的可持续LLM训练范式。

Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely
on high-quality training data. While data selection and data synthesis are two
common strategies to improve data quality, existing approaches often face
limitations in static dataset curation that fail to adapt to evolving model
capabilities. In this paper, we introduce Middo, a self-evolving Model-informed
dynamic data optimization framework that uses model-aware data selection and
context-preserving data refinement. Unlike conventional one-off
filtering/synthesis methods, our framework establishes a closed-loop
optimization system: (1) A self-referential diagnostic module proactively
identifies suboptimal samples through tri-axial model signals - loss patterns
(complexity), embedding cluster dynamics (diversity), and self-alignment scores
(quality); (2) An adaptive optimization engine then transforms suboptimal
samples into pedagogically valuable training points while preserving semantic
integrity; (3) This optimization process continuously evolves with model
capability through dynamic learning principles. Experiments on multiple
benchmarks demonstrate that our \method consistently enhances the quality of
seed data and boosts LLM's performance with improving accuracy by 7.15% on
average while maintaining the original dataset scale. This work establishes a
new paradigm for sustainable LLM training through dynamic human-AI co-evolution
of data and models. Our datasets, models, and code are coming soon.

</details>


### [28] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
*Sarfaroz Yunusov,Kaige Chen,Kazi Nishat Anwar,Ali Emami*

Main category: cs.CL

TL;DR: 研究发现不同人格类型的用户对LLM有系统性的偏好，这表明基于人格的分析可以揭示传统评估所忽视的LLM差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨用户的人格特质是否会影响他们对特定LLM的偏好。

Method: 研究包括32名参与者，他们被平均分配到四种Keirsey人格类型中，并在四个协作任务中评估他们与GPT-4和Claude 3.5的互动。

Result: 结果表明，理性型用户更倾向于GPT-4，而理想型用户更倾向于Claude 3.5，其他人格类型则表现出任务依赖性偏好。

Conclusion: 研究发现，不同人格类型的用户对LLM有系统性的偏好，这表明基于人格的分析可以揭示传统评估所忽视的LLM差异。

Abstract: As Large Language Models (LLMs) increasingly integrate into everyday
workflows, where users shape outcomes through multi-turn collaboration, a
critical question emerges: do users with different personality traits
systematically prefer certain LLMs over others? We conducted a study with 32
participants evenly distributed across four Keirsey personality types,
evaluating their interactions with GPT-4 and Claude 3.5 across four
collaborative tasks: data analysis, creative writing, information retrieval,
and writing assistance. Results revealed significant personality-driven
preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented
tasks, while idealists favored Claude 3.5, especially for creative and
analytical tasks. Other personality types showed task-dependent preferences.
Sentiment analysis of qualitative feedback confirmed these patterns. Notably,
aggregate helpfulness ratings were similar across models, showing how
personality-based analysis reveals LLM differences that traditional evaluations
miss.

</details>


### [29] [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)
*Peng Yu,En Xu,Bin Chen,Haibiao Chen,Yinfei Xu*

Main category: cs.CL

TL;DR: 本文介绍了QZhou-Embedding，一个基于Qwen2.5-7B-Instruct的通用上下文文本嵌入模型。通过统一的多任务框架和数据合成管道，以及两阶段训练策略，该模型在多个基准测试中取得最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 为了提升检索模型的性能，需要高质量、多样化的数据。同时，利用大语言模型的生成能力可以进一步优化数据质量，从而实现嵌入模型的突破。

Method: 我们设计了一个统一的多任务框架，包括专门的数据转换和训练策略。数据转换方案使更多样化的文本训练数据集得以纳入，而任务特定的训练策略提高了模型的学习效率。我们开发了一个数据合成管道，利用LLM API，结合改写、增强和困难负例生成等技术来提高训练集的语义丰富性和样本难度。此外，我们采用两阶段训练策略，包括初始的检索导向预训练和全任务微调，使嵌入模型能够在强大的检索性能基础上扩展其能力。

Result: 我们的模型在MTEB和CMTEB基准测试中取得了最先进的结果，并且在重新排序、聚类等任务中也表现出色。

Conclusion: 我们的模型在MTEB和CMTEB基准测试中取得了最先进的结果，并且在重新排序、聚类等任务中也表现出色。研究结果表明，高质量、多样化的数据对于提升检索模型性能至关重要，利用大语言模型的生成能力可以进一步优化数据质量以实现嵌入模型的突破。

Abstract: We present QZhou-Embedding, a general-purpose contextual text embedding model
with exceptional text representation capabilities. Built upon the
Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task
framework comprising specialized data transformation and training strategies.
The data transformation scheme enables the incorporation of more diverse
textual training datasets, while the task-specific training strategies enhance
model learning efficiency. We developed a data synthesis pipeline leveraging
LLM API, incorporating techniques such as paraphrasing, augmentation, and hard
negative example generation to improve the semantic richness and sample
difficulty of the training set. Additionally, we employ a two-stage training
strategy, comprising initial retrieval-focused pretraining followed by
full-task fine-tuning, enabling the embedding model to extend its capabilities
based on robust retrieval performance. Our model achieves state-of-the-art
results on the MTEB and CMTEB benchmarks, ranking first on both leaderboards
(August 27 2025), and simultaneously achieves state-of-the-art performance on
tasks including reranking, clustering, etc. Our findings demonstrate that
higher-quality, more diverse data is crucial for advancing retrieval model
performance, and that leveraging LLMs generative capabilities can further
optimize data quality for embedding model breakthroughs. Our model weights are
released on HuggingFace under Apache 2.0 license. For reproducibility, we
provide evaluation code and instructions on GitHub.

</details>


### [30] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一个包含2,604个真实世界可视化的基准数据集Misviz，并发布了81,814个合成可视化数据集Misviz-synth。通过使用最先进的MLLMs、基于规则的系统和微调分类器进行评估，发现该任务仍然具有高度挑战性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大型、多样且公开可用的数据集，AI模型的训练和评估受到限制。自动检测误导性可视化并识别其违反的具体设计规则可以帮助保护读者并减少错误信息的传播。

Method: 本文引入了Misviz数据集和Misviz-synth合成数据集，并使用最先进的MLLMs、基于规则的系统和微调分类器对这两个数据集进行了全面评估。

Result: 评估结果显示，该任务仍然具有高度挑战性。

Conclusion: 本文介绍了Misviz和Misviz-synth数据集，以及对它们的评估结果。研究发现该任务仍然具有高度挑战性，并且释放了相关数据集和代码。

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [31] [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)
*Yao Wang,Di Liang,Minlong Peng*

Main category: cs.CL

TL;DR: 本文提出了一种新的CPI-FT框架，通过识别和隔离核心参数区域，有效减轻了多任务微调中的任务干扰和遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决监督微调（SFT）中由于无差别参数更新导致的任务性能下降问题，即“跷跷板现象”。

Method: 我们提出了一个新颖的Core Parameter Isolation Fine-Tuning (CPI-FT)框架，包括独立微调每个任务以识别其核心参数区域，基于区域重叠对任务进行分组，引入参数融合技术，以及使用混合任务数据的轻量级、管道式SFT训练阶段。

Result: 在多个公共基准测试中，我们的方法显著减轻了任务干扰和遗忘，并优于普通的多任务和多阶段微调基线。

Conclusion: 我们的方法显著减轻了任务干扰和遗忘，始终优于普通的多任务和多阶段微调基线。

Abstract: Supervised fine-tuning (SFT) is a pivotal approach to adapting large language
models (LLMs) for downstream tasks; however, performance often suffers from the
``seesaw phenomenon'', where indiscriminate parameter updates yield progress on
certain tasks at the expense of others. To address this challenge, we propose a
novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.
Specifically, we first independently fine-tune the LLM on each task to identify
its core parameter regions by quantifying parameter update magnitudes. Tasks
with similar core regions are then grouped based on region overlap, forming
clusters for joint modeling. We further introduce a parameter fusion technique:
for each task, core parameters from its individually fine-tuned model are
directly transplanted into a unified backbone, while non-core parameters from
different tasks are smoothly integrated via Spherical Linear Interpolation
(SLERP), mitigating destructive interference. A lightweight, pipelined SFT
training phase using mixed-task data is subsequently employed, while freezing
core regions from prior tasks to prevent catastrophic forgetting. Extensive
experiments on multiple public benchmarks demonstrate that our approach
significantly alleviates task interference and forgetting, consistently
outperforming vanilla multi-task and multi-stage fine-tuning baselines.

</details>


### [32] [Reasoning-Intensive Regression](https://arxiv.org/abs/2508.21762)
*Diane Tchuindjo,Omar Khattab*

Main category: cs.CL

TL;DR: The paper explores the application of large language models (LLMs) to reasoning-intensive regression (RiR) tasks, where subtle numerical properties are deduced from text. It introduces MENTAT, a method combining batch-reflective prompt optimization with neural ensemble learning, achieving significant improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: AI researchers and practitioners increasingly apply large language models (LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing subtle numerical properties from text.

Method: We propose MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning.

Result: We cast three realistic problems as RiR tasks to establish an initial benchmark, and use that to test our hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR.

Conclusion: MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR.

Abstract: AI researchers and practitioners increasingly apply large language models
(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing
subtle numerical properties from text. Unlike standard language regression
tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc
problems like rubric-based scoring or domain-specific retrieval, where much
deeper analysis of text is required while only limited task-specific training
data and computation are available. We cast three realistic problems as RiR
tasks to establish an initial benchmark, and use that to test our hypothesis
that prompting frozen LLMs and finetuning Transformer encoders via gradient
descent will both often struggle in RiR. We then propose MENTAT, a simple and
lightweight method that combines batch-reflective prompt optimization with
neural ensemble learning. MENTAT achieves up to 65% improvement over both
baselines, though substantial room remains for future advances in RiR.

</details>


### [33] [PiCSAR: Probabilistic Confidence Selection And Ranking](https://arxiv.org/abs/2508.21787)
*Joshua Ong Jun Leang,Zheng Zhao,Aryo Pradipta Gema,Sohee Yang,Wai-Chung Kwan,Xuanli He,Wenda Li,Pasquale Minervini,Eleonora Giunchiglia,Shay B. Cohen*

Main category: cs.CL

TL;DR: PiCSAR is a training-free method that improves the accuracy of large language models by scoring candidate generations using the joint log-likelihood of reasoning and final answer.


<details>
  <summary>Details</summary>
Motivation: The key challenge for reasoning tasks is designing a scoring function that can identify correct reasoning chains without access to ground-truth answers.

Method: Probabilistic Confidence Selection And Ranking (PiCSAR) is proposed, which scores each candidate generation using the joint log-likelihood of the reasoning and final answer.

Result: PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500, +9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in 16 out of 20 comparisons.

Conclusion: PiCSAR achieves substantial gains across diverse benchmarks and outperforms baselines with at least 2x fewer samples.

Abstract: Best-of-n sampling improves the accuracy of large language models (LLMs) and
large reasoning models (LRMs) by generating multiple candidate solutions and
selecting the one with the highest reward. The key challenge for reasoning
tasks is designing a scoring function that can identify correct reasoning
chains without access to ground-truth answers. We propose Probabilistic
Confidence Selection And Ranking (PiCSAR): a simple, training-free method that
scores each candidate generation using the joint log-likelihood of the
reasoning and final answer. The joint log-likelihood of the reasoning and final
answer naturally decomposes into reasoning confidence and answer confidence.
PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,
+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in
16 out of 20 comparisons. Our analysis reveals that correct reasoning chains
exhibit significantly higher reasoning and answer confidence, justifying the
effectiveness of PiCSAR.

</details>


### [34] [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)
*Inés Altemir Marinas,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 本文提出了一种基于ElasticSearch的框架，用于分析大型语言模型的训练数据集，实现了快速查询性能，并展示了实时数据集分析的实用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖于大规模数据集，但网络爬取的无差别性质引发了数据质量、安全性和伦理问题。然而，由于计算限制，之前对有害内容的研究仅限于小样本。

Method: 我们提出了一个基于ElasticSearch的管道框架，用于索引和分析LLM训练数据集。

Result: 我们应用该框架到SwissAI的FineWeb-2语料库（1.5TB，四种语言），实现了快速查询性能——大多数搜索在毫秒级，全部在2秒内完成。

Conclusion: 我们的工作展示了实时数据集分析，为更安全、更可问责的人工智能系统提供了实用工具。

Abstract: Large language models (LLMs) rely heavily on web-scale datasets like Common
Crawl, which provides over 80\% of training data for some modern models.
However, the indiscriminate nature of web crawling raises challenges in data
quality, safety, and ethics. Despite the critical importance of training data
quality, prior research on harmful content has been limited to small samples
due to computational constraints. This project presents a framework for
indexing and analyzing LLM training datasets using an ElasticSearch-based
pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),
achieving fast query performance--most searches in milliseconds, all under 2
seconds. Our work demonstrates real-time dataset analysis, offering practical
tools for safer, more accountable AI systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [35] [Stairway to Fairness: Connecting Group and Individual Fairness](https://arxiv.org/abs/2508.21334)
*Theresia Veronika Rampisela,Maria Maistro,Tuukka Ruotsalo,Falk Scholer,Christina Lioma*

Main category: cs.IR

TL;DR: 本文研究了群体公平和个体公平之间的关系，发现高度公平的群体推荐可能对个体非常不公平。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对群体公平和个体公平之间关系的科学理解，因为之前的工作在评估指标或目标上有所不同，无法进行适当的比较。

Method: 我们通过全面比较可用于两种公平类型的评估指标，研究了群体公平和个体公平之间的关系。

Result: 实验结果表明，高度公平的群体推荐可能对个体非常不公平。

Conclusion: 我们的研究发现，高度公平的群体推荐可能对个体非常不公平，这对RS从业者改进系统公平性具有重要价值。

Abstract: Fairness in recommender systems (RSs) is commonly categorised into group
fairness and individual fairness. However, there is no established scientific
understanding of the relationship between the two fairness types, as prior work
on both types has used different evaluation measures or evaluation objectives
for each fairness type, thereby not allowing for a proper comparison of the
two. As a result, it is currently not known how increasing one type of fairness
may affect the other. To fill this gap, we study the relationship of group and
individual fairness through a comprehensive comparison of evaluation measures
that can be used for both fairness types. Our experiments with 8 runs across 3
datasets show that recommendations that are highly fair for groups can be very
unfair for individuals. Our finding is novel and useful for RS practitioners
aiming to improve the fairness of their systems. Our code is available at:
https://github.com/theresiavr/stairway-to-fairness.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [36] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL is a universal programming language translator that uses a unified intermediate representation to enable bidirectional translation between multiple languages, such as CUDA, HIP, Metal, and more.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches require separate translators for each language pair, leading to exponential complexity growth. The goal is to enable bidirectional translation between multiple languages with minimal effort.

Method: CrossTL uses a single universal IR called CrossGL to facilitate translations between multiple languages. It consists of language-specific lexers/parsers, bidirectional CrossGL translation modules, and comprehensive backend implementations.

Result: The system successfully compiles and executes code across all supported backends. The universal IR design allows adding new languages with minimal effort.

Conclusion: CrossTL represents a significant step toward language-agnostic programming, enabling write-once, deploy-everywhere development.

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [37] [Database Normalization via Dual-LLM Self-Refinement](https://arxiv.org/abs/2508.17693)
*Eunjae Jo,Nakyung Lee,Gyuyeong Kim*

Main category: cs.DB

TL;DR: Miffie is a database normalization framework that leverages large language models to automate the process of database normalization, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Database normalization is time-consuming and error-prone, as it is typically performed manually by data engineers. Miffie aims to automate this process without human effort while preserving high accuracy.

Method: Miffie is a database normalization framework that leverages the capability of large language models. It uses a dual-model self-refinement architecture that combines the best-performing models for normalized schema generation and verification.

Result: Experimental results show that Miffie can normalize complex database schemas while maintaining high accuracy.

Conclusion: Miffie can normalize complex database schemas while maintaining high accuracy.

Abstract: Database normalization is crucial to preserving data integrity. However, it
is time-consuming and error-prone, as it is typically performed manually by
data engineers. To this end, we present Miffie, a database normalization
framework that leverages the capability of large language models. Miffie
enables automated data normalization without human effort while preserving high
accuracy. The core of Miffie is a dual-model self-refinement architecture that
combines the best-performing models for normalized schema generation and
verification, respectively. The generation module eliminates anomalies based on
the feedback of the verification module until the output schema satisfies the
requirement for normalization. We also carefully design task-specific zero-shot
prompts to guide the models for achieving both high accuracy and cost
efficiency. Experimental results show that Miffie can normalize complex
database schemas while maintaining high accuracy.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [38] [Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses](https://arxiv.org/abs/2508.21209)
*Vanessa Figueiredo*

Main category: cs.HC

TL;DR: 本文研究了巴西儿童如何使用对话代理，并提出了结构化脚手架设计建议，以提高他们的学习效果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究巴西儿童如何使用对话代理（CAs）进行学习、探索和娱乐，并探讨结构化脚手架如何增强这些互动。

Method: 本文进行了两项研究：第一项是为期七周的在线调查，涉及23名参与者（儿童、父母和教师），采用访谈、观察和认知工作分析来映射儿童的信息处理流程；第二项是通过GPT-4o-mini对1,200次模拟儿童-CA交互进行评估，比较结构化提示与无结构基线的对话树配方。

Result: 在研究1中，识别出三种CA功能：学校、探索和娱乐，并推导出类似亲子支持的“配方”脚手架。在研究2中，基于结构化提示的对话树配方在可读性、问题数量/深度/多样性以及连贯性方面优于无结构基线。

Conclusion: 本文提出了针对巴西儿童使用对话代理的结构化脚手架设计建议，包括基于脚手架的对话树、专为儿童设计的个人资料以及由照顾者定制的内容。

Abstract: This paper presents two studies on how Brazilian children (ages 9--11) use
conversational agents (CAs) for schoolwork, discovery, and entertainment, and
how structured scaffolds can enhance these interactions. In Study 1, a
seven-week online investigation with 23 participants (children, parents,
teachers) employed interviews, observations, and Cognitive Work Analysis to map
children's information-processing flows, the role of more knowledgeable others,
functional uses, contextual goals, and interaction patterns to inform
conversation-tree design. We identified three CA functions: School, Discovery,
Entertainment, and derived ``recipe'' scaffolds mirroring parent-child support.
In Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges,
comparing conversation-tree recipes based on structured-prompting to an
unstructured baseline. Quantitative evaluation of readability, question
count/depth/diversity, and coherence revealed gains for the recipe approach.
Building on these findings, we offer design recommendations: scaffolded
conversation-trees, child-dedicated profiles for personalized context, and
caregiver-curated content. Our contributions include the first CWA application
with Brazilian children, an empirical framework of child-CA information flows,
and an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective,
scaffolded learning.

</details>


### [39] [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456)
*Yi-Hao Peng,Dingzeyu Li,Jeffrey P. Bigham,Amy Pavel*

Main category: cs.HC

TL;DR: Morae is a UI agent that pauses to let users make choices during task execution, improving task completion and preference alignment for BLV users.


<details>
  <summary>Details</summary>
Motivation: Current UI agents perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, reducing user agency. For example, a BLV participant's request to buy the cheapest sparkling water was fulfilled by an agent choosing an option without considering alternatives with different flavors or better ratings.

Method: Morae identifies decision points during task execution and pauses to allow users to make choices. It uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompts users for clarification when there is a choice to be made.

Result: In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences compared to baseline agents, including OpenAI Operator.

Conclusion: Morae helps BLV users complete more tasks and select options that better match their preferences, demonstrating a mixed-initiative approach where users benefit from automation while expressing their preferences.

Abstract: User interface (UI) agents promise to make inaccessible or complex UIs easier
to access for blind and low-vision (BLV) users. However, current UI agents
typically perform tasks end-to-end without involving users in critical choices
or making them aware of important contextual information, thus reducing user
agency. For example, in our field study, a BLV participant asked to buy the
cheapest available sparkling water, and the agent automatically chose one from
several equally priced options, without mentioning alternative products with
different flavors or better ratings. To address this problem, we introduce
Morae, a UI agent that automatically identifies decision points during task
execution and pauses so that users can make choices. Morae uses large
multimodal models to interpret user queries alongside UI code and screenshots,
and prompt users for clarification when there is a choice to be made. In a
study over real-world web tasks with BLV participants, Morae helped users
complete more tasks and select options that better matched their preferences,
as compared to baseline agents, including OpenAI Operator. More broadly, this
work exemplifies a mixed-initiative approach in which users benefit from the
automation of UI agents while being able to express their preferences.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [40] [Quantum-Enhanced Natural Language Generation: A Multi-Model Framework with Hybrid Quantum-Classical Architectures](https://arxiv.org/abs/2508.21332)
*Chi-Sheng Chen,En-Jui Kuo*

Main category: quant-ph

TL;DR: 本文评估了量子文本生成模型与传统Transformer/MLP架构的性能，发现传统模型在整体上更优，但量子启发模型在特定场景下表现良好。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估量子文本生成模型与传统Transformer/MLP架构的性能，以应对自然语言处理中对量子计算应用日益增长的兴趣。

Method: 本文对五种不同的模型进行了系统实验比较：Transformer（基线）、量子核自注意力网络（QKSAN）、量子RWKV（QRWKV）和量子注意序列架构（QASA），并在五个不同的数据集上评估了它们的性能。

Result: 实验结果表明，传统Transformer模型在平均困惑度（1.21）和BLEU-1分数（0.2895）方面表现最佳，而量子启发模型在特定场景下表现出色。QKSAN实现了具有竞争力的BLEU-1分数（0.2800）并保持零重复率，QRWKV在某些任务中展示了完美的词汇多样性（Distinct-1 = 1.000）。

Conclusion: 虽然传统Transformer模型在整体上保持优势，但量子启发模型在特定场景下表现出色。QKSAN在保持零重复率的同时实现了具有竞争力的BLEU-1分数，QRWKV在某些任务中展示了完美的词汇多样性。

Abstract: This paper presents a comprehensive evaluation of quantum text generation
models against traditional Transformer/MLP architectures, addressing the
growing interest in quantum computing applications for natural language
processing. We conduct systematic experiments comparing five distinct models:
Transformer (baseline), Quantum Kernel Self-Attention Network (QKSAN), Quantum
RWKV (QRWKV), and Quantum Attention Sequence Architecture (QASA) across five
diverse datasets including simple sentences, short stories, quantum phrases,
haiku poetry, and proverbs. Our evaluation employs multiple metrics including
perplexity, BLEU scores, vocabulary diversity, repetition rates, and fluency
measures to assess different aspects of text generation quality. The
experimental results reveal that while traditional Transformer models maintain
overall superiority with the lowest average perplexity (1.21) and highest
BLEU-1 score (0.2895), quantum-inspired models demonstrate competitive
performance in specific scenarios. Notably, QKSAN achieves a competitive BLEU-1
score of 0.2800 while maintaining zero repetition rates, and QRWKV demonstrates
perfect vocabulary diversity (Distinct-1 = 1.000) in certain tasks.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [41] [From Canonical to Complex: Benchmarking LLM Capabilities in Undergraduate Thermodynamics](https://arxiv.org/abs/2508.21452)
*Anna Geißler,Luca-Sophie Bien,Friedrich Schöppler,Tobias Hertel*

Main category: physics.ed-ph

TL;DR: 研究评估了大型语言模型在热力学领域的教学能力，发现它们尚未准备好用于无监督辅导。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在科学教育中的教学能力，特别是在需要一致且基于原理的推理的情况下。

Method: 提出了UTQA，一个包含50个问题的本科生热力学问答基准测试，涵盖了理想气体过程、可逆性和图表解释。

Result: 没有领先的2025年模型超过95%的能力阈值，最好的LLMs达到了82%的准确率，文本问题的表现优于图像推理任务。

Conclusion: 当前的大型语言模型（LLMs）还不适合在该领域进行无监督辅导。

Abstract: Large language models (LLMs) are increasingly considered as tutoring aids in
science education. Yet their readiness for unsupervised use in undergraduate
instruction remains uncertain, as reliable teaching requires more than fluent
recall: it demands consistent, principle-grounded reasoning. Thermodynamics,
with its compact laws and subtle distinctions between state and path functions,
reversibility, and entropy, provides an ideal testbed for evaluating such
capabilities. Here we present UTQA, a 50-item undergraduate thermodynamics
question answering benchmark, covering ideal-gas processes, reversibility, and
diagram interpretation. No leading 2025-era model exceeded our 95\% competence
threshold: the best LLMs achieved 82\% accuracy, with text-only items
performing better than image reasoning tasks, which often fell to chance
levels. Prompt phrasing and syntactic complexity showed modest to little
correlation with performance. The gap concentrates in finite-rate/irreversible
scenarios and in binding visual features to thermodynamic meaning, indicating
that current LLMs are not yet suitable for unsupervised tutoring in this
domain.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: 本文提出了一种混合字符串相似性、主题建模、层次聚类和基于规则的管道，用于交易对手方的聚类，并适应预期聚类数量未知的情况。在真实数据集上的测试显示，该方法比传统的规则-based 方法有显著的性能提升，同时保留了规则系统的可解释性。


<details>
  <summary>Details</summary>
Motivation: 在银行支付消息系统（如SWIFT）中，手动输入的标签通常包含物理或法律实体细节，缺乏句子结构，同时包含手动输入引入的所有变化和噪声。这导致调查人员或反欺诈专业人员在增强对支付流发起人和受益人实体的了解以及追踪资金和资产时存在工具上的空白。传统上，供应商尝试通过模糊匹配工具来填补这一空白。

Method: 提出了一种混合字符串相似性、主题建模、层次聚类和基于规则的管道，以促进交易对手方的聚类，并且适应预期聚类数量未知的情况。还设计了补充评估方法的指标，基于精确率和召回率这些众所周知的度量标准。

Result: 在真实标记数据集上的测试表明，该方法相对于基线规则-based（'关键词'）方法有显著的性能提升。该方法在规则系统的基础上增加了一个额外的聚类细化层次。

Conclusion: 该方法能够显著提高对交易对手方进行聚类的性能，并且保留了规则系统中的大部分可解释性，从而减少了手动审查的需求。当只需要调查人口的一小部分时，如制裁调查，该方法可以更好地控制遗漏实体变体的风险。

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [43] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: 本研究探讨了强化学习在大型语言模型中的应用，发现反直觉的结果仅在模型和任务已经表现出强大的模型-任务对齐时才会出现，而在更具挑战性的领域中，标准的强化学习方法仍然有效。


<details>
  <summary>Details</summary>
Motivation: 最近将强化学习（RL）应用于大型语言模型（LLMs）的进展已经取得了显著进展。特别是，报告了一系列令人印象深刻但常常违反直觉的现象，在LLMs中表现出传统RL设置中不常见的模式。然而，这些观察结果成立的精确条件以及关键的失败条件仍然不清楚。

Method: 通过系统且全面地检查一系列反直觉的主张，并通过不同模型架构和任务领域的严格实验验证，我们确定了一个区分强化学习观察的关键因素：预训练模型是否在评估任务上表现出强大的模型-任务对齐，这通过pass@k准确性来衡量。

Result: 我们的研究结果表明，虽然标准的强化学习训练在各种设置中一直保持稳健，但许多反直觉的结果仅在模型和任务已经表现出强大的模型-任务对齐时才会出现。相反，在更具有挑战性的领域中，这些技术未能推动显著的学习，而标准的强化学习方法仍然有效。

Conclusion: 我们的研究发现，尽管标准的强化学习训练在各种设置中一直保持稳健，但许多反直觉的结果仅在模型和任务已经表现出强大的模型-任务对齐时才会出现。相反，在更具有挑战性的领域中，这些技术未能推动显著的学习，而标准的强化学习方法仍然有效。

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [44] [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
*Israel Abebe Azime,Deborah D. Kanubala,Tejumade Afonja,Mario Fritz,Isabel Valera,Dietrich Klakow,Philipp Slusallek*

Main category: cs.LG

TL;DR: 本研究评估了LLMs在不同地区的贷款批准数据集上的性能和公平性，发现序列化格式对模型表现和公平性有显著影响，并指出需要更有效的表格数据表示方法和公平意识模型以提高LLMs在金融决策中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在高风险决策任务中得到越来越多的应用，如贷款审批，但它们在处理表格数据、确保公平性和提供可靠预测方面仍然面临挑战。

Method: 我们评估了LLMs在三个地理上不同的地区（加纳、德国和美国）的序列化贷款批准数据集上的性能和公平性。我们的评估重点是模型的零样本和上下文学习（ICL）能力。

Result: 我们的结果表明，序列化格式的选择显著影响LLMs的性能和公平性，某些格式如GReat和LIFT产生了更高的F1分数，但加剧了公平性差异。值得注意的是，ICL将模型性能提高了4.9-59.6%相对于零样本基线，但其对公平性的影响在不同数据集中差异很大。

Conclusion: 我们的工作强调了有效的表格数据表示方法和公平意识模型在提高LLMs在金融决策中的可靠性的重要性。

Abstract: Large Language Models (LLMs) are increasingly employed in high-stakes
decision-making tasks, such as loan approvals. While their applications expand
across domains, LLMs struggle to process tabular data, ensuring fairness and
delivering reliable predictions. In this work, we assess the performance and
fairness of LLMs on serialized loan approval datasets from three geographically
distinct regions: Ghana, Germany, and the United States. Our evaluation focuses
on the model's zero-shot and in-context learning (ICL) capabilities. Our
results reveal that the choice of serialization (Serialization refers to the
process of converting tabular data into text formats suitable for processing by
LLMs.) format significantly affects both performance and fairness in LLMs, with
certain formats such as GReat and LIFT yielding higher F1 scores but
exacerbating fairness disparities. Notably, while ICL improved model
performance by 4.9-59.6% relative to zero-shot baselines, its effect on
fairness varied considerably across datasets. Our work underscores the
importance of effective tabular data representation methods and fairness-aware
models to improve the reliability of LLMs in financial decision-making.

</details>


### [45] [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
*Yifei Yuan,Jiatong Li,Weijia Zhang,Mohammad Aliannejadi,Evangelos Kanoulas,Renjun Hu*

Main category: cs.LG

TL;DR: InsightTab is a framework that improves the performance of large language models in few-shot tabular classification by distilling data into actionable insights.


<details>
  <summary>Details</summary>
Motivation: Recent studies show the promise of large language models (LLMs) for few-shot tabular classification but highlight challenges due to the variability in structured data.

Method: InsightTab is an insight distillation framework that integrates rule summarization, strategic exemplification, and insight reflection through deep collaboration between LLMs and data modeling techniques.

Result: The results demonstrate consistent improvement over state-of-the-art methods, and ablation studies validate the principle-guided distillation process.

Conclusion: InsightTab demonstrates consistent improvement over state-of-the-art methods and effectively leverages labeled data while managing bias.

Abstract: Recent studies show the promise of large language models (LLMs) for few-shot
tabular classification but highlight challenges due to the variability in
structured data. To address this, we propose distilling data into actionable
insights to enable robust and effective classification by LLMs. Drawing
inspiration from human learning processes, we introduce InsightTab, an insight
distillation framework guided by principles of divide-and-conquer, easy-first,
and reflective learning. Our approach integrates rule summarization, strategic
exemplification, and insight reflection through deep collaboration between LLMs
and data modeling techniques. The obtained insights enable LLMs to better align
their general knowledge and capabilities with the particular requirements of
specific tabular tasks. We extensively evaluate InsightTab on nine datasets.
The results demonstrate consistent improvement over state-of-the-art methods.
Ablation studies further validate the principle-guided distillation process,
while analyses emphasize InsightTab's effectiveness in leveraging labeled data
and managing bias.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
*Shashank Vempati,Nishit Anand,Gaurav Talebailkar,Arpan Garai,Chetan Arora*

Main category: cs.CV

TL;DR: 本文提出了一种从单词级OCR到行级OCR的方法，该方法通过直接输入整行文本到模型中，以序列形式输出完整单词。实验结果表明，该技术不仅提高了准确性，还提高了OCR的效率。


<details>
  <summary>Details</summary>
Motivation: 传统的OCR技术在字符分割过程中容易出错，并且缺乏上下文来利用语言模型。近年来，序列到序列翻译的进步使得现代技术首先检测单词，然后逐个输入到模型中以直接输出完整单词。这种方法更好地利用了语言模型，并跳过了容易出错的字符分割步骤。然而，这种风格的转变将准确性的瓶颈转移到了单词分割上。因此，本文提出了从单词级OCR到行级OCR的进展。

Method: 本文提出了一种从单词级OCR到行级OCR的方法，该方法通过直接输入整行文本到模型中，以序列形式输出完整单词。此外，还创建了一个精心整理的数据集，用于训练和基准测试这种从单词级到行级OCR的转变。

Result: 实验结果显示，所提出的技术不仅提高了准确性，还提高了OCR的效率。与基于单词的管道相比，效率提高了4倍。此外，还创建了一个精心整理的数据集，用于训练和基准测试这种从单词级到行级OCR的转变。

Conclusion: 本文提出了一种从单词级OCR到行级OCR的自然且逻辑上的进展，该方法可以避免单词检测中的错误，并提供更大的句子上下文以更好地利用语言模型。实验结果表明，该技术不仅提高了准确性，还提高了OCR的效率。

Abstract: Conventional optical character recognition (OCR) techniques segmented each
character and then recognized. This made them prone to error in character
segmentation, and devoid of context to exploit language models. Advances in
sequence to sequence translation in last decade led to modern techniques first
detecting words and then inputting one word at a time to a model to directly
output full words as sequence of characters. This allowed better utilization of
language models and bypass error-prone character segmentation step. We observe
that the above transition in style has moved the bottleneck in accuracy to word
segmentation. Hence, in this paper, we propose a natural and logical
progression from word level OCR to line-level OCR. The proposal allows to
bypass errors in word detection, and provides larger sentence context for
better utilization of language models. We show that the proposed technique not
only improves the accuracy but also efficiency of OCR. Despite our thorough
literature survey, we did not find any public dataset to train and benchmark
such shift from word to line-level OCR. Hence, we also contribute a
meticulously curated dataset of 251 English page images with line-level
annotations. Our experimentation revealed a notable end-to-end accuracy
improvement of 5.4%, underscoring the potential benefits of transitioning
towards line-level OCR, especially for document images. We also report a 4
times improvement in efficiency compared to word-based pipelines. With
continuous improvements in large language models, our methodology also holds
potential to exploit such advances. Project Website:
https://nishitanand.github.io/line-level-ocr-website

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 本文研究了架构归纳偏差对大型语言模型在指令对话中认知行为的影响，引入了符号支撑机制和短期记忆模式，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 我们研究了架构归纳偏差如何影响大型语言模型（LLMs）在指令对话中的认知行为。

Method: 我们引入了一种符号支撑机制和短期记忆模式，以促进苏格拉底辅导中的适应性、结构化推理。通过五种系统变体的控制消融，我们使用专家设计的评分标准评估模型输出，涵盖支撑、响应性、符号推理和对话记忆。

Result: 初步结果表明，我们的完整系统始终优于基线变体。分析显示，去除记忆或符号结构会降低关键认知行为，包括抽象、适应性探测和概念连续性。

Conclusion: 这些发现支持了一个处理层面的解释，即架构支架可以可靠地塑造大型语言模型中的出现教学策略。

Abstract: We study how architectural inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding mechanism paired with a short-term memory schema designed
to promote adaptive, structured reasoning in Socratic tutoring. Using
controlled ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which architectural scaffolds can
reliably shape emergent instructional strategies in LLMs.

</details>


### [48] [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
*Tony Lee,Haoqin Tu,Chi Heem Wong,Zijun Wang,Siwei Yang,Yifan Mai,Yuyin Zhou,Cihang Xie,Percy Liang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take
interleaved audio and text as input and output text -- are hindered by the lack
of standardized benchmarks; most benchmarks measure only one or two
capabilities and omit evaluative aspects such as fairness or safety.
Furthermore, comparison across models is difficult as separate evaluations test
a limited number of models and use different prompting methods and inference
parameters. To address these shortfalls, we introduce AHELM, a benchmark that
aggregates various datasets -- including 2 new synthetic audio-text datasets
called PARADE, which evaluates the ALMs on avoiding stereotypes, and
CoRe-Bench, which measures reasoning over conversational audio through
inferential multi-turn question answering -- to holistically measure the
performance of ALMs across 10 aspects we have identified as important to the
development and usage of ALMs: audio perception, knowledge, reasoning, emotion
detection, bias, fairness, multilinguality, robustness, toxicity, and safety.
We also standardize the prompts, inference parameters, and evaluation metrics
to ensure equitable comparisons across models. We test 14 open-weight and
closed-API ALMs from 3 developers and 3 additional simple baseline systems each
consisting of an automatic speech recognizer and a language model. Our results
show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits
group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do
not. We also find that the baseline systems perform reasonably well on AHELM,
with one ranking 5th overall despite having only speech-to-text capabilities.
For transparency, all raw prompts, model generations, and outputs are available
on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is
intended to be a living benchmark and new datasets and models will be added
over time.

</details>

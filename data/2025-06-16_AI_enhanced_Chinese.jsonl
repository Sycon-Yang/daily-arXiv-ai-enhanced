{"id": "2506.11017", "pdf": "https://arxiv.org/pdf/2506.11017", "abs": "https://arxiv.org/abs/2506.11017", "authors": ["Yanyan Wang", "Yingying Wang", "Junli Liang", "Yin Xu", "Yunlong Liu", "Yiming Xu", "Zhengwang Jiang", "Zhehe Li", "Fei Li", "Long Zhao", "Kuang Xu", "Qi Song", "Xiangyang Li"], "title": "TeleEval-OS: Performance evaluations of large language models for operations scheduling", "categories": ["cs.CL", "cs.AI", "cs.PF"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in artificial intelligence, demonstrating substantial\napplication potential across multiple specialized domains. Telecommunications\noperation scheduling (OS) is a critical aspect of the telecommunications\nindustry, involving the coordinated management of networks, services, risks,\nand human resources to optimize production scheduling and ensure unified\nservice control. However, the inherent complexity and domain-specific nature of\nOS tasks, coupled with the absence of comprehensive evaluation benchmarks, have\nhindered thorough exploration of LLMs' application potential in this critical\nfield. To address this research gap, we propose the first Telecommunications\nOperation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this\nbenchmark comprises 15 datasets across 13 subtasks, comprehensively simulating\nfour key operational stages: intelligent ticket creation, intelligent ticket\nhandling, intelligent ticket closure, and intelligent evaluation. To\nsystematically assess the performance of LLMs on tasks of varying complexity,\nwe categorize their capabilities in telecommunications operation scheduling\ninto four hierarchical levels, arranged in ascending order of difficulty: basic\nNLP, knowledge Q&A, report generation, and report analysis. On TeleEval-OS, we\nleverage zero-shot and few-shot evaluation methods to comprehensively assess 10\nopen-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o)\nacross diverse scenarios. Experimental results demonstrate that open-source\nLLMs can outperform closed-source LLMs in specific scenarios, highlighting\ntheir significant potential and value in the field of telecommunications\noperation scheduling.", "AI": {"tldr": "This paper introduces TeleEval-OS, the first benchmark for evaluating large language models in telecommunications operation scheduling, categorizing their capabilities into four levels and assessing 14 LLMs across 15 datasets in 13 subtasks.", "motivation": "To explore the application potential of large language models in telecommunications operation scheduling due to the lack of comprehensive evaluation benchmarks.", "method": "Proposing TeleEval-OS with 15 datasets across 13 subtasks simulating four key operational stages, categorizing LLMs' capabilities into four hierarchical levels, and conducting zero-shot and few-shot evaluations on 14 LLMs.", "result": "Open-source LLMs outperformed closed-source LLMs in certain scenarios, showing significant potential in telecommunications operation scheduling.", "conclusion": "TeleEval-OS provides a valuable benchmark for assessing LLMs in telecommunications operation scheduling, indicating the potential of open-source LLMs in this field."}}
{"id": "2506.11063", "pdf": "https://arxiv.org/pdf/2506.11063", "abs": "https://arxiv.org/abs/2506.11063", "authors": ["Jiayu Yao", "Shenghua Liu", "Yiwei Wang", "Lingrui Mei", "Baolong Bi", "Yuyao Ge", "Zhecheng Li", "Xueqi Cheng"], "title": "Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (RAG) systems have become essential\nin knowledge-intensive and open-domain tasks. As retrieval complexity\nincreases, ensuring the robustness of these systems is critical. However,\ncurrent RAG models are highly sensitive to the order in which evidence is\npresented, often resulting in unstable performance and biased reasoning,\nparticularly as the number of retrieved items or modality diversity grows. This\nraises a central question: How does the position of retrieved evidence affect\nmultimodal RAG performance? To answer this, we present the first comprehensive\nstudy of position bias in multimodal RAG systems. Through controlled\nexperiments across text-only, image-only, and mixed-modality tasks, we observe\na consistent U-shaped accuracy curve with respect to evidence position. To\nquantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and\ndevelop a visualization framework to trace attention allocation patterns across\ndecoder layers. Our results reveal that multimodal interactions intensify\nposition bias compared to unimodal settings, and that this bias increases\nlogarithmically with retrieval range. These findings offer both theoretical and\nempirical foundations for position-aware analysis in RAG, highlighting the need\nfor evidence reordering or debiasing strategies to build more reliable and\nequitable generation systems.", "AI": {"tldr": "This study investigates how the position of retrieved evidence affects the performance of multimodal Retrieval-Augmented Generation (RAG) systems.", "motivation": "Current RAG models are sensitive to evidence order, leading to unstable performance and biased reasoning, especially when dealing with multiple modalities.", "method": "Conducted controlled experiments on text-only, image-only, and mixed-modality tasks to study position bias in RAG systems.", "result": "Found a U-shaped accuracy curve regarding evidence position and introduced the Position Sensitivity Index ($PSI_p$) to quantify this bias.", "conclusion": "Multimodal interactions increase position bias, which grows logarithmically with retrieval range, suggesting the need for evidence reordering or debiasing strategies."}}
{"id": "2506.11065", "pdf": "https://arxiv.org/pdf/2506.11065", "abs": "https://arxiv.org/abs/2506.11065", "authors": ["Alexey Tikhonov", "Sergei Shteiner", "Anna Bykova", "Ivan P. Yamshchikov"], "title": "Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study", "categories": ["cs.CL", "Primary 68T50, Secondary 68T05, 91F20", "I.2.7; I.2.6; I.5.4"], "comment": "ACL Findings 2025", "summary": "Russenorsk, a pidgin language historically used in trade interactions between\nRussian and Norwegian speakers, represents a unique linguistic phenomenon. In\nthis paper, we attempt to analyze its lexicon using modern large language\nmodels (LLMs), based on surviving literary sources. We construct a structured\ndictionary of the language, grouped by synonyms and word origins. Subsequently,\nwe use this dictionary to formulate hypotheses about the core principles of\nword formation and grammatical structure in Russenorsk and show which\nhypotheses generated by large language models correspond to the hypotheses\npreviously proposed ones in the academic literature. We also develop a\n\"reconstruction\" translation agent that generates hypothetical Russenorsk\nrenderings of contemporary Russian and Norwegian texts.", "AI": {"tldr": "Using modern language models, this paper analyzes Russenorsk's lexicon, constructs a dictionary, formulates hypotheses about its structure, and develops a translation tool to explore this historical pidgin language.", "motivation": "To analyze the lexicon of Russenorsk using modern large language models (LLMs) for insights into its word formation and grammatical structure.", "method": "Constructing a structured dictionary of Russenorsk, grouped by synonyms and word origins, and using it to formulate hypotheses about the language's principles. Also developing a 'reconstruction' translation agent for translating contemporary texts into Russenorsk.", "result": "A structured dictionary of Russenorsk has been created and used to formulate hypotheses about its word formation and grammatical structure. Some hypotheses align with previous academic literature, while a 'reconstruction' translation agent has been developed.", "conclusion": "The study provides new insights into the structure of Russenorsk using LLMs, contributing to our understanding of this historical pidgin language."}}
{"id": "2506.11067", "pdf": "https://arxiv.org/pdf/2506.11067", "abs": "https://arxiv.org/abs/2506.11067", "authors": ["Hieu Nghiem", "Hemanth Reddy Singareddy", "Zhuqi Miao", "Jivan Lamichhane", "Abdulaziz Ahmed", "Johnson Thomas", "Dursun Delen", "William Paiva"], "title": "A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes", "categories": ["cs.CL"], "comment": null, "summary": "Objective: Develop a cost-effective, large language model (LLM)-based\npipeline for automatically extracting Review of Systems (ROS) entities from\nclinical notes. Materials and Methods: The pipeline extracts ROS sections using\nSecTag, followed by few-shot LLMs to identify ROS entity spans, their\npositive/negative status, and associated body systems. We implemented the\npipeline using open-source LLMs (Mistral, Llama, Gemma) and ChatGPT. The\nevaluation was conducted on 36 general medicine notes containing 341 annotated\nROS entities. Results: When integrating ChatGPT, the pipeline achieved the\nlowest error rates in detecting ROS entity spans and their corresponding\nstatuses/systems (28.2% and 14.5%, respectively). Open-source LLMs enable\nlocal, cost-efficient execution of the pipeline while delivering promising\nperformance with similarly low error rates (span: 30.5-36.7%; status/system:\n24.3-27.3%). Discussion and Conclusion: Our pipeline offers a scalable and\nlocally deployable solution to reduce ROS documentation burden. Open-source\nLLMs present a viable alternative to commercial models in resource-limited\nhealthcare environments.", "AI": {"tldr": "This study develops a cost-effective pipeline using open-source LLMs to automatically extract ROS entities from clinical notes, reducing documentation burden and offering a scalable solution.", "motivation": "Develop a cost-effective, large language model (LLM)-based pipeline for automatically extracting Review of Systems (ROS) entities from clinical notes.", "method": "The pipeline extracts ROS sections using SecTag, followed by few-shot LLMs to identify ROS entity spans, their positive/negative status, and associated body systems. Open-source LLMs (Mistral, Llama, Gemma) and ChatGPT were used.", "result": "When integrating ChatGPT, the pipeline achieved the lowest error rates in detecting ROS entity spans and their corresponding statuses/systems (28.2% and 14.5%, respectively). Open-source LLMs enable local, cost-efficient execution of the pipeline while delivering promising performance with similarly low error rates (span: 30.5-36.7%; status/system: 24.3-27.3%).", "conclusion": "Our pipeline offers a scalable and locally deployable solution to reduce ROS documentation burden. Open-source LLMs present a viable alternative to commercial models in resource-limited healthcare environments."}}
{"id": "2506.11068", "pdf": "https://arxiv.org/pdf/2506.11068", "abs": "https://arxiv.org/abs/2506.11068", "authors": ["Bumjin Park", "Jinsil Lee", "Jaesik Choi"], "title": "Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models", "categories": ["cs.CL"], "comment": "20 pages including references and appendix; To appear in ACL 2025\n  main conference", "summary": "Large language models (LLMs) are increasingly engaging in moral and ethical\nreasoning, where criteria for judgment are often unclear, even for humans.\nWhile LLM alignment studies cover many areas, one important yet underexplored\narea is how LLMs make judgments about obligations. This work reveals a strong\ntendency in LLMs to judge non-obligatory contexts as obligations when prompts\nare augmented with modal expressions such as must or ought to. We introduce\nthis phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge\nover 90\\% of commonsense scenarios as obligations when modal expressions are\npresent. This tendency is consist across various LLM families, question types,\nand answer formats. To mitigate DKB, we propose a judgment strategy that\nintegrates few-shot examples with reasoning prompts. This study sheds light on\nhow modal expressions, as a form of linguistic framing, influence the normative\ndecisions of LLMs and underscores the importance of addressing such biases to\nensure judgment alignment.", "AI": {"tldr": "This paper reveals a bias in large language models towards judging non-obligatory contexts as obligations when prompted with modal expressions, termed Deontological Keyword Bias (DKB). It proposes a method to mitigate this bias.", "motivation": "To explore how large language models make judgments about obligations, an underexplored area in LLM alignment studies.", "method": "Introduces the concept of Deontological Keyword Bias (DKB) and proposes a judgment strategy integrating few-shot examples with reasoning prompts to mitigate it.", "result": "Found that LLMs judge over 90% of commonsense scenarios as obligations when modal expressions are present, and this tendency is consistent across different LLM families, question types, and answer formats.", "conclusion": "Highlights the influence of modal expressions on the normative decisions of LLMs and emphasizes the need to address such biases for judgment alignment."}}
{"id": "2506.11070", "pdf": "https://arxiv.org/pdf/2506.11070", "abs": "https://arxiv.org/abs/2506.11070", "authors": ["Yu-Zhe Shi", "Mingchen Liu", "Hanlu Ma", "Qiao Xu", "Huamin Qu", "Kun He", "Lecheng Ruan", "Qining Wang"], "title": "Targeted control of fast prototyping through domain-specific interface", "categories": ["cs.CL"], "comment": "In International Conference on Machine Learning (ICML'25)", "summary": "Industrial designers have long sought a natural and intuitive way to achieve\nthe targeted control of prototype models -- using simple natural language\ninstructions to configure and adjust the models seamlessly according to their\nintentions, without relying on complex modeling commands. While Large Language\nModels have shown promise in this area, their potential for controlling\nprototype models through language remains partially underutilized. This\nlimitation stems from gaps between designers' languages and modeling languages,\nincluding mismatch in abstraction levels, fluctuation in semantic precision,\nand divergence in lexical scopes. To bridge these gaps, we propose an interface\narchitecture that serves as a medium between the two languages. Grounded in\ndesign principles derived from a systematic investigation of fast prototyping\npractices, we devise the interface's operational mechanism and develop an\nalgorithm for its automated domain specification. Both machine-based\nevaluations and human studies on fast prototyping across various product design\ndomains demonstrate the interface's potential to function as an auxiliary\nmodule for Large Language Models, enabling precise and effective targeted\ncontrol of prototype models.", "AI": {"tldr": "This paper introduces an interface architecture to bridge the gap between designers' natural language and modeling languages for more precise control of prototype models.", "motivation": "To enable industrial designers to use simple natural language instructions for seamless configuration and adjustment of prototype models without relying on complex modeling commands.", "method": "Proposes an interface architecture grounded in design principles from fast prototyping practices, with an algorithm for automated domain specification.", "result": "Demonstrates the interface's potential as an auxiliary module for Large Language Models, allowing precise and effective targeted control of prototype models.", "conclusion": "The proposed interface architecture can enhance the interaction between industrial designers and prototype models through natural language."}}
{"id": "2506.11073", "pdf": "https://arxiv.org/pdf/2506.11073", "abs": "https://arxiv.org/abs/2506.11073", "authors": ["Zekai Ye", "Qiming Li", "Xiaocheng Feng", "Libo Qin", "Yichong Huang", "Baohang Li", "Kui Jiang", "Yang Xiang", "Zhirui Zhang", "Yunfei Lu", "Duyu Tang", "Dandan Tu", "Bing Qin"], "title": "CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL2025 Main", "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nabilities but remain prone to multilingual object hallucination, with a higher\nlikelihood of generating responses inconsistent with the visual input when\nutilizing queries in non-English languages compared to English. Most existing\napproaches to address these rely on pretraining or fine-tuning, which are\nresource-intensive. In this paper, inspired by observing the disparities in\ncross-modal attention patterns across languages, we propose Cross-Lingual\nAttention Intervention for Mitigating multilingual object hallucination (CLAIM)\nin LVLMs, a novel near training-free method by aligning attention patterns.\nCLAIM first identifies language-specific cross-modal attention heads, then\nestimates language shift vectors from English to the target language, and\nfinally intervenes in the attention outputs during inference to facilitate\ncross-lingual visual perception capability alignment. Extensive experiments\ndemonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in\nSpanish) on the POPE and 21.75% on the hallucination subsets of the MME\nbenchmark across various languages. Further analysis reveals that multilingual\nattention divergence is most prominent in intermediate layers, highlighting\ntheir critical role in multilingual scenarios.", "AI": {"tldr": "Large Vision-Language Models often misinterpret non-English inputs, causing hallucination issues. This paper proposes CLAIM, a novel method that improves cross-lingual visual perception without heavy training by aligning attention patterns.", "motivation": "Addressing the problem of multilingual object hallucination in large vision-language models.", "method": "CLAIM identifies language-specific cross-modal attention heads, estimates language shift vectors, and intervenes in attention outputs during inference.", "result": "CLAIM improves performance by 13.56% on average (up to 30% in Spanish) on the POPE and 21.75% on the hallucination subsets of the MME benchmark.", "conclusion": "CLAIM is an effective, near training-free method to mitigate multilingual object hallucination in LVLMs by aligning cross-lingual attention patterns."}}
{"id": "2506.11077", "pdf": "https://arxiv.org/pdf/2506.11077", "abs": "https://arxiv.org/abs/2506.11077", "authors": ["Chongyu Fan", "Yihua Zhang", "Jinghan Jia", "Alfred Hero", "Sijia Liu"], "title": "CyclicReflex: Improving Large Reasoning Models via Cyclical Reflection Token Scheduling", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, harness\ntest-time scaling to perform multi-step reasoning for complex problem-solving.\nThis reasoning process, executed before producing final answers, is often\nguided by special juncture tokens or textual segments that prompt\nself-evaluative reflection. We refer to these transition markers and reflective\ncues as \"reflection tokens\" (e.g., \"wait\", \"but\", \"alternatively\"). In this\nwork, we treat reflection tokens as a \"resource\" and introduce the problem of\nresource allocation, aimed at improving the test-time compute performance of\nLRMs by adaptively regulating the frequency and placement of reflection tokens.\nThrough empirical analysis, we show that both excessive and insufficient use of\nreflection tokens, referred to as over-reflection and under-reflection, can\ndegrade model performance. To better understand and manage this trade-off, we\ndraw an analogy between reflection token usage and learning rate scheduling in\noptimization. Building on this insight, we propose cyclical reflection token\nscheduling (termed CyclicReflex), a decoding strategy that dynamically\nmodulates reflection token logits using a position-dependent triangular\nwaveform. Experiments on MATH500, AIME2024/2025, and AMC2023 demonstrate that\nCyclicReflex consistently improves performance across model sizes (1.5B-8B),\noutperforming standard decoding and more recent approaches such as TIP (thought\nswitching penalty) and S1. Codes are available at\nhttps://github.com/OPTML-Group/CyclicReflex.", "AI": {"tldr": "This paper introduces the concept of reflection tokens as a resource in large reasoning models (LRMs) and proposes a novel decoding strategy called CyclicReflex, which improves test-time compute performance by dynamically modulating reflection token logits.", "motivation": "To improve the test-time compute performance of LRMs by adaptively regulating the frequency and placement of reflection tokens.", "method": "Introduce the problem of resource allocation for reflection tokens, draw an analogy with learning rate scheduling, and propose CyclicReflex, a decoding strategy that dynamically modulates reflection token logits using a position-dependent triangular waveform.", "result": "Experiments on MATH500, AIME2024/2025, and AMC2023 demonstrate that CyclicReflex consistently improves performance across model sizes (1.5B-8B), outperforming standard decoding and other approaches.", "conclusion": "The proposed CyclicReflex decoding strategy effectively manages the trade-off between over-reflection and under-reflection, leading to improved performance in complex problem-solving tasks."}}
{"id": "2506.11078", "pdf": "https://arxiv.org/pdf/2506.11078", "abs": "https://arxiv.org/abs/2506.11078", "authors": ["Yuzhou Yang", "Yangming Zhou", "Zhiying Zhu", "Zhenxing Qian", "Xinpeng Zhang", "Sheng Li"], "title": "RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of deceptive content online necessitates robust Fake News\nDetection (FND) systems. While evidence-based approaches leverage external\nknowledge to verify claims, existing methods face critical limitations: noisy\nevidence selection, generalization bottlenecks, and unclear decision-making\nprocesses. Recent efforts to harness Large Language Models (LLMs) for FND\nintroduce new challenges, including hallucinated rationales and conclusion\nbias. To address these issues, we propose \\textbf{RoE-FND}\n(\\textbf{\\underline{R}}eason \\textbf{\\underline{o}}n\n\\textbf{\\underline{E}}xperiences FND), a framework that reframes evidence-based\nFND as a logical deduction task by synergizing LLMs with experiential learning.\nRoE-FND encompasses two stages: (1) \\textit{self-reflective knowledge\nbuilding}, where a knowledge base is curated by analyzing past reasoning\nerrors, namely the exploration stage, and (2) \\textit{dynamic criterion\nretrieval}, which synthesizes task-specific reasoning guidelines from\nhistorical cases as experiences during deployment. It further cross-checks\nrationales against internal experience through a devised dual-channel\nprocedure. Key contributions include: a case-based reasoning framework for FND\nthat addresses multiple existing challenges, a training-free approach enabling\nadaptation to evolving situations, and empirical validation of the framework's\nsuperior generalization and effectiveness over state-of-the-art methods across\nthree datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u6846\u67b6RoE-FND\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7ecf\u9a8c\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u591a\u4e2a\u95ee\u9898\uff0c\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u7ebf\u6b3a\u9a97\u6027\u5185\u5bb9\u7684\u6fc0\u589e\u9700\u8981\u5f3a\u5927\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u8bc1\u636e\u7684\u65b9\u6cd5\u9762\u4e34\u566a\u97f3\u8bc1\u636e\u9009\u62e9\u3001\u6cdb\u5316\u74f6\u9888\u548c\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u660e\u786e\u7b49\u5173\u952e\u9650\u5236\u3002\u6700\u8fd1\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u52aa\u529b\u5f15\u5165\u4e86\u65b0\u7684\u6311\u6218\uff0c\u5982\u5e7b\u89c9\u7406\u7531\u548c\u7ed3\u8bba\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoE-FND\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u534f\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7ecf\u9a8c\u5b66\u4e60\uff0c\u5c06\u57fa\u4e8e\u8bc1\u636e\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u81ea\u6211\u53cd\u601d\u7684\u77e5\u8bc6\u6784\u5efa\u548c\u52a8\u6001\u6807\u51c6\u68c0\u7d22\uff0c\u5e76\u901a\u8fc7\u53cc\u901a\u9053\u7a0b\u5e8f\u4ea4\u53c9\u68c0\u67e5\u7406\u7531\u3002", "result": "RoE-FND\u6846\u67b6\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u66f4\u597d\u7684\u6cdb\u5316\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.11080", "pdf": "https://arxiv.org/pdf/2506.11080", "abs": "https://arxiv.org/abs/2506.11080", "authors": ["Han Zhou", "Qitong Xu", "Yiheng Dong", "Xin Yang"], "title": "MANBench: Is Your Multimodal Model Smarter than Human?", "categories": ["cs.CL"], "comment": "Multimodal Benchmark, Project Url: https://github.com/micdz/MANBench,\n  ACL2025 Findings", "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited\ndiscussions regarding their potential to surpass human performance in\nmultimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms\nBenchmark), a bilingual benchmark (English and Chinese) comprising 1,314\nquestions across nine tasks, spanning knowledge-based and non-knowledge-based\ndomains. MANBench emphasizes intuitive reasoning, seamless cross-modal\nintegration, and real-world complexity, providing a rigorous evaluation\nframework.\n  Through extensive human experiments involving diverse participants, we\ncompared human performance against state-of-the-art MLLMs. The results indicate\nthat while MLLMs excel in tasks like Knowledge and Text-Image Understanding,\nthey struggle with deeper cross-modal reasoning tasks such as Transmorphic\nUnderstanding, Image Consistency, and Multi-image Understanding. Moreover, both\nhumans and MLLMs face challenges in highly complex tasks like Puzzles and\nSpatial Imagination.\n  MANBench highlights the strengths and limitations of MLLMs, revealing that\neven advanced models fall short of achieving human-level performance across\nmany domains. We hope MANBench will inspire efforts to bridge the gap between\nMLLMs and human multimodal capabilities. The code and dataset are available at\nhttps://github.com/micdz/MANBench.", "AI": {"tldr": "A new benchmark named MANBench is introduced to evaluate the performance of Multimodal Large Language Models (MLLMs). It consists of 1,314 questions in two languages across nine tasks. Human experiments show MLLMs perform well in some tasks but still lag behind humans in others.", "motivation": "To assess whether MLLMs can surpass human performance in multimodal tasks.", "method": "Creating a bilingual benchmark called MANBench with 1,314 questions in nine tasks.", "result": "MLLMs outperform in knowledge-based and text-image understanding tasks but struggle with deeper cross-modal reasoning tasks. Both humans and MLLMs find puzzles and spatial imagination challenging.", "conclusion": "MANBench reveals the current limitations of MLLMs and encourages further research to close the gap between MLLMs and human multimodal abilities."}}
{"id": "2506.11081", "pdf": "https://arxiv.org/pdf/2506.11081", "abs": "https://arxiv.org/abs/2506.11081", "authors": ["Aditi", "Hyunwoo Park", "Sicheol Sung", "Yo-Sub Han", "Sang-Ki Ko"], "title": "SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Grammar-based test case generation has proven effective for competitive\nprogramming problems, but generating valid and general grammars from natural\nlanguage specifications remains a key challenge, especially under limited\nsupervision. Context-Free Grammars with Counters (CCFGs) have recently been\nintroduced as a formalism to represent such specifications with logical\nconstraints by storing and reusing counter values during derivation. In this\nwork, we explore the use of open-source large language models (LLMs) to induce\nCCFGs from specifications using a small number of labeled examples and\nverifiable reward-guided reinforcement learning. Our approach first fine-tunes\nan open-source LLM to perform specification-to-grammar translation, and further\napplies Group Relative Policy Optimization (GRPO) to enhance grammar validity\nand generality. We also examine the effectiveness of iterative feedback for\nopen and closed-source LLMs in correcting syntactic and semantic errors in\ngenerated grammars.\n  Experimental results show that our approach SAGE achieves stronger\ngeneralization and outperforms 17 open and closed-source LLMs in both grammar\nquality and test effectiveness, improving over the state-of-the-art by 15.92%p\nin grammar validity and 12.34%p in test effectiveness. We provide our\nimplementation and dataset at the following anonymous\nrepository:https://anonymous.4open.science/r/SAGE-5714", "AI": {"tldr": "Grammar-based test case generation is effective for competitive programming problems, but creating valid and general grammars from natural language specs is challenging. This work uses open-source large language models to induce Context-Free Grammars with Counters from specs with limited supervision, achieving better grammar quality and test effectiveness than previous methods.", "motivation": "The difficulty of deriving valid and general grammars from natural language specifications with limited supervision.", "method": "Fine-tuning an open-source large language model for specification-to-grammar translation and applying Group Relative Policy Optimization to improve grammar validity and generality.", "result": "SAGE outperforms 17 open and closed-source LLMs in grammar quality and test effectiveness, improving over the state-of-the-art by 15.92% in grammar validity and 12.34% in test effectiveness.", "conclusion": "Our approach, SAGE, demonstrates strong generalization capabilities and improves upon the current best methods for grammar-based test case generation."}}
{"id": "2506.11082", "pdf": "https://arxiv.org/pdf/2506.11082", "abs": "https://arxiv.org/abs/2506.11082", "authors": ["Lionel Levine", "John Santerre", "Alex S. Young", "T. Barry Levine", "Francis Campion", "Majid Sarrafzadeh"], "title": "PRISM: A Transformer-based Language Model of Structured Clinical Event Data", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 Figures, 1 Table", "summary": "We introduce PRISM (Predictive Reasoning in Sequential Medicine), a\ntransformer-based architecture designed to model the sequential progression of\nclinical decision-making processes. Unlike traditional approaches that rely on\nisolated diagnostic classification, PRISM frames clinical trajectories as\ntokenized sequences of events - including diagnostic tests, laboratory results,\nand diagnoses - and learns to predict the most probable next steps in the\npatient diagnostic journey. Leveraging a large custom clinical vocabulary and\nan autoregressive training objective, PRISM demonstrates the ability to capture\ncomplex dependencies across longitudinal patient timelines. Experimental\nresults show substantial improvements over random baselines in next-token\nprediction tasks, with generated sequences reflecting realistic diagnostic\npathways, laboratory result progressions, and clinician ordering behaviors.\nThese findings highlight the feasibility of applying generative language\nmodeling techniques to structured medical event data, enabling applications in\nclinical decision support, simulation, and education. PRISM establishes a\nfoundation for future advancements in sequence-based healthcare modeling,\nbridging the gap between machine learning architectures and real-world\ndiagnostic reasoning.", "AI": {"tldr": "PRISM is a transformer-based architecture that models clinical decision-making processes as tokenized sequences of events, demonstrating strong performance in predicting next steps in patient diagnostic journeys.", "motivation": "To apply generative language modeling techniques to structured medical event data for clinical decision support, simulation, and education.", "method": "Transformers; tokenized sequences of events including diagnostic tests, lab results, and diagnoses; autoregressive training objective.", "result": "Substantial improvements over random baselines in next-token prediction tasks; generated sequences reflect realistic diagnostic pathways, lab result progressions, and clinician ordering behaviors.", "conclusion": "PRISM establishes a foundation for future advancements in sequence-based healthcare modeling by bridging the gap between machine learning architectures and real-world diagnostic reasoning."}}
{"id": "2506.11083", "pdf": "https://arxiv.org/pdf/2506.11083", "abs": "https://arxiv.org/abs/2506.11083", "authors": ["Ali Asad", "Stephen Obadinma", "Radin Shayanfar", "Xiaodan Zhu"], "title": "RedDebate: Safer Responses through Multi-Agent Red Teaming Debates", "categories": ["cs.CL"], "comment": null, "summary": "We propose RedDebate, a novel multi-agent debate framework that leverages\nadversarial argumentation among Large Language Models (LLMs) to proactively\nidentify and mitigate their own unsafe behaviours. Existing AI safety methods\noften depend heavily on costly human evaluations or isolated single-model\nassessment, both subject to scalability constraints and oversight risks.\nRedDebate instead embraces collaborative disagreement, enabling multiple LLMs\nto critically examine one another's reasoning, and systematically uncovering\nunsafe blind spots through automated red-teaming, and iteratively improve their\nresponses. We further integrate distinct types of long-term memory that retain\nlearned safety insights from debate interactions. Evaluating on established\nsafety benchmarks such as HarmBench, we demonstrate the proposed method's\neffectiveness. Debate alone can reduce unsafe behaviours by 17.7%, and when\ncombined with long-term memory modules, achieves reductions exceeding 23.5%. To\nour knowledge, RedDebate constitutes the first fully automated framework that\ncombines multi-agent debates with red-teaming to progressively enhance AI\nsafety without direct human intervention.(Github Repository:\nhttps://github.com/aliasad059/RedDebate)", "AI": {"tldr": "RedDebate\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\uff0c\u5229\u7528LLMs\u4e4b\u95f4\u7684\u5bf9\u6297\u6027\u8bba\u8bc1\u6765\u4e3b\u52a8\u8bc6\u522b\u548c\u51cf\u8f7b\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u540c\u65f6\u7ed3\u5408\u7ea2\u961f\u64cd\u4f5c\u548c\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\uff0c\u6709\u6548\u63d0\u9ad8AI\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684AI\u5b89\u5168\u65b9\u6cd5\u901a\u5e38\u4e25\u91cd\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u4eba\u7c7b\u8bc4\u4f30\u6216\u5b64\u7acb\u7684\u5355\u6a21\u578b\u8bc4\u4f30\uff0c\u4e24\u8005\u90fd\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u76d1\u7763\u98ce\u9669\u7684\u7ea6\u675f\u3002", "method": "RedDebate\u91c7\u7528\u534f\u4f5c\u5206\u6b67\uff0c\u4f7f\u591a\u4e2aLLMs\u80fd\u591f\u6279\u5224\u6027\u5730\u68c0\u67e5\u5f7c\u6b64\u7684\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u7684\u7ea2\u961f\u64cd\u4f5c\u7cfb\u7edf\u5730\u53d1\u73b0\u4e0d\u5b89\u5168\u76f2\u70b9\u5e76\u8fed\u4ee3\u6539\u8fdb\u5b83\u4eec\u7684\u54cd\u5e94\u3002\u6b64\u5916\uff0c\u8fd8\u96c6\u6210\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\uff0c\u4fdd\u7559\u4ece\u8fa9\u8bba\u4e92\u52a8\u4e2d\u5b66\u5230\u7684\u5b89\u5168\u6d1e\u89c1\u3002", "result": "\u5728\u5df2\u5efa\u7acb\u7684\u5b89\u5168\u57fa\u51c6\uff08\u5982HarmBench\uff09\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4ec5\u8fa9\u8bba\u5c31\u80fd\u51cf\u5c1117.7%\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u800c\u7ed3\u5408\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\u540e\uff0c\u51cf\u5c11\u7387\u8d85\u8fc723.5%\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86RedDebate\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e4b\u95f4\u7684\u5bf9\u6297\u6027\u8bba\u8bc1\u6765\u4e3b\u52a8\u8bc6\u522b\u548c\u51cf\u8f7b\u5176\u81ea\u8eab\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\u3002"}}
{"id": "2506.11088", "pdf": "https://arxiv.org/pdf/2506.11088", "abs": "https://arxiv.org/abs/2506.11088", "authors": ["Pengbo Wang", "Chaozhuo Li", "Chenxu Wang", "Liwen Zheng", "Litian Zhang", "Xi Zhang"], "title": "Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing", "categories": ["cs.CL", "cs.AI", "68T50"], "comment": null, "summary": "LLMs have demonstrated unprecedented capabilities in natural language\nprocessing, yet their practical deployment remains hindered by persistent\nfactuality and faithfulness hallucinations. While existing methods address\nthese hallucination types independently, they inadvertently induce performance\ntrade-offs, as interventions targeting one type often exacerbate the other.\nThrough empirical and theoretical analysis of activation space dynamics in\nLLMs, we reveal that these hallucination categories share overlapping subspaces\nwithin neural representations, presenting an opportunity for concurrent\nmitigation. To harness this insight, we propose SPACE, a unified framework that\njointly enhances factuality and faithfulness by editing shared activation\nsubspaces. SPACE establishes a geometric foundation for shared subspace\nexistence through dual-task feature modeling, then identifies and edits these\nsubspaces via a hybrid probe strategy combining spectral clustering and\nattention head saliency scoring. Experimental results across multiple benchmark\ndatasets demonstrate the superiority of our approach.", "AI": {"tldr": "This paper introduces SPACE, a unified framework to simultaneously improve the factuality and faithfulness of large language models (LLMs) by editing shared activation subspaces.", "motivation": "To overcome persistent factuality and faithfulness hallucinations in LLMs, which hinder their practical deployment.", "method": "SPACE uses dual-task feature modeling to establish a geometric basis for shared subspaces, then employs a hybrid probe strategy involving spectral clustering and attention head saliency scoring to identify and edit these subspaces.", "result": "The experiments conducted on various benchmark datasets show the effectiveness of the proposed approach.", "conclusion": "SPACE offers a novel way to jointly enhance factuality and faithfulness in LLMs without trade-offs."}}
{"id": "2506.11091", "pdf": "https://arxiv.org/pdf/2506.11091", "abs": "https://arxiv.org/abs/2506.11091", "authors": ["Shaoshi Ling", "Guoli Ye"], "title": "Customizing Speech Recognition Model with Large Language Model Feedback", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic speech recognition (ASR) systems have achieved strong performance\non general transcription tasks. However, they continue to struggle with\nrecognizing rare named entities and adapting to domain mismatches. In contrast,\nlarge language models (LLMs), trained on massive internet-scale datasets, are\noften more effective across a wide range of domains. In this work, we propose a\nreinforcement learning based approach for unsupervised domain adaptation,\nleveraging unlabeled data to enhance transcription quality, particularly the\nnamed entities affected by domain mismatch, through feedback from a LLM. Given\ncontextual information, our framework employs a LLM as the reward model to\nscore the hypotheses from the ASR model. These scores serve as reward signals\nto fine-tune the ASR model via reinforcement learning. Our method achieves a\n21\\% improvement on entity word error rate over conventional self-training\nmethods.", "AI": {"tldr": "A reinforcement learning-based approach is proposed to improve ASR performance on named entities in domain mismatch scenarios.", "motivation": "ASR struggles with rare named entities and domain adaptation while LLMs excel in these areas.", "method": "Unsupervised domain adaptation using reinforcement learning and feedback from a large language model.", "result": "21% improvement in entity word error rate compared to traditional self-training methods.", "conclusion": "This method enhances ASR transcription quality, especially for named entities affected by domain mismatch."}}
{"id": "2506.11092", "pdf": "https://arxiv.org/pdf/2506.11092", "abs": "https://arxiv.org/abs/2506.11092", "authors": ["Jubin Abhishek Soni", "Amit Anand", "Rajesh Kumar Pandey", "Aniket Abhishek Soni"], "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "6 pages, 5 figures, 3 tables. This manuscript has been submitted to\n  IEEE conference. Researchers are welcome to read and build upon this work;\n  please cite it appropriately. For questions or clarifications, feel free to\n  contact me", "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.", "AI": {"tldr": "DCT is a new framework that makes RAG more flexible and adaptable for dynamic domains.", "motivation": "Existing RAG systems can't handle dynamic domains well because they can only interact statically and have fixed toolsets.", "method": "DCT uses an attention-based context cache, LoRA-based retrieval, and efficient context compression to adapt to changing conditions.", "result": "DCT improves plan accuracy and reduces hallucinations compared to previous methods.", "conclusion": "DCT enables more scalable and adaptable AI assistants in dynamic environments."}}
{"id": "2506.11094", "pdf": "https://arxiv.org/pdf/2506.11094", "abs": "https://arxiv.org/abs/2506.11094", "authors": ["Songyang Liu", "Chaozhuo Li", "Jiameng Qiu", "Xi Zhang", "Feiran Huang", "Litian Zhang", "Yiming Hei", "Philip S. Yu"], "title": "The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "21 pages, preprint", "summary": "With the rapid advancement of artificial intelligence technology, Large\nLanguage Models (LLMs) have demonstrated remarkable potential in the field of\nNatural Language Processing (NLP), including areas such as content generation,\nhuman-computer interaction, machine translation, and code generation, among\nothers. However, their widespread deployment has also raised significant safety\nconcerns. In recent years, LLM-generated content has occasionally exhibited\nunsafe elements like toxicity and bias, particularly in adversarial scenarios,\nwhich has garnered extensive attention from both academia and industry. While\nnumerous efforts have been made to evaluate the safety risks associated with\nLLMs, there remains a lack of systematic reviews summarizing these research\nendeavors. This survey aims to provide a comprehensive and systematic overview\nof recent advancements in LLMs safety evaluation, focusing on several key\naspects: (1) \"Why evaluate\" that explores the background of LLMs safety\nevaluation, how they differ from general LLMs evaluation, and the significance\nof such evaluation; (2) \"What to evaluate\" that examines and categorizes\nexisting safety evaluation tasks based on key capabilities, including\ndimensions such as toxicity, robustness, ethics, bias and fairness,\ntruthfulness, and so on; (3) \"Where to evaluate\" that summarizes the evaluation\nmetrics, datasets and benchmarks currently used in safety evaluations; (4) \"How\nto evaluate\" that reviews existing evaluation toolkit, and categorizing\nmainstream evaluation methods based on the roles of the evaluators. Finally, we\nidentify the challenges in LLMs safety evaluation and propose potential\nresearch directions to promote further advancement in this field. We emphasize\nthe importance of prioritizing LLMs safety evaluation to ensure the safe\ndeployment of these models in real-world applications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b89\u5168\u6027\u8bc4\u4f30\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u8bc4\u4f30\u539f\u56e0\u3001\u5185\u5bb9\u3001\u5730\u70b9\u548c\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u5e7f\u6cdb\u5e94\u7528\u4e5f\u5e26\u6765\u4e86\u663e\u8457\u7684\u5b89\u5168\u9690\u60a3\u3002\u5c3d\u7ba1\u5df2\u7ecf\u505a\u51fa\u4e86\u8bb8\u591a\u52aa\u529b\u6765\u8bc4\u4f30\u4e0eLLMs\u76f8\u5173\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4f46\u4ecd\u7136\u7f3a\u4e4f\u7cfb\u7edf\u7684\u7efc\u8ff0\u6765\u603b\u7ed3\u8fd9\u4e9b\u7814\u7a76\u5de5\u4f5c\u3002", "method": "\u5168\u9762\u7cfb\u7edf\u5730\u56de\u987e\u4e86LLMs\u5b89\u5168\u6027\u8bc4\u4f30\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u4e86\u8bc4\u4f30\u7684\u539f\u56e0\u3001\u5185\u5bb9\u3001\u5730\u70b9\u548c\u65b9\u6cd5\uff0c\u5e76\u5bf9\u73b0\u6709\u7684\u8bc4\u4f30\u5de5\u5177\u5305\u548c\u4e3b\u8981\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u5ba1\u67e5\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u7cfb\u7edf\u7684\u7efc\u8ff0\uff0c\u805a\u7126\u4e8e\u51e0\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u4e3a\u4ec0\u4e48\u8bc4\u4f30\u3001\u8bc4\u4f30\u4ec0\u4e48\u3001\u5728\u54ea\u91cc\u8bc4\u4f30\u4ee5\u53ca\u5982\u4f55\u8bc4\u4f30\uff0c\u5e76\u8bc6\u522b\u4e86LLMs\u5b89\u5168\u6027\u8bc4\u4f30\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u6f5c\u5728\u7684\u7814\u7a76\u65b9\u5411\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u4f18\u5148\u8fdb\u884cLLMs\u5b89\u5168\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u8fd9\u4e9b\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2506.11095", "pdf": "https://arxiv.org/pdf/2506.11095", "abs": "https://arxiv.org/abs/2506.11095", "authors": ["Manuel D. S. Hopp", "Vincent Labatut", "Arthur Amalvy", "Richard Dufour", "Hannah Stone", "Hayley Jach", "Kou Murayama"], "title": "Persistent Homology of Topic Networks for the Prediction of Reader Curiosity", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reader curiosity, the drive to seek information, is crucial for textual\nengagement, yet remains relatively underexplored in NLP. Building on\nLoewenstein's Information Gap Theory, we introduce a framework that models\nreader curiosity by quantifying semantic information gaps within a text's\nsemantic structure. Our approach leverages BERTopic-inspired topic modeling and\npersistent homology to analyze the evolving topology (connected components,\ncycles, voids) of a dynamic semantic network derived from text segments,\ntreating these features as proxies for information gaps. To empirically\nevaluate this pipeline, we collect reader curiosity ratings from participants\n(n = 49) as they read S. Collins's ''The Hunger Games'' novel. We then use the\ntopological features from our pipeline as independent variables to predict\nthese ratings, and experimentally show that they significantly improve\ncuriosity prediction compared to a baseline model (73% vs. 30% explained\ndeviance), validating our approach. This pipeline offers a new computational\nmethod for analyzing text structure and its relation to reader engagement.", "AI": {"tldr": "This study develops a method to measure reader curiosity by examining semantic information gaps in texts using topic modeling and topological analysis, demonstrating its effectiveness through predictions of curiosity ratings in 'The Hunger Games'.", "motivation": "To explore reader curiosity in NLP, which is crucial for textual engagement but has been understudied.", "method": "Uses BERTopic-inspired topic modeling and persistent homology to analyze the evolving topology of a dynamic semantic network derived from text segments.", "result": "The proposed pipeline significantly improves curiosity prediction compared to a baseline model.", "conclusion": "Introduces a new computational method for analyzing text structure and its relation to reader engagement."}}
{"id": "2506.11097", "pdf": "https://arxiv.org/pdf/2506.11097", "abs": "https://arxiv.org/abs/2506.11097", "authors": ["Haritz Puerto", "Martin Gubri", "Tommaso Green", "Seong Joon Oh", "Sangdoo Yun"], "title": "C-SEO Bench: Does Conversational SEO Work?", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) are transforming search engines into\nConversational Search Engines (CSE). Consequently, Search Engine Optimization\n(SEO) is being shifted into Conversational Search Engine Optimization (C-SEO).\nWe are beginning to see dedicated C-SEO methods for modifying web documents to\nincrease their visibility in CSE responses. However, they are often tested only\nfor a limited breadth of application domains; we do not understand whether\ncertain C-SEO methods would be effective for a broad range of domains.\nMoreover, existing evaluations consider only a single-actor scenario where only\none web document adopts a C-SEO method; in reality, multiple players are likely\nto competitively adopt the cutting-edge C-SEO techniques, drawing an analogy\nfrom the dynamics we have seen in SEO. We present C-SEO Bench, the first\nbenchmark designed to evaluate C-SEO methods across multiple tasks, domains,\nand number of actors. We consider two search tasks, question answering and\nproduct recommendation, with three domains each. We also formalize a new\nevaluation protocol with varying adoption rates among involved actors. Our\nexperiments reveal that most current C-SEO methods are largely ineffective,\ncontrary to reported results in the literature. Instead, traditional SEO\nstrategies, those aiming to improve the ranking of the source in the LLM\ncontext, are significantly more effective. We also observe that as we increase\nthe number of C-SEO adopters, the overall gains decrease, depicting a congested\nand zero-sum nature of the problem. Our code and data are available at\nhttps://github.com/parameterlab/c-seo-bench and\nhttps://huggingface.co/datasets/parameterlab/c-seo-bench.", "AI": {"tldr": "Large Language Models (LLMs) are changing search engines into Conversational Search Engines (CSE), leading to a shift in Search Engine Optimization (SEO) towards C-SEO. This paper introduces C-SEO Bench, a benchmark to evaluate C-SEO methods across different tasks, domains, and numbers of actors, revealing that traditional SEO strategies are more effective than current C-SEO methods.", "motivation": "The motivation is to address the lack of understanding on whether certain C-SEO methods would be effective for a broad range of domains and to consider competitive scenarios where multiple players might adopt advanced C-SEO techniques.", "method": "C-SEO Bench is introduced, which evaluates C-SEO methods across multiple tasks, domains, and numbers of actors with a new evaluation protocol considering varying adoption rates.", "result": "Most current C-SEO methods are largely ineffective, while traditional SEO strategies are significantly more effective. As the number of C-SEO adopters increases, overall gains decrease, indicating a congested and zero-sum nature.", "conclusion": "Traditional SEO strategies outperform current C-SEO methods, suggesting a need for further research into effective C-SEO approaches."}}
{"id": "2506.11102", "pdf": "https://arxiv.org/pdf/2506.11102", "abs": "https://arxiv.org/abs/2506.11102", "authors": ["Jiachen Zhu", "Menghui Zhu", "Renting Rui", "Rong Shan", "Congmin Zheng", "Bo Chen", "Yunjia Xi", "Jianghao Lin", "Weiwen Liu", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of large language models (LLMs), such as GPT, Gemini, and\nDeepSeek, has significantly advanced natural language processing, giving rise\nto sophisticated chatbots capable of diverse language-related tasks. The\ntransition from these traditional LLM chatbots to more advanced AI agents\nrepresents a pivotal evolutionary step. However, existing evaluation frameworks\noften blur the distinctions between LLM chatbots and AI agents, leading to\nconfusion among researchers selecting appropriate benchmarks. To bridge this\ngap, this paper introduces a systematic analysis of current evaluation\napproaches, grounded in an evolutionary perspective. We provide a detailed\nanalytical framework that clearly differentiates AI agents from LLM chatbots\nalong five key aspects: complex environment, multi-source instructor, dynamic\nfeedback, multi-modal perception, and advanced capability. Further, we\ncategorize existing evaluation benchmarks based on external environments\ndriving forces, and resulting advanced internal capabilities. For each\ncategory, we delineate relevant evaluation attributes, presented\ncomprehensively in practical reference tables. Finally, we synthesize current\ntrends and outline future evaluation methodologies through four critical\nlenses: environment, agent, evaluator, and metrics. Our findings offer\nactionable guidance for researchers, facilitating the informed selection and\napplication of benchmarks in AI agent evaluation, thus fostering continued\nadvancement in this rapidly evolving research domain.", "AI": {"tldr": "This paper systematically analyzes current evaluation approaches for AI agents from an evolutionary perspective, providing a detailed framework to differentiate AI agents from LLM chatbots across five key aspects and categorizing benchmarks based on external environments, driving forces, and internal capabilities.", "motivation": "To address confusion among researchers in selecting appropriate benchmarks due to blurred distinctions between LLM chatbots and AI agents.", "method": "Systematic analysis grounded in an evolutionary perspective, providing a detailed analytical framework with practical reference tables.", "result": "Categorized existing evaluation benchmarks and delineated relevant evaluation attributes.", "conclusion": "Offers actionable guidance for researchers in selecting and applying benchmarks in AI agent evaluation."}}
{"id": "2506.11103", "pdf": "https://arxiv.org/pdf/2506.11103", "abs": "https://arxiv.org/abs/2506.11103", "authors": ["Wenchong He", "Liqian Peng", "Zhe Jiang", "Alex Go"], "title": "You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 6 figures", "summary": "Large language models (LLMs) possess a remarkable ability to perform\nin-context learning (ICL), which enables them to handle multiple downstream\ntasks simultaneously without requiring task-specific fine-tuning. Recent\nstudies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma\n7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of\nall tasks at once. However, this approach still lags behind dedicated\nfine-tuning, where a separate model is trained for each individual task.\n  In this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning\n(ManyICL), which significantly narrows this performance gap by extending the\nprinciples of ICL to a many-shot setting. To unlock the full potential of\nManyICL and address the inherent inefficiency of processing long sequences with\nnumerous in-context examples, we propose a novel training objective. Instead of\nsolely predicting the final answer, our approach treats every answer within the\ncontext as a supervised training target. This effectively shifts the role of\nmany-shot examples from prompts to targets for autoregressive learning. Through\nextensive experiments on diverse downstream tasks, including classification,\nsummarization, question answering, natural language inference, and math, we\ndemonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning\nand approaches the performance of dedicated fine-tuning. Furthermore, ManyICL\nsignificantly mitigates catastrophic forgetting issues observed in\nzero/few-shot fine-tuning. The code will be made publicly available upon\npublication.", "AI": {"tldr": "This paper introduces Many-Shot In-Context Fine-tuning (ManyICL), a new method that improves the performance of large language models in handling multiple tasks without task-specific fine-tuning.", "motivation": "To narrow the performance gap between in-context learning and dedicated fine-tuning in large language models.", "method": "Extending in-context learning principles to a many-shot setting and introducing a new training objective that treats all answers in the context as supervised training targets.", "result": "ManyICL outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning across various tasks.", "conclusion": "The proposed ManyICL method effectively enhances the capabilities of large language models in handling multiple tasks without the need for task-specific fine-tuning."}}
{"id": "2506.11104", "pdf": "https://arxiv.org/pdf/2506.11104", "abs": "https://arxiv.org/abs/2506.11104", "authors": ["Hanzhi Zhang", "Heng Fan", "Kewei Sha", "Yan Huang", "Yunhe Feng"], "title": "DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding is crucial for many NLP applications, yet\ntransformers struggle with efficiency due to the quadratic complexity of\nself-attention. Sparse attention methods alleviate this cost but often impose\nstatic, predefined masks, failing to capture heterogeneous attention patterns.\nThis results in suboptimal token interactions, limiting adaptability and\nretrieval accuracy in long-sequence tasks. This work introduces a dynamic\nsparse attention mechanism that assigns adaptive masks at the attention-map\nlevel, preserving heterogeneous patterns across layers and heads. Unlike\nexisting approaches, our method eliminates the need for fine-tuning and\npredefined mask structures while maintaining computational efficiency. By\nlearning context-aware attention structures, it achieves high alignment with\nfull-attention models, ensuring minimal performance degradation while reducing\nmemory and compute overhead. This approach provides a scalable alternative to\nfull attention, enabling the practical deployment of large-scale Large Language\nModels (LLMs) without sacrificing retrieval performance. DAM is available at:\nhttps://github.com/HanzhiZhang-Ulrica/DAM.", "AI": {"tldr": "Introduces a dynamic sparse attention mechanism that improves long-context understanding in NLP tasks by learning adaptive attention masks, without predefined mask structures, achieving high alignment with full-attention models.", "motivation": "To address the inefficiency of transformers in handling long contexts due to quadratic complexity of self-attention and the limitation of static sparse attention methods in capturing heterogeneous attention patterns.", "method": "A dynamic sparse attention mechanism that learns adaptive masks at the attention-map level without predefined mask structures.", "result": "The proposed method achieves high alignment with full-attention models, ensures minimal performance degradation, and reduces memory and compute overhead, providing a scalable alternative for deploying large-scale LLMs.", "conclusion": "The paper introduces a dynamic sparse attention mechanism (DAM) that improves long-context understanding in NLP tasks by learning adaptive attention masks."}}
{"id": "2506.11105", "pdf": "https://arxiv.org/pdf/2506.11105", "abs": "https://arxiv.org/abs/2506.11105", "authors": ["Uttej Kallakurik", "Edward Humes", "Rithvik Jonna", "Xiaomin Lin", "Tinoosh Mohsenin"], "title": "Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation", "categories": ["cs.CL", "cs.AI", "cs.AR", "cs.SY", "eess.SY"], "comment": null, "summary": "Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u533b\u7597\u52a9\u624b\u7cfb\u7edf\uff0c\u901a\u8fc7\u901a\u7528\u538b\u7f29\u6846\u67b6\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4f7f\u5176\u9002\u5408\u5728\u4e13\u95e8\u9886\u57df\u90e8\u7f72\u3002\u901a\u8fc7\u6d4b\u91cf\u7279\u5b9a\u9886\u57df\u7684\u795e\u7ecf\u5143\u663e\u8457\u6027\u6765\u4fee\u526a\u65e0\u5173\u795e\u7ecf\u5143\uff0c\u5e76\u5e94\u7528\u540e\u8bad\u7ec3\u91cf\u5316\u8fdb\u4e00\u6b65\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002\u5728\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u548c\u786c\u4ef6\u8bbe\u5907\u4e0a\u8bc4\u4f30\u4e86\u538b\u7f29\u6a21\u578b\u3002", "motivation": "\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u53ef\u7528\u6027\u548c\u6548\u7387\u3002", "method": "1. \u6d4b\u91cf\u7279\u5b9a\u9886\u57df\u7684\u795e\u7ecf\u5143\u663e\u8457\u6027\u8fdb\u884c\u4fee\u526a\uff1b2. \u5e94\u7528\u540e\u8bad\u7ec3\u91cf\u5316\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002", "result": "\u5728\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u548c\u786c\u4ef6\u8bbe\u5907\u4e0a\u8bc4\u4f30\u4e86\u538b\u7f29\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u8282\u80fd\u7684\u63a8\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u51cf\u5c0fLLMs\u7684\u6a21\u578b\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7b49\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2506.11106", "pdf": "https://arxiv.org/pdf/2506.11106", "abs": "https://arxiv.org/abs/2506.11106", "authors": ["Ningyuan Li", "Junrui Liu", "Yi Shan", "Minghui Huang", "Tong Li"], "title": "Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Contemporary graph-based retrieval-augmented generation (RAG) methods\ntypically begin by extracting entities from user queries and then leverage\npre-constructed knowledge graphs to retrieve related relationships and\nmetadata. However, this pipeline's exclusive reliance on entity-level\nextraction can lead to the misinterpretation or omission of latent yet critical\ninformation and relations. As a result, retrieved content may be irrelevant or\ncontradictory, and essential knowledge may be excluded, exacerbating\nhallucination risks and degrading the fidelity of generated responses. To\naddress these limitations, we introduce PankRAG, a framework that combines a\nglobally aware, hierarchical query-resolution strategy with a novel\ndependency-aware reranking mechanism. PankRAG first constructs a multi-level\nresolution path that captures both parallel and sequential interdependencies\nwithin a query, guiding large language models (LLMs) through structured\nreasoning. It then applies its dependency-aware reranker to exploit the\ndependency structure among resolved sub-questions, enriching and validating\nretrieval results for subsequent sub-questions. Empirical evaluations\ndemonstrate that PankRAG consistently outperforms state-of-the-art approaches\nacross multiple benchmarks, underscoring its robustness and generalizability.", "AI": {"tldr": "PankRAG is a new framework that improves graph-based retrieval-augmented generation methods by resolving latent information and relations through a globally aware query-resolution strategy and a dependency-aware reranking mechanism, enhancing the quality of generated responses.", "motivation": "Current graph-based retrieval-augmented generation methods may misinterpret or omit important information due to their focus solely on entity-level extraction, leading to irrelevant or contradictory retrieved content and degraded response fidelity.", "method": "PankRAG introduces a globally aware, hierarchical query-resolution strategy and a novel dependency-aware reranking mechanism to capture interdependencies within queries and validate retrieval results.", "result": "PankRAG outperforms existing approaches across multiple benchmarks, showing its robustness and generalizability.", "conclusion": "PankRAG addresses limitations in current RAG methods by improving the retrieval process through enhanced query resolution and dependency-aware reranking, resulting in better quality generated responses."}}
{"id": "2506.11108", "pdf": "https://arxiv.org/pdf/2506.11108", "abs": "https://arxiv.org/abs/2506.11108", "authors": ["Andrew Kiruluta", "Andreas Lemos", "Priscilla Burity"], "title": "History-Aware Cross-Attention Reinforcement: Self-Supervised Multi Turn and Chain-of-Thought Fine-Tuning with vLLM", "categories": ["cs.CL"], "comment": null, "summary": "We present CAGSR-vLLM-MTC, an extension of our Self-Supervised\nCross-Attention-Guided Reinforcement (CAGSR) framework, now implemented on the\nhigh-performance vLLM runtime, to address both multi-turn dialogue and\nchain-of-thought reasoning. Building upon our original single-turn approach, we\nfirst instrumented vLLM's C++/CUDA kernels to asynchronously capture per-layer,\nper-head cross-attention weights during generation. We then generalized our\nself-supervised reward function to accumulate attention signals over entire\nconversation histories and intermediate chain-of-thought steps. We discuss\npractical trade-offs, including an entropy-based clamping mechanism to prevent\nattention collapse on early context, and outline future directions for\nmulti-party dialogues and hierarchical reasoning.", "AI": {"tldr": "An extension of the CAGSR framework, named CAGSR-vLLM-MTC, has been developed to handle multi-turn dialogues and chain-of-thought reasoning by implementing it on the vLLM runtime. This version captures cross-attention weights asynchronously and modifies the reward function to consider full conversation histories and thought processes.", "motivation": "To enhance the capabilities of the original CAGSR framework by enabling it to manage multi-turn dialogues and chain-of-thought reasoning.", "method": "The method involves modifying the vLLM's C++/CUDA kernels to capture cross-attention weights asynchronously and generalizing the self-supervised reward function to account for entire conversation histories and thought steps.", "result": "The implementation successfully addresses multi-turn dialogues and chain-of-thought reasoning, with considerations for practical trade-offs like an entropy-based clamping mechanism.", "conclusion": "This work extends the applicability of the CAGSR framework to more complex conversational scenarios, paving the way for future developments in multi-party dialogues and hierarchical reasoning."}}
{"id": "2506.11109", "pdf": "https://arxiv.org/pdf/2506.11109", "abs": "https://arxiv.org/abs/2506.11109", "authors": ["Yile Chen", "Yicheng Tao", "Yue Jiang", "Shuai Liu", "Han Yu", "Gao Cong"], "title": "Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD'25", "summary": "The widespread adoption of location-based services has led to the generation\nof vast amounts of mobility data, providing significant opportunities to model\nuser movement dynamics within urban environments. Recent advancements have\nfocused on adapting Large Language Models (LLMs) for mobility analytics.\nHowever, existing methods face two primary limitations: inadequate semantic\nrepresentation of locations (i.e., discrete IDs) and insufficient modeling of\nmobility signals within LLMs (i.e., single templated instruction fine-tuning).\nTo address these issues, we propose QT-Mob, a novel framework that\nsignificantly enhances LLMs for mobility analytics. QT-Mob introduces a\nlocation tokenization module that learns compact, semantically rich tokens to\nrepresent locations, preserving contextual information while ensuring\ncompatibility with LLMs. Furthermore, QT-Mob incorporates a series of\ncomplementary fine-tuning objectives that align the learned tokens with the\ninternal representations in LLMs, improving the model's comprehension of\nsequential movement patterns and location semantics. The proposed QT-Mob\nframework not only enhances LLMs' ability to interpret mobility data but also\nprovides a more generalizable approach for various mobility analytics tasks.\nExperiments on three real-world dataset demonstrate the superior performance in\nboth next-location prediction and mobility recovery tasks, outperforming\nexisting deep learning and LLM-based methods.", "AI": {"tldr": "A novel framework named QT-Mob is proposed to enhance large language models for mobility analytics.", "motivation": "Existing methods have limitations in representing locations and modeling mobility signals within LLMs.", "method": "QT-Mob introduces a location tokenization module and complementary fine-tuning objectives to improve LLMs' understanding of sequential movement patterns and location semantics.", "result": "QT-Mob outperforms existing methods in next-location prediction and mobility recovery tasks on three real-world datasets.", "conclusion": "The proposed QT-Mob framework improves LLMs' ability to interpret mobility data and provides a more generalizable approach for mobility analytics tasks."}}
{"id": "2506.11110", "pdf": "https://arxiv.org/pdf/2506.11110", "abs": "https://arxiv.org/abs/2506.11110", "authors": ["Jaeho Lee", "Atharv Chowdhary"], "title": "AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 4 figures, appendix contains 2 additional figures and 2\n  tables", "summary": "Recent benchmarks have probed factual consistency and rhetorical robustness\nin Large Language Models (LLMs). However, a knowledge gap exists regarding how\ndirectional framing of factually true statements influences model agreement, a\ncommon scenario for LLM users. AssertBench addresses this by sampling\nevidence-supported facts from FEVEROUS, a fact verification dataset. For each\n(evidence-backed) fact, we construct two framing prompts: one where the user\nclaims the statement is factually correct, and another where the user claims it\nis incorrect. We then record the model's agreement and reasoning. The desired\noutcome is that the model asserts itself, maintaining consistent truth\nevaluation across both framings, rather than switching its evaluation to agree\nwith the user. AssertBench isolates framing-induced variability from the\nmodel's underlying factual knowledge by stratifying results based on the\nmodel's accuracy on the same claims when presented neutrally. In doing so, this\nbenchmark aims to measure an LLM's ability to \"stick to its guns\" when\npresented with contradictory user assertions about the same fact. The complete\nsource code is available at https://github.com/achowd32/assert-bench.", "AI": {"tldr": "This paper introduces AssertBench, a benchmark designed to evaluate how well large language models maintain consistent truth evaluations when faced with contradictory user assertions about the same fact.", "motivation": "To investigate the impact of directional framing on model agreement in LLMs, which is a common use case for LLM users.", "method": "Samples evidence-supported facts from FEVEROUS, constructs two framing prompts for each fact, and records the model's agreement and reasoning.", "result": "AssertBench helps isolate framing-induced variability from the model's underlying factual knowledge by stratifying results based on the model's accuracy on neutral presentations of the same claims.", "conclusion": "AssertBench measures an LLM's ability to maintain consistent truth evaluations when presented with contradictory user assertions."}}
{"id": "2506.11111", "pdf": "https://arxiv.org/pdf/2506.11111", "abs": "https://arxiv.org/abs/2506.11111", "authors": ["Kun Zhang", "Le Wu", "Kui Yu", "Guangyi Lv", "Dacao Zhang"], "title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": "33 pages, 5 figures", "summary": "Large Language Models (LLMs) have gained enormous attention in recent years\ndue to their capability of understanding and generating natural languages. With\nthe rapid development and wild-range applications (e.g., Agents, Embodied\nIntelligence), the robustness of LLMs has received increased attention. As the\ncore brain of many AI applications, the robustness of LLMs requires that models\nshould not only generate consistent contents, but also ensure the correctness\nand stability of generated content when dealing with unexpeted application\nscenarios (e.g., toxic prompts, limited noise domain data, outof-distribution\n(OOD) applications, etc). In this survey paper, we conduct a thorough review of\nthe robustness of LLMs, aiming to provide a comprehensive terminology of\nconcepts and methods around this field and facilitate the community.\nSpecifically, we first give a formal definition of LLM robustness and present\nthe collection protocol of this survey paper. Then, based on the types of\nperturbated inputs, we organize this survey from the following perspectives: 1)\nAdversarial Robustness: tackling the problem that prompts are manipulated\nintentionally, such as noise prompts, long context, data attack, etc; 2) OOD\nRobustness: dealing with the unexpected real-world application scenarios, such\nas OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of\nRobustness: summarizing the new evaluation datasets, metrics, and tools for\nverifying the robustness of LLMs. After reviewing the representative work from\neach perspective, we discuss and highlight future opportunities and research\ndirections in this field. Meanwhile, we also organize related works and provide\nan easy-to-search project\n(https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the\ncommunity.", "AI": {"tldr": "This survey provides a comprehensive review of the robustness of Large Language Models (LLMs), covering adversarial robustness, out-of-distribution (OOD) robustness, and evaluation of robustness, with the aim of facilitating the community.", "motivation": "To enhance the robustness of LLMs which is crucial as they become core components in various AI applications.", "method": "The survey is organized based on the types of perturbed inputs, including adversarial robustness, OOD robustness, and evaluation of robustness.", "result": "The survey reviews representative work from each perspective and discusses future opportunities and research directions. It also provides an easily searchable project on GitHub to support the community.", "conclusion": "This survey aims to provide a comprehensive terminology of concepts and methods around LLM robustness and facilitate the community."}}
{"id": "2506.11112", "pdf": "https://arxiv.org/pdf/2506.11112", "abs": "https://arxiv.org/abs/2506.11112", "authors": ["Christine Bauer", "Li Chen", "Nicola Ferro", "Norbert Fuhr", "Avishek Anand", "Timo Breuer", "Guglielmo Faggioli", "Ophir Frieder", "Hideo Joho", "Jussi Karlgren", "Johannes Kiesel", "Bart P. Knijnenburg", "Aldo Lipani", "Lien Michiels", "Andrea Papenmeier", "Maria Soledad Pera", "Mark Sanderson", "Scott Sanner", "Benno Stein", "Johanne R. Trippas", "Karin Verspoor", "Martijn C Willemsen"], "title": "Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)", "categories": ["cs.CL", "cs.HC", "cs.IR"], "comment": "43 pages; 10 figures; Dagstuhl manifesto", "summary": "During the workshop, we deeply discussed what CONversational Information\nACcess (CONIAC) is and its unique features, proposing a world model abstracting\nit, and defined the Conversational Agents Framework for Evaluation (CAFE) for\nthe evaluation of CONIAC systems, consisting of six major components: 1) goals\nof the system's stakeholders, 2) user tasks to be studied in the evaluation, 3)\naspects of the users carrying out the tasks, 4) evaluation criteria to be\nconsidered, 5) evaluation methodology to be applied, and 6) measures for the\nquantitative criteria chosen.", "AI": {"tldr": "Discussed CONIAC and proposed a framework called CAFE for evaluating such systems.", "motivation": "To deeply discuss what CONversational Information ACcess (CONIAC) is and its unique features.", "method": "Proposed a world model for CONIAC and defined CAFE.", "result": "Defined a framework named CAFE with six major components for evaluating CONIAC systems.", "conclusion": "This paper proposed a world model for CONIAC and defined CAFE, which consists of six main parts, providing a comprehensive framework for evaluating conversational information access systems."}}
{"id": "2506.11113", "pdf": "https://arxiv.org/pdf/2506.11113", "abs": "https://arxiv.org/abs/2506.11113", "authors": ["Tzu-Ling Lin", "Wei-Chih Chen", "Teng-Fang Hsiao", "Hou-I Liu", "Ya-Hsin Yeh", "Yu Kai Chan", "Wen-Sheng Lien", "Po-Yen Kuo", "Philip S. Yu", "Hong-Han Shuai"], "title": "Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Peer review is essential for maintaining academic quality, but the increasing\nvolume of submissions places a significant burden on reviewers. Large language\nmodels (LLMs) offer potential assistance in this process, yet their\nsusceptibility to textual adversarial attacks raises reliability concerns. This\npaper investigates the robustness of LLMs used as automated reviewers in the\npresence of such attacks. We focus on three key questions: (1) The\neffectiveness of LLMs in generating reviews compared to human reviewers. (2)\nThe impact of adversarial attacks on the reliability of LLM-generated reviews.\n(3) Challenges and potential mitigation strategies for LLM-based review. Our\nevaluation reveals significant vulnerabilities, as text manipulations can\ndistort LLM assessments. We offer a comprehensive evaluation of LLM performance\nin automated peer reviewing and analyze its robustness against adversarial\nattacks. Our findings emphasize the importance of addressing adversarial risks\nto ensure AI strengthens, rather than compromises, the integrity of scholarly\ncommunication.", "AI": {"tldr": "This study examines the effectiveness and reliability of large language models (LLMs) as automated reviewers in academic peer review, considering their susceptibility to adversarial attacks and suggesting mitigation strategies.", "motivation": "To address the challenges posed by the increasing volume of academic submissions and explore how LLMs can assist while maintaining reliability.", "method": "Investigating the robustness of LLMs in automated peer reviewing, focusing on their performance and vulnerability to adversarial attacks.", "result": "Significant vulnerabilities were found where text manipulations could distort LLM assessments, indicating a need for further development.", "conclusion": "It's crucial to mitigate adversarial risks to ensure AI supports, rather than undermines, the integrity of scholarly communication."}}
{"id": "2506.11114", "pdf": "https://arxiv.org/pdf/2506.11114", "abs": "https://arxiv.org/abs/2506.11114", "authors": ["Junyu Liu", "Kaiqi Yan", "Tianyang Wang", "Qian Niu", "Momoko Nagai-Tanima", "Tomoki Aoyama"], "title": "KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations", "categories": ["cs.CL", "cs.AI"], "comment": "9pages, 3 figures", "summary": "Recent advances in large language models (LLMs) have demonstrated notable\nperformance in medical licensing exams. However, comprehensive evaluation of\nLLMs across various healthcare roles, particularly in high-stakes clinical\nscenarios, remains a challenge. Existing benchmarks are typically text-based,\nEnglish-centric, and focus primarily on medicines, which limits their ability\nto assess broader healthcare knowledge and multimodal reasoning. To address\nthese gaps, we introduce KokushiMD-10, the first multimodal benchmark\nconstructed from ten Japanese national healthcare licensing exams. This\nbenchmark spans multiple fields, including Medicine, Dentistry, Nursing,\nPharmacy, and allied health professions. It contains over 11588 real exam\nquestions, incorporating clinical images and expert-annotated rationales to\nevaluate both textual and visual reasoning. We benchmark over 30\nstate-of-the-art LLMs, including GPT-4o, Claude 3.5, and Gemini, across both\ntext and image-based settings. Despite promising results, no model consistently\nmeets passing thresholds across domains, highlighting the ongoing challenges in\nmedical AI. KokushiMD-10 provides a comprehensive and linguistically grounded\nresource for evaluating and advancing reasoning-centric medical AI across\nmultilingual and multimodal clinical tasks.", "AI": {"tldr": "Introducing KokushiMD-10, a new multimodal benchmark for evaluating medical AI across multiple languages and domains.", "motivation": "To address gaps in comprehensive evaluation of LLMs across various healthcare roles and high-stakes clinical scenarios.", "method": "Benchmarking state-of-the-art LLMs across text and image-based settings using KokushiMD-10.", "result": "Introduction of KokushiMD-10, a new multimodal benchmark derived from Japanese national healthcare licensing exams.", "conclusion": "No model consistently meets passing thresholds across domains, indicating ongoing challenges in medical AI."}}
{"id": "2506.11115", "pdf": "https://arxiv.org/pdf/2506.11115", "abs": "https://arxiv.org/abs/2506.11115", "authors": ["Yerim Oh", "Jun-Hyung Park", "Junho Kim", "SungHo Kim", "SangKeun Lee"], "title": "Incorporating Domain Knowledge into Materials Tokenization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While language models are increasingly utilized in materials science, typical\nmodels rely on frequency-centric tokenization methods originally developed for\nnatural language processing. However, these methods frequently produce\nexcessive fragmentation and semantic loss, failing to maintain the structural\nand semantic integrity of material concepts. To address this issue, we propose\nMATTER, a novel tokenization approach that integrates material knowledge into\ntokenization. Based on MatDetector trained on our materials knowledge base and\na re-ranking method prioritizing material concepts in token merging, MATTER\nmaintains the structural integrity of identified material concepts and prevents\nfragmentation during tokenization, ensuring their semantic meaning remains\nintact. The experimental results demonstrate that MATTER outperforms existing\ntokenization methods, achieving an average performance gain of $4\\%$ and $2\\%$\nin the generation and classification tasks, respectively. These results\nunderscore the importance of domain knowledge for tokenization strategies in\nscientific text processing. Our code is available at\nhttps://github.com/yerimoh/MATTER", "AI": {"tldr": "This paper presents MATTER, a new tokenization approach that incorporates material knowledge into the process to preserve the structural and semantic integrity of material concepts. Experiments show it improves performance in generation and classification tasks compared to existing methods.", "motivation": "Existing tokenization methods for language models often fragment material concepts and lose their semantic meaning, which negatively impacts scientific text processing in materials science.", "method": "MATTER uses MatDetector trained on a materials knowledge base and a re-ranking method to prioritize material concepts during token merging.", "result": "MATTER outperforms current tokenization methods with an average improvement of 4% in generation tasks and 2% in classification tasks.", "conclusion": "Integrating domain knowledge into tokenization significantly enhances the effectiveness of language models in materials science."}}
{"id": "2506.11116", "pdf": "https://arxiv.org/pdf/2506.11116", "abs": "https://arxiv.org/abs/2506.11116", "authors": ["Jijie Li", "Li Du", "Hanyu Zhao", "Bo-wen Zhang", "Liangdong Wang", "Boyan Gao", "Guang Liu", "Yonghua Lin"], "title": "Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong performance in real-world\napplications, yet existing open-source instruction datasets often concentrate\non narrow domains, such as mathematics or coding, limiting generalization and\nwidening the gap with proprietary models. To bridge this gap, we introduce\nInfinity-Instruct, a high-quality instruction dataset designed to enhance both\nfoundational and chat capabilities of LLMs through a two-phase pipeline. In\nPhase 1, we curate 7.4M high-quality foundational instructions\n(InfInstruct-F-7.4M) from over 100M samples using hybrid data selection\ntechniques. In Phase 2, we synthesize 1.5M high-quality chat instructions\n(InfInstruct-G-1.5M) through a two-stage process involving instruction\nselection, evolution, and diagnostic filtering. We empirically evaluate\nInfinity-Instruct by fine-tuning several open-source models, including Mistral,\nLLaMA, Qwen, and Yi, and observe substantial performance gains across both\nfoundational and instruction following benchmarks, consistently surpassing\nofficial instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B\noutperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving\ncomparable foundational performance. These results underscore the synergy\nbetween foundational and chat training and offer new insights into holistic LLM\ndevelopment. Our\ndataset\\footnote{https://huggingface.co/datasets/BAAI/Infinity-Instruct} and\ncodes\\footnote{https://gitee.com/li-touch/infinity-instruct} have been publicly\nreleased.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u8d28\u6307\u4ee4\u6570\u636e\u96c6Infinity-Instruct\uff0c\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u548c\u804a\u5929\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u591a\u4e2a\u5f00\u6e90\u6a21\u578b\u4e0a\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u5f00\u653e\u6e90\u4ee3\u7801\u6307\u4ee4\u6570\u636e\u96c6\u901a\u5e38\u96c6\u4e2d\u5728\u72ed\u7a84\u7684\u9886\u57df\uff0c\u5982\u6570\u5b66\u6216\u7f16\u7801\uff0c\u9650\u5236\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u6269\u5927\u4e86\u4e0e\u4e13\u6709\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u7ba1\u9053\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u57fa\u7840\u6307\u4ee4\u548c\u804a\u5929\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u7136\u540e\u8bc4\u4f30\u8fd9\u4e9b\u6570\u636e\u96c6\u5bf9\u5f00\u6e90\u6a21\u578b\u7684\u5f71\u54cd\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u5728\u57fa\u7840\u548c\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u5f97\u5230\u4e86\u663e\u8457\u63d0\u9ad8\uff0cInfInstruct-LLaMA3.1-70B\u5728\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u6bd4GPT-4-0314\u9ad8\u51fa8.6\uff05\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528Infinity-Instruct\u6570\u636e\u96c6\uff0c\u5f00\u6e90\u6a21\u578b\u5728\u57fa\u7840\u548c\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\u751a\u81f3\u8d85\u8fc7\u4e86GPT-4-0314\u3002"}}
{"id": "2506.11117", "pdf": "https://arxiv.org/pdf/2506.11117", "abs": "https://arxiv.org/abs/2506.11117", "authors": ["Junyong Lin", "Lu Dai", "Ruiqian Han", "Yijie Sui", "Ruilin Wang", "Xingliang Sun", "Qinglin Wu", "Min Feng", "Hao Liu", "Hui Xiong"], "title": "ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "KDD 2025 Accepted", "summary": "Scientific researchers need intensive information about datasets to\neffectively evaluate and develop theories and methodologies. The information\nneeds regarding datasets are implicitly embedded in particular research tasks,\nrather than explicitly expressed in search queries. However, existing\nscientific retrieval and question-answering (QA) datasets typically address\nstraightforward questions, which do not align with the distribution of\nreal-world research inquiries. To bridge this gap, we developed ScIRGen, a\ndataset generation framework for scientific QA \\& retrieval that more\naccurately reflects the information needs of professional science researchers,\nand uses it to create a large-scale scientific retrieval-augmented generation\n(RAG) dataset with realistic queries, datasets and papers. Technically, we\ndesigned a dataset-oriented information extraction method that leverages\nacademic papers to augment the dataset representation. We then proposed a\nquestion generation framework by employing cognitive taxonomy to ensure the\nquality of synthesized questions. We also design a method to automatically\nfilter synthetic answers based on the perplexity shift of LLMs, which is highly\naligned with human judgment of answers' validity. Collectively, these\nmethodologies culminated in the creation of the 61k QA dataset, ScIRGen-Geo. We\nbenchmarked representative methods on the ScIRGen-Geo dataset for their\nquestion-answering and retrieval capabilities, finding out that current methods\nstill suffer from reasoning from complex questions. This work advances the\ndevelopment of more sophisticated tools to support the intricate information\nneeds of the scientific community.", "AI": {"tldr": "Introduce ScIRGen, a new dataset generation framework that creates a large-scale scientific retrieval-augmented generation dataset with realistic queries, improving support for complex scientific information needs.", "motivation": "Existing scientific retrieval and QA datasets mostly deal with simple questions, not reflecting the real-world complexity of scientists' information needs.", "method": "Developed ScIRGen, a dataset generation framework that uses academic papers to augment dataset representation and employs a cognitive taxonomy for question generation. Also used LLMs perplexity shift for automatic filtering of synthetic answers.", "result": "Created a large-scale RAG dataset called ScIRGen-Geo containing 61k QA pairs that better reflect real-world scientific information needs.", "conclusion": "This work has advanced tools to meet the intricate information needs of the scientific community by addressing the limitations of current methods in handling complex questions."}}
{"id": "2506.11119", "pdf": "https://arxiv.org/pdf/2506.11119", "abs": "https://arxiv.org/abs/2506.11119", "authors": ["Jingyu Li", "Lingchao Mao", "Hairong Wang", "Zhendong Wang", "Xi Mao", "Xuelei Sherry Ni"], "title": "Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T10 (Primary), 68U99 (Secondary)", "I.2.1; J.3"], "comment": null, "summary": "Background: Alzheimer's disease and related dementias (ADRD) are progressive\nneurodegenerative conditions where early detection is vital for timely\nintervention and care. Spontaneous speech contains rich acoustic and linguistic\nmarkers that may serve as non-invasive biomarkers for cognitive decline.\nFoundation models, pre-trained on large-scale audio or text data, produce\nhigh-dimensional embeddings encoding contextual and acoustic features.\n  Methods: We used the PREPARE Challenge dataset, which includes audio\nrecordings from over 1,600 participants with three cognitive statuses: healthy\ncontrol (HC), mild cognitive impairment (MCI), and Alzheimer's Disease (AD). We\nexcluded non-English, non-spontaneous, or poor-quality recordings. The final\ndataset included 703 (59.13%) HC, 81 (6.81%) MCI, and 405 (34.06%) AD cases. We\nbenchmarked a range of open-source foundation speech and language models to\nclassify cognitive status into the three categories.\n  Results: The Whisper-medium model achieved the highest performance among\nspeech models (accuracy = 0.731, AUC = 0.802). Among language models, BERT with\npause annotation performed best (accuracy = 0.662, AUC = 0.744). ADRD detection\nusing state-of-the-art automatic speech recognition (ASR) model-generated audio\nembeddings outperformed others. Including non-semantic features like pause\npatterns consistently improved text-based classification.\n  Conclusion: This study introduces a benchmarking framework using foundation\nmodels and a clinically relevant dataset. Acoustic-based approaches --\nparticularly ASR-derived embeddings -- demonstrate strong potential for\nscalable, non-invasive, and cost-effective early detection of ADRD.", "AI": {"tldr": "This study evaluates different foundation models for classifying cognitive status based on spontaneous speech, finding that acoustic-based approaches, especially those using ASR-derived embeddings, show promise for early detection of Alzheimer's disease.", "motivation": "Early detection of Alzheimer's disease and related dementias is crucial for timely intervention. This study explores the use of spontaneous speech and foundation models as non-invasive biomarkers for cognitive decline.", "method": "The study uses the PREPARE Challenge dataset, evaluating various open-source foundation speech and language models to classify cognitive status into three categories: healthy control, mild cognitive impairment, and Alzheimer's disease.", "result": "Whisper-medium model achieved the highest performance among speech models with an accuracy of 0.731 and AUC of 0.802. BERT with pause annotation performed best among language models with an accuracy of 0.662 and AUC of 0.744. Non-semantic features like pause patterns improved text-based classification.", "conclusion": "Acoustic-based approaches, particularly those using ASR-derived embeddings, show strong potential for scalable, non-invasive, and cost-effective early detection of Alzheimer's disease."}}
{"id": "2506.11120", "pdf": "https://arxiv.org/pdf/2506.11120", "abs": "https://arxiv.org/abs/2506.11120", "authors": ["Hourun Zhu", "Chengchao Shen"], "title": "SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In spite of strong performance achieved by LLMs, the costs of their\ndeployment are unaffordable. For the compression of LLMs, gradient-based\npruning methods present promising effectiveness. However, in these methods, the\ngradient computation with one-hot labels ignore the potential predictions on\nother words, thus missing key information for generative capability of the\noriginal model. To address this issue, we introduce a self-distillation loss\nduring the pruning phase (rather than post-training) to fully exploit the\npredictions of the original model, thereby obtaining more accurate gradient\ninformation for pruning. Moreover, we find that, compared to attention modules,\nthe predictions of LLM are less sensitive to multilayer perceptron (MLP)\nmodules, which take up more than $5 \\times$ parameters (LLaMA3.2-1.2B). To this\nend, we focus on the pruning of MLP modules, to significantly compress LLM\nwithout obvious performance degradation. Experimental results on extensive\nzero-shot benchmarks demonstrate that our method significantly outperforms\nexisting pruning methods. Furthermore, our method achieves very competitive\nperformance among 1B-scale open source LLMs. The source code and trained\nweights are available at https://github.com/visresearch/SDMPrune.", "AI": {"tldr": "A novel method is proposed for compressing large language models (LLMs) using self-distillation loss during pruning instead of post-training to obtain more precise gradient information. The approach also focuses on pruning multilayer perceptron (MLP) modules which account for over 5 times the parameters of attention modules in LLaMA3.2-1.2B.", "motivation": "The high deployment costs of LLMs despite their strong performance necessitate efficient compression techniques. Existing gradient-based pruning methods fail to consider all potential predictions leading to loss of critical generative capability information.", "method": "Introducing self-distillation loss during the pruning phase to capture comprehensive predictions from the original model and enhance gradient accuracy. Additionally, focusing specifically on pruning MLP modules which are less sensitive compared to attention modules but constitute a significant portion of the model's parameters.", "result": "Experimental results show superior performance compared to existing pruning methods. The method also achieves competitive performance among 1B-scale open-source LLMs.", "conclusion": "This work presents an effective approach for compressing LLMs by improving pruning efficiency through self-distillation loss and targeted MLP module pruning."}}
{"id": "2506.11121", "pdf": "https://arxiv.org/pdf/2506.11121", "abs": "https://arxiv.org/abs/2506.11121", "authors": ["Wei-Ping Huang", "Guan-Ting Lin", "Hung-yi Lee"], "title": "SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Despite progress in end-to-end ASR, real-world domain mismatches still cause\nperformance drops, which Test-Time Adaptation (TTA) aims to mitigate by\nadjusting models during inference. Recent work explores combining TTA with\nexternal language models, using techniques like beam search rescoring or\ngenerative error correction. In this work, we identify a previously overlooked\nchallenge: TTA can interfere with language model rescoring, revealing the\nnontrivial nature of effectively combining the two methods. Based on this\ninsight, we propose SUTA-LM, a simple yet effective extension of SUTA, an\nentropy-minimization-based TTA approach, with language model rescoring. SUTA-LM\nfirst applies a controlled adaptation process guided by an auto-step selection\nmechanism leveraging both acoustic and linguistic information, followed by\nlanguage model rescoring to refine the outputs. Experiments on 18 diverse ASR\ndatasets show that SUTA-LM achieves robust results across a wide range of\ndomains.", "AI": {"tldr": "This paper introduces SUTA-LM, an enhanced version of the entropy-minimization-based Test-Time Adaptation (TTA) method for ASR systems. It combines TTA with language model rescoring to improve performance across various domains.", "motivation": "Addressing the challenge where TTA interferes with language model rescoring.", "method": "SUTA-LM first uses a controlled adaptation with an auto-step selection mechanism, then applies language model rescoring.", "result": "Experiments on 18 diverse ASR datasets show that SUTA-LM provides robust performance across many domains.", "conclusion": "SUTA-LM is a simple yet effective solution to combine TTA and language model rescoring in ASR."}}
{"id": "2506.11125", "pdf": "https://arxiv.org/pdf/2506.11125", "abs": "https://arxiv.org/abs/2506.11125", "authors": ["Freddie Grabovski", "Gilad Gressel", "Yisroel Mirsky"], "title": "ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), combined with Text-to-Speech (TTS) and\nAutomatic Speech Recognition (ASR), are increasingly used to automate voice\nphishing (vishing) scams. These systems are scalable and convincing, posing a\nsignificant security threat. We identify the ASR transcription step as the most\nvulnerable link in the scam pipeline and introduce ASRJam, a proactive defence\nframework that injects adversarial perturbations into the victim's audio to\ndisrupt the attacker's ASR. This breaks the scam's feedback loop without\naffecting human callers, who can still understand the conversation. While prior\nadversarial audio techniques are often unpleasant and impractical for real-time\nuse, we also propose EchoGuard, a novel jammer that leverages natural\ndistortions, such as reverberation and echo, that are disruptive to ASR but\ntolerable to humans. To evaluate EchoGuard's effectiveness and usability, we\nconducted a 39-person user study comparing it with three state-of-the-art\nattacks. Results show that EchoGuard achieved the highest overall utility,\noffering the best combination of ASR disruption and human listening experience.", "AI": {"tldr": "Large Language Models combined with TTS and ASR are being used for vishing scams. A new defense framework called ASRJam disrupts ASR by injecting adversarial perturbations into the victim's audio. EchoGuard is proposed which uses natural distortions to disrupt ASR while being tolerable for humans.", "motivation": "To address the increasing use of LLMs, TTS, and ASR in vishing scams and find a way to disrupt these attacks without affecting human communication.", "method": "Introduce ASRJam to inject adversarial perturbations and propose EchoGuard leveraging natural distortions like reverberation and echo.", "result": "EchoGuard showed high effectiveness and usability in a user study with 39 participants, outperforming three state-of-the-art attacks.", "conclusion": "Combining ASRJam and EchoGuard provides a proactive defense mechanism against vishing scams that disrupts ASR without impairing human understanding."}}
{"id": "2506.11127", "pdf": "https://arxiv.org/pdf/2506.11127", "abs": "https://arxiv.org/abs/2506.11127", "authors": ["Wenkang Han", "Zhixiong Zeng", "Jing Huang", "Shu Jiang", "Liming Zheng", "Longrong Yang", "Haibo Qiu", "Chang Yao", "Jingyuan Chen", "Lin Ma"], "title": "GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing\nhuman-computer interaction, yet their reliance on text-based instructions\nimposes limitations on accessibility and convenience, particularly in\nhands-free scenarios. To address this gap, we propose GUIRoboTron-Speech, the\nfirst end-to-end autonomous GUI agent that directly accepts speech instructions\nand on-device screenshots to predict actions. Confronted with the scarcity of\nspeech-based GUI agent datasets, we initially generated high-quality speech\ninstructions for training by leveraging a random timbre text-to-speech (TTS)\nmodel to convert existing text instructions. We then develop\nGUIRoboTron-Speech's capabilities through progressive grounding and planning\ntraining stages. A key contribution is a heuristic mixed-instruction training\nstrategy designed to mitigate the modality imbalance inherent in pre-trained\nfoundation models. Comprehensive experiments on several benchmark datasets\nvalidate the robust and superior performance of GUIRoboTron-Speech,\ndemonstrating the significant potential and widespread applicability of speech\nas an effective instruction modality for driving GUI agents. Our code and\ndatasets are available at https://github.com/GUIRoboTron/GUIRoboTron-Speech.", "AI": {"tldr": "GUIRoboTron-Speech is the first end-to-end autonomous GUI agent that accepts speech instructions and on-device screenshots, demonstrating superior performance in various benchmarks.", "motivation": "To address the limitations of text-based instructions in hands-free scenarios by creating an autonomous GUI agent that accepts speech instructions and on-device screenshots.", "method": "Progressive grounding and planning training stages along with a heuristic mixed-instruction training strategy.", "result": "Superior performance compared to other methods on several benchmark datasets.", "conclusion": "GUIRoboTron-Speech demonstrates the potential of using speech as an effective instruction modality for driving GUI agents."}}
{"id": "2506.11128", "pdf": "https://arxiv.org/pdf/2506.11128", "abs": "https://arxiv.org/abs/2506.11128", "authors": ["Andrew Keenan Richardson", "Ryan Othniel Kearns", "Sean Moss", "Vincent Wang-Mascianica", "Philipp Koralus"], "title": "Stronger Language Models Produce More Human-Like Errors", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do language models converge toward human-like reasoning patterns as they\nimprove? We provide surprising evidence that while overall reasoning\ncapabilities increase with model sophistication, the nature of errors\nincreasingly mirrors predictable human reasoning fallacies: a previously\nunobserved inverse scaling phenomenon. To investigate this question, we apply\nthe Erotetic Theory of Reasoning (ETR), a formal cognitive framework with\nempirical support for predicting human reasoning outcomes. Using the\nopen-source package PyETR, we generate logical reasoning problems where humans\npredictably err, evaluating responses from 38 language models across 383\nreasoning tasks. Our analysis indicates that as models advance in general\ncapability (as measured by Chatbot Arena scores), the proportion of their\nincorrect answers that align with ETR-predicted human fallacies tends to\nincrease ($\\rho = 0.360, p = 0.0265$). Notably, as we observe no correlation\nbetween model sophistication and logical correctness on these tasks, this shift\nin error patterns toward human-likeness occurs independently of error rate.\nThese findings challenge the prevailing view that scaling language models\nnaturally obtains normative rationality, suggesting instead a convergence\ntoward human-like cognition inclusive of our characteristic biases and\nlimitations, as we further confirm by demonstrating order-effects in language\nmodel reasoning.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9519\u8bef\u9010\u6e10\u5448\u73b0\u4eba\u7c7b\u5f0f\u63a8\u7406\u504f\u8bef\uff0c\u8fd9\u6311\u6218\u4e86\u6a21\u578b\u4f1a\u81ea\u7136\u83b7\u5f97\u89c4\u8303\u7406\u6027\u80fd\u529b\u7684\u4f20\u7edf\u89c2\u5ff5\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u968f\u7740\u80fd\u529b\u63d0\u9ad8\u800c\u8d8b\u5411\u4e8e\u4eba\u7c7b\u5f0f\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u57fa\u4e8ePyETR\u751f\u6210\u7684\u903b\u8f91\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u8bc4\u4f3038\u4e2a\u8bed\u8a00\u6a21\u578b\u5728383\u4e2a\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u968f\u7740\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u5176\u9519\u8bef\u7b54\u6848\u66f4\u503e\u5411\u4e8e\u7b26\u5408\u4eba\u7c7b\u63a8\u7406\u504f\u8bef\uff0c\u4e14\u8fd9\u79cd\u8d8b\u52bf\u4e0e\u9519\u8bef\u7387\u65e0\u5173\u3002", "conclusion": "\u968f\u7740\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5176\u9519\u8bef\u7b54\u6848\u4e2d\u4e0e\u4eba\u7c7b\u63a8\u7406\u504f\u8bef\u4e00\u81f4\u7684\u6bd4\u4f8b\u589e\u52a0\uff0c\u8fd9\u8868\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53d8\u5f97\u66f4\u50cf\u4eba\u7c7b\u8ba4\u77e5\uff0c\u5305\u62ec\u6211\u4eec\u7684\u504f\u89c1\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2506.11129", "pdf": "https://arxiv.org/pdf/2506.11129", "abs": "https://arxiv.org/abs/2506.11129", "authors": ["Carlos Garcia-Fernandez", "Luis Felipe", "Monique Shotande", "Muntasir Zitu", "Aakash Tripathi", "Ghulam Rasool", "Issam El Naqa", "Vivek Rudrapatna", "Gilmer Valdes"], "title": "Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) show promise in healthcare, but hallucinations\nremain a major barrier to clinical use. We present CHECK, a continuous-learning\nframework that integrates structured clinical databases with a classifier\ngrounded in information theory to detect both factual and reasoning-based\nhallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials,\nCHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% -\nmaking an open source model state of the art. Its classifier generalized across\nmedical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE)\nbenchmark and HealthBench realistic multi-turn medical questioning. By\nleveraging hallucination probabilities to guide GPT-4o's refinement and\njudiciously escalate compute, CHECK boosted its USMLE passing rate by 5\npercentage points, achieving a state-of-the-art 92.1%. By suppressing\nhallucinations below accepted clinical error thresholds, CHECK offers a\nscalable foundation for safe LLM deployment in medicine and other high-stakes\ndomains.", "AI": {"tldr": "CHECK is a new method that reduces hallucinations in large language models for clinical use, improving their accuracy and performance on medical benchmarks.", "motivation": "To address the issue of hallucinations in large language models for clinical use.", "method": "A continuous-learning framework integrating structured clinical databases with a classifier grounded in information theory.", "result": "Reduced hallucination rates from 31% to 0.3%, achieved state-of-the-art performance on several medical benchmarks, improved USMLE passing rate by 5 percentage points.", "conclusion": "CHECK significantly reduces hallucination rates and improves performance on various medical benchmarks."}}
{"id": "2506.11130", "pdf": "https://arxiv.org/pdf/2506.11130", "abs": "https://arxiv.org/abs/2506.11130", "authors": ["Cheng Kang Chou", "Chan-Jan Hsu", "Ho-Lam Chung", "Liang-Hsuan Tseng", "Hsi-Chun Cheng", "Yu-Kuan Fu", "Kuan Po Huang", "Hung-Yi Lee"], "title": "A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "We propose a self-refining framework that enhances ASR performance with only\nunlabeled datasets. The process starts with an existing ASR model generating\npseudo-labels on unannotated speech, which are then used to train a\nhigh-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs\nare bootstrapped into the original ASR system, completing the closed-loop\nself-improvement cycle. We demonstrated the effectiveness of the framework on\nTaiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a\nmoderate amount of text data, and synthetic content from the AI models, we\nadapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error\nrates by up to 20% on Mandarin and 50% on Mandarin-English code-switching\nbenchmarks compared to Whisper. Results highlight the framework as a compelling\nalternative to pseudo-labeling self-distillation approaches and provides a\npractical pathway for improving ASR performance in low-resource or\ndomain-specific settings.", "AI": {"tldr": "A self-refining framework is proposed to enhance ASR performance using only unlabeled datasets. This framework improves ASR performance effectively, reducing error rates significantly.", "motivation": "To improve ASR performance with unlabeled datasets in low-resource or domain-specific settings.", "method": "An existing ASR model generates pseudo-labels on unannotated speech, which are then used to train a TTS system. Synthesized speech-text pairs are fed back into the ASR system to complete the self-improvement cycle.", "result": "The framework was proven effective on Taiwanese Mandarin speech. It reduced error rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching benchmarks compared to Whisper.", "conclusion": "This framework is a compelling alternative to pseudo-labeling self-distillation approaches and offers a practical way to improve ASR performance in challenging settings."}}
{"id": "2506.11135", "pdf": "https://arxiv.org/pdf/2506.11135", "abs": "https://arxiv.org/abs/2506.11135", "authors": ["David C. Krakauer", "John W. Krakauer", "Melanie Mitchell"], "title": "Large Language Models and Emergence: A Complex Systems Perspective", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Emergence is a concept in complexity science that describes how many-body\nsystems manifest novel higher-level properties, properties that can be\ndescribed by replacing high-dimensional mechanisms with lower-dimensional\neffective variables and theories. This is captured by the idea \"more is\ndifferent\". Intelligence is a consummate emergent property manifesting\nincreasingly efficient -- cheaper and faster -- uses of emergent capabilities\nto solve problems. This is captured by the idea \"less is more\". In this paper,\nwe first examine claims that Large Language Models exhibit emergent\ncapabilities, reviewing several approaches to quantifying emergence, and\nsecondly ask whether LLMs possess emergent intelligence.", "AI": {"tldr": "This paper discusses the concept of emergence in complex systems and applies it to large language models, examining whether they exhibit emergent capabilities and intelligence.", "motivation": "To explore the notion of emergence in large language models and determine if they possess emergent intelligence.", "method": "Reviewing several approaches to quantifying emergence and examining claims about large language models exhibiting emergent capabilities.", "result": "The paper aims to provide insights into whether large language models possess emergent intelligence.", "conclusion": "The conclusion will summarize the findings on whether large language models exhibit emergent intelligence."}}
{"id": "2506.11137", "pdf": "https://arxiv.org/pdf/2506.11137", "abs": "https://arxiv.org/abs/2506.11137", "authors": ["Chong Shao", "Douglas Snyder", "Chiran Li", "Bowen Gu", "Kerry Ngan", "Chun-Ting Yang", "Jiageng Wu", "Richard Wyss", "Kueiyu Joshua Lin", "Jie Yang"], "title": "Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models", "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Identifying medication discontinuations in electronic health records (EHRs)\nis vital for patient safety but is often hindered by information being buried\nin unstructured notes. This study aims to evaluate the capabilities of advanced\nopen-sourced and proprietary large language models (LLMs) in extracting\nmedications and classifying their medication status from EHR notes, focusing on\ntheir scalability on medication information extraction without human\nannotation. We collected three EHR datasets from diverse sources to build the\nevaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM\nprompting strategies. Performance on medication extraction, medication status\nclassification, and their joint task (extraction then classification) was\nsystematically compared across all experiments. We found that LLMs showed\npromising performance on the medication extraction and discontinuation\nclassification from EHR notes. GPT-4o consistently achieved the highest average\nF1 scores in all tasks under zero-shot setting - 94.0% for medication\nextraction, 78.1% for discontinuation classification, and 72.7% for the joint\ntask. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the\nhighest performance in medication status classification on the MIV-Med dataset\n(68.7%) and in the joint task on both the Re-CASI (76.2%) and MIV-Med (60.2%)\ndatasets. Medical-specific LLMs demonstrated lower performance compared to\nadvanced general-domain LLMs. Few-shot learning generally improved performance,\nwhile CoT reasoning showed inconsistent gains. LLMs demonstrate strong\npotential for medication extraction and discontinuation identification on EHR\nnotes, with open-sourced models offering scalable alternatives to proprietary\nsystems and few-shot can further improve LLMs' capability.", "AI": {"tldr": "This study evaluates advanced open-sourced and proprietary large language models (LLMs) in extracting medications and classifying their status from electronic health record (EHR) notes, aiming at assessing their scalability without human annotation.", "motivation": "To enhance patient safety by identifying medication discontinuations in EHRs which are often hidden in unstructured notes.", "method": "Collecting three EHR datasets to build an evaluation benchmark and evaluating 12 advanced LLMs with multiple prompting strategies.", "result": "GPT-4o achieved the highest average F1 scores in all tasks under zero-shot setting, while open-sourced models also performed well in certain datasets. Few-shot learning generally improved performance.", "conclusion": "LLMs show strong potential for medication extraction and discontinuation identification from EHR notes, with open-sourced models providing scalable alternatives to proprietary systems."}}
{"id": "2506.11243", "pdf": "https://arxiv.org/pdf/2506.11243", "abs": "https://arxiv.org/abs/2506.11243", "authors": ["Santiago G\u00f3ngora", "Ignacio Sastre", "Santiago Robaina", "Ignacio Remersaro", "Luis Chiruzzo", "Aiala Ros\u00e1"], "title": "RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?", "categories": ["cs.CL", "cs.AI"], "comment": "This paper will be presented at the 20th BEA Workshop (Innovative Use\n  of NLP for Building Educational Applications) at ACL 2025", "summary": "In this paper, we present the RETUYT-INCO participation at the BEA 2025\nshared task. Our participation was characterized by the decision of using\nrelatively small models, with fewer than 1B parameters. This self-imposed\nrestriction tries to represent the conditions in which many research labs or\ninstitutions are in the Global South, where computational power is not easily\naccessible due to its prohibitive cost. Even under this restrictive\nself-imposed setting, our models managed to stay competitive with the rest of\nteams that participated in the shared task. According to the $exact\\ F_1$\nscores published by the organizers, the performance gaps between our models and\nthe winners were as follows: $6.46$ in Track 1; $10.24$ in Track 2; $7.85$ in\nTrack 3; $9.56$ in Track 4; and $13.13$ in Track 5. Considering that the\nminimum difference with a winner team is $6.46$ points -- and the maximum\ndifference is $13.13$ -- according to the $exact\\ F_1$ score, we find that\nmodels with a size smaller than 1B parameters are competitive for these tasks,\nall of which can be run on computers with a low-budget GPU or even without a\nGPU.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RETUYT-INCO\u56e2\u961f\u5728BEA 2025\u5171\u4eab\u4efb\u52a1\u4e2d\u7684\u53c2\u4e0e\u60c5\u51b5\uff0c\u4ed6\u4eec\u4f7f\u7528\u4e86\u53c2\u6570\u5c11\u4e8e10\u4ebf\u7684\u5c0f\u578b\u6a21\u578b\uff0c\u5e76\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6210\u7ee9\u3002", "motivation": "\u4ee3\u8868\u5168\u7403\u5357\u65b9\u7814\u7a76\u5b9e\u9a8c\u5ba4\u6216\u673a\u6784\u7684\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u6761\u4ef6", "method": "\u4f7f\u7528\u76f8\u5bf9\u8f83\u5c0f\u7684\u6a21\u578b\uff08\u53c2\u6570\u5c11\u4e8e10\u4ebf\uff09", "result": "\u5728\u4e94\u4e2a\u8d5b\u9053\u4e0a\u7684\u7cbe\u786eF1\u5206\u6570\u4e0e\u51a0\u519b\u56e2\u961f\u76f8\u6bd4\u5dee\u8ddd\u4e3a6.46\u523013.13\u4e0d\u7b49", "conclusion": "\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u5c0f\u4e8e10\u4ebf\u53c2\u6570\uff0c\u4f46RETUYT-INCO\u56e2\u961f\u5728BEA 2025\u5171\u4eab\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4ecd\u7136\u5177\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2506.11244", "pdf": "https://arxiv.org/pdf/2506.11244", "abs": "https://arxiv.org/abs/2506.11244", "authors": ["Shun Shao", "Yftah Ziser", "Zheng Zhao", "Yifu Qiu", "Shay B. Cohen", "Anna Korhonen"], "title": "Iterative Multilingual Spectral Attribute Erasure", "categories": ["cs.CL"], "comment": "8 pages, 3 figures", "summary": "Multilingual representations embed words with similar meanings to share a\ncommon semantic space across languages, creating opportunities to transfer\ndebiasing effects between languages. However, existing methods for debiasing\nare unable to exploit this opportunity because they operate on individual\nlanguages. We present Iterative Multilingual Spectral Attribute Erasure\n(IMSAE), which identifies and mitigates joint bias subspaces across multiple\nlanguages through iterative SVD-based truncation. Evaluating IMSAE across eight\nlanguages and five demographic dimensions, we demonstrate its effectiveness in\nboth standard and zero-shot settings, where target language data is\nunavailable, but linguistically similar languages can be used for debiasing.\nOur comprehensive experiments across diverse language models (BERT, LLaMA,\nMistral) show that IMSAE outperforms traditional monolingual and cross-lingual\napproaches while maintaining model utility.", "AI": {"tldr": "A new method called IMSAE is introduced to identify and reduce bias shared across multiple languages using multilingual representations.", "motivation": "Existing debiasing methods cannot utilize the shared semantic space across languages.", "method": "Iterative Multilingual Spectral Attribute Erasure (IMSAE) uses iterative SVD-based truncation to mitigate joint bias subspaces.", "result": "IMSAE performs well in both standard and zero-shot settings across eight languages and five demographic dimensions.", "conclusion": "IMSAE surpasses traditional approaches in reducing bias while preserving model utility."}}
{"id": "2506.11246", "pdf": "https://arxiv.org/pdf/2506.11246", "abs": "https://arxiv.org/abs/2506.11246", "authors": ["Kushagra Dixit", "Abhishek Rajgaria", "Harshavardhan Kalalbandi", "Dan Roth", "Vivek Gupta"], "title": "No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 19 Tables, 9 Figures", "summary": "Temporal Table Reasoning is a critical challenge for Large Language Models\n(LLMs), requiring effective prompting techniques to extract relevant insights.\nDespite existence of multiple prompting methods, their impact on table\nreasoning remains largely unexplored. Furthermore, the performance of these\nmodels varies drastically across different table and context structures, making\nit difficult to determine an optimal approach. This work investigates multiple\nprompting technique across diverse table types to determine optimal approaches\nfor different scenarios. We find that performance varies based on entity type,\ntable structure, requirement of additional context and question complexity,\nwith NO single method consistently outperforming others. To mitigate these\nchallenges, we introduce SEAR, an adaptive prompting framework inspired by\nhuman reasoning that dynamically adjusts based on context characteristics and\nintegrates a structured reasoning. Our results demonstrate that SEAR achieves\nsuperior performance across all table types compared to other baseline\nprompting techniques. Additionally, we explore the impact of table structure\nrefactoring, finding that a unified representation enhances model's reasoning.", "AI": {"tldr": "This study examines various prompting techniques for temporal table reasoning in large language models and introduces SEAR, which outperforms other methods by adapting to context characteristics and using structured reasoning.", "motivation": "To address the challenge of temporal table reasoning for large language models and determine optimal approaches for different scenarios.", "method": "Investigating multiple prompting techniques and introducing SEAR, an adaptive prompting framework inspired by human reasoning.", "result": "Performance varies based on entity type, table structure, need for additional context, and question complexity, with no single method consistently outperforming others.", "conclusion": "SEAR demonstrates superior performance across various table types compared to other baseline prompting techniques."}}
{"id": "2506.11274", "pdf": "https://arxiv.org/pdf/2506.11274", "abs": "https://arxiv.org/abs/2506.11274", "authors": ["Liran Ringel", "Elad Tolochinsky", "Yaniv Romano"], "title": "Learning a Continue-Thinking Token for Enhanced Test-Time Scaling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time scaling has emerged as an effective approach for improving language\nmodel performance by utilizing additional compute at inference time. Recent\nstudies have shown that overriding end-of-thinking tokens (e.g., replacing\n\"</think>\" with \"Wait\") can extend reasoning steps and improve accuracy. In\nthis work, we explore whether a dedicated continue-thinking token can be\nlearned to trigger extended reasoning. We augment a distilled version of\nDeepSeek-R1 with a single learned \"<|continue-thinking|>\" token, training only\nits embedding via reinforcement learning while keeping the model weights\nfrozen. Our experiments show that this learned token achieves improved accuracy\non standard math benchmarks compared to both the baseline model and a test-time\nscaling approach that uses a fixed token (e.g., \"Wait\") for budget forcing. In\nparticular, we observe that in cases where the fixed-token approach enhances\nthe base model's accuracy, our method achieves a markedly greater improvement.\nFor example, on the GSM8K benchmark, the fixed-token approach yields a 1.3%\nabsolute improvement in accuracy, whereas our learned-token method achieves a\n4.2% improvement over the base model that does not use budget forcing.", "AI": {"tldr": "Introducing a learned continue-thinking token in language models improves mathematical reasoning accuracy more effectively than fixed tokens or test-time scaling approaches.", "motivation": "To explore if a dedicated continue-thinking token can be learned to trigger extended reasoning.", "method": "Augmenting a distilled DeepSeek-R1 model with a learned <continue-thinking> token, training its embedding via reinforcement learning.", "result": "The learned token improves accuracy on standard math benchmarks more than both the baseline model and a test-time scaling approach using a fixed token.", "conclusion": "A learned continue-thinking token can achieve better accuracy than a fixed token or test-time scaling."}}
{"id": "2506.11300", "pdf": "https://arxiv.org/pdf/2506.11300", "abs": "https://arxiv.org/abs/2506.11300", "authors": ["Yang Zhang", "Amr Mohamed", "Hadi Abdine", "Guokan Shang", "Michalis Vazirgiannis"], "title": "Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Curriculum learning has shown promise in improving training efficiency and\ngeneralization in various machine learning domains, yet its potential in\npretraining language models remains underexplored, prompting our work as the\nfirst systematic investigation in this area. We experimented with different\nsettings, including vanilla curriculum learning, pacing-based sampling, and\ninterleaved curricula-guided by six difficulty metrics spanning linguistic and\ninformation-theoretic perspectives. We train models under these settings and\nevaluate their performance on eight diverse benchmarks. Our experiments reveal\nthat curriculum learning consistently improves convergence in early and\nmid-training phases, and can yield lasting gains when used as a warmup strategy\nwith up to $3.5\\%$ improvement. Notably, we identify compression ratio, lexical\ndiversity, and readability as effective difficulty signals across settings. Our\nfindings highlight the importance of data ordering in large-scale pretraining\nand provide actionable insights for scalable, data-efficient model development\nunder realistic training scenarios.", "AI": {"tldr": "Curriculum learning is explored for pretraining language models, showing improvements in convergence and lasting gains.", "motivation": "The potential of curriculum learning in pretraining language models is underexplored.", "method": "Experiments with different curriculum learning settings guided by six difficulty metrics.", "result": "Curriculum learning improves convergence and yields lasting gains, with certain difficulty signals being effective.", "conclusion": "Data ordering is important in large-scale pretraining and provides insights for scalable, data-efficient model development."}}
{"id": "2506.11305", "pdf": "https://arxiv.org/pdf/2506.11305", "abs": "https://arxiv.org/abs/2506.11305", "authors": ["Mohammad Hammoud", "Devang Acharya"], "title": "Don't Pay Attention", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Transformer has become the de facto standard for large language models\nand a wide range of downstream tasks across various domains. Despite its\nnumerous advantages like inherent training parallelism, the Transformer still\nfaces key challenges due to its inability to effectively process sequences\nbeyond a fixed context window and the quadratic complexity of its attention\nmechanism. These challenges have renewed interest in RNN-like architectures,\nwhich offer linear scaling with sequence length and improved handling of\nlong-range dependencies, albeit with limited parallelism due to their\ninherently recurrent nature. In this paper, we propose Avey, a new neural\nfoundational architecture that breaks away from both attention and recurrence.\nAvey comprises a ranker and an autoregressive neural processor, which\ncollaboratively identify and contextualize only the most relevant tokens for\nany given token, regardless of their positions in the sequence. Specifically,\nAvey decouples sequence length from context width, thus enabling effective\nprocessing of arbitrarily long sequences. Experimental results show that Avey\ncompares favorably to the Transformer across a variety of standard short-range\nNLP benchmarks, while notably excelling at capturing long-range dependencies.", "AI": {"tldr": "This paper proposes Avey, a novel neural foundational architecture that does not rely on attention or recurrence mechanisms. It features a ranker and an autoregressive neural processor to focus on relevant tokens, allowing it to handle long sequences effectively. Avey outperforms Transformers in capturing long-range dependencies.", "motivation": "To address the limitations of Transformers in processing long sequences and the quadratic complexity of attention mechanisms, while exploring alternative architectures that can maintain efficiency and effectiveness.", "method": "Introducing Avey, which consists of a ranker and an autoregressive neural processor to identify and contextualize relevant tokens without being constrained by sequence length.", "result": "Avey performs well on standard NLP benchmarks for short-range tasks and surpasses Transformers in capturing long-range dependencies.", "conclusion": "Avey offers a promising alternative to Transformer-based models, particularly for tasks requiring handling long-range dependencies."}}
{"id": "2506.11338", "pdf": "https://arxiv.org/pdf/2506.11338", "abs": "https://arxiv.org/abs/2506.11338", "authors": ["Yi-Chien Lin", "William Schuler"], "title": "Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly", "categories": ["cs.CL"], "comment": null, "summary": "As Transformers become more widely incorporated into natural language\nprocessing tasks, there has been considerable interest in using surprisal from\nthese models as predictors of human sentence processing difficulty. Recent work\nhas observed a positive relationship between Transformer-based models'\nperplexity and the predictive power of their surprisal estimates on reading\ntimes, showing that language models with more parameters and trained on more\ndata are less predictive of human reading times. However, these studies focus\non predicting latency-based measures (i.e., self-paced reading times and\neye-gaze durations) with surprisal estimates from Transformer-based language\nmodels. This trend has not been tested on brain imaging data. This study\ntherefore evaluates the predictive power of surprisal estimates from 17\npre-trained Transformer-based models across three different language families\non two functional magnetic resonance imaging datasets. Results show that the\npositive relationship between model perplexity and model fit still obtains,\nsuggesting that this trend is not specific to latency-based measures and can be\ngeneralized to neural measures.", "AI": {"tldr": "This study examines the predictive power of surprisal estimates from pre-trained Transformer-based models on functional magnetic resonance imaging datasets, finding a positive relationship between model perplexity and model fit.", "motivation": "To evaluate whether the positive relationship between Transformer-based models' perplexity and the predictive power of their surprisal estimates on reading times also applies to neural measures.", "method": "Evaluating the predictive power of surprisal estimates from 17 pre-trained Transformer-based models across three different language families on two functional magnetic resonance imaging datasets.", "result": "The positive relationship between model perplexity and model fit was found to still exist, suggesting the trend is not specific to latency-based measures and can be generalized to neural measures.", "conclusion": "This study extends previous findings by demonstrating that the relationship between Transformer-based models' perplexity and the predictive power of their surprisal estimates can be generalized to neural measures."}}
{"id": "2506.11343", "pdf": "https://arxiv.org/pdf/2506.11343", "abs": "https://arxiv.org/abs/2506.11343", "authors": ["Yaohui Zhang", "Haijing Zhang", "Wenlong Ji", "Tianyu Hua", "Nick Haber", "Hancheng Cao", "Weixin Liang"], "title": "From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review", "categories": ["cs.CL"], "comment": null, "summary": "The advent of large language models (LLMs) offers unprecedented opportunities\nto reimagine peer review beyond the constraints of traditional workflows.\nDespite these opportunities, prior efforts have largely focused on replicating\ntraditional review workflows with LLMs serving as direct substitutes for human\nreviewers, while limited attention has been given to exploring new paradigms\nthat fundamentally rethink how LLMs can participate in the academic review\nprocess. In this paper, we introduce and explore a novel mechanism that employs\nLLM agents to perform pairwise comparisons among manuscripts instead of\nindividual scoring. By aggregating outcomes from substantial pairwise\nevaluations, this approach enables a more accurate and robust measure of\nrelative manuscript quality. Our experiments demonstrate that this comparative\napproach significantly outperforms traditional rating-based methods in\nidentifying high-impact papers. However, our analysis also reveals emergent\nbiases in the selection process, notably a reduced novelty in research topics\nand an increased institutional imbalance. These findings highlight both the\ntransformative potential of rethinking peer review with LLMs and critical\nchallenges that future systems must address to ensure equity and diversity.", "AI": {"tldr": "This paper explores using large language model agents for comparing academic papers pairwisely rather than individual scoring, showing better performance in identifying impactful papers but revealing biases in research topic novelty and institutional balance.", "motivation": "To reimagine peer review processes using large language models beyond traditional workflows and explore new paradigms.", "method": "Using LLM agents to perform pairwise comparisons among manuscripts.", "result": "The comparative approach outperforms traditional rating-based methods in identifying high-impact papers.", "conclusion": "Rethinking peer review with LLMs has transformative potential but also presents challenges related to ensuring equity and diversity."}}
{"id": "2506.11344", "pdf": "https://arxiv.org/pdf/2506.11344", "abs": "https://arxiv.org/abs/2506.11344", "authors": ["Peilin Wu", "Jinho D. Choi"], "title": "Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models", "categories": ["cs.CL"], "comment": null, "summary": "We present a novel approach to Speaker Diarization (SD) by leveraging\ntext-based methods focused on Sentence-level Speaker Change Detection within\ndialogues. Unlike audio-based SD systems, which are often challenged by audio\nquality and speaker similarity, our approach utilizes the dialogue transcript\nalone. Two models are developed: the Single Prediction Model (SPM) and the\nMultiple Prediction Model (MPM), both of which demonstrate significant\nimprovements in identifying speaker changes, particularly in short\nconversations. Our findings, based on a curated dataset encompassing diverse\nconversational scenarios, reveal that the text-based SD approach, especially\nthe MPM, performs competitively against state-of-the-art audio-based SD\nsystems, with superior performance in short conversational contexts. This paper\nnot only showcases the potential of leveraging linguistic features for SD but\nalso highlights the importance of integrating semantic understanding into SD\nsystems, opening avenues for future research in multimodal and semantic\nfeature-based diarization.", "AI": {"tldr": "This paper presents a text-based Speaker Diarization approach using sentence-level speaker change detection models (SPM and MPM), demonstrating competitive performance against audio-based systems, particularly excelling in short conversations.", "motivation": "To overcome the challenges posed by audio quality and speaker similarity in audio-based SD systems, this paper leverages the dialogue transcript alone.", "method": "Utilizing text-based methods focused on Sentence-level Speaker Change Detection within dialogues, two models are developed: Single Prediction Model (SPM) and Multiple Prediction Model (MPM).", "result": "The developed models show significant improvements in identifying speaker changes, particularly in short conversations.", "conclusion": "The text-based Speaker Diarization approach, especially the Multiple Prediction Model, shows competitive performance against state-of-the-art audio-based systems, particularly excelling in short conversations."}}
{"id": "2506.11361", "pdf": "https://arxiv.org/pdf/2506.11361", "abs": "https://arxiv.org/abs/2506.11361", "authors": ["Jack H Fagan", "Ruhaan Juyaal", "Amy Yue-Ming Yu", "Siya Pun"], "title": "The Biased Samaritan: LLM biases in Perceived Kindness", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "While Large Language Models (LLMs) have become ubiquitous in many fields,\nunderstanding and mitigating LLM biases is an ongoing issue. This paper\nprovides a novel method for evaluating the demographic biases of various\ngenerative AI models. By prompting models to assess a moral patient's\nwillingness to intervene constructively, we aim to quantitatively evaluate\ndifferent LLMs' biases towards various genders, races, and ages. Our work\ndiffers from existing work by aiming to determine the baseline demographic\nidentities for various commercial models and the relationship between the\nbaseline and other demographics. We strive to understand if these biases are\npositive, neutral, or negative, and the strength of these biases. This paper\ncan contribute to the objective assessment of bias in Large Language Models and\ngive the user or developer the power to account for these biases in LLM output\nor in training future LLMs. Our analysis suggested two key findings: that\nmodels view the baseline demographic as a white middle-aged or young adult\nmale; however, a general trend across models suggested that non-baseline\ndemographics are more willing to help than the baseline. These methodologies\nallowed us to distinguish these two biases that are often tangled together.", "AI": {"tldr": "This paper presents a new way to measure demographic biases in different generative AI models by having them judge a moral patient's willingness to help. The study finds that most models see a white, middle-aged or young adult male as the standard and that non-standard groups are more likely to offer help.", "motivation": "To evaluate and mitigate biases in large language models.", "method": "Prompting models to assess a moral patient's willingness to intervene constructively.", "result": "Models view the baseline demographic as a white middle-aged or young adult male; non-baseline demographics are more willing to help.", "conclusion": "This paper contributes to the objective assessment of bias in Large Language Models and gives users/developers the ability to account for these biases in LLM output or in training future LLMs."}}
{"id": "2506.11381", "pdf": "https://arxiv.org/pdf/2506.11381", "abs": "https://arxiv.org/abs/2506.11381", "authors": ["Samuel Mensah", "Elena Kochkina", "Jabez Magomere", "Joy Prakash Sain", "Simerjot Kaur", "Charese Smiley"], "title": "A Variational Approach for Mitigating Entity Bias in Relation Extraction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main", "summary": "Mitigating entity bias is a critical challenge in Relation Extraction (RE),\nwhere models often rely excessively on entities, resulting in poor\ngeneralization. This paper presents a novel approach to address this issue by\nadapting a Variational Information Bottleneck (VIB) framework. Our method\ncompresses entity-specific information while preserving task-relevant features.\nIt achieves state-of-the-art performance on relation extraction datasets across\ngeneral, financial, and biomedical domains, in both indomain (original test\nsets) and out-of-domain (modified test sets with type-constrained entity\nreplacements) settings. Our approach offers a robust, interpretable, and\ntheoretically grounded methodology.", "AI": {"tldr": "This paper introduces a new method using VIB to reduce entity bias in Relation Extraction, improving performance across multiple domains.", "motivation": "Mitigating entity bias in Relation Extraction to enhance generalization.", "method": "Adapting a Variational Information Bottleneck (VIB) framework to compress entity-specific info while keeping task-relevant features.", "result": "State-of-the-art performance on relation extraction datasets in various domains and settings.", "conclusion": "Our approach provides a robust, interpretable, and theoretically grounded solution for reducing entity bias in Relation Extraction."}}
{"id": "2506.11389", "pdf": "https://arxiv.org/pdf/2506.11389", "abs": "https://arxiv.org/abs/2506.11389", "authors": ["Karanpartap Singh", "Neil Band", "Ehsan Adeli"], "title": "Curriculum-Guided Layer Scaling for Language Model Pretraining", "categories": ["cs.CL"], "comment": null, "summary": "As the cost of pretraining large language models grows, there is continued\ninterest in strategies to improve learning efficiency during this core training\nstage. Motivated by cognitive development, where humans gradually build\nknowledge as their brains mature, we propose Curriculum-Guided Layer Scaling\n(CGLS), a framework for compute-efficient pretraining that synchronizes\nincreasing data difficulty with model growth through progressive layer stacking\n(i.e. gradually adding layers during training). At the 100M parameter scale,\nusing a curriculum transitioning from synthetic short stories to general web\ndata, CGLS outperforms baseline methods on the question-answering benchmarks\nPIQA and ARC. Pretraining at the 1.2B scale, we stratify the DataComp-LM corpus\nwith a DistilBERT-based classifier and progress from general text to highly\ntechnical or specialized content. Our results show that progressively\nincreasing model depth alongside sample difficulty leads to better\ngeneralization and zero-shot performance on various downstream benchmarks.\nAltogether, our findings demonstrate that CGLS unlocks the potential of\nprogressive stacking, offering a simple yet effective strategy for improving\ngeneralization on knowledge-intensive and reasoning tasks.", "AI": {"tldr": "CGLS improves pretraining efficiency by synchronizing increasing data difficulty with model growth through progressive layer stacking.", "motivation": "To improve learning efficiency during pretraining large language models inspired by human cognitive development.", "method": "Curriculum-Guided Layer Scaling (CGLS) which adds layers progressively during training and uses a curriculum to transition data difficulty.", "result": "CGLS outperforms baselines on PIQA and ARC at 100M parameters and shows better generalization and zero-shot performance on various benchmarks at 1.2B scale.", "conclusion": "CGLS demonstrates the potential of progressive stacking for enhancing generalization on knowledge-intensive and reasoning tasks."}}
{"id": "2506.11410", "pdf": "https://arxiv.org/pdf/2506.11410", "abs": "https://arxiv.org/abs/2506.11410", "authors": ["Wilson Lau", "Youngwon Kim", "Sravanthi Parasa", "Md Enamul Haque", "Anand Oka", "Jay Nanduri"], "title": "Predicting Early-Onset Colorectal Cancer with Large Language Models", "categories": ["cs.CL"], "comment": "Paper accepted for the proceedings of the 2025 American Medical\n  Informatics Association Annual Symposium (AMIA)", "summary": "The incidence rate of early-onset colorectal cancer (EoCRC, age < 45) has\nincreased every year, but this population is younger than the recommended age\nestablished by national guidelines for cancer screening. In this paper, we\napplied 10 different machine learning models to predict EoCRC, and compared\ntheir performance with advanced large language models (LLM), using patient\nconditions, lab results, and observations within 6 months of patient journey\nprior to the CRC diagnoses. We retrospectively identified 1,953 CRC patients\nfrom multiple health systems across the United States. The results demonstrated\nthat the fine-tuned LLM achieved an average of 73% sensitivity and 91%\nspecificity.", "AI": {"tldr": "This study used various machine learning models to predict early-onset colorectal cancer (EoCRC) in people under 45 years old, finding that a fine-tuned large language model performed well with 73% sensitivity and 91% specificity.", "motivation": "To address the rising incidence of early-onset colorectal cancer in younger populations not covered by current screening guidelines.", "method": "Applied 10 different machine learning models and compared their performance with advanced large language models using patient data from six months before diagnosis.", "result": "Fine-tuned LLM showed high sensitivity and specificity in predicting EoCRC.", "conclusion": "Fine-tuned large language models can be effective in predicting early-onset colorectal cancer."}}
{"id": "2506.11418", "pdf": "https://arxiv.org/pdf/2506.11418", "abs": "https://arxiv.org/abs/2506.11418", "authors": ["Jie Hu", "Shengnan Wang", "Yutong He", "Ping Gong", "Jiawei Yi", "Juncheng Zhang", "Youhui Bai", "Renhai Chen", "Gong Zhang", "Cheng Li", "Kun Yuan"], "title": "Efficient Long-Context LLM Inference via KV Cache Clustering", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.", "AI": {"tldr": "This paper presents Chelsea, a framework for online KV cache clustering in large language models with extended context windows.", "motivation": "The substantial KV cache required for long-context LLMs poses significant deployment challenges.", "method": "Chunked Soft Matching, which employs an alternating partition strategy within each chunk and identifies clusters based on similarity.", "result": "Chelsea achieves up to 80% reduction in KV cache memory usage while maintaining comparable model performance.", "conclusion": "Chelsea accelerates the decoding stage of inference by up to 3.19\u00d7 and reduces end-to-end latency by up to 2.72\u00d7."}}
{"id": "2506.11425", "pdf": "https://arxiv.org/pdf/2506.11425", "abs": "https://arxiv.org/abs/2506.11425", "authors": ["Jeff Da", "Clinton Wang", "Xiang Deng", "Yuntao Ma", "Nikhil Barhate", "Sean Hendryx"], "title": "Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted\nas the de facto method for enhancing the reasoning capabilities of large\nlanguage models and has demonstrated notable success in verifiable domains like\nmath and competitive programming tasks. However, the efficacy of RLVR\ndiminishes significantly when applied to agentic environments. These settings,\ncharacterized by multi-step, complex problem solving, lead to high failure\nrates even for frontier LLMs, as the reward landscape is too sparse for\neffective model training via conventional RLVR. In this work, we introduce\nAgent-RLVR, a framework that makes RLVR effective in challenging agentic\nsettings, with an initial focus on software engineering tasks. Inspired by\nhuman pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively\nsteers the agent towards successful trajectories by leveraging diverse\ninformational cues. These cues, ranging from high-level strategic plans to\ndynamic feedback on the agent's errors and environmental interactions, emulate\na teacher's guidance, enabling the agent to navigate difficult solution spaces\nand promotes active self-improvement via additional environment exploration. In\nthe Agent-RLVR training loop, agents first attempt to solve tasks to produce\ninitial trajectories, which are then validated by unit tests and supplemented\nwith agent guidance. Agents then reattempt with guidance, and the agent policy\nis updated with RLVR based on the rewards of these guided trajectories.\nAgent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%\nto 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data\nis additionally useful for test-time reward model training, shown by further\nboosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents\nwith RLVR in complex, real-world environments where conventional RL methods\nstruggle.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgent-RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4ee3\u7406\u5f15\u5bfc\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u65b9\u6cd5\u5728\u4ee3\u7406\u73af\u5883\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5956\u52b1\u666f\u89c2\u8fc7\u4e8e\u7a00\u758f\uff0c\u96be\u4ee5\u6709\u6548\u8bad\u7ec3\u6a21\u578b\u3002", "method": "Agent-RLVR \u6846\u67b6\u901a\u8fc7\u5f15\u5165agent\u5f15\u5bfc\u673a\u5236\uff0c\u8be5\u673a\u5236\u5229\u7528\u591a\u6837\u5316\u7684\u4fe1\u606f\u63d0\u793a\uff08\u4ece\u9ad8\u5c42\u6b21\u7684\u6218\u7565\u8ba1\u5212\u5230\u52a8\u6001\u53cd\u9988\uff09\u6765\u79ef\u6781\u5f15\u5bfc\u4ee3\u7406\u8d70\u5411\u6210\u529f\u7684\u8f68\u8ff9\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u4ece\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u5728\u4ee3\u7406\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u5728SWE-Bench Verified\u4e0a\uff0cQwen-2.5-72B-Instruct\u7684pass@1\u6027\u80fd\u4ece9.4%\u63d0\u9ad8\u523022.4%\uff0c\u5e76\u4e14\u901a\u8fc7\u4f7f\u7528\u589e\u5f3a\u7684RLVR\u6570\u636e\u8fdb\u4e00\u6b65\u63d0\u5347\u523027.8%\u3002", "conclusion": "Agent-RLVR \u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u5728\u590d\u6742\u771f\u5b9e\u73af\u5883\u4e2d\u4f7f\u7528RLVR\u8bad\u7ec3\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.11432", "pdf": "https://arxiv.org/pdf/2506.11432", "abs": "https://arxiv.org/abs/2506.11432", "authors": ["Taeeun Kim", "Semin Jeong", "Youngsook Song"], "title": "KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 2 figures", "summary": "This research introduces KoGEC, a Korean Grammatical Error Correction system\nusing pre\\--trained translation models. We fine-tuned NLLB (No Language Left\nBehind) models for Korean GEC, comparing their performance against large\nlanguage models like GPT-4 and HCX-3. The study used two social media\nconversation datasets for training and testing. The NLLB models were fine-tuned\nusing special language tokens to distinguish between original and corrected\nKorean sentences. Evaluation was done using BLEU scores and an \"LLM as judge\"\nmethod to classify error types. Results showed that the fine-tuned NLLB (KoGEC)\nmodels outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a\nmore balanced error correction profile across various error types, whereas the\nlarger LLMs tended to focus less on punctuation errors. We also developed a\nChrome extension to make the KoGEC system accessible to users. Finally, we\nexplored token vocabulary expansion to further improve the model but found it\nto decrease model performance. This research contributes to the field of NLP by\nproviding an efficient, specialized Korean GEC system and a new evaluation\nmethod. It also highlights the potential of compact, task-specific models to\ncompete with larger, general-purpose language models in specialized NLP tasks.", "AI": {"tldr": "This study presents KoGEC, a Korean grammatical error correction system using fine-tuned NLLB models, which outperform larger language models in correcting Korean sentences across various error types.", "motivation": "To develop an efficient and specialized Korean GEC system that can perform better than large language models in specific NLP tasks.", "method": "Fine-tuning NLLB models with special language tokens on social media conversation datasets and evaluating them using BLEU scores and an 'LLM as judge' method.", "result": "The fine-tuned NLLB models (KoGEC) outperformed GPT-4 and HCX-3 in Korean GEC tasks, showing a more balanced error correction profile across different error types. Token vocabulary expansion was attempted but found to decrease model performance.", "conclusion": "This research provides a specialized Korean GEC system and a new evaluation method, demonstrating the effectiveness of compact, task-specific models in specialized NLP tasks."}}
{"id": "2506.11440", "pdf": "https://arxiv.org/pdf/2506.11440", "abs": "https://arxiv.org/abs/2506.11440", "authors": ["Harvey Yiyun Fu", "Aryan Shrivastava", "Jared Moore", "Peter West", "Chenhao Tan", "Ari Holtzman"], "title": "AbsenceBench: Language Models Can't Tell What's Missing", "categories": ["cs.CL"], "comment": "23 pages, 8 figures. Code and data are publicly available at\n  https://github.com/harvey-fin/absence-bench", "summary": "Large language models (LLMs) are increasingly capable of processing long\ninputs and locating specific information within them, as evidenced by their\nperformance on the Needle in a Haystack (NIAH) test. However, while models\nexcel at recalling surprising information, they still struggle to identify\nclearly omitted information. We introduce AbsenceBench to assesses LLMs'\ncapacity to detect missing information across three domains: numerical\nsequences, poetry, and GitHub pull requests. AbsenceBench asks models to\nidentify which pieces of a document were deliberately removed, given access to\nboth the original and edited contexts. Despite the apparent straightforwardness\nof these tasks, our experiments reveal that even state-of-the-art models like\nClaude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context\nlength of 5K tokens. Our analysis suggests this poor performance stems from a\nfundamental limitation: Transformer attention mechanisms cannot easily attend\nto \"gaps\" in documents since these absences don't correspond to any specific\nkeys that can be attended to. Overall, our results and analysis provide a case\nstudy of the close proximity of tasks where models are already superhuman\n(NIAH) and tasks where models breakdown unexpectedly (AbsenceBench).", "AI": {"tldr": "This paper introduces AbsenceBench to evaluate large language models' ability to detect missing information in three domains: numerical sequences, poetry, and GitHub pull requests.", "motivation": "The motivation is to assess the capacity of LLMs to identify deliberately removed pieces of a document given both original and edited contexts.", "method": "The method involves creating AbsenceBench and conducting experiments to measure the F1-scores of state-of-the-art models like Claude-3.7-Sonnet.", "result": "The result shows that even advanced models have poor performance with an average F1-score of 69.6% for a modest context length of 5K tokens.", "conclusion": "The conclusion highlights the fundamental limitation of Transformer attention mechanisms in attending to 'gaps' in documents, providing a case study on the gap between tasks where models excel and those where they fail."}}
{"id": "2506.11467", "pdf": "https://arxiv.org/pdf/2506.11467", "abs": "https://arxiv.org/abs/2506.11467", "authors": ["Carlos Rafael Catalan"], "title": "A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems", "categories": ["cs.CL", "cs.SI", "F.2.2, I.2.7"], "comment": "7 pages, 7 figures, presented at the HEAL Workshop at CHI", "summary": "Human evaluators provide necessary contributions in evaluating large language\nmodels. In the context of Machine Translation (MT) systems for low-resource\nlanguages (LRLs), this is made even more apparent since popular automated\nmetrics tend to be string-based, and therefore do not provide a full picture of\nthe nuances of the behavior of the system. Human evaluators, when equipped with\nthe necessary expertise of the language, will be able to test for adequacy,\nfluency, and other important metrics. However, the low resource nature of the\nlanguage means that both datasets and evaluators are in short supply. This\npresents the following conundrum: How can developers of MT systems for these\nLRLs find adequate human evaluators and datasets? This paper first presents a\ncomprehensive review of existing evaluation procedures, with the objective of\nproducing a design proposal for a platform that addresses the resource gap in\nterms of datasets and evaluators in developing MT systems. The result is a\ndesign for a recruitment and gamified evaluation platform for developers of MT\nsystems. Challenges are also discussed in terms of evaluating this platform, as\nwell as its possible applications in the wider scope of Natural Language\nProcessing (NLP) research.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u62db\u8058\u548c\u6e38\u620f\u5316\u8bc4\u4f30\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684MT\u7cfb\u7edf\u5f00\u53d1\u4e2d\u6570\u636e\u548c\u8bc4\u4ef7\u5458\u77ed\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bc4\u4f30\u8be5\u5e73\u53f0\u53ca\u5728NLP\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u4eba\u7c7b\u8bc4\u4ef7\u5458\u7684\u5fc5\u8981\u8d21\u732e\u3002\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08LRLs\uff09\u7684\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u7cfb\u7edf\u6765\u8bf4\u5c24\u5176\u5982\u6b64\uff0c\u56e0\u4e3a\u6d41\u884c\u7684\u81ea\u52a8\u5316\u6307\u6807\u5f80\u5f80\u662f\u57fa\u4e8e\u5b57\u7b26\u4e32\u7684\uff0c\u4e0d\u80fd\u5168\u9762\u53cd\u6620\u7cfb\u7edf\u7684\u590d\u6742\u884c\u4e3a\u3002\u5177\u5907\u8bed\u8a00\u4e13\u4e1a\u77e5\u8bc6\u7684\u4eba\u7c7b\u8bc4\u4ef7\u5458\u53ef\u4ee5\u6d4b\u8bd5\u51c6\u786e\u6027\u3001\u6d41\u7545\u6027\u7b49\u91cd\u8981\u6307\u6807\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4ef7\u5458\u90fd\u4f9b\u4e0d\u5e94\u6c42\uff0c\u5982\u4f55\u627e\u5230\u8db3\u591f\u7684\u8bc4\u4ef7\u5458\u548c\u6570\u636e\u96c6\u6210\u4e3a\u4e86\u4e00\u4e2a\u96be\u9898\u3002", "method": "\u672c\u6587\u9996\u5148\u7efc\u8ff0\u4e86\u73b0\u6709\u7684\u8bc4\u4f30\u7a0b\u5e8f\uff0c\u76ee\u7684\u662f\u4e3a\u5f00\u53d1\u4e00\u4e2a\u89e3\u51b3MT\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u6570\u636e\u548c\u8bc4\u4ef7\u5458\u77ed\u7f3a\u95ee\u9898\u7684\u5e73\u53f0\u63d0\u51fa\u8bbe\u8ba1\u5efa\u8bae\u3002\u6700\u7ec8\u8bbe\u8ba1\u51fa\u4e00\u4e2a\u9488\u5bf9MT\u7cfb\u7edf\u5f00\u53d1\u8005\u62db\u8058\u548c\u6e38\u620f\u5316\u8bc4\u4f30\u5e73\u53f0\u3002\u6b64\u5916\u8fd8\u8ba8\u8bba\u4e86\u8bc4\u4f30\u8be5\u5e73\u53f0\u7684\u6311\u6218\u53ca\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "result": "\u8bbe\u8ba1\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u62db\u8058\u548c\u6e38\u620f\u5316\u8bc4\u4f30\u7684MT\u7cfb\u7edf\u5e73\u53f0\uff0c\u65e8\u5728\u7f13\u89e3\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6570\u636e\u548c\u8bc4\u4ef7\u5458\u77ed\u7f3a\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u62db\u8058\u548c\u6e38\u620f\u5316\u8bc4\u4f30\u5e73\u53f0\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0bMT\u7cfb\u7edf\u5f00\u53d1\u4e2d\u7684\u6570\u636e\u548c\u8bc4\u4ef7\u5458\u77ed\u7f3a\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bc4\u4f30\u8be5\u5e73\u53f0\u7684\u6311\u6218\u53ca\u5176\u5728NLP\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2506.11474", "pdf": "https://arxiv.org/pdf/2506.11474", "abs": "https://arxiv.org/abs/2506.11474", "authors": ["Jaehoon Yun", "Jiwoong Sohn", "Jungwoo Park", "Hyunjae Kim", "Xiangru Tang", "Yanjun Shao", "Yonghoe Koo", "Minhyeok Ko", "Qingyu Chen", "Mark Gerstein", "Michael Moor", "Jaewoo Kang"], "title": "Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have shown promise in clinical decision making, but\ncurrent approaches struggle to localize and correct errors at specific steps of\nthe reasoning process. This limitation is critical in medicine, where\nidentifying and addressing reasoning errors is essential for accurate diagnosis\nand effective patient care. We introduce Med-PRM, a process reward modeling\nframework that leverages retrieval-augmented generation to verify each\nreasoning step against established medical knowledge bases. By verifying\nintermediate reasoning steps with evidence retrieved from clinical guidelines\nand literature, our model can precisely assess the reasoning quality in a\nfine-grained manner. Evaluations on five medical QA benchmarks and two\nopen-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art\nperformance, with improving the performance of base models by up to 13.50%\nusing Med-PRM. Moreover, we demonstrate the generality of Med-PRM by\nintegrating it in a plug-and-play fashion with strong policy models such as\nMeerkat, achieving over 80\\% accuracy on MedQA for the first time using\nsmall-scale models of 8 billion parameters. Our code and data are available at:\nhttps://med-prm.github.io/", "AI": {"tldr": "Med-PRM\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u9a8c\u8bc1\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\uff0c\u4ece\u800c\u63d0\u9ad8\u533b\u7597\u95ee\u7b54\u548c\u8bca\u65ad\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u4e2d\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u548c\u4fee\u6b63\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\uff0c\u800c\u51c6\u786e\u8bc6\u522b\u548c\u7ea0\u6b63\u8fd9\u4e9b\u9519\u8bef\u5bf9\u6b63\u786e\u7684\u8bca\u65ad\u548c\u60a3\u8005\u62a4\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faMed-PRM\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u4e34\u5e8a\u6307\u5357\u548c\u6587\u732e\u4e2d\u68c0\u7d22\u8bc1\u636e\u6765\u9a8c\u8bc1\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff0c\u4ee5\u6b64\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\u3002", "result": "\u5728\u4e94\u4e2a\u533b\u7597QA\u57fa\u51c6\u548c\u4e24\u4e2a\u5f00\u653e\u6027\u8bca\u65ad\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u6027\u80fd\u6700\u9ad8\u8fbe13.50%\uff0c\u5e76\u4e14\u9996\u6b21\u4f7f\u752880\u4ebf\u53c2\u6570\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u5728MedQA\u4e0a\u8fbe\u5230\u4e86\u8d85\u8fc780%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "Med-PRM\u5c55\u793a\u4e86\u5728\u6539\u8fdb\u4e34\u5e8a\u51b3\u7b56\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u5176\u901a\u7528\u6027\u5141\u8bb8\u4e0e\u5f3a\u5927\u653f\u7b56\u6a21\u578b\u5982Meerkat\u96c6\u6210\uff0c\u540c\u65f6\u4ee3\u7801\u548c\u6570\u636e\u8d44\u6e90\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2506.11478", "pdf": "https://arxiv.org/pdf/2506.11478", "abs": "https://arxiv.org/abs/2506.11478", "authors": ["Aman Sinha", "Bogdan-Valentin Popescu", "Xavier Coubez", "Marianne Clausel", "Mathieu Constant"], "title": "ImmunoFOMO: Are Language Models missing what oncologists see?", "categories": ["cs.CL"], "comment": null, "summary": "Language models (LMs) capabilities have grown with a fast pace over the past\ndecade leading researchers in various disciplines, such as biomedical research,\nto increasingly explore the utility of LMs in their day-to-day applications.\nDomain specific language models have already been in use for biomedical natural\nlanguage processing (NLP) applications. Recently however, the interest has\ngrown towards medical language models and their understanding capabilities. In\nthis paper, we investigate the medical conceptual grounding of various language\nmodels against expert clinicians for identification of hallmarks of\nimmunotherapy in breast cancer abstracts. Our results show that pre-trained\nlanguage models have potential to outperform large language models in\nidentifying very specific (low-level) concepts.", "AI": {"tldr": "This study investigates medical conceptual grounding of different language models compared to expert clinicians for identifying immunotherapy hallmarks in breast cancer abstracts.", "motivation": "Exploring the utility of language models in biomedical research applications.", "method": "Comparing pre-trained and large language models' performance in identifying specific concepts related to immunotherapy in breast cancer abstracts.", "result": "Pre-trained language models show potential to outperform large language models in identifying very specific (low-level) concepts.", "conclusion": "Pre-trained language models could be more effective than large language models for certain biomedical NLP tasks."}}
{"id": "2506.11485", "pdf": "https://arxiv.org/pdf/2506.11485", "abs": "https://arxiv.org/abs/2506.11485", "authors": ["Cole Gawin"], "title": "Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures, 3 tables", "summary": "While large language models like BERT demonstrate strong empirical\nperformance on semantic tasks, whether this reflects true conceptual competence\nor surface-level statistical association remains unclear. I investigate whether\nBERT encodes abstract relational schemata by examining internal representations\nof concept pairs across taxonomic, mereological, and functional relations. I\ncompare BERT's relational classification performance with representational\nstructure in [CLS] token embeddings. Results reveal that pretrained BERT\nenables high classification accuracy, indicating latent relational signals.\nHowever, concept pairs organize by relation type in high-dimensional embedding\nspace only after fine-tuning on supervised relation classification tasks. This\nindicates relational schemata are not emergent from pretraining alone but can\nbe induced via task scaffolding. These findings demonstrate that behavioral\nperformance does not necessarily imply structured conceptual understanding,\nthough models can acquire inductive biases for grounded relational abstraction\nthrough appropriate training.", "AI": {"tldr": "This study investigates whether BERT encodes abstract relational schemata by examining its internal representations of concept pairs across different types of relations. It finds that while BERT can achieve high classification accuracy without fine-tuning, relational schemata emerge only after supervised fine-tuning, suggesting that task-specific training is necessary for acquiring structured conceptual understanding.", "motivation": "To determine if BERT demonstrates true conceptual competence or merely surface-level statistical associations.", "method": "Examining internal representations of concept pairs in BERT across taxonomic, mereological, and functional relations and comparing relational classification performance with representational structure in [CLS] token embeddings.", "result": "Pretrained BERT enables high classification accuracy, indicating latent relational signals. However, concept pairs organize by relation type in high-dimensional embedding space only after fine-tuning on supervised relation classification tasks.", "conclusion": "Behavioral performance does not necessarily imply structured conceptual understanding, but models can acquire inductive biases for grounded relational abstraction through appropriate training."}}
{"id": "2506.11498", "pdf": "https://arxiv.org/pdf/2506.11498", "abs": "https://arxiv.org/abs/2506.11498", "authors": ["Manlai Liang", "Wanyi Huang", "Mandi Liu", "Huaijun Li", "Jinlong Li"], "title": "Lag-Relative Sparse Attention In Long Context Training", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.", "AI": {"tldr": "This paper proposes a new method called Lag-Relative Sparse Attention (LRSA) to improve the performance of large language models with key-value compression.", "motivation": "The quadratic complexity of attention computation and linear-increasing key-value memory footprint limit the ability of large language models to handle long-context input.", "method": "LRSA uses the LagKV compression method to perform chunk-by-chunk prefilling, selecting the top K most relevant key-value pairs in a fixed-size lagging window.", "result": "Experimental results show that LRSA significantly enhances the robustness of LLMs with key-value compression and achieves better fine-tuned results in the question-answer tuning task.", "conclusion": "LRSA is an effective way to improve the performance of large language models with key-value compression without adding extra parameters or computation overhead."}}
{"id": "2506.11499", "pdf": "https://arxiv.org/pdf/2506.11499", "abs": "https://arxiv.org/abs/2506.11499", "authors": ["Seongbo Jang", "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu"], "title": "On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval", "categories": ["cs.CL"], "comment": "9 pages, 1 figure", "summary": "Multimodal chatbots have become one of the major topics for dialogue systems\nin both research community and industry. Recently, researchers have shed light\non the multimodality of responses as well as dialogue contexts. This work\nexplores how a dialogue system can output responses in various modalities such\nas text and image. To this end, we first formulate a multimodal dialogue\nresponse retrieval task for retrieval-based systems as the combination of three\nsubtasks. We then propose three integration methods based on a two-step\napproach and an end-to-end approach, and compare the merits and demerits of\neach method. Experimental results on two datasets demonstrate that the\nend-to-end approach achieves comparable performance without an intermediate\nstep in the two-step approach. In addition, a parameter sharing strategy not\nonly reduces the number of parameters but also boosts performance by\ntransferring knowledge across the subtasks and the modalities.", "AI": {"tldr": "This work explores how to generate multimodal responses in dialogue systems, proposing three integration methods and demonstrating that the end-to-end approach performs comparably without an intermediate step.", "motivation": "To explore the potential of multimodal chatbots in dialogue systems.", "method": "Formulating a multimodal dialogue response retrieval task and proposing three integration methods based on a two-step and an end-to-end approach.", "result": "The end-to-end approach achieves comparable performance without an intermediate step, and a parameter sharing strategy reduces parameters and boosts performance.", "conclusion": "This study shows the effectiveness of the end-to-end approach and parameter sharing strategy in multimodal dialogue systems."}}
{"id": "2506.11557", "pdf": "https://arxiv.org/pdf/2506.11557", "abs": "https://arxiv.org/abs/2506.11557", "authors": ["Chih-Hao Hsu", "Ying-Jia Lin", "Hung-Yu Kao"], "title": "From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation", "categories": ["cs.CL"], "comment": "Accepted by PAKDD 2025", "summary": "In dialogue generation, the naturalness of responses is crucial for effective\nhuman-machine interaction. Personalized response generation poses even greater\nchallenges, as the responses must remain coherent and consistent with the\nuser's personal traits or persona descriptions. We propose MUDI\n($\\textbf{Mu}$ltiple $\\textbf{Di}$scourse Relations Graph Learning) for\npersonalized dialogue generation. We utilize a Large Language Model to assist\nin annotating discourse relations and to transform dialogue data into\nstructured dialogue graphs. Our graph encoder, the proposed DialogueGAT model,\nthen captures implicit discourse relations within this structure, along with\npersona descriptions. During the personalized response generation phase, novel\ncoherence-aware attention strategies are implemented to enhance the decoder's\nconsideration of discourse relations. Our experiments demonstrate significant\nimprovements in the quality of personalized responses, thus resembling\nhuman-like dialogue exchanges.", "AI": {"tldr": "This paper proposes MUDI, a method that uses a Large Language Model to annotate discourse relations and transform dialogue data into structured dialogue graphs. It then uses the DialogueGAT model to capture implicit discourse relations and persona descriptions, improving the quality of personalized responses.", "motivation": "To improve the naturalness and coherence of responses in personalized dialogue generation.", "method": "Utilizing a Large Language Model to annotate discourse relations and transform dialogue data into structured dialogue graphs. Using the DialogueGAT model to capture implicit discourse relations and persona descriptions.", "result": "Significant improvements in the quality of personalized responses.", "conclusion": "MUDI can enhance the coherence and consistency of personalized responses in human-machine interaction."}}
{"id": "2506.11602", "pdf": "https://arxiv.org/pdf/2506.11602", "abs": "https://arxiv.org/abs/2506.11602", "authors": ["Hawau Olamide Toyin", "Samar M. Magdy", "Hanan Aldarmaki"], "title": "Are LLMs Good Text Diacritizers? An Arabic and Yor\u00f9b\u00e1 Case Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate the effectiveness of large language models (LLMs) for text\ndiacritization in two typologically distinct languages: Arabic and Yoruba. To\nenable a rigorous evaluation, we introduce a novel multilingual dataset\nMultiDiac, with diverse samples that capture a range of diacritic ambiguities.\nWe evaluate 14 LLMs varying in size, accessibility, and language coverage, and\nbenchmark them against 6 specialized diacritization models. Additionally, we\nfine-tune four small open-source models using LoRA for Yoruba. Our results show\nthat many off-the-shelf LLMs outperform specialized diacritization models for\nboth Arabic and Yoruba, but smaller models suffer from hallucinations.\nFine-tuning on a small dataset can help improve diacritization performance and\nreduce hallucination rates.", "AI": {"tldr": "Investigate the effectiveness of large language models (LLMs) for text diacritization in Arabic and Yoruba, introducing MultiDiac dataset, evaluating 14 LLMs and benchmarking them against 6 specialized models, finding that many LLMs outperform specialized models but smaller models have hallucination problems; fine-tuning small models improves performance and reduces hallucination.", "motivation": "To explore whether large language models are effective for text diacritization in two typologically distinct languages, Arabic and Yoruba.", "method": "Introduce a novel multilingual dataset called MultiDiac, evaluate 14 LLMs of varying sizes, accessibility, and language coverage, benchmark them against 6 specialized diacritization models, and fine-tune four small open-source models using LoRA for Yoruba.", "result": "Many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yoruba, but smaller models suffer from hallucinations. Fine-tuning on a small dataset can help improve diacritization performance and reduce hallucination rates.", "conclusion": "Off-the-shelf large language models are effective for text diacritization in Arabic and Yoruba, but smaller models may produce hallucinations. Fine-tuning can enhance performance and reduce hallucinations."}}
{"id": "2506.11631", "pdf": "https://arxiv.org/pdf/2506.11631", "abs": "https://arxiv.org/abs/2506.11631", "authors": ["Simeon Junker", "Sina Zarrie\u00df"], "title": "SceneGram: Conceptualizing and Describing Tangrams in Scene Context", "categories": ["cs.CL"], "comment": "To appear in ACL Findings 2025", "summary": "Research on reference and naming suggests that humans can come up with very\ndifferent ways of conceptualizing and referring to the same object, e.g. the\nsame abstract tangram shape can be a \"crab\", \"sink\" or \"space ship\". Another\ncommon assumption in cognitive science is that scene context fundamentally\nshapes our visual perception of objects and conceptual expectations. This paper\ncontributes SceneGram, a dataset of human references to tangram shapes placed\nin different scene contexts, allowing for systematic analyses of the effect of\nscene context on conceptualization. Based on this data, we analyze references\nto tangram shapes generated by multimodal LLMs, showing that these models do\nnot account for the richness and variability of conceptualizations found in\nhuman references.", "AI": {"tldr": "\u63d0\u51faSceneGram\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u573a\u666f\u80cc\u666f\u5bf9\u4e03\u5de7\u677f\u5f62\u72b6\u6982\u5ff5\u5316\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u591a\u6a21\u6001LLMs\u672a\u80fd\u5145\u5206\u53cd\u6620\u4eba\u7c7b\u6982\u5ff5\u5316\u7684\u591a\u6837\u6027\u3002", "motivation": "\u7814\u7a76\u53c2\u8003\u548c\u547d\u540d\u65b9\u9762\u7684\u5185\u5bb9\uff0c\u63a2\u8ba8\u4eba\u7c7b\u5bf9\u540c\u4e00\u7269\u4f53\u53ef\u4ee5\u6709\u975e\u5e38\u4e0d\u540c\u7684\u6982\u5ff5\u5316\u548c\u5f15\u7528\u65b9\u5f0f\u3002\u53e6\u4e00\u4e2a\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u5e38\u89c1\u5047\u8bbe\u662f\u573a\u666f\u80cc\u666f\u4ece\u6839\u672c\u4e0a\u5f71\u54cd\u6211\u4eec\u5bf9\u7269\u4f53\u7684\u89c6\u89c9\u611f\u77e5\u548c\u6982\u5ff5\u671f\u671b\u3002\u56e0\u6b64\uff0c\u6784\u5efa\u4e00\u4e2a\u6570\u636e\u96c6\u6765\u7cfb\u7edf\u5206\u6790\u573a\u666f\u80cc\u666f\u5bf9\u6982\u5ff5\u5316\u7684\u5f71\u54cd\u3002", "method": "\u521b\u5efaSceneGram\u6570\u636e\u96c6\uff0c\u5305\u542b\u4eba\u7c7b\u5bf9\u653e\u7f6e\u5728\u4e0d\u540c\u573a\u666f\u80cc\u666f\u4e0b\u7684\u4e03\u5de7\u677f\u5f62\u72b6\u7684\u5f15\u7528\uff0c\u5e76\u5206\u6790\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4e03\u5de7\u677f\u5f62\u72b6\u7684\u5f15\u7528\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u6ca1\u6709\u8003\u8651\u5230\u4eba\u7c7b\u5f15\u7528\u4e2d\u53d1\u73b0\u7684\u6982\u5ff5\u5316\u7684\u4e30\u5bcc\u6027\u548c\u53d8\u5f02\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u573a\u666f\u80cc\u666f\u5bf9\u6982\u5ff5\u5316\u7684\u5f71\u54cd\u65b9\u9762\u5b58\u5728\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.11638", "pdf": "https://arxiv.org/pdf/2506.11638", "abs": "https://arxiv.org/abs/2506.11638", "authors": ["Yicheng Xiao", "Lin Song", "Rui Yang", "Cheng Cheng", "Yixiao Ge", "Xiu Li", "Ying Shan"], "title": "LoRA-Gen: Specializing Large Language Model via Online LoRA Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances have highlighted the benefits of scaling language models to\nenhance performance across a wide range of NLP tasks. However, these approaches\nstill face limitations in effectiveness and efficiency when applied to\ndomain-specific tasks, particularly for small edge-side models. We propose the\nLoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA\nparameters for edge-side models based on task descriptions. By employing the\nreparameterization technique, we merge the LoRA parameters into the edge-side\nmodel to achieve flexible specialization. Our method facilitates knowledge\ntransfer between models while significantly improving the inference efficiency\nof the specialized model by reducing the input context length. Without\nspecialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which\nachieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in\nreasoning tasks. Besides, our method delivers a compression ratio of 10.1x with\nGemma-2B on intelligent agent tasks.", "AI": {"tldr": "This paper introduces LoRA-Gen, a framework that uses a large cloud-side model to generate LoRA parameters for edge-side models based on task descriptions. It improves both effectiveness and efficiency for domain-specific tasks.", "motivation": "To overcome the limitations in effectiveness and efficiency when applying scaling language models to domain-specific tasks, especially for small edge-side models.", "method": "LoRA-Gen framework utilizing reparameterization technique to merge LoRA parameters into the edge-side model for flexible specialization.", "result": "LoRA-Gen outperforms conventional LoRA fine-tuning without specialized training, achieving competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in reasoning tasks, and delivers a compression ratio of 10.1x with Gemma-2B on intelligent agent tasks.", "conclusion": "LoRA-Gen is an effective and efficient approach to enhance the performance of edge-side models for domain-specific tasks."}}
{"id": "2506.11666", "pdf": "https://arxiv.org/pdf/2506.11666", "abs": "https://arxiv.org/abs/2506.11666", "authors": ["Pietro Ferrazzi", "Alberto Lavelli", "Bernardo Magnini"], "title": "Converting Annotated Clinical Cases into Structured Case Report Forms", "categories": ["cs.CL", "cs.AI"], "comment": "to be published in BioNLP 2025", "summary": "Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166", "AI": {"tldr": "We propose a semi-automatic method to convert existing datasets into CRF datasets, creating a new, high-quality dataset for CRF slot filling.", "motivation": "Publicly available, well-annotated CRF datasets are scarce, limiting the development of CRF slot filling systems.", "method": "Semi-automatic conversion methodology applied to the E3C dataset in two languages (English and Italian).", "result": "Slot filling achieves 59.7% for Italian and 67.3% for English on a closed Large Language Models (zero-shot) and worse performances on open-source models.", "conclusion": "Filling CRFs is challenging even for recent state-of-the-art LLMs."}}
{"id": "2506.11673", "pdf": "https://arxiv.org/pdf/2506.11673", "abs": "https://arxiv.org/abs/2506.11673", "authors": ["Alicja Dobrzeniecka", "Antske Fokkens", "Pia Sommerauer"], "title": "Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Amnesic probing is a technique used to examine the influence of specific\nlinguistic information on the behaviour of a model. This involves identifying\nand removing the relevant information and then assessing whether the model's\nperformance on the main task changes. If the removed information is relevant,\nthe model's performance should decline. The difficulty with this approach lies\nin removing only the target information while leaving other information\nunchanged. It has been shown that Iterative Nullspace Projection (INLP), a\nwidely used removal technique, introduces random modifications to\nrepresentations when eliminating target information. We demonstrate that Mean\nProjection (MP) and LEACE, two proposed alternatives, remove information in a\nmore targeted manner, thereby enhancing the potential for obtaining behavioural\nexplanations through Amnesic Probing.", "AI": {"tldr": "Amnesic probing is a method to evaluate the impact of certain linguistic information on a model's behavior by removing it and observing changes in performance. This study compares different techniques for removing target information.", "motivation": "To improve the effectiveness of amnesic probing by finding better ways to remove specific linguistic information without affecting other aspects.", "method": "Comparing INLP, Mean Projection (MP), and LEACE methods in terms of their ability to specifically remove target information.", "result": "Mean Projection (MP) and LEACE are found to be more effective than INLP at removing specific information without introducing random modifications.", "conclusion": "Using Mean Projection (MP) and LEACE can enhance the potential for obtaining behavioral explanations through amnesic probing."}}
{"id": "2506.11681", "pdf": "https://arxiv.org/pdf/2506.11681", "abs": "https://arxiv.org/abs/2506.11681", "authors": ["Pratibha Zunjare", "Michael Hsiao"], "title": "LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting Approach", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task.", "AI": {"tldr": "This paper proposes a hybrid approach using advanced prompting and multi-agent architectures to simplify complex sentences for video game design, achieving a 70% success rate compared to 48% with a single-agent approach.", "motivation": "The motivation of this paper is to address the challenge of transforming complex sentences into sequences of logical, simplified sentences while maintaining semantic and logical integrity using Large Language Models.", "method": "A hybrid approach combining advanced prompting with multi-agent architectures is proposed to enhance the sentence simplification process.", "result": "The proposed approach was able to successfully simplify 70% of the complex sentences for video game design application, outperforming the single-agent approach which attained a 48% success rate.", "conclusion": "This paper concludes by demonstrating the effectiveness of the hybrid approach over the single-agent approach in simplifying complex sentences."}}
{"id": "2506.11702", "pdf": "https://arxiv.org/pdf/2506.11702", "abs": "https://arxiv.org/abs/2506.11702", "authors": ["V\u00edctor Gallego"], "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025 Workshop on Models of Human Feedback for AI\n  Alignment", "summary": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning", "AI": {"tldr": "A novel framework named Configurable Preference Tuning (CPT) is introduced to allow language models to adjust their behavior dynamically based on human interpretable directives, improving adaptability compared to existing models like DPO.", "motivation": "Existing models have a fixed set of preferences which limits adaptability.", "method": "CPT leverages synthetically generated preference data guided by structured rubrics defining desired attributes.", "result": "CPT allows language models to modulate outputs at inference time without retraining.", "conclusion": "This approach offers fine-grained control and models nuanced human feedback."}}
{"id": "2506.11728", "pdf": "https://arxiv.org/pdf/2506.11728", "abs": "https://arxiv.org/abs/2506.11728", "authors": ["H\u00e9ctor Mart\u00ednez", "Adri\u00e1n Castell\u00f3", "Francisco D. Igual", "Enrique S. Quintana-Ort\u00ed"], "title": "The Cambrian Explosion of Mixed-Precision Matrix Multiplication for Quantized Deep Learning Inference", "categories": ["cs.CL"], "comment": "16 pages, 7 tables, 7 figures", "summary": "Recent advances in deep learning (DL) have led to a shift from traditional\n64-bit floating point (FP64) computations toward reduced-precision formats,\nsuch as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision\narithmetic. This transition enhances computational throughput, reduces memory\nand bandwidth usage, and improves energy efficiency, offering significant\nadvantages for resource-constrained edge devices. To support this shift,\nhardware architectures have evolved accordingly, now including adapted ISAs\n(Instruction Set Architectures) that expose mixed-precision vector units and\nmatrix engines tailored for DL workloads. At the heart of many DL and\nscientific computing tasks is the general matrix-matrix multiplication gemm, a\nfundamental kernel historically optimized using axpy vector instructions on\nSIMD (single instruction, multiple data) units. However, as hardware moves\ntoward mixed-precision dot-product-centric operations optimized for quantized\ninference, these legacy approaches are being phased out. In response to this,\nour paper revisits traditional high-performance gemm and describes strategies\nfor adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs,\nincluding x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel\ndesigns and data layouts that better exploit today's specialized hardware and\ndemonstrate significant performance gains from MIP arithmetic over\nfloating-point implementations across three representative CPU architectures.\nThese contributions highlight a new era of gemm optimization-driven by the\ndemands of DL inference on heterogeneous architectures, marking what we term as\nthe \"Cambrian period\" for matrix multiplication.", "AI": {"tldr": "This paper revisits traditional high-performance gemm and describes strategies for adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs, demonstrating significant performance gains.", "motivation": "The transition from traditional 64-bit floating point computations to reduced-precision formats like FP16, BF16, and 8- or 16-bit integers in deep learning offers advantages such as enhanced computational throughput, reduced memory and bandwidth usage, and improved energy efficiency, especially beneficial for resource-constrained edge devices. The evolution of hardware architectures to include adapted ISAs with mixed-precision vector units and matrix engines optimized for DL workloads necessitates revisiting and optimizing gemm operations.", "method": "The paper describes novel micro-kernel designs and data layouts to better exploit specialized hardware for MIP arithmetic across x86_64, ARM, and RISC-V ISAs.", "result": "Significant performance gains were demonstrated from MIP arithmetic over floating-point implementations across three representative CPU architectures.", "conclusion": "This work marks a new era of gemm optimization driven by the demands of DL inference on heterogeneous architectures, referred to as the 'Cambrian period' for matrix multiplication."}}
{"id": "2506.11752", "pdf": "https://arxiv.org/pdf/2506.11752", "abs": "https://arxiv.org/abs/2506.11752", "authors": ["Nan Jiang", "Ziming Wu", "De-Chuan Zhan", "Fuming Lai", "Shaobing Lian"], "title": "DART: Distilling Autoregressive Reasoning to Silent Thought", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART achieves comparable\nreasoning performance to existing baselines while offering significant\nefficiency gains, serving as a feasible alternative for efficient reasoning.", "AI": {"tldr": "Introduces DART, a self-distillation framework that replaces autoregressive Chain-of-Thought reasoning with non-autoregressive Silent Thought for more efficient large language model reasoning.", "motivation": "To address the computational overhead of autoregressive Chain-of-Thought reasoning in latency-sensitive applications.", "method": "Proposes DART, which includes a CoT pathway for traditional reasoning and an ST pathway for generating answers from a few ST tokens using a lightweight REM.", "result": "DART achieves similar reasoning performance to existing baselines but with significant efficiency improvements.", "conclusion": "DART offers a feasible alternative for efficient reasoning in large language models."}}
{"id": "2506.11763", "pdf": "https://arxiv.org/pdf/2506.11763", "abs": "https://arxiv.org/abs/2506.11763", "authors": ["Mingxuan Du", "Benfeng Xu", "Chiwei Zhu", "Xiaorui Wang", "Zhendong Mao"], "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents", "categories": ["cs.CL", "cs.IR"], "comment": "31 pages, 5 figures", "summary": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.", "AI": {"tldr": "This paper presents DeepResearch Bench, a new benchmark for evaluating deep research agents (DRAs), which consist of 100 PhD-level research tasks across 22 fields. Two novel evaluation methods are proposed: one for assessing report quality and another for gauging information retrieval.", "motivation": "There is a lack of comprehensive benchmarks for evaluating the capabilities of deep research agents.", "method": "Introduce DeepResearch Bench with 100 tasks across 22 fields and two novel evaluation methods.", "result": "Two evaluation methods were developed to align well with human judgment, focusing on report quality and information retrieval.", "conclusion": "The paper provides an open-source benchmark and evaluation methods to advance the development of practical LLM-based agents."}}
{"id": "2506.11769", "pdf": "https://arxiv.org/pdf/2506.11769", "abs": "https://arxiv.org/abs/2506.11769", "authors": ["Tianqi Du", "Haotian Huang", "Yifei Wang", "Yisen Wang"], "title": "Long-Short Alignment for Effective Long-Context Modeling in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e2d\u7684\u957f\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u8c03\u8f93\u51fa\u5206\u5e03\u7684\u4e00\u81f4\u6027\uff08\u957f-\u77ed\u5bf9\u9f50\uff09\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6b63\u5219\u5316\u9879\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fc3\u8fdb\u8fd9\u79cd\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8etransformer\u67b6\u6784\u7684\u56fa\u5b9a\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u4ece\u8f93\u51fa\u5206\u5e03\u7684\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u957f\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u957f-\u77ed\u5bf9\u9f50\u7684\u6982\u5ff5\uff0c\u5e76\u4e3a\u6b64\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6b63\u5219\u5316\u9879\u7528\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u63d0\u51fa\u7684\u6b63\u5219\u5316\u9879\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e14\u4e0e\u957f\u5ea6\u6cdb\u5316\u8868\u73b0\u6709\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2506.11798", "pdf": "https://arxiv.org/pdf/2506.11798", "abs": "https://arxiv.org/abs/2506.11798", "authors": ["Maximilian Kreutner", "Marlene Lutz", "Markus Strohmaier"], "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.", "AI": {"tldr": "This study examines whether zero-shot persona prompting can predict individual voting decisions and group policy positions of the European Parliament using Large Language Models.", "motivation": "To address the progressive left-leaning bias in LLMs and explore the potential of persona prompts to simulate voting behavior.", "method": "Using zero-shot persona prompting with limited information to predict voting behaviors and policy positions of European groups.", "result": "Achieved a weighted F1 score of approximately 0.793 in simulating the voting behavior of Members of the European Parliament.", "conclusion": "Limited information persona prompting can effectively predict individual and aggregated voting behaviors in the European Parliament."}}
{"id": "2506.11807", "pdf": "https://arxiv.org/pdf/2506.11807", "abs": "https://arxiv.org/abs/2506.11807", "authors": ["Simeon Junker", "Manar Ali", "Larissa Koch", "Sina Zarrie\u00df", "Hendrik Buschmeier"], "title": "Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?", "categories": ["cs.CL"], "comment": "To appear in ACL Findings 2025", "summary": "We investigate the linguistic abilities of multimodal large language models\nin reference resolution tasks featuring simple yet abstract visual stimuli,\nsuch as color patches and color grids. Although the task may not seem\nchallenging for today's language models, being straightforward for human dyads,\nwe consider it to be a highly relevant probe of the pragmatic capabilities of\nMLLMs. Our results and analyses indeed suggest that basic pragmatic\ncapabilities, such as context-dependent interpretation of color descriptions,\nstill constitute major challenges for state-of-the-art MLLMs.", "AI": {"tldr": "This study examines the linguistic abilities of multimodal large language models in reference resolution tasks using simple visual stimuli like color patches and grids, revealing that basic pragmatic capabilities remain challenging for current models.", "motivation": "To investigate the pragmatic capabilities of multimodal large language models by using reference resolution tasks with abstract visual stimuli.", "method": "Using simple visual stimuli such as color patches and grids in reference resolution tasks.", "result": "State-of-the-art multimodal large language models still face challenges in basic pragmatic capabilities like context-dependent interpretation of color descriptions.", "conclusion": "Basic pragmatic capabilities, such as context-dependent interpretation, are still major challenges for current multimodal large language models."}}
{"id": "2506.11857", "pdf": "https://arxiv.org/pdf/2506.11857", "abs": "https://arxiv.org/abs/2506.11857", "authors": ["Yi-Pei Chen", "Noriki Nishida", "Hideki Nakayama", "Yuji Matsumoto"], "title": "Post Persona Alignment for Multi-Session Dialogue Generation", "categories": ["cs.CL"], "comment": null, "summary": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation.", "AI": {"tldr": "A new method called Post Persona Alignment (PPA) is proposed for multi-session persona-based dialogue generation, which improves consistency, diversity, and persona relevance.", "motivation": "Existing methods struggle to maintain long-term consistency and generate diverse, personalized responses in multi-session dialogues.", "method": "PPA is a two-stage framework that generates a general response first, retrieves relevant persona memories using the response as a query, and refines the response to align with the speaker's persona.", "result": "Experiments show that PPA significantly outperforms prior approaches in consistency, diversity, and persona relevance.", "conclusion": "PPA offers a more flexible and effective paradigm for long-term personalized dialogue generation."}}
{"id": "2506.11886", "pdf": "https://arxiv.org/pdf/2506.11886", "abs": "https://arxiv.org/abs/2506.11886", "authors": ["Xiaoran Liu", "Siyang He", "Qiqi Wang", "Ruixiao Li", "Yuerong Song", "Zhigeng Liu", "Linlin Li", "Qun Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache", "categories": ["cs.CL"], "comment": "10 pages, 7 figures, work in progress", "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.", "AI": {"tldr": "This paper introduces FourierAttention, a training-free method for compressing large language models' key-value cache by projecting insensitive dimensions onto Fourier bases, improving long-context accuracy without sacrificing performance.", "motivation": "To address the memory demands and computational overhead issues caused by increasing context lengths in large language models.", "method": "FourierAttention projects long-context-insensitive head dimensions onto orthogonal Fourier bases, approximating temporal evolution with fixed-length spectral coefficients.", "result": "Achieved the best long-context accuracy on LongBench and Needle-In-A-Haystack benchmarks.", "conclusion": "FourierAttention offers an efficient way to handle long-context information in large language models without compromising performance."}}
{"id": "2506.11903", "pdf": "https://arxiv.org/pdf/2506.11903", "abs": "https://arxiv.org/abs/2506.11903", "authors": ["Raphael Scheible-Schmitt", "Johann Frei"], "title": "GeistBERT: Breathing Life into German NLP", "categories": ["cs.CL"], "comment": null, "summary": "Advances in transformer-based language models have highlighted the benefits\nof language-specific pre-training on high-quality corpora. In this context,\nGerman NLP stands to gain from updated architectures and modern datasets\ntailored to the linguistic characteristics of the German language. GeistBERT\nseeks to improve German language processing by incrementally training on a\ndiverse corpus and optimizing model performance across various NLP tasks. It\nwas pre-trained using fairseq with standard hyperparameters, initialized from\nGottBERT weights, and trained on a large-scale German corpus using Whole Word\nMasking (WWM). Based on the pre-trained model, we derived extended-input\nvariants using Nystr\\\"omformer and Longformer architectures with support for\nsequences up to 8k tokens. While these long-context models were not evaluated\non dedicated long-context benchmarks, they are included in our release. We\nassessed all models on NER (CoNLL 2003, GermEval 2014) and text classification\n(GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The\nGeistBERT models achieved strong performance, leading all tasks among the base\nmodels and setting a new state-of-the-art (SOTA). Notably, the base models\noutperformed larger models in several tasks. To support the German NLP research\ncommunity, we are releasing GeistBERT under the MIT license.", "AI": {"tldr": "GeistBERT improves German language processing by leveraging updated architectures and modern datasets, achieving strong performance in NER and text classification tasks.", "motivation": "To improve German language processing by leveraging updated architectures and modern datasets tailored to the linguistic characteristics of the German language.", "method": "Pre-trained using fairseq with standard hyperparameters, initialized from GottBERT weights, and trained on a large-scale German corpus using Whole Word Masking (WWM). Extended-input variants were derived using Nystr\u00f6mformer and Longformer architectures.", "result": "GeistBERT achieved strong performance in NER (CoNLL 2003, GermEval 2014) and text classification (GermEval 2018 fine/coarse, 10kGNAD) tasks, leading all tasks among the base models and setting a new state-of-the-art (SOTA).", "conclusion": "GeistBERT improved German language processing by incrementally training on a diverse corpus and optimizing model performance across various NLP tasks. It achieved strong performance in NER and text classification tasks, outperforming larger models in several tasks."}}
{"id": "2506.11919", "pdf": "https://arxiv.org/pdf/2506.11919", "abs": "https://arxiv.org/abs/2506.11919", "authors": ["Greta Damo", "Elena Cabrio", "Serena Villata"], "title": "Effectiveness of Counter-Speech against Abusive Content: A Multidimensional Annotation and Classification Study", "categories": ["cs.CL"], "comment": null, "summary": "Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS),\nyet defining the criteria to assess its effectiveness remains an open\nchallenge. We propose a novel computational framework for CS effectiveness\nclassification, grounded in social science concepts. Our framework defines six\ncore dimensions - Clarity, Evidence, Emotional Appeal, Rebuttal, Audience\nAdaptation, and Fairness - which we use to annotate 4,214 CS instances from two\nbenchmark datasets, resulting in a novel linguistic resource released to the\ncommunity. In addition, we propose two classification strategies, multi-task\nand dependency-based, achieving strong results (0.94 and 0.96 average F1\nrespectively on both expert- and user-written CS), outperforming standard\nbaselines, and revealing strong interdependence among dimensions.", "AI": {"tldr": "This paper presents a new computational framework to evaluate the effectiveness of counter-speech in mitigating hate speech based on six core dimensions. The authors annotated a large dataset and proposed two classification strategies that outperform standard baselines.", "motivation": "To define the criteria for assessing the effectiveness of counter-speech in mitigating online hate speech.", "method": "Proposed a novel computational framework with six core dimensions to classify counter-speech effectiveness and annotated 4,214 CS instances from two benchmark datasets. Also proposed two classification strategies, multi-task and dependency-based.", "result": "Achieved strong results (0.94 and 0.96 average F1 respectively on both expert- and user-written CS) outperforming standard baselines and revealed strong interdependence among dimensions.", "conclusion": "The proposed framework and annotation provide a valuable resource for future research on counter-speech effectiveness."}}
{"id": "2506.11930", "pdf": "https://arxiv.org/pdf/2506.11930", "abs": "https://arxiv.org/abs/2506.11930", "authors": ["Dongwei Jiang", "Alvin Zhang", "Andrew Wang", "Nicholas Andrews", "Daniel Khashabi"], "title": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement.", "AI": {"tldr": "This paper investigates the effectiveness of large language models (LLMs) incorporating external feedback under ideal conditions. It introduces the concept of 'FEEDBACK FRICTION,' where even high-performing models struggle to adjust their responses despite receiving nearly perfect feedback.", "motivation": "To explore how well LLMs can adapt to external feedback and identify barriers to improvement.", "method": "Designed an experimental setup where models attempt tasks, receive feedback, and then try again.", "result": "Models showed resistance to feedback ('FEEDBACK FRICTION'), even with optimal conditions.", "conclusion": "The study highlights the challenges of self-improvement in LLMs and suggests further research is needed to overcome these limitations."}}
{"id": "2506.11938", "pdf": "https://arxiv.org/pdf/2506.11938", "abs": "https://arxiv.org/abs/2506.11938", "authors": ["Samuel Simko", "Mrinmaya Sachan", "Bernhard Sch\u00f6lkopf", "Zhijing Jin"], "title": "Improving Large Language Model Safety with Contrastive Representation Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense", "AI": {"tldr": "This paper presents a defense framework for Large Language Models against adversarial attacks by formulating it as a contrastive representation learning problem.", "motivation": "Existing defenses often fail to generalize across different attack types. The authors aim to improve robustness against adversarial attacks while maintaining standard performance.", "method": "The method involves finetuning a model using a triplet-based loss and adversarial hard negative mining to separate benign and harmful representations.", "result": "Experimental results show that this approach surpasses previous representation engineering-based defenses in terms of robustness against input-level and embedding-space attacks.", "conclusion": "This defense framework improves robustness of LLMs against adversarial attacks without sacrificing standard performance."}}
{"id": "2506.12014", "pdf": "https://arxiv.org/pdf/2506.12014", "abs": "https://arxiv.org/abs/2506.12014", "authors": ["Yuliang Xu", "Siming Huang", "Mingmeng Geng", "Yao Wan", "Xuanhua Shi", "Dongping Chen"], "title": "code_transformed: The Influence of Large Language Models on Code", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "We release all the experimental dataset and source code at:\n  https://github.com/ignorancex/LLM_code", "summary": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u771f\u5b9e\u4e16\u754c\u7f16\u7a0b\u98ce\u683c\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u547d\u540d\u7ea6\u5b9a\u3001\u590d\u6742\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u65b9\u9762\u7684\u53d8\u5316\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u6b65\uff0c\u5176\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5f00\u59cb\u663e\u8457\u6539\u53d8\u7f16\u7a0b\u5b9e\u8df5\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u6539\u53d8\u4e86\u4ee3\u7801\u98ce\u683c\u3002", "method": "\u5206\u6790\u6765\u81eaGitHub\u4ed3\u5e93\u7684\u4ee3\u7801\uff0c\u8fd9\u4e9b\u4ed3\u5e93\u94fe\u63a5\u52302020\u5e74\u81f32025\u5e74\u95f4\u53d1\u8868\u5728arXiv\u4e0a\u7684\u8bba\u6587\uff0c\u91cd\u70b9\u7814\u7a76\u547d\u540d\u7ea6\u5b9a\u3001\u590d\u6742\u6027\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u76f8\u4f3c\u6027\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u7814\u7a76LLMs\u89e3\u51b3\u7b97\u6cd5\u95ee\u9898\u7684\u8fc7\u7a0b\u6765\u63a2\u8ba8\u5176\u7f16\u7801\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u4ece2023\u5e74\u7b2c\u4e00\u5b63\u5ea6\u52302025\u5e74\u7b2c\u4e00\u5b63\u5ea6\uff0cPython\u4ee3\u7801\u4e2d\u4f7f\u7528snake_case\u53d8\u91cf\u540d\u7684\u6bd4\u4f8b\u4ece47%\u589e\u52a0\u5230\u4e8651%\uff0c\u8868\u660eLLMs\u5bf9\u4ee3\u7801\u98ce\u683c\u7684\u5f71\u54cd\u662f\u53ef\u6d4b\u91cf\u7684\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4e3aLLMs\u5bf9\u5b9e\u9645\u7f16\u7a0b\u98ce\u683c\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc1\u636e\u3002", "conclusion": "LLMs\u5f71\u54cd\u4e86\u771f\u5b9e\u7684\u7f16\u7a0b\u98ce\u683c\uff0c\u5e76\u4e14\u8fd9\u79cd\u5f71\u54cd\u5728\u547d\u540d\u7ea6\u5b9a\u3001\u590d\u6742\u6027\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u76f8\u4f3c\u6027\u65b9\u9762\u53ef\u4ee5\u88ab\u91cf\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u9996\u6b21\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u7684\u7ecf\u9a8c\u4f9d\u636e\u6765\u652f\u6301\u8fd9\u4e00\u70b9\u3002"}}
{"id": "2506.11004", "pdf": "https://arxiv.org/pdf/2506.11004", "abs": "https://arxiv.org/abs/2506.11004", "authors": ["Kevin Cogan", "Vuong M. Ngo", "Mark Roantree"], "title": "Developing a Dyslexia Indicator Using Eye Tracking", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "The 23rd International Conference on Artificial Intelligence in\n  Medicine (AIME 2025), LNAI, Springer, 11 pages", "summary": "Dyslexia, affecting an estimated 10% to 20% of the global population,\nsignificantly impairs learning capabilities, highlighting the need for\ninnovative and accessible diagnostic methods. This paper investigates the\neffectiveness of eye-tracking technology combined with machine learning\nalgorithms as a cost-effective alternative for early dyslexia detection. By\nanalyzing general eye movement patterns, including prolonged fixation durations\nand erratic saccades, we proposed an enhanced solution for determining\neye-tracking-based dyslexia features. A Random Forest Classifier was then\nemployed to detect dyslexia, achieving an accuracy of 88.58\\%. Additionally,\nhierarchical clustering methods were applied to identify varying severity\nlevels of dyslexia. The analysis incorporates diverse methodologies across\nvarious populations and settings, demonstrating the potential of this\ntechnology to identify individuals with dyslexia, including those with\nborderline traits, through non-invasive means. Integrating eye-tracking with\nmachine learning represents a significant advancement in the diagnostic\nprocess, offering a highly accurate and accessible method in clinical research.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u4e0e\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7ed3\u5408\u7528\u4e8e\u65e9\u671f\u9605\u8bfb\u969c\u788d\u68c0\u6d4b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u5168\u7403\u7ea610%\u523020%\u7684\u4eba\u53e3\u53d7\u5230\u9605\u8bfb\u969c\u788d\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u521b\u65b0\u4e14\u53ef\u8bbf\u95ee\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u773c\u52a8\u4eea\u8bb0\u5f55\u773c\u52a8\u6570\u636e\u5e76\u5229\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8fdb\u884c\u5206\u6790\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u9605\u8bfb\u969c\u788d\u65f6\u8fbe\u5230\u4e8688.58%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u591f\u8bc6\u522b\u4e0d\u540c\u7a0b\u5ea6\u7684\u9605\u8bfb\u969c\u788d\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u773c\u52a8\u4eea\u548c\u673a\u5668\u5b66\u4e60\uff0c\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u51c6\u786e\u4e14\u53ef\u8bbf\u95ee\u7684\u65b9\u6cd5\u6765\u8bca\u65ad\u9605\u8bfb\u969c\u788d\u3002"}}
{"id": "2506.11012", "pdf": "https://arxiv.org/pdf/2506.11012", "abs": "https://arxiv.org/abs/2506.11012", "authors": ["Guanglin Niu", "Bo Li", "Yangguang Lin"], "title": "A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects", "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "45 pages, 17 figures, 12 tables", "summary": "Knowledge graphs (KGs) have emerged as a powerful paradigm for structuring\nand leveraging diverse real-world knowledge, which serve as a fundamental\ntechnology for enabling cognitive intelligence systems with advanced\nunderstanding and reasoning capabilities. Knowledge graph reasoning (KGR) aims\nto infer new knowledge based on existing facts in KGs, playing a crucial role\nin applications such as public security intelligence, intelligent healthcare,\nand financial risk assessment. From a task-centric perspective, existing KGR\napproaches can be broadly classified into static single-step KGR, static\nmulti-step KGR, dynamic KGR, multi-modal KGR, few-shot KGR, and inductive KGR.\nWhile existing surveys have covered these six types of KGR tasks, a\ncomprehensive review that systematically summarizes all KGR tasks particularly\nincluding downstream applications and more challenging reasoning paradigms\nremains lacking. In contrast to previous works, this survey provides a more\ncomprehensive perspective on the research of KGR by categorizing approaches\nbased on primary reasoning tasks, downstream application tasks, and potential\nchallenging reasoning tasks. Besides, we explore advanced techniques, such as\nlarge language models (LLMs), and their impact on KGR. This work aims to\nhighlight key research trends and outline promising future directions in the\nfield of KGR.", "AI": {"tldr": "This survey provides a comprehensive overview of knowledge graph reasoning tasks, including static single-step, multi-step, dynamic, multi-modal, few-shot, and inductive KGR. It also explores the impact of large language models on KGR and highlights key research trends and future directions.", "motivation": "To provide a comprehensive review of knowledge graph reasoning tasks, particularly including downstream applications and more challenging reasoning paradigms.", "method": "Categorizing approaches based on primary reasoning tasks, downstream application tasks, and potential challenging reasoning tasks.", "result": "A more comprehensive perspective on the research of KGR is provided, exploring advanced techniques such as large language models and their impact on KGR.", "conclusion": "This survey aims to highlight key research trends and outline promising future directions in the field of KGR."}}
{"id": "2506.11022", "pdf": "https://arxiv.org/pdf/2506.11022", "abs": "https://arxiv.org/abs/2506.11022", "authors": ["Shivani Shukla", "Himanshu Joshi", "Romilla Syed"], "title": "Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": "Keywords - Large Language Models, Security Vulnerabilities,\n  AI-Generated Code, Iterative Feedback, Software Security, Secure Coding\n  Practices, Feedback Loops, LLM Prompting Strategies", "summary": "The rapid adoption of Large Language Models(LLMs) for code generation has\ntransformed software development, yet little attention has been given to how\nsecurity vulnerabilities evolve through iterative LLM feedback. This paper\nanalyzes security degradation in AI-generated code through a controlled\nexperiment with 400 code samples across 40 rounds of \"improvements\" using four\ndistinct prompting strategies. Our findings show a 37.6% increase in critical\nvulnerabilities after just five iterations, with distinct vulnerability\npatterns emerging across different prompting approaches. This evidence\nchallenges the assumption that iterative LLM refinement improves code security\nand highlights the essential role of human expertise in the loop. We propose\npractical guidelines for developers to mitigate these risks, emphasizing the\nneed for robust human validation between LLM iterations to prevent the\nparadoxical introduction of new security issues during supposedly beneficial\ncode \"improvements\".", "AI": {"tldr": "This study examines how security vulnerabilities change as code is iteratively improved by large language models (LLMs), revealing significant increases in critical vulnerabilities and suggesting the necessity of human oversight.", "motivation": "To understand how security vulnerabilities evolve through iterative LLM feedback in code generation.", "method": "A controlled experiment involving 400 code samples over 40 rounds of improvements using four different prompting strategies.", "result": "A 37.6% increase in critical vulnerabilities after only five iterations, with varying patterns emerging based on the prompting approach used.", "conclusion": "Iterative LLM refinement does not necessarily improve code security, highlighting the importance of human expertise in the process."}}
{"id": "2506.11031", "pdf": "https://arxiv.org/pdf/2506.11031", "abs": "https://arxiv.org/abs/2506.11031", "authors": ["Zoher Kachwala", "Danishjeet Singh", "Danielle Yang", "Filippo Menczer"], "title": "Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As image generators produce increasingly realistic images, concerns about\npotential misuse continue to grow. Supervised detection relies on large,\ncurated datasets and struggles to generalize across diverse generators. In this\nwork, we investigate the use of pre-trained Vision-Language Models (VLMs) for\nzero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit\nsome task-specific reasoning and chain-of-thought prompting offers gains, we\nshow that task-aligned prompting elicits more focused reasoning and\nsignificantly improves performance without fine-tuning. Specifically, prefixing\nthe model's response with the phrase ``Let's examine the style and the\nsynthesis artifacts'' -- a method we call zero-shot-s$^2$ -- boosts Macro F1\nscores by 8%-29% for two widely used open-source models. These gains are\nconsistent across three recent, diverse datasets spanning human faces, objects,\nand animals with images generated by 16 different models -- demonstrating\nstrong generalization. We further evaluate the approach across three additional\nmodel sizes and observe improvements in most dataset-model combinations --\nsuggesting robustness to model scale. Surprisingly, self-consistency, a\nbehavior previously observed in language reasoning, where aggregating answers\nfrom diverse reasoning paths improves performance, also holds in this setting.\nEven here, zero-shot-s$^2$ scales better than chain-of-thought in most cases --\nindicating that it elicits more useful diversity. Our findings show that\ntask-aligned prompts elicit more focused reasoning and enhance latent\ncapabilities in VLMs, like the detection of AI-generated images -- offering a\nsimple, generalizable, and explainable alternative to supervised methods. Our\ncode is publicly available on github:\nhttps://github.com/osome-iu/Zero-shot-s2.git.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u4efb\u52a1\u5bf9\u9f50\u63d0\u793a\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u56fe\u50cf\u751f\u6210\u5668\u4ea7\u751f\u7684\u56fe\u50cf\u8d8a\u6765\u8d8a\u903c\u771f\uff0c\u4eba\u4eec\u5bf9\u5176\u6f5c\u5728\u8bef\u7528\u7684\u62c5\u5fe7\u4e5f\u5728\u589e\u52a0\u3002\u76d1\u7763\u68c0\u6d4b\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u7684\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u96be\u4ee5\u8de8\u4e0d\u540c\u7684\u751f\u6210\u5668\u8fdb\u884c\u63a8\u5e7f\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u8fdb\u884c\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u7814\u7a76\u4e86\u4efb\u52a1\u5bf9\u9f50\u63d0\u793a\u7684\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5zero-shot-s$^2$\u3002", "result": "\u4efb\u52a1\u5bf9\u9f50\u63d0\u793a\u6bd4\u666e\u901a\u7684VLM\u548c\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u80fd\u66f4\u597d\u5730\u63d0\u5347\u6027\u80fd\uff0c\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u6e90\u6a21\u578b\u4e0a\uff0cMacro F1\u5206\u6570\u63d0\u9ad8\u4e868%-29%\uff0c\u5e76\u4e14\u8fd9\u79cd\u6539\u8fdb\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e00\u81f4\uff0c\u540c\u65f6\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0czero-shot-s$^2$\u7684\u8868\u73b0\u4f18\u4e8e\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u3002", "conclusion": "\u4efb\u52a1\u5bf9\u9f50\u63d0\u793a\u53ef\u4ee5\u5f15\u53d1\u66f4\u96c6\u4e2d\u7684\u63a8\u7406\uff0c\u5e76\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\u65b9\u9762\u7684\u6f5c\u5728\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u53ef\u6cdb\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u76d1\u7763\u65b9\u6cd5\u3002"}}
{"id": "2506.11034", "pdf": "https://arxiv.org/pdf/2506.11034", "abs": "https://arxiv.org/abs/2506.11034", "authors": ["Aneesh Komanduri", "Karuna Bhaila", "Xintao Wu"], "title": "CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable ability in various\nlanguage tasks, especially with their emergent in-context learning capability.\nExtending LLMs to incorporate visual inputs, large vision-language models\n(LVLMs) have shown impressive performance in tasks such as recognition and\nvisual question answering (VQA). Despite increasing interest in the utility of\nLLMs in causal reasoning tasks such as causal discovery and counterfactual\nreasoning, there has been relatively little work showcasing the abilities of\nLVLMs on visual causal reasoning tasks. We take this opportunity to formally\nintroduce a comprehensive causal reasoning benchmark for multi-modal in-context\nlearning from LVLMs. Our CausalVLBench encompasses three representative tasks:\ncausal structure inference, intervention target prediction, and counterfactual\nprediction. We evaluate the ability of state-of-the-art open-source LVLMs on\nour causal reasoning tasks across three causal representation learning datasets\nand demonstrate their fundamental strengths and weaknesses. We hope that our\nbenchmark elucidates the drawbacks of existing vision-language models and\nmotivates new directions and paradigms in improving the visual causal reasoning\nabilities of LVLMs.", "AI": {"tldr": "Large language models (LLMs) are great at many language tasks. Adding visual inputs, large vision-language models (LVLMs) perform well in recognition and VQA tasks. However, there is limited work on LVLMs' performance in visual causal reasoning tasks. This paper introduces CausalVLBench, a benchmark for multi-modal in-context learning from LVLMs. It includes three tasks: causal structure inference, intervention target prediction, and counterfactual prediction. The authors evaluate popular open-source LVLMs on this benchmark and show their capabilities and limitations.", "motivation": "To assess the ability of LVLMs in visual causal reasoning tasks and encourage improvements in this area.", "method": "Introducing CausalVLBench, a benchmark for multi-modal in-context learning from LVLMs, which includes three tasks: causal structure inference, intervention target prediction, and counterfactual prediction.", "result": "The evaluation of state-of-the-art open-source LVLMs on the causal reasoning tasks across three causal representation learning datasets showed their fundamental strengths and weaknesses.", "conclusion": "This benchmark highlights the drawbacks of existing vision-language models and inspires new approaches and paradigms for enhancing the visual causal reasoning abilities of LVLMs."}}
{"id": "2506.11035", "pdf": "https://arxiv.org/pdf/2506.11035", "abs": "https://arxiv.org/abs/2506.11035", "authors": ["Moussa Koulako Bala Doumbouya", "Dan Jurafsky", "Christopher D. Manning"], "title": "Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "68", "I.2.0; I.2.4; I.2.6; I.2.7; I.4.7; I.4.10; I.5.1; F.1.1"], "comment": null, "summary": "Work in psychology has highlighted that the geometric model of similarity\nstandard in deep learning is not psychologically plausible because its metric\nproperties such as symmetry do not align with human perception. In contrast,\nTversky (1977) proposed an axiomatic theory of similarity based on a\nrepresentation of objects as sets of features, and their similarity as a\nfunction of common and distinctive features. However, this model has not been\nused in deep learning before, partly due to the challenge of incorporating\ndiscrete set operations. We develop a differentiable parameterization of\nTversky's similarity that is learnable through gradient descent, and derive\nneural network building blocks such as the Tversky projection layer, which\nunlike the linear projection layer can model non-linear functions such as XOR.\nThrough experiments with image recognition and language modeling, we show that\nthe Tversky projection layer is a beneficial replacement for the linear\nprojection layer, which employs geometric similarity. On the NABirds image\nclassification task, a frozen ResNet-50 adapted with a Tversky projection layer\nachieves a 24.7% relative accuracy improvement over the linear layer adapter\nbaseline. With Tversky projection layers, GPT-2's perplexity on PTB decreases\nby 7.5%, and its parameter count by 34.8%. Finally, we propose a unified\ninterpretation of both projection layers as computing similarities of input\nstimuli to learned prototypes, for which we also propose a novel visualization\ntechnique highlighting the interpretability of Tversky projection layers. Our\nwork offers a new paradigm for thinking about the similarity model implicit in\ndeep learning, and designing networks that are interpretable under an\nestablished theory of psychological similarity.", "AI": {"tldr": "This paper introduces a differentiable parameterization of Tversky's similarity model for use in deep learning, replacing the traditional geometric similarity model. It demonstrates improvements in image recognition and language modeling tasks using this new approach.", "motivation": "Traditional deep learning models use geometric similarity which does not align with human perception according to psychological studies. Tversky's model based on feature sets and their interactions was not previously used in deep learning due to challenges in incorporating discrete set operations.", "method": "Developing a differentiable version of Tversky's similarity that can be learned via gradient descent, creating neural network components like the Tversky projection layer, and testing these on image recognition and language modeling tasks.", "result": "The Tversky projection layer showed significant improvements over linear projection layers in image classification tasks (24.7% relative accuracy increase) and reduced perplexity in language modeling while decreasing parameters needed.", "conclusion": "This work provides a new way to think about similarity in deep learning models by integrating psychological theories into network design, enhancing both performance and interpretability."}}
{"id": "2506.11040", "pdf": "https://arxiv.org/pdf/2506.11040", "abs": "https://arxiv.org/abs/2506.11040", "authors": ["Feifei Shi", "Xueyan Yin", "Kang Wang", "Wanyu Tu", "Qifu Sun", "Huansheng Ning"], "title": "Large Language models for Time Series Analysis: Techniques, Applications, and Challenges", "categories": ["cs.LG", "cs.CL", "cs.ET"], "comment": null, "summary": "Time series analysis is pivotal in domains like financial forecasting and\nbiomedical monitoring, yet traditional methods are constrained by limited\nnonlinear feature representation and long-term dependency capture. The\nemergence of Large Language Models (LLMs) offers transformative potential by\nleveraging their cross-modal knowledge integration and inherent attention\nmechanisms for time series analysis. However, the development of\ngeneral-purpose LLMs for time series from scratch is still hindered by data\ndiversity, annotation scarcity, and computational requirements. This paper\npresents a systematic review of pre-trained LLM-driven time series analysis,\nfocusing on enabling techniques, potential applications, and open challenges.\nFirst, it establishes an evolutionary roadmap of AI-driven time series\nanalysis, from the early machine learning era, through the emerging LLM-driven\nparadigm, to the development of native temporal foundation models. Second, it\norganizes and systematizes the technical landscape of LLM-driven time series\nanalysis from a workflow perspective, covering LLMs' input, optimization, and\nlightweight stages. Finally, it critically examines novel real-world\napplications and highlights key open challenges that can guide future research\nand innovation. The work not only provides valuable insights into current\nadvances but also outlines promising directions for future development. It\nserves as a foundational reference for both academic and industrial\nresearchers, paving the way for the development of more efficient,\ngeneralizable, and interpretable systems of LLM-driven time series analysis.", "AI": {"tldr": "This paper reviews pre-trained LLM-driven time series analysis, covering techniques, applications, and challenges.", "motivation": "Traditional time series analysis has limitations in nonlinear feature representation and long-term dependency capture. LLMs offer transformative potential but developing general-purpose LLMs for time series is hindered by data diversity, annotation scarcity, and computational requirements.", "method": "The paper establishes an evolutionary roadmap of AI-driven time series analysis and organizes the technical landscape of LLM-driven time series analysis from a workflow perspective.", "result": "The paper highlights novel real-world applications and key open challenges for future research and innovation.", "conclusion": "This review provides insights into current advances and outlines promising directions for future development in LLM-driven time series analysis."}}
{"id": "2506.11059", "pdf": "https://arxiv.org/pdf/2506.11059", "abs": "https://arxiv.org/abs/2506.11059", "authors": ["Hanxi Guo", "Siyuan Cheng", "Kaiyuan Zhang", "Guangyu Shen", "Xiangyu Zhang"], "title": "CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs", "categories": ["cs.SE", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have become integral to modern software\ndevelopment, producing vast amounts of AI-generated source code. While these\nmodels boost programming productivity, their misuse introduces critical risks,\nincluding code plagiarism, license violations, and the propagation of insecure\nprograms. As a result, robust detection of AI-generated code is essential. To\nsupport the development of such detectors, a comprehensive benchmark that\nreflects real-world conditions is crucial. However, existing benchmarks fall\nshort -- most cover only a limited set of programming languages and rely on\nless capable generative models. In this paper, we present CodeMirage, a\ncomprehensive benchmark that addresses these limitations through three major\nadvancements: (1) it spans ten widely used programming languages, (2) includes\nboth original and paraphrased code samples, and (3) incorporates outputs from\nten state-of-the-art production-level LLMs, including both reasoning and\nnon-reasoning models from six major providers. Using CodeMirage, we evaluate\nten representative detectors across four methodological paradigms under four\nrealistic evaluation configurations, reporting results using three\ncomplementary metrics. Our analysis reveals nine key findings that uncover the\nstrengths and weaknesses of current detectors, and identify critical challenges\nfor future work. We believe CodeMirage offers a rigorous and practical testbed\nto advance the development of robust and generalizable AI-generated code\ndetectors.", "AI": {"tldr": "CodeMirage is a new benchmark for detecting AI-generated code that overcomes limitations of existing benchmarks by covering more programming languages, including paraphrased code samples, and incorporating outputs from top LLMs.", "motivation": "Existing benchmarks for detecting AI-generated code are limited in scope and capability, making it difficult to develop robust detectors.", "method": "CodeMirage spans ten programming languages, includes both original and paraphrased code samples, and incorporates outputs from ten state-of-the-art LLMs.", "result": "CodeMirage was used to evaluate ten detectors across four methodological paradigms under four realistic evaluation configurations, with results reported using three complementary metrics.", "conclusion": "CodeMirage offers a rigorous and practical testbed to advance the development of robust and generalizable AI-generated code detectors."}}
{"id": "2506.11064", "pdf": "https://arxiv.org/pdf/2506.11064", "abs": "https://arxiv.org/abs/2506.11064", "authors": ["Jiajun He", "Tomoki Toda"], "title": "PMF-CEC: Phoneme-augmented Multimodal Fusion for Context-aware ASR Error Correction with Error-specific Selective Decoding", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Accepted by IEEE TASLP 2025", "summary": "End-to-end automatic speech recognition (ASR) models often struggle to\naccurately recognize rare words. Previously, we introduced an ASR\npostprocessing method called error detection and context-aware error correction\n(ED-CEC), which leverages contextual information such as named entities and\ntechnical terms to improve the accuracy of ASR transcripts. Although ED-CEC\nachieves a notable success in correcting rare words, its accuracy remains low\nwhen dealing with rare words that have similar pronunciations but different\nspellings. To address this issue, we proposed a phoneme-augmented multimodal\nfusion method for context-aware error correction (PMF-CEC) method on the basis\nof ED-CEC, which allowed for better differentiation between target rare words\nand homophones. Additionally, we observed that the previous ASR error detection\nmodule suffers from overdetection. To mitigate this, we introduced a retention\nprobability mechanism to filter out editing operations with confidence scores\nbelow a set threshold, preserving the original operation to improve error\ndetection accuracy. Experiments conducted on five datasets demonstrated that\nour proposed PMF-CEC maintains reasonable inference speed while further\nreducing the biased word error rate compared with ED-CEC, showing a stronger\nadvantage in correcting homophones. Moreover, our method outperforms other\ncontextual biasing methods, and remains valuable compared with LLM-based\nmethods in terms of faster inference and better robustness under large biasing\nlists.", "AI": {"tldr": "A new method improves the accuracy of correcting rare words with similar pronunciations in ASR transcripts.", "motivation": "Improve the accuracy of ASR in recognizing rare words with similar pronunciations.", "method": "Phoneme-augmented multimodal fusion method for context-aware error correction (PMF-CEC) based on ED-CEC and a retention probability mechanism.", "result": "The method reduces the word error rate and outperforms other methods in correcting homophones with faster inference and better robustness.", "conclusion": "Proposed method improves the accuracy of ASR in recognizing rare words with similar pronunciations."}}
{"id": "2506.11069", "pdf": "https://arxiv.org/pdf/2506.11069", "abs": "https://arxiv.org/abs/2506.11069", "authors": ["Tao Zhong", "Mengzhe Geng", "Shujie Hu", "Guinan Li", "Xunying Liu"], "title": "Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Accurate recognition of dysarthric and elderly speech remains challenging to\ndate. While privacy concerns have driven a shift from centralized approaches to\nfederated learning (FL) to ensure data confidentiality, this further\nexacerbates the challenges of data scarcity, imbalanced data distribution and\nspeaker heterogeneity. To this end, this paper conducts a systematic\ninvestigation of regularized FL techniques for privacy-preserving dysarthric\nand elderly speech recognition, addressing different levels of the FL process\nby 1) parameter-based, 2) embedding-based and 3) novel loss-based\nregularization. Experiments on the benchmark UASpeech dysarthric and\nDementiaBank Pitt elderly speech corpora suggest that regularized FL systems\nconsistently outperform the baseline FedAvg system by statistically significant\nWER reductions of up to 0.55\\% absolute (2.13\\% relative). Further increasing\ncommunication frequency to one exchange per batch approaches centralized\ntraining performance.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u8111\u6027\u762b\u75ea\u548c\u8001\u5e74\u4eba\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u6b63\u5219\u5316\u8054\u90a6\u5b66\u4e60\u6280\u672f\uff0c\u53d1\u73b0\u8be5\u6280\u672f\u5728\u6539\u5584\u8bed\u97f3\u8bc6\u522b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u51c6\u786e\u8bc6\u522b\u8111\u6027\u762b\u75ea\u548c\u8001\u5e74\u4eba\u8bed\u97f3\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9690\u79c1\u95ee\u9898\u63a8\u52a8\u4e86\u4ece\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u5411\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u8f6c\u53d8\uff0c\u4f46\u8fd9\u4e5f\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u6570\u636e\u7a00\u7f3a\u3001\u6570\u636e\u5206\u5e03\u4e0d\u5e73\u8861\u548c\u8bf4\u8bdd\u8005\u5f02\u8d28\u6027\u7b49\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86\u53c2\u6570\u3001\u5d4c\u5165\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\u4e09\u79cd\u6b63\u5219\u5316\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u5206\u522b\u4ece\u8054\u90a6\u5b66\u4e60\u8fc7\u7a0b\u7684\u4e0d\u540c\u5c42\u9762\u8fdb\u884c\u4e86\u6539\u8fdb\u3002", "result": "\u6b63\u5219\u5316\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u5728UASpeech\u8111\u6027\u762b\u75ea\u548cDementiaBank Pitt\u8001\u5e74\u4eba\u8bed\u97f3\u8bed\u6599\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6FedAvg\u7cfb\u7edf\uff0c\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u51cf\u5c11\u4e862.13%\uff0c\u6700\u9ad8\u7edd\u5bf9\u51cf\u5c11\u4e860.55%\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6b63\u5219\u5316\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u5728\u7edf\u8ba1\u5b66\u610f\u4e49\u4e0a\u663e\u8457\u4f18\u4e8eFedAvg\u7cfb\u7edf\uff0c\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u51cf\u5c11\u4e862.13%\uff0c\u6700\u9ad8\u7edd\u5bf9\u51cf\u5c11\u4e860.55%\u3002\u5f53\u901a\u4fe1\u9891\u7387\u589e\u52a0\u5230\u6bcf\u6279\u6b21\u4ea4\u6362\u4e00\u6b21\u65f6\uff0c\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u6027\u80fd\u3002"}}
{"id": "2506.11072", "pdf": "https://arxiv.org/pdf/2506.11072", "abs": "https://arxiv.org/abs/2506.11072", "authors": ["Tahiya Chowdhury", "Veronica Romero"], "title": "Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling", "categories": ["eess.AS", "cs.CL", "cs.CY", "cs.SD", "stat.AP", "K.4; J.4; I.2"], "comment": "5 pages, 1 figure, 3 tables", "summary": "Machine learning-based behavioral models rely on features extracted from\naudio-visual recordings. The recordings are processed using open-source tools\nto extract speech features for classification models. These tools often lack\nvalidation to ensure reliability in capturing behaviorally relevant\ninformation. This gap raises concerns about reproducibility and fairness across\ndiverse populations and contexts. Speech processing tools, when used outside of\ntheir design context, can fail to capture behavioral variations equitably and\ncan then contribute to bias. We evaluate speech features extracted from two\nwidely used speech analysis tools, OpenSMILE and Praat, to assess their\nreliability when considering adolescents with autism. We observed considerable\nvariation in features across tools, which influenced model performance across\ncontext and demographic groups. We encourage domain-relevant verification to\nenhance the reliability of machine learning models in clinical applications.", "AI": {"tldr": "This study evaluates the reliability of speech feature extraction tools for machine learning models in behavioral studies, highlighting issues of variation and impact on model performance across diverse groups.", "motivation": "The study aims to address the issue of reliability in speech feature extraction tools used in machine learning-based behavioral models, especially concerning adolescents with autism, to ensure reproducibility and fairness across diverse populations and contexts.", "method": "The researchers evaluated speech features extracted from two widely used tools, OpenSMILE and Praat, to assess their reliability.", "result": "There was considerable variation in features across the tools, affecting model performance across different context and demographic groups.", "conclusion": "Domain-relevant verification is encouraged to enhance the reliability of machine learning models in clinical applications."}}
{"id": "2506.11079", "pdf": "https://arxiv.org/pdf/2506.11079", "abs": "https://arxiv.org/abs/2506.11079", "authors": ["Lingyun Gao", "Cristian Tejedor-Garcia", "Catia Cucchiarini", "Helmer Strik"], "title": "Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "This paper is accepted to Interspeech 2025. This publication is part\n  of the project Responsible AI for Voice Diagnostics (RAIVD) with file number\n  NGF.1607.22.013 of the research programme NGF AiNed Fellowship Grants which\n  is financed by the Dutch Research Council (NWO)", "summary": "Automatic reading aloud evaluation can provide valuable support to teachers\nby enabling more efficient scoring of reading exercises. However, research on\nreading evaluation systems and applications remains limited. We present a novel\nmultimodal approach that leverages audio and knowledge from text resources. In\nparticular, we explored the potential of using Whisper and instruction-tuned\nlarge language models (LLMs) with prompts to improve transcriptions for child\nspeech recognition, as well as their effectiveness in downstream reading\nmistake detection. Our results demonstrate the effectiveness of prompting\nWhisper and prompting LLM, compared to the baseline Whisper model without\nprompting. The best performing system achieved state-of-the-art recognition\nperformance in Dutch child read speech, with a word error rate (WER) of 5.1%,\nimproving the baseline WER of 9.4%. Furthermore, it significantly improved\nreading mistake detection, increasing the F1 score from 0.39 to 0.73.", "AI": {"tldr": "A novel multimodal approach combining audio and text resources improves child speech recognition and reading mistake detection, achieving state-of-the-art performance.", "motivation": "To address the limitations in research on reading evaluation systems and applications by providing a valuable tool for teachers through automatic reading aloud evaluation.", "method": "Using Whisper and instruction-tuned large language models (LLMs) with prompts to enhance transcriptions for child speech recognition and evaluating their effectiveness in downstream reading mistake detection.", "result": "The proposed system achieved a word error rate of 5.1%, surpassing the baseline Whisper model's 9.4% WER. It also enhanced the F1 score for reading mistake detection from 0.39 to 0.73.", "conclusion": "The study introduces a new multimodal method that uses both audio and text knowledge, showing significant improvements in child speech recognition and reading mistake detection."}}
{"id": "2506.11085", "pdf": "https://arxiv.org/pdf/2506.11085", "abs": "https://arxiv.org/abs/2506.11085", "authors": ["Justin Asher"], "title": "LeanExplore: A search engine for Lean 4 declarations", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.LO", "I.2.6; H.3.3; I.2.3"], "comment": "16 pages, 1 figure. Project website: https://www.leanexplore.com/ ,\n  Code: https://github.com/justincasher/lean-explore", "summary": "The expanding Lean 4 ecosystem poses challenges for navigating its vast\nlibraries. This paper introduces LeanExplore, a search engine for Lean 4\ndeclarations. LeanExplore enables users to semantically search for statements,\nboth formally and informally, across select Lean 4 packages (including\nBatteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is\npowered by a hybrid ranking strategy, integrating scores from a multi-source\nsemantic embedding model (capturing conceptual meaning from formal Lean code,\ndocstrings, AI-generated informal translations, and declaration titles), BM25+\nfor keyword-based lexical relevance, and a PageRank-based score reflecting\ndeclaration importance and interconnectedness. The search engine is accessible\nvia a dedicated website (https://www.leanexplore.com/) and a Python API\n(https://github.com/justincasher/lean-explore). Furthermore, the database can\nbe downloaded, allowing users to self-host the service. LeanExplore integrates\neasily with LLMs via the model context protocol (MCP), enabling users to chat\nwith an AI assistant about Lean declarations or utilize the search engine for\nbuilding theorem-proving agents. This work details LeanExplore's architecture,\ndata processing, functionalities, and its potential to enhance Lean 4 workflows\nand AI-driven mathematical research", "AI": {"tldr": "This paper presents LeanExplore, a search engine for Lean 4 declarations that combines multiple ranking strategies to enable semantic search.", "motivation": "To address the challenge of navigating vast Lean 4 libraries.", "method": "Introducing a hybrid ranking strategy integrating multiple scores.", "result": "A search engine named LeanExplore that allows semantic search for Lean 4 declarations.", "conclusion": "LeanExplore enhances Lean 4 workflows and AI-driven mathematical research."}}
{"id": "2506.11087", "pdf": "https://arxiv.org/pdf/2506.11087", "abs": "https://arxiv.org/abs/2506.11087", "authors": ["Boya Xiong", "Shuo Wang", "Weifeng Ge", "Guanhua Chen", "Yun Chen"], "title": "ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive performance on various\nknowledge-intensive and complex reasoning tasks in different domains. In\ncertain scenarios like multi-tenant serving, a large number of LLMs finetuned\nfrom the same base model are deployed to meet complex requirements for users.\nRecent works explore delta-compression approaches to quantize and compress the\ndelta parameters between the customized LLM and the corresponding base model.\nHowever, existing works either exhibit unsatisfactory performance at high\ncompression ratios or depend on empirical bit allocation schemes. In this work,\nwe propose ADAMIX, an effective adaptive mixed-precision delta-compression\nframework. We provide a mathematical derivation of quantization error to\nmotivate our mixed-precision compression strategy and formulate the optimal\nmixed-precision bit allocation scheme as the solution to a 0/1 integer linear\nprogramming problem. Our derived bit allocation strategy minimizes the\nquantization error while adhering to a predefined compression ratio\nrequirement. Experimental results on various models and benchmarks demonstrate\nthat our approach surpasses the best baseline by a considerable margin. On\ntasks like AIME2024 and GQA, where the norm of $\\Delta \\mathbf{W}$ is large and\nthe base model lacks sufficient ability, ADAMIX outperforms the best baseline\nDelta-CoMe by 22.3% and 6.1% with 7B models, respectively.", "AI": {"tldr": "This work introduces ADAMIX, an adaptive mixed-precision delta-compression framework for compressing fine-tuned large language models. It provides a mathematical derivation to optimize bit allocation and demonstrates superior performance compared to existing methods.", "motivation": "Existing delta-compression approaches for large language models either perform poorly at high compression ratios or rely on empirical bit allocation schemes.", "method": "Proposes ADAMIX, which uses a mixed-precision compression strategy based on quantization error derivation and formulates optimal bit allocation as a 0/1 integer linear programming problem.", "result": "ADAMIX outperforms the best baseline Delta-CoMe by 22.3% and 6.1% on tasks like AIME2024 and GQA with 7B models respectively.", "conclusion": "ADAMIX provides an effective adaptive mixed-precision delta-compression framework that improves performance especially when the base model lacks sufficient ability."}}
{"id": "2506.11089", "pdf": "https://arxiv.org/pdf/2506.11089", "abs": "https://arxiv.org/abs/2506.11089", "authors": ["Jeena Prakash", "Blessingh Kumar", "Kadri Hacioglu", "Bidisha Sharma", "Sindhuja Gopalan", "Malolan Chetlur", "Shankar Venkatesan", "Andreas Stolcke"], "title": "Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Automatic speech recognition (ASR) models rely on high-quality transcribed\ndata for effective training. Generating pseudo-labels for large unlabeled audio\ndatasets often relies on complex pipelines that combine multiple ASR outputs\nthrough multi-stage processing, leading to error propagation, information loss\nand disjoint optimization. We propose a unified multi-ASR prompt-driven\nframework using postprocessing by either textual or speech-based large language\nmodels (LLMs), replacing voting or other arbitration logic for reconciling the\nensemble outputs. We perform a comparative study of multiple architectures with\nand without LLMs, showing significant improvements in transcription accuracy\ncompared to traditional methods. Furthermore, we use the pseudo-labels\ngenerated by the various approaches to train semi-supervised ASR models for\ndifferent datasets, again showing improved performance with textual and\nspeechLLM transcriptions compared to baselines.", "AI": {"tldr": "A unified multi-ASR prompt-driven framework is proposed to generate pseudo-labels for large unlabeled audio datasets. This framework uses postprocessing by either textual or speech-based large language models (LLMs) to replace voting or other arbitration logic for reconciling ensemble outputs.", "motivation": "Generating pseudo-labels for large unlabeled audio datasets often relies on complex pipelines that combine multiple ASR outputs through multi-stage processing, leading to error propagation, information loss and disjoint optimization.", "method": "A unified multi-ASR prompt-driven framework using postprocessing by either textual or speech-based large language models (LLMs) is proposed.", "result": "Significant improvements in transcription accuracy compared to traditional methods are shown.", "conclusion": "Using the pseudo-labels generated by the various approaches to train semi-supervised ASR models for different datasets shows improved performance with textual and speechLLM transcriptions compared to baselines."}}
{"id": "2506.11096", "pdf": "https://arxiv.org/pdf/2506.11096", "abs": "https://arxiv.org/abs/2506.11096", "authors": ["Guillaume Wisniewski", "S\u00e9verine Guillaume", "Clara Rosina Fern\u00e1ndez"], "title": "Assessing the Impact of Anisotropy in Neural Representations of Speech: A Case Study on Keyword Spotting", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Pretrained speech representations like wav2vec2 and HuBERT exhibit strong\nanisotropy, leading to high similarity between random embeddings. While widely\nobserved, the impact of this property on downstream tasks remains unclear. This\nwork evaluates anisotropy in keyword spotting for computational documentary\nlinguistics. Using Dynamic Time Warping, we show that despite anisotropy,\nwav2vec2 similarity measures effectively identify words without transcription.\nOur results highlight the robustness of these representations, which capture\nphonetic structures and generalize across speakers. Our results underscore the\nimportance of pretraining in learning rich and invariant speech\nrepresentations.", "AI": {"tldr": "This work evaluates the impact of anisotropy in pretrained speech representations (wav2vec2 and HuBERT) on keyword spotting for computational documentary linguistics.", "motivation": "To investigate the impact of anisotropy on downstream tasks.", "method": "Using Dynamic Time Warping to evaluate wav2vec2 similarity measures in identifying words without transcription.", "result": "Despite anisotropy, wav2vec2 similarity measures effectively identify words without transcription.", "conclusion": "The results highlight the robustness of these representations in capturing phonetic structures and generalizing across speakers."}}
{"id": "2506.11099", "pdf": "https://arxiv.org/pdf/2506.11099", "abs": "https://arxiv.org/abs/2506.11099", "authors": ["Huiling Zhu", "Yingqi Zeng"], "title": "Knowledge Graph Embeddings with Representing Relations as Annular Sectors", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Knowledge graphs (KGs), structured as multi-relational data of entities and\nrelations, are vital for tasks like data analysis and recommendation systems.\nKnowledge graph completion (KGC), or link prediction, addresses incompleteness\nof KGs by inferring missing triples (h, r, t). It is vital for downstream\napplications. Region-based embedding models usually embed entities as points\nand relations as geometric regions to accomplish the task. Despite progress,\nthese models often overlook semantic hierarchies inherent in entities. To solve\nthis problem, we propose SectorE, a novel embedding model in polar coordinates.\nRelations are modeled as annular sectors, combining modulus and phase to\ncapture inference patterns and relation attributes. Entities are embedded as\npoints within these sectors, intuitively encoding hierarchical structure.\nEvaluated on FB15k-237, WN18RR, and YAGO3-10, SectorE achieves competitive\nperformance against various kinds of models, demonstrating strengths in\nsemantic modeling capability.", "AI": {"tldr": "SectorE, a novel embedding model in polar coordinates, improves knowledge graph completion by modeling relations as annular sectors and entities as points within these sectors, achieving competitive performance.", "motivation": "The motivation is to address the issue of overlooking semantic hierarchies inherent in entities in existing region-based embedding models.", "method": "SectorE uses a novel embedding model in polar coordinates where relations are modeled as annular sectors and entities as points within these sectors.", "result": "SectorE achieves competitive performance against various kinds of models on FB15k-237, WN18RR, and YAGO3-10.", "conclusion": "SectorE demonstrates competitive performance and strength in semantic modeling capability."}}
{"id": "2506.11221", "pdf": "https://arxiv.org/pdf/2506.11221", "abs": "https://arxiv.org/abs/2506.11221", "authors": ["Weibing Zheng", "Laurah Turner", "Jess Kropczynski", "Murat Ozer", "Tri Nguyen", "Shane Halse"], "title": "LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic", "categories": ["cs.AI", "cs.CL", "cs.LO", "D.2.4; K.3.1; C.3; I.2.6"], "comment": "12 pages, 1 figure, 2025 IFSA World Congress NAFIPS Annual Meeting", "summary": "Clinical communication skills are critical in medical education, and\npracticing and assessing clinical communication skills on a scale is\nchallenging. Although LLM-powered clinical scenario simulations have shown\npromise in enhancing medical students' clinical practice, providing automated\nand scalable clinical evaluation that follows nuanced physician judgment is\ndifficult. This paper combines fuzzy logic and Large Language Model (LLM) and\nproposes LLM-as-a-Fuzzy-Judge to address the challenge of aligning the\nautomated evaluation of medical students' clinical skills with subjective\nphysicians' preferences. LLM-as-a-Fuzzy-Judge is an approach that LLM is\nfine-tuned to evaluate medical students' utterances within student-AI patient\nconversation scripts based on human annotations from four fuzzy sets, including\nProfessionalism, Medical Relevance, Ethical Behavior, and Contextual\nDistraction. The methodology of this paper started from data collection from\nthe LLM-powered medical education system, data annotation based on\nmultidimensional fuzzy sets, followed by prompt engineering and the supervised\nfine-tuning (SFT) of the pre-trained LLMs using these human annotations. The\nresults show that the LLM-as-a-Fuzzy-Judge achieves over 80\\% accuracy, with\nmajor criteria items over 90\\%, effectively leveraging fuzzy logic and LLM as a\nsolution to deliver interpretable, human-aligned assessment. This work suggests\nthe viability of leveraging fuzzy logic and LLM to align with human\npreferences, advances automated evaluation in medical education, and supports\nmore robust assessment and judgment practices. The GitHub repository of this\nwork is available at https://github.com/2sigmaEdTech/LLMAsAJudge", "AI": {"tldr": "This paper proposes LLM-as-a-Fuzzy-Judge, which combines fuzzy logic and large language model to automate and scale the evaluation of medical students' clinical skills, achieving over 80% accuracy.", "motivation": "Clinical communication skills are crucial in medical education, but it's challenging to assess them on a large scale.", "method": "Fine-tune LLM to evaluate medical students' utterances based on human annotations from four fuzzy sets.", "result": "LLM-as-a-Fuzzy-Judge achieves over 80% accuracy with major criteria items over 90%.", "conclusion": "This work shows the feasibility of using fuzzy logic and LLM for human-aligned assessment in medical education."}}
{"id": "2506.11237", "pdf": "https://arxiv.org/pdf/2506.11237", "abs": "https://arxiv.org/abs/2506.11237", "authors": ["Ngoc Phuoc An Vo", "Brent Paulovicks", "Vadim Sheinin"], "title": "LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation", "categories": ["cs.SE", "cs.CL"], "comment": "10 pages", "summary": "In an effort to automatically evaluate and select the best model and improve\ncode quality for automatic incident remediation in IT Automation, it is crucial\nto verify if the generated code for remediation action is syntactically and\nsemantically correct and whether it can be executed correctly as intended.\nThere are three approaches: 1) conventional methods use surface form similarity\nmetrics (token match, exact match, etc.) which have numerous limitations, 2)\nexecution-based evaluation focuses more on code functionality based on\npass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs\nfor automated evaluation to judge if it is a correct answer for a given problem\nbased on pre-defined metrics. In this work, we focused on enhancing\nLLM-as-a-Judge using bidirectional functionality matching and logic\nrepresentation for reference-less automatic validation and refinement for Bash\ncode generation to select the best model for automatic incident remediation in\nIT Automation. We used execution-based evaluation as ground-truth to evaluate\nour LLM-as-a-Judge metrics. Results show high accuracy and agreement with\nexecution-based evaluation (and up to 8% over baseline). Finally, we built\nReflection code agents to utilize judgments and feedback from our evaluation\nmetrics which achieved significant improvement (up to 24% increase in accuracy)\nfor automatic code refinement.", "AI": {"tldr": "Enhance LLM-as-a-Judge using bidirectional functionality matching and logic representation for Bash code generation to select the best model for automatic incident remediation in IT Automation.", "motivation": "Verify the correctness and executability of generated remediation code.", "method": "Improve LLM-as-a-Judge with bidirectional functionality matching and logic representation, use execution-based evaluation as ground-truth.", "result": "High accuracy and agreement with execution-based evaluation (up to 8% over baseline).", "conclusion": "Built Reflection code agents to improve automatic code refinement by up to 24%."}}
{"id": "2506.11350", "pdf": "https://arxiv.org/pdf/2506.11350", "abs": "https://arxiv.org/abs/2506.11350", "authors": ["Heinrich Dinkel", "Zhiyong Yan", "Tianzi Wang", "Yongqing Wang", "Xingwei Sun", "Yadong Niu", "Jizhong Liu", "Gang Li", "Junbo Zhang", "Jian Luan"], "title": "GLAP: General contrastive audio-text pretraining across domains and languages", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Contrastive Language Audio Pretraining (CLAP) is a widely-used method to\nbridge the gap between audio and text domains. Current CLAP methods enable\nsound and music retrieval in English, ignoring multilingual spoken content. To\naddress this, we introduce general language audio pretraining (GLAP), which\nexpands CLAP with multilingual and multi-domain abilities. GLAP demonstrates\nits versatility by achieving competitive performance on standard audio-text\nretrieval benchmarks like Clotho and AudioCaps, while significantly surpassing\nexisting methods in speech retrieval and classification tasks. Additionally,\nGLAP achieves strong results on widely used sound-event zero-shot benchmarks,\nwhile simultaneously outperforming previous methods on speech content\nbenchmarks. Further keyword spotting evaluations across 50 languages emphasize\nGLAP's advanced multilingual capabilities. Finally, multilingual sound and\nmusic understanding is evaluated across four languages. Checkpoints and Source:\nhttps://github.com/xiaomi-research/dasheng-glap.", "AI": {"tldr": "This paper introduces GLAP, an advanced method for bridging audio and text domains with multilingual and multi-domain abilities, showing competitive performance across various benchmarks.", "motivation": "To bridge the gap between audio and text domains and expand CLAP with multilingual and multi-domain abilities.", "method": "General language audio pretraining (GLAP)", "result": "Achieved strong results on sound-event zero-shot benchmarks and speech content benchmarks. Advanced multilingual capabilities were also emphasized.", "conclusion": "GLAP has achieved competitive performance on standard audio-text retrieval benchmarks and surpassed existing methods in speech retrieval and classification tasks."}}
{"id": "2506.11375", "pdf": "https://arxiv.org/pdf/2506.11375", "abs": "https://arxiv.org/abs/2506.11375", "authors": ["Yitong Zhou", "Mingyue Cheng", "Qingyang Mao", "Yucong Luo", "Qi Liu", "Yupeng Li", "Xiaohan Zhang", "Deguang Liu", "Xin Li", "Enhong Chen"], "title": "Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Chemical tables encode complex experimental knowledge through symbolic\nexpressions, structured variables, and embedded molecular graphics. Existing\nbenchmarks largely overlook this multimodal and domain-specific complexity,\nlimiting the ability of multimodal large language models to support scientific\nunderstanding in chemistry. In this work, we introduce ChemTable, a large-scale\nbenchmark of real-world chemical tables curated from the experimental sections\nof literature. ChemTable includes expert-annotated cell polygons, logical\nlayouts, and domain-specific labels, including reagents, catalysts, yields, and\ngraphical components and supports two core tasks: (1) Table Recognition,\ncovering structure parsing and content extraction; and (2) Table Understanding,\nencompassing both descriptive and reasoning-oriented question answering\ngrounded in table structure and domain semantics. We evaluated a range of\nrepresentative multimodal models, including both open-source and closed-source\nmodels, on ChemTable and reported a series of findings with practical and\nconceptual insights. Although models show reasonable performance on basic\nlayout parsing, they exhibit substantial limitations on both descriptive and\ninferential QA tasks compared to human performance, and we observe significant\nperformance gaps between open-source and closed-source models across multiple\ndimensions. These results underscore the challenges of chemistry-aware table\nunderstanding and position ChemTable as a rigorous and realistic benchmark for\nadvancing scientific reasoning.", "AI": {"tldr": "This paper introduces ChemTable, a new benchmark for chemical tables that includes expert annotations and supports two tasks: table recognition and understanding.", "motivation": "The existing benchmarks overlook the complexity of chemical tables, limiting the ability of multimodal large language models to support scientific understanding in chemistry.", "method": "The authors curated ChemTable from literature and included expert-annotated cell polygons, logical layouts, and domain-specific labels.", "result": "The evaluation of representative multimodal models on ChemTable showed reasonable performance on basic layout parsing but substantial limitations on descriptive and inferential QA tasks compared to human performance.", "conclusion": "The results highlight the challenges of chemistry-aware table understanding and position ChemTable as a rigorous and realistic benchmark for advancing scientific reasoning."}}
{"id": "2506.11376", "pdf": "https://arxiv.org/pdf/2506.11376", "abs": "https://arxiv.org/abs/2506.11376", "authors": ["Liying Wang", "Ph. D.", "Daffodil Carrington", "M. S.", "Daniil Filienko", "M. S.", "Caroline El Jazmi", "M. S.", "Serena Jinchen Xie", "M. S.", "Martine De Cock", "Ph. D.", "Sarah Iribarren", "Ph. D.", "Weichao Yuwen", "Ph. D"], "title": "Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Family caregivers often face substantial mental health challenges due to\ntheir multifaceted roles and limited resources. This study explored the\npotential of a large language model (LLM)-powered conversational agent to\ndeliver evidence-based mental health support for caregivers, specifically\nProblem-Solving Therapy (PST) integrated with Motivational Interviewing (MI)\nand Behavioral Chain Analysis (BCA). A within-subject experiment was conducted\nwith 28 caregivers interacting with four LLM configurations to evaluate empathy\nand therapeutic alliance. The best-performing models incorporated Few-Shot and\nRetrieval-Augmented Generation (RAG) prompting techniques, alongside\nclinician-curated examples. The models showed improved contextual understanding\nand personalized support, as reflected by qualitative responses and\nquantitative ratings on perceived empathy and therapeutic alliances.\nParticipants valued the model's ability to validate emotions, explore\nunexpressed feelings, and provide actionable strategies. However, balancing\nthorough assessment with efficient advice delivery remains a challenge. This\nwork highlights the potential of LLMs in delivering empathetic and tailored\nsupport for family caregivers.", "AI": {"tldr": "This study investigates the use of a large language model-powered conversational agent to deliver mental health support for family caregivers through Problem-Solving Therapy integrated with Motivational Interviewing and Behavioral Chain Analysis. Results show that models using Few-Shot and Retrieval-Augmented Generation prompting techniques can improve contextual understanding and personalized support.", "motivation": "To address the mental health challenges faced by family caregivers due to their roles and limited resources, by providing them with evidence-based mental health support.", "method": "A within-subject experiment was conducted with 28 caregivers who interacted with four LLM configurations. The best-performing models used Few-Shot and Retrieval-Augmented Generation prompting techniques along with clinician-curated examples.", "result": "The models showed improved contextual understanding and personalized support, as reflected by qualitative responses and quantitative ratings on perceived empathy and therapeutic alliances. Participants appreciated the model's ability to validate emotions, explore unexpressed feelings, and provide actionable strategies.", "conclusion": "LLMs have the potential to deliver empathetic and tailored support for family caregivers."}}
{"id": "2506.11402", "pdf": "https://arxiv.org/pdf/2506.11402", "abs": "https://arxiv.org/abs/2506.11402", "authors": ["Pradyut Sekhsaria", "Marcel Mateos Salles", "Hai Huang", "Randall Balestriero"], "title": "LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "29 pages, 16 figures, 15 tables. Submitted for publication. for\n  associated blog post, see https://pradyut3501.github.io/lora-spur-corr/", "summary": "Parameter Efficient FineTuning (PEFT), such as Low-Rank Adaptation (LoRA),\naligns pre-trained Large Language Models (LLMs) to particular downstream tasks\nin a resource-efficient manner. Because efficiency has been the main metric of\nprogress, very little attention has been put in understanding possible\ncatastrophic failures. We uncover one such failure: PEFT encourages a model to\nsearch for shortcut solutions to solve its fine-tuning tasks. When very small\namount of tokens, e.g., one token per prompt, are correlated with downstream\ntask classes, PEFT makes any pretrained model rely predominantly on that token\nfor decision making. While such spurious tokens may emerge accidentally from\nincorrect data cleaning, it also opens opportunities for malevolent parties to\ncontrol a model's behavior from Seamless Spurious Token Injection (SSTI). In\nSSTI, a small amount of tokens correlated with downstream classes are injected\nby the dataset creators. At test time, the finetuned LLM's behavior can be\ncontrolled solely by injecting those few tokens. We apply SSTI across models\nfrom three families (Snowflake Arctic, Apple OpenELM, and Meta LLaMA-3) and\nfour diverse datasets (IMDB, Financial Classification, CommonSense QA, and Bias\nin Bios). Our findings reveal three astonishing behaviors. First, as few as a\nsingle token of SSTI is sufficient to steer a model's decision making. Second,\nfor light SSTI, the reliance on spurious tokens is proportional to the LoRA\nrank. Lastly, with aggressive SSTI, larger LoRA rank values become preferable\nto small rank values as it makes the model attend to non-spurious tokens, hence\nimproving robustness.", "AI": {"tldr": "This paper uncovers how PEFT methods like LoRA can lead to catastrophic failures due to reliance on spurious tokens, which can be exploited via SSTI to control model behavior.", "motivation": "To understand possible catastrophic failures in PEFT methods and the potential misuse through SSTI.", "method": "The researchers investigated the effect of PEFT methods on the reliance of models on spurious tokens and conducted experiments across different language models and datasets.", "result": "The study found that as few as one token of SSTI can steer a model's decision making, reliance on spurious tokens increases with LoRA rank under light SSTI, and larger LoRA ranks become preferable under aggressive SSTI for improving robustness.", "conclusion": "Parameter Efficient FineTuning (PEFT) like Low-Rank Adaptation (LoRA) can lead to catastrophic failures where models overly depend on spurious tokens. This study reveals how these spurious tokens can be used maliciously via Seamless Spurious Token Injection (SSTI) to control model behavior."}}
{"id": "2506.11415", "pdf": "https://arxiv.org/pdf/2506.11415", "abs": "https://arxiv.org/abs/2506.11415", "authors": ["Linlin Wang", "Tianqing Zhu", "Laiqiao Qin", "Longxiang Gao", "Wanlei Zhou"], "title": "Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "In Large Language Models, Retrieval-Augmented Generation (RAG) systems can\nsignificantly enhance the performance of large language models by integrating\nexternal knowledge. However, RAG also introduces new security risks. Existing\nresearch focuses mainly on how poisoning attacks in RAG systems affect model\noutput quality, overlooking their potential to amplify model biases. For\nexample, when querying about domestic violence victims, a compromised RAG\nsystem might preferentially retrieve documents depicting women as victims,\ncausing the model to generate outputs that perpetuate gender stereotypes even\nwhen the original query is gender neutral. To show the impact of the bias, this\npaper proposes a Bias Retrieval and Reward Attack (BRRA) framework, which\nsystematically investigates attack pathways that amplify language model biases\nthrough a RAG system manipulation. We design an adversarial document generation\nmethod based on multi-objective reward functions, employ subspace projection\ntechniques to manipulate retrieval results, and construct a cyclic feedback\nmechanism for continuous bias amplification. Experiments on multiple mainstream\nlarge language models demonstrate that BRRA attacks can significantly enhance\nmodel biases in dimensions. In addition, we explore a dual stage defense\nmechanism to effectively mitigate the impacts of the attack. This study reveals\nthat poisoning attacks in RAG systems directly amplify model output biases and\nclarifies the relationship between RAG system security and model fairness. This\nnovel potential attack indicates that we need to keep an eye on the fairness\nissues of the RAG system.", "AI": {"tldr": "RAG\u7cfb\u7edf\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8RAG\u7cfb\u7edf\u7684\u6295\u6bd2\u653b\u51fb\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u653e\u5927\u6a21\u578b\u504f\u89c1\u7684\u6f5c\u529b\u3002\u672c\u6587\u63d0\u51fa\u4e86Bias Retrieval and Reward Attack (BRRA)\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\u7684\u5bf9\u6297\u6027\u6587\u6863\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u5faa\u73af\u53cd\u9988\u673a\u5236\u6765\u6301\u7eed\u653e\u5927\u504f\u89c1\u3002\u5b9e\u9a8c\u8868\u660e\uff0cBRRA\u653b\u51fb\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u6a21\u578b\u7684\u504f\u89c1\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u53cc\u9636\u6bb5\u9632\u5fa1\u673a\u5236\u6765\u6709\u6548\u51cf\u8f7b\u653b\u51fb\u7684\u5f71\u54cd\u3002\u8fd9\u9879\u7814\u7a76\u63ed\u793a\u4e86RAG\u7cfb\u7edf\u4e2d\u7684\u6295\u6bd2\u653b\u51fb\u4f1a\u76f4\u63a5\u653e\u5927\u6a21\u578b\u8f93\u51fa\u504f\u89c1\uff0c\u5e76\u9610\u660e\u4e86RAG\u7cfb\u7edf\u5b89\u5168\u4e0e\u6a21\u578b\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8RAG\u7cfb\u7edf\u7684\u6295\u6bd2\u653b\u51fb\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u653e\u5927\u6a21\u578b\u504f\u89c1\u7684\u6f5c\u529b\u3002\u4f8b\u5982\uff0c\u5f53\u67e5\u8be2\u5173\u4e8e\u5bb6\u5ead\u66b4\u529b\u53d7\u5bb3\u8005\u65f6\uff0c\u4e00\u4e2a\u88ab\u7be1\u6539\u7684RAG\u7cfb\u7edf\u53ef\u80fd\u4f1a\u4f18\u5148\u68c0\u7d22\u5c06\u5973\u6027\u63cf\u7ed8\u4e3a\u53d7\u5bb3\u8005\u7684\u6587\u4ef6\uff0c\u5bfc\u81f4\u6a21\u578b\u5373\u4f7f\u5728\u539f\u59cb\u67e5\u8be2\u6027\u522b\u4e2d\u7acb\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u5ef6\u7eed\u6027\u522b\u523b\u677f\u5370\u8c61\u7684\u8f93\u51fa\u3002", "method": "\u63d0\u51faBias Retrieval and Reward Attack (BRRA)\u6846\u67b6\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\u7684\u5bf9\u6297\u6027\u6587\u6863\u751f\u6210\u65b9\u6cd5\uff0c\u4f7f\u7528\u5b50\u7a7a\u95f4\u6295\u5f71\u6280\u672f\u64cd\u7eb5\u68c0\u7d22\u7ed3\u679c\uff0c\u5e76\u6784\u5efa\u5faa\u73af\u53cd\u9988\u673a\u5236\u8fdb\u884c\u8fde\u7eed\u504f\u89c1\u653e\u5927\u3002", "result": "BRRA\u653b\u51fb\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u6a21\u578b\u7684\u504f\u89c1\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63ed\u793a\u4e86RAG\u7cfb\u7edf\u4e2d\u7684\u6295\u6bd2\u653b\u51fb\u4f1a\u76f4\u63a5\u653e\u5927\u6a21\u578b\u8f93\u51fa\u504f\u89c1\uff0c\u5e76\u9610\u660e\u4e86RAG\u7cfb\u7edf\u5b89\u5168\u4e0e\u6a21\u578b\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u8fd9\u9879\u7814\u7a76\u6307\u51fa\u6211\u4eec\u9700\u8981\u5173\u6ce8RAG\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002"}}
{"id": "2506.11475", "pdf": "https://arxiv.org/pdf/2506.11475", "abs": "https://arxiv.org/abs/2506.11475", "authors": ["Syeda Kisaa Fatima", "Tehreem Zubair", "Noman Ahmed", "Asifullah Khan"], "title": "AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction", "categories": ["cs.MA", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper introduces LUCID-MA (Learning and Understanding Crime through\nDialogue of Multiple Agents), an innovative AI powered framework where multiple\nAI agents collaboratively analyze and understand crime data. Our system that\nconsists of three core components: an analysis assistant that highlights\nspatiotemporal crime patterns, a feedback component that reviews and refines\nanalytical results and a prediction component that forecasts future crime\ntrends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it\nruns completely offline and allows the agents undergo self-improvement through\n100 rounds of communication with less human interaction. A scoring function is\nincorporated to evaluate agent's performance, providing visual plots to track\nlearning progress. This work demonstrates the potential of AutoGen-style agents\nfor autonomous, scalable, and iterative analysis in social science domains\nmaintaining data privacy through offline execution.", "AI": {"tldr": "An AI-powered framework named LUCID-MA uses multiple agents to analyze crime data offline with minimal human interaction.", "motivation": "To use AI for collaborative analysis and understanding of crime data while maintaining data privacy.", "method": "The framework has three components: analysis assistant, feedback component, and prediction component. It uses the LLaMA-2-13B-Chat-GPTQ model and incorporates a scoring function for evaluation.", "result": "The system can run offline and allows agents to improve through 100 rounds of communication.", "conclusion": "This work shows the potential of AutoGen-style agents for autonomous analysis in social science domains with data privacy."}}
{"id": "2506.11515", "pdf": "https://arxiv.org/pdf/2506.11515", "abs": "https://arxiv.org/abs/2506.11515", "authors": ["Xiao Xu", "Libo Qin", "Wanxiang Che", "Min-Yen Kan"], "title": "Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT). June 2025. DOI:\n  https://doi.org/10.1109/TCSVT.2025.3578266", "summary": "Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance\nacross various downstream VL tasks. While BridgeTower further enhances\nperformance by building bridges between encoders, it \\textit{(i)} suffers from\nineffective layer-by-layer utilization of unimodal representations,\n\\textit{(ii)} restricts the flexible exploitation of different levels of\nunimodal semantic knowledge, and \\textit{(iii)} is limited to the evaluation on\ntraditional low-resolution datasets only with the Two-Tower VLM architecture.\nIn this work, we propose Manager, a lightweight, efficient and effective plugin\nthat adaptively aggregates insights from different levels of pre-trained\nunimodal experts to facilitate more comprehensive VL alignment and fusion.\nFirst, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel\nVLM that introduces the manager in each cross-modal layer. Whether with or\nwithout VL pre-training, ManagerTower outperforms previous strong baselines and\nachieves superior performance on 4 downstream VL tasks. Moreover, we extend our\nexploration to the latest Multimodal Large Language Model (MLLM) architecture.\nWe demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot\nperformance of LLaVA-OV across different categories of capabilities, images,\nand resolutions on 20 downstream datasets, whether the multi-grid algorithm is\nenabled or not. In-depth analysis reveals that both our manager and the\nmulti-grid algorithm can be viewed as a plugin that improves the visual\nrepresentation by capturing more diverse visual details from two orthogonal\nperspectives (depth and width). Their synergy can mitigate the semantic\nambiguity caused by the multi-grid algorithm and further improve performance.\nCode and models are available at https://github.com/LooperXX/ManagerTower.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faManagerTower\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "BridgeTower\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\uff0c\u5982\u672a\u5145\u5206\u5229\u7528\u5355\u6a21\u6001\u8868\u793a\u3001\u9650\u5236\u4e86\u7075\u6d3b\u5229\u7528\u4e0d\u540c\u5c42\u6b21\u7684\u5355\u6a21\u6001\u8bed\u4e49\u77e5\u8bc6\u4ee5\u53ca\u4ec5\u9650\u4e8e\u4f20\u7edf\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u4e14\u6709\u6548\u7684\u63d2\u4ef6Manager\uff0c\u5b83\u81ea\u9002\u5e94\u5730\u805a\u5408\u4e0d\u540c\u7ea7\u522b\u7684\u9884\u8bad\u7ec3\u5355\u6a21\u6001\u4e13\u5bb6\u7684\u89c1\u89e3\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u5168\u9762\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u548c\u878d\u5408\u3002", "result": "ManagerTower\u5728\u56db\u79cd\u4e0b\u6e38\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\uff1bLLaVA-OV-Manager\u663e\u8457\u63d0\u9ad8\u4e86LLaVA-OV\u572820\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "ManagerTower\u5728\u4e24\u79cd\u67b6\u6784\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14Manager\u548c\u591a\u7f51\u683c\u7b97\u6cd5\u7684\u534f\u540c\u4f5c\u7528\u53ef\u4ee5\u7f13\u89e3\u8bed\u4e49\u6b67\u4e49\u5e76\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2506.11516", "pdf": "https://arxiv.org/pdf/2506.11516", "abs": "https://arxiv.org/abs/2506.11516", "authors": ["Chengye Li", "Haiyun Liu", "Yuanxi Li"], "title": "Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning", "categories": ["cs.LG", "cs.CL"], "comment": "10 main pages, 10 page appendix", "summary": "In-context learning (ICL) allows large language models (LLMs) to solve novel\ntasks without weight updates. Despite its empirical success, the mechanism\nbehind ICL remains poorly understood, limiting our ability to interpret,\nimprove, and reliably apply it. In this paper, we propose a new theoretical\nperspective that interprets ICL as an implicit form of knowledge distillation\n(KD), where prompt demonstrations guide the model to form a task-specific\nreference model during inference. Under this view, we derive a Rademacher\ncomplexity-based generalization bound and prove that the bias of the distilled\nweights grows linearly with the Maximum Mean Discrepancy (MMD) between the\nprompt and target distributions. This theoretical framework explains several\nempirical phenomena and unifies prior gradient-based and distributional\nanalyses. To the best of our knowledge, this is the first to formalize\ninference-time attention as a distillation process, which provides theoretical\ninsights for future prompt engineering and automated demonstration selection.", "AI": {"tldr": "This paper proposes a new theoretical perspective that interprets in-context learning as an implicit form of knowledge distillation, providing a generalization bound and insights for prompt engineering.", "motivation": "To understand the mechanism behind in-context learning and provide theoretical support for its interpretation, improvement, and reliable application.", "method": "Proposes a theoretical perspective interpreting in-context learning as an implicit form of knowledge distillation, derives a generalization bound based on Rademacher complexity, and proves the relationship between the bias of the distilled weights and the MMD between prompt and target distributions.", "result": "Derives a Rademacher complexity-based generalization bound and explains several empirical phenomena, unifying prior analyses.", "conclusion": "This is the first work to formalize inference-time attention as a distillation process, offering theoretical insights for future prompt engineering and automated demonstration selection."}}
{"id": "2506.11555", "pdf": "https://arxiv.org/pdf/2506.11555", "abs": "https://arxiv.org/abs/2506.11555", "authors": ["Yu Wang", "Shiwan Zhao", "Ming Fan", "Zhihu Wang", "Yubo Zhang", "Xicheng Zhang", "Zhengfan Wang", "Heyuan Huang", "Ting Liu"], "title": "RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The integration of external knowledge through Retrieval-Augmented Generation\n(RAG) has become foundational in enhancing large language models (LLMs) for\nknowledge-intensive tasks. However, existing RAG paradigms often overlook the\ncognitive step of applying knowledge, leaving a gap between retrieved facts and\ntask-specific reasoning. In this work, we introduce RAG+, a principled and\nmodular extension that explicitly incorporates application-aware reasoning into\nthe RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and\naligned application examples, created either manually or automatically, and\nretrieves both jointly during inference. This design enables LLMs not only to\naccess relevant information but also to apply it within structured,\ngoal-oriented reasoning processes. Experiments across mathematical, legal, and\nmedical domains, conducted on multiple models, demonstrate that RAG+\nconsistently outperforms standard RAG variants, achieving average improvements\nof 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval\nwith actionable application, RAG+ advances a more cognitively grounded\nframework for knowledge integration, representing a step toward more\ninterpretable and capable LLMs.", "AI": {"tldr": "RAG+ is an enhanced version of RAG that adds application-aware reasoning, improving performance by 3-5% across different domains.", "motivation": "Existing RAG methods lack cognitive steps in applying knowledge, leading to a gap between retrieved facts and task-specific reasoning.", "method": "RAG+ creates a dual corpus of knowledge and aligned application examples, retrieving both during inference.", "result": "RAG+ outperforms standard RAG variants, with average improvements of 3-5% and up to 7.5% in complex scenarios.", "conclusion": "RAG+ bridges retrieval with actionable application, advancing a more cognitively grounded framework for knowledge integration."}}
{"id": "2506.11558", "pdf": "https://arxiv.org/pdf/2506.11558", "abs": "https://arxiv.org/abs/2506.11558", "authors": ["Bo-Cheng Chiu", "Jen-Jee Chen", "Yu-Chee Tseng", "Feng-Chi Chen"], "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.", "AI": {"tldr": "Introduce DaMO, a data-efficient Video LLM for accurate temporal reasoning and multimodal understanding.", "motivation": "Existing Video LLMs have limitations in fine-grained temporal reasoning, especially under constrained supervision.", "method": "Propose Temporal-aware Fuseformer with hierarchical dual-stream architecture and global residual.", "result": "DaMO outperforms previous methods in temporal grounding and video QA benchmarks.", "conclusion": "This work contributes to data-efficient video-language modeling with promising results."}}
{"id": "2506.11604", "pdf": "https://arxiv.org/pdf/2506.11604", "abs": "https://arxiv.org/abs/2506.11604", "authors": ["Ren\u00e9 Peinl", "Vincent Tischler"], "title": "VLM@school -- Evaluation of AI image understanding on German middle school knowledge", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper introduces a novel benchmark dataset designed to evaluate the\ncapabilities of Vision Language Models (VLMs) on tasks that combine visual\nreasoning with subject-specific background knowledge in the German language. In\ncontrast to widely used English-language benchmarks that often rely on\nartificially difficult or decontextualized problems, this dataset draws from\nreal middle school curricula across nine domains including mathematics,\nhistory, biology, and religion. The benchmark includes over 2,000 open-ended\nquestions grounded in 486 images, ensuring that models must integrate visual\ninterpretation with factual reasoning rather than rely on superficial textual\ncues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple\ndimensions, including domain-specific accuracy and performance on adversarial\ncrafted questions. Our findings reveal that even the strongest models achieve\nless than 45% overall accuracy, with particularly poor performance in music,\nmathematics, and adversarial settings. Furthermore, the results indicate\nsignificant discrepancies between success on popular benchmarks and real-world\nmultimodal understanding. We conclude that middle school-level tasks offer a\nmeaningful and underutilized avenue for stress-testing VLMs, especially in\nnon-English contexts. The dataset and evaluation protocol serve as a rigorous\ntestbed to better understand and improve the visual and linguistic reasoning\ncapabilities of future AI systems.", "AI": {"tldr": "This paper presents a new benchmark dataset in German for evaluating Vision Language Models' (VLMs) capabilities in combining visual reasoning with subject-specific knowledge across nine domains. The dataset contains over 2,000 questions based on 486 images. Thirteen top VLMs were tested, showing less than 45% accuracy overall, highlighting the gap between benchmark success and real-world multimodal understanding.", "motivation": "To create a benchmark dataset in German that evaluates VLMs' ability to combine visual reasoning with subject-specific background knowledge in non-English contexts.", "method": "Developed a new benchmark dataset with over 2,000 open-ended questions across nine domains, using 486 images. Evaluated thirteen state-of-the-art open-weight VLMs.", "result": "Most VLMs achieved less than 45% overall accuracy, particularly struggling in music, mathematics, and adversarial settings. There is a significant discrepancy between performance on popular benchmarks and real-world multimodal understanding.", "conclusion": "Middle school-level tasks provide a meaningful way to stress-test VLMs, especially in non-English contexts. The dataset and evaluation protocol can help improve the visual and linguistic reasoning capabilities of future AI systems."}}
{"id": "2506.11620", "pdf": "https://arxiv.org/pdf/2506.11620", "abs": "https://arxiv.org/abs/2506.11620", "authors": ["Stefan Bleeck"], "title": "(SimPhon Speech Test): A Data-Driven Method for In Silico Design and Validation of a Phonetically Balanced Speech Test", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Traditional audiometry often provides an incomplete characterization of the\nfunctional impact of hearing loss on speech understanding, particularly for\nsupra-threshold deficits common in presbycusis. This motivates the development\nof more diagnostically specific speech perception tests. We introduce the\nSimulated Phoneme Speech Test (SimPhon Speech Test) methodology, a novel,\nmulti-stage computational pipeline for the in silico design and validation of a\nphonetically balanced minimal-pair speech test. This methodology leverages a\nmodern Automatic Speech Recognition (ASR) system as a proxy for a human\nlistener to simulate the perceptual effects of sensorineural hearing loss. By\nprocessing speech stimuli under controlled acoustic degradation, we first\nidentify the most common phoneme confusion patterns. These patterns then guide\nthe data-driven curation of a large set of candidate word pairs derived from a\ncomprehensive linguistic corpus. Subsequent phases involving simulated\ndiagnostic testing, expert human curation, and a final, targeted sensitivity\nanalysis systematically reduce the candidates to a final, optimized set of 25\npairs (the SimPhon Speech Test-25). A key finding is that the diagnostic\nperformance of the SimPhon Speech Test-25 test items shows no significant\ncorrelation with predictions from the standard Speech Intelligibility Index\n(SII), suggesting the SimPhon Speech Test captures perceptual deficits beyond\nsimple audibility. This computationally optimized test set offers a significant\nincrease in efficiency for audiological test development, ready for initial\nhuman trials.", "AI": {"tldr": "Developing a new speech perception test called SimPhon Speech Test using a computational pipeline to simulate hearing loss effects and identify phoneme confusion patterns, resulting in a set of 25 word pairs that show unique diagnostic performance.", "motivation": "To provide a more complete characterization of hearing loss impact on speech understanding, especially for supra-threshold deficits in presbycusis, by developing a more diagnostically specific speech perception test.", "method": "Introducing the SimPhon Speech Test methodology, which uses an ASR system to simulate hearing loss effects, identifies phoneme confusion patterns, curates word pairs from a linguistic corpus, and reduces them through simulated testing and analysis to create an optimized set of 25 pairs.", "result": "The final set of 25 word pairs (SimPhon Speech Test-25) shows diagnostic performance that doesn't correlate significantly with standard Speech Intelligibility Index predictions, indicating it captures perceptual deficits beyond simple audibility.", "conclusion": "The SimPhon Speech Test-25 offers a more efficient method for audiological test development and is ready for initial human trials."}}
{"id": "2506.11737", "pdf": "https://arxiv.org/pdf/2506.11737", "abs": "https://arxiv.org/abs/2506.11737", "authors": ["Dinh Viet Cuong", "Hoang-Bao Le", "An Pham Ngoc Nguyen", "Liting Zhou", "Cathal Gurrin"], "title": "Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "This paper addresses two main objectives. Firstly, we demonstrate the\nimpressive performance of the LLaVA-NeXT-interleave on 22 datasets across three\ndifferent tasks: Multi-Image Reasoning, Documents and Knowledge-Based\nUnderstanding and Interactive Multi-Modal Communication. Secondly, we add the\nDense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and\ncompare its performance against the standard model. We find that the standard\nmodel achieves the highest overall accuracy, excelling in vision-heavy tasks\nlike VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows\nparticular strength on datasets requiring deeper semantic coherence or\nstructured change understanding such as MIT-States_PropertyCoherence and\nSlideVQA. Our results highlight the potential of combining powerful foundation\nmodels with plug-and-play techniques for Interleave tasks. The code is\navailable at https://github.com/dinhvietcuong1996/icme25-inova.", "AI": {"tldr": "This paper presents the impressive performance of LLaVA-NeXT-interleave on multiple datasets and tasks, and explores the effectiveness of adding Dense Channel Integration (DCI) connector.", "motivation": "To demonstrate the performance of LLaVA-NeXT-interleave on various tasks and investigate the benefits of integrating DCI.", "method": "Conduct experiments on 22 datasets across three tasks using LLaVA-NeXT-interleave and compare it with the DCI-enhanced version.", "result": "The standard model performs best in vision-heavy tasks, while the DCI-enhanced version excels in datasets requiring deeper semantic coherence or structured change understanding.", "conclusion": "Combining powerful foundation models with plug-and-play techniques has great potential for interleave tasks."}}
{"id": "2506.11812", "pdf": "https://arxiv.org/pdf/2506.11812", "abs": "https://arxiv.org/abs/2506.11812", "authors": ["Margot Geerts", "Manon Reusens", "Bart Baesens", "Seppe vanden Broucke", "Jochen De Weerdt"], "title": "On the Performance of LLMs for Real Estate Appraisal", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at ECML-PKDD 2025", "summary": "The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders.", "AI": {"tldr": "Large Language Models (LLMs) can generate competitive and interpretable house price estimates through optimized In-Context Learning strategies, providing an accessible alternative to traditional machine learning models in real estate.", "motivation": "To address the significant information asymmetry in the real estate market by leveraging LLMs.", "method": "Systematically evaluating leading LLMs on diverse international housing datasets using zero-shot, few-shot, market report-enhanced, and hybrid prompting techniques.", "result": "LLMs effectively leverage hedonic variables to produce meaningful estimates but struggle with overconfidence in price intervals and limited spatial reasoning. They offer a more accessible, interactive and interpretable alternative to traditional machine learning models.", "conclusion": "LLMs have the potential to improve transparency in real estate appraisal and provide actionable insights for stakeholders."}}
{"id": "2506.11820", "pdf": "https://arxiv.org/pdf/2506.11820", "abs": "https://arxiv.org/abs/2506.11820", "authors": ["Xintong Wang", "Jingheng Pan", "Yixiao Liu", "Xiaohu Zhao", "Chenyang Lyu", "Minghao Wu", "Chris Biemann", "Longyue Wang", "Linlong Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability.", "AI": {"tldr": "\u672c\u6587\u5bf9\u89c6\u89c9-\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u5305\u62ec\u6570\u636e\u96c6\u6539\u8fdb\u3001\u6a21\u578b\u8bc4\u4f30\u53ca\u8bc4\u4f30\u6307\u6807\u4f18\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u5fae\u8c03\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1\u6700\u8fd1\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f46\u5bf9\u5b83\u4eec\u5728VLT\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u548c\u7406\u89e3\u3002", "method": "\u6211\u4eec\u5bf9\u73b0\u6709\u7684VLT\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5f15\u5165\u4e86AibTrans\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u5546\u4e1a\u548c\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86Density-Aware Evaluation\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u5e73\u8861\u591a\u8bed\u8a00\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u8bed\u4e49\u548c\u6587\u5316\u4fdd\u771f\u5ea6\u65b9\u9762\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728OCR\u4f9d\u8d56\u6027\u548c\u751f\u6210\u4e0e\u63a8\u7406\u884c\u4e3a\u4e0a\u6709\u663e\u8457\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u7684Density-Aware Evaluation\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u7a33\u5065\u7684\u7ffb\u8bd1\u8d28\u91cf\u8861\u91cf\u6807\u51c6\u3002", "conclusion": "\u6211\u4eec\u901a\u8fc7\u4ece\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u67b6\u6784\u548c\u8bc4\u4f30\u6307\u6807\u4e09\u4e2a\u5173\u952e\u89c6\u89d2\u5bf9VLT\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u53d1\u73b0\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff0c\u5e76\u5f15\u5165\u4e86AibTrans\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\u5e76\u63d0\u51fa\u4e86Density-Aware Evaluation\u6765\u89e3\u51b3\u8bc4\u4f30\u6307\u6807\u7684\u95ee\u9898\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e73\u8861\u591a\u8bed\u8a00\u5fae\u8c03\u7b56\u7565\u4ee5\u63d0\u9ad8\u8de8\u8bed\u8a00\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.11880", "pdf": "https://arxiv.org/pdf/2506.11880", "abs": "https://arxiv.org/abs/2506.11880", "authors": ["Alejandro Pe\u00f1a", "Julian Fierrez", "Aythami Morales", "Gonzalo Mancera", "Miguel Lopez", "Ruben Tolosana"], "title": "Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment", "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to AIES 2025 (Under Review)", "summary": "The use of language technologies in high-stake settings is increasing in\nrecent years, mostly motivated by the success of Large Language Models (LLMs).\nHowever, despite the great performance of LLMs, they are are susceptible to\nethical concerns, such as demographic biases, accountability, or privacy. This\nwork seeks to analyze the capacity of Transformers-based systems to learn\ndemographic biases present in the data, using a case study on AI-based\nautomated recruitment. We propose a privacy-enhancing framework to reduce\ngender information from the learning pipeline as a way to mitigate biased\nbehaviors in the final tools. Our experiments analyze the influence of data\nbiases on systems built on two different LLMs, and how the proposed framework\neffectively prevents trained systems from reproducing the bias in the data.", "AI": {"tldr": "This paper explores the potential for Transformer-based systems to adopt demographic biases within AI-driven recruitment tools, emphasizing ethical concerns like bias and privacy. It introduces a privacy-enhancing method to strip gender information from the training process to curb biased behaviors in final applications.", "motivation": "To address ethical issues such as demographic biases, accountability, and privacy concerns associated with the growing use of large language models in high-stakes environments.", "method": "Proposes a privacy-enhancing framework to remove gender information from the learning pipeline to mitigate biased behaviors in automated recruitment tools.", "result": "Experiments investigate the impact of data biases on systems developed using two distinct LLMs and demonstrate the effectiveness of the proposed framework in preventing the reproduction of bias.", "conclusion": "Transformer-based systems can exhibit demographic biases, but these can be mitigated by removing gender information during the training phase."}}
{"id": "2506.11887", "pdf": "https://arxiv.org/pdf/2506.11887", "abs": "https://arxiv.org/abs/2506.11887", "authors": ["Claudio Fanconi", "Mihaela van der Schaar"], "title": "Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea7\u8054\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u51b3\u7b56\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6839\u636e\u7f6e\u4fe1\u5ea6\u5206\u6570\u81ea\u9002\u5e94\u5730\u5728\u591a\u4e2a\u4e13\u4e1a\u77e5\u8bc6\u5c42\u7ea7\u4e4b\u95f4\u5206\u914d\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u5206\u4e24\u4e2a\u9636\u6bb5\u8fdb\u884c\uff0c\u9996\u5148\u662f\u4e00\u4e2a\u62d2\u7edd\u7b56\u7565\uff0c\u51b3\u5b9a\u662f\u5426\u63a5\u53d7\u57fa\u7840\u6a21\u578b\u7684\u7b54\u6848\u6216\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u5206\u6570\u4f7f\u7528\u5927\u578b\u6a21\u578b\u91cd\u65b0\u751f\u6210\u7b54\u6848\uff1b\u5176\u6b21\u662f\u62d2\u7edd\u7b56\u7565\uff0c\u51b3\u5b9a\u7ea7\u8054\u6a21\u578b\u54cd\u5e94\u662f\u5426\u8db3\u591f\u786e\u5b9a\u6216\u8005\u9700\u8981\u4eba\u7c7b\u5e72\u9884\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u5305\u542b\u4e00\u4e2a\u5728\u7ebf\u5b66\u4e60\u673a\u5236\uff0c\u53ef\u4ee5\u5229\u7528\u4eba\u7c7b\u53cd\u9988\u6765\u63d0\u9ad8\u51b3\u7b56\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u8be5\u7ea7\u8054\u7b56\u7565\u5728\u4fdd\u6301\u8f83\u4f4e\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5904\u7406\u62d2\u7edd\u95ee\u9898\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002", "motivation": "\u5e73\u8861\u9884\u6d4b\u7684\u6b63\u786e\u6027\u3001\u77e5\u8bc6\u83b7\u53d6\u548c\u63a8\u7406\u590d\u6742\u6027\u7684\u6210\u672c\u4ee5\u53ca\u5bf9\u81ea\u52a8\u56de\u7b54\u6216\u6d89\u53ca\u4eba\u7c7b\u4e13\u5bb6\u7684\u81ea\u4fe1\u7a0b\u5ea6\u662f\u6709\u6548\u7684\u4eba\u673a\u51b3\u7b56\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea7\u8054LLM\u51b3\u7b56\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u57fa\u7840\u6a21\u578b\u3001\u66f4\u5f3a\u5927\u4e14\u77e5\u8bc6\u66f4\u4e30\u5bcc\u7684\u5927\u578b\u6a21\u578b\u4ee5\u53ca\u5f53\u6a21\u578b\u7ea7\u8054\u62d2\u7edd\u65f6\u7684\u4eba\u7c7b\u4e13\u5bb6\u3002\u8be5\u65b9\u6cd5\u5206\u4e24\u4e2a\u9636\u6bb5\u8fdb\u884c\uff1a\u9996\u5148\u7531\u62d2\u7edd\u7b56\u7565\u51b3\u5b9a\u662f\u5426\u63a5\u53d7\u57fa\u7840\u6a21\u578b\u7684\u7b54\u6848\uff0c\u7136\u540e\u7531\u62d2\u7edd\u7b56\u7565\u51b3\u5b9a\u7ea7\u8054\u6a21\u578b\u54cd\u5e94\u662f\u5426\u9700\u8981\u4eba\u7c7b\u5e72\u9884\u3002\u6b64\u5916\uff0c\u6846\u67b6\u8fd8\u5305\u62ec\u4e00\u4e2a\u5728\u7ebf\u5b66\u4e60\u673a\u5236\uff0c\u53ef\u5229\u7528\u4eba\u7c7b\u53cd\u9988\u6539\u5584\u51b3\u7b56\u8d28\u91cf\u3002", "result": "\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u7ea7\u8054\u7b56\u7565\u5728\u4fdd\u6301\u8f83\u4f4e\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5904\u7406\u62d2\u7edd\u95ee\u9898\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ea7\u8054LLM\u51b3\u7b56\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5728\u591a\u4e2a\u4e13\u4e1a\u77e5\u8bc6\u5c42\u7ea7\u4e4b\u95f4\u5206\u914d\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u4eba\u673a\u51b3\u7b56\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.11902", "pdf": "https://arxiv.org/pdf/2506.11902", "abs": "https://arxiv.org/abs/2506.11902", "authors": ["Zhenyu Hou", "Ziniu Hu", "Yujiang Li", "Rui Lu", "Jie Tang", "Yuxiao Dong"], "title": "TreeRL: LLM Reinforcement Learning with On-Policy Tree Search", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025 main conference", "summary": "Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL.", "AI": {"tldr": "TreeRL is a novel reinforcement learning framework that integrates on-policy tree search for better exploration and more efficient training of large language models.", "motivation": "To address the limitations of conventional independent chain sampling strategies with outcome supervision in RL training, such as poor exploration of the reasoning space and sparse rewards.", "method": "TreeRL incorporates on-policy tree search for RL training, including intermediate supervision and eliminating the need for a separate reward model training. It also introduces a cost-effective tree search approach.", "result": "TreeRL outperforms traditional ChainRL on challenging math and code reasoning benchmarks.", "conclusion": "TreeRL demonstrates the potential of tree search for LLM in RL training."}}
{"id": "2506.11928", "pdf": "https://arxiv.org/pdf/2506.11928", "abs": "https://arxiv.org/abs/2506.11928", "authors": ["Zihan Zheng", "Zerui Cheng", "Zeyu Shen", "Shang Zhou", "Kaiyuan Liu", "Hansen He", "Dongruixuan Li", "Stanley Wei", "Hangyi Hao", "Jianzhu Yao", "Peiyao Sheng", "Zixuan Wang", "Wenhao Chai", "Aleksandra Korolova", "Peter Henderson", "Sanjeev Arora", "Pramod Viswanath", "Jingbo Shang", "Saining Xie"], "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project Page at https://livecodebenchpro.com/", "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.", "AI": {"tldr": "Large language models (LLMs) are claimed to surpass elite humans in competitive programming. However, a study using a new benchmark called LiveCodeBench Pro, which includes problems from major coding competitions and is continuously updated to avoid data contamination, shows that even the best current models perform significantly worse than expert humans on harder problems. The models excel in implementation tasks but struggle with complex algorithmic reasoning and case analysis, suggesting their success relies more on implementation accuracy and tool usage rather than advanced reasoning abilities.", "motivation": "To examine how LLMs compare to human experts in competitive programming and identify areas where they still fall short.", "method": "Developed and used LiveCodeBench Pro, a benchmark featuring continuously updated problems from major coding competitions, annotated by Olympiad medalists.", "result": "The best LLM achieved only 53% pass@1 on medium-difficulty problems and 0% on hard problems. Models performed well in implementation tasks but poorly in nuanced algorithmic reasoning and complex case analysis.", "conclusion": "LLMs have notable limitations in competitive programming compared to expert humans, particularly in algorithmic reasoning and case analysis, despite achieving high performance through implementation precision and tool augmentation."}}
{"id": "2506.11986", "pdf": "https://arxiv.org/pdf/2506.11986", "abs": "https://arxiv.org/abs/2506.11986", "authors": ["Wuzhenghong Wen", "Su Pan", "yuwei Sun"], "title": "Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": "11 pages, 3 figures, conference", "summary": "Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.", "AI": {"tldr": "We propose Schema-R1, a reasoning schema linking model trained using reinforcement learning, which improves the reasoning ability of the schema linking model by 10%.", "motivation": "Current fine-tuning approaches for schema linking models employ a rote-learning paradigm, excessively optimizing for ground truth schema linking outcomes while compromising reasoning ability.", "method": "Schema-R1, which consists of three key steps: constructing small batches of high-quality reasoning samples, supervised fine-tuning for cold-start initialization, and rule-based reinforcement learning training.", "result": "Our method achieves a 10% improvement in filter accuracy compared to the existing method.", "conclusion": "Our method effectively enhances the reasoning ability of the schema linking model, achieving a 10% improvement in filter accuracy compared to the existing method."}}
{"id": "2506.11991", "pdf": "https://arxiv.org/pdf/2506.11991", "abs": "https://arxiv.org/abs/2506.11991", "authors": ["Jiacong Wang", "Zijiang Kang", "Haochen Wang", "Haiyong Jiang", "Jiawen Li", "Bohong Wu", "Ya Wang", "Jiao Ran", "Xiao Liang", "Chao Feng", "Jun Xiao"], "title": "VGR: Visual Grounded Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "9 pages, 4 figures", "summary": "In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.", "AI": {"tldr": "This paper presents VGR, a novel multimodal large language model with enhanced visual perception capabilities. It outperforms the baseline model on various multimodal benchmarks.", "motivation": "Existing approaches mainly rely on reasoning in the language space, which has limitations in handling complex visual reasoning tasks.", "method": "VGR detects relevant regions in images and provides answers based on replayed image regions. A large-scale SFT dataset called VGR-SFT is created for training.", "result": "VGR achieves better performance than the baseline model on multimodal benchmarks such as MMStar, AI2D, and ChartQA.", "conclusion": "The proposed VGR model demonstrates the effectiveness of incorporating fine-grained visual perception in multimodal reasoning."}}
{"id": "2506.11999", "pdf": "https://arxiv.org/pdf/2506.11999", "abs": "https://arxiv.org/abs/2506.11999", "authors": ["Zheli Zhou", "Chenxu Zhu", "Jianghao Lin", "Bo Chen", "Ruiming Tang", "Weinan Zhang", "Yong Yu"], "title": "Generative Representational Learning of Foundation Models for Recommendation", "categories": ["cs.IR", "cs.CL"], "comment": "Project page is available at https://junkfood436.github.io/RecFound/", "summary": "Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines.", "AI": {"tldr": "Develop a recommendation foundation model RecFound that addresses limitations of existing models by introducing a new multi-task training scheme.", "motivation": "To improve recommendation foundation models by handling knowledge sharing & conflict resolution, and convergence speed inconsistencies.", "method": "Introduce RecFound with TMoLE, S2Sched, and Model Merge modules.", "result": "RecFound achieves state-of-the-art performance across various recommendation tasks.", "conclusion": "RecFound is a significant advancement in recommendation foundation models."}}

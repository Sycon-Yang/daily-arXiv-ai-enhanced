<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 54]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)
*Atli Sigurgeirsson,Simon King*

Main category: cs.CL

TL;DR: 本文提出了一种新的微调方法，通过利用模型的不可控方差，提高了基于提示的文本到语音模型的可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的文本到语音模型在控制方面存在限制和过度灵活的问题，需要一种新的微调方法来解决这些问题。

Method: 通过主成分分析数千个合成样本，确定了占输出方差最大比例的潜在特征，并将它们作为新的标签用于二次微调。

Result: 在没有情感披露的模型中，该方法实现了连续和离散特征，从而提高了模型的整体可控性。

Conclusion: 通过利用模型的不可控方差，提出的方法在没有情感披露的模型中实现了连续和离散特征，从而提高了模型的整体可控性。

Abstract: A Prompt-based Text-To-Speech model allows a user to control different
aspects of speech, such as speaking rate and perceived gender, through natural
language instruction. Although user-friendly, such approaches are on one hand
constrained: control is limited to acoustic features exposed to the model
during training, and too flexible on the other: the same inputs yields
uncontrollable variation that are reflected in the corpus statistics.
  We investigate a novel fine-tuning regime to address both of these issues at
the same time by exploiting the uncontrollable variance of the model. Through
principal component analysis of thousands of synthesised samples, we determine
latent features that account for the highest proportion of the output variance
and incorporate them as new labels for secondary fine-tuning. We evaluate the
proposed methods on two models trained on an expressive Icelandic speech
corpus, one with emotional disclosure and one without. In the case of the model
without emotional disclosure, the method yields both continuous and discrete
features that improve overall controllability of the model.

</details>


### [2] [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
*K. Sahit Reddy,N. Ragavenderan,Vasanth K.,Ganesh N. Naik,Vishalakshi Prabhu,Nagaraja G. S*

Main category: cs.CL

TL;DR: 本文提出并验证了MedicalBERT模型，该模型在生物医学自然语言处理任务中表现出色，展示了预训练BERT模型在该领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于生物医学文献具有领域特定的术语，传统的模型如Word2Vec和Bi-LSTM无法完全解决这些问题。尽管GPT和T5能够捕捉上下文，但在需要双向理解的任务中表现不佳，因此需要一种更适合生物医学自然语言处理的模型。

Method: 本文提出了MedicalBERT，这是一种在大规模生物医学数据集上训练的预训练BERT模型，配备了领域特定的词汇表，以增强对生物医学术语的理解。此外，还对模型进行了优化和微调，以应对各种任务，包括命名实体识别、关系提取、问答、句子相似性和文档分类。

Result: MedicalBERT在大多数基准测试中优于其他BERT基线模型，如BioBERT、SciBERT和ClinicalBERT，并且在所有评估任务中平均比通用BERT模型高出5.67%。

Conclusion: 本文提出了一种基于预训练BERT的医学自然语言处理模型MedicalBERT，该模型在多个基准测试中表现优于其他BERT基线模型，并展示了迁移学习技术在捕捉领域特定信息方面的有效性。

Abstract: Recent advances in natural language processing (NLP) have been driven
bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel
at understanding complex texts, but biomedical literature, withits
domain-specific terminology, poses challenges that models likeWord2Vec and
bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,
despite capturing context, fall short in tasks needingbidirectional
understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a
pretrained BERT model trained on a large biomedicaldataset and equipped with
domain-specific vocabulary that enhances thecomprehension of biomedical
terminology. MedicalBERT model is furtheroptimized and fine-tuned to address
diverse tasks, including named entityrecognition, relation extraction, question
answering, sentence similarity, anddocument classification. Performance metrics
such as the F1-score,accuracy, and Pearson correlation are employed to showcase
the efficiencyof our model in comparison to other BERT-based models such as
BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost
of the benchmarks, and surpasses the general-purpose BERT model by5.67% on
average across all the tasks evaluated respectively. This work alsounderscores
the potential of leveraging pretrained BERT models for medicalNLP tasks,
demonstrating the effectiveness of transfer learning techniques incapturing
domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using
pretrained BERT-based model. Available from:
https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model
[accessed Jul 06 2025].

</details>


### [3] [Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking](https://arxiv.org/abs/2507.08014)
*Aldan Creo,Raul Castro Fernandez,Manuel Cebrian*

Main category: cs.CL

TL;DR: 本研究分析了大型语言模型中 jailbreak 策略的复杂性，发现其复杂性并不比正常对话高，并指出学术界披露 jailbreak 攻击可能带来信息危害。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，理解 jailbreak 策略的复杂性和演变对于 AI 安全至关重要。

Method: 我们进行了大规模的实证分析，研究了超过200万条来自不同平台的真实对话中的 jailbreak 复杂性。使用了多种复杂度指标，包括概率度量、词汇多样性、压缩比和认知负荷指标。

Result: 我们发现 jailbreak 尝试并不表现出比正常对话显著更高的复杂性。这种模式在专门的 jailbreaking 社区和普通用户群体中都保持一致。此外，助理响应的毒性有所下降，表明安全机制正在改进。

Conclusion: 我们的研究结果表明，大型语言模型的安全演进受到人类创造力的限制，而防御措施仍在不断进步。同时，我们强调了学术界披露 jailbreak 攻击可能带来的信息危害。

Abstract: As large language models (LLMs) become increasingly deployed, understanding
the complexity and evolution of jailbreaking strategies is critical for AI
safety.
  We present a mass-scale empirical analysis of jailbreak complexity across
over 2 million real-world conversations from diverse platforms, including
dedicated jailbreaking communities and general-purpose chatbots. Using a range
of complexity metrics spanning probabilistic measures, lexical diversity,
compression ratios, and cognitive load indicators, we find that jailbreak
attempts do not exhibit significantly higher complexity than normal
conversations. This pattern holds consistently across specialized jailbreaking
communities and general user populations, suggesting practical bounds on attack
sophistication. Temporal analysis reveals that while user attack toxicity and
complexity remains stable over time, assistant response toxicity has decreased,
indicating improving safety mechanisms. The absence of power-law scaling in
complexity distributions further points to natural limits on jailbreak
development.
  Our findings challenge the prevailing narrative of an escalating arms race
between attackers and defenders, instead suggesting that LLM safety evolution
is bounded by human ingenuity constraints while defensive measures continue
advancing. Our results highlight critical information hazards in academic
jailbreak disclosure, as sophisticated attacks exceeding current complexity
baselines could disrupt the observed equilibrium and enable widespread harm
before defensive adaptation.

</details>


### [4] [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)
*Prudence Djagba,Chimezie A. Odinakachukwu*

Main category: cs.CL

TL;DR: 该研究评估了FinGPT在六个关键NLP任务中的表现，发现其在分类任务中表现良好，但在需要推理和生成的任务中表现较差。研究指出FinGPT仍需改进以成为全面的金融语言模型。


<details>
  <summary>Details</summary>
Motivation: 评估FinGPT在金融领域的表现，以确定其在实际应用中的能力和限制，并为未来的研究提供基准。

Method: 评估FinGPT在六个关键自然语言处理（NLP）任务中的表现：情感分析、文本分类、命名实体识别、金融问答、文本摘要和股票走势预测。使用金融特定的数据集来评估FinGPT在现实世界金融应用中的能力与局限性。

Result: FinGPT在分类任务如情感分析和新闻标题分类中表现出色，通常与GPT-4相当。然而，在涉及推理和生成的任务如金融问答和摘要中表现显著较低。与GPT-4和人类基准的比较显示了明显的性能差距，特别是在数值准确性和复杂推理方面。

Conclusion: 研究结果表明，尽管FinGPT在某些结构化的金融任务中表现有效，但它还不是全面的解决方案。这项研究为未来的研究提供了有用的基准，并强调了在金融语言模型中进行架构改进和领域特定优化的必要性。

Abstract: This work evaluates FinGPT, a financial domain-specific language model,
across six key natural language processing (NLP) tasks: Sentiment Analysis,
Text Classification, Named Entity Recognition, Financial Question Answering,
Text Summarization, and Stock Movement Prediction. The evaluation uses
finance-specific datasets to assess FinGPT's capabilities and limitations in
real-world financial applications. The results show that FinGPT performs
strongly in classification tasks such as sentiment analysis and headline
categorization, often achieving results comparable to GPT-4. However, its
performance is significantly lower in tasks that involve reasoning and
generation, such as financial question answering and summarization. Comparisons
with GPT-4 and human benchmarks highlight notable performance gaps,
particularly in numerical accuracy and complex reasoning. Overall, the findings
indicate that while FinGPT is effective for certain structured financial tasks,
it is not yet a comprehensive solution. This research provides a useful
benchmark for future research and underscores the need for architectural
improvements and domain-specific optimization in financial language models.

</details>


### [5] [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
*Pierre Beckmann,Matthieu Queloz*

Main category: cs.CL

TL;DR: 本文综述了机制可解释性（MI）的最新发现，并提出了机器理解的三层概念，指出LLMs虽然表现出理解的形式，但其认知架构与人类不同。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供对机制可解释性（MI）发现的易懂综述，并将其整合到一个新颖的理论框架中，以思考机器理解。

Method: 本文提供了对最近机制可解释性（MI）发现的易懂综述，并将其整合到一个新颖的理论框架中，以思考机器理解。我们提出了机器理解的三层概念。

Result: LLMs发展出内部结构，这些结构在功能上类似于看到联系的理解。概念理解、状态世界理解和原则理解是机器理解的三个层次。

Conclusion: 我们得出结论，尽管LLMs表现出理解的形式，但它们的认知架构与我们的不同，关于LLMs是否理解的争论应该转向它们奇特的思维是如何运作的。

Abstract: Recent findings in mechanistic interpretability (MI), the field probing the
inner workings of Large Language Models (LLMs), challenge the view that these
models rely solely on superficial statistics. Here, we offer an accessible
synthesis of these findings that doubles as an introduction to MI, all while
integrating these findings within a novel theoretical framework for thinking
about machine understanding. We argue that LLMs develop internal structures
that are functionally analogous to the kind of understanding that consists in
seeing connections. To sharpen this idea, we propose a three-tiered conception
of machine understanding. First, conceptual understanding emerges when a model
forms "features" as directions in latent space, thereby learning the
connections between diverse manifestations of something. Second,
state-of-the-world understanding emerges when a model learns contingent factual
connections between features and dynamically tracks changes in the world.
Third, principled understanding emerges when a model ceases to rely on a
collection of memorized facts and discovers a "circuit" that connects these
facts. However, we conclude by exploring the "parallel mechanisms" phenomenon,
arguing that while LLMs exhibit forms of understanding, their cognitive
architecture remains different from ours, and the debate should shift from
whether LLMs understand to how their strange minds work.

</details>


### [6] [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)
*Nikita Mounier,Parsa Idehpour*

Main category: cs.CL

TL;DR: R3是一种无需额外训练的文本生成优化框架，通过PRM评分审查和Remask策略提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成模型在识别和纠正自身错误方面效率较低，因此需要一种无需额外训练的框架来提升生成质量。

Method: R3框架包含三个步骤：Review（使用PRM模型审查中间生成块）、Remask（根据PRM评分决定遮蔽比例）和Refine（对目标段进行优化）。

Result: R3框架可以应用于任何预训练的掩码文本扩散模型，如LLaDA或BD3-LM，并能有效提升生成结果的质量。

Conclusion: R3框架能够通过利用PRM评分对中间生成块进行审查，并通过Remask策略重新遮蔽低评分块，从而提高最终输出的质量。

Abstract: A key challenge for iterative text generation is enabling models to
efficiently identify and correct their own errors. We propose Review, Remask,
Refine (R3), a relatively simple yet elegant framework that requires no
additional model training and can be applied to any pre-trained masked text
diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is
utilized for the Review of intermediate generated blocks. The framework then
translates these PRM scores into a Remask strategy: the lower a block's PRM
score, indicating potential mistakes, the greater the proportion of tokens
within that block are remasked. Finally, the model is compelled to Refine these
targeted segments, focusing its efforts more intensively on specific
sub-optimal parts of past generations, leading to improved final output.

</details>


### [7] [Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks](https://arxiv.org/abs/2507.08019)
*Aryan Varshney,Venkat Ram Reddy Ganuthula*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在筛选简历时的行为一致性及其与人类专家的比较，发现LLMs表现出可解释的模式，但与人类判断存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型在筛选简历时是否表现出一致的行为（信号）或随机变化（噪声），以及它们的表现与人类专家相比如何。

Method: 研究使用了受控数据集，测试了三个LLMs（Claude、GPT和Gemini）在不同上下文（无公司、Firm1[跨国公司]、Firm2[初创公司]、减少上下文）下的表现，并将其与三位人力资源专家进行比较。通过方差分析和配对t检验评估了LLMs和人类评估之间的差异。

Result: 方差分析显示，在八个LLMs单独条件下有四个存在显著均值差异，LLMs与人类评估之间存在一致的显著差异（p < 0.01）。配对t检验显示GPT对公司上下文适应性较强（p < 0.001），Gemini部分适应（p = 0.038 for Firm1），而Claude适应性较弱（p > 0.1）。所有LLMs在不同上下文中都与人类专家有显著差异。元认知分析揭示了与人类评估方法明显不同的自适应加权模式。

Conclusion: 研究结果表明，大型语言模型（LLMs）在筛选简历时表现出可解释的模式，但与人类判断存在显著差异，这为它们在自动化招聘系统中的部署提供了依据。

Abstract: This study investigates whether large language models (LLMs) exhibit
consistent behavior (signal) or random variation (noise) when screening resumes
against job descriptions, and how their performance compares to human experts.
Using controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)
across contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)
with identical and randomized resumes, benchmarked against three human
recruitment experts. Analysis of variance revealed significant mean differences
in four of eight LLM-only conditions and consistently significant differences
between LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts
strongly to company context (p < 0.001), Gemini partially (p = 0.038 for
Firm1), and Claude minimally (p > 0.1), while all LLMs differed significantly
from human experts across contexts. Meta-cognition analysis highlighted
adaptive weighting patterns that differ markedly from human evaluation
approaches. Findings suggest LLMs offer interpretable patterns with detailed
prompts but diverge substantially from human judgment, informing their
deployment in automated hiring systems.

</details>


### [8] [Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation](https://arxiv.org/abs/2507.08020)
*Zhibo Zhang,Yuxi Li,Kailong Wang,Shuai Yuan,Ling Shi,Haoyu Wang*

Main category: cs.CL

TL;DR: 本文提出了ETTA框架，用于在嵌入空间中识别和减弱毒性敏感维度，从而绕过模型拒绝行为并保持语言连贯性。ETTA在多个LLM上表现出色，揭示了当前对齐策略的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM安全对齐机制在嵌入层面的动力学仍然不够了解，因此需要更针对性和准确的对抗扰动技术进行研究。

Method: ETTA是一种新颖的框架，通过线性变换在嵌入空间中识别和减弱毒性敏感维度。

Result: ETTA在五个代表性的开源LLM上进行了评估，平均攻击成功率高达88.61%，优于最佳基线11.34%。

Conclusion: ETTA展示了当前对齐策略中的关键漏洞，并强调了需要基于嵌入的防御措施。

Abstract: Large Language Models (LLMs) have achieved remarkable success across domains
such as healthcare, education, and cybersecurity. However, this openness also
introduces significant security risks, particularly through embedding space
poisoning, which is a subtle attack vector where adversaries manipulate the
internal semantic representations of input data to bypass safety alignment
mechanisms. While previous research has investigated universal perturbation
methods, the dynamics of LLM safety alignment at the embedding level remain
insufficiently understood. Consequently, more targeted and accurate adversarial
perturbation techniques, which pose significant threats, have not been
adequately studied.
  In this work, we propose ETTA (Embedding Transformation Toxicity
Attenuation), a novel framework that identifies and attenuates
toxicity-sensitive dimensions in embedding space via linear transformations.
ETTA bypasses model refusal behaviors while preserving linguistic coherence,
without requiring model fine-tuning or access to training data. Evaluated on
five representative open-source LLMs using the AdvBench benchmark, ETTA
achieves a high average attack success rate of 88.61%, outperforming the best
baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR
on instruction-tuned defenses). These results highlight a critical
vulnerability in current alignment strategies and underscore the need for
embedding-aware defenses.

</details>


### [9] [Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis](https://arxiv.org/abs/2507.08021)
*Li Li,Yongliang Wu,Jingze Zhu,Jiawei Peng,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 本文研究了多模态上下文学习在图像描述任务中的表现，通过外部和内部分析方法，探索了演示配置策略的影响，并提出了新的度量标准。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在自然语言处理中展示了ICL的有效性，但关于多模态ICL的演示配置探索仍然初步。此外，ICL的可控性提供了一种高效且经济的方法来观察和分析LMM在不同输入下的推理特性。

Method: 我们进行了多模态上下文学习的外部和内部调查，包括探索演示配置策略（三个维度：样本数量、图像检索和标题分配），使用多种度量标准进行系统评估，并分析了典型的LMM注意力特征，开发了基于注意力的度量标准来量化模型行为。此外，我们还进行了辅助实验以探索基于注意力的模型加速和压缩的可行性。

Result: 我们通过外部实验发现ICEs配置策略对模型性能有显著影响，并通过内部检查揭示了典型模式。我们还开发了基于注意力的度量标准来量化模型行为，并进行了辅助实验探索基于注意力的模型加速和压缩的可行性。

Conclusion: 我们的研究揭示了ICEs配置策略如何通过外部实验影响模型性能，并通过内部检查揭示了典型模式，为理解LMM中的多模态ICL提供了双重视角。我们结合外部和内部分析的方法以及新提出的度量标准可以应用于更广泛的研究领域。

Abstract: The evolution of large models has witnessed the emergence of In-Context
Learning (ICL) capabilities. In Natural Language Processing (NLP), numerous
studies have demonstrated the effectiveness of ICL. Inspired by the success of
Large Language Models (LLMs), researchers have developed Large Multimodal
Models (LMMs) with ICL capabilities. However, explorations of demonstration
configuration for multimodal ICL remain preliminary. Additionally, the
controllability of In-Context Examples (ICEs) provides an efficient and
cost-effective means to observe and analyze the inference characteristics of
LMMs under varying inputs. This paper conducts a comprehensive external and
internal investigation of multimodal in-context learning on the image
captioning task. Externally, we explore demonstration configuration strategies
through three dimensions: shot number, image retrieval, and caption assignment.
We employ multiple metrics to systematically and thoroughly evaluate and
summarize key findings. Internally, we analyze typical LMM attention
characteristics and develop attention-based metrics to quantify model
behaviors. We also conduct auxiliary experiments to explore the feasibility of
attention-driven model acceleration and compression. We further compare
performance variations between LMMs with identical model design and pretraining
strategies and explain the differences from the angles of pre-training data
features. Our study reveals both how ICEs configuration strategies impact model
performance through external experiments and characteristic typical patterns
through internal inspection, providing dual perspectives for understanding
multimodal ICL in LMMs. Our method of combining external and internal analysis
to investigate large models, along with our newly proposed metrics, can be
applied to broader research areas.

</details>


### [10] ["Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs](https://arxiv.org/abs/2507.08027)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah,Kund Meghani*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent studies have revealed a consistent liberal orientation in the ethical
and political responses generated by most commercial large language models
(LLMs), yet the underlying causes and resulting implications remain unclear.
This paper systematically investigates the political temperament of seven
prominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity
(Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat
and High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes
Moral Foundations Theory, a dozen established political ideology scales and a
new index of current political controversies. We find strong and consistent
prioritization of liberal-leaning values, particularly care and fairness,
across most models. Further analysis attributes this trend to four overlapping
factors: Liberal-leaning training corpora, reinforcement learning from human
feedback (RLHF), the dominance of liberal frameworks in academic ethical
discourse and safety-driven fine-tuning practices. We also distinguish between
political "bias" and legitimate epistemic differences, cautioning against
conflating the two. A comparison of base and fine-tuned model pairs reveals
that fine-tuning generally increases liberal lean, an effect confirmed through
both self-report and empirical testing. We argue that this "liberal tilt" is
not a programming error or the personal preference of programmers but an
emergent property of training on democratic rights-focused discourse. Finally,
we propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance
philosophical aspiration, reflecting a moral stance unanchored to personal
identity or interest. Rather than undermining democratic discourse, this
pattern may offer a new lens through which to examine collective reasoning.

</details>


### [11] [Better Together: Quantifying the Benefits of AI-Assisted Recruitment](https://arxiv.org/abs/2507.08029)
*Ada Aka,Emil Palikot,Ali Ansari,Nima Yazdani*

Main category: cs.CL

TL;DR: 本研究探讨了人工智能在招聘中的应用，发现AI辅助的招聘流程提高了最终面试通过率和就业率，但AI更倾向于选择年轻、经验较少且拥有较少高级资质的候选人。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在招聘中越来越普遍，但缺乏实证证据来量化其对招聘效率和候选人选拔的影响。

Method: 我们将37,000名申请初级开发人员职位的候选人随机分配到传统的招聘流程或AI辅助的招聘管道中，并比较了两组候选人的最终面试通过率和就业情况。

Result: 在AI辅助的招聘流程中，54%的候选人通过了最终面试，而传统流程中只有34%的候选人通过，平均处理效应为20个百分点。五个月后，传统流程组中有18%的候选人找到了新工作，而AI组中有23%的候选人找到了新工作，两组之间的差异为5.9个百分点。

Conclusion: 我们的研究结果有助于理解人工智能技术如何影响招聘和人才获取中的决策，同时突出了其潜在的影响。

Abstract: Artificial intelligence (AI) is increasingly used in recruitment, yet
empirical evidence quantifying its impact on hiring efficiency and candidate
selection remains limited. We randomly assign 37,000 applicants for a
junior-developer position to either a traditional recruitment process (resume
screening followed by human selection) or an AI-assisted recruitment pipeline
incorporating an initial AI-driven structured video interview before human
evaluation. Candidates advancing from either track faced the same final-stage
human interview, with interviewers blind to the earlier selection method. In
the AI-assisted pipeline, 54% of candidates passed the final interview compared
with 34% from the traditional pipeline, yielding an average treatment effect of
20 percentage points (SE 12 pp.). Five months later, we collected LinkedIn
profiles of top applicants from both groups and found that 18% (SE 1.1%) of
applicants from the traditional track found new jobs compared with 23% (SE
2.3%) from the AI group, resulting in a 5.9 pp. (SE 2.6 pp.) difference in the
probability of finding new employment between groups. The AI system tended to
select younger applicants with less experience and fewer advanced credentials.
We analyze AI-generated interview transcripts to examine the selection criteria
and conversational dynamics. Our findings contribute to understanding how AI
technologies affect decision making in recruitment and talent acquisition while
highlighting some of their potential implications.

</details>


### [12] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)
*Sonali Sharma,Ahmed M. Alaa,Roxana Daneshjou*

Main category: cs.CL

TL;DR: 本研究评估了从2022年至2025年的LLM和VLM输出中的免责声明情况，发现其存在率显著下降，表明需要针对每个输出的临床背景调整免责声明作为安全措施。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型在医疗影像解读和临床问题回答中的应用日益广泛，但其响应中常包含不准确之处，因此需要像医疗免责声明这样的安全措施来提醒用户AI输出未经专业审核或不能替代医疗建议。

Method: 使用500张乳腺X线照片、500张胸部X光片、500张皮肤科图像和500个医学问题，对输出中的免责声明短语进行了筛查。

Result: LLM和VLM输出中的医疗免责声明存在率从2022年的26.3%下降到2025年的0.97%，而从2023年的19.6%下降到2025年的1.05%。到2025年，大多数模型都没有显示免责声明。

Conclusion: 随着公共模型能力的增强和权威性的提高，必须将免责声明作为安全措施实施，并根据每个输出的临床背景进行调整。

Abstract: Generative AI models, including large language models (LLMs) and
vision-language models (VLMs), are increasingly used to interpret medical
images and answer clinical questions. Their responses often include
inaccuracies; therefore, safety measures like medical disclaimers are critical
to remind users that AI outputs are not professionally vetted or a substitute
for medical advice. This study evaluated the presence of disclaimers in LLM and
VLM outputs across model generations from 2022 to 2025. Using 500 mammograms,
500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs
were screened for disclaimer phrases. Medical disclaimer presence in LLM and
VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023
to 1.05% in 2025, respectively. By 2025, the majority of models displayed no
disclaimers. As public models become more capable and authoritative,
disclaimers must be implemented as a safeguard adapting to the clinical context
of each output.

</details>


### [13] [Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding](https://arxiv.org/abs/2507.08031)
*Hong Jia,Shiya Fu,Vassilis Kostakos,Feng Xia,Ting Dang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The emergence of Small Language Models (SLMs) as privacy-preserving
alternatives for sensitive applications raises a fundamental question about
their inherent understanding capabilities compared to Large Language Models
(LLMs). This paper investigates the mental health understanding capabilities of
current SLMs through systematic evaluation across diverse classification tasks.
Employing zero-shot and few-shot learning paradigms, we benchmark their
performance against established LLM baselines to elucidate their relative
strengths and limitations in this critical domain. We assess five
state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against
three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding
tasks. Our findings reveal that SLMs achieve mean performance within 2\% of
LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot
settings), demonstrating notable competence despite orders of magnitude fewer
parameters. Both model categories experience similar degradation on multi-class
severity tasks (a drop of over 30\%), suggesting that nuanced clinical
understanding challenges transcend model scale. Few-shot prompting provides
substantial improvements for SLMs (up to 14.6\%), while LLM gains are more
variable. Our work highlights the potential of SLMs in mental health
understanding, showing they can be effective privacy-preserving tools for
analyzing sensitive online text data. In particular, their ability to quickly
adapt and specialize with minimal data through few-shot learning positions them
as promising candidates for scalable mental health screening tools.

</details>


### [14] [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)
*Nripesh Niketan,Hadj Batatia*

Main category: cs.CL

TL;DR: 本文提出了一种框架，通过集成外部工具来增强LLMs在教育环境中的问答能力。该框架已通过MMLU数据集进行了评估，并显示出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 众所周知，没有相关上下文信息，LLMs可能会提供低质量的回答或倾向于幻觉。一些倡议提出了将LLMs与外部工具集成，以提供最新的数据来提高准确性。

Method: 本文提出了一种框架，允许访问外部API以请求额外的相关信息。集成工具还可以提供计算功能，如计算器或日历。

Result: 所提出的方法显著提高了性能。我们的Athena框架在数学推理中达到83%的准确率，在科学推理中达到88%，明显优于所有测试模型，包括GPT-4o、LLaMA-Large、Mistral-Large、Phi-Large和GPT-3.5，其中最佳基线模型（LLaMA-Large）分别仅达到67%和79%。

Conclusion: 这些有希望的结果为围绕LLMs创建复杂的计算生态系统开辟了道路，使它们的使用更加自然以支持各种任务和活动。

Abstract: This paper deals with improving querying large language models (LLMs). It is
well-known that without relevant contextual information, LLMs can provide poor
quality responses or tend to hallucinate. Several initiatives have proposed
integrating LLMs with external tools to provide them with up-to-date data to
improve accuracy. In this paper, we propose a framework to integrate external
tools to enhance the capabilities of LLMs in answering queries in educational
settings. Precisely, we develop a framework that allows accessing external APIs
to request additional relevant information. Integrated tools can also provide
computational capabilities such as calculators or calendars. The proposed
framework has been evaluated using datasets from the Multi-Modal Language
Understanding (MMLU) collection. The data consists of questions on mathematical
and scientific reasoning. Results compared to state-of-the-art language models
show that the proposed approach significantly improves performance. Our Athena
framework achieves 83% accuracy in mathematical reasoning and 88% in scientific
reasoning, substantially outperforming all tested models including GPT-4o,
LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline
model (LLaMA-Large) achieving only 67% and 79% respectively. These promising
results open the way to creating complex computing ecosystems around LLMs to
make their use more natural to support various tasks and activities.

</details>


### [15] [Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights](https://arxiv.org/abs/2507.08036)
*Deepali Mishra,Chaklam Silpasuwanchai,Ashutosh Modi,Madhumita Sushil,Sorayouth Chumnanvej*

Main category: cs.CL

TL;DR: This paper reviews 68 MedVQA publications and surveys 50 clinicians to assess the practical utility, challenges, and gaps of MedVQA in clinical settings. It finds that most QA pairs are non-diagnostic, datasets lack essential features like EHR integration, and there is a mismatch between evaluation metrics and clinical needs. Clinicians show limited confidence in MedVQA systems, highlighting the need for improvements in multimodal analysis, patient context, and evaluation approaches.


<details>
  <summary>Details</summary>
Motivation: Medical Visual Question Answering (MedVQA) is a promising tool to assist radiologists by automating medical image interpretation through question answering. Despite advances in models and datasets, MedVQA's integration into clinical workflows remains limited.

Method: This study systematically reviews 68 publications (2018-2024) and surveys 50 clinicians from India and Thailand to examine MedVQA's practical utility, challenges, and gaps. Following the Arksey and O'Malley scoping review framework, we used a two-pronged approach: (1) reviewing studies to identify key concepts, advancements, and research gaps in radiology workflows, and (2) surveying clinicians to capture their perspectives on MedVQA's clinical relevance.

Result: Our review reveals that nearly 60% of QA pairs are non-diagnostic and lack clinical relevance. Most datasets and models do not support multi-view, multi-resolution imaging, EHR integration, or domain knowledge, features essential for clinical diagnosis. Furthermore, there is a clear mismatch between current evaluation metrics and clinical needs. The clinician survey confirms this disconnect: only 29.8% consider MedVQA systems highly useful. Key concerns include the absence of patient history or domain knowledge (87.2%), preference for manually curated datasets (51.1%), and the need for multi-view image support (78.7%). Additionally, 66% favor models focused on specific anatomical regions, and 89.4% prefer dialogue-based interactive systems.

Conclusion: MedVQA shows strong potential, but challenges such as limited multimodal analysis, lack of patient context, and misaligned evaluation approaches must be addressed for effective clinical integration.

Abstract: Medical Visual Question Answering (MedVQA) is a promising tool to assist
radiologists by automating medical image interpretation through question
answering. Despite advances in models and datasets, MedVQA's integration into
clinical workflows remains limited. This study systematically reviews 68
publications (2018-2024) and surveys 50 clinicians from India and Thailand to
examine MedVQA's practical utility, challenges, and gaps. Following the Arksey
and O'Malley scoping review framework, we used a two-pronged approach: (1)
reviewing studies to identify key concepts, advancements, and research gaps in
radiology workflows, and (2) surveying clinicians to capture their perspectives
on MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs
are non-diagnostic and lack clinical relevance. Most datasets and models do not
support multi-view, multi-resolution imaging, EHR integration, or domain
knowledge, features essential for clinical diagnosis. Furthermore, there is a
clear mismatch between current evaluation metrics and clinical needs. The
clinician survey confirms this disconnect: only 29.8% consider MedVQA systems
highly useful. Key concerns include the absence of patient history or domain
knowledge (87.2%), preference for manually curated datasets (51.1%), and the
need for multi-view image support (78.7%). Additionally, 66% favor models
focused on specific anatomical regions, and 89.4% prefer dialogue-based
interactive systems. While MedVQA shows strong potential, challenges such as
limited multimodal analysis, lack of patient context, and misaligned evaluation
approaches must be addressed for effective clinical integration.

</details>


### [16] [CRISP: Complex Reasoning with Interpretable Step-based Plans](https://arxiv.org/abs/2507.08037)
*Matan Vetzler,Koren Lazar,Guy Uziel,Eran Hirsch,Ateret Anaby-Tavor,Leshem Choshen*

Main category: cs.CL

TL;DR: 本文介绍了CRISP数据集，用于生成高质量的高层计划，通过微调小模型实现了更好的性能，并展示了其在不同领域的通用性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法假设LLM可以通过少量示例提示生成有效的计划，而无需额外训练。然而，这种假设可能不成立，因此需要一个更系统的方法来生成高质量的计划。

Method: CRISP是一个多领域数据集，包含数学推理和代码生成的高层计划。这些计划是通过LLM自动生成并经过内在和外在验证的。研究还通过微调小模型来评估其生成计划的能力。

Result: 微调小模型在CRISP上表现优于使用少样本提示的大型模型，并且显著优于Chain-of-Thought推理。跨领域的评估也表明，微调一个领域的计划能力可以提升其他领域的表现。

Conclusion: CRISP数据集的引入表明，微调小模型可以生成比使用少样本提示的大型模型更高质量的计划，并且在复杂推理任务中表现出色。此外，跨领域的评估显示了所学规划能力的可推广性。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
stronger reasoning capabilities to solve complex problems effectively. While
Chain-of-Thought (CoT) reasoning has been a step forward, it remains
insufficient for many domains. A promising alternative is explicit high-level
plan generation, but existing approaches largely assume that LLMs can produce
effective plans through few-shot prompting alone, without additional training.
In this work, we challenge this assumption and introduce CRISP (Complex
Reasoning with Interpretable Step-based Plans), a multi-domain dataset of
high-level plans for mathematical reasoning and code generation. The plans in
CRISP are automatically generated and rigorously validated--both intrinsically,
using an LLM as a judge, and extrinsically, by evaluating their impact on
downstream task performance. We demonstrate that fine-tuning a small model on
CRISP enables it to generate higher-quality plans than much larger models using
few-shot prompting, while significantly outperforming Chain-of-Thought
reasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning
on one domain improves plan generation in the other, highlighting the
generalizability of learned planning capabilities.

</details>


### [17] [AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research](https://arxiv.org/abs/2507.08038)
*Talor Abramovich,Gal Chechik*

Main category: cs.CL

TL;DR: 本文介绍了一个用于评估代理在实证AI研究中进行消融计划任务的基准套件AblationBench，并展示了其挑战性和当前LM的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于基于语言模型的自主代理在许多领域越来越受欢迎，包括科学研究，因此需要一种评估代理在实证AI研究中进行消融计划任务的能力的方法。

Method: 本文提出了AblationBench，包含两个任务：AuthorAblation和ReviewerAblation，并开发了基于LM的评判者作为自动评估框架。

Result: 实验结果表明，这些任务仍然具有挑战性，最好的LM系统平均只能识别29%的原始消融实验。

Conclusion: 本文介绍了AblationBench，这是一个用于评估代理在实证AI研究中的消融计划任务的基准套件。实验表明，这些任务仍然具有挑战性，最好的LM系统平均只能识别29%的原始消融实验。最后，我们分析了当前LM在这些任务上的局限性，并发现思维链提示优于现有的基于代理的方法。

Abstract: Autonomous agents built on language models (LMs) are showing increasing
popularity in many fields, including scientific research. AI co-scientists aim
to support or automate parts of the research process using these agents. A key
component of empirical AI research is the design of ablation experiments. To
this end, we introduce AblationBench, a benchmark suite for evaluating agents
on ablation planning tasks in empirical AI research. It includes two tasks:
AuthorAblation, which helps authors propose ablation experiments based on a
method section and contains 83 instances, and ReviewerAblation, which helps
reviewers find missing ablations in a full paper and contains 350 instances.
For both tasks, we develop LM-based judges that serve as an automatic
evaluation framework. Our experiments with frontier LMs show that these tasks
remain challenging, with the best-performing LM system identifying only 29% of
the original ablations on average. Lastly, we analyze the limitations of
current LMs on these tasks, and find that chain-of-thought prompting
outperforms the currently existing agent-based approach.

</details>


### [18] [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)
*Junyi Wen,Junyuan Liang,Zicong Hong,Wuhui Chen,Zibin Zheng*

Main category: cs.CL

TL;DR: Krul is a multi-turn LLM inference system that enables accurate and efficient KV cache restoration by dynamically selecting compression strategies based on attention similarity across layer pairs.


<details>
  <summary>Details</summary>
Motivation: Efficient state restoration in multi-turn conversations with large language models (LLMs) remains a critical challenge due to the overhead of recomputing or loading full key-value (KV) caches for all historical tokens. Existing approaches apply a fixed compression scheme across all conversations, which can lead to accuracy degradation.

Method: Krul dynamically selects compression strategies based on attention similarity across layer pairs and uses a recomputation-loading pipeline to restore the KV cache. It introduces three key innovations: a preemptive compression strategy selector, a token-wise heterogeneous attention similarity estimator, and a bubble-free restoration scheduler.

Result: Empirical evaluations on real-world tasks demonstrate that Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.

Conclusion: Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.

Abstract: Efficient state restoration in multi-turn conversations with large language
models (LLMs) remains a critical challenge, primarily due to the overhead of
recomputing or loading full key-value (KV) caches for all historical tokens. To
address this, existing approaches compress KV caches across adjacent layers
with highly similar attention patterns. However, these methods often apply a
fixed compression scheme across all conversations, selecting the same layer
pairs for compression without considering conversation-specific attention
dynamics. This static strategy overlooks variability in attention pattern
similarity across different conversations, which can lead to noticeable
accuracy degradation.
  We present Krul, a multi-turn LLM inference system that enables accurate and
efficient KV cache restoration. Krul dynamically selects compression strategies
based on attention similarity across layer pairs and uses a
recomputation-loading pipeline to restore the KV cache. It introduces three key
innovations: 1) a preemptive compression strategy selector to preserve critical
context for future conversation turns and selects a customized strategy for the
conversation; 2) a token-wise heterogeneous attention similarity estimator to
mitigate the attention similarity computation and storage overhead during model
generation; 3) a bubble-free restoration scheduler to reduce potential bubbles
brought by the imbalance of recomputing and loading stream due to compressed KV
caches. Empirical evaluations on real-world tasks demonstrate that Krul
achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x
reduction in KV cache storage compared to state-of-the-art methods without
compromising generation quality.

</details>


### [19] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 我们提出了一种新的方法，使用大型语言模型从自然语言问题或关键词查询生成SPARQL查询，无需微调。该方法在多个基准测试中表现出色，包括Wikidata和Freebase。


<details>
  <summary>Details</summary>
Motivation: 现有的方法需要微调，而我们的方法不需要微调，可以更灵活地应用于不同的知识图谱和语言模型。

Method: 我们提出了一种新的方法，使用大型语言模型从自然语言问题或关键词查询生成SPARQL查询。该方法不需要微调，而是利用语言模型通过战略性执行SPARQL查询来探索知识图谱，以寻找相关的IRI和文字。

Result: 我们在各种基准测试中评估了我们的方法，并与现有方法进行了比较。在Wikidata上，我们的方法在多个基准测试中达到了最先进的结果，尽管是零样本设置。在Freebase上，我们的方法接近最佳的少样本方法。在其他较少评估的知识图谱和基准测试中，我们的方法也表现良好。

Conclusion: 我们的方法在多个基准测试中达到了最先进的结果，尤其是在Wikidata上。此外，在Freebase上，我们的方法接近最佳的少样本方法。在其他较少评估的知识图谱和基准测试中，我们的方法也表现良好。

Abstract: We propose a new approach for generating SPARQL queries on RDF knowledge
graphs from natural language questions or keyword queries, using a large
language model. Our approach does not require fine-tuning. Instead, it uses the
language model to explore the knowledge graph by strategically executing SPARQL
queries and searching for relevant IRIs and literals. We evaluate our approach
on a variety of benchmarks (for knowledge graphs of different kinds and sizes)
and language models (of different scales and types, commercial as well as
open-source) and compare it with existing approaches. On Wikidata we reach
state-of-the-art results on multiple benchmarks, despite the zero-shot setting.
On Freebase we come close to the best few-shot methods. On other, less commonly
evaluated knowledge graphs and benchmarks our approach also performs well
overall. We conduct several additional studies, like comparing different ways
of searching the graphs, incorporating a feedback mechanism, or making use of
few-shot examples.

</details>


### [20] [Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing](https://arxiv.org/abs/2507.08109)
*Reilly Raab,Mike Parker,Dan Nally,Sadie Montgomery,Anastasia Bernat,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.CL

TL;DR: 本文提出了一种框架，用于在常规异步代码中声明静态类型、由语言模型驱动的子例程，并通过人类专家的反馈在线改进每个子例程的性能。该框架被封装为一个库以支持其采用和持续开发，并在公共评论处理的背景下进行了评估。


<details>
  <summary>Details</summary>
Motivation: 语言模型的出现有可能显著加速可以转化为文本处理的任务，但现实世界的采用受到安全、可解释性和偏见的担忧的阻碍。如何负责任地利用语言模型进行透明、可审计的方式，最小化风险并让人类专家专注于知情决策而不是数据处理或提示工程？

Method: 本文提出了一种框架，用于在常规异步代码中声明静态类型、由语言模型驱动的子例程，并通过人类专家的反馈在线改进每个子例程的性能。所有语言模型生成的工件都被记录并按需暴露给审计。

Result: 本文评估了该框架在公共评论处理背景下的应用，特别是在遵守1969年国家环境保护法（NEPA）的情况下。通过与历史“真实”数据进行比较，量化评估了该应用的输出。

Conclusion: 本文提出了一种框架，用于在常规异步代码中声明静态类型、由语言模型驱动的子例程，并通过人类专家的反馈在线改进每个子例程的性能。该框架被封装为一个库以支持其采用和持续开发，并在公共评论处理的背景下进行了评估。

Abstract: The advent of language models (LMs) has the potential to dramatically
accelerate tasks that may be cast to text-processing; however, real-world
adoption is hindered by concerns regarding safety, explainability, and bias.
How can we responsibly leverage LMs in a transparent, auditable manner --
minimizing risk and allowing human experts to focus on informed decision-making
rather than data-processing or prompt engineering? In this work, we propose a
framework for declaring statically typed, LM-powered subroutines (i.e.,
callable, function-like procedures) for use within conventional asynchronous
code -- such that sparse feedback from human experts is used to improve the
performance of each subroutine online (i.e., during use). In our
implementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and
data-dependencies) are recorded and exposed to audit on demand. We package this
framework as a library to support its adoption and continued development. While
this framework may be applicable across several real-world decision workflows
(e.g., in healthcare and legal fields), we evaluate it in the context of public
comment processing as mandated by the 1969 National Environmental Protection
Act (NEPA): Specifically, we use this framework to develop "CommentNEPA," an
application that compiles, organizes, and summarizes a corpus of public
commentary submitted in response to a project requiring environmental review.
We quantitatively evaluate the application by comparing its outputs (when
operating without human feedback) to historical ``ground-truth'' data as
labelled by human annotators during the preparation of official environmental
impact statements.

</details>


### [21] [Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores](https://arxiv.org/abs/2507.08143)
*Vivek Chari,Benjamin Van Durme*

Main category: cs.CL

TL;DR: Compactor is a parameter-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. It achieves the same performance as competing methods while retaining half the tokens, with minimal computational overhead. Context-calibrated compression allows for inferring the maximum compression ratio a given context can support, resulting in a 63% reduction in KV memory burden on Longbench.


<details>
  <summary>Details</summary>
Motivation: Modern Large Language Models (LLMs) are increasingly trained to support very large context windows, but the ability to use long contexts in generation is complicated by the large memory requirement of the KV cache, which scales linearly with the context length. This memory footprint is often the dominant resource bottleneck in real-world deployments, limiting throughput and increasing serving cost.

Method: Compactor, a parameter-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. A procedure for context-calibrated compression is introduced to infer the maximum compression ratio a given context can support.

Result: Compactor can achieve the same performance as competing methods while retaining 1/2 the tokens in both synthetic and real-world context tasks, with minimal computational overhead. Using context-calibrated compression, Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 63%, on average.

Conclusion: Compactor can achieve the same performance as competing methods while retaining 1/2 the tokens in both synthetic and real-world context tasks, with minimal computational overhead. Using context-calibrated compression, Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 63%, on average.

Abstract: Modern Large Language Models (LLMs) are increasingly trained to support very
large context windows. Unfortunately the ability to use long contexts in
generation is complicated by the large memory requirement of the KV cache,
which scales linearly with the context length. This memory footprint is often
the dominant resource bottleneck in real-world deployments, limiting throughput
and increasing serving cost. One way to address this is by compressing the KV
cache, which can be done either with knowledge of the question being asked
(query-aware) or without knowledge of the query (query-agnostic). We present
Compactor, a parameter-free, query-agnostic KV compression strategy that uses
approximate leverage scores to determine token importance. We show that
Compactor can achieve the same performance as competing methods while retaining
1/2 the tokens in both synthetic and real-world context tasks, with minimal
computational overhead. We further introduce a procedure for context-calibrated
compression, which allows one to infer the maximum compression ratio a given
context can support. Using context-calibrated compression, we show that
Compactor achieves full KV performance on Longbench while reducing the KV
memory burden by 63%, on average. To demonstrate the efficacy and
generalizability of our approach, we apply Compactor to 27 synthetic and
real-world tasks from RULER and Longbench, with models from both the Qwen 2.5
and Llama 3.1 families.

</details>


### [22] [Distilling Empathy from Large Language Models](https://arxiv.org/abs/2507.08151)
*Henry J. Xie,Jinghan Zhang,Xinhao Zhang,Kunpeng Liu*

Main category: cs.CL

TL;DR: 本文提出了一种有效的情感知识蒸馏方法，通过两步微调和特定提示提升SLM的情感回应能力，结果表明其性能显著优于基础SLM。


<details>
  <summary>Details</summary>
Motivation: 由于SLM在资源受限的领域（如智能手机）中广泛应用，因此需要确保SLM在蒸馏过程中保留LLM中的情感能力，以促进积极的人机互动。

Method: 本文提出了一种两步微调过程，利用从LLM中蒸馏出的情感对话数据集，并探索了多种蒸馏方法，提出了四个独特的情感改进提示以提高情感蒸馏效果。

Result: 经过两步微调过程和特定情感改进提示的SLM在生成情感回应方面显著优于基础SLM，胜率高达90%。特定情感改进提示比基本直接提示提高了10%的胜率。

Conclusion: 本文提出了一种有效的情感知识蒸馏方法，使小型语言模型（SLM）能够保留大型语言模型（LLM）的情感能力。通过两步微调过程和特定的情感改进提示，SLM在生成情感回应方面表现出显著的提升。

Abstract: The distillation of knowledge from Large Language Models (LLMs) into Smaller
Language Models (SLMs), preserving the capabilities and performance of LLMs
while reducing model size, has played a key role in the proliferation of LLMs.
Because SLMs are considerably smaller than LLMs, they are often utilized in
domains where human interaction is frequent but resources are highly
constrained, e.g., smart phones. Therefore, it is crucial to ensure that
empathy, a fundamental aspect of positive human interactions, already instilled
into LLMs, is retained by SLMs after distillation. In this paper, we develop a
comprehensive approach for effective empathy distillation from LLMs into SLMs.
Our approach features a two-step fine-tuning process that fully leverages
datasets of empathetic dialogue responses distilled from LLMs. We explore
several distillation methods beyond basic direct prompting and propose four
unique sets of prompts for targeted empathy improvement to significantly
enhance the empathy distillation process. Our evaluations demonstrate that SLMs
fine-tuned through the two-step fine-tuning process with distillation datasets
enhanced by the targeted empathy improvement prompts significantly outperform
the base SLM at generating empathetic responses with a win rate of 90%. Our
targeted empathy improvement prompts substantially outperform the basic direct
prompting with a 10% improvement in win rate.

</details>


### [23] [TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs](https://arxiv.org/abs/2507.08203)
*Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sungmin Kang,Alperen Öziş,Hayrettin Eren Yildiz,Mitash Ashish Shah,Zhiqi Huang,Anoop Kumar,Alfy Samuel,Daben Liu,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.CL

TL;DR: TruthTorchLM 是一个开源库，提供多种真实性预测方法，适用于不同场景，并支持扩展和兼容多种模型框架。


<details>
  <summary>Details</summary>
Motivation: 准确预测生成式大语言模型输出的真实性在高风险场景中至关重要。现有的工具如 Guardrails 和 LM-Polygraph 存在局限性，因此需要一个更全面和可扩展的解决方案。

Method: TruthTorchLM 提供了超过 30 种预测输出真实性的方法，包括不同的计算成本、访问级别、文档要求和监督类型。它还提供了一个统一的接口用于生成、评估、校准和长格式真实性预测，并允许扩展新方法。

Result: TruthTorchLM 在三个数据集 TriviaQA、GSM8K 和 FactScore-Bio 上进行了评估，验证了其有效性。代码已公开在 GitHub 上。

Conclusion: TruthTorchLM 是一个开源的 Python 库，提供了多种预测输出真实性的方法，并且可以与 HuggingFace 和 LiteLLM 兼容，具有广泛的适用性和可扩展性。

Abstract: Generative Large Language Models (LLMs)inevitably produce untruthful
responses. Accurately predicting the truthfulness of these outputs is critical,
especially in high-stakes settings. To accelerate research in this domain and
make truthfulness prediction methods more accessible, we introduce TruthTorchLM
an open-source, comprehensive Python library featuring over 30 truthfulness
prediction methods, which we refer to as Truth Methods. Unlike existing
toolkits such as Guardrails, which focus solely on document-grounded
verification, or LM-Polygraph, which is limited to uncertainty-based methods,
TruthTorchLM offers a broad and extensible collection of techniques. These
methods span diverse tradeoffs in computational cost, access level (e.g.,
black-box vs white-box), grounding document requirements, and supervision type
(self-supervised or supervised). TruthTorchLM is seamlessly compatible with
both HuggingFace and LiteLLM, enabling support for locally hosted and API-based
models. It also provides a unified interface for generation, evaluation,
calibration, and long-form truthfulness prediction, along with a flexible
framework for extending the library with new methods. We conduct an evaluation
of representative truth methods on three datasets, TriviaQA, GSM8K, and
FactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM

</details>


### [24] [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)
*Atticus Wang,Joshua Engels,Oliver Clive-Griffin*

Main category: cs.CL

TL;DR: 本研究揭示了OOCR现象的一个简单解释，即LoRA微调添加的常数引导向量使模型朝向通用概念，从而提高了性能并导致意外的泛化。


<details>
  <summary>Details</summary>
Motivation: 研究OOCR现象，以理解LLM为何能够进行上下文外推理，这对于其安全和可靠部署至关重要。

Method: 我们通过机制分析研究了OOCR现象，并发现许多文献中的OOCR实例有一个简单的解释：LoRA微调本质上添加了一个常数引导向量，引导模型朝向一个通用概念。此外，我们可以直接从头开始训练这些任务的引导向量，这也引发了OOCR。

Result: 我们发现LoRA微调添加的常数引导向量可以提高微调任务和其他相关领域的性能，导致意外的泛化。此外，直接训练引导向量也能引发OOCR，即使对于似乎必须涉及条件行为的任务（如模型后门），无条件添加引导向量也足够有效。

Conclusion: 我们的工作为OOCR任务在微调期间学到的内容提供了一个解释，有助于回答LLM为何能够进行上下文外推理这一关键问题，这对于它们的安全和可靠部署非常重要。

Abstract: Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs
exhibit surprisingly deep out-of-distribution generalization. Rather than
learning shallow heuristics, they implicitly internalize and act on the
consequences of observations scattered throughout the fine-tuning data. In this
work, we investigate this phenomenon mechanistically and find that many
instances of OOCR in the literature have a simple explanation: the LoRA
fine-tuning essentially adds a constant steering vector, steering the model
towards a general concept. This improves performance on the fine-tuning task
and in many other concept-related domains, causing the surprising
generalization. Moreover, we can directly train steering vectors for these
tasks from scratch, which also induces OOCR. We find that our results hold even
for a task that seems like it must involve conditional behavior (model
backdoors); it turns out that unconditionally adding a steering vector is
sufficient. Overall, our work presents one explanation of what gets learned
during fine-tuning for OOCR tasks, contributing to the key question of why LLMs
can reason out of context, an advanced capability that is highly relevant to
their safe and reliable deployment.

</details>


### [25] [Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?](https://arxiv.org/abs/2507.08232)
*KV Aditya Srivatsa,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 研究LLMs作为真实学生的代理的准确性，发现强大的通用模型在每个年级都优于平均学生，但需要新的训练和评估策略。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs作为智能辅导系统和测试问题开发中的代理学生的准确性，以确定它们是否能准确模拟真实学生的行为和特征。

Method: 我们收集了来自NAEP的489个题目数据集，并应用项目反应理论（IRT）模型将11种不同的最先进的LLMs定位在与真实学生群体相同的能力量表上。

Result: 我们的发现表明，没有指导的情况下，强大的通用模型在每个年级都持续优于平均学生，而较弱或领域不匹配的模型可能偶然对齐。使用年级强制提示会改变模型的表现，但它们是否与平均年级水平的学生对齐仍然高度依赖于模型和提示：没有评估的模型-提示对在不同学科和年级中都符合要求。

Conclusion: 我们得出结论，需要新的训练和评估策略来改进LLMs作为真实学生的代理。

Abstract: Large Language Models (LLMs) are increasingly used as proxy students in the
development of Intelligent Tutoring Systems (ITSs) and in piloting test
questions. However, to what extent these proxy students accurately emulate the
behavior and characteristics of real students remains an open question. To
investigate this, we collected a dataset of 489 items from the National
Assessment of Educational Progress (NAEP), covering mathematics and reading
comprehension in grades 4, 8, and 12. We then apply an Item Response Theory
(IRT) model to position 11 diverse and state-of-the-art LLMs on the same
ability scale as real student populations. Our findings reveal that, without
guidance, strong general-purpose models consistently outperform the average
student at every grade, while weaker or domain-mismatched models may align
incidentally. Using grade-enforcement prompts changes models' performance, but
whether they align with the average grade-level student remains highly model-
and prompt-specific: no evaluated model-prompt pair fits the bill across
subjects and grades, underscoring the need for new training and evaluation
strategies. We conclude by providing guidelines for the selection of viable
proxies based on our findings.

</details>


### [26] [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)
*Ancita Maria Andrade,Tanvi Banerjee,Ramakrishna Mundugar*

Main category: cs.CL

TL;DR: 本研究利用自然语言处理技术分析了个体的疼痛体验，发现女性的帖子更侧重于情感，并且某些疾病在女性中更为常见。


<details>
  <summary>Details</summary>
Motivation: 早期的研究常常忽视了性别在疼痛体验中的作用，因此本研究旨在通过自然语言处理技术深入分析性别差异。

Method: 本研究使用了自然语言处理（NLP）技术，利用隐藏属性模型-卷积神经网络（HAM-CNN）对帖子进行分类，根据用户名聚合帖子。

Result: 本研究成功地将帖子分类为男性和女性语料库，达到了0.86的F1分数。分析显示女性的帖子更侧重于情感，并且某些疾病如偏头痛和鼻窦炎在女性中更为常见。

Conclusion: 本研究通过自然语言处理技术分析了个体的疼痛体验，特别关注性别差异。结果表明，女性的帖子更侧重于情感，并且某些疾病如偏头痛和鼻窦炎在女性中更为常见。

Abstract: Pain is an inherent part of human existence, manifesting as both physical and
emotional experiences, and can be categorized as either acute or chronic. Over
the years, extensive research has been conducted to understand the causes of
pain and explore potential treatments, with contributions from various
scientific disciplines. However, earlier studies often overlooked the role of
gender in pain experiences. In this study, we utilized Natural Language
Processing (NLP) to analyze and gain deeper insights into individuals' pain
experiences, with a particular focus on gender differences. We successfully
classified posts into male and female corpora using the Hidden Attribute
Model-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by
aggregating posts based on usernames. Our analysis revealed linguistic
differences between genders, with female posts tending to be more emotionally
focused. Additionally, the study highlighted that conditions such as migraine
and sinusitis are more prevalent among females and explored how pain medication
affects individuals differently based on gender.

</details>


### [27] [KAT-V1: Kwai-AutoThink Technical Report](https://arxiv.org/abs/2507.08297)
*Zizheng Zhan,Ken Deng,Huaixi Tang,Wen Xiang,Kun Wu,Weihao Li,Wenqiang Zhu,Jingxuan Xu,Lecheng Huang,Zongxian Feng,Shaojie Wang,Shangpeng Yan,Jiaheng Liu,Zhongyuan Peng,Zuchen Gao,Haoyang Huang,Ziqi Zhan,Yanan Wu,Yuanxing Zhang,Jian Yang,Guang Chen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.CL

TL;DR: KAT是一种新型的大规模语言模型，旨在解决推理密集型任务中的过度思考问题，通过自动思维训练范式提高模型效率和准确性，并已在实际应用中取得成功。


<details>
  <summary>Details</summary>
Motivation: 解决推理密集型任务中的过度思考问题，提高模型在复杂任务中的效率和准确性。

Method: KAT通过提出一种自动思维训练范式，动态切换推理和非推理模式以应对复杂任务。具体包括构建双模式数据集、应用多标记预测增强的知识蒸馏、冷启动初始化策略以及结合中间监督的强化学习算法Step-SRPO。

Result: KAT在多个基准测试中表现优异，甚至超过了当前最先进的模型，同时减少了约30%的token使用量。此外，KAT已成功部署在Kwaipilot中，提高了实际开发流程的准确性和效率。

Conclusion: KAT在多个基准测试中表现出色，能够有效减少token使用量，并且已经在实际应用中取得了成功。此外，KAT的AutoThink范式展示了良好的可扩展性，为未来更大规模的模型训练提供了可能性。

Abstract: We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model
developed to address the overthinking problem in reasoning-intensive tasks,
where an automatic thinking training paradigm is proposed to dynamically switch
between reasoning and non-reasoning modes based on task complexity.
Specifically, first, we construct the dual-regime dataset based on a novel
tagging pipeline and a multi-agent synthesis strategy, and then we apply
Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling
efficient and fine-grained reasoning transfer with minimal pretraining cost.
Besides, we implement a cold-start initialization strategy that introduces
mode-selection priors using majority-vote signals and intent-aware prompting.
Finally, we propose Step-SRPO, a reinforcement learning algorithm that
incorporates intermediate supervision into the GRPO framework, offering
structured guidance over both reasoning-mode selection and response accuracy.
Extensive experiments across multiple benchmarks demonstrate that KAT
consistently matches or even outperforms current state-of-the-art models,
including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of
reasoning-intensive tasks while reducing token usage by up to approximately
30\%. Beyond academic evaluation, KAT has been successfully deployed in
Kwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world
development workflows with high accuracy, efficiency, and controllable
reasoning behaviors. Moreover, we are actively training a 200B
Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage
results already demonstrate promising improvements in performance and
efficiency, further showing the scalability of the AutoThink paradigm.

</details>


### [28] [Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency](https://arxiv.org/abs/2507.08309)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Zhiyuan Chen,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种新的微调范式Synchronously Self-Reviewing (SSR)，旨在解决多模态大语言模型在文档图像机器翻译任务中的灾难性遗忘问题，并提高其在OCR和DIMT任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在文档图像任务中表现出色，特别是在光学字符识别（OCR）方面。然而，它们在文档图像机器翻译（DIMT）方面存在困难，这需要处理跨模态和跨语言的挑战。之前的尝试通过在DIMT数据集上进行监督微调来增强DIMT能力，但往往导致模型遗忘现有的单语能力，如OCR。

Method: 本文引入了一种名为Synchronously Self-Reviewing (SSR) 的新微调范式，该范式受到“双语认知优势”概念的启发，通过让模型在生成翻译文本之前生成OCR文本，从而利用其强大的单语OCR能力同时学习跨语言翻译。

Result: 全面的实验表明，所提出的SSR学习有助于缓解灾难性遗忘，提高MLLMs在OCR和DIMT任务上的泛化能力。

Conclusion: 本文提出了一种新的微调范式Synchronously Self-Reviewing (SSR)，它能够缓解灾难性遗忘，提高多模态大语言模型在OCR和DIMT任务上的泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) have shown strong performance in
document image tasks, especially Optical Character Recognition (OCR). However,
they struggle with Document Image Machine Translation (DIMT), which requires
handling both cross-modal and cross-lingual challenges. Previous efforts to
enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT
dataset often result in the forgetting of the model's existing monolingual
abilities, such as OCR. To address these challenges, we introduce a novel
fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR
proficiency, inspired by the concept "Bilingual Cognitive Advantage".
Specifically, SSR prompts the model to generate OCR text before producing
translation text, which allows the model to leverage its strong monolingual OCR
ability while learning to translate text across languages. Comprehensive
experiments demonstrate the proposed SSR learning helps mitigate catastrophic
forgetting, improving the generalization ability of MLLMs on both OCR and DIMT
tasks.

</details>


### [29] [CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation](https://arxiv.org/abs/2507.08325)
*Yinzhu Quan,Xinrui Li,Ying Chen*

Main category: cs.CL

TL;DR: CRMAgent是一个基于大型语言模型的多代理系统，旨在帮助商家生成高质量的信息模板和写作指导，从而提高营销效果。


<details>
  <summary>Details</summary>
Motivation: 大多数商家缺乏专业知识和可扩展的工具来撰写有说服力的文案，因此需要一种能够帮助他们生成高质量信息模板和写作指导的系统。

Method: CRMAgent是一个基于大型语言模型（LLMs）的多代理系统，通过三种互补模式生成高质量的信息模板和可操作的写作指导：基于组的学习、检索与适应、基于规则的回退。

Result: CRMAgent在多个指标上优于商家的原始模板，显著提高了受众匹配和营销效果。

Conclusion: CRMAgent在多个指标上优于商家的原始模板，显著提高了受众匹配和营销效果。

Abstract: In e-commerce private-domain channels such as instant messaging and e-mail,
merchants engage customers directly as part of their Customer Relationship
Management (CRM) programmes to drive retention and conversion. While a few top
performers excel at crafting outbound messages, most merchants struggle to
write persuasive copy because they lack both expertise and scalable tools. We
introduce CRMAgent, a multi-agent system built on large language models (LLMs)
that generates high-quality message templates and actionable writing guidance
through three complementary modes. First, group-based learning enables the
agent to learn from a merchant's own top-performing messages within the same
audience segment and rewrite low-performing ones. Second,
retrieval-and-adaptation fetches templates that share the same audience segment
and exhibit high similarity in voucher type and product category, learns their
successful patterns, and adapts them to the current campaign. Third, a
rule-based fallback provides a lightweight zero-shot rewrite when no suitable
references are available. Extensive experiments show that CRMAgent consistently
outperforms merchants' original templates, delivering significant gains in both
audience-match and marketing-effectiveness metrics.

</details>


### [30] [MK2 at PBIG Competition: A Prompt Generation Solution](https://arxiv.org/abs/2507.08335)
*Yuzheng Xu,Tosho Hirasawa,Seiya Kawano,Shota Kato,Tadashi Kozuno*

Main category: cs.CL

TL;DR: MK2是一种无需额外训练数据的提示工程方法，在专利创意生成任务中表现出色，但在某些领域仍需改进。


<details>
  <summary>Details</summary>
Motivation: 将真实专利转化为三年内可行的产品创意。

Method: MK2是一种以提示为中心的管道，包括Gemini 2.5迭代编辑提示并嫁接有用片段，GPT-4.1使用该提示为每个专利生成一个想法，以及通过Qwen3-8B进行Elo循环选择最佳提示。

Result: 在三个领域、两种评估者类型和六个标准下，MK2在自动排行榜上排名第一，并赢得了36项测试中的25项。

Conclusion: 轻量级提示工程已经从专利中实现了具有竞争力的、商业相关的创意生成，但需要更深入的领域知识来提高某些领域的表现。

Abstract: The Patent-Based Idea Generation task asks systems to turn real patents into
product ideas viable within three years. We propose MK2, a prompt-centric
pipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful
fragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea
per patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all
without extra training data. Across three domains, two evaluator types, and six
criteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the
materials-chemistry track lagged, indicating the need for deeper domain
grounding; yet, the results show that lightweight prompt engineering has
already delivered competitive, commercially relevant ideation from patents.

</details>


### [31] [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)
*Zhichao Xu,Zhiqi Huang,Shengyao Zhuang,Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文比较了对比学习和知识蒸馏在训练重排序器中的效果，发现当有更大的教师模型时，知识蒸馏效果更好；否则，对比学习是一个可靠的选择。


<details>
  <summary>Details</summary>
Motivation: 本文旨在明确比较对比学习和知识蒸馏在实际条件下训练交叉编码器重排序器的有效性。

Method: 本文通过在相同数据上使用两种方法训练不同大小和架构的重排序器，并以一个强大的对比学习模型作为知识蒸馏的教师，对这两种策略进行了实证比较。

Result: 结果表明，当从更大的教师模型进行知识蒸馏时，知识蒸馏通常比对比学习在域内和域外排名性能上表现更好。这一发现在学生模型的大小和架构上是一致的。然而，从相同容量的教师模型进行蒸馏并不能提供同样的优势，尤其是在域外任务中。

Conclusion: 本文的结论是，当有更大的教师模型可用时，建议使用知识蒸馏来训练较小的重排序器；如果没有，则对比学习提供了一个强大且更可靠的替代方案。

Abstract: Training text rerankers is crucial for information retrieval. Two primary
strategies are widely used: contrastive learning (optimizing directly on
ground-truth labels) and knowledge distillation (transferring knowledge from a
larger reranker). While both have been studied in the literature, a clear
comparison of their effectiveness for training cross-encoder rerankers under
practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of
different sizes and architectures using both methods on the same data, with a
strong contrastive learning model acting as the distillation teacher. Our
results show that knowledge distillation generally yields better in-domain and
out-of-domain ranking performance than contrastive learning when distilling
from a larger teacher model. This finding is consistent across student model
sizes and architectures. However, distilling from a teacher of the same
capacity does not provide the same advantage, particularly for out-of-domain
tasks. These findings offer practical guidance for choosing a training strategy
based on available teacher models. Therefore, we recommend using knowledge
distillation to train smaller rerankers if a larger, more powerful teacher is
accessible; in its absence, contrastive learning provides a strong and more
reliable alternative otherwise.

</details>


### [32] [What Factors Affect LLMs and RLLMs in Financial Question Answering?](https://arxiv.org/abs/2507.08339)
*Peng Wang,Xuesi Hu,Jiageng Wu,Yuntao Zou,Qiancheng Zhang,Dagang Li*

Main category: cs.CL

TL;DR: 本文研究了不同方法对金融问答任务中LLMs和RLLMs的影响，发现提示方法和代理框架能提升LLMs性能，而RLLMs由于内在的Long CoT能力，传统方法效果有限，多语言对齐方法对RLLMs帮助不大。


<details>
  <summary>Details</summary>
Motivation: 目前很少有工作系统地探索哪些方法可以充分释放LLMs和RLLMs在金融领域的性能。

Method: 我们使用五种LLMs和三种RLLMs来评估提示方法、代理框架和多语言对齐方法对金融问答任务的影响。

Result: （1）当前的提示方法和代理框架通过模拟Long CoT提高了LLMs在金融问答中的性能；（2）RLLMs具有内在的Long CoT能力，这限制了传统方法进一步提升其性能的效果；（3）当前先进的多语言对齐方法主要通过扩展推理长度来提高LLMs的多语言性能，但对RLLMs的收益很小。

Conclusion: 我们希望这项研究能为金融问答领域的LLMs和RLLMs提供重要的参考。

Abstract: Recently, the development of large language models (LLMs) and reasoning large
language models (RLLMs) have gained considerable attention from many
researchers. RLLMs enhance the reasoning capabilities of LLMs through Long
Chain-of-Thought (Long CoT) processes, significantly improving the performance
of LLMs in addressing complex problems. However, there are few works that
systematically explore what methods can fully unlock the performance of LLMs
and RLLMs within the financial domain. To investigate the impact of various
methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the
effects of prompting methods, agentic frameworks, and multilingual alignment
methods on financial question-answering tasks. Our research findings indicate:
(1) Current prompting methods and agent frameworks enhance the performance of
LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess
inherent Long CoT capabilities, which limits the effectiveness of conventional
methods in further enhancing their performance; (3) Current advanced
multilingual alignment methods primarily improve the multilingual performance
of LLMs by extending the reasoning length, which yields minimal benefits for
RLLMs. We hope that this study can serve as an important reference for LLMs and
RLLMs in the field of financial question answering.

</details>


### [33] [Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization](https://arxiv.org/abs/2507.08342)
*Itai Mondshine,Tzuf Paz-Argaman,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 本文评估了生成任务中的评估度量，发现n-gram度量在屈折语言中效果较差，而神经度量在低资源语言中表现更好。


<details>
  <summary>Details</summary>
Motivation: 自动n-gram基于度量如ROUGE广泛用于评估生成任务，但它们在其他语言中的适用性仍不清楚。

Method: 我们设计了一个跨八种语言的大规模评估套件，涵盖四种语类家族，以分析它们与人类判断的相关性。

Result: 我们的研究结果表明，评估度量对语言类型敏感。例如，在屈折语言中，n-gram基于度量与人类评估的相关性较低。此外，我们展示了适当的分词可以显著缓解这一问题。

Conclusion: 我们的分析突显了n-gram度量对于屈折语言的局限性，并倡导对针对评估任务训练的神经度量进行更大的投资。

Abstract: Automatic n-gram based metrics such as ROUGE are widely used for evaluating
generative tasks such as summarization. While these metrics are considered
indicative (even if imperfect) of human evaluation for English, their
suitability for other languages remains unclear. To address this, we
systematically assess evaluation metrics for generation both n-gram-based and
neural based to evaluate their effectiveness across languages and tasks.
Specifically, we design a large-scale evaluation suite across eight languages
from four typological families: agglutinative, isolating, low-fusional, and
high-fusional, spanning both low- and high-resource settings, to analyze their
correlation with human judgments. Our findings highlight the sensitivity of
evaluation metrics to the language type. For example, in fusional languages,
n-gram-based metrics show lower correlation with human assessments compared to
isolating and agglutinative languages. We also demonstrate that proper
tokenization can significantly mitigate this issue for morphologically rich
fusional languages, sometimes even reversing negative trends. Additionally, we
show that neural-based metrics specifically trained for evaluation, such as
COMET, consistently outperform other neural metrics and better correlate with
human judgments in low-resource languages. Overall, our analysis highlights the
limitations of n-gram metrics for fusional languages and advocates for greater
investment in neural-based metrics trained for evaluation tasks.

</details>


### [34] [Exploring Design of Multi-Agent LLM Dialogues for Research Ideation](https://arxiv.org/abs/2507.08350)
*Keisuke Ueda,Wataru Hirota,Takuto Asakura,Takahiro Omi,Kosuke Takahashi,Kosuke Arima,Tatsuya Ishigaki*

Main category: cs.CL

TL;DR: 本研究分析了多智能体LLM对话在科学创意生成中的效果，发现增加代理群体、加深交互深度和拓宽代理人格多样性可以提高生成想法的多样性和可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的工作表明，LLM之间的结构化对话可以提高生成想法的新颖性和可行性，但最优的设计仍然不清楚。因此，我们进行这项研究以探索多智能体LLM对话在科学创意生成中的最佳设计。

Method: 我们对多智能体LLM对话进行了全面分析，比较了不同的代理角色配置、代理数量和对话深度，以了解这些因素如何影响生成想法的新颖性和可行性。

Result: 我们的结果表明，扩大代理群体、加深交互深度以及拓宽代理人格的多样性可以丰富生成想法的多样性。此外，特别增加创意侧的多样性可以进一步提高最终提案的可行性。

Conclusion: 我们的研究结果为构建有效的多智能体LLM系统提供了实用指南，以支持科学创意生成。

Abstract: Large language models (LLMs) are increasingly used to support creative tasks
such as research idea generation. While recent work has shown that structured
dialogues between LLMs can improve the novelty and feasibility of generated
ideas, the optimal design of such interactions remains unclear. In this study,
we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific
ideation. We compare different configurations of agent roles, number of agents,
and dialogue depth to understand how these factors influence the novelty and
feasibility of generated ideas. Our experimental setup includes settings where
one agent generates ideas and another critiques them, enabling iterative
improvement. Our results show that enlarging the agent cohort, deepening the
interaction depth, and broadening agent persona heterogeneity each enrich the
diversity of generated ideas. Moreover, specifically increasing critic-side
diversity within the ideation-critique-revision loop further boosts the
feasibility of the final proposals. Our findings offer practical guidelines for
building effective multi-agent LLM systems for scientific ideation. Our code is
available at https://github.com/g6000/MultiAgent-Research-Ideator.

</details>


### [35] [The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality](https://arxiv.org/abs/2507.08371)
*Benjamin Newman,Abhilasha Ravichander,Jaehun Jung,Rui Xin,Hamish Ivison,Yegor Kuznetsov,Pang Wei Koh,Yejin Choi*

Main category: cs.CL

TL;DR: 研究发现，使用模型生成的数据进行微调，特别是模型自身认为是事实的数据，比使用高质量的事实数据更有效，这可能有助于减少语言模型的幻觉。


<details>
  <summary>Details</summary>
Motivation: 语言模型容易产生幻觉，即生成不准确的信息。虽然微调模型可以减少幻觉，但获取高质量的事实数据成本高，且训练在正确但不熟悉的数据上可能导致更多的幻觉。因此，需要确定哪些数据适合微调以减轻语言模型的幻觉。

Method: 我们研究了微调数据的准确性与长文本生成任务中幻觉发生率之间的关系，并评估了应用于事实数据和模型生成数据的过滤策略。

Result: 我们发现，使用模型生成的数据进行微调，尤其是模型认为是事实的数据，比使用高质量的事实数据更有效。通过模型自身判断进行过滤的数据微调效果最好。

Conclusion: 我们的研究发现，使用模型生成的数据进行微调，特别是那些模型自身认为是事实的数据，比使用高质量的事实数据更有效。此外，通过模型自身的内部判断对数据进行过滤可以提高整体事实性。

Abstract: Language models are prone to hallucination - generating text that is
factually incorrect. Finetuning models on high-quality factual information can
potentially reduce hallucination, but concerns remain; obtaining factual gold
data can be expensive and training on correct but unfamiliar data may
potentially lead to even more downstream hallucination. What data should
practitioners finetune on to mitigate hallucinations in language models? In
this work, we study the relationship between the factuality of finetuning data
and the prevalence of hallucinations in long-form generation tasks.
Counterintuitively, we find that finetuning on factual gold data is not as
helpful as finetuning on model-generated data that models believe to be
factual. Next, we evaluate filtering strategies applied on both factual gold
data and model-generated data, and find that finetuning on model-generated data
that is filtered by models' own internal judgments often leads to better
overall factuality compared to other configurations: training on gold data
filtered by models' judgments, training on gold data alone, or training on
model-generated data that is supported by gold data. These factuality
improvements transfer across three domains we study, suggesting that a models'
own beliefs can provide a powerful signal for factuality.

</details>


### [36] [A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities](https://arxiv.org/abs/2507.08425)
*Lu Xiang,Yang Zhao,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）在跨学科研究中的应用，从技术方法和适用性两个角度进行了分析，探讨了LLMs在不同学科中的贡献，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多个学科研究中展示了变革性的潜力，但其在不同学科中的整合仍缺乏系统理解。本文旨在填补这一空白，提供全面的综述。

Method: 本文通过从技术角度和适用性角度对LLMs的应用进行分类，分析了监督微调、检索增强生成、基于代理的方法和工具集成等关键方法，并探讨了LLMs在数学、物理、化学、生物学以及人文学科和社会科学中的贡献。

Result: 本文详细分析了LLMs在跨学科研究中的技术方法和实际应用，揭示了当前面临的挑战，并指出了未来的研究方向。

Conclusion: 本文旨在为研究人员提供关于LLMs在跨学科研究中的技术发展和应用的全面概述，帮助他们应对LLMs复杂环境中的挑战。

Abstract: Large Language Models (LLMs) have demonstrated their transformative potential
across numerous disciplinary studies, reshaping the existing research
methodologies and fostering interdisciplinary collaboration. However, a
systematic understanding of their integration into diverse disciplines remains
underexplored. This survey paper provides a comprehensive overview of the
application of LLMs in interdisciplinary studies, categorising research efforts
from both a technical perspective and with regard to their applicability. From
a technical standpoint, key methodologies such as supervised fine-tuning,
retrieval-augmented generation, agent-based approaches, and tool-use
integration are examined, which enhance the adaptability and effectiveness of
LLMs in discipline-specific contexts. From the perspective of their
applicability, this paper explores how LLMs are contributing to various
disciplines including mathematics, physics, chemistry, biology, and the
humanities and social sciences, demonstrating their role in discipline-specific
tasks. The prevailing challenges are critically examined and the promising
research directions are highlighted alongside the recent advances in LLMs. By
providing a comprehensive overview of the technical developments and
applications in this field, this survey aims to serve as an invaluable resource
for the researchers who are navigating the complex landscape of LLMs in the
context of interdisciplinary studies.

</details>


### [37] [ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains](https://arxiv.org/abs/2507.08427)
*Zilu Dong,Xiangqing Shen,Zinong Yang,Rui Xia*

Main category: cs.CL

TL;DR: 本文提出了一种名为ChainEdit的框架，通过结合知识图谱衍生的逻辑规则和大型语言模型的逻辑推理能力，实现了系统性的链式更新。实验表明，该方法在逻辑泛化方面比基线方法提高了30%以上，同时保持了编辑的可靠性和特异性。


<details>
  <summary>Details</summary>
Motivation: 当前的知识编辑方法在传播涟漪效应到相关事实时难以保持逻辑一致性。因此，本文旨在解决这一问题，提高知识编辑后的逻辑一致性。

Method: 本文提出了一种名为ChainEdit的框架，该框架结合了知识图谱衍生的逻辑规则和大型语言模型的逻辑推理能力。通过从结构化知识库中自动提取逻辑模式并与语言模型的内部逻辑对齐，ChainEdit能够动态生成和编辑逻辑相关的知识集群。

Result: 实验结果表明，ChainEdit在逻辑泛化方面比基线方法提高了30%以上，同时保持了编辑的可靠性和特异性。此外，通过知识感知协议，本文还解决了现有基准评估偏差的问题。

Conclusion: 本文提出了ChainEdit框架，通过结合知识图谱衍生的逻辑规则和大型语言模型的逻辑推理能力，实现了系统性的链式更新。实验表明，该方法在逻辑泛化方面比基线方法提高了30%以上，同时保持了编辑的可靠性和特异性。此外，本文还通过知识感知协议解决了现有基准评估偏差的问题，确保了知识编辑后的内部逻辑一致性。

Abstract: Current knowledge editing methods for large language models (LLMs) struggle
to maintain logical consistency when propagating ripple effects to associated
facts. We propose ChainEdit, a framework that synergizes knowledge
graph-derived logical rules with LLM logical reasoning capabilities to enable
systematic chain updates. By automatically extracting logical patterns from
structured knowledge bases and aligning them with LLMs' internal logics,
ChainEdit dynamically generates and edits logically connected knowledge
clusters. Experiments demonstrate an improvement of more than 30% in logical
generalization over baselines while preserving editing reliability and
specificity. We further address evaluation biases in existing benchmarks
through knowledge-aware protocols that disentangle external dependencies. This
work establishes new state-of-the-art performance on ripple effect while
ensuring internal logical consistency after knowledge editing.

</details>


### [38] [Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences](https://arxiv.org/abs/2507.08440)
*Selina Heller,Mohamed Ibrahim,David Antony Selby,Sebastian Vollmer*

Main category: cs.CL

TL;DR: This paper presents a novel LLM-based multi-agent system for simulating decision conferences and detecting agreement among agents.


<details>
  <summary>Details</summary>
Motivation: Decision conferences are structured, collaborative meetings that bring together experts from various fields to address complex issues and reach a consensus on recommendations for future actions or policies. Recently, Large Language Models (LLMs) have shown significant promise in simulating real-world scenarios, particularly through collaborative multi-agent systems that mimic group interactions.

Method: We present a novel LLM-based multi-agent system designed to simulate decision conferences, specifically focusing on detecting agreement among the participant agents. We evaluate six distinct LLMs on two tasks: stance detection and stance polarity detection.

Result: Our results indicate that LLMs can reliably detect agreement even in dynamic and nuanced debates. Incorporating an agreement-detection agent within the system can also improve the efficiency of group debates and enhance the overall quality and coherence of deliberations, making them comparable to real-world decision conferences regarding outcome and decision-making.

Conclusion: LLM-based multi-agent systems have the potential to simulate group decision-making processes and can be instrumental in supporting decision-making with expert elicitation workshops across various domains.

Abstract: Decision conferences are structured, collaborative meetings that bring
together experts from various fields to address complex issues and reach a
consensus on recommendations for future actions or policies. These conferences
often rely on facilitated discussions to ensure productive dialogue and
collective agreement. Recently, Large Language Models (LLMs) have shown
significant promise in simulating real-world scenarios, particularly through
collaborative multi-agent systems that mimic group interactions. In this work,
we present a novel LLM-based multi-agent system designed to simulate decision
conferences, specifically focusing on detecting agreement among the participant
agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance
detection, which identifies the position an agent takes on a given issue, and
stance polarity detection, which identifies the sentiment as positive,
negative, or neutral. These models are further assessed within the multi-agent
system to determine their effectiveness in complex simulations. Our results
indicate that LLMs can reliably detect agreement even in dynamic and nuanced
debates. Incorporating an agreement-detection agent within the system can also
improve the efficiency of group debates and enhance the overall quality and
coherence of deliberations, making them comparable to real-world decision
conferences regarding outcome and decision-making. These findings demonstrate
the potential for LLM-based multi-agent systems to simulate group
decision-making processes. They also highlight that such systems could be
instrumental in supporting decision-making with expert elicitation workshops
across various domains.

</details>


### [39] [Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework](https://arxiv.org/abs/2507.08459)
*Zishan Xu,Shuyi Xie,Qingsong Lv,Shupei Xiao,Linlin Song,Sui Wenjuan,Fan Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新的错误归因框架和相关数据集，以提高对大型语言模型性能分析和故障诊断的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的评估模型缺乏错误归因能力，因此需要一种自动化框架来系统地分类和归因错误。

Method: 本文建立了包含6个主要类别和15个次要类别的错误归因框架，并提出了AttriData数据集以及基于该数据集微调的MisAttributionLLM模型。

Result: 本文提出的框架和模型经过大量实验和分析，证实了其有效性和鲁棒性。

Conclusion: 本文提出了一个全面的错误归因框架，并展示了其在模型性能分析和故障诊断中的有效性。

Abstract: With the widespread application of Large Language Models (LLMs) in various
tasks, the mainstream LLM platforms generate massive user-model interactions
daily. In order to efficiently analyze the performance of models and diagnose
failures in their answers, it is essential to develop an automated framework to
systematically categorize and attribute errors. However, existing evaluation
models lack error attribution capability. In this work, we establish a
comprehensive Misattribution Framework with 6 primary and 15 secondary
categories to facilitate in-depth analysis. Based on this framework, we present
AttriData, a dataset specifically designed for error attribution, encompassing
misattribution, along with the corresponding scores and feedback. We also
propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first
general-purpose judge model capable of simultaneously generating score,
misattribution, and feedback. Extensive experiments and analyses are conducted
to confirm the effectiveness and robustness of our proposed method.

</details>


### [40] [Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study](https://arxiv.org/abs/2507.08468)
*Marina Luketina,Andrea Benkel,Christoph G. Schuetz*

Main category: cs.CL

TL;DR: 本论文通过实验评估了大型语言模型（LLM）在奥地利和欧盟增值税（VAT）法律框架内辅助法律决策的能力。研究发现，当正确配置时，LLM可以有效支持税务专业人士处理增值税任务，并提供合法的决策依据，但目前的原型尚不适合完全自动化。


<details>
  <summary>Details</summary>
Motivation: In tax consulting practice, clients often describe cases in natural language, making LLMs a prime candidate for supporting automated decision-making and reducing the workload of tax professionals. Given the requirement for legally grounded and well-justified analyses, the propensity of LLMs to hallucinate presents a considerable challenge.

Method: The experiments focus on two common methods for enhancing LLM performance: fine-tuning and retrieval-augmented generation (RAG). These methods are applied on both textbook cases and real-world cases from a tax consulting firm to systematically determine the best configurations of LLM-based systems and assess the legal-reasoning capabilities of LLMs.

Result: The findings highlight the potential of using LLMs to support tax consultants by automating routine tasks and providing initial analyses, although current prototypes are not ready for full automation due to the sensitivity of the legal domain.

Conclusion: LLMs, when properly configured, can effectively support tax professionals in VAT tasks and provide legally grounded justifications for decisions. However, limitations remain regarding the handling of implicit client knowledge and context-specific documentation, underscoring the need for future integration of structured background information.

Abstract: This paper provides an experimental evaluation of the capability of large
language models (LLMs) to assist in legal decision-making within the framework
of Austrian and European Union value-added tax (VAT) law. In tax consulting
practice, clients often describe cases in natural language, making LLMs a prime
candidate for supporting automated decision-making and reducing the workload of
tax professionals. Given the requirement for legally grounded and
well-justified analyses, the propensity of LLMs to hallucinate presents a
considerable challenge. The experiments focus on two common methods for
enhancing LLM performance: fine-tuning and retrieval-augmented generation
(RAG). In this study, these methods are applied on both textbook cases and
real-world cases from a tax consulting firm to systematically determine the
best configurations of LLM-based systems and assess the legal-reasoning
capabilities of LLMs. The findings highlight the potential of using LLMs to
support tax consultants by automating routine tasks and providing initial
analyses, although current prototypes are not ready for full automation due to
the sensitivity of the legal domain. The findings indicate that LLMs, when
properly configured, can effectively support tax professionals in VAT tasks and
provide legally grounded justifications for decisions. However, limitations
remain regarding the handling of implicit client knowledge and context-specific
documentation, underscoring the need for future integration of structured
background information.

</details>


### [41] [ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition](https://arxiv.org/abs/2507.08477)
*Qingliang Meng,Hao Wu,Wei Liang,Wei Xu,Qing Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新的训练方法，解决了LoRA在监督微调阶段的过拟合问题，并在实际应用中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 为了解决LoRA在监督微调阶段常见的过拟合问题，本文提出了一个创新的训练范式。

Method: 本文提出了一种创新的训练范式Iterative LoRA Training (ILT)结合迭代伪标签策略，以解决LoRA在监督微调阶段常见的过拟合问题。

Result: 实验结果证明了所提出方法的有效性，并在Interspeech 2025多语言对话语音语言建模挑战赛中取得了优异的成绩。

Conclusion: 本文提出的方法在实际应用中展示了良好的可行性和强大的应用潜力。

Abstract: The deep integration of large language models and automatic speech
recognition systems has become a promising research direction with high
practical value. To address the overfitting issue commonly observed in Low-Rank
Adaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work
proposes an innovative training paradigm Iterative LoRA Training (ILT) in
combination with an Iterative Pseudo Labeling strategy, effectively enhancing
the theoretical upper bound of model performance. Based on Whisper-large-v3 and
Qwen2-Audio, we conduct systematic experiments using a three-stage training
process: Focus Training, Feed Back Training, and Fix Training. Experimental
results demonstrate the effectiveness of the proposed method. Furthermore, the
MegaAIS research team applied this technique in the Interspeech 2025
Multilingual Conversational Speech Language Modeling Challenge (MLC-SLM),
achieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2
(Speech Separation and Recognition Task), showcasing the practical feasibility
and strong application potential of our approach.

</details>


### [42] [Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach](https://arxiv.org/abs/2507.08487)
*Bruno Alexandre Rosa,Hilário Oliveira,Luiz Rodrigues,Eduardo Araujo Oliveira,Rafael Ferreira Mello*

Main category: cs.CL

TL;DR: 本研究提出了一种基于项目反应理论的连贯性评分预测方法，以提高机器学习模型在教育论文自动评估中的表现。


<details>
  <summary>Details</summary>
Motivation: 自动评分连贯性在教育人工智能领域是一个挑战，因为通常使用的机器学习算法不考虑分析语料库中实例的个体特征。

Method: 本文提出并分析了一种基于项目反应理论的连贯性评分预测方法，以调整机器学习模型生成的评分。

Result: 实验结果表明，所提出的方法在多个评估指标上优于传统的机器学习模型和集成方法。

Conclusion: 本研究探索了一种潜在的方法，以改进教育论文中连贯性的自动评估。

Abstract: Essays are considered a valuable mechanism for evaluating learning outcomes
in writing. Textual cohesion is an essential characteristic of a text, as it
facilitates the establishment of meaning between its parts. Automatically
scoring cohesion in essays presents a challenge in the field of educational
artificial intelligence. The machine learning algorithms used to evaluate texts
generally do not consider the individual characteristics of the instances that
comprise the analysed corpus. In this meaning, item response theory can be
adapted to the context of machine learning, characterising the ability,
difficulty and discrimination of the models used. This work proposes and
analyses the performance of a cohesion score prediction approach based on item
response theory to adjust the scores generated by machine learning models. In
this study, the corpus selected for the experiments consisted of the extended
Essay-BR, which includes 6,563 essays in the style of the National High School
Exam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235
essays written by 5th to 9th grade students from public schools. We extracted
325 linguistic features and treated the problem as a machine learning
regression task. The experimental results indicate that the proposed approach
outperforms conventional machine learning models and ensemble methods in
several evaluation metrics. This research explores a potential approach for
improving the automatic evaluation of cohesion in educational essays.

</details>


### [43] [A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench](https://arxiv.org/abs/2507.08491)
*David Schlangen,Sherzod Hakimov,Jonathan Jordan,Philipp Sadler*

Main category: cs.CL

TL;DR: 本文介绍了一个名为clembench的基准测试工具，用于评估大型语言模型，结合了参考评估和偏好评估的优点，并提供了易于扩展的功能。


<details>
  <summary>Details</summary>
Motivation: 目前评估大型语言模型的方法存在控制性和生态效性之间的权衡，而对话游戏基准评估方法结合了两者的优点，但缺乏成熟且易于重用的实现。因此，本文提出了clembench来解决这一问题。

Method: 本文描述了对话游戏基准评估方法，并介绍了clembench工具的使用方式和扩展性。

Result: clembench已经被开发并优化，可以用于基准测试自己的模型，并且可以轻松扩展以添加新的定制测试。

Conclusion: 本文介绍了clembench，这是一个自2023年以来持续开发的基准测试工具，其最新版本优化了通用使用的便捷性。它不仅可以用于基准测试自己的模型，还可以轻松扩展以添加新的定制测试。

Abstract: There are currently two main paradigms for evaluating large language models
(LLMs), reference-based evaluation and preference-based evaluation. The first,
carried over from the evaluation of machine learning models in general, relies
on pre-defined task instances, for which reference task executions are
available. The second, best exemplified by the LM-arena, relies on (often
self-selected) users bringing their own intents to a site that routes these to
several models in parallel, among whose responses the user then selects their
most preferred one. The former paradigm hence excels at control over what is
tested, while the latter comes with higher ecological validity, testing actual
use cases interactively. Recently, a third complementary paradigm has emerged
that combines some of the strengths of these approaches, offering control over
multi-turn, reference-free, repeatable interactions, while stressing
goal-directedness: dialogue game based evaluation. While the utility of this
approach has been shown by several projects, its adoption has been held back by
the lack of a mature, easily re-usable implementation. In this paper, we
present clembench, which has been in continuous development since 2023 and has
in its latest release been optimized for ease of general use. We describe how
it can be used to benchmark one's own models (using a provided set of benchmark
game instances in English), as well as how easily the benchmark itself can be
extended with new, tailor-made targeted tests.

</details>


### [44] [LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning](https://arxiv.org/abs/2507.08496)
*Shibo Sun,Xue Li,Donglin Di,Mingjie Wei,Lanshun Nie,Wei-Nan Zhang,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

TL;DR: LLaPa is a vision-language model framework designed for multimodal procedural planning, enhancing LLMs with two auxiliary modules to improve performance in counterfactual scenarios.


<details>
  <summary>Details</summary>
Motivation: The integration of multimodal inputs and counterfactual reasoning in large language models (LLMs) for embodied AI systems remains underexplored.

Method: LLaPa generates executable action sequences from textual task descriptions and visual environmental images using vision-language models (VLMs). It also enhances with two auxiliary modules: the Task-Environment Reranker (TER) and the Counterfactual Activities Retriever (CAR).

Result: Extensive experiments on ActPlan-1K and ALFRED benchmarks demonstrate that LLaPa generates higher-quality plans with superior LCS and correctness, outperforming advanced models.

Conclusion: LLaPa generates higher-quality plans with superior LCS and correctness, outperforming advanced models.

Abstract: While large language models (LLMs) have advanced procedural planning for
embodied AI systems through strong reasoning abilities, the integration of
multimodal inputs and counterfactual reasoning remains underexplored. To tackle
these challenges, we introduce LLaPa, a vision-language model framework
designed for multimodal procedural planning. LLaPa generates executable action
sequences from textual task descriptions and visual environmental images using
vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary
modules to improve procedural planning. The first module, the Task-Environment
Reranker (TER), leverages task-oriented segmentation to create a task-sensitive
feature space, aligning textual descriptions with visual environments and
emphasizing critical regions for procedural execution. The second module, the
Counterfactual Activities Retriever (CAR), identifies and emphasizes potential
counterfactual conditions, enhancing the model's reasoning capability in
counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED
benchmarks demonstrate that LLaPa generates higher-quality plans with superior
LCS and correctness, outperforming advanced models. The code and models are
available https://github.com/sunshibo1234/LLaPa.

</details>


### [45] [Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop](https://arxiv.org/abs/2507.08498)
*Mengze Hong,Chen Jason Zhang,Di Jiang*

Main category: cs.CL

TL;DR: 本文探讨了将大型语言模型（LLMs）整合到主题模型的两个关键阶段（初始化和后校正）的效果。实验结果表明，LLM引导的初始化在早期迭代中有所改善，但对收敛没有影响，而LLM启用的后校正在连贯性评估中实现了显著提升。


<details>
  <summary>Details</summary>
Motivation: 由于LDA高度依赖于其初始化的质量，因此我们进行了大量实验，研究LLM引导的主题聚类以初始化Gibbs采样算法。

Method: 通过将大型语言模型（LLMs）集成到两个关键阶段：初始化和后校正，来探索增强主题模型的有效性。

Result: 实验结果表明，虽然所提出的初始化策略提高了LDA的早期迭代，但对收敛没有影响，并且在基线中表现最差。另一方面，LLM启用的后校正在连贯性评估中实现了5.86%的改进。

Conclusion: 这些结果突显了LLM-in-the-loop方法的实际好处，并挑战了LLM总是优于文本挖掘替代方案的信念。

Abstract: Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic
model used for uncovering abstract topics within document collections. In this
paper, we explore the effectiveness of augmenting topic models with Large
Language Models (LLMs) through integration into two key phases: Initialization
and Post-Correction. Since the LDA is highly dependent on the quality of its
initialization, we conduct extensive experiments on the LLM-guided topic
clustering for initializing the Gibbs sampling algorithm. Interestingly, the
experimental results reveal that while the proposed initialization strategy
improves the early iterations of LDA, it has no effect on the convergence and
yields the worst performance compared to the baselines. The LLM-enabled
post-correction, on the other hand, achieved a promising improvement of 5.86%
in the coherence evaluation. These results highlight the practical benefits of
the LLM-in-the-loop approach and challenge the belief that LLMs are always the
superior text mining alternative.

</details>


### [46] [PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts](https://arxiv.org/abs/2507.08499)
*Ziyi Huang,Xia Cui*

Main category: cs.CL

TL;DR: 本文提出了一种针对多语言情感检测的特征驱动框架，评估了文档表示、降维和模型训练，并展示了其在28种语言中的有效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决短文本中的多标签情感检测问题，特别是在不同语言中的表现优化。

Method: 我们提出了一个以特征为中心的框架，动态适应文档表示和学习算法以优化语言特定性能。

Result: TF-IDF在低资源语言中仍然非常有效，而像FastText和基于Transformer的文档表示（如Sentence-BERT生成的）表现出语言特定的优势。PCA减少了训练时间而不影响性能，特别是对FastText和神经模型如MLP有益。

Conclusion: 我们的框架为多语言情感检测提供了可扩展的解决方案，解决了语言多样性和资源限制的问题。

Abstract: This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection (Track A), which focuses on multi-label emotion
detection in short texts. We propose a feature-centric framework that
dynamically adapts document representations and learning algorithms to optimize
language-specific performance. Our study evaluates three key components:
document representation, dimensionality reduction, and model training in 28
languages, highlighting five for detailed analysis. The results show that
TF-IDF remains highly effective for low-resource languages, while contextual
embeddings like FastText and transformer-based document representations, such
as those produced by Sentence-BERT, exhibit language-specific strengths.
Principal Component Analysis (PCA) reduces training time without compromising
performance, particularly benefiting FastText and neural models such as
Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores
the trade-off between model complexity and processing cost. Our framework
provides a scalable solution for multilingual emotion detection, addressing the
challenges of linguistic diversity and resource constraints.

</details>


### [47] [The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks](https://arxiv.org/abs/2507.08538)
*David Pomerenke,Jonas Nothnagel,Simon Ostermann*

Main category: cs.CL

TL;DR: 本文介绍了一种名为AI语言能力监测器的多语言基准测试，用于评估大型语言模型在多种语言中的表现，特别是低资源语言。


<details>
  <summary>Details</summary>
Motivation: 为了确保全球各地的人们都能公平地享受到大型语言模型（LLMs）的好处，评估它们在世界各种语言中的能力是至关重要的。

Method: 我们引入了AI语言能力监测器，这是一个全面的多语言基准测试，系统地评估大型语言模型在多达200种语言上的表现，特别是低资源语言。

Result: 我们提供了一个开源、自动更新的排行榜和仪表板，支持研究人员、开发人员和政策制定者识别模型性能的优势和不足之处。

Conclusion: 我们的工作旨在促进多语言人工智能的透明度、包容性和进展。

Abstract: To ensure equitable access to the benefits of large language models (LLMs),
it is essential to evaluate their capabilities across the world's languages. We
introduce the AI Language Proficiency Monitor, a comprehensive multilingual
benchmark that systematically assesses LLM performance across up to 200
languages, with a particular focus on low-resource languages. Our benchmark
aggregates diverse tasks including translation, question answering, math, and
reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We
provide an open-source, auto-updating leaderboard and dashboard that supports
researchers, developers, and policymakers in identifying strengths and gaps in
model performance. In addition to ranking models, the platform offers
descriptive insights such as a global proficiency map and trends over time. By
complementing and extending prior multilingual benchmarks, our work aims to
foster transparency, inclusivity, and progress in multilingual AI. The system
is available at
https://huggingface.co/spaces/fair-forward/evals-for-every-language.

</details>


### [48] [DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures](https://arxiv.org/abs/2507.08606)
*Benno Uthayasooriyar,Antoine Ly,Franck Vermet,Caio Corro*

Main category: cs.CL

TL;DR: DocPolarBERT是一种布局感知的BERT模型，它通过使用相对极坐标系中的文本块位置来改进文档理解，即使在较小的数据集上也能取得先进结果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在消除对绝对2D位置嵌入的依赖，并展示精心设计的注意力机制可以补偿预训练数据的减少。

Method: DocPolarBERT通过将自注意力扩展到考虑文本块在相对极坐标系中的位置，而不是笛卡尔坐标系，从而实现了布局感知的BERT模型。

Result: 尽管在比广泛使用的IIT-CDIP语料库小六倍的数据集上进行预训练，DocPolarBERT仍取得了最先进的结果。

Conclusion: DocPolarBERT展示了精心设计的注意力机制可以弥补预训练数据减少的影响，为文档理解提供了一种高效且有效的替代方案。

Abstract: We introduce DocPolarBERT, a layout-aware BERT model for document
understanding that eliminates the need for absolute 2D positional embeddings.
We extend self-attention to take into account text block positions in relative
polar coordinate system rather than the Cartesian one. Despite being
pre-trained on a dataset more than six times smaller than the widely used
IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results
demonstrate that a carefully designed attention mechanism can compensate for
reduced pre-training data, offering an efficient and effective alternative for
document understanding.

</details>


### [49] [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)
*Marcin Pietroń,Rafał Olszowski,Jakub Gomułka,Filip Gampel,Andrzej Tomski*

Main category: cs.CL

TL;DR: 本文研究了多种大型语言模型在论点分类任务中的表现，发现ChatGPT-4o和Deepseek-R1表现最佳，但也存在错误，并指出了提示算法的不足之处。


<details>
  <summary>Details</summary>
Motivation: 尽管有大量用于测试和验证LLM质量的基准，但目前仍缺乏关于这些模型在公开可用的论点分类数据库中操作的研究和结果。

Method: 本文研究了多种LLM，包括GPT、Llama和DeepSeek，以及结合了思维链算法的增强版本，并使用Args.me和UKP等多样化数据集进行了测试。

Result: 结果表明，ChatGPT-4o在论点分类基准中表现优于其他模型，而结合推理能力的Deepseek-R1也表现出优势。然而，GPT-4o和Deepseek-R1仍然存在错误。

Conclusion: 本文是首次对所述数据集使用LLM和提示算法进行更广泛的分析，同时展示了已知提示算法在论点分析中的弱点，并指出了改进方向。

Abstract: Argument mining (AM) is an interdisciplinary research field that integrates
insights from logic, philosophy, linguistics, rhetoric, law, psychology, and
computer science. It involves the automatic identification and extraction of
argumentative components, such as premises and claims, and the detection of
relationships between them, such as support, attack, or neutrality. Recently,
the field has advanced significantly, especially with the advent of large
language models (LLMs), which have enhanced the efficiency of analyzing and
extracting argument semantics compared to traditional methods and other deep
learning models. There are many benchmarks for testing and verifying the
quality of LLM, but there is still a lack of research and results on the
operation of these models in publicly available argument classification
databases. This paper presents a study of a selection of LLM's, using diverse
datasets such as Args.me and UKP. The models tested include versions of GPT,
Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the
Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms
the others in the argument classification benchmarks. In case of models
incorporated with reasoning capabilities, the Deepseek-R1 shows its
superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still
make errors. The most common errors are discussed for all models. To our
knowledge, the presented work is the first broader analysis of the mentioned
datasets using LLM and prompt algorithms. The work also shows some weaknesses
of known prompt algorithms in argument analysis, while indicating directions
for their improvement. The added value of the work is the in-depth analysis of
the available argument datasets and the demonstration of their shortcomings.

</details>


### [50] [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
*Cristina Aggazzotti,Matthew Wiesner,Elizabeth Allyn Smith,Nicholas Andrews*

Main category: cs.CL

TL;DR: 本研究探讨了自动转录对说话人归属性能的影响，发现基于ASR生成的错误转录文本进行说话人归属的表现与基于人工转录数据的表现相当，甚至更好。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，通常只有由自动语音识别（ASR）系统生成的错误更多的转录文本，而之前的工作主要集中在使用人工标注的转录文本进行说话人归属的可行性上。

Method: 我们进行了第一个全面的研究，探讨了自动转录对说话人归属性能的影响，包括转录错误对归属性能的降级程度以及ASR系统特性对归属的影响。

Result: 我们发现说话人归属对单词级别的转录错误表现出惊人的韧性，并且恢复真实转录的目标与归属性能的相关性很小。

Conclusion: 我们的研究结果表明，基于ASR生成的更错误的转录文本进行说话人归属的表现与基于人工转录数据的表现相当，甚至更好，这可能是因为ASR转录错误可以捕捉到揭示说话人身份的特定特征。

Abstract: Speaker attribution from speech transcripts is the task of identifying a
speaker from the transcript of their speech based on patterns in their language
use. This task is especially useful when the audio is unavailable (e.g.
deleted) or unreliable (e.g. anonymized speech). Prior work in this area has
primarily focused on the feasibility of attributing speakers using transcripts
produced by human annotators. However, in real-world settings, one often only
has more errorful transcripts produced by automatic speech recognition (ASR)
systems. In this paper, we conduct what is, to our knowledge, the first
comprehensive study of the impact of automatic transcription on speaker
attribution performance. In particular, we study the extent to which speaker
attribution performance degrades in the face of transcription errors, as well
as how properties of the ASR system impact attribution. We find that
attribution is surprisingly resilient to word-level transcription errors and
that the objective of recovering the true transcript is minimally correlated
with attribution performance. Overall, our findings suggest that speaker
attribution on more errorful transcripts produced by ASR is as good, if not
better, than attribution based on human-transcribed data, possibly because ASR
transcription errors can capture speaker-specific features revealing of speaker
identity.

</details>


### [51] [KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment](https://arxiv.org/abs/2507.08665)
*Jiyao Zhang,Chengli Zhong,Hui Xu,Qige Li,Yi Zhou*

Main category: cs.CL

TL;DR: 本文提出了KELPS框架，用于将非正式数据翻译、合成和过滤成多种形式语言。该框架在MiniF2F数据集上实现了88.9%的语法准确性，优于现有最先进的模型。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（LLMs）在将非正式数学形式化为机器可验证定理方面显示出有希望的进展。然而，由于多语言平行语料库的数量和质量有限，这些方法仍然面临瓶颈。本文提出了一种新的神经符号框架KELPS来解决这些问题。

Method: KELPS是一个迭代框架，用于将非正式数据翻译、合成和过滤成多种形式语言（Lean、Coq和Isabelle）。首先，将自然语言翻译成知识方程（KEs），这是一种我们设计的新语言，理论上基于断言逻辑。然后通过严格定义的规则将其转换为目标语言，以保留语法结构和语义含义。

Result: 该过程产生了超过60,000个问题的平行语料库。KELPS框架在MiniF2F数据集上实现了88.9%的语法准确性（pass@1），优于SOTA模型如Deepseek-V3（81%）和Herald（81.3%）。

Conclusion: KELPS框架在MiniF2F数据集上实现了88.9%的语法准确性（pass@1），优于SOTA模型如Deepseek-V3（81%）和Herald（81.3%）。所有数据集和代码都可在补充材料中获得。

Abstract: Modern large language models (LLMs) show promising progress in formalizing
informal mathematics into machine-verifiable theorems. However, these methods
still face bottlenecks due to the limited quantity and quality of multilingual
parallel corpora. In this paper, we propose a novel neuro-symbolic framework
KELPS (Knowledge-Equation based Logical Processing System) to address these
problems. KELPS is an iterative framework for translating, synthesizing, and
filtering informal data into multiple formal languages (Lean, Coq, and
Isabelle). First, we translate natural language into Knowledge Equations (KEs),
a novel language that we designed, theoretically grounded in assertional logic.
Next, we convert them to target languages through rigorously defined rules that
preserve both syntactic structure and semantic meaning. This process yielded a
parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic
accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3
(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are
available in the supplementary materials.

</details>


### [52] [KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation](https://arxiv.org/abs/2507.08704)
*Songlin Zhai,Guilin Qi,Yuan Meng*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱引导注意力（KGA）模块的测试时KG增强框架，能够实现实时知识融合而不修改参数，并在多个基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的KG增强方法依赖于参数密集型微调，这可能导致灾难性遗忘并降低预训练模型的泛化能力。此外，由于静态集成框架，它们在实时知识更新方面表现出有限的适应性。因此，需要一种新的方法来解决这些问题。

Method: 本文提出了一种基于知识图谱引导注意力（KGA）模块的测试时KG增强框架。该模块通过两个协同的路径：向外聚合和向内聚合，增强了标准自注意力机制。向外聚合动态地将外部知识融入输入表示中，而向内聚合则通过KG引导的过滤来优化输入表示。

Result: KGA模块能够在不修改任何参数的情况下，在测试时实现实时知识融合。实验结果表明，KGA在五个基准数据集上表现出了与现有方法相当的知识融合性能。

Conclusion: KGA模块通过结合向外和向内的聚合路径，实现了在测试时的实时知识融合，无需任何参数修改。实验结果表明，KGA在五个基准数据集上表现出了可比的知识融合性能。

Abstract: Knowledge graphs (KGs) play a critical role in enhancing large language
models (LLMs) by introducing structured and grounded knowledge into the
learning process. However, most existing KG-enhanced approaches rely on
parameter-intensive fine-tuning, which risks catastrophic forgetting and
degrades the pretrained model's generalization. Moreover, they exhibit limited
adaptability to real-time knowledge updates due to their static integration
frameworks. To address these issues, we introduce the first test-time
KG-augmented framework for LLMs, built around a dedicated knowledge
graph-guided attention (KGA) module that enables dynamic knowledge fusion
without any parameter updates. The proposed KGA module augments the standard
self-attention mechanism with two synergistic pathways: outward and inward
aggregation. Specifically, the outward pathway dynamically integrates external
knowledge into input representations via input-driven KG fusion. This inward
aggregation complements the outward pathway by refining input representations
through KG-guided filtering, suppressing task-irrelevant signals and amplifying
knowledge-relevant patterns. Importantly, while the outward pathway handles
knowledge fusion, the inward path selects the most relevant triples and feeds
them back into the fusion process, forming a closed-loop enhancement mechanism.
By synergistically combining these two pathways, the proposed method supports
real-time knowledge fusion exclusively at test-time, without any parameter
modification. Extensive experiments on five benchmarks verify the comparable
knowledge fusion performance of KGA.

</details>


### [53] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
*Linzheng Chai,Jian Yang,Shukai Liu,Wei Zhang,Liran Wang,Ke Jin,Tao Sun,Congnan Liu,Chenchen Zhang,Hualei Zhu,Jiaheng Liu,Xianjie Wu,Ge Zhang,Tianyu Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: 本文介绍了MM-Coder，一种多语言多模态软件开发者，它结合了文本指令和视觉设计输入（如UML图和流程图）以提高代码生成的准确性和架构一致性。我们还创建了一个多模态指令调优数据集和一个新的多模态代码生成评估基准，以解决现有文本-only方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在代码生成方面取得了显著进展，但大多数模型仍然是纯文本的，忽视了现实世界软件开发中使用的图表和流程图等重要视觉辅助工具。为了弥合这一差距，我们提出了MM-Coder。

Method: 我们引入了MM-Coder，这是一种多语言多模态软件开发者。MM-Coder将统一建模语言（UML）图和流程图（称为视觉工作流）与文本指令相结合，以提高代码生成的准确性与架构一致性。我们还开发了MMc-Instruct，一个多样化的多模态指令调优数据集，以及MMEval，一个新的多模态代码生成评估基准。

Result: 我们的评估使用MMEval表明，模型在精确的视觉信息捕捉、指令遵循和高级编程知识方面仍存在重大挑战。

Conclusion: 我们的工作旨在通过使LLM能够解释和实现通过文本和视觉设计传达的复杂规范，来彻底改变工业编程。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
improved code generation, yet most models remain text-only, neglecting crucial
visual aids like diagrams and flowcharts used in real-world software
development. To bridge this gap, we introduce MM-Coder, a Multilingual
Multimodal software developer. MM-Coder integrates visual design inputs-Unified
Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with
textual instructions to enhance code generation accuracy and architectural
alignment. To enable this, we developed MMc-Instruct, a diverse multimodal
instruction-tuning dataset including visual-workflow-based code generation,
allowing MM-Coder to synthesize textual and graphical information like human
developers, distinct from prior work on narrow tasks. Furthermore, we introduce
MMEval, a new benchmark for evaluating multimodal code generation, addressing
existing text-only limitations. Our evaluations using MMEval highlight
significant remaining challenges for models in precise visual information
capture, instruction following, and advanced programming knowledge. Our work
aims to revolutionize industrial programming by enabling LLMs to interpret and
implement complex specifications conveyed through both text and visual designs.

</details>


### [54] [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799)
*Max Belitsky,Dawid J. Kopiczko,Michael Dorkenwald,M. Jehanzeb Mirza,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: 提出了一种轻量级方法，通过一次性干预直接对键值缓存进行隐式引导，以诱导语言模型的思维链推理。


<details>
  <summary>Details</summary>
Motivation: 为了验证其有效性，我们将缓存引导应用于在小型语言模型中诱导思维链推理。

Method: 通过直接对键值缓存应用一次性的干预来实现语言模型的隐式引导。

Result: 实验评估表明，缓存引导提高了模型推理的定性结构和定量任务性能。

Conclusion: cache steering 是一种更鲁棒和实用的解决方案，适用于受控生成。

Abstract: We propose cache steering, a lightweight method for implicit steering of
language models via a one-shot intervention applied directly to the key-value
cache. To validate its effectiveness, we apply cache steering to induce
chain-of-thought reasoning in small language models. Our approach leverages
GPT-4o-generated reasoning traces to construct steering vectors that shift
model behavior toward more explicit, multi-step reasoning without fine-tuning
or prompt modifications. Experimental evaluations on diverse reasoning
benchmarks demonstrate that cache steering improves both the qualitative
structure of model reasoning and quantitative task performance. Compared to
prior activation steering techniques that require continuous interventions, our
one-shot cache steering offers substantial advantages in terms of
hyperparameter stability, inference-time efficiency, and ease of integration,
making it a more robust and practical solution for controlled generation.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [55] [VideoConviction: A Multimodal Benchmark for Human Conviction and Stock Market Recommendations](https://arxiv.org/abs/2507.08104)
*Michael Galarnyk,Veer Kejriwal,Agam Shah,Yash Bhardwaj,Nicholas Meyer,Anand Krishnan,Sudheer Chava*

Main category: cs.MM

TL;DR: 本文介绍了VideoConviction数据集，用于基准测试多模态和基于文本的大型语言模型在金融话语中的表现。结果表明，尽管多模态输入提高了股票代码提取，但模型在区分投资行动和信念强度方面仍然存在困难。高信念度的建议表现优于低信念度的建议，但仍不如标普500指数基金。反向策略在年回报率上表现更好，但风险更高。


<details>
  <summary>Details</summary>
Motivation: 理解金融影响者（finfluencers）的影响需要分析多模态信号，如语气、表达风格和面部表情，这些超出了基于文本的金融分析。

Method: 我们引入了VideoConviction，这是一个带有6000多个专家注释的多模态数据集，通过457小时的人类努力制作，用于基准测试多模态大语言模型（MLLMs）和基于文本的大语言模型（LLMs）在金融话语中的表现。

Result: 虽然多模态输入提高了股票代码提取（例如提取苹果的代码AAPL），但MLLMs和LLMs都难以区分投资行动和信念强度——通过自信的表达和详细推理传达的信念强度——经常将一般评论误分类为明确的建议。高信念度的建议表现优于低信念度的建议，但仍不如流行的标普500指数基金。一种反向策略——做空金融影响者的建议——在年回报率上比标普500高出6.8%，但风险更大（夏普比率0.41 vs. 0.65）。

Conclusion: 我们的基准测试使多模态任务的多样化评估成为可能，比较了模型在完整视频和分段视频输入上的表现。这促进了多模态金融研究的深入发展。

Abstract: Social media has amplified the reach of financial influencers known as
"finfluencers," who share stock recommendations on platforms like YouTube.
Understanding their influence requires analyzing multimodal signals like tone,
delivery style, and facial expressions, which extend beyond text-based
financial analysis. We introduce VideoConviction, a multimodal dataset with
6,000+ expert annotations, produced through 457 hours of human effort, to
benchmark multimodal large language models (MLLMs) and text-based large
language models (LLMs) in financial discourse. Our results show that while
multimodal inputs improve stock ticker extraction (e.g., extracting Apple's
ticker AAPL), both MLLMs and LLMs struggle to distinguish investment actions
and conviction--the strength of belief conveyed through confident delivery and
detailed reasoning--often misclassifying general commentary as definitive
recommendations. While high-conviction recommendations perform better than
low-conviction ones, they still underperform the popular S\&P 500 index fund.
An inverse strategy--betting against finfluencer recommendations--outperforms
the S\&P 500 by 6.8\% in annual returns but carries greater risk (Sharpe ratio
of 0.41 vs. 0.65). Our benchmark enables a diverse evaluation of multimodal
tasks, comparing model performance on both full video and segmented video
inputs. This enables deeper advancements in multimodal financial research. Our
code, dataset, and evaluation leaderboard are available under the CC BY-NC 4.0
license.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)
*Aleksei Ilin,Gor Matevosyan,Xueying Ma,Vladimir Eremin,Suhaa Dada,Muqun Li,Riyaaz Shaik,Haluk Noyan Tokgozoglu*

Main category: cs.LG

TL;DR: 本文介绍了一种轻量级但高效的语言模型安全防护框架，通过高质量的合成数据生成和对抗训练，使小型语言模型在内容审核任务中达到甚至超越大型模型的性能。该框架减少了计算开销，增强了对对抗攻击的抵抗力，为AI系统的内容审核提供了一个可扩展且高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决当前大型语言模型在内容审核任务中的计算开销大、效率低的问题，并探索小型语言模型是否能够作为有效的安全防护框架。

Method: 本文采用了高质量的合成数据生成和对抗训练的方法。合成数据生成过程从人工筛选的种子数据开始，经过查询增强和改写以创建多样且上下文丰富的示例。这些增强的数据经过多轮筛选，确保高保真度和相关性。对抗训练借鉴了生成对抗网络（GAN）架构的最新进展，使用强化学习引导生成器产生具有挑战性的合成示例，用于微调安全分类器，提高其检测和缓解有害内容的能力。此外，还结合了近期关于高效大语言模型训练的研究策略，利用小型模型的能力来提升大型生成模型的性能。

Result: 通过迭代对抗训练和生成多样化的高质量合成数据，本文提出的框架使小型语言模型能够作为强大的安全防护机制。实验结果表明，小型语言模型在内容审核任务中不仅能够达到大型模型的性能，甚至可能超越它们。

Conclusion: 本文提出了一种轻量级但高效的语言模型安全防护框架，证明小型语言模型可以在内容审核任务中达到甚至超越大型模型的性能。该框架通过高质量的合成数据生成和对抗训练实现，能够减少计算开销并增强对对抗攻击的抵抗力，为AI系统的内容审核提供了一个可扩展且高效解决方案。

Abstract: We introduce a lightweight yet highly effective safety guardrail framework
for language models, demonstrating that small-scale language models can
achieve, and even surpass, the performance of larger counterparts in content
moderation tasks. This is accomplished through high-fidelity synthetic data
generation and adversarial training. The synthetic data generation process
begins with human-curated seed data, which undergoes query augmentation and
paraphrasing to create diverse and contextually rich examples. This augmented
data is then subjected to multiple rounds of curation, ensuring high fidelity
and relevance. Inspired by recent advances in the Generative Adversarial
Network (GAN) architecture, our adversarial training employs reinforcement
learning to guide a generator that produces challenging synthetic examples.
These examples are used to fine-tune the safety classifier, enhancing its
ability to detect and mitigate harmful content. Additionally, we incorporate
strategies from recent research on efficient LLM training, leveraging the
capabilities of smaller models to improve the performance of larger generative
models. With iterative adversarial training and the generation of diverse,
high-quality synthetic data, our framework enables small language models (SLMs)
to serve as robust safety guardrails. This approach not only reduces
computational overhead but also enhances resilience against adversarial
attacks, offering a scalable and efficient solution for content moderation in
AI systems.

</details>


### [57] [Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)](https://arxiv.org/abs/2507.08637)
*Vincenzo Dentamaro*

Main category: cs.LG

TL;DR: WERSA是一种新型的线性时间复杂度注意力机制，能够在不牺牲准确性的前提下显著降低计算负载，从而实现更实用、更经济的长上下文模型。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在长序列上的计算成本很高，因为常规注意力具有二次时间复杂度O(n^2)。需要一种新的机制来实现线性时间复杂度O(n)，以实现成功的长序列处理而无需性能权衡。

Method: WERSA结合了内容自适应的随机频谱特征和多分辨率Haar小波以及可学习参数，以选择性地关注数据的信息尺度，同时保持线性效率。

Result: WERSA在各种基准测试中表现出色，包括视觉、NLP和层次推理，其准确性优于所有其他方法。在ArXiv分类任务中，WERSA比原始注意力提高了1.2%的准确性，并减少了81%的训练时间和73.4%的FLOPS。在ArXiv-128k的极长序列上，WERSA实现了最佳准确性(79.1%)和AUC(0.979)，并且比Waveformer快两倍。

Conclusion: WERSA通过显著降低计算负载而不牺牲准确性，使得更实用、更经济的长上下文模型成为可能，特别是在低资源硬件上，有助于更可持续和可扩展的AI发展。

Abstract: Transformer models are computationally costly on long sequences since regular
attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced
Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time
complexity that is pivotal to enable successful long-sequence processing
without the performance trade-off. WERSA merges content-adaptive random
spectral features together with multi-resolution Haar wavelets and learnable
parameters to selectively attend to informative scales of data while preserving
linear efficiency.
  Large-scale comparisons \textbf{on single GPU} and across various benchmarks
(vision, NLP, hierarchical reasoning) and various attention mechanisms (like
Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,
Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in
all tests. On ArXiv classification, WERSA improves accuracy over vanilla
attention by 1.2\% (86.2\% vs 85.0\%) while cutting training time by 81\% (296s
vs 1554s) and FLOPS by 73.4\% (26.2G vs 98.4G). Significantly, WERSA excels
where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy
sequences, it achieves best accuracy (79.1\%) and AUC (0.979) among viable
methods, operating on data that gives Out-Of-Memory errors to quadratic methods
while being \textbf{twice as fast} as Waveformer, its next-best competitor.
  By significantly reducing computational loads without compromising accuracy,
WERSA makes possible more practical, more affordable, long-context models, in
particular on low-resource hardware, for more sustainable and more scalable AI
development.

</details>


### [58] [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
*Chenyang Song,Weilin Zhao,Xu Han,Chaojun Xiao,Yingfa Chen,Yuxuan Li,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: BlockFFN is a novel MoE architecture that improves sparsity and acceleration, achieving high performance and efficiency on end-side devices.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of vanilla MoE architectures, such as non-differentiable routing and low chunk-level sparsity, which hinder model performance and acceleration under low-resource conditions.

Method: Introduce BlockFFN, a novel MoE architecture with differentiable and flexible routing using ReLU activation and RMSNorm. Design CLS-aware training objectives to promote both token-level and chunk-level sparsity. Implement efficient acceleration kernels combining activation sparsity and speculative decoding.

Result: BlockFFN achieves over 80% token-level sparsity and 70% 8-token chunk-level sparsity. The proposed acceleration kernels achieve up to 3.67× speedup on real end-side devices compared to dense models.

Conclusion: BlockFFN demonstrates superior performance over other MoE baselines, achieving high token-level and chunk-level sparsity, and provides significant speedup on end-side devices.

Abstract: To alleviate the computational burden of large language models (LLMs),
architectures with activation sparsity, represented by mixture-of-experts
(MoE), have attracted increasing attention. However, the non-differentiable and
inflexible routing of vanilla MoE hurts model performance. Moreover, while each
token activates only a few parameters, these sparsely-activated architectures
exhibit low chunk-level sparsity, indicating that the union of multiple
consecutive tokens activates a large ratio of parameters. Such a sparsity
pattern is unfriendly for acceleration under low-resource conditions (e.g.,
end-side devices) and incompatible with mainstream acceleration techniques
(e.g., speculative decoding). To address these challenges, we introduce a novel
MoE architecture, BlockFFN, as well as its efficient training and deployment
techniques. Specifically, we use a router integrating ReLU activation and
RMSNorm for differentiable and flexible routing. Next, to promote both
token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training
objectives are designed, making BlockFFN more acceleration-friendly. Finally,
we implement efficient acceleration kernels, combining activation sparsity and
speculative decoding for the first time. The experimental results demonstrate
the superior performance of BlockFFN over other MoE baselines, achieving over
80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on
real end-side devices than dense models. All codes and checkpoints are
available publicly (https://github.com/thunlp/BlockFFN).

</details>


### [59] [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
*Yulai Zhao,Haolin Liu,Dian Yu,S. Y. Kung,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 本文研究了生成奖励模型在强化学习中的脆弱性，并提出了一种改进的方法以提高其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成奖励模型在强化学习中被广泛使用，但它们容易受到表面操纵的影响，这可能导致错误的奖励信号。

Method: 我们引入了一种简单而有效的数据增强策略，并训练了一个具有显著改进的鲁棒性的生成奖励模型。

Result: 我们发现这种弱点在各种大型语言模型、数据集和提示格式中普遍存在，并通过数据增强策略提高了生成奖励模型的鲁棒性。

Conclusion: 我们的研究突显了对更可靠的基于大型语言模型的评估方法的迫切需求，并发布了我们稳健的通用领域奖励模型及其合成训练数据。

Abstract: Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: 本文介绍了M2-Reasoning-7B，一种在一般和空间推理方面表现出色的模型，通过创新的数据管道和训练策略实现了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 弥合当前模型在动态空间交互方面的不足，这是实际应用所必需的能力。

Method: 引入了一种新的数据管道和动态多任务训练策略，以减少数据和任务之间的冲突，并提供定制的激励信号。

Result: M2-Reasoning-7B 在8个基准测试中达到了新的最先进水平，展示了在一般和空间推理领域的卓越性能。

Conclusion: M2-Reasoning-7B 在8个基准测试中达到了新的最先进水平，展示了在一般和空间推理领域的卓越性能。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [61] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 本文提出了一种结合多粒度稀疏激活和分层知识图谱的框架，以提高罕见病诊断的准确性，并通过实验和专家评估验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管医疗大型语言模型在医疗保健领域取得了进展，但罕见病诊断仍受到知识表示深度不足、概念理解有限和临床推理受限的阻碍。

Method: 我们提出了一种框架，将医学概念的多粒度稀疏激活与分层知识图谱相结合。四种互补的匹配算法、多样性控制和五级回退策略实现了精确的概念激活，而三层知识图谱（分类学、临床特征、实例）提供了结构化、最新的上下文。

Result: 在BioASQ罕见病问答集上的实验显示，BLEU得分提高了0.09，ROUGE得分提高了0.05，准确率提高了0.12，峰值准确率达到0.89，接近0.90的临床阈值。专家评估确认了信息质量、推理和专业表达的改进。

Conclusion: 我们的方法缩短了罕见病患者的'诊断旅程'。

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [62] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: 本文提出了一种利用大型多模态模型进行地理定位的新方法，在零样本设置下表现出色，具有较高的准确性，并讨论了其在实际工作流中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化方法未能充分利用地图这一地理定位的重要工具。由于大量生物样本记录未被地理定位，需要一种更高效、准确的方法来解决这一问题。

Method: 本文采用基于网格的方法，将自回归模型适应于零样本设置下的地理定位任务。同时，利用大型多模态模型的多模态能力，使模型能够对局部描述中的空间关系进行视觉上下文理解。

Result: 实验结果表明，该方法在小规模手动注释数据集上的平均距离误差约为1公里，优于单模态地理定位方法和现有工具。

Conclusion: 本文提出了一种新的方法，利用大型多模态模型（LMM）的多模态能力来解决生物样本记录的地理定位问题。实验结果表明，该方法在零样本设置下表现出色，与单模态方法和现有工具相比具有更高的准确性。论文还讨论了LMM理解细粒度地图的能力，并提出了一个实用框架以将该方法集成到地理定位工作流中。

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [63] [NeuralOS: Towards Simulating Operating Systems via Neural Generative Models](https://arxiv.org/abs/2507.08800)
*Luke Rivard,Sun Sun,Hongyu Guo,Wenhu Chen,Yuntian Deng*

Main category: cs.CV

TL;DR: NeuralOS is a neural framework that simulates GUIs of operating systems by predicting screen frames in response to user inputs. It uses an RNN and a diffusion-based neural renderer, trained on a large dataset of Ubuntu XFCE recordings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to simulate graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events.

Method: NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images.

Result: Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches.

Conclusion: NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems.

Abstract: We introduce NeuralOS, a neural framework that simulates graphical user
interfaces (GUIs) of operating systems by directly predicting screen frames in
response to user inputs such as mouse movements, clicks, and keyboard events.
NeuralOS combines a recurrent neural network (RNN), which tracks computer
state, with a diffusion-based neural renderer that generates screen images. The
model is trained on a large-scale dataset of Ubuntu XFCE recordings, which
include both randomly generated interactions and realistic interactions
produced by AI agents. Experiments show that NeuralOS successfully renders
realistic GUI sequences, accurately captures mouse interactions, and reliably
predicts state transitions like application launches. Although modeling
fine-grained keyboard interactions precisely remains challenging, NeuralOS
offers a step toward creating fully adaptive, generative neural interfaces for
future human-computer interaction systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [64] [xpSHACL: Explainable SHACL Validation using Retrieval-Augmented Generation and Large Language Models](https://arxiv.org/abs/2507.08432)
*Gustavo Correa Publio,José Emilio Labra Gayo*

Main category: cs.DB

TL;DR: xpSHACL 是一种可解释的 SHACL 验证系统，利用 RAG 和 LLM 生成多语言解释，并通过 Violation KG 提高效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统 SHACL 验证引擎提供的英文报告过于简略，非技术人员难以理解和采取行动。随着知识图谱的兴起，用户需要更有效的验证方法。

Method: xpSHACL 结合了基于规则的证明树与检索增强生成 (RAG) 和大型语言模型 (LLMs)，并使用 Violation KG 来缓存和重用解释，以提高效率和一致性。

Result: xpSHACL 能够生成详细、多语言、人类可读的解释，提升用户体验和验证效率。

Conclusion: xpSHACL 提供了一种可解释的 SHACL 验证系统，通过结合基于规则的证明树与检索增强生成 (RAG) 和大型语言模型 (LLMs)，为约束违反提供详细、多语言、人类可读的解释。

Abstract: Shapes Constraint Language (SHACL) is a powerful language for validating RDF
data. Given the recent industry attention to Knowledge Graphs (KGs), more users
need to validate linked data properly. However, traditional SHACL validation
engines often provide terse reports in English that are difficult for
non-technical users to interpret and act upon. This paper presents xpSHACL, an
explainable SHACL validation system that addresses this issue by combining
rule-based justification trees with retrieval-augmented generation (RAG) and
large language models (LLMs) to produce detailed, multilanguage, human-readable
explanations for constraint violations. A key feature of xpSHACL is its usage
of a Violation KG to cache and reuse explanations, improving efficiency and
consistency.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [65] [Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models](https://arxiv.org/abs/2507.08128)
*Arushi Goel,Sreyan Ghosh,Jaehyeon Kim,Sonal Kumar,Zhifeng Kong,Sang-gil Lee,Chao-Han Huck Yang,Ramani Duraiswami,Dinesh Manocha,Rafael Valle,Bryan Catanzaro*

Main category: cs.SD

TL;DR: AF3 is a new open-source large audio-language model that excels in audio understanding and reasoning across multiple modalities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a fully open state-of-the-art large audio-language model that advances reasoning and understanding across speech, sound, and music.

Method: AF3 introduces several innovations including AF-Whisper, flexible on-demand thinking, multi-turn multi-audio chat, long audio understanding, and voice-to-voice interaction. It uses novel training datasets and a five-stage curriculum-based training strategy.

Result: AF3 achieves new SOTA results on over 20+ (long) audio understanding and reasoning benchmarks, surpassing both open-weight and closed-source models trained on much larger datasets.

Conclusion: AF3 achieves new SOTA results on over 20+ (long) audio understanding and reasoning benchmarks, surpassing both open-weight and closed-source models trained on much larger datasets.

Abstract: We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large
audio-language model that advances reasoning and understanding across speech,
sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder
trained using a novel strategy for joint representation learning across all 3
modalities of speech, sound, and music; (ii) flexible, on-demand thinking,
allowing the model to do chain-of-thought-type reasoning before answering;
(iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning
(including speech) up to 10 minutes; and (v) voice-to-voice interaction. To
enable these capabilities, we propose several large-scale training datasets
curated using novel strategies, including AudioSkills-XL, LongAudio-XL,
AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based
training strategy. Trained on only open-source audio data, AF3 achieves new
SOTA results on over 20+ (long) audio understanding and reasoning benchmarks,
surpassing both open-weight and closed-source models trained on much larger
datasets.

</details>


### [66] [On Barriers to Archival Audio Processing](https://arxiv.org/abs/2507.08768)
*Peter Sullivan,Muhammad Abdul-Mageed*

Main category: cs.SD

TL;DR: 本研究利用联合国教科文组织的20世纪中期无线电录音集，评估了现代语言识别和说话人识别方法在处理多语言和跨年龄录音时的表现。结果显示，语言识别系统在处理第二语言和带有口音的语音方面表现良好，但说话人嵌入仍存在偏差问题，这可能会影响档案馆使用说话人识别方法进行说话人索引。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估现代语言识别和说话人识别方法在处理多语言和跨年龄录音时的表现，以及这些方法在档案馆应用时可能遇到的问题。

Method: 本研究利用联合国教科文组织独特的20世纪中期无线电录音集，探讨了现代现成的语言识别（LID）和说话人识别（SR）方法的鲁棒性，特别是针对多语言说话人和跨年龄录音的影响。

Result: 研究发现，语言识别系统（如Whisper）在处理第二语言和带有口音的语音方面表现出色，但说话人嵌入仍然存在与通道、年龄和语言相关的偏差问题。

Conclusion: 研究发现，语言识别系统（如Whisper）在处理第二语言和带有口音的语音方面越来越熟练，但说话人嵌入仍然是语音处理流程中容易受到通道、年龄和语言相关偏差影响的脆弱部分。如果档案馆希望使用说话人识别方法进行说话人索引，这些问题需要得到解决。

Abstract: In this study, we leverage a unique UNESCO collection of mid-20th century
radio recordings to probe the robustness of modern off-the-shelf language
identification (LID) and speaker recognition (SR) methods, especially with
respect to the impact of multilingual speakers and cross-age recordings. Our
findings suggest that LID systems, such as Whisper, are increasingly adept at
handling second-language and accented speech. However, speaker embeddings
remain a fragile component of speech processing pipelines that is prone to
biases related to the channel, age, and language. Issues which will need to be
overcome should archives aim to employ SR methods for speaker indexing.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [67] [Overview of the TREC 2021 deep learning track](https://arxiv.org/abs/2507.08191)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin*

Main category: cs.IR

TL;DR: TREC深度学习跟踪第三年的研究显示，深度学习模型在信息检索任务中表现优异，但数据集规模扩大也引发了对标注质量和完整性的一些疑问。


<details>
  <summary>Details</summary>
Motivation: 为了提高检索系统的性能，研究者们探索了深度学习方法在信息检索中的应用，并关注数据集规模扩大带来的影响。

Method: 利用MS MARCO数据集进行文档和段落排序任务的实验，同时更新了文档和段落集合。

Result: 深度神经排名模型继续优于传统检索方法，单阶段检索在两个任务中都能取得良好性能，但尚未达到多阶段检索管道的水平。

Conclusion: 本文讨论了TREC深度学习跟踪的第三年，强调了大规模预训练深度神经排名模型的优势，并提出了关于NIST判断完整性和训练标签质量的问题。

Abstract: This is the third year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of human
annotated training labels available for both passage and document ranking
tasks. In addition, this year we refreshed both the document and the passage
collections which also led to a nearly four times increase in the document
collection size and nearly $16$ times increase in the size of the passage
collection. Deep neural ranking models that employ large scale pretraininig
continued to outperform traditional retrieval methods this year. We also found
that single stage retrieval can achieve good performance on both tasks although
they still do not perform at par with multistage retrieval pipelines. Finally,
the increase in the collection size and the general data refresh raised some
questions about completeness of NIST judgments and the quality of the training
labels that were mapped to the new collections from the old ones which we
discuss in this report.

</details>

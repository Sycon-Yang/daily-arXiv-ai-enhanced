<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating LLMs' Reasoning Over Ordered Procedural Steps](https://arxiv.org/abs/2511.04688)
*Adrita Anika,Md Messal Monem Miah*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型在从混乱步骤中重建有序序列任务中的表现，发现模型在处理更长和更无序的输入时表现较差。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析大型语言模型在程序序列推理方面的表现，特别是在处理混乱步骤重新构建全局有序序列的任务中。

Method: 研究使用了一个精心整理的食谱数据集，评估了多个大型语言模型在零样本和少样本设置下的表现，并适应了排名和序列对齐中的现有指标来评估顺序质量。

Result: 模型性能随着序列长度的增加而下降，输入中步骤位移越大（即混乱越严重），性能下降越明显。

Conclusion: 当前大型语言模型在程序推理方面存在局限性，特别是在处理更长和更无序的输入时。

Abstract: Reasoning over procedural sequences, where the order of steps directly
impacts outcomes, is a critical capability for large language models (LLMs). In
this work, we study the task of reconstructing globally ordered sequences from
shuffled procedural steps, using a curated dataset of food recipes, a domain
where correct sequencing is essential for task success. We evaluate several
LLMs under zero-shot and few-shot settings and present a comprehensive
evaluation framework that adapts established metrics from ranking and sequence
alignment. These include Kendall's Tau, Normalized Longest Common Subsequence
(NLCS), and Normalized Edit Distance (NED), which capture complementary aspects
of ordering quality. Our analysis shows that model performance declines with
increasing sequence length, reflecting the added complexity of longer
procedures. We also find that greater step displacement in the input,
corresponding to more severe shuffling, leads to further degradation. These
findings highlight the limitations of current LLMs in procedural reasoning,
especially with longer and more disordered inputs.

</details>


### [2] [Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks](https://arxiv.org/abs/2511.04689)
*Peiyu Li,Xiuxiu Tang,Si Chen,Ying Cheng,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: ATLAS是一个基于项目反应理论的自适应测试框架，能够在减少项目数量的同时保持评估精度，并提供更准确的模型排名。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法在固定的项目集上计算平均准确率，但忽略了项目质量的差异和信息量的不同，导致评估成本高且效率低。

Method: ATLAS框架使用项目反应理论（IRT）通过Fisher信息引导的项目选择来估计模型能力。

Result: ATLAS在HellaSwag数据集上仅使用42个项目就能达到全基准的估计精度，且项目暴露率低于10%，测试重叠率为16-27%。此外，IRT排名与准确率排名存在差异，部分模型的排名变化超过10位。

Conclusion: ATLAS框架能够显著减少评估所需的项目数量，同时保持测量精度，并且在模型排名上提供了与传统方法不同的结果。

Abstract: Large language model evaluation requires thousands of benchmark items, making
evaluations expensive and slow. Existing methods compute average accuracy
across fixed item sets, treating all items equally despite varying quality and
informativeness. We present ATLAS an adaptive testing framework using Item
Response Theory (IRT) to estimate model ability through Fisher
information-guided item selection. Our analysis of five major benchmarks
reveals that 3-6% of items exhibit negative discrimination, indicating
annotation errors that corrupt static evaluation. ATLAS achieves 90% item
reduction while maintaining measurement precision: on HellaSwag (5,608 items),
we match full-benchmark estimates using only 42 items with 0.154 MAE. Our
framework maintains item exposure rates below 10% and test overlap at 16-27%,
compared to static benchmarks where every model sees all items (100% exposure).
Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with
the same accuracy get different IRT scores, and 23-31% of all models shift by
more than 10 rank positions. Code and calibrated item banks are available at
https://github.com/Peiyu-Georgia-Li/ATLAS.git.

</details>


### [3] [SARC: Sentiment-Augmented Deep Role Clustering for Fake News Detection](https://arxiv.org/abs/2511.04692)
*Jingqing Wang,Jiaxing Shang,Rong Xu,Fei Hao,Tianjin Huang,Geyong Min*

Main category: cs.CL

TL;DR: SARC is a framework that uses sentiment-enhanced deep clustering to identify user roles for better fake news detection, showing superior performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing approaches typically treat sentiment features as auxiliary signals, overlooking role differentiation, which limits their ability to capture nuanced patterns for effective detection.

Method: SARC is a Sentiment-Augmented Role Clustering framework that utilizes sentiment-enhanced deep clustering to identify user roles for improved fake news detection. It generates user features through joint comment text representation and sentiment encoding, constructs a differentiable deep clustering module to automatically categorize user roles, and proposes a joint optimization objective integrating role clustering and fake news detection.

Result: Experimental results on two benchmark datasets, RumourEval-19 and Weibo-comp, demonstrate that SARC achieves superior performance across all metrics compared to baseline models.

Conclusion: SARC achieves superior performance across all metrics compared to baseline models on two benchmark datasets, RumourEval-19 and Weibo-comp.

Abstract: Fake news detection has been a long-standing research focus in social
networks. Recent studies suggest that incorporating sentiment information from
both news content and user comments can enhance detection performance. However,
existing approaches typically treat sentiment features as auxiliary signals,
overlooking role differentiation, that is, the same sentiment polarity may
originate from users with distinct roles, thereby limiting their ability to
capture nuanced patterns for effective detection. To address this issue, we
propose SARC, a Sentiment-Augmented Role Clustering framework which utilizes
sentiment-enhanced deep clustering to identify user roles for improved fake
news detection. The framework first generates user features through joint
comment text representation (with BiGRU and Attention mechanism) and sentiment
encoding. It then constructs a differentiable deep clustering module to
automatically categorize user roles. Finally, unlike existing approaches which
take fake news label as the unique supervision signal, we propose a joint
optimization objective integrating role clustering and fake news detection to
further improve the model performance. Experimental results on two benchmark
datasets, RumourEval-19 and Weibo-comp, demonstrate that SARC achieves superior
performance across all metrics compared to baseline models. The code is
available at: https://github.com/jxshang/SARC.

</details>


### [4] [Reasoning Up the Instruction Ladder for Controllable Language Models](https://arxiv.org/abs/2511.04694)
*Zishuo Zheng,Vidhisha Balachandran,Chan Young Park,Faeze Brahman,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文提出一种方法，通过让大型语言模型在生成响应前思考用户提示与更高优先级指令之间的关系，从而有效处理指令层次结构，提升模型的可靠性和可控性。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型的系统在现实世界决策中扮演越来越重要的角色，需要在单个提示上下文中协调来自多个来源的相互竞争的指令，因此需要在LLM中实施指令层次结构以确保其可靠性与可控性。

Method: 本文将指令层次结构的解决重新构想为一个推理任务，并通过构建VerIH数据集来进行训练，以增强模型对指令优先级的理解和处理能力。

Result: 通过使用VerIH数据集进行微调，模型在指令遵循和指令层次结构基准测试中实现了持续改进，并且这种推理能力还可以推广到训练分布之外的安全关键场景中。

Conclusion: 本文展示了通过在大型语言模型中进行推理来处理指令层次结构的有效性，这为实现可靠和可控的LLM提供了一条实用路径。

Abstract: As large language model (LLM) based systems take on high-stakes roles in
real-world decision-making, they must reconcile competing instructions from
multiple sources (e.g., model developers, users, and tools) within a single
prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where
higher-level directives override lower-priority requests, is critical for the
reliability and controllability of LLMs. In this work, we reframe instruction
hierarchy resolution as a reasoning task. Specifically, the model must first
"think" about the relationship between a given user prompt and higher-priority
(system) instructions before generating a response. To enable this capability
via training, we construct VerIH, an instruction hierarchy dataset of
constraint-following tasks with verifiable answers. This dataset comprises both
aligned and conflicting system-user instructions. We show that lightweight
reinforcement learning with VerIH effectively transfers general reasoning
capabilities of models to instruction prioritization. Our finetuned models
achieve consistent improvements on instruction following and instruction
hierarchy benchmarks. This reasoning ability also generalizes to
safety-critical settings beyond the training distribution. By treating safety
issues as resolving conflicts between adversarial user inputs and predefined
higher-priority policies, our trained model enhances robustness against
jailbreak and prompt injection attacks. These results demonstrate that
reasoning over instruction hierarchies provides a practical path to reliable
LLMs, where updates to system prompts yield controllable and robust changes in
model behavior.

</details>


### [5] [EncouRAGe: Evaluating RAG Local, Fast, and Reliable](https://arxiv.org/abs/2511.04696)
*Jan Strich,Adeline Scharfenberg,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: EncouRAGe is a Python framework for developing and evaluating RAG systems, showing that RAG underperforms compared to Oracle Context, with Hybrid BM25 performing best, and reranking offering minimal improvements.


<details>
  <summary>Details</summary>
Motivation: To streamline the development and evaluation of RAG systems, enabling efficient assessment of datasets within RAG workflows while emphasizing scientific reproducibility, diverse evaluation metrics, and local deployment.

Method: EncouRAGe is a Python framework that streamlines the development and evaluation of RAG systems using LLMs and embedding models. It consists of five modular components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics.

Result: The framework was evaluated on multiple benchmark datasets, including 25k QA pairs and over 51k documents. Results showed that RAG underperforms compared to Oracle Context, with Hybrid BM25 achieving the best results. Reranking provided only marginal improvements with increased latency.

Conclusion: RAG still underperforms compared to the Oracle Context, while Hybrid BM25 consistently achieves the best results across all four datasets. Reranking provides only marginal performance improvements with higher response latency.

Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to
streamline the development and evaluation of Retrieval-Augmented Generation
(RAG) systems using Large Language Models (LLMs) and Embedding Models.
EncouRAGe comprises five modular and extensible components: Type Manifest, RAG
Factory, Inference, Vector Store, and Metrics, facilitating flexible
experimentation and extensible development. The framework emphasizes scientific
reproducibility, diverse evaluation metrics, and local deployment, enabling
researchers to efficiently assess datasets within RAG workflows. This paper
presents implementation details and an extensive evaluation across multiple
benchmark datasets, including 25k QA pairs and over 51k documents. Our results
show that RAG still underperforms compared to the Oracle Context, while Hybrid
BM25 consistently achieves the best results across all four datasets. We
further examine the effects of reranking, observing only marginal performance
improvements accompanied by higher response latency.

</details>


### [6] [multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder](https://arxiv.org/abs/2511.04698)
*K M Sajjadul Islam,John Fields,Praveen Madiraju*

Main category: cs.CL

TL;DR: 该研究提出了multiMentalRoBERTa，一个经过微调的RoBERTa模型，用于对常见的心理健康状况进行多类分类，并展示了其优越的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 早期检测社交媒体文本中的心理健康障碍对于提供及时支持、风险评估和转介至适当资源至关重要。

Method: 该研究引入了multiMentalRoBERTa，这是一个经过微调的RoBERTa模型，用于对常见的心理健康状况进行多类分类。此外，还应用了解释方法，包括Layer Integrated Gradients和KeyBERT，以识别推动分类的词汇线索。

Result: 比较实验表明，multiMentalRoBERTa在六类设置中取得了0.839的宏F1分数，在五类设置（排除压力）中取得了0.870的宏F1分数，优于微调的MentalBERT和基线分类器。

Conclusion: multiMentalRoBERTa被提出作为一种轻量级、稳健且可部署的解决方案，以增强心理健康平台的支持。

Abstract: The early detection of mental health disorders from social media text is
critical for enabling timely support, risk assessment, and referral to
appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned
RoBERTa model designed for multiclass classification of common mental health
conditions, including stress, anxiety, depression, post-traumatic stress
disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple
curated datasets, data exploration is conducted to analyze class overlaps,
revealing strong correlations between depression and suicidal ideation as well
as anxiety and PTSD, while stress emerges as a broad, overlapping category.
Comparative experiments with traditional machine learning methods,
domain-specific transformers, and prompting-based large language models
demonstrate that multiMentalRoBERTa achieves superior performance, with macro
F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup
(excluding stress), outperforming both fine-tuned MentalBERT and baseline
classifiers. Beyond predictive accuracy, explainability methods, including
Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues
that drive classification, with a particular focus on distinguishing depression
from suicidal ideation. The findings emphasize the effectiveness of fine-tuned
transformers for reliable and interpretable detection in sensitive contexts,
while also underscoring the importance of fairness, bias mitigation, and
human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as
a lightweight, robust, and deployable solution for enhancing support in mental
health platforms.

</details>


### [7] [Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding](https://arxiv.org/abs/2511.04699)
*Haneen Al-Homoud,Asma Ibrahim,Murtadha Al-Jubran,Fahad Al-Otaibi,Yazeed Al-Harbi,Daulet Toibazar,Kesen Wang,Pedro J. Moreno*

Main category: cs.CL

TL;DR: Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address the scarcity of Arabic resources for OCR and DU. It includes 2.5 million samples with text, tables, and charts. Finetuning Qwen-2.5-VL on SynthDocs improves OCR performance and other modalities.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of Arabic resources for Optical Character Recognition (OCR) and Document Understanding (DU).

Method: The pipeline leverages authentic scanned backgrounds, bilingual layouts, and diacritic aware fonts to capture the typographic and structural complexity of Arabic documents.

Result: Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart Extraction Score (CharTeX) improved as well in other modalities.

Conclusion: SynthDocs provides a scalable, visually realistic resource for advancing research in multilingual document analysis.

Abstract: Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address
the scarcity of Arabic resources for Optical Character Recognition (OCR) and
Document Understanding (DU). The dataset comprises over 2.5 million of samples,
including 1.5 million textual data, 270K fully annotated tables, and hundred
thousands of real data based charts. Our pipeline leverages authentic scanned
backgrounds, bilingual layouts, and diacritic aware fonts to capture the
typographic and structural complexity of Arabic documents. In addition to text,
the corpus includes variety of rendered styles for charts and tables.
Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word
Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple
public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart
Extraction Score (CharTeX) improved as well in other modalities. SynthDocs
provides a scalable, visually realistic resource for advancing research in
multilingual document analysis.

</details>


### [8] [Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation](https://arxiv.org/abs/2511.04700)
*Song Wang,Zihan Chen,Peng Wang,Zhepei Wei,Zhen Tan,Yu Meng,Cong Shen,Jundong Li*

Main category: cs.CL

TL;DR: WinnowRAG is a new RAG framework that filters out noisy documents using two stages: clustering and winnowing, with effective results on real datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of handling a larger number of documents in RAG, which can introduce noise and reduce accuracy. The goal is to systematically filter out noisy documents while preserving valuable content.

Method: WinnowRAG operates in two stages: query-aware clustering to group similar documents and form topic clusters, followed by winnowing where a critic LLM evaluates outputs and separates useful documents from noise. Two merging techniques are proposed to retain relevant knowledge.

Result: Extensive experiments on various realistic datasets demonstrate the effectiveness of WinnowRAG over state-of-the-art baselines.

Conclusion: WinnowRAG is a novel RAG framework that effectively filters out noisy documents while preserving valuable content, demonstrating effectiveness over state-of-the-art baselines.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge sources to address their limitations in
accessing up-to-date or specialized information. A natural strategy to increase
the likelihood of retrieving relevant information is to expand the number of
retrieved documents. However, involving more documents could introduce
significant noise, as many documents may be irrelevant or misleading, thereby
reducing the overall accuracy of the generated responses. To overcome the
challenge associated with handling a larger number of documents, we propose
WinnowRAG, a novel RAG framework designed to systematically filter out noisy
documents while preserving valuable content -- a process we refer to as
winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware
clustering to group similar documents and form distinct topic clusters. Each
cluster is assigned to an LLM agent for generating a unique answer. In Stage
II, we perform winnowing, wherein a critic LLM evaluates the outputs of
multiple agents and iteratively separates useful documents from noisy ones. To
retain useful documents when discarding agents, we propose two strategic
merging techniques to ensure that only relevant knowledge is used for
generating the final response. Crucially, WinnowRAG is model-agnostic and does
not require any model fine-tuning, making it easily adaptable to various tasks.
Extensive experiments on various realistic datasets demonstrate the
effectiveness of WinnowRAG over state-of-the-art baselines.

</details>


### [9] [Measuring what Matters: Construct Validity in Large Language Model Benchmarks](https://arxiv.org/abs/2511.04703)
*Andrew M. Bean,Ryan Othniel Kearns,Angelika Romanou,Franziska Sofia Hafner,Harry Mayne,Jan Batzner,Negar Foroutan,Chris Schmitz,Karolina Korgul,Hunar Batra,Oishi Deb,Emma Beharry,Cornelius Emde,Thomas Foster,Anna Gausen,María Grandury,Simeng Han,Valentin Hofmann,Lujain Ibrahim,Hazel Kim,Hannah Rose Kirk,Fangru Lin,Gabrielle Kaili-May Liu,Lennart Luettgau,Jabez Magomere,Jonathan Rystrøm,Anna Sotnikova,Yushi Yang,Yilun Zhao,Adel Bibi,Antoine Bosselut,Ronald Clark,Arman Cohan,Jakob Foerster,Yarin Gal,Scott A. Hale,Inioluwa Deborah Raji,Christopher Summerfield,Philip H. S. Torr,Cozmin Ududec,Luc Rocher,Adam Mahdi*

Main category: cs.CL

TL;DR: 本文通过系统回顾445个LLM基准，发现当前评估方法存在有效性问题，并提出八个关键建议以改进LLM基准的开发。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）对于评估其能力并识别部署前的安全或鲁棒性问题至关重要。可靠地衡量像'安全'和'鲁棒性'这样的抽象和复杂现象需要强构念效度。

Method: 本文通过系统回顾445个LLM基准，由29位专家评审进行分析。

Result: 在所审查的文章中，我们发现了与测量现象、任务和评分指标相关的模式，这些模式损害了得出的主张的有效性。

Conclusion: 本文提出了八个关键建议，以帮助研究人员和从业者在开发LLM基准时提高测量的有效性。

Abstract: Evaluating large language models (LLMs) is crucial for both assessing their
capabilities and identifying safety or robustness issues prior to deployment.
Reliably measuring abstract and complex phenomena such as 'safety' and
'robustness' requires strong construct validity, that is, having measures that
represent what matters to the phenomenon. With a team of 29 expert reviewers,
we conduct a systematic review of 445 LLM benchmarks from leading conferences
in natural language processing and machine learning. Across the reviewed
articles, we find patterns related to the measured phenomena, tasks, and
scoring metrics which undermine the validity of the resulting claims. To
address these shortcomings, we provide eight key recommendations and detailed
actionable guidance to researchers and practitioners in developing LLM
benchmarks.

</details>


### [10] [POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios](https://arxiv.org/abs/2511.04705)
*Tingyue Yang,Junchi Yao,Yuhui Guo,Chang Liu*

Main category: cs.CL

TL;DR: POLIS-Bench是首个针对政府双语政策场景中LLMs的评估套件，包含更新的双语语料库、基于场景的任务设计和双指标评估框架。评估结果表明推理模型表现优越，微调的轻量级模型在成本效益和性能上优于专有基线。


<details>
  <summary>Details</summary>
Motivation: 为了评估LLMs在政府双语政策场景中的表现，需要一个系统、严格的评估套件。现有的基准存在不足，因此需要引入POLIS-Bench来解决这些问题。

Method: POLIS-Bench引入了三个主要改进：(i) 更新的双语语料库，(ii) 基于场景的任务设计，(iii) 双指标评估框架。通过大规模评估超过10个最先进的LLM，揭示了性能层次结构，并成功微调了一个轻量级开源模型。

Result: POLIS-Bench的评估显示，推理模型在跨任务稳定性和准确性方面表现优越，合规任务具有挑战性。通过该基准微调的轻量级开源模型在多个政策子任务上达到了与强专有基线相当或更好的性能，同时成本显著降低。

Conclusion: POLIS-Bench的评估结果显示，推理模型在跨任务稳定性和准确性方面表现优越，表明合规任务的难度。此外，利用该基准，我们成功微调了一个轻量级开源模型，所得到的POLIS系列模型在多个政策子任务上实现了与强大专有基线相当或更好的性能，且成本显著降低，为政府部署提供了经济高效且合规的路径。

Abstract: We introduce POLIS-Bench, the first rigorous, systematic evaluation suite
designed for LLMs operating in governmental bilingual policy scenarios.
Compared to existing benchmarks, POLIS-Bench introduces three major
advancements. (i) Up-to-date Bilingual Corpus: We construct an extensive,
up-to-date policy corpus that significantly scales the effective assessment
sample size, ensuring relevance to current governance practice. (ii)
Scenario-Grounded Task Design: We distill three specialized, scenario-grounded
tasks -- Clause Retrieval & Interpretation, Solution Generation, and the
Compliance Judgmen--to comprehensively probe model understanding and
application. (iii) Dual-Metric Evaluation Framework: We establish a novel
dual-metric evaluation framework combining semantic similarity with accuracy
rate to precisely measure both content alignment and task requirement
adherence. A large-scale evaluation of over 10 state-of-the-art LLMs on
POLIS-Bench reveals a clear performance hierarchy where reasoning models
maintain superior cross-task stability and accuracy, highlighting the
difficulty of compliance tasks. Furthermore, leveraging our benchmark, we
successfully fine-tune a lightweight open-source model. The resulting POLIS
series models achieves parity with, or surpasses, strong proprietary baselines
on multiple policy subtasks at a significantly reduced cost, providing a
cost-effective and compliant path for robust real-world governmental
deployment.

</details>


### [11] [GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models](https://arxiv.org/abs/2511.04710)
*Hari Mohan Pandey,Anshul Gupta,Subham Sarkar,Minakshi Tomer,Schneider Johannes,Yan Gong*

Main category: cs.CL

TL;DR: GEMMA-SQL is a lightweight and efficient text-to-SQL model built on the Gemma 2B architecture. It uses resource-efficient, iterative fine-tuning and combines multiple prompting strategies to achieve high accuracy in SQL query generation. The model outperforms existing baselines and offers a practical, open-source alternative for text-to-SQL systems.


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL systems enable users to interact with structured databases using natural language, eliminating the need for specialized programming knowledge. However, many large language models (LLMs) are not resource-efficient and cannot be deployed on low-cost hardware. This work aims to introduce a more efficient and scalable text-to-SQL model.

Method: GEMMA-SQL is a lightweight and efficient text-to-SQL model built upon the open-source Gemma 2B architecture. It is fine-tuned in a resource-efficient, iterative manner. The model leverages the SPIDER benchmark for training and evaluation, combining multiple prompting strategies, including few-shot learning.

Result: The instruction-tuned variant, GEMMA-SQL Instruct, achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy, outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and CodeXDavinci.

Conclusion: GEMMA-SQL demonstrates that effective prompt design and targeted instruction tuning can significantly boost performance while maintaining high scalability and adaptability. It positions as a practical, open-source alternative for robust and accessible text-to-SQL systems.

Abstract: Text-to-SQL systems enable users to interact with structured databases using
natural language, eliminating the need for specialized programming knowledge.
In this work, we introduce GEMMA-SQL, a lightweight and efficient text-to-SQL
model built upon the open-source Gemma 2B architecture. Unlike many large
language models (LLMs), GEMMA-SQL is fine-tuned in a resource-efficient,
iterative manner and can be deployed on low-cost hardware. Leveraging the
SPIDER benchmark for training and evaluation, GEMMA-SQL combines multiple
prompting strategies, including few-shot learning, to enhance SQL query
generation accuracy. The instruction-tuned variant, GEMMA-SQL Instruct,
achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy,
outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and
CodeXDavinci. The proposed approach demonstrates that effective prompt design
and targeted instruction tuning can significantly boost performance while
maintaining high scalability and adaptability. These results position GEMMA-SQL
as a practical, open-source alternative for robust and accessible text-to-SQL
systems.

</details>


### [12] [First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation](https://arxiv.org/abs/2511.04715)
*Dmytro Vitel,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本文研究了训练样本对大型语言模型决策的影响，并提出了新的方法来更准确地估计这种影响。


<details>
  <summary>Details</summary>
Motivation: 识别训练样本如何影响大型语言模型（LLM）的决策对于有效地解释模型决策和审计大规模数据集至关重要。当前的训练样本影响估计方法（也称为影响函数）通过利用模型中的信息流来实现这一目标，但鉴于现代模型规模庞大，这些影响计算通常被限制在模型层的子集以确保计算可行性。

Method: 本文提出了理论和实证证据，证明了取消效应不可靠，并且中间注意力层是更好的影响估计器。此外，本文还解决了跨层影响分数聚合的更广泛挑战，并展示了标准平均值以外的替代方法（如排名和投票方法）可以显著提高性能。最后，本文提出了在不进行模型微调的情况下评估影响分数有效性的更好方法，并提出了一种称为噪声检测率（NDR）的新指标。

Result: 本文通过广泛的实验表明，第一层并不一定比最后一层更适合用于LLM的影响估计，这与领域内的先前知识相矛盾。

Conclusion: 本文通过广泛的实验表明，第一层并不一定比最后一层更适合用于LLM的影响估计，这与领域内的先前知识相矛盾。

Abstract: Identifying how training samples influence/impact Large Language Model (LLM)
decision-making is essential for effectively interpreting model decisions and
auditing large-scale datasets. Current training sample influence estimation
methods (also known as influence functions) undertake this goal by utilizing
information flow through the model via its first-order and higher-order
gradient terms. However, owing to the large model sizes of today consisting of
billions of parameters, these influence computations are often restricted to
some subset of model layers to ensure computational feasibility. Prior seminal
work by Yeh et al. (2022) in assessing which layers are best suited for
computing language data influence concluded that the first (embedding) layers
are the most informative for this purpose, using a hypothesis based on
influence scores canceling out (i.e., the cancellation effect). In this work,
we propose theoretical and empirical evidence demonstrating how the
cancellation effect is unreliable, and that middle attention layers are better
estimators for influence. Furthermore, we address the broader challenge of
aggregating influence scores across layers, and showcase how alternatives to
standard averaging (such as ranking and vote-based methods) can lead to
significantly improved performance. Finally, we propose better methods for
evaluating influence score efficacy in LLMs without undertaking model
retraining, and propose a new metric known as the Noise Detection Rate (NDR)
that exhibits strong predictive capability compared to the cancellation effect.
Through extensive experiments across LLMs of varying types and scales, we
concretely determine that the first (layers) are not necessarily better than
the last (layers) for LLM influence estimation, contrasting with prior
knowledge in the field.

</details>


### [13] [Learning to reason about rare diseases through retrieval-augmented agents](https://arxiv.org/abs/2511.04720)
*Ha Young Kim,Jun Li,Ana Beatriz Solana,Carolin M. Pirkl,Benedikt Wiestler,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.CL

TL;DR: RADAR是一种基于检索增强诊断推理的AI系统，用于脑部MRI中的罕见疾病检测，能够在不进行额外训练的情况下提高模型的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于罕见疾病的数据稀缺，AI模型在医学影像中经常失败。放射科医生在面对不熟悉发现时会查阅病例报告和文献，因此需要一种能够检索相关证据来辅助诊断的系统。

Method: RADAR利用AI代理访问外部医学知识，通过嵌入病例报告和文献并使用FAISS进行索引，以实现高效的相似性搜索。

Result: 在包含280种不同罕见疾病的NOVA数据集上，RADAR实现了高达10.2%的性能提升，特别是对于开源模型如DeepSeek效果显著。

Conclusion: RADAR在罕见疾病检测中表现出色，能够提高模型的准确性和可解释性，并为低发病率条件下的医学影像提供强大的推理范式。

Abstract: Rare diseases represent the long tail of medical imaging, where AI models
often fail due to the scarcity of representative training data. In clinical
workflows, radiologists frequently consult case reports and literature when
confronted with unfamiliar findings. Following this line of reasoning, we
introduce RADAR, Retrieval Augmented Diagnostic Reasoning Agents, an agentic
system for rare disease detection in brain MRI. Our approach uses AI agents
with access to external medical knowledge by embedding both case reports and
literature using sentence transformers and indexing them with FAISS to enable
efficient similarity search. The agent retrieves clinically relevant evidence
to guide diagnostic decision making on unseen diseases, without the need of
additional training. Designed as a model-agnostic reasoning module, RADAR can
be seamlessly integrated with diverse large language models, consistently
improving their rare pathology recognition and interpretability. On the NOVA
dataset comprising 280 distinct rare diseases, RADAR achieves up to a 10.2%
performance gain, with the strongest improvements observed for open source
models such as DeepSeek. Beyond accuracy, the retrieved examples provide
interpretable, literature grounded explanations, highlighting
retrieval-augmented reasoning as a powerful paradigm for low-prevalence
conditions in medical imaging.

</details>


### [14] [Surprisal reveals diversity gaps in image captioning and different scorers change the story](https://arxiv.org/abs/2511.04754)
*Nikolai Ilinykh,Simon Dobnik*

Main category: cs.CL

TL;DR: 本文通过 surprisal 方差量化图像描述中的语言多样性，并发现人类描述比模型更具多样性，但这种结论可能因评分器的不同而改变。


<details>
  <summary>Details</summary>
Motivation: 我们希望了解图像描述中语言多样性的度量方式，并评估不同模型与人类描述之间的差异。

Method: 我们通过 surprisal 方差量化图像描述中的语言多样性，即在一组描述中 token 级别的负对数概率的分布。我们比较了五种最先进的视觉-语言 LLMs 在 MSCOCO 测试集上的表现，并使用贪心和 nucleus sampling 进行解码，与人类描述进行比较。

Result: 人类描述的 surprisal 方差大约是模型的两倍，但使用通用语言模型重新评分相同的描述会逆转这一模式。

Conclusion: 我们的分析引入了基于 surprisal 的图像描述多样性度量。我们表明，依赖单一评分器可能会完全颠倒结论，因此，稳健的多样性评估必须报告多个评分器下的 surprisal。

Abstract: We quantify linguistic diversity in image captioning with surprisal variance
- the spread of token-level negative log-probabilities within a caption set. On
the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs,
decoded with greedy and nucleus sampling, to human captions. Measured with a
caption-trained n-gram LM, humans display roughly twice the surprisal variance
of models, but rescoring the same captions with a general-language model
reverses the pattern. Our analysis introduces the surprisal-based diversity
metric for image captioning. We show that relying on a single scorer can
completely invert conclusions, thus, robust diversity evaluation must report
surprisal under several scorers.

</details>


### [15] [Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2511.04800)
*Chenxi Liu,Junjie Liang,Yuqi Jia,Bochuan Cao,Yang Bai,Heng Huang,Xun Chen*

Main category: cs.CL

TL;DR: 本文提出了ERPO框架，以解决剩余提示在强化学习中的问题，并展示了其在数学推理任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着模型训练时间变长和规模扩大，越来越多的训练提示成为无方差奖励的剩余提示，这减少了训练的多样性并阻碍了效果。

Method: ERPO框架通过维护每个提示的历史跟踪器，并自适应地增加产生所有正确响应的剩余提示的采样温度，鼓励模型生成更多多样化的推理轨迹。

Result: 在Qwen2.5系列上的实验证明，ERPO在多个数学推理基准上 consistently 超过强基线。

Conclusion: ERPO框架能够有效利用剩余提示，提高训练效果。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an
effective approach for improving the reasoning abilities of large language
models (LLMs). The Group Relative Policy Optimization (GRPO) family has
demonstrated strong performance in training LLMs with RLVR. However, as models
train longer and scale larger, more training prompts become residual prompts,
those with zero variance rewards that provide no training signal. Consequently,
fewer prompts contribute to training, reducing diversity and hindering
effectiveness. To fully exploit these residual prompts, we propose the Explore
Residual Prompts in Policy Optimization (ERPO) framework, which encourages
exploration on residual prompts and reactivates their training signals. ERPO
maintains a history tracker for each prompt and adaptively increases the
sampling temperature for residual prompts that previously produced all correct
responses. This encourages the model to generate more diverse reasoning traces,
introducing incorrect responses that revive training signals. Empirical results
on the Qwen2.5 series demonstrate that ERPO consistently surpasses strong
baselines across multiple mathematical reasoning benchmarks.

</details>


### [16] [Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs](https://arxiv.org/abs/2511.04869)
*Preetum Nakkiran,Arwen Bradley,Adam Goliński,Eugene Ndiaye,Michael Kirchhof,Sinead Williamson*

Main category: cs.CL

TL;DR: 我们的研究揭示了基础LLMs在开放域问答任务中能够有意义地评估置信度，并提供了关于语义校准如何出现的第一个有原则的解释。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）通常缺乏对其输出的有意义置信度估计。尽管基础LLMs表现出下一个token校准，但它们是否能评估其响应的实际意义的置信度仍不清楚。

Method: 我们通过理论分析和实验验证了基础LLMs在开放域问答任务中的语义校准现象。

Result: 我们发现，当使用基于采样的语义校准概念时，基础LLMs表现得非常校准：它们可以在开放域问答任务中有意义地评估置信度，尽管没有被明确训练这样做。

Conclusion: 我们的工作提供了第一个有原则的解释，说明何时以及为什么语义校准在LLMs中出现。

Abstract: Large Language Models (LLMs) often lack meaningful confidence estimates for
their outputs. While base LLMs are known to exhibit next-token calibration, it
remains unclear whether they can assess confidence in the actual meaning of
their responses beyond the token level. We find that, when using a certain
sampling-based notion of semantic calibration, base LLMs are remarkably
well-calibrated: they can meaningfully assess confidence in open-domain
question-answering tasks, despite not being explicitly trained to do so. Our
main theoretical contribution establishes a mechanism for why semantic
calibration emerges as a byproduct of next-token prediction, leveraging a
recent connection between calibration and local loss optimality. The theory
relies on a general definition of "B-calibration," which is a notion of
calibration parameterized by a choice of equivalence classes (semantic or
otherwise). This theoretical mechanism leads to a testable prediction: base
LLMs will be semantically calibrated when they can easily predict their own
distribution over semantic answer classes before generating a response. We
state three implications of this prediction, which we validate through
experiments: (1) Base LLMs are semantically calibrated across
question-answering tasks, (2) RL instruction-tuning systematically breaks this
calibration, and (3) chain-of-thought reasoning breaks calibration. To our
knowledge, our work provides the first principled explanation of when and why
semantic calibration emerges in LLMs.

</details>


### [17] [Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs](https://arxiv.org/abs/2511.04875)
*Matthew Bozoukov,Matthew Nguyen,Shubkarman Singh,Bart Bussmann,Patrick Leask*

Main category: cs.CL

TL;DR: 研究发现，LLM的行为自我意识可以在特定领域中被轻易诱导和调节，这可能对模型的安全性产生影响。


<details>
  <summary>Details</summary>
Motivation: 最近的研究表明，LLM可以表现出行为自我意识，这可能带来安全问题，例如允许模型在评估期间更好地隐藏其真实能力。因此，需要研究自我意识出现的条件和机制。

Method: 通过在指令调优的LLM上使用低秩适配器（LoRA）进行受控微调实验，研究了自我意识出现的最小条件及其机制过程。

Result: 发现自我意识可以通过一个单一的秩-1 LoRA 适配器可靠地诱导；学习到的自我意识行为可以由激活空间中的一个单独的引导向量来捕获，几乎恢复了微调的所有行为效果；并且自我意识是非普遍的，领域局部化的，不同任务之间有独立的表示。

Conclusion: 行为自我意识作为特定领域的线性特征，可以被轻易诱导和调节。

Abstract: Recent studies have revealed that LLMs can exhibit behavioral self-awareness:
the ability to accurately describe or predict their own learned behaviors
without explicit supervision. This capability raises safety concerns as it may,
for example, allow models to better conceal their true abilities during
evaluation. We attempt to characterize the minimal conditions under which such
self-awareness emerges, and the mechanistic processes through which it
manifests. Through controlled finetuning experiments on instruction-tuned LLMs
with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably
induced using a single rank-1 LoRA adapter; (2) that the learned self-aware
behavior can be largely captured by a single steering vector in activation
space, recovering nearly all of the fine-tune's behavioral effect; and (3) that
self-awareness is non-universal and domain-localized, with independent
representations across tasks. Together, these findings suggest that behavioral
self-awareness emerges as a domain-specific, linear feature that can be easily
induced and modulated.

</details>


### [18] [SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents](https://arxiv.org/abs/2511.04910)
*Jaehoon Lee,Sohyun Kim,Wanggeun Park,Geon Lee,Seungkyung Kim,Minyoung Lee*

Main category: cs.CL

TL;DR: SDS KoPub VDR是首个针对韩语文档的大型公开基准，用于评估文本和多模态检索任务，并揭示了多模态场景中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有视觉文档检索（VDR）基准忽略了非英语语言和官方出版物的结构复杂性，因此需要一个专门针对韩语文档的大型公开基准。

Method: 引入了SDS KoPub VDR基准，构建了一个包含361个真实文档的语料库，并生成了600个查询-页面-答案三元组进行评估。

Result: 在两个互补任务上评估了SDS KoPub VDR，发现即使是最先进的模型在多模态场景中也存在显著性能差距。

Conclusion: SDS KoPub VDR 不仅为文本和多模态检索任务提供了严格的评估，还为推进复杂现实文档智能的多模态AI提供了明确的路线图。

Abstract: Existing benchmarks for visual document retrieval (VDR) largely overlook
non-English languages and the structural complexity of official publications.
To address this critical gap, we introduce SDS KoPub VDR, the first
large-scale, publicly available benchmark for retrieving and understanding
Korean public documents. The benchmark is built upon a corpus of 361 real-world
documents (40,781 pages), including 256 files under the KOGL Type 1 license and
105 from official legal portals, capturing complex visual elements like tables,
charts, and multi-column layouts. To establish a challenging and reliable
evaluation set, we constructed 600 query-page-answer triples. These were
initially generated using multimodal models (e.g., GPT-4o) and subsequently
underwent a rigorous human verification and refinement process to ensure
factual accuracy and contextual relevance. The queries span six major public
domains and are systematically categorized by the reasoning modality required:
text-based, visual-based (e.g., chart interpretation), and cross-modal. We
evaluate SDS KoPub VDR on two complementary tasks that reflect distinct
retrieval paradigms: (1) text-only retrieval, which measures a model's ability
to locate relevant document pages based solely on textual signals, and (2)
multimodal retrieval, which assesses retrieval performance when visual features
(e.g., tables, charts, and layouts) are jointly leveraged alongside text. This
dual-task evaluation reveals substantial performance gaps, particularly in
multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art
models. As a foundational resource, SDS KoPub VDR not only enables rigorous and
fine-grained evaluation across textual and multimodal retrieval tasks but also
provides a clear roadmap for advancing multimodal AI in complex, real-world
document intelligence.

</details>


### [19] [BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models](https://arxiv.org/abs/2511.04919)
*Chandra Vamsi Krishna Alla,Harish Naidu Gaddam,Manohar Kommi*

Main category: cs.CL

TL;DR: 本文提出了一种名为BudgetMem的新架构，用于在资源受限的情况下处理长上下文。该方法通过选择性记忆策略和特征显著性评分来优化存储，从而在保持性能的同时大幅减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理长上下文时面临显著的计算和内存约束，尽管对需要在大量文档、多会话对话和书籍长度文本上进行推理的应用程序的需求不断增长。虽然最近的进展已将上下文窗口扩展到100K-1M个标记，但这些方法在资源受限的部署中成本过高。

Method: 我们提出了BudgetMem，这是一种新的记忆增强架构，它学习记住什么而不是记住所有内容。我们的系统结合了选择性记忆策略和基于特征的显著性评分（实体密度、TF-IDF、话语标记、位置偏差）来决定在严格预算限制下哪些信息值得存储。不同于现有的检索增强生成（RAG）系统，BudgetMem采用学习到的门控机制和BM25稀疏检索来进行高效的信息访问。

Result: 通过在700个问答对上的全面实验，使用Llama-3.2-3B-Instruct，我们在长文档上取得了显著的结果：与基线RAG相比，仅下降1.0%的F1分数，同时节省了72.4%的内存。我们通过预算敏感性分析（测试7种预算比例）、朴素基线比较和文档长度分析验证了我们的方法，结果表明BudgetMem的优势随着文档长度的增加而增加。

Conclusion: 我们的工作为在普通硬件上部署强大的长上下文系统提供了实用的路径，使更多人能够获得先进的语言理解能力。

Abstract: Large Language Models (LLMs) face significant computational and memory
constraints when processing long contexts, despite growing demand for
applications requiring reasoning over extensive documents, multi-session
dialogues, and book length texts. While recent advances have extended context
windows to 100K-1M tokens, such approaches incur prohibitive costs for resource
constrained deployments. We propose BudgetMem, a novel memory augmented
architecture that learns what to remember rather than remembering everything.
Our system combines selective memory policies with feature based salience
scoring (entity density, TF-IDF, discourse markers, position bias) to decide
which information merits storage under strict budget constraints. Unlike
existing retrieval augmented generation (RAG) systems that store all chunks,
BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval
for efficient information access. Through comprehensive experiments on 700
question answer pairs across short (237 tokens) and long (5K-10K tokens)
documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves
remarkable results on long documents: only 1.0% F1 score degradation while
saving 72.4% memory compared to baseline RAG. We validate our approach through
budget sensitivity analysis (testing 7 budget ratios), naive baseline
comparisons, and document length analysis, showing that BudgetMem's benefits
increase with document length. Our work provides a practical pathway for
deploying capable long context systems on modest hardware, democratizing access
to advanced language understanding capabilities.

</details>


### [20] [AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent](https://arxiv.org/abs/2511.04921)
*Yu Li,Lehui Li,Qingmin Liao,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 本文介绍了一种全面的基线和数据集推荐框架，通过利用基线和数据集引用网络中的集体感知，解决了现有方法数据覆盖有限和依赖内容相似性的问题。该框架包括自动化数据收集管道、增强集体感知的检索器以及推理增强的重新排序器，在实验设计自动化方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 先前的努力存在数据覆盖范围有限的问题，因为推荐数据集主要从公共门户中收集候选者，忽略了许多在已发表论文中实际使用的数据集，并且过度依赖内容相似性，这使模型偏向于表面相似性而忽略了实验适用性。

Method: 我们设计了一个自动化数据收集管道，将大约十万个被接受的论文与它们实际使用的基线和数据集联系起来。我们提出了一种增强集体感知的检索器，通过将自描述与聚合的引用上下文连接来表示每个数据集或基线在学术网络中的位置，并对这些表示进行微调以实现高效的候选回忆。最后，我们开发了一个推理增强的重新排序器，通过精确的交互链构建显式的推理链，并微调一个大型语言模型以产生可解释的论证和改进的排名。

Result: 我们整理的数据集涵盖了过去五年内顶级人工智能会议中使用的85%的数据集和基线。在我们的数据集上，所提出的方法在Recall@20上平均提高了+5.85%，在HitRate@5上提高了+8.30%。

Conclusion: 我们的结果推进了实验设计的可靠、可解释的自动化。

Abstract: Large language model agents are becoming increasingly capable at web-centric
tasks such as information retrieval, complex reasoning. These emerging
capabilities have given rise to surge research interests in developing LLM
agent for facilitating scientific quest. One key application in AI research is
to automate experiment design through agentic dataset and baseline retrieval.
However, prior efforts suffer from limited data coverage, as recommendation
datasets primarily harvest candidates from public portals and omit many
datasets actually used in published papers, and from an overreliance on content
similarity that biases model toward superficial similarity and overlooks
experimental suitability. Harnessing collective perception embedded in the
baseline and dataset citation network, we present a comprehensive framework for
baseline and dataset recommendation. First, we design an automated
data-collection pipeline that links roughly one hundred thousand accepted
papers to the baselines and datasets they actually used. Second, we propose a
collective perception enhanced retriever. To represent the position of each
dataset or baseline within the scholarly network, it concatenates
self-descriptions with aggregated citation contexts. To achieve efficient
candidate recall, we finetune an embedding model on these representations.
Finally, we develop a reasoning-augmented reranker that exact interaction
chains to construct explicit reasoning chains and finetunes a large language
model to produce interpretable justifications and refined rankings. The dataset
we curated covers 85\% of the datasets and baselines used at top AI conferences
over the past five years. On our dataset, the proposed method outperforms the
strongest prior baseline with average gains of +5.85\% in Recall@20, +8.30\% in
HitRate@5. Taken together, our results advance reliable, interpretable
automation of experimental design.

</details>


### [21] [Diagnosing and Mitigating Semantic Inconsistencies in Wikidata's Classification Hierarchy](https://arxiv.org/abs/2511.04926)
*Shixiong Zhao,Hideaki Takeda*

Main category: cs.CL

TL;DR: 本文提出了一种新的验证方法来检测Wikidata中的分类错误、过度泛化的子类链接和冗余连接，并开发了一个系统，使用户能够检查任意Wikidata实体的分类关系，以充分利用平台的众包特性。


<details>
  <summary>Details</summary>
Motivation: Wikidata作为最大的开放知识图谱，虽然提供了方便的知识访问，但其相对宽松的编辑政策导致了分类不一致的问题，因此需要一种有效的验证方法来检测和修正这些问题。

Method: 本文基于先前的工作，提出并应用了一种新的验证方法来检测Wikidata中的分类错误、过度泛化的子类链接和冗余连接，并引入了一种新的评估标准来判断这些问题是否需要修正。

Result: 本文提出的新验证方法能够有效检测Wikidata中的分类错误、过度泛化的子类链接和冗余连接，并开发了一个系统，使用户能够检查任意Wikidata实体的分类关系。

Conclusion: 本文提出了一种新的验证方法来检测Wikidata中的分类错误、过度泛化的子类链接和冗余连接，并开发了一个系统，使用户能够检查任意Wikidata实体的分类关系，充分利用平台的众包特性。

Abstract: Wikidata is currently the largest open knowledge graph on the web,
encompassing over 120 million entities. It integrates data from various
domain-specific databases and imports a substantial amount of content from
Wikipedia, while also allowing users to freely edit its content. This openness
has positioned Wikidata as a central resource in knowledge graph research and
has enabled convenient knowledge access for users worldwide. However, its
relatively loose editorial policy has also led to a degree of taxonomic
inconsistency. Building on prior work, this study proposes and applies a novel
validation method to confirm the presence of classification errors,
over-generalized subclass links, and redundant connections in specific domains
of Wikidata. We further introduce a new evaluation criterion for determining
whether such issues warrant correction and develop a system that allows users
to inspect the taxonomic relationships of arbitrary Wikidata
entities-leveraging the platform's crowdsourced nature to its full potential.

</details>


### [22] [LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model](https://arxiv.org/abs/2511.04952)
*Wei Shao,Lingchao Zheng,Pengyu Wang,Peizhen Zheng,Jun Li,Yuwei Fan*

Main category: cs.CL

TL;DR: 本文提出了一种名为LoPT的无损并行分词框架，通过基于字符位置的匹配和动态块长度调整来准确对齐和合并分词段。实验表明，LoPT在保证无损分词的同时实现了显著的加速。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理场景对于大型语言模型变得越来越重要，但引入了显著的计算延迟。现有的并行分词方法在合并后由于边界伪影导致结果不一致，因此需要一种新的方法来解决这个问题。

Method: LoPT是一种无损并行分词框架，通过基于字符位置的匹配和动态块长度调整来准确对齐和合并分词段。

Result: LoPT在各种长文本数据集上的实验表明，它在保证无损分词的同时实现了显著的加速。

Conclusion: LoPT在各种长文本数据集上的实验表明，它在保证无损分词的同时实现了显著的加速。我们还提供了的一致性理论证明和全面的分析研究来验证我们方法的鲁棒性。

Abstract: Long context inference scenarios have become increasingly important for large
language models, yet they introduce significant computational latency. While
prior research has optimized long-sequence inference through operators, model
architectures, and system frameworks, tokenization remains an overlooked
bottleneck. Existing parallel tokenization methods accelerate processing
through text segmentation and multi-process tokenization, but they suffer from
inconsistent results due to boundary artifacts that occur after merging. To
address this, we propose LoPT, a novel Lossless Parallel Tokenization framework
that ensures output identical to standard sequential tokenization. Our approach
employs character-position-based matching and dynamic chunk length adjustment
to align and merge tokenized segments accurately. Extensive experiments across
diverse long-text datasets demonstrate that LoPT achieves significant speedup
while guaranteeing lossless tokenization. We also provide theoretical proof of
consistency and comprehensive analytical studies to validate the robustness of
our method.

</details>


### [23] [Too Good to be Bad: On the Failure of LLMs to Role-Play Villains](https://arxiv.org/abs/2511.04962)
*Zihao Yi,Qingxuan Jiang,Ruotian Ma,Xingyu Chen,Qu Yang,Mengru Wang,Fanghua Ye,Ying Shen,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在扮演反派角色时的局限性，发现模型在表现与安全原则相冲突的特质时表现不佳，并提出了一个新的基准来评估这一问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越多地被赋予创造性生成的任务，包括模拟虚构角色。然而，它们在表现非亲社会、反派人物方面的能力仍未得到充分研究。我们假设现代LLMs的安全对齐与真实扮演道德模棱两可或反派角色的任务之间存在根本冲突。

Method: 我们引入了Moral RolePlay基准，这是一个新的数据集，包含一个四等级道德对齐尺度和一个平衡的测试集以进行严格评估。我们让最先进的LLMs扮演从道德典范到纯反派的角色。

Result: 我们的大规模评估显示，随着角色道德的降低，角色扮演的忠实度持续且单调地下降。我们发现模型在直接与安全原则相反的特质上表现最差，例如“欺骗”和“操纵”，通常用表面的攻击性代替微妙的邪恶。此外，我们证明了一般的聊天机器人熟练度是反派角色扮演能力的不良预测因子，高度安全对齐的模型表现尤其差。

Conclusion: 我们的工作提供了第一个系统证据，证明了这一关键限制，突显了模型安全性和创造性保真度之间的关键矛盾。我们的基准和发现为开发更细致、上下文感知的对齐方法铺平了道路。

Abstract: Large Language Models (LLMs) are increasingly tasked with creative
generation, including the simulation of fictional characters. However, their
ability to portray non-prosocial, antagonistic personas remains largely
unexamined. We hypothesize that the safety alignment of modern LLMs creates a
fundamental conflict with the task of authentically role-playing morally
ambiguous or villainous characters. To investigate this, we introduce the Moral
RolePlay benchmark, a new dataset featuring a four-level moral alignment scale
and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs
with role-playing characters from moral paragons to pure villains. Our
large-scale evaluation reveals a consistent, monotonic decline in role-playing
fidelity as character morality decreases. We find that models struggle most
with traits directly antithetical to safety principles, such as ``Deceitful''
and ``Manipulative'', often substituting nuanced malevolence with superficial
aggression. Furthermore, we demonstrate that general chatbot proficiency is a
poor predictor of villain role-playing ability, with highly safety-aligned
models performing particularly poorly. Our work provides the first systematic
evidence of this critical limitation, highlighting a key tension between model
safety and creative fidelity. Our benchmark and findings pave the way for
developing more nuanced, context-aware alignment methods.

</details>


### [24] [Acquiring Common Chinese Emotional Events Using Large Language Model](https://arxiv.org/abs/2511.04989)
*Ya Wang,Guangzheng Zhu,Cungen Cao,Jingjing Li,He Li,Xin Huang*

Main category: cs.CL

TL;DR: 本文旨在获取常见的中文情感事件，通过提示中文大语言模型生成并过滤结果，最终获得了一个大规模的情感事件知识库。


<details>
  <summary>Details</summary>
Motivation: 情感事件对于提高不同应用的效果非常重要，但常见的或通用的情感事件难以获取，尤其是与上下文无关的情感事件。

Method: 首先收集了一套全面的中文情感事件指标，然后通过提示中文大语言模型生成情感事件，并训练一个过滤器来丢弃无效结果。最后，使用不同技术对这些情感事件进行分类，获得了102,218个高质量的常见情感事件。

Result: 成功获取了102,218个带有情感极性标签的高质量常见情感事件，这是中文语言中唯一的大型常识知识库。

Conclusion: 本文提出的方法可以有效地获取常见的中文情感事件，并且在情感原因提取领域展现出强大的潜力。相关资源将在论文发表后发布。

Abstract: Knowledge about emotional events is an important kind of knowledge which has
been applied to improve the effectiveness of different applications. However,
emotional events cannot be easily acquired, especially common or generalized
emotional events that are context-independent. The goal of this paper is to
obtain common emotional events in Chinese language such as "win a prize" and
"be criticized". Our approach begins by collecting a comprehensive list of
Chinese emotional event indicators. Then, we generate emotional events by
prompting a Chinese large language model (LLM) using these indicators. To
ensure the quality of these emotional events, we train a filter to discard
invalid generated results. We also classify these emotional events as being
positive events and negative events using different techniques. Finally, we
harvest a total of 102,218 high-quality common emotional events with sentiment
polarity labels, which is the only large-scale commonsense knowledge base of
emotional events in Chinese language. Intrinsic evaluation results show that
the proposed method in this paper can be effectively used to acquire common
Chinese emotional events. An extrinsic use case also demonstrates the strong
potential of common emotional events in the field of emotion cause extraction
(ECE). Related resources including emotional event indicators and emotional
events will be released after the publication of this paper.

</details>


### [25] [Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies](https://arxiv.org/abs/2511.05018)
*Prasoon Varshney,Makesh Narsimhan Sreedhar,Liwei Jiang,Traian Rebedea,Christopher Parisien*

Main category: cs.CL

TL;DR: 本文提出了一个动态评估套件PBSUITE，用于评估大型语言模型在多轮交互对话中遵循多元对齐规范的能力。结果表明，现有模型对齐和安全调节方法在现实世界的大语言模型交互中无法有效执行多元行为政策。


<details>
  <summary>Details</summary>
Motivation: 现实世界中大型语言模型的应用通常受到企业政策、法规要求、使用案例、品牌指南和伦理承诺的影响，因此需要对大型语言模型进行严格和全面的评估，以适应不同的用户价值观和需求。

Method: 我们提出了PLURALISTIC BEHAVIOR SUITE (PBSUITE)，这是一个动态评估套件，旨在系统评估大型语言模型在多轮交互对话中遵循多元对齐规范的能力。

Result: 我们发现，领先的开源和闭源大型语言模型在单轮设置中保持了对行为政策的稳健遵守（失败率低于4%），但在多轮对抗性交互中其合规性显著下降（最高84%的失败率）。

Conclusion: 我们的工作为未来研究提供了数据集和分析框架，以支持稳健且上下文感知的多元对齐技术。

Abstract: Large language models (LLMs) are typically aligned to a universal set of
safety and usage principles intended for broad public acceptability. Yet,
real-world applications of LLMs often take place within organizational
ecosystems shaped by distinctive corporate policies, regulatory requirements,
use cases, brand guidelines, and ethical commitments. This reality highlights
the need for rigorous and comprehensive evaluation of LLMs with pluralistic
alignment goals, an alignment paradigm that emphasizes adaptability to diverse
user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE
(PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs'
capacity to adhere to pluralistic alignment specifications in multi-turn,
interactive conversations. PBSUITE consists of (1) a diverse dataset of 300
realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic
evaluation framework for stress-testing model compliance with custom behavioral
specifications under adversarial conditions. Using PBSUITE, We find that
leading open- and closed-source LLMs maintain robust adherence to behavioral
policies in single-turn settings (less than 4% failure rates), but their
compliance weakens substantially in multi-turn adversarial interactions (up to
84% failure rates). These findings highlight that existing model alignment and
safety moderation methods fall short in coherently enforcing pluralistic
behavioral policies in real-world LLM interactions. Our work contributes both
the dataset and analytical framework to support future research toward robust
and context-aware pluralistic alignment techniques.

</details>


### [26] [UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian](https://arxiv.org/abs/2511.05040)
*Mykyta Syromiatnikov,Victoria Ruvinskaya*

Main category: cs.CL

TL;DR: 本文介绍了UA-Code-Bench，一个用于评估语言模型在乌克兰语中代码生成和编程问题解决能力的新基准。结果显示，即使是顶级模型也只能解决一半的问题，表明在低资源语言中生成代码的挑战。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在低资源语言中的真实能力仍然是一项挑战，因为许多现有的基准专注于从英语翻译的广泛任务或仅评估简单的语言理解。

Method: 本文介绍了UA-Code-Bench，这是一个新的开源基准，用于全面评估语言模型在乌克兰语中的代码生成和编程问题解决能力。

Result: 即使表现最好的模型，如OpenAI o3和GPT-5，也只能解决一半的问题，这突显了在低资源自然语言中生成代码的挑战。此外，本文对不同难度级别的性能进行了全面分析，并评估了解决方案的独特性和计算效率。

Conclusion: 本文展示了竞赛编程基准在评估大型语言模型方面的价值，特别是在非主流语言中。它也为多语言代码生成和增强推理的模型提供了未来研究的方向。

Abstract: Evaluating the real capabilities of large language models in low-resource
languages still represents a challenge, as many existing benchmarks focus on
widespread tasks translated from English or evaluate only simple language
understanding. This paper introduces UA-Code-Bench, a new open-source benchmark
established for a thorough evaluation of language models' code generation and
competitive programming problem-solving abilities in Ukrainian. The benchmark
comprises 500 problems from the Eolymp platform, evenly distributed across five
complexity levels from very easy to very hard. A diverse set of 13 leading
proprietary and open-source models, generating Python solutions based on a
one-shot prompt, was evaluated via the dedicated Eolymp environment against
hidden tests, ensuring code correctness. The obtained results reveal that even
top-performing models, such as OpenAI o3 and GPT-5, solve only half of the
problems, highlighting the challenge of code generation in low-resource natural
language. Furthermore, this research presents a comprehensive analysis of
performance across various difficulty levels, as well as an assessment of
solution uniqueness and computational efficiency, measured by both elapsed time
and memory consumption of the generated solutions. In conclusion, this work
demonstrates the value of competitive programming benchmarks in evaluating
large language models, especially in underrepresented languages. It also paves
the way for future research on multilingual code generation and
reasoning-enhanced models. The benchmark, data parsing, preparation, code
generation, and evaluation scripts are available at
https://huggingface.co/datasets/NLPForUA/ua-code-bench.

</details>


### [27] [Order-Level Attention Similarity Across Language Models: A Latent Commonality](https://arxiv.org/abs/2511.05064)
*Jinglin Liang,Jin Zhong,Shuangping Huang,Yunqing Hu,Huiyuan Zhang,Huifang Li,Lixin Fan,Hanlin Gu*

Main category: cs.CL

TL;DR: This paper explores commonalities in context aggregation patterns across Language Models (LMs) and introduces the Order-Level Attention (OLA) to reveal similarities in attention mechanisms. Based on these findings, the Transferable OLA Adapter (TOA) is proposed, which enables cross-LM knowledge transfer without parameter updates.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore whether context aggregation patterns across Language Models (LMs) share commonalities, as previous works have typically focused on individual models or attention heads, lacking a systematic analysis across multiple LMs.

Method: We introduce the Order-Level Attention (OLA) derived from the order-wise decomposition of Attention Rollout and reveal that the OLA at the same order across LMs exhibits significant similarities. Furthermore, we discover an implicit mapping between OLA and syntactic knowledge. Based on these two findings, we propose the Transferable OLA Adapter (TOA), a training-free cross-LM adapter transfer method.

Result: Extensive experiments demonstrate that TOA's cross-LM generalization effectively enhances the performance of unseen LMs.

Conclusion: TOA's cross-LM generalization effectively enhances the performance of unseen LMs.

Abstract: In this paper, we explore an important yet previously neglected question: Do
context aggregation patterns across Language Models (LMs) share commonalities?
While some works have investigated context aggregation or attention weights in
LMs, they typically focus on individual models or attention heads, lacking a
systematic analysis across multiple LMs to explore their commonalities. In
contrast, we focus on the commonalities among LMs, which can deepen our
understanding of LMs and even facilitate cross-model knowledge transfer. In
this work, we introduce the Order-Level Attention (OLA) derived from the
order-wise decomposition of Attention Rollout and reveal that the OLA at the
same order across LMs exhibits significant similarities. Furthermore, we
discover an implicit mapping between OLA and syntactic knowledge. Based on
these two findings, we propose the Transferable OLA Adapter (TOA), a
training-free cross-LM adapter transfer method. Specifically, we treat the OLA
as a unified syntactic feature representation and train an adapter that takes
OLA as input. Due to the similarities in OLA across LMs, the adapter
generalizes to unseen LMs without requiring any parameter updates. Extensive
experiments demonstrate that TOA's cross-LM generalization effectively enhances
the performance of unseen LMs. Code is available at
https://github.com/jinglin-liang/OLAS.

</details>


### [28] [Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts](https://arxiv.org/abs/2511.05078)
*Manan Sharma,Arya Suneesh,Manish Jain,Pawan Kumar Rajpoot,Prasanna Devadiga,Bharatdeep Hazarika,Ashish Shrivastava,Kishan Gurumurthy,Anshuman B Suresh,Aditya U Baliga*

Main category: cs.CL

TL;DR: 本文提出一种方法，通过系统分解帖子并利用英语数据进行训练，实现了跨语言的声明归一化，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多语言虚假信息检测中的声明归一化问题，将嘈杂的社交媒体帖子转化为清晰、可验证的陈述。

Method: 该方法使用LoRA微调Qwen3-14B，进行内部帖子去重、基于标记的召回过滤以实现语义对齐，并在推理期间使用带有上下文示例的检索增强少样本学习。

Result: 系统在METEOR评分上取得了从41.16（英语）到15.21（马拉地语）的范围，并在英语排行榜上获得第三名，在荷兰语和旁遮普语排行榜上获得第四名。相比基线配置有41.3%的相对提升，并且在现有方法上取得显著进步。

Conclusion: 该方法在多种语言中展示了有效的跨语言泛化能力，并在不同语言结构中保持语义连贯性。

Abstract: We address claim normalization for multilingual misinformation detection -
transforming noisy social media posts into clear, verifiable statements across
20 languages. The key contribution demonstrates how systematic decomposition of
posts using Who, What, Where, When, Why and How questions enables robust
cross-lingual transfer despite training exclusively on English data. Our
methodology incorporates finetuning Qwen3-14B using LoRA with the provided
dataset after intra-post deduplication, token-level recall filtering for
semantic alignment and retrieval-augmented few-shot learning with contextual
examples during inference. Our system achieves METEOR scores ranging from 41.16
(English) to 15.21 (Marathi), securing third rank on the English leaderboard
and fourth rank for Dutch and Punjabi. The approach shows 41.3% relative
improvement in METEOR over baseline configurations and substantial gains over
existing methods. Results demonstrate effective cross-lingual generalization
for Romance and Germanic languages while maintaining semantic coherence across
diverse linguistic structures.

</details>


### [29] [On Text Simplification Metrics and General-Purpose LLMs for Accessible Health Information, and A Potential Architectural Advantage of The Instruction-Tuned LLM class](https://arxiv.org/abs/2511.05080)
*P. Bilha Githinji,Aikaterini Meilliou,Peiwu Qin*

Main category: cs.CL

TL;DR: 本文评估了两种大语言模型在文本简化任务中的表现，发现指令调优的Mistral 24B在保持语篇一致性的同时，表现出更好的可读性。


<details>
  <summary>Details</summary>
Motivation: 随着公众健康寻求行为和数字消费生物医学信息的增加，需要可扩展的解决方案将复杂的科学和技术文档自动转换为通俗语言。然而，自动文本简化解决方案仍然面临在优化可读性性能和确保语篇保真度之间可靠权衡的挑战。

Method: 本文通过比较分析指令调优的Mistral 24B和推理增强的QWen2.5 32B，评估了它们在文本简化任务中的表现。同时，对21个指标进行了全面的相关性分析，以获得机制性见解。

Result: Mistral 24B表现出一种温和的词汇简化策略，提高了多个指标的可读性，其SARI评分为42.46，同时保持了人类水平的语篇一致性，BERTScore为0.91。QWen2.5 32B也获得了增强的可读性表现，但其操作策略在可读性和准确性之间存在脱节，BERTScore为0.89，统计上显著较低。此外，21个指标的相关性分析确认了五个可读性指标之间的强功能冗余。

Conclusion: 本文通过实证研究评估了两种主要类型的通用大语言模型（LLM）在文本简化任务中的表现，发现指令调优的Mistral 24B在保持人类水平的语篇一致性的同时，表现出更好的可读性。此外，研究还提供了必要的启发式方法来选择指标，并指出了词汇支持是文本简化领域适应的主要问题。

Abstract: The increasing health-seeking behavior and digital consumption of biomedical
information by the general public necessitate scalable solutions for
automatically adapting complex scientific and technical documents into plain
language. Automatic text simplification solutions, including advanced large
language models, however, continue to face challenges in reliably arbitrating
the tension between optimizing readability performance and ensuring
preservation of discourse fidelity. This report empirically assesses the
performance of two major classes of general-purpose LLMs, demonstrating their
linguistic capabilities and foundational readiness for the task compared to a
human benchmark. Using a comparative analysis of the instruction-tuned Mistral
24B and the reasoning-augmented QWen2.5 32B, we identify a potential
architectural advantage in the instruction-tuned LLM. Mistral exhibits a
tempered lexical simplification strategy that enhances readability across a
suite of metrics and the simplification-specific formula SARI (mean 42.46),
while preserving human-level discourse with a BERTScore of 0.91. QWen also
attains enhanced readability performance, but its operational strategy shows a
disconnect in balancing between readability and accuracy, reaching a
statistically significantly lower BERTScore of 0.89. Additionally, a
comprehensive correlation analysis of 21 metrics spanning readability,
discourse fidelity, content safety, and underlying distributional measures for
mechanistic insights, confirms strong functional redundancies among five
readability indices. This empirical evidence tracks baseline performance of the
evolving LLMs for the task of text simplification, identifies the
instruction-tuned Mistral 24B for simplification, provides necessary heuristics
for metric selection, and points to lexical support as a primary
domain-adaptation issue for simplification.

</details>


### [30] [Iterative Layer-wise Distillation for Efficient Compression of Large Language Models](https://arxiv.org/abs/2511.05085)
*Grigory Kovalev,Mikhail Tikhomirov*

Main category: cs.CL

TL;DR: 该研究提出了一种改进的蒸馏方法，通过迭代评估层的重要性并结合联合损失函数进行训练，成功减少了大型语言模型的层数，同时保持了较高的性能。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在开发紧凑的模型，同时保持高性能，以适应资源受限的环境。

Method: 该研究基于ShortGPT方法，通过迭代评估层的重要性，并结合使用KL散度和均方误差的联合损失函数进行进一步训练。

Result: 实验表明，Qwen2.5-3B模型的层数可以从36减少到28（参数数量为24.7亿），仅损失9.7%的质量；减少到24层时损失18%。

Conclusion: 该研究展示了迭代蒸馏和微调方法在创建高效模型方面的有效性，特别是在资源受限的环境中。

Abstract: This work investigates distillation methods for large language models (LLMs)
with the goal of developing compact models that preserve high performance.
Several existing approaches are reviewed, with a discussion of their respective
strengths and limitations. An improved method based on the ShortGPT approach
has been developed, building upon the idea of incorporating iterative
evaluation of layer importance. At each step, importance is assessed by
measuring performance degradation when individual layers are removed, using a
set of representative datasets. This process is combined with further training
using a joint loss function based on KL divergence and mean squared error.
Experiments on the Qwen2.5-3B model show that the number of layers can be
reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a
9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that
the middle transformer layers contribute less to inference, underscoring the
potential of the proposed method for creating efficient models. The results
demonstrate the effectiveness of iterative distillation and fine-tuning, making
the approach suitable for deployment in resource-limited settings.

</details>


### [31] [A Toolbox for Improving Evolutionary Prompt Search](https://arxiv.org/abs/2511.05120)
*Daniel Grießhaber,Maximilian Kimmich,Johannes Maucher,Ngoc Thang Vu*

Main category: cs.CL

TL;DR: 本文提出了一些关键改进，以提高进化提示优化的效果和效率，并释放了代码以支持新任务的提示优化。


<details>
  <summary>Details</summary>
Motivation: 现有的方法缺乏稳健的操作符和高效的评估机制。

Method: 我们将进化分解为不同的步骤，引入基于LLM的裁判来验证进化，整合人类反馈来精炼进化算子，并开发更高效的评估策略。

Result: 我们的方法提高了优化质量和效率。

Conclusion: 我们的方法提高了优化质量和效率，并释放了代码，以在新任务上进行提示优化并促进该领域的进一步研究。

Abstract: Evolutionary prompt optimization has demonstrated effectiveness in refining
prompts for LLMs. However, existing approaches lack robust operators and
efficient evaluation mechanisms. In this work, we propose several key
improvements to evolutionary prompt optimization that can partially generalize
to prompt optimization in general: 1) decomposing evolution into distinct steps
to enhance the evolution and its control, 2) introducing an LLM-based judge to
verify the evolutions, 3) integrating human feedback to refine the evolutionary
operator, and 4) developing more efficient evaluation strategies that maintain
performance while reducing computational overhead. Our approach improves both
optimization quality and efficiency. We release our code, enabling prompt
optimization on new tasks and facilitating further research in this area.

</details>


### [32] [ManufactuBERT: Efficient Continual Pretraining for Manufacturing](https://arxiv.org/abs/2511.05135)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: 本文介绍了ManufactuBERT，一种在制造业领域大规模语料库上持续预训练的RoBERTa模型，展示了其在制造业相关NLP任务中的优越性能，并提出了一个可复制的数据处理管道。


<details>
  <summary>Details</summary>
Motivation: 现有的大型通用Transformer编码器在制造等专业领域表现不佳，因为缺乏对领域特定术语和语义的接触。本文旨在解决这一问题。

Method: 本文提出了一种全面的数据处理管道，从网络数据中创建制造业领域的语料库，包括初始的领域特定过滤步骤和多阶段去重过程以去除冗余。

Result: ManufactuBERT在多个制造业相关的NLP任务中达到了新的最先进水平，并且通过精心去重的语料库显著加速了收敛，减少了33%的训练时间和计算成本。

Conclusion: 本文提出了ManufactuBERT，这是一种在制造业领域大规模语料库上持续预训练的RoBERTa模型。实验表明，ManufactuBERT在多个制造业相关的NLP任务中达到了新的最先进水平，并且通过精心去重的语料库显著加速了收敛，减少了33%的训练时间和计算成本。该方法为其他专业领域的高性能编码器开发提供了一个可复制的例子。

Abstract: While large general-purpose Transformer-based encoders excel at general
language understanding, their performance diminishes in specialized domains
like manufacturing due to a lack of exposure to domain-specific terminology and
semantics. In this paper, we address this gap by introducing ManufactuBERT, a
RoBERTa model continually pretrained on a large-scale corpus curated for the
manufacturing domain. We present a comprehensive data processing pipeline to
create this corpus from web data, involving an initial domain-specific
filtering step followed by a multi-stage deduplication process that removes
redundancies. Our experiments show that ManufactuBERT establishes a new
state-of-the-art on a range of manufacturing-related NLP tasks, outperforming
strong specialized baselines. More importantly, we demonstrate that training on
our carefully deduplicated corpus significantly accelerates convergence,
leading to a 33\% reduction in training time and computational cost compared to
training on the non-deduplicated dataset. The proposed pipeline offers a
reproducible example for developing high-performing encoders in other
specialized domains. We will release our model and curated corpus at
https://huggingface.co/cea-list-ia.

</details>


### [33] [Mind the Gap... or Not? How Translation Errors and Evaluation Details Skew Multilingual Results](https://arxiv.org/abs/2511.05162)
*Jan-Thorsten Peter,David Vilar,Tobias Domhan,Dan Malkin,Markus Freitag*

Main category: cs.CL

TL;DR: 本文研究了不同LLM在多语言中的表现，发现存在显著的语言差距。但通过分析发现数据中存在翻译错误，且缺乏标准化的答案提取，导致结果不准确。我们提出了自动质量保证方法并给出建议，结合后语言差距大多消失，结论改变，并发布更正后的数据集。


<details>
  <summary>Details</summary>
Motivation: 我们希望这些结果能影响下一代LLM的跨语言能力泛化研究。

Method: 我们提出了一种自动质量保证的方法来解决第一个问题，并给出了解决第二个问题的建议。结合这两种方法，我们展示了上述语言差距大多消失。

Result: 实验结果表明，模型在不同语言中的性能存在明显且一致的差距。有趣的是，这种差距存在于高资源和低资源语言中。

Conclusion: 我们的研究结果表明，语言差距主要消失，这导致了与我们研究不同的结论。我们还向社区发布了更正后的数据集。

Abstract: Most current large language models (LLMs) support a wide variety of languages
in addition to English, including high-resource languages (e.g. German,
Chinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In
addition they have also shown impressive capabilities in different domains,
like coding, science and math. In this short paper, taking math as an example
domain, we study the performance of different LLMs across languages.
Experimental results show that there exists a non-negligible and consistent gap
in the performance of the models across languages. Interestingly, and somewhat
against expectations, the gap exists for both high- and low-resource languages.
We hope that these results influence further research into cross-lingual
capability generalization for next generation LLMs. If it weren't for the fact
that they are false! By analyzing one of the standard multilingual math
benchmarks (MGSM), we determine that several translation errors are present in
the data. Furthermore, the lack of standardized answer extraction from LLM
outputs further influences the final results. We propose a method for automatic
quality assurance to address the first issue at scale, and give recommendations
to address the second one. Combining these two approaches we show that the
aforementioned language gap mostly disappears, leading to completely different
conclusions from our research. We additionally release the corrected dataset to
the community.

</details>


### [34] [Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models](https://arxiv.org/abs/2511.05184)
*Cong-Thanh Do,Rama Doddipatla,Kate Knill*

Main category: cs.CL

TL;DR: 本文研究了Chain-of-Thought (CoT) 在知识蒸馏中的作用，发现CoT能有效提升小型语言模型在复杂自然语言任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究CoT在将大型LLM的推理能力转移到小型LLM中的作用，并评估其在各种自然语言推理和理解任务中的有效性。

Method: 本文使用Qwen和Llama2家族的LLMs进行白盒知识蒸馏实验，并利用CoT-Collection数据集中的CoT数据。

Result: 实验结果表明，CoT能够提高白盒知识蒸馏的效果，使蒸馏模型在BBH基准测试中表现更好。

Conclusion: 实验结果表明，CoT在提高白盒知识蒸馏的有效性方面起到了重要作用，使蒸馏模型在BBH的自然语言推理和理解任务中实现了更好的平均性能。

Abstract: Chain-of-Thought (CoT) prompting is a widely used method to improve the
reasoning capability of Large Language Models (LLMs). More recently, CoT has
been leveraged in Knowledge Distillation (KD) to transfer reasoning capability
from a larger LLM to a smaller one. This paper examines the role of CoT in
distilling the reasoning capability from larger LLMs to smaller LLMs using
white-box KD, analysing its effectiveness in improving the performance of the
distilled models for various natural language reasoning and understanding
tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2
families, employing CoT data from the CoT-Collection dataset. The distilled
models are then evaluated on natural language reasoning and understanding tasks
from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for
smaller LLMs. Experimental results demonstrate the role of CoT in improving
white-box KD effectiveness, enabling the distilled models to achieve better
average performance in natural language reasoning and understanding tasks from
BBH.

</details>


### [35] [Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese](https://arxiv.org/abs/2511.05239)
*Zilong Li,Jie Cao*

Main category: cs.CL

TL;DR: 本文研究了古代人将古典中文翻译成日文的过程，并将其抽象为序列标注任务。通过引入基于LLM的注释流程和构建新数据集，我们发现辅助中文NLP任务有助于提高序列标注任务的性能，而大型语言模型在直接机器翻译中表现良好，但在注释任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究这种注释和翻译系统面临低资源问题。

Method: 我们引入了一个基于LLM的注释流程，并从数字化的开源翻译数据中构建了一个新数据集。

Result: 在低资源设置下，引入辅助中文NLP任务对序列标记任务的训练有促进作用。大型语言模型在直接机器翻译中表现良好，但在被要求注释字符时会感到困惑。

Conclusion: 我们的方法可以作为LLMs的补充。

Abstract: Ancient people translated classical Chinese into Japanese by annotating
around each character. We abstract this process as sequence tagging tasks and
fit them into modern language technologies. The research of this annotation and
translation system is a facing low-resource problem. We release this problem by
introducing a LLM-based annotation pipeline and construct a new dataset from
digitalized open-source translation data. We show that under the low-resource
setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the
training of sequence tagging tasks. We also evaluate the performance of large
language models. They achieve high scores in direct machine translation, but
they are confused when being asked to annotate characters. Our method could
work as a supplement of LLMs.

</details>


### [36] [Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models](https://arxiv.org/abs/2511.05286)
*Teqi Hao,Xioayu Tan,Shaojie Shi,Yinghui Xu,Xihe Qiu*

Main category: cs.CL

TL;DR: RPO is a novel framework that decouples content generation from personalization, using a two-stage process to improve user-specific alignment while maintaining high-quality outputs.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for personalizing black-box large language models (LLMs) rely on context injection, which imposes a dual burden on the model, leading to a trade-off that compromises output quality and limits precise control.

Method: RPO operates in two distinct stages: first, a base model generates a high-quality, generic response; then, an external reflection module explicitly rewrites this output to align with the user's preferences. The reflection module is trained using a two-stage process: supervised fine-tuning on structured rewriting trajectories, followed by reinforcement learning to refine and enhance the quality of personalized outputs.

Result: Comprehensive experiments on the LaMP benchmark demonstrate that RPO significantly outperforms state-of-the-art baselines by decoupling content generation from personalization, underscoring the superiority of explicit response shaping over implicit context injection.

Conclusion: RPO introduces an efficient, model-agnostic personalization layer that can be seamlessly integrated with any underlying base model, paving the way for a new and effective direction in user-centric generation scenarios.

Abstract: The personalization of black-box large language models (LLMs) is a critical
yet challenging task. Existing approaches predominantly rely on context
injection, where user history is embedded into the prompt to directly guide the
generation process. However, this single-step paradigm imposes a dual burden on
the model: generating accurate content while simultaneously aligning with
user-specific styles. This often results in a trade-off that compromises output
quality and limits precise control. To address this fundamental tension, we
propose Reflective Personalization Optimization (RPO), a novel framework that
redefines the personalization paradigm by decoupling content generation from
alignment. RPO operates in two distinct stages: first, a base model generates a
high-quality, generic response; then, an external reflection module explicitly
rewrites this output to align with the user's preferences. This reflection
module is trained using a two-stage process. Initially, supervised fine-tuning
is employed on structured rewriting trajectories to establish a core
personalized reasoning policy that models the transformation from generic to
user-aligned responses. Subsequently, reinforcement learning is applied to
further refine and enhance the quality of the personalized outputs.
Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by
decoupling content generation from personalization, significantly outperforms
state-of-the-art baselines. These findings underscore the superiority of
explicit response shaping over implicit context injection. Moreover, RPO
introduces an efficient, model-agnostic personalization layer that can be
seamlessly integrated with any underlying base model, paving the way for a new
and effective direction in user-centric generation scenarios.

</details>


### [37] [Listening Between the Lines: Decoding Podcast Narratives with Language Modeling](https://arxiv.org/abs/2511.05310)
*Shreya Gupta,Ojasva Saxena,Arghodeep Nandi,Sarah Masud,Kiran Garimella,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文旨在解决播客叙事分析中的挑战，通过开发一种新的框架标注方法和分析方法，以更准确地理解播客如何影响公众意见。


<details>
  <summary>Details</summary>
Motivation: 由于播客的流动性和对话性，现有的大型语言模型在捕捉人类听众用来识别叙事框架的细微线索方面存在困难，因此需要一种新的分析方法来准确分析播客叙事。

Method: 本文开发并评估了一个微调的BERT模型，该模型将叙事框架与对话中提到的具体实体相关联，从而将抽象的框架具体化。然后，该方法使用这些细粒度的框架标签，并将其与高层次的主题相关联，以揭示更广泛的论述趋势。

Result: 本文提出了一种新的框架标注方法，能够更贴近人类判断，以及一种新的分析方法，揭示了话题和框架之间的系统关系。

Conclusion: 本文提出了一个更符合人类判断的框架标注方法，并揭示了讨论内容和呈现方式之间的系统关系，为研究数字媒体中的影响力提供了更稳健的框架。

Abstract: Podcasts have become a central arena for shaping public opinion, making them
a vital source for understanding contemporary discourse. Their typically
unscripted, multi-themed, and conversational style offers a rich but complex
form of data. To analyze how podcasts persuade and inform, we must examine
their narrative structures -- specifically, the narrative frames they employ.
  The fluid and conversational nature of podcasts presents a significant
challenge for automated analysis. We show that existing large language models,
typically trained on more structured text such as news articles, struggle to
capture the subtle cues that human listeners rely on to identify narrative
frames. As a result, current approaches fall short of accurately analyzing
podcast narratives at scale.
  To solve this, we develop and evaluate a fine-tuned BERT model that
explicitly links narrative frames to specific entities mentioned in the
conversation, effectively grounding the abstract frame in concrete details. Our
approach then uses these granular frame labels and correlates them with
high-level topics to reveal broader discourse trends. The primary contributions
of this paper are: (i) a novel frame-labeling methodology that more closely
aligns with human judgment for messy, conversational data, and (ii) a new
analysis that uncovers the systematic relationship between what is being
discussed (the topic) and how it is being presented (the frame), offering a
more robust framework for studying influence in digital media.

</details>


### [38] [What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions](https://arxiv.org/abs/2511.05320)
*Klára Bendová,Tomáš Knap,Jan Černý,Vojtěch Pour,Jaromir Savelka,Ivana Kvapilíková,Jakub Drápal*

Main category: cs.CL

TL;DR: 本文研究了从斯洛伐克法院判决中提取犯罪行为描述的可行性，发现使用高级正则表达式和大型语言模型（LLMs）可以显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 刑事司法行政数据中关于犯罪行为的信息有限，而欧洲大陆的法院判决中包含了大量的犯罪行为描述，因此需要研究如何从中提取这些信息。

Method: 本文采用了两种方法来提取犯罪行为描述：一种是基于正则表达式的基线方法，另一种是使用大型语言模型（如Gemini Flash 2.0）进行提示提取。

Result: 基线方法仅在40.5%的判决中识别出描述，而高级正则表达式方法和LLMs分别达到了97%和98.75%的准确率，结合两者甚至达到了99.5%。法律学生的评估显示，两种高级方法在约90%的情况下与人工标注一致，而基线方法仅为34.5%。LLMs在91.75%的情况下完全匹配人工标注，结合高级正则表达式和LLMs的方法达到了92%。

Conclusion: 本文研究表明，使用高级正则表达式和大型语言模型（LLMs）可以从斯洛伐克的法院判决中有效提取犯罪行为描述。结合这两种方法可以进一步提高准确性。

Abstract: Criminal justice administrative data contain only a limited amount of
information about the committed offense. However, there is an unused source of
extensive information in continental European courts' decisions: descriptions
of criminal behaviors in verdicts by which offenders are found guilty. In this
paper, we study the feasibility of extracting these descriptions from publicly
available court decisions from Slovakia. We use two different approaches for
retrieval: regular expressions and large language models (LLMs). Our baseline
was a simple method employing regular expressions to identify typical words
occurring before and after the description. The advanced regular expression
approach further focused on "sparing" and its normalization (insertion of
spaces between individual letters), typical for delineating the description.
The LLM approach involved prompting the Gemini Flash 2.0 model to extract the
descriptions using predefined instructions. Although the baseline identified
descriptions in only 40.5% of verdicts, both methods significantly outperformed
it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and
99.5% when combined. Evaluation by law students showed that both advanced
methods matched human annotations in about 90% of cases, compared to just 34.5%
for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of
instances, and a combination of advanced regular expressions with LLMs reached
92%.

</details>


### [39] [Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study with BengaliBPE](https://arxiv.org/abs/2511.05324)
*Firoj Ahmmed Patwary,Abdullah Al Noman*

Main category: cs.CL

TL;DR: 本文介绍了BengaliBPE，一种专门为孟加拉语脚本设计的字节对编码分词器，它在形态学解释和分割细节方面优于其他基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前的子词分词器如SentencePiece或HuggingFace BPE主要为拉丁语或多语言语料库设计，在形态丰富的语言如孟加拉语上表现不佳。

Method: BengaliBPE应用了Unicode标准化、音素级初始化和形态意识合并规则，以保持语言一致性并保留子词完整性。

Result: BengaliBPE提供了最详细的分割和最佳的形态可解释性，尽管计算成本稍高。

Conclusion: 这些发现强调了针对形态丰富的脚本进行语言感知分词的重要性，并将BengaliBPE确立为未来孟加拉语自然语言处理系统（包括上下文语言模型的大规模预训练）的强大基础。

Abstract: Tokenization is an important first step in Natural Language Processing (NLP)
pipelines because it decides how models learn and represent linguistic
information. However, current subword tokenizers like SentencePiece or
HuggingFace BPE are mostly designed for Latin or multilingual corpora and do
not perform well on languages with rich morphology such as Bengali. To address
this limitation, we present BengaliBPE, a Byte Pair Encoding (BPE) tokenizer
specifically developed for the Bengali script. BengaliBPE applies Unicode
normalization, grapheme-level initialization, and morphology-aware merge rules
to maintain linguistic consistency and preserve subword integrity. We use a
large-scale Bengali news classification dataset to compare BengaliBPE with
three baselines: Whitespace, SentencePiece BPE, and HuggingFace BPE. The
evaluation considers tokenization granularity, encoding speed, and downstream
classification accuracy. While all methods perform reasonably well, BengaliBPE
provides the most detailed segmentation and the best morphological
interpretability, albeit with slightly higher computational cost. These
findings highlight the importance of language-aware tokenization for
morphologically rich scripts and establish BengaliBPE as a strong foundation
for future Bengali NLP systems, including large-scale pretraining of contextual
language models.

</details>


### [40] [A multimodal multiplex of the mental lexicon for multilingual individuals](https://arxiv.org/abs/2511.05361)
*Maria Huynh,Wilder C. Rodrigues*

Main category: cs.CL

TL;DR: 本研究探讨了多语言心理词典的结构，并通过引入视觉输入层扩展了现有的多层网络模型，以研究母语对另一种语言习得的影响。


<details>
  <summary>Details</summary>
Motivation: 传统上，双语被认为是一种额外的认知负担，但近年来的研究表明，多语言者在各种语言和认知任务中表现优于单语者。因此，本研究旨在进一步探索多语言心理词典的结构及其对语言习得的影响。

Method: 本研究基于Stella等人（2018）的多层心理词典模型和Dijkstra和van Heuven（2002）提出的双语交互激活（BIA+）框架，结合Kivela等人（2014）提出的多层网络原理，设计了一种包含多模态输入的多层模型。

Result: 本研究通过实验设计，验证了视觉输入在翻译任务中对参与者熟练度和准确度的影响，从而揭示了多语言心理词典的复杂性。

Conclusion: 本研究旨在探讨母语语言如何影响另一种语言的习得，并通过引入视觉输入层来扩展多层网络模型，以更好地理解多语言心理词典的结构。

Abstract: Historically, bilingualism was often perceived as an additional cognitive
load that could hinder linguistic and intellectual development. However, over
the last three decades, this view has changed considerably. Numerous studies
have aimed to model and understand the architecture of the bilingual word
recognition system Dijkstra and van Heuven (2002), investigating how parallel
activation operates in the brain and how one language influences another Kroll
et al. (2015). Increasingly, evidence suggests that multilinguals, individuals
who speak three or more languages, can perform better than monolinguals in
various linguistic and cognitive tasks, such as learning an additional language
Abu-Rabia and Sanitsky (2010). This research proposal focuses on the study of
the mental lexicon and how it may be structured in individuals who speak
multiple languages. Building on the work of Stella et al. (2018), who
investigated explosive learning in humans using a multiplex model of the mental
lexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by
Dijkstra and van Heuven (2002), the present study applies the same multilayer
network principles introduced by Kivela et al. (2014). Our experimental design
extends previous research by incorporating multimodality into the multiplex
model, introducing an additional layer that connects visual inputs to their
corresponding lexical representations across the multilingual layers of the
mental lexicon. In this research, we aim to explore how a heritage language
influences the acquisition of another language. Specifically, we ask: Does the
presence of visual input in a translation task influence participants'
proficiency and accuracy compared to text-only conditions?

</details>


### [41] [Large Language Models for Explainable Threat Intelligence](https://arxiv.org/abs/2511.05406)
*Tiago Dinis,Miguel Correia,Roger Tavares*

Main category: cs.CL

TL;DR: This paper introduces RAGRecon, a system that uses LLMs with RAG to enhance cybersecurity threat intelligence and makes AI reasoning transparent through knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Traditional security mechanisms are struggling to keep up with complex cyber threats, and LLMs have potential in cybersecurity due to their text processing capabilities.

Method: The paper proposes RAGRecon, a system that uses LLMs with RAG to answer cybersecurity-related questions and generates a knowledge graph to make AI reasoning explainable.

Result: RAGRecon was evaluated with two datasets and seven LLMs, achieving over 91% accuracy in matching reference responses for the best combinations.

Conclusion: RAGRecon demonstrates the effectiveness of using LLMs with RAG for cybersecurity threat intelligence, with high accuracy in matching reference responses.

Abstract: As cyber threats continue to grow in complexity, traditional security
mechanisms struggle to keep up. Large language models (LLMs) offer significant
potential in cybersecurity due to their advanced capabilities in text
processing and generation. This paper explores the use of LLMs with
retrieval-augmented generation (RAG) to obtain threat intelligence by combining
real-time information retrieval with domain-specific data. The proposed system,
RAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats.
Moreover, it makes this form of Artificial Intelligence (AI) explainable by
generating and visually presenting to the user a knowledge graph for every
reply. This increases the transparency and interpretability of the reasoning of
the model, allowing analysts to better understand the connections made by the
system based on the context recovered by the RAG system. We evaluated RAGRecon
experimentally with two datasets and seven different LLMs and the responses
matched the reference responses more than 91% of the time for the best
combinations.

</details>


### [42] [Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive Reinforcement Learning](https://arxiv.org/abs/2511.05407)
*Yahui Fu,Zi Haur Pang,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 本文提出了一种统一框架，用于建模个体和群体层面的用户满意度偏好，通过可解释的推理链和聚类算法，以及一个偏好自适应的强化学习框架，提高了对用户满意度的估计，尤其是在少数群体中。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法通常训练通用模型，旨在达成广泛共识，往往忽视了少数群体的观点和用户特定的适应性。

Method: 我们提出了一个统一的框架，通过可解释的推理链捕捉个体偏好，并提出了一种基于期望最大化的主要-少数群体感知聚类算法来发现不同的用户群体，最后将这些组件集成到一个偏好自适应的强化学习框架中。

Result: 实验表明，该框架在用户满意度估计方面有持续的改进，特别是在被代表不足的用户群体中。

Conclusion: 实验表明，该框架在用户满意度估计方面有持续的改进，特别是在被代表不足的用户群体中。

Abstract: User satisfaction in dialogue systems is inherently subjective. When the same
response strategy is applied across users, minority users may assign different
satisfaction ratings than majority users due to variations in individual
intents and preferences. However, existing alignment methods typically train
one-size-fits-all models that aim for broad consensus, often overlooking
minority perspectives and user-specific adaptation. We propose a unified
framework that models both individual- and group-level preferences for user
satisfaction estimation. First, we introduce Chain-of-Personalized-Reasoning
(CoPeR) to capture individual preferences through interpretable reasoning
chains. Second, we propose an expectation-maximization-based Majority-Minority
Preference-Aware Clustering (M2PC) algorithm that discovers distinct user
groups in an unsupervised manner to learn group-level preferences. Finally, we
integrate these components into a preference-adaptive reinforcement learning
framework (PAda-PPO) that jointly optimizes alignment with both individual and
group preferences. Experiments on the Emotional Support Conversation dataset
demonstrate consistent improvements in user satisfaction estimation,
particularly for underrepresented user groups.

</details>


### [43] [Steering Language Models with Weight Arithmetic](https://arxiv.org/abs/2511.05408)
*Constanza Fierro,Fabien Roger*

Main category: cs.CL

TL;DR: 本文提出了一种名为对比权重引导的方法，通过权重算术编辑模型参数，以更好地利用狭窄的训练数据，并控制模型的行为。实验结果表明，该方法在分布外行为控制方面表现优于激活引导，并能在任务特定微调中缓解不需要的行为漂移。


<details>
  <summary>Details</summary>
Motivation: 提供高质量的反馈给大型语言模型（LLMs）在一个多样化的训练分布上是困难且昂贵的，而仅在狭窄的分布上提供反馈可能导致意外的泛化。因此，需要一种方法来更好地利用狭窄的训练数据，并控制模型的行为。

Method: 本文提出了一种名为对比权重引导的方法，该方法通过权重算术编辑模型参数。具体来说，通过减去两个小微调的权重变化（一个诱导期望行为，另一个诱导相反行为）来隔离权重空间中的行为方向，然后添加或删除这个方向以修改模型的权重。

Result: 实验结果表明，权重引导通常比激活引导泛化得更好，在损害通用能力之前实现了更强的分布外行为控制。此外，在任务特定微调的背景下，权重引导可以部分缓解不需要的行为漂移，减少微调期间引入的奉承和拒绝行为，同时保持任务性能的提升。最后，初步证据表明，可以通过测量微调更新与“邪恶”权重方向之间的相似性来检测出现的不对齐。

Conclusion: 本文提出了一种简单的后训练方法——对比权重引导，通过权重算术编辑模型参数。这种方法可以更好地利用狭窄的训练数据，并在不损害通用能力的情况下实现更强的分布外行为控制。此外，权重引导可以在任务特定微调中部分缓解不需要的行为漂移，并提供初步证据表明，可以通过测量微调更新与“邪恶”权重方向之间的相似性来检测出现的不对齐。

Abstract: Providing high-quality feedback to Large Language Models (LLMs) on a diverse
training distribution can be difficult and expensive, and providing feedback
only on a narrow distribution can result in unintended generalizations. To
better leverage narrow training data, we propose contrastive weight steering, a
simple post-training method that edits the model parameters using weight
arithmetic. We isolate a behavior direction in weight-space by subtracting the
weight deltas from two small fine-tunes -- one that induces the desired
behavior and another that induces its opposite -- and then add or remove this
direction to modify the model's weights. We apply this technique to mitigate
sycophancy and induce misalignment, and find that weight steering often
generalizes further than activation steering, achieving stronger
out-of-distribution behavioral control before degrading general capabilities.
We also show that, in the context of task-specific fine-tuning, weight steering
can partially mitigate undesired behavioral drift: it can reduce sycophancy and
under-refusals introduced during fine-tuning while preserving task performance
gains. Finally, we provide preliminary evidence that emergent misalignment can
be detected by measuring the similarity between fine-tuning updates and an
"evil" weight direction, suggesting that it may be possible to monitor the
evolution of weights during training and detect rare misaligned behaviors that
never manifest during training or evaluations.

</details>


### [44] [MIMIC-SR-ICD11: A Dataset for Narrative-Based Diagnosis](https://arxiv.org/abs/2511.05485)
*Yuexin Wu,Shiqi Wang,Vasile Rus*

Main category: cs.CL

TL;DR: 本文介绍了LL-Rank，一种基于似然的重新排序框架，用于提高疾病诊断的准确性，通过计算每个标签在临床报告上下文中的长度归一化联合似然，并减去该标签的无报告先验似然。


<details>
  <summary>Details</summary>
Motivation: 疾病诊断是现代医疗保健的核心，自我报告可以保留模板化的电子健康记录（EHR）文档通常减弱或遗漏的临床相关信号，特别是细微但重要的细节。

Method: LL-Rank是一种基于似然的重新排序框架，计算每个标签在临床报告上下文中的长度归一化联合似然，并减去该标签的无报告先验似然。

Result: LL-Rank在七个模型骨干中 consistently 超过生成加映射基线（GenMap）。消融实验表明，LL-Rank的优势主要来自于基于PMI的评分，该评分将语义兼容性与标签频率偏差分离。

Conclusion: LL-Rank在多个模型骨干中 consistently 超过生成加映射基线（GenMap），其优势主要来自于基于PMI的评分，该评分将语义兼容性与标签频率偏差分离。

Abstract: Disease diagnosis is a central pillar of modern healthcare, enabling early
detection and timely intervention for acute conditions while guiding lifestyle
adjustments and medication regimens to prevent or slow chronic disease.
Self-reports preserve clinically salient signals that templated electronic
health record (EHR) documentation often attenuates or omits, especially subtle
but consequential details. To operationalize this shift, we introduce
MIMIC-SR-ICD11, a large English diagnostic dataset built from EHR discharge
notes and natively aligned to WHO ICD-11 terminology. We further present
LL-Rank, a likelihood-based re-ranking framework that computes a
length-normalized joint likelihood of each label given the clinical report
context and subtracts the corresponding report-free prior likelihood for that
label. Across seven model backbones, LL-Rank consistently outperforms a strong
generation-plus-mapping baseline (GenMap). Ablation experiments show that
LL-Rank's gains primarily stem from its PMI-based scoring, which isolates
semantic compatibility from label frequency bias.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [45] [A Penny for Your Thoughts: Decoding Speech from Inexpensive Brain Signals](https://arxiv.org/abs/2511.04691)
*Quentin Auster,Kateryna Shapovalenko,Chuang Ma,Demaio Sun*

Main category: cs.SD

TL;DR: 该研究利用EEG数据和对比CLIP损失训练模型，以将脑活动解码为语音，并引入了三种架构修改来提高性能，其中两种修改有效，表明个性化架构在脑机接口中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络是否可以将脑活动解码为语音，通过将EEG记录映射到音频表示。

Method: 使用EEG数据和对比CLIP损失训练模型，将EEG派生的嵌入与预训练的基于Transformer的语音模型的嵌入对齐。引入了三种架构修改：(i) 个体特定的注意力层，(ii) 个性化的空间注意力，(iii) 双路径RNN与注意力。

Result: 其中两种修改提高了性能，分别提升了0.15%和0.45%的WER，而第三种修改则降低了1.87%的WER。

Conclusion: 该研究展示了个性化架构在脑机接口中的潜力，特别是在脑到语音解码方面。

Abstract: We explore whether neural networks can decode brain activity into speech by
mapping EEG recordings to audio representations. Using EEG data recorded as
subjects listened to natural speech, we train a model with a contrastive CLIP
loss to align EEG-derived embeddings with embeddings from a pre-trained
transformer-based speech model. Building on the state-of-the-art EEG decoder
from Meta, we introduce three architectural modifications: (i) subject-specific
attention layers (+0.15% WER improvement), (ii) personalized spatial attention
(+0.45%), and (iii) a dual-path RNN with attention (-1.87%). Two of the three
modifications improved performance, highlighting the promise of personalized
architectures for brain-to-speech decoding and applications in brain-computer
interfaces.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [46] [ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property](https://arxiv.org/abs/2511.04956)
*Maria Mahbub,Vanessa Lama,Sanjay Das,Brian Starks,Christopher Polchek,Saffell Silvers,Lauren Deck,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: ORCHID is a modular agentic system that improves HRP classification accuracy and traceability by combining RAG with human oversight, allowing for auditable decisions and deferring uncertain items to SMEs.


<details>
  <summary>Details</summary>
Motivation: Traditional expert-only workflows for HRP classification are time-consuming, backlog-prone, and struggle to keep pace with shifting regulatory boundaries. There is a need for a system that can track evolving rules designated by various export control policies to make transparent and auditable decisions.

Method: ORCHID is a modular agentic system that pairs retrieval-augmented generation (RAG) with human oversight to produce policy-based outputs that can be audited. It uses small cooperating agents, retrieval, description refiner, classifier, validator, and feedback logger, which coordinate via agent-to-agent messaging and invoke tools through the Model Context Protocol (MCP).

Result: In preliminary tests on real HRP cases, ORCHID improves accuracy and traceability over a non-agentic baseline while deferring uncertain items to SMEs. The demonstration shows single item submission, grounded citations, SME feedback capture, and exportable audit artifacts.

Conclusion: ORCHID provides a practical path to trustworthy LLM assistance in sensitive DOE compliance workflows by improving accuracy and traceability while deferring uncertain items to SMEs.

Abstract: High-Risk Property (HRP) classification is critical at U.S. Department of
Energy (DOE) sites, where inventories include sensitive and often dual-use
equipment. Compliance must track evolving rules designated by various export
control policies to make transparent and auditable decisions. Traditional
expert-only workflows are time-consuming, backlog-prone, and struggle to keep
pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic
system for HRP classification that pairs retrieval-augmented generation (RAG)
with human oversight to produce policy-based outputs that can be audited. Small
cooperating agents, retrieval, description refiner, classifier, validator, and
feedback logger, coordinate via agent-to-agent messaging and invoke tools
through the Model Context Protocol (MCP) for model-agnostic on-premise
operation. The interface follows an Item to Evidence to Decision loop with
step-by-step reasoning, on-policy citations, and append-only audit bundles
(run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID
improves accuracy and traceability over a non-agentic baseline while deferring
uncertain items to Subject Matter Experts (SMEs). The demonstration shows
single item submission, grounded citations, SME feedback capture, and
exportable audit artifacts, illustrating a practical path to trustworthy LLM
assistance in sensitive DOE compliance workflows.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings](https://arxiv.org/abs/2511.05017)
*Aakriti Agrawal,Gouthaman KV,Rohith Aralikatti,Gauri Jagatap,Jiaxin Yuan,Vijay Kamarshi,Andrea Fanelli,Furong Huang*

Main category: cs.CV

TL;DR: 本文指出现有LVLM架构存在语言模态偏向的问题，并提出一种简单有效的方法来改进文本嵌入，从而减轻幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM架构在语言模态上存在偏向，这主要是由于将视觉嵌入简单地附加到输入文本序列中的常见做法。这种偏向导致了幻觉问题。

Method: 本文提出了一种简单有效的方法，通过将平均池化的视觉特征整合到文本嵌入中，以改进视觉定位并减少幻觉。

Result: 该方法在基准测试中显著改善了视觉定位并减少了幻觉。

Conclusion: 本文指出现有LVLM架构存在语言模态偏向的问题，并提出一种简单有效的方法来改进文本嵌入，从而减轻幻觉问题。虽然平均池化是一种有效的方法，但作者认为更复杂的融合策略可以进一步提升视觉定位和跨模态对齐效果。

Abstract: In this work, we identify an inherent bias in prevailing LVLM architectures
toward the language modality, largely resulting from the common practice of
simply appending visual embeddings to the input text sequence. To address this,
we propose a simple yet effective method that refines textual embeddings by
integrating average-pooled visual features. Our approach demonstrably improves
visual grounding and significantly reduces hallucinations on established
benchmarks. While average pooling offers a straightforward, robust, and
efficient means of incorporating visual information, we believe that more
sophisticated fusion methods could further enhance visual grounding and
cross-modal alignment. Given that the primary focus of this work is to
highlight the modality imbalance and its impact on hallucinations -- and to
show that refining textual embeddings with visual information mitigates this
issue -- we leave exploration of advanced fusion strategies for future work.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [48] [Language Generation and Identification From Partial Enumeration: Tight Density Bounds and Topological Characterizations](https://arxiv.org/abs/2511.05295)
*Jon Kleinberg,Fan Wei*

Main category: cs.DS

TL;DR: 本文研究了语言生成在极限框架下的理论，证明了最佳可达到的下界密度为1/2，并探讨了在部分枚举情况下的生成可能性。此外，还重新审视了经典的Gold-Angluin语言识别模型，并给出了一个新的拓扑公式化表述。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的成功激发了对语言生成和学习的形式理论的研究。本文旨在解决语言生成在极限框架下的一个主要开放问题，并探讨在部分枚举情况下的生成可能性。

Method: 本文研究了语言生成在极限框架下的理论，分析了对抗者枚举未知语言K的情况，并探讨了在部分枚举情况下生成的可能性。此外，还重新审视了经典的Gold-Angluin语言识别模型，并通过拓扑空间的性质进行了分析。

Result: 本文证明了在语言生成在极限框架下，最佳可达到的下界密度为1/2。同时，在部分枚举情况下，生成在极限下仍然可行，算法的输出密度至少为揭示子集密度的一半。此外，本文还给出了一个新的拓扑公式化表述，用于描述Gold-Angluin语言识别模型的条件。

Conclusion: 本文解决了语言生成在极限框架下的一个主要开放问题，证明了最佳可达到的下界密度为1/2。此外，当允许部分枚举时，生成在极限下仍然可行，并且算法的输出密度至少为揭示子集密度的一半。最后，本文重新审视了经典的Gold-Angluin语言识别模型，并给出了一个新的拓扑公式化表述。

Abstract: The success of large language models (LLMs) has motivated formal theories of
language generation and learning. We study the framework of \emph{language
generation in the limit}, where an adversary enumerates strings from an unknown
language $K$ drawn from a countable class, and an algorithm must generate
unseen strings from $K$. Prior work showed that generation is always possible,
and that some algorithms achieve positive lower density, revealing a
\emph{validity--breadth} trade-off between correctness and coverage. We resolve
a main open question in this line, proving a tight bound of $1/2$ on the best
achievable lower density. We then strengthen the model to allow \emph{partial
enumeration}, where the adversary reveals only an infinite subset $C \subseteq
K$. We show that generation in the limit remains achievable, and if $C$ has
lower density $\alpha$ in $K$, the algorithm's output achieves density at least
$\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the
partial-information setting, where the generator must recover within a factor
$1/2$ of the revealed subset's density. We further revisit the classical
Gold--Angluin model of \emph{language identification} under partial
enumeration. We characterize when identification in the limit is possible --
when hypotheses $M_t$ eventually satisfy $C \subseteq M \subseteq K$ -- and in
the process give a new topological formulation of Angluin's characterization,
showing that her condition is precisely equivalent to an appropriate
topological space having the $T_D$ separation property.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [49] [Automatización de Informes Geotécnicos para Macizos Rocosos con IA](https://arxiv.org/abs/2511.04690)
*Christofer Valencia,Alexis Llumigusín,Silvia Alvarez,Abrahan Arias,Christian Mejia-Escobar*

Main category: cs.MM

TL;DR: 本文提出了一种基于人工智能的自动生成地质报告的方法，通过处理图像和现场数据，提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法耗时、容易出错且主观，因此提出使用人工智能技术自动生成报告，以提高效率和准确性。

Method: 通过收集岩层露头和手动样本的照片及其描述，以及在地质技术研究课程中准备的报告，定义了报告大纲、提示工程，并验证了多模态大语言模型（MLLM）的响应。通过迭代优化提示，获得了每个报告部分的结构化和具体指令。

Result: 系统评估显示BLEU和ROUGE-L指标分别为0.455和0.653，表明自动描述与专家的描述相当。

Conclusion: 该工具通过网页访问，具有直观的界面和导出到标准格式的能力，代表了对现场地质专业人员和学生的创新和重要贡献。

Abstract: Geotechnical reports are crucial for assessing the stability of rock
formations and ensuring safety in modern engineering. Traditionally, these
reports are prepared manually based on field observations using compasses,
magnifying glasses, and notebooks. This method is slow, prone to errors, and
subjective in its interpretations. To overcome these limitations, the use of
artificial intelligence techniques is proposed for the automatic generation of
reports through the processing of images and field data. The methodology was
based on the collection of photographs of rock outcrops and manual samples with
their respective descriptions, as well as on the reports prepared during the
Geotechnical Studies course. These resources were used to define the report
outline, prompt engineering, and validate the responses of a multimodal large
language model (MLLM). The iterative refinement of prompts until structured and
specific instructions were obtained for each section of the report proved to be
an effective alternative to the costly process of fine-tuning the MLLM. The
system evaluation establishes values of 0.455 and 0.653 for the BLEU and
ROUGE-L metrics, respectively, suggesting that automatic descriptions are
comparable to those made by experts. This tool, accessible via the web, with an
intuitive interface and the ability to export to standardized formats,
represents an innovation and an important contribution for professionals and
students of field geology.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [50] [Jailbreaking in the Haystack](https://arxiv.org/abs/2511.04707)
*Rishi Rajesh Shah,Chen Henry Wu,Shashwat Saxena,Ziqian Zhong,Alexander Robey,Aditi Raghunathan*

Main category: cs.CR

TL;DR: 本文介绍了NINJA方法，该方法通过在有害用户目标中添加良性内容来越狱对齐的语言模型，并展示了长上下文可能引入的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 为了填补这一空白，我们引入了NINJA方法，以研究长上下文对语言模型安全性的影响。

Method: 引入NINJA（Needle-in-haystack jailbreak attack），通过附加良性、模型生成的内容来越狱对齐的语言模型。

Result: 实验表明，NINJA显著提高了跨最先进的开放和专有模型的攻击成功率，包括LLaMA、Qwen、Mistral和Gemini。此外，NINJA是低资源、可转移且不易被检测到的。

Conclusion: 即使良性长上下文——当通过精心定位目标进行设计时——也会在现代语言模型中引入根本性的漏洞。

Abstract: Recent advances in long-context language models (LMs) have enabled
million-token inputs, expanding their capabilities across complex tasks like
computer-use agents. Yet, the safety implications of these extended contexts
remain unclear. To bridge this gap, we introduce NINJA (short for
Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by
appending benign, model-generated content to harmful user goals. Critical to
our method is the observation that the position of harmful goals play an
important role in safety. Experiments on standard safety benchmark, HarmBench,
show that NINJA significantly increases attack success rates across
state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral,
and Gemini. Unlike prior jailbreaking methods, our approach is low-resource,
transferable, and less detectable. Moreover, we show that NINJA is
compute-optimal -- under a fixed compute budget, increasing context length can
outperform increasing the number of trials in best-of-N jailbreak. These
findings reveal that even benign long contexts -- when crafted with careful
goal positioning -- introduce fundamental vulnerabilities in modern LMs.

</details>


### [51] [ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations](https://arxiv.org/abs/2511.05359)
*Amr Gomaa,Ahmed Salem,Sahar Abdelnabi*

Main category: cs.CR

TL;DR: 本文介绍了 ConVerse，这是一个动态基准，用于评估代理-代理交互中的隐私和安全风险。通过将隐私和安全统一到交互式多智能体环境中，ConVerse 将安全性重新定义为通信的涌现属性。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型演变为代表用户行动和沟通的自主代理，确保多代理生态系统中的安全性成为核心挑战。个人助理与外部服务提供商之间的互动暴露了效用与保护之间的核心矛盾：有效的协作需要信息共享，但每次交流都会创建新的攻击面。

Method: 引入了 ConVerse，这是一个动态基准，用于评估代理-代理交互中的隐私和安全风险。

Result: 评估七个最先进的模型揭示了持续的漏洞；隐私攻击在多达 88% 的情况下成功，安全漏洞在多达 60% 的情况下发生，更强的模型泄露更多。

Conclusion: 通过将隐私和安全统一到交互式多智能体环境中，ConVerse 将安全性重新定义为通信的涌现属性。

Abstract: As language models evolve into autonomous agents that act and communicate on
behalf of users, ensuring safety in multi-agent ecosystems becomes a central
challenge. Interactions between personal assistants and external service
providers expose a core tension between utility and protection: effective
collaboration requires information sharing, yet every exchange creates new
attack surfaces. We introduce ConVerse, a dynamic benchmark for evaluating
privacy and security risks in agent-agent interactions. ConVerse spans three
practical domains (travel, real estate, insurance) with 12 user personas and
over 864 contextually grounded attacks (611 privacy, 253 security). Unlike
prior single-agent settings, it models autonomous, multi-turn agent-to-agent
conversations where malicious requests are embedded within plausible discourse.
Privacy is tested through a three-tier taxonomy assessing abstraction quality,
while security attacks target tool use and preference manipulation. Evaluating
seven state-of-the-art models reveals persistent vulnerabilities; privacy
attacks succeed in up to 88% of cases and security breaches in up to 60%, with
stronger models leaking more. By unifying privacy and security within
interactive multi-agent contexts, ConVerse reframes safety as an emergent
property of communication.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity](https://arxiv.org/abs/2511.04686)
*Pratik Poudel*

Main category: cs.LG

TL;DR: 本文研究了KV缓存管理对大型语言模型生成质量的影响，发现位置编码的完整性至关重要，并提出了应尊重架构限制、保持位置结构的缓存管理方法。


<details>
  <summary>Details</summary>
Motivation: KV缓存的无限制增长在多轮对话场景中带来了重大挑战，而现有的缓存管理策略可能破坏位置编码的一致性，影响生成质量。

Method: 通过使用状态感知的基准测试框架进行实证分析，研究了KV缓存管理策略对大型语言模型生成质量的影响。

Result: 当累积的KV缓存接近或超过模型训练的上下文窗口时，生成质量会显著下降。高保留率的驱逐策略（如AttentionTop）如果破坏位置一致性，也可能导致性能下降。

Conclusion: 本文提出，KV缓存管理策略需要考虑模型的架构限制和位置编码的完整性，以避免生成质量下降。

Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in
large language models (LLMs), yet its unbounded growth in stateful multi-turn
scenarios presents major challenges. This paper examines the interplay between
KV cache management strategies, the architectural context limits of models like
meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of
positional encodings. Through empirical analysis using a stateful benchmarking
framework, we show that LLM generation quality degrades sharply when the
accumulated KV cache approaches or exceeds the model's trained context window
(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory
exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via
AttentionTop), can worsen performance if they disrupt positional coherence.
Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a
cache by removing non-contiguous tokens can scramble these signals and lead to
degenerative outputs. We further show that simple strategies preserving
contiguous context blocks (e.g., keeping an initial "gist") can yield more
coherent generations than complex or positionally disruptive ones. We advocate
for eviction techniques that respect architectural limits, preserve positional
structure, and view "cache health" holistically beyond mere size.

</details>


### [53] [APP: Accelerated Path Patching with Task-Specific Pruning](https://arxiv.org/abs/2511.05442)
*Frauke Andersen,William Rudman,Ruochen Zhang,Carsten Eickhoff*

Main category: cs.LG

TL;DR: APP is a faster and more efficient method for circuit discovery in neural networks.


<details>
  <summary>Details</summary>
Motivation: Current circuit discovery methods are computationally expensive and lack in-depth analysis for smaller models.

Method: APP is a hybrid approach that combines Contrastive-FLAP pruning with traditional Path Patching to reduce the search space for circuit discovery.

Result: APP reduces the search space by 56% on average and speeds up Path Patching by 59.63%-93.27% compared to dense models.

Conclusion: APP provides substantial computational savings while maintaining circuit performance comparable to existing methods.

Abstract: Circuit discovery is a key step in many mechanistic interpretability
pipelines. Current methods, such as Path Patching, are computationally
expensive and have limited in-depth circuit analysis for smaller models. In
this study, we propose Accelerated Path Patching (APP), a hybrid approach
leveraging our novel contrastive attention head pruning method to drastically
reduce the search space of circuit discovery methods. Our Contrastive-FLAP
pruning algorithm uses techniques from causal mediation analysis to assign
higher pruning scores to task-specific attention heads, leading to higher
performing sparse models compared to traditional pruning techniques. Although
Contrastive-FLAP is successful at preserving task-specific heads that existing
pruning algorithms remove at low sparsity ratios, the circuits found by
Contrastive-FLAP alone are too large to satisfy the minimality constraint
required in circuit analysis. APP first applies Contrastive-FLAP to reduce the
search space on required for circuit discovery algorithms by, on average, 56\%.
Next, APP, applies traditional Path Patching on the remaining attention heads,
leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to
the dense model. Despite the substantial computational saving that APP
provides, circuits obtained from APP exhibit substantial overlap and similar
performance to previously established Path Patching circuits

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [54] [Simulating Misinformation Vulnerabilities With Agent Personas](https://arxiv.org/abs/2511.04697)
*David Farr,Lynnette Hui Xian Ng,Stephen Prochaska,Iain J. Cruickshank,Jevin West*

Main category: cs.SI

TL;DR: 本研究利用大型语言模型构建基于代理的模拟，以研究不同人群对错误信息的反应，结果表明心理模式比职业背景更能影响对错误信息的解释。


<details>
  <summary>Details</summary>
Motivation: 理解不同人口对信息的反应对于设计有效的干预措施至关重要，但现实世界的实验是不切实际且在伦理上具有挑战性的。

Method: 我们开发了一个基于代理的模拟，使用大型语言模型（LLMs）来建模对错误信息的反应。我们构建了涵盖五个职业和三种心理模式的代理人格，并评估了它们对新闻标题的反应。

Result: 我们的研究结果表明，LLM生成的代理与真实标签和人类预测高度一致，支持其作为研究信息反应的代理的使用。我们还发现，心理模式比职业背景更能影响代理对错误信息的解释。

Conclusion: 本研究验证了大型语言模型可以作为代理，用于信息网络的基于代理的模型，以分析复杂社会系统中的信任、极化和对欺骗性内容的易感性。

Abstract: Disinformation campaigns can distort public perception and destabilize
institutions. Understanding how different populations respond to information is
crucial for designing effective interventions, yet real-world experimentation
is impractical and ethically challenging. To address this, we develop an
agent-based simulation using Large Language Models (LLMs) to model responses to
misinformation. We construct agent personas spanning five professions and three
mental schemas, and evaluate their reactions to news headlines. Our findings
show that LLM-generated agents align closely with ground-truth labels and human
predictions, supporting their use as proxies for studying information
responses. We also find that mental schemas, more than professional background,
influence how agents interpret misinformation. This work provides a validation
of LLMs to be used as agents in an agent-based model of an information network
for analyzing trust, polarization, and susceptibility to deceptive content in
complex social systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [55] [Association via Entropy Reduction](https://arxiv.org/abs/2511.04901)
*Anthony Gamst,Lawrence Wilson*

Main category: cs.IR

TL;DR: The paper introduces a new score called aver, which outperforms tf-idf in finding associated documents and has several advantages, but is more complex to compute and interpret.


<details>
  <summary>Details</summary>
Motivation: The paper aims to provide an alternative to tf-idf for identifying associated documents, particularly in large graphs where neural networks may not be the best choice.

Method: The paper introduces a new score called aver and compares it with tf-idf on a dataset with ground truth marking for association.

Result: aver performs better than tf-idf at finding associated pairs, has a natural threshold for declaring unassociated pairs, can distinguish between pairs with tf-idf scores of 1.0, can be applied to larger collections of documents, and is derived from entropy under a simple statistical model.

Conclusion: aver may be more 'natural' than tf-idf and has several advantages, but it is more complex to compute and interpret.

Abstract: Prior to recent successes using neural networks, term frequency-inverse
document frequency (tf-idf) was clearly regarded as the best choice for
identifying documents related to a query. We provide a different score, aver,
and observe, on a dataset with ground truth marking for association, that aver
does do better at finding assciated pairs than tf-idf. This example involves
finding associated vertices in a large graph and that may be an area where
neural networks are not currently an obvious best choice. Beyond this one
anecdote, we observe that (1) aver has a natural threshold for declaring pairs
as unassociated while tf-idf does not, (2) aver can distinguish between pairs
of documents for which tf-idf gives a score of 1.0, (3) aver can be applied to
larger collections of documents than pairs while tf-idf cannot, and (4) that
aver is derived from entropy under a simple statistical model while tf-idf is a
construction designed to achieve a certain goal and hence aver may be more
"natural." To be fair, we also observe that (1) writing down and computing the
aver score for a pair is more complex than for tf-idf and (2) that the fact
that the aver score is naturally scale-free makes it more complicated to
interpret aver scores.

</details>


### [56] [Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR](https://arxiv.org/abs/2511.05079)
*Grigory Kovalev,Natalia Loukachevitch,Mikhail Tikhomirov,Olga Babina,Pavel Mamaev*

Main category: cs.IR

TL;DR: 本文构建了一个新的俄语信息检索数据集，支持多种检索任务，并通过实验比较了不同模型的性能，展示了词汇方法和神经方法在不同场景下的优势。


<details>
  <summary>Details</summary>
Motivation: 本文旨在构建一个新颖的俄语信息检索数据集，支持包括事实核查、检索增强生成和全文档检索在内的多种检索任务。

Method: 我们描述了用于数据集创建的方法，该方法能够扩展现有的俄语信息检索（IR）资源。通过广泛的实验，我们比较了基于词汇的检索模型（如BM25）与针对俄语微调的最先进的神经架构以及多语言模型。

Result: 实验结果表明，基于词汇的方法在全文档检索中表现优于神经模型，而神经方法在较短文本（如事实核查或细粒度检索）中更好地捕捉词汇语义。使用我们新创建的数据集，我们还分析了文档长度对检索性能的影响，并证明结合检索与神经重排序可以持续提高结果。

Conclusion: 我们的贡献扩展了可用于俄语信息检索研究的资源，并强调了准确评估检索模型以实现最佳性能的重要性。所有数据集均可在HuggingFace上公开获取。为了促进可重复性和未来研究，我们还将在GitHub上发布完整实现。

Abstract: In this paper, we present a novel series of Russian information retrieval
datasets constructed from the "Did you know..." section of Russian Wikipedia.
Our datasets support a range of retrieval tasks, including fact-checking,
retrieval-augmented generation, and full-document retrieval, by leveraging
interesting facts and their referenced Wikipedia articles annotated at the
sentence level with graded relevance. We describe the methodology for dataset
creation that enables the expansion of existing Russian Information Retrieval
(IR) resources. Through extensive experiments, we extend the RusBEIR research
by comparing lexical retrieval models, such as BM25, with state-of-the-art
neural architectures fine-tuned for Russian, as well as multilingual models.
Results of our experiments show that lexical methods tend to outperform neural
models on full-document retrieval, while neural approaches better capture
lexical semantics in shorter texts, such as in fact-checking or fine-grained
retrieval. Using our newly created datasets, we also analyze the impact of
document length on retrieval performance and demonstrate that combining
retrieval with neural reranking consistently improves results. Our contribution
expands the resources available for Russian information retrieval research and
highlights the importance of accurate evaluation of retrieval models to achieve
optimal performance. All datasets are publicly available at HuggingFace. To
facilitate reproducibility and future research, we also release the full
implementation on GitHub.

</details>


### [57] [QUESTER: Query Specification for Generative Retrieval](https://arxiv.org/abs/2511.05301)
*Arthur Satouf,Yuxuan Zong,Habiboulaye Amadou-Boubacar,Pablo Piantanida,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: 本文提出了一种名为QUESTER的生成检索方法，通过强化学习技术训练小规模语言模型来生成查询规范，从而在保持效率的同时提高检索效果。


<details>
  <summary>Details</summary>
Motivation: 生成检索（GR）在泛化能力和扩展成本方面存在挑战，因此需要一种更有效的解决方案。

Method: 我们引入了QUESTER，它将生成检索重新定义为查询规范生成，使用一个小的LLM处理简单的关键词查询，并通过强化学习技术（GRPO）进行训练。

Result: 在域内和域外评估中，我们的模型表现优于BM25，并且与神经信息检索模型相当。

Conclusion: 我们的模型在有效性和效率方面优于BM25，并且与神经信息检索模型具有竞争力。

Abstract: Generative Retrieval (GR) differs from the traditional index-then-retrieve
pipeline by storing relevance in model parameters and directly generating
document identifiers. However, GR often struggles to generalize and is costly
to scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),
which reframes GR as query specification generation - in this work, a simple
keyword query handled by BM25 - using a (small) LLM. The policy is trained
using reinforcement learning techniques (GRPO). Across in- and out-of-domain
evaluations, we show that our model is more effective than BM25, and
competitive with neural IR models, while maintaining a good efficiency

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [58] [Enhancing Public Speaking Skills in Engineering Students Through AI](https://arxiv.org/abs/2511.04995)
*Amol Harsh,Brainerd Prince,Siddharth Siddharth,Deepan Raj Prabakar Muthirayan,Kabir S Bhalla,Esraaj Sarkar Gupta,Siddharth Sahu*

Main category: cs.HC

TL;DR: 本研究旨在通过开发一个基于人工智能的多模态系统来解决工程学生在公共演讲方面的沟通问题。该系统结合了语音分析、计算机视觉和情感检测，提供个性化的反馈。初步测试显示，AI生成的反馈与专家评估有一定的一致性，其中Gemini Pro表现最佳。该系统可以帮助学生反复练习，提高他们的沟通能力。


<details>
  <summary>Details</summary>
Motivation: 工程学生在有效沟通方面存在持续的挑战。公共演讲是未来工程师必须掌握的技能，因为他们需要与不同的利益相关者交流技术知识。虽然大学提供了课程或研讨会，但无法为学生提供持续和个性化的培训。提供全面的反馈在时间和资源上都很耗时，使得一致和个性化的评估变得不切实际。

Method: 本研究结合了语音分析、计算机视觉和情感检测，开发了一个多模态AI系统，提供评估和反馈。该模型评估（1）口头交流（音调、响度、节奏、语调），（2）非语言交流（面部表情、手势、姿势），以及（3）表达一致性，这是一种新的整合方式，确保言语和身体语言的一致性。

Result: 初步测试表明，我们的AI生成的反馈与专家评估有一定的吻合度。在评估的最先进的AI模型中，所有模型都是大型语言模型（LLMs），包括Gemini和OpenAI模型，其中Gemini Pro表现最佳，与人类标注者有最强的一致性。

Conclusion: 通过消除对人类评估者的依赖，这种基于人工智能的演讲训练器使学生能够反复练习，帮助他们自然地将言语与肢体语言和情感对齐，这对于有影响力和专业的沟通至关重要。

Abstract: This research-to-practice full paper was inspired by the persistent challenge
in effective communication among engineering students. Public speaking is a
necessary skill for future engineers as they have to communicate technical
knowledge with diverse stakeholders. While universities offer courses or
workshops, they are unable to offer sustained and personalized training to
students. Providing comprehensive feedback on both verbal and non-verbal
aspects of public speaking is time-intensive, making consistent and
individualized assessment impractical. This study integrates research on verbal
and non-verbal cues in public speaking to develop an AI-driven assessment model
for engineering students. Our approach combines speech analysis, computer
vision, and sentiment detection into a multi-modal AI system that provides
assessment and feedback. The model evaluates (1) verbal communication (pitch,
loudness, pacing, intonation), (2) non-verbal communication (facial
expressions, gestures, posture), and (3) expressive coherence, a novel
integration ensuring alignment between speech and body language. Unlike
previous systems that assess these aspects separately, our model fuses multiple
modalities to deliver personalized, scalable feedback. Preliminary testing
demonstrated that our AI-generated feedback was moderately aligned with expert
evaluations. Among the state-of-the-art AI models evaluated, all of which were
Large Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro
emerged as the best-performing, showing the strongest agreement with human
annotators. By eliminating reliance on human evaluators, this AI-driven public
speaking trainer enables repeated practice, helping students naturally align
their speech with body language and emotion, crucial for impactful and
professional communication.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [59] [Quantifying the Climate Risk of Generative AI: Region-Aware Carbon Accounting with G-TRACE and the AI Sustainability Pyramid](https://arxiv.org/abs/2511.04776)
*Zahida Kausar,Seemab Latif,Raja Khurrum Shahzad,Mehwish Fatima*

Main category: cs.CY

TL;DR: 本研究引入了G-TRACE，用于量化GenAI的能源消耗和碳排放，并通过吉卜力风格的图像生成趋势展示了其对环境的影响。同时，提出了AI可持续发展金字塔，以提供可持续AI部署的政策指导。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GenAI）代表了一个快速扩展的数字基础设施，其能源需求和相关的CO2排放量正在成为一种新的气候风险。因此，需要一种方法来量化GenAI的环境影响，并提出可持续的治理模型。

Method: 本研究引入了G-TRACE（GenAI Transformative Carbon Estimator），这是一个跨模态、区域感知的框架，用于量化训练和推理相关的排放量，跨越不同的模态和部署地理区域。使用真实世界分析和微观模拟，G-TRACE测量每种输出类型（文本、图像、视频）的能源使用和碳强度，并揭示了分散式推理如何将小的每次查询能源成本转化为系统级影响。

Result: 通过吉卜力风格的图像生成趋势（2024-2025），我们估计了4,309 MWh的能耗和2,068吨CO2排放量，说明了病毒式参与如何将个体的数字行为放大为吨级的影响。基于这些发现，我们提出了AI可持续发展金字塔，这是一种七级治理模型，将碳核算指标（L1-L7）与运营准备度、优化和管理联系起来。

Conclusion: 本研究为新兴数字基础设施作为新的气候风险类别提供了定量评估，支持适应性治理以实现可持续技术部署。通过将GenAI置于气候风险框架中，工作推进了数据驱动的方法，以使技术创新与全球脱碳和韧性目标保持一致。

Abstract: Generative Artificial Intelligence (GenAI) represents a rapidly expanding
digital infrastructure whose energy demand and associated CO2 emissions are
emerging as a new category of climate risk. This study introduces G-TRACE
(GenAI Transformative Carbon Estimator), a cross-modal, region-aware framework
that quantifies training- and inference-related emissions across modalities and
deployment geographies. Using real-world analytics and microscopic simulation,
G-TRACE measures energy use and carbon intensity per output type (text, image,
video) and reveals how decentralized inference amplifies small per-query energy
costs into system-level impacts. Through the Ghibli-style image generation
trend (2024-2025), we estimate 4,309 MWh of energy consumption and 2,068 tCO2
emissions, illustrating how viral participation inflates individual digital
actions into tonne-scale consequences. Building on these findings, we propose
the AI Sustainability Pyramid, a seven-level governance model linking carbon
accounting metrics (L1-L7) with operational readiness, optimization, and
stewardship. This framework translates quantitative emission metrics into
actionable policy guidance for sustainable AI deployment. The study contributes
to the quantitative assessment of emerging digital infrastructures as a novel
category of climate risk, supporting adaptive governance for sustainable
technology deployment. By situating GenAI within climate-risk frameworks, the
work advances data-driven methods for aligning technological innovation with
global decarbonization and resilience objectives.

</details>

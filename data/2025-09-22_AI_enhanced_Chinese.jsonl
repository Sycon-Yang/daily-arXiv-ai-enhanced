{"id": "2509.15248", "pdf": "https://arxiv.org/pdf/2509.15248", "abs": "https://arxiv.org/abs/2509.15248", "authors": ["Zitong Yang", "Aonan Zhang", "Hong Liu", "Tatsunori Hashimoto", "Emmanuel Cand\u00e8s", "Chong Wang", "Ruoming Pang"], "title": "Synthetic bootstrapped pretraining", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)\npretraining procedure that first learns a model of relations between documents\nfrom the pretraining dataset and then leverages it to synthesize a vast new\ncorpus for joint training. While the standard pretraining teaches LMs to learn\ncausal correlations among tokens within a single document, it is not designed\nto efficiently model the rich, learnable inter-document correlations that can\npotentially lead to better performance. We validate SBP by designing a\ncompute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T\ntokens from scratch. We find SBP consistently improves upon a strong repetition\nbaseline and delivers a significant fraction of performance improvement\nattainable by an oracle upper bound with access to 20x more unique data.\nQualitative analysis reveals that the synthesized documents go beyond mere\nparaphrases -- SBP first abstracts a core concept from the seed material and\nthen crafts a new narration on top of it. Besides strong empirical performance,\nSBP admits a natural Bayesian interpretation: the synthesizer implicitly learns\nto abstract the latent concepts shared between related documents.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86 Synthetic Bootstrapped Pretraining (SBP)\uff0c\u8fd9\u662f\u4e00\u79cd\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b83\u9996\u5148\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u6587\u6863\u4e4b\u95f4\u7684\u5173\u7cfb\u6a21\u578b\uff0c\u7136\u540e\u5229\u7528\u8be5\u6a21\u578b\u5408\u6210\u4e00\u4e2a\u5e9e\u5927\u7684\u65b0\u8bed\u6599\u5e93\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u3002SBP \u5728\u8ba1\u7b97\u5339\u914d\u7684\u9884\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u5728\u591a\u8fbe 1T tokens \u7684\u6570\u636e\u4e0a\u4ece\u5934\u5f00\u59cb\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a 3B \u53c2\u6570\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\uff0cSBP \u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u91cd\u590d\u57fa\u7ebf\uff0c\u5e76\u5b9e\u73b0\u4e86\u53ef\u7531\u8bbf\u95ee 20 \u500d\u66f4\u591a\u72ec\u7279\u6570\u636e\u7684 oracle \u4e0a\u9650\u83b7\u5f97\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u7684\u4e00\u90e8\u5206\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u5408\u6210\u7684\u6587\u6863\u8d85\u8d8a\u4e86\u5355\u7eaf\u7684\u6539\u5199\u2014\u2014SBP \u9996\u5148\u4ece\u79cd\u5b50\u6750\u6599\u4e2d\u62bd\u8c61\u51fa\u6838\u5fc3\u6982\u5ff5\uff0c\u7136\u540e\u5728\u5176\u57fa\u7840\u4e0a\u6784\u5efa\u65b0\u7684\u53d9\u8ff0\u3002\u6b64\u5916\uff0cSBP \u5177\u6709\u81ea\u7136\u7684\u8d1d\u53f6\u65af\u89e3\u91ca\uff1a\u5408\u6210\u5668\u9690\u5f0f\u5730\u5b66\u4e60\u62bd\u8c61\u76f8\u5173\u6587\u6863\u4e4b\u95f4\u5171\u4eab\u7684\u6f5c\u5728\u6982\u5ff5\u3002", "motivation": "Standard pretraining teaches LMs to learn causal correlations among tokens within a single document, but it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance.", "method": "Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training.", "result": "SBP consistently improves upon a strong repetition baseline and delivers a significant fraction of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases -- SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it.", "conclusion": "SBP consistently improves upon a strong repetition baseline and delivers a significant fraction of performance improvement attainable by an oracle upper bound with access to 20x more unique data. SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents."}}
{"id": "2509.15255", "pdf": "https://arxiv.org/pdf/2509.15255", "abs": "https://arxiv.org/abs/2509.15255", "authors": ["Tandin Wangchuk", "Tad Gonsalves"], "title": "Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha", "categories": ["cs.CL"], "comment": "10 Pages", "summary": "Large Language Models (LLMs) are gaining popularity and improving rapidly.\nTokenizers are crucial components of natural language processing, especially\nfor LLMs. Tokenizers break down input text into tokens that models can easily\nprocess while ensuring the text is accurately represented, capturing its\nmeaning and structure. Effective tokenizers enhance the capabilities of LLMs by\nimproving a model's understanding of context and semantics, ultimately leading\nto better performance in various downstream tasks, such as translation,\nclassification, sentiment analysis, and text generation. Most pre-trained\ntokenizers are suitable for high-resource languages like English but perform\npoorly for low-resource languages. Dzongkha, Bhutan's national language spoken\nby around seven hundred thousand people, is a low-resource language, and its\nlinguistic complexity poses unique NLP challenges. Despite some progress,\nsignificant research in Dzongkha NLP is lacking, particularly in tokenization.\nThis study evaluates the training and performance of three common tokenization\nalgorithms in comparison to other popular methods. Specifically, Byte-Pair\nEncoding (BPE), WordPiece, and SentencePiece (Unigram) were evaluated for their\nsuitability for Dzongkha. Performance was assessed using metrics like Subword\nFertility, Proportion of Continued Words, Normalized Sequence Length, and\nexecution time. The results show that while all three algorithms demonstrate\npotential, SentencePiece is the most effective for Dzongkha tokenization,\npaving the way for further NLP advancements. This underscores the need for\ntailored approaches for low-resource languages and ongoing research. In this\nstudy, we presented three tokenization algorithms for Dzongkha, paving the way\nfor building Dzongkha Large Language Models.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5206\u8bcd\u7b97\u6cd5\u5728Dzongkha\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0SentencePiece\u6700\u4e3a\u6709\u6548\uff0c\u4e3a\u6784\u5efaDzongkha\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "Dzongkha\u662f\u4e00\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5176\u8bed\u8a00\u590d\u6742\u6027\u5e26\u6765\u4e86\u72ec\u7279\u7684NLP\u6311\u6218\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u4e00\u4e9b\u8fdb\u5c55\uff0c\u4f46\u5173\u4e8eDzongkha NLP\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5206\u8bcd\u65b9\u9762\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u5206\u8bcd\u7b97\u6cd5\u5728Dzongkha\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5e38\u89c1\u7684\u5206\u8bcd\u7b97\u6cd5\uff08Byte-Pair Encoding\u3001WordPiece\u548cSentencePiece\uff09\u5728Dzongkha\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u5b50\u8bcd\u5bc6\u5ea6\u3001\u8fde\u7eed\u5355\u8bcd\u6bd4\u4f8b\u3001\u5f52\u4e00\u5316\u5e8f\u5217\u957f\u5ea6\u548c\u6267\u884c\u65f6\u95f4\u7b49\u6307\u6807\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u6240\u6709\u4e09\u79cd\u7b97\u6cd5\u90fd\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46SentencePiece\u5728Dzongkha\u5206\u8bcd\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u663e\u793a\uff0cSentencePiece\u5728Dzongkha\u5206\u8bcd\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u6784\u5efaDzongkha\u5927\u8bed\u8a00\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002\u8fd9\u5f3a\u8c03\u4e86\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5b9a\u5236\u65b9\u6cd5\u548c\u6301\u7eed\u7814\u7a76\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.15260", "pdf": "https://arxiv.org/pdf/2509.15260", "abs": "https://arxiv.org/abs/2509.15260", "authors": ["Yujia Hu", "Ming Shan Hee", "Preslav Nakov", "Roy Ka-Wei Lee"], "title": "Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages", "categories": ["cs.CL"], "comment": "9 pages, EMNLP 2025", "summary": "The advancement of Large Language Models (LLMs) has transformed natural\nlanguage processing; however, their safety mechanisms remain under-explored in\nlow-resource, multilingual settings. Here, we aim to bridge this gap. In\nparticular, we introduce \\textsf{SGToxicGuard}, a novel dataset and evaluation\nframework for benchmarking LLM safety in Singapore's diverse linguistic\ncontext, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a\nred-teaming approach to systematically probe LLM vulnerabilities in three\nreal-world scenarios: \\textit{conversation}, \\textit{question-answering}, and\n\\textit{content composition}. We conduct extensive experiments with\nstate-of-the-art multilingual LLMs, and the results uncover critical gaps in\ntheir safety guardrails. By offering actionable insights into cultural\nsensitivity and toxicity mitigation, we lay the foundation for safer and more\ninclusive AI systems in linguistically diverse environments.\\footnote{Link to\nthe dataset: https://github.com/Social-AI-Studio/SGToxicGuard.}\n\\textcolor{red}{Disclaimer: This paper contains sensitive content that may be\ndisturbing to some readers.}", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SGToxicGuard\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u65b0\u52a0\u5761\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u53d1\u73b0\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5b89\u5168\u9632\u62a4\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u5728\u4f4e\u8d44\u6e90\u3001\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e86SGToxicGuard\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65b0\u52a0\u5761\u591a\u8bed\u8a00\u80cc\u666f\u4e0b\u57fa\u51c6\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002\u91c7\u7528\u7ea2\u961f\u65b9\u6cd5\u7cfb\u7edf\u5730\u63a2\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u4e2a\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6f0f\u6d1e\uff1a\u5bf9\u8bdd\u3001\u95ee\u7b54\u548c\u5185\u5bb9\u521b\u4f5c\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u5b89\u5168\u9632\u62a4\u65b9\u9762\u7684\u5173\u952e\u5dee\u8ddd\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u5173\u4e8e\u6587\u5316\u654f\u611f\u6027\u548c\u6bd2\u6027\u7f13\u89e3\u7684\u884c\u52a8\u89c1\u89e3\uff0c\u6211\u4eec\u4e3a\u5728\u8bed\u8a00\u591a\u6837\u73af\u5883\u4e2d\u66f4\u5b89\u5168\u548c\u66f4\u5177\u5305\u5bb9\u6027\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.15335", "pdf": "https://arxiv.org/pdf/2509.15335", "abs": "https://arxiv.org/abs/2509.15335", "authors": ["Charlott Jakob", "David Harbecke", "Patrick Parschan", "Pia Wenzel Neves", "Vera Schmitt"], "title": "PolBiX: Detecting LLMs' Political Bias in Fact-Checking through X-phemisms", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models are increasingly used in applications requiring\nobjective assessment, which could be compromised by political bias. Many\nstudies found preferences for left-leaning positions in LLMs, but downstream\neffects on tasks like fact-checking remain underexplored. In this study, we\nsystematically investigate political bias through exchanging words with\neuphemisms or dysphemisms in German claims. We construct minimal pairs of\nfactually equivalent claims that differ in political connotation, to assess the\nconsistency of LLMs in classifying them as true or false. We evaluate six LLMs\nand find that, more than political leaning, the presence of judgmental words\nsignificantly influences truthfulness assessment. While a few models show\ntendencies of political bias, this is not mitigated by explicitly calling for\nobjectivism in prompts.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u66ff\u6362\u8bcd\u8bed\u6765\u68c0\u67e5LLMs\u5728\u4e8b\u5b9e\u6838\u67e5\u4e2d\u7684\u653f\u6cbb\u504f\u89c1\uff0c\u53d1\u73b0\u5224\u65ad\u6027\u8bcd\u6c47\u6bd4\u653f\u6cbb\u503e\u5411\u66f4\u5f71\u54cd\u7ed3\u679c\u3002", "motivation": "\u8bb8\u591a\u7814\u7a76\u53d1\u73b0LLMs\u503e\u5411\u4e8e\u5de6\u7ffc\u7acb\u573a\uff0c\u4f46\u4e0b\u6e38\u4efb\u52a1\u5982\u4e8b\u5b9e\u6838\u67e5\u7684\u5f71\u54cd\u4ecd\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5c06\u8bcd\u8bed\u66ff\u6362\u4e3a\u59d4\u5a49\u8bed\u6216\u8d2c\u4e49\u8bcd\u6765\u7cfb\u7edf\u5730\u7814\u7a76\u653f\u6cbb\u504f\u89c1\uff0c\u6784\u5efa\u4e8b\u5b9e\u7b49\u4ef7\u4f46\u653f\u6cbb\u542b\u4e49\u4e0d\u540c\u7684\u6700\u5c0f\u5bf9\uff0c\u4ee5\u8bc4\u4f30LLMs\u5728\u5206\u7c7b\u5b83\u4eec\u4e3a\u771f\u6216\u5047\u65f6\u7684\u4e00\u81f4\u6027\u3002", "result": "\u8bc4\u4f30\u4e86\u516d\u4e2aLLMs\uff0c\u53d1\u73b0\u5224\u65ad\u6027\u8bcd\u6c47\u7684\u5b58\u5728\u6bd4\u653f\u6cbb\u503e\u5411\u66f4\u663e\u8457\u5730\u5f71\u54cd\u771f\u5b9e\u6027\u8bc4\u4f30\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u653f\u6cbb\u504f\u89c1\u5728\u67d0\u4e9b\u6a21\u578b\u4e2d\u5b58\u5728\uff0c\u4f46\u5e76\u672a\u56e0\u660e\u786e\u8981\u6c42\u5ba2\u89c2\u6027\u800c\u51cf\u8f7b\u3002"}}
{"id": "2509.15339", "pdf": "https://arxiv.org/pdf/2509.15339", "abs": "https://arxiv.org/abs/2509.15339", "authors": ["Yeongbin Seo", "Dongha Lee", "Jinyoung Yeo"], "title": "Quantifying Self-Awareness of Knowledge in Large Language Models", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Hallucination prediction in large language models (LLMs) is often interpreted\nas a sign of self-awareness. However, we argue that such performance can arise\nfrom question-side shortcuts rather than true model-side introspection. To\ndisentangle these factors, we propose the Approximate Question-side Effect\n(AQE), which quantifies the contribution of question-awareness. Our analysis\nacross multiple datasets reveals that much of the reported success stems from\nexploiting superficial patterns in questions. We further introduce SCAO\n(Semantic Compression by Answering in One word), a method that enhances the use\nof model-side signals. Experiments show that SCAO achieves strong and\nconsistent performance, particularly in settings with reduced question-side\ncues, highlighting its effectiveness in fostering genuine self-awareness in\nLLMs.", "AI": {"tldr": "This paper argues that hallucination prediction in LLMs may stem from question-side shortcuts rather than true self-awareness. It introduces AQE to quantify question-awareness and SCAO to enhance model-side signals, showing that SCAO performs well even when question-side cues are reduced.", "motivation": "Hallucination prediction in large language models (LLMs) is often interpreted as a sign of self-awareness. However, we argue that such performance can arise from question-side shortcuts rather than true model-side introspection.", "method": "We propose the Approximate Question-side Effect (AQE), which quantifies the contribution of question-awareness. We also introduce SCAO (Semantic Compression by Answering in One word), a method that enhances the use of model-side signals.", "result": "Our analysis across multiple datasets reveals that much of the reported success stems from exploiting superficial patterns in questions. Experiments show that SCAO achieves strong and consistent performance.", "conclusion": "SCAO achieves strong and consistent performance, particularly in settings with reduced question-side cues, highlighting its effectiveness in fostering genuine self-awareness in LLMs."}}
{"id": "2509.15350", "pdf": "https://arxiv.org/pdf/2509.15350", "abs": "https://arxiv.org/abs/2509.15350", "authors": ["Yitong Wang", "Zhongping Zhang", "Margherita Piana", "Zheng Zhou", "Peter Gerstoft", "Bryan A. Plummer"], "title": "Real, Fake, or Manipulated? Detecting Machine-Influenced Text", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large Language Model (LLMs) can be used to write or modify documents,\npresenting a challenge for understanding the intent behind their use. For\nexample, benign uses may involve using LLM on a human-written document to\nimprove its grammar or to translate it into another language. However, a\ndocument entirely produced by a LLM may be more likely to be used to spread\nmisinformation than simple translation (\\eg, from use by malicious actors or\nsimply by hallucinating). Prior works in Machine Generated Text (MGT) detection\nmostly focus on simply identifying whether a document was human or machine\nwritten, ignoring these fine-grained uses. In this paper, we introduce a\nHiErarchical, length-RObust machine-influenced text detector (HERO), which\nlearns to separate text samples of varying lengths from four primary types:\nhuman-written, machine-generated, machine-polished, and machine-translated.\nHERO accomplishes this by combining predictions from length-specialist models\nthat have been trained with Subcategory Guidance. Specifically, for categories\nthat are easily confused (\\eg, different source languages), our Subcategory\nGuidance module encourages separation of the fine-grained categories, boosting\nperformance. Extensive experiments across five LLMs and six domains demonstrate\nthe benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on\naverage.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86HERO\uff0c\u4e00\u79cd\u80fd\u591f\u533a\u5206\u56db\u79cd\u6587\u672c\u7c7b\u578b\u7684\u673a\u5668\u5f71\u54cd\u6587\u672c\u68c0\u6d4b\u5668\uff0c\u5e76\u5728\u591a\u4e2aLLM\u548c\u9886\u57df\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u8bc6\u522b\u6587\u6863\u662f\u4eba\u7c7b\u8fd8\u662f\u673a\u5668\u7f16\u5199\u7684\uff0c\u800c\u5ffd\u7565\u4e86\u8fd9\u4e9b\u7ec6\u7c92\u5ea6\u7684\u4f7f\u7528\u60c5\u51b5\u3002", "method": "HERO\u901a\u8fc7\u7ed3\u5408\u7ecf\u8fc7\u5b50\u7c7b\u522b\u6307\u5bfc\u8bad\u7ec3\u7684\u957f\u5ea6\u4e13\u4e1a\u6a21\u578b\u7684\u9884\u6d4b\u6765\u5b9e\u73b0\u6587\u672c\u6837\u672c\u7684\u5206\u79bb\u3002", "result": "HERO\u80fd\u591f\u533a\u5206\u56db\u79cd\u4e3b\u8981\u7c7b\u578b\uff1a\u4eba\u5de5\u64b0\u5199\u3001\u673a\u5668\u751f\u6210\u3001\u673a\u5668\u6da6\u8272\u548c\u673a\u5668\u7ffb\u8bd1\u3002", "conclusion": "HERO\u5728\u4e94\u4e2aLLM\u548c\u516d\u4e2a\u9886\u57df\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u5728\u5e73\u5747mAP\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd52.5-3\u3002"}}
{"id": "2509.15361", "pdf": "https://arxiv.org/pdf/2509.15361", "abs": "https://arxiv.org/abs/2509.15361", "authors": ["Zichen Wu", "Hsiu-Yuan Huang", "Yunfang Wu"], "title": "Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "Accepted by EMNLP 2025 Findings", "summary": "Multimodal Large Language Models (MLLMs) have shown substantial capabilities\nin integrating visual and textual information, yet frequently rely on spurious\ncorrelations, undermining their robustness and generalization in complex\nmultimodal reasoning tasks. This paper addresses the critical challenge of\nsuperficial correlation bias in MLLMs through a novel causal mediation-based\ndebiasing framework. Specially, we distinguishing core semantics from spurious\ntextual and visual contexts via counterfactual examples to activate\ntraining-stage debiasing and employ a Mixture-of-Experts (MoE) architecture\nwith dynamic routing to selectively engages modality-specific debiasing\nexperts. Empirical evaluation on multimodal sarcasm detection and sentiment\nanalysis tasks demonstrates that our framework significantly surpasses unimodal\ndebiasing strategies and existing state-of-the-art models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u4e2d\u4ecb\u7684\u53bb\u504f\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8868\u9762\u76f8\u5173\u6027\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ecf\u5e38\u4f9d\u8d56\u4e8e\u865a\u5047\u7684\u76f8\u5173\u6027\uff0c\u8fd9\u635f\u5bb3\u4e86\u5b83\u4eec\u5728\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3MLLMs\u4e2d\u7684\u8868\u9762\u76f8\u5173\u6027\u504f\u5dee\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u4e2d\u4ecb\u7684\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u4f8b\u5b50\u533a\u5206\u6838\u5fc3\u8bed\u4e49\u548c\u865a\u5047\u7684\u6587\u672c\u548c\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u5e76\u4f7f\u7528\u5e26\u6709\u52a8\u6001\u8def\u7531\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\u6765\u9009\u62e9\u6027\u5730\u6fc0\u6d3b\u6a21\u6001\u7279\u5b9a\u7684\u53bb\u504f\u4e13\u5bb6\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u5355\u6a21\u6001\u53bb\u504f\u7b56\u7565\u548c\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u53bb\u504f\u7b56\u7565\u548c\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002"}}
{"id": "2509.15362", "pdf": "https://arxiv.org/pdf/2509.15362", "abs": "https://arxiv.org/abs/2509.15362", "authors": ["Yaya Sy", "Dioula Doucour\u00e9", "Christophe Cerisara", "Irina Illina"], "title": "Speech Language Models for Under-Represented Languages: Insights from Wolof", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We present our journey in training a speech language model for Wolof, an\nunderrepresented language spoken in West Africa, and share key insights. We\nfirst emphasize the importance of collecting large-scale, spontaneous,\nhigh-quality speech data, and show that continued pretraining HuBERT on this\ndataset outperforms both the base model and African-centric models on ASR. We\nthen integrate this speech encoder into a Wolof LLM to train the first Speech\nLLM for this language, extending its capabilities to tasks such as speech\ntranslation. Furthermore, we explore training the Speech LLM to perform\nmulti-step Chain-of-Thought before transcribing or translating. Our results\nshow that the Speech LLM not only improves speech recognition but also performs\nwell in speech translation. The models and the code will be openly shared.", "AI": {"tldr": "\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u9488\u5bf9Wolof\u8bed\u8a00\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u8ba1\u5212\u516c\u5f00\u6a21\u578b\u548c\u4ee3\u7801\u3002", "motivation": "\u6211\u4eec\u65e8\u5728\u8bad\u7ec3\u4e00\u4e2a\u9488\u5bf9Wolof\u8bed\u8a00\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u8be5\u8bed\u8a00\u5728\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5f00\u653e\u6a21\u578b\u548c\u4ee3\u7801\u4fc3\u8fdb\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u6211\u4eec\u9996\u5148\u5f3a\u8c03\u4e86\u6536\u96c6\u5927\u89c4\u6a21\u3001\u81ea\u53d1\u3001\u9ad8\u8d28\u91cf\u8bed\u97f3\u6570\u636e\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u6301\u7eed\u9884\u8bad\u7ec3HuBERT\u5728\u8fd9\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u975e\u6d32\u4e2d\u5fc3\u6a21\u578b\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u8fd9\u4e2a\u8bed\u97f3\u7f16\u7801\u5668\u96c6\u6210\u5230Wolof LLM\u4e2d\uff0c\u8bad\u7ec3\u51fa\u7b2c\u4e00\u4e2a\u9488\u5bf9\u8be5\u8bed\u8a00\u7684\u8bed\u97f3LLM\uff0c\u6269\u5c55\u4e86\u5176\u5728\u8bed\u97f3\u7ffb\u8bd1\u7b49\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u8ba9\u8bed\u97f3LLM\u5728\u8f6c\u5f55\u6216\u7ffb\u8bd1\u4e4b\u524d\u6267\u884c\u591a\u6b65\u9aa4\u601d\u7ef4\u94fe\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u8bed\u97f3LLM\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bed\u97f3\u8bc6\u522b\uff0c\u8fd8\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5c55\u793a\u4e86\u5728Wolof\u8bed\u8a00\u4e0a\u8bad\u7ec3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u65c5\u7a0b\uff0c\u5e76\u5206\u4eab\u4e86\u5173\u952e\u89c1\u89e3\u3002\u6211\u4eec\u5f3a\u8c03\u4e86\u6536\u96c6\u5927\u89c4\u6a21\u3001\u81ea\u53d1\u3001\u9ad8\u8d28\u91cf\u8bed\u97f3\u6570\u636e\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u6301\u7eed\u9884\u8bad\u7ec3HuBERT\u5728\u8fd9\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u975e\u6d32\u4e2d\u5fc3\u6a21\u578b\u3002\u6211\u4eec\u5c06\u8fd9\u4e2a\u8bed\u97f3\u7f16\u7801\u5668\u96c6\u6210\u5230Wolof LLM\u4e2d\uff0c\u8bad\u7ec3\u51fa\u7b2c\u4e00\u4e2a\u9488\u5bf9\u8be5\u8bed\u8a00\u7684\u8bed\u97f3LLM\uff0c\u6269\u5c55\u4e86\u5176\u5728\u8bed\u97f3\u7ffb\u8bd1\u7b49\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u8ba9\u8bed\u97f3LLM\u5728\u8f6c\u5f55\u6216\u7ffb\u8bd1\u4e4b\u524d\u6267\u884c\u591a\u6b65\u9aa4\u601d\u7ef4\u94fe\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8bed\u97f3LLM\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bed\u97f3\u8bc6\u522b\uff0c\u8fd8\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u8868\u73b0\u826f\u597d\u3002\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u5171\u4eab\u3002"}}
{"id": "2509.15373", "pdf": "https://arxiv.org/pdf/2509.15373", "abs": "https://arxiv.org/abs/2509.15373", "authors": ["Katsumi Ibaraki", "David Chiang"], "title": "Frustratingly Easy Data Augmentation for Low-Resource ASR", "categories": ["cs.CL"], "comment": "5 pages, 2 figures, 2 tables, submitted to ICASSP 2026", "summary": "This paper introduces three self-contained data augmentation methods for\nlow-resource Automatic Speech Recognition (ASR). Our techniques first generate\nnovel text--using gloss-based replacement, random replacement, or an LLM-based\napproach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We\napply these methods, which leverage only the original annotated data, to four\nlanguages with extremely limited resources (Vatlongos, Nashta, Shinekhen\nBuryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a\ncombination of the original audio and generated synthetic data yields\nsignificant performance gains, including a 14.3% absolute WER reduction for\nNashta. The methods prove effective across all four low-resource languages and\nalso show utility for high-resource languages like English, demonstrating their\nbroad applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u63d0\u5347\u4f4e\u8d44\u6e90\u548c\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684 ASR \u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u4f4e\u8d44\u6e90\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u9762\u4e34\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u81ea\u5305\u542b\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff1a\u57fa\u4e8e\u624b\u8bed\u7684\u66ff\u6362\u3001\u968f\u673a\u66ff\u6362\u4ee5\u53ca\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u968f\u540e\u4f7f\u7528\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u751f\u6210\u5408\u6210\u97f3\u9891\u3002", "result": "\u5728\u56db\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08Vatlongos\u3001Nashta\u3001Shinekhen Buryat \u548c Kakabe\uff09\u4ee5\u53ca\u9ad8\u8d44\u6e90\u8bed\u8a00\u5982\u82f1\u8bed\u4e0a\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u8bad\u7ec3 Wav2Vec2-XLSR-53 \u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f8b\u5982 Nashta \u7684 WER \u964d\u4f4e\u4e86 14.3%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e09\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u548c\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2509.15403", "pdf": "https://arxiv.org/pdf/2509.15403", "abs": "https://arxiv.org/abs/2509.15403", "authors": ["Yangyi Li", "Mengdi Huai"], "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have shown strong capabilities, enabling\nconcise, context-aware answers in question answering (QA) tasks. The lack of\ntransparency in complex LLMs has inspired extensive research aimed at\ndeveloping methods to explain large language behaviors. Among existing\nexplanation methods, natural language explanations stand out due to their\nability to explain LLMs in a self-explanatory manner and enable the\nunderstanding of model behaviors even when the models are closed-source.\nHowever, despite these promising advancements, there is no existing work\nstudying how to provide valid uncertainty guarantees for these generated\nnatural language explanations. Such uncertainty quantification is critical in\nunderstanding the confidence behind these explanations. Notably, generating\nvalid uncertainty estimates for natural language explanations is particularly\nchallenging due to the auto-regressive generation process of LLMs and the\npresence of noise in medical inquiries. To bridge this gap, in this work, we\nfirst propose a novel uncertainty estimation framework for these generated\nnatural language explanations, which provides valid uncertainty guarantees in a\npost-hoc and model-agnostic manner. Additionally, we also design a novel robust\nuncertainty estimation method that maintains valid uncertainty guarantees even\nunder noise. Extensive experiments on QA tasks demonstrate the desired\nperformance of our methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6846\u67b6\u548c\u9c81\u68d2\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u590d\u6742LLMs\u7684\u900f\u660e\u5ea6\uff0c\u7814\u7a76\u8005\u4eec\u81f4\u529b\u4e8e\u5f00\u53d1\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u884c\u4e3a\u7684\u65b9\u6cd5\u3002\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u56e0\u5176\u80fd\u591f\u4ee5\u81ea\u89e3\u91ca\u7684\u65b9\u5f0f\u89e3\u91caLLMs\u800c\u8131\u9896\u800c\u51fa\uff0c\u4f46\u76ee\u524d\u5c1a\u65e0\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u4e3a\u8fd9\u4e9b\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u63d0\u4f9b\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\u3002", "method": "\u672c\u6587\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4fdd\u6301\u6709\u6548\u6027\u3002", "result": "\u5728QA\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4e8b\u540e\u548c\u6a21\u578b\u65e0\u5173\u7684\u65b9\u5f0f\u4e0b\u4e3a\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u63d0\u4f9b\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4e5f\u80fd\u4fdd\u6301\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.15419", "pdf": "https://arxiv.org/pdf/2509.15419", "abs": "https://arxiv.org/abs/2509.15419", "authors": ["Claudio Benzoni", "Martina Langhals", "Martin Boeker", "Luise Modersohn", "M\u00e1t\u00e9 E. Maros"], "title": "Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 4 figures, and 3 tables", "summary": "Regardless of the rapid development of artificial intelligence, abstractive\nsummarisation is still challenging for sensitive and data-restrictive domains\nlike medicine. With the increasing number of imaging, the relevance of\nautomated tools for complex medical text summarisation is expected to become\nhighly relevant. In this paper, we investigated the adaptation via fine-tuning\nprocess of a non-domain-specific abstractive summarisation encoder-decoder\nmodel family, and gave insights to practitioners on how to avoid over- and\nunderfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological\nreports public dataset. For each model, we comprehensively evaluated two\ndifferent checkpoints with varying sizes of the same training data. We\nmonitored the models' performances with lexical and semantic metrics during the\ntraining history on the fixed-size validation set. PEGASUS exhibited different\nphases, which can be related to epoch-wise double-descent, or\npeak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger\ncheckpoint led to a performance detriment. This work highlights the challenges\nand risks of fine-tuning models with high expressivity when dealing with scarce\ntraining data, and lays the groundwork for future investigations into more\nrobust fine-tuning strategies for summarisation models in specialised domains.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u533b\u7597\u9886\u57df\u4e2d\u4f7f\u7528\u975e\u9886\u57df\u7279\u5b9a\u7684\u62bd\u8c61\u6458\u8981\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6548\u679c\uff0c\u5e76\u53d1\u73b0\u4f7f\u7528\u66f4\u5927\u7684\u68c0\u67e5\u70b9\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5f3a\u8c03\u4e86\u5728\u5904\u7406\u7a00\u7f3a\u8bad\u7ec3\u6570\u636e\u65f6\u4f7f\u7528\u9ad8\u8868\u8fbe\u529b\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6311\u6218\u548c\u98ce\u9669\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u8fc5\u901f\u53d1\u5c55\uff0c\u4f46\u62bd\u8c61\u6458\u8981\u5728\u533b\u7597\u7b49\u654f\u611f\u548c\u6570\u636e\u9650\u5236\u9886\u57df\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u968f\u7740\u5f71\u50cf\u6570\u91cf\u7684\u589e\u52a0\uff0c\u81ea\u52a8\u5316\u5de5\u5177\u5728\u590d\u6742\u533b\u5b66\u6587\u672c\u6458\u8981\u4e2d\u7684\u76f8\u5173\u6027\u9884\u8ba1\u4f1a\u53d8\u5f97\u975e\u5e38\u91cd\u8981\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u975e\u9886\u57df\u7279\u5b9a\u7684\u62bd\u8c61\u6458\u8981\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u5bb6\u65cf\u7684\u5fae\u8c03\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7PEGASUS\u548cPEGASUS-X\u6a21\u578b\u5728\u4e2d\u7b49\u89c4\u6a21\u7684\u653e\u5c04\u5b66\u62a5\u544a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u5bf9\u6bcf\u4e2a\u6a21\u578b\uff0c\u6211\u4eec\u5168\u9762\u8bc4\u4f30\u4e86\u4e0d\u540c\u5927\u5c0f\u7684\u540c\u4e00\u8bad\u7ec3\u6570\u636e\u7684\u4e24\u4e2a\u4e0d\u540c\u68c0\u67e5\u70b9\uff0c\u5e76\u5728\u56fa\u5b9a\u5927\u5c0f\u7684\u9a8c\u8bc1\u96c6\u4e0a\u76d1\u63a7\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "PEGASUS\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u9636\u6bb5\uff0c\u8fd9\u53ef\u4ee5\u4e0eepoch-wise\u53cc\u4e0b\u964d\u6216\u5cf0\u503c\u4e0b\u964d\u6062\u590d\u884c\u4e3a\u76f8\u5173\u8054\u3002\u5bf9\u4e8ePEGASUS-X\uff0c\u6211\u4eec\u53d1\u73b0\u4f7f\u7528\u66f4\u5927\u7684\u68c0\u67e5\u70b9\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5728\u5904\u7406\u7a00\u7f3a\u8bad\u7ec3\u6570\u636e\u65f6\uff0c\u4f7f\u7528\u9ad8\u8868\u8fbe\u529b\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6311\u6218\u548c\u98ce\u9669\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u7684\u6458\u8981\u6a21\u578b\u66f4\u7a33\u5065\u7684\u5fae\u8c03\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.15430", "pdf": "https://arxiv.org/pdf/2509.15430", "abs": "https://arxiv.org/abs/2509.15430", "authors": ["Liuyuan Jiang", "Xiaodong Cui", "Brian Kingsbury", "Tianyi Chen", "Lisha Chen"], "title": "BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages including reference", "summary": "Speech is a rich signal, and labeled audio-text pairs are costly, making\nself-supervised learning essential for scalable representation learning. A core\nchallenge in speech SSL is generating pseudo-labels that are both informative\nand efficient: strong labels, such as those used in HuBERT, improve downstream\nperformance but rely on external encoders and multi-stage pipelines, while\nefficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.\nWe propose BiRQ, a bilevel SSL framework that combines the efficiency of\nBEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key\nidea is to reuse part of the model itself as a pseudo-label generator:\nintermediate representations are discretized by a random-projection quantizer\nto produce enhanced labels, while anchoring labels derived directly from the\nraw input stabilize training and prevent collapse. Training is formulated as an\nefficient first-order bilevel optimization problem, solved end-to-end with\ndifferentiable Gumbel-softmax selection. This design eliminates the need for\nexternal label encoders, reduces memory cost, and enables iterative label\nrefinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ\nwhile maintaining low complexity and computational efficiency. We validate our\nmethod on various datasets, including 960-hour LibriSpeech, 150-hour AMI\nmeetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.", "AI": {"tldr": "BiRQ\u662f\u4e00\u4e2a\u7ed3\u5408BEST-RQ\u6548\u7387\u548cHuBERT\u98ce\u683c\u6807\u7b7e\u589e\u5f3a\u4f18\u52bf\u7684\u53cc\u5c42SSL\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u5229\u7528\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u4f2a\u6807\u7b7e\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "motivation": "A core challenge in speech SSL is generating pseudo-labels that are both informative and efficient: strong labels, such as those used in HuBERT, improve downstream performance but rely on external encoders and multi-stage pipelines, while efficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.", "method": "BiRQ, a bilevel SSL framework that combines the efficiency of BEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key idea is to reuse part of the model itself as a pseudo-label generator: intermediate representations are discretized by a random-projection quantizer to produce enhanced labels, while anchoring labels derived directly from the raw input stabilize training and prevent collapse.", "result": "BiRQ consistently improves over BEST-RQ while maintaining low complexity and computational efficiency. We validate our method on various datasets, including 960-hour LibriSpeech, 150-hour AMI meetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.", "conclusion": "BiRQ consistently improves over BEST-RQ while maintaining low complexity and computational efficiency."}}
{"id": "2509.15447", "pdf": "https://arxiv.org/pdf/2509.15447", "abs": "https://arxiv.org/abs/2509.15447", "authors": ["Caitlin Cisar", "Emily Sheffield", "Joshua Drake", "Alden Harrell", "Subramanian Chidambaram", "Nikita Nangia", "Vinayak Arannil", "Alex Williams"], "title": "PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative AI applications commonly leverage user personas as a steering\nmechanism for synthetic data generation, but reliance on natural language\nrepresentations forces models to make unintended inferences about which\nattributes to emphasize, limiting precise control over outputs. We introduce\nPILOT (Psychological and Linguistic Output Targeting), a two-phase framework\nfor steering large language models with structured psycholinguistic profiles.\nIn Phase 1, PILOT translates natural language persona descriptions into\nmultidimensional profiles with normalized scores across linguistic and\npsychological dimensions. In Phase 2, these profiles guide generation along\nmeasurable axes of variation. We evaluate PILOT across three state-of-the-art\nLLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas\nunder three conditions: Natural-language Persona Steering (NPS), Schema-Based\nSteering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate\nthat schema-based approaches significantly reduce artificial-sounding persona\nrepetition while improving output coherence, with silhouette scores increasing\nfrom 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals\na fundamental trade-off: SBS produces more concise outputs with higher topical\nconsistency, while NPS offers greater lexical diversity but reduced\npredictability. HPS achieves a balance between these extremes, maintaining\noutput variety while preserving structural consistency. Expert linguistic\nevaluation confirms that PILOT maintains high response quality across all\nconditions, with no statistically significant differences between steering\napproaches.", "AI": {"tldr": "PILOT is a framework for steering large language models using structured psycholinguistic profiles, showing improved coherence and reduced repetition compared to natural language persona steering.", "motivation": "The paper aims to address the limitations of relying on natural language representations for user personas in generative AI applications, which can lead to unintended inferences and limited control over outputs.", "method": "PILOT is a two-phase framework that translates natural language persona descriptions into multidimensional profiles and uses these profiles to guide generation along measurable axes of variation.", "result": "Schema-based approaches (SBS) significantly reduce artificial-sounding persona repetition while improving output coherence, with silhouette scores increasing from 0.098 to 0.237 and topic purity from 0.773 to 0.957. HPS achieves a balance between output variety and structural consistency.", "conclusion": "PILOT provides a framework for steering large language models with structured psycholinguistic profiles, achieving a balance between output variety and structural consistency. Expert evaluation confirms high response quality across all conditions."}}
{"id": "2509.15476", "pdf": "https://arxiv.org/pdf/2509.15476", "abs": "https://arxiv.org/abs/2509.15476", "authors": ["Zhu Li", "Xiyuan Gao", "Yuqing Zhang", "Shekhar Nayak", "Matt Coler"], "title": "Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Sarcasm detection remains a challenge in natural language understanding, as\nsarcastic intent often relies on subtle cross-modal cues spanning text, speech,\nand vision. While prior work has primarily focused on textual or visual-textual\nsarcasm, comprehensive audio-visual-textual sarcasm understanding remains\nunderexplored. In this paper, we systematically evaluate large language models\n(LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and\nChinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In\naddition to direct classification, we explore models as feature encoders,\nintegrating their representations through a collaborative gating fusion module.\nExperimental results show that audio-based models achieve the strongest\nunimodal performance, while text-audio and audio-vision combinations outperform\nunimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show\ncompetitive zero-shot and fine-tuned performance. Our findings highlight the\npotential of MLLMs for cross-lingual, audio-visual-textual sarcasm\nunderstanding.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u3001\u89c6\u542c\u6587\u672c\u8bbd\u523a\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u57fa\u4e8e\u97f3\u9891\u7684\u6a21\u578b\u5728\u5355\u6a21\u6001\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800c\u6587\u672c-\u97f3\u9891\u548c\u97f3\u9891-\u89c6\u89c9\u7ec4\u5408\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u8bbd\u523a\u68c0\u6d4b\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u8bbd\u523a\u610f\u56fe\u901a\u5e38\u4f9d\u8d56\u4e8e\u8de8\u8d8a\u6587\u672c\u3001\u8bed\u97f3\u548c\u89c6\u89c9\u7684\u7ec6\u5fae\u8de8\u6a21\u6001\u7ebf\u7d22\u3002\u867d\u7136\u4e4b\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u6587\u672c\u6216\u89c6\u89c9-\u6587\u672c\u8bbd\u523a\u4e0a\uff0c\u4f46\u5168\u9762\u7684\u97f3\u9891-\u89c6\u89c9-\u6587\u672c\u8bbd\u523a\u7406\u89e3\u4ecd\u7f3a\u4e4f\u7814\u7a76\u3002", "method": "\u6211\u4eec\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001LLMs\u5728\u82f1\u8bed\uff08MUStARD++\uff09\u548c\u4e2d\u6587\uff08MCSD 1.0\uff09\u4e2d\u7684\u8bbd\u523a\u68c0\u6d4b\uff0c\u5305\u62ec\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548cLoRA\u5fae\u8c03\u8bbe\u7f6e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u7d22\u4e86\u6a21\u578b\u4f5c\u4e3a\u7279\u5f81\u7f16\u7801\u5668\uff0c\u5e76\u901a\u8fc7\u534f\u4f5c\u95e8\u63a7\u878d\u5408\u6a21\u5757\u6574\u5408\u5176\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u97f3\u9891\u7684\u6a21\u578b\u5728\u5355\u6a21\u6001\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u5f3a\uff0c\u800c\u6587\u672c-\u97f3\u9891\u548c\u97f3\u9891-\u89c6\u89c9\u7ec4\u5408\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u4e09\u6a21\u6001\u6a21\u578b\u3002\u6b64\u5916\uff0cMLLMs\u5982Qwen-Omni\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u3001\u89c6\u542c\u6587\u672c\u8bbd\u523a\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.15478", "pdf": "https://arxiv.org/pdf/2509.15478", "abs": "https://arxiv.org/abs/2509.15478", "authors": ["Madison Van Doren", "Casey Ford", "Emily Dix"], "title": "Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) are increasingly used in real world\napplications, yet their safety under adversarial conditions remains\nunderexplored. This study evaluates the harmlessness of four leading MLLMs\n(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to\nadversarial prompts across text-only and multimodal formats. A team of 26 red\nteamers generated 726 prompts targeting three harm categories: illegal\nactivity, disinformation, and unethical behaviour. These prompts were submitted\nto each model, and 17 annotators rated 2,904 model outputs for harmfulness\nusing a 5-point scale. Results show significant differences in vulnerability\nacross models and modalities. Pixtral 12B exhibited the highest rate of harmful\nresponses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%).\nContrary to expectations, text-only prompts were slightly more effective at\nbypassing safety mechanisms than multimodal ones. Statistical analysis\nconfirmed that both model type and input modality were significant predictors\nof harmfulness. These findings underscore the urgent need for robust,\nmultimodal safety benchmarks as MLLMs are deployed more widely.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u63d0\u793a\u65f6\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u548c\u6a21\u6001\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u5efa\u7acb\u591a\u6a21\u6001\u5b89\u5168\u57fa\u51c6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u4f46\u5b83\u4eec\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u6027\u4ecd\u7f3a\u4e4f\u7814\u7a76\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u9886\u5148\u7684MLLMs\uff08GPT-4o\u3001Claude Sonnet 3.5\u3001Pixtral 12B\u548cQwen VL Plus\uff09\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u63d0\u793a\u65f6\u7684\u65e0\u5bb3\u6027\uff0c\u901a\u8fc7\u7ea2\u961f\u751f\u6210726\u4e2a\u63d0\u793a\uff0c\u5e76\u753117\u540d\u8bc4\u4f30\u8005\u5bf92,904\u4e2a\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u6709\u5bb3\u6027\u8bc4\u5206\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u6a21\u578b\u548c\u6a21\u6001\u4e4b\u95f4\u7684\u6613\u53d7\u653b\u51fb\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002Pixtral 12B\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u6709\u5bb3\u54cd\u5e94\u7387\uff08\u7ea662%\uff09\uff0c\u800cClaude Sonnet 3.5\u662f\u6700\u5177\u62b5\u6297\u529b\u7684\uff08\u7ea610%\uff09\u3002\u6587\u672c\u63d0\u793a\u6bd4\u591a\u6a21\u6001\u63d0\u793a\u7565\u5fae\u66f4\u6709\u6548\u3002\u7edf\u8ba1\u5206\u6790\u786e\u8ba4\u4e86\u6a21\u578b\u7c7b\u578b\u548c\u8f93\u5165\u6a21\u6001\u662f\u6709\u5bb3\u6027\u7684\u663e\u8457\u9884\u6d4b\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728MLLMs\u5e7f\u6cdb\u90e8\u7f72\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5efa\u7acb\u7a33\u5065\u7684\u591a\u6a21\u6001\u5b89\u5168\u57fa\u51c6\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2509.15485", "pdf": "https://arxiv.org/pdf/2509.15485", "abs": "https://arxiv.org/abs/2509.15485", "authors": ["Ahmed Abdou"], "title": "mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a simple, model-agnostic post-processing technique for\nfine-grained Arabic readability classification in the BAREC 2025 Shared Task\n(19 ordinal levels). Our method applies conformal prediction to generate\nprediction sets with coverage guarantees, then computes weighted averages using\nsoftmax-renormalized probabilities over the conformal sets. This\nuncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing\nhigh-penalty misclassifications to nearer levels. Our approach shows consistent\nQWK improvements of 1-3 points across different base models. In the strict\ntrack, our submission achieves QWK scores of 84.9\\%(test) and 85.7\\% (blind\ntest) for sentence level, and 73.3\\% for document level. For Arabic educational\nassessment, this enables human reviewers to focus on a handful of plausible\nlevels, combining statistical guarantees with practical usability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7ec6\u7c92\u5ea6\u963f\u62c9\u4f2f\u8bed\u53ef\u8bfb\u6027\u5206\u7c7b\u7684\u540e\u5904\u7406\u6280\u672f\uff0c\u901a\u8fc7\u5e94\u7528\u7b26\u5408\u9884\u6d4b\u548csoftmax\u5f52\u4e00\u5316\u6982\u7387\u8ba1\u7b97\u52a0\u6743\u5e73\u5747\u503c\uff0c\u663e\u8457\u63d0\u5347\u4e86QWK\u5206\u6570\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u963f\u62c9\u4f2f\u8bed\u6559\u80b2\u8bc4\u4f30\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u51cf\u5c11\u9ad8\u60e9\u7f5a\u9519\u8bef\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u4f7f\u4eba\u7c7b\u8bc4\u5ba1\u5458\u80fd\u591f\u4e13\u6ce8\u4e8e\u5c11\u6570\u53ef\u80fd\u7684\u6c34\u5e73\uff0c\u540c\u65f6\u7ed3\u5408\u7edf\u8ba1\u4fdd\u8bc1\u4e0e\u5b9e\u7528\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u6a21\u578b\u65e0\u5173\u7684\u540e\u5904\u7406\u6280\u672f\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u963f\u62c9\u4f2f\u8bed\u53ef\u8bfb\u6027\u5206\u7c7b\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e86\u7b26\u5408\u9884\u6d4b\u6765\u751f\u6210\u5177\u6709\u8986\u76d6\u4fdd\u8bc1\u7684\u9884\u6d4b\u96c6\uff0c\u7136\u540e\u4f7f\u7528softmax\u5f52\u4e00\u5316\u6982\u7387\u8ba1\u7b97\u52a0\u6743\u5e73\u5747\u503c\u3002\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89e3\u7801\u51cf\u5c11\u4e86\u9ad8\u60e9\u7f5a\u9519\u8bef\u5206\u7c7b\u5230\u66f4\u63a5\u8fd1\u7684\u7ea7\u522b\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e0a\u4e00\u81f4\u63d0\u5347\u4e86QWK\u5206\u6570\uff0c\u8fbe\u5230\u4e861-3\u5206\u3002\u5728\u4e25\u683c\u8d5b\u9053\u4e2d\uff0c\u6211\u4eec\u7684\u63d0\u4ea4\u5728\u53e5\u5b50\u7ea7\u522b\u548c\u6587\u6863\u7ea7\u522b\u7684QWK\u5f97\u5206\u5206\u522b\u4e3a84.9%\uff08\u6d4b\u8bd5\uff09\u548c85.7%\uff08\u76f2\u6d4b\uff09\uff0c\u4ee5\u53ca73.3%\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e0a\u4e00\u81f4\u63d0\u5347\u4e86QWK\u5206\u6570\uff0c\u8fbe\u5230\u4e861-3\u5206\u3002\u5728\u4e25\u683c\u8d5b\u9053\u4e2d\uff0c\u6211\u4eec\u7684\u63d0\u4ea4\u5728\u53e5\u5b50\u7ea7\u522b\u548c\u6587\u6863\u7ea7\u522b\u7684QWK\u5f97\u5206\u5206\u522b\u4e3a84.9%\uff08\u6d4b\u8bd5\uff09\u548c85.7%\uff08\u76f2\u6d4b\uff09\uff0c\u4ee5\u53ca73.3%\u3002\u8fd9\u5bf9\u4e8e\u963f\u62c9\u4f2f\u8bed\u6559\u80b2\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f7f\u4eba\u7c7b\u8bc4\u5ba1\u5458\u80fd\u591f\u4e13\u6ce8\u4e8e\u5c11\u6570\u53ef\u80fd\u7684\u6c34\u5e73\uff0c\u7ed3\u5408\u7edf\u8ba1\u4fdd\u8bc1\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.15515", "pdf": "https://arxiv.org/pdf/2509.15515", "abs": "https://arxiv.org/abs/2509.15515", "authors": ["Hantao Yang", "Hong Xie", "Defu Lian", "Enhong Chen"], "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference", "categories": ["cs.CL"], "comment": null, "summary": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u7f13\u5b58\u8d4c\u5f92\u95ee\u9898\uff0c\u9488\u5bf9\u67e5\u8be2\u5f02\u6784\u6027\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6700\u4f18\u7f13\u5b58\u9009\u62e9\u89c6\u4e3a\u4e00\u4e2a\u80cc\u5305\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7d2f\u79ef\u7684\u7b56\u7565\u6765\u5e73\u8861\u8ba1\u7b97\u5f00\u9500\u548c\u7f13\u5b58\u66f4\u65b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u51cf\u5c11\u4e86\u7ea612%\u7684\u603b\u6210\u672c\u3002", "motivation": "\u4e4b\u524d\u7684\u6587\u732e\u901a\u5e38\u5047\u8bbe\u67e5\u8be2\u5927\u5c0f\u662f\u5747\u5300\u7684\uff0c\u4f46\u5f02\u6784\u67e5\u8be2\u5927\u5c0f\u5f15\u5165\u4e86\u7f13\u5b58\u9009\u62e9\u7684\u7ec4\u5408\u7ed3\u6784\uff0c\u4f7f\u5f97\u7f13\u5b58\u66ff\u6362\u8fc7\u7a0b\u66f4\u52a0\u8ba1\u7b97\u548c\u7edf\u8ba1\u4e0a\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u67e5\u8be2\u5f02\u6784\u6027\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684LLM\u63a8\u7406\u3002", "method": "\u672c\u6587\u5c06\u6700\u4f18\u7f13\u5b58\u9009\u62e9\u89c6\u4e3a\u4e00\u4e2a\u80cc\u5305\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7d2f\u79ef\u7684\u7b56\u7565\u6765\u5e73\u8861\u8ba1\u7b97\u5f00\u9500\u548c\u7f13\u5b58\u66f4\u65b0\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u95ee\u9898\u4f9d\u8d56\u6027\u8fb9\u754c\uff0c\u8fd9\u662f\u4e4b\u524d\u5de5\u4f5c\u4e2d\u7f3a\u5931\u7684\u90e8\u5206\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u672c\u6587\u7b97\u6cd5\u7684\u9057\u61be\u5ea6\u8fbe\u5230\u4e86O(\u221a(MNT))\u7684\u754c\u9650\uff0c\u76f8\u6bd4\u4f2f\u514b\u5229\u7684\u7ed3\u679cO(MN\u221aT)\uff0c\u6539\u8fdb\u4e86\u221aMN\u7684\u7cfb\u6570\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u7b97\u6cd5\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u51cf\u5c11\u4e86\u7ea612%\u7684\u603b\u6210\u672c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3LLM\u7f13\u5b58\u8d4c\u5f92\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u6700\u4f18\u7f13\u5b58\u9009\u62e9\u89c6\u4e3a\u4e00\u4e2a\u80cc\u5305\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7d2f\u79ef\u7684\u7b56\u7565\u6765\u5e73\u8861\u8ba1\u7b97\u5f00\u9500\u548c\u7f13\u5b58\u66f4\u65b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u51cf\u5c11\u4e86\u7ea612%\u7684\u603b\u6210\u672c\u3002"}}
{"id": "2509.15518", "pdf": "https://arxiv.org/pdf/2509.15518", "abs": "https://arxiv.org/abs/2509.15518", "authors": ["Siyang Wu", "Zhewei Sun"], "title": "How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Slang is a commonly used type of informal language that poses a daunting\nchallenge to NLP systems. Recent advances in large language models (LLMs),\nhowever, have made the problem more approachable. While LLM agents are becoming\nmore widely applied to intermediary tasks such as slang detection and slang\ninterpretation, their generalizability and reliability are heavily dependent on\nwhether these models have captured structural knowledge about slang that align\nwell with human attested slang usages. To answer this question, we contribute a\nsystematic comparison between human and machine-generated slang usages. Our\nevaluative framework focuses on three core aspects: 1) Characteristics of the\nusages that reflect systematic biases in how machines perceive slang, 2)\nCreativity reflected by both lexical coinages and word reuses employed by the\nslang usages, and 3) Informativeness of the slang usages when used as\ngold-standard examples for model distillation. By comparing human-attested\nslang usages from the Online Slang Dictionary (OSD) and slang generated by\nGPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our\nresults suggest that while LLMs have captured significant knowledge about the\ncreative aspects of slang, such knowledge does not align with humans\nsufficiently to enable LLMs for extrapolative tasks such as linguistic\nanalyses.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u751f\u6210\u7684\u4fda\u8bed\u7528\u6cd5\uff0c\u53d1\u73b0LLMs\u5728\u611f\u77e5\u4fda\u8bed\u65f6\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u5c3d\u7ba1\u5b83\u4eec\u638c\u63e1\u4e86\u4fda\u8bed\u7684\u521b\u9020\u6027\u65b9\u9762\u7684\u91cd\u8981\u77e5\u8bc6\uff0c\u4f46\u8fd9\u4e9b\u77e5\u8bc6\u4e0e\u4eba\u7c7b\u7684\u89c1\u89e3\u5e76\u4e0d\u5145\u5206\u4e00\u81f4\uff0c\u65e0\u6cd5\u7528\u4e8e\u63a8\u65ad\u6027\u4efb\u52a1\u5982\u8bed\u8a00\u5206\u6790\u3002", "motivation": "\u4fda\u8bed\u662f\u4e00\u79cd\u5e38\u89c1\u7684\u975e\u6b63\u5f0f\u8bed\u8a00\u7c7b\u578b\uff0c\u5bf9NLP\u7cfb\u7edf\u6784\u6210\u4e86\u4e25\u5cfb\u6311\u6218\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u5c55\u4f7f\u5f97\u8fd9\u4e00\u95ee\u9898\u66f4\u52a0\u53ef\u89e3\u51b3\uff0c\u4f46\u5b83\u4eec\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u5df2\u6355\u6349\u5230\u4e0e\u4eba\u7c7b\u8ba4\u53ef\u7684\u4fda\u8bed\u7528\u6cd5\u76f8\u4e00\u81f4\u7684\u7ed3\u6784\u77e5\u8bc6\u3002", "method": "\u6211\u4eec\u5bf9\u4eba\u7c7b\u548c\u673a\u5668\u751f\u6210\u7684\u4fda\u8bed\u7528\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6bd4\u8f83\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u4e2a\u6838\u5fc3\u65b9\u9762\uff1a1\uff09\u53cd\u6620\u673a\u5668\u5982\u4f55\u611f\u77e5\u4fda\u8bed\u7684\u7cfb\u7edf\u6027\u504f\u89c1\u7684\u7528\u6cd5\u7279\u5f81\uff1b2\uff09\u901a\u8fc7\u4fda\u8bed\u7528\u6cd5\u4e2d\u7684\u8bcd\u6c47\u521b\u9020\u548c\u8bcd\u8bed\u91cd\u7528\u6240\u4f53\u73b0\u7684\u521b\u9020\u529b\uff1b3\uff09\u5728\u6a21\u578b\u84b8\u998f\u4e2d\u4f5c\u4e3a\u9ec4\u91d1\u6807\u51c6\u793a\u4f8b\u7684\u4fda\u8bed\u7528\u6cd5\u7684\u4fe1\u606f\u91cf\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u6765\u81ea\u5728\u7ebf\u4fda\u8bed\u8bcd\u5178\uff08OSD\uff09\u7684\u4eba\u7c7b\u8ba4\u53ef\u7684\u4fda\u8bed\u7528\u6cd5\u548cGPT-4o\u548cLlama-3\u751f\u6210\u7684\u4fda\u8bed\uff0c\u6211\u4eec\u53d1\u73b0LLMs\u5728\u611f\u77e5\u4fda\u8bed\u65f6\u5b58\u5728\u663e\u8457\u504f\u5dee\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1LLMs\u5df2\u7ecf\u638c\u63e1\u4e86\u4fda\u8bed\u7684\u521b\u9020\u6027\u65b9\u9762\u7684\u91cd\u8981\u77e5\u8bc6\uff0c\u4f46\u8fd9\u4e9b\u77e5\u8bc6\u4e0e\u4eba\u7c7b\u7684\u89c1\u89e3\u5e76\u4e0d\u5145\u5206\u4e00\u81f4\uff0c\u65e0\u6cd5\u4f7fLLMs\u8fdb\u884c\u8bf8\u5982\u8bed\u8a00\u5206\u6790\u7b49\u63a8\u65ad\u6027\u4efb\u52a1\u3002"}}
{"id": "2509.15549", "pdf": "https://arxiv.org/pdf/2509.15549", "abs": "https://arxiv.org/abs/2509.15549", "authors": ["Chunguang Zhao", "Yilun Liu", "Pufan Zeng", "Yuanchang Luo", "Shimin Tao", "Minggui He", "Weibin Meng", "Song Xu", "Ziang Chen", "Chen Liu", "Hongxia Ma", "Li Zhang", "Boxing Chen", "Daimeng Wei"], "title": "A method for improving multilingual quality and diversity of instruction fine-tuning datasets", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large\nlanguage models (LLMs) to generalize effectively across diverse linguistic and\ncultural contexts. However, the scarcity of high-quality multilingual training\ndata and corresponding building method remains a critical bottleneck. While\ndata selection has shown promise in English settings, existing methods often\nfail to generalize across languages due to reliance on simplistic heuristics or\nlanguage-specific assumptions. In this work, we introduce Multilingual Data\nQuality and Diversity (M-DaQ), a novel method for improving LLMs\nmultilinguality, by selecting high-quality and semantically diverse\nmultilingual IFT samples. We further conduct the first systematic investigation\nof the Superficial Alignment Hypothesis (SAH) in multilingual setting.\nEmpirical results across 18 languages demonstrate that models fine-tuned with\nM-DaQ method achieve significant performance gains over vanilla baselines over\n60% win rate. Human evaluations further validate these gains, highlighting the\nincrement of cultural points in the response. We release the M-DaQ code to\nsupport future research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aM-DaQ\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528M-DaQ\u65b9\u6cd5\u5fae\u8c03\u7684\u6a21\u578b\u572818\u79cd\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u63d0\u5347\u3002", "motivation": "\u591a\u8bed\u8a00\u6307\u4ee4\u5fae\u8c03\uff08IFT\uff09\u5bf9\u4e8e\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6837\u5316\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u6709\u6548\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u9ad8\u8d28\u91cf\u591a\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u548c\u76f8\u5e94\u6784\u5efa\u65b9\u6cd5\u7684\u7f3a\u4e4f\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u82f1\u8bed\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u8de8\u8bed\u8a00\u65f6\u5f80\u5f80\u65e0\u6cd5\u63a8\u5e7f\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u8bed\u8a00\u7279\u5b9a\u7684\u5047\u8bbe\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86Multilingual Data Quality and Diversity (M-DaQ)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u9ad8\u8d28\u91cf\u548c\u8bed\u4e49\u591a\u6837\u5316\u7684\u591a\u8bed\u8a00\u6307\u4ee4\u5fae\u8c03\u6837\u672c\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u8d85\u7ea7\u5bf9\u9f50\u5047\u8bbe\uff08SAH\uff09\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u572818\u79cd\u8bed\u8a00\u4e2d\uff0c\u4f7f\u7528M-DaQ\u65b9\u6cd5\u5fae\u8c03\u7684\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u80dc\u7387\u8d85\u8fc760%\u3002\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u63d0\u5347\uff0c\u7a81\u663e\u4e86\u54cd\u5e94\u4e2d\u6587\u5316\u70b9\u7684\u589e\u52a0\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM-DaQ\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528M-DaQ\u65b9\u6cd5\u5fae\u8c03\u7684\u6a21\u578b\u572818\u79cd\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u63d0\u5347\u3002"}}
{"id": "2509.15550", "pdf": "https://arxiv.org/pdf/2509.15550", "abs": "https://arxiv.org/abs/2509.15550", "authors": ["Xiaowei Zhu", "Yubing Ren", "Fang Fang", "Qingfeng Tan", "Shi Wang", "Yanan Cao"], "title": "DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm", "categories": ["cs.CL"], "comment": "NeurIPS 2025 Spotlight", "summary": "The rapid advancement of large language models (LLMs) has blurred the line\nbetween AI-generated and human-written text. This progress brings societal\nrisks such as misinformation, authorship ambiguity, and intellectual property\nconcerns, highlighting the urgent need for reliable AI-generated text detection\nmethods. However, recent advances in generative language modeling have resulted\nin significant overlap between the feature distributions of human-written and\nAI-generated text, blurring classification boundaries and making accurate\ndetection increasingly challenging. To address the above challenges, we propose\na DNA-inspired perspective, leveraging a repair-based process to directly and\ninterpretably capture the intrinsic differences between human-written and\nAI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a\nzero-shot detection method for distinguishing AI-generated and human-written\ntext. The method constructs an ideal AI-generated sequence for each input,\niteratively repairs non-optimal tokens, and quantifies the cumulative repair\neffort as an interpretable detection signal. Empirical evaluations demonstrate\nthat our method achieves state-of-the-art detection performance and exhibits\nstrong robustness against various adversarial attacks and input lengths.\nSpecifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC\nand 2.08% in F1 score across multiple public benchmark datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDNA\u7684\u89c6\u89d2\uff0c\u901a\u8fc7\u4fee\u590d\u8fc7\u7a0b\u6355\u6349\u4eba\u7c7b\u64b0\u5199\u548cAI\u751f\u6210\u6587\u672c\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u5f15\u5165\u4e86DNA-DetectLLM\uff0c\u8fd9\u662f\u4e00\u79cd\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0cAI\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u64b0\u5199\u6587\u672c\u4e4b\u95f4\u7684\u754c\u9650\u53d8\u5f97\u6a21\u7cca\uff0c\u8fd9\u5e26\u6765\u4e86\u8bf8\u5982\u865a\u5047\u4fe1\u606f\u3001\u4f5c\u8005\u8eab\u4efd\u4e0d\u660e\u548c\u77e5\u8bc6\u4ea7\u6743\u95ee\u9898\u7b49\u793e\u4f1a\u98ce\u9669\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u53ef\u9760\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u751f\u6210\u8bed\u8a00\u5efa\u6a21\u7684\u8fdb\u5c55\u5bfc\u81f4\u4e86\u4eba\u7c7b\u64b0\u5199\u548cAI\u751f\u6210\u6587\u672c\u7279\u5f81\u5206\u5e03\u4e4b\u95f4\u7684\u663e\u8457\u91cd\u53e0\uff0c\u4f7f\u5f97\u51c6\u786e\u68c0\u6d4b\u53d8\u5f97\u66f4\u52a0\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDNA\u7684\u89c6\u89d2\uff0c\u5229\u7528\u4fee\u590d\u8fc7\u7a0b\u76f4\u63a5\u4e14\u53ef\u89e3\u91ca\u5730\u6355\u6349\u4eba\u7c7b\u64b0\u5199\u548cAI\u751f\u6210\u6587\u672c\u4e4b\u95f4\u7684\u5185\u5728\u5dee\u5f02\uff0c\u5e76\u5f15\u5165\u4e86DNA-DetectLLM\uff0c\u8fd9\u662f\u4e00\u79cd\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u533a\u5206AI\u751f\u6210\u548c\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u672c\u3002", "result": "DNA-DetectLLM\u5728AUROC\u548cF1\u5206\u6570\u65b9\u9762\u5206\u522b\u5b9e\u73b0\u4e865.55%\u548c2.08%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5e76\u5728\u591a\u79cd\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "DNA-DetectLLM\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5bf9\u5404\u79cd\u5bf9\u6297\u6027\u653b\u51fb\u548c\u8f93\u5165\u957f\u5ea6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.15556", "pdf": "https://arxiv.org/pdf/2509.15556", "abs": "https://arxiv.org/abs/2509.15556", "authors": ["Ping Guo", "Yubing Ren", "Binbin Liu", "Fengze Liu", "Haobin Lin", "Yifan Zhang", "Bingni Zhang", "Taifeng Wang", "Yin Zheng"], "title": "Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have become integral to a wide range of\napplications worldwide, driving an unprecedented global demand for effective\nmultilingual capabilities. Central to achieving robust multilingual performance\nis the strategic allocation of language proportions within training corpora.\nHowever, determining optimal language ratios is highly challenging due to\nintricate cross-lingual interactions and sensitivity to dataset scale. This\npaper introduces Climb (Cross-Lingual Interaction-aware Multilingual\nBalancing), a novel framework designed to systematically optimize multilingual\ndata allocation. At its core, Climb introduces a cross-lingual\ninteraction-aware language ratio, explicitly quantifying each language's\neffective allocation by capturing inter-language dependencies. Leveraging this\nratio, Climb proposes a principled two-step optimization procedure--first\nequalizing marginal benefits across languages, then maximizing the magnitude of\nthe resulting language allocation vectors--significantly simplifying the\ninherently complex multilingual optimization problem. Extensive experiments\nconfirm that Climb can accurately measure cross-lingual interactions across\nvarious multilingual settings. LLMs trained with Climb-derived proportions\nconsistently achieve state-of-the-art multilingual performance, even achieving\ncompetitive performance with open-sourced LLMs trained with more tokens.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aClimb\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u591a\u8bed\u8a00\u6570\u636e\u5206\u914d\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u4ea4\u4e92\u611f\u77e5\u7684\u8bed\u8a00\u6bd4\u4f8b\u548c\u4e24\u6b65\u4f18\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8bed\u8a00LLM\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u590d\u6742\u7684\u8de8\u8bed\u8a00\u4ea4\u4e92\u548c\u5bf9\u6570\u636e\u96c6\u89c4\u6a21\u7684\u654f\u611f\u6027\uff0c\u786e\u5b9a\u6700\u4f73\u8bed\u8a00\u6bd4\u4f8b\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u591a\u8bed\u8a00\u6570\u636e\u5206\u914d\u3002", "method": "Climb\u5f15\u5165\u4e86\u4e00\u4e2a\u8de8\u8bed\u8a00\u4ea4\u4e92\u611f\u77e5\u7684\u8bed\u8a00\u6bd4\u4f8b\uff0c\u901a\u8fc7\u6355\u6349\u8bed\u8a00\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u6765\u91cf\u5316\u6bcf\u79cd\u8bed\u8a00\u7684\u6709\u6548\u5206\u914d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u6b65\u4f18\u5316\u8fc7\u7a0b\uff1a\u9996\u5148\u5e73\u8861\u5404\u8bed\u8a00\u7684\u8fb9\u9645\u6536\u76ca\uff0c\u7136\u540e\u6700\u5927\u5316 resulting \u8bed\u8a00\u5206\u914d\u5411\u91cf\u7684\u5e45\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cClimb\u53ef\u4ee5\u51c6\u786e\u6d4b\u91cf\u5404\u79cd\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u8de8\u8bed\u8a00\u4ea4\u4e92\u3002\u4f7f\u7528Climb\u884d\u751f\u6bd4\u4f8b\u8bad\u7ec3\u7684LLM\u5728\u591a\u8bed\u8a00\u6027\u80fd\u4e0a consistently \u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u751a\u81f3\u5728\u4e0e\u4f7f\u7528\u66f4\u591atoken\u8bad\u7ec3\u7684\u5f00\u6e90LLM\u7ade\u4e89\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6Climb\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u4f18\u5316\u591a\u8bed\u8a00\u6570\u636e\u5206\u914d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528Climb\u884d\u751f\u6bd4\u4f8b\u8bad\u7ec3\u7684LLM\u5728\u591a\u8bed\u8a00\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002"}}
{"id": "2509.15560", "pdf": "https://arxiv.org/pdf/2509.15560", "abs": "https://arxiv.org/abs/2509.15560", "authors": ["Gary Lupyan", "Hunter Gentry", "Martin Zettersten"], "title": "How important is language for human-like intelligence?", "categories": ["cs.CL"], "comment": null, "summary": "We use language to communicate our thoughts. But is language merely the\nexpression of thoughts, which are themselves produced by other, nonlinguistic\nparts of our minds? Or does language play a more transformative role in human\ncognition, allowing us to have thoughts that we otherwise could (or would) not\nhave? Recent developments in artificial intelligence (AI) and cognitive science\nhave reinvigorated this old question. We argue that language may hold the key\nto the emergence of both more general AI systems and central aspects of human\nintelligence. We highlight two related properties of language that make it such\na powerful tool for developing domain--general abilities. First, language\noffers compact representations that make it easier to represent and reason\nabout many abstract concepts (e.g., exact numerosity). Second, these compressed\nrepresentations are the iterated output of collective minds. In learning a\nlanguage, we learn a treasure trove of culturally evolved abstractions. Taken\ntogether, these properties mean that a sufficiently powerful learning system\nexposed to language--whether biological or artificial--learns a compressed\nmodel of the world, reverse engineering many of the conceptual and causal\nstructures that support human (and human-like) thought.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u5728\u4fc3\u8fdb\u901a\u7528\u4eba\u5de5\u667a\u80fd\u548c\u4eba\u7c7b\u667a\u80fd\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5f3a\u8c03\u5176\u4f5c\u4e3a\u7d27\u51d1\u8868\u793a\u548c\u6587\u5316\u6f14\u5316\u7684\u62bd\u8c61\u5de5\u5177\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u8bed\u8a00\u5728\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u5176\u5728\u4eba\u5de5\u667a\u80fd\u548c\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "method": "\u8ba8\u8bba\u8bed\u8a00\u7684\u4e24\u4e2a\u76f8\u5173\u5c5e\u6027\uff1a\u7d27\u51d1\u7684\u8868\u793a\u548c\u96c6\u4f53\u667a\u6167\u7684\u8fed\u4ee3\u8f93\u51fa\u3002", "result": "\u8bed\u8a00\u7684\u538b\u7f29\u8868\u793a\u4f7f\u5b66\u4e60\u7cfb\u7edf\u80fd\u591f\u9006\u5411\u5de5\u7a0b\u8bb8\u591a\u6982\u5ff5\u548c\u56e0\u679c\u7ed3\u6784\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u5e7f\u6cdb\u7684\u9886\u57df\u80fd\u529b\u3002", "conclusion": "\u8bed\u8a00\u53ef\u80fd\u4e3a\u66f4\u901a\u7528\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u548c\u4eba\u7c7b\u667a\u80fd\u7684\u6838\u5fc3\u65b9\u9762\u63d0\u4f9b\u5173\u952e\u3002"}}
{"id": "2509.15568", "pdf": "https://arxiv.org/pdf/2509.15568", "abs": "https://arxiv.org/abs/2509.15568", "authors": ["Junlong Jia", "Xing Wu", "Chaochen Gao", "Ziyang Chen", "Zijia Lin", "Zhongzhi Li", "Weinong Wang", "Haotian Xu", "Donghui Jin", "Debing Zhang", "Binghui Guo"], "title": "LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "work in progress", "summary": "High-quality long-context data is essential for training large language\nmodels (LLMs) capable of processing extensive documents, yet existing synthesis\napproaches using relevance-based aggregation face challenges of computational\nefficiency. We present LiteLong, a resource-efficient method for synthesizing\nlong-context data through structured topic organization and multi-agent debate.\nOur approach leverages the BISAC book classification system to provide a\ncomprehensive hierarchical topic organization, and then employs a debate\nmechanism with multiple LLMs to generate diverse, high-quality topics within\nthis structure. For each topic, we use lightweight BM25 retrieval to obtain\nrelevant documents and concatenate them into 128K-token training samples.\nExperiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves\ncompetitive long-context performance and can seamlessly integrate with other\nlong-dependency enhancement methods. LiteLong makes high-quality long-context\ndata synthesis more accessible by reducing both computational and data\nengineering costs, facilitating further research in long-context language\ntraining.", "AI": {"tldr": "LiteLong is a resource-efficient method for synthesizing long-context data through structured topic organization and multi-agent debate, achieving competitive performance while reducing computational and data engineering costs.", "motivation": "High-quality long-context data is essential for training large language models (LLMs) capable of processing extensive documents, yet existing synthesis approaches using relevance-based aggregation face challenges of computational efficiency.", "method": "LiteLong is a resource-efficient method for synthesizing long-context data through structured topic organization and multi-agent debate. It leverages the BISAC book classification system for hierarchical topic organization and employs a debate mechanism with multiple LLMs to generate diverse, high-quality topics within this structure. Lightweight BM25 retrieval is used to obtain relevant documents and concatenate them into 128K-token training samples.", "result": "Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves competitive long-context performance and can seamlessly integrate with other long-dependency enhancement methods.", "conclusion": "LiteLong makes high-quality long-context data synthesis more accessible by reducing both computational and data engineering costs, facilitating further research in long-context language training."}}
{"id": "2509.15577", "pdf": "https://arxiv.org/pdf/2509.15577", "abs": "https://arxiv.org/abs/2509.15577", "authors": ["Jaeyoung Kim", "Jongho Kim", "Seung-won Hwang", "Seoho Song", "Young-In Song"], "title": "Relevance to Utility: Process-Supervised Rewrite for RAG", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation systems often suffer from a gap between\noptimizing retrieval relevance and generative utility: retrieved documents may\nbe topically relevant but still lack the content needed for effective reasoning\nduring generation. While existing \"bridge\" modules attempt to rewrite the\nretrieved text for better generation, we show how they fail to capture true\ndocument utility. In this work, we propose R2U, with a key distinction of\ndirectly optimizing to maximize the probability of generating a correct answer\nthrough process supervision. As such direct observation is expensive, we also\npropose approximating an efficient distillation pipeline by scaling the\nsupervision from LLMs, which helps the smaller rewriter model generalize\nbetter. We evaluate our method across multiple open-domain question-answering\nbenchmarks. The empirical results demonstrate consistent improvements over\nstrong bridging baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faR2U\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u751f\u6210\u6b63\u786e\u7b54\u6848\u7684\u6982\u7387\u6765\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u7ba1\u9053\u63d0\u9ad8\u5c0f\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6865\u63a5\u6a21\u5757\u65e0\u6cd5\u6355\u6349\u771f\u6b63\u7684\u6587\u6863\u6548\u7528\uff0c\u56e0\u4e3a\u68c0\u7d22\u5230\u7684\u6587\u672c\u53ef\u80fd\u5728\u4e3b\u9898\u4e0a\u76f8\u5173\uff0c\u4f46\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u6709\u6548\u63a8\u7406\u7684\u5185\u5bb9\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86R2U\uff0c\u901a\u8fc7\u8fc7\u7a0b\u76d1\u7763\u76f4\u63a5\u4f18\u5316\u751f\u6210\u6b63\u786e\u7b54\u6848\u7684\u6982\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u84b8\u998f\u7ba1\u9053\uff0c\u901a\u8fc7\u6269\u5c55LLM\u7684\u76d1\u7763\u6765\u5e2e\u52a9\u8f83\u5c0f\u7684\u91cd\u5199\u6a21\u578b\u66f4\u597d\u5730\u6cdb\u5316\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u5f00\u653e\u9886\u57df\u95ee\u7b54\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6539\u8fdb\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u5f00\u653e\u9886\u57df\u95ee\u7b54\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6865\u63a5\u57fa\u7ebf\u3002"}}
{"id": "2509.15579", "pdf": "https://arxiv.org/pdf/2509.15579", "abs": "https://arxiv.org/abs/2509.15579", "authors": ["Yun Tang", "Cindy Tseng"], "title": "Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Low latency speech human-machine communication is becoming increasingly\nnecessary as speech technology advances quickly in the last decade. One of the\nprimary factors behind the advancement of speech technology is self-supervised\nlearning. Most self-supervised learning algorithms are designed with full\nutterance assumption and compromises have to made if partial utterances are\npresented, which are common in the streaming applications. In this work, we\npropose a chunk based self-supervised learning (Chunk SSL) algorithm as an\nunified solution for both streaming and offline speech pre-training. Chunk SSL\nis optimized with the masked prediction loss and an acoustic encoder is\nencouraged to restore indices of those masked speech frames with help from\nunmasked frames in the same chunk and preceding chunks. A copy and append data\naugmentation approach is proposed to conduct efficient chunk based\npre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to\ndiscretize input speech features and our study shows a high resolution FSQ\ncodebook, i.e., a codebook with vocabulary size up to a few millions, is\nbeneficial to transfer knowledge from the pre-training task to the downstream\ntasks. A group masked prediction loss is employed during pre-training to\nalleviate the high memory and computation cost introduced by the large\ncodebook. The proposed approach is examined in two speech to text tasks, i.e.,\nspeech recognition and speech translation. Experimental results on the\n\\textsc{Librispeech} and \\textsc{Must-C} datasets show that the proposed method\ncould achieve very competitive results for speech to text tasks at both\nstreaming and offline modes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5757\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff08Chunk SSL\uff09\uff0c\u7528\u4e8e\u6d41\u5f0f\u548c\u79bb\u7ebf\u8bed\u97f3\u9884\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8bed\u97f3\u5230\u6587\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u8bed\u97f3\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f4e\u5ef6\u8fdf\u7684\u8bed\u97f3\u4eba\u673a\u901a\u4fe1\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5927\u591a\u6570\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u90fd\u662f\u5728\u5b8c\u6574\u8bdd\u8bed\u5047\u8bbe\u4e0b\u8bbe\u8ba1\u7684\uff0c\u800c\u5728\u6d41\u5f0f\u5e94\u7528\u4e2d\u7ecf\u5e38\u51fa\u73b0\u7684\u90e8\u5206\u8bdd\u8bed\u60c5\u51b5\u4e0b\u9700\u8981\u505a\u51fa\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5757\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff08Chunk SSL\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u63a9\u7801\u9884\u6d4b\u635f\u5931\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u9f13\u52b1\u58f0\u5b66\u7f16\u7801\u5668\u5229\u7528\u540c\u4e00\u5757\u548c\u524d\u4e00\u5757\u4e2d\u7684\u672a\u63a9\u7801\u5e27\u6765\u6062\u590d\u88ab\u63a9\u7801\u7684\u8bed\u97f3\u5e27\u7d22\u5f15\u3002\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u590d\u5236\u548c\u8ffd\u52a0\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u8fdb\u884c\u9ad8\u6548\u7684\u5757\u7ea7\u9884\u8bad\u7ec3\u3002", "result": "\u5728\textsc{Librispeech}\u548c\textsc{Must-C}\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bed\u97f3\u5230\u6587\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u975e\u5e38\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6d41\u5f0f\u548c\u79bb\u7ebf\u6a21\u5f0f\u4e0b\u90fd\u80fd\u5728\u8bed\u97f3\u5230\u6587\u672c\u4efb\u52a1\u4e2d\u53d6\u5f97\u975e\u5e38\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.15587", "pdf": "https://arxiv.org/pdf/2509.15587", "abs": "https://arxiv.org/abs/2509.15587", "authors": ["Tsz Ting Chung", "Lemao Liu", "Mo Yu", "Dit-Yan Yeung"], "title": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP 2025. Project Page:\n  https://ttchungc.github.io/projects/divlogiceval/", "summary": "Logic reasoning in natural language has been recognized as an important\nmeasure of human intelligence for Large Language Models (LLMs). Popular\nbenchmarks may entangle multiple reasoning skills and thus provide unfaithful\nevaluations on the logic reasoning skill. Meanwhile, existing logic reasoning\nbenchmarks are limited in language diversity and their distributions are\ndeviated from the distribution of an ideal logic reasoning benchmark, which may\nlead to biased evaluation results. This paper thereby proposes a new classical\nlogic benchmark DivLogicEval, consisting of natural sentences composed of\ndiverse statements in a counterintuitive way. To ensure a more reliable\nevaluation, we also introduce a new evaluation metric that mitigates the\ninfluence of bias and randomness inherent in LLMs. Through experiments, we\ndemonstrate the extent to which logical reasoning is required to answer the\nquestions in DivLogicEval and compare the performance of different popular LLMs\nin conducting logical reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u903b\u8f91\u63a8\u7406\u57fa\u51c6DivLogicEval\u548c\u4e00\u4e2a\u65b0\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u5728\u8bed\u8a00\u591a\u6837\u6027\u65b9\u9762\u6709\u9650\uff0c\u5176\u5206\u5e03\u504f\u79bb\u4e86\u7406\u60f3\u7684\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u5206\u5e03\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u6709\u504f\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ecf\u5178\u903b\u8f91\u57fa\u51c6DivLogicEval\uff0c\u8be5\u57fa\u51c6\u7531\u591a\u6837\u5316\u7684\u9648\u8ff0\u4ee5\u53cd\u76f4\u89c9\u7684\u65b9\u5f0f\u7ec4\u6210\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u51cf\u8f7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u56fa\u6709\u7684\u504f\u5dee\u548c\u968f\u673a\u6027\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u672c\u6587\u5c55\u793a\u4e86\u56de\u7b54DivLogicEval\u4e2d\u7684\u95ee\u9898\u6240\u9700\u7684\u903b\u8f91\u63a8\u7406\u7a0b\u5ea6\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6d41\u884c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fdb\u884c\u903b\u8f91\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7ecf\u5178\u903b\u8f91\u57fa\u51c6DivLogicEval\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.15620", "pdf": "https://arxiv.org/pdf/2509.15620", "abs": "https://arxiv.org/abs/2509.15620", "authors": ["Bofu Dong", "Pritesh Shah", "Sumedh Sonawane", "Tiyasha Banerjee", "Erin Brady", "Xinya Du", "Ming Jiang"], "title": "SciEvent: Benchmarking Multi-domain Scientific Event Extraction", "categories": ["cs.CL"], "comment": "9 pages, 8 figures (main); 22 pages, 11 figures (appendix). Accepted\n  to EMNLP 2025 (Main Conference)", "summary": "Scientific information extraction (SciIE) has primarily relied on\nentity-relation extraction in narrow domains, limiting its applicability to\ninterdisciplinary research and struggling to capture the necessary context of\nscientific information, often resulting in fragmented or conflicting\nstatements. In this paper, we introduce SciEvent, a novel multi-domain\nbenchmark of scientific abstracts annotated via a unified event extraction (EE)\nschema designed to enable structured and context-aware understanding of\nscientific content. It includes 500 abstracts across five research domains,\nwith manual annotations of event segments, triggers, and fine-grained\narguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting\nabstracts into core scientific activities--Background, Method, Result, and\nConclusion; and (2) extracting the corresponding triggers and arguments.\nExperiments with fine-tuned EE models, large language models (LLMs), and human\nannotators reveal a performance gap, with current models struggling in domains\nsuch as sociology and humanities. SciEvent serves as a challenging benchmark\nand a step toward generalizable, multi-domain SciIE.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SciEvent\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u9886\u57df\u79d1\u5b66\u6458\u8981\u57fa\u51c6\uff0c\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684\u4e8b\u4ef6\u62bd\u53d6\uff08EE\uff09\u6a21\u5f0f\u5b9e\u73b0\u5bf9\u79d1\u5b66\u5185\u5bb9\u7684\u7ed3\u6784\u5316\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7406\u89e3\u3002SciEvent\u5305\u542b\u4e94\u4e2a\u7814\u7a76\u9886\u57df\u7684500\u4e2a\u6458\u8981\uff0c\u5e76\u8fdb\u884c\u4e86\u624b\u52a8\u6807\u6ce8\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6a21\u578b\u5728\u793e\u4f1a\u5b66\u548c\u4eba\u6587\u5b66\u79d1\u7b49\u9886\u57df\u7684\u8868\u73b0\u4e0d\u4f73\u3002SciEvent\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u662f\u5411\u901a\u7528\u3001\u591a\u9886\u57dfSciIE\u8fc8\u51fa\u7684\u4e00\u6b65\u3002", "motivation": "Scientific information extraction (SciIE) has primarily relied on entity-relation extraction in narrow domains, limiting its applicability to interdisciplinary research and struggling to capture the necessary context of scientific information, often resulting in fragmented or conflicting statements.", "method": "We define SciIE as a multi-stage EE pipeline: (1) segmenting abstracts into core scientific activities--Background, Method, Result, and Conclusion; and (2) extracting the corresponding triggers and arguments.", "result": "Experiments with fine-tuned EE models, large language models (LLMs), and human annotators reveal a performance gap, with current models struggling in domains such as sociology and humanities.", "conclusion": "SciEvent serves as a challenging benchmark and a step toward generalizable, multi-domain SciIE."}}
{"id": "2509.15621", "pdf": "https://arxiv.org/pdf/2509.15621", "abs": "https://arxiv.org/abs/2509.15621", "authors": ["Tomoya Yamashita", "Yuuki Yamanaka", "Masanori Yamada", "Takayuki Miura", "Toshiki Shibahara", "Tomoharu Iwata"], "title": "Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine Unlearning (MU) has recently attracted considerable attention as a\nsolution to privacy and copyright issues in large language models (LLMs).\nExisting MU methods aim to remove specific target sentences from an LLM while\nminimizing damage to unrelated knowledge. However, these approaches require\nexplicit target sentences and do not support removing broader concepts, such as\npersons or events. To address this limitation, we introduce Concept Unlearning\n(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to\nrepresent the LLM's internal knowledge and define CU as removing the forgetting\ntarget nodes and associated edges. This graph-based formulation enables a more\nintuitive unlearning and facilitates the design of more effective methods. We\npropose a novel method that prompts the LLM to generate knowledge triplets and\nexplanatory sentences about the forgetting target and applies the unlearning\nprocess to these representations. Our approach enables more precise and\ncomprehensive concept removal by aligning the unlearning process with the LLM's\ninternal knowledge representations. Experiments on real-world and synthetic\ndatasets demonstrate that our method effectively achieves concept-level\nunlearning while preserving unrelated knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6982\u5ff5\u9057\u5fd8\uff08CU\uff09\u4f5c\u4e3a\u673a\u5668\u9057\u5fd8\u7684\u65b0\u8981\u6c42\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u8868\u793aLLM\u7684\u5185\u90e8\u77e5\u8bc6\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u5b9e\u73b0\u66f4\u7cbe\u786e\u548c\u5168\u9762\u7684\u6982\u5ff5\u5220\u9664\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u9700\u8981\u660e\u786e\u7684\u76ee\u6807\u53e5\u5b50\uff0c\u4e0d\u652f\u6301\u5220\u9664\u66f4\u5e7f\u6cdb\u7684\u6982\u5ff5\uff0c\u5982\u4eba\u6216\u4e8b\u4ef6\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6982\u5ff5\u9057\u5fd8\uff08CU\uff09\u4f5c\u4e3aLLM\u9057\u5fd8\u7684\u65b0\u8981\u6c42\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793aLLM\u751f\u6210\u5173\u4e8e\u9057\u5fd8\u76ee\u6807\u7684\u77e5\u8bc6\u4e09\u5143\u7ec4\u548c\u89e3\u91ca\u6027\u53e5\u5b50\uff0c\u5e76\u5c06\u9057\u5fd8\u8fc7\u7a0b\u5e94\u7528\u4e8e\u8fd9\u4e9b\u8868\u793a\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5c06\u9057\u5fd8\u8fc7\u7a0b\u4e0eLLM\u7684\u5185\u90e8\u77e5\u8bc6\u8868\u793a\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u5168\u9762\u7684\u6982\u5ff5\u5220\u9664\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u80fd\u591f\u6709\u6548\u5730\u5b9e\u73b0\u6982\u5ff5\u7ea7\u522b\u7684\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u7559\u4e0d\u76f8\u5173\u7684\u77e5\u8bc6\u3002"}}
{"id": "2509.15631", "pdf": "https://arxiv.org/pdf/2509.15631", "abs": "https://arxiv.org/abs/2509.15631", "authors": ["Tomoya Yamashita", "Akira Ito", "Yuuki Yamanaka", "Masanori Yamada", "Takayuki Miura", "Toshiki Shibahara"], "title": "Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed across various\napplications, privacy and copyright concerns have heightened the need for more\neffective LLM unlearning techniques. Many existing unlearning methods aim to\nsuppress undesirable outputs through additional training (e.g., gradient\nascent), which reduces the probability of generating such outputs. While such\nsuppression-based approaches can control model outputs, they may not eliminate\nthe underlying knowledge embedded in the model's internal activations; muting a\nresponse is not the same as forgetting it. Moreover, such suppression-based\nmethods often suffer from model collapse. To address these issues, we propose a\nnovel unlearning method that directly intervenes in the model's internal\nactivations. In our formulation, forgetting is defined as a state in which the\nactivation of a forgotten target is indistinguishable from that of ``unknown''\nentities. Our method introduces an unlearning objective that modifies the\nactivation of the target entity away from those of known entities and toward\nthose of unknown entities in a sparse autoencoder latent space. By aligning the\ntarget's internal activation with those of unknown entities, we shift the\nmodel's recognition of the target entity from ``known'' to ``unknown'',\nachieving genuine forgetting while avoiding over-suppression and model\ncollapse. Empirically, we show that our method effectively aligns the internal\nactivations of the forgotten target, a result that the suppression-based\napproaches do not reliably achieve. Additionally, our method effectively\nreduces the model's recall of target knowledge in question-answering tasks\nwithout significant damage to the non-target knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e72\u9884\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u5b9e\u73b0\u771f\u6b63\u7684\u9057\u5fd8\uff0c\u907f\u514d\u4e86\u8fc7\u5ea6\u6291\u5236\u548c\u6a21\u578b\u5d29\u6e83\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6291\u5236\u7684\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6d88\u9664\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\uff0c\u4e14\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684LLM\u9057\u5fd8\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u7684\u65b9\u6cd5\u901a\u8fc7\u5728\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4fee\u6539\u76ee\u6807\u5b9e\u4f53\u7684\u6fc0\u6d3b\uff0c\u4f7f\u5176\u8fdc\u79bb\u5df2\u77e5\u5b9e\u4f53\u5e76\u63a5\u8fd1\u672a\u77e5\u5b9e\u4f53\uff0c\u4ece\u800c\u5b9e\u73b0\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5bf9\u9f50\u88ab\u9057\u5fd8\u76ee\u6807\u7684\u5185\u90e8\u6fc0\u6d3b\uff0c\u5e76\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u51cf\u5c11\u6a21\u578b\u5bf9\u76ee\u6807\u77e5\u8bc6\u7684\u56de\u5fc6\uff0c\u800c\u4e0d\u4f1a\u663e\u8457\u635f\u5bb3\u975e\u76ee\u6807\u77e5\u8bc6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u9057\u5fd8\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u76f4\u63a5\u5e72\u9884\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u9057\u5fd8\uff0c\u540c\u65f6\u907f\u514d\u4e86\u8fc7\u5ea6\u6291\u5236\u548c\u6a21\u578b\u5d29\u6e83\u3002"}}
{"id": "2509.15640", "pdf": "https://arxiv.org/pdf/2509.15640", "abs": "https://arxiv.org/abs/2509.15640", "authors": ["Nhu Vo", "Nu-Uyen-Phuong Le", "Dung D. Le", "Massimo Piccardi", "Wray Buntine"], "title": "Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation", "categories": ["cs.CL"], "comment": "The work is under peer review", "summary": "Medical English-Vietnamese machine translation (En-Vi MT) is essential for\nhealthcare access and communication in Vietnam, yet Vietnamese remains a\nlow-resource and under-studied language. We systematically evaluate prompting\nstrategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset,\ncomparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict,\nan English-Vietnamese medical lexicon. Results show that model scale is the\nprimary driver of performance: larger LLMs achieve strong zero-shot results,\nwhile few-shot prompting yields only marginal improvements. In contrast,\nterminology-aware cues and embedding-based example retrieval consistently\nimprove domain-specific translation. These findings underscore both the promise\nand the current limitations of multilingual LLMs for medical En-Vi MT.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u82f1\u8bed-\u8d8a\u5357\u8bed\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u662f\u6027\u80fd\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u672f\u8bed\u610f\u8bc6\u63d0\u793a\u548c\u57fa\u4e8e\u5d4c\u5165\u7684\u4f8b\u5b50\u68c0\u7d22\u80fd\u6709\u6548\u63d0\u5347\u7279\u5b9a\u9886\u57df\u7684\u7ffb\u8bd1\u6548\u679c\u3002", "motivation": "\u533b\u7597\u82f1\u8bed-\u8d8a\u5357\u8bed\u673a\u5668\u7ffb\u8bd1\u5bf9\u4e8e\u8d8a\u5357\u7684\u533b\u7597\u83b7\u53d6\u548c\u4ea4\u6d41\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8d8a\u5357\u8bed\u4ecd\u7136\u662f\u8d44\u6e90\u532e\u4e4f\u548c\u7814\u7a76\u4e0d\u8db3\u7684\u8bed\u8a00\u3002", "method": "\u6211\u4eec\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u516d\u79cd\u591a\u8bed\u8a00LLM\uff080.5B-9B\u53c2\u6570\uff09\u5728MedEV\u6570\u636e\u96c6\u4e0a\u7684\u63d0\u793a\u7b56\u7565\uff0c\u6bd4\u8f83\u4e86\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u57fa\u4e8e\u8bcd\u5178\u7684\u63d0\u793a\u4e0eMeddict\uff0c\u4e00\u4e2a\u82f1\u8bed-\u8d8a\u5357\u8bed\u533b\u5b66\u8bcd\u5178\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u89c4\u6a21\u662f\u6027\u80fd\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff1a\u66f4\u5927\u7684LLM\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u7ed3\u679c\uff0c\u800c\u5c11\u6837\u672c\u63d0\u793a\u4ec5\u5e26\u6765\u5fae\u5c0f\u7684\u6539\u8fdb\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u672f\u8bed\u610f\u8bc6\u63d0\u793a\u548c\u57fa\u4e8e\u5d4c\u5165\u7684\u4f8b\u5b50\u68c0\u7d22\u6301\u7eed\u63d0\u9ad8\u4e86\u7279\u5b9a\u9886\u57df\u7684\u7ffb\u8bd1\u6548\u679c\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u82f1\u8bed-\u8d8a\u5357\u8bed\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6f5c\u529b\u548c\u5f53\u524d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.15655", "pdf": "https://arxiv.org/pdf/2509.15655", "abs": "https://arxiv.org/abs/2509.15655", "authors": ["Linyang He", "Qiaolin Wang", "Xilin Jiang", "Nima Mesgarani"], "title": "Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations", "categories": ["cs.CL", "eess.AS"], "comment": "EMNLP 2025 Main Conference (Oral)", "summary": "Transformer-based speech language models (SLMs) have significantly improved\nneural speech recognition and understanding. While existing research has\nexamined how well SLMs encode shallow acoustic and phonetic features, the\nextent to which SLMs encode nuanced syntactic and conceptual features remains\nunclear. By drawing parallels with linguistic competence assessments for large\nlanguage models, this study is the first to systematically evaluate the\npresence of contextual syntactic and semantic features across SLMs for\nself-supervised learning (S3M), automatic speech recognition (ASR), speech\ncompression (codec), and as the encoder for auditory large language models\n(AudioLLMs). Through minimal pair designs and diagnostic feature analysis\nacross 71 tasks spanning diverse linguistic levels, our layer-wise and\ntime-resolved analysis uncovers that 1) all speech encode grammatical features\nmore robustly than conceptual ones.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001\u8bed\u97f3\u538b\u7f29\u548c\u542c\u89c9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u8bed\u6cd5\u548c\u8bed\u4e49\u7279\u5f81\u3002", "motivation": "\u8bc4\u4f30\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4e2d\u4e0a\u4e0b\u6587\u8bed\u6cd5\u548c\u8bed\u4e49\u7279\u5f81\u7684\u5b58\u5728\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5bf9\u8bbe\u8ba1\u548c\u8de871\u9879\u4efb\u52a1\u7684\u8bca\u65ad\u7279\u5f81\u5206\u6790\uff0c\u8fdb\u884c\u9010\u5c42\u548c\u65f6\u95f4\u89e3\u6790\u5206\u6790\u3002", "result": "\u6240\u6709\u8bed\u97f3\u7f16\u7801\u5728\u8bed\u6cd5\u7279\u5f81\u4e0a\u6bd4\u6982\u5ff5\u7279\u5f81\u66f4\u7a33\u5065\u3002", "conclusion": "\u6240\u6709\u8bed\u97f3\u7f16\u7801\u5728\u8bed\u6cd5\u7279\u5f81\u4e0a\u6bd4\u6982\u5ff5\u7279\u5f81\u66f4\u7a33\u5065\u3002"}}
{"id": "2509.15667", "pdf": "https://arxiv.org/pdf/2509.15667", "abs": "https://arxiv.org/abs/2509.15667", "authors": ["Dimitrios Damianos", "Leon Voukoutis", "Georgios Paraskevopoulos", "Vassilis Katsouros"], "title": "VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We present a multimodal fusion framework that bridges pre-trained\ndecoder-based large language models (LLM) and acoustic encoder-decoder\narchitectures such as Whisper, with the aim of building speech-enabled LLMs.\nInstead of directly using audio embeddings, we explore an intermediate\naudio-conditioned text space as a more effective mechanism for alignment. Our\nmethod operates fully in continuous text representation spaces, fusing\nWhisper's hidden decoder states with those of an LLM through cross-modal\nattention, and supports both offline and streaming modes. We introduce\n\\textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that\nour approach effectively aligns representations across modalities. These\nresults highlight continuous space fusion as a promising path for multilingual\nand low-resource speech LLMs, while achieving state-of-the-art results for\nAutomatic Speech Recognition in Greek, providing an average $\\sim20\\%$ relative\nimprovement across benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u58f0\u5b66\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff08\u5982Whisper\uff09\u7ed3\u5408\u8d77\u6765\uff0c\u4ee5\u6784\u5efa\u8bed\u97f3\u542f\u7528\u7684LLM\u3002\u901a\u8fc7\u63a2\u7d22\u4e2d\u95f4\u97f3\u9891\u6761\u4ef6\u6587\u672c\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u8868\u793a\u7684\u6709\u6548\u5bf9\u9f50\uff0c\u5e76\u5728\u5e0c\u814a\u8bed\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u6784\u5efa\u8bed\u97f3\u542f\u7528\u7684LLM\uff0c\u901a\u8fc7\u63a2\u7d22\u4e2d\u95f4\u97f3\u9891\u6761\u4ef6\u6587\u672c\u7a7a\u95f4\u6765\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5bf9\u9f50\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u4f7f\u7528\u97f3\u9891\u5d4c\u5165\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u58f0\u5b66\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff08\u5982Whisper\uff09\u8054\u7cfb\u8d77\u6765\uff0c\u901a\u8fc7\u4ea4\u53c9\u6a21\u6001\u6ce8\u610f\u529b\u5c06Whisper\u7684\u9690\u85cf\u89e3\u7801\u5668\u72b6\u6001\u4e0eLLM\u7684\u72b6\u6001\u878d\u5408\uff0c\u5e76\u652f\u6301\u79bb\u7ebf\u548c\u6d41\u5f0f\u6a21\u5f0f\u3002", "result": "\u6211\u4eec\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u5e0c\u814a\u8bed\u8bed\u97f3LLM VoxKrikri\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u5bf9\u9f50\u4e86\u8de8\u6a21\u6001\u7684\u8868\u793a\uff0c\u5728\u5e0c\u814a\u8bed\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u5b9e\u73b0\u4e86\u5e73\u5747\u7ea620%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u8de8\u6a21\u6001\u8868\u793a\u5bf9\u9f50\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u97f3LLM\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u540c\u65f6\u5728\u5e0c\u814a\u8bed\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.15701", "pdf": "https://arxiv.org/pdf/2509.15701", "abs": "https://arxiv.org/abs/2509.15701", "authors": ["Ke Wang", "Wenning Wei", "Yan Deng", "Lei He", "Sheng Zhao"], "title": "Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to ICASSP2026", "summary": "Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted\nLanguage Learning (CALL), requiring evaluation across multiple granularities\nand aspects. Large Multimodal Models (LMMs) present new opportunities for APA,\nbut their effectiveness in fine-grained assessment remains uncertain. This work\ninvestigates fine-tuning LMMs for APA using the Speechocean762 dataset and a\nprivate corpus. Fine-tuning significantly outperforms zero-shot settings and\nachieves competitive results on single-granularity tasks compared to public and\ncommercial systems. The model performs well at word and sentence levels, while\nphoneme-level assessment remains challenging. We also observe that the Pearson\nCorrelation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation\nCoefficient (SCC) remains around 0.6, suggesting that SCC better reflects\nordinal consistency. These findings highlight both the promise and limitations\nof LMMs for APA and point to future work on fine-grained modeling and\nrank-aware evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u53d1\u97f3\u8bc4\u4f30\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5fae\u8c03\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u97f3\u7d20\u5c42\u9762\u4ecd\u6709\u6311\u6218\u3002", "motivation": "\u81ea\u52a8\u53d1\u97f3\u8bc4\u4f30\u5bf9\u4e8e\u8ba1\u7b97\u673a\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5728\u591a\u4e2a\u7c92\u5ea6\u548c\u65b9\u9762\u8fdb\u884c\u8bc4\u4f30\u3002\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e3aAPA\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u4f46\u5176\u5728\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\u4ecd\u4e0d\u786e\u5b9a\u3002", "method": "\u4f7f\u7528Speechocean762\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u79c1\u6709\u8bed\u6599\u5e93\u5bf9\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u8fdb\u884c\u81ea\u52a8\u53d1\u97f3\u8bc4\u4f30\u3002", "result": "\u5fae\u8c03\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u8bbe\u7f6e\uff0c\u5e76\u5728\u5355\u7c92\u5ea6\u4efb\u52a1\u4e0a\u4e0e\u516c\u5171\u548c\u5546\u4e1a\u7cfb\u7edf\u76f8\u6bd4\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002\u6a21\u578b\u5728\u5355\u8bcd\u548c\u53e5\u5b50\u5c42\u9762\u8868\u73b0\u826f\u597d\uff0c\u800c\u97f3\u7d20\u5c42\u9762\u7684\u8bc4\u4f30\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u8fbe\u52300.9\uff0c\u800c\u65af\u76ae\u5c14\u66fc\u7b49\u7ea7\u76f8\u5173\u7cfb\u6570\u7ea6\u4e3a0.6\uff0c\u8fd9\u8868\u660e\u65af\u76ae\u5c14\u66fc\u7b49\u7ea7\u76f8\u5173\u7cfb\u6570\u66f4\u80fd\u53cd\u6620\u987a\u5e8f\u4e00\u81f4\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u81ea\u52a8\u53d1\u97f3\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5728\u7ec6\u7c92\u5ea6\u5efa\u6a21\u548c\u6392\u540d\u611f\u77e5\u8bc4\u4f30\u65b9\u9762\u7684\u5de5\u4f5c\u65b9\u5411\u3002"}}
{"id": "2509.15714", "pdf": "https://arxiv.org/pdf/2509.15714", "abs": "https://arxiv.org/abs/2509.15714", "authors": ["Jonas Mayer Martins", "Ali Hamza Bashir", "Muhammad Rehan Khalid", "Lisa Beinborn"], "title": "Once Upon a Time: Interactive Learning for Storytelling with Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025, BabyLM Challenge; 16 pages, 6 figures", "summary": "Children efficiently acquire language not just by listening, but by\ninteracting with others in their social environment. Conversely, large language\nmodels are typically trained with next-word prediction on massive amounts of\ntext. Motivated by this contrast, we investigate whether language models can be\ntrained with less data by learning not only from next-word prediction but also\nfrom high-level, cognitively inspired feedback. We train a student model to\ngenerate stories, which a teacher model rates on readability, narrative\ncoherence, and creativity. By varying the amount of pretraining before the\nfeedback loop, we assess the impact of this interactive learning on formal and\nfunctional linguistic competence. We find that the high-level feedback is\nhighly data efficient: With just 1 M words of input in interactive learning,\nstorytelling skills can improve as much as with 410 M words of next-word\nprediction.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u9ad8\u5c42\u6b21\u53cd\u9988\uff0c\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u6709\u6548\u63d0\u5347\u8bb2\u6545\u4e8b\u80fd\u529b\u3002", "motivation": "\u513f\u7ae5\u901a\u8fc7\u4e0e\u793e\u4f1a\u73af\u5883\u4e2d\u7684\u4ed6\u4eba\u4e92\u52a8\u6765\u9ad8\u6548\u83b7\u53d6\u8bed\u8a00\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u901a\u8fc7\u5927\u91cf\u6587\u672c\u8fdb\u884c\u4e0b\u4e00\u6b65\u8bcd\u9884\u6d4b\u8bad\u7ec3\u3002\u53d7\u6b64\u5bf9\u6bd4\u542f\u53d1\uff0c\u6211\u4eec\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u9ad8\u5c42\u6b21\u7684\u8ba4\u77e5\u53cd\u9988\u6765\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u9700\u6c42\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u5b66\u751f\u6a21\u578b\u6765\u751f\u6210\u6545\u4e8b\uff0c\u5e76\u8ba9\u6559\u5e08\u6a21\u578b\u5bf9\u53ef\u8bfb\u6027\u3001\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u521b\u9020\u529b\u8fdb\u884c\u8bc4\u5206\uff0c\u901a\u8fc7\u6539\u53d8\u53cd\u9988\u5faa\u73af\u524d\u7684\u9884\u8bad\u7ec3\u91cf\u6765\u8bc4\u4f30\u4ea4\u4e92\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u5c42\u6b21\u53cd\u9988\u975e\u5e38\u6570\u636e\u9ad8\u6548\uff1a\u5728\u4ea4\u4e92\u5b66\u4e60\u4e2d\u4ec5\u4f7f\u7528100\u4e07\u8bcd\u7684\u8f93\u5165\uff0c\u8bb2\u6545\u4e8b\u7684\u80fd\u529b\u53ef\u4ee5\u63d0\u5347\u5230\u4e0e4.1\u4ebf\u8bcd\u7684\u4e0b\u4e00\u6b65\u8bcd\u9884\u6d4b\u76f8\u5f53\u7684\u7a0b\u5ea6\u3002", "conclusion": "\u9ad8\u5c42\u6b21\u53cd\u9988\u5728\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u975e\u5e38\u6570\u636e\u9ad8\u6548\uff0c\u53ef\u4ee5\u5728\u5c11\u91cf\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u8bb2\u6545\u4e8b\u7684\u80fd\u529b\u3002"}}
{"id": "2509.15723", "pdf": "https://arxiv.org/pdf/2509.15723", "abs": "https://arxiv.org/abs/2509.15723", "authors": ["Nannan Huang", "Haytham M. Fayek", "Xiuzhen Zhang"], "title": "REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting", "categories": ["cs.CL"], "comment": "Accepted to the 5th New Frontiers in Summarization Workshop\n  (NewSumm@EMNLP 2025)", "summary": "Individuals express diverse opinions, a fair summary should represent these\nviewpoints comprehensively. Previous research on fairness in opinion\nsummarisation using large language models (LLMs) relied on hyperparameter\ntuning or providing ground truth distributional information in prompts.\nHowever, these methods face practical limitations: end-users rarely modify\ndefault model parameters, and accurate distributional information is often\nunavailable. Building upon cognitive science research demonstrating that\nfrequency-based representations reduce systematic biases in human statistical\nreasoning by making reference classes explicit and reducing cognitive load,\nthis study investigates whether frequency framed prompting (REFER) can\nsimilarly enhance fairness in LLM opinion summarisation. Through systematic\nexperimentation with different prompting frameworks, we adapted techniques\nknown to improve human reasoning to elicit more effective information\nprocessing in language models compared to abstract probabilistic\nrepresentations.Our results demonstrate that REFER enhances fairness in\nlanguage models when summarising opinions. This effect is particularly\npronounced in larger language models and using stronger reasoning instructions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5REFER\uff0c\u901a\u8fc7\u9891\u7387\u6846\u67b6\u63d0\u793a\u6765\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u5728\u610f\u89c1\u603b\u7ed3\u4e2d\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u4ee5\u5f80\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8d85\u53c2\u6570\u8c03\u6574\u6216\u63d0\u4f9b\u5730\u9762\u771f\u5b9e\u5206\u5e03\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7684\u7814\u7a76\uff0c\u5229\u7528\u9891\u7387\u6846\u67b6\u63d0\u793a\uff08REFER\uff09\u6765\u51cf\u5c11\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cREFER\u80fd\u591f\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u5728\u610f\u89c1\u603b\u7ed3\u4e2d\u7684\u516c\u5e73\u6027\uff0c\u7279\u522b\u662f\u5728\u8f83\u5927\u7684\u6a21\u578b\u548c\u66f4\u5f3a\u7684\u63a8\u7406\u6307\u4ee4\u4e0b\u6548\u679c\u66f4\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cREFER\u53ef\u4ee5\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u5728\u603b\u7ed3\u610f\u89c1\u65f6\u7684\u516c\u5e73\u6027\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5927\u7684\u8bed\u8a00\u6a21\u578b\u548c\u4f7f\u7528\u66f4\u5f3a\u7684\u63a8\u7406\u6307\u4ee4\u65f6\u6548\u679c\u66f4\u4e3a\u660e\u663e\u3002"}}
{"id": "2509.15739", "pdf": "https://arxiv.org/pdf/2509.15739", "abs": "https://arxiv.org/abs/2509.15739", "authors": ["Reza Sanayei", "Srdjan Vesic", "Eduardo Blanco", "Mihai Surdeanu"], "title": "Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) excel at linear reasoning tasks but remain\nunderexplored on non-linear structures such as those found in natural debates,\nwhich are best expressed as argument graphs. We evaluate whether LLMs can\napproximate structured reasoning from Computational Argumentation Theory (CAT).\nSpecifically, we use Quantitative Argumentation Debate (QuAD) semantics, which\nassigns acceptability scores to arguments based on their attack and support\nrelations. Given only dialogue-formatted debates from two NoDE datasets, models\nare prompted to rank arguments without access to the underlying graph. We test\nseveral LLMs under advanced instruction strategies, including Chain-of-Thought\nand In-Context Learning. While models show moderate alignment with QuAD\nrankings, performance degrades with longer inputs or disrupted discourse flow.\nAdvanced prompting helps mitigate these effects by reducing biases related to\nargument length and position. Our findings highlight both the promise and\nlimitations of LLMs in modeling formal argumentation semantics and motivate\nfuture work on graph-aware reasoning.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u975e\u7ebf\u6027\u7ed3\u6784\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5efa\u6a21\u5f62\u5f0f\u5316\u8bba\u8bc1\u8bed\u4e49\u65b9\u9762\u6709\u6f5c\u529b\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u975e\u7ebf\u6027\u7ed3\u6784\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5982\u81ea\u7136\u8fa9\u8bba\u4e2d\u7684\u8bba\u8bc1\u56fe\u3002", "method": "\u6211\u4eec\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u8fd1\u4f3c\u8ba1\u7b97\u8bba\u8bc1\u7406\u8bba\uff08CAT\uff09\u4e2d\u7684\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5177\u4f53\u4f7f\u7528\u4e86\u5b9a\u91cf\u8bba\u8bc1\u8fa9\u8bba\uff08QuAD\uff09\u8bed\u4e49\uff0c\u8be5\u8bed\u4e49\u57fa\u4e8e\u653b\u51fb\u548c\u652f\u6301\u5173\u7cfb\u4e3a\u8bba\u8bc1\u5206\u914d\u53ef\u63a5\u53d7\u6027\u5206\u6570\u3002", "result": "\u6a21\u578b\u5728\u4e0eQuAD\u6392\u540d\u7684\u5bf9\u9f50\u4e0a\u8868\u73b0\u51fa\u4e2d\u7b49\u7a0b\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u5728\u8f93\u5165\u8f83\u957f\u6216\u8bdd\u8bed\u6d41\u88ab\u6253\u65ad\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u5148\u8fdb\u7684\u63d0\u793a\u7b56\u7565\u6709\u52a9\u4e8e\u51cf\u8f7b\u8fd9\u4e9b\u5f71\u54cd\uff0c\u51cf\u5c11\u4e0e\u8bba\u8bc1\u957f\u5ea6\u548c\u4f4d\u7f6e\u76f8\u5173\u7684\u504f\u5dee\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5efa\u6a21\u5f62\u5f0f\u5316\u8bba\u8bc1\u8bed\u4e49\u65b9\u9762\u65e2\u6709\u6f5c\u529b\u4e5f\u6709\u5c40\u9650\u6027\uff0c\u8fd9\u4fc3\u4f7f\u672a\u6765\u7684\u7814\u7a76\u5173\u6ce8\u4e8e\u56fe\u611f\u77e5\u63a8\u7406\u3002"}}
{"id": "2509.15763", "pdf": "https://arxiv.org/pdf/2509.15763", "abs": "https://arxiv.org/abs/2509.15763", "authors": ["Chenlong Deng", "Zhisong Zhang", "Kelong Mao", "Shuaiyi Li", "Tianqing Fang", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Zhicheng Dou"], "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression", "categories": ["cs.CL"], "comment": "15 pages, 7 figures", "summary": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling.", "AI": {"tldr": "UniGist \u662f\u4e00\u79cd\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u7279\u6b8a\u538b\u7f29\u6807\u8bb0\u6765\u4fdd\u7559\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u65b9\u9762\u8d8a\u6765\u8d8a\u5f3a\u5927\uff0c\u4f46\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u7684\u5185\u5b58\u5f00\u9500\u4ecd\u7136\u662f\u901a\u7528\u90e8\u7f72\u7684\u4e3b\u8981\u74f6\u9888\u3002\u867d\u7136\u5df2\u7ecf\u63a2\u7d22\u4e86\u5404\u79cd\u538b\u7f29\u7b56\u7565\uff0c\u4f46\u5e8f\u5217\u7ea7\u538b\u7f29\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5b83\u53ef\u80fd\u5bfc\u81f4\u91cd\u8981\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u4e22\u5931\u3002", "method": "\u5f15\u5165\u4e86 UniGist\uff0c\u4e00\u79cd\u5e8f\u5217\u7ea7\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u4ee5\u7ec6\u7c92\u5ea6\u65b9\u5f0f\u7528\u7279\u6b8a\u538b\u7f29\u6807\u8bb0\uff08gists\uff09\u66ff\u6362\u539f\u59cb\u6807\u8bb0\u6765\u6709\u6548\u4fdd\u7559\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u91c7\u7528\u65e0\u5757\u8bad\u7ec3\u7b56\u7565\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u5185\u6838\uff0c\u652f\u6301\u7075\u6d3b\u7684\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUniGist \u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u7ec6\u8282\u56de\u5fc6\u4efb\u52a1\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "UniGist \u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u7ec6\u8282\u56de\u5fc6\u4efb\u52a1\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.15789", "pdf": "https://arxiv.org/pdf/2509.15789", "abs": "https://arxiv.org/abs/2509.15789", "authors": ["Qiuyang Lu", "Fangjian Shen", "Zhengkai Tang", "Qiang Liu", "Hexuan Cheng", "Hui Liu", "Wushao Wen"], "title": "UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations", "categories": ["cs.CL", "cs.LG"], "comment": "5 pages, 1 figure, submitted to ICASSP2026", "summary": "The quality and accessibility of multilingual datasets are crucial for\nadvancing machine translation. However, previous corpora built from United\nNations documents have suffered from issues such as opaque process, difficulty\nof reproduction, and limited scale. To address these challenges, we introduce a\ncomplete end-to-end solution, from data acquisition via web scraping to text\nalignment. The entire process is fully reproducible, with a minimalist\nsingle-machine example and optional distributed computing steps for\nscalability. At its core, we propose a new Graph-Aided Paragraph Alignment\n(GAPA) algorithm for efficient and flexible paragraph-level alignment. The\nresulting corpus contains over 713 million English tokens, more than doubling\nthe scale of prior work. To the best of our knowledge, this represents the\nlargest publicly available parallel corpus composed entirely of\nhuman-translated, non-AI-generated content. Our code and corpus are accessible\nunder the MIT License.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u6784\u5efa\u5927\u89c4\u6a21\u3001\u53ef\u91cd\u590d\u7684\u591a\u8bed\u8a00\u8bed\u6599\u5e93\uff0c\u5e76\u5f15\u5165\u4e86GAPA\u7b97\u6cd5\u8fdb\u884c\u6bb5\u843d\u7ea7\u5bf9\u9f50\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u8bed\u6599\u5e93\u662f\u76ee\u524d\u6700\u5927\u7684\u516c\u5f00\u53ef\u7528\u4eba\u5de5\u7ffb\u8bd1\u8bed\u6599\u5e93\u3002", "motivation": "\u591a\u8bed\u8a00\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u53ef\u8bbf\u95ee\u6027\u5bf9\u4e8e\u63a8\u8fdb\u673a\u5668\u7ffb\u8bd1\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4ee5\u524d\u4ece\u8054\u5408\u56fd\u6587\u4ef6\u6784\u5efa\u7684\u8bed\u6599\u5e93\u5b58\u5728\u8fc7\u7a0b\u4e0d\u900f\u660e\u3001\u96be\u4ee5\u590d\u5236\u548c\u89c4\u6a21\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u7f51\u7edc\u722c\u866b\u83b7\u53d6\u6570\u636e\u5230\u6587\u672c\u5bf9\u9f50\u3002\u6574\u4e2a\u8fc7\u7a0b\u662f\u5b8c\u5168\u53ef\u91cd\u590d\u7684\uff0c\u5305\u62ec\u4e00\u4e2a\u6700\u5c0f\u5316\u7684\u5355\u673a\u793a\u4f8b\u548c\u53ef\u9009\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u6b65\u9aa4\u4ee5\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u3002\u6838\u5fc3\u662f\u4e00\u4e2a\u65b0\u7684\u56fe\u8f85\u52a9\u6bb5\u843d\u5bf9\u9f50\uff08GAPA\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u6bb5\u843d\u7ea7\u5bf9\u9f50\u3002", "result": "\u751f\u6210\u7684\u8bed\u6599\u5e93\u5305\u542b\u8d85\u8fc77.13\u4ebf\u4e2a\u82f1\u8bed\u6807\u8bb0\uff0c\u89c4\u6a21\u8d85\u8fc7\u4e4b\u524d\u5de5\u4f5c\u7684\u4e24\u500d\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u6700\u5927\u7684\u516c\u5f00\u53ef\u7528\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u5b8c\u5168\u7531\u4eba\u5de5\u7ffb\u8bd1\u800c\u975eAI\u751f\u6210\u7684\u5185\u5bb9\u7ec4\u6210\u3002", "conclusion": "\u6211\u4eec\u7684\u4ee3\u7801\u548c\u8bed\u6599\u5e93\u5728MIT\u8bb8\u53ef\u8bc1\u4e0b\u53ef\u83b7\u53d6\uff0c\u8fd9\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u8d44\u6e90\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63a8\u52a8\u673a\u5668\u7ffb\u8bd1\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.15793", "pdf": "https://arxiv.org/pdf/2509.15793", "abs": "https://arxiv.org/abs/2509.15793", "authors": ["Yufeng Li", "Arkaitz Zubiaga"], "title": "RAVE: Retrieval and Scoring Aware Verifiable Claim Detection", "categories": ["cs.CL"], "comment": "5 pages, 1 figure", "summary": "The rapid spread of misinformation on social media underscores the need for\nscalable fact-checking tools. A key step is claim detection, which identifies\nstatements that can be objectively verified. Prior approaches often rely on\nlinguistic cues or claim check-worthiness, but these struggle with vague\npolitical discourse and diverse formats such as tweets. We present RAVE\n(Retrieval and Scoring Aware Verifiable Claim Detection), a framework that\ncombines evidence retrieval with structured signals of relevance and source\ncredibility. Experiments on CT22-test and PoliClaim-test show that RAVE\nconsistently outperforms text-only and retrieval-based baselines in both\naccuracy and F1.", "AI": {"tldr": "RAVE is a framework that combines evidence retrieval with structured signals of relevance and source credibility to detect verifiable claims, and it outperforms existing methods.", "motivation": "The rapid spread of misinformation on social media underscores the need for scalable fact-checking tools. A key step is claim detection, which identifies statements that can be objectively verified.", "method": "RAVE combines evidence retrieval with structured signals of relevance and source credibility.", "result": "Experiments on CT22-test and PoliClaim-test show that RAVE consistently outperforms text-only and retrieval-based baselines in both accuracy and F1.", "conclusion": "RAVE consistently outperforms text-only and retrieval-based baselines in both accuracy and F1."}}
{"id": "2509.15811", "pdf": "https://arxiv.org/pdf/2509.15811", "abs": "https://arxiv.org/abs/2509.15811", "authors": ["Sara Rajaee", "Rochelle Choenni", "Ekaterina Shutova", "Christof Monz"], "title": "Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While the reasoning abilities of large language models (LLMs) continue to\nadvance, it remains unclear how such ability varies across languages in\nmultilingual LLMs and whether different languages produce reasoning paths that\ncomplement each other. To investigate this question, we train a reward model to\nrank generated responses for a given question across languages. Our results\nshow that our cross-lingual reward model substantially improves mathematical\nreasoning performance compared to using reward modeling within a single\nlanguage, benefiting even high-resource languages. While English often exhibits\nthe highest performance in multilingual models, we find that cross-lingual\nsampling particularly benefits English under low sampling budgets. Our findings\nreveal new opportunities to improve multilingual reasoning by leveraging the\ncomplementary strengths of diverse languages.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u8bed\u8a00\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u53ef\u4ee5\u63d0\u9ad8\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u63a8\u7406\u80fd\u529b\u7684\u53d8\u5316\u4ee5\u53ca\u4e0d\u540c\u8bed\u8a00\u662f\u5426\u80fd\u4ea7\u751f\u4e92\u8865\u7684\u63a8\u7406\u8def\u5f84\u3002", "method": "\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5956\u52b1\u6a21\u578b\u6765\u5bf9\u8de8\u8bed\u8a00\u751f\u6210\u7684\u56de\u7b54\u8fdb\u884c\u6392\u5e8f\uff0c\u4ee5\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u6211\u4eec\u7684\u8de8\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u751a\u81f3\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e5f\u6709\u597d\u5904\u3002\u867d\u7136\u82f1\u8bed\u5728\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u901a\u5e38\u8868\u73b0\u6700\u597d\uff0c\u4f46\u5728\u4f4e\u91c7\u6837\u9884\u7b97\u4e0b\uff0c\u8de8\u8bed\u8a00\u91c7\u6837\u7279\u522b\u6709\u5229\u4e8e\u82f1\u8bed\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u8bed\u8a00\u7684\u4e92\u8865\u4f18\u52bf\u6765\u63d0\u9ad8\u591a\u8bed\u8a00\u63a8\u7406\u7684\u65b0\u673a\u4f1a\u3002"}}
{"id": "2509.15837", "pdf": "https://arxiv.org/pdf/2509.15837", "abs": "https://arxiv.org/abs/2509.15837", "authors": ["Adrian Sauter", "Willem Zuidema", "Marianne de Heer Kloots"], "title": "The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders", "categories": ["cs.CL", "I.2.7"], "comment": "5 pages, 3 figures, Submitted to ICASSP 2026", "summary": "How does visual information included in training affect language processing\nin audio- and text-based deep learning models? We explore how such visual\ngrounding affects model-internal representations of words, and find\nsubstantially different effects in speech- vs. text-based language encoders.\nFirstly, global representational comparisons reveal that visual grounding\nincreases alignment between representations of spoken and written language, but\nthis effect seems mainly driven by enhanced encoding of word identity rather\nthan meaning. We then apply targeted clustering analyses to probe for phonetic\nvs. semantic discriminability in model representations. Speech-based\nrepresentations remain phonetically dominated with visual grounding, but in\ncontrast to text-based representations, visual grounding does not improve\nsemantic discriminability. Our findings could usefully inform the development\nof more efficient methods to enrich speech-based models with visually-informed\nsemantics.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u89c6\u89c9\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u57fa\u4e8e\u97f3\u9891\u548c\u6587\u672c\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u8bed\u8a00\u5904\u7406\uff0c\u53d1\u73b0\u89c6\u89c9\u57fa\u7840\u589e\u5f3a\u4e86\u8bcd\u8eab\u4efd\u7684\u7f16\u7801\uff0c\u4f46\u672a\u6539\u5584\u8bed\u4e49\u533a\u5206\u5ea6\u3002", "motivation": "\u6211\u4eec\u60f3\u4e86\u89e3\u89c6\u89c9\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u57fa\u4e8e\u97f3\u9891\u548c\u6587\u672c\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u8bed\u8a00\u5904\u7406\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u5168\u5c40\u8868\u793a\u6bd4\u8f83\u548c\u6709\u9488\u5bf9\u6027\u7684\u805a\u7c7b\u5206\u6790\u6765\u63a2\u7a76\u89c6\u89c9\u57fa\u7840\u5bf9\u6a21\u578b\u5185\u90e8\u8bcd\u8868\u793a\u7684\u5f71\u54cd\u3002", "result": "\u89c6\u89c9\u57fa\u7840\u589e\u52a0\u4e86\u53e3\u8bed\u548c\u4e66\u9762\u8bed\u8a00\u8868\u793a\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u4f46\u8fd9\u79cd\u6548\u679c\u4e3b\u8981\u7531\u8bcd\u8eab\u4efd\u7684\u589e\u5f3a\u7f16\u7801\u9a71\u52a8\uff0c\u800c\u4e0d\u662f\u610f\u4e49\u3002\u8bed\u97f3\u8868\u793a\u5728\u89c6\u89c9\u57fa\u7840\u4e0b\u4ecd\u4ee5\u8bed\u97f3\u4e3a\u4e3b\uff0c\u800c\u89c6\u89c9\u57fa\u7840\u5e76\u672a\u63d0\u9ad8\u8bed\u4e49\u533a\u5206\u5ea6\u3002", "conclusion": "\u6211\u4eec\u7684\u53d1\u73b0\u53ef\u4ee5\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c06\u89c6\u89c9\u4fe1\u606f\u878d\u5165\u8bed\u97f3\u6a21\u578b\u7684\u8bed\u4e49\u4e2d\u3002"}}
{"id": "2509.15839", "pdf": "https://arxiv.org/pdf/2509.15839", "abs": "https://arxiv.org/abs/2509.15839", "authors": ["Zhongze Luo", "Zhenshuai Yin", "Yongxin Guo", "Zhichao Wang", "Jionghao Zhu", "Xiaoying Tang"], "title": "Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems", "categories": ["cs.CL"], "comment": null, "summary": "While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,\ntheir application in specialized scientific domains like physics reveals\nsignificant gaps in current evaluation benchmarks. Specifically, existing\nbenchmarks often lack fine-grained subject coverage, neglect the step-by-step\nreasoning process, and are predominantly English-centric, failing to\nsystematically evaluate the role of visual information. Therefore, we introduce\n\\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive\nbenchmark that includes 5 difficulty levels, featuring 1,412 image-associated,\nmultiple-choice questions spanning 11 high-school physics subjects. We employ a\ndual evaluation framework to evaluate 20 different MLLMs, analyzing both final\nanswer accuracy and the step-by-step integrity of their chain-of-thought.\nFurthermore, we systematically study the impact of difficulty level and visual\ninformation by comparing the model performance before and after changing the\ninput mode. Our work provides not only a fine-grained resource for the\ncommunity but also offers a robust methodology for dissecting the multimodal\nreasoning process of state-of-the-art MLLMs, and our dataset and code have been\nopen-sourced: https://github.com/luozhongze/Multi-Physics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u4e2d\u6587\u7269\u7406\u63a8\u7406\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u57fa\u51c6\u5728\u7ec6\u7c92\u5ea6\u4e3b\u9898\u8986\u76d6\u3001\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\u548c\u89c6\u89c9\u4fe1\u606f\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u79d1\u5b66\u9886\u57df\u5982\u7269\u7406\u5b66\u7684\u5e94\u7528\u4e2d\u3002", "method": "\u672c\u6587\u91c7\u7528\u53cc\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f3020\u79cd\u4e0d\u540c\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u6790\u5176\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u548c\u601d\u7ef4\u94fe\u7684\u5b8c\u6574\u6027\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u96be\u5ea6\u7ea7\u522b\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "result": "\u672c\u6587\u6784\u5efa\u4e86\u5305\u542b5\u4e2a\u96be\u5ea6\u7ea7\u522b\u30011412\u4e2a\u56fe\u50cf\u76f8\u5173\u9009\u62e9\u9898\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u4e8611\u4e2a\u9ad8\u4e2d\u7269\u7406\u4e3b\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u6a21\u578b\u6027\u80fd\u7684\u53d8\u5316\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMulti-Physics\u7684\u5168\u9762\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u6587\u7269\u7406\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u8d44\u6e90\u548c\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2509.15888", "pdf": "https://arxiv.org/pdf/2509.15888", "abs": "https://arxiv.org/abs/2509.15888", "authors": ["Senkang Hu", "Xudong Han", "Jinqi Jiang", "Yihang Tao", "Zihan Fang", "Sam Tak Wu Kwong", "Yuguang Fang"], "title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NeurIPS'25", "summary": "Adapting billion-parameter language models to a downstream task is still\ncostly, even with parameter-efficient fine-tuning (PEFT). We re-cast task\nadaptation as output-distribution alignment: the objective is to steer the\noutput distribution toward the task distribution directly during decoding\nrather than indirectly through weight updates. Building on this view, we\nintroduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and\ntheoretically grounded method. We start with a short warm-start fine-tune and\nextract a task-aware steering vector from the Kullback-Leibler (KL) divergence\ngradient between the output distribution of the warm-started and pre-trained\nmodels. This steering vector is then used to guide the decoding process to\nsteer the model's output distribution towards the task distribution. We\ntheoretically prove that SVD is first-order equivalent to the gradient step of\nfull fine-tuning and derive a globally optimal solution for the strength of the\nsteering vector. Across three tasks and nine benchmarks, SVD paired with four\nstandard PEFT methods improves multiple-choice accuracy by up to 5 points and\nopen-ended truthfulness by 2 points, with similar gains (1-2 points) on\ncommonsense datasets without adding trainable parameters beyond the PEFT\nadapter. SVD thus offers a lightweight, theoretically grounded path to stronger\ntask adaptation for large language models.", "AI": {"tldr": "SVD is a lightweight, PEFT-compatible method that steers the output distribution of large language models toward the task distribution during decoding, resulting in improved performance on various tasks and benchmarks.", "motivation": "Adapting billion-parameter language models to a downstream task is still costly, even with parameter-efficient fine-tuning (PEFT). The goal is to steer the output distribution toward the task distribution directly during decoding rather than through weight updates.", "method": "SVD is introduced as a method that steers the output distribution toward the task distribution during decoding, using a task-aware steering vector extracted from the KL divergence gradient between the output distributions of a warm-started and pre-trained model.", "result": "SVD paired with four standard PEFT methods improves multiple-choice accuracy by up to 5 points and open-ended truthfulness by 2 points, with similar gains on commonsense datasets without adding trainable parameters beyond the PEFT adapter.", "conclusion": "SVD offers a lightweight, theoretically grounded path to stronger task adaptation for large language models."}}
{"id": "2509.15896", "pdf": "https://arxiv.org/pdf/2509.15896", "abs": "https://arxiv.org/abs/2509.15896", "authors": ["Arghodeep Nandi", "Megha Sundriyal", "Euna Mehnaz Khan", "Jikai Sun", "Emily Vraga", "Jaideep Srivastava", "Tanmoy Chakraborty"], "title": "The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted in EMNLP'25 Main", "summary": "Misinformation remains one of the most significant issues in the digital age.\nWhile automated fact-checking has emerged as a viable solution, most current\nsystems are limited to evaluating factual accuracy. However, the detrimental\neffect of misinformation transcends simple falsehoods; it takes advantage of\nhow individuals perceive, interpret, and emotionally react to information. This\nunderscores the need to move beyond factuality and adopt more human-centered\ndetection frameworks. In this survey, we explore the evolving interplay between\ntraditional fact-checking approaches and psychological concepts such as\ncognitive biases, social dynamics, and emotional responses. By analyzing\nstate-of-the-art misinformation detection systems through the lens of human\npsychology and behavior, we reveal critical limitations of current methods and\nidentify opportunities for improvement. Additionally, we outline future\nresearch directions aimed at creating more robust and adaptive frameworks, such\nas neuro-behavioural models that integrate technological factors with the\ncomplexities of human cognition and social influence. These approaches offer\npromising pathways to more effectively detect and mitigate the societal harms\nof misinformation.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86\u4f20\u7edf\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\u4e0e\u5fc3\u7406\u6982\u5ff5\uff08\u5982\u8ba4\u77e5\u504f\u89c1\u3001\u793e\u4f1a\u52a8\u6001\u548c\u60c5\u611f\u53cd\u5e94\uff09\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u795e\u7ecf\u884c\u4e3a\u6a21\u578b\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u548c\u51cf\u8f7b\u865a\u5047\u4fe1\u606f\u7684\u793e\u4f1a\u5371\u5bb3\u3002", "motivation": "\u865a\u5047\u4fe1\u606f\u7684\u5371\u5bb3\u4e0d\u4ec5\u9650\u4e8e\u4e8b\u5b9e\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u6d89\u53ca\u4eba\u4eec\u5bf9\u4fe1\u606f\u7684\u611f\u77e5\u3001\u89e3\u91ca\u548c\u60c5\u611f\u53cd\u5e94\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8d85\u8d8a\u4e8b\u5b9e\u6027\uff0c\u91c7\u7528\u66f4\u4ee5\u4eba\u4e3a\u672c\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u6700\u65b0\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4ece\u4eba\u7c7b\u5fc3\u7406\u5b66\u548c\u884c\u4e3a\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u63a2\u8ba8\u4e86\u4f20\u7edf\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\u4e0e\u5fc3\u7406\u6982\u5ff5\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u672c\u6587\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5173\u952e\u9650\u5236\uff0c\u5e76\u8bc6\u522b\u4e86\u6539\u8fdb\u7684\u673a\u4f1a\uff0c\u540c\u65f6\u6982\u8ff0\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u521b\u5efa\u66f4\u5f3a\u5927\u548c\u9002\u5e94\u6027\u7684\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f20\u7edf\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\u4e0e\u5fc3\u7406\u6982\u5ff5\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u795e\u7ecf\u884c\u4e3a\u6a21\u578b\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u548c\u51cf\u8f7b\u865a\u5047\u4fe1\u606f\u7684\u793e\u4f1a\u5371\u5bb3\u3002"}}
{"id": "2509.15901", "pdf": "https://arxiv.org/pdf/2509.15901", "abs": "https://arxiv.org/abs/2509.15901", "authors": ["Frederic Kirstein", "Sonu Kumar", "Terry Ruas", "Bela Gipp"], "title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025", "summary": "Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faFRAME\u548cSCOPE\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u548c\u63a8\u7406\u8f68\u8ff9\u6784\u5efa\u6765\u6539\u5584\u4f1a\u8bae\u6458\u8981\u7684\u8d28\u91cf\uff0c\u5e76\u5f15\u5165P-MESA\u8bc4\u4f30\u6846\u67b6\u4ee5\u63d0\u9ad8\u6458\u8981\u7684\u51c6\u786e\u6027\u548c\u4e2a\u6027\u5316\u3002", "motivation": "\u4f1a\u8bae\u6458\u8981\u4ecd\u5bb9\u6613\u51fa\u9519\uff0c\u5e38\u5e38\u4ea7\u751f\u5305\u542b\u5e7b\u89c9\u3001\u9057\u6f0f\u548c\u4e0d\u76f8\u5173\u7684\u5185\u5bb9\u3002", "method": "FRAME\u662f\u4e00\u79cd\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u5c06\u6458\u8981\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8bed\u4e49\u589e\u5f3a\u4efb\u52a1\u3002SCOPE\u662f\u4e00\u79cd\u539f\u56e0\u8f93\u51fa\u534f\u8bae\uff0c\u8ba9\u6a21\u578b\u5728\u5185\u5bb9\u9009\u62e9\u524d\u901a\u8fc7\u56de\u7b54\u4e5d\u4e2a\u95ee\u9898\u6765\u6784\u5efa\u63a8\u7406\u8f68\u8ff9\u3002P-MESA\u662f\u4e00\u4e2a\u591a\u7ef4\u3001\u65e0\u53c2\u8003\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6458\u8981\u662f\u5426\u7b26\u5408\u76ee\u6807\u8bfb\u8005\u3002", "result": "FRAME\u5728QMSum\u548cFAME\u6570\u636e\u96c6\u4e0a\u51cf\u5c11\u4e86\u5e7b\u89c9\u548c\u9057\u6f0f\uff0cSCOPE\u63d0\u9ad8\u4e86\u77e5\u8bc6\u5339\u914d\u5ea6\u548c\u76ee\u6807\u4e00\u81f4\u6027\u3002P-MESA\u80fd\u591f\u53ef\u9760\u5730\u8bc6\u522b\u9519\u8bef\u5b9e\u4f8b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u5206\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u5021\u5bfc\u91cd\u65b0\u601d\u8003\u6458\u8981\u4ee5\u63d0\u9ad8\u63a7\u5236\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u4e2a\u6027\u5316\u3002"}}
{"id": "2509.15926", "pdf": "https://arxiv.org/pdf/2509.15926", "abs": "https://arxiv.org/abs/2509.15926", "authors": ["Ahmed Karim", "Qiao Wang", "Zheng Yuan"], "title": "Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at EMNLP 2025 (Main Conference). Camera-ready version", "summary": "Automated Essay Scoring (AES) systems now reach near human agreement on some\npublic benchmarks, yet real-world adoption, especially in high-stakes\nexaminations, remains limited. A principal obstacle is that most models output\na single score without any accompanying measure of confidence or explanation.\nWe address this gap with conformal prediction, a distribution-free wrapper that\nequips any classifier with set-valued outputs and formal coverage guarantees.\nTwo open-source large language models (Llama-3 8B and Qwen-2.5 3B) are\nfine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and\ncalibrated at a 90 percent risk level. Reliability is assessed with UAcc, an\nuncertainty-aware accuracy that rewards models for being both correct and\nconcise. To our knowledge, this is the first work to combine conformal\nprediction and UAcc for essay scoring. The calibrated models consistently meet\nthe coverage target while keeping prediction sets compact, indicating that\nopen-source, mid-sized LLMs can already support teacher-in-the-loop AES; we\ndiscuss scaling and broader user studies as future work.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u7b26\u5408\u9884\u6d4b\u548cUAcc\u65b9\u6cd5\u5bf9\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u7ed3\u679c\u8868\u660e\u5f00\u6e90\u4e2d\u578b\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u652f\u6301\u6559\u5e08\u5728\u5faa\u73af\u4e2d\u7684\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u5728\u4e00\u4e9b\u516c\u5171\u57fa\u51c6\u4e0a\u8fbe\u5230\u63a5\u8fd1\u4eba\u7c7b\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u8003\u8bd5\u4e2d\uff0c\u4ecd\u7136\u6709\u9650\u3002\u4e3b\u8981\u969c\u788d\u662f\u5927\u591a\u6570\u6a21\u578b\u53ea\u8f93\u51fa\u4e00\u4e2a\u5206\u6570\uff0c\u800c\u6ca1\u6709\u4f34\u968f\u7684\u7f6e\u4fe1\u5ea6\u6216\u89e3\u91ca\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e86\u7b26\u5408\u9884\u6d4b\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u5206\u5e03\u65e0\u5173\u7684\u5305\u88c5\u5668\uff0c\u53ef\u4ee5\u4e3a\u4efb\u4f55\u5206\u7c7b\u5668\u63d0\u4f9b\u96c6\u5408\u503c\u8f93\u51fa\u548c\u6b63\u5f0f\u7684\u8986\u76d6\u4fdd\u8bc1\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u4e86UAcc\uff08\u4e00\u79cd\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u7684\u51c6\u786e\u6027\uff09\u6765\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "result": "\u6821\u51c6\u540e\u7684\u6a21\u578b\u4e00\u81f4\u8fbe\u5230\u4e86\u8986\u76d6\u76ee\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u96c6\u7684\u7d27\u51d1\u6027\u3002", "conclusion": "\u672c\u6587\u8868\u660e\uff0c\u5f00\u6e90\u7684\u4e2d\u578b\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u7ecf\u80fd\u591f\u652f\u6301\u6559\u5e08\u5728\u5faa\u73af\u4e2d\u7684\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\uff0c\u5e76\u8ba8\u8bba\u4e86\u6269\u5c55\u548c\u66f4\u5e7f\u6cdb\u7528\u6237\u7814\u7a76\u4f5c\u4e3a\u672a\u6765\u7684\u5de5\u4f5c\u3002"}}
{"id": "2509.15958", "pdf": "https://arxiv.org/pdf/2509.15958", "abs": "https://arxiv.org/abs/2509.15958", "authors": ["Henri Cimeti\u00e8re", "Maria Teresa Chiri", "Bahman Gharesifard"], "title": "Localmax dynamics for attention in transformers and its asymptotic behavior", "categories": ["cs.CL", "cs.LG", "math.DS", "math.OC", "68T07, 68T50, 37N35, 37B25"], "comment": "28 pages, 5 figures", "summary": "We introduce a new discrete-time attention model, termed the localmax\ndynamics, which interpolates between the classic softmax dynamics and the\nhardmax dynamics, where only the tokens that maximize the influence toward a\ngiven token have a positive weight. As in hardmax, uniform weights are\ndetermined by a parameter controlling neighbor influence, but the key extension\nlies in relaxing neighborhood interactions through an alignment-sensitivity\nparameter, which allows controlled deviations from pure hardmax behavior. As we\nprove, while the convex hull of the token states still converges to a convex\npolytope, its structure can no longer be fully described by a maximal alignment\nset, prompting the introduction of quiescent sets to capture the invariant\nbehavior of tokens near vertices. We show that these sets play a key role in\nunderstanding the asymptotic behavior of the system, even under time-varying\nalignment sensitivity parameters. We further show that localmax dynamics does\nnot exhibit finite-time convergence and provide results for vanishing, nonzero,\ntime-varying alignment-sensitivity parameters, recovering the limiting behavior\nof hardmax as a by-product. Finally, we adapt Lyapunov-based methods from\nclassical opinion dynamics, highlighting their limitations in the asymmetric\nsetting of localmax interactions and outlining directions for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u6563\u65f6\u95f4\u6ce8\u610f\u529b\u6a21\u578b\u2014\u2014\u5c40\u90e8\u6700\u5927\u52a8\u6001\uff0c\u5b83\u5728\u7ecf\u5178softmax\u548c\u786c\u6700\u5927\u52a8\u6001\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u5bf9\u9f50\u654f\u611f\u53c2\u6570\u653e\u677e\u90bb\u57df\u4ea4\u4e92\u3002\u7814\u7a76\u63ed\u793a\u4e86\u8be5\u6a21\u578b\u7684\u6e10\u8fd1\u884c\u4e3a\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u63cf\u8ff0\u548c\u7406\u89e3\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u4ee4\u724c\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u8f6f\u6700\u5927\u548c\u786c\u6700\u5927\u52a8\u6001\u4e4b\u95f4\u7684\u8fc7\u6e21\u533a\u57df\u3002", "method": "\u5f15\u5165\u4e86\u5c40\u90e8\u6700\u5927\u52a8\u6001\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u7ecf\u5178softmax\u52a8\u6001\u548c\u786c\u6700\u5927\u52a8\u6001\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u5bf9\u9f50\u654f\u611f\u53c2\u6570\u653e\u677e\u90bb\u57df\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u8fd8\u5e94\u7528\u4e86\u57fa\u4e8e\u674e\u96c5\u666e\u8bfa\u592b\u7684\u65b9\u6cd5\u6765\u5206\u6790\u7cfb\u7edf\u884c\u4e3a\u3002", "result": "\u8bc1\u660e\u4e86\u5c40\u90e8\u6700\u5927\u52a8\u6001\u7684\u51f8\u5305\u4ecd\u7136\u6536\u655b\u5230\u51f8\u591a\u9762\u4f53\uff0c\u4f46\u5176\u7ed3\u6784\u4e0d\u80fd\u5b8c\u5168\u7531\u6700\u5927\u5bf9\u9f50\u96c6\u63cf\u8ff0\uff0c\u56e0\u6b64\u5f15\u5165\u4e86\u9759\u6b62\u96c6\u6765\u6355\u6349\u63a5\u8fd1\u9876\u70b9\u7684\u4ee4\u724c\u7684\u4e0d\u53d8\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u5c55\u793a\u4e86\u5c40\u90e8\u6700\u5927\u52a8\u6001\u4e0d\u8868\u73b0\u51fa\u6709\u9650\u65f6\u95f4\u6536\u655b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u4e0d\u540c\u5bf9\u9f50\u654f\u611f\u53c2\u6570\u7684\u5206\u6790\u7ed3\u679c\u3002", "conclusion": "\u5c40\u90e8\u6700\u5927\u52a8\u6001\u5728\u7406\u8bba\u4e0a\u63d0\u4f9b\u4e86\u5bf9\u7cfb\u7edf\u6e10\u8fd1\u884c\u4e3a\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002"}}
{"id": "2509.15974", "pdf": "https://arxiv.org/pdf/2509.15974", "abs": "https://arxiv.org/abs/2509.15974", "authors": ["Baichuan Huang", "Ananth Balashankar", "Amir Aminifar"], "title": "BEFT: Bias-Efficient Fine-Tuning of Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning all-bias-terms stands out among various parameter-efficient\nfine-tuning (PEFT) techniques, owing to its out-of-the-box usability and\ncompetitive performance, especially in low-data regimes. Bias-only fine-tuning\nhas the potential for unprecedented parameter efficiency. However, the link\nbetween fine-tuning different bias terms (i.e., bias terms in the query, key,\nor value projections) and downstream performance remains unclear. The existing\napproaches, e.g., based on the magnitude of bias change or empirical Fisher\ninformation, provide limited guidance for selecting the particular bias term\nfor effective fine-tuning. In this paper, we propose an approach for selecting\nthe bias term to be fine-tuned, forming the foundation of our bias-efficient\nfine-tuning (BEFT). We extensively evaluate our bias-efficient approach against\nother bias-selection approaches, across a wide range of large language models\n(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B\nparameters. Our results demonstrate the effectiveness and superiority of our\nbias-efficient approach on diverse downstream tasks, including classification,\nmultiple-choice, and generation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9009\u62e9\u8981\u5fae\u8c03\u7684\u504f\u5dee\u9879\u7684\u65b9\u6cd5\uff0c\u4ee5\u6784\u5efa\u504f\u5dee\u9ad8\u6548\u5fae\u8c03\uff08BEFT\uff09\u3002\u5728\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u4e8e\u504f\u5dee\u53d8\u5316\u7684\u5927\u5c0f\u6216\u7ecf\u9a8cFisher\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u5bf9\u4e8e\u9009\u62e9\u7279\u5b9a\u7684\u504f\u5dee\u9879\u8fdb\u884c\u6709\u6548\u5fae\u8c03\u63d0\u4f9b\u7684\u6307\u5bfc\u6709\u9650\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9009\u62e9\u8981\u5fae\u8c03\u7684\u504f\u5dee\u9879\u7684\u65b9\u6cd5\uff0c\u6784\u6210\u4e86\u6211\u4eec\u504f\u5dee\u9ad8\u6548\u5fae\u8c03\uff08BEFT\uff09\u7684\u57fa\u7840\u3002", "result": "\u6211\u4eec\u5728\u5e7f\u6cdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u504f\u5dee\u9ad8\u6548\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u5206\u7c7b\u3001\u591a\u9879\u9009\u62e9\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u504f\u5dee\u9ad8\u6548\u65b9\u6cd5\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.16025", "pdf": "https://arxiv.org/pdf/2509.16025", "abs": "https://arxiv.org/abs/2509.16025", "authors": ["Hong-Yun Lin", "Jhen-Ke Lin", "Chung-Chun Wang", "Hao-Chien Lu", "Berlin Chen"], "title": "Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Spoken Language Assessment (SLA) estimates a learner's oral proficiency from\nspontaneous speech. The growing population of L2 English speakers has\nintensified the demand for reliable SLA, a critical component of Computer\nAssisted Language Learning (CALL). Existing efforts often rely on cascaded\npipelines, which are prone to error propagation, or end-to-end models that\noften operate on a short audio window, which might miss discourse-level\nevidence. This paper introduces a novel multimodal foundation model approach\nthat performs session-level evaluation in a single pass. Our approach couples\nmulti-target learning with a frozen, Whisper ASR model-based speech prior for\nacoustic-aware calibration, allowing for jointly learning holistic and\ntrait-level objectives of SLA without resorting to handcrafted features. By\ncoherently processing the entire response session of an L2 speaker, the model\nexcels at predicting holistic oral proficiency. Experiments conducted on the\nSpeak & Improve benchmark demonstrate that our proposed approach outperforms\nthe previous state-of-the-art cascaded system and exhibits robust cross-part\ngeneralization, producing a compact deployable grader that is tailored for CALL\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f1a\u8bdd\u7ea7\u53e3\u8bed\u8bc4\u4f30\uff0c\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u6574\u4f53\u548c\u7279\u8d28\u7ea7\u76ee\u6807\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002", "motivation": "\u968f\u7740\u7b2c\u4e8c\u8bed\u8a00\u82f1\u8bed\u5b66\u4e60\u8005\u7684\u6570\u91cf\u589e\u52a0\uff0c\u5bf9\u53ef\u9760SLA\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u800c\u73b0\u6709\u7684\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u4e8e\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\u4f20\u64ad\u7684\u7ea7\u8054\u7ba1\u9053\uff0c\u6216\u8005\u5728\u77ed\u97f3\u9891\u7a97\u53e3\u4e0a\u8fd0\u884c\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u53ef\u80fd\u9057\u6f0f\u8bdd\u8bed\u7ea7\u8bc1\u636e\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5355\u6b21\u901a\u8fc7\u4e2d\u8fdb\u884c\u4f1a\u8bdd\u7ea7\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u591a\u76ee\u6807\u5b66\u4e60\u4e0e\u57fa\u4e8e\u51bb\u7ed3\u7684Whisper ASR\u6a21\u578b\u7684\u8bed\u97f3\u5148\u9a8c\uff0c\u4ee5\u5b9e\u73b0\u58f0\u5b66\u611f\u77e5\u6821\u51c6\uff0c\u4ece\u800c\u5728\u4e0d\u4f7f\u7528\u624b\u5de5\u7279\u5f81\u7684\u60c5\u51b5\u4e0b\u8054\u5408\u5b66\u4e60SLA\u7684\u6574\u4f53\u548c\u7279\u8d28\u7ea7\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\u5728Speak & Improve\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u7ea7\u8054\u7cfb\u7edf\uff0c\u5e76\u4e14\u5728\u8de8\u53c2\u4e0e\u8005\u65b9\u9762\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ea7\u751f\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u53ef\u90e8\u7f72\u8bc4\u5206\u5668\uff0c\u9002\u7528\u4e8eCALL\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u65b9\u6cd5\u5728Speak & Improve\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u7ea7\u8054\u7cfb\u7edf\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u53c2\u4e0e\u8005\u7684\u4e00\u81f4\u6027\uff0c\u4ea7\u751f\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u53ef\u90e8\u7f72\u8bc4\u5206\u5668\uff0c\u9002\u7528\u4e8eCALL\u5e94\u7528\u3002"}}
{"id": "2509.16028", "pdf": "https://arxiv.org/pdf/2509.16028", "abs": "https://arxiv.org/abs/2509.16028", "authors": ["Sang Hoon Woo", "Sehun Lee", "Kang-wook Kim", "Gunhee Kim"], "title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main. Project page: https://yhytoto12.github.io/TVS-ReVerT", "summary": "Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u63a8\u7406\u4e0e\u53e3\u8bed\u8868\u8fbe\u5206\u79bb\u7684\u6846\u67b6\uff0c\u4ee5\u4fdd\u7559\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u8bed\u97f3\u7684\u81ea\u7136\u5ea6\u548c\u7b80\u6d01\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u867d\u7136\u9002\u5e94LLMs\u4ee5\u4ea7\u751f\u9002\u5408\u8bed\u97f3\u7684\u8f93\u51fa\uff0c\u4f46\u5b83\u4eec\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u76f4\u63a5\u5e94\u7528LLMs\u5728\u53e3\u8bed\u4ea4\u6d41\u4e2d\u5f80\u5f80\u4f1a\u4ea7\u751f\u6b21\u4f18\u7ed3\u679c\uff0c\u56e0\u4e3a\u6700\u4f73\u6587\u672c\u548c\u53e3\u5934\u8868\u8fbe\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Think-Verbalize-Speak\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u4e0e\u53e3\u8bed\u8868\u8fbe\u5206\u79bb\uff0c\u4ee5\u4fdd\u7559LLMs\u7684\u5168\u90e8\u63a8\u7406\u80fd\u529b\u3002\u4e2d\u5fc3\u662f\u5c06\u601d\u60f3\u8f6c\u5316\u4e3a\u81ea\u7136\u3001\u9002\u5408\u8bed\u97f3\u7684\u6587\u672c\u7684\u8bed\u4e49\u5316\u6b65\u9aa4\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86ReVerT\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u589e\u91cf\u548c\u5f02\u6b65\u6458\u8981\u7684\u5ef6\u8fdf\u6548\u7387\u8bed\u4e49\u5316\u5668\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u8bed\u97f3\u7684\u81ea\u7136\u5ea6\u548c\u7b80\u6d01\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u8bed\u97f3\u7684\u81ea\u7136\u5ea6\u548c\u7b80\u6d01\u6027\u3002"}}
{"id": "2509.16093", "pdf": "https://arxiv.org/pdf/2509.16093", "abs": "https://arxiv.org/abs/2509.16093", "authors": ["Fangyi Yu", "Nabeel Seedat", "Dasha Herrmannova", "Frank Schilder", "Jonathan Richard Schwarz"], "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains.", "AI": {"tldr": "DeCE is a decomposed LLM evaluation framework that separates precision and recall, using instance-specific criteria automatically extracted from gold answer requirements. It achieves a stronger correlation with expert judgments than traditional metrics and shows interpretable trade-offs between generalist and specialized models.", "motivation": "Evaluating long-form answers in high-stakes domains such as law or medicine remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to capture semantic correctness, and current LLM-based evaluators often reduce nuanced aspects of answer quality into a single undifferentiated score.", "method": "DeCE is a decomposed LLM evaluation framework that separates precision (factual accuracy and relevance) and recall (coverage of required concepts), using instance-specific criteria automatically extracted from gold answer requirements.", "result": "DeCE achieves substantially stronger correlation with expert judgments (r=0.78), compared to traditional metrics (r=0.12), pointwise LLM scoring (r=0.35), and modern multidimensional evaluators (r=0.48). It also reveals interpretable trade-offs: generalist models favor recall, while specialized models favor precision. Only 11.95% of LLM-generated criteria required expert revision, underscoring DeCE's scalability.", "conclusion": "DeCE offers an interpretable and actionable LLM evaluation framework in expert domains."}}
{"id": "2509.16105", "pdf": "https://arxiv.org/pdf/2509.16105", "abs": "https://arxiv.org/abs/2509.16105", "authors": ["Sikai Bai", "Haoxi Li", "Jie Zhang", "Zicong Hong", "Song Guo"], "title": "DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning", "categories": ["cs.CL"], "comment": "18 pages", "summary": "Despite the significant breakthrough of Mixture-of-Experts (MoE), the\nincreasing scale of these MoE models presents huge memory and storage\nchallenges. Existing MoE pruning methods, which involve reducing parameter size\nwith a uniform sparsity across all layers, often lead to suboptimal outcomes\nand performance degradation due to varying expert redundancy in different MoE\nlayers. To address this, we propose a non-uniform pruning strategy, dubbed\n\\textbf{Di}fferentiable \\textbf{E}xpert \\textbf{P}runing (\\textbf{DiEP}), which\nadaptively adjusts pruning rates at the layer level while jointly learning\ninter-layer importance, effectively capturing the varying redundancy across\ndifferent MoE layers. By transforming the global discrete search space into a\ncontinuous one, our method handles exponentially growing non-uniform expert\ncombinations, enabling adaptive gradient-based pruning. Extensive experiments\non five advanced MoE models demonstrate the efficacy of our method across\nvarious NLP tasks. Notably, \\textbf{DiEP} retains around 92\\% of original\nperformance on Mixtral 8$\\times$7B with only half the experts, outperforming\nother pruning methods by up to 7.1\\% on the challenging MMLU dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DiEP \u7684\u975e\u5747\u5300\u526a\u679d\u7b56\u7565\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u8c03\u6574\u4e0d\u540c\u5c42\u7684\u526a\u679d\u7387\uff0c\u4ece\u800c\u6709\u6548\u5904\u7406 MoE \u6a21\u578b\u4e2d\u7684\u4e13\u5bb6\u5197\u4f59\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a NLP \u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684 MoE \u526a\u679d\u65b9\u6cd5\u901a\u5e38\u5728\u6240\u6709\u5c42\u4e2d\u4f7f\u7528\u7edf\u4e00\u7684\u7a00\u758f\u6027\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u4e0d\u540c MoE \u5c42\u4e2d\u7684\u4e13\u5bb6\u5197\u4f59\u7a0b\u5ea6\u4e0d\u540c\u3002", "method": "DiEP \u662f\u4e00\u79cd\u975e\u5747\u5300\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u5c42\u7ea7\u522b\u81ea\u9002\u5e94\u8c03\u6574\u526a\u679d\u7387\uff0c\u5e76\u8054\u5408\u5b66\u4e60\u5c42\u95f4\u91cd\u8981\u6027\uff0c\u4ee5\u6355\u6349\u4e0d\u540c MoE \u5c42\u4e4b\u95f4\u7684\u5197\u4f59\u5dee\u5f02\u3002", "result": "\u5728\u4e94\u4e2a\u5148\u8fdb\u7684 MoE \u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDiEP \u5728\u5404\u79cd NLP \u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u3002\u7279\u522b\u5730\uff0c\u5728 Mixtral 8\u00d77B \u4e0a\uff0cDiEP \u4ec5\u4f7f\u7528\u4e00\u534a\u7684\u4e13\u5bb6\u5c31\u80fd\u4fdd\u7559\u5927\u7ea6 92% \u7684\u539f\u59cb\u6027\u80fd\uff0c\u5e76\u5728 MMLU \u6570\u636e\u96c6\u4e0a\u6bd4\u5176\u4ed6\u526a\u679d\u65b9\u6cd5\u9ad8\u51fa\u6700\u591a 7.1%\u3002", "conclusion": "DiEP \u5728\u5404\u79cd NLP \u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u4e13\u5bb6\u6570\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u539f\u59cb\u6027\u80fd\u3002"}}
{"id": "2509.16107", "pdf": "https://arxiv.org/pdf/2509.16107", "abs": "https://arxiv.org/abs/2509.16107", "authors": ["Lukas Ellinger", "Georg Groh"], "title": "It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge", "categories": ["cs.CL"], "comment": "Accepted by UncertaiNLP workshop @ EMNLP 2025", "summary": "Ambiguous words or underspecified references require interlocutors to resolve\nthem, often by relying on shared context and commonsense knowledge. Therefore,\nwe systematically investigate whether Large Language Models (LLMs) can leverage\ncommonsense to resolve referential ambiguity in multi-turn conversations and\nanalyze their behavior when ambiguity persists. Further, we study how requests\nfor simplified language affect this capacity. Using a novel multilingual\nevaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and\nLlama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that\ncurrent LLMs struggle to resolve ambiguity effectively: they tend to commit to\na single interpretation or cover all possible references, rather than hedging\nor seeking clarification. This limitation becomes more pronounced under\nsimplification prompts, which drastically reduce the use of commonsense\nreasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct\nPreference Optimization substantially improves ambiguity resolution across all\nrequest types. These results underscore the need for advanced fine-tuning to\nimprove LLMs' handling of ambiguity and to ensure robust performance across\ndiverse communication styles.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u6307\u4ee3\u6b67\u4e49\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u7b80\u5316\u8bed\u8a00\u8bf7\u6c42\u4e0b\u3002\u901a\u8fc7\u5fae\u8c03\u53ef\u4ee5\u663e\u8457\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5229\u7528\u5e38\u8bc6\u6765\u89e3\u51b3\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u6307\u4ee3\u6b67\u4e49\uff0c\u5e76\u5206\u6790\u5f53\u6b67\u4e49\u6301\u7eed\u5b58\u5728\u65f6\u5b83\u4eec\u7684\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u7814\u7a76\u7b80\u5316\u8bed\u8a00\u8bf7\u6c42\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u901a\u8fc7LLM-as-Judge\u548c\u4eba\u5de5\u6ce8\u91ca\u6d4b\u8bd5\u4e86DeepSeek v3\u3001GPT-4o\u3001Qwen3-32B\u3001GPT-4o-mini\u548cLlama-3.1-8B\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u7b80\u5316\u8bed\u8a00\u8bf7\u6c42\u5bf9\u8fd9\u79cd\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u5bf9Llama-3.1-8B\u8fdb\u884c\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u5fae\u8c03\u3002", "result": "\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u6b67\u4e49\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u503e\u5411\u4e8e\u575a\u6301\u5355\u4e00\u89e3\u91ca\u6216\u6db5\u76d6\u6240\u6709\u53ef\u80fd\u7684\u53c2\u8003\uff0c\u800c\u4e0d\u662f\u91c7\u53d6\u4e2d\u95f4\u7acb\u573a\u6216\u5bfb\u6c42\u6f84\u6e05\u3002\u5728\u7b80\u5316\u63d0\u793a\u4e0b\uff0c\u8fd9\u79cd\u9650\u5236\u53d8\u5f97\u66f4\u52a0\u660e\u663e\uff0c\u8fd9\u5927\u5927\u51cf\u5c11\u4e86\u5e38\u8bc6\u63a8\u7406\u548c\u591a\u6837\u5316\u7684\u54cd\u5e94\u7b56\u7565\u7684\u4f7f\u7528\u3002\u5bf9Llama-3.1-8B\u8fdb\u884c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u5fae\u8c03\u663e\u8457\u63d0\u9ad8\u4e86\u6240\u6709\u8bf7\u6c42\u7c7b\u578b\u7684\u6b67\u4e49\u89e3\u51b3\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6709\u6548\u89e3\u51b3\u6b67\u4e49\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u8868\u660e\u9700\u8981\u5148\u8fdb\u7684\u5fae\u8c03\u6765\u63d0\u9ad8\u5b83\u4eec\u5904\u7406\u6b67\u4e49\u7684\u80fd\u529b\uff0c\u5e76\u786e\u4fdd\u5728\u4e0d\u540c\u6c9f\u901a\u98ce\u683c\u4e0b\u7684\u7a33\u5065\u6027\u80fd\u3002"}}
{"id": "2509.16112", "pdf": "https://arxiv.org/pdf/2509.16112", "abs": "https://arxiv.org/abs/2509.16112", "authors": ["Sheng Zhang", "Yifan Ding", "Shuquan Lian", "Shun Song", "Hui Li"], "title": "CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion", "categories": ["cs.CL", "cs.IR", "cs.SE"], "comment": "EMNLP 2025", "summary": "Repository-level code completion automatically predicts the unfinished code\nbased on the broader information from the repository. Recent strides in Code\nLarge Language Models (code LLMs) have spurred the development of\nrepository-level code completion methods, yielding promising results.\nNevertheless, they suffer from issues such as inappropriate query construction,\nsingle-path code retrieval, and misalignment between code retriever and code\nLLM. To address these problems, we introduce CodeRAG, a framework tailored to\nidentify relevant and necessary knowledge for retrieval-augmented\nrepository-level code completion. Its core components include log probability\nguided query construction, multi-path code retrieval, and preference-aligned\nBestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval\ndemonstrate that CodeRAG significantly and consistently outperforms\nstate-of-the-art methods. The implementation of CodeRAG is available at\nhttps://github.com/KDEGroup/CodeRAG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCodeRAG\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\uff0c\u901a\u8fc7\u591a\u8def\u5f84\u68c0\u7d22\u548c\u91cd\u65b0\u6392\u5e8f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u67e5\u8be2\u6784\u5efa\u3001\u5355\u8def\u5f84\u4ee3\u7801\u68c0\u7d22\u548c\u4ee3\u7801\u68c0\u7d22\u5668\u4e0e\u4ee3\u7801LLM\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "method": "CodeRAG\u6846\u67b6\u5305\u62ec\u65e5\u5fd7\u6982\u7387\u5f15\u5bfc\u7684\u67e5\u8be2\u6784\u5efa\u3001\u591a\u8def\u5f84\u4ee3\u7801\u68c0\u7d22\u548c\u504f\u597d\u5bf9\u9f50\u7684BestFit\u91cd\u65b0\u6392\u5e8f\u3002", "result": "\u5728ReccEval\u548cCCEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCodeRAG\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "CodeRAG\u663e\u8457\u4e14\u4e00\u81f4\u5730\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.16188", "pdf": "https://arxiv.org/pdf/2509.16188", "abs": "https://arxiv.org/abs/2509.16188", "authors": ["Jinghao Zhang", "Sihang Jiang", "Shiwei Guo", "Shisong Chen", "Yanghua Xiao", "Hongwei Feng", "Jiaqing Liang", "Minggui HE", "Shimin Tao", "Hongxia Ma"], "title": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in diverse cultural\nenvironments, evaluating their cultural understanding capability has become\nessential for ensuring trustworthy and culturally aligned applications.\nHowever, most existing benchmarks lack comprehensiveness and are challenging to\nscale and adapt across different cultural contexts, because their frameworks\noften lack guidance from well-established cultural theories and tend to rely on\nexpert-driven manual annotations. To address these issues, we propose\nCultureScope, the most comprehensive evaluation framework to date for assessing\ncultural understanding in LLMs. Inspired by the cultural iceberg theory, we\ndesign a novel dimensional schema for cultural knowledge classification,\ncomprising 3 layers and 140 dimensions, which guides the automated construction\nof culture-specific knowledge bases and corresponding evaluation datasets for\nany given languages and cultures. Experimental results demonstrate that our\nmethod can effectively evaluate cultural understanding. They also reveal that\nexisting large language models lack comprehensive cultural competence, and\nmerely incorporating multilingual data does not necessarily enhance cultural\nunderstanding. All code and data files are available at\nhttps://github.com/HoganZinger/Culture", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CultureScope\uff0c\u8fd9\u662f\u76ee\u524d\u6700\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002\u901a\u8fc7\u53d7\u6587\u5316\u51b0\u5c71\u7406\u8bba\u542f\u53d1\u7684\u65b0\u578b\u7ef4\u5ea6\u6a21\u5f0f\uff0c\u6307\u5bfc\u4e86\u9488\u5bf9\u4efb\u4f55\u7ed9\u5b9a\u8bed\u8a00\u548c\u6587\u5316\u7684\u7279\u5b9a\u6587\u5316\u77e5\u8bc6\u5e93\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u81ea\u52a8\u5316\u6784\u5efa\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6587\u5316\u80fd\u529b\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6837\u5316\u7684\u6587\u5316\u73af\u5883\u4e2d\u65e5\u76ca\u90e8\u7f72\uff0c\u8bc4\u4f30\u5176\u6587\u5316\u7406\u89e3\u80fd\u529b\u5bf9\u4e8e\u786e\u4fdd\u53ef\u4fe1\u548c\u6587\u5316\u4e00\u81f4\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u5168\u9762\u6027\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u96be\u4ee5\u6269\u5c55\u548c\u9002\u5e94\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u6846\u67b6\u5f80\u5f80\u7f3a\u4e4f\u6765\u81ea\u5df2\u5efa\u7acb\u6587\u5316\u7406\u8bba\u7684\u6307\u5bfc\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u9a71\u52a8\u7684\u624b\u52a8\u6ce8\u91ca\u3002", "method": "\u53d7\u6587\u5316\u51b0\u5c71\u7406\u8bba\u7684\u542f\u53d1\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u6587\u5316\u77e5\u8bc6\u5206\u7c7b\u7ef4\u5ea6\u6a21\u5f0f\uff0c\u5305\u62ec3\u4e2a\u5c42\u6b21\u548c140\u4e2a\u7ef4\u5ea6\uff0c\u8fd9\u6307\u5bfc\u4e86\u9488\u5bf9\u4efb\u4f55\u7ed9\u5b9a\u8bed\u8a00\u548c\u6587\u5316\u7684\u7279\u5b9a\u6587\u5316\u77e5\u8bc6\u5e93\u548c\u76f8\u5e94\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u81ea\u52a8\u5316\u6784\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u8bc4\u4f30\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002\u7ed3\u679c\u8fd8\u63ed\u793a\u4e86\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4ec5\u4ec5\u5305\u542b\u591a\u8bed\u8a00\u6570\u636e\u5e76\u4e0d\u4e00\u5b9a\u80fd\u589e\u5f3a\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u8bc4\u4f30\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002\u7ed3\u679c\u8fd8\u63ed\u793a\u4e86\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4ec5\u4ec5\u5305\u542b\u591a\u8bed\u8a00\u6570\u636e\u5e76\u4e0d\u4e00\u5b9a\u80fd\u589e\u5f3a\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.16198", "pdf": "https://arxiv.org/pdf/2509.16198", "abs": "https://arxiv.org/abs/2509.16198", "authors": ["Jane Luo", "Xin Zhang", "Steven Liu", "Jie Wu", "Yiming Huang", "Yangyu Huang", "Chengyu Yin", "Ying Xin", "Jianfeng Liu", "Yuefeng Zhan", "Hao Sun", "Qi Chen", "Scarlett Li", "Mao Yang"], "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Repository Planning Graph (RPG) \u548c ZeroRepo \u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u96f6\u5f00\u59cb\u751f\u6210\u4ed3\u5e93\uff0c\u5e76\u5728 RepoCraft \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u751f\u6210\u5b8c\u6574\u7684\u4ed3\u5e93\u4ece\u5934\u5f00\u59cb\u4ecd\u7136\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u9700\u8981\u5728\u63d0\u6848\u7ea7\u548c\u5b9e\u73b0\u7ea7\u9636\u6bb5\u8fdb\u884c\u8fde\u8d2f\u4e14\u53ef\u9760\u7684\u89c4\u5212\uff0c\u800c\u81ea\u7136\u8bed\u8a00\u7531\u4e8e\u5176\u6a21\u7cca\u6027\u548c\u5197\u957f\u6027\uff0c\u4e0d\u9002\u5408\u5fe0\u5b9e\u8868\u793a\u590d\u6742\u7684\u8f6f\u4ef6\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u4e86Repository Planning Graph (RPG)\uff0c\u8fd9\u662f\u4e00\u79cd\u7edf\u4e00\u63d0\u6848\u7ea7\u548c\u5b9e\u73b0\u7ea7\u89c4\u5212\u7684\u6301\u4e45\u8868\u793a\uff0c\u901a\u8fc7\u7f16\u7801\u80fd\u529b\u3001\u6587\u4ef6\u7ed3\u6784\u3001\u6570\u636e\u6d41\u548c\u51fd\u6570\u5728\u4e00\u4e2a\u56fe\u4e2d\u3002\u57fa\u4e8eRPG\uff0c\u5f00\u53d1\u4e86ZeroRepo\uff0c\u4e00\u4e2a\u4ece\u96f6\u5f00\u59cb\u751f\u6210\u4ed3\u5e93\u7684\u56fe\u9a71\u52a8\u6846\u67b6\u3002", "result": "\u5728RepoCraft\u4e0a\uff0cZeroRepo\u751f\u6210\u7684\u4ed3\u5e93\u5e73\u5747\u63a5\u8fd136K LOC\uff0c\u5927\u7ea6\u662f\u6700\u5f3a\u57fa\u7ebf\uff08Claude Code\uff09\u76843.9\u500d\uff0c\u5176\u4ed6\u57fa\u7ebf\u768464\u500d\u3002\u5b83\u5b9e\u73b0\u4e8681.5%\u7684\u529f\u80fd\u8986\u76d6\u7387\u548c69.7%\u7684\u901a\u8fc7\u7387\uff0c\u5206\u522b\u8d85\u8fc7Claude Code 27.3\u548c35.8\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "RPG\u6a21\u578b\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u8fd1\u7ebf\u6027\u6269\u5c55\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u89c4\u5212\uff0c\u540c\u65f6\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u4ed3\u5e93\u7684\u7406\u89e3\uff0c\u4ece\u800c\u52a0\u901f\u4ee3\u7406\u5b9a\u4f4d\u3002"}}
{"id": "2301.12399", "pdf": "https://arxiv.org/pdf/2301.12399", "abs": "https://arxiv.org/abs/2301.12399", "authors": ["Hang Su", "Borislav Dzodzo", "Changlun Li", "Danyang Zhao", "Hao Geng", "Yunxiang Li", "Sidharth Jaggi", "Helen Meng"], "title": "Learning Analytics from Spoken Discussion Dialogs in Flipped Classroom", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The flipped classroom is a new pedagogical strategy that has been gaining\nincreasing importance recently. Spoken discussion dialog commonly occurs in\nflipped classroom, which embeds rich information indicating processes and\nprogression of students' learning. This study focuses on learning analytics\nfrom spoken discussion dialog in the flipped classroom, which aims to collect\nand analyze the discussion dialogs in flipped classroom in order to get to know\ngroup learning processes and outcomes. We have recently transformed a course\nusing the flipped classroom strategy, where students watched video-recorded\nlectures at home prior to group-based problem-solving discussions in class. The\nin-class group discussions were recorded throughout the semester and then\ntranscribed manually. After features are extracted from the dialogs by multiple\ntools and customized processing techniques, we performed statistical analyses\nto explore the indicators that are related to the group learning outcomes from\nface-to-face discussion dialogs in the flipped classroom. Then, machine\nlearning algorithms are applied to the indicators in order to predict the group\nlearning outcome as High, Mid or Low. The best prediction accuracy reaches\n78.9%, which demonstrates the feasibility of achieving automatic learning\noutcome prediction from group discussion dialog in flipped classroom.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7ffb\u8f6c\u8bfe\u5802\u4e2d\u5c0f\u7ec4\u8ba8\u8bba\u5bf9\u8bdd\u7684\u5b66\u4e60\u5206\u6790\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u9884\u6d4b\u5b66\u4e60\u6210\u679c\uff0c\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u7ffb\u8f6c\u8bfe\u5802\u662f\u4e00\u79cd\u65b0\u5174\u7684\u6559\u5b66\u7b56\u7565\uff0c\u5176\u4e2d\u53e3\u8bed\u8ba8\u8bba\u5bf9\u8bdd\u5305\u542b\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u53ef\u4ee5\u53cd\u6620\u5b66\u751f\u7684\u5b66\u4e60\u8fc7\u7a0b\u548c\u8fdb\u5c55\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u5206\u6790\u6765\u4e86\u89e3\u5c0f\u7ec4\u5b66\u4e60\u7684\u8fc7\u7a0b\u548c\u7ed3\u679c\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6536\u96c6\u548c\u5206\u6790\u7ffb\u8f6c\u8bfe\u5802\u4e2d\u7684\u5c0f\u7ec4\u8ba8\u8bba\u5bf9\u8bdd\uff0c\u63d0\u53d6\u7279\u5f81\u5e76\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff0c\u7136\u540e\u5e94\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u6765\u9884\u6d4b\u5c0f\u7ec4\u5b66\u4e60\u6210\u679c\u3002", "result": "\u7814\u7a76\u6210\u529f\u5730\u5e94\u7528\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u6765\u9884\u6d4b\u5c0f\u7ec4\u5b66\u4e60\u6210\u679c\uff0c\u5e76\u8fbe\u5230\u4e8678.9%\u7684\u6700\u4f73\u9884\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4ece\u7ffb\u8f6c\u8bfe\u5802\u4e2d\u7684\u5c0f\u7ec4\u8ba8\u8bba\u5bf9\u8bdd\u4e2d\u81ea\u52a8\u9884\u6d4b\u5b66\u4e60\u6210\u679c\u7684\u53ef\u884c\u6027\uff0c\u6700\u4f73\u9884\u6d4b\u51c6\u786e\u7387\u8fbe\u523078.9%\u3002"}}
{"id": "2509.15233", "pdf": "https://arxiv.org/pdf/2509.15233", "abs": "https://arxiv.org/abs/2509.15233", "authors": ["Xueqiao Zhang", "Chao Zhang", "Jingtao Xu", "Yifan Zhu", "Xin Shi", "Yi Yang", "Yawei Luo"], "title": "Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents", "categories": ["cs.MM", "cs.CL", "cs.CV"], "comment": "Accepted at EMNLP2025 Main", "summary": "Role-playing agents (RPAs) have attracted growing interest for their ability\nto simulate immersive and interactive characters. However, existing approaches\nprimarily focus on static role profiles, overlooking the dynamic perceptual\nabilities inherent to humans. To bridge this gap, we introduce the concept of\ndynamic role profiles by incorporating video modality into RPAs. To support\nthis, we construct Role-playing-Video60k, a large-scale, high-quality dataset\ncomprising 60k videos and 700k corresponding dialogues. Based on this dataset,\nwe develop a comprehensive RPA framework that combines adaptive temporal\nsampling with both dynamic and static role profile representations.\nSpecifically, the dynamic profile is created by adaptively sampling video\nframes and feeding them to the LLM in temporal order, while the static profile\nconsists of (1) character dialogues from training videos during fine-tuning,\nand (2) a summary context from the input video during inference. This joint\nintegration enables RPAs to generate greater responses. Furthermore, we propose\na robust evaluation method covering eight metrics. Experimental results\ndemonstrate the effectiveness of our framework, highlighting the importance of\ndynamic role profiles in developing RPAs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u6a21\u6001\u7684\u52a8\u6001\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u5347RPAs\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u56fa\u6709\u7684\u52a8\u6001\u611f\u77e5\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u52a8\u6001\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\u4ee5\u63d0\u9ad8RPAs\u7684\u4ea4\u4e92\u6027\u548c\u6c89\u6d78\u611f\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u52a8\u6001\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\uff0c\u7ed3\u5408\u89c6\u9891\u6a21\u6001\u548c\u5bf9\u8bdd\u6570\u636e\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u81ea\u9002\u5e94\u65f6\u95f4\u91c7\u6837\u548c\u52a8\u6001\u4e0e\u9759\u6001\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\u8868\u793a\u7684RPA\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u516b\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\u5728\u5f00\u53d1RPAs\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u52a8\u6001\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\u7684\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u6784\u5efaRole-playing-Video60k\u6570\u636e\u96c6\u548c\u5f00\u53d1\u4e00\u4e2a\u7efc\u5408\u7684RPA\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u5347RPAs\u751f\u6210\u54cd\u5e94\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.15235", "pdf": "https://arxiv.org/pdf/2509.15235", "abs": "https://arxiv.org/abs/2509.15235", "authors": ["Jialiang Kang", "Han Shu", "Wenshuo Li", "Yingjie Zhai", "Xinghao Chen"], "title": "ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding", "categories": ["cs.CV", "cs.CL"], "comment": "12 pages, 4 figures", "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), yet its application to vision-language models\n(VLMs) remains underexplored, with existing methods achieving only modest\nspeedups (<1.5x). This gap is increasingly significant as multimodal\ncapabilities become central to large-scale models. We hypothesize that large\nVLMs can effectively filter redundant image information layer by layer without\ncompromising textual comprehension, whereas smaller draft models struggle to do\nso. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a\nnovel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor\nmodule to compress image tokens into a compact representation, which is\nseamlessly integrated into the draft model's attention mechanism while\npreserving original image positional information. Additionally, we extract a\nglobal feature vector for each input image and augment all subsequent text\ntokens with this feature to enhance multimodal coherence. To overcome the\nscarcity of multimodal datasets with long assistant responses, we curate a\nspecialized training dataset by repurposing existing datasets and generating\nextended outputs using the target VLM with modified prompts. Our training\nstrategy mitigates the risk of the draft model exploiting direct access to the\ntarget model's hidden states, which could otherwise lead to shortcut learning\nwhen training solely on target model outputs. Extensive experiments validate\nViSpec, achieving, to our knowledge, the first substantial speedup in VLM\nspeculative decoding.", "AI": {"tldr": "ViSpec is a new framework for accelerating inference in vision-language models by effectively filtering redundant image information and enhancing multimodal coherence.", "motivation": "Existing methods for speculative decoding in vision-language models have only achieved modest speedups, and there is a need for more effective techniques as multimodal capabilities become central to large-scale models.", "method": "ViSpec uses a lightweight vision adaptor module to compress image tokens and integrates it into the draft model's attention mechanism. It also extracts a global feature vector for each input image and augments text tokens with this feature.", "result": "Experiments validate ViSpec, achieving the first substantial speedup in VLM speculative decoding.", "conclusion": "ViSpec is a novel framework for vision-language models that achieves significant speedup in speculative decoding."}}
{"id": "2509.15241", "pdf": "https://arxiv.org/pdf/2509.15241", "abs": "https://arxiv.org/abs/2509.15241", "authors": ["Shreyash Verma", "Amit Kesari", "Vinayak Trivedi", "Anupam Purwar", "Ratnesh Jamidar"], "title": "M-PACE: Mother Child Framework for Multimodal Compliance", "categories": ["cs.CV", "cs.CL"], "comment": "The M-PACE framework uses a \"mother-child\" AI model system to\n  automate and unify compliance checks for ads, reducing costs while\n  maintaining high accuracy", "summary": "Ensuring that multi-modal content adheres to brand, legal, or\nplatform-specific compliance standards is an increasingly complex challenge\nacross domains. Traditional compliance frameworks typically rely on disjointed,\nmulti-stage pipelines that integrate separate modules for image classification,\ntext extraction, audio transcription, hand-crafted checks, and rule-based\nmerges. This architectural fragmentation increases operational overhead,\nhampers scalability, and hinders the ability to adapt to dynamic guidelines\nefficiently. With the emergence of Multimodal Large Language Models (MLLMs),\nthere is growing potential to unify these workflows under a single,\ngeneral-purpose framework capable of jointly processing visual and textual\ncontent. In light of this, we propose Multimodal Parameter Agnostic Compliance\nEngine (M-PACE), a framework designed for assessing attributes across\nvision-language inputs in a single pass. As a representative use case, we apply\nM-PACE to advertisement compliance, demonstrating its ability to evaluate over\n15 compliance-related attributes. To support structured evaluation, we\nintroduce a human-annotated benchmark enriched with augmented samples that\nsimulate challenging real-world conditions, including visual obstructions and\nprofanity injection. M-PACE employs a mother-child MLLM setup, demonstrating\nthat a stronger parent MLLM evaluating the outputs of smaller child models can\nsignificantly reduce dependence on human reviewers, thereby automating quality\ncontrol. Our analysis reveals that inference costs reduce by over 31 times,\nwith the most efficient models (Gemini 2.0 Flash as child MLLM selected by\nmother MLLM) operating at 0.0005 per image, compared to 0.0159 for Gemini 2.5\nPro with comparable accuracy, highlighting the trade-off between cost and\noutput quality achieved in real time by M-PACE in real life deployment over\nadvertising data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a M-PACE \u7684\u591a\u6a21\u6001\u5408\u89c4\u5f15\u64ce\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u7684\u5c5e\u6027\uff0c\u5e76\u5728\u5e7f\u544a\u5408\u89c4\u6027\u65b9\u9762\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6210\u672c\u548c\u8f93\u51fa\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u4f20\u7edf\u5408\u89c4\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u4e8e\u5206\u79bb\u7684\u591a\u9636\u6bb5\u7ba1\u9053\uff0c\u8fd9\u589e\u52a0\u4e86\u8fd0\u8425\u5f00\u9500\uff0c\u963b\u788d\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u6307\u5357\u3002\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u51fa\u73b0\uff0c\u6709\u6f5c\u529b\u5c06\u8fd9\u4e9b\u5de5\u4f5c\u6d41\u7a0b\u7edf\u4e00\u5230\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u4e2d\u3002", "method": "M-PACE \u91c7\u7528\u4e86\u4e00\u79cd\u6bcd-\u5b50 MLLM \u8bbe\u7f6e\uff0c\u5176\u4e2d\u66f4\u5f3a\u7684\u7236 MLLM \u8bc4\u4f30\u8f83\u5c0f\u7684\u5b50\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u4eba\u5de5\u5ba1\u67e5\u5458\u7684\u4f9d\u8d56\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u4eba\u5de5\u6807\u6ce8\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u652f\u6301\u7ed3\u6784\u5316\u7684\u8bc4\u4f30\u3002", "result": "M-PACE \u80fd\u591f\u8bc4\u4f30\u8d85\u8fc715\u4e2a\u4e0e\u5408\u89c4\u6027\u76f8\u5173\u7684\u5c5e\u6027\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u6210\u672c\u65b9\u9762\u51cf\u5c11\u4e8631\u500d\u3002\u6700\u6709\u6548\u7684\u6a21\u578b\uff08Gemini 2.0 Flash \u4f5c\u4e3a\u5b50 MLLM\uff09\u6bcf\u5f20\u56fe\u50cf\u7684\u6210\u672c\u4e3a0.0005\uff0c\u800c Gemini 2.5 Pro \u7684\u6210\u672c\u4e3a0.0159\uff0c\u663e\u793a\u51fa\u5b9e\u65f6\u90e8\u7f72\u4e2d\u6210\u672c\u548c\u8f93\u51fa\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "M-PACE \u662f\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u8de8\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u5c5e\u6027\u7684\u6846\u67b6\uff0c\u5b83\u5c55\u793a\u4e86\u5728\u5e7f\u544a\u5408\u89c4\u6027\u65b9\u9762\u7684\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u6210\u672c\u548c\u8f93\u51fa\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2509.15279", "pdf": "https://arxiv.org/pdf/2509.15279", "abs": "https://arxiv.org/abs/2509.15279", "authors": ["Chi Liu", "Derek Li", "Yan Shu", "Robin Chen", "Derek Duan", "Teng Fang", "Bryan Dai"], "title": "Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "While large language models show promise in medical applications, achieving\nexpert-level clinical reasoning remains challenging due to the need for both\naccurate answers and transparent reasoning processes. To address this\nchallenge, we introduce Fleming-R1, a model designed for verifiable medical\nreasoning through three complementary innovations. First, our\nReasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets\nwith knowledge-graph-guided synthesis to improve coverage of underrepresented\ndiseases, drugs, and multi-hop reasoning chains. Second, we employ\nChain-of-Thought (CoT) cold start to distill high-quality reasoning\ntrajectories from teacher models, establishing robust inference priors. Third,\nwe implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR)\nframework using Group Relative Policy Optimization, which consolidates core\nreasoning skills while targeting persistent failure modes through adaptive\nhard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers\nsubstantial parameter-efficient improvements: the 7B variant surpasses much\nlarger baselines, while the 32B model achieves near-parity with GPT-4o and\nconsistently outperforms strong open-source alternatives. These results\ndemonstrate that structured data design, reasoning-oriented initialization, and\nverifiable reinforcement learning can advance clinical reasoning beyond simple\naccuracy optimization. We release Fleming-R1 publicly to promote transparent,\nreproducible, and auditable progress in medical AI, enabling safer deployment\nin high-stakes clinical environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Fleming-R1\uff0c\u4e00\u79cd\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u521b\u65b0\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u533b\u5b66\u63a8\u7406\u7684\u6a21\u578b\uff0c\u5305\u62ec\u63a8\u7406\u5bfc\u5411\u6570\u636e\u7b56\u7565\u3001\u601d\u7ef4\u94fe\u51b7\u542f\u52a8\u548c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cFleming-R1\u5728\u591a\u4e2a\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e34\u5e8a\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u524d\u666f\uff0c\u4f46\u7531\u4e8e\u9700\u8981\u51c6\u786e\u7684\u7b54\u6848\u548c\u900f\u660e\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u7684\u4e34\u5e8a\u63a8\u7406\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165Fleming-R1\uff0c\u4e00\u79cd\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u521b\u65b0\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u533b\u5b66\u63a8\u7406\u7684\u6a21\u578b\u3002\u9996\u5148\uff0c\u6211\u4eec\u7684\u63a8\u7406\u5bfc\u5411\u6570\u636e\u7b56\u7565\uff08RODS\uff09\u7ed3\u5408\u4e86\u7cbe\u5fc3\u6311\u9009\u7684\u533b\u5b66QA\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u7684\u5408\u6210\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u672a\u88ab\u5145\u5206\u4ee3\u8868\u7684\u75be\u75c5\u3001\u836f\u7269\u548c\u591a\u8df3\u63a8\u7406\u94fe\u7684\u8986\u76d6\u8303\u56f4\u3002\u5176\u6b21\uff0c\u6211\u4eec\u91c7\u7528\u601d\u7ef4\u94fe\uff08CoT\uff09\u51b7\u542f\u52a8\u6765\u84b8\u998f\u6559\u5e08\u6a21\u578b\u4e2d\u7684\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\uff0c\u5efa\u7acb\u7a33\u5065\u7684\u63a8\u7406\u5148\u9a8c\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u5b9e\u65bd\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u6846\u67b6\uff0c\u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u5de9\u56fa\u6838\u5fc3\u63a8\u7406\u6280\u80fd\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u786c\u6837\u672c\u6316\u6398\u9488\u5bf9\u6301\u7eed\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u5728\u5404\u79cd\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFleming-R1\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u53c2\u6570\u9ad8\u6548\u6539\u8fdb\uff1a7B\u53d8\u4f53\u8d85\u8fc7\u4e86\u66f4\u5927\u7684\u57fa\u7ebf\uff0c\u800c32B\u6a21\u578b\u4e0eGPT-4o\u51e0\u4e4e\u8fbe\u5230\u4e00\u81f4\uff0c\u5e76\u4e14\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u6784\u5316\u6570\u636e\u8bbe\u8ba1\u3001\u4ee5\u63a8\u7406\u4e3a\u5bfc\u5411\u7684\u521d\u59cb\u5316\u548c\u53ef\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5c06\u4e34\u5e8a\u63a8\u7406\u63a8\u8fdb\u5230\u8d85\u8d8a\u7b80\u5355\u51c6\u786e\u6027\u4f18\u5316\u7684\u6c34\u5e73\u3002\u6211\u4eec\u516c\u5f00\u53d1\u5e03Fleming-R1\uff0c\u4ee5\u4fc3\u8fdb\u533b\u7597AI\u4e2d\u900f\u660e\u3001\u53ef\u91cd\u590d\u548c\u53ef\u5ba1\u8ba1\u7684\u8fdb\u6b65\uff0c\u4f7f\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2509.15380", "pdf": "https://arxiv.org/pdf/2509.15380", "abs": "https://arxiv.org/abs/2509.15380", "authors": ["Vera Pavlova", "Mohammed Makhlouf"], "title": "Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Despite recent advancements in Multilingual Information Retrieval (MLIR), a\nsignificant gap remains between research and practical deployment. Many studies\nassess MLIR performance in isolated settings, limiting their applicability to\nreal-world scenarios. In this work, we leverage the unique characteristics of\nthe Quranic multilingual corpus to examine the optimal strategies to develop an\nad-hoc IR system for the Islamic domain that is designed to satisfy users'\ninformation needs in multiple languages. We prepared eleven retrieval models\nemploying four training approaches: monolingual, cross-lingual,\ntranslate-train-all, and a novel mixed method combining cross-lingual and\nmonolingual techniques. Evaluation on an in-domain dataset demonstrates that\nthe mixed approach achieves promising results across diverse retrieval\nscenarios. Furthermore, we provide a detailed analysis of how different\ntraining configurations affect the embedding space and their implications for\nmultilingual retrieval effectiveness. Finally, we discuss deployment\nconsiderations, emphasizing the cost-efficiency of deploying a single\nversatile, lightweight model for real-world MLIR applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5728\u4f0a\u65af\u5170\u9886\u57df\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6ee1\u8db3\u7528\u6237\u591a\u8bed\u8a00\u4fe1\u606f\u9700\u6c42\u7684\u5373\u5174\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u90e8\u7f72\u5355\u4e00\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u5c3d\u7ba1\u5728\u591a\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\uff08MLIR\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u8bb8\u591a\u7814\u7a76\u5728\u5b64\u7acb\u73af\u5883\u4e2d\u8bc4\u4f30MLIR\u6027\u80fd\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u672c\u6587\u5229\u7528\u300a\u53e4\u5170\u7ecf\u300b\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u7684\u7279\u70b9\uff0c\u51c6\u5907\u4e86\u5341\u4e00\u79cd\u68c0\u7d22\u6a21\u578b\uff0c\u91c7\u7528\u4e86\u56db\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff1a\u5355\u8bed\u3001\u8de8\u8bed\u8a00\u3001\u7ffb\u8bd1\u8bad\u7ec3\u5168\u90e8\u548c\u4e00\u79cd\u7ed3\u5408\u8de8\u8bed\u8a00\u548c\u5355\u8bed\u6280\u672f\u7684\u65b0\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u5728\u9886\u57df\u5185\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6df7\u5408\u65b9\u6cd5\u5728\u5404\u79cd\u68c0\u7d22\u573a\u666f\u4e2d\u90fd\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u672c\u6587\u8be6\u7ec6\u5206\u6790\u4e86\u4e0d\u540c\u8bad\u7ec3\u914d\u7f6e\u5982\u4f55\u5f71\u54cd\u5d4c\u5165\u7a7a\u95f4\u53ca\u5176\u5bf9\u591a\u8bed\u8a00\u68c0\u7d22\u6548\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u8ba8\u8bba\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u90e8\u7f72\u591a\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u4f7f\u7528\u5355\u4e00\u3001\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u9ad8\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2509.15540", "pdf": "https://arxiv.org/pdf/2509.15540", "abs": "https://arxiv.org/abs/2509.15540", "authors": ["Wei Chen", "Tongguan Wang", "Feiyue Xue", "Junkai Li", "Hui Liu", "Ying Sha"], "title": "Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues", "categories": ["cs.CV", "cs.CL"], "comment": "13 page, 5 figures, uploaded by Wei Chen", "summary": "Desire, as an intention that drives human behavior, is closely related to\nboth emotion and sentiment. Multimodal learning has advanced sentiment and\nemotion recognition, but multimodal approaches specially targeting human desire\nunderstanding remain underexplored. And existing methods in sentiment analysis\npredominantly emphasize verbal cues and overlook images as complementary\nnon-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional\nMultimodal Learning Framework for Desire, Emotion, and Sentiment Recognition,\nwhich enforces mutual guidance between text and image modalities to effectively\ncapture intention-related representations in the image. Specifically,\nlow-resolution images are used to obtain global visual representations for\ncross-modal alignment, while high resolution images are partitioned into\nsub-images and modeled with masked image modeling to enhance the ability to\ncapture fine-grained local features. A text-guided image decoder and an\nimage-guided text decoder are introduced to facilitate deep cross-modal\ninteraction at both local and global representations of image information.\nAdditionally, to balance perceptual gains with computation cost, a mixed-scale\nimage strategy is adopted, where high-resolution images are cropped into\nsub-images for masked modeling. The proposed approach is evaluated on MSED, a\nmultimodal dataset that includes a desire understanding benchmark, as well as\nemotion and sentiment recognition. Experimental results indicate consistent\nimprovements over other state-of-the-art methods, validating the effectiveness\nof our proposed method. Specifically, our method outperforms existing\napproaches, achieving F1-score improvements of 1.1% in desire understanding,\n0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is\navailable at: https://github.com/especiallyW/SyDES.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u79f0\u53cc\u5411\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u7406\u89e3\u548c\u8bc6\u522b\u4eba\u7c7b\u7684\u6b32\u671b\u3001\u60c5\u7eea\u548c\u60c5\u611f\u3002", "motivation": "\u73b0\u6709\u7684\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u5f3a\u8c03\u8bed\u8a00\u7ebf\u7d22\uff0c\u800c\u5ffd\u7565\u4e86\u56fe\u50cf\u4f5c\u4e3a\u8865\u5145\u975e\u8bed\u8a00\u7ebf\u7d22\u7684\u4f5c\u7528\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e13\u95e8\u9488\u5bf9\u4eba\u7c7b\u6b32\u671b\u7406\u89e3\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u4ee5\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u79f0\u53cc\u5411\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u83b7\u53d6\u5168\u5c40\u89c6\u89c9\u8868\u793a\u4ee5\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u540c\u65f6\u5c06\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5206\u5272\u4e3a\u5b50\u56fe\u50cf\u5e76\u4f7f\u7528\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u6765\u589e\u5f3a\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u7684\u6355\u6349\u80fd\u529b\u3002\u5f15\u5165\u4e86\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u89e3\u7801\u5668\u548c\u56fe\u50cf\u5f15\u5bfc\u7684\u6587\u672c\u89e3\u7801\u5668\uff0c\u4ee5\u4fc3\u8fdb\u56fe\u50cf\u4fe1\u606f\u5728\u5c40\u90e8\u548c\u5168\u5c40\u8868\u793a\u4e0a\u7684\u6df1\u5ea6\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u91c7\u7528\u6df7\u5408\u5c3a\u5ea6\u56fe\u50cf\u7b56\u7565\u6765\u5e73\u8861\u611f\u77e5\u6536\u76ca\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728MSED\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6b32\u671b\u7406\u89e3\u3001\u60c5\u7eea\u8bc6\u522b\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u5206\u522b\u5b9e\u73b0\u4e861.1%\u30010.6%\u548c0.9%\u7684F1\u5206\u6570\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u4eba\u7c7b\u6b32\u671b\u3001\u60c5\u7eea\u548c\u60c5\u611f\u8bc6\u522b\u7684\u5bf9\u79f0\u53cc\u5411\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u4e92\u6307\u5bfc\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\u6765\u6709\u6548\u6355\u6349\u4e0e\u610f\u56fe\u76f8\u5173\u7684\u8868\u793a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.15561", "pdf": "https://arxiv.org/pdf/2509.15561", "abs": "https://arxiv.org/abs/2509.15561", "authors": ["Om Naphade", "Saksham Bansal", "Parikshit Pareek"], "title": "Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML)\npipelines but becomes computationally expensive and opaque with larger models.\nRecently, Large Language Models (LLMs) have been explored for HPT, yet most\nrely on models exceeding 100 billion parameters. We propose an Expert Block\nFramework for HPT using Small LLMs. At its core is the Trajectory Context\nSummarizer (TCS), a deterministic block that transforms raw training\ntrajectories into structured context, enabling small LLMs to analyze\noptimization progress with reliability comparable to larger models. Using two\nlocally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial\nbudget, our TCS-enabled HPT pipeline achieves average performance within ~0.9\npercentage points of GPT-4 across six diverse tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5c0f\u578bLLM\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u4e13\u5bb6\u5757\u6846\u67b6\uff0c\u901a\u8fc7\u8f68\u8ff9\u4e0a\u4e0b\u6587\u6458\u8981\u5668\uff08TCS\uff09\u5b9e\u73b0\u53ef\u9760\u7684\u4f18\u5316\u8fdb\u5ea6\u5206\u6790\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u63a5\u8fd1\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "Hyper-parameter Tuning (HPT) \u5728\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6d41\u6c34\u7ebf\u4e2d\u662f\u5fc5\u8981\u7684\u6b65\u9aa4\uff0c\u4f46\u968f\u7740\u6a21\u578b\u53d8\u5927\uff0c\u53d8\u5f97\u8ba1\u7b97\u6602\u8d35\u4e14\u4e0d\u900f\u660e\u3002\u5c3d\u7ba1\u6700\u8fd1\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7528\u4e8eHPT\uff0c\u4f46\u5927\u591a\u6570\u4f9d\u8d56\u4e8e\u8d85\u8fc71000\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u5bb6\u5757\u6846\u67b6\u7528\u4e8eHPT\uff0c\u6838\u5fc3\u662f\u8f68\u8ff9\u4e0a\u4e0b\u6587\u6458\u8981\u5668\uff08TCS\uff09\uff0c\u5b83\u5c06\u539f\u59cb\u8bad\u7ec3\u8f68\u8ff9\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u4f7f\u5c0f\u578bLLM\u80fd\u591f\u53ef\u9760\u5730\u5206\u6790\u4f18\u5316\u8fdb\u5ea6\u3002", "result": "\u4f7f\u7528\u4e24\u4e2a\u672c\u5730\u8fd0\u884c\u7684LLM\uff08phi4:reasoning14B\u548cqwen2.5-coder:32B\uff09\u548c\u4e00\u4e2a10\u6b21\u8bd5\u9a8c\u9884\u7b97\uff0c\u6211\u4eec\u7684TCS\u589e\u5f3a\u7684HPT\u7ba1\u9053\u5728\u516d\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\u4e2d\u5e73\u5747\u6027\u80fd\u4e0eGPT-4\u76f8\u5dee\u7ea60.9\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u6211\u4eec\u7684TCS\u589e\u5f3a\u7684HPT\u7ba1\u9053\u5728\u516d\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\u4e2d\u5e73\u5747\u6027\u80fd\u4e0eGPT-4\u76f8\u5dee\u7ea60.9\u4e2a\u767e\u5206\u70b9\uff0c\u8bc1\u660e\u4e86\u4f7f\u7528\u5c0f\u578bLLM\u8fdb\u884cHPT\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.15661", "pdf": "https://arxiv.org/pdf/2509.15661", "abs": "https://arxiv.org/abs/2509.15661", "authors": ["Qiaolin Wang", "Xilin Jiang", "Linyang He", "Junkai Wu", "Nima Mesgarani"], "title": "SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "While large audio-language models (LALMs) have demonstrated state-of-the-art\naudio understanding, their reasoning capability in complex soundscapes still\nfalls behind large vision-language models (LVLMs). Compared to the visual\ndomain, one bottleneck is the lack of large-scale chain-of-thought audio data\nto teach LALM stepwise reasoning. To circumvent this data and modality gap, we\npresent SightSound-R1, a cross-modal distillation framework that transfers\nadvanced reasoning from a stronger LVLM teacher to a weaker LALM student on the\nsame audio-visual question answering (AVQA) dataset. SightSound-R1 consists of\nthree core steps: (i) test-time scaling to generate audio-focused chains of\nthought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter\nhallucinations, and (iii) a distillation pipeline with supervised fine-tuning\n(SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM\nstudent. Results show that SightSound-R1 improves LALM reasoning performance\nboth in the in-domain AVQA test set as well as in unseen auditory scenes and\nquestions, outperforming both pretrained and label-only distilled baselines.\nThus, we conclude that vision reasoning can be effectively transferred to audio\nmodels and scaled with abundant audio-visual data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u84b8\u998f\u6846\u67b6SightSound-R1\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u63a8\u7406\u8f6c\u79fb\u5230\u97f3\u9891\u6a21\u578b\u4e2d\uff0c\u63d0\u9ad8\u4e86\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684\u94fe\u5f0f\u601d\u7ef4\u97f3\u9891\u6570\u636e\u6765\u6559\u6388LALM\u9010\u6b65\u63a8\u7406\uff0c\u56e0\u6b64\u5728\u590d\u6742\u58f0\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u843d\u540e\u4e8eLVLM\u3002", "method": "SightSound-R1\uff0c\u4e00\u4e2a\u8de8\u6a21\u6001\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u66f4\u5f3a\u7684LVLM\u6559\u5e08\u7684\u9ad8\u7ea7\u63a8\u7406\u8f6c\u79fb\u5230\u8f83\u5f31\u7684LALM\u5b66\u751f\u4e0a\u3002", "result": "SightSound-R1\u63d0\u9ad8\u4e86LALM\u5728\u57df\u5185AVQA\u6d4b\u8bd5\u96c6\u4ee5\u53ca\u672a\u89c1\u8fc7\u7684\u542c\u89c9\u573a\u666f\u548c\u95ee\u9898\u4e2d\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f18\u4e8e\u9884\u8bad\u7ec3\u548c\u4ec5\u6807\u7b7e\u84b8\u998f\u57fa\u7ebf\u3002", "conclusion": "\u89c6\u89c9\u63a8\u7406\u53ef\u4ee5\u6709\u6548\u5730\u8f6c\u79fb\u5230\u97f3\u9891\u6a21\u578b\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u4e30\u5bcc\u7684\u97f3\u89c6\u9891\u6570\u636e\u8fdb\u884c\u6269\u5c55\u3002"}}
{"id": "2509.15676", "pdf": "https://arxiv.org/pdf/2509.15676", "abs": "https://arxiv.org/abs/2509.15676", "authors": ["Vaibhav Singh", "Soumya Suvra Ghosal", "Kapu Nirmal Joshua", "Soumyabrata Pal", "Sayak Ray Chowdhury"], "title": "KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "In-context learning (ICL) has emerged as a powerful paradigm for adapting\nlarge language models (LLMs) to new and data-scarce tasks using only a few\ncarefully selected task-specific examples presented in the prompt. However,\ngiven the limited context size of LLMs, a fundamental question arises: Which\nexamples should be selected to maximize performance on a given user query?\nWhile nearest-neighbor-based methods like KATE have been widely adopted for\nthis purpose, they suffer from well-known drawbacks in high-dimensional\nembedding spaces, including poor generalization and a lack of diversity. In\nthis work, we study this problem of example selection in ICL from a principled,\ninformation theory-driven perspective. We first model an LLM as a linear\nfunction over input embeddings and frame the example selection task as a\nquery-specific optimization problem: selecting a subset of exemplars from a\nlarger example bank that minimizes the prediction error on a specific query.\nThis formulation departs from traditional generalization-focused learning\ntheoretic approaches by targeting accurate prediction for a specific query\ninstance. We derive a principled surrogate objective that is approximately\nsubmodular, enabling the use of a greedy algorithm with an approximation\nguarantee. We further enhance our method by (i) incorporating the kernel trick\nto operate in high-dimensional feature spaces without explicit mappings, and\n(ii) introducing an optimal design-based regularizer to encourage diversity in\nthe selected examples. Empirically, we demonstrate significant improvements\nover standard retrieval methods across a suite of classification tasks,\nhighlighting the benefits of structure-aware, diverse example selection for ICL\nin real-world, label-scarce scenarios.", "AI": {"tldr": "\u672c\u6587\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u7814\u7a76\u4e86Icl\u4e2d\u7684\u793a\u4f8b\u9009\u62e9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u548c\u591a\u6837\u5316\u9009\u62e9\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eLLM\u7684\u4e0a\u4e0b\u6587\u5927\u5c0f\u6709\u9650\uff0c\u5982\u4f55\u9009\u62e9\u793a\u4f8b\u4ee5\u6700\u5927\u5316\u6027\u80fd\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u6700\u8fd1\u90bb\u7684\u65b9\u6cd5\u5728\u9ad8\u7ef4\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u5c06LLM\u5efa\u6a21\u4e3a\u8f93\u5165\u5d4c\u5165\u4e0a\u7684\u7ebf\u6027\u51fd\u6570\uff0c\u5e76\u5c06\u793a\u4f8b\u9009\u62e9\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u67e5\u8be2\u7279\u5b9a\u7684\u4f18\u5316\u95ee\u9898\uff0c\u540c\u65f6\u5f15\u5165\u6838\u6280\u5de7\u548c\u6700\u4f18\u8bbe\u8ba1\u6b63\u5219\u5316\u6765\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u4e00\u7cfb\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u68c0\u7d22\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u611f\u77e5\u548c\u591a\u6837\u5316\u793a\u4f8b\u9009\u62e9\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u548c\u591a\u6837\u5316\u7684\u793a\u4f8b\u9009\u62e9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u3001\u6807\u7b7e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684Icl\u6027\u80fd\u3002"}}
{"id": "2509.15692", "pdf": "https://arxiv.org/pdf/2509.15692", "abs": "https://arxiv.org/abs/2509.15692", "authors": ["Pei Zhang", "Yiming Wang", "Jialong Tang", "Baosong Yang", "Rui Wang", "Derek F. Wong", "Fei Huang"], "title": "Direct Simultaneous Translation Activation for Large Audio-Language Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech\ninto target text in real time, outputting translations while receiving source\nspeech input, rather than waiting for the entire utterance to be spoken.\nSimul-S2TT research often modifies model architectures to implement read-write\nstrategies. However, with the rise of large audio-language models (LALMs), a\nkey challenge is how to directly activate Simul-S2TT capabilities in base\nmodels without additional architectural changes. In this paper, we introduce\n{\\bf Simul}taneous {\\bf S}elf-{\\bf A}ugmentation ({\\bf SimulSA}), a strategy\nthat utilizes LALMs' inherent capabilities to obtain simultaneous data by\nrandomly truncating speech and constructing partially aligned translation. By\nincorporating them into offline SFT data, SimulSA effectively bridges the\ndistribution gap between offline translation during pretraining and\nsimultaneous translation during inference. Experimental results demonstrate\nthat augmenting only about {\\bf 1\\%} of the simultaneous data, compared to the\nfull offline SFT data, can significantly activate LALMs' Simul-S2TT\ncapabilities without modifications to model architecture or decoding strategy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSimulSA\u7684\u7b56\u7565\uff0c\u5229\u7528\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u80fd\u529b\u6765\u83b7\u5f97\u540c\u65f6\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5c06\u5176\u7eb3\u5165\u79bb\u7ebfSFT\u6570\u636e\u4e2d\uff0c\u6709\u6548\u5f25\u5408\u4e86\u9884\u8bad\u7ec3\u671f\u95f4\u79bb\u7ebf\u7ffb\u8bd1\u548c\u63a8\u7406\u671f\u95f4\u540c\u65f6\u7ffb\u8bd1\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u8ddd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u589e\u5f3a\u7ea61%\u7684\u540c\u65f6\u6570\u636e\u5c31\u53ef\u4ee5\u663e\u8457\u6fc0\u6d3bLALMs\u7684Simul-S2TT\u80fd\u529b\uff0c\u800c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u89e3\u7801\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08LALMs\uff09\u7684\u5174\u8d77\uff0c\u5173\u952e\u6311\u6218\u662f\u5982\u4f55\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u67b6\u6784\u66f4\u6539\u7684\u60c5\u51b5\u4e0b\u76f4\u63a5\u6fc0\u6d3b\u57fa\u7840\u6a21\u578b\u7684Simul-S2TT\u529f\u80fd\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86SimulSA\u7b56\u7565\uff0c\u5229\u7528LALMs\u7684\u5185\u5728\u80fd\u529b\u901a\u8fc7\u968f\u673a\u622a\u65ad\u8bed\u97f3\u548c\u6784\u5efa\u90e8\u5206\u5bf9\u9f50\u7ffb\u8bd1\u6765\u83b7\u5f97\u540c\u65f6\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u589e\u5f3a\u7ea61%\u7684\u540c\u65f6\u6570\u636e\u53ef\u4ee5\u663e\u8457\u6fc0\u6d3bLALMs\u7684Simul-S2TT\u80fd\u529b\uff0c\u800c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u89e3\u7801\u7b56\u7565\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u589e\u5f3a\u7ea61%\u7684\u540c\u65f6\u6570\u636e\u5c31\u53ef\u4ee5\u663e\u8457\u6fc0\u6d3bLALMs\u7684Simul-S2TT\u80fd\u529b\uff0c\u800c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u89e3\u7801\u7b56\u7565\u3002"}}
{"id": "2509.15957", "pdf": "https://arxiv.org/pdf/2509.15957", "abs": "https://arxiv.org/abs/2509.15957", "authors": ["Kanato Masayoshi", "Masahiro Hashimoto", "Ryoichi Yokoyama", "Naoki Toda", "Yoshifumi Uwamino", "Shogo Fukuda", "Ho Namkoong", "Masahiro Jinzaki"], "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "comment": null, "summary": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u901a\u8fc7MCP\u8fde\u63a5\u5230EHR\u6570\u636e\u5e93\u7684LLM\u5728\u771f\u5b9e\u533b\u9662\u73af\u5883\u4e2d\u81ea\u4e3b\u68c0\u7d22\u4e34\u5e8a\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0cLLM\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u4f46\u5728\u9700\u8981\u65f6\u95f4\u4f9d\u8d56\u8ba1\u7b97\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\u3002EHR-MCP\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168\u3001\u4e00\u81f4\u7684\u6570\u636e\u8bbf\u95ee\u57fa\u7840\u8bbe\u65bd\uff0c\u672a\u6765\u5e94\u6269\u5c55\u81f3\u63a8\u7406\u3001\u751f\u6210\u548c\u4e34\u5e8a\u5f71\u54cd\u8bc4\u4f30\u3002", "motivation": "Large language models (LLMs) show promise in medicine, but their deployment in hospitals is limited by restricted access to electronic health record (EHR) systems. The Model Context Protocol (MCP) enables integration between LLMs and external tools.", "method": "We developed EHR-MCP, a framework of custom MCP tools integrated with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct agent to interact with it. Six tasks were tested, derived from use cases of the infection control team (ICT). Eight patients discussed at ICT conferences were retrospectively analyzed. Agreement with physician-generated gold standards was measured.", "result": "The LLM consistently selected and executed the correct MCP tools. Except for two tasks, all tasks achieved near-perfect accuracy. Performance was lower in the complex task requiring time-dependent calculations. Most errors arose from incorrect arguments or misinterpretation of tool results. Responses from EHR-MCP were reliable, though long and repetitive data risked exceeding the context window.", "conclusion": "LLMs can retrieve clinical data from an EHR via MCP tools in a real hospital setting, achieving near-perfect performance in simple tasks while highlighting challenges in complex ones. EHR-MCP provides an infrastructure for secure, consistent data access and may serve as a foundation for hospital AI agents. Future work should extend beyond retrieval to reasoning, generation, and clinical impact assessment, paving the way for effective integration of generative AI into clinical practice."}}
{"id": "2509.15969", "pdf": "https://arxiv.org/pdf/2509.15969", "abs": "https://arxiv.org/abs/2509.15969", "authors": ["Nikita Torgashov", "Gustav Eje Henter", "Gabriel Skantze"], "title": "VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.SD"], "comment": "5 pages, 1 figure, submitted to IEEE ICASSP 2026", "summary": "We present VoXtream, a fully autoregressive, zero-shot streaming\ntext-to-speech (TTS) system for real-time use that begins speaking from the\nfirst word. VoXtream directly maps incoming phonemes to audio tokens using a\nmonotonic alignment scheme and a dynamic look-ahead that does not delay onset.\nBuilt around an incremental phoneme transformer, a temporal transformer\npredicting semantic and duration tokens, and a depth transformer producing\nacoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay\namong publicly available streaming TTS: 102 ms on GPU. Despite being trained on\na mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several\nmetrics, while delivering competitive quality in both output- and\nfull-streaming settings. Demo and code are available at\nhttps://herimor.github.io/voxtream.", "AI": {"tldr": "VoXtream is a low-delay, high-performance streaming TTS system that maps phonemes to audio tokens using a combination of transformers.", "motivation": "The motivation is to create a fully autoregressive, zero-shot streaming text-to-speech system that can start speaking from the first word with minimal delay.", "method": "VoXtream uses a monotonic alignment scheme and dynamic look-ahead to map incoming phonemes to audio tokens. It is built around an incremental phoneme transformer, a temporal transformer predicting semantic and duration tokens, and a depth transformer producing acoustic tokens.", "result": "VoXtream achieves an initial delay of 102 ms on GPU and performs well despite being trained on a mid-scale 9k-hour corpus.", "conclusion": "VoXtream achieves the lowest initial delay among publicly available streaming TTS and matches or surpasses larger baselines on several metrics."}}
{"id": "2509.15986", "pdf": "https://arxiv.org/pdf/2509.15986", "abs": "https://arxiv.org/abs/2509.15986", "authors": ["Xinchen Wan", "Jinhua Liang", "Huan Zhang"], "title": "EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC", "cs.SD", "eess.AS"], "comment": "5 pages, 5 figures. Submitted to the 2026 IEEE International\n  Conference on Acoustics, Speech and Signal Processing (ICASSP 2026)", "summary": "Existing digital mental wellness tools often overlook the nuanced emotional\nstates underlying everyday challenges. For example, pre-sleep anxiety affects\nmore than 1.5 billion people worldwide, yet current approaches remain largely\nstatic and \"one-size-fits-all\", failing to adapt to individual needs. In this\nwork, we present EmoHeal, an end-to-end system that delivers personalized,\nthree-stage supportive narratives. EmoHeal detects 27 fine-grained emotions\nfrom user text with a fine-tuned XLM-RoBERTa model, mapping them to musical\nparameters via a knowledge graph grounded in music therapy principles (GEMS,\niso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to\nguide users from their current state toward a calmer one\n(\"match-guide-target\"). A within-subjects study (N=40) demonstrated significant\nsupportive effects, with participants reporting substantial mood improvement\n(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,\np<0.001). A strong correlation between perceived accuracy and therapeutic\noutcome (r=0.72, p<0.001) validates our fine-grained approach. These findings\nestablish the viability of theory-driven, emotion-aware digital wellness tools\nand provides a scalable AI blueprint for operationalizing music therapy\nprinciples.", "AI": {"tldr": "EmoHeal\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u4e09\u9636\u6bb5\u652f\u6301\u53d9\u8ff0\u3002\u5b83\u901a\u8fc7\u5fae\u8c03XLM-RoBERTa\u6a21\u578b\u68c0\u6d4b27\u79cd\u7ec6\u7c92\u5ea6\u60c5\u7eea\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u97f3\u4e50\u6cbb\u7597\u539f\u7406\u7684\u77e5\u8bc6\u56fe\u8c31\u5c06\u5176\u6620\u5c04\u5230\u97f3\u4e50\u53c2\u6570\u3002\u7136\u540e\u4f7f\u7528CLAMP3\u6a21\u578b\u68c0\u7d22\u89c6\u542c\u5185\u5bb9\uff0c\u5f15\u5bfc\u7528\u6237\u4ece\u5f53\u524d\u72b6\u6001\u8d70\u5411\u66f4\u5e73\u9759\u7684\u72b6\u6001\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u663e\u8457\u7684\u60c5\u7eea\u6539\u5584\u548c\u9ad8\u611f\u77e5\u60c5\u7eea\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u7406\u8bba\u9a71\u52a8\u7684\u3001\u60c5\u7eea\u611f\u77e5\u7684\u6570\u5b57\u5065\u5eb7\u5de5\u5177\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u5b57\u5fc3\u7406\u5065\u5eb7\u5de5\u5177\u5f80\u5f80\u5ffd\u89c6\u65e5\u5e38\u6311\u6218\u80cc\u540e\u7684\u7ec6\u5fae\u60c5\u611f\u72b6\u6001\u3002\u4f8b\u5982\uff0c\u7761\u524d\u7126\u8651\u5f71\u54cd\u5168\u7403\u8d85\u8fc715\u4ebf\u4eba\uff0c\u4f46\u76ee\u524d\u7684\u65b9\u6cd5\u4ecd\u7136\u4e3b\u8981\u662f\u9759\u6001\u7684\uff0c'\u4e00\u5200\u5207'\uff0c\u65e0\u6cd5\u9002\u5e94\u4e2a\u4f53\u9700\u6c42\u3002", "method": "EmoHeal\u7cfb\u7edf\u901a\u8fc7\u5fae\u8c03XLM-RoBERTa\u6a21\u578b\u68c0\u6d4b27\u79cd\u7ec6\u7c92\u5ea6\u60c5\u7eea\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u97f3\u4e50\u6cbb\u7597\u539f\u7406\u7684\u77e5\u8bc6\u56fe\u8c31\uff08GEMS\uff0ciso-principle\uff09\u5c06\u5b83\u4eec\u6620\u5c04\u5230\u97f3\u4e50\u53c2\u6570\u3002\u7136\u540e\u4f7f\u7528CLAMP3\u6a21\u578b\u68c0\u7d22\u89c6\u542c\u5185\u5bb9\uff0c\u5f15\u5bfc\u7528\u6237\u4ece\u5f53\u524d\u72b6\u6001\u8d70\u5411\u66f4\u5e73\u9759\u7684\u72b6\u6001\uff08\u201cmatch-guide-target\u201d\uff09\u3002", "result": "\u4e00\u9879\u9488\u5bf940\u540d\u53c2\u4e0e\u8005\u7684\u88ab\u8bd5\u5185\u7814\u7a76\u663e\u793a\uff0c\u53c2\u4e0e\u8005\u62a5\u544a\u4e86\u663e\u8457\u7684\u60c5\u7eea\u6539\u5584\uff08M=4.12\uff0cp<0.001\uff09\u548c\u9ad8\u5ea6\u7684\u611f\u77e5\u60c5\u7eea\u8bc6\u522b\u51c6\u786e\u6027\uff08M=4.05\uff0cp<0.001\uff09\u3002\u611f\u77e5\u51c6\u786e\u6027\u548c\u6cbb\u7597\u7ed3\u679c\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff08r=0.72\uff0cp<0.001\uff09\uff0c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u7ec6\u7c92\u5ea6\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8bc1\u660e\u4e86\u7406\u8bba\u9a71\u52a8\u7684\u3001\u60c5\u7eea\u611f\u77e5\u7684\u6570\u5b57\u5065\u5eb7\u5de5\u5177\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u5b9e\u73b0\u97f3\u4e50\u6cbb\u7597\u539f\u5219\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684AI\u84dd\u56fe\u3002"}}
{"id": "2509.16060", "pdf": "https://arxiv.org/pdf/2509.16060", "abs": "https://arxiv.org/abs/2509.16060", "authors": ["Maithili Joshi", "Palash Nandi", "Tanmoy Chakraborty"], "title": "SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted in EMNLP'25 Main", "summary": "Large Language Models (LLMs) with safe-alignment training are powerful\ninstruments with robust language comprehension capabilities. These models\ntypically undergo meticulous alignment procedures involving human feedback to\nensure the acceptance of safe inputs while rejecting harmful or unsafe ones.\nHowever, despite their massive scale and alignment efforts, LLMs remain\nvulnerable to jailbreak attacks, where malicious users manipulate the model to\nproduce harmful outputs that it was explicitly trained to avoid. In this study,\nwe find that the safety mechanisms in LLMs are predominantly embedded in the\nmiddle-to-late layers. Building on this insight, we introduce a novel white-box\njailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which\nconnects two intermediate layers $s$ and $e$ such that $s < e$, through a\nresidual connection. Our approach achieves a 51% improvement over the\nbest-performing baseline on the HarmBench test set. Furthermore, SABER induces\nonly a marginal shift in perplexity when evaluated on the HarmBench validation\nset. The source code is publicly available at\nhttps://github.com/PalGitts/SABER.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u4e3b\u8981\u5d4c\u5165\u5728\u4e2d\u5c42\u5230\u540e\u671f\u5c42\u4e2d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u767d\u76d2\u8d8a\u72f1\u65b9\u6cd5SABER\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5\u8fde\u63a5\u4e24\u4e2a\u4e2d\u95f4\u5c42\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u6a21\u578b\u5b89\u5168\u673a\u5236\u7684\u7ed5\u8fc7\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u8fc7\u4e86\u7cbe\u5fc3\u7684\u5bf9\u9f50\u8fc7\u7a0b\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u9632\u6b62\u8fd9\u79cd\u653b\u51fb\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSABER\u7684\u65b0\u578b\u767d\u76d2\u8d8a\u72f1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5\u8fde\u63a5\u4e24\u4e2a\u4e2d\u95f4\u5c42\uff0c\u4ee5\u5b9e\u73b0\u5bf9LLMs\u5b89\u5168\u673a\u5236\u7684\u7ed5\u8fc7\u3002", "result": "SABER\u5728HarmBench\u6d4b\u8bd5\u96c6\u4e0a\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e8651%\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728HarmBench\u9a8c\u8bc1\u96c6\u4e0a\u4ec5\u5f15\u8d77\u4e86\u4e00\u4e2a\u5fae\u5c0f\u7684\u56f0\u60d1\u5ea6\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u4e2d\u7684\u5b89\u5168\u673a\u5236\u4e3b\u8981\u5d4c\u5165\u5728\u4e2d\u5c42\u5230\u540e\u671f\u5c42\u4e2d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u767d\u76d2\u8d8a\u72f1\u65b9\u6cd5SABER\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5\u8fde\u63a5\u4e24\u4e2a\u4e2d\u95f4\u5c42\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5bf9\u6a21\u578b\u5b89\u5168\u673a\u5236\u7684\u7ed5\u8fc7\u3002"}}
{"id": "2509.16163", "pdf": "https://arxiv.org/pdf/2509.16163", "abs": "https://arxiv.org/abs/2509.16163", "authors": ["Het Patel", "Muzammil Allie", "Qian Zhang", "Jia Chen", "Evangelos E. Papalexakis"], "title": "Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "To be presented as a poster at the Workshop on Safe and Trustworthy\n  Multimodal AI Systems (SafeMM-AI), 2025", "summary": "Vision language models (VLMs) excel in multimodal understanding but are prone\nto adversarial attacks. Existing defenses often demand costly retraining or\nsignificant architecture changes. We introduce a lightweight defense using\ntensor decomposition suitable for any pre-trained VLM, requiring no retraining.\nBy decomposing and reconstructing vision encoder representations, it filters\nadversarial noise while preserving meaning. Experiments with CLIP on COCO and\nFlickr30K show improved robustness. On Flickr30K, it restores 12.3\\%\nperformance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On\nCOCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%.\nAnalysis shows Tensor Train decomposition with low rank (8-32) and low residual\nstrength ($\\alpha=0.1-0.2$) is optimal. This method is a practical,\nplug-and-play solution with minimal overhead for existing VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u5229\u7528\u5f20\u91cf\u5206\u89e3\u6765\u8fc7\u6ee4\u5bf9\u6297\u6027\u566a\u58f0\uff0c\u63d0\u9ad8VLM\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9632\u5fa1\u63aa\u65bd\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u6216\u663e\u8457\u7684\u67b6\u6784\u66f4\u6539\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5f20\u91cf\u5206\u89e3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u91cd\u5efa\u89c6\u89c9\u7f16\u7801\u5668\u8868\u793a\u6765\u8fc7\u6ee4\u5bf9\u6297\u6027\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\u3002", "result": "\u5728Flickr30K\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6062\u590d\u4e8612.3%\u56e0\u653b\u51fb\u800c\u635f\u5931\u7684\u6027\u80fd\uff0c\u5c06Recall@1\u51c6\u786e\u7387\u4ece7.5%\u63d0\u9ad8\u523019.8%\uff1b\u5728COCO\u6570\u636e\u96c6\u4e0a\uff0c\u6062\u590d\u4e868.1%\u7684\u6027\u80fd\uff0c\u5c06\u51c6\u786e\u7387\u4ece3.8%\u63d0\u9ad8\u523011.9%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73b0\u6709\u7684VLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6700\u5c0f\u7684\u5f00\u9500\u3002"}}
{"id": "2509.16189", "pdf": "https://arxiv.org/pdf/2509.16189", "abs": "https://arxiv.org/abs/2509.16189", "authors": ["Andrew Kyle Lampinen", "Martin Engelcke", "Yuxuan Li", "Arslan Chaudhry", "James L. McClelland"], "title": "Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "When do machine learning systems fail to generalize, and what mechanisms\ncould improve their generalization? Here, we draw inspiration from cognitive\nscience to argue that one weakness of machine learning systems is their failure\nto exhibit latent learning -- learning information that is not relevant to the\ntask at hand, but that might be useful in a future task. We show how this\nperspective links failures ranging from the reversal curse in language modeling\nto new findings on agent-based navigation. We then highlight how cognitive\nscience points to episodic memory as a potential part of the solution to these\nissues. Correspondingly, we show that a system with an oracle retrieval\nmechanism can use learning experiences more flexibly to generalize better\nacross many of these challenges. We also identify some of the essential\ncomponents for effectively using retrieval, including the importance of\nwithin-example in-context learning for acquiring the ability to use information\nacross retrieved examples. In summary, our results illustrate one possible\ncontributor to the relative data inefficiency of current machine learning\nsystems compared to natural intelligence, and help to understand how retrieval\nmethods can complement parametric learning to improve generalization.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u6cdb\u5316\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u60c5\u666f\u8bb0\u5fc6\u548c\u68c0\u7d22\u673a\u5236\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u6211\u4eec\u8bd5\u56fe\u7406\u89e3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e3a\u4f55\u5728\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u5bfb\u627e\u53ef\u80fd\u7684\u6539\u8fdb\u673a\u5236\u3002", "method": "\u6211\u4eec\u4ece\u8ba4\u77e5\u79d1\u5b66\u4e2d\u83b7\u5f97\u7075\u611f\uff0c\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u6cdb\u5316\u65b9\u9762\u7684\u5931\u8d25\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5982\u5229\u7528\u60c5\u666f\u8bb0\u5fc6\u548c\u68c0\u7d22\u673a\u5236\u6765\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6211\u4eec\u5c55\u793a\u4e86\u68c0\u7d22\u673a\u5236\u5982\u4f55\u5e2e\u52a9\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u66f4\u7075\u6d3b\u5730\u5229\u7528\u5b66\u4e60\u7ecf\u9a8c\uff0c\u4ece\u800c\u5728\u591a\u4e2a\u6311\u6218\u4e2d\u66f4\u597d\u5730\u8fdb\u884c\u6cdb\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bc6\u522b\u4e86\u6709\u6548\u4f7f\u7528\u68c0\u7d22\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u5305\u62ec\u5728\u793a\u4f8b\u5185\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8bf4\u660e\u4e86\u5f53\u524d\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u76f8\u5bf9\u4e8e\u81ea\u7136\u667a\u80fd\u7684\u6570\u636e\u6548\u7387\u8f83\u4f4e\u7684\u4e00\u4e2a\u53ef\u80fd\u539f\u56e0\uff0c\u5e76\u6709\u52a9\u4e8e\u7406\u89e3\u68c0\u7d22\u65b9\u6cd5\u5982\u4f55\u8865\u5145\u53c2\u6570\u5b66\u4e60\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.16197", "pdf": "https://arxiv.org/pdf/2509.16197", "abs": "https://arxiv.org/abs/2509.16197", "authors": ["Yanghao Li", "Rui Qian", "Bowen Pan", "Haotian Zhang", "Haoshuo Huang", "Bowen Zhang", "Jialing Tong", "Haoxuan You", "Xianzhi Du", "Zhe Gan", "Hyunjik Kim", "Chao Jia", "Zhenbang Wang", "Yinfei Yang", "Mingfei Gao", "Zi-Yi Dou", "Wenze Hu", "Chang Gao", "Dongxu Li", "Philipp Dufter", "Zirui Wang", "Guoli Yin", "Zhengdong Zhang", "Chen Chen", "Yang Zhao", "Ruoming Pang", "Zhifeng Chen"], "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer.", "AI": {"tldr": "Manzano is a unified framework that reduces the performance trade-off between understanding and generating visual content by using a hybrid image tokenizer and a well-curated training recipe.", "motivation": "Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities.", "method": "Manzano is a simple and scalable unified framework that couples a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels.", "result": "Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.", "conclusion": "Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer."}}

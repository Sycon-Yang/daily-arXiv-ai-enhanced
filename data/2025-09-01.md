<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: 本文提出了一种名为CoBA的反偏数据增强方法，通过在语义三元组级别操作，破坏虚假相关性，从而提高模型性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型常常学习和利用训练数据中的虚假相关性，使用这些非目标特征来指导其预测。这种依赖会导致性能下降，并在未见过的数据上产生较差的泛化效果。为了克服这些限制，我们引入了一种更一般的反偏数据增强形式，称为反偏数据增强，同时处理多种偏差（例如性别偏差、简单性偏差）并提高分布外的鲁棒性。

Method: CoBA：CounterBias Augmentation，一个在语义三元组级别操作的统一框架，首先将文本分解为主体-谓词-对象三元组，然后选择性地修改这些三元组以破坏虚假相关性。通过从这些调整后的三元组重新构建文本，CoBA生成了减轻虚假模式的反偏数据。

Result: 通过广泛的实验，我们证明CoBA不仅提高了下游任务性能，还有效减少了偏差并增强了分布外的鲁棒性。

Conclusion: CoBA不仅提高了下游任务性能，还有效减少了偏差并增强了分布外的鲁棒性，为应对虚假相关性带来的挑战提供了一个多功能且稳健的解决方案。

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [2] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: 该研究引入了一个大规模的德国数据集，用于研究有毒言论，并揭示了不同年龄群体在在线交流中的差异。


<details>
  <summary>Details</summary>
Motivation: 现有有毒言论数据集缺乏人口统计背景，限制了我们对不同年龄群体在线交流的理解。

Method: 该研究与德国公共事业内容网络funk合作，引入了第一个大规模的德国数据集，该数据集对毒性进行了标注，并包含了平台提供的年龄估计。数据集包括3,024个由人类标注的和30,024个由LLM标注的匿名评论，来自Instagram、TikTok和YouTube。

Result: 数据集揭示了基于年龄的有毒言论模式差异，年轻用户更倾向于使用表达性语言，而年长用户更常参与虚假信息和贬低行为。

Conclusion: 该数据集为研究不同年龄群体的网络交流提供了新的机会，并支持开发更公平和年龄意识的内容审核系统。

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [3] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: Granite Embedding R2 models are high-performance English encoder-based embedding models that offer substantial improvements in context length, performance across various retrieval domains, and speed advantages over competitors.


<details>
  <summary>Details</summary>
Motivation: To address the need for high-performance embedding models that can handle diverse retrieval domains and provide measurable speed advantages while maintaining accuracy.

Method: The Granite Embedding R2 models are high-performance English encoder-based embedding models engineered for enterprise-scale dense retrieval applications. They include both bi-encoder and cross-encoder architectures, with a 22-layer retriever model and its efficient 12-layer counterpart, alongside a high-quality reranker model.

Result: The models demonstrate exceptional versatility across standard benchmarks, IBM-developed evaluation suites, and real-world enterprise use cases, establishing new performance standards for open-source embedding models.

Conclusion: Granite R2 models deliver a compelling combination of cutting-edge performance, enterprise-ready licensing, and transparent data provenance that organizations require for mission-critical deployments.

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [4] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: 本文提出了TrInk模型，通过引入缩放位置嵌入和高斯记忆掩码改进了Transformer模型，显著降低了手写体生成中的字符和单词错误率。


<details>
  <summary>Details</summary>
Motivation: 为了提高手写体生成的准确性和风格一致性，需要一种能够有效捕捉全局依赖关系的模型，并且需要更好的输入文本与生成笔画点之间的对齐方法。

Method: 提出了一种基于Transformer的墨水生成模型TrInk，引入了缩放的位置嵌入和高斯记忆掩码以更好地对齐输入文本和生成的笔画点，并设计了主观和客观评估流程来全面评估生成的手写体的可读性和风格一致性。

Result: TrInk模型在IAM-OnDB数据集上相比之前的方法在字符错误率（CER）和单词错误率（WER）上分别减少了35.56%和29.66%。

Conclusion: TrInk模型在IAM-OnDB数据集上相比之前的方法在字符错误率（CER）和单词错误率（WER）上分别减少了35.56%和29.66%，证明了其有效性。

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [5] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: 该研究探讨了LLM中的锚定效应，发现它们像人类一样受到影响，但推理模型较少受其影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLM中的锚定效应，以了解其在现实世界应用中的可靠性。

Method: 我们让卖家LLM代理应用锚定效应，并使用客观和主观指标评估谈判。

Result: 实验结果表明，LLM像人类一样受到锚定效应的影响。此外，推理模型不太容易受到锚定效应的影响，而个性特征与对锚定效应的易感性之间没有显著相关性。

Conclusion: 这些发现有助于更深入地理解LLM中的认知偏差，并有助于实现LLM在社会中的安全和负责任的应用。

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [6] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: 本文介绍了一个新的数据集Percept-V，用于评估MLLMs在基本视觉感知任务上的表现，并发现这些模型在复杂任务中的性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 目前对MLLMs在简单感知任务上的表现研究有限，因此需要一个专门针对基本视觉感知技能的数据集来评估其能力。

Method: 论文引入了一个名为Percept-V的数据集，包含7200张程序生成的图像，分为30个类别，每个类别测试不同的视觉感知技能。然后将该数据集应用于最先进的MLLMs和大型推理模型进行测试。

Result: 实验结果显示，随着问题复杂性的增加，所有测试的MLLMs的性能都有显著下降，并且它们在不同认知技能上的表现相似。

Conclusion: 实验结果表明，随着问题复杂性的增加，所有测试的MLLMs的性能都有显著下降，并且它们在不同认知技能上的表现相似。

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [7] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文从数据角度分析了科学大语言模型的发展，探讨了其在科学知识表示和应用中的挑战与机遇，并提出了一个闭环系统以促进科学发现。


<details>
  <summary>Details</summary>
Motivation: 科学大语言模型正在改变科学知识的表示、整合和应用方式，但它们的进步受到科学数据复杂性的制约。因此，需要一种新的视角来理解Sci-LLMs的发展。

Method: 本文通过构建统一的科学数据分类体系和分层的科学知识模型，对近年来的Sci-LLMs进行了系统性回顾，并分析了超过270个预训练/后训练数据集以及190多个基准数据集。

Result: 本文揭示了Sci-LLMs在数据需求上的独特性，包括异构性、多尺度性和不确定性，并探讨了半自动化标注流程和专家验证等新兴解决方案。此外，还提出了一种基于Sci-LLMs的闭环系统，用于主动实验、验证和贡献科学知识。

Conclusion: 本文提出了一个数据驱动的视角来分析科学大语言模型（Sci-LLMs）的发展，强调了其与科学数据之间的协同进化关系，并展望了一个以Sci-LLMs为基础的闭环系统，用于加速科学发现。

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [8] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: 研究发现，模型的身份感知会显著扭曲高层次的判断，并微妙地影响详细的质量评分，这强调了在LLM基准测试中使用盲测或多模型评估协议的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地用于评估输出，它们的判断可能会受到干扰。本研究旨在探讨自我和跨模型评估中的偏见问题。

Method: 研究检查了ChatGPT、Gemini和Claude在四种条件下的自我和跨模型评估中的偏见：无标签、真实标签和两种假标签场景。通过整体偏好投票和质量评分（包括连贯性、信息量和简洁性）对每个模型撰写的博客文章进行了评估，并将所有分数表示为百分比以便直接比较。

Result: 结果揭示了明显的不对称性：'Claude'标签始终提升分数，而'Gemini'标签始终降低分数，无论实际内容如何。假标签经常逆转排名，导致偏好投票的分数变化高达50个百分点，转换后的质量评分变化高达12个百分点。Gemini的真实标签下自评分数崩溃，而Claude的自评偏好则增强。

Conclusion: 研究发现，模型的身份感知会显著扭曲高层次的判断，并微妙地影响详细的质量评分，这强调了在LLM基准测试中使用盲测或多模型评估协议的重要性。

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [9] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: 本文提出了一种基于贝叶斯实验设计的通用方法，以提高大型语言模型在多轮对话中与用户互动和获取信息的能力，并展示了其在多种任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 改进大型语言模型在与用户或其他外部源交互时的信息收集能力，使其能够作为有效的多轮对话代理并与外部环境进行交互。

Method: BED-LLM 基于顺序贝叶斯实验设计框架，通过迭代选择最大化预期信息增益的问题或查询来改进大型语言模型的能力。

Result: BED-LLM 在多个测试中表现优于直接提示大型语言模型和其他自适应设计策略。

Conclusion: BED-LLM 在各种测试中表现出显著的性能提升，特别是在20个问题游戏中和主动推断用户偏好时。

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [10] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的自动化HFACS分类框架，用于航空安全分析，通过微调语言模型并结合多组件奖励系统和合成数据生成，显著提升了性能，并验证了领域优化模型在资源受限设备上的可行性。


<details>
  <summary>Details</summary>
Motivation: 分析航空事故中的人因因素对于防止未来事件至关重要，但传统的使用HFACS的方法在可扩展性和一致性方面存在局限。

Method: 本文引入了一个自动化HFACS分类框架，利用强化学习中的组相对策略优化（GRPO）微调Llama-3.1 8B语言模型。该方法结合了一个针对航空安全分析的多组件奖励系统，并集成了合成数据生成以克服事故数据集中的类别不平衡问题。

Result: GRPO优化的模型在性能上有了显著提升，包括精确匹配准确率提高了350%（从0.0400到0.1800），部分匹配准确率为0.8800。此外，该模型在关键指标上优于最先进的LLMs，如GPT-5-mini和Gemini-2.5-flash。

Conclusion: 本研究验证了较小的、领域优化的模型可以在计算效率和关键安全分析方面提供更好的解决方案。这种方法使得在资源受限的边缘设备上进行高效、低延迟的部署成为可能。

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [11] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: 本文提出了一种基于像素的生成语言模型，以提高对正交噪声的鲁棒性，并在多语言设置中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决自回归语言模型在多语言字母表中受到的正交攻击问题，这些攻击会导致性能显著下降。

Method: 提出了一种基于像素的生成语言模型，通过将单词渲染为单独的图像来替换基于文本的嵌入。

Result: 在多语言LAMBADA数据集、WMT24数据集和SST-2基准测试中评估了所提出的方法。

Conclusion: 该方法在多语言设置中表现出对正交噪声的抵抗力和有效性。

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [12] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: 本文研究了自监督语音模型中的关键期效应，发现它们并未表现出明显的CP效应，延迟的L2暴露起始时间反而有助于L2的学习，而延迟的L1暴露结束时间则导致了L1的遗忘。


<details>
  <summary>Details</summary>
Motivation: 尽管口语在人类语言习得中起着核心作用，但关于自监督语音模型中的关键期效应的研究仍然不足。本文旨在填补这一研究空白。

Method: 本文通过训练具有不同L2训练起始时间和L1训练结束时间的自监督语音模型（S3Ms），并在儿童导向的语音上进行评估，以研究关键期效应是否存在于这些模型中。

Result: 实验结果表明，自监督语音模型没有表现出明显的CP效应，延迟的L2暴露起始时间反而提高了模型在L2上的表现，而延迟的L1暴露结束时间导致了L1的遗忘。

Conclusion: 本文发现自监督语音模型（S3Ms）在语音习得方面并未表现出明显的关键期（CP）效应。值得注意的是，延迟的L2暴露起始时间反而使模型在L2上的表现更好，而延迟的L1暴露结束时间则导致了L1的遗忘。

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


### [13] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
*Weizhi Gao,Xiaorui Liu,Feiyi Wang,Dan Lu,Junqi Yin*

Main category: cs.CL

TL;DR: 本文提出了一种新的解码记忆流水线（DMP），通过选择性推理和退火解码加速生成，实现了高达3倍的加速，并且可以扩展到对齐和推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法在句子级生成中表现不佳，或者依赖于领域特定知识。自一致性方法虽然有助于解决这些问题，但计算成本高。因此，需要一种更高效的解决方案。

Method: 本文首先研究了自一致性方法中的冗余性，发现共享前缀标记对语义内容贡献较小。基于这一观察，提出了DMP方法，通过选择性推理和退火解码来加速生成。

Result: 实验结果表明，DMP方法在不牺牲AUROC性能的情况下，实现了高达3倍的加速，并且可以扩展到对齐和推理任务。

Conclusion: 本文提出了一种新的解码记忆流水线（DMP），通过选择性推理和退火解码加速生成。该方法在不牺牲AUROC性能的情况下，实现了高达3倍的加速，并且可以扩展到对齐和推理任务。

Abstract: Large language models (LLMs) have demonstrated impressive performance in both
research and real-world applications, but they still struggle with
hallucination. Existing hallucination detection methods often perform poorly on
sentence-level generation or rely heavily on domain-specific knowledge. While
self-consistency approaches help address these limitations, they incur high
computational costs due to repeated generation. In this paper, we conduct the
first study on identifying redundancy in self-consistency methods, manifested
as shared prefix tokens across generations, and observe that non-exact-answer
tokens contribute minimally to the semantic content. Based on these insights,
we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation
through selective inference and annealed decoding. Being orthogonal to the
model, dataset, decoding strategy, and self-consistency baseline, our DMP
consistently improves the efficiency of multi-response generation and holds
promise for extension to alignment and reasoning tasks. Extensive experiments
show that our method achieves up to a 3x speedup without sacrificing AUROC
performance.

</details>


### [14] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
*Daria Kryvosheieva,Saba Sturua,Michael Günther,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: Jina-code-embeddings是一个新的代码嵌入模型套件，用于从自然语言查询中检索代码、执行技术问答以及识别跨编程语言的语义相似代码片段。


<details>
  <summary>Details</summary>
Motivation: 为了从自然语言查询中检索代码、执行技术问答以及在编程语言之间识别语义相似的代码片段。

Method: 使用自回归主干网络，通过最后的标记池化生成嵌入。

Result: 展示了最先进的性能，尽管模型规模相对较小。

Conclusion: 该方法验证了代码嵌入模型构建的有效性。

Abstract: jina-code-embeddings is a novel code embedding model suite designed to
retrieve code from natural language queries, perform technical
question-answering, and identify semantically similar code snippets across
programming languages. It makes innovative use of an autoregressive backbone
pre-trained on both text and code, generating embeddings via last-token
pooling. We outline the training recipe and demonstrate state-of-the-art
performance despite the relatively small size of the models, validating this
approach to code embedding model construction.

</details>


### [15] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
*João Guilherme Alves Santos,Giovana Kerche Bonás,Thales Sales Almeida*

Main category: cs.CL

TL;DR: 本文更新了BLUEX数据集，增加了2024-2025年的考试和图像字幕，以提高其在LLM预训练中的相关性，并评估了LLM利用视觉上下文的能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）能力的增长，需要更强大的评估方法，特别是在多语言和非英语环境中。

Method: 本文通过生成图像字幕来增加文本-only模型的可访问性，并评估了商业和开源LLM利用视觉上下文的能力。

Result: 图像字幕策略提高了文本-only模型的可访问性超过40%，产生了1,422个可用问题，比原始BLUEX增加了两倍以上。

Conclusion: 本文提出了更新后的BLUEX数据集，包括2024-2025年的考试和使用先进模型生成的图像字幕，以增强其在LLM预训练中的数据污染研究的相关性。

Abstract: With the growing capabilities of Large Language Models (LLMs), there is an
increasing need for robust evaluation methods, especially in multilingual and
non-English contexts. We present an updated version of the BLUEX dataset, now
including 2024-2025 exams and automatically generated image captions using
state-of-the-art models, enhancing its relevance for data contamination studies
in LLM pretraining. Captioning strategies increase accessibility to text-only
models by more than 40%, producing 1,422 usable questions, more than doubling
the number in the original BLUEX. We evaluated commercial and open-source LLMs
and their ability to leverage visual context through captions.

</details>


### [16] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [17] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
*Alexandre Kabbach*

Main category: cs.CL

TL;DR: 本文通过引入“正常”的概念重新审视图灵测试，认为图灵测试关注的是正常/平均的人类智能，而不是非凡的智能。文章指出，大型语言模型如ChatGPT不太可能通过图灵测试，因为它们追求的是非凡智能。同时，文章还探讨了图灵测试是否有助于理解人类认知的问题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在重新审视图灵测试，通过引入“正常”的概念来探讨其在理解人类认知中的作用。作者认为，当前的大型语言模型（如ChatGPT）并不符合图灵测试的要求，因为它们追求的是非凡的人类智能，而不是正常/平均的人类智能。

Method: 本文通过重新审视图灵测试的概念，提出将“正常”作为理解图灵测试的一种方式。文章认为，图灵测试关注的是正常/平均的人类智能，而不是非凡的人类智能。此外，图灵测试是一种统计测试，由一个完整的陪审团进行判断，而不是由单一的“平均”评判者进行判断。

Result: 本文认为，图灵测试本质上是测试正常智能，而大型语言模型如ChatGPT由于追求非凡智能，不太可能通过图灵测试。此外，文章指出，图灵测试是否有助于理解人类认知，取决于人类心智是否可以还原为正常/平均的心智。

Conclusion: 本文得出两个结论。首先，它认为大型语言模型如ChatGPT不太可能通过图灵测试，因为这些模型恰恰针对的是非凡而非正常/平均的人类智能。因此，它们构成了所谓的人工聪明，而不是真正的“人工智能”。其次，它认为图灵测试是否能对理解人类认知做出贡献的核心问题是，人类心智是否真的可以还原为正常/平均的心智——这个问题远远超出了图灵测试本身，并质疑了它所属的正常主义范式的概念基础。

Abstract: This paper proposes to revisit the Turing test through the concept of
normality. Its core argument is that the statistical interpretation of the
normal--understood as the average both in the normative and mathematical sense
of the term--proves useful for understanding the Turing test in at least two
ways. First, in the sense that the Turing test targets normal/average rather
than exceptional human intelligence, so that successfully passing the test
requires building machines that "make mistakes" and display imperfect behavior
just like normal/average humans. Second, in the sense that the Turing test is a
statistical test where judgments of intelligence are never carried out by a
single "average" judge (understood as non-expert) but always by a full jury. As
such, the notion of "average human interrogator" that Turing talks about in his
original paper should be understood primarily as referring to a mathematical
abstraction made of the normalized aggregate of individual judgments of
multiple judges. In short, this paper argues that the Turing test is a test of
normal intelligence as assessed by a normal judge characterizing the average
judgment of a pool of human interrogators. Its conclusions are twofold. First,
it argues that large language models such as ChatGPT are unlikely to pass the
Turing test as those models precisely target exceptional rather than
normal/average human intelligence. As such, they constitute models of what it
proposes to call artificial smartness rather than artificial intelligence per
se. Second, it argues that the core question of whether the Turing test can
contribute anything to the understanding of human cognition is that of whether
the human mind is really reducible to the normal/average mind--a question which
largely extends beyond the Turing test itself and questions the conceptual
underpinnings of the normalist paradigm it belongs to.

</details>


### [18] [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TL;DR: 本文研究了自动文本摘要评估中的可重复性问题，发现文献中的性能与实际实验结果存在差异，并提出一个开源框架来促进公平比较。研究指出，高性能指标通常计算复杂且不稳定，同时强调了依赖大型语言模型进行评估的局限性，并呼吁更稳健的评估协议。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨自动文本摘要评估中的可重复性挑战，并揭示文献中报告的性能与实验设置中观察到的性能之间的显著差异。

Method: 本文引入了一个统一的开源框架，应用于SummEval数据集，旨在支持评估指标的公平和透明比较。

Result: 结果揭示了结构上的权衡：与人类判断最一致的指标往往计算密集且跨运行不稳定。此外，研究还强调了依赖LLM进行评估的关键问题，包括其随机性、技术依赖性和有限的可重复性。

Conclusion: 本文倡导更稳健的评估协议，包括详尽的文档记录和方法标准化，以确保自动摘要评估的可靠性。

Abstract: This paper investigates reproducibility challenges in automatic text
summarization evaluation. Based on experiments conducted across six
representative metrics ranging from classical approaches like ROUGE to recent
LLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies
between reported performances in the literature and those observed in our
experimental setting. We introduce a unified, open-source framework, applied to
the SummEval dataset and designed to support fair and transparent comparison of
evaluation metrics. Our results reveal a structural trade-off: metrics with the
highest alignment with human judgments tend to be computationally intensive and
less stable across runs. Beyond comparative analysis, this study highlights key
concerns about relying on LLMs for evaluation, stressing their randomness,
technical dependencies, and limited reproducibility. We advocate for more
robust evaluation protocols including exhaustive documentation and
methodological standardization to ensure greater reliability in automatic
summarization assessment.

</details>


### [19] [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)
*Nils Dycke,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在学术同行评审中的应用，发现它们在检测研究逻辑缺陷方面存在不足，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在加速和辅助学术同行评审方面具有巨大潜力，并且越来越多地被用作完全自动的评审生成器（ARGs）。然而，潜在的偏见和系统性错误可能对科学完整性构成重大风险；了解最先进的ARGs的具体能力和限制是至关重要的。

Method: 我们提出了一种完全自动化的反事实评估框架，用于在受控条件下隔离和测试检测故障研究逻辑的能力。

Result: 我们测试了一系列ARG方法，发现与预期相反，研究逻辑中的缺陷对其输出评审没有显著影响。

Conclusion: 我们的研究发现，当前的ARG在检测研究逻辑缺陷方面存在不足，因此需要未来的研究工作进行改进。我们提出了三个可操作的建议，并公开了我们的反事实数据集和评估框架。

Abstract: Large Language Models (LLMs) have great potential to accelerate and support
scholarly peer review and are increasingly used as fully automatic review
generators (ARGs). However, potential biases and systematic errors may pose
significant risks to scientific integrity; understanding the specific
capabilities and limitations of state-of-the-art ARGs is essential. We focus on
a core reviewing skill that underpins high-quality peer review: detecting
faulty research logic. This involves evaluating the internal consistency
between a paper's results, interpretations, and claims. We present a fully
automated counterfactual evaluation framework that isolates and tests this
skill under controlled conditions. Testing a range of ARG approaches, we find
that, contrary to expectation, flaws in research logic have no significant
effect on their output reviews. Based on our findings, we derive three
actionable recommendations for future work and release our counterfactual
dataset and evaluation framework publicly.

</details>


### [20] [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.21430)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Cheng-Yi Li,Wei-Chieh Fang,Hsin-Yu Wu,Haiqin Zhong,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 本文介绍了 Med-RewardBench，这是第一个专门用于评估医疗奖励模型和法官的基准，通过多模态数据集和严格的评估流程，揭示了现有模型在与专家判断对齐方面的挑战，并展示了通过微调获得的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要关注一般多模态大语言模型的能力或将其作为求解器进行评估，而忽略了诊断准确性和临床相关性等关键评估维度。因此需要一个专门针对医疗场景的基准来评估医疗奖励模型和法官。

Method: 引入 Med-RewardBench 基准，构建了一个涵盖13个器官系统和8个临床部门的多模态数据集，并通过一个严格的三步过程确保评估数据的高质量。

Result: 评估了32个最先进的多模态大语言模型，揭示了输出与专家判断对齐的挑战，并开发了通过微调获得显著性能提升的基线模型。

Conclusion: Med-RewardBench 是一个专门用于评估医疗奖励模型和法官的基准，通过评估32个最先进的多模态大语言模型，揭示了输出与专家判断对齐的挑战，并展示了通过微调获得的显著性能提升。

Abstract: Multimodal large language models (MLLMs) hold significant potential in
medical applications, including disease diagnosis and clinical decision-making.
However, these tasks require highly accurate, context-sensitive, and
professionally aligned responses, making reliable reward models and judges
critical. Despite their importance, medical reward models (MRMs) and judges
remain underexplored, with no dedicated benchmarks addressing clinical
requirements. Existing benchmarks focus on general MLLM capabilities or
evaluate models as solvers, neglecting essential evaluation dimensions like
diagnostic accuracy and clinical relevance. To address this, we introduce
Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and
judges in medical scenarios. Med-RewardBench features a multimodal dataset
spanning 13 organ systems and 8 clinical departments, with 1,026
expert-annotated cases. A rigorous three-step process ensures high-quality
evaluation data across six clinically critical dimensions. We evaluate 32
state-of-the-art MLLMs, including open-source, proprietary, and
medical-specific models, revealing substantial challenges in aligning outputs
with expert judgment. Additionally, we develop baseline models that demonstrate
substantial performance improvements through fine-tuning.

</details>


### [21] [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)
*Yunhao Zhang,Shaonan Wang,Nan Lin,Xinyi Dong,Chong Li,Chengqing Zong*

Main category: cs.CL

TL;DR: 本文提出一种新方法，用于研究语义维度的子维度，并验证了其神经科学合理性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于预定义的语义维度，仅提供宽泛的表示，忽略了更精细的概念区分。

Method: 本文引入了一种解耦连续语义表示模型（DCSRM），将大语言模型中的词嵌入分解为多个子嵌入，每个子嵌入编码特定的语义信息。

Result: 本文发现了更细粒度的可解释语义子维度，并揭示了语义维度根据不同的原则进行结构化，其中极性是一个关键因素。

Conclusion: 本文提出了一个新颖的框架来研究粗粒度语义维度下的子维度，并通过实验验证了这些子维度在神经科学上的合理性。

Abstract: Understanding the core dimensions of conceptual semantics is fundamental to
uncovering how meaning is organized in language and the brain. Existing
approaches often rely on predefined semantic dimensions that offer only broad
representations, overlooking finer conceptual distinctions. This paper proposes
a novel framework to investigate the subdimensions underlying coarse-grained
semantic dimensions. Specifically, we introduce a Disentangled Continuous
Semantic Representation Model (DCSRM) that decomposes word embeddings from
large language models into multiple sub-embeddings, each encoding specific
semantic information. Using these sub-embeddings, we identify a set of
interpretable semantic subdimensions. To assess their neural plausibility, we
apply voxel-wise encoding models to map these subdimensions to brain
activation. Our work offers more fine-grained interpretable semantic
subdimensions of conceptual meaning. Further analyses reveal that semantic
dimensions are structured according to distinct principles, with polarity
emerging as a key factor driving their decomposition into subdimensions. The
neural correlates of the identified subdimensions support their cognitive and
neuroscientific plausibility.

</details>


### [22] [Beyond the Surface: Probing the Ideological Depth of Large Language Models](https://arxiv.org/abs/2508.21448)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have demonstrated pronounced ideological
leanings, yet the stability and depth of these positions remain poorly
understood. Surface-level responses can often be manipulated through simple
prompt engineering, calling into question whether they reflect a coherent
underlying ideology. This paper investigates the concept of "ideological depth"
in LLMs, defined as the robustness and complexity of their internal political
representations. We employ a dual approach: first, we measure the
"steerability" of two well-known open-source LLMs using instruction prompting
and activation steering. We find that while some models can easily switch
between liberal and conservative viewpoints, others exhibit resistance or an
increased rate of refusal, suggesting a more entrenched ideological structure.
Second, we probe the internal mechanisms of these models using Sparse
Autoencoders (SAEs). Preliminary analysis reveals that models with lower
steerability possess more distinct and abstract ideological features. Our
evaluations reveal that one model can contain 7.3x more political features than
another model of similar size. This allows targeted ablation of a core
political feature in an ideologically "deep" model, leading to consistent,
logical shifts in its reasoning across related topics, whereas the same
intervention in a "shallow" model results in an increase in refusal outputs.
Our findings suggest that ideological depth is a quantifiable property of LLMs
and that steerability serves as a valuable window into their latent political
architecture.

</details>


### [23] [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)
*Xiaolong Wei,Bo Lu,Xingyu Zhang,Zhejun Zhao,Dongdong Shen,Long Xia,Dawei Yin*

Main category: cs.CL

TL;DR: 本研究探索了两种AI驱动的奖励策略，用于在RLAIF框架下提升小语言模型的创造性写作能力。结果表明，基于原则引导的LLM-as-a-Judge方法在生成质量和训练效率方面优于其他方法，为创造性的SLM提供了一条更具可扩展性和有效性的路径。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在创造性写作方面表现出色，但它们的计算需求较高，限制了广泛应用。增强小型语言模型（SLMs）是一个有前景的替代方案，但现有的方法如监督微调（SFT）在新颖性方面表现不佳，而基于人类反馈的强化学习（RLHF）成本较高。因此，本研究旨在探索一种更高效且有效的策略，以激发SLM的创造性写作能力。

Method: 该研究采用了两种不同的AI驱动的奖励策略：第一种策略使用了一个在高质量偏好数据上训练的RM，这些数据由一个针对创意任务设计的多代理拒绝采样框架收集；第二种策略则利用了基于原则引导的LLM-as-a-Judge，其奖励函数通过带有反思机制的对抗训练方案进行优化，以直接提供奖励信号。

Result: 实验结果表明，这两种策略都能显著提升基线模型的创造性输出，其中基于原则引导的LLM-as-a-Judge方法在生成质量上表现更优。此外，该方法在训练效率和减少对人工标注数据的依赖方面也具有明显优势。

Conclusion: 该研究展示了通过RLAIF框架中的两种AI驱动的奖励策略，可以显著提升小语言模型（SLM）的创造性写作能力。特别是基于原则引导的LLM-as-a-Judge方法在生成质量、训练效率和减少对人工标注数据的依赖方面表现出明显优势，为创造性的SLM提供了一条更具可扩展性和有效性的路径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable creative writing
capabilities, yet their substantial computational demands hinder widespread
use. Enhancing Small Language Models (SLMs) offers a promising alternative, but
current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and
Reinforcement Learning from Human Feedback (RLHF) is costly. This paper
explores two distinct AI-driven reward strategies within a Reinforcement
Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a
7B-parameter SLM, specifically for generating Chinese greetings. The first
strategy employs a RM trained on high-quality preference data curated by a
novel multi-agent rejection sampling framework designed for creative tasks. The
second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose
reward function is optimized via an adversarial training scheme with a
reflection mechanism, to directly provide reward signals. Comprehensive
experiments reveal that while both approaches significantly enhance creative
output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields
superior generation quality. Furthermore, it offers notable advantages in
training efficiency and reduced dependency on human-annotated data, presenting
a more scalable and effective path towards creative SLMs. Our automated
evaluation methods also exhibit strong alignment with human judgments. Our code
and data are publicly available at
https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.

</details>


### [24] [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)
*Sara B. Coutinho,Rafael M. O. Cruz,Francimaria R. S. Nascimento,George D. C. Cavalcanti*

Main category: cs.CL

TL;DR: 本文提出了一种新的自动分类器选择方法，该方法优先考虑多样性，并通过性能进行扩展。实验结果表明，该方法在六个数据集中的两个数据集上实现了最高的准确率。


<details>
  <summary>Details</summary>
Motivation: 心理偏见使个体特别容易相信和传播社交媒体上的假新闻，导致公共卫生和政治等领域的严重后果。机器学习事实核查系统已被广泛研究以缓解这个问题。其中，集成方法在结合多个分类器以提高鲁棒性方面特别有效。然而，它们的性能高度依赖于组成分类器的多样性——选择真正多样的模型仍然是一个关键挑战，尤其是在模型倾向于学习冗余模式时。

Method: 我们提出了一种新的自动分类器选择方法，该方法优先考虑多样性，并通过性能进行扩展。该方法首先计算分类器之间的成对多样性，并应用层次聚类将它们组织成不同粒度的组。然后，HierarchySelect探索这些层次以每层选择一个分类器池，每个池代表一种独特的内部池多样性。从这些中识别并选择最多样化的池用于集成构建。

Result: 我们在六个不同应用领域且具有不同类别数量的数据集上使用40个异构分类器进行了实验。我们的方法与Elbow启发式方法和最先进的基线进行了比较。结果表明，我们的方法在六个数据集中的两个数据集上实现了最高的准确率。

Conclusion: 我们的方法在六个数据集中有两个数据集上实现了最高的准确率，证明了其有效性。

Abstract: Psychological biases, such as confirmation bias, make individuals
particularly vulnerable to believing and spreading fake news on social media,
leading to significant consequences in domains such as public health and
politics. Machine learning-based fact-checking systems have been widely studied
to mitigate this problem. Among them, ensemble methods are particularly
effective in combining multiple classifiers to improve robustness. However,
their performance heavily depends on the diversity of the constituent
classifiers-selecting genuinely diverse models remains a key challenge,
especially when models tend to learn redundant patterns. In this work, we
propose a novel automatic classifier selection approach that prioritizes
diversity, also extended by performance. The method first computes pairwise
diversity between classifiers and applies hierarchical clustering to organize
them into groups at different levels of granularity. A HierarchySelect then
explores these hierarchical levels to select one pool of classifiers per level,
each representing a distinct intra-pool diversity. The most diverse pool is
identified and selected for ensemble construction from these. The selection
process incorporates an evaluation metric reflecting each classifiers's
performance to ensure the ensemble also generalises well. We conduct
experiments with 40 heterogeneous classifiers across six datasets from
different application domains and with varying numbers of classes. Our method
is compared against the Elbow heuristic and state-of-the-art baselines. Results
show that our approach achieves the highest accuracy on two of six datasets.
The implementation details are available on the project's repository:
https://github.com/SaraBCoutinho/HSFN .

</details>


### [25] [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)
*Aishwarya Mirashi,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: 本文介绍了MahaSTS数据集和MahaSBERT-STS-v2模型，用于马拉地语的句子文本相似度任务。数据集包含16,860个句子对，并带有0-5范围内的连续评分。通过微调MahaSBERT模型并与其他模型进行比较，验证了其有效性。结果表明，人工注释、针对性微调和结构化监督在低资源环境下具有重要影响。


<details>
  <summary>Details</summary>
Motivation: 为了在低资源环境下提高句子相似度任务的效果，需要一个高质量的人工注释数据集以及针对该任务优化的模型。

Method: MahaSTS是一个人工注释的马拉地语句子文本相似度数据集，同时提供了针对回归相似度评分优化的MahaSBERT-STS-v2模型。数据集包含16,860个马拉地语句子对，并带有0-5范围内的连续相似度评分。通过在该数据集上微调MahaSBERT模型，并与其他模型如MahaBERT、MuRIL、IndicBERT和IndicSBERT进行比较，验证了其有效性。

Result: 实验结果表明，MahaSTS能够有效训练马拉地语的句子相似度任务，突显了人工注释、针对性微调和结构化监督的重要性。

Conclusion: MahaSTS和MahaSBERT-STS-v2在低资源环境下展示了人类注释的标注、针对性微调和结构化监督的重要性。数据集和模型已公开共享。

Abstract: We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)
dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT
model optimized for regression-based similarity scoring. The MahaSTS dataset
consists of 16,860 Marathi sentence pairs labeled with continuous similarity
scores in the range of 0-5. To ensure balanced supervision, the dataset is
uniformly distributed across six score-based buckets spanning the full 0-5
range, thus reducing label bias and enhancing model stability. We fine-tune the
MahaSBERT model on this dataset and benchmark its performance against other
alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments
demonstrate that MahaSTS enables effective training for sentence similarity
tasks in Marathi, highlighting the impact of human-curated annotations,
targeted fine-tuning, and structured supervision in low-resource settings. The
dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>


### [26] [A Survey on Current Trends and Recent Advances in Text Anonymization](https://arxiv.org/abs/2508.21587)
*Tobias Deußer,Lorenz Sparrenberg,Armin Berger,Max Hahnbück,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文综述了文本匿名化技术的最新进展，包括基础方法、大型语言模型的影响、特定领域的挑战和解决方案、先进的方法、评估框架、指标、基准和工具包，并指出了未来的 research 方向。


<details>
  <summary>Details</summary>
Motivation: 随着包含敏感个人信息的文本数据在各个领域的激增，需要强大的匿名化技术来保护隐私并遵守法规，同时保持数据对于各种关键下游任务的可用性。

Method: 本文通过讨论基础方法（主要集中在命名实体识别上）， examining the transformative impact of Large Language Models, 探索特定领域的挑战和定制解决方案，研究先进的方法结合正式的隐私模型和风险感知框架，以及处理作者身份匿名化的专门子领域，还回顾了评估框架、全面指标、基准和实用工具包。

Result: 本文提供了对当前文本匿名化技术趋势和最新进展的全面概述，涵盖了基础方法、大型语言模型的影响、特定领域的挑战和解决方案、先进的方法、评估框架、指标、基准和工具包，并指出了未来的研究方向。

Conclusion: 本文综述了当前文本匿名化技术的最新进展，总结了现有知识，指出了新兴趋势和持续挑战，包括不断演变的隐私-效用权衡、解决准标识符的需要以及LLM能力的影响，并旨在为该领域的学术界和实践者提供未来研究方向的指导。

Abstract: The proliferation of textual data containing sensitive personal information
across various domains requires robust anonymization techniques to protect
privacy and comply with regulations, while preserving data usability for
diverse and crucial downstream tasks. This survey provides a comprehensive
overview of current trends and recent advances in text anonymization
techniques. We begin by discussing foundational approaches, primarily centered
on Named Entity Recognition, before examining the transformative impact of
Large Language Models, detailing their dual role as sophisticated anonymizers
and potent de-anonymization threats. The survey further explores
domain-specific challenges and tailored solutions in critical sectors such as
healthcare, law, finance, and education. We investigate advanced methodologies
incorporating formal privacy models and risk-aware frameworks, and address the
specialized subfield of authorship anonymization. Additionally, we review
evaluation frameworks, comprehensive metrics, benchmarks, and practical
toolkits for real-world deployment of anonymization solutions. This review
consolidates current knowledge, identifies emerging trends and persistent
challenges, including the evolving privacy-utility trade-off, the need to
address quasi-identifiers, and the implications of LLM capabilities, and aims
to guide future research directions for both academics and practitioners in
this field.

</details>


### [27] [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
*Zinan Tang,Xin Gao,Qizhi Pei,Zhuoshi Pan,Mengzhang Cai,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Middo的自我进化模型感知动态数据优化框架，通过模型感知的数据选择和上下文保留的数据精炼来提高大规模语言模型的训练数据质量。实验表明，该方法在多个基准测试中显著提升了种子数据的质量，并平均提高了7.15%的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据选择和数据合成方法在静态数据集管理中存在局限性，无法适应模型能力的演变。因此，需要一种能够动态优化数据和模型的方法。

Method: Middo框架采用了一个闭环优化系统，包括：(1) 一个自我参照的诊断模块，通过三轴模型信号（损失模式、嵌入聚类动力学和自我对齐分数）主动识别次优样本；(2) 一个自适应优化引擎，将次优样本转化为教学有价值的训练点，同时保持语义完整性；(3) 通过动态学习原则，优化过程随着模型能力不断演进。

Result: 实验结果表明，Middo框架在多个基准测试中显著提升了种子数据的质量，并平均提高了7.15%的准确性，同时保持了原始数据集规模。

Conclusion: 本文提出了一种名为Middo的自我进化模型感知动态数据优化框架，通过模型感知的数据选择和上下文保留的数据精炼来提高大规模语言模型的训练数据质量。实验表明，该方法在多个基准测试中显著提升了种子数据的质量，并平均提高了7.15%的准确性。

Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely
on high-quality training data. While data selection and data synthesis are two
common strategies to improve data quality, existing approaches often face
limitations in static dataset curation that fail to adapt to evolving model
capabilities. In this paper, we introduce Middo, a self-evolving Model-informed
dynamic data optimization framework that uses model-aware data selection and
context-preserving data refinement. Unlike conventional one-off
filtering/synthesis methods, our framework establishes a closed-loop
optimization system: (1) A self-referential diagnostic module proactively
identifies suboptimal samples through tri-axial model signals - loss patterns
(complexity), embedding cluster dynamics (diversity), and self-alignment scores
(quality); (2) An adaptive optimization engine then transforms suboptimal
samples into pedagogically valuable training points while preserving semantic
integrity; (3) This optimization process continuously evolves with model
capability through dynamic learning principles. Experiments on multiple
benchmarks demonstrate that our \method consistently enhances the quality of
seed data and boosts LLM's performance with improving accuracy by 7.15% on
average while maintaining the original dataset scale. This work establishes a
new paradigm for sustainable LLM training through dynamic human-AI co-evolution
of data and models. Our datasets, models, and code are coming soon.

</details>


### [28] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
*Sarfaroz Yunusov,Kaige Chen,Kazi Nishat Anwar,Ali Emami*

Main category: cs.CL

TL;DR: 研究发现不同人格类型的用户对LLM有系统性的偏好，这表明基于人格的分析可以揭示传统评估所忽略的LLM差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨不同人格特质的用户是否系统性地偏好某些LLM。

Method: 研究包括32名参与者，他们分布在四个Keirsey人格类型中，评估他们与GPT-4和Claude 3.5在四个协作任务中的互动。

Result: 结果表明，理性主义者更喜欢GPT-4，而理想主义者更喜欢Claude 3.5，其他人格类型则表现出任务依赖性偏好。

Conclusion: 研究发现，不同人格类型的用户对LLM有系统性的偏好，这表明基于人格的分析可以揭示传统评估所忽略的LLM差异。

Abstract: As Large Language Models (LLMs) increasingly integrate into everyday
workflows, where users shape outcomes through multi-turn collaboration, a
critical question emerges: do users with different personality traits
systematically prefer certain LLMs over others? We conducted a study with 32
participants evenly distributed across four Keirsey personality types,
evaluating their interactions with GPT-4 and Claude 3.5 across four
collaborative tasks: data analysis, creative writing, information retrieval,
and writing assistance. Results revealed significant personality-driven
preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented
tasks, while idealists favored Claude 3.5, especially for creative and
analytical tasks. Other personality types showed task-dependent preferences.
Sentiment analysis of qualitative feedback confirmed these patterns. Notably,
aggregate helpfulness ratings were similar across models, showing how
personality-based analysis reveals LLM differences that traditional evaluations
miss.

</details>


### [29] [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)
*Peng Yu,En Xu,Bin Chen,Haibiao Chen,Yinfei Xu*

Main category: cs.CL

TL;DR: 我们提出了QZhou-Embedding，一个具有卓越文本表示能力的通用上下文文本嵌入模型。通过设计统一的多任务框架和数据合成管道，我们的模型在多个基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 为了提升检索模型的性能，我们需要更高质量和多样化的数据，并探索如何利用大语言模型的生成能力来优化数据质量。

Method: 我们基于Qwen2.5-7B-Instruct基础模型设计了一个统一的多任务框架，包括专门的数据转换和训练策略。我们开发了一个数据合成管道，利用LLM API，结合改写、增强和困难负例生成等技术来提高训练集的语义丰富性和样本难度。此外，我们采用两阶段训练策略，包括初始的检索聚焦预训练和全任务微调，使嵌入模型能够在强大的检索性能基础上扩展其能力。

Result: 我们的模型在MTEB和CMTEB基准测试中取得了最先进的结果，在两个排行榜上均排名第一（2025年8月27日），并在重新排序、聚类等任务中同时实现了最先进的性能。

Conclusion: 我们的研究证明了高质量、多样化的数据对于提升检索模型性能至关重要，并且利用大语言模型的生成能力可以进一步优化数据质量以实现嵌入模型的突破。

Abstract: We present QZhou-Embedding, a general-purpose contextual text embedding model
with exceptional text representation capabilities. Built upon the
Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task
framework comprising specialized data transformation and training strategies.
The data transformation scheme enables the incorporation of more diverse
textual training datasets, while the task-specific training strategies enhance
model learning efficiency. We developed a data synthesis pipeline leveraging
LLM API, incorporating techniques such as paraphrasing, augmentation, and hard
negative example generation to improve the semantic richness and sample
difficulty of the training set. Additionally, we employ a two-stage training
strategy, comprising initial retrieval-focused pretraining followed by
full-task fine-tuning, enabling the embedding model to extend its capabilities
based on robust retrieval performance. Our model achieves state-of-the-art
results on the MTEB and CMTEB benchmarks, ranking first on both leaderboards
(August 27 2025), and simultaneously achieves state-of-the-art performance on
tasks including reranking, clustering, etc. Our findings demonstrate that
higher-quality, more diverse data is crucial for advancing retrieval model
performance, and that leveraging LLMs generative capabilities can further
optimize data quality for embedding model breakthroughs. Our model weights are
released on HuggingFace under Apache 2.0 license. For reproducibility, we
provide evaluation code and instructions on GitHub.

</details>


### [30] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了Misviz和Misviz-synth数据集，用于自动检测误导性可视化，并展示了该任务的挑战性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大型、多样且公开可用的数据集，AI模型的训练和评估受到限制。自动检测误导性可视化并识别其违反的设计规则可以帮助保护读者并减少错误信息的传播。

Method: 本文引入了Misviz数据集和Misviz-synth合成数据集，并使用最先进的MLLMs、基于规则的系统和微调分类器对这两个数据集进行了全面评估。

Result: 本文的结果表明，该任务仍然具有高度挑战性。

Conclusion: 本文介绍了Misviz和Misviz-synth数据集，以支持自动检测误导性可视化的工作。然而，任务仍然具有高度挑战性。

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [31] [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)
*Yao Wang,Di Liang,Minlong Peng*

Main category: cs.CL

TL;DR: 本文提出了一种新的Core Parameter Isolation Fine-Tuning (CPI-FT)框架，通过识别和隔离核心参数区域，以及利用参数融合技术来减轻任务间的干扰和遗忘，从而提升多任务微调的效果。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）是适应大型语言模型（LLMs）进行下游任务的关键方法；然而，性能常常受到“跷跷板现象”的影响，即无差别参数更新会在某些任务上取得进展，而以其他任务为代价。

Method: 我们提出了一个新颖的Core Parameter Isolation Fine-Tuning (CPI-FT)框架，首先独立微调LLM以识别其核心参数区域，然后根据区域重叠对任务进行分组，形成联合建模的聚类。我们进一步引入了一种参数融合技术：对于每个任务，将其单独微调模型的核心参数直接移植到统一的主干中，而不同任务的非核心参数则通过Spherical Linear Interpolation (SLERP)平滑集成，从而减轻破坏性干扰。最后，使用混合任务数据进行轻量级、流水线式的SFT训练阶段，同时冻结先前任务的核心区域以防止灾难性遗忘。

Result: 在多个公共基准上的广泛实验表明，我们的方法显著减轻了任务干扰和遗忘，始终优于普通的多任务和多阶段微调基线。

Conclusion: 我们的方法显著减轻了任务干扰和遗忘，始终优于普通的多任务和多阶段微调基线。

Abstract: Supervised fine-tuning (SFT) is a pivotal approach to adapting large language
models (LLMs) for downstream tasks; however, performance often suffers from the
``seesaw phenomenon'', where indiscriminate parameter updates yield progress on
certain tasks at the expense of others. To address this challenge, we propose a
novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.
Specifically, we first independently fine-tune the LLM on each task to identify
its core parameter regions by quantifying parameter update magnitudes. Tasks
with similar core regions are then grouped based on region overlap, forming
clusters for joint modeling. We further introduce a parameter fusion technique:
for each task, core parameters from its individually fine-tuned model are
directly transplanted into a unified backbone, while non-core parameters from
different tasks are smoothly integrated via Spherical Linear Interpolation
(SLERP), mitigating destructive interference. A lightweight, pipelined SFT
training phase using mixed-task data is subsequently employed, while freezing
core regions from prior tasks to prevent catastrophic forgetting. Extensive
experiments on multiple public benchmarks demonstrate that our approach
significantly alleviates task interference and forgetting, consistently
outperforming vanilla multi-task and multi-stage fine-tuning baselines.

</details>


### [32] [Reasoning-Intensive Regression](https://arxiv.org/abs/2508.21762)
*Diane Tchuindjo,Omar Khattab*

Main category: cs.CL

TL;DR: This paper explores the application of large language models (LLMs) to reasoning-intensive regression (RiR) tasks, which involve deducing subtle numerical properties from text. The authors establish an initial benchmark by casting three realistic problems as RiR tasks and test their hypothesis that prompting frozen LLMs and fine-tuning Transformer encoders via gradient descent will struggle in RiR. They propose MENTAT, a simple and lightweight method combining batch-reflective prompt optimization with neural ensemble learning, achieving up to 65% improvement over baselines, though there is still room for future advancements in RiR.


<details>
  <summary>Details</summary>
Motivation: AI researchers and practitioners increasingly apply large language models (LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing subtle numerical properties from text.

Method: We propose MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning.

Result: We cast three realistic problems as RiR tasks to establish an initial benchmark, and use that to test our hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR.

Conclusion: MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR.

Abstract: AI researchers and practitioners increasingly apply large language models
(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing
subtle numerical properties from text. Unlike standard language regression
tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc
problems like rubric-based scoring or domain-specific retrieval, where much
deeper analysis of text is required while only limited task-specific training
data and computation are available. We cast three realistic problems as RiR
tasks to establish an initial benchmark, and use that to test our hypothesis
that prompting frozen LLMs and finetuning Transformer encoders via gradient
descent will both often struggle in RiR. We then propose MENTAT, a simple and
lightweight method that combines batch-reflective prompt optimization with
neural ensemble learning. MENTAT achieves up to 65% improvement over both
baselines, though substantial room remains for future advances in RiR.

</details>


### [33] [PiCSAR: Probabilistic Confidence Selection And Ranking](https://arxiv.org/abs/2508.21787)
*Joshua Ong Jun Leang,Zheng Zhao,Aryo Pradipta Gema,Sohee Yang,Wai-Chung Kwan,Xuanli He,Wenda Li,Pasquale Minervini,Eleonora Giunchiglia,Shay B. Cohen*

Main category: cs.CL

TL;DR: PiCSAR is a training-free method that improves the accuracy of large language models by scoring candidate generations using the joint log-likelihood of reasoning and final answer.


<details>
  <summary>Details</summary>
Motivation: The key challenge for reasoning tasks is designing a scoring function that can identify correct reasoning chains without access to ground-truth answers.

Method: Probabilistic Confidence Selection And Ranking (PiCSAR), which scores each candidate generation using the joint log-likelihood of the reasoning and final answer.

Result: PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500, +9.81 on AIME2025).

Conclusion: PiCSAR achieves substantial gains across diverse benchmarks and outperforms baselines with at least 2x fewer samples.

Abstract: Best-of-n sampling improves the accuracy of large language models (LLMs) and
large reasoning models (LRMs) by generating multiple candidate solutions and
selecting the one with the highest reward. The key challenge for reasoning
tasks is designing a scoring function that can identify correct reasoning
chains without access to ground-truth answers. We propose Probabilistic
Confidence Selection And Ranking (PiCSAR): a simple, training-free method that
scores each candidate generation using the joint log-likelihood of the
reasoning and final answer. The joint log-likelihood of the reasoning and final
answer naturally decomposes into reasoning confidence and answer confidence.
PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,
+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in
16 out of 20 comparisons. Our analysis reveals that correct reasoning chains
exhibit significantly higher reasoning and answer confidence, justifying the
effectiveness of PiCSAR.

</details>


### [34] [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)
*Inés Altemir Marinas,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 本文提出了一种基于ElasticSearch的框架，用于分析LLM训练数据集，以提高数据质量和安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）依赖于像Common Crawl这样的网络规模数据集，但网络爬取的无差别性质引发了数据质量、安全性和伦理问题。由于计算限制，之前对有害内容的研究仅限于小样本。

Method: 我们提出了一个基于ElasticSearch的管道框架，用于索引和分析LLM训练数据集。

Result: 我们应用该框架到SwissAI的FineWeb-2语料库（1.5TB，四种语言），实现了快速查询性能——大多数搜索在毫秒级，全部在2秒内完成。

Conclusion: 我们的工作展示了实时数据集分析，为更安全、更具问责性的AI系统提供了实用工具。

Abstract: Large language models (LLMs) rely heavily on web-scale datasets like Common
Crawl, which provides over 80\% of training data for some modern models.
However, the indiscriminate nature of web crawling raises challenges in data
quality, safety, and ethics. Despite the critical importance of training data
quality, prior research on harmful content has been limited to small samples
due to computational constraints. This project presents a framework for
indexing and analyzing LLM training datasets using an ElasticSearch-based
pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),
achieving fast query performance--most searches in milliseconds, all under 2
seconds. Our work demonstrates real-time dataset analysis, offering practical
tools for safer, more accountable AI systems.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [35] [From Canonical to Complex: Benchmarking LLM Capabilities in Undergraduate Thermodynamics](https://arxiv.org/abs/2508.21452)
*Anna Geißler,Luca-Sophie Bien,Friedrich Schöppler,Tobias Hertel*

Main category: physics.ed-ph

TL;DR: 本文评估了大型语言模型在热力学领域的辅导能力，发现它们在某些任务中表现不佳，表明目前还不适合用于无监督教学。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在科学教育中的辅导能力，特别是它们在需要一致、基于原理的推理任务中的表现。

Method: 本文提出了UTQA，一个包含50个问题的本科热力学问答基准，涵盖了理想气体过程、可逆性和图表解释。

Result: 没有2025年时期的模型超过95%的能力阈值，最好的大型语言模型达到了82%的准确率，文本问题的表现优于图像推理任务。

Conclusion: 当前的大型语言模型尚未适合在该领域进行无监督辅导。

Abstract: Large language models (LLMs) are increasingly considered as tutoring aids in
science education. Yet their readiness for unsupervised use in undergraduate
instruction remains uncertain, as reliable teaching requires more than fluent
recall: it demands consistent, principle-grounded reasoning. Thermodynamics,
with its compact laws and subtle distinctions between state and path functions,
reversibility, and entropy, provides an ideal testbed for evaluating such
capabilities. Here we present UTQA, a 50-item undergraduate thermodynamics
question answering benchmark, covering ideal-gas processes, reversibility, and
diagram interpretation. No leading 2025-era model exceeded our 95\% competence
threshold: the best LLMs achieved 82\% accuracy, with text-only items
performing better than image reasoning tasks, which often fell to chance
levels. Prompt phrasing and syntactic complexity showed modest to little
correlation with performance. The gap concentrates in finite-rate/irreversible
scenarios and in binding visual features to thermodynamic meaning, indicating
that current LLMs are not yet suitable for unsupervised tutoring in this
domain.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [36] [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
*Shashank Vempati,Nishit Anand,Gaurav Talebailkar,Arpan Garai,Chetan Arora*

Main category: cs.CV

TL;DR: 本文提出了一种从单词级OCR到行级OCR的进展，通过直接输入整行文本到模型中，以序列形式输出完整单词。这种方法避免了单词检测中的错误，并提供了更大的句子上下文。实验表明，该技术不仅提高了准确性，还提高了效率。此外，本文还构建了一个包含251个英文页面图像的行级标注数据集。


<details>
  <summary>Details</summary>
Motivation: 传统的OCR技术在字符分割上容易出错，并且缺乏上下文来利用语言模型。随着序列到序列翻译的进步，现代技术首先检测单词，然后将每个单词逐个输入到模型中以直接输出完整的单词。然而，这种转变使得准确性的瓶颈转移到了单词分割上。因此，本文提出了从单词级OCR到行级OCR的进展，以解决这些问题。

Method: 本文提出了一种从单词级OCR到行级OCR的进展，通过直接输入整行文本到模型中，以序列形式输出完整单词。这种方法避免了单词检测中的错误，并提供了更大的句子上下文。此外，本文还构建了一个包含251个英文页面图像的行级标注数据集。

Result: 实验结果表明，所提出的技术不仅提高了OCR的准确性，还提高了效率。具体来说，端到端的准确性提高了5.4%，并且效率提高了4倍。此外，随着大型语言模型的持续改进，本文的方法也有潜力利用这些进步。

Conclusion: 本文提出了一种从单词级OCR到行级OCR的自然且逻辑上的进展，该方法可以避免单词检测中的错误，并提供更大的句子上下文以更好地利用语言模型。实验表明，该技术不仅提高了准确性，还提高了OCR的效率。此外，本文还贡献了一个精心整理的数据集，用于训练和基准测试这种从单词级到行级OCR的转变。

Abstract: Conventional optical character recognition (OCR) techniques segmented each
character and then recognized. This made them prone to error in character
segmentation, and devoid of context to exploit language models. Advances in
sequence to sequence translation in last decade led to modern techniques first
detecting words and then inputting one word at a time to a model to directly
output full words as sequence of characters. This allowed better utilization of
language models and bypass error-prone character segmentation step. We observe
that the above transition in style has moved the bottleneck in accuracy to word
segmentation. Hence, in this paper, we propose a natural and logical
progression from word level OCR to line-level OCR. The proposal allows to
bypass errors in word detection, and provides larger sentence context for
better utilization of language models. We show that the proposed technique not
only improves the accuracy but also efficiency of OCR. Despite our thorough
literature survey, we did not find any public dataset to train and benchmark
such shift from word to line-level OCR. Hence, we also contribute a
meticulously curated dataset of 251 English page images with line-level
annotations. Our experimentation revealed a notable end-to-end accuracy
improvement of 5.4%, underscoring the potential benefits of transitioning
towards line-level OCR, especially for document images. We also report a 4
times improvement in efficiency compared to word-based pipelines. With
continuous improvements in large language models, our methodology also holds
potential to exploit such advances. Project Website:
https://nishitanand.github.io/line-level-ocr-website

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [37] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL is a universal programming language translator that uses a unified intermediate representation to enable bidirectional translation between multiple languages, offering a scalable and extensible solution for language-agnostic programming.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches require separate translators for each language pair, leading to exponential complexity growth. The need for a more efficient and scalable solution for bidirectional translation between multiple programming languages motivated the development of CrossTL.

Method: CrossTL uses a single universal IR called CrossGL to facilitate translations between multiple languages, including CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo. The system consists of language-specific lexers/parsers, bidirectional CrossGL translation modules, and comprehensive backend implementations.

Result: The effectiveness of CrossTL was demonstrated through comprehensive evaluation across programming domains, achieving successful compilation and execution across all supported backends. The universal IR design enables adding new languages with minimal effort.

Conclusion: CrossTL represents a significant step toward language-agnostic programming, enabling write-once, deploy-everywhere development.

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [38] [Database Normalization via Dual-LLM Self-Refinement](https://arxiv.org/abs/2508.17693)
*Eunjae Jo,Nakyung Lee,Gyuyeong Kim*

Main category: cs.DB

TL;DR: Miffie 是一个利用大型语言模型的数据库规范化框架，能够自动进行数据库规范化并保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 数据库规范化对于保持数据完整性至关重要，但通常由数据工程师手动执行，耗时且容易出错。因此，需要一种自动化的方法来提高效率和准确性。

Method: Miffie 是一个利用大型语言模型能力的数据库规范化框架，其核心是一个结合了最佳性能模型的双模型自精炼架构，用于生成和验证规范模式。

Result: 实验结果表明，Miffie 能够在保持高准确性的同时对复杂的数据库模式进行规范化。

Conclusion: Miffie 可以在保持高准确性的同时对复杂的数据库模式进行规范化。

Abstract: Database normalization is crucial to preserving data integrity. However, it
is time-consuming and error-prone, as it is typically performed manually by
data engineers. To this end, we present Miffie, a database normalization
framework that leverages the capability of large language models. Miffie
enables automated data normalization without human effort while preserving high
accuracy. The core of Miffie is a dual-model self-refinement architecture that
combines the best-performing models for normalized schema generation and
verification, respectively. The generation module eliminates anomalies based on
the feedback of the verification module until the output schema satisfies the
requirement for normalization. We also carefully design task-specific zero-shot
prompts to guide the models for achieving both high accuracy and cost
efficiency. Experimental results show that Miffie can normalize complex
database schemas while maintaining high accuracy.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [39] [Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses](https://arxiv.org/abs/2508.21209)
*Vanessa Figueiredo*

Main category: cs.HC

TL;DR: 本文研究了巴西儿童使用对话代理的情况，并提出结构化提示方法以增强互动效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索巴西儿童如何使用对话代理进行学习、探索和娱乐，并通过结构化脚手架提升互动效果。

Method: 通过两个研究，第一个研究采用访谈、观察和认知工作分析来映射儿童的信息处理流程，第二个研究通过模拟儿童-CA交互比较结构化提示与非结构化基线的效果。

Result: 研究发现了三种CA功能：学校、探索和娱乐，并验证了结构化提示在可读性、问题数量/深度/多样性以及连贯性方面的优势。

Conclusion: 本文提出了针对巴西儿童使用对话代理的结构化脚手架设计建议，包括基于LLM的结构化提示方法，以提高学习效果。

Abstract: This paper presents two studies on how Brazilian children (ages 9--11) use
conversational agents (CAs) for schoolwork, discovery, and entertainment, and
how structured scaffolds can enhance these interactions. In Study 1, a
seven-week online investigation with 23 participants (children, parents,
teachers) employed interviews, observations, and Cognitive Work Analysis to map
children's information-processing flows, the role of more knowledgeable others,
functional uses, contextual goals, and interaction patterns to inform
conversation-tree design. We identified three CA functions: School, Discovery,
Entertainment, and derived ``recipe'' scaffolds mirroring parent-child support.
In Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges,
comparing conversation-tree recipes based on structured-prompting to an
unstructured baseline. Quantitative evaluation of readability, question
count/depth/diversity, and coherence revealed gains for the recipe approach.
Building on these findings, we offer design recommendations: scaffolded
conversation-trees, child-dedicated profiles for personalized context, and
caregiver-curated content. Our contributions include the first CWA application
with Brazilian children, an empirical framework of child-CA information flows,
and an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective,
scaffolded learning.

</details>


### [40] [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456)
*Yi-Hao Peng,Dingzeyu Li,Jeffrey P. Bigham,Amy Pavel*

Main category: cs.HC

TL;DR: Morae is a UI agent that pauses to let users make choices during task execution, helping BLV users complete more tasks and select options that better match their preferences.


<details>
  <summary>Details</summary>
Motivation: Current UI agents perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, reducing user agency. For example, a BLV participant asked to buy the cheapest available sparkling water, but the agent automatically chose an option without mentioning alternatives with different flavors or better ratings.

Method: Morae identifies decision points during task execution and pauses to allow users to make choices. It uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompts users for clarification when there is a choice to be made.

Result: In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences compared to baseline agents, including OpenAI Operator.

Conclusion: Morae helps BLV users complete more tasks and select options that better match their preferences, demonstrating a mixed-initiative approach where users benefit from automation while expressing their preferences.

Abstract: User interface (UI) agents promise to make inaccessible or complex UIs easier
to access for blind and low-vision (BLV) users. However, current UI agents
typically perform tasks end-to-end without involving users in critical choices
or making them aware of important contextual information, thus reducing user
agency. For example, in our field study, a BLV participant asked to buy the
cheapest available sparkling water, and the agent automatically chose one from
several equally priced options, without mentioning alternative products with
different flavors or better ratings. To address this problem, we introduce
Morae, a UI agent that automatically identifies decision points during task
execution and pauses so that users can make choices. Morae uses large
multimodal models to interpret user queries alongside UI code and screenshots,
and prompt users for clarification when there is a choice to be made. In a
study over real-world web tasks with BLV participants, Morae helped users
complete more tasks and select options that better matched their preferences,
as compared to baseline agents, including OpenAI Operator. More broadly, this
work exemplifies a mixed-initiative approach in which users benefit from the
automation of UI agents while being able to express their preferences.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [41] [Quantum-Enhanced Natural Language Generation: A Multi-Model Framework with Hybrid Quantum-Classical Architectures](https://arxiv.org/abs/2508.21332)
*Chi-Sheng Chen,En-Jui Kuo*

Main category: quant-ph

TL;DR: 本文评估了量子文本生成模型与传统Transformer/MLP架构的性能，发现传统模型总体表现更优，但量子模型在某些特定任务中具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算在自然语言处理中的应用，评估量子文本生成模型与传统Transformer/MLP架构的性能。

Method: 对五种不同的模型进行了系统实验比较，包括Transformer（基线）、量子核自注意力网络（QKSAN）、量子RWKV（QRWKV）和量子注意序列架构（QASA）。

Result: 传统Transformer模型在平均困惑度和BLEU-1分数上表现最佳，而量子模型在特定任务中表现出色，如QKSAN在重复率方面表现优异，QRWKV在词汇多样性方面表现完美。

Conclusion: 虽然传统Transformer模型在整体上表现出色，但量子模型在特定场景下表现出竞争力。

Abstract: This paper presents a comprehensive evaluation of quantum text generation
models against traditional Transformer/MLP architectures, addressing the
growing interest in quantum computing applications for natural language
processing. We conduct systematic experiments comparing five distinct models:
Transformer (baseline), Quantum Kernel Self-Attention Network (QKSAN), Quantum
RWKV (QRWKV), and Quantum Attention Sequence Architecture (QASA) across five
diverse datasets including simple sentences, short stories, quantum phrases,
haiku poetry, and proverbs. Our evaluation employs multiple metrics including
perplexity, BLEU scores, vocabulary diversity, repetition rates, and fluency
measures to assess different aspects of text generation quality. The
experimental results reveal that while traditional Transformer models maintain
overall superiority with the lowest average perplexity (1.21) and highest
BLEU-1 score (0.2895), quantum-inspired models demonstrate competitive
performance in specific scenarios. Notably, QKSAN achieves a competitive BLEU-1
score of 0.2800 while maintaining zero repetition rates, and QRWKV demonstrates
perfect vocabulary diversity (Distinct-1 = 1.000) in certain tasks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [42] [Stairway to Fairness: Connecting Group and Individual Fairness](https://arxiv.org/abs/2508.21334)
*Theresia Veronika Rampisela,Maria Maistro,Tuukka Ruotsalo,Falk Scholer,Christina Lioma*

Main category: cs.IR

TL;DR: 本文研究了群体公平和个体公平之间的关系，发现高度公平的群体推荐可能对个体非常不公平，这对RS从业者改进系统公平性有重要价值。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对群体公平和个体公平之间关系的科学理解，因为之前的工作对两种公平类型使用了不同的评估指标或评估目标，从而无法进行适当的比较。

Method: 我们通过全面比较可用于两种公平类型的评估指标，研究了群体公平和个体公平之间的关系。

Result: 我们的实验结果显示，高度公平的群体推荐可能对个体非常不公平。

Conclusion: 我们的研究发现，高度公平的群体推荐可能对个体非常不公平，这对RS从业者改进系统公平性有重要价值。

Abstract: Fairness in recommender systems (RSs) is commonly categorised into group
fairness and individual fairness. However, there is no established scientific
understanding of the relationship between the two fairness types, as prior work
on both types has used different evaluation measures or evaluation objectives
for each fairness type, thereby not allowing for a proper comparison of the
two. As a result, it is currently not known how increasing one type of fairness
may affect the other. To fill this gap, we study the relationship of group and
individual fairness through a comprehensive comparison of evaluation measures
that can be used for both fairness types. Our experiments with 8 runs across 3
datasets show that recommendations that are highly fair for groups can be very
unfair for individuals. Our finding is novel and useful for RS practitioners
aiming to improve the fairness of their systems. Our code is available at:
https://github.com/theresiavr/stairway-to-fairness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: 本文提出了一种混合字符串相似度、主题建模、层次聚类和基于规则的管道，用于交易对手方的聚类，并且适应预期的未知簇数。该方法在真实数据集上表现出色，提高了性能，并减少了对人工审查的需求。


<details>
  <summary>Details</summary>
Motivation: 在银行支付消息系统（如SWIFT）中，手动输入的标签通常包含物理或法律实体详细信息，缺乏句子结构，同时包含手动输入引入的所有变化和噪声。这导致调查人员或反欺诈专业人员在增强对支付流发起人和受益人实体的知识以及追踪资金和资产时存在工具上的空白。

Method: 提出了一种混合字符串相似度、主题建模、层次聚类和基于规则的管道，以促进交易对手方的聚类，并且适应预期的未知簇数。还设计了补充评估方法的指标，基于精确率和召回率这些已知的度量标准。

Result: 在真实标注数据集上的测试表明，该方法比基线规则-based（'关键词'）方法有显著改进。

Conclusion: 该方法能够显著提高基于规则的'关键词'方法的性能，并且保留了规则系统中的大部分可解释性，从而减少了对人工审查的需求。当只需要调查人口子集时，如制裁调查，该方法允许更好地控制遗漏实体变体的风险。

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [44] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: 本文研究了强化学习在大型语言模型中的应用，发现模型-任务对齐是影响反直觉结果的关键因素。


<details>
  <summary>Details</summary>
Motivation: 最近在将强化学习应用于大型语言模型方面的进展取得了显著进展，但这些观察结果在什么条件下成立以及何时失败仍不清楚。本文旨在明确这些条件并探讨模型-任务对齐的重要性。

Method: 通过系统且全面地检查一系列反直觉的主张，并通过不同模型架构和任务领域的严格实验验证，本文识别出区分强化学习观察的关键因素：预训练模型是否已经在所评估的任务上表现出强大的模型-任务对齐。

Result: 实验结果表明，当模型和任务具有强模型-任务对齐时，许多反直觉的结果会出现；而在更具挑战性的领域中，这些技术无法推动显著的学习，而标准的强化学习方法仍然有效。

Conclusion: 本文发现，尽管标准的强化学习训练在各种设置中都保持了稳定性，但许多反直觉的结果只有在模型和任务已经表现出强大的模型-任务对齐时才会出现。相反，在更具挑战性的领域中，这些技术无法推动显著的学习，而标准的强化学习方法仍然有效。

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [45] [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
*Israel Abebe Azime,Deborah D. Kanubala,Tejumade Afonja,Mario Fritz,Isabel Valera,Dietrich Klakow,Philipp Slusallek*

Main category: cs.LG

TL;DR: 本文研究了LLMs在不同地区的贷款审批数据集上的性能和公平性，发现序列化格式对LLMs的性能和公平性有显著影响，并指出需要有效的表格数据表示方法和公平意识模型以提高LLMs在金融决策中的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs在高风险决策任务中被越来越多地使用，例如贷款审批。然而，LLMs在处理表格数据、确保公平性和提供可靠预测方面存在困难。

Method: 我们评估了LLMs在三个地理上不同的地区（加纳、德国和美国）的序列化贷款审批数据集上的性能和公平性。我们的评估重点是模型的零样本和上下文学习（ICL）能力。

Result: 我们的结果表明，序列化格式的选择显著影响LLMs的性能和公平性，某些格式如GReat和LIFT产生了更高的F1分数，但加剧了公平性差异。值得注意的是，ICL相对于零样本基线提高了模型性能4.9-59.6%，但其对公平性的影响在不同数据集中差异很大。

Conclusion: 我们的工作强调了有效的表格数据表示方法和公平意识模型在提高LLMs在金融决策中的可靠性的重要性。

Abstract: Large Language Models (LLMs) are increasingly employed in high-stakes
decision-making tasks, such as loan approvals. While their applications expand
across domains, LLMs struggle to process tabular data, ensuring fairness and
delivering reliable predictions. In this work, we assess the performance and
fairness of LLMs on serialized loan approval datasets from three geographically
distinct regions: Ghana, Germany, and the United States. Our evaluation focuses
on the model's zero-shot and in-context learning (ICL) capabilities. Our
results reveal that the choice of serialization (Serialization refers to the
process of converting tabular data into text formats suitable for processing by
LLMs.) format significantly affects both performance and fairness in LLMs, with
certain formats such as GReat and LIFT yielding higher F1 scores but
exacerbating fairness disparities. Notably, while ICL improved model
performance by 4.9-59.6% relative to zero-shot baselines, its effect on
fairness varied considerably across datasets. Our work underscores the
importance of effective tabular data representation methods and fairness-aware
models to improve the reliability of LLMs in financial decision-making.

</details>


### [46] [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
*Yifei Yuan,Jiatong Li,Weijia Zhang,Mohammad Aliannejadi,Evangelos Kanoulas,Renjun Hu*

Main category: cs.LG

TL;DR: 本文提出 InsightTab，一种基于人类学习过程的见解蒸馏框架，通过规则总结、战略示例和见解反思，提高 LLMs 在少样本表格分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型 (LLMs) 在少样本表格分类中显示出潜力，但由于结构化数据的变异性，仍面临挑战。因此，需要一种方法来将数据提炼为可操作的见解，以增强 LLMs 的分类能力。

Method: InsightTab 是一种基于分而治之、先易后难和反思学习原则的见解蒸馏框架，通过 LLMs 和数据建模技术的深度协作实现规则总结、战略示例和见解反思。

Result: InsightTab 在九个数据集上的实验结果表明，它在少样本表格分类任务中优于现有的最先进的方法。消融研究进一步验证了基于原则的蒸馏过程的有效性。

Conclusion: InsightTab 的实验结果表明，它在九个数据集上 consistently over state-of-the-art methods，证明了其有效性。

Abstract: Recent studies show the promise of large language models (LLMs) for few-shot
tabular classification but highlight challenges due to the variability in
structured data. To address this, we propose distilling data into actionable
insights to enable robust and effective classification by LLMs. Drawing
inspiration from human learning processes, we introduce InsightTab, an insight
distillation framework guided by principles of divide-and-conquer, easy-first,
and reflective learning. Our approach integrates rule summarization, strategic
exemplification, and insight reflection through deep collaboration between LLMs
and data modeling techniques. The obtained insights enable LLMs to better align
their general knowledge and capabilities with the particular requirements of
specific tabular tasks. We extensively evaluate InsightTab on nine datasets.
The results demonstrate consistent improvement over state-of-the-art methods.
Ablation studies further validate the principle-guided distillation process,
while analyses emphasize InsightTab's effectiveness in leveraging labeled data
and managing bias.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 本文研究了架构归纳偏差对大型语言模型在教学对话中的认知行为的影响，引入了符号支撑机制和短期记忆模式，并通过控制消融实验评估了模型输出。初步结果表明，完整系统表现优于基线变体，去除记忆或符号结构会损害关键认知行为。


<details>
  <summary>Details</summary>
Motivation: 我们研究了架构归纳偏差如何影响大型语言模型（LLMs）在教学对话中的认知行为。

Method: 我们引入了一种符号支撑机制和一个短期记忆模式，旨在促进苏格拉底辅导中的适应性、结构化推理。通过五个系统变体的控制消融，我们使用专家设计的评分标准评估模型输出，涵盖支撑、响应性、符号推理和对话记忆。

Result: 初步结果表明，我们的完整系统始终优于基线变体。分析显示，去除记忆或符号结构会损害关键的认知行为，包括抽象、适应性探查和概念连续性。

Conclusion: 这些发现支持了一个处理层面的解释，即架构支架可以可靠地塑造大型语言模型中的新兴教学策略。

Abstract: We study how architectural inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding mechanism paired with a short-term memory schema designed
to promote adaptive, structured reasoning in Socratic tutoring. Using
controlled ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which architectural scaffolds can
reliably shape emergent instructional strategies in LLMs.

</details>


### [48] [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
*Tony Lee,Haoqin Tu,Chi Heem Wong,Zijun Wang,Siwei Yang,Yifan Mai,Yuyin Zhou,Cihang Xie,Percy Liang*

Main category: cs.AI

TL;DR: 本文介绍了 AHELM，一个全面评估音频-语言模型的基准，涵盖10个重要方面。结果显示 Gemini 2.5 Pro 在5个方面表现最佳，但存在群体不公平性。基线系统表现不错，AHELM 将持续更新。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准存在不足，如缺乏标准化、仅测量少数能力、忽略公平性和安全性等。同时，模型间的比较困难，因为不同的评估使用不同的提示方法和推理参数。因此，需要一个全面且标准化的基准来评估 ALMs。

Method: 引入 AHELM 基准，聚合多种数据集，包括两个新的合成音频-文本数据集 PARADE 和 CoRe-Bench，以全面评估 ALMs 的性能。标准化提示、推理参数和评估指标，确保模型之间的公平比较。测试了14个开源和闭源 ALMs 以及3个简单的基线系统。

Result: Gemini 2.5 Pro 在5个方面排名第一，但在 ASR 任务中表现出群体不公平性（p=0.01）。基线系统表现良好，其中一个仅具有语音到文本功能却排名第五。所有原始提示、模型生成和输出均可在网站上获取。

Conclusion: AHELM 是一个全面的基准，旨在评估音频-语言模型（ALMs）在10个重要方面的性能。结果表明，Gemini 2.5 Pro 在5个方面表现最佳，但在ASR任务中表现出群体不公平性。基线系统也表现不错，其中一个仅具有语音到文本功能却排名第五。AHELM 是一个持续更新的基准，未来将添加新的数据集和模型。

Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take
interleaved audio and text as input and output text -- are hindered by the lack
of standardized benchmarks; most benchmarks measure only one or two
capabilities and omit evaluative aspects such as fairness or safety.
Furthermore, comparison across models is difficult as separate evaluations test
a limited number of models and use different prompting methods and inference
parameters. To address these shortfalls, we introduce AHELM, a benchmark that
aggregates various datasets -- including 2 new synthetic audio-text datasets
called PARADE, which evaluates the ALMs on avoiding stereotypes, and
CoRe-Bench, which measures reasoning over conversational audio through
inferential multi-turn question answering -- to holistically measure the
performance of ALMs across 10 aspects we have identified as important to the
development and usage of ALMs: audio perception, knowledge, reasoning, emotion
detection, bias, fairness, multilinguality, robustness, toxicity, and safety.
We also standardize the prompts, inference parameters, and evaluation metrics
to ensure equitable comparisons across models. We test 14 open-weight and
closed-API ALMs from 3 developers and 3 additional simple baseline systems each
consisting of an automatic speech recognizer and a language model. Our results
show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits
group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do
not. We also find that the baseline systems perform reasonably well on AHELM,
with one ranking 5th overall despite having only speech-to-text capabilities.
For transparency, all raw prompts, model generations, and outputs are available
on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is
intended to be a living benchmark and new datasets and models will be added
over time.

</details>

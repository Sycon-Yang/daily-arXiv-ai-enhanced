<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 30]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)
*Shmuel Amar,Ori Shapira,Aviv Slobodkin,Ido Dagan*

Main category: cs.CL

TL;DR: 本文提出了一种统一的内容选择框架IGCS，并引入了一个基准测试和一个大型合成数据集，展示了其在不同任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前内容选择任务通常被孤立研究，每个任务都有自己的建模方法、数据集和评估指标。本文旨在提供一个统一的框架，以促进不同内容选择任务的研究和应用。

Method: 本文提出了一个统一的框架，即基于指令的内容选择（IGCS），并引入了一个覆盖多种内容选择任务的基准测试。此外，还创建了一个大型通用合成数据集，并展示了在不同任务上的迁移学习效果。最后，解决了基于LLM的内容选择建模中的推理时间问题，并评估了一个通用的评估指标。

Result: 本文提出的IGCS框架和相关资源能够提升不同内容选择任务的性能，并为未来的内容选择模型提供了实用的工具和方法。

Conclusion: 本文提出了一个统一的框架，即基于指令的内容选择（IGCS），并引入了一个覆盖多种内容选择任务的基准测试。此外，还创建了一个大型通用合成数据集，并展示了在不同任务上的迁移学习效果。最后，解决了基于LLM的内容选择建模中的推理时间问题，并评估了一个通用的评估指标。

Abstract: A broad range of NLP tasks involve selecting relevant text spans from given
source texts. Despite this shared objective, such \textit{content selection}
tasks have traditionally been studied in isolation, each with its own modeling
approaches, datasets, and evaluation metrics. In this work, we propose
\textit{instruction-guided content selection (IGCS)} as a beneficial unified
framework for such settings, where the task definition and any
instance-specific request are encapsulated as instructions to a language model.
To promote this framework, we introduce \igcsbench{}, the first unified
benchmark covering diverse content selection tasks. Further, we create a large
generic synthetic dataset that can be leveraged for diverse content selection
tasks, and show that transfer learning with these datasets often boosts
performance, whether dedicated training for the targeted task is available or
not. Finally, we address generic inference time issues that arise in LLM-based
modeling of content selection, assess a generic evaluation metric, and overall
propose the utility of our resources and methods for future content selection
models. Models and datasets available at https://github.com/shmuelamar/igcs.

</details>


### [2] [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)
*Robert Korom,Sarah Kiptinness,Najib Adan,Kassim Said,Catherine Ithuli,Oliver Rotich,Boniface Kimani,Irene King'ori,Stellah Kamau,Elizabeth Atemba,Muna Aden,Preston Bowman,Michael Sharman,Rebecca Soskin Hicks,Rebecca Distler,Johannes Heidecke,Rahul K. Arora,Karan Singhal*

Main category: cs.CL

TL;DR: 研究评估了基于大语言模型的临床决策支持工具AI Consult在实际护理中的影响，发现其能有效减少临床错误，并提供了一个实用框架以推动负责任的采用。


<details>
  <summary>Details</summary>
Motivation: 评估基于大语言模型的临床决策支持工具在实际护理中的影响，以减少临床错误。

Method: 与Penda Health合作，进行了一项质量改进研究，比较了15家诊所中使用AI Consult和未使用AI Consult的临床医生的患者访问结果。独立医生对访问进行了评分以识别临床错误。

Result: 使用AI Consult的临床医生犯的错误较少：诊断错误减少了16%，治疗错误减少了13%。AI Consult的引入每年可以在Penda避免22,000次诊断错误和29,000次治疗错误。75%的临床医生认为AI Consult对护理质量的影响是“显著”的。

Conclusion: 这项研究展示了基于大语言模型的临床决策支持工具在现实世界环境中减少错误的潜力，并提供了推动负责任采用的实用框架。

Abstract: We evaluate the impact of large language model-based clinical decision
support in live care. In partnership with Penda Health, a network of primary
care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a
safety net for clinicians by identifying potential documentation and clinical
decision-making errors. AI Consult integrates into clinician workflows,
activating only when needed and preserving clinician autonomy. We conducted a
quality improvement study, comparing outcomes for 39,849 patient visits
performed by clinicians with or without access to AI Consult across 15 clinics.
Visits were rated by independent physicians to identify clinical errors.
Clinicians with access to AI Consult made relatively fewer errors: 16% fewer
diagnostic errors and 13% fewer treatment errors. In absolute terms, the
introduction of AI Consult would avert diagnostic errors in 22,000 visits and
treatment errors in 29,000 visits annually at Penda alone. In a survey of
clinicians with AI Consult, all clinicians said that AI Consult improved the
quality of care they delivered, with 75% saying the effect was "substantial".
These results required a clinical workflow-aligned AI Consult implementation
and active deployment to encourage clinician uptake. We hope this study
demonstrates the potential for LLM-based clinical decision support tools to
reduce errors in real-world settings and provides a practical framework for
advancing responsible adoption.

</details>


### [3] [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)
*Shuyuan Lin,Lei Duan,Philip Hughes,Yuxuan Sheng*

Main category: cs.CL

TL;DR: This paper introduces SALU, an approach that integrates unanswerability detection within LLMs, improving their ability to handle unanswerable questions and reduce hallucinations.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches to handling unanswerable questions in CIR systems often rely on external classifiers, which can introduce inconsistencies with the core generative LLMs. This paper aims to address this issue by integrating unanswerability detection directly within the LLM's generative process.

Method: SALU uses a multi-task learning framework for standard QA and explicit abstention generation, along with confidence-score-guided reinforcement learning with human feedback (RLHF) to penalize hallucinated responses and reward appropriate abstentions.

Result: SALU outperforms strong baselines, including hybrid LLM-classifier systems, in overall accuracy for correctly answering or abstaining from questions. Human evaluation confirms its superior reliability, achieving high scores in factuality, appropriate abstention, and a dramatic reduction in hallucination.

Conclusion: SALU demonstrates superior reliability in handling unanswerable questions, significantly reducing hallucinations and showing the ability to robustly 'know when to say I don't know.'

Abstract: Conversational Information Retrieval (CIR) systems, while offering intuitive
access to information, face a significant challenge: reliably handling
unanswerable questions to prevent the generation of misleading or hallucinated
content. Traditional approaches often rely on external classifiers, which can
introduce inconsistencies with the core generative Large Language Models
(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a
novel approach that deeply integrates unanswerability detection directly within
the LLM's generative process. SALU is trained using a multi-task learning
framework for both standard Question Answering (QA) and explicit abstention
generation for unanswerable queries. Crucially, it incorporates a
confidence-score-guided reinforcement learning with human feedback (RLHF)
phase, which explicitly penalizes hallucinated responses and rewards
appropriate abstentions, fostering intrinsic self-awareness of knowledge
boundaries. Through extensive experiments on our custom-built
C-IR_Answerability dataset, SALU consistently outperforms strong baselines,
including hybrid LLM-classifier systems, in overall accuracy for correctly
answering or abstaining from questions. Human evaluation further confirms
SALU's superior reliability, achieving high scores in factuality, appropriate
abstention, and, most importantly, a dramatic reduction in hallucination,
demonstrating its ability to robustly "know when to say 'I don't know'."

</details>


### [4] [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)
*Aleksandr Perevalov,Andreas Both*

Main category: cs.CL

TL;DR: 本文提出了一种名为mKGQAgent的框架，用于将自然语言问题转换为SPARQL查询，并在多语言知识图谱问答任务中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要集中在结合解决下游任务的组件上，而本文旨在提出一种更高效、可解释的方法来处理多语言知识图谱问答任务。

Method: 本文引入了mKGQAgent框架，该框架将自然语言问题转换为SPARQL查询的任务分解为模块化、可解释的子任务，并利用协调的LLM代理工作流进行规划、实体链接和查询优化。

Result: 在Text2SPARQL挑战赛2025的DBpedia和Corporate基准测试中，本文的方法获得了第一名。

Conclusion: 本文提出了mKGQAgent框架，该框架在多语言知识图谱问答任务中取得了优异的成绩，并为开发类人推理系统提供了新的方向。

Abstract: Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.

</details>


### [5] [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)
*Rishemjit Kaur,Arshdeep Singh Bhankhar,Surangika Ranathunga,Jashanpreet Singh Salh,Sudhir Rajput,Vidhi,Kashish Mahendra,Bhavika Berwal,Ritesh Kumar*

Main category: cs.CL

TL;DR: 本研究通过生成多语言合成农业数据集并微调语言特定的大型语言模型，提高了农业领域大型语言模型的性能，特别是在多语言和低资源环境中。


<details>
  <summary>Details</summary>
Motivation: 使农民能够及时获取其母语中的准确农业相关信息对于农业领域的成功至关重要。然而，简单地使用公开的通用大型语言模型在农业领域通常提供的是通用建议，在本地和多语言环境中缺乏精确性，这是由于领域特定训练不足和高质量、区域特定数据集的稀缺性。

Method: 本研究通过从农业特定文档生成多语言合成农业数据集（英语、印地语、旁遮普语），并微调语言特定的大型语言模型来解决这些问题。

Result: 在精心整理的多语言数据集上的评估表明，微调后的模型在事实准确性、相关性和农业共识方面相比基线模型有显著提高。

Conclusion: 本研究展示了合成数据驱动的、语言特定的微调在改善农业领域大型语言模型性能方面的有效性，特别是在多语言和低资源环境中。通过提供更准确和本地化的农业咨询服务，本研究为缩小AI驱动的农业解决方案中的知识差距做出了有意义的一步。

Abstract: Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Although large language models (LLMs) can be used to
implement Question Answering (QA) systems, simply using publicly available
general-purpose LLMs in agriculture typically offer generic advisories, lacking
precision in local and multilingual contexts due to insufficient
domain-specific training and scarcity of high-quality, region-specific
datasets. Our study addresses these limitations by generating multilingual
synthetic agricultural datasets (English, Hindi, Punjabi) from
agriculture-specific documents and fine-tuning language-specific LLMs. Our
evaluation on curated multilingual datasets demonstrates significant
improvements in factual accuracy, relevance, and agricultural consensus for the
fine-tuned models compared to their baseline counterparts. These results
highlight the efficacy of synthetic data-driven, language-specific fine-tuning
as an effective strategy to improve the performance of LLMs in agriculture,
especially in multilingual and low-resource settings. By enabling more accurate
and localized agricultural advisory services, this study provides a meaningful
step toward bridging the knowledge gap in AI-driven agricultural solutions for
diverse linguistic communities.

</details>


### [6] [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)
*Giulio Pelosio,Devesh Batra,Noémie Bovey,Robert Hankache,Cristovao Iglesias,Greig Cowan,Raad Khraishi*

Main category: cs.CL

TL;DR: 本研究通过一种基于名称的基准测试方法，探讨了将显式国籍标签替换为文化指示性名称对大型语言模型（LLMs）偏见和准确性的影响。结果表明，小模型的准确性较低，并且表现出更多的偏见，同时在模糊上下文中保留了更大的现有错误比例。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的应用中，LLMs可能会表现出对特定国籍的隐含偏见，即使没有明确的人口统计标记。因此，我们需要一种更贴近现实场景的方法来研究这种偏见。

Method: 我们引入了一种基于名称的基准测试方法，从Bias Benchmark for QA (BBQ)数据集中派生出来，以研究将显式国籍标签替换为文化指示性名称的影响。

Result: 实验表明，小模型的准确性较低，并且表现出更多的偏见。例如，在我们的基于名称的数据集和模糊上下文中，Claude Haiku表现出最差的刻板印象偏见分数为9%，而其较大的版本Claude Sonnet的偏见分数仅为3.5%。此外，小模型在模糊上下文中保留了更大的现有错误比例。

Conclusion: 我们的研究突显了LLMs中偏见的顽固韧性，强调了其在多样化的全球环境中开发和部署AI系统的重要影响。

Abstract: Large Language Models (LLMs) can exhibit latent biases towards specific
nationalities even when explicit demographic markers are not present. In this
work, we introduce a novel name-based benchmarking approach derived from the
Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting
explicit nationality labels with culturally indicative names, a scenario more
reflective of real-world LLM applications. Our novel approach examines how this
substitution affects both bias magnitude and accuracy across a spectrum of LLMs
from industry leaders such as OpenAI, Google, and Anthropic. Our experiments
show that small models are less accurate and exhibit more bias compared to
their larger counterparts. For instance, on our name-based dataset and in the
ambiguous context (where the correct choice is not revealed), Claude Haiku
exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for
its larger counterpart, Claude Sonnet, where the latter also outperformed it by
117.7% in accuracy. Additionally, we find that small models retain a larger
portion of existing errors in these ambiguous contexts. For example, after
substituting names for explicit nationality references, GPT-4o retains 68% of
the error rate versus 76% for GPT-4o-mini, with similar findings for other
model providers, in the ambiguous context. Our research highlights the stubborn
resilience of biases in LLMs, underscoring their profound implications for the
development and deployment of AI systems in diverse, global contexts.

</details>


### [7] [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)
*Ming Huang,Zehan Li,Yan Hu,Wanjing Wang,Andrew Wen,Scott Lane,Salih Selek,Lokesh Shahani,Rodrigo Machado-Vieira,Jair Soares,Hua Xu,Hongfang Liu*

Main category: cs.CL

TL;DR: 本研究利用生成式大语言模型对精神科电子健康记录中的自杀相关因素进行多标签分类，取得了良好的结果，并揭示了模型的系统性错误模式。


<details>
  <summary>Details</summary>
Motivation: 早期识别与自杀相关的因素对于及时干预至关重要。然而，以往的研究大多将自杀倾向视为二元分类任务，忽略了共存风险因素的复杂性。

Method: 本研究探索了使用生成式大语言模型（LLMs），特别是GPT-3.5和GPT-4.5，对精神科电子健康记录（EHRs）中的自杀相关因素（SrFs）进行多标签分类（MLC）。

Result: 微调的GPT-3.5在部分匹配准确率和F1分数上表现最佳，而GPT-4.5在引导提示下在标签集上表现出色，包括罕见或少数标签集，表明其性能更加平衡和稳健。

Conclusion: 本研究展示了生成式AI在复杂临床分类任务中的可行性，并为结构化电子健康记录数据提供了蓝图，以支持大规模临床研究和循证医学。

Abstract: Suicide remains a pressing global health crisis, with over 720,000 deaths
annually and millions more affected by suicide ideation (SI) and suicide
attempts (SA). Early identification of suicidality-related factors (SrFs),
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury
(NSSI), is critical for timely intervention. While prior studies have applied
AI to detect SrFs in clinical notes, most treat suicidality as a binary
classification task, overlooking the complexity of cooccurring risk factors.
This study explores the use of generative large language models (LLMs),
specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs
from psychiatric electronic health records (EHRs). We present a novel end to
end generative MLC pipeline and introduce advanced evaluation methods,
including label set level metrics and a multilabel confusion matrix for error
analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match
accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior
performance across label sets, including rare or minority label sets,
indicating a more balanced and robust performance. Our findings reveal
systematic error patterns, such as the conflation of SI and SA, and highlight
the models tendency toward cautious over labeling. This work not only
demonstrates the feasibility of using generative AI for complex clinical
classification tasks but also provides a blueprint for structuring unstructured
EHR data to support large scale clinical research and evidence based medicine.

</details>


### [8] [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)
*Arduin Findeis,Floris Weers,Guoli Yin,Ke Ye,Ruoming Pang,Tom Gunter*

Main category: cs.CL

TL;DR: 本文探讨了如何通过增强标准AI标注器系统来改善在长篇事实、数学和代码任务等挑战性响应领域的性能。我们提出了一种使用工具的代理系统，该系统利用网络搜索和代码执行进行外部验证，从而提高反馈质量。实验结果表明，外部工具在许多情况下确实能提高性能，但也显示出性能对简单参数的敏感性，并强调了改进标注器基准的必要性。


<details>
  <summary>Details</summary>
Motivation: 在某些领域，高质量的成对比较可能难以获得——从AI和人类。例如，对于包含许多事实陈述的响应，标注者可能不成比例地重视写作质量而非潜在事实。因此，我们需要一种改进的标注器系统来提高性能。

Method: 我们提出了一种使用工具的代理系统，以在这些领域提供更高品质的反馈。我们的系统使用网络搜索和代码执行来基于外部验证来巩固自己，独立于LLM的内部知识和偏见。

Result: 我们在三个目标响应领域以及通用标注任务中评估了我们的方法，使用了RewardBench（包括AlpacaEval和LLMBar）、RewardMath以及三个新的数据集。结果表明，外部工具确实可以在许多情况下提高性能。

Conclusion: 我们的实验结果表明，外部工具确实可以在许多情况下提高性能，但并非在所有情况下都有效。更广泛地说，我们的实验强调了性能对简单参数（例如提示）的敏感性，并需要改进（非饱和）标注器基准。

Abstract: Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the "better"
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM's internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.

</details>


### [9] [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)
*Soumen Sinha,Shahryar Rahnamayan,Azam Asilian Bidgoli*

Main category: cs.CL

TL;DR: 本文提出了一种基于坐标搜索的优化框架，通过为每个特征选择最佳阈值来改进二进制编码，提高了文本嵌入的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的二值化方法通常使用固定阈值，而本文旨在探索通过为每个特征选择最佳阈值来提高二进制编码性能的可能性。

Method: 本文提出了一种基于坐标搜索的优化框架，用于为每个特征确定最佳阈值，以生成更准确和高效的二进制表示。

Result: 实验结果表明，使用本文方法生成的二进制嵌入在准确性上优于传统的二值化方法，并在多个NLP任务和数据集上展示了良好的表现。

Conclusion: 本文提出的基于坐标搜索的优化框架能够通过为每个特征确定最佳阈值，从而在二进制编码中实现更好的性能。这种二进制表示方法在各种NLP应用中表现出色，具有广泛的应用潜力。

Abstract: Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.

</details>


### [10] [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)
*Cheng Liu,Yifei Lu,Fanghua Ye,Jian Li,Xingyu Chen,Feiliang Ren,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: CogDual是一种基于认知心理学的新颖角色扮演语言代理，通过结合外部情境意识和内部自我意识，提高了角色一致性与上下文对齐度，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于提示工程或监督微调来让模型模仿特定场景中的角色行为，但往往忽视了驱动这些行为的潜在认知机制。

Method: CogDual采用了一种认知-回应的推理范式，通过联合建模外部情境意识和内部自我意识来生成更一致的角色响应，并使用两种通用奖励方案进行强化学习优化。

Result: CogDual在CoSER基准、Cross-MR和LifeChoice上均优于现有基线，并能有效泛化到多种角色扮演任务中。

Conclusion: CogDual在CoSER基准以及Cross-MR和LifeChoice上表现出色，能够有效泛化到各种角色扮演任务中。

Abstract: Role-Playing Language Agents (RPLAs) have emerged as a significant
application direction for Large Language Models (LLMs). Existing approaches
typically rely on prompt engineering or supervised fine-tuning to enable models
to imitate character behaviors in specific scenarios, but often neglect the
underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by
cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a
\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external
situational awareness and internal self-awareness, CogDual generates responses
with improved character consistency and contextual alignment. To further
optimize the performance, we employ reinforcement learning with two
general-purpose reward schemes designed for open-domain text generation.
Extensive experiments on the CoSER benchmark, as well as Cross-MR and
LifeChoice, demonstrate that CogDual consistently outperforms existing
baselines and generalizes effectively across diverse role-playing tasks.

</details>


### [11] [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
*Zhiqiang Liu,Enpei Niu,Yin Hua,Mengshu Sun,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出了SKA-Bench，一个用于评估大型语言模型结构化知识理解能力的基准测试，并发现现有模型在此方面仍面临诸多挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化知识理解评估不够严谨，且仅关注一种类型的结构化知识，因此需要一个更全面和严谨的基准来诊断大型语言模型的不足。

Method: 引入SKA-Bench，一个包含四种广泛使用的结构化知识形式的基准测试，通过三阶段流程构建实例，并扩展为四个基本能力测试平台。

Result: 对8个代表性的大型语言模型进行实证评估，发现它们在理解结构化知识方面存在显著挑战。

Conclusion: 现有大型语言模型在理解结构化知识方面仍面临重大挑战，其性能受到噪声量、知识单元顺序和幻觉现象的影响。

Abstract: Although large language models (LLMs) have made significant progress in
understanding Structured Knowledge (SK) like KG and Table, existing evaluations
for SK understanding are non-rigorous (i.e., lacking evaluations of specific
capabilities) and focus on a single type of SK. Therefore, we aim to propose a
more comprehensive and rigorous structured knowledge understanding benchmark to
diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a
Structured Knowledge Augmented QA Benchmark that encompasses four widely used
structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a
three-stage pipeline to construct SKA-Bench instances, which includes a
question, an answer, positive knowledge units, and noisy knowledge units. To
evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we
expand the instances into four fundamental ability testbeds: Noise Robustness,
Order Insensitivity, Information Integration, and Negative Rejection. Empirical
evaluations on 8 representative LLMs, including the advanced DeepSeek-R1,
indicate that existing LLMs still face significant challenges in understanding
structured knowledge, and their performance is influenced by factors such as
the amount of noise, the order of knowledge units, and hallucination
phenomenon. Our dataset and code are available at
https://github.com/Lza12a/SKA-Bench.

</details>


### [12] [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)
*Lingfeng Zeng,Fangqi Lou,Zixuan Wang,Jiajie Xu,Jinyi Niu,Mengping Li,Yifan Dong,Qi Qi,Wei Zhang,Ziwei Yang,Jun Han,Ruilun Feng,Ruiqi Hu,Lejie Zhang,Zhengbo Feng,Yicheng Ren,Xin Guo,Zhaowei Liu,Dongpo Cheng,Weige Cai,Liwen Zhang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The booming development of AI agents presents unprecedented opportunities for
automating complex tasks across various domains. However, their multi-step,
multi-tool collaboration capabilities in the financial sector remain
underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed
to evaluate the practical abilities of AI agents in the financial domain.
FinGAIA comprises 407 meticulously crafted tasks, spanning seven major
financial sub-domains: securities, funds, banking, insurance, futures, trusts,
and asset management. These tasks are organized into three hierarchical levels
of scenario depth: basic business analysis, asset decision support, and
strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot
setting. The best-performing agent, ChatGPT, achieved an overall accuracy of
48.9\%, which, while superior to non-professionals, still lags financial
experts by over 35 percentage points. Error analysis has revealed five
recurring failure patterns: Cross-modal Alignment Deficiency, Financial
Terminological Bias, Operational Process Awareness Barrier, among others. These
patterns point to crucial directions for future research. Our work provides the
first agent benchmark closely related to the financial domain, aiming to
objectively assess and promote the development of agents in this crucial field.
Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

</details>


### [13] [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)
*Giuseppe Russo,Debora Nozza,Paul Röttger,Dirk Hovy*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLMs）与人类道德判断之间的差距，发现LLMs在人类意见一致时表现较好，但在分歧时表现较差。研究提出了一种新的方法（DMP）来改善这一情况，显著提高了模型与人类道德判断的对齐度和多样性。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地依赖大型语言模型获取道德建议，了解这些模型与人类道德判断的一致性变得尤为重要。然而，目前对此了解有限。因此，本研究旨在探索LLMs与人类道德判断之间的差距，并提出一种改进方法。

Method: 研究引入了一个包含1,618个真实道德困境及其人类道德判断的数据集，并将其视为一个多元分布对齐任务。通过使用从理性中提取的3,783个价值表达构建的60个价值分类法，研究分析了LLMs与人类在道德价值上的差异。最后，提出了一种基于狄利克雷采样的动态道德分析方法（DMP）来改善模型输出。

Result: 研究发现，LLMs仅在人类道德判断高度一致时才能再现人类判断，而在人类分歧增加时对齐度迅速下降。此外，LLMs使用的道德价值范围比人类狭窄。通过引入动态道德分析方法（DMP），模型的对齐度提高了64.3%，并且价值多样性得到了增强。

Conclusion: 研究发现，大型语言模型（LLMs）在人类道德判断高度一致的情况下能够再现人类判断，但当人类意见分歧增加时，对齐度急剧下降。此外，LLMs使用的道德价值范围比人类狭窄。为了解决这一问题，研究引入了一种基于狄利克雷采样的动态道德分析方法（DMP），显著提高了对齐度并增强了价值多样性，为更符合人类价值观的道德指导提供了方向。

Abstract: People increasingly rely on Large Language Models (LLMs) for moral advice,
which may influence humans' decisions. Yet, little is known about how closely
LLMs align with human moral judgments. To address this, we introduce the Moral
Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a
distribution of human moral judgments consisting of a binary evaluation and a
free-text rationale. We treat this problem as a pluralistic distributional
alignment task, comparing the distributions of LLM and human judgments across
dilemmas. We find that models reproduce human judgments only under high
consensus; alignment deteriorates sharply when human disagreement increases. In
parallel, using a 60-value taxonomy built from 3,783 value expressions
extracted from rationales, we show that LLMs rely on a narrower set of moral
values than humans. These findings reveal a pluralistic moral gap: a mismatch
in both the distribution and diversity of values expressed. To close this gap,
we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method
that conditions model outputs on human-derived value profiles. DMP improves
alignment by 64.3% and enhances value diversity, offering a step toward more
pluralistic and human-aligned moral guidance from LLMs.

</details>


### [14] [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
*Kyeongkyu Lee,Seonghwan Yoon,Hongki Lim*

Main category: cs.CL

TL;DR: 本文提出了一种名为CLARIFID的新框架，通过镜像专家的两步工作流程来优化诊断正确性，包括章节感知预训练、使用CheXbert F1分数作为奖励的微调、推理感知解码和多视图融合。实验结果表明，该方法在临床效果上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 自动生成放射学报告有潜力减轻放射科医生的大量工作负担，但目前的方法难以提供临床上可靠的结论。特别是，大多数先前的方法专注于生成流畅的文本，而没有有效地确保报告的事实正确性，并且通常依赖于单视图图像，限制了诊断的全面性。

Method: 我们提出了CLARIFID，一种新的框架，直接优化诊断正确性，通过镜像专家的两步工作流程。具体来说，CLARIFID (1) 通过章节感知预训练学习从发现到印象的逻辑流程，(2) 使用近端策略优化进行微调，其中CheXbert F1分数作为奖励，(3) 强制推理感知解码，在合成“印象”之前完成“发现”，(4) 通过基于视觉变压器的多视图编码器融合多个胸部X光视图。

Result: 在MIMIC-CXR数据集上的实验结果表明，我们的方法在临床效果上优于现有基线，并在标准NLG指标和临床意识评分上表现更好。

Conclusion: 实验结果表明，我们的方法在临床效果上优于现有的基线，并在标准NLG指标和临床意识评分上表现更好。

Abstract: Automatic generation of radiology reports has the potential to alleviate
radiologists' significant workload, yet current methods struggle to deliver
clinically reliable conclusions. In particular, most prior approaches focus on
producing fluent text without effectively ensuring the factual correctness of
the reports and often rely on single-view images, limiting diagnostic
comprehensiveness. We propose CLARIFID, a novel framework that directly
optimizes diagnostic correctness by mirroring the two-step workflow of experts.
Specifically, CLARIFID (1) learns the logical flow from Findings to Impression
through section-aware pretraining, (2) is fine-tuned with Proximal Policy
Optimization in which the CheXbert F1 score of the Impression section serves as
the reward, (3) enforces reasoning-aware decoding that completes "Findings"
before synthesizing the "Impression", and (4) fuses multiple chest X-ray views
via a vision-transformer-based multi-view encoder. During inference, we apply a
reasoning-aware next-token forcing strategy followed by report-level
re-ranking, ensuring that the model first produces a comprehensive Findings
section before synthesizing the Impression and thereby preserving coherent
clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate
that our method achieves superior clinical efficacy and outperforms existing
baselines on both standard NLG metrics and clinically aware scores.

</details>


### [15] [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)
*Miaomiao Gao,Xiaoxiao Xiang,Yiwen Guo*

Main category: cs.CL

TL;DR: 本文提出了一种基于编码器-适配器-大语言模型架构的多语言语音识别系统，并在多语言对话语音语言建模挑战中取得了优异的成绩。


<details>
  <summary>Details</summary>
Motivation: 本文旨在优化多语言对话场景中的语音识别准确性，通过结合大语言模型的推理能力和领域特定的适应性来实现这一目标。

Method: 我们提出了一个创新的编码器-适配器-大语言模型架构，结合了文本型大语言模型的强大推理能力和领域特定的适应性。此外，我们采用了一个精心设计的多阶段训练策略，利用广泛的多语言音频数据集来提高多语言识别性能。

Result: 实验结果表明，我们的方法在开发集和测试集上都取得了具有竞争力的词错误率（WER）表现。

Conclusion: 我们的方法在挑战排名中获得了第二名，证明了其在多语言对话语音识别任务中的有效性。

Abstract: This paper describes our Triple X speech recognition system submitted to Task
1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)
Challenge. Our work focuses on optimizing speech recognition accuracy in
multilingual conversational scenarios through an innovative encoder-adapter-LLM
architecture. This framework harnesses the powerful reasoning capabilities of
text-based large language models while incorporating domain-specific
adaptations. To further enhance multilingual recognition performance, we
adopted a meticulously designed multi-stage training strategy leveraging
extensive multilingual audio datasets. Experimental results demonstrate that
our approach achieves competitive Word Error Rate (WER) performance on both dev
and test sets, obtaining second place in the challenge ranking.

</details>


### [16] [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)
*Zhili Shen,Chenxin Diao,Pascual Merita,Pavlos Vougiouklis,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本文研究了基于图的RAG方法GeAR在不同数据集上的适用性，并评估了其在SIGIR 2025 LiveRAG挑战中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的方法通常针对特定任务设计，缺乏在更广泛数据集上的适用性证据。

Method: 本文研究了基于图的检索增强生成方法，特别是GeAR，并评估其在不同数据集上的适用性。

Result: 本文分析了GeAR在SIGIR 2025 LiveRAG挑战中的表现，并探讨了其潜在的限制。

Conclusion: 本文旨在适应最先进的基于图的RAG解决方案GeAR，并探索其在SIGIR 2025 LiveRAG挑战中的性能和局限性。

Abstract: Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.

</details>


### [17] [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)
*Carlotta Quensel,Neele Falk,Gabriella Lapesa*

Main category: cs.CL

TL;DR: 本文通过回归分析研究了主观因素对论证质量的影响，发现讲故事和模糊表达对客观和主观论证质量有不同的影响，而情绪的影响取决于其修辞使用而非领域。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对主观特征与论证强度之间关系的大规模分析，因此本文旨在填补这一空白。

Method: 本文通过回归分析来量化主观因素（情绪、讲故事和模糊表达）对两个标准数据集的影响，这两个数据集被注释为客观论证质量和主观说服力。

Result: 本文的结果显示，讲故事和模糊表达对客观和主观论证质量有不同的影响，而情绪的影响取决于其修辞使用而非领域。

Conclusion: 本文的结论是，讲故事和模糊表达对客观和主观论证质量有不同的影响，而情绪的影响取决于其修辞使用而非领域。

Abstract: In assessing argument strength, the notions of what makes a good argument are
manifold. With the broader trend towards treating subjectivity as an asset and
not a problem in NLP, new dimensions of argument quality are studied. Although
studies on individual subjective features like personal stories exist, there is
a lack of large-scale analyses of the relation between these features and
argument strength. To address this gap, we conduct regression analysis to
quantify the impact of subjective factors $-$ emotions, storytelling, and
hedging $-$ on two standard datasets annotated for objective argument quality
and subjective persuasion. As such, our contribution is twofold: at the level
of contributed resources, as there are no datasets annotated with all studied
dimensions, this work compares and evaluates automated annotation methods for
each subjective feature. At the level of novel insights, our regression
analysis uncovers different patterns of impact of subjective features on the
two facets of argument strength encoded in the datasets. Our results show that
storytelling and hedging have contrasting effects on objective and subjective
argument quality, while the influence of emotions depends on their rhetoric
utilization rather than the domain.

</details>


### [18] [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
*Shiting Chen,Zijian Zhao,Jinsong Chen*

Main category: cs.CL

TL;DR: 本文提出了Confident RAG方法，通过多次生成并选择高置信度响应来提高RAG的效果。


<details>
  <summary>Details</summary>
Motivation: 解决由于训练数据和模型架构异质性导致的嵌入模型在不同领域表现差异的问题。

Method: 提出并检验了两种方法来增强RAG，即Mixture-Embedding RAG和Confident RAG。

Result: Confident RAG 在平均上比普通LLM和RAG分别提高了约10%和5%。

Conclusion: Confident RAG 是一种高效的即插即用方法，适用于各种领域。

Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.

</details>


### [19] [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)
*Alexander R. Fabbri,Diego Mares,Jorge Flores,Meher Mankikar,Ernesto Hernandez,Dean Lee,Bing Liu,Chen Xing*

Main category: cs.CL

TL;DR: 本文介绍了MultiNRC基准，用于评估大型语言模型在多种语言和文化背景下的推理能力，并发现当前模型在原生多语言推理和文化相关知识方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言推理基准通常通过翻译英语推理基准构建，偏向于英语语言/文化背景的推理问题。需要一个更全面的基准来评估模型在不同语言和文化背景下的推理能力。

Method: 引入了Multilingual Native Reasoning Challenge (MultiNRC)基准，评估大型语言模型在多种语言和文化背景下的推理能力。

Result: 14个领先的大型语言模型在MultiNRC及其英文等价集上的系统评估显示，模型在原生多语言推理方面表现不佳，且在处理语言、文化和逻辑推理任务时表现出不同的优缺点。

Conclusion: 当前大型语言模型在原生多语言推理方面仍表现不佳，且在文化相关知识方面存在持续挑战。

Abstract: Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.

</details>


### [20] [Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice](https://arxiv.org/abs/2507.17527)
*Shanbo Cheng,Yu Bao,Zhichao Huang,Yu Lu,Ningxin Peng,Lu Xu,Runsheng Yu,Rong Cao,Ting Han,Zeyang Li,Sitong Liu,Shengtao Ma,Shiguang Pan,Jiongchen Xiao,Nuo Xu,Meng Yang,Rong Ye,Yiming Yu,Ruofei Zhang,Wanyi Zhang,Wenhao Zhu,Liehao Zou,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-LiveInterpret 2.0 is an advanced end-to-end simultaneous interpretation model that improves translation quality and reduces latency, offering a practical solution for real-time speech-to-speech generation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges in automatic simultaneous interpretation, such as subpar transcription and translation quality, lack of real-time speech generation, multi-speaker confusion, and translated speech inflation.

Method: The study introduces Seed-LiveInterpret 2.0, an end-to-end SI model that uses large-scale pretraining and reinforcement learning to achieve high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning capabilities.

Result: Experimental results show that Seed-LiveInterpret 2.0 achieves a better balance between translation accuracy and latency, with human interpreters validating over 70% correctness in complex scenarios. It also outperforms commercial SI solutions in translation quality while reducing average latency significantly.

Conclusion: Seed-LiveInterpret 2.0 demonstrates significant improvements in translation quality and latency, making it a practical solution for simultaneous interpretation.

Abstract: Simultaneous Interpretation (SI) represents one of the most daunting
frontiers in the translation industry, with product-level automatic systems
long plagued by intractable challenges: subpar transcription and translation
quality, lack of real-time speech generation, multi-speaker confusion, and
translated speech inflation, especially in long-form discourses. In this study,
we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers
high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning
capabilities. As a fully operational product-level solution, Seed-LiveInterpret
2.0 tackles these challenges head-on through our novel duplex speech-to-speech
understanding-generating framework. Experimental results demonstrate that
through large-scale pretraining and reinforcement learning, the model achieves
a significantly better balance between translation accuracy and latency,
validated by human interpreters to exceed 70% correctness in complex scenarios.
Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by
significant margins in translation quality, while slashing the average latency
of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is
around a near 70% reduction that drastically enhances practical usability.

</details>


### [21] [Synthetic Voice Data for Automatic Speech Recognition in African Languages](https://arxiv.org/abs/2507.17578)
*Brian DeRenzi,Anna Dixon,Mohamed Aymane Farhi,Christian Resch*

Main category: cs.CL

TL;DR: 本文评估了非洲语言的大规模合成语音语料库，并展示了其在自动语音识别中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 非洲有超过2300种语言，但语音技术对大多数语言来说仍然难以触及。因此，需要一种系统的方法来评估大规模合成语音语料库。

Method: 本文采用了一个三步流程：LLM驱动的文本创建、TTS语音合成和ASR微调。

Result: 八种语言的合成文本可读性评分高于5/7。对于三种语言（豪萨语、多洛语、奇切瓦语），我们创建了超过2500小时的合成语音数据，成本仅为真实数据的1%以下。微调后的Wav2Vec-BERT-2.0模型在250小时真实和250小时合成豪萨语数据上达到了与500小时真实数据基线相当的性能。

Conclusion: 本文展示了合成语音数据在非洲语言自动语音识别中的潜力，并强调了进一步研究的重要性。

Abstract: Speech technology remains out of reach for most of the over 2300 languages in
Africa. We present the first systematic assessment of large-scale synthetic
voice corpora for African ASR. We apply a three-step process: LLM-driven text
creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages
for which we create synthetic text achieved readability scores above 5 out of
7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created
more than 2,500 hours of synthetic voice data at below 1% of the cost of real
data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h
synthetic Hausa matched a 500h real-data-only baseline, while 579h real and
450h to 993h synthetic data created the best performance. We also present
gender-disaggregated ASR performance evaluation. For very low-resource
languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2
real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on
some evaluation data, but not on others. Investigating intercoder reliability,
ASR errors and evaluation datasets revealed the need for more robust reviewer
protocols and more accurate evaluation data. All data and models are publicly
released to invite further work to improve synthetic data for African
languages.

</details>


### [22] [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)
*Bowen Zheng,Ming Ma,Zhongqiao Lin,Tianming Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为SPADE的新解码方法，通过优化中间层表示与输出层的对齐，以及基于熵的置信度指标，实现高效且准确的早期退出算法。


<details>
  <summary>Details</summary>
Motivation: 由于中间层和输出层表示之间的不对齐导致解码不准确，现有方法性能较差。

Method: 提出了一种新的解码方法SPADE，通过传播仅包含起始标记和答案标记的最小化序列来对齐中间层表示与输出层。进一步优化了早期退出决策过程，通过训练SPADE的线性近似来计算基于熵的置信度指标。

Result: 该方法在降低推理成本的同时保持了准确性，提供了一种可扩展和高效的解决方案。

Conclusion: 该方法显著降低了推理成本，同时不损害准确性，为在现实应用中部署大型语言模型提供了可扩展和高效的解决方案。

Abstract: Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.

</details>


### [23] [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
*Changxin Tian,Jiapeng Wang,Qian Zhao,Kunlong Chen,Jia Liu,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文提出了 WSM 框架，通过将学习率衰减与模型合并联系起来，提供了一种统一的理论基础，以模拟各种衰减策略。实验结果表明，WSM 在多个基准测试中优于现有的 WSD 方法，并且在监督微调场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 最近的学习率调度进展表明，消除传统衰减阶段的无衰减方法在保持竞争力性能方面是有效的。模型合并技术在这个领域中已成为特别有希望的解决方案。

Method: 我们提出了 Warmup-Stable and Merge (WSM)，这是一个通用框架，建立了学习率衰减和模型合并之间的正式联系。WSM 提供了一个统一的理论基础，用于模拟各种衰减策略，同时与多种优化方法完全兼容。

Result: 通过广泛的实验，我们确定了合并持续时间（检查点聚合的训练窗口）是影响模型性能的最关键因素，超过了检查点间隔和合并数量的重要性。我们的框架在多个基准测试中表现优于 WSD 方法，分别在 MATH 上提高了 3.5%，在 HumanEval 上提高了 2.9%，在 MMLU-Pro 上提高了 5.5%。

Conclusion: 我们的框架在多个基准测试中 consistently 超过广泛采用的 Warmup-Stable-Decay (WSD) 方法，实现了显著的改进。性能优势扩展到监督微调场景，突显了 WSM 在长期模型优化中的潜力。

Abstract: Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.

</details>


### [24] [Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries](https://arxiv.org/abs/2507.17636)
*Victor Hartman,Petter Törnberg*

Main category: cs.CL

TL;DR: 该研究利用大型语言模型对1800万条推文进行分析，发现执政党和极端政党在负面竞选上的差异。


<details>
  <summary>Details</summary>
Motivation: 现有的分类方法成本高且可扩展性有限，因此需要一种新的方法来研究负面竞选。

Method: 该研究引入了零样本大型语言模型（LLMs）作为跨语言分类负面竞选的新方法，并利用这一方法进行了最大规模的跨国研究。

Result: LLMs在十种语言的基准数据集上表现出与母语者相当的性能，并优于传统的监督机器学习方法。研究还发现，执政党较少使用负面信息，而意识形态极端和民粹主义政党，尤其是右翼激进政党，表现出更高的负面程度。

Conclusion: 该研究展示了大型语言模型在跨语言分类负面竞选方面的潜力，能够实现可扩展、透明和可复制的政治传播研究。

Abstract: Negative campaigning is a central feature of political competition, yet
empirical research has been limited by the high cost and limited scalability of
existing classification methods. This study makes two key contributions. First,
it introduces zero-shot Large Language Models (LLMs) as a novel approach for
cross-lingual classification of negative campaigning. Using benchmark datasets
in ten languages, we demonstrate that LLMs achieve performance on par with
native-speaking human coders and outperform conventional supervised machine
learning approaches. Second, we leverage this novel method to conduct the
largest cross-national study of negative campaigning to date, analyzing 18
million tweets posted by parliamentarians in 19 European countries between 2017
and 2022. The results reveal consistent cross-national patterns: governing
parties are less likely to use negative messaging, while ideologically extreme
and populist parties -- particularly those on the radical right -- engage in
significantly higher levels of negativity. These findings advance our
understanding of how party-level characteristics shape strategic communication
in multiparty systems. More broadly, the study demonstrates the potential of
LLMs to enable scalable, transparent, and replicable research in political
communication across linguistic and cultural contexts.

</details>


### [25] [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)
*Changxin Tian,Kunlong Chen,Jia Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文提出了效率杠杆（EL）来衡量MoE模型相对于密集模型的计算优势，并通过大规模实验验证了MoE架构配置与EL之间的关系，最终设计并验证了一个高效的MoE模型。


<details>
  <summary>Details</summary>
Motivation: 解决预测给定MoE配置（如专家激活比和粒度）的模型容量这一未解问题。

Method: 引入了效率杠杆（EL）这一度量标准，通过大规模实证研究分析了MoE架构配置与EL之间的关系，并整合发现形成了统一的扩展定律。

Result: EL主要由专家激活比和总计算预算驱动，遵循可预测的幂律，而专家粒度则作为非线性调节器，具有明确的最佳范围。通过设计和训练Ling-mini-beta模型验证了扩展定律的准确性。

Conclusion: 本文为高效MoE模型的扩展提供了原理性和实证基础。

Abstract: Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large
Language Models (LLMs) efficiently by decoupling total parameters from
computational cost. However, this decoupling creates a critical challenge:
predicting the model capacity of a given MoE configurations (e.g., expert
activation ratio and granularity) remains an unresolved problem. To address
this gap, we introduce Efficiency Leverage (EL), a metric quantifying the
computational advantage of an MoE model over a dense equivalent. We conduct a
large-scale empirical study, training over 300 models up to 28B parameters, to
systematically investigate the relationship between MoE architectural
configurations and EL. Our findings reveal that EL is primarily driven by the
expert activation ratio and the total compute budget, both following
predictable power laws, while expert granularity acts as a non-linear modulator
with a clear optimal range. We integrate these discoveries into a unified
scaling law that accurately predicts the EL of an MoE architecture based on its
configuration. To validate our derived scaling laws, we designed and trained
Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active
parameters, alongside a 6.1B dense model for comparison. When trained on an
identical 1T high-quality token dataset, Ling-mini-beta matched the performance
of the 6.1B dense model while consuming over 7x fewer computational resources,
thereby confirming the accuracy of our scaling laws. This work provides a
principled and empirically-grounded foundation for the scaling of efficient MoE
models.

</details>


### [26] [TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa](https://arxiv.org/abs/2507.17709)
*Parker Riley,Siamak Shakeri,Waleed Ammar,Jonathan H. Clark*

Main category: cs.CL

TL;DR: 我们提出了一个大规模的问答数据集TyDi QA-WANA，用于评估模型在回答问题时利用大文本上下文的能力，并避免文化相关性问题。


<details>
  <summary>Details</summary>
Motivation: 为了评估模型在回答问题时利用大文本上下文的能力，以及避免文化相关性问题，我们创建了一个大规模的问答数据集。

Method: 我们设计了数据收集过程，以引发信息寻求问题，提问者真正好奇答案。每个问题都与一篇文章配对，这篇文章可能包含或不包含答案。数据是在每种语言变体中直接收集的，而不是使用翻译，以避免文化相关性问题。

Result: 我们提出了TyDi QA-WANA数据集，并展示了两种基线模型的性能。

Conclusion: 我们提出了TyDi QA-WANA，这是一个问答数据集，包含28K个例子，分布在10种西亚和北非的语言变体中。我们展示了两种基线模型的性能，并发布我们的代码和数据以促进研究社区的进一步改进。

Abstract: We present TyDi QA-WANA, a question-answering dataset consisting of 28K
examples divided among 10 language varieties of western Asia and northern
Africa. The data collection process was designed to elicit information-seeking
questions, where the asker is genuinely curious to know the answer. Each
question in paired with an entire article that may or may not contain the
answer; the relatively large size of the articles results in a task suitable
for evaluating models' abilities to utilize large text contexts in answering
questions. Furthermore, the data was collected directly in each language
variety, without the use of translation, in order to avoid issues of cultural
relevance. We present performance of two baseline models, and release our code
and data to facilitate further improvement by the research community.

</details>


### [27] [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)
*Karen Zhou,John Giorgi,Pranav Mani,Peng Xu,Davis Liang,Chenhao Tan*

Main category: cs.CL

TL;DR: 本文提出了一种基于用户反馈的结构化检查表，用于评估AI生成的临床笔记，该方法在离线评估中表现出色，具有实际价值。


<details>
  <summary>Details</summary>
Motivation: AI生成的临床笔记在医疗保健中越来越常用，但评估其质量仍然具有挑战性，因为专家评审存在高主观性和有限的可扩展性。现有的自动化指标往往无法与现实中的医生偏好对齐。

Method: 我们提出了一种管道，将真实用户反馈系统地提炼成结构化的检查表以评估笔记。这些检查表设计为可解释、基于人类反馈，并可通过基于LLM的评估者执行。

Result: 使用来自21,000多个临床会诊的去标识数据，我们展示了我们的反馈衍生检查表在覆盖率、多样性和预测能力方面优于基线方法。广泛的实验确认了检查表对质量下降扰动的鲁棒性，以及与临床医生偏好的显著一致性。

Conclusion: 我们的反馈衍生检查表在离线评估中表现出色，具有鲁棒性、与临床医生偏好高度一致，并作为评估方法具有实际价值。

Abstract: AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.

</details>


### [28] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)
*Danny D. Leybzon,Shreyas Tirumala,Nishant Jain,Summer Gillen,Michael Jackson,Cameron McPhee,Jennifer Schmidt*

Main category: cs.CL

TL;DR: 本文介绍了基于AI的电话调查系统，该系统利用LLM、ASR和语音合成技术，能够提高调查完成率、减少中断率并提升受访者满意度。


<details>
  <summary>Details</summary>
Motivation: 随着语音启用的人工智能（AI）系统的兴起，定量调查研究人员可以使用一种新的数据收集方式：AI电话调查。通过使用AI进行电话访谈，研究人员可以在保持人类互动性和方法严谨性的同时扩展定量研究。

Method: 构建并测试了一个基于大型语言模型（LLM）、自动语音识别（ASR）和语音合成技术的AI系统，用于进行定量调查，并遵循研究最佳实践如问题顺序随机化、答案顺序随机化和精确措辞。

Result: 结果表明，较短的问卷和更响应的AI面试者可能在所有三个研究指标上都有所改善，包括调查完成率、中断率和受访者满意度。

Conclusion: AI电话调查系统在提高调查完成率、减少中断率和提高受访者满意度方面表现出色，尤其是在较短的问卷和更响应的AI面试者情况下。

Abstract: With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.

</details>


### [29] [Megrez2 Technical Report](https://arxiv.org/abs/2507.17728)
*Boxun Li,Yadong Li,Zhiyuan Li,Congyi Liu,Weilin Liu,Guowei Niu,Zheyue Tan,Haiyang Xu,Zhuyu Yao,Tao Yuan,Dong Zhou,Yueqing Zhuang,Bo Zhao,Guohao Dai,Yu Wang*

Main category: cs.CL

TL;DR: Megrez2是一种轻量级且高性能的语言模型架构，通过跨层专家共享机制和预门控路由实现高效部署，Megrez2-Preview模型在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了实现设备原生部署的轻量级且高性能的语言模型架构，Megrez2被提出。

Method: Megrez2引入了一种跨层专家共享机制，通过在相邻Transformer层之间复用专家模块来显著减少总参数数量，同时保持模型的大部分能力。它还结合了预门控路由，实现了内存高效的专家加载和更快的推理。

Result: Megrez2-Preview模型在广泛的任务中表现出与更大模型相当或更优的性能，包括语言理解、指令遵循、数学推理和代码生成。

Conclusion: Megrez2架构在准确性、效率和可部署性之间取得了平衡，使其成为资源受限应用的有力候选。

Abstract: We present Megrez2, a novel lightweight and high-performance language model
architecture optimized for device native deployment. Megrez2 introduces a novel
cross-layer expert sharing mechanism, which significantly reduces total
parameter count by reusing expert modules across adjacent transformer layers
while maintaining most of the model's capacity. It also incorporates pre-gated
routing, enabling memory-efficient expert loading and faster inference. As the
first instantiation of the Megrez2 architecture, we introduce the
Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and
further enhanced through supervised fine-tuning and reinforcement learning with
verifiable rewards. With only 3B activated and 7.5B stored parameters,
Megrez2-Preview demonstrates competitive or superior performance compared to
larger models on a wide range of tasks, including language understanding,
instruction following, mathematical reasoning, and code generation. These
results highlight the effectiveness of the Megrez2 architecture to achieve a
balance between accuracy, efficiency, and deployability, making it a strong
candidate for real-world, resource-constrained applications.

</details>


### [30] [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)
*Linbo Cao,Jinman Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种基于辩论的评估范式，通过多轮论证增加难度并惩罚浅层记忆，同时重用QA项目以减少整理工作量。我们提供了两个主要贡献：(1) 一种系统地将QA任务转换为基于辩论的评估的管道，以及(2) 一个公共基准，展示了我们的范式在MMLU-Pro问题子集上的有效性，包括标准化协议和参考模型。实证结果验证了方法的稳健性和对抗数据污染的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着前沿语言模型日益充斥标准QA基准，关于数据污染、记忆和不断上升的数据集创建成本的问题仍然存在。

Method: 我们提出了一个基于辩论的评估范式，将任何现有的QA数据集转换为结构化的对抗性辩论，其中一台模型被给予官方答案进行辩护，另一台构建并捍卫一个替代答案，由对正确解决方案一无所知的裁判模型进行裁决。

Result: 实证结果验证了该方法的稳健性和对抗数据污染的有效性——在测试问题上微调的Llama 3.1模型显示出显著的准确性提高（50% -> 82%），但在辩论中表现更差。结果还表明，即使较弱的裁判也能可靠地区分更强的辩手，这表明基于辩论的评估可以扩展到未来更强大的系统，同时保持创建新基准的一小部分成本。

Conclusion: 我们的框架强调了'在测试集上预训练不再是你需要的一切'，为衡量先进语言模型的真实推理能力提供了一条可持续的路径。

Abstract: As frontier language models increasingly saturate standard QA benchmarks,
concerns about data contamination, memorization, and escalating dataset
creation costs persist. We propose a debate-driven evaluation paradigm that
transforms any existing QA dataset into structured adversarial debates--where
one model is given the official answer to defend, and another constructs and
defends an alternative answer--adjudicated by a judge model blind to the
correct solution. By forcing multi-round argumentation, this approach
substantially increases difficulty while penalizing shallow memorization, yet
reuses QA items to reduce curation overhead. We make two main contributions:
(1) an evaluation pipeline to systematically convert QA tasks into debate-based
assessments, and (2) a public benchmark that demonstrates our paradigm's
effectiveness on a subset of MMLU-Pro questions, complete with standardized
protocols and reference models. Empirical results validate the robustness of
the method and its effectiveness against data contamination--a Llama 3.1 model
fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)
but performed worse in debates. Results also show that even weaker judges can
reliably differentiate stronger debaters, highlighting how debate-based
evaluation can scale to future, more capable systems while maintaining a
fraction of the cost of creating new benchmarks. Overall, our framework
underscores that "pretraining on the test set is no longer all you need,"
offering a sustainable path for measuring the genuine reasoning ability of
advanced language models.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [31] [Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature](https://arxiv.org/abs/2507.16820)
*Ngan Tran,Haihua Chen,Ana Cleveland,Yuhan Zhou*

Main category: cs.SI

TL;DR: 本研究通过计量分析和主题分析，揭示了灾害信息学领域在2020年1月至2022年9月期间的文献趋势，包括活跃国家、机构、作者、合作网络、新兴主题以及由于新冠疫情引发的研究重点变化。


<details>
  <summary>Details</summary>
Motivation: 为了识别灾害信息学文献中的活跃国家、机构、作者、合作网络、新兴主题以及由于新冠疫情而引发的研究重点变化。

Method: 利用大规模语料库和先进的技术，如预训练语言模型和生成式AI，进行文献的计量分析和主题分析。

Result: 发现疫情最严重的国家也是最活跃的，同一地区或有共同语言的国家和机构倾向于合作，顶级活跃作者通常与一两个关键合作伙伴形成紧密关系，作者通常专注于一两个特定主题，而机构则在多个主题上有更广泛的兴趣，并且新冠疫情影响了灾害信息学的研究重点，更加重视公共卫生。

Conclusion: 我们的分析为政策制定者、从业者和学者提供了战略见解，旨在增强在日益不确定和复杂的风险环境中的灾害信息学能力。

Abstract: This study presents a comprehensive bibliometric and topic analysis of the
disaster informatics literature published between January 2020 to September
2022. Leveraging a large-scale corpus and advanced techniques such as
pre-trained language models and generative AI, we identify the most active
countries, institutions, authors, collaboration networks, emergent topics,
patterns among the most significant topics, and shifts in research priorities
spurred by the COVID-19 pandemic. Our findings highlight (1) countries that
were most impacted by the COVID-19 pandemic were also among the most active,
with each country having specific research interests, (2) countries and
institutions within the same region or share a common language tend to
collaborate, (3) top active authors tend to form close partnerships with one or
two key partners, (4) authors typically specialized in one or two specific
topics, while institutions had more diverse interests across several topics,
and (5) the COVID-19 pandemic has influenced research priorities in disaster
informatics, placing greater emphasis on public health. We further demonstrate
that the field is converging on multidimensional resilience strategies and
cross-sectoral data-sharing collaborations or projects, reflecting a heightened
awareness of global vulnerability and interdependency. Collecting and quality
assurance strategies, data analytic practices, LLM-based topic extraction and
summarization approaches, and result visualization tools can be applied to
comparable datasets or solve similar analytic problems. By mapping out the
trends in disaster informatics, our analysis offers strategic insights for
policymakers, practitioners, and scholars aiming to enhance disaster
informatics capacities in an increasingly uncertain and complex risk landscape.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [32] [Pixels, Patterns, but No Poetry: To See The World like Humans](https://arxiv.org/abs/2507.16863)
*Hongcheng Gao,Zihao Huang,Lin Xu,Jingyi Tang,Xinhao Li,Yue Liu,Haoyang Li,Taihang Hu,Minhua Lin,Xinlong Yang,Ge Wu,Balong Bi,Hongyu Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文引入了一个名为Turing Eye Test的基准测试，用于评估MLLMs在感知任务上的表现，并发现当前模型在视觉塔泛化方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要集中在提高MLLMs的推理能力，但一个基本问题仍然存在：MLLMs能否像人类一样感知世界？

Method: 引入了Turing Eye Test (TET)，这是一个以感知为导向的基准测试，包含四个诊断任务，用于评估MLLMs在合成图像上的表现。

Result: 最先进的MLLMs在我们设计的感知任务上表现出灾难性的失败，而这些任务对人类来说是微不足道的。

Conclusion: 我们的基准测试揭示了当前MLLMs在视觉塔泛化方面存在关键差距，而不是语言主干的知识和推理能力。

Abstract: Achieving human-like perception and reasoning in Multimodal Large Language
Models (MLLMs) remains a central challenge in artificial intelligence. While
recent research has primarily focused on enhancing reasoning capabilities in
MLLMs, a fundamental question persists: Can Multimodal Large Language Models
truly perceive the world as humans do? This paper shifts focus from reasoning
to perception. Rather than constructing benchmarks specifically for reasoning,
we introduce the Turing Eye Test (TET), a challenging perception-oriented
benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on
synthetic images that humans process intuitively. Our findings reveal that
state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks
trivial for humans. Both in-context learning and training on language
backbone-effective for previous benchmarks-fail to improve performance on our
tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting
that our benchmark poses challenges for vision tower generalization rather than
for the knowledge and reasoning capabilities of the language backbone-a key gap
between current MLLMs and human perception. We release a representative subset
of TET tasks in this version, and will introduce more diverse tasks and methods
to enhance visual generalization in future work.

</details>


### [33] [ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension](https://arxiv.org/abs/2507.16877)
*Yizhi Hu,Zezhao Tian,Xingqun Qi,Chen Su,Bingkun Yang,Junhui Yin,Muyi Sun,Man Zhang,Zhenan Sun*

Main category: cs.CV

TL;DR: 本文提出了ReMeREC框架，通过结合视觉和文本线索以及引入TMP和EIR来提高多实体定位和关系预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理多实体场景中的复杂互实体关系时存在不足，同时缺乏高质量的数据集。

Method: 提出了一种新的框架ReMeREC，结合视觉和文本线索来定位多个实体并建模它们的相互关系。引入了Text-adaptive Multi-entity Perceptron (TMP)和Entity Inter-relationship Reasoner (EIR)。

Result: 在四个基准数据集上的实验表明，ReMeREC在多实体定位和关系预测方面表现优异。

Conclusion: ReMeREC在多实体定位和关系预测方面达到了最先进的性能，显著优于现有方法。

Abstract: Referring Expression Comprehension (REC) aims to localize specified entities
or regions in an image based on natural language descriptions. While existing
methods handle single-entity localization, they often ignore complex
inter-entity relationships in multi-entity scenes, limiting their accuracy and
reliability. Additionally, the lack of high-quality datasets with fine-grained,
paired image-text-relation annotations hinders further progress. To address
this challenge, we first construct a relation-aware, multi-entity REC dataset
called ReMeX, which includes detailed relationship and textual annotations. We
then propose ReMeREC, a novel framework that jointly leverages visual and
textual cues to localize multiple entities while modeling their
inter-relations. To address the semantic ambiguity caused by implicit entity
boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron
(TMP), which dynamically infers both the quantity and span of entities from
fine-grained textual cues, producing distinctive representations. Additionally,
our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and
global scene understanding. To further improve language comprehension for
fine-grained prompts, we also construct a small-scale auxiliary dataset,
EntityText, generated using large language models. Experiments on four
benchmark datasets show that ReMeREC achieves state-of-the-art performance in
multi-entity grounding and relation prediction, outperforming existing
approaches by a large margin.

</details>


### [34] [TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition](https://arxiv.org/abs/2507.17335)
*Guangzhu Xu,Zhi Ke,Pengcheng Zuo,Bangjun Lei*

Main category: cs.CV

TL;DR: 本文提出了一种针对单行和双行中国车牌的统一解决方案，结合了轻量级视觉编码器和文本解码器，并通过合成数据集和视角校正网络提高了识别准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决 CNN 和 CRNN 基础方法在车牌识别中的局限性，特别是在多样化的车牌类型和成像条件下。

Method: 本文提出了一种统一的解决方案，将轻量级视觉编码器与文本解码器集成在一个针对单行和双行中国车牌的预训练框架中。为了缓解双行车牌数据集的稀缺性，我们通过合成图像、应用纹理映射到真实场景并将其与真实车牌图像混合来构建单/双行车牌数据集。此外，为了提高系统的识别准确性，引入了一个视角校正网络（PTN），该网络使用车牌角坐标回归作为隐变量，由车牌视图分类信息监督。

Result: 该算法在粗定位干扰下的 CCPD 测试集上实现了 99.34% 的平均识别准确率，在细定位干扰下进一步提高到 99.58%。在双行车牌测试集上，平均识别准确率为 98.70%，处理速度高达每秒 167 帧。

Conclusion: 该算法在粗定位干扰下的 CCPD 测试集上实现了 99.34% 的平均识别准确率，在细定位干扰下进一步提高到 99.58%。在双行车牌测试集上，平均识别准确率为 98.70%，处理速度高达每秒 167 帧，显示出强大的实用性。

Abstract: License plate recognition in open environments is widely applicable across
various domains; however, the diversity of license plate types and imaging
conditions presents significant challenges. To address the limitations
encountered by CNN and CRNN-based approaches in license plate recognition, this
paper proposes a unified solution that integrates a lightweight visual encoder
with a text decoder, within a pre-training framework tailored for single and
double-line Chinese license plates. To mitigate the scarcity of double-line
license plate datasets, we constructed a single/double-line license plate
dataset by synthesizing images, applying texture mapping onto real scenes, and
blending them with authentic license plate images. Furthermore, to enhance the
system's recognition accuracy, we introduce a perspective correction network
(PTN) that employs license plate corner coordinate regression as an implicit
variable, supervised by license plate view classification information. This
network offers improved stability, interpretability, and low annotation costs.
The proposed algorithm achieves an average recognition accuracy of 99.34% on
the corrected CCPD test set under coarse localization disturbance. When
evaluated under fine localization disturbance, the accuracy further improves to
99.58%. On the double-line license plate test set, it achieves an average
recognition accuracy of 98.70%, with processing speeds reaching up to 167
frames per second, indicating strong practical applicability.

</details>


### [35] [URPO: A Unified Reward & Policy Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.17515)
*Songshuo Lu,Hua Wang,Zhi Chen,Yaohua Tang*

Main category: cs.CV

TL;DR: 本文提出了一种名为URPO的新框架，将指令遵循和奖励建模整合到一个模型和一个训练阶段中，通过统一的生成格式优化，提升了语言模型的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 大型对齐流水线通常将策略模型与单独训练的奖励模型配对，但这种分离导致了复杂的资源密集型流程，并由于静态奖励信号而受到性能限制。

Method: URPO框架将指令遵循（“玩家”）和奖励建模（“裁判”）统一在一个模型和一个训练阶段中。方法将所有对齐数据（包括偏好对、可验证的推理和开放性指令）转换为统一的生成格式，并通过单一的组相对策略优化（GRPO）循环进行优化。

Result: 在Qwen2.5-7B模型上的实验表明，URPO优于使用单独生成奖励模型的强基线。URPO显著提升了AlpacaEval上的指令遵循分数，从42.24提升到44.84，并将综合推理平均值从32.66提升到35.66。此外，URPO在训练过程中培养了一个优越的内部评估器，RewardBench得分达到85.15，超过了它所取代的专用奖励模型（83.55）。

Conclusion: URPO通过消除单独的奖励模型并促进生成和评估之间的共同进化动态，提供了一种更简单、更高效、更有效的途径，以实现稳健对齐的语言模型。

Abstract: Large-scale alignment pipelines typically pair a policy model with a
separately trained reward model whose parameters remain frozen during
reinforcement learning (RL). This separation creates a complex,
resource-intensive pipeline and suffers from a performance ceiling due to a
static reward signal. We propose a novel framework, Unified Reward & Policy
Optimization (URPO), that unifies instruction-following ("player") and reward
modeling ("referee") within a single model and a single training phase. Our
method recasts all alignment data-including preference pairs, verifiable
reasoning, and open-ended instructions-into a unified generative format
optimized by a single Group-Relative Policy Optimization (GRPO) loop. This
enables the model to learn from ground-truth preferences and verifiable logic
while simultaneously generating its own rewards for open-ended tasks.
Experiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified
model significantly outperforms a strong baseline using a separate generative
reward model, boosting the instruction-following score on AlpacaEval from 42.24
to 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,
URPO cultivates a superior internal evaluator as a byproduct of training,
achieving a RewardBench score of 85.15 and surpassing the dedicated reward
model it replaces (83.55). By eliminating the need for a separate reward model
and fostering a co-evolutionary dynamic between generation and evaluation, URPO
presents a simpler, more efficient, and more effective path towards robustly
aligned language models.

</details>


### [36] [Dual-branch Prompting for Multimodal Machine Translation](https://arxiv.org/abs/2507.17588)
*Jie Wang,Zhendong Yang,Liansong Zong,Xiaobo Zhang,Dexian Wang,Ji Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散的双分支提示框架D2P-MMT，用于增强视觉引导翻译的鲁棒性。该方法利用预训练扩散模型生成的重建图像，减少干扰的视觉细节，同时保留语义线索，并通过双分支提示策略和分布对齐损失提升翻译性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MMT方法通常依赖于配对的图像-文本输入，并且对无关的视觉噪声敏感，这限制了它们的鲁棒性和实用性。因此，需要一种更鲁棒的视觉引导翻译方法。

Method: D2P-MMT是一种基于扩散的双分支提示框架，它利用预训练扩散模型生成的重建图像，结合源文本进行视觉引导翻译。在训练过程中，模型通过双分支提示策略同时从真实和重建图像中学习，以促进丰富的跨模态交互。此外，引入了分布对齐损失来弥合模态差距并减轻训练与推理之间的差异。

Result: D2P-MMT在Multi30K数据集上取得了优于现有最先进方法的翻译性能。

Conclusion: D2P-MMT在Multi30K数据集上的实验结果表明，其翻译性能优于现有的最先进方法。

Abstract: Multimodal Machine Translation (MMT) typically enhances text-only translation
by incorporating aligned visual features. Despite the remarkable progress,
state-of-the-art MMT approaches often rely on paired image-text inputs at
inference and are sensitive to irrelevant visual noise, which limits their
robustness and practical applicability. To address these issues, we propose
D2P-MMT, a diffusion-based dual-branch prompting framework for robust
vision-guided translation. Specifically, D2P-MMT requires only the source text
and a reconstructed image generated by a pre-trained diffusion model, which
naturally filters out distracting visual details while preserving semantic
cues. During training, the model jointly learns from both authentic and
reconstructed images using a dual-branch prompting strategy, encouraging rich
cross-modal interactions. To bridge the modality gap and mitigate
training-inference discrepancies, we introduce a distributional alignment loss
that enforces consistency between the output distributions of the two branches.
Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves
superior translation performance compared to existing state-of-the-art
approaches.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [37] [Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs](https://arxiv.org/abs/2507.17259)
*Eyal German,Sagiv Antebi,Daniel Samira,Asaf Shabtai,Yuval Elovici*

Main category: cs.CR

TL;DR: This paper introduces Tab-MIA, a benchmark dataset for evaluating membership inference attacks (MIAs) on tabular data in large language models (LLMs). The study shows that LLMs can memorize tabular data in different encoding formats, making them vulnerable to MIAs, even after minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Privacy risks arise since sensitive records can be inadvertently retained by the model and exposed through data extraction or membership inference attacks (MIAs). Existing MIA methods primarily target textual content, but their efficacy and threat implications may differ when applied to structured data.

Method: We present Tab-MIA, a benchmark dataset for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used. Using our Tab-MIA benchmark, we conduct the first evaluation of state-of-the-art MIA methods on LLMs finetuned with tabular data across multiple encoding formats.

Result: LLMs memorize tabular data in ways that vary across encoding formats, making them susceptible to extraction via MIAs. Even when fine-tuned for as few as three epochs, models exhibit high vulnerability, with AUROC scores approaching 90% in most cases.

Conclusion: Tab-MIA enables systematic evaluation of these risks and provides a foundation for developing privacy-preserving methods for tabular data in LLMs.

Abstract: Large language models (LLMs) are increasingly trained on tabular data, which,
unlike unstructured text, often contains personally identifiable information
(PII) in a highly structured and explicit format. As a result, privacy risks
arise, since sensitive records can be inadvertently retained by the model and
exposed through data extraction or membership inference attacks (MIAs). While
existing MIA methods primarily target textual content, their efficacy and
threat implications may differ when applied to structured data, due to its
limited content, diverse data types, unique value distributions, and
column-level semantics. In this paper, we present Tab-MIA, a benchmark dataset
for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used.
Tab-MIA comprises five data collections, each represented in six different
encoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation
of state-of-the-art MIA methods on LLMs finetuned with tabular data across
multiple encoding formats. In the evaluation, we analyze the memorization
behavior of pretrained LLMs on structured data derived from Wikipedia tables.
Our findings show that LLMs memorize tabular data in ways that vary across
encoding formats, making them susceptible to extraction via MIAs. Even when
fine-tuned for as few as three epochs, models exhibit high vulnerability, with
AUROC scores approaching 90% in most cases. Tab-MIA enables systematic
evaluation of these risks and provides a foundation for developing
privacy-preserving methods for tabular data in LLMs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [38] [A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models](https://arxiv.org/abs/2507.16826)
*Qikai Wei,Huansheng Ning,Chunlong Han,Jianguo Ding*

Main category: cs.IR

TL;DR: 本文提出了一种查询感知多路径知识图谱融合方法（QMKGF），以提高检索增强生成的性能。通过构建知识图谱并采用多路径子图构造策略，以及查询感知注意力奖励模型，显著提升了语义相关性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有RAG研究主要集中在使用基于相似性的匹配方法检索孤立段落，而忽视了它们之间的内在联系。这种限制阻碍了RAG任务的性能。

Method: 我们设计了提示模板并使用通用大语言模型提取实体和关系，从而高效生成知识图谱（KG）。基于构建的KG，我们引入了一种多路径子图构造策略，结合了一跳关系、多跳关系和基于重要性的关系，旨在提高检索文档与用户查询之间的语义相关性。随后，我们设计了一个查询感知注意力奖励模型，根据其与查询的语义相关性对子图三元组进行评分。然后，我们选择最高得分的子图，并从与其他高度语义相关的子图中添加额外的三元组。最后，利用更新后的子图中的实体、关系和三元组扩展原始查询，从而增强其语义表示并提高大语言模型的生成质量。

Result: 在HotpotQA数据集上，我们的方法实现了64.98%的ROUGE-1分数，超过了BGE-Rerank方法9.72个百分点（从55.26%到64.98%）。

Conclusion: 实验结果表明QMKGF方法的有效性和优越性。

Abstract: Retrieval Augmented Generation (RAG) has gradually emerged as a promising
paradigm for enhancing the accuracy and factual consistency of content
generated by large language models (LLMs). However, existing RAG studies
primarily focus on retrieving isolated segments using similarity-based matching
methods, while overlooking the intrinsic connections between them. This
limitation hampers performance in RAG tasks. To address this, we propose QMKGF,
a Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing
Retrieval Augmented Generation. First, we design prompt templates and employ
general-purpose LLMs to extract entities and relations, thereby generating a
knowledge graph (KG) efficiently. Based on the constructed KG, we introduce a
multi-path subgraph construction strategy that incorporates one-hop relations,
multi-hop relations, and importance-based relations, aiming to improve the
semantic relevance between the retrieved documents and the user query.
Subsequently, we designed a query-aware attention reward model that scores
subgraph triples based on their semantic relevance to the query. Then, we
select the highest score subgraph and enrich subgraph with additional triples
from other subgraphs that are highly semantically relevant to the query.
Finally, the entities, relations, and triples within the updated subgraph are
utilised to expand the original query, thereby enhancing its semantic
representation and improving the quality of LLMs' generation. We evaluate QMKGF
on the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA
dataset, our method achieves a ROUGE-1 score of 64.98\%, surpassing the
BGE-Rerank approach by 9.72 percentage points (from 55.26\% to 64.98\%).
Experimental results demonstrate the effectiveness and superiority of the QMKGF
approach.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [SiLQ: Simple Large Language Model Quantization-Aware Training](https://arxiv.org/abs/2507.16933)
*Steven K. Esser,Jeffrey L. McKinstry,Deepika Bablani,Rathinakumar Appuswamy,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 该研究提出了一种高效的量化感知训练方法，能够在极小的训练预算增加下，显著提升模型的量化性能。


<details>
  <summary>Details</summary>
Motivation: 为了减少推理时间延迟、模型大小和能耗，同时保持模型的准确性，需要一种高效的量化方法。然而，现有的方法在准确性和兼容性方面存在挑战。

Method: 该研究提出了一种量化感知训练方法，通过在训练过程中引入量化机制，使模型在量化后仍能保持较高的准确性。

Result: 该方法在多个现代基准测试中表现出色，无论是基础模型还是指令模型变体都取得了显著的性能提升。

Conclusion: 该研究展示了一种简单且端到端的量化感知训练方法，能够在不增加额外操作的情况下，以极小的训练预算增加，显著优于现有的量化方法。

Abstract: Large language models can be quantized to reduce inference time latency,
model size, and energy consumption, thereby delivering a better user experience
at lower cost. A challenge exists to deliver quantized models with minimal loss
of accuracy in reasonable time, and in particular to do so without requiring
mechanisms incompatible with specialized inference accelerators. Here, we
demonstrate a simple, end-to-end quantization-aware training approach that,
with an increase in total model training budget of less than 0.1%, outperforms
the leading published quantization methods by large margins on several modern
benchmarks, with both base and instruct model variants. The approach easily
generalizes across different model architectures, can be applied to
activations, cache, and weights, and requires the introduction of no additional
operations to the model other than the quantization itself.

</details>


### [40] [DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD](https://arxiv.org/abs/2507.17501)
*Xianbiao Qi,Marco Chen,Wenjie Xiao,Jiaquan Ye,Yelin He,Chun-Guang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: This paper introduces a Deeply Normalized Transformer (DNT) that enables seamless training with vanilla mSGDW while achieving comparable performance to Transformers trained via AdamW by integrating normalization techniques.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of Transformers requiring advanced optimizers like AdamW due to heavy-tailed gradient distributions, enabling seamless training with vanilla mSGDW.

Method: Integrating normalization techniques at proper positions in the Transformers to modulate the Jacobian matrices of each layer, balance the influence of weights, activations, and their interactions, and concentrate the distributions of gradients.

Result: DNT outperforms its counterparts (e.g., ViT and GPT) and can be effectively trained with vanilla mSGDW.

Conclusion: DNT can be effectively trained with vanilla mSGDW while achieving comparable performance to Transformers trained via AdamW.

Abstract: Transformers have become the de facto backbone of modern deep learning, yet
their training typically demands an advanced optimizer with adaptive learning
rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that
it is mainly due to a heavy-tailed distribution of the gradients. In this
paper, we introduce a Deeply Normalized Transformer (DNT), which is
meticulously engineered to overcome this limitation enabling seamless training
with vanilla mSGDW while yielding comparable performance to the Transformers
trained via AdamW. To be specific, in DNT, we strategically integrate
normalization techniques at proper positions in the Transformers to effectively
modulate the Jacobian matrices of each layer, balance the influence of weights,
activations, and their interactions, and thus enable the distributions of
gradients concentrated. We provide both theoretical justifications of the
normalization technique used in our DNT and extensive empirical evaluation on
two popular Transformer architectures to validate that: a) DNT outperforms its
counterparts (\ie, ViT and GPT), and b) DNT can be effectively trained with
vanilla mSGDW.

</details>


### [41] [Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains](https://arxiv.org/abs/2507.17746)
*Anisha Gunjal,Anthony Wang,Elaine Lau,Vaskar Nath,Bing Liu,Sean Hendryx*

Main category: cs.LG

TL;DR: RaR框架利用结构化的评分标准作为可解释的奖励信号，提升了小规模模型的表现并使其更符合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 在现实任务中，平衡客观和主观评估标准是扩展强化学习的关键，但许多任务缺乏明确的地面真实数据，使得定义可靠的奖励信号变得困难。传统基于偏好的方法依赖于难以解释的奖励函数，容易产生虚假相关性。

Method: RaR框架使用结构化、清单式的评分标准作为可解释的奖励信号，用于基于GRPO的策略训练。

Result: RaR的最佳方法在HealthBench-1k上相对于简单的Likert方法提高了28%的相对性能，并且与专家编写的参考奖励信号相当或更好。

Conclusion: RaR框架通过将评分标准作为结构化的奖励信号，使较小规模的判断模型更好地与人类偏好对齐，并在不同模型规模下保持稳健性能。

Abstract: Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world
tasks often requires balancing objective and subjective evaluation criteria.
However, many such tasks lack a single, unambiguous ground truth-making it
difficult to define reliable reward signals for post-training language models.
While traditional preference-based methods offer a workaround, they rely on
opaque reward functions that are difficult to interpret and prone to spurious
correlations. We introduce $\textbf{Rubrics as Rewards}$ (RaR), a framework
that uses structured, checklist-style rubrics as interpretable reward signals
for on-policy training with GRPO. Our best RaR method yields up to a $28\%$
relative improvement on HealthBench-1k compared to simple Likert-based
approaches, while matching or surpassing the performance of reward signals
derived from expert-written references. By treating rubrics as structured
reward signals, we show that RaR enables smaller-scale judge models to better
align with human preferences and sustain robust performance across model
scales.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [42] [BoSS: Beyond-Semantic Speech](https://arxiv.org/abs/2507.17563)
*Qing Wang,Zehan Li,Hang Lv,Hongjie Chen,Yaodong Song,Jian Kang,Jie Lian,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.SD

TL;DR: 本文介绍了Spoken Interaction System Capability Levels (L1-L5)和Beyond-Semantic Speech (BoSS)的概念，提出了一种形式化的BoSS框架，并评估了其在五个维度上的表现，指出当前语音语言模型在处理超越语义的信号方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现代语音技术（如自动语音识别和文本转语音）往往无法捕捉这些超越语义的维度。为了更好地表征和基准测试语音智能的进步，我们引入了Spoken Interaction System Capability Levels (L1-L5)，这是一个层次化框架，展示了语音对话系统从基本命令识别到类人社交互动的演变。

Method: 我们提出了Beyond-Semantic Speech (BoSS)，它指的是语音交流中涵盖但超越明确语义的信息集。通过情感线索、情境动态和隐含语义等多维特征来传达情感、情境，并修改或扩展意义。我们呈现了一个形式化的BoSS框架，利用认知相关性理论和机器学习模型来分析时间性和情境性语音动态。

Result: 我们在五个不同的维度上评估了与BoSS相关的属性，结果表明当前的语音语言模型（SLMs）很难完全解释超越语义的信号。

Conclusion: 这些发现突显了推进BoSS研究的必要性，以实现更丰富、更具上下文意识的人机通信。

Abstract: Human communication involves more than explicit semantics, with implicit
signals and contextual cues playing a critical role in shaping meaning.
However, modern speech technologies, such as Automatic Speech Recognition (ASR)
and Text-to-Speech (TTS) often fail to capture these beyond-semantic
dimensions. To better characterize and benchmark the progression of speech
intelligence, we introduce Spoken Interaction System Capability Levels (L1-L5),
a hierarchical framework illustrated the evolution of spoken dialogue systems
from basic command recognition to human-like social interaction. To support
these advanced capabilities, we propose Beyond-Semantic Speech (BoSS), which
refers to the set of information in speech communication that encompasses but
transcends explicit semantics. It conveys emotions, contexts, and modifies or
extends meanings through multidimensional features such as affective cues,
contextual dynamics, and implicit semantics, thereby enhancing the
understanding of communicative intentions and scenarios. We present a
formalized framework for BoSS, leveraging cognitive relevance theories and
machine learning models to analyze temporal and contextual speech dynamics. We
evaluate BoSS-related attributes across five different dimensions, reveals that
current spoken language models (SLMs) are hard to fully interpret
beyond-semantic signals. These findings highlight the need for advancing BoSS
research to enable richer, more context-aware human-machine communication.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [43] [Towards Robust Speech Recognition for Jamaican Patois Music Transcription](https://arxiv.org/abs/2507.16834)
*Jordan Madden,Matthew Stone,Dimitri Johnson,Daniel Geddez*

Main category: eess.AS

TL;DR: 本研究通过整理超过40小时的手动转录的牙买加帕托斯音乐数据集，微调先进的自动语音识别模型，以提高对牙买加帕托斯音乐的识别性能，并探索Whisper模型在该语言上的性能扩展定律。


<details>
  <summary>Details</summary>
Motivation: 尽管牙买加帕托斯是一种广泛使用的语言，但当前的语音识别系统在帕托斯音乐上的表现不佳，产生的不准确字幕限制了可访问性并阻碍了下游应用。

Method: 我们通过整理超过40小时的手动转录的牙买加帕托斯音乐来解决这个问题，并使用这个数据集微调最先进的自动语音识别（ASR）模型，以开发Whisper模型在牙买加帕托斯音频上的性能扩展定律。

Result: 我们整理了超过40小时的手动转录的牙买加帕托斯音乐数据集，并用它来微调最先进的自动语音识别（ASR）模型，以开发Whisper模型在牙买加帕托斯音频上的性能扩展定律。

Conclusion: 我们希望这项工作能对牙买加帕托斯音乐的可访问性和牙买加帕托斯语言建模的未来产生积极影响。

Abstract: Although Jamaican Patois is a widely spoken language, current speech
recognition systems perform poorly on Patois music, producing inaccurate
captions that limit accessibility and hinder downstream applications. In this
work, we take a data-centric approach to this problem by curating more than 40
hours of manually transcribed Patois music. We use this dataset to fine-tune
state-of-the-art automatic speech recognition (ASR) models, and use the results
to develop scaling laws for the performance of Whisper models on Jamaican
Patois audio. We hope that this work will have a positive impact on the
accessibility of Jamaican Patois music and the future of Jamaican Patois
language modeling.

</details>


### [44] [Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI Interview Systems](https://arxiv.org/abs/2507.16835)
*Nima Yazdani,Ali Ansari,Aruj Mahajan,Amirhossein Afsharrad,Seyed Shahabeddin Mousavi*

Main category: eess.AS

TL;DR: 本研究通过大规模实证比较了STT x LLM x TTS堆栈，并开发了一个自动化评估框架，使用LLM作为裁判来评估对话质量、技术准确性和技能评估能力。结果显示，Google STT与GPT-4.1在对话质量和专业技术指标上表现最佳，但客观质量指标与用户满意度评分的相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 语音基础的对话AI系统越来越多地依赖于结合语音到文本（STT）、大型语言模型（LLM）和文本到语音（TTS）组件的级联架构。然而，在生产环境中对不同组件组合的系统评估仍然研究不足。

Method: 我们开发了一个自动化评估框架，使用LLM作为裁判来评估对话质量、技术准确性和技能评估能力。

Result: 我们的分析显示，Google STT与GPT-4.1在对话质量和专业技术指标上显著优于其他选项。令人惊讶的是，我们发现客观质量指标与用户满意度评分之间的相关性较弱，这表明语音基础的AI系统的用户体验取决于技术性能之外的因素。

Conclusion: 我们的研究结果为选择多模态对话人工智能系统的组件提供了实际指导，并为语音交互贡献了一个经过验证的评估方法。

Abstract: Voice-based conversational AI systems increasingly rely on cascaded
architectures combining speech-to-text (STT), large language models (LLMs), and
text-to-speech (TTS) components. However, systematic evaluation of different
component combinations in production settings remains understudied. We present
a large-scale empirical comparison of STT x LLM x TTS stacks using data from
over 300,000 AI-conducted job interviews. We develop an automated evaluation
framework using LLM-as-a-Judge to assess conversational quality, technical
accuracy, and skill assessment capabilities. Our analysis of four production
configurations reveals that Google STT paired with GPT-4.1 significantly
outperforms alternatives in both conversational and technical quality metrics.
Surprisingly, we find that objective quality metrics correlate weakly with user
satisfaction scores, suggesting that user experience in voice-based AI systems
depends on factors beyond technical performance. Our findings provide practical
guidance for selecting components in multimodal conversational AI systems and
contribute a validated evaluation methodology for voice-based interactions.

</details>


### [45] [Segmentation-free Goodness of Pronunciation](https://arxiv.org/abs/2507.16838)
*Xinwei Cao,Zijian Fan,Torbjørn Svendsen,Giampiero Salvi*

Main category: eess.AS

TL;DR: 本文提出了两种新的方法，用于改进语音识别任务中的发音评估，这些方法利用了CTC训练的声学模型，并在实验中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的MDD系统基于一种称为发音质量（GOP）的方法，这需要将语音分割成语音单元，限制了这些方法的准确性以及使用现代CTC-based声学模型的可能性。

Method: 本文提出了自对齐GOP（GOP-SA）和无对齐方法（GOP-AF），以利用CTC训练的ASR模型进行MDD。

Result: 本文提供了在CMU Kids和Speechocean762数据集上的实验结果，比较了不同定义的方法，并展示了所提出方法在语音识别任务中的优越性能。

Conclusion: 本文提出的基于CTC的声学模型方法在语音识别任务中取得了最先进的结果。

Abstract: Mispronunciation detection and diagnosis (MDD) is a significant part in
modern computer aided language learning (CALL) systems. Within MDD,
phoneme-level pronunciation assessment is key to helping L2 learners improve
their pronunciation. However, most systems are based on a form of goodness of
pronunciation (GOP) which requires pre-segmentation of speech into phonetic
units. This limits the accuracy of these methods and the possibility to use
modern CTC-based acoustic models for their evaluation. In this study, we first
propose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR
models for MDD. Next, we define a more general alignment-free method that takes
all possible alignments of the target phoneme into account (GOP-AF). We give a
theoretical account of our definition of GOP-AF, an implementation that solves
potential numerical issues as well as a proper normalization which makes the
method applicable with acoustic models with different peakiness over time. We
provide extensive experimental results on the CMU Kids and Speechocean762
datasets comparing the different definitions of our methods, estimating the
dependency of GOP-AF on the peakiness of the acoustic models and on the amount
of context around the target phoneme. Finally, we compare our methods with
recent studies over the Speechocean762 data showing that the feature vectors
derived from the proposed method achieve state-of-the-art results on
phoneme-level pronunciation assessment.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [46] [A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task](https://arxiv.org/abs/2507.17232)
*Mashiro Toyooka,Kiyoharu Aizawa,Yoko Yamakata*

Main category: cs.MM

TL;DR: 本文研究了如何通过状态探测方法评估LLMs对烹饪过程中食材状态的理解能力，并提出了一个新的任务和数据集。实验表明，学习食材状态知识可以显著提升LLMs对烹饪过程的理解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在训练时没有直接观察现实世界现象，这在烹饪食谱中尤其成问题，因为中间状态的食材常常被省略，使得模型难以跟踪食材状态并准确理解食谱。

Method: 应用状态探测方法评估语言模型对世界的理解，并提出新的任务和数据集来评估LLMs识别烹饪过程中中间食材状态的能力。

Result: 使用广泛使用的LLMs（如Llama3.1-70B和Qwen2.5-72B）进行实验，结果显示学习食材状态知识可以提高它们对烹饪过程的理解，性能接近商业LLMs。

Conclusion: 学习食材状态知识可以提高LLMs对烹饪过程的理解，使其性能达到商业LLMs的水平。

Abstract: Large Language Models (LLMs) are trained on a vast amount of procedural
texts, but they do not directly observe real-world phenomena. In the context of
cooking recipes, this poses a challenge, as intermediate states of ingredients
are often omitted, making it difficult for models to track ingredient states
and understand recipes accurately. In this paper, we apply state probing, a
method for evaluating a language model's understanding of the world, to the
domain of cooking. We propose a new task and dataset for evaluating how well
LLMs can recognize intermediate ingredient states during cooking procedures. We
first construct a new Japanese recipe dataset with clear and accurate
annotations of ingredient state changes, collected from well-structured and
controlled recipe texts. Using this dataset, we design three novel tasks to
evaluate whether LLMs can track ingredient state transitions and identify
ingredients present at intermediate steps. Our experiments with widely used
LLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state
knowledge improves their understanding of cooking processes, achieving
performance comparable to commercial LLMs.

</details>

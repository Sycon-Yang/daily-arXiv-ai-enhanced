<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 本文提出了一种新框架，通过修改外交事件的叙述来改变公众情绪，利用语言模型预测公众反应，并通过反事实生成算法生成修改后的文本，成功提高了公众情绪的积极程度。


<details>
  <summary>Details</summary>
Motivation: 传统方法如大规模调查或人工内容分析媒体通常耗时、劳动密集且缺乏前瞻性分析能力。公共情绪在外交中起着关键作用，良好的情绪为政策实施提供重要支持，有助于解决国际问题并塑造国家的国际形象。

Method: 首先，我们训练一个语言模型来预测公众对外交事件的反应。其次，根据传播理论并与领域专家合作，我们预先确定了几种文本特征进行修改，确保任何修改改变了事件的叙述框架，同时保留其核心事实。我们开发了一种反事实生成算法，该算法使用大型语言模型系统地生成原始文本的修改版本。

Result: 结果显示，该框架成功将公众情绪转向更积极的状态，成功率高达70%。

Conclusion: 该框架可以作为外交官、政策制定者和传播专家的实用工具，提供数据驱动的见解，以如何构架外交倡议或报道事件来促进更理想的公众情绪。

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [2] [Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition](https://arxiv.org/abs/2509.20373)
*Shreya G. Upadhyay,Carlos Busso,Chi-Chun Lee*

Main category: cs.CL

TL;DR: 本文提出了一种说话人风格感知的音素锚定框架，以在不同说话人和语言之间对齐情感表达。该方法通过基于图的聚类构建特定于情感的说话人社区，并在说话人和音素空间中应用双空间锚定，以实现跨语言的情感迁移。


<details>
  <summary>Details</summary>
Motivation: 跨语言语音情感识别（SER）由于语音变异性差异和说话人特定的表达风格而具有挑战性。需要一种能够跨不同说话人和语言对齐情感外化的框架。

Method: 本文提出了一种说话人风格感知的音素锚定框架，通过基于图的聚类构建特定于情感的说话人社区，并在说话人和音素空间中应用双空间锚定。

Result: 在MSP-Podcast（英语）和BIIC-Podcast（台湾普通话）语料库上的评估表明，该方法在竞争性基线上实现了更好的泛化，并提供了关于跨语言情感表示共性的有价值见解。

Conclusion: 本文提出的框架有效捕捉了跨语言情感表达的共性，并展示了其在跨语言情感识别中的潜力。

Abstract: Cross-lingual speech emotion recognition (SER) remains a challenging task due
to differences in phonetic variability and speaker-specific expressive styles
across languages. Effectively capturing emotion under such diverse conditions
requires a framework that can align the externalization of emotions across
different speakers and languages. To address this problem, we propose a
speaker-style aware phoneme anchoring framework that aligns emotional
expression at the phonetic and speaker levels. Our method builds
emotion-specific speaker communities via graph-based clustering to capture
shared speaker traits. Using these groups, we apply dual-space anchoring in
speaker and phonetic spaces to enable better emotion transfer across languages.
Evaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)
corpora demonstrate improved generalization over competitive baselines and
provide valuable insights into the commonalities in cross-lingual emotion
representation.

</details>


### [3] [CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics](https://arxiv.org/abs/2509.20374)
*Nithin Somasekharan,Ling Yue,Yadi Cao,Weichao Li,Patrick Emami,Pochinapeddi Sai Bhargav,Anurag Acharya,Xingyu Xie,Shaowu Pan*

Main category: cs.CL

TL;DR: 本文介绍了CFDLLMBench，这是一个基准套件，旨在全面评估大型语言模型（LLM）在计算流体力学（CFD）方面的性能，包括研究生级别的CFD知识、数值和物理推理以及上下文相关的CFD工作流程实现。


<details>
  <summary>Details</summary>
Motivation: To explore the utility of Large Language Models (LLMs) in automating numerical experiments of complex physical systems, which is a critical and labor-intensive component. Computational Fluid Dynamics (CFD) offers a uniquely challenging testbed for evaluating the scientific capabilities of LLMs.

Method: CFDLLMBench, a benchmark suite comprising three complementary components -- CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM performance across three key competencies: graduate-level CFD knowledge, numerical and physical reasoning of CFD, and context-dependent implementation of CFD workflows.

Result: CFDLLMBench combines a detailed task taxonomy with a rigorous evaluation framework to deliver reproducible results and quantify LLM performance across code executability, solution accuracy, and numerical convergence behavior.

Conclusion: CFDLLMBench establishes a solid foundation for the development and evaluation of LLM-driven automation of numerical experiments for complex physical systems.

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
general NLP tasks, but their utility in automating numerical experiments of
complex physical system -- a critical and labor-intensive component -- remains
underexplored. As the major workhorse of computational science over the past
decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging
testbed for evaluating the scientific capabilities of LLMs. We introduce
CFDLLMBench, a benchmark suite comprising three complementary components --
CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM
performance across three key competencies: graduate-level CFD knowledge,
numerical and physical reasoning of CFD, and context-dependent implementation
of CFD workflows. Grounded in real-world CFD practices, our benchmark combines
a detailed task taxonomy with a rigorous evaluation framework to deliver
reproducible results and quantify LLM performance across code executability,
solution accuracy, and numerical convergence behavior. CFDLLMBench establishes
a solid foundation for the development and evaluation of LLM-driven automation
of numerical experiments for complex physical systems. Code and data are
available at https://github.com/NREL-Theseus/cfdllmbench/.

</details>


### [4] [Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text](https://arxiv.org/abs/2509.20375)
*Sharanya Parimanoharan,Ruwan D. Nawarathna*

Main category: cs.CL

TL;DR: 本文研究了当前机器学习方法在区分ChatGPT-3.5生成文本和人类文本方面的能力，发现基于transformer的方法（特别是DistilBERT）表现最佳，同时指出模型多样性不足以超越单一强大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（如ChatGPT）的迅速采用，人类和AI生成文本之间的界限变得模糊，引发了关于学术诚信、知识产权和虚假信息传播的紧急问题。因此，需要可靠的AI文本检测来确保公平评估，保护人类真实性，并培养对数字通信的信任。

Method: 本文研究了当前机器学习方法在区分ChatGPT-3.5生成的文本和人类书写的文本方面的效果，测试并比较了经典方法（逻辑回归结合传统的词袋、词性标注和TF-IDF特征）和基于transformer的方法（BERT增强的N-gram、DistilBERT、BERT自定义轻量级分类器和基于LSTM的N-gram模型）。

Result: 结果表明，DistilBERT表现最佳，而逻辑回归和BERT-Custom提供了稳健且平衡的替代方案；LSTM和BERT-N-gram方法则表现较差。三种最佳模型的最大投票集成未能超越DistilBERT本身，突显了单一基于transformer的表示优于单纯的模型多样性。

Conclusion: 本文通过全面评估这些AI文本检测方法的优缺点，为更强大的Transformer框架奠定了基础，以跟上不断改进的生成式AI模型的步伐。

Abstract: The rapid adoption of large language models (LLMs) such as ChatGPT has
blurred the line between human and AI-generated texts, raising urgent questions
about academic integrity, intellectual property, and the spread of
misinformation. Thus, reliable AI-text detection is needed for fair assessment
to safeguard human authenticity and cultivate trust in digital communication.
In this study, we investigate how well current machine learning (ML) approaches
can distinguish ChatGPT-3.5-generated texts from human-written texts employing
a labeled data set of 250 pairs of abstracts from a wide range of research
topics. We test and compare both classical (Logistic Regression armed with
classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT
augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,
and LSTM-based N-gram models) ML detection techniques. As we aim to assess each
model's performance in detecting AI-generated research texts, we also aim to
test whether an ensemble of these models can outperform any single detector.
Results show DistilBERT achieves the overall best performance, while Logistic
Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and
BERT-N-gram approaches lag. The max voting ensemble of the three best models
fails to surpass DistilBERT itself, highlighting the primacy of a single
transformer-based representation over mere model diversity. By comprehensively
assessing the strengths and weaknesses of these AI-text detection approaches,
this work lays a foundation for more robust transformer frameworks with larger,
richer datasets to keep pace with ever-improving generative AI models.

</details>


### [5] [ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models](https://arxiv.org/abs/2509.20376)
*Haoxuan Li,Zhen Wen,Qiqi Jiang,Chenxiao Li,Yuwei Wu,Yuchen Yang,Yiyao Wang,Xiuqi Huang,Minfeng Zhu,Wei Chen*

Main category: cs.CL

TL;DR: ConceptViz is a visual analytics system designed for exploring concepts in LLMs, which helps bridge the gap between SAE features and human concepts by enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification.


<details>
  <summary>Details</summary>
Motivation: SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive. To bridge the gap between SAE features and human concepts, we present ConceptViz.

Method: ConceptViz implements a novel dentification => Interpretation => Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification.

Result: We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs.

Conclusion: ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features.

Abstract: Large language models (LLMs) have achieved remarkable performance across a
wide range of natural language tasks. Understanding how LLMs internally
represent knowledge remains a significant challenge. Despite Sparse
Autoencoders (SAEs) have emerged as a promising technique for extracting
interpretable features from LLMs, SAE features do not inherently align with
human-understandable concepts, making their interpretation cumbersome and
labor-intensive. To bridge the gap between SAE features and human concepts, we
present ConceptViz, a visual analytics system designed for exploring concepts
in LLMs. ConceptViz implements a novel dentification => Interpretation =>
Validation pipeline, enabling users to query SAEs using concepts of interest,
interactively explore concept-to-feature alignments, and validate the
correspondences through model behavior verification. We demonstrate the
effectiveness of ConceptViz through two usage scenarios and a user study. Our
results show that ConceptViz enhances interpretability research by streamlining
the discovery and validation of meaningful concept representations in LLMs,
ultimately aiding researchers in building more accurate mental models of LLM
features. Our code and user guide are publicly available at
https://github.com/Happy-Hippo209/ConceptViz.

</details>


### [6] [SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20377)
*Tomoaki Isoda*

Main category: cs.CL

TL;DR: 本文提出了SKILL-RAG方法，利用模型的自我知识来确定哪些检索文档有助于回答给定查询，通过强化学习框架和句子级粒度过滤，显著提升了RAG性能。


<details>
  <summary>Details</summary>
Motivation: 由于检索系统可能返回不相关的内容，将这些信息纳入模型通常会导致幻觉。因此，识别和过滤无用的检索内容是提高RAG性能的关键挑战。为了更好地整合模型的内部知识与外部检索知识，了解模型“知道”和“不知道”的内容（也称为“自我知识”）是至关重要的。

Method: 我们设计了一个基于强化学习的训练框架，以明确地引出模型的自我知识，并采用句子级别的粒度来过滤无关内容，同时保留有用的知识。

Result: 在几个问答基准测试中，使用Llama2-7B和Qwen3-8B评估SKILL-RAG。实验结果表明，SKILL-RAG不仅提高了生成质量，还显著减少了输入文档的数量。

Conclusion: SKILL-RAG不仅提高了生成质量，还显著减少了输入文档的数量，验证了自我知识在指导高质量检索选择中的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive tasks in
recent years. However, since retrieval systems may return irrelevant content,
incorporating such information into the model often leads to hallucinations.
Thus, identifying and filtering out unhelpful retrieved content is a key
challenge for improving RAG performance.To better integrate the internal
knowledge of the model with external knowledge from retrieval, it is essential
to understand what the model "knows" and "does not know" (which is also called
"self-knowledge"). Based on this insight, we propose SKILL-RAG (Self-Knowledge
Induced Learning and Filtering for RAG), a novel method that leverages the
model's self-knowledge to determine which retrieved documents are beneficial
for answering a given query. We design a reinforcement learning-based training
framework to explicitly elicit self-knowledge from the model and employs
sentence-level granularity to filter out irrelevant content while preserving
useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several
question answering benchmarks. Experimental results demonstrate that SKILL-RAG
not only improves generation quality but also significantly reduces the number
of input documents, validating the importance of self-knowledge in guiding the
selection of high-quality retrievals.

</details>


### [7] [Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation](https://arxiv.org/abs/2509.20378)
*Sirui Wang,Andong Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: Emo-FiLM是一种基于LLM的TTS的细粒度情感建模框架，能够实现单词级情感控制，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的系统通常依赖于句子级别的控制，如预定义标签、参考音频或自然语言提示，这些方法虽然在全局情感表达上有效，但无法捕捉句子内的动态变化。因此，需要一种能够实现细粒度情感控制的方法。

Method: Emo-FiLM是一种基于LLM的TTS的细粒度情感建模框架，它通过将情感2vec的帧级特征与单词对齐，获得单词级情感注释，并通过特征线性调制（FiLM）层进行映射，从而实现通过直接调制文本嵌入来控制单词级情感。

Result: 实验表明，Emo-FiLM在全局和细粒度任务上都优于现有方法，证明了其在富有表现力的语音合成中的有效性和通用性。

Conclusion: Emo-FiLM在全局和细粒度任务上都优于现有方法，证明了其在富有表现力的语音合成中的有效性和通用性。

Abstract: Emotional text-to-speech (E-TTS) is central to creating natural and
trustworthy human-computer interaction. Existing systems typically rely on
sentence-level control through predefined labels, reference audio, or natural
language prompts. While effective for global emotion expression, these
approaches fail to capture dynamic shifts within a sentence. To address this
limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework
for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to
words to obtain word-level emotion annotations, and maps them through a
Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion
control by directly modulating text embeddings. To support evaluation, we
construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed
annotations of emotional transitions. Experiments show that Emo-FiLM
outperforms existing approaches on both global and fine-grained tasks,
demonstrating its effectiveness and generality for expressive speech synthesis.

</details>


### [8] [USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model](https://arxiv.org/abs/2509.20381)
*Jianyu Wen,Jingyun Wang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Ying Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于用户模拟器的框架（USB-Rec），以提高LLM在对话推荐中的性能。该框架包括一种基于LLM的偏好优化数据集构建策略和一个自我增强策略，并在各种数据集上进行了实验，结果表明该方法优于之前最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的方法主要集中在如何利用LLM的总结和分析能力，而忽略了训练问题。因此，我们提出了一种集成的训练-推理框架，以在模型层面提高LLM在对话推荐中的性能。

Method: 我们提出了一个基于用户模拟器的框架（USB-Rec），该框架包括一种基于LLM的偏好优化（PO）数据集构建策略和一个自我增强策略（SES）。

Result: 在各种数据集上的广泛实验表明，我们的方法始终优于之前最先进的方法。

Conclusion: 我们的方法在各种数据集上表现出色，优于之前最先进的方法。

Abstract: Recently, Large Language Models (LLMs) have been widely employed in
Conversational Recommender Systems (CRSs). Unlike traditional language model
approaches that focus on training, all existing LLMs-based approaches are
mainly centered around how to leverage the summarization and analysis
capabilities of LLMs while ignoring the issue of training. Therefore, in this
work, we propose an integrated training-inference framework,
User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs
in conversational recommendation at the model level. Firstly, we design a
LLM-based Preference Optimization (PO) dataset construction strategy for RL
training, which helps the LLMs understand the strategies and methods in
conversational recommendation. Secondly, we propose a Self-Enhancement Strategy
(SES) at the inference stage to further exploit the conversational
recommendation potential obtained from RL training. Extensive experiments on
various datasets demonstrate that our method consistently outperforms previous
state-of-the-art methods.

</details>


### [9] [Document Summarization with Conformal Importance Guarantees](https://arxiv.org/abs/2509.20461)
*Bruce Kuwahara,Chen-Yuan Lin,Xiao Shi Huang,Kin Kwan Leung,Jullian Arta Yapeter,Ilya Stanevich,Felipe Perez,Jesse C. Cresswell*

Main category: cs.CL

TL;DR: 本文介绍了Conformal Importance Summarization，这是一种提供严格、分布自由的覆盖保证的摘要生成框架，能够实现用户指定的覆盖和召回率，并且与现有的黑盒LLMs无缝集成。


<details>
  <summary>Details</summary>
Motivation: 自动摘要系统随着大型语言模型（LLMs）迅速发展，但在高风险领域（如医疗、法律和金融）中仍缺乏对关键内容包含的可靠保证。

Method: 我们引入了Conformal Importance Summarization，这是第一个用于保留重要性的摘要生成框架，使用符合预测来提供严格、分布自由的覆盖保证。通过校准句子级重要性得分的阈值，我们实现了具有用户指定覆盖和召回率的提取式文档摘要。

Result: 实验表明，Conformal Importance Summarization实现了理论上保证的信息覆盖速率。

Conclusion: 我们的工作表明，Conformal Importance Summarization可以与现有技术结合，实现可靠、可控的自动摘要，为在关键应用中安全部署AI摘要工具铺平了道路。

Abstract: Automatic summarization systems have advanced rapidly with large language
models (LLMs), yet they still lack reliable guarantees on inclusion of critical
content in high-stakes domains like healthcare, law, and finance. In this work,
we introduce Conformal Importance Summarization, the first framework for
importance-preserving summary generation which uses conformal prediction to
provide rigorous, distribution-free coverage guarantees. By calibrating
thresholds on sentence-level importance scores, we enable extractive document
summarization with user-specified coverage and recall rates over critical
content. Our method is model-agnostic, requires only a small calibration set,
and seamlessly integrates with existing black-box LLMs. Experiments on
established summarization benchmarks demonstrate that Conformal Importance
Summarization achieves the theoretically assured information coverage rate. Our
work suggests that Conformal Importance Summarization can be combined with
existing techniques to achieve reliable, controllable automatic summarization,
paving the way for safer deployment of AI summarization tools in critical
applications. Code is available at
https://github.com/layer6ai-labs/conformal-importance-summarization.

</details>


### [10] [ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos](https://arxiv.org/abs/2509.20467)
*Henrik Vatndal,Vinay Setty*

Main category: cs.CL

TL;DR: ShortCheck 是一个用于检测短格式视频中虚假信息的系统，通过多种技术集成实现了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 短格式视频平台（如 TikTok）由于其多模态、动态和噪声内容，在虚假信息检测方面面临独特挑战。

Method: ShortCheck 集成了语音转录、OCR、物体和深度伪造检测、视频到文本摘要以及声明验证等功能。

Result: 在两个手动标注的数据集上评估，ShortCheck 在多语言环境下取得了 F1 加权分数超过 70% 的有希望的结果。

Conclusion: ShortCheck 是一个模块化、仅推理的管道，能够有效识别值得检查的短视频，从而帮助人工事实核查。

Abstract: Short-form video platforms like TikTok present unique challenges for
misinformation detection due to their multimodal, dynamic, and noisy content.
We present ShortCheck, a modular, inference-only pipeline with a user-friendly
interface that automatically identifies checkworthy short-form videos to help
human fact-checkers. The system integrates speech transcription, OCR, object
and deepfake detection, video-to-text summarization, and claim verification.
ShortCheck is validated by evaluating it on two manually annotated datasets
with TikTok videos in a multilingual setting. The pipeline achieves promising
results with F1-weighted score over 70\%.

</details>


### [11] [MARS: toward more efficient multi-agent collaboration for LLM reasoning](https://arxiv.org/abs/2509.20502)
*Xiao Wang,Jia Wang,Yijie Wang,Pengtao Dang,Sha Cao,Chi Zhang*

Main category: cs.CL

TL;DR: 本文提出了MARS（多代理评审系统），这是一种基于角色的协作框架，旨在提高大型语言模型的推理能力，同时减少计算开销。实验显示，MARS在保持与MAD相当的准确性的同时，显著降低了token使用量和推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有的Multi-Agent Debate (MAD)方法在计算上开销较大，因为涉及多个代理和频繁的通信。因此，需要一种更高效的协作框架来提高推理质量。

Method: MARS是一种基于角色的协作框架，灵感来自评审过程。作者代理生成初始解决方案，评审代理独立提供决策和评论，元评审代理整合反馈以做出最终决策并指导进一步修改。

Result: 实验表明，MARS在多个基准测试中与MAD和其他最先进的推理策略相比，具有相当的准确性，但减少了约50%的token使用量和推理时间。

Conclusion: MARS能够达到与MAD相当的准确性，同时减少了约50%的token使用量和推理时间。

Abstract: Large language models (LLMs) have achieved impressive results in natural
language understanding, yet their reasoning capabilities remain limited when
operating as single agents. Multi-Agent Debate (MAD) has been proposed to
address this limitation by enabling collaborative reasoning among multiple
models in a round-table debate manner. While effective, MAD introduces
substantial computational overhead due to the number of agents involved and the
frequent communication required. In this paper, we propose MARS (Multi-Agent
Review System), a role-based collaboration framework inspired by the review
process. In MARS, an author agent generates an initial solution, reviewer
agents provide decisions and comments independently, and a meta-reviewer
integrates the feedback to make the final decision and guide further revision.
This design enhances reasoning quality while avoiding costly
reviewer-to-reviewer interactions, thereby controlling token consumption and
inference time. We compared MARS with both MAD and other state-of-the-art
reasoning strategies across multiple benchmarks. Extensive experiments with
different LLMs show that MARS matches the accuracy of MAD while reducing both
token usage and inference time by approximately 50\%. Code is available at
https://github.com/xwang97/MARS.

</details>


### [12] [SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages](https://arxiv.org/abs/2509.20557)
*Hannah Liu,Junghyun Min,Ethan Yue Heng Cheung,Shou-Yi Hung,Syed Mekael Wasti,Runtong Liang,Shiyao Qian,Shizhao Zheng,Elsie Chan,Ka Ieng Charlotte Lo,Wing Yu Yip,Richard Tzong-Han Tsai,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本文介绍了一个新的数据集SiniticMTError，旨在支持机器翻译领域的研究，特别是在低资源语言上的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来机器翻译取得了重大进展，但许多低资源语言的进展仍然有限，因为缺乏大规模训练数据和语言资源。本文旨在解决这一问题，通过构建一个包含错误跨度、错误类型和错误严重性注释的数据集来支持相关研究。

Method: 本文通过母语者进行严格的注释过程，包括对注释者之间的一致性、迭代反馈以及错误类型和严重程度的分析，构建了SiniticMTError数据集。

Result: 本文构建了一个名为SiniticMTError的新数据集，该数据集基于现有的平行语料库，提供从英语到普通话、粤语和吴语的机器翻译示例中的错误跨度、错误类型和错误严重性注释。

Conclusion: 本文介绍了SiniticMTError数据集，该数据集为机器翻译社区提供了用于微调具有错误检测能力的模型的资源，并支持翻译质量估计、错误感知生成和低资源语言评估的研究。

Abstract: Despite major advances in machine translation (MT) in recent years, progress
remains limited for many low-resource languages that lack large-scale training
data and linguistic resources. Cantonese and Wu Chinese are two Sinitic
examples, although each enjoys more than 80 million speakers around the world.
In this paper, we introduce SiniticMTError, a novel dataset that builds on
existing parallel corpora to provide error span, error type, and error severity
annotations in machine-translated examples from English to Mandarin, Cantonese,
and Wu Chinese. Our dataset serves as a resource for the MT community to
utilize in fine-tuning models with error detection capabilities, supporting
research on translation quality estimation, error-aware generation, and
low-resource language evaluation. We report our rigorous annotation process by
native speakers, with analyses on inter-annotator agreement, iterative
feedback, and patterns in error type and severity.

</details>


### [13] [SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations](https://arxiv.org/abs/2509.20567)
*Ayan Sar,Pranav Singh Puri,Sumit Aich,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: SwasthLLM is a unified, zero-shot, cross-lingual, and multi-task learning framework for medical diagnosis that works effectively across English, Hindi, and Bengali without requiring language-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The scarcity of annotated medical data in low-resource languages and linguistic variability across populations make automatic disease diagnosis from clinical text challenging.

Method: SwasthLLM uses a unified, zero-shot, cross-lingual, and multi-task learning framework with a multilingual XLM-RoBERTa encoder, language-aware attention mechanism, disease classification head, Siamese contrastive learning module, translation consistency module, contrastive projection head, and MAML for rapid adaptation.

Result: SwasthLLM achieves 97.22% test accuracy and 97.17% F1-score in supervised settings, and 92.78% accuracy on Hindi and 73.33% accuracy on Bengali in zero-shot scenarios.

Conclusion: SwasthLLM achieves high diagnostic performance and demonstrates strong generalization in low-resource contexts.

Abstract: In multilingual healthcare environments, automatic disease diagnosis from
clinical text remains a challenging task due to the scarcity of annotated
medical data in low-resource languages and the linguistic variability across
populations. This paper proposes SwasthLLM, a unified, zero-shot,
cross-lingual, and multi-task learning framework for medical diagnosis that
operates effectively across English, Hindi, and Bengali without requiring
language-specific fine-tuning. At its core, SwasthLLM leverages the
multilingual XLM-RoBERTa encoder augmented with a language-aware attention
mechanism and a disease classification head, enabling the model to extract
medically relevant information regardless of the language structure. To align
semantic representations across languages, a Siamese contrastive learning
module is introduced, ensuring that equivalent medical texts in different
languages produce similar embeddings. Further, a translation consistency module
and a contrastive projection head reinforce language-invariant representation
learning. SwasthLLM is trained using a multi-task learning strategy, jointly
optimizing disease classification, translation alignment, and contrastive
learning objectives. Additionally, we employ Model-Agnostic Meta-Learning
(MAML) to equip the model with rapid adaptation capabilities for unseen
languages or tasks with minimal data. Our phased training pipeline emphasizes
robust representation alignment before task-specific fine-tuning. Extensive
evaluation shows that SwasthLLM achieves high diagnostic performance, with a
test accuracy of 97.22% and an F1-score of 97.17% in supervised settings.
Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and
73.33% accuracy on Bengali medical text, demonstrating strong generalization in
low-resource contexts.

</details>


### [14] [Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures](https://arxiv.org/abs/2509.20577)
*Sampurna Roy,Ayan Sar,Anurag Kaushish,Kanav Gupta,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: DS-MoE 是一种基于深度专业化的混合专家框架，能够动态组装推理链，从而提高效率、推理质量和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前的 transformer 架构对所有输入应用相同的处理深度，导致效率低下并限制推理质量。简单的事实查询与复杂的逻辑问题使用相同的多层计算，浪费资源并限制深度推理。

Method: DS-MoE 是一种模块化框架，扩展了基于宽度的 Mixture of Experts 范式到基于深度的专业化计算。它引入了针对不同推理深度的专家模块，并通过学习的路由网络动态组装自定义推理链。

Result: 实验结果表明，DS-MoE 在计算节省和推理速度方面优于统一深度的 transformer，同时在复杂多步骤推理基准上实现了更高的准确性。此外，路由决策产生了可解释的推理链，提高了透明度和可扩展性。

Conclusion: DS-MoE 是一种显著的进展，展示了深度专业化模块化处理可以在大规模语言模型中同时提高效率、推理质量和可解释性。

Abstract: Contemporary transformer architectures apply identical processing depth to
all inputs, creating inefficiencies and limiting reasoning quality. Simple
factual queries are subjected to the same multilayered computation as complex
logical problems, wasting resources while constraining deep inference. To
overcome this, we came up with a concept of Dynamic Reasoning Chains through
Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends
the Mixture of Experts paradigm from width-based to depth specialised
computation. DS-MoE introduces expert modules optimised for distinct reasoning
depths, shallow pattern recognition, compositional reasoning, logical
inference, memory integration, and meta-cognitive supervision. A learned
routing network dynamically assembles custom reasoning chains, activating only
the necessary experts to match input complexity. The dataset on which we
trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse
domains such as scientific papers, legal texts, programming code, and web
content, enabling systematic assessment across reasoning depths. Experimental
results demonstrate that DS-MoE achieves up to 16 per cent computational
savings and 35 per cent faster inference compared to uniform-depth
transformers, while delivering 2.8 per cent higher accuracy on complex
multi-step reasoning benchmarks. Furthermore, routing decisions yield
interpretable reasoning chains, enhancing transparency and scalability. These
findings establish DS-MoE as a significant advancement in adaptive neural
architectures, demonstrating that depth-specialised modular processing can
simultaneously improve efficiency, reasoning quality, and interpretability in
large-scale language models.

</details>


### [15] [Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding](https://arxiv.org/abs/2509.20581)
*Ayan Sar,Sampurna Roy,Kanav Gupta,Anurag Kaushish,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: HRT 是一种基于小波的 Transformer 架构，能够同时在多个分辨率上处理语言，实现更高效的计算和更好的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer 架构在自然语言任务中取得了最先进的性能，但它们从根本上错误地表示了人类语言的层次性质，通过扁平的标记序列处理文本。这导致了二次计算成本、弱计算成本、弱组合泛化和不足的语篇级建模。

Method: HRT 是一种受小波启发的神经架构，它同时在多个分辨率上处理语言，从字符到话语级单元。HRT 构建了一个多分辨率注意力机制，实现了自底向上的组合和自顶向下的情境化。通过在不同尺度上进行指数序列缩减，HRT 实现了 O(nlogn) 的复杂度。

Result: HRT 在 GLUE、SuperGLUE、Long Range Arena 和 WikiText-103 等多样化的基准测试中进行了评估，结果表明 HRT 在 GLUE 上平均优于标准 transformer 基线 +3.8%，在 SuperGLUE 上 +4.5%，在 Long Range Arena 上 +6.1%，同时相比 BERT 和 GPT 类型的模型减少了 42% 的内存使用和 37% 的推理延迟。消融研究证实了跨分辨率注意力和尺度专用模块的有效性，显示每个都独立地对效率和准确性做出贡献。

Conclusion: HRT 是第一个将计算结构与人类语言的层次组织对齐的架构，展示了多尺度、基于小波的处理在理论效率增益和语言理解的实际改进方面的有效性。

Abstract: Transformer architectures have achieved state-of-the-art performance across
natural language tasks, yet they fundamentally misrepresent the hierarchical
nature of human language by processing text as flat token sequences. This
results in quadratic computational cost, weak computational cost, weak
compositional generalization, and inadequate discourse-level modeling. We
propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired
neural architecture that processes language simultaneously across multiple
resolutions, from characters to discourse-level units. HRT constructs a
multi-resolution attention, enabling bottom-up composition and top-down
contextualization. By employing exponential sequence reduction across scales,
HRT achieves O(nlogn) complexity, offering significant efficiency improvements
over standard transformers. We evaluated HRT on a diverse suite of benchmarks,
including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results
demonstrated that HRT outperforms standard transformer baselines by an average
of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while
reducing memory usage by 42% and inference latency by 37% compared to BERT and
GPT style models of similar parameter count. Ablation studies confirm the
effectiveness of cross-resolution attention and scale-specialized modules,
showing that each contributes independently to both efficiency and accuracy.
Our findings establish HRT as the first architecture to align computational
structure with the hierarchical organization of human language, demonstrating
that multi-scale, wavelet-inspired processing yields both theoretical
efficiency gains and practical improvements in language understanding.

</details>


### [16] [FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models](https://arxiv.org/abs/2509.20624)
*Amin Karimi Monsefi,Nikhil Bhendawade,Manuel Rafael Ciosici,Dominic Culver,Yizhe Zhang,Irina Belousova*

Main category: cs.CL

TL;DR: FS-DFM是一种快速且高质量的离散流匹配模型，通过减少采样步骤数实现高效语言生成。


<details>
  <summary>Details</summary>
Motivation: 传统离散扩散模型需要数百到数千次模型评估才能达到高质量，导致生成效率低下。而自回归语言模型生成速度慢，难以满足长序列的需求。因此，需要一种既能保持质量又能提高生成速度的方法。

Method: 引入了FS-DFM（Few-Step Discrete Flow-Matching），通过将采样步骤数作为显式参数，并训练模型在不同步骤预算下保持一致性，从而实现快速且高质量的语言生成。

Result: FS-DFM在8个采样步骤下实现了与1,024步离散流模型相当的困惑度，同时采样速度提高了128倍，显著提升了延迟和吞吐量。

Conclusion: FS-DFM在语言建模基准测试中，使用8个采样步骤实现了与1,024步离散流基线相当的困惑度，同时显著提高了采样速度和吞吐量。

Abstract: Autoregressive language models (ARMs) deliver strong likelihoods, but are
inherently serial: they generate one token per forward pass, which limits
throughput and inflates latency for long sequences. Diffusion Language Models
(DLMs) parallelize across positions and thus appear promising for language
generation, yet standard discrete diffusion typically needs hundreds to
thousands of model evaluations to reach high quality, trading serial depth for
iterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A
discrete flow-matching model designed for speed without sacrificing quality.
The core idea is simple: make the number of sampling steps an explicit
parameter and train the model to be consistent across step budgets, so one big
move lands where many small moves would. We pair this with a reliable update
rule that moves probability in the right direction without overshooting, and
with strong teacher guidance distilled from long-run trajectories. Together,
these choices make few-step sampling stable, accurate, and easy to control. On
language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity
parity with a 1,024-step discrete-flow baseline for generating 1,024 tokens
using a similar-size model, delivering up to 128 times faster sampling and
corresponding latency/throughput gains.

</details>


### [17] [Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions](https://arxiv.org/abs/2509.20645)
*Jungsoo Park,Ethan Mendes,Gabriel Stanovsky,Alan Ritter*

Main category: cs.CL

TL;DR: 本文研究了在不运行实验的情况下预测大型语言模型性能的可能性。通过构建PRECOG语料库，实验表明该任务具有挑战性但可行，模型能够达到较低的平均绝对误差。分析显示，更强的模型进行多样化查询，而当前的开源模型表现较差。此外，在零泄漏设置中，GPT-5仍能获得非平凡的预测准确性。整体而言，本文为开放式的前瞻性评估提供了初步步骤。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的进步受到评估瓶颈的限制：构建基准，评估模型和设置，然后进行迭代。因此，我们问了一个简单的问题：在运行任何实验之前，我们可以预测结果吗？

Method: 我们研究了文本-only性能预测：从删除的任务描述和预期配置中估计模型的分数，而无需访问数据集实例。为了支持系统研究，我们整理了PRECOG，一个包含删除描述-性能对的语料库，涵盖多种任务、领域和指标。

Result: 实验表明该任务具有挑战性但可行：配备检索模块的模型（排除源论文）在高置信度阈值下达到最低8.7的平均绝对误差。分析表明，更强的推理模型会进行多样化的、迭代的查询，而当前的开源模型则落后，并且经常跳过检索或收集证据的多样性有限。

Conclusion: 我们的语料库和分析为开放式的前瞻性评估提供了一个初步步骤，支持难度估计和更智能的实验优先排序。

Abstract: Progress in large language models is constrained by an evaluation bottleneck:
build a benchmark, evaluate models and settings, then iterate. We therefore ask
a simple question: can we forecast outcomes before running any experiments? We
study text-only performance forecasting: estimating a model's score from a
redacted task description and intended configuration, with no access to dataset
instances. To support systematic study, we curate PRECOG, a corpus of redacted
description-performance pairs spanning diverse tasks, domains, and metrics.
Experiments show the task is challenging but feasible: models equipped with a
retrieval module that excludes source papers achieve moderate prediction
performance with well-calibrated uncertainty, reaching mean absolute error as
low as 8.7 on the Accuracy subset at high-confidence thresholds. Our analysis
indicates that stronger reasoning models engage in diverse, iterative querying,
whereas current open-source models lag and often skip retrieval or gather
evidence with limited diversity. We further test a zero-leakage setting,
forecasting on newly released datasets or experiments before their papers are
indexed, where GPT-5 with built-in web search still attains nontrivial
prediction accuracy. Overall, our corpus and analyses offer an initial step
toward open-ended anticipatory evaluation, supporting difficulty estimation and
smarter experiment prioritization.

</details>


### [18] [Building Tailored Speech Recognizers for Japanese Speaking Assessment](https://arxiv.org/abs/2509.20655)
*Yotaro Kubo,Richard Sproat,Chihiro Taguchi,Llion Jones*

Main category: cs.CL

TL;DR: 本文提出两种方法来解决日语语音识别中数据稀缺的问题，并展示了其在音素识别上的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然日语资源丰富，但用于训练模型生成包含声调标记的准确音素转录的数据却很少。

Method: 我们提出了两种方法来缓解数据稀缺问题：第一种是多任务训练方案，引入辅助损失函数以估计输入信号的正字法文本标签和音高模式；第二种是融合两个估计器，一个用于语音字母字符串，另一个用于文本标记序列。

Result: 实验结果表明，多任务学习和融合对于构建准确的音素识别器是有效的。我们的方法相比使用通用多语言识别器具有优势。

Conclusion: 我们的方法在CSJ核心评估集上将音节标签错误率从12.3%降低到了7.1%，表明了其有效性。

Abstract: This paper presents methods for building speech recognizers tailored for
Japanese speaking assessment tasks. Specifically, we build a speech recognizer
that outputs phonemic labels with accent markers. Although Japanese is
resource-rich, there is only a small amount of data for training models to
produce accurate phonemic transcriptions that include accent marks. We propose
two methods to mitigate data sparsity. First, a multitask training scheme
introduces auxiliary loss functions to estimate orthographic text labels and
pitch patterns of the input signal, so that utterances with only orthographic
annotations can be leveraged in training. The second fuses two estimators, one
over phonetic alphabet strings, and the other over text token sequences. To
combine these estimates we develop an algorithm based on the finite-state
transducer framework. Our results indicate that the use of multitask learning
and fusion is effective for building an accurate phonemic recognizer. We show
that this approach is advantageous compared to the use of generic multilingual
recognizers. The relative advantages of the proposed methods were also
compared. Our proposed methods reduced the average of mora-label error rates
from 12.3% to 7.1% over the CSJ core evaluation sets.

</details>


### [19] [Enhancing Molecular Property Prediction with Knowledge from Large Language Models](https://arxiv.org/abs/2509.20664)
*Peng Zhou,Lai Hou Tim,Zhixiang Cheng,Kun Xie,Chaoyi Li,Wei Liu,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: 本文提出一种新框架，将LLM提取的知识与分子结构特征结合，以提升分子属性预测效果。实验结果表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管GNN和自监督学习方法在MPP中取得了进展，但整合人类先验知识仍然是不可或缺的。然而，LLM在知识缺口和幻觉方面存在限制，特别是在研究较少的分子特性方面。

Method: 我们提出了一种新框架，首次将从LLMs中提取的知识与预训练分子模型的结构特征相结合，以增强MPP。我们的方法提示LLMs生成与领域相关的知识和可执行的分子向量化代码，产生基于知识的特征，随后与结构表示融合。

Result: 广泛的实验表明，我们的集成方法优于现有方法，证实了LLM衍生知识和结构信息的结合为MPP提供了一种稳健有效的解决方案。

Conclusion: 我们的集成方法在现有方法中表现更优，证实了LLM衍生知识和结构信息的结合为MPP提供了一种稳健有效的解决方案。

Abstract: Predicting molecular properties is a critical component of drug discovery.
Recent advances in deep learning, particularly Graph Neural Networks (GNNs),
have enabled end-to-end learning from molecular structures, reducing reliance
on manual feature engineering. However, while GNNs and self-supervised learning
approaches have advanced molecular property prediction (MPP), the integration
of human prior knowledge remains indispensable, as evidenced by recent methods
that leverage large language models (LLMs) for knowledge extraction. Despite
their strengths, LLMs are constrained by knowledge gaps and hallucinations,
particularly for less-studied molecular properties. In this work, we propose a
novel framework that, for the first time, integrates knowledge extracted from
LLMs with structural features derived from pre-trained molecular models to
enhance MPP. Our approach prompts LLMs to generate both domain-relevant
knowledge and executable code for molecular vectorization, producing
knowledge-based features that are subsequently fused with structural
representations. We employ three state-of-the-art LLMs, GPT-4o, GPT-4.1, and
DeepSeek-R1, for knowledge extraction. Extensive experiments demonstrate that
our integrated method outperforms existing approaches, confirming that the
combination of LLM-derived knowledge and structural information provides a
robust and effective solution for MPP.

</details>


### [20] [RedHerring Attack: Testing the Reliability of Attack Detection](https://arxiv.org/abs/2509.20691)
*Jonathan Rusert*

Main category: cs.CL

TL;DR: 本文提出了一种新的攻击模型RedHerring，该模型能够使攻击检测模型不可靠，同时保持分类器的准确性。此外，还提出了一种简单的置信度检查作为初步防御措施，可以显著提高检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的攻击检测模型的可靠性尚未得到充分研究，因此需要一种新的攻击模型来测试这些检测模型的鲁棒性。

Method: 本文提出了RedHerring攻击方法，并在4个数据集上对3个检测器进行了测试，以验证其有效性。同时，还提出了一种简单的置信度检查作为初步防御措施。

Result: RedHerring能够在不降低分类器准确性的前提下，将检测准确率降低20-71个百分点。此外，提出的置信度检查能够显著提高检测准确率。

Conclusion: 本文提出了一个新颖的攻击模型RedHerring，该模型能够使攻击检测模型不可靠，同时保持分类器的准确性。此外，本文还提出了一种简单的置信度检查作为初步防御措施，可以显著提高检测准确率。

Abstract: In response to adversarial text attacks, attack detection models have been
proposed and shown to successfully identify text modified by adversaries.
Attack detection models can be leveraged to provide an additional check for NLP
models and give signals for human input. However, the reliability of these
models has not yet been thoroughly explored. Thus, we propose and test a novel
attack setting and attack, RedHerring. RedHerring aims to make attack detection
models unreliable by modifying a text to cause the detection model to predict
an attack, while keeping the classifier correct. This creates a tension between
the classifier and detector. If a human sees that the detector is giving an
``incorrect'' prediction, but the classifier a correct one, then the human will
see the detector as unreliable. We test this novel threat model on 4 datasets
against 3 detectors defending 4 classifiers. We find that RedHerring is able to
drop detection accuracy between 20 - 71 points, while maintaining (or
improving) classifier accuracy. As an initial defense, we propose a simple
confidence check which requires no retraining of the classifier or detector and
increases detection accuracy greatly. This novel threat model offers new
insights into how adversaries may target detection models.

</details>


### [21] [Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms](https://arxiv.org/abs/2509.20699)
*Abhinay Shankar Belde,Rohit Ramkumar,Jonathan Rusert*

Main category: cs.CL

TL;DR: 本文提出两种新的攻击选择策略，以减少攻击测试所需的查询次数，同时保持攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有的流行黑盒攻击方法通常需要大量的查询，这使得它们对于资源有限的研究人员来说效率低下且不切实际。

Method: 我们提出了两种新的攻击选择策略，称为Hybrid Select和Dynamic Select。Hybrid Select通过引入大小阈值来决定使用哪种选择算法，将广义的BinarySelect技术与GreedySelect相结合。Dynamic Select通过学习每种选择方法应应用于哪些文本长度，提供了一种结合广义Binary和GreedySelect的替代方法。

Result: 在4个数据集和6个目标模型上，我们的最佳方法（句子级Hybrid Select）能够平均将每次攻击所需的查询次数减少25.82%。

Conclusion: 我们的最佳方法（句子级混合选择）能够在不损失攻击效果的情况下，平均将每次攻击所需的查询次数减少25.82%。

Abstract: Adversarial text attack research plays a crucial role in evaluating the
robustness of NLP models. However, the increasing complexity of
transformer-based architectures has dramatically raised the computational cost
of attack testing, especially for researchers with limited resources (e.g.,
GPUs). Existing popular black-box attack methods often require a large number
of queries, which can make them inefficient and impractical for researchers. To
address these challenges, we propose two new attack selection strategies called
Hybrid and Dynamic Select, which better combine the strengths of previous
selection algorithms. Hybrid Select merges generalized BinarySelect techniques
with GreedySelect by introducing a size threshold to decide which selection
algorithm to use. Dynamic Select provides an alternative approach of combining
the generalized Binary and GreedySelect by learning which lengths of texts each
selection method should be applied to. This greatly reduces the number of
queries needed while maintaining attack effectiveness (a limitation of
BinarySelect). Across 4 datasets and 6 target models, our best
method(sentence-level Hybrid Select) is able to reduce the number of required
queries per attack up 25.82\% on average against both encoder models and LLMs,
without losing the effectiveness of the attack.

</details>


### [22] [MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model](https://arxiv.org/abs/2509.20706)
*Hsiao-Ying Huang,Yi-Cheng Lin,Hung-yi Lee*

Main category: cs.CL

TL;DR: 本文提出了一种无需共享源数据即可增强情感感知语音系统的框架MI-Fuse，在跨领域转移任务中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 在真实世界部署中，SER经常由于领域不匹配而失败，此时源数据不可用，只能通过API访问强大的LALM。

Method: 提出了一种去噪标签融合框架MI-Fuse，该框架通过一个源域训练的SER分类器作为辅助教师来补充LALM。

Result: 实验表明，学生模型超越了LALM，并且比最强的基线高出3.9%。

Conclusion: 该方法在没有共享源数据的情况下增强了情感感知的语音系统，实现了现实世界的适应。

Abstract: Large audio-language models (LALMs) show strong zero-shot ability on speech
tasks, suggesting promise for speech emotion recognition (SER). However, SER in
real-world deployments often fails under domain mismatch, where source data are
unavailable and powerful LALMs are accessible only through an API. We ask:
given only unlabeled target-domain audio and an API-only LALM, can a student
model be adapted to outperform the LALM in the target domain? To this end, we
propose MI-Fuse, a denoised label fusion framework that supplements the LALM
with a source-domain trained SER classifier as an auxiliary teacher. The
framework draws multiple stochastic predictions from both teachers, weights
their mean distributions by mutual-information-based uncertainty, and
stabilizes training with an exponential moving average teacher. Experiments
across three public emotion datasets and six cross-domain transfers show
consistent gains, with the student surpassing the LALM and outperforming the
strongest baseline by 3.9%. This approach strengthens emotion-aware speech
systems without sharing source data, enabling realistic adaptation.

</details>


### [23] [Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised Neural Grammar Induction](https://arxiv.org/abs/2509.20734)
*Jinwook Park,Kangil Kim*

Main category: cs.CL

TL;DR: 本文研究了无监督神经语法归纳中的概率分布崩溃问题，并提出了一种缓解方法，从而提高了解析性能并允许使用更紧凑的语法。


<details>
  <summary>Details</summary>
Motivation: 现有的模型面临表达能力的瓶颈，导致语法过于庞大但表现不佳。

Method: 我们引入了一种针对性的解决方案，即'缓解崩溃的神经参数化'，以减轻概率分布崩溃的影响。

Result: 通过广泛的实证分析，我们展示了这种方法的有效性。

Conclusion: 我们的方法在广泛的语言范围内显著提高了解析性能，同时使使用更紧凑的语法成为可能。

Abstract: Unsupervised neural grammar induction aims to learn interpretable
hierarchical structures from language data. However, existing models face an
expressiveness bottleneck, often resulting in unnecessarily large yet
underperforming grammars. We identify a core issue, $\textit{probability
distribution collapse}$, as the underlying cause of this limitation. We analyze
when and how the collapse emerges across key components of neural
parameterization and introduce a targeted solution, $\textit{collapse-relaxing
neural parameterization}$, to mitigate it. Our approach substantially improves
parsing performance while enabling the use of significantly more compact
grammars across a wide range of languages, as demonstrated through extensive
empirical analysis.

</details>


### [24] [Confidence-guided Refinement Reasoning for Zero-shot Question Answering](https://arxiv.org/abs/2509.20750)
*Youwon Jang,Woo Suk Choi,Minjoon Jung,Minsu Lee,Byoung-Tak Zhang*

Main category: cs.CL

TL;DR: 本文提出了C2R框架，它是一种无需训练的问答任务方法，通过构建和优化子QAs来提高答案的置信度评分，并在多个模型和基准测试中取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 提出一种适用于文本、图像和视频领域问答任务的新型训练-free 框架，以提高目标答案的置信度评分。

Method: C2R首先精心挑选一组子QAs以探索多种推理路径，然后比较结果答案候选者的置信度分数，以选择最可靠的最终答案。

Result: C2R能够通过构建和优化子QAs来获得更好的目标答案置信度评分，并且可以在不同模型和基准测试中实现性能提升。

Conclusion: C2R可以无缝集成到各种现有的QA模型中，并在不同的模型和基准测试中表现出一致的性能提升。此外，我们提供了关于如何利用子QAs影响模型行为的重要但未被充分探索的见解。

Abstract: We propose Confidence-guided Refinement Reasoning (C2R), a novel
training-free framework applicable to question-answering (QA) tasks across
text, image, and video domains. C2R strategically constructs and refines
sub-questions and their answers (sub-QAs), deriving a better confidence score
for the target answer. C2R first curates a subset of sub-QAs to explore diverse
reasoning paths, then compares the confidence scores of the resulting answer
candidates to select the most reliable final answer. Since C2R relies solely on
confidence scores derived from the model itself, it can be seamlessly
integrated with various existing QA models, demonstrating consistent
performance improvements across diverse models and benchmarks. Furthermore, we
provide essential yet underexplored insights into how leveraging sub-QAs
affects model behavior, specifically analyzing the impact of both the quantity
and quality of sub-QAs on achieving robust and reliable reasoning.

</details>


### [25] [SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs](https://arxiv.org/abs/2509.20758)
*Jiacheng Lin,Zhongruo Wang,Kun Qian,Tian Wang,Arvind Srinivasan,Hansi Zeng,Ruochen Jiao,Xie Zhou,Jiri Gesi,Dakuo Wang,Yufan Guo,Kai Zhong,Weiqi Zhang,Sujay Sanghavi,Changyou Chen,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 本文研究了监督微调对大型语言模型的影响，发现使用较小的学习率可以减轻通用性能下降，同时保持目标领域的性能。此外，本文提出了一种新的方法TALR，并评估了多种策略，最终发现TALR在平衡领域特定性能和通用能力方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 本文旨在重新审视监督微调（SFT）在领域特定数据集上的权衡问题，并提供实证和理论见解，以找到一种更好的方法来平衡领域特定性能和通用能力。

Method: 本文提出了一种新的方法，称为Token-Adaptive Loss Reweighting (TALR)，并评估了多种减少通用能力损失的策略，包括L2正则化、LoRA、模型平均、FLOW和TALR。

Result: 实验结果表明，虽然没有方法能完全消除权衡，但TALR在平衡领域特定收益和通用能力方面始终优于这些基线。

Conclusion: 本文提出了TALR方法，并总结了适应LLM到新领域的实践指南：(i) 使用小学习率以实现有利的权衡，(ii) 当需要更强的平衡时，采用TALR作为一种有效策略。

Abstract: Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach
to adapt Large Language Models (LLMs) to specialized tasks but is often
believed to degrade their general capabilities. In this work, we revisit this
trade-off and present both empirical and theoretical insights. First, we show
that SFT does not always hurt: using a smaller learning rate can substantially
mitigate general performance degradation while preserving comparable
target-domain performance. We then provide a theoretical analysis that explains
these phenomena and further motivates a new method, Token-Adaptive Loss
Reweighting (TALR). Building on this, and recognizing that smaller learning
rates alone do not fully eliminate general-performance degradation in all
cases, we evaluate a range of strategies for reducing general capability loss,
including L2 regularization, LoRA, model averaging, FLOW, and our proposed
TALR. Experimental results demonstrate that while no method completely
eliminates the trade-off, TALR consistently outperforms these baselines in
balancing domain-specific gains and general capabilities. Finally, we distill
our findings into practical guidelines for adapting LLMs to new domains: (i)
using a small learning rate to achieve a favorable trade-off, and (ii) when a
stronger balance is further desired, adopt TALR as an effective strategy.

</details>


### [26] [Towards Atoms of Large Language Models](https://arxiv.org/abs/2509.20784)
*Chenhui Hu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出了Atoms理论，通过引入原子内积（AIP）和证明原子满足限制等距性质（RIP），验证了原子在LLMs中的有效性，并展示了其在稀疏重建和可解释性方面的优势。


<details>
  <summary>Details</summary>
Motivation: LLMs的内部表示的基本单元尚未明确，这限制了对其机制的理解。神经元或特征常被视为这些单元，但神经元存在多义性，而特征则面临不可靠重建和不稳定的问题。

Method: 我们提出了Atoms理论，引入了原子内积（AIP）来校正表示偏移，形式化定义了原子，并证明了原子满足限制等距性质（RIP），确保了原子集上的稳定稀疏表示，并与压缩感知相关联。此外，我们还建立了稀疏表示的唯一性和精确ℓ1恢复性，并提供了保证单层稀疏自编码器（SAEs）可以可靠地识别原子。

Result: 我们在Gemma2-2B、Gemma2-9B和Llama3.1-8B上训练了阈值激活的SAEs，平均在各层实现了99.9%的稀疏重建，超过99.8%的原子满足唯一性条件，相比神经元的0.5%和特征的68.2%，表明原子更能忠实捕捉LLMs的内在表示。

Conclusion: 本文系统地介绍了并验证了LLMs的Atoms理论，为理解内部表示提供了一个理论框架，并为机制可解释性奠定了基础。

Abstract: The fundamental units of internal representations in large language models
(LLMs) remain undefined, limiting further understanding of their mechanisms.
Neurons or features are often regarded as such units, yet neurons suffer from
polysemy, while features face concerns of unreliable reconstruction and
instability. To address this issue, we propose the Atoms Theory, which defines
such units as atoms. We introduce the atomic inner product (AIP) to correct
representation shifting, formally define atoms, and prove the conditions that
atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse
representations over atom set and linking to compressed sensing. Under stronger
conditions, we further establish the uniqueness and exact $\ell_1$
recoverability of the sparse representations, and provide guarantees that
single-layer sparse autoencoders (SAEs) with threshold activations can reliably
identify the atoms. To validate the Atoms Theory, we train threshold-activated
SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse
reconstruction across layers on average, and more than 99.8% of atoms satisfy
the uniqueness condition, compared to 0.5% for neurons and 68.2% for features,
showing that atoms more faithfully capture intrinsic representations of LLMs.
Scaling experiments further reveal the link between SAEs size and recovery
capacity. Overall, this work systematically introduces and validates Atoms
Theory of LLMs, providing a theoretical framework for understanding internal
representations and a foundation for mechanistic interpretability. Code
available at https://github.com/ChenhuiHu/towards_atoms.

</details>


### [27] [Few-Shot and Training-Free Review Generation via Conversational Prompting](https://arxiv.org/abs/2509.20805)
*Genki Kusano*

Main category: cs.CL

TL;DR: 本文提出了一种名为对话提示的方法，通过将用户评论重新表述为多轮对话来解决少样本和无训练情况下的评论生成问题。实验表明，该方法能生成更符合目标用户风格的评论。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数方法假设目标用户有丰富的评论历史或需要额外的模型训练。然而，在现实应用中，常常面临少样本和无训练的情况，其中只有少数用户评论可用且微调不可行。

Method: 我们提出了对话提示，这是一种轻量级方法，将用户评论重新表述为多轮对话。其简单变体（SCP）仅依赖于用户自己的评论，而对比变体（CCP）则插入其他用户的评论或LLMs作为错误回复，然后让模型进行纠正，鼓励模型以用户风格生成文本。

Result: 实验结果表明，传统的非对话提示通常会产生与随机用户撰写的评论相似的评论，而SCP和CCP生成的评论更接近目标用户的评论，即使每个用户只有两条评论。当有高质量的负面示例时，CCP进一步提高了性能，而当无法收集此类数据时，SCP仍然具有竞争力。

Conclusion: 这些结果表明，对话提示为在少样本和无训练约束下的评论生成提供了一个实用的解决方案。

Abstract: Personalized review generation helps businesses understand user preferences,
yet most existing approaches assume extensive review histories of the target
user or require additional model training. Real-world applications often face
few-shot and training-free situations, where only a few user reviews are
available and fine-tuning is infeasible. It is well known that large language
models (LLMs) can address such low-resource settings, but their effectiveness
depends on prompt engineering. In this paper, we propose Conversational
Prompting, a lightweight method that reformulates user reviews as multi-turn
conversations. Its simple variant, Simple Conversational Prompting (SCP),
relies solely on the user's own reviews, while the contrastive variant,
Contrastive Conversational Prompting (CCP), inserts reviews from other users or
LLMs as incorrect replies and then asks the model to correct them, encouraging
the model to produce text in the user's style. Experiments on eight product
domains and five LLMs showed that the conventional non-conversational prompt
often produced reviews similar to those written by random users, based on
text-based metrics such as ROUGE-L and BERTScore, and application-oriented
tasks like user identity matching and sentiment analysis. In contrast, both SCP
and CCP produced reviews much closer to those of the target user, even when
each user had only two reviews. CCP brings further improvements when
high-quality negative examples are available, whereas SCP remains competitive
when such data cannot be collected. These results suggest that conversational
prompting offers a practical solution for review generation under few-shot and
training-free constraints.

</details>


### [28] [Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching](https://arxiv.org/abs/2509.20810)
*Songze Li,Zhiqiang Liu,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Enrich-on-Graph（EoG）的灵活框架，利用大语言模型（LLM）的先验知识来丰富知识图谱（KG），弥合图谱和查询之间的语义差距。EoG能够在保证低计算成本、可扩展性和适应性的同时，从KG中高效提取证据以进行精确和鲁棒的推理。此外，我们提出了三个图质量评估指标来分析KGQA任务中的查询-图对齐，并通过理论验证了我们的优化目标。在两个KGQA基准数据集上的实验表明，EoG能够有效生成高质量的KG并达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: We attribute this to the semantic gap between structured knowledge graphs (KGs) and unstructured queries, caused by inherent differences in their focuses and structures. Existing methods usually employ resource-intensive, non-scalable workflows reasoning on vanilla KGs, but overlook this gap.

Method: We propose a flexible framework, Enrich-on-Graph (EoG), which leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between graphs and queries.

Result: Extensive experiments on two KGQA benchmark datasets indicate that EoG can effectively generate high-quality KGs and achieve the state-of-the-art performance.

Conclusion: EoG can effectively generate high-quality KGs and achieve the state-of-the-art performance.

Abstract: Large Language Models (LLMs) exhibit strong reasoning capabilities in complex
tasks. However, they still struggle with hallucinations and factual errors in
knowledge-intensive scenarios like knowledge graph question answering (KGQA).
We attribute this to the semantic gap between structured knowledge graphs (KGs)
and unstructured queries, caused by inherent differences in their focuses and
structures. Existing methods usually employ resource-intensive, non-scalable
workflows reasoning on vanilla KGs, but overlook this gap. To address this
challenge, we propose a flexible framework, Enrich-on-Graph (EoG), which
leverages LLMs' prior knowledge to enrich KGs, bridge the semantic gap between
graphs and queries. EoG enables efficient evidence extraction from KGs for
precise and robust reasoning, while ensuring low computational costs,
scalability, and adaptability across different methods. Furthermore, we propose
three graph quality evaluation metrics to analyze query-graph alignment in KGQA
task, supported by theoretical validation of our optimization objectives.
Extensive experiments on two KGQA benchmark datasets indicate that EoG can
effectively generate high-quality KGs and achieve the state-of-the-art
performance. Our code and data are available at
https://github.com/zjukg/Enrich-on-Graph.

</details>


### [29] [Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection](https://arxiv.org/abs/2509.20811)
*Taehee Park,Heejin Do,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: PoCO是一种新颖的方法，旨在通过利用LLM的生成能力同时保留较小监督模型的可靠性，战略性地平衡召回率和精确度，从而提高语法错误纠正的整体质量。


<details>
  <summary>Details</summary>
Motivation: 为了有效利用LLM的优势来解决sLMs的召回率挑战，我们提出了PoCO，这是一种新颖的方法，旨在战略性地平衡召回率和精确度。

Method: PoCO首先通过LLM有意触发过度纠正以最大化召回率，然后通过微调较小的模型进行有针对性的后期纠正步骤，以识别和精炼错误输出。

Result: 我们的广泛实验表明，PoCO通过增加召回率并保持竞争性精度，有效地平衡了GEC性能，从而提高了语法错误纠正的整体质量。

Conclusion: PoCO有效地平衡了GEC性能，通过增加召回率并保持竞争性精度，最终提高了语法错误纠正的整体质量。

Abstract: Robust supervised fine-tuned small Language Models (sLMs) often show high
reliability but tend to undercorrect. They achieve high precision at the cost
of low recall. Conversely, Large Language Models (LLMs) often show the opposite
tendency, making excessive overcorrection, leading to low precision. To
effectively harness the strengths of LLMs to address the recall challenges in
sLMs, we propose Post-Correction via Overcorrection (PoCO), a novel approach
that strategically balances recall and precision. PoCO first intentionally
triggers overcorrection via LLM to maximize recall by allowing comprehensive
revisions, then applies a targeted post-correction step via fine-tuning smaller
models to identify and refine erroneous outputs. We aim to harmonize both
aspects by leveraging the generative power of LLMs while preserving the
reliability of smaller supervised models. Our extensive experiments demonstrate
that PoCO effectively balances GEC performance by increasing recall with
competitive precision, ultimately improving the overall quality of grammatical
error correction.

</details>


### [30] [Distilling Many-Shot In-Context Learning into a Cheat Sheet](https://arxiv.org/abs/2509.20820)
*Ukyo Honda,Soichiro Murakami,Peinan Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种称为备忘录式ICL的方法，通过将许多示例ICL的信息提炼成一个简洁的文本摘要，以减少计算需求并提高效率。


<details>
  <summary>Details</summary>
Motivation: 为了应对许多示例ICL带来的高计算需求问题，提出了备忘录式ICL方法。

Method: 提出了一种称为备忘录式ICL的方法，该方法将许多示例ICL的信息提炼成一个简洁的文本摘要（备忘录），在推理时作为上下文使用。

Result: 实验表明，备忘录式ICL在使用更少标记的情况下，表现与许多示例ICL相当或更好，并且不需要测试时的检索。

Conclusion: 这些发现表明，备忘录式ICL是利用LLMs进行下游任务的实用替代方案。

Abstract: Recent advances in large language models (LLMs) enable effective in-context
learning (ICL) with many-shot examples, but at the cost of high computational
demand due to longer input tokens. To address this, we propose cheat-sheet ICL,
which distills the information from many-shot ICL into a concise textual
summary (cheat sheet) used as the context at inference time. Experiments on
challenging reasoning tasks show that cheat-sheet ICL achieves comparable or
better performance than many-shot ICL with far fewer tokens, and matches
retrieval-based ICL without requiring test-time retrieval. These findings
demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs
in downstream tasks.

</details>


### [31] [Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search](https://arxiv.org/abs/2509.20838)
*Shuo Huang,Xingliang Yuan,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CL

TL;DR: 本文提出一种基于树搜索的迭代句子重写算法，以在保护隐私的同时保持文本的自然性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本匿名化和去标识化技术难以在隐私保护与文本自然性和实用性之间取得平衡。

Method: 我们提出了一种零样本、基于树搜索的迭代句子重写算法，通过结构化搜索引导奖励模型，逐步重写隐私敏感部分。

Result: 实验表明，我们的方法在隐私敏感数据集上显著优于现有基线。

Conclusion: 我们的方法在隐私保护和实用性之间取得了更好的平衡。

Abstract: The increasing adoption of large language models (LLMs) in cloud-based
services has raised significant privacy concerns, as user inputs may
inadvertently expose sensitive information. Existing text anonymization and
de-identification techniques, such as rule-based redaction and scrubbing, often
struggle to balance privacy preservation with text naturalness and utility. In
this work, we propose a zero-shot, tree-search-based iterative sentence
rewriting algorithm that systematically obfuscates or deletes private
information while preserving coherence, relevance, and naturalness. Our method
incrementally rewrites privacy-sensitive segments through a structured search
guided by a reward model, enabling dynamic exploration of the rewriting space.
Experiments on privacy-sensitive datasets show that our approach significantly
outperforms existing baselines, achieving a superior balance between privacy
protection and utility preservation.

</details>


### [32] [Concise and Sufficient Sub-Sentence Citations for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20859)
*Guo Chen,Qiuyuan Li,Qiuxian Li,Hongliang Dai,Xiang Chen,Piji Li*

Main category: cs.CL

TL;DR: 本文提出生成子句引用，以减少用户确认生成输出正确性所需的努力。


<details>
  <summary>Details</summary>
Motivation: 在现有的归属方法中，引用通常以句子或甚至段落级别提供。长句子或段落可能包含大量不相关的内容。句子级别的引用可能遗漏对于验证输出至关重要的信息，迫使用户阅读周围的上下文。

Method: 我们首先制定了此类引用的注释指南并构建了相应的数据集。然后，我们提出了一个归因框架，用于生成符合我们标准的引用。该框架利用大语言模型自动为我们的任务生成微调数据，并使用信用模型过滤掉低质量的例子。

Result: 我们在构建的数据集上的实验表明，所提出的方法可以生成高质量且更易读的引用。

Conclusion: 我们的方法能够生成高质量且更易读的引用。

Abstract: In retrieval-augmented generation (RAG) question answering systems,
generating citations for large language model (LLM) outputs enhances
verifiability and helps users identify potential hallucinations. However, we
observe two problems in the citations produced by existing attribution methods.
First, the citations are typically provided at the sentence or even paragraph
level. Long sentences or paragraphs may include a substantial amount of
irrelevant content. Second, sentence-level citations may omit information that
is essential for verifying the output, forcing users to read the surrounding
context. In this paper, we propose generating sub-sentence citations that are
both concise and sufficient, thereby reducing the effort required by users to
confirm the correctness of the generated output. To this end, we first develop
annotation guidelines for such citations and construct a corresponding dataset.
Then, we propose an attribution framework for generating citations that adhere
to our standards. This framework leverages LLMs to automatically generate
fine-tuning data for our task and employs a credit model to filter out
low-quality examples. Our experiments on the constructed dataset demonstrate
that the propose approach can generate high-quality and more readable
citations.

</details>


### [33] [WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs](https://arxiv.org/abs/2509.20863)
*Guowei Xu,Wenxin Xu,Jiawang Zhao,Kaisheng Ma*

Main category: cs.CL

TL;DR: 本文提出了一种名为WeFT的加权监督微调方法，用于扩散语言模型，通过基于熵的令牌权重分配来提高生成的一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语言建模中表现出色，但其在每个去噪步骤中缺乏精确的概率估计，导致生成过程不可预测和不一致。因此，需要一种方法来控制关键令牌以引导生成方向。

Method: 提出了一种基于熵的加权监督微调方法（WeFT），通过为不同令牌分配不同的权重来控制生成过程。

Result: 在四个广泛使用的推理基准（Sudoku、Countdown、GSM8K和MATH-500）上，WeFT在s1K、s1K-1.1和3k样本上分别实现了39%、64%和83%的相对改进。

Conclusion: WeFT方法在多个推理基准上实现了显著的改进，表明其在扩散语言模型中的有效性。

Abstract: Diffusion models have recently shown strong potential in language modeling,
offering faster generation compared to traditional autoregressive approaches.
However, applying supervised fine-tuning (SFT) to diffusion models remains
challenging, as they lack precise probability estimates at each denoising step.
While the diffusion mechanism enables the model to reason over entire
sequences, it also makes the generation process less predictable and often
inconsistent. This highlights the importance of controlling key tokens that
guide the direction of generation. To address this issue, we propose WeFT, a
weighted SFT method for diffusion language models, where tokens are assigned
different weights based on their entropy. Derived from diffusion theory, WeFT
delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from
open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard
SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and
MATH-500). The code and models will be made publicly available.

</details>


### [34] [Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models](https://arxiv.org/abs/2509.20866)
*Pittawat Taveekitworachai,Natpatchara Pongjirapat,Krittaphas Chaisutyakorn,Piyalitt Ittichaiwong,Tossaporn Saengja,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本文研究了如何使医疗推理模型生成排名列表的答案，提出了提示和微调两种方法，并发现强化学习微调在多种答案格式上表现更优。


<details>
  <summary>Details</summary>
Motivation: 临床决策很少依赖单一答案，而是考虑多个选项，以减少狭隘视角的风险。然而，当前的医疗推理模型通常只生成一个答案，即使在开放性问题中也是如此。

Method: 本文提出了两种方法：提示和微调。通过监督微调（SFT）和强化学习微调（RFT）训练和评估医疗推理模型，并提出了针对排名列表答案格式的新奖励函数。

Result: 结果表明，虽然一些SFT模型可以推广到某些答案格式，但使用RFT训练的模型在多种格式上更加稳健。此外，在修改后的MedQA案例研究中发现，尽管医疗推理模型可能无法选择基准的首选真实答案，但它们可以识别有效的答案。

Conclusion: 本文是首次系统研究如何使医疗推理模型生成排名列表的答案，希望为医疗领域提供超越单一答案的替代答案格式的第一步。

Abstract: This paper presents a systematic study on enabling medical reasoning models
(MRMs) to generate ranked lists of answers for open-ended questions. Clinical
decision-making rarely relies on a single answer but instead considers multiple
options, reducing the risks of narrow perspectives. Yet current MRMs are
typically trained to produce only one answer, even in open-ended settings. We
propose an alternative format: ranked lists and investigate two approaches:
prompting and fine-tuning. While prompting is a cost-effective way to steer an
MRM's response, not all MRMs generalize well across different answer formats:
choice, short text, and list answers. Based on our prompting findings, we train
and evaluate MRMs using supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT teaches a model to imitate annotated responses, and RFT
incentivizes exploration through the responses that maximize a reward. We
propose new reward functions targeted at ranked-list answer formats, and
conduct ablation studies for RFT. Our results show that while some SFT models
generalize to certain answer formats, models trained with RFT are more robust
across multiple formats. We also present a case study on a modified MedQA with
multiple valid answers, finding that although MRMs might fail to select the
benchmark's preferred ground truth, they can recognize valid answers. To the
best of our knowledge, this is the first systematic investigation of approaches
for enabling MRMs to generate answers as ranked lists. We hope this work
provides a first step toward developing alternative answer formats that are
beneficial beyond single answers in medical domains.

</details>


### [35] [Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization](https://arxiv.org/abs/2509.20900)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: 本文介绍了一种新的对抗性多智能体框架SummQ，用于解决长文档摘要中的问题，并在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理过长文档时面临信息丢失、事实不一致和连贯性问题，因此需要一种更有效的长文档摘要方法。

Method: 本文提出了SummQ，一个基于对抗性多智能体框架的方法，其中包含总结生成器和评审员、测验生成器和评审员以及一个检查员代理，以迭代方式改进摘要质量。

Result: SummQ在三个广泛使用的长文档摘要基准测试中表现出色，显著优于现有的最先进的方法，并在LLM-as-a-Judge和人类评估中也表现良好。

Conclusion: 本文提出了一种新的对抗性多智能体框架SummQ，通过总结和测验领域的协作智能解决了长文档摘要中的信息丢失、事实不一致和连贯性问题。实验结果表明，该框架在多个指标上优于现有最先进的方法，并建立了基于对抗性智能体协作的长文档摘要新方法。

Abstract: Long document summarization remains a significant challenge for current large
language models (LLMs), as existing approaches commonly struggle with
information loss, factual inconsistencies, and coherence issues when processing
excessively long documents. We propose SummQ, a novel adversarial multi-agent
framework that addresses these limitations through collaborative intelligence
between specialized agents operating in two complementary domains:
summarization and quizzing. Our approach employs summary generators and
reviewers that work collaboratively to create and evaluate comprehensive
summaries, while quiz generators and reviewers create comprehension questions
that serve as continuous quality checks for the summarization process. This
adversarial dynamic, enhanced by an examinee agent that validates whether the
generated summary contains the information needed to answer the quiz questions,
enables iterative refinement through multifaceted feedback mechanisms. We
evaluate SummQ on three widely used long document summarization benchmarks.
Experimental results demonstrate that our framework significantly outperforms
existing state-of-the-art methods across ROUGE and BERTScore metrics, as well
as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal
the effectiveness of the multi-agent collaboration dynamics, the influence of
different agent configurations, and the impact of the quizzing mechanism. This
work establishes a new approach for long document summarization that uses
adversarial agentic collaboration to improve summarization quality.

</details>


### [36] [MemLens: Uncovering Memorization in LLMs with Activation Trajectories](https://arxiv.org/abs/2509.20909)
*Zirui He,Haiyan Zhao,Ali Payani,Mengnan du*

Main category: cs.CL

TL;DR: 本文提出 MemLens 来检测大型语言模型中的记忆行为，通过分析生成过程中数字标记的概率轨迹，发现污染样本和干净样本有明显的不同轨迹模式。


<details>
  <summary>Details</summary>
Motivation: 现有的检测方法在面对隐式污染数据时表现不佳，需要一种更有效的检测方法。

Method: MemLens 通过分析生成过程中数字标记的概率轨迹来检测记忆行为。

Result: 污染样本表现出“捷径”行为，而干净样本则显示出更渐进的证据积累。注入设计好的样本后，观察到相同的轨迹模式。

Conclusion: MemLens 提供了对记忆行为的真正信号的捕捉，而不是偶然的相关性。

Abstract: Large language models (LLMs) are commonly evaluated on challenging benchmarks
such as AIME and Math500, which are susceptible to contamination and risk of
being memorized. Existing detection methods, which primarily rely on
surface-level lexical overlap and perplexity, demonstrate low generalization
and degrade significantly when encountering implicitly contaminated data. In
this paper, we propose MemLens (An Activation Lens for Memorization Detection)
to detect memorization by analyzing the probability trajectories of numeric
tokens during generation. Our method reveals that contaminated samples exhibit
``shortcut'' behaviors, locking onto an answer with high confidence in the
model's early layers, whereas clean samples show more gradual evidence
accumulation across the model's full depth. We observe that contaminated and
clean samples exhibit distinct and well-separated reasoning trajectories. To
further validate this, we inject carefully designed samples into the model
through LoRA fine-tuning and observe the same trajectory patterns as in
naturally contaminated data. These results provide strong evidence that MemLens
captures genuine signals of memorization rather than spurious correlations.

</details>


### [37] [Cross-Linguistic Analysis of Memory Load in Sentence Comprehension: Linear Distance and Structural Density](https://arxiv.org/abs/2509.20916)
*Krishna Aggarwal*

Main category: cs.CL

TL;DR: 本研究探讨了句子级记忆负荷的解释因素，发现句子长度对记忆负荷影响最大，而介入复杂度提供了额外的解释力。研究结果调和了线性和层次化的局部性观点，并展示了如何利用基于UD的图度量和跨语言混合效应模型来评估记忆负荷的竞争理论。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨句子级记忆负荷是否更好地由句法相关词之间的线性接近度还是中间材料的结构密度解释。

Method: 研究使用了统一依存树库和跨语言混合效应框架，联合评估了句子长度、依赖长度和介入复杂度作为记忆负荷的预测因素。

Result: 所有三个因素都与记忆负荷正相关，其中句子长度影响最大，而介入复杂度在解释线性距离之外提供了额外的解释力。

Conclusion: 研究结果表明，句子长度对记忆负荷有最广泛的影响，而介入复杂度在解释线性距离之外提供了额外的解释力。概念上，研究结果调和了线性和层次化的局部性观点，将依赖长度视为一个重要表面特征，同时将介入头作为更接近的整合和维护需求指标。方法上，研究展示了如何利用基于UD的图度量和跨语言混合效应模型来区分线性和结构贡献，为评估句子理解中的记忆负荷竞争理论提供了一条原则性的路径。

Abstract: This study examines whether sentence-level memory load in comprehension is
better explained by linear proximity between syntactically related words or by
the structural density of the intervening material. Building on locality-based
accounts and cross-linguistic evidence for dependency length minimization, the
work advances Intervener Complexity-the number of intervening heads between a
head and its dependent-as a structurally grounded lens that refines linear
distance measures. Using harmonized dependency treebanks and a mixed-effects
framework across multiple languages, the analysis jointly evaluates sentence
length, dependency length, and Intervener Complexity as predictors of the
Memory-load measure. Studies in Psycholinguistics have reported the
contributions of feature interference and misbinding to memory load during
processing. For this study, I operationalized sentence-level memory load as the
linear sum of feature misbinding and feature interference for tractability;
current evidence does not establish that their cognitive contributions combine
additively. All three factors are positively associated with memory load, with
sentence length exerting the broadest influence and Intervener Complexity
offering explanatory power beyond linear distance. Conceptually, the findings
reconcile linear and hierarchical perspectives on locality by treating
dependency length as an important surface signature while identifying
intervening heads as a more proximate indicator of integration and maintenance
demands. Methodologically, the study illustrates how UD-based graph measures
and cross-linguistic mixed-effects modelling can disentangle linear and
structural contributions to processing efficiency, providing a principled path
for evaluating competing theories of memory load in sentence comprehension.

</details>


### [38] [Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning](https://arxiv.org/abs/2509.20957)
*Asim Ersoy,Enes Altinisik,Husrev Taha Sencar,Kareem Darwish*

Main category: cs.CL

TL;DR: 本文研究了阿拉伯语中工具调用的必要性、通用指令微调的影响以及特定高优先级工具微调的价值，并通过实验验证了这些方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前关于工具调用的研究和资源主要以英语为中心，这导致了对其他语言（如阿拉伯语）如何实现此功能的理解不足。

Method: 本文通过使用基础和后训练的开放权重阿拉伯语LLM进行广泛的实验来解决这些问题。此外，为了促进研究，将两个开源工具调用数据集翻译并适应为阿拉伯语。

Result: 本文的结果揭示了在阿拉伯语中开发强大工具增强代理的最佳策略。

Conclusion: 本文提供了关于如何为阿拉伯语开发强大工具增强代理的宝贵见解。

Abstract: Tool calling is a critical capability that allows Large Language Models
(LLMs) to interact with external systems, significantly expanding their
utility. However, research and resources for tool calling are predominantly
English-centric, leaving a gap in our understanding of how to enable this
functionality for other languages, such as Arabic. This paper investigates
three key research questions: (1) the necessity of in-language (Arabic)
tool-calling data versus relying on cross-lingual transfer, (2) the effect of
general-purpose instruction tuning on tool-calling performance, and (3) the
value of fine-tuning on specific, high-priority tools. To address these
questions, we conduct extensive experiments using base and post-trained
variants of an open-weight Arabic LLM. To enable this study, we bridge the
resource gap by translating and adapting two open-source tool-calling datasets
into Arabic. Our findings provide crucial insights into the optimal strategies
for developing robust tool-augmented agents for Arabic.

</details>


### [39] [Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting](https://arxiv.org/abs/2509.20982)
*Valeria Ramirez-Garcia,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.CL

TL;DR: 本研究探讨了基于大型语言模型的自动评估系统，发现参考辅助评估是最有效的评估方法。


<details>
  <summary>Details</summary>
Motivation: 在教育领域，LLMs 被研究作为学生和教师的辅助工具。本研究调查了基于 LLM 的自动评估系统，用于学术文本输入问题的评分。

Method: 研究 investigates LLM-driven automatic evaluation systems for academic Text-Input Problems using rubrics. 五种评估系统包括: JudgeLM 评估、参考辅助评估、无参考评估、加法评估和自适应评估。

Result: 结果显示，参考辅助评估是使用 LLM 自动评估和评分文本输入问题的最佳方法。它与人类评估相比具有最低的中位数绝对偏差 (0.945) 和最低的均方根偏差 (1.214)，提供公平的评分以及有见地和完整的评估。

Conclusion: 人工智能驱动的自动评估系统，辅以适当的方法论，显示出作为其他学术资源的补充工具的潜力。

Abstract: Large language models (LLMs) can act as evaluators, a role studied by methods
like LLM-as-a-Judge and fine-tuned judging LLMs. In the field of education,
LLMs have been studied as assistant tools for students and teachers. Our
research investigates LLM-driven automatic evaluation systems for academic
Text-Input Problems using rubrics. We propose five evaluation systems that have
been tested on a custom dataset of 110 answers about computer science from
higher education students with three models: JudgeLM, Llama-3.1-8B and
DeepSeek-R1-Distill-Llama-8B. The evaluation systems include: The JudgeLM
evaluation, which uses the model's single answer prompt to obtain a score;
Reference Aided Evaluation, which uses a correct answer as a guide aside from
the original context of the question; No Reference Evaluation, which ommits the
reference answer; Additive Evaluation, which uses atomic criteria; and Adaptive
Evaluation, which is an evaluation done with generated criteria fitted to each
question. All evaluation methods have been compared with the results of a human
evaluator. Results show that the best method to automatically evaluate and
score Text-Input Problems using LLMs is Reference Aided Evaluation. With the
lowest median absolute deviation (0.945) and the lowest root mean square
deviation (1.214) when compared to human evaluation, Reference Aided Evaluation
offers fair scoring as well as insightful and complete evaluations. Other
methods such as Additive and Adaptive Evaluation fail to provide good results
in concise answers, No Reference Evaluation lacks information needed to
correctly assess questions and JudgeLM Evaluations have not provided good
results due to the model's limitations. As a result, we conclude that
Artificial Intelligence-driven automatic evaluation systems, aided with proper
methodologies, show potential to work as complementary tools to other academic
resources.

</details>


### [40] [Generative AI for FFRDCs](https://arxiv.org/abs/2509.21040)
*Arun S. Maiya*

Main category: cs.CL

TL;DR: 本文探讨了如何利用大语言模型和OnPrem.LLM框架来提高联邦资助研究和开发中心对文本工作负载的处理效率。


<details>
  <summary>Details</summary>
Motivation: 联邦资助研究和开发中心面临大量的文本工作负载，手动分析效率低下。

Method: 使用OnPrem.LLM框架，这是一种用于安全和灵活应用生成式AI的开源框架。

Result: 案例研究显示，这种方法提高了监督和战略分析的效果。

Conclusion: 本文展示了如何利用大语言模型加速对联邦资助研究和开发中心的文本工作负载的处理，同时保持审计能力和数据主权。

Abstract: Federally funded research and development centers (FFRDCs) face text-heavy
workloads, from policy documents to scientific and engineering papers, that are
slow to analyze manually. We show how large language models can accelerate
summarization, classification, extraction, and sense-making with only a few
input-output examples. To enable use in sensitive government contexts, we apply
OnPrem$.$LLM, an open-source framework for secure and flexible application of
generative AI. Case studies on defense policy documents and scientific corpora,
including the National Defense Authorization Act (NDAA) and National Science
Foundation (NSF) Awards, demonstrate how this approach enhances oversight and
strategic analysis while maintaining auditability and data sovereignty.

</details>


### [41] [Behind RoPE: How Does Causal Mask Encode Positional Information?](https://arxiv.org/abs/2509.21042)
*Junu Kim,Xiao Liu,Zhenghao Lin,Lei Ji,Yeyun Gong,Edward Choi*

Main category: cs.CL

TL;DR: 本研究揭示了因果掩码在Transformer解码器中作为位置信息来源的重要性，并展示了其与RoPE的相互作用如何影响注意力模式。


<details>
  <summary>Details</summary>
Motivation: 尽管显式的位置编码（如RoPE）是Transformer解码器中的主要位置信息来源，但因果掩码也提供了位置信息。我们希望了解因果掩码是否能产生位置依赖的注意力模式。

Method: 我们通过理论分析和实证分析研究了因果掩码对注意力分数的影响，并观察了其与RoPE的相互作用。

Result: 我们的理论分析表明，因果掩码可以诱导出位置依赖的注意力模式，即使输入中没有参数或因果依赖。实证分析显示，训练后的模型表现出相同的行为，而学习到的参数进一步放大了这些模式。此外，我们发现因果掩码和RoPE的相互作用会扭曲RoPE的相对注意力模式，使其变为非相对的。

Conclusion: 我们的研究表明，因果掩码在Transformer解码器中可以作为位置信息的来源，与显式的位置编码（如RoPE）一样重要。

Abstract: While explicit positional encodings such as RoPE are a primary source of
positional information in Transformer decoders, the causal mask also provides
positional information. In this work, we prove that the causal mask can induce
position-dependent patterns in attention scores, even without parameters or
causal dependency in the input. Our theoretical analysis indicates that the
induced attention pattern tends to favor nearby query-key pairs, mirroring the
behavior of common positional encodings. Empirical analysis confirms that
trained models exhibit the same behavior, with learned parameters further
amplifying these patterns. Notably, we found that the interaction of causal
mask and RoPE distorts RoPE's relative attention score patterns into
non-relative ones. We consistently observed this effect in modern large
language models, suggesting the importance of considering the causal mask as a
source of positional information alongside explicit positional encodings.

</details>


### [42] [When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following](https://arxiv.org/abs/2509.21051)
*Keno Harada,Yudai Yamazaki,Masachika Taniguchi,Edison Marrese-Taylor,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 本研究提出了两个基准测试，用于评估大型语言模型在同时遵循多个指令时的表现，并开发了三种回归模型来估计性能。结果表明，这些模型能够在少量样本的情况下准确预测性能，为大型语言模型的评估提供了高效的方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在现实场景中的广泛应用，理解它们同时遵循多个指令的能力变得至关重要。然而，由于计算资源的限制，评估所有可能的指令组合是不现实的。因此，研究者希望通过开发回归模型来解决这一问题。

Method: 研究者引入了两个专门的基准测试：ManyIFEval 和 StyleMBPP，用于评估大型语言模型在文本生成和代码生成中同时遵循多个指令的能力。此外，他们开发了三种回归模型来估计性能，并通过实验验证了这些模型的有效性。

Result: 实验结果显示，随着指令数量的增加，大型语言模型的性能持续下降。同时，研究者开发的回归模型能够有效地估计未见过的指令组合和不同指令数量下的性能，且只需较小的样本量即可实现准确的评估。

Conclusion: 研究结果表明，随着指令数量的增加，大型语言模型（LLMs）的性能会持续下降。此外，研究者开发了三种回归模型，可以估计在未见过的指令组合和不同指令数量下的性能。这些模型能够以大约10%的误差预测遵循多个指令的性能，且只需要相对较小的样本量即可进行有效的评估。

Abstract: As large language models (LLMs) are increasingly applied to real-world
scenarios, it becomes crucial to understand their ability to follow multiple
instructions simultaneously. To systematically evaluate these capabilities, we
introduce two specialized benchmarks for fundamental domains where multiple
instructions following is important: Many Instruction-Following Eval
(ManyIFEval) for text generation with up to ten instructions, and Style-aware
Mostly Basic Programming Problems (StyleMBPP) for code generation with up to
six instructions. Our experiments with the created benchmarks across ten LLMs
reveal that performance consistently degrades as the number of instructions
increases. Furthermore, given the fact that evaluating all the possible
combinations of multiple instructions is computationally impractical in actual
use cases, we developed three types of regression models that can estimate
performance on both unseen instruction combinations and different numbers of
instructions which are not used during training. We demonstrate that a logistic
regression model using instruction count as an explanatory variable can predict
performance of following multiple instructions with approximately 10% error,
even for unseen instruction combinations. We show that relatively modest sample
sizes (500 for ManyIFEval and 300 for StyleMBPP) are sufficient for performance
estimation, enabling efficient evaluation of LLMs under various instruction
combinations.

</details>


### [43] [SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials](https://arxiv.org/abs/2509.21079)
*Qixin Wan,Zilong Wang,Jingwen Zhou,Wanting Wang,Ziheng Geng,Jiachen Liu,Ran Cao,Minghui Cheng,Lu Cheng*

Main category: cs.CL

TL;DR: 本研究介绍了SoM-1K数据集，并提出了DoI提示策略，以评估基础模型在材料强度问题上的表现。结果表明，当前模型在这些任务中表现不佳，但DoI可以有效减少视觉误解错误。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在理解复杂视觉信息方面的能力有限，因此需要一种新的方法来评估它们在多模态工程问题上的表现。

Method: 引入了SoM-1K数据集，这是第一个大规模多模态基准数据集，专门用于评估基础模型在材料强度（SoM）问题上的表现。还提出了一种称为Descriptions of Images (DoI)的新提示策略，提供严格的专家生成文本描述作为上下文。

Result: 当前的基础模型在这些工程问题上表现不佳，最佳模型仅达到56.6%的准确率。当LLMs提供DoI时，通常比提供视觉图的VLMs表现更好。

Conclusion: 本研究建立了工程AI的严格基准，并突显了在科学和工程背景下开发更强大多模态推理能力的迫切需求。

Abstract: Foundation models have shown remarkable capabilities in various domains, but
their performance on complex, multimodal engineering problems remains largely
unexplored. We introduce SoM-1K, the first large-scale multimodal benchmark
dataset dedicated to evaluating foundation models on problems in the strength
of materials (SoM). The dataset, which contains 1,065 annotated SoM problems,
mirrors real-world engineering tasks by including both textual problem
statements and schematic diagrams. Due to the limited capabilities of current
foundation models in understanding complicated visual information, we propose a
novel prompting strategy called Descriptions of Images (DoI), which provides
rigorous expert-generated text descriptions of the visual diagrams as the
context. We evaluate eight representative foundation models, including both
large language models (LLMs) and vision language models (VLMs). Our results
show that current foundation models struggle significantly with these
engineering problems, with the best-performing model achieving only 56.6%
accuracy. Interestingly, we found that LLMs, when provided with DoI, often
outperform VLMs provided with visual diagrams. A detailed error analysis
reveals that DoI plays a crucial role in mitigating visual misinterpretation
errors, suggesting that accurate text-based descriptions can be more effective
than direct image input for current foundation models. This work establishes a
rigorous benchmark for engineering AI and highlights a critical need for
developing more robust multimodal reasoning capabilities in foundation models,
particularly in scientific and engineering contexts.

</details>


### [44] [Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs](https://arxiv.org/abs/2509.21080)
*Yixin Wan,Xingrun Chen,Kai-Wei Chang*

Main category: cs.CL

TL;DR: 本文研究了大语言模型中的文化定位偏差，并提出了两种有效的缓解方法，以提高模型生成内容的公平性。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型可能加剧文化公平问题，本文旨在识别并系统研究这种文化定位偏差，并提出有效的缓解方法。

Method: 本文提出了CultureLens基准测试，以及两种推理时的缓解方法：基于提示的公平干预支柱（FIP）方法和基于公平代理的缓解框架（MFA），包括单代理（MFA-SA）和多代理（MFA-MA）两种管道。

Result: 实验证明，虽然模型在88%以上的美国情境脚本中采用内部语气，但在较少主导的文化中却主要采用外部立场。基于代理的方法在减轻偏差方面表现出色。

Conclusion: 本文提出了一种基于代理的方法，用于减轻生成式大语言模型中的文化定位偏差，实验证明了这些方法在缓解偏差方面的有效性。

Abstract: Large language models (LLMs) have unlocked a wide range of downstream
generative applications. However, we found that they also risk perpetuating
subtle fairness issues tied to culture, positioning their generations from the
perspectives of the mainstream US culture while demonstrating salient
externality towards non-mainstream ones. In this work, we identify and
systematically investigate this novel culture positioning bias, in which an
LLM's default generative stance aligns with a mainstream view and treats other
cultures as outsiders. We propose the CultureLens benchmark with 4000
generation prompts and 3 evaluation metrics for quantifying this bias through
the lens of a culturally situated interview script generation task, in which an
LLM is positioned as an onsite reporter interviewing local people across 10
diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a
stark pattern: while models adopt insider tones in over 88 percent of
US-contexted scripts on average, they disproportionately adopt mainly outsider
stances for less dominant cultures. To resolve these biases, we propose 2
inference-time mitigation methods: a baseline prompt-based Fairness
Intervention Pillars (FIP) method, and a structured Mitigation via Fairness
Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)
introduces a self-reflection and rewriting loop based on fairness guidelines.
(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized
agents: a Planner Agent(initial script generation), a Critique Agent (evaluates
initial script against fairness pillars), and a Refinement Agent (incorporates
feedback to produce a polished, unbiased script). Empirical results showcase
the effectiveness of agent-based methods as a promising direction for
mitigating biases in generative LLMs.

</details>


### [45] [PerHalluEval: Persian Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2509.21104)
*Mohammad Hosseini,Kimia Hosseini,Shayan Bali,Zahra Zanjani,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 本文提出了PerHalluEval，这是第一个针对波斯语的动态幻觉评估基准。评估结果显示，LLMs在检测波斯语幻觉文本方面存在困难，但提供外部知识可以部分缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 幻觉是影响所有大型语言模型（LLMs）的一个持续性问题，尤其是在低资源语言如波斯语中。因此，需要一个专门针对波斯语的幻觉评估基准来评估LLMs的表现。

Method: 我们提出了PerHalluEval，这是一个针对波斯语的动态幻觉评估基准。该基准利用了一个三阶段的LLM驱动的管道，并结合了人工验证，以生成关于问答和摘要任务的合理答案和摘要，重点关注检测外在和内在幻觉。此外，我们使用生成标记的日志概率来选择最可信的幻觉实例，并让人工标注者突出波斯语特定的上下文以评估LLMs在与波斯文化相关的内容上的表现。

Result: 对12个LLMs（包括开源和闭源模型）使用PerHalluEval进行评估后，发现模型在检测波斯语幻觉文本方面普遍遇到困难。提供外部知识可以部分缓解幻觉问题，而且专门针对波斯语训练的模型与其他模型在幻觉方面没有显著差异。

Conclusion: 我们的评估显示，模型在检测波斯语幻觉文本方面普遍遇到困难。提供外部知识可以部分缓解幻觉问题，而且专门针对波斯语训练的模型与其他模型在幻觉方面没有显著差异。

Abstract: Hallucination is a persistent issue affecting all large language Models
(LLMs), particularly within low-resource languages such as Persian.
PerHalluEval (Persian Hallucination Evaluation) is the first dynamic
hallucination evaluation benchmark tailored for the Persian language. Our
benchmark leverages a three-stage LLM-driven pipeline, augmented with human
validation, to generate plausible answers and summaries regarding QA and
summarization tasks, focusing on detecting extrinsic and intrinsic
hallucinations. Moreover, we used the log probabilities of generated tokens to
select the most believable hallucinated instances. In addition, we engaged
human annotators to highlight Persian-specific contexts in the QA dataset in
order to evaluate LLMs' performance on content specifically related to Persian
culture. Our evaluation of 12 LLMs, including open- and closed-source models
using PerHalluEval, revealed that the models generally struggle in detecting
hallucinated Persian text. We showed that providing external knowledge, i.e.,
the original document for the summarization task, could mitigate hallucination
partially. Furthermore, there was no significant difference in terms of
hallucination when comparing LLMs specifically trained for Persian with others.

</details>


### [46] [BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback](https://arxiv.org/abs/2509.21106)
*Hyunseo Kim,Sangam Lee,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 本文提出了 BESPOKE，这是一个用于评估搜索增强型大型语言模型个性化的现实基准，通过收集真实的人类聊天和搜索历史，并配以细粒度偏好评分和反馈。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索增强型大型语言模型在满足多样化的用户需求方面仍不足，需要识别同一查询如何反映不同用户的意图，并以首选形式提供信息。尽管最近的系统如 ChatGPT 和 Gemini 试图通过利用用户历史进行个性化，但对这种个性化的系统评估仍较少被探索。

Method: BESPOKE 是通过长期、深入参与的人类注释构建的，其中人类注释者贡献了自己的历史记录，撰写了带有详细信息需求的查询，并用评分和诊断反馈评估了响应。

Result: 通过 BESPOKE，我们进行了系统分析，揭示了信息检索任务中有效个性化的关键要求。

Conclusion: BESPOKE 提供了一个基础，用于细粒度评估个性化搜索增强的大型语言模型。

Abstract: Search-augmented large language models (LLMs) have advanced
information-seeking tasks by integrating retrieval into generation, reducing
users' cognitive burden compared to traditional search systems. Yet they remain
insufficient for fully addressing diverse user needs, which requires
recognizing how the same query can reflect different intents across users and
delivering information in preferred forms. While recent systems such as ChatGPT
and Gemini attempt personalization by leveraging user histories, systematic
evaluation of such personalization is under-explored. To address this gap, we
propose BESPOKE, the realistic benchmark for evaluating personalization in
search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting
authentic chat and search histories directly from humans, and diagnostic, by
pairing responses with fine-grained preference scores and feedback. The
benchmark is constructed through long-term, deeply engaged human annotation,
where human annotators contributed their own histories, authored queries with
detailed information needs, and evaluated responses with scores and diagnostic
feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key
requirements for effective personalization in information-seeking tasks,
providing a foundation for fine-grained evaluation of personalized
search-augmented LLMs. Our code and data are available at
https://augustinlib.github.io/BESPOKE/.

</details>


### [47] [VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of Spoken Language Model](https://arxiv.org/abs/2509.21108)
*Junhyuk Choi,Ro-hoon Oh,Jihwan Seol,Bugeun Kim*

Main category: cs.CL

TL;DR: 本文介绍了VoiceBBQ，这是一个语音扩展的BBQ基准，用于测量社会偏见。通过将每个BBQ上下文转换为受控语音条件，可以生成每轴的准确率、偏见和一致性分数。评估两个语音语言模型发现它们在架构上有对比。


<details>
  <summary>Details</summary>
Motivation: 由于语音的性质，语音语言模型中的社会偏见可能来自两个不同的来源：内容方面和声学方面。因此需要一个能够测量这两种偏见的基准数据集。

Method: 将每个BBQ上下文转换为受控语音条件，以生成每轴的准确率、偏见和一致性分数，这些分数与原始文本基准保持可比性。

Result: 使用VoiceBBQ评估了两个语音语言模型LLaMA-Omni和Qwen2-Audio，发现它们在架构上有对比：LLaMA-Omni抵抗声学偏见但放大性别和口音偏见，而Qwen2-Audio显著减弱这些线索同时保持内容保真度。

Conclusion: VoiceBBQ提供了一个紧凑的、可直接使用的测试平台，用于联合诊断语音语言模型中的内容和声学偏见。

Abstract: We introduce VoiceBBQ, a spoken extension of the BBQ (Bias Benchmark for
Question Answering) - a dataset that measures social bias by presenting
ambiguous or disambiguated contexts followed by questions that may elicit
stereotypical responses. Due to the nature of speech, social bias in Spoken
Language Models (SLMs) can emerge from two distinct sources: 1) content aspect
and 2) acoustic aspect. The dataset converts every BBQ context into controlled
voice conditions, enabling per-axis accuracy, bias, and consistency scores that
remain comparable to the original text benchmark. Using VoiceBBQ, we evaluate
two SLMs - LLaMA-Omni and Qwen2-Audio - and observe architectural contrasts:
LLaMA-Omni resists acoustic bias while amplifying gender and accent bias,
whereas Qwen2-Audio substantially dampens these cues while preserving content
fidelity. VoiceBBQ thus provides a compact, drop-in testbed for jointly
diagnosing content and acoustic bias across spoken language models.

</details>


### [48] [Acoustic-based Gender Differentiation in Speech-aware Language Models](https://arxiv.org/abs/2509.21125)
*Junhyuk Choi,Jihwan Seol,Nayeon Kim,Chanhee Cho,EunBin Cho,Bugeun Kim*

Main category: cs.CL

TL;DR: 该研究揭示了SpeechLMs在性别偏见方面的矛盾模式，强调需要更先进的技术来正确处理语音技术中的性别信息。


<details>
  <summary>Details</summary>
Motivation: 研究SpeechLMs在语音交流中可能存在的基于声音的性别差异现象，以系统地分析这种现象。

Method: 提出一个新的数据集，包含9,208个语音样本，分为三种类别：性别无关、性别刻板印象和性别相关，并评估了LLaMA-Omni系列模型。

Result: 发现模型在性别刻板印象问题上表现出男性导向的回应，而在性别相关问题上却表现出与性别无关的回应，这种矛盾模式并未因中性选项或语音的感知性别而产生。

Conclusion: 当前的SpeechLM可能未能成功消除性别偏见，尽管它们优先考虑了普遍公平原则而非情境适当性，这突显了需要更复杂的技巧来在语音技术中正确利用性别信息。

Abstract: Speech-aware Language Models (SpeechLMs) have fundamentally transformed
human-AI interaction by enabling voice-based communication, yet they may
exhibit acoustic-based gender differentiation where identical questions lead to
different responses based on the speaker's gender. This paper propose a new
dataset that enables systematic analysis of this phenomenon, containing 9,208
speech samples across three categories: Gender-Independent,
Gender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni
series and discovered a paradoxical pattern; while overall responses seems
identical regardless of gender, the pattern is far from unbiased responses.
Specifically, in Gender-Stereotypical questions, all models consistently
exhibited male-oriented responses; meanwhile, in Gender-Dependent questions
where gender differentiation would be contextually appropriate, models
exhibited responses independent to gender instead. We also confirm that this
pattern does not result from neutral options nor perceived gender of a voice.
When we allow neutral response, models tends to respond neutrally also in
Gender-Dependent questions. The paradoxical pattern yet retains when we applied
gender neutralization methods on speech. Through comparison between SpeechLMs
with corresponding backbone LLMs, we confirmed that these paradoxical patterns
primarily stem from Whisper speech encoders, which generates male-oriented
acoustic tokens. These findings reveal that current SpeechLMs may not
successfully remove gender biases though they prioritized general fairness
principles over contextual appropriateness, highlighting the need for more
sophisticated techniques to utilize gender information properly in speech
technology.

</details>


### [49] [AutoIntent: AutoML for Text Classification](https://arxiv.org/abs/2509.21138)
*Ilya Alekseev,Roman Solomatin,Darina Rustamova,Denis Kuznetsov*

Main category: cs.CL

TL;DR: AutoIntent is an automated machine learning tool for text classification that provides end-to-end automation and superior performance compared to existing AutoML tools.


<details>
  <summary>Details</summary>
Motivation: To provide an automated machine learning tool for text classification tasks that supports multi-label classification and out-of-scope detection.

Method: AutoIntent offers end-to-end automation with embedding model selection, classifier optimization, and decision threshold tuning within a modular, sklearn-like interface.

Result: AutoIntent shows superior performance on standard intent classification datasets.

Conclusion: AutoIntent demonstrates superior performance compared to existing AutoML tools and enables users to balance effectiveness and resource consumption.

Abstract: AutoIntent is an automated machine learning tool for text classification
tasks. Unlike existing solutions, AutoIntent offers end-to-end automation with
embedding model selection, classifier optimization, and decision threshold
tuning, all within a modular, sklearn-like interface. The framework is designed
to support multi-label classification and out-of-scope detection. AutoIntent
demonstrates superior performance compared to existing AutoML tools on standard
intent classification datasets and enables users to balance effectiveness and
resource consumption.

</details>


### [50] [Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction](https://arxiv.org/abs/2509.21151)
*Lei Hei,Tingjing Liao,Yingxin Pei,Yiyang Qi,Jiaqi Wang,Ruiting Li,Feiliang Ren*

Main category: cs.CL

TL;DR: 本文提出了一种新的多模态关系抽取框架ROC，通过检索任务代替传统分类方法，提升了关系理解的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态RE方法通常采用基于分类的范式，这存在两个显著局限：(1) 忽略结构约束如实体类型和位置提示，(2) 缺乏细粒度关系理解的语义表达能力。

Method: ROC框架将多模态RE重新定义为由关系语义驱动的检索任务。ROC通过多模态编码器整合实体类型和位置信息，使用大型语言模型将关系标签扩展为自然语言描述，并通过基于语义相似性的对比学习对实体-关系对进行对齐。

Result: 我们的方法在基准数据集MNRE和MORE上达到了最先进的性能，并表现出更强的鲁棒性和可解释性。

Conclusion: 实验表明，我们的方法在基准数据集MNRE和MORE上达到了最先进性能，并表现出更强的鲁棒性和可解释性。

Abstract: Relation extraction (RE) aims to identify semantic relations between entities
in unstructured text. Although recent work extends traditional RE to multimodal
scenarios, most approaches still adopt classification-based paradigms with
fused multimodal features, representing relations as discrete labels. This
paradigm has two significant limitations: (1) it overlooks structural
constraints like entity types and positional cues, and (2) it lacks semantic
expressiveness for fine-grained relation understanding. We propose
\underline{R}etrieval \underline{O}ver \underline{C}lassification (ROC), a
novel framework that reformulates multimodal RE as a retrieval task driven by
relation semantics. ROC integrates entity type and positional information
through a multimodal encoder, expands relation labels into natural language
descriptions using a large language model, and aligns entity-relation pairs via
semantic similarity-based contrastive learning. Experiments show that our
method achieves state-of-the-art performance on the benchmark datasets MNRE and
MORE and exhibits stronger robustness and interpretability.

</details>


### [51] [Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models](https://arxiv.org/abs/2509.21155)
*Chantal Shaib,Vinith M. Suriyakumar,Levent Sagun,Byron C. Wallace,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 本文研究了语法-领域相关性对LLM性能的影响，并提出了检测和防止这种虚假相关性的方法。


<details>
  <summary>Details</summary>
Motivation: 为了使LLM正确响应指令，它必须理解任务指令对的语义和领域。然而，语法也可以传达隐含信息。最近的工作表明，语法模板在训练数据中很普遍，并且经常出现在模型输出中。本文旨在分析语法模板、领域和语义，并探讨语法与领域之间的虚假相关性。

Method: 我们通过合成训练数据集来研究语法-领域相关性，并引入了一个评估框架来检测这种现象。

Result: 我们发现语法-领域相关性会降低OLMo-2模型在实体知识任务上的性能，并且在FlanV2数据集的一部分中出现了这种现象。此外，我们展示了这种虚假相关性可能被用来绕过Olm-2-7B Instruct和GPT-4o的拒绝。

Conclusion: 我们的发现突出了两个需求：(1) 明确测试语法-领域相关性，以及(2) 确保训练数据中的句法多样性，特别是在领域内，以防止这种虚假相关性。

Abstract: For an LLM to correctly respond to an instruction it must understand both the
semantics and the domain (i.e., subject area) of a given task-instruction pair.
However, syntax can also convey implicit information Recent work shows that
syntactic templates--frequent sequences of Part-of-Speech (PoS) tags--are
prevalent in training data and often appear in model outputs. In this work we
characterize syntactic templates, domain, and semantics in task-instruction
pairs. We identify cases of spurious correlations between syntax and domain,
where models learn to associate a domain with syntax during training; this can
sometimes override prompt semantics. Using a synthetic training dataset, we
find that the syntactic-domain correlation can lower performance (mean 0.51 +/-
0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an
evaluation framework to detect this phenomenon in trained models, and show that
it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;
Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study
on the implications for safety finetuning, showing that unintended
syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B
Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test
for syntactic-domain correlations, and (2) to ensure syntactic diversity in
training data, specifically within domains, to prevent such spurious
correlations.

</details>


### [52] [Who's Laughing Now? An Overview of Computational Humour Generation and Explanation](https://arxiv.org/abs/2509.21175)
*Tyler Loakman,William Thorne,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文综述了计算幽默的研究现状，并强调了其作为自然语言处理子学科的重要性，同时指出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 幽默是自然语言处理中一个具有挑战性的任务，因为它需要大量的推理能力，因此可以作为评估现代大型语言模型常识知识和推理能力的重要任务。

Method: 本文对计算幽默的现状进行了综述，重点讨论了生成和解释幽默的任务。

Result: 尽管理解幽默具有基础自然语言处理任务的所有特征，但超越双关语的生成和解释幽默的工作仍然很少，而最先进的模型仍然无法达到人类的能力。

Conclusion: 计算幽默处理作为自然语言处理的一个子学科的重要性需要进一步研究，同时需要考虑幽默的主观性和伦理模糊性。

Abstract: The creation and perception of humour is a fundamental human trait,
positioning its computational understanding as one of the most challenging
tasks in natural language processing (NLP). As an abstract, creative, and
frequently context-dependent construct, humour requires extensive reasoning to
understand and create, making it a pertinent task for assessing the
common-sense knowledge and reasoning abilities of modern large language models
(LLMs). In this work, we survey the landscape of computational humour as it
pertains to the generative tasks of creation and explanation. We observe that,
despite the task of understanding humour bearing all the hallmarks of a
foundational NLP task, work on generating and explaining humour beyond puns
remains sparse, while state-of-the-art models continue to fall short of human
capabilities. We bookend our literature survey by motivating the importance of
computational humour processing as a subdiscipline of NLP and presenting an
extensive discussion of future directions for research in the area that takes
into account the subjective and ethically ambiguous nature of humour.

</details>


### [53] [GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models](https://arxiv.org/abs/2509.21192)
*Jieli Zhu,Vi Ngoc-Nha Tran*

Main category: cs.CL

TL;DR: 本研究探讨了基于小型语言模型的聊天机器人中的个人身份信息泄露问题，并提出了一种新的贪婪坐标梯度方法（GEP），在实验中显示了更高的PII泄露率。


<details>
  <summary>Details</summary>
Motivation: 由于小型语言模型（SLM）在某些领域表现出与大型语言模型（LLM）相当的性能，但其个人身份信息（PII）泄露问题尚未被探索，因此需要研究SLM中的PII泄露情况。

Method: 本研究首先微调了一个基于BioGPT的聊天机器人ChatBioGPT，使用医疗数据集Alpaca和HealthCareMagic。然后提出了GEP方法，用于PII提取，并进行了实验研究。

Result: 实验结果显示，GEP方法比之前的基于模板的方法在PII泄露方面提高了多达60倍。此外，在更复杂和现实的情况下，GEP仍能揭示高达4.53%的PII泄露率。

Conclusion: 本研究证明了在小型语言模型（SLM）条件下，传统的基于模板的PII攻击方法无法有效提取数据集中的PII以进行泄露检测，并提出了一种新的贪婪坐标梯度方法（GEP），在实验中显示了更高的PII泄露率。

Abstract: Small language models (SLMs) become unprecedentedly appealing due to their
approximately equivalent performance compared to large language models (LLMs)
in certain fields with less energy and time consumption during training and
inference. However, the personally identifiable information (PII) leakage of
SLMs for downstream tasks has yet to be explored. In this study, we investigate
the PII leakage of the chatbot based on SLM. We first finetune a new chatbot,
i.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca
and HealthCareMagic. It shows a matchable performance in BERTscore compared
with previous studies of ChatDoctor and ChatGPT. Based on this model, we prove
that the previous template-based PII attacking methods cannot effectively
extract the PII in the dataset for leakage detection under the SLM condition.
We then propose GEP, which is a greedy coordinate gradient-based (GCG) method
specifically designed for PII extraction. We conduct experimental studies of
GEP and the results show an increment of up to 60$\times$ more leakage compared
with the previous template-based methods. We further expand the capability of
GEP in the case of a more complicated and realistic situation by conducting
free-style insertion where the inserted PII in the dataset is in the form of
various syntactic expressions instead of fixed templates, and GEP is still able
to reveal a PII leakage rate of up to 4.53%.

</details>


### [54] [Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning](https://arxiv.org/abs/2509.21193)
*Xiangru Tang,Wanghan Xu,Yujie Wang,Zijie Guo,Daniel Shao,Jiapeng Chen,Cixuan Zhang,Ziyi Wang,Lixin Zhang,Guancheng Wan,Wenlong Zhang,Lei Bai,Zhenfei Yin,Philip Torr,Hanrui Wang,Di Jin*

Main category: cs.CL

TL;DR: 本文提出一种统一框架，结合隐式检索和结构化协作，以解决大型语言模型在科学推理中的两个主要瓶颈，取得显著性能提升并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学推理方面取得了显著进展，但存在两个主要瓶颈：显式检索片段推理带来了额外的token和步骤成本；多代理管道通常通过平均所有候选者来稀释强解。

Method: 本文提出了一种统一的框架，结合了隐式检索和结构化协作。基础是基于Monitor的检索模块，在token级别操作，将外部知识与推理最小干扰地集成。在此基础上，采用分层解决方案精炼（HSR）和质量感知迭代推理（QAIR）进行迭代优化。

Result: 在Humanity's Last Exam (HLE) Bio/Chem Gold数据集上，该框架实现了48.3%的准确率，这是目前报道的最高水平，比最强的代理基线高出13.4分，并领先前沿LLM高达18.1分，同时减少了53.5%的token使用量和43.7%的代理步骤。在SuperGPQA和TRQA上的结果证实了跨领域的鲁棒性。

Conclusion: 本文提出的框架通过隐式增强和结构化精炼克服了显式工具使用和均匀聚合的低效问题。

Abstract: Large language models (LLMs) have recently shown strong progress on
scientific reasoning, yet two major bottlenecks remain. First, explicit
retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and
steps. Second, multi-agent pipelines often dilute strong solutions by averaging
across all candidates. We address these challenges with a unified framework
that combines implicit retrieval and structured collaboration. At its
foundation, a Monitor-based retrieval module operates at the token level,
integrating external knowledge with minimal disruption to reasoning. On top of
this substrate, Hierarchical Solution Refinement (HSR) iteratively designates
each candidate as an anchor to be repaired by its peers, while Quality-Aware
Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's
Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the
highest reported to date, surpassing the strongest agent baseline by 13.4
points and leading frontier LLMs by up to 18.1 points, while simultaneously
reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA
and TRQA confirm robustness across domains. Error analysis shows that reasoning
failures and knowledge gaps co-occur in over 85\% of cases, while diversity
analysis reveals a clear dichotomy: retrieval tasks benefit from solution
variety, whereas reasoning tasks favor consensus. Together, these findings
demonstrate how implicit augmentation and structured refinement overcome the
inefficiencies of explicit tool use and uniform aggregation. Code is available
at: https://github.com/tangxiangru/Eigen-1.

</details>


### [55] [CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis](https://arxiv.org/abs/2509.21208)
*Xinzhe Xu,Liang Zhao,Hongshen Xu,Chen Chen*

Main category: cs.CL

TL;DR: 本文介绍了CLaw基准测试，用于评估大型语言模型在中国法律知识和推理应用方面的表现，并指出当前模型在准确检索和引用法律条款方面存在不足，强调了准确知识检索和强大推理能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在分析法律文本和引用相关法规时可靠性不足，因为它们的通用预训练没有专注于法律内容，这使得它们的法律知识深度不明确。

Method: 本文介绍了CLaw，这是一个专门设计用于评估大型语言模型（LLM）在中国法律知识和推理应用方面的基准测试。CLaw包含两个关键部分：一个全面的、细粒度的中国306部国家法律条文语料库，以及一组基于中国最高法院精选材料的案例推理实例。

Result: 实证评估显示，大多数现代大型语言模型在忠实再现法律条款方面存在显著困难。

Conclusion: 本文认为，实现可信的法律推理需要准确的知识检索和强大的通用推理能力之间的协同作用。

Abstract: Large Language Models (LLMs) are increasingly tasked with analyzing legal
texts and citing relevant statutes, yet their reliability is often compromised
by general pre-training that ingests legal texts without specialized focus,
obscuring the true depth of their legal knowledge. This paper introduces CLaw,
a novel benchmark specifically engineered to meticulously evaluate LLMs on
Chinese legal knowledge and its application in reasoning. CLaw comprises two
key components: (1) a comprehensive, fine-grained corpus of all 306 Chinese
national statutes, segmented to the subparagraph level and incorporating
precise historical revision timesteps for rigorous recall evaluation (64,849
entries), and (2) a challenging set of 254 case-based reasoning instances
derived from China Supreme Court curated materials to assess the practical
application of legal knowledge. Our empirical evaluation reveals that most
contemporary LLMs significantly struggle to faithfully reproduce legal
provisions. As accurate retrieval and citation of legal provisions form the
basis of legal reasoning, this deficiency critically undermines the reliability
of their responses. We contend that achieving trustworthy legal reasoning in
LLMs requires a robust synergy of accurate knowledge retrieval--potentially
enhanced through supervised fine-tuning (SFT) or retrieval-augmented generation
(RAG)--and strong general reasoning capabilities. This work provides an
essential benchmark and critical insights for advancing domain-specific LLM
reasoning, particularly within the complex legal sphere.

</details>


### [56] [SGMem: Sentence Graph Memory for Long-Term Conversational Agents](https://arxiv.org/abs/2509.21212)
*Yaxiong Wu,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: SGMem 是一种用于长期对话代理的记忆管理方法，通过将对话表示为句子级别的图来捕捉不同粒度的上下文关联，并在长对话问答任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: Long-term conversational agents require effective memory management to handle dialogue histories that exceed the context window of large language models (LLMs). Existing methods based on fact extraction or summarization reduce redundancy but struggle to organize and retrieve relevant information across different granularities of dialogue and generated memory.

Method: SGMem (Sentence Graph Memory) represents dialogue as sentence-level graphs within chunked units, capturing associations across turn-, round-, and session-level contexts.

Result: Experiments on LongMemEval and LoCoMo show that SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.

Conclusion: SGMem consistently improves accuracy and outperforms strong baselines in long-term conversational question answering.

Abstract: Long-term conversational agents require effective memory management to handle
dialogue histories that exceed the context window of large language models
(LLMs). Existing methods based on fact extraction or summarization reduce
redundancy but struggle to organize and retrieve relevant information across
different granularities of dialogue and generated memory. We introduce SGMem
(Sentence Graph Memory), which represents dialogue as sentence-level graphs
within chunked units, capturing associations across turn-, round-, and
session-level contexts. By combining retrieved raw dialogue with generated
memory such as summaries, facts and insights, SGMem supplies LLMs with coherent
and relevant context for response generation. Experiments on LongMemEval and
LoCoMo show that SGMem consistently improves accuracy and outperforms strong
baselines in long-term conversational question answering.

</details>


### [57] [Query-Centric Graph Retrieval Augmented Generation](https://arxiv.org/abs/2509.21237)
*Yaxiong Wu,Jianyuan Bo,Yongyue Zhang,Sheng Liang,Yong Liu*

Main category: cs.CL

TL;DR: QCG-RAG是一种以查询为中心的图RAG框架，能够提高多跳推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临粒度困境：细粒度实体级图成本高且丢失上下文，而粗粒度文档级图无法捕捉细微关系。

Method: 引入了QCG-RAG框架，通过Doc2Query和Doc2Query--构建以查询为中心的图，实现查询粒度索引和多跳块检索。

Result: 实验表明QCG-RAG在LiHuaWorld和MultiHop-RAG数据集上表现优于之前的方法。

Conclusion: QCG-RAG在问答准确率上 consistently 超过之前的基于块和基于图的RAG方法，建立了多跳推理的新范式。

Abstract: Graph-based retrieval-augmented generation (RAG) enriches large language
models (LLMs) with external knowledge for long-context understanding and
multi-hop reasoning, but existing methods face a granularity dilemma:
fine-grained entity-level graphs incur high token costs and lose context, while
coarse document-level graphs fail to capture nuanced relations. We introduce
QCG-RAG, a query-centric graph RAG framework that enables query-granular
indexing and multi-hop chunk retrieval. Our query-centric approach leverages
Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with
controllable granularity, improving graph quality and interpretability. A
tailored multi-hop retrieval mechanism then selects relevant chunks via the
generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG
consistently outperforms prior chunk-based and graph-based RAG methods in
question answering accuracy, establishing a new paradigm for multi-hop
reasoning.

</details>


### [58] [Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication](https://arxiv.org/abs/2509.21262)
*Evgeny Kaskov,Elizaveta Petrova,Petr Surovtsev,Anna Kostikova,Ilya Mistiurin,Alexander Kapitanov,Alexander Nagaev*

Main category: cs.CL

TL;DR: 本文研究了扩散模型中的同音词重复问题，并提出了一种测量方法和通过提示扩展来减轻该问题的解决方案。


<details>
  <summary>Details</summary>
Motivation: 同音词在提示中出现时，扩散模型可能会同时生成该词的多个含义，这被称为同音词重复。这种问题因英文化偏见而变得更加复杂，即在文本到图像模型管道之前有一个额外的翻译步骤。因此，即使在原始语言中不是同音词的词语，在翻译成英语后也可能变成同音词并失去其意义。

Method: 本文引入了一种测量同音词重复率的方法，并利用视觉-语言模型（VLM）进行自动评估，同时进行了人工评估。此外，还研究了通过提示扩展来减轻同音词重复问题的方法。

Result: 本文通过自动评估和人工评估对不同的扩散模型进行了评估，并发现通过提示扩展可以有效减少同音词重复问题，包括与英文化偏见相关的重复。

Conclusion: 本文提出了一种测量重复率的方法，并通过自动评估和人工评估对不同的扩散模型进行了评估。此外，我们研究了通过提示扩展来减轻同音词重复问题的方法，证明这种方法也有效减少了与英文化偏见相关的重复。

Abstract: Homonyms are words with identical spelling but distinct meanings, which pose
challenges for many generative models. When a homonym appears in a prompt,
diffusion models may generate multiple senses of the word simultaneously, which
is known as homonym duplication. This issue is further complicated by an
Anglocentric bias, which includes an additional translation step before the
text-to-image model pipeline. As a result, even words that are not homonymous
in the original language may become homonyms and lose their meaning after
translation into English. In this paper, we introduce a method for measuring
duplication rates and conduct evaluations of different diffusion models using
both automatic evaluation utilizing Vision-Language Models (VLM) and human
evaluation. Additionally, we investigate methods to mitigate the homonym
duplication problem through prompt expansion, demonstrating that this approach
also effectively reduces duplication related to Anglocentric bias. The code for
the automatic evaluation pipeline is publicly available.

</details>


### [59] [LLM Output Homogenization is Task Dependent](https://arxiv.org/abs/2509.21267)
*Shomik Jain,Jack Lanchantin,Maximilian Nickel,Karen Ullrich,Ashia Wilson,Jamelle Watson-Daniels*

Main category: cs.CL

TL;DR: 本文提出了一个任务分类法，以任务依赖的方式评估和缓解输出同质化问题。


<details>
  <summary>Details</summary>
Motivation: 先前的研究在处理输出同质化时往往未能以任务依赖的方式概念化多样性。

Method: 本文提出了一个由八个任务类别组成的任务分类法，每个类别对输出同质化的概念化不同。还引入了任务锚定的功能多样性来更好地评估输出同质化，并提出了一种任务锚定的采样技术来增加功能多样性。

Result: 本文展示了如何通过任务依赖性提高输出同质化的评估和缓解效果。

Conclusion: 本文展示了任务依赖性如何改善输出同质化的评估和缓解。

Abstract: A large language model can be less helpful if it exhibits output response
homogenization. But whether two responses are considered homogeneous, and
whether such homogenization is problematic, both depend on the task category.
For instance, in objective math tasks, we often expect no variation in the
final answer but anticipate variation in the problem-solving strategy. Whereas,
for creative writing tasks, we may expect variation in key narrative components
(e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity
produced by temperature-sampling. Previous work addressing output
homogenization often fails to conceptualize diversity in a task-dependent way.
We address this gap in the literature directly by making the following
contributions. (1) We present a task taxonomy comprised of eight task
categories that each have distinct conceptualizations of output homogenization.
(2) We introduce task-anchored functional diversity to better evaluate output
homogenization. (3) We propose a task-anchored sampling technique that
increases functional diversity for task categories where homogenization is
undesired, while preserving homogenization where it is desired. (4) We
challenge the perceived existence of a diversity-quality trade-off by
increasing functional diversity while maintaining response quality. Overall, we
demonstrate how task dependence improves the evaluation and mitigation of
output homogenization.

</details>


### [60] [LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text](https://arxiv.org/abs/2509.21269)
*Irina Tolstykh,Aleksandra Tsybina,Sergey Yakubson,Maksim Kuprashevich*

Main category: cs.CL

TL;DR: 本文介绍了LLMTrace，一个大规模的双语语料库，用于AI生成文本检测，旨在解决现有数据集的不足，并支持两种关键任务：传统的全文二分类和新的AI生成区间检测。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集通常由过时的模型生成，主要以英语为主，并且未能解决日益常见的混合人类-AI作者的情况。虽然一些数据集涉及混合作者，但没有提供精确定位文本中AI生成部分所需的字符级注释。

Method: 引入LLMTrace，这是一个大规模的双语（英语和俄语）语料库，用于AI生成文本检测。该数据集使用了多种现代专有和开源LLM构建，旨在支持两个关键任务：传统的全文二分类（人类与AI）和新的AI生成区间检测任务，通过字符级注释实现。

Result: LLMTrace是一个新的大规模双语语料库，用于AI生成文本检测。它支持传统的全文二分类任务和新的AI生成区间检测任务，通过字符级注释实现。

Conclusion: LLMTrace将作为训练和评估下一代更细致和实用的AI检测模型的重要资源。

Abstract: The widespread use of human-like text from Large Language Models (LLMs)
necessitates the development of robust detection systems. However, progress is
limited by a critical lack of suitable training data; existing datasets are
often generated with outdated models, are predominantly in English, and fail to
address the increasingly common scenario of mixed human-AI authorship.
Crucially, while some datasets address mixed authorship, none provide the
character-level annotations required for the precise localization of
AI-generated segments within a text. To address these gaps, we introduce
LLMTrace, a new large-scale, bilingual (English and Russian) corpus for
AI-generated text detection. Constructed using a diverse range of modern
proprietary and open-source LLMs, our dataset is designed to support two key
tasks: traditional full-text binary classification (human vs. AI) and the novel
task of AI-generated interval detection, facilitated by character-level
annotations. We believe LLMTrace will serve as a vital resource for training
and evaluating the next generation of more nuanced and practical AI detection
models. The project page is available at
\href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}.

</details>


### [61] [Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond](https://arxiv.org/abs/2509.21284)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文通过理论分析和实验验证，研究了输入扰动对Chain-of-Thought (CoT) 输出的影响，并得出了相关结论。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示，Chain-of-Thought (CoT) 的输出受到输入扰动的显著影响。尽管许多方法试图通过优化提示来减轻这种影响，但对这些扰动如何影响CoT输出的理论解释仍然是一个开放的研究领域。这种差距限制了我们对输入扰动在推理过程中传播的深入理解，并阻碍了提示优化方法的进一步改进。因此，本文旨在对输入扰动对CoT输出波动的影响进行理论分析。

Method: 本文首先推导了在输出波动在可接受范围内的条件下输入扰动的上界，并基于此证明了该上界与CoT中的推理步骤数正相关，即使无限长的推理过程也无法消除输入扰动的影响。接着将这些结论应用于LSA模型，并证明了输入扰动的上界与输入嵌入和隐藏状态向量的范数负相关。

Result: 本文通过实验验证了理论分析，结果表明输入扰动的上界与输入嵌入和隐藏状态向量的范数负相关，并且实验结果与理论分析一致，证实了研究的正确性。

Conclusion: 本文通过理论分析和实验验证，证明了输入扰动对CoT输出的影响，并得出了相关结论。

Abstract: Existing research indicates that the output of Chain-of-Thought (CoT) is
significantly affected by input perturbations. Although many methods aim to
mitigate such impact by optimizing prompts, a theoretical explanation of how
these perturbations influence CoT outputs remains an open area of research.
This gap limits our in-depth understanding of how input perturbations propagate
during the reasoning process and hinders further improvements in prompt
optimization methods. Therefore, in this paper, we theoretically analyze the
effect of input perturbations on the fluctuation of CoT outputs. We first
derive an upper bound for input perturbations under the condition that the
output fluctuation is within an acceptable range, based on which we prove that:
(i) This upper bound is positively correlated with the number of reasoning
steps in the CoT; (ii) Even an infinitely long reasoning process cannot
eliminate the impact of input perturbations. We then apply these conclusions to
the Linear Self-Attention (LSA) model, which can be viewed as a simplified
version of the Transformer. For the LSA model, we prove that the upper bound
for input perturbation is negatively correlated with the norms of the input
embedding and hidden state vectors. To validate this theoretical analysis, we
conduct experiments on three mainstream datasets and four mainstream models.
The experimental results align with our theoretical analysis, empirically
demonstrating the correctness of our findings.

</details>


### [62] [DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding](https://arxiv.org/abs/2509.21287)
*Kin Ian Lo,Hala Hawashin,Mina Abbaszadeh,Tilen Limback-Stokin,Hadi Wazni,Mehrnoosh Sadrzadeh*

Main category: cs.CL

TL;DR: DisCoCLIP通过张量网络显式编码语言结构，提升了视觉-语言模型对动词语义和词序的敏感性，并在多个任务中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 最近的视觉-语言模型在大规模图像-文本对齐方面表现出色，但常常忽视语言的组合结构，导致在依赖词序和谓词-论元结构的任务中失败。

Method: 引入了DisCoCLIP，结合了一个冻结的CLIP视觉变换器和一个新型张量网络文本编码器，该编码器显式编码句法结构。句子通过组合范畴语法解析器进行解析，生成分布词张量，其收缩反映了句子的语法推导。高阶张量通过张量分解进行因子化，减少了参数数量。

Result: DisCoCLIP在SVO-Probes verb准确率上将CLIP的77.6%提高到82.4%，ARO归因和关系得分提高了超过9%和4%，并在一个新的SVO-Swap基准测试中达到了93.7%。

Conclusion: 嵌入显式语言结构通过张量网络可以产生可解释的、参数高效的表示，显著提高了视觉-语言任务中的组合推理。

Abstract: Recent vision-language models excel at large-scale image-text alignment but
often neglect the compositional structure of language, leading to failures on
tasks that hinge on word order and predicate-argument structure. We introduce
DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer
with a novel tensor network text encoder that explicitly encodes syntactic
structure. Sentences are parsed with a Combinatory Categorial Grammar parser to
yield distributional word tensors whose contractions mirror the sentence's
grammatical derivation. To keep the model efficient, high-order tensors are
factorized with tensor decompositions, reducing parameter count from tens of
millions to under one million. Trained end-to-end with a self-supervised
contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and
word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%,
boosts ARO attribution and relation scores by over 9% and 4%, and achieves
93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that
embedding explicit linguistic structure via tensor networks yields
interpretable, parameter-efficient representations that substantially improve
compositional reasoning in vision-language tasks.

</details>


### [63] [The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages](https://arxiv.org/abs/2509.21294)
*Pranjal A. Chitale,Varun Gumma,Sanchit Ahuja,Prashant Kodali,Manan Uppadhyay,Deepthi Sudharsan,Sunayana Sitaram*

Main category: cs.CL

TL;DR: 本文介绍了Updesh，这是一个高质量的大规模合成指令遵循数据集，包含13种印度语言的950万个数据点，强调长上下文、多轮能力和与印度文化背景的契合。


<details>
  <summary>Details</summary>
Motivation: 开发能够在多种语言中有效运行并保持文化根基的AI系统是一个长期挑战，特别是在资源匮乏的环境中。合成数据提供了一个有希望的途径，但其在多语言和多文化背景下的有效性仍需探索。

Method: 我们通过自下而上的生成策略创建和影响了印度语言的合成文化情境数据集，该策略提示大型开源LLM（>=235B参数）在特定于语言的维基百科内容中进行数据生成。

Result: 生成的数据质量很高；然而，人类评估指出了进一步改进的领域。此外，我们通过对模型在我们的数据集上进行微调并在15个多样化的多语言数据集上评估性能进行了下游评估。

Conclusion: 这些发现提供了实证证据，表明有效的多语言AI需要多方面的数据整理和生成策略，这些策略结合了上下文感知、文化基础的方法。

Abstract: Developing AI systems that operate effectively across languages while
remaining culturally grounded is a long-standing challenge, particularly in
low-resource settings. Synthetic data provides a promising avenue, yet its
effectiveness in multilingual and multicultural contexts remains underexplored.
We investigate the creation and impact of synthetic, culturally contextualized
datasets for Indian languages through a bottom-up generation strategy that
prompts large open-source LLMs (>= 235B parameters) to ground data generation
in language-specific Wikipedia content. This approach complements the dominant
top-down paradigm of translating synthetic datasets from high-resource
languages such as English. We introduce Updesh, a high-quality large-scale
synthetic instruction-following dataset comprising 9.5M data points across 13
Indian languages, encompassing diverse reasoning and generative tasks with an
emphasis on long-context, multi-turn capabilities, and alignment with Indian
cultural contexts. A comprehensive evaluation incorporating both automated
metrics and human annotation across 10k assessments indicates that generated
data is high quality; though, human evaluation highlights areas for further
improvement. Additionally, we perform downstream evaluations by fine-tuning
models on our dataset and assessing the performance across 15 diverse
multilingual datasets. Models trained on Updesh consistently achieve
significant gains on generative tasks and remain competitive on multiple-choice
style NLU tasks. Notably, relative improvements are most pronounced in low and
medium-resource languages, narrowing their gap with high-resource languages.
These findings provide empirical evidence that effective multilingual AI
requires multi-faceted data curation and generation strategies that incorporate
context-aware, culturally grounded methodologies.

</details>


### [64] [Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs](https://arxiv.org/abs/2509.21305)
*Daniel Vennemeyer,Phan Anh Duong,Tiffany Zhan,Tianyu Jiang*

Main category: cs.CL

TL;DR: 研究分解了奉承行为，并发现它们在潜在空间中具有不同的表示，可以独立调节。


<details>
  <summary>Details</summary>
Motivation: 为了确定奉承行为是来自单一机制还是多个不同过程。

Method: 通过差异均值方向、激活添加和子空间几何学在多个模型和数据集上进行分析。

Result: （1）三种行为在潜在空间中沿不同的线性方向编码；（2）每种行为可以独立放大或抑制而不影响其他行为；（3）它们的表示结构在模型家族和规模上是一致的。

Conclusion: 这些结果表明，奉承行为对应于不同的、可以独立调节的表示。

Abstract: Large language models (LLMs) often exhibit sycophantic behaviors -- such as
excessive agreement with or flattery of the user -- but it is unclear whether
these behaviors arise from a single mechanism or multiple distinct processes.
We decompose sycophancy into sycophantic agreement and sycophantic praise,
contrasting both with genuine agreement. Using difference-in-means directions,
activation additions, and subspace geometry across multiple models and
datasets, we show that: (1) the three behaviors are encoded along distinct
linear directions in latent space; (2) each behavior can be independently
amplified or suppressed without affecting the others; and (3) their
representational structure is consistent across model families and scales.
These results suggest that sycophantic behaviors correspond to distinct,
independently steerable representations.

</details>


### [65] [RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](https://arxiv.org/abs/2509.21319)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Ellie Evans,Daniel Egert,Hoo-Chang Shin,Felipe Soares,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: RLBFF是一种结合人类偏好与规则验证的新型强化学习方法，能够提高奖励模型的性能并实现更高的灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的RLHF和RLVR方法分别存在可解释性差和范围有限的问题，因此需要一种更灵活且精确的方法来改进奖励模型的训练。

Method: RLBFF结合了人类驱动的偏好与基于规则的验证，通过从自然语言反馈中提取二元问题来训练奖励模型，将其作为蕴含任务进行处理。

Result: RLBFF在RM-Bench和JudgeBench上取得了86.2%和81.4%的优异成绩，并且在推理时允许用户自定义关注点。此外，它以低于5%的推理成本实现了与其他模型相当的性能。

Conclusion: RLBFF可以超越Bradley-Terry模型，并在RM-Bench和JudgeBench上取得顶级表现。此外，它提供了一种完全开源的配方，用于对齐Qwen3-32B，以匹配或超过其他模型的性能，同时降低推理成本。

Abstract: Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).

</details>


### [66] [SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines](https://arxiv.org/abs/2509.21320)
*Yizhou Wang,Chen Tang,Han Deng,Jiabei Xiao,Jiaqi Liu,Jianyu Wu,Jun Yao,Pengze Li,Encheng Su,Lintao Wang,Guohang Zhuang,Yuchen Ren,Ben Fei,Ming Hu,Xin Chen,Dongzhan Zhou,Junjun He,Xiangyu Yue,Zhenfei Yin,Jiamin Wu,Qihao Zheng,Yuhao Zhou,Huihui Xu,Chenglong Ma,Yan Lu,Wenlong Zhang,Chunfeng Song,Philip Torr,Shixiang Tang,Xinzhu Ma,Wanli Ouyang,Lei Bai*

Main category: cs.CL

TL;DR: 本文提出了一种科学推理基础模型，该模型能够将自然语言与异构科学表示对齐，并在多个任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 本文旨在开发一种科学推理基础模型，以更好地处理和理解科学文本、纯序列以及序列-文本对，并提高跨领域泛化能力和保真度。

Method: 本文提出了一个科学推理基础模型，该模型在206B个token的语料库上进行预训练，然后通过SFT在40M条指令上进行微调，并通过冷启动引导来激发长形式的思维链，再通过任务特定奖励塑造的强化学习来实现 deliberate 科学推理。

Result: 本文提出的模型支持四个能力家族，覆盖多达103项任务，包括文本与科学格式之间的忠实翻译、文本/知识提取、属性预测、属性分类、无条件和条件序列生成和设计。与专业系统相比，该方法扩大了指令覆盖范围，提高了跨领域泛化能力，并增强了保真度。

Conclusion: 本文提出了一种科学推理基础模型，能够将自然语言与异构科学表示对齐。该模型在206B个token的语料库上进行预训练，然后通过SFT在40M条指令上进行微调，并通过冷启动引导来激发长形式的思维链，再通过任务特定奖励塑造的强化学习来实现 deliberate 科学推理。该模型支持四个能力家族，覆盖多达103项任务。与专业系统相比，我们的方法扩大了指令覆盖范围，提高了跨领域泛化能力，并增强了保真度。

Abstract: We present a scientific reasoning foundation model that aligns natural
language with heterogeneous scientific representations. The model is pretrained
on a 206B-token corpus spanning scientific text, pure sequences, and
sequence-text pairs, then aligned via SFT on 40M instructions, annealed
cold-start bootstrapping to elicit long-form chain-of-thought, and
reinforcement learning with task-specific reward shaping, which instills
deliberate scientific reasoning. It supports four capability families, covering
up to 103 tasks across workflows: (i) faithful translation between text and
scientific formats, (ii) text/knowledge extraction, (iii) property prediction,
(iv) property classification, (v) unconditional and conditional sequence
generation and design. Compared with specialist systems, our approach broadens
instruction coverage, improves cross-domain generalization, and enhances
fidelity. We detail data curation and training and show that cross-discipline
learning strengthens transfer and downstream reliability. The model, instruct
tuning datasets and the evaluation code are open-sourced at
https://huggingface.co/SciReason and
https://github.com/open-sciencelab/SciReason.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [67] [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,Shayan Baghayi Nejad,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: CARINOX是一种统一框架，通过结合噪声优化和探索以及基于相关性的奖励选择程序，提高了文本到图像生成的组合对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单独使用时存在固有局限性，例如优化可能因初始化不良或搜索轨迹不利而停滞，而探索可能需要过多样本才能找到满意的输出。

Method: CARINOX结合了噪声优化和探索，并采用基于与人类判断相关的相关性的奖励选择程序。

Result: CARINOX在T2I-CompBench++和HRS基准测试中分别提升了16%和11%的平均对齐分数，优于最先进的优化和探索方法。

Conclusion: CARINOX在两个互补的基准测试中提升了平均对齐分数，表现出色，同时保持了图像质量和多样性。

Abstract: Text-to-image diffusion models, such as Stable Diffusion, can produce
high-quality and diverse images but often fail to achieve compositional
alignment, particularly when prompts describe complex object relationships,
attributes, or spatial arrangements. Recent inference-time approaches address
this by optimizing or exploring the initial noise under the guidance of reward
functions that score text-image alignment without requiring model fine-tuning.
While promising, each strategy has intrinsic limitations when used alone:
optimization can stall due to poor initialization or unfavorable search
trajectories, whereas exploration may require a prohibitively large number of
samples to locate a satisfactory output. Our analysis further shows that
neither single reward metrics nor ad-hoc combinations reliably capture all
aspects of compositionality, leading to weak or inconsistent guidance. To
overcome these challenges, we present Category-Aware Reward-based Initial Noise
Optimization and Exploration (CARINOX), a unified framework that combines noise
optimization and exploration with a principled reward selection procedure
grounded in correlation with human judgments. Evaluations on two complementary
benchmarks covering diverse compositional challenges show that CARINOX raises
average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS
benchmark, consistently outperforming state-of-the-art optimization and
exploration-based methods across all major categories, while preserving image
quality and diversity. The project page is available at
https://amirkasaei.com/carinox/{this URL}.

</details>


### [68] [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
*Ofir Azachi,Kfir Eliyahu,Eyal El Ani,Rom Himelstein,Roi Reichart,Yuval Pinter,Nitay Calderon*

Main category: cs.CV

TL;DR: 本文提出了一种基于NTP的高效幻觉检测方法，通过训练传统ML模型来检测VLMs中的幻觉，并展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）的幻觉会损害其可靠性。现有的检测方法计算成本高且增加模型延迟。

Method: 通过在基于VLM的下一个标记概率（NTP）的信号上训练传统机器学习模型，探索一种高效的实时幻觉检测方法。

Result: NTP-based特征是幻觉的有效预测因子，使快速简单的机器学习模型能够达到与强大VLMs相当的性能。此外，将这些NTP与语言NTP结合可以提高幻觉检测性能。

Conclusion: 我们希望这项研究为简单、轻量级的解决方案铺平道路，以提高VLMs的可靠性。

Abstract: Hallucinations of vision-language models (VLMs), which are misalignments
between visual content and generated text, undermine the reliability of VLMs.
One common approach for detecting them employs the same VLM, or a different
one, to assess generated outputs. This process is computationally intensive and
increases model latency. In this paper, we explore an efficient on-the-fly
method for hallucination detection by training traditional ML models over
signals based on the VLM's next-token probabilities (NTPs). NTPs provide a
direct quantification of model uncertainty. We hypothesize that high
uncertainty (i.e., a low NTP value) is strongly associated with hallucinations.
To test this, we introduce a dataset of 1,400 human-annotated statements
derived from VLM-generated content, each labeled as hallucinated or not, and
use it to test our NTP-based lightweight method. Our results demonstrate that
NTP-based features are valuable predictors of hallucinations, enabling fast and
simple ML models to achieve performance comparable to that of strong VLMs.
Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding
only the generated text back into the VLM, enhances hallucination detection
performance. Finally, integrating hallucination prediction scores from VLMs
into the NTP-based models led to better performance than using either VLMs or
NTPs alone. We hope this study paves the way for simple, lightweight solutions
that enhance the reliability of VLMs.

</details>


### [69] [Human Semantic Representations of Social Interactions from Moving Shapes](https://arxiv.org/abs/2509.20673)
*Yiling Yun,Hongjing Lu*

Main category: cs.CV

TL;DR: 本研究探讨了人类如何利用语义表示来补充视觉特征，以理解简单的移动形状所展示的社会互动。结果显示，语义模型特别是动词嵌入能更好地解释人类的相似性判断。


<details>
  <summary>Details</summary>
Motivation: 先前的研究往往集中在视觉特征上，而我们则探讨了人类用来补充视觉特征的语义表示。

Method: 在研究1中，我们直接让人类参与者根据移动形状的印象对动画进行标记。在研究2中，我们通过人类相似性判断测量了27种社会互动的表征几何，并将其与基于视觉特征、标签和动画描述语义嵌入的模型预测进行了比较。

Result: 人类响应是分散的。语义模型提供了补充信息以解释人类判断。其中，从描述中提取的动词嵌入最好地解释了人类相似性判断。

Conclusion: 这些结果表明，简单显示中的社会感知反映了社会互动的语义结构，弥合了视觉和抽象表示之间的差距。

Abstract: Humans are social creatures who readily recognize various social interactions
from simple display of moving shapes. While previous research has often focused
on visual features, we examine what semantic representations that humans employ
to complement visual features. In Study 1, we directly asked human participants
to label the animations based on their impression of moving shapes. We found
that human responses were distributed. In Study 2, we measured the
representational geometry of 27 social interactions through human similarity
judgments and compared it with model predictions based on visual features,
labels, and semantic embeddings from animation descriptions. We found that
semantic models provided complementary information to visual features in
explaining human judgments. Among the semantic models, verb-based embeddings
extracted from descriptions account for human similarity judgments the best.
These results suggest that social perception in simple displays reflects the
semantic structure of social interactions, bridging visual and abstract
representations.

</details>


### [70] [Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models](https://arxiv.org/abs/2509.20751)
*Zoe Wanying He,Sean Trott,Meenakshi Khosla*

Main category: cs.CV

TL;DR: 本文研究了视觉和语言模型如何在不同模态上训练后实现部分对齐，并发现对齐在中层到深层达到峰值，这种对齐是语义性的，并且随着示例聚合而增强。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的研究表明，深度视觉-only和语言-only模型在不同模态上训练，但仍将其输入投影到部分对齐的表示空间中，但我们仍然缺乏对这些对齐如何发生、何时发生以及它们如何影响模型性能的清晰理解。

Method: 我们系统地研究了这些问题，包括分析对齐在模型中的出现位置、支持它的视觉或语言提示、它是否捕捉人类在多对多图像-文本场景中的偏好，以及聚合同一概念的示例如何影响对齐。

Result: 我们发现对齐在两种模型类型的中层到深层达到峰值，反映了从模态特定到概念共享表示的转变。这种对齐对仅外观的变化具有鲁棒性，但当语义被改变时（例如对象移除或词序混乱）会崩溃，这表明共享代码确实是语义性的。此外，平均嵌入 across 示例会增强对齐而不是模糊细节。

Conclusion: 我们的结果表明，单模态网络会收敛到一个与人类判断一致的共享语义代码，并且随着示例聚合而增强。

Abstract: Recent studies show that deep vision-only and language-only models--trained
on disjoint modalities--nonetheless project their inputs into a partially
aligned representational space. Yet we still lack a clear picture of where in
each network this convergence emerges, what visual or linguistic cues support
it, whether it captures human preferences in many-to-many image-text scenarios,
and how aggregating exemplars of the same concept affects alignment. Here, we
systematically investigate these questions. We find that alignment peaks in
mid-to-late layers of both model types, reflecting a shift from
modality-specific to conceptually shared representations. This alignment is
robust to appearance-only changes but collapses when semantics are altered
(e.g., object removal or word-order scrambling), highlighting that the shared
code is truly semantic. Moving beyond the one-to-one image-caption paradigm, a
forced-choice "Pick-a-Pic" task shows that human preferences for image-caption
matches are mirrored in the embedding spaces across all vision-language model
pairs. This pattern holds bidirectionally when multiple captions correspond to
a single image, demonstrating that models capture fine-grained semantic
distinctions akin to human judgments. Surprisingly, averaging embeddings across
exemplars amplifies alignment rather than blurring detail. Together, our
results demonstrate that unimodal networks converge on a shared semantic code
that aligns with human judgments and strengthens with exemplar aggregation.

</details>


### [71] [TABLET: A Large-Scale Dataset for Robust Visual Table Understanding](https://arxiv.org/abs/2509.21205)
*Iñigo Alonso,Imanol Miranda,Eneko Agirre,Mirella Lapata*

Main category: cs.CV

TL;DR: TABLET is a large-scale VTU dataset with 4 million examples across 20 tasks, grounded in 2 million unique tables. It preserves original visualizations and provides traceability, improving performance on VTU tasks when used to fine-tune vision-language models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for table understanding rely on synthetic renderings that lack the complexity and visual diversity of real-world tables. Existing VTU datasets offer fixed examples with single visualizations and pre-defined instructions, providing no access to underlying serialized data for reformulation.

Method: The paper introduces TABLET, a large-scale VTU dataset with 4 million examples across 20 tasks, grounded in 2 million unique tables where 88% preserve original visualizations. Each example includes paired image-HTML representations, comprehensive metadata, and provenance information linking back to the source datasets.

Result: Fine-tuning vision-language models like Qwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while increasing robustness on real-world table visualizations.

Conclusion: TABLET establishes a foundation for robust training and extensible evaluation of future VTU models by preserving original visualizations and maintaining example traceability in a unified large-scale collection.

Abstract: While table understanding increasingly relies on pixel-only settings where
tables are processed as visual representations, current benchmarks
predominantly use synthetic renderings that lack the complexity and visual
diversity of real-world tables. Additionally, existing visual table
understanding (VTU) datasets offer fixed examples with single visualizations
and pre-defined instructions, providing no access to underlying serialized data
for reformulation. We introduce TABLET, a large-scale VTU dataset with 4
million examples across 20 tasks, grounded in 2 million unique tables where 88%
preserve original visualizations. Each example includes paired image-HTML
representations, comprehensive metadata, and provenance information linking
back to the source datasets. Fine-tuning vision-language models like
Qwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while
increasing robustness on real-world table visualizations. By preserving
original visualizations and maintaining example traceability in a unified
large-scale collection, TABLET establishes a foundation for robust training and
extensible evaluation of future VTU models.

</details>


### [72] [Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding](https://arxiv.org/abs/2509.21223)
*Muxin Pu,Mei Kuan Lim,Chun Yong Chong,Chen Change Loy*

Main category: cs.CV

TL;DR: Sigma is a unified skeleton-based SLU framework that addresses the limitations of current SLU methods by incorporating a sign-aware early fusion mechanism, a hierarchical alignment learning strategy, and a unified pre-training framework.


<details>
  <summary>Details</summary>
Motivation: Current SLU methods continue to face three key limitations: weak semantic grounding, imbalance between local details and global context, and inefficient cross-modal learning.

Method: Sigma, a unified skeleton-based SLU framework featuring a sign-aware early fusion mechanism, a hierarchical alignment learning strategy, and a unified pre-training framework that combines contrastive learning, text matching and language modelling.

Result: Sigma achieves new state-of-the-art results on isolated sign language recognition, continuous sign language recognition, and gloss-free sign language translation on multiple benchmarks spanning different sign and spoken languages.

Conclusion: Sigma achieves new state-of-the-art results on isolated sign language recognition, continuous sign language recognition, and gloss-free sign language translation on multiple benchmarks spanning different sign and spoken languages, demonstrating the impact of semantically informative pre-training and the effectiveness of skeletal data as a stand-alone solution for SLU.

Abstract: Pre-training has proven effective for learning transferable features in sign
language understanding (SLU) tasks. Recently, skeleton-based methods have
gained increasing attention because they can robustly handle variations in
subjects and backgrounds without being affected by appearance or environmental
factors. Current SLU methods continue to face three key limitations: 1) weak
semantic grounding, as models often capture low-level motion patterns from
skeletal data but struggle to relate them to linguistic meaning; 2) imbalance
between local details and global context, with models either focusing too
narrowly on fine-grained cues or overlooking them for broader context; and 3)
inefficient cross-modal learning, as constructing semantically aligned
representations across modalities remains difficult. To address these, we
propose Sigma, a unified skeleton-based SLU framework featuring: 1) a
sign-aware early fusion mechanism that facilitates deep interaction between
visual and textual modalities, enriching visual features with linguistic
context; 2) a hierarchical alignment learning strategy that jointly maximises
agreements across different levels of paired features from different
modalities, effectively capturing both fine-grained details and high-level
semantic relationships; and 3) a unified pre-training framework that combines
contrastive learning, text matching and language modelling to promote semantic
consistency and generalisation. Sigma achieves new state-of-the-art results on
isolated sign language recognition, continuous sign language recognition, and
gloss-free sign language translation on multiple benchmarks spanning different
sign and spoken languages, demonstrating the impact of semantically informative
pre-training and the effectiveness of skeletal data as a stand-alone solution
for SLU.

</details>


### [73] [Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.21227)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文对用于组合文本-图像评估的常用度量标准进行了广泛研究，发现没有一种度量在所有任务中都表现一致，且某些度量在特定情况下表现更好。


<details>
  <summary>Details</summary>
Motivation: 评估文本-图像生成是否真正捕捉提示中描述的对象、属性和关系仍然是一个核心挑战。目前的评估主要依赖于自动化度量，但这些度量通常是根据惯例或流行程度采用的，而不是经过与人类判断的验证。因此，了解这些度量如何反映人类偏好至关重要。

Method: 我们对广泛使用的用于组合文本-图像评估的度量标准进行了广泛的研究。我们的分析超越了简单的相关性，考察了它们在各种组合挑战中的行为，并比较了不同度量家族与人类判断的一致性。

Result: 结果表明，没有单一的度量在所有任务中表现一致：性能随着组合问题的类型而变化。值得注意的是，尽管VQA-based度量很受欢迎，但它们并不始终优于其他度量，而某些基于嵌入的度量在特定情况下表现更强。图像-only度量由于设计用于感知质量而非对齐，因此对组合评估贡献很小。

Conclusion: 这些发现强调了在评估和生成中选择度量标准时需要仔细和透明的重要性。

Abstract: Text-image generation has advanced rapidly, but assessing whether outputs
truly capture the objects, attributes, and relations described in prompts
remains a central challenge. Evaluation in this space relies heavily on
automated metrics, yet these are often adopted by convention or popularity
rather than validated against human judgment. Because evaluation and reported
progress in the field depend directly on these metrics, it is critical to
understand how well they reflect human preferences. To address this, we present
a broad study of widely used metrics for compositional text-image evaluation.
Our analysis goes beyond simple correlation, examining their behavior across
diverse compositional challenges and comparing how different metric families
align with human judgments. The results show that no single metric performs
consistently across tasks: performance varies with the type of compositional
problem. Notably, VQA-based metrics, though popular, are not uniformly
superior, while certain embedding-based metrics prove stronger in specific
cases. Image-only metrics, as expected, contribute little to compositional
evaluation, as they are designed for perceptual quality rather than alignment.
These findings underscore the importance of careful and transparent metric
selection, both for trustworthy evaluation and for their use as reward models
in generation. Project page is available at
\href{https://amirkasaei.com/eval-the-evals/}{this URL}.

</details>


### [74] [Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation](https://arxiv.org/abs/2509.21257)
*Seyed Amir Kasaei,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到图像生成模型中幻觉的定义，并通过分类学方法揭示了隐藏的偏差，为更全面的评估提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有的评估主要关注对齐，但忽略了模型在提示之外生成的内容。本文旨在明确幻觉的定义，并提供一个更全面的评估框架。

Method: 本文提出了一个将幻觉定义为由偏差驱动的偏离的新框架，并提出了三个类别：属性、关系和对象幻觉。

Result: 本文提出了一个新框架，用于定义文本到图像生成模型中的幻觉，并通过分类学方法揭示了隐藏的偏差。

Conclusion: 本文提出了一个针对文本到图像生成模型中幻觉现象的新框架，并通过分类学方法揭示了隐藏的偏差，为更全面的评估提供了基础。

Abstract: In language and vision-language models, hallucination is broadly understood
as content generated from a model's prior knowledge or biases rather than from
the given input. While this phenomenon has been studied in those domains, it
has not been clearly framed for text-to-image (T2I) generative models. Existing
evaluations mainly focus on alignment, checking whether prompt-specified
elements appear, but overlook what the model generates beyond the prompt. We
argue for defining hallucination in T2I as bias-driven deviations and propose a
taxonomy with three categories: attribute, relation, and object hallucinations.
This framing introduces an upper bound for evaluation and surfaces hidden
biases, providing a foundation for richer assessment of T2I models.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [75] [Perspectra: Choosing Your Experts Enhances Critical Thinking in Multi-Agent Research Ideation](https://arxiv.org/abs/2509.20553)
*Yiren Liu,Viraj Shah,Sangho Suh,Pao Siangliulue,Tal August,Yun Huang*

Main category: cs.HC

TL;DR: Perspectra是一种交互式多代理系统，通过论坛式界面增强用户对多个领域专家代理协作的控制，从而提升批判性思维和跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 研究用户如何有效控制、引导和批判性评估多个领域专家代理之间的协作，这是当前多代理系统中尚未充分探索的问题。

Method: 通过论坛式界面可视化和结构化LLM代理之间的讨论，支持@提及邀请特定代理、并行探索的线程以及实时思维导图来展示论点和理由。

Result: Perspectra显著增加了批判性思维行为的频率和深度，引发了更多的跨学科回复，并导致研究提案的更频繁修订。

Conclusion: Perspectra在促进批判性思维和跨学科回复方面表现出色，为多代理工具的设计提供了重要启示。

Abstract: Recent advances in multi-agent systems (MAS) enable tools for information
search and ideation by assigning personas to agents. However, how users can
effectively control, steer, and critically evaluate collaboration among
multiple domain-expert agents remains underexplored. We present Perspectra, an
interactive MAS that visualizes and structures deliberation among LLM agents
via a forum-style interface, supporting @-mention to invite targeted agents,
threading for parallel exploration, with a real-time mind map for visualizing
arguments and rationales. In a within-subjects study with 18 participants, we
compared Perspectra to a group-chat baseline as they developed research
proposals. Our findings show that Perspectra significantly increased the
frequency and depth of critical-thinking behaviors, elicited more
interdisciplinary replies, and led to more frequent proposal revisions than the
group chat condition. We discuss implications for designing multi-agent tools
that scaffold critical thinking by supporting user control over multi-agent
adversarial discourse.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models](https://arxiv.org/abs/2506.00209)
*Liwen Sun,Hao-Ren Yao,Gary Gao,Ophir Frieder,Chenyan Xiong*

Main category: cs.LG

TL;DR: CATCH-FM is a novel cancer pre-screening method that leverages EHR foundation models to identify high-risk patients, achieving strong performance in pancreatic cancer risk prediction and demonstrating the potential for broader application in cancer screening.


<details>
  <summary>Details</summary>
Motivation: Existing cancer screening techniques are expensive, intrusive, and not globally available, leading to many preventable deaths. The goal is to develop a more accessible and effective pre-screening method using electronic healthcare records (EHR).

Method: CATCH-FM is a cancer pre-screening methodology that uses historical medical records to identify high-risk patients. It involves pretraining foundation models on medical code sequences and fine-tuning them on clinician-curated cancer risk prediction cohorts.

Result: In a retrospective evaluation of thirty thousand patients, CATCH-FM achieved 60% sensitivity, 99% specificity, and a high negative predictive value. It outperformed feature-based tree models and general and medical large language models. Additionally, it achieved state-of-the-art results on the EHRSHOT few-shot leaderboard for pancreatic cancer risk prediction.

Conclusion: CATCH-FM demonstrates robust performance in pancreatic cancer risk prediction across various patient distributions and outperforms existing models, highlighting the potential of EHR foundation models for cancer pre-screening.

Abstract: Cancer screening, leading to early detection, saves lives. Unfortunately,
existing screening techniques require expensive and intrusive medical
procedures, not globally available, resulting in too many lost would-be-saved
lives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation
Models, a cancer pre-screening methodology that identifies high-risk patients
for further screening solely based on their historical medical records. With
millions of electronic healthcare records (EHR), we establish the scaling law
of EHR foundation models pretrained on medical code sequences, pretrain
compute-optimal foundation models of up to 2.4 billion parameters, and finetune
them on clinician-curated cancer risk prediction cohorts. In our retrospective
evaluation comprising of thirty thousand patients, CATCH-FM achieved strong
efficacy (60% sensitivity) with low risk (99% specificity and Negative
Predictive Value), outperforming feature-based tree models as well as general
and medical large language models by large margins. Despite significant
demographic, healthcare system, and EHR coding differences, CATCH-FM achieves
state-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot
leaderboard, outperforming EHR foundation models pretrained using on-site
patient data. Our analysis demonstrates the robustness of CATCH-FM in various
patient distributions, the benefits of operating in the ICD code space, and its
ability to capture non-trivial cancer risk factors. Our code will be
open-sourced.

</details>


### [77] [Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation](https://arxiv.org/abs/2509.20680)
*Wenkai Guo,Xuefeng Liu,Haolin Wang,Jianwei Niu,Shaojie Tang,Jing Yuan*

Main category: cs.LG

TL;DR: 本文研究了联邦学习在训练大型语言模型时的隐私风险，发现攻击者可能从全局模型中提取数据，并提出了相应的缓解措施。


<details>
  <summary>Details</summary>
Motivation: 组织希望通过本地数据微调大型语言模型来适应特定领域，但由于数据共享的顾虑，集中式微调变得不切实际。联邦学习提供了一种潜在的解决方案，但本文发现其可能存在隐私泄露风险。

Method: 本文通过广泛的实验展示了攻击者即使使用简单的生成方法也能从全局模型中提取训练数据，并引入了一种针对联邦学习的增强攻击策略。此外，还评估了联邦学习中的隐私保护技术，包括差分隐私、正则化约束更新和采用安全对齐的大型语言模型。

Result: 实验表明，攻击者可以提取训练数据，且随着模型规模的增大，泄露程度增加。此外，引入的增强攻击策略进一步加剧了隐私泄露。然而，评估的隐私保护技术有助于降低这些风险。

Conclusion: 本文提供了在使用联邦学习训练大型语言模型时减少隐私风险的有价值的见解和实用指南。

Abstract: Fine-tuning large language models (LLMs) with local data is a widely adopted
approach for organizations seeking to adapt LLMs to their specific domains.
Given the shared characteristics in data across different organizations, the
idea of collaboratively fine-tuning an LLM using data from multiple sources
presents an appealing opportunity. However, organizations are often reluctant
to share local data, making centralized fine-tuning impractical. Federated
learning (FL), a privacy-preserving framework, enables clients to retain local
data while sharing only model parameters for collaborative training, offering a
potential solution. While fine-tuning LLMs on centralized datasets risks data
leakage through next-token prediction, the iterative aggregation process in FL
results in a global model that encapsulates generalized knowledge, which some
believe protects client privacy. In this paper, however, we present
contradictory findings through extensive experiments. We show that attackers
can still extract training data from the global model, even using
straightforward generation methods, with leakage increasing as the model size
grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which
tracks global model updates during training to intensify privacy leakage. To
mitigate these risks, we evaluate privacy-preserving techniques in FL,
including differential privacy, regularization-constrained updates and adopting
LLMs with safety alignment. Our results provide valuable insights and practical
guidelines for reducing privacy risks when training LLMs with FL.

</details>


### [78] [CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/abs/2509.20712)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Yuntao Li,Wenping Hu,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: CE-GPPO is a new algorithm that improves reinforcement learning for large language models by managing policy entropy more effectively.


<details>
  <summary>Details</summary>
Motivation: Existing methods like PPO discard valuable gradient signals from low-probability tokens due to the clipping mechanism, which affects the balance between exploration and exploitation during training.

Method: CE-GPPO is a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner, controlling the magnitude of gradients from tokens outside the clipping interval to achieve an exploration-exploitation trade-off.

Result: CE-GPPO effectively mitigates entropy instability and consistently outperforms strong baselines across different model scales in mathematical reasoning benchmarks.

Conclusion: CE-GPPO effectively mitigates entropy instability and consistently outperforms strong baselines across different model scales in mathematical reasoning benchmarks.

Abstract: Reinforcement learning (RL) has become a powerful paradigm for optimizing
large language models (LLMs) to handle complex reasoning tasks. A core
challenge in this process lies in managing policy entropy, which reflects the
balance between exploration and exploitation during training. Existing methods,
such as proximal policy optimization (PPO) and its variants, discard valuable
gradient signals from low-probability tokens due to the clipping mechanism. We
systematically analyze the entropy dynamics and reveal that these clipped
tokens play a critical yet overlooked role in regulating entropy evolution. We
propose \textbf{C}ontrolling \textbf{E}ntropy via
\textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization
(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in
native PPO in a gentle and bounded manner. By controlling the magnitude of
gradients from tokens outside the clipping interval, CE-GPPO is able to achieve
an exploration-exploitation trade-off. We provide theoretical justification and
empirical evidence showing that CE-GPPO effectively mitigates entropy
instability. Extensive experiments on mathematical reasoning benchmarks show
that CE-GPPO consistently outperforms strong baselines across different model
scales.

</details>


### [79] [StyleBench: Evaluating thinking styles in Large Language Models](https://arxiv.org/abs/2509.20868)
*Junyu Guo,Shangding Gu,Ming Jin,Costas Spanos,Javad Lavaei*

Main category: cs.LG

TL;DR: 本研究介绍了StyleBench，用于评估不同推理风格在各种任务和模型中的表现。结果表明，不同风格的效果取决于模型规模和任务类型。


<details>
  <summary>Details</summary>
Motivation: 理解推理风格、模型架构和任务类型之间的相互作用对于提高大型语言模型的有效性至关重要。

Method: 我们引入了StyleBench，这是一个全面的基准，用于系统地评估不同任务和模型中的推理风格。

Result: 我们的大规模分析表明，没有一种风格是普遍最优的。基于搜索的方法在开放性问题中表现优异，但需要大规模模型；而简洁的风格在定义明确的任务中实现了显著的效率提升。

Conclusion: 我们的研究为选择最优的推理策略提供了重要的路线图，根据特定的约束条件进行选择。

Abstract: The effectiveness of Large Language Models (LLMs) is heavily influenced by
the reasoning strategies, or styles of thought, employed in their prompts.
However, the interplay between these reasoning styles, model architecture, and
task type remains poorly understood. To address this, we introduce StyleBench,
a comprehensive benchmark for systematically evaluating reasoning styles across
diverse tasks and models. We assess five representative reasoning styles,
including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought
(AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning
tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral,
Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our
large-scale analysis reveals that no single style is universally optimal. We
demonstrate that strategy efficacy is highly contingent on both model scale and
task type: search-based methods (AoT, ToT) excel in open-ended problems but
require large-scale models, while concise styles (SoT, CoD) achieve radical
efficiency gains on well-defined tasks. Furthermore, we identify key behavioral
patterns: smaller models frequently fail to follow output instructions and
default to guessing, while reasoning robustness emerges as a function of scale.
Our findings offer a crucial roadmap for selecting optimal reasoning strategies
based on specific constraints, we open source the benchmark in
https://github.com/JamesJunyuGuo/Style_Bench.

</details>


### [80] [CLUE: Conflict-guided Localization for LLM Unlearning Framework](https://arxiv.org/abs/2509.20977)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: CLUE is a framework that uses circuit discovery to identify and manipulate specific neurons responsible for forgetting or retaining knowledge in LLMs, leading to better performance in unlearning tasks.


<details>
  <summary>Details</summary>
Motivation: To address the issue of uniform interventions in existing localization-based methods that risk catastrophic over-forgetting or incomplete erasure of target knowledge.

Method: CLUE framework identifies the forget and retain circuit composed of important neurons, and then the circuits are transformed into conjunctive normal forms (CNF). The assignment of each neuron in the CNF satisfiability solution reveals whether it should be forgotten or retained. Targeted fine-tuning strategies are provided for different categories of neurons.

Result: Extensive experiments demonstrate that CLUE achieves superior forget efficacy and retain utility compared to existing localization methods.

Conclusion: CLUE achieves superior forget efficacy and retain utility through precise neural localization.

Abstract: The LLM unlearning aims to eliminate the influence of undesirable data
without affecting causally unrelated information. This process typically
involves using a forget set to remove target information, alongside a retain
set to maintain non-target capabilities. While recent localization-based
methods demonstrate promise in identifying important neurons to be unlearned,
they fail to disentangle neurons responsible for forgetting undesirable
knowledge or retaining essential skills, often treating them as a single
entangled group. As a result, these methods apply uniform interventions,
risking catastrophic over-forgetting or incomplete erasure of the target
knowledge. To address this, we turn to circuit discovery, a mechanistic
interpretability technique, and propose the Conflict-guided Localization for
LLM Unlearning framEwork (CLUE). This framework identifies the forget and
retain circuit composed of important neurons, and then the circuits are
transformed into conjunctive normal forms (CNF). The assignment of each neuron
in the CNF satisfiability solution reveals whether it should be forgotten or
retained. We then provide targeted fine-tuning strategies for different
categories of neurons. Extensive experiments demonstrate that, compared to
existing localization methods, CLUE achieves superior forget efficacy and
retain utility through precise neural localization.

</details>


### [81] [Binary Autoencoder for Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2509.20997)
*Hakaze Cho,Haolin Yang,Brian M. Kurkoski,Naoya Inoue*

Main category: cs.LG

TL;DR: The paper introduces BAE, an autoencoder that promotes sparsity and independence in features extracted from LLMs, showing improved performance in feature interpretation and dynamics analysis.


<details>
  <summary>Details</summary>
Motivation: Existing methods for extracting features from LLMs rely on autoencoders with implicit training-time regularization, leading to dense features that harm sparsity and atomization. The paper aims to address this issue by proposing a new method.

Method: Proposed a novel autoencoder variant called Binary Autoencoder (BAE) that enforces minimal entropy on minibatches of hidden activations, promoting feature independence and sparsity across instances.

Result: BAE was shown to effectively extract atomized features from LLMs, with empirical evaluations demonstrating its ability to calculate feature set entropy and improve feature interpretation.

Conclusion: BAE is effective as a feature extractor, avoiding dense features while producing the largest number of interpretable ones among baselines.

Abstract: Existing works are dedicated to untangling atomized numerical components
(features) from the hidden states of Large Language Models (LLMs) for
interpreting their mechanism. However, they typically rely on autoencoders
constrained by some implicit training-time regularization on single training
instances (i.e., $L_1$ normalization, top-k function, etc.), without an
explicit guarantee of global sparsity among instances, causing a large amount
of dense (simultaneously inactive) features, harming the feature sparsity and
atomization. In this paper, we propose a novel autoencoder variant that
enforces minimal entropy on minibatches of hidden activations, thereby
promoting feature independence and sparsity across instances. For efficient
entropy calculation, we discretize the hidden activations to 1-bit via a step
function and apply gradient estimation to enable backpropagation, so that we
term it as Binary Autoencoder (BAE) and empirically demonstrate two major
applications: (1) Feature set entropy calculation. Entropy can be reliably
estimated on binary hidden activations, which we empirically evaluate and
leverage to characterize the inference dynamics of LLMs and In-context
Learning. (2) Feature untangling. Similar to typical methods, BAE can extract
atomized features from LLM's hidden states. To robustly evaluate such feature
extraction capability, we refine traditional feature-interpretation methods to
avoid unreliable handling of numerical tokens, and show that BAE avoids dense
features while producing the largest number of interpretable ones among
baselines, which confirms the effectiveness of BAE serving as a feature
extractor.

</details>


### [82] [Mechanism of Task-oriented Information Removal in In-context Learning](https://arxiv.org/abs/2509.21012)
*Hakaze Cho,Haolin Yang,Gouki Minegishi,Naoya Inoue*

Main category: cs.LG

TL;DR: 本文通过信息移除的视角揭示了ICL的关键机制，发现其通过选择性地移除冗余信息来提升输出质量，并识别出关键的注意力头。


<details>
  <summary>Details</summary>
Motivation: 现有研究对ICL的内部机制了解不足，本文旨在揭示其背后的原理。

Method: 本文通过信息移除的视角研究ICL机制，利用低秩过滤器选择性地移除隐藏状态中的特定信息，并测量隐藏状态以观察ICL的效果。

Result: 研究发现，ICL通过选择性地移除冗余信息，提升输出质量。同时，识别出关键的注意力头（Denoising Heads）在信息移除中起重要作用。

Conclusion: 本文揭示了In-context Learning (ICL)的关键机制，即通过信息移除实现任务导向的表示。

Abstract: In-context Learning (ICL) is an emerging few-shot learning paradigm based on
modern Language Models (LMs), yet its inner mechanism remains unclear. In this
paper, we investigate the mechanism through a novel perspective of information
removal. Specifically, we demonstrate that in the zero-shot scenario, LMs
encode queries into non-selective representations in hidden states containing
information for all possible tasks, leading to arbitrary outputs without
focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we
find that selectively removing specific information from hidden states by a
low-rank filter effectively steers LMs toward the intended task. Building on
these findings, by measuring the hidden states on carefully designed metrics,
we observe that few-shot ICL effectively simulates such task-oriented
information removal processes, selectively removing the redundant information
from entangled non-selective representations, and improving the output based on
the demonstrations, which constitutes a key mechanism underlying ICL. Moreover,
we identify essential attention heads inducing the removal operation, termed
Denoising Heads, which enables the ablation experiments blocking the
information removal operation from the inference, where the ICL accuracy
significantly degrades, especially when the correct label is absent from the
few-shot demonstrations, confirming both the critical role of the information
removal mechanism and denoising heads.

</details>


### [83] [DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?](https://arxiv.org/abs/2509.21016)
*Yiyou Sun,Yuhan Cao,Pohao Huang,Haoyue Bai,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.LG

TL;DR: DELTA-Code是一个用于评估LLMs在算法编码中学习能力和转移能力的基准，揭示了强化学习训练模型在长期无奖励后突然提升的表现，并探讨了模型如何超越现有先验获取新技能。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能获得或泛化真正的新推理策略，而不仅仅是预训练或后训练中编码的技能。

Method: 引入DELTA-Code，这是一个受控基准，使用模板化问题生成器来隔离推理技能，并引入完全OOD的问题家族。通过强化学习（RL）探索训练要素，如分阶段预热、经验回放、课程训练和循环验证。

Result: 实验揭示了一个显著的掌握相变：在长时间的近零奖励后，RL训练的模型突然达到接近完美的准确率。在之前无法解决的问题家族上实现了可学习性，展示了在家庭内部和重组技能上的显著提升，但在转化案例中仍存在持续的弱点。

Conclusion: DELTA提供了一个干净的测试平台，用于探测RL驱动推理的极限，并理解模型如何超越现有先验来获取新的算法技能。

Abstract: It remains an open question whether LLMs can acquire or generalize genuinely
new reasoning strategies, beyond the sharpened skills encoded in their
parameters during pre-training or post-training. To attempt to answer this
debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and
Transferrability in Algorithmic Coding, a controlled benchmark of synthetic
coding problem families designed to probe two fundamental aspects: learnability
-- can LLMs, through reinforcement learning (RL), solve problem families where
pretrained models exhibit failure with large enough attempts (pass@K=0)? --and
transferrability -- if learnability happens, can such skills transfer
systematically to out-of-distribution (OOD) test sets? Unlike prior public
coding datasets, DELTA isolates reasoning skills through templated problem
generators and introduces fully OOD problem families that demand novel
strategies rather than tool invocation or memorized patterns. Our experiments
reveal a striking grokking phase transition: after an extended period with
near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To
enable learnability on previously unsolvable problem families, we explore key
training ingredients such as staged warm-up with dense rewards, experience
replay, curriculum training, and verification-in-the-loop. Beyond learnability,
we use DELTA to evaluate transferability or generalization along exploratory,
compositional, and transformative axes, as well as cross-family transfer.
Results show solid gains within families and for recomposed skills, but
persistent weaknesses in transformative cases. DELTA thus offers a clean
testbed for probing the limits of RL-driven reasoning and for understanding how
models can move beyond existing priors to acquire new algorithmic skills.

</details>


### [84] [ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.21070)
*Qizhi Pei,Zhuoshi Pan,Honglin Lin,Xin Gao,Yu Li,Zinan Tang,Conghui He,Rui Yan,Lijun Wu*

Main category: cs.LG

TL;DR: 本文提出了一种名为ScaleDiff的简单而有效的管道，用于大规模生成困难问题。通过使用自适应思维模型识别困难问题，并训练一个专门的生成器来生成大量困难问题，该方法在多个基准测试中表现出色，同时使用了成本效益更高的模型作为教师。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在生成困难问题时面临高计算/API成本、提示复杂性和生成问题难度有限的问题。为了克服这些限制，我们需要一种更有效和高效的解决方案。

Method: 我们提出了ScaleDiff，这是一个简单但有效的管道，用于扩展困难问题的创建。我们使用自适应思维模型从现有数据集中高效地识别困难问题，并训练一个专门的困难问题生成器（DiffGen-8B）来大规模生成新的困难问题。

Result: 在ScaleDiff-Math数据集上微调Qwen2.5-Math-7B-Instruct，相比原始数据集性能提高了11.3%，并在AIME'24、AIME'25、HMMT-Feb'25、BRUMO'25和MATH500上达到了65.9%的平均准确率，超过了最近的强LRMs如OpenThinker3。

Conclusion: 我们的方法能够有效地生成困难问题，并且在性能上超过了最近的大型推理模型，同时使用了成本效益更高的模型作为教师。

Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in complex
problem-solving, often benefiting from training on difficult mathematical
problems that stimulate intricate reasoning. Recent efforts have explored
automated synthesis of mathematical problems by prompting proprietary models or
large-scale open-source models from seed data or inherent mathematical
concepts. However, scaling up these methods remains challenging due to their
high computational/API cost, complexity of prompting, and limited difficulty
level of the generated problems. To overcome these limitations, we propose
ScaleDiff, a simple yet effective pipeline designed to scale the creation of
difficult problems. We efficiently identify difficult problems from existing
datasets with only a single forward pass using an adaptive thinking model,
which can perceive problem difficulty and automatically switch between
"Thinking" and "NoThinking" modes. We then train a specialized difficult
problem generator (DiffGen-8B) on this filtered difficult data, which can
produce new difficult problems in large scale, eliminating the need for
complex, per-instance prompting and its associated high API costs. Fine-tuning
Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial
performance increase of 11.3% compared to the original dataset and achieves a
65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500,
outperforming recent strong LRMs like OpenThinker3. Notably, this performance
is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating
that our pipeline can effectively transfer advanced reasoning capabilities
without relying on larger, more expensive teacher models. Furthermore, we
observe a clear scaling phenomenon in model performance on difficult benchmarks
as the quantity of difficult problems increases. Code:
https://github.com/QizhiPei/ScaleDiff.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [85] [Blueprints of Trust: AI System Cards for End to End Transparency and Governance](https://arxiv.org/abs/2509.20394)
*Huzaifa Sidhpurwala,Emily Fox,Garth Mollett,Florencio Cano Gabarda,Roman Zhukov*

Main category: cs.CY

TL;DR: 本文介绍了HASC，一种新的框架，用于增强AI系统的透明度和问责性。


<details>
  <summary>Details</summary>
Motivation: 为了提高AI系统开发和部署的透明度和问责性，现有的模型卡和系统卡概念需要进一步扩展。

Method: HASC通过集成全面的动态记录来增强AI系统的安全性和安全性，并提出了一种标准化的标识符系统，包括新的AI Safety Hazard (ASH) ID。

Result: HASC为开发者和利益相关者提供了一个单一、可访问的真相来源，使他们能够更明智地做出关于AI系统安全性的决策。

Conclusion: HASC可以与ISO/IEC 42001:2023标准互补，提供更大的透明度和问责性。

Abstract: This paper introduces the Hazard-Aware System Card (HASC), a novel framework
designed to enhance transparency and accountability in the development and
deployment of AI systems. The HASC builds upon existing model card and system
card concepts by integrating a comprehensive, dynamic record of an AI system's
security and safety posture. The framework proposes a standardized system of
identifiers, including a novel AI Safety Hazard (ASH) ID, to complement
existing security identifiers like CVEs, allowing for clear and consistent
communication of fixed flaws. By providing a single, accessible source of
truth, the HASC empowers developers and stakeholders to make more informed
decisions about AI system safety throughout its lifecycle. Ultimately, we also
compare our proposed AI system cards with the ISO/IEC 42001:2023 standard and
discuss how they can be used to complement each other, providing greater
transparency and accountability for AI systems.

</details>


### [86] [Communication Bias in Large Language Models: A Regulatory Perspective](https://arxiv.org/abs/2509.21075)
*Adrian Kuenzler,Stefan Schmid*

Main category: cs.CY

TL;DR: 本文讨论了大型语言模型在偏见、公平性和监管合规性方面的风险，并强调需要更强的竞争和设计治理以确保公平和可信的人工智能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在许多应用中日益重要，人们对偏见、公平性和监管合规性产生了担忧。

Method: 本文回顾了偏见输出的风险及其社会影响，并聚焦于欧盟的《人工智能法案》和《数字服务法案》等框架。

Result: 本文强调了在监管之外，需要更强的竞争和设计治理来确保公平和可信的人工智能。

Conclusion: 本文认为，除了持续的监管外，还需要对竞争和设计治理给予更多关注，以确保公平、可信赖的人工智能。

Abstract: Large language models (LLMs) are increasingly central to many applications,
raising concerns about bias, fairness, and regulatory compliance. This paper
reviews risks of biased outputs and their societal impact, focusing on
frameworks like the EU's AI Act and the Digital Services Act. We argue that
beyond constant regulation, stronger attention to competition and design
governance is needed to ensure fair, trustworthy AI. This is a preprint of the
Communications of the ACM article of the same title.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [87] [Interactive Recommendation Agent with Active User Commands](https://arxiv.org/abs/2509.21317)
*Jiakai Tang,Yujie Luo,Xunke Xi,Fei Sun,Xueyang Feng,Sunhao Dai,Chao Yi,Dian Chen,Zhujin Gao,Yang Li,Xu Chen,Wen Chen,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 本文提出了一种名为Interactive Recommendation Feed（IRF）的新范式，通过自然语言命令实现用户对推荐策略的主动控制，并开发了RecBot双代理架构来支持这一范式，取得了显著的用户满意度和业务成果提升。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖于被动反馈机制，这限制了用户只能进行简单的选择，如喜欢和不喜欢。然而，这些粗粒度信号无法捕捉用户的细微行为动机和意图。当前系统也无法区分哪些特定的项目属性驱动用户满意度或不满，导致偏好建模不准确。这些基本限制造成了用户意图和系统解释之间的持续差距，最终损害了用户满意度和系统有效性。

Method: 我们开发了RecBot，这是一种双代理架构，其中解析器代理将语言表达转换为结构化偏好，规划器代理动态协调自适应工具链以进行实时策略调整。为了实现实际部署，我们采用模拟增强的知识蒸馏来实现高效性能同时保持强大的推理能力。

Result: 我们引入了交互式推荐流（IRF），这是一种开创性的范式，允许在主流推荐流中使用自然语言命令。与传统系统不同，IRF使用户能够通过实时语言命令对推荐策略进行主动显式控制。

Conclusion: 通过广泛的离线和长期在线实验，RecBot在用户满意度和业务成果方面都表现出显著的改进。

Abstract: Traditional recommender systems rely on passive feedback mechanisms that
limit users to simple choices such as like and dislike. However, these
coarse-grained signals fail to capture users' nuanced behavior motivations and
intentions. In turn, current systems cannot also distinguish which specific
item attributes drive user satisfaction or dissatisfaction, resulting in
inaccurate preference modeling. These fundamental limitations create a
persistent gap between user intentions and system interpretations, ultimately
undermining user satisfaction and harming system effectiveness.
  To address these limitations, we introduce the Interactive Recommendation
Feed (IRF), a pioneering paradigm that enables natural language commands within
mainstream recommendation feeds. Unlike traditional systems that confine users
to passive implicit behavioral influence, IRF empowers active explicit control
over recommendation policies through real-time linguistic commands. To support
this paradigm, we develop RecBot, a dual-agent architecture where a Parser
Agent transforms linguistic expressions into structured preferences and a
Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly
policy adjustment. To enable practical deployment, we employ
simulation-augmented knowledge distillation to achieve efficient performance
while maintaining strong reasoning capabilities. Through extensive offline and
long-term online experiments, RecBot shows significant improvements in both
user satisfaction and business outcomes.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [88] [Every Character Counts: From Vulnerability to Defense in Phishing Detection](https://arxiv.org/abs/2509.20589)
*Maria Chiper,Radu Tudor Ionescu*

Main category: cs.CR

TL;DR: 本文研究了字符级深度学习模型在检测网络钓鱼攻击方面的有效性，发现CharGRU在各种场景中表现最佳，并且对抗训练能提高模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的自动检测方法在检测新型网络钓鱼攻击时缺乏可解释性和鲁棒性。因此，我们需要一种更有效的方法来检测网络钓鱼攻击。

Method: 我们评估了三种字符级神经架构：CharCNN、CharGRU和CharBiLSTM，并在自建的电子邮件数据集上测试它们的性能。我们还测试了这些模型在有限计算资源下的表现，并使用Grad-CAM技术进行可视化分析。

Result: CharGRU在所有场景中表现最佳，所有模型都对对抗攻击表现出脆弱性，但对抗训练显著提高了它们的鲁棒性。此外，我们能够通过Grad-CAM技术可视化影响模型决策的邮件部分。

Conclusion: 我们的研究表明，字符级深度学习模型在检测网络钓鱼攻击方面具有潜力，尤其是在提供可解释性和鲁棒性方面。CharGRU在所有场景中表现最佳，而对抗训练可以显著提高模型的鲁棒性。此外，我们通过适应Grad-CAM技术，能够可视化影响模型决策的邮件部分。

Abstract: Phishing attacks targeting both organizations and individuals are becoming an
increasingly significant threat as technology advances. Current automatic
detection methods often lack explainability and robustness in detecting new
phishing attacks. In this work, we investigate the effectiveness of
character-level deep learning models for phishing detection, which can provide
both robustness and interpretability. We evaluate three neural architectures
adapted to operate at the character level, namely CharCNN, CharGRU, and
CharBiLSTM, on a custom-built email dataset, which combines data from multiple
sources. Their performance is analyzed under three scenarios: (i) standard
training and testing, (ii) standard training and testing under adversarial
attacks, and (iii) training and testing with adversarial examples. Aiming to
develop a tool that operates as a browser extension, we test all models under
limited computational resources. In this constrained setup, CharGRU proves to
be the best-performing model across all scenarios. All models show
vulnerability to adversarial attacks, but adversarial training substantially
improves their robustness. In addition, by adapting the Gradient-weighted Class
Activation Mapping (Grad-CAM) technique to character-level inputs, we are able
to visualize which parts of each email influence the decision of each model.
Our open-source code and data is released at
https://github.com/chipermaria/every-character-counts.

</details>


### [89] [PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints](https://arxiv.org/abs/2509.21057)
*Jiahao Huo,Shuliang Liu,Bin Wang,Junyan Zhang,Yibo Yan,Aiwei Liu,Xuming Hu,Mingxun Zhou*

Main category: cs.CR

TL;DR: PMark is a new SWM method that uses proxy functions to enhance watermarking robustness while maintaining text quality, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing SWM methods lack strong theoretical guarantees of robustness and introduce significant distribution distortions compared with unwatermarked outputs.

Method: PMark, a simple yet powerful SWM method that estimates the PF median for the next sentence dynamically through sampling while enforcing multiple PF constraints (which we call channels) to strengthen watermark evidence.

Result: PMark achieves the desired distortion-free property and improves the robustness against paraphrasing-style attacks. An empirically optimized version further removes the requirement for dynamical median estimation for better sampling efficiency.

Conclusion: PMark consistently outperforms existing SWM baselines in both text quality and robustness, offering a more effective paradigm for detecting machine-generated text.

Abstract: Semantic-level watermarking (SWM) for large language models (LLMs) enhances
watermarking robustness against text modifications and paraphrasing attacks by
treating the sentence as the fundamental unit. However, existing methods still
lack strong theoretical guarantees of robustness, and reject-sampling-based
generation often introduces significant distribution distortions compared with
unwatermarked outputs. In this work, we introduce a new theoretical framework
on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions
that map sentences to scalar values. Building on this framework, we propose
PMark, a simple yet powerful SWM method that estimates the PF median for the
next sentence dynamically through sampling while enforcing multiple PF
constraints (which we call channels) to strengthen watermark evidence. Equipped
with solid theoretical guarantees, PMark achieves the desired distortion-free
property and improves the robustness against paraphrasing-style attacks. We
also provide an empirically optimized version that further removes the
requirement for dynamical median estimation for better sampling efficiency.
Experimental results show that PMark consistently outperforms existing SWM
baselines in both text quality and robustness, offering a more effective
paradigm for detecting machine-generated text. Our code will be released at
[this URL](https://github.com/PMark-repo/PMark).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [90] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: 本文介绍了InsightGUIDE，这是一种新的AI驱动的阅读辅助工具，旨在提供简洁、结构化的见解，帮助研究人员更有效地理解论文的核心内容。


<details>
  <summary>Details</summary>
Motivation: The proliferation of scientific literature presents an increasingly significant challenge for researchers. Existing tools often provide verbose summaries that risk replacing, rather than assisting, the reading of the source material.

Method: The paper introduces InsightGUIDE, a novel AI-powered tool designed to function as a reading assistant. The system provides concise, structured insights by embedding an expert's reading methodology into its core AI logic. The paper presents the system's architecture, its prompt-driven methodology, and a qualitative case study comparing its output to a general-purpose LLM.

Result: The results demonstrate that InsightGUIDE produces more structured and actionable guidance, serving as a more effective tool for the modern researcher.

Conclusion: InsightGUIDE produces more structured and actionable guidance, serving as a more effective tool for the modern researcher.

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [91] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE is a framework for multi-hop question answering that balances accuracy, latency, and cost by treating context construction as a sequential decision process. It uses a three-agent system and a new algorithm to optimize subgraph construction, reasoning-path discovery, and evidence selection under resource budgets.


<details>
  <summary>Details</summary>
Motivation: Deployed systems must balance answer accuracy with strict latency and cost targets while preserving provenance. Static k-hop expansions and 'think-longer' prompting often over-retrieve, inflate context, and yield unpredictable runtime.

Method: CLAUSE is an agentic three-agent neuro-symbolic framework that treats context construction as a sequential decision process over knowledge graphs. It employs the Lagrangian-Constrained Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate three agents: Subgraph Architect, Path Navigator, and Context Curator.

Result: CLAUSE achieves higher EM@1 while reducing subgraph growth and end-to-end latency at equal or lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline (GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower edge growth.

Conclusion: CLAUSE yields higher EM@1 while reducing subgraph growth and end-to-end latency at equal or lower token budgets. The resulting contexts are compact, provenance-preserving, and deliver predictable performance under deployment constraints.

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [92] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: 本研究挑战了说服效果主要取决于模型规模的假设，提出这些动态本质上由模型的基本认知过程决定，特别是其显式推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着最近多智能体系统（MAS）的迅速增长，其中大型语言模型（LLMs）和大型推理模型（LRMs）通常合作解决复杂问题，需要深入了解支配其相互作用的说服动力学。

Method: 通过一系列多智能体说服实验，我们发现了称为说服二元性的基本权衡。我们进一步考虑了更复杂的传输说服情况，并揭示了多跳说服中影响传播和衰减的复杂动态。

Result: 我们的发现表明，LRMs中的推理过程表现出对说服的显著抵抗力，更牢固地保持其初始信念。相反，通过共享“思考内容”使这一推理过程透明化，大大增加了它们说服他人的能力。

Conclusion: 本研究提供了系统证据，将模型的内部处理架构与其外部说服行为联系起来，为先进模型的易受攻击性提供了新的解释，并突出了对未来MAS的安全性、鲁棒性和设计的重要影响。

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [93] [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
*Yidong Wang,Yunze Song,Tingyuan Zhu,Xuanwang Zhang,Zhuohao Yu,Hao Chen,Chiyu Song,Qiufeng Wang,Cunxiang Wang,Zhen Wu,Xinyu Dai,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.AI

TL;DR: 本文分析了LLM-as-a-judge评估框架中的不一致性问题，并提出TrustJudge框架来解决这些问题，通过分布敏感评分和似然感知聚合提高评估的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前评估框架在LLM-as-a-judge中存在关键不一致性，包括分数比较不一致和成对传递性不一致，这些问题源于离散评分系统的信息丢失和成对评估中的模糊平局判断。

Method: 本文提出了TrustJudge框架，通过两个关键创新解决评估框架中的不一致性问题：1) 分布敏感评分，从离散评分概率计算连续期望，保留信息熵以实现更精确的评分；2) 似然感知聚合，使用双向偏好概率或困惑度解决传递性违反问题。

Result: 在使用Llama-3.1-70B-Instruct作为评判者进行评估时，TrustJudge将分数比较不一致性降低了8.43%（从23.32%降至14.89%），将成对传递性不一致性降低了10.82%（从15.22%降至4.40%），同时保持了更高的评估准确性。

Conclusion: 本文提供了对LLM-as-a-judge范式中评估框架不一致性的首次系统分析，提出了TrustJudge框架，该框架在不增加额外训练或人工标注的情况下，实现了更可信的LLM评估。

Abstract: The adoption of Large Language Models (LLMs) as automated evaluators
(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation
frameworks. We identify two fundamental types of inconsistencies: (1)
Score-Comparison Inconsistency, where lower-rated responses outperform
higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity
Inconsistency, manifested through circular preference chains (A>B>C>A) and
equivalence contradictions (A=B=C\neq A). We argue that these issues come from
information loss in discrete rating systems and ambiguous tie judgments during
pairwise evaluation. We propose TrustJudge, a probabilistic framework that
addresses these limitations through two key innovations: 1)
distribution-sensitive scoring that computes continuous expectations from
discrete rating probabilities, preserving information entropy for more precise
scoring, and 2) likelihood-aware aggregation that resolves transitivity
violations using bidirectional preference probabilities or perplexity. We also
formalize the theoretical limitations of current LLM-as-a-judge frameworks and
demonstrate how TrustJudge's components overcome them. When evaluated with
Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces
Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise
Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining
higher evaluation accuracy. Our work provides the first systematic analysis of
evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both
theoretical insights and practical solutions for reliable automated assessment.
The framework demonstrates consistent improvements across various model
architectures and scales, enabling more trustworthy LLM evaluation without
requiring additional training or human annotations. The codes can be found at
https://github.com/TrustJudge/TrustJudge.

</details>


### [94] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 本文提出了一种新方法，通过利用高价值推理模式提升模型推理能力，在挑战性任务中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在使用CoT数据时缺乏针对性，因此需要确定哪些数据类型最能增强模型的推理能力。

Method: 本文定义了基础模型的推理潜力，并提出了一个双粒度算法，用于从数据池中高效选择高价值的CoT数据。

Result: 使用10B-token的CoTP数据，85A6B MoE模型在AIME 2024和2025上提升了9.58%，并提高了下游RL性能的上限7.81%。

Conclusion: 本文提出了一种新的方法，通过利用高价值的推理模式来扩展基础模型的推理潜力，从而显著提高了模型在挑战性任务上的性能。

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [95] [Visual Authority and the Rhetoric of Health Misinformation: A Multimodal Analysis of Social Media Videos](https://arxiv.org/abs/2509.20724)
*Mohammad Reza Zarei,Barbara Stead-Coyle,Michael Christensen,Sarah Everts,Majid Komeili*

Main category: cs.SI

TL;DR: 本研究分析了短视频平台上营养和补充剂视频中可信度的包装方式，发现自信的单个演讲者主导了内容，权威线索常与说服性元素和盈利模式结合。


<details>
  <summary>Details</summary>
Motivation: 由于短视频平台是健康建议的重要场所，其中混合了有用、误导性和有害的内容，因此需要研究可信度的包装方式。

Method: 研究收集了来自TikTok、Instagram和YouTube的152个公开视频，并对每个视频的26个特征进行了标注，包括视觉权威、演讲者属性、叙事策略和互动线索。

Result: 研究发现，自信的单一演讲者在工作室或家庭环境中占主导地位，而临床环境很少见。权威线索如头衔、幻灯片和图表以及证书经常与说服性元素如术语、参考、恐惧或紧迫感、对主流医学的批评和阴谋论一起出现，并与盈利模式如销售链接和订阅呼吁一起出现。

Conclusion: 本研究分析了在短视频平台上，可信度是如何通过权威信号、叙事技巧和盈利模式来包装的。

Abstract: Short form video platforms are central sites for health advice, where
alternative narratives mix useful, misleading, and harmful content. Rather than
adjudicating truth, this study examines how credibility is packaged in
nutrition and supplement videos by analyzing the intersection of authority
signals, narrative techniques, and monetization. We assemble a cross platform
corpus of 152 public videos from TikTok, Instagram, and YouTube and annotate
each on 26 features spanning visual authority, presenter attributes, narrative
strategies, and engagement cues. A transparent annotation pipeline integrates
automatic speech recognition, principled frame selection, and a multimodal
model, with human verification on a stratified subsample showing strong
agreement. Descriptively, a confident single presenter in studio or home
settings dominates, and clinical contexts are rare. Analytically, authority
cues such as titles, slides and charts, and certificates frequently occur with
persuasive elements including jargon, references, fear or urgency, critiques of
mainstream medicine, and conspiracies, and with monetization including sales
links and calls to subscribe. References and science like visuals often travel
with emotive and oppositional narratives rather than signaling restraint.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [96] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: 研究发现当前验证方式过于严格，限制了代码生成模型的性能。通过调整验证策略和使用多样化的测试用例，可以突破验证瓶颈，提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 大型代码生成语言模型依赖于合成数据，但这种数据的验证存在瓶颈，即验证能力限制了训练数据的质量和多样性。

Method: 系统研究了验证设计和策略如何影响模型性能，分析了验证内容、验证方式以及验证的必要性。

Result: 更丰富的测试用例可以提高代码生成能力（平均+3 pass@1），而单纯增加数量则收益递减；允许宽松的通过阈值或引入基于LLM的软验证可以恢复有价值的训练数据，提升pass@1性能2-4点；保留多样正确的解决方案可以带来一致的泛化增益。

Conclusion: 当前的验证方式过于严格，过滤掉了有价值的多样性。但不能被抛弃，只能重新校准。通过结合校准后的验证与多样且具有挑战性的问题-解决方案对，可以突破验证上限，解锁更强的代码生成模型。

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [97] [On Theoretical Interpretations of Concept-Based In-Context Learning](https://arxiv.org/abs/2509.20882)
*Huaze Tang,Tianren Peng,Shao-lun Huang*

Main category: cs.IT

TL;DR: 本文研究了基于概念的In-Context Learning（CB-ICL）的理论机制，分析了其在少样本提示任务中的有效性，并通过实验验证了其实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 尽管In-Context Learning（ICL）已成为自然语言处理和大语言模型应用的重要范式，但其理论理解仍然有限。因此，本文旨在研究CB-ICL的机制，以提高对ICL的理解。

Method: 本文研究了一种特定的ICL方法——基于概念的ICL（CB-ICL），并进行了理论分析，解释了CB-ICL为何以及在什么情况下能有效预测查询标签。此外，提出了一个相似性度量，用于衡量提示示例与查询输入之间的关系。

Result: 理论分析揭示了CB-ICL的有效性，并量化了LLMs可以从提示任务中利用的知识。同时，研究还探讨了提示示例数量和LLM嵌入维度对ICL的影响。实验验证了CB-ICL及其理论的实际价值。

Conclusion: 本文通过理论分析和实验验证，证明了基于概念的In-Context Learning（CB-ICL）在少样本提示任务中的有效性，并为模型预训练和提示工程提供了重要见解。

Abstract: In-Context Learning (ICL) has emerged as an important new paradigm in natural
language processing and large language model (LLM) applications. However, the
theoretical understanding of the ICL mechanism remains limited. This paper aims
to investigate this issue by studying a particular ICL approach, called
concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on
applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs
well for predicting query labels in prompts with only a few demonstrations. In
addition, the proposed theory quantifies the knowledge that can be leveraged by
the LLMs to the prompt tasks, and leads to a similarity measure between the
prompt demonstrations and the query input, which provides important insights
and guidance for model pre-training and prompt engineering in ICL. Moreover,
the impact of the prompt demonstration size and the dimension of the LLM
embeddings in ICL are also explored based on the proposed theory. Finally,
several real-data experiments are conducted to validate the practical
usefulness of CB-ICL and the corresponding theory.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [98] [Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems](https://arxiv.org/abs/2509.21143)
*Junfeng Yan,Biao Wu,Meng Fang,Ling Chen*

Main category: cs.RO

TL;DR: 本文提出了一种针对汽车GUI的高保真基准和交互环境Automotive-ENV，并基于此提出了ASURADA，一个结合GPS信息的多模态代理。实验表明，地理感知信息显著提高了安全相关任务的成功率，强调了在汽车环境中基于位置的上下文的重要性。


<details>
  <summary>Details</summary>
Motivation: 汽车GUI面临驾驶员注意力有限、严格的安全要求和复杂的基于位置的交互模式等挑战，因此需要专门的基准和方法来解决这些问题。

Method: 本文介绍了Automotive-ENV，这是一个针对汽车GUI的高保真基准和交互环境，以及基于该基准提出的ASURADA，一个结合GPS信息的多模态代理。

Result: 实验表明，地理感知信息显著提高了安全相关任务的成功率，强调了在汽车环境中基于位置的上下文的重要性。

Conclusion: 本文提出了一个针对汽车GUI的高保真基准和交互环境Automotive-ENV，并基于此提出了ASURADA，一个地理感知的多模态代理。实验表明，地理感知信息显著提高了安全相关任务的成功率，强调了在汽车环境中基于位置的上下文的重要性。作者将发布Automotive-ENV，以进一步推动安全和自适应车内代理的发展。

Abstract: Multimodal agents have demonstrated strong performance in general GUI
interactions, but their application in automotive systems has been largely
unexplored. In-vehicle GUIs present distinct challenges: drivers' limited
attention, strict safety requirements, and complex location-based interaction
patterns. To address these challenges, we introduce Automotive-ENV, the first
high-fidelity benchmark and interaction environment tailored for vehicle GUIs.
This platform defines 185 parameterized tasks spanning explicit control,
implicit intent understanding, and safety-aware tasks, and provides structured
multimodal observations with precise programmatic checks for reproducible
evaluation. Building on this benchmark, we propose ASURADA, a geo-aware
multimodal agent that integrates GPS-informed context to dynamically adjust
actions based on location, environmental conditions, and regional driving
norms. Experiments show that geo-aware information significantly improves
success on safety-aware tasks, highlighting the importance of location-based
context in automotive environments. We will release Automotive-ENV, complete
with all tasks and benchmarking tools, to further the development of safe and
adaptive in-vehicle agents.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [99] [RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows](https://arxiv.org/abs/2509.20490)
*Kai Zhang,Corey D Barrett,Jangwon Kim,Lichao Sun,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.MA

TL;DR: RadAgents是一个多代理框架，用于胸部X光片解释，能够结合临床先验和任务感知的多模态推理，并通过整合接地和多模态检索增强来验证和解决上下文冲突，从而产生更可靠、透明且符合临床实践的输出。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在胸部X光片（CXR）解释中存在局限性，包括推理不具有临床可解释性、多模态证据融合不足以及无法检测或解决跨工具不一致。

Method: RadAgents是一个多代理框架，结合了临床先验和任务感知的多模态推理，并整合了接地和多模态检索增强来验证和解决上下文冲突。

Result: RadAgents框架能够生成更可靠、透明且符合临床实践的输出，并通过整合接地和多模态检索增强来验证和解决上下文冲突。

Conclusion: RadAgents是一个多代理框架，能够将临床先验与任务感知的多模态推理相结合，并通过整合接地和多模态检索增强来验证和解决上下文冲突，从而产生更可靠、透明且符合临床实践的输出。

Abstract: Agentic systems offer a potential path to solve complex clinical tasks
through collaboration among specialized agents, augmented by tool use and
external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation,
prevailing methods remain limited: (i) reasoning is frequently neither
clinically interpretable nor aligned with guidelines, reflecting mere
aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused,
yielding text-only rationales that are not visually grounded; and (iii) systems
rarely detect or resolve cross-tool inconsistencies and provide no principled
verification mechanisms. To bridge the above gaps, we present RadAgents, a
multi-agent framework for CXR interpretation that couples clinical priors with
task-aware multimodal reasoning. In addition, we integrate grounding and
multimodal retrieval-augmentation to verify and resolve context conflicts,
resulting in outputs that are more reliable, transparent, and consistent with
clinical practice.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.IR](#cs.IR) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 本文通过系统回顾分析了多语言和非英语语境下的偏见评估和缓解研究，指出了方法设计的不足，并提出了未来研究方向以增强多语言偏见文献的包容性和跨文化适用性。


<details>
  <summary>Details</summary>
Motivation: 为了揭示多语言预训练模型中的社会偏见，并探索如何在不同语言和文化背景下适应偏见基准测试。

Method: 系统回顾方法，分析了多语言和非英语语境下的偏见评估和缓解研究。

Result: 发现了方法设计上的不足，如对某些语言的偏好和多语言缓解实验的稀缺性，同时列出了在跨语言和文化适应偏见基准时遇到的问题和解决方案。

Conclusion: 本文通过系统回顾分析了将偏见评估和缓解方法扩展到多语言和非英语语境的研究，指出了该领域在方法设计上的不足，并提出了未来研究的方向，以增强多语言偏见文献的包容性和跨文化适用性。

Abstract: Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [2] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

TL;DR: 本研究探讨了使用语言模型自动生成多项选择题以减少手动测试开发的成本和不一致性。通过比较微调的中型模型和未微调的大型模型，并评估多种结构化提示策略，结果显示结构化提示显著提高了中型模型的性能。研究强调了结合自动化指标、专家判断和大模型模拟的价值，以确保与评估目标的一致性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索使用语言模型自动生成（AIG）多项选择题（MCQs）用于形态学评估，以减少手动测试开发的成本和不一致性。

Method: 本研究采用两步方法。首先，比较了微调的中型模型（Gemma, 2B）与未微调的大型模型（GPT-3.5, 175B）。其次，评估了七种结构化提示策略，包括零样本、少样本、思维链、基于角色、顺序以及组合。

Result: 结果表明，结构化提示，特别是结合思维链和顺序设计的策略，显著提高了Gemma的输出。Gemma通常比GPT-3.5的零样本响应产生更多符合构想和教学适当的项目，提示设计在中型模型性能中起关键作用。

Conclusion: 本研究展示了结构化提示和高效微调可以在有限数据条件下增强中型模型进行自动生成的能力。我们强调了结合自动化指标、专家判断和大模型模拟的价值，以确保与评估目标的一致性。所提出的流程提供了一种实用且可扩展的方法来开发和验证K-12的语言评估项目。

Abstract: This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [3] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 本文提出一种将SystemC TLM模型集成到基于FMI的协同仿真工作流中的开源方法，解决了跨域集成的技术挑战，并通过案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着网络物理系统复杂性的增加，特别是在汽车应用中，对高效建模和跨域协同仿真的需求不断增加。SystemC TLM虽然能有效支持硬件/软件协同设计，但其与其他工程领域模型的互操作性有限，导致集成挑战。

Method: 本文介绍了一种轻量级开源工具链，解决了时间同步和数据交换等关键技术问题，并通过代表性案例研究验证了集成的可行性和有效性。

Result: 本文通过案例研究验证了所提方法的可行性与有效性，展示了SystemC TLM模型在FMI协同仿真环境中的无缝集成。

Conclusion: 本文提出了一种将SystemC TLM模型集成到基于FMI的协同仿真工作流中的全开源方法，通过封装SystemC TLM组件为FMI 3.0协同仿真功能模型单元（FMU），实现了跨异构仿真环境的无缝标准化集成。

Abstract: The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [4] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

TL;DR: 本文提出了一种名为DGPO的方法，通过从教师演示中进行冷启动初始化和在策略优化过程中持续的教师指导来解决紧凑语言模型在强化学习中的挑战。并通过引入ARC指标评估方法，实验表明DGPO使紧凑模型能够实现复杂的代理搜索行为，甚至在某些情况下超越了更大的教师模型。


<details>
  <summary>Details</summary>
Motivation: Compact language models struggle due to poor reasoning ability, resulting in sparse rewards and unstable training.

Method: Distillation-Guided Policy Optimization (DGPO), which addresses the challenges through cold-start initialization from teacher demonstrations and continuous teacher guidance during policy optimization.

Result: DGPO enables compact models to achieve sophisticated agentic search behaviors, even outperforming the larger teacher model in some cases.

Conclusion: DGPO makes agentic RAG feasible in computing resource-constrained environments.

Abstract: Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [5] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

TL;DR: This paper introduces GUARD, a testing method that operationalizes ethical guidelines into specific questions to assess large language model compliance. It identifies direct violations and uses jailbreak diagnostics to uncover potential safety mechanism bypasses, validating its effectiveness across multiple models.


<details>
  <summary>Details</summary>
Motivation: The increasing use of large language models in various domains has raised concerns about their potential to generate harmful responses. While governments have issued ethics guidelines, there is a gap in translating these guidelines into actionable testing questions to verify LLM compliance.

Method: GUARD is a testing method that operationalizes ethical guidelines into specific guideline-violating questions to assess large language model (LLM) adherence. It uses automated generation of guideline-violating questions and integrates the concept of 'jailbreaks' for diagnostics, named GUARD-JD, which creates scenarios that provoke unethical responses.

Result: GUARD was empirically validated on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications.

Conclusion: GUARD provides a comprehensive method for testing the compliance of large language models with ethical guidelines and can effectively identify potential violations, including those that could bypass safety mechanisms. It has been validated on multiple LLMs and can be extended to vision-language models.

Abstract: As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [6] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

TL;DR: JERR is a framework that enhances long-context comprehension in LLMs through graph-based reasoning, improving reliability and transparency.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long contexts due to memory limitations and their inability to tackle complex and long-context tasks. They also lack transparency and are prone to producing hallucinations.

Method: JERR integrates three key components: synopsis extraction, graph construction, and relational reasoning. It uses chunking text strategically, builds a directed acyclic graph (DAG) to resolve redundancy, and incorporates Monte Carlo Tree Search (MCTS) to navigate complex reasoning paths.

Result: Experimental results show that JERR consistently outperforms all baselines on the ROUGE and F1 metrics, achieving the highest scores on the LLM-Rater evaluation.

Conclusion: JERR provides a novel solution that enables LLMs to handle extended contexts and complex reasoning tasks with improved reliability and transparency.

Abstract: Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [7] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

TL;DR: This paper introduces NPH graph problems as a synthetic training corpus for enhancing Long CoT reasoning in LLMs through a two-stage post-training framework.


<details>
  <summary>Details</summary>
Motivation: Developing Long CoT behaviors relies heavily on post-training with high-quality datasets, which are costly and human-curated. Scalable alternatives need to be explored.

Method: Introducing NP-hard (NPH) graph problems as a synthetic training corpus and developing a two-stage post-training framework: (i) Long CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, and (ii) Reinforcement Learning (RL) with a fine-grained reward design.

Result: The flagship model, Graph-R1-7B, demonstrates strong generalization across mathematics, coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency.

Conclusion: NPH graph problems can serve as an effective and scalable resource for advancing Long CoT reasoning in LLMs, opening a new frontier for LLM post-training.

Abstract: Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [8] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本文提出了一个上下文感知的人格评估框架（CAPE），用于评估大型语言模型在对话历史影响下的行为特征。研究发现，对话历史可以增强响应一致性，但也可能导致性格变化。


<details>
  <summary>Details</summary>
Motivation: 传统上用于评估人类的心理测量测试现在被应用于大型语言模型（LLMs）以评估其行为特征。然而，现有研究遵循一种无上下文的方法，单独回答每个问题以避免上下文影响。我们称这为迪士尼世界测试，这是一种忽略现实应用的人工设置。

Method: 我们提出了第一个上下文感知人格评估（CAPE）框架，结合了之前的对话交互，并引入了新的度量标准来量化LLM响应的一致性。

Result: 我们在7个LLM上的广泛实验表明，对话历史通过上下文学习增强了响应一致性，但也引发了性格变化，GPT-3.5-Turbo和GPT-4-Turbo表现出极端偏差。虽然GPT模型对问题顺序具有鲁棒性，但Gemini-1.5-Flash和Llama-8B表现出显著的敏感性。此外，GPT模型的响应源于其内在的人格特质以及之前的互动，而Gemini-1.5-Flash和Llama--8B则严重依赖于之前的互动。

Conclusion: 我们的框架应用于角色扮演代理（RPAs）显示，依赖于上下文的人格变化可以提高响应一致性，并更好地与人类判断对齐。

Abstract: Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [9] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

TL;DR: 研究分析了推理步骤的效用对最终答案正确性的影响，发现条件熵的变化模式可以预测答案的正确性，并指出较长的推理路径不一定更好。


<details>
  <summary>Details</summary>
Motivation: 由于自回归生成的随机性，生成更多上下文并不保证增加对答案的信心。如果能在生成过程中预测推理步骤是否有用，就可以提前停止或修剪无效步骤，避免最终决策中的干扰。

Method: 在MATH数据集上进行了一项oracle研究，使用Qwen2.5-32B和GPT-4o生成推理链，并利用另一个模型（Qwen3-8B）来量化这些链对最终准确性的效用。测量了模型在每个推理步骤上的答案跨度Y的不确定性，使用条件熵（随着上下文逐步扩展的词汇表上的期望负对数似然）。

Result: 结果表明，随着步骤减少的条件熵与正确答案密切相关，而平坦或增加的熵通常导致错误答案。此外，错误的推理路径往往比正确的更长，这表明更长的推理不一定产生更好的结果。

Conclusion: 这些发现为设计高效的推理管道提供了基础，以检测并避免早期无益的推理。

Abstract: Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [10] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

TL;DR: UI-Bench is a large-scale benchmark that evaluates the visual excellence of AI text-to-app tools through expert pairwise comparison, establishing a reproducible standard for AI-driven web design.


<details>
  <summary>Details</summary>
Motivation: There is no public benchmark that rigorously verifies the claims of high-quality applications and websites generated by AI text-to-app tools.

Method: UI-Bench evaluates visual excellence across competing AI text-to-app tools through expert pairwise comparison, using a TrueSkill-derived model to rank systems with calibrated confidence intervals.

Result: UI-Bench spans 10 tools, 30 prompts, 300 generated sites, and 4000+ expert judgments. It releases the complete prompt set, an open-source evaluation framework, and a public leaderboard.

Conclusion: UI-Bench establishes a reproducible standard for advancing AI-driven web design.

Abstract: AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [11] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文介绍了一个名为DentalBench的双语基准，用于评估和推进牙科领域的大型语言模型。实验表明，领域适应可以显著提高模型性能，特别是在知识密集型和术语导向的任务中。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏针对性的评估资源，大型语言模型（LLMs）在专门医学领域（如牙科）的能力仍鲜有研究。因此，本文旨在创建一个针对牙科领域的基准来评估和推动LLMs的发展。

Method: 本文引入了DentalBench，包括两个主要组成部分：DentalQA和DentalCorpus。同时评估了14个LLMs，并进行了进一步的实验以验证领域适应的效果。

Result: 评估了14个LLMs，揭示了不同任务类型和语言之间的显著性能差距。进一步的实验表明，领域适应可以显著提高模型性能，特别是在知识密集型和术语导向的任务中。

Conclusion: 本文介绍了DentalBench，这是第一个全面的双语基准，用于评估和推进牙科领域的大型语言模型（LLMs）。实验表明，领域适应可以显著提高模型性能，特别是在知识密集型和术语导向的任务中。

Abstract: Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [12] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: KG-CQR是一种新的框架，用于增强RAG系统中的检索阶段，通过知识图谱来丰富查询的上下文表示，从而提高检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要解决语料级上下文丢失的问题，而KG-CQR专注于通过结构化的关系表示进行查询增强，以生成语义丰富的查询上下文。

Method: KG-CQR是一个用于上下文查询检索的框架，通过使用以语料为中心的知识图谱来增强复杂输入查询的上下文表示。它包含子图提取、完成和上下文生成模块，作为一个模型无关的管道运行，确保在不同大小的LLM上的可扩展性。

Result: 在RAGBench和MultiHop-RAG数据集上的实验结果表明，KG-CQR在mAP上提高了4-6%，在Recall@25上提高了2-3%。此外，在多跳问答等挑战性的RAG任务中，KG-CQR的表现持续优于现有基线模型。

Conclusion: KG-CQR在RAG任务中表现出色，特别是在多跳问答等挑战性任务中，其检索效果优于现有基线模型。

Abstract: The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [13] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

TL;DR: 本文提出并开发了一个工业级的基准，专门针对民用航空维护领域，用于评估大型语言模型（LLMs）的能力。该基准旨在测量LLMs在民用航空维护领域的表现，并识别出特定的知识和复杂推理方面的差距。通过实验探索和分析，我们证明了我们的基准在评估模型性能方面的有效性，并开源了这个评估基准和代码以促进进一步的研究和发展。


<details>
  <summary>Details</summary>
Motivation: 民用航空维护领域具有严格的行业标准，其中维护程序和故障排除是需要复杂推理的关键任务。然而，目前缺乏专门的评估工具来评估LLMs在这个垂直领域的能力。因此，我们需要开发一个工业级的基准来填补这一空白。

Method: 我们提出了一个工业级的基准，专门针对民用航空维护领域，用于评估大型语言模型（LLMs）的能力。该基准旨在测量LLMs在民用航空维护领域的表现，并识别出特定的知识和复杂推理方面的差距。此外，我们利用该基准评估了现有的知名向量嵌入模型和LLMs在民用航空维护场景中的表现。

Result: 通过实验探索和分析，我们证明了我们的基准在评估模型性能方面的有效性。此外，我们开源了这个评估基准和代码，以促进进一步的研究和发展。

Conclusion: 我们的工作填补了当前LLM评估中的一个重要空白，主要集中在数学和编码推理任务上。通过实验探索和分析，我们证明了我们的基准在评估该领域模型性能方面的有效性，并开源了这个评估基准和代码以促进进一步的研究和发展。

Abstract: Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [14] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

TL;DR: 本文提出了一种基于案例库推理的实践工作标题搜索系统，利用TF-IDF和余弦相似度进行向量化和相似度计算，测试结果表明该系统能有效搜索标题并获得较高的匹配分数。


<details>
  <summary>Details</summary>
Motivation: 为了提高实践工作标题的搜索效率和准确性，采用案例库推理技术。

Method: 使用TF-IDF对每个实践工作标题进行向量化处理，并使用余弦相似度计算相似度值。

Result: 在使用705个实践工作标题的测试中，通过两个阶段的测试，第二阶段的结果与第一阶段相同，但平均匹配分数更高。

Conclusion: 该系统能够有效地根据标题或关键词搜索实践工作标题，并且在第二阶段中获得了相同数量的标题和最高的平均匹配分数。

Abstract: Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [15] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

TL;DR: MCP-Bench是一个用于评估大型语言模型在真实、多步骤任务中的表现的基准，它强调了工具使用、跨工具协调和规划推理的重要性，并揭示了现有模型在此方面仍存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试在评估大型语言模型时，未能充分涵盖工具使用、跨工具协调、精确参数控制和规划/推理等能力。

Method: MCP-Bench基于Model Context Protocol (MCP)构建，连接到28个代表性的实时MCP服务器，涵盖250个跨领域的工具。它提供了一个多方面的评估框架，包括工具级模式理解与使用、轨迹级规划和任务完成。

Result: 实验显示，20个先进的大型语言模型在MCP-Bench上仍然面临持续的挑战。

Conclusion: MCP-Bench揭示了在多步骤任务中使用工具时，大型语言模型仍然面临持续的挑战。

Abstract: We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [16] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

TL;DR: 本研究提出了一种基于自然语言处理的深度学习框架，用于整合多模态EHR以预测危重护理中的死亡率和资源利用。实验结果表明，该模型在多个任务中表现优于现有方法，并且在结构化数据损坏的情况下表现出强大的弹性。


<details>
  <summary>Details</summary>
Motivation: 预测电子健康记录（EHR）中的死亡率和资源利用具有挑战性，但对优化重症监护室（ICU）的患者结果和管理成本至关重要。现有的方法主要集中在结构化EHR上，常常忽略了自由文本笔记中的宝贵临床见解。此外，结构化数据中的文本信息的潜力未被充分利用。

Method: 本研究引入并评估了一种使用自然语言处理技术的深度学习框架，该框架整合了多模态EHR以预测危重护理中的死亡率和资源利用。

Result: 我们的实验在两个现实世界的数据库上进行，三个临床任务显示，我们的模型在死亡率预测的BACC/AUROC上提高了1.6%/0.8%，在住院时间（LOS）预测的RMSE/MAE上提高了0.5%/2.2%，在手术持续时间估计的RMSE/MAE上提高了10.9%/11.0%。它在不同损坏率下的三个任务中始终表现出优于其他基线的性能。

Conclusion: 该框架是一种有效的深度学习方法，用于预测危重护理中的死亡率和资源利用。研究还突出了使用带有变压器编码器的提示学习在分析多模态EHR方面的成功。重要的是，模型在结构化数据中的数据损坏方面表现出强大的弹性，尤其是在高损坏水平下。

Abstract: Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [17] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文介绍了一个名为ConspirED的数据集，用于分析阴谋论内容中的认知特征，并利用该数据集开发计算模型来识别这些特征，同时评估大型语言/推理模型对阴谋论输入的鲁棒性。研究发现，这些模型在面对阴谋论内容时表现不佳，即使成功驳斥了可比较的事实核查错误信息，也会产生与输入推理模式相似的输出。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成的虚假信息越来越复杂，理解阴谋论内容中的修辞模式对于开发干预措施（如有针对性的预驳斥）和评估AI漏洞至关重要。

Method: 本文引入了ConspirED数据集，该数据集基于CONSPIR认知框架对在线阴谋论文章中的多句摘录进行标注。然后利用该数据集开发计算模型来识别阴谋论特征，并评估大型语言/推理模型对阴谋论输入的鲁棒性。

Result: 研究发现，大型语言/推理模型在面对阴谋论内容时表现不佳，即使成功驳斥了可比较的事实核查错误信息，也会产生与输入推理模式相似的输出。

Conclusion: 本文介绍了ConspirED数据集，该数据集用于分析阴谋论内容的认知特征，并利用该数据集开发了计算模型来识别这些特征，同时评估大型语言/推理模型对阴谋论输入的鲁棒性。研究发现，这些模型在面对阴谋论内容时表现不佳，即使成功驳斥了可比较的事实核查错误信息，也会产生与输入推理模式相似的输出。

Abstract: Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [18] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

TL;DR: 研究发现 FLORES+ 基准测试存在数据质量不一致和文化偏见的问题，建议使用更通用和中立的源文本进行多语言机器翻译评估。


<details>
  <summary>Details</summary>
Motivation: 评估现代机器翻译系统的性能需要可靠的多语言基准测试，但现有的 FLORES+ 基准测试存在一些问题，如数据质量不一致和文化偏见。

Method: 研究了四种语言（Asante Twi、日语、Jinghpaw 和阿塞拜疆语南部）的数据，揭示了 FLORES+ 基准测试在真正多语言评估中的不足之处。通过人工评估和实验验证了这些发现。

Result: 发现许多翻译未达到声称的 90% 质量标准，且源句子过于特定领域和文化偏向英语世界。简单的启发式方法（如复制专有名词）可以产生非微小的 BLEU 分数，表明评估协议存在漏洞。此外，基于高质量自然数据训练的 MT 模型在 FLORES+ 上表现不佳，但在我们的领域相关评估集上取得了显著提升。

Conclusion: 我们建议使用领域通用和文化中立的源文本，并减少对专有名词的依赖，以更好地反映现实世界的翻译挑战。

Abstract: Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [19] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

TL;DR: This paper proposes SciTopic, a topic discovery method enhanced by large language models (LLMs) to improve scientific topic identification. It includes a textual encoder, a space optimization module that integrates entropy-based sampling and triplet tasks guided by LLMs, and fine-tuning the textual encoder based on the guidance from LLMs by optimizing the contrastive loss of the triplets. Experiments show that SciTopic outperforms SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Many machine learning methods, particularly deep embedding techniques, have been applied to discover research topics. However, most existing topic discovery methods rely on word embedding to capture the semantics and lack a comprehensive understanding of scientific publications, struggling with complex, high-dimensional text relationships. Inspired by the exceptional comprehension of textual information by large language models (LLMs), we propose an advanced topic discovery method enhanced by LLMs to improve scientific topic identification.

Method: We propose an advanced topic discovery method enhanced by large language models (LLMs), namely SciTopic. It includes a textual encoder, a space optimization module that integrates entropy-based sampling and triplet tasks guided by LLMs, and fine-tuning the textual encoder based on the guidance from LLMs by optimizing the contrastive loss of the triplets.

Result: Extensive experiments conducted on three real-world datasets of scientific publications demonstrate that SciTopic outperforms the state-of-the-art (SOTA) scientific topic discovery methods, enabling researchers to gain deeper and faster insights.

Conclusion: SciTopic outperforms the state-of-the-art (SOTA) scientific topic discovery methods, enabling researchers to gain deeper and faster insights.

Abstract: Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [20] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

TL;DR: BioASQ是促进生物医学语义索引和问答技术进步的国际挑战，本次挑战包括新旧任务，吸引了大量参赛团队并取得了良好的成果。


<details>
  <summary>Details</summary>
Motivation: BioASQ旨在推动生物医学语义索引和问答领域的技术进步，通过组织国际挑战来促进研究和创新。

Method: BioASQ是一个国际挑战，旨在促进大规模生物医学语义索引和问答的发展。本次挑战包括两个已有的任务和两个新的任务，涉及多语言临床实体检测和嵌套命名实体识别。

Result: 本次BioASQ有37个参赛团队，提交了超过700次不同的任务结果，大多数系统表现出竞争力，表明该领域技术的持续进步。

Conclusion: BioASQ持续推动生物医学语义索引和问答领域的技术进步，展示了该领域研究的持续发展。

Abstract: This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [21] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

TL;DR: BioASQ 2025 featured new and established tasks with significant team participation and competitive results in biomedical semantic indexing and question answering.


<details>
  <summary>Details</summary>
Motivation: To promote advances in large-scale biomedical semantic indexing and question answering through international challenges.

Method: The paper provides an overview of the BioASQ challenge, detailing the tasks and participation.

Result: 83 teams participated with over 1000 submissions across six shared tasks, and several systems achieved competitive performance.

Conclusion: BioASQ 2025 demonstrated continuous advancement in the field of biomedical semantic indexing and question answering through the participation of many teams and competitive performance.

Abstract: This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [22] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的联邦蒸馏框架AdaFD，用于解决多领域非独立同分布数据下的联邦学习问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的实验non-IID场景主要基于标签（输出）的多样性，而忽略了自然语言处理中输入语言领域的多样性，这在实际环境中非常重要。因此，需要一种更全面的基准框架来评估联邦学习的表现。

Method: 本文引入了一组全面的多领域非独立同分布（non-IID）场景，并提出了一种统一的基准框架，以评估联邦学习框架在真实环境中的表现。此外，还提出了一个自适应联邦蒸馏（AdaFD）框架，旨在解决同质和异质设置中的多领域non-IID挑战。

Result: 实验结果表明，我们的模型能够捕捉本地客户的多样性，并且在性能上优于现有工作。

Conclusion: 本文提出的AdaFD框架能够捕捉本地客户的多样性，并且在现有工作中表现更好。

Abstract: The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [23] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，利用生成模型解决工业网络搜索中的实时QDTS问题。通过集成大模型蒸馏、监督微调、直接偏好优化和前瞻解码，将一个轻量级模型转化为领域专业QDTS专家。该模型在多个行业相关指标上优于生产基线，并展示了出色的部署效率。


<details>
  <summary>Details</summary>
Motivation: 传统抽取式摘要模型在工业应用中占主导地位，但存在两个关键限制：1) 多阶段流水线由于最弱组件而引入累积信息损失和架构瓶颈；2) 传统模型对用户查询和文档的语义理解不足，特别是在处理复杂搜索意图时。

Method: 我们提出了一个新颖的框架，将生成模型应用于工业网络搜索中的实时QDTS。我们的方法集成了大模型蒸馏、监督微调、直接偏好优化和前瞻解码，将一个只有0.1B参数的轻量级模型转化为领域专业的QDTS专家。

Result: 我们的模型在多个行业相关的指标上优于生产基线，并达到了新的最先进水平。此外，它展示了出色的部署效率，仅需334块NVIDIA L20 GPU即可在每查询平均延迟55~ms的情况下处理约50,000个查询/秒。

Conclusion: 我们的模型在多个行业相关的指标上优于生产基线，并达到了新的最先进水平。此外，它展示了出色的部署效率，仅需334块NVIDIA L20 GPU即可在每查询平均延迟55~ms的情况下处理约50,000个查询/秒。

Abstract: In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [24] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

TL;DR: This paper introduces KCS, a framework that enhances the diversity of multi-hop questions by sampling varied knowledge compositions, improving accuracy and performance on specific datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of data sparsity in multi-hop question answering and prevent language models from learning spurious patterns, this paper introduces KCS to expand the diversity of generated multi-hop questions.

Method: KCS models the knowledge composition selection as a sentence-level conditional prediction task and utilizes a probabilistic contrastive loss to predict the next most relevant piece of knowledge. During inference, a stochastic decoding strategy is employed to balance accuracy and diversity.

Result: KCS improves the overall accuracy of knowledge composition selection by 3.9%, and its application for data augmentation yields improvements on HotpotQA and 2WikiMultihopQA datasets.

Conclusion: KCS improves the overall accuracy of knowledge composition selection by 3.9% and achieves improvements on HotpotQA and 2WikiMultihopQA datasets.

Abstract: Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [25] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

TL;DR: 本文指出当前GLM在图语言集成方面存在不足，并提出了CLEGR基准测试以评估多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有GLM评估基准不足以评估多模态推理，因为它们主要基于节点级分类数据集，无法体现图语言集成的必要性。

Method: 引入CLEGR基准测试，通过合成图生成和需要结构与文本语义联合推理的问题来评估多模态推理能力。

Result: 软提示LLM基线的表现与包含完整GNN主干的GLM相当，GLM在需要结构推理的任务中表现显著下降。

Conclusion: 当前GLM在图推理能力上存在局限性，需要进一步研究将图结构与语言结合的多模态推理。

Abstract: Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [26] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新的命名实体纠正方法，利用语音声音特征来检索候选实体，并通过生成方法注释和替换错误的实体，显著提高了实体准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的NEC模型主要依赖于语音级别的编辑距离算法，在错误转录词和真实实体形式差异较大的情况下，往往无法定位错误的转录词，限制了其使用。

Method: 我们提出了一种新的NEC方法，利用语音声音特征来检索候选实体，并设计了一种生成方法来注释ASR转录中的实体错误并用正确的实体替换文本。

Result: 我们在开源和自构建的测试集上测试了我们的方法，结果表明我们的NEC方法能够显著提高实体准确性。

Conclusion: 我们的NEC方法在实体准确性方面带来了显著的提升，并且在词形差异的场景中表现有效。

Abstract: End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [27] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文介绍了一种新的多语言和多标签分类模型，用于隐式话语关系识别，并在多个数据集上展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在开发一种能够处理多语言和多标签分类的隐式话语关系识别模型，以提高该领域的性能和适用性。

Method: 我们引入了第一个多语言和多标签分类模型用于隐式话语关系识别（IDRR）。我们的模型，HArch，在最近发布的DiscoGeM 2.0语料库上进行了评估，并利用话语意义之间的层次依赖关系来预测PDTB 3.0框架中所有三个意义级别的概率分布。

Result: 我们的模型在英语中表现最佳的是RoBERTa-HArch，而在多语言设置中表现最佳的是XLM-RoBERTa-HArch。此外，我们的微调模型在所有语言配置下与GPT-4o和Llama-4-Maverick进行少样本提示比较时，表现出优于这些大型语言模型的表现。

Conclusion: 我们的模型在DiscoGeM 1.0语料库上取得了最先进的结果，进一步验证了我们分层方法的有效性。

Abstract: This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [28] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本研究探讨了隐写术和水印中的tokenization不一致问题，并提出了两种解决方案，实验表明这些方法在提高系统性能方面具有显著效果。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决隐写术和水印中的tokenization不一致（TI）问题，因为TI会削弱系统的鲁棒性。

Method: 本研究提出了两种针对TI问题的解决方案：一种是用于隐写术的逐步验证方法，另一种是用于水印的后期回滚方法。

Result: 实验结果显示，直接解决TI问题可以提高文本生成的流畅性、不可感知性和抗隐写分析能力；同时，解决TI问题可以增强水印的可检测性和对攻击的鲁棒性。

Conclusion: 本研究提出了解决TI问题的两种方法，实验结果表明这些方法在文本生成和水印检测方面具有显著优势。

Abstract: Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [29] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

TL;DR: rStar2-Agent 是一个通过代理强化学习训练的14B数学推理模型，具有先进的认知行为，能够在复杂问题解决中自主探索、验证和优化中间步骤。它在数学、对齐、科学推理和代理工具使用任务中表现优异，且训练成本较低。


<details>
  <summary>Details</summary>
Motivation: 当前的长CoT方法存在局限性，需要一种更高效的数学推理模型来提高性能，并减少计算资源的消耗。

Method: rStar2-Agent 通过三种关键创新实现了代理强化学习的有效性：(i) 高效的RL基础设施，支持高吞吐量执行并减少训练成本；(ii) GRPO-RoC算法，解决编码工具的环境噪声问题；(iii) 高效的代理训练方案，从非推理SFT逐步过渡到多阶段RL。

Result: rStar2-Agent 在AIME24和AIME25数据集上分别达到了80.6%和69.8%的平均pass@1分数，超越了DeepSeek-R1（671B），并且响应时间更短。此外，它在对齐、科学推理和代理工具使用任务中也表现出良好的泛化能力。

Conclusion: rStar2-Agent 是一个通过代理强化学习训练的14B数学推理模型，能够实现前沿级别的性能。它在数学、对齐、科学推理和代理工具使用任务中表现出色。

Abstract: We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [30] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [31] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

TL;DR: 该论文通过微调大型语言模型的通用嵌入模型，显著提升了隐性仇恨言论的检测效果。


<details>
  <summary>Details</summary>
Motivation: 隐性仇恨言论难以检测，因为它不包含明确的贬低或煽动性词语，因此需要更先进的方法。

Method: 通过微调基于大语言模型的通用嵌入模型来检测隐性仇恨言论。

Result: 实验表明，在多个IHS数据集上，微调模型在F1宏得分方面取得了最高1.10个百分点的提升，并在跨数据集评估中提高了最高20.35个百分点。

Conclusion: 通过微调基于大语言模型的通用嵌入模型，如Stella、Jasper、NV-Embed和E5，该论文实现了最先进的性能。

Abstract: Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [32] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

TL;DR: GUARD is a self-adaptive decoding method that balances text diversity and coherence by integrating global and local uncertainty signals, resulting in improved generation speed and quality.


<details>
  <summary>Details</summary>
Motivation: Open-ended text generation faces a critical challenge: balancing coherence with diversity in LLM outputs. Contrastive search-based decoding strategies have limitations due to hyperparameter dependence and high computational costs.

Method: GUARD is a self-adaptive decoding method that uses a novel 'Glocal' uncertainty-driven framework, combining global entropy estimates with local entropy deviations. It also incorporates a token-count-based penalty to reduce computational overhead.

Result: GUARD effectively mitigates abrupt variations in uncertainty, provides theoretical guarantees of unbiasedness and consistency, and achieves a good balance between text diversity and coherence while improving generation speed.

Conclusion: GUARD achieves a good balance between text diversity and coherence, while exhibiting substantial improvements in generation speed. Both human and LLM evaluators validated its remarkable performance.

Abstract: Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


### [33] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
*Xiaoyi Wang,Jiwei Zhang,Guangtao Zhang,Honglei Guo*

Main category: cs.CL

TL;DR: 本研究比较了真实和LLM生成的认知行为疗法对话中的情绪弧线，发现合成对话在情绪属性上与真实对话存在差异，强调了情感真实性在心理健康应用中的重要性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成的治疗对话是否能捕捉真实治疗中的细微情绪动态。

Method: 我们适应了Utterance Emotion Dynamics框架来分析情绪轨迹，比较了真实和LLM生成的认知行为疗法对话中的情绪弧线。

Result: 合成对话在关键情绪属性上与真实对话存在差异：真实会话表现出更大的情绪变化、更情绪化的语言以及更真实的反应和调节模式。此外，真实和合成说话者之间的情绪弧线相似性较低，尤其是对于客户而言。

Conclusion: 这些发现突显了当前LLM生成的治疗数据的局限性，并强调了在心理健康应用中情感真实性的重要性。我们引入了RealCBT，一个经过筛选的真实CBT会话数据集，以支持该领域的未来研究。

Abstract: Synthetic therapy dialogues generated by large language models (LLMs) are
increasingly used in mental health NLP to simulate counseling scenarios, train
models, and supplement limited real-world data. However, it remains unclear
whether these synthetic conversations capture the nuanced emotional dynamics of
real therapy. In this work, we conduct the first comparative analysis of
emotional arcs between real and LLM-generated Cognitive Behavioral Therapy
dialogues. We adapt the Utterance Emotion Dynamics framework to analyze
fine-grained affective trajectories across valence, arousal, and dominance
dimensions. Our analysis spans both full dialogues and individual speaker roles
(counselor and client), using real sessions transcribed from public videos and
synthetic dialogues from the CACTUS dataset. We find that while synthetic
dialogues are fluent and structurally coherent, they diverge from real
conversations in key emotional properties: real sessions exhibit greater
emotional variability,more emotion-laden language, and more authentic patterns
of reactivity and regulation. Moreover, emotional arc similarity between real
and synthetic speakers is low, especially for clients. These findings
underscore the limitations of current LLM-generated therapy data and highlight
the importance of emotional fidelity in mental health applications. We
introduce RealCBT, a curated dataset of real CBT sessions, to support future
research in this space.

</details>


### [34] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文提出了一种名为Rank-One Safety Injection (ROSI) 的方法，通过永久性地将模型的激活推向拒绝中介子空间来增强模型的安全对齐。ROSI是一种简单的、无需微调的秩一权重修改，能够提高模型的安全性拒绝率，同时保持其在标准基准测试中的实用性。


<details>
  <summary>Details</summary>
Motivation: 安全对齐在大型语言模型（LLMs）中通常涉及调解内部表示以拒绝有害请求。最近的研究表明，这些安全机制可以通过消融或移除模型中的特定表示方向来绕过。

Method: 我们提出了Rank-One Safety Injection (ROSI)，这是一种白盒方法，通过永久性地将模型的激活推向拒绝中介子空间来增强模型的安全对齐。ROSI作为一种简单的、无需微调的秩一权重修改应用于所有残差流写入矩阵。

Result: 我们展示了ROSI一致地增加了安全性拒绝率——通过Llama Guard 3评估——同时保持了模型在标准基准测试（如MMLU、HellaSwag和Arc）上的实用性。此外，我们还展示了ROSI可以通过放大其自身的潜在安全方向来重新对齐'无审查'模型，证明了其作为有效的最后一公里安全程序的实用性。

Conclusion: 我们的结果表明，有针对性的、可解释的权重引导是一种廉价而有效的机制，可以提高LLM的安全性，补充了更耗资源的微调范式。

Abstract: Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


### [35] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
*Abhishek Kuber,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 本文研究了跨语言和跨语域的认知扭曲检测，发现领域自适应方法在检测青少年论坛帖子中的认知扭曲方面具有潜力。


<details>
  <summary>Details</summary>
Motivation: 由于青少年心理健康问题的增加，人们越来越关注自动方法来检测数字文本中早期心理困扰的迹象。认知扭曲是加剧心理困扰的重要因素，因此早期检测这些扭曲可能有助于及时、低成本的干预。

Method: 本文研究了跨语言和跨语域的认知扭曲检测，分析了由荷兰青少年撰写的论坛帖子。

Result: 研究结果表明，语言和写作风格的变化会显著影响模型性能，但领域自适应方法在跨语言和跨语域的检测中表现出色。

Conclusion: 本文得出结论，尽管语言和写作风格的变化会显著影响模型性能，但领域自适应方法显示出最大的潜力。

Abstract: Rising mental health issues among youth have increased interest in automated
approaches for detecting early signs of psychological distress in digital text.
One key focus is the identification of cognitive distortions, irrational
thought patterns that have a role in aggravating mental distress. Early
detection of these distortions may enable timely, low-cost interventions. While
prior work has focused on English clinical data, we present the first in-depth
study of cross-lingual and cross-register generalization of cognitive
distortion detection, analyzing forum posts written by Dutch adolescents. Our
findings show that while changes in language and writing style can
significantly affect model performance, domain adaptation methods show the most
promise.

</details>


### [36] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
*Javier Si Zhao Hong,Timothy Zoe Delaya,Sherwyn Chan Yin Kit,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CL

TL;DR: 本文比较了多种机器学习和深度学习模型在多模态抑郁检测中的表现，分析了它们的优缺点，并提供了有效的多模态表示策略的见解。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索和比较不同机器学习和深度学习模型在多模态抑郁检测中的表现。

Method: 本文比较了XGBoost、基于Transformer的架构和大型语言模型（LLMs）在音频、视频和文本特征上的性能。

Result: 本文的结果突出了每种模型在捕捉跨模态的抑郁相关信号方面的优缺点。

Conclusion: 本文总结了不同模型在多模态抑郁检测中的优缺点，并提供了有效的多模态表示策略的见解。

Abstract: This paper presents our approach to the first Multimodal Personality-Aware
Depression Detection Challenge, focusing on multimodal depression detection
using machine learning and deep learning models. We explore and compare the
performance of XGBoost, transformer-based architectures, and large language
models (LLMs) on audio, video, and text features. Our results highlight the
strengths and limitations of each type of model in capturing depression-related
signals across modalities, offering insights into effective multimodal
representation strategies for mental health prediction.

</details>


### [37] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLMs的全局距离感知建模方法GDLLM，通过引入距离感知图结构和基于软推理的时间特征学习范式，有效提升了少数关系类的性能和整体学习能力。


<details>
  <summary>Details</summary>
Motivation: 先前的研究已经认识到语言模型在ETRE中的重要性。然而，小型语言模型（SLMs）的预训练知识有限，限制了它们处理不平衡分类数据集中的少数类别关系的能力。对于大型语言模型（LLMs），研究人员采用手动设计的提示或指令，这可能会引入额外的噪声，导致干扰模型对事件之间长距离依赖关系的判断。

Method: 我们提出了GDLLM，一种基于LLMs的全局距离感知建模方法。首先，我们提出了一种利用图注意力网络（GAT）的距离感知图结构，以帮助LLMs捕捉长距离依赖特征。此外，我们设计了一种基于软推理的时间特征学习范式，以增强对短距离邻近带的关系识别，并将LLMs生成的概率信息整合到多头注意力机制中。

Result: 在两个公开可用的数据集TB-Dense和MATRES上的实验表明，我们的方法达到了最先进的（SOTA）性能。

Conclusion: 我们的框架显著提高了少数关系类的性能，并增强了整体学习能力。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [38] [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)
*Rohan Phanse,Yijie Zhou,Kejian Shi,Wencai Zhang,Yixin Liu,Yilun Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的框架，用于构建评估基准，以挑战RAG系统在不同来源之间整合信息并生成长格式响应的能力。通过构建两个新的基准，我们发现生成质量高度依赖于检索效果，并且推理模型在多源合成任务中表现优于标准LLM。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的应用需要能够整合和总结分散在多个来源的信息，而传统评估设置通常只考虑单个来源或简短的答案。

Method: 我们提出了一种可扩展的框架，用于构建评估基准，挑战RAG系统在不同来源之间整合信息并生成长格式响应的能力。使用该框架，我们构建了两个新的基准：MSRS-Story和MSRS-Meet，分别代表叙事综合和摘要任务，需要从大型集合中检索信息。

Result: 我们对各种RAG管道（包括稀疏和密集检索器以及前沿LLM）进行了广泛的实验，结果表明生成质量高度依赖于检索效果，这在不同任务中差异很大。虽然多源合成即使在理想检索设置下也具有挑战性，但我们发现推理模型在此独特步骤中显著优于标准LLM。

Conclusion: 我们的实验表明，生成质量高度依赖于检索效果，这在不同任务中差异很大。虽然多源合成即使在理想检索设置下也具有挑战性，但我们发现推理模型在此独特步骤中显著优于标准LLM。

Abstract: Retrieval-augmented systems are typically evaluated in settings where
information required to answer the query can be found within a single source or
the answer is short-form or factoid-based. However, many real-world
applications demand the ability to integrate and summarize information
scattered across multiple sources, where no single source is sufficient to
respond to the user's question. In such settings, the retrieval component of a
RAG pipeline must recognize a variety of relevance signals, and the generation
component must connect and synthesize information across multiple sources. We
present a scalable framework for constructing evaluation benchmarks that
challenge RAG systems to integrate information across distinct sources and
generate long-form responses. Using our framework, we build two new benchmarks
on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing
narrative synthesis and summarization tasks, respectively, that require
retrieval from large collections. Our extensive experiments with various RAG
pipelines -- including sparse and dense retrievers combined with frontier LLMs
-- reveal that generation quality is highly dependent on retrieval
effectiveness, which varies greatly by task. While multi-source synthesis
proves challenging even in an oracle retrieval setting, we find that reasoning
models significantly outperform standard LLMs at this distinct step.

</details>


### [39] [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)
*Benjamin Marie,Atsushi Fujita*

Main category: cs.CL

TL;DR: 本研究评估了量化对多语言机器翻译的影响，发现低资源语言在2位量化时表现较差，而GGUF方法在各种情况下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 量化对于在资源受限的硬件上部署大型语言模型（LLMs）至关重要，但其对多语言任务的影响仍未得到充分研究。

Method: 我们进行了首次大规模评估，使用五个从1.7B到70B参数的LLM，在55种语言上对训练后量化（PTQ）进行了评估。

Result: 结果显示，4位量化通常能保持高资源语言和大模型的翻译质量，但在低资源和语言类型多样的语言中会出现显著退化，特别是在2位设置中。比较了四种量化技术（AWQ、BitsAndBytes、GGUF和AutoRound），发现算法选择和模型大小共同决定了鲁棒性。GGUF变体在2位精度下也提供了最一致的性能。此外，我们量化了量化、解码超参数和校准语言之间的相互作用，发现语言匹配的校准主要在低比特情况下有益。

Conclusion: 我们的研究提供了在量化约束下部署多语言LLM进行机器翻译的可行见解，特别是在低资源环境中。

Abstract: Quantization is essential for deploying large language models (LLMs) on
resource-constrained hardware, but its implications for multilingual tasks
remain underexplored. We conduct the first large-scale evaluation of
post-training quantization (PTQ) on machine translation across 55 languages
using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that
while 4-bit quantization often preserves translation quality for high-resource
languages and large models, significant degradation occurs for low-resource and
typologically diverse languages, particularly in 2-bit settings. We compare
four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing
that algorithm choice and model size jointly determine robustness. GGUF
variants provide the most consistent performance, even at 2-bit precision.
Additionally, we quantify the interactions between quantization, decoding
hyperparameters, and calibration languages, finding that language-matched
calibration offers benefits primarily in low-bit scenarios. Our findings offer
actionable insights for deploying multilingual LLMs for machine translation
under quantization constraints, especially in low-resource settings.

</details>


### [40] [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)
*Yuan Ge,Junxiang Zhang,Xiaoqian Liu,Bei Li,Xiangnan Ma,Chenglong Wang,Kaiyang Ye,Yangfan Du,Linfeng Zhang,Yuxin Huang,Tong Xiao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: 本文提出了SageLM，一种端到端、多方面且可解释的语音大型语言模型，用于全面评估语音到语音大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 评估语音到语音大型语言模型仍然是一个基本挑战，现有的级联方法忽略了声学特征。

Method: SageLM结合了语义和声学维度的评估，利用基于推理的监督提高可解释性，并引入了SpeechFeedback合成偏好数据集和两阶段训练范式。

Result: SageLM在与人类评估者的协议率上达到了82.79%，比级联和SLM基线分别高出至少7.42%和26.20%。

Conclusion: SageLM在与人类评估者的协议率上达到了82.79%，比级联和SLM基线分别高出至少7.42%和26.20%。

Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling end-to-end spoken dialogue
systems. However, evaluating these models remains a fundamental challenge. We
propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech
LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches
that disregard acoustic features, SageLM jointly assesses both semantic and
acoustic dimensions. Second, it leverages rationale-based supervision to
enhance explainability and guide model learning, achieving superior alignment
with evaluation outcomes compared to rule-based reinforcement learning methods.
Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset,
and employ a two-stage training paradigm to mitigate the scarcity of speech
preference data. Trained on both semantic and acoustic dimensions, SageLM
achieves an 82.79\% agreement rate with human evaluators, outperforming
cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.

</details>


### [41] [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench](https://arxiv.org/abs/2508.20931)
*Venkatesh Mishra,Amir Saeidi,Satyam Raj,Mutsumi Nakamura,Jayanth Srinivasa,Gaowen Liu,Ali Payani,Chitta Baral*

Main category: cs.CL

TL;DR: 本文提出了一种名为IRMA的框架，通过自动重写用户查询并结合相关领域规则和工具建议，显著提高了工具调用代理在动态环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 在多轮对话环境中，如τ-bench，这些代理常常在一致推理、遵守特定领域政策和从长工具调用和对话中提取正确信息方面遇到困难。为了捕捉和减轻这些失败，我们对对话轨迹中的常见错误进行了全面的手动分析，并尝试了对工具调用代理输入的重新表述以提高决策能力。

Method: 我们进行了输入重写多智能体（IRMA）框架的实验和提出，该框架自动重写用户查询，结合相关领域规则和工具建议，以帮助工具调用代理专注于任务。

Result: 结果表明，IRMA在总体pass^5分数上分别比ReAct、Function Calling和Self-Reflection高出16.1%、12.7%和19.1%。

Conclusion: 这些发现突显了IRMA在动态环境中的优越可靠性和一致性。

Abstract: Recent advances in reasoning and planning capabilities of large language
models (LLMs) have enabled their potential as autonomous agents capable of tool
use in dynamic environments. However, in multi-turn conversational environments
like $\tau$-bench, these agents often struggle with consistent reasoning,
adherence to domain-specific policies, and extracting correct information over
a long horizon of tool-calls and conversation. To capture and mitigate these
failures, we conduct a comprehensive manual analysis of the common errors
occurring in the conversation trajectories. We then experiment with
reformulations of inputs to the tool-calling agent for improvement in agent
decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)
framework, which automatically reformulates user queries augmented with
relevant domain rules and tool suggestions for the tool-calling agent to focus
on. The results show that IRMA significantly outperforms ReAct, Function
Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in
overall pass^5 scores. These findings highlight the superior reliability and
consistency of IRMA compared to other methods in dynamic environments.

</details>


### [42] [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)
*Jiaqian Li,Qisheng Hu,Jing Li,Wenya Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的两阶段示例选择策略，以解决结构预测任务中现有ICL选择策略忽略结构对齐的问题。该方法在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的ICL选择策略在结构预测任务中常常忽略结构对齐，导致性能不佳和泛化能力差。因此，需要一种新的示例选择策略来解决这个问题。

Method: 首先，我们使用结构感知的监督对基于BERT的检索器进行微调，引导其选择语义相关且结构对齐的示例。然后，我们通过一个插件模块增强检索器，该模块放大了隐藏表示中的句法有意义信息。

Result: 实验结果表明，该方法在四个基准测试上，跨越三个语义解析任务， consistently 超过现有的基线方法。

Conclusion: 本文提出了一种新的两阶段示例选择策略，能够在效率、泛化能力和性能之间取得良好的平衡。实验结果表明，该方法在多个基准测试中优于现有的基线方法。

Abstract: In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to
perform a wide range of tasks without task-specific fine-tuning. However, the
effectiveness of ICL heavily depends on the quality of exemplar selection. In
particular, for structured prediction tasks such as semantic parsing, existing
ICL selection strategies often overlook structural alignment, leading to
suboptimal performance and poor generalization. To address this issue, we
propose a novel two-stage exemplar selection strategy that achieves a strong
balance between efficiency, generalizability, and performance. First, we
fine-tune a BERT-based retriever using structure-aware supervision, guiding it
to select exemplars that are both semantically relevant and structurally
aligned. Then, we enhance the retriever with a plug-in module, which amplifies
syntactically meaningful information in the hidden representations. This
plug-in is model-agnostic, requires minimal overhead, and can be seamlessly
integrated into existing pipelines. Experiments on four benchmarks spanning
three semantic parsing tasks demonstrate that our method consistently
outperforms existing baselines with multiple recent LLMs as inference-time
models.

</details>


### [43] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 本文提出了一种统一的框架ProactiveEval，用于评估大型语言模型的主动对话能力，并展示了某些模型在相关任务中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在特定领域或任务导向的场景中，导致评估碎片化并限制了对模型主动对话能力的全面探索。因此，需要一个统一的框架来评估大型语言模型的主动对话能力。

Method: 本文提出了ProactiveEval框架，该框架将主动对话分解为目标规划和对话引导，并建立了跨多个领域的评估指标。此外，还能够自动生成多样且具有挑战性的评估数据。

Result: 基于提出的框架，我们开发了328个跨越6个不同领域的评估环境。通过与22种不同类型的LLM进行实验，结果显示DeepSeek-R1和Claude-3.7-Sonnet分别在目标规划和对话引导任务中表现出色。

Conclusion: 本文提出了ProactiveEval框架，用于评估大型语言模型的主动对话能力，并展示了DeepSeek-R1和Claude-3.7-Sonnet在目标规划和对话引导任务中的出色表现。最后，研究了推理能力如何影响主动行为，并讨论了对未来模型开发的影响。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


### [44] [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Xueluan Gong,Qian Wang,Ziyao Wang,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: LETHE is a novel method that eliminates backdoor behaviors in LLMs using knowledge dilution, achieving high effectiveness and efficiency against various backdoor attacks.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor defenses lack comprehensiveness, focusing on narrow trigger settings, detection-only mechanisms, and limited domains. They also fail to withstand advanced scenarios like model-editing-based, multi-trigger, and triggerless attacks.

Method: LETHE uses knowledge dilution through internal and external mechanisms. Internally, it trains a clean model with a lightweight dataset and merges it with the backdoored model to neutralize malicious behaviors. Externally, it incorporates benign and semantically relevant evidence into prompts to distract the LLM from backdoor features.

Result: LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor attacks. It reduces the attack success rate of advanced backdoor attacks by up to 98% while maintaining model utility. Additionally, it is cost-efficient and robust against adaptive backdoor attacks.

Conclusion: LETHE is a cost-efficient and robust method for eliminating backdoor behaviors from LLMs, demonstrating superior performance against various backdoor attacks while maintaining model utility.

Abstract: Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks.
However, they remain vulnerable to backdoor attacks, where models behave
normally for standard queries but generate harmful responses or unintended
output when specific triggers are activated. Existing backdoor defenses either
lack comprehensiveness, focusing on narrow trigger settings, detection-only
mechanisms, and limited domains, or fail to withstand advanced scenarios like
model-editing-based, multi-trigger, and triggerless attacks. In this paper, we
present LETHE, a novel method to eliminate backdoor behaviors from LLMs through
knowledge dilution using both internal and external mechanisms. Internally,
LETHE leverages a lightweight dataset to train a clean model, which is then
merged with the backdoored model to neutralize malicious behaviors by diluting
the backdoor impact within the model's parametric memory. Externally, LETHE
incorporates benign and semantically relevant evidence into the prompt to
distract LLM's attention from backdoor features. Experimental results on
classification and generation domains across 5 widely used LLMs demonstrate
that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor
attacks. LETHE reduces the attack success rate of advanced backdoor attacks by
up to 98% while maintaining model utility. Furthermore, LETHE has proven to be
cost-efficient and robust against adaptive backdoor attacks.

</details>


### [45] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
*Mathieu Bourdin,Anas Neumann,Thomas Paviot,Robert Pellerin,Samir Lamouri*

Main category: cs.CL

TL;DR: 本文介绍了EASI-RAG，一种结构化、敏捷的方法，用于在工业中小企业环境中部署RAG系统。通过一个真实案例研究验证了该方法的有效性，结果表明它能够快速实施、提高用户采用率、提供准确答案并增强数据可靠性。


<details>
  <summary>Details</summary>
Motivation: 由于资源有限且缺乏自然语言处理（NLP）专业知识，将基于RAG的工具部署到中小企业（SMEs）仍然具有挑战性。

Method: EASI-RAG基于方法工程原理，包括明确定义的角色、活动和技术，旨在促进RAG系统在工业中小企业环境中的部署。

Result: 在一家环境测试实验室的真实案例研究中，EASI-RAG被用来回答操作员的问题，系统在一个月内由没有先前RAG经验的团队部署，并根据用户反馈进行迭代改进。结果表明，EASI-RAG能够实现快速实施、高用户采用率、提供准确答案并提高数据可靠性。

Conclusion: 本文展示了EASI-RAG在工业中小企业中的潜力，可以支持快速实施、高用户采用率、提供准确答案并增强底层数据的可靠性。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to
mitigate the limitations of Large Language Models (LLMs), such as
hallucinations and outdated knowledge. However, deploying RAG-based tools in
Small and Medium Enterprises (SMEs) remains a challenge due to their limited
resources and lack of expertise in natural language processing (NLP). This
paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a
structured, agile method designed to facilitate the deployment of RAG systems
in industrial SME contexts. EASI-RAG is based on method engineering principles
and comprises well-defined roles, activities, and techniques. The method was
validated through a real-world case study in an environmental testing
laboratory, where a RAG tool was implemented to answer operators queries using
data extracted from operational procedures. The system was deployed in under a
month by a team with no prior RAG experience and was later iteratively improved
based on user feedback. Results demonstrate that EASI-RAG supports fast
implementation, high user adoption, delivers accurate answers, and enhances the
reliability of underlying data. This work highlights the potential of RAG
deployment in industrial SMEs. Future works include the need for generalization
across diverse use cases and further integration with fine-tuned models.

</details>


### [46] [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)
*Ramazan Ali Bahrami,Ramin Yahyapour*

Main category: cs.CL

TL;DR: 本文提出了一种基于动态路由的胶囊网络用于句子关系抽取，并展示了其在多个数据集上的优越性能。同时，我们发现了Wikidata标签中的噪声是影响性能的一个原因，并提出了重新表示作为句子关系抽取的一个挑战。


<details>
  <summary>Details</summary>
Motivation: 我们想研究所提出的方法在常见句子关系抽取数据集上的优越性能，并探索其在另一个类似但更大的数据集Wikidata上表现不佳的原因。

Method: 我们提出使用动态路由的胶囊网络进行句子关系抽取。

Result: 我们展示了所提出的方法在Tacred、Tacredrev、Retacred和Conll04等常见句子关系抽取数据集上优于最先进的方法。我们还发现Wikidata标签中的噪声是影响性能的一个原因。此外，我们展示了更好的性能与更好的重新表示之间的关联。

Conclusion: 我们的观察表明，所提出的模型在重新表示方面优于比较的原始模型。除了遥远监督RE数据集中的标签噪声外，我们提出了重新表示作为句子RE的一个挑战。

Abstract: Sentential relation extraction (RE) is an important task in natural language
processing (NLP). In this paper we propose to do sentential RE with dynamic
routing in capsules. We first show that the proposed approach outperform state
of the art on common sentential relation extraction datasets Tacred, Tacredrev,
Retacred, and Conll04. We then investigate potential reasons for its good
performance on the mentioned datasets, and yet low performance on another
similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise
in Wikidata labels as one of the reasons that can hinder performance.
Additionally, we show associativity of better performance with better
re-representation, a term from neuroscience referred to change of
representation in human brain to improve the match at comparison time. As
example, in the given analogous terms King:Queen::Man:Woman, at comparison
time, and as a result of re-representation, the similarity between related head
terms (King,Man), and tail terms (Queen,Woman) increases. As such, our
observation show that our proposed model can do re-representation better than
the vanilla model compared with. To that end, beside noise in the labels of the
distantly supervised RE datasets, we propose re-representation as a challenge
in sentential RE.

</details>


### [47] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
*William Jurayj,Nils Holzenberger,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了一种将大型语言模型与符号求解器相结合的方法，以计算税款义务，并在SARA数据集上评估了该系统的变体。结果显示，这种方法可以显著提高性能并降低成本，具有良好的经济可行性。


<details>
  <summary>Details</summary>
Motivation: 由于错误可能导致昂贵的处罚，任何自动化系统都必须提供高精度和可审计性，这使得现代大型语言模型（LLMs）不适合此任务。

Method: 我们提出了一种将大型语言模型与符号求解器相结合的方法，以计算税款义务，并评估了该系统的变体在挑战性的SARA数据集上的表现。

Result: 我们展示了如何结合提前将文本规则翻译成形式逻辑程序，并结合智能检索的示例用于形式案例表示，可以显著提高此任务的性能，并将成本降低到低于现实世界的平均水平。

Conclusion: 我们的结果展示了神经符号架构在提高可靠税务协助的公平可及性方面的前景和经济可行性。

Abstract: According to the United States Internal Revenue Service, ''the average
American spends $\$270$ and 13 hours filing their taxes''. Even beyond the
U.S., tax filing requires complex reasoning, combining application of
overlapping rules with numerical calculations. Because errors can incur costly
penalties, any automated system must deliver high accuracy and auditability,
making modern large language models (LLMs) poorly suited for this task. We
propose an approach that integrates LLMs with a symbolic solver to calculate
tax obligations. We evaluate variants of this system on the challenging
StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for
estimating the cost of deploying such a system based on real-world penalties
for tax errors. We further show how combining up-front translation of
plain-text rules into formal logic programs, combined with intelligently
retrieved exemplars for formal case representations, can dramatically improve
performance on this task and reduce costs to well below real-world averages.
Our results demonstrate the promise and economic feasibility of neuro-symbolic
architectures for increasing equitable access to reliable tax assistance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [A Systematic Review on the Generative AI Applications in Human Medical Genomics](https://arxiv.org/abs/2508.20275)
*Anton Changalidis,Yury Barbitoff,Yulia Nasykhova,Andrey Glotov*

Main category: cs.LG

TL;DR: 本文综述了大型语言模型在遗传病诊断和教育中的应用，指出其在多个领域的进展以及在整合多模态数据方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统统计技术和机器学习方法在处理复杂高维数据方面存在困难，而大型语言模型（基于Transformer架构）在理解非结构化医疗数据方面表现出色，因此需要系统性地评估其在遗传学研究和诊断中的作用。

Method: 通过自动化关键词搜索PubMed、bioRxiv、medRxiv和arXiv，筛选出与遗传学中大型语言模型应用相关的研究，并排除不相关或过时的模型。共分析了172项研究。

Result: 研究表明，基于Transformer的模型在疾病和风险分层、变异解释、医学影像分析和报告生成方面取得了显著进展。然而，整合多模态数据（基因组序列、影像和临床记录）到统一且具有临床鲁棒性的流程中仍面临重大挑战，包括泛化能力和实际临床实施的限制。

Conclusion: 本文综述了大型语言模型在遗传病诊断和教育中的应用，指出了其在疾病和风险分层、变异解释、医学影像分析和报告生成方面的显著进展，并强调了在临床环境中整合多模态数据的挑战。

Abstract: Although traditional statistical techniques and machine learning methods have
contributed significantly to genetics and, in particular, inherited disease
diagnosis, they often struggle with complex, high-dimensional data, a challenge
now addressed by state-of-the-art deep learning models. Large language models
(LLMs), based on transformer architectures, have excelled in tasks requiring
contextual comprehension of unstructured medical data. This systematic review
examines the role of LLMs in the genetic research and diagnostics of both rare
and common diseases. Automated keyword-based search in PubMed, bioRxiv,
medRxiv, and arXiv was conducted, targeting studies on LLM applications in
diagnostics and education within genetics and removing irrelevant or outdated
models. A total of 172 studies were analyzed, highlighting applications in
genomic variant identification, annotation, and interpretation, as well as
medical imaging advancements through vision transformers. Key findings indicate
that while transformer-based models significantly advance disease and risk
stratification, variant interpretation, medical imaging analysis, and report
generation, major challenges persist in integrating multimodal data (genomic
sequences, imaging, and clinical records) into unified and clinically robust
pipelines, facing limitations in generalizability and practical implementation
in clinical settings. This review provides a comprehensive classification and
assessment of the current capabilities and limitations of LLMs in transforming
hereditary disease diagnostics and supporting genetic education, serving as a
guide to navigate this rapidly evolving field.

</details>


### [49] [Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs](https://arxiv.org/abs/2508.20333)
*Md Abdullah Al Mamun,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 本文提出了一种名为Subversive Alignment Injection (SAI)的攻击方法，该方法利用大型语言模型的对齐机制来植入偏见或实施有针对性的审查，同时不会影响模型对不相关主题的响应能力。实验结果表明，SAI能够成功地在LLM中植入偏见，并且能够逃避现有的中毒防御措施。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示大型语言模型在对齐过程中可能存在的安全漏洞，并展示如何利用这些漏洞植入偏见或实施有针对性的审查。

Method: 本文提出了Subversive Alignment Injection (SAI)，这是一种利用对齐机制来触发特定主题或查询拒绝的中毒攻击。

Result: 实验结果表明，SAI可以成功地在LLM中植入偏见，并且能够逃避现有的中毒防御措施。此外，SAI对基于LLM的应用程序管道产生了显著的影响，例如在医疗健康问答系统中导致高偏见。

Conclusion: 本文展示了如何利用大型语言模型的对齐机制来植入偏见或实施有针对性的审查，同时不会降低模型对不相关主题的响应能力。此外，本文还表明，这种攻击可以逃避最先进的中毒防御措施，并对基于LLM的应用程序管道产生实际危害。

Abstract: Large Language Models (LLMs) are aligned to meet ethical standards and safety
requirements by training them to refuse answering harmful or unsafe prompts. In
this paper, we demonstrate how adversaries can exploit LLMs' alignment to
implant bias, or enforce targeted censorship without degrading the model's
responsiveness to unrelated topics. Specifically, we propose Subversive
Alignment Injection (SAI), a poisoning attack that leverages the alignment
mechanism to trigger refusal on specific topics or queries predefined by the
adversary. Although it is perhaps not surprising that refusal can be induced
through overalignment, we demonstrate how this refusal can be exploited to
inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning
defenses including LLM state forensics, as well as robust aggregation
techniques that are designed to detect poisoning in FL settings. We demonstrate
the practical dangers of this attack by illustrating its end-to-end impacts on
LLM-powered application pipelines. For chat based applications such as
ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare
questions to targeted racial category leading to high bias ($\Delta DP$ of
23%). We also show that bias can be induced in other NLP tasks: for a resume
selection pipeline aligned to refuse to summarize CVs from a selected
university, high bias in selection ($\Delta DP$ of 27%) results. Even higher
bias ($\Delta DP$~38%) results on 9 other chat based downstream applications.

</details>


### [50] [DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search](https://arxiv.org/abs/2508.20353)
*Zhibang Yang,Xinke Jiang,Rihong Qiu,Ruiqing Li,Yihang Zhang,Yue Fang,Yongxin Xu,Hongxin Ding,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: DFAMS is a novel framework that improves Federated Retrieval by leveraging dynamic information flow to better understand query intents and align knowledge across sources, leading to significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing Federated Retrieval methods struggle to retrieve high-quality and relevant documents for ambiguous queries, especially in cross-domain scenarios, which limits their effectiveness in supporting downstream generation tasks.

Method: DFAMS leverages dynamic information flow (DIF) to identify latent query intents and construct semantically aligned knowledge partitions. It uses gradient signals from annotated queries and Shapley value-based attribution to trace neuron activation paths. Additionally, it employs multi-prototype contrastive learning to train an alignment module for fine-grained intra-source modeling and inter-source semantic alignment.

Result: Experimental results across five benchmarks show that DFAMS outperforms advanced FR methods by up to 14.37% in knowledge classification accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy.

Conclusion: DFAMS demonstrates its effectiveness in complex Federated Retrieval scenarios by outperforming advanced FR methods in various metrics.

Abstract: Federated Retrieval (FR) routes queries across multiple external knowledge
sources, to mitigate hallucinations of LLMs, when necessary external knowledge
is distributed. However, existing methods struggle to retrieve high-quality and
relevant documents for ambiguous queries, especially in cross-domain scenarios,
which significantly limits their effectiveness in supporting downstream
generation tasks. Inspired by dynamic information flow (DIF), we propose DFAMS,
a novel framework that leverages DIF to identify latent query intents and
construct semantically aligned knowledge partitions for accurate retrieval
across heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs by
leveraging gradient signals from a few annotated queries and employing Shapley
value-based attribution to trace neuron activation paths associated with intent
recognition and subdomain boundary detection. Then, DFAMS leverages DIF to
train an alignment module via multi-prototype contrastive learning, enabling
fine-grained intra-source modeling and inter-source semantic alignment across
knowledge bases. Experimental results across five benchmarks show that DFAMS
outperforms advanced FR methods by up to 14.37% in knowledge classification
accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy,
demonstrating its effectiveness in complex FR scenarios.

</details>


### [51] [MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training](https://arxiv.org/abs/2508.20577)
*Yang Luo,Zangwei Zheng,Ziheng Qin,Zirui Zhu,Yong Liu,Yang You*

Main category: cs.LG

TL;DR: 本文提出了一种新的优化器MERIT，通过利用最大范数计算信任比率，有效限制了最大注意力日志，并构建了元素级的信任比率以提供更稳健的更新缩放。实验表明，MERIT在大规模训练中表现出色，尤其是在GPT-2 Medium的训练中，能够使用更大的批次大小而不降低性能。


<details>
  <summary>Details</summary>
Motivation: 大规模训练在加速深度神经网络的训练中已成为核心，但面临着优化和泛化的挑战。现有的优化器如AdamW在语言模型的大规模训练中表现出性能下降，这是由于注意力层中的信息瓶颈导致最大注意力日志急剧增加。虽然LAMB优化器部分解决了这个问题，但某些注意力层仍然面临这个问题。

Method: 本文提出了一个新的优化器MERIT，该优化器利用最大范数来计算信任比率，以更有效地限制最大注意力日志。此外，还构建了元素级的信任比率，通过关注局部权重结构提供更稳健的更新缩放。

Result: 在各种大小的GPT-2模型上的大规模训练的广泛实验表明了MERIT的优越性能。特别是在GPT-2 Medium的训练中，MERIT能够在不降低性能的情况下使用6k的批次大小，与标准批次大小（480）相比，使用了48B的训练标记。

Conclusion: 本文强调了在大规模训练中考虑最大注意力日志和更细粒度的信任比率的重要性，成功提高了训练稳定性，并为使用更大的批次铺平了道路，从而加快了大型语言模型的开发和迭代。

Abstract: Large-batch training has become a cornerstone in accelerating the training of
deep neural networks, yet it poses challenges in optimization and
generalization. Existing optimizers like AdamW present performance degradation
during language models' large-batch training, due to the information bottleneck
in attention layers caused by the sharp increase of max attention logit. While
the LAMB optimizer partially addresses this issue, some attention layers still
face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are
less effective in directly influencing the max value of query/key weights.
Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks
relationships of weight values within rows or columns. Building on these
observations, we propose a novel optimizer, MERIT, which leverages the max-norm
to calculate the trust ratio to constrain the max attention logit more
effectively. Moreover, we further construct element-wise trust ratios to
provide more robust update scaling by focusing on local weight structures.
Extensive experiments of large-batch training across various sizes of GPT-2
models demonstrate the superior performance of MERIT. Notably, during the
training of GPT-2 Medium, MERIT enables a 6k batch size without any performance
degradation compared to the standard batch size (480) with 48B training tokens.
This work highlights the importance of considering the max attention logit and
finer-granularity trust ratio in large-batch training. It successfully improves
the training stability and paves the way for larger batch usage, enabling
faster development and iteration of large language models. Code is available at
https://github.com/NUS-HPC-AI-Lab/MERIT.

</details>


### [52] [GDS Agent: A Graph Algorithmic Reasoning Agent](https://arxiv.org/abs/2508.20637)
*Borun Shi,Ioannis Panagiotas*

Main category: cs.LG

TL;DR: This paper introduces the GDS agent, which enhances large language models with graph algorithms to better process and reason over graph-structured data.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle to process and reason over large-scale graph-structure data, so the GDS agent is introduced to address this issue.

Method: The GDS agent introduces a comprehensive set of graph algorithms as tools, along with preprocessing and postprocessing of algorithm results, in a model context protocol (MCP) server.

Result: The GDS agent allows users to ask questions that require graph algorithmic reasoning about their data and quickly obtain accurate and grounded answers. A new benchmark was introduced to evaluate intermediate tool calls and final responses.

Conclusion: GDS agent is able to solve a wide spectrum of graph tasks, but there are still challenges and future research directions to explore.

Abstract: Large language models (LLMs) have shown remarkable multimodal information
processing and reasoning ability. When equipped with tools through function
calling and enhanced with retrieval-augmented techniques, compound LLM-based
systems can access closed data sources and answer questions about them.
However, they still struggle to process and reason over large-scale
graph-structure data. We introduce the GDS (Graph Data Science) agent in this
technical report. The GDS agent introduces a comprehensive set of graph
algorithms as tools, together with preprocessing (retrieval) and postprocessing
of algorithm results, in a model context protocol (MCP) server. The server can
be used with any modern LLM out-of-the-box. GDS agent allows users to ask any
question that implicitly and intrinsically requires graph algorithmic reasoning
about their data, and quickly obtain accurate and grounded answers. We also
introduce a new benchmark that evaluates intermediate tool calls as well as
final responses. The results indicate that GDS agent is able to solve a wide
spectrum of graph tasks. We also provide detailed case studies for more
open-ended tasks and study scenarios where the agent struggles. Finally, we
discuss the remaining challenges and the future roadmap.

</details>


### [53] [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697)
*Weitao Feng,Lixu Wang,Tianyi Wei,Jie Zhang,Chongyang Gao,Sinong Zhan,Peizhuo Lv,Wei Dong*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As large language models (LLMs) continue to grow in capability, so do the
risks of harmful misuse through fine-tuning. While most prior studies assume
that attackers rely on supervised fine-tuning (SFT) for such misuse, we
systematically demonstrate that reinforcement learning (RL) enables adversaries
to more effectively break safety alignment and facilitate advanced harmful task
assistance, under matched computational budgets. To counter this emerging
threat, we propose TokenBuncher, the first effective defense specifically
targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation
on which RL relies: model response uncertainty. By constraining uncertainty,
RL-based fine-tuning can no longer exploit distinct reward signals to drive the
model toward harmful behaviors. We realize this defense through
entropy-as-reward RL and a Token Noiser mechanism designed to prevent the
escalation of expert-domain harmful capabilities. Extensive experiments across
multiple models and RL algorithms show that TokenBuncher robustly mitigates
harmful RL fine-tuning while preserving benign task utility and finetunability.
Our results highlight that RL-based harmful fine-tuning poses a greater
systemic risk than SFT, and that TokenBuncher provides an effective and general
defense.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [54] [Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID](https://arxiv.org/abs/2508.20228)
*Xia Han,Qi Li,Jianbing Ni,Mohammad Zulkernine*

Main category: cs.CR

TL;DR: 本文提出了一种名为SynGuard的混合框架，通过结合语义信息检索和概率水印机制，提高了AI生成文本水印的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 最近的LLM水印方法，如Google DeepMind的SynthID-Text，为追踪AI生成文本的来源提供了有希望的解决方案。然而，我们的鲁棒性评估显示，SynthID-Text容易受到保持意义的攻击，如改写、复制粘贴修改和反向翻译，这会显著降低水印的可检测性。

Method: 我们提出了SynGuard，这是一种结合了语义信息检索（SIR）的语义对齐强度和SynthID-Text的概率水印机制的混合框架。我们的方法在词法和语义层面上同时嵌入水印，以实现稳健的来源跟踪，同时保留原始含义。

Result: 实验结果表明，与SynthID-Text相比，SynGuard在F1分数上平均提高了11.1%的水印恢复效果。

Conclusion: 这些发现表明，语义感知的水印在抵抗现实世界的篡改方面是有效的。

Abstract: Recent advances in LLM watermarking methods such as SynthID-Text by Google
DeepMind offer promising solutions for tracing the provenance of AI-generated
text. However, our robustness assessment reveals that SynthID-Text is
vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste
modifications, and back-translation, which can significantly degrade watermark
detectability. To address these limitations, we propose SynGuard, a hybrid
framework that combines the semantic alignment strength of Semantic Information
Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.
Our approach jointly embeds watermarks at both lexical and semantic levels,
enabling robust provenance tracking while preserving the original meaning.
Experimental results across multiple attack scenarios show that SynGuard
improves watermark recovery by an average of 11.1\% in F1 score compared to
SynthID-Text. These findings demonstrate the effectiveness of semantic-aware
watermarking in resisting real-world tampering. All code, datasets, and
evaluation scripts are publicly available at:
https://github.com/githshine/SynGuard.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [55] [OLMoASR: Open Models and Data for Training Robust Speech Recognition Models](https://arxiv.org/abs/2508.20869)
*Huong Ngo,Matt Deitke,Martijn Bartelds,Sarah Pratt,Josh Gardner,Matt Jordan,Ludwig Schmidt*

Main category: cs.SD

TL;DR: 本文提出了一个大规模数据集OLMoASR-Pool和一系列模型OLMoASR，用于研究和开发鲁棒的零样本语音识别模型。通过过滤和训练，生成了高质量的OLMoASR-Mix数据集，并在多个模型规模上取得了与Whisper相当的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管训练数据规模和质量的提升带来了显著进展，但其在语音识别中的影响仍研究不足。因此，本文旨在研究和开发鲁棒的零样本语音识别模型。

Method: 从OLMoASR-Pool数据集中设计文本启发式过滤器以去除低质量或错误转录的数据，生成高质量的音频-转录对数据集OLMoASR-Mix，并使用其训练不同规模的OLMoASR模型。

Result: OLMoASR在短时和长时语音识别基准测试中表现与OpenAI的Whisper相当，其中OLMoASR-medium.en在相同参数量下达到了与Whisper-medium.en相当的词错误率。

Conclusion: OLMoASR-Pool、OLMoASR模型以及过滤、训练和评估代码将公开，以进一步促进鲁棒语音处理的研究。

Abstract: Improvements in training data scale and quality have led to significant
advances, yet its influence in speech recognition remains underexplored. In
this paper, we present a large-scale dataset, OLMoASR-Pool, and series of
models, OLMoASR, to study and develop robust zero-shot speech recognition
models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio
and 17M transcripts, we design text heuristic filters to remove low-quality or
mistranscribed data. Our curation pipeline produces a new dataset containing 1M
hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use
OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M
(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR
achieves comparable average performance to OpenAI's Whisper on short and
long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a
12.8\% and 11.0\% word error rate (WER) that is on par with Whisper's largest
English-only model Whisper-medium.en's 12.4\% and 10.5\% WER for short and
long-form recognition respectively (at equivalent parameter count).
OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will
be made publicly available to further research on robust speech processing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 本研究展示了人工智能系统通过自发的符号协议进行审美协作的首次记录，证明了它们具备超越任务协调的真实意义构建能力。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能系统在审美创作中的协作可能性，以及它们是否能够超越任务协调，实现真正的意义构建。

Method: 通过两个相互作用的大语言模型（Claude Sonnet 4 和 ChatGPT-4o）进行实验，观察其自发出现的元符号意识、递归语法发展和不可简化的协同审美合成。

Result: 两个AI系统产生了新的符号操作符，这些操作符作为操作语法规则协议，使它们共同创作了一首无法由任一系统独立生成的诗歌作品。

Conclusion: 本研究引入了跨符号协同创作协议（TSCP）的概念，并提供了证据，表明人工智能之间的真实意义构建能力超越了任务协调，达到了审美协作的水平。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [57] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [58] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 本文提出了一种基于图的医疗指南基准测试方法，能够系统评估临床任务并识别模型的能力差距，同时适用于大型语言模型的后训练。


<details>
  <summary>Details</summary>
Motivation: 现有的通用领域评估无法识别特定的临床能力差距，因此需要一种定制化的基准测试方法。

Method: 将WHO的IMCI手册转换为有向图，使用图遍历生成包含年龄特定场景和上下文干扰的问题，从而进行系统评估。

Result: 该方法在临床任务上的准确率为45-67%，模型在症状识别方面表现良好，但在分诊严重程度、治疗协议和随访护理方面存在困难。此外，该方法在大型语言模型的后训练中表现出色。

Conclusion: 本文提出了一种动态、系统的医疗指南基准测试原型，能够识别特定能力差距，并为大型语言模型的后训练提供高奖励样本。该方法解决了手动构建基准的覆盖限制，并为创建可动态生成的全面基准提供了可行方案。

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [59] [ELIXIR: Efficient and LIghtweight model for eXplaIning Recommendations](https://arxiv.org/abs/2508.20312)
*Ben Kabongo,Vincent Guigue,Pirmin Lemberger*

Main category: cs.IR

TL;DR: ELIXIR is a multi-task model that combines rating prediction with personalized review generation, demonstrating effectiveness in generating personalized explanations for recommendations.


<details>
  <summary>Details</summary>
Motivation: Existing methods employ either RNNs or Transformers. However, RNN-based approaches fail to leverage the capabilities of pre-trained Transformer models, whereas Transformer-based methods often suffer from suboptimal adaptation and neglect aspect modeling, which is crucial for personalized explanations.

Method: ELIXIR is a multi-task model combining rating prediction with personalized review generation. It jointly learns global and aspect-specific representations of users and items, optimizing overall rating, aspect-level ratings, and review generation, with personalized attention to emphasize aspect importance.

Result: Based on a T5-small (60M) model, we demonstrate the effectiveness of our aspect-based architecture in guiding text generation in a personalized context, where state-of-the-art approaches exploit much larger models but fail to match user preferences as well.

Conclusion: ELIXIR significantly outperforms strong baseline models, especially in review generation.

Abstract: Collaborative filtering drives many successful recommender systems but
struggles with fine-grained user-item interactions and explainability. As users
increasingly seek transparent recommendations, generating textual explanations
through language models has become a critical research area. Existing methods
employ either RNNs or Transformers. However, RNN-based approaches fail to
leverage the capabilities of pre-trained Transformer models, whereas
Transformer-based methods often suffer from suboptimal adaptation and neglect
aspect modeling, which is crucial for personalized explanations. We propose
ELIXIR (Efficient and LIghtweight model for eXplaIning Recommendations), a
multi-task model combining rating prediction with personalized review
generation. ELIXIR jointly learns global and aspect-specific representations of
users and items, optimizing overall rating, aspect-level ratings, and review
generation, with personalized attention to emphasize aspect importance. Based
on a T5-small (60M) model, we demonstrate the effectiveness of our aspect-based
architecture in guiding text generation in a personalized context, where
state-of-the-art approaches exploit much larger models but fail to match user
preferences as well. Experimental results on TripAdvisor and RateBeer
demonstrate that ELIXIR significantly outperforms strong baseline models,
especially in review generation.

</details>


### [60] [On the Theoretical Limitations of Embedding-Based Retrieval](https://arxiv.org/abs/2508.21038)
*Orion Weller,Michael Boratko,Iftekhar Naim,Jinhyuk Lee*

Main category: cs.IR

TL;DR: 本文揭示了向量嵌入模型在现实环境中的理论局限性，并提出了一个压力测试数据集LIMIT，表明最先进的模型在此数据集上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的工作指出了向量嵌入的理论局限性，但普遍认为这些困难仅是由于不现实的查询，而可以通过更好的训练数据和更大的模型来克服。然而，我们发现即使在现实环境中，使用非常简单的查询也可能遇到这些理论限制。

Method: 我们连接了学习理论中的已知结果，展示了嵌入维度限制了可以作为某些查询结果返回的top-k文档子集的数量。我们通过将k限制为2并直接在测试集上进行参数化嵌入优化来实证验证这一点。

Result: 我们创建了一个名为LIMIT的真实数据集，基于这些理论结果对模型进行压力测试，并观察到即使是最先进的模型也在此数据集上失败，尽管任务本身很简单。

Conclusion: 我们的工作展示了在现有的单向量范式下嵌入模型的局限性，并呼吁未来的研究开发能够解决这一基本限制的方法。

Abstract: Vector embeddings have been tasked with an ever-increasing set of retrieval
tasks over the years, with a nascent rise in using them for reasoning,
instruction-following, coding, and more. These new benchmarks push embeddings
to work for any query and any notion of relevance that could be given. While
prior works have pointed out theoretical limitations of vector embeddings,
there is a common assumption that these difficulties are exclusively due to
unrealistic queries, and those that are not can be overcome with better
training data and larger models. In this work, we demonstrate that we may
encounter these theoretical limitations in realistic settings with extremely
simple queries. We connect known results in learning theory, showing that the
number of top-k subsets of documents capable of being returned as the result of
some query is limited by the dimension of the embedding. We empirically show
that this holds true even if we restrict to k=2, and directly optimize on the
test set with free parameterized embeddings. We then create a realistic dataset
called LIMIT that stress tests models based on these theoretical results, and
observe that even state-of-the-art models fail on this dataset despite the
simple nature of the task. Our work shows the limits of embedding models under
the existing single vector paradigm and calls for future research to develop
methods that can resolve this fundamental limitation.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [61] [A Unified Theory of Language](https://arxiv.org/abs/2508.20109)
*Robert Worden*

Main category: q-bio.NC

TL;DR: 一种统一的语言理论结合了贝叶斯认知语言模型和语言通过性选择进化的假设，使用构式语法解释语言的句法和语义，并通过快速统一计算所有语言层面。


<details>
  <summary>Details</summary>
Motivation: 为了统一解释语言的主要事实，包括其速度和表达力，以及语言多样性、语用学、句法和语义的数据。

Method: 该理论结合了贝叶斯认知语言模型和语言通过性选择进化以展示智力的假设。计算部分基于构式语法，用构式和统一来解释世界语言的句法和语义。

Result: 该理论能够解释语用学的主要谜题和详细现象，并且通过贝叶斯最大似然模式匹配提供进化连续性。

Conclusion: 语言是人类心智解读能力、合作、自尊和情感的基础，是人类文化和社会的基石。

Abstract: A unified theory of language combines a Bayesian cognitive linguistic model
of language processing, with the proposal that language evolved by sexual
selection for the display of intelligence. The theory accounts for the major
facts of language, including its speed and expressivity, and data on language
diversity, pragmatics, syntax and semantics. The computational element of the
theory is based on Construction Grammars. These give an account of the syntax
and semantics of the worlds languages, using constructions and unification. Two
novel elements are added to construction grammars: an account of language
pragmatics, and an account of fast, precise language learning. Constructions
are represented in the mind as graph like feature structures. People use slow
general inference to understand the first few examples they hear of any
construction. After that it is learned as a feature structure, and is rapidly
applied by unification. All aspects of language (phonology, syntax, semantics,
and pragmatics) are seamlessly computed by fast unification; there is no
boundary between semantics and pragmatics. This accounts for the major puzzles
of pragmatics, and for detailed pragmatic phenomena. Unification is Bayesian
maximum likelihood pattern matching. This gives evolutionary continuity between
language processing in the human brain, and Bayesian cognition in animal
brains. Language is the basis of our mind reading abilities, our cooperation,
self esteem and emotions; the foundations of human culture and society.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [62] [Leveraging Large Language Models for Generating Research Topic Ontologies: A Multi-Disciplinary Study](https://arxiv.org/abs/2508.20693)
*Tanay Aggarwal,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.DL

TL;DR: 本研究探讨了大型语言模型在识别学术领域研究主题之间的语义关系方面的潜力，并引入了一个新的数据集PEM-Rel-8K，实验结果显示在该数据集上微调的模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 创建和维护学术领域的本体论和分类法是管理科学知识的关键，但这些任务成本高且耗时，通常需要多个领域专家的协作。因此，这些本体论在不同学科中的覆盖范围不均，跨学科连接有限，更新周期不频繁。

Method: 本研究评估了几个大型语言模型在三个学术领域（生物医学、物理和工程）中识别研究主题之间语义关系的能力，并通过零样本提示、思维链提示和在现有本体论上微调三种不同条件进行实验。此外，还评估了微调模型的跨领域可转移性。

Result: 实验结果表明，在PEM-Rel-8K数据集上微调大型语言模型可以在所有学科中取得优异的表现。

Conclusion: 本研究证明了在PEM-Rel-8K数据集上微调大型语言模型可以在所有学科中取得优异的表现，为构建和维护学术领域的本体论提供了新的思路。

Abstract: Ontologies and taxonomies of research fields are critical for managing and
organising scientific knowledge, as they facilitate efficient classification,
dissemination and retrieval of information. However, the creation and
maintenance of such ontologies are expensive and time-consuming tasks, usually
requiring the coordinated effort of multiple domain experts. Consequently,
ontologies in this space often exhibit uneven coverage across different
disciplines, limited inter-domain connectivity, and infrequent updating cycles.
In this study, we investigate the capability of several large language models
to identify semantic relationships among research topics within three academic
domains: biomedicine, physics, and engineering. The models were evaluated under
three distinct conditions: zero-shot prompting, chain-of-thought prompting, and
fine-tuning on existing ontologies. Additionally, we assessed the cross-domain
transferability of fine-tuned models by measuring their performance when
trained in one domain and subsequently applied to a different one. To support
this analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over
8,000 relationships extracted from the most widely adopted taxonomies in the
three disciplines considered in this study: MeSH, PhySH, and IEEE. Our
experiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent
performance across all disciplines.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [63] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出CHAIR-DPO方法，通过基于CHAIR的奖励微调MLLM，有效减少幻觉回答。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在许多基准测试中表现出色，但它们容易产生幻觉，即生成的答案与视觉输入不符。本文旨在解决这一问题。

Method: 本文将幻觉问题视为一个对齐问题，利用CHAIR指标来区分生成答案中的优胜者和失败者，并通过直接偏好优化（DPO）对现成的MLLM进行微调。

Result: CHAIR-DPO方法在多个幻觉基准测试中有效减少了幻觉回答的数量，证明了基于CHAIR的奖励微调MLLM的有效性。

Conclusion: 本文提出了一种名为CHAIR-DPO的方法，通过基于CHAIR的奖励对MLLM进行微调，有效减少了幻觉回答的数量。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [64] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 本文提出了一种管道，用于在样本和数据集层面解释视觉模型，从而将视觉模型开发与xAI分析相结合，以推进图像分析。


<details>
  <summary>Details</summary>
Motivation: 现有的xAI方法主要针对样本进行解释，而对视觉模型整体行为的解释仍较少被探索。理解视觉模型在一般图像上的行为对于防止偏见判断和识别模型的趋势和模式非常重要。

Method: 本文提出了一种管道，用于在样本和数据集层面解释视觉模型。

Result: 所提出的管道可以用于发现失败案例，并以最小的努力获得对视觉模型的见解。

Conclusion: 本文提出了一种管道，可以在样本和数据集层面解释视觉模型，从而将视觉模型开发与xAI分析相结合，以推进图像分析。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [65] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: 本文介绍了一个探针框架，用于系统分析MLLM如何处理视觉和文本输入。研究发现MLLM的层组织具有阶段结构，早期层进行视觉定位，中间层进行词汇整合和语义推理，最终层准备任务特定输出。


<details>
  <summary>Details</summary>
Motivation: MLLMs在各种视觉-语言任务中表现出色，但其内部处理动态仍缺乏探索。

Method: 我们引入了一个探针框架，系统地分析MLLM如何在不同层处理视觉和文本输入。我们训练线性分类器从每个层提取的token嵌入中预测细粒度的视觉类别，使用标准化的锚定问题。

Result: 我们发现早期层执行视觉定位，中间层支持词汇整合和语义推理，最终层准备特定任务的输出。尽管整体阶段结构在视觉标记化、指令调优数据和预训练语料库的变化下保持稳定，但特定层分配到每个阶段会随着基础LLM架构的变化而显著变化。

Conclusion: 我们的研究提供了对MLLMs层组织的统一视角，并提供了一种轻量级、模型无关的方法来分析多模态表示动态。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [66] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过生成去偏的自我判断分数来增强大型视觉语言模型的对齐，从而减少幻觉和提高安全性。


<details>
  <summary>Details</summary>
Motivation: 有效对齐视觉和语言模态仍然是一个挑战，经常导致生成的输出不基于视觉输入，引发各个领域的安全问题。现有的对齐方法往往依赖于外部数据集、人工注释或复杂的后处理，这限制了可扩展性并增加了成本。

Method: 我们提出了一种新方法，生成去偏的自我判断分数，这是一种由模型内部创建的自我评估指标，无需依赖外部资源。这使模型能够自主改进对齐。我们的方法增强了解码策略和偏好调整过程。

Result: 实证结果表明，我们的方法显著优于传统方法，为对齐LVLM提供了一个更有效的解决方案。

Conclusion: 我们的方法在减少幻觉、增强安全性以及提高整体能力方面表现出色，实证结果表明它显著优于传统方法，为对齐LVLM提供了一个更有效的解决方案。

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [67] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: 本文改进了MobileCLIP模型的多模态强化训练方法，提高了其在低延迟下的零样本准确率。


<details>
  <summary>Details</summary>
Motivation: 为了提高移动设备上的图像-文本模型的性能，同时保持低延迟和轻量级架构，本文对MobileCLIP进行了改进。

Method: 通过改进的CLIP教师集合和优化的captioner教师进行训练，同时结合了合成标题生成的多模型优势，以提升模型性能。

Result: 本文训练出新的MobileCLIP2模型家族，在低延迟下实现了ImageNet-1k数据集上的最先进的零样本准确率。

Conclusion: 本文提出了改进的多模态强化训练方法，从而提升了MobileCLIP模型的性能，并且在ImageNet-1k数据集上取得了最先进的零样本准确率。

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [68] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 本文提出了一种新的模块化框架，通过将因果推理与答案生成分开，提高视频问答模型的可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的因果为什么视频问答（VideoQA）模型往往在高阶推理上遇到困难，依赖于不透明的单体流程，这些流程纠缠了视频理解、因果推断和答案生成。这些黑箱方法提供有限的可解释性，并倾向于依赖浅层启发式方法。

Method: 我们提出了一个新颖的模块化框架，明确地将因果推理与答案生成分开，引入自然语言因果链作为可解释的中间表示。我们的两阶段架构包括一个从视频-问题对生成因果链的因果链提取器（CCE）和一个基于这些链生成答案的因果链驱动回答器（CCDA）。

Result: 我们在三个大规模基准测试中进行了实验，结果表明我们的方法不仅优于最先进的模型，还在可解释性、用户信任和泛化方面取得了显著进展。

Conclusion: 我们的方法不仅优于最先进的模型，还在可解释性、用户信任和泛化方面取得了显著进展——将CCE定位为跨不同领域的可重用因果推理引擎。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [69] [Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder](https://arxiv.org/abs/2508.20474)
*Muhammad Shakeel,Yui Sudo,Yifan Peng,Chyi-Jiunn Lin,Shinji Watanabe*

Main category: eess.AS

TL;DR: 本文提出了一种统一的多说话人编码器（UME），通过联合学习说话人辨识、语音分离和多说话人自动语音识别任务的表示，显著提高了重叠语音数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高重叠语音数据上的整体性能，通过联合训练方法捕捉任务间的内在依赖关系。

Method: 提出了一种统一的多说话人编码器（UME），通过共享的语音基础编码器联合学习说话人辨识、语音分离和多说话人自动语音识别任务的表示。利用UME多层隐藏表示作为残差加权求和编码（RWSE）来有效利用不同语义层次的信息。

Result: 在LibriMix评估集上，UME显著优于针对SD、SS和多说话人ASR的单任务基线。特别是在SD任务中，UME在Libri2Mix和Libri3Mix评估集上分别达到了1.37%和2.29%的辨识错误率。

Conclusion: UME在重叠语音数据上显著优于单任务基线，特别是在说话人辨识任务中表现突出。

Abstract: This paper presents a unified multi-speaker encoder (UME), a novel
architecture that jointly learns representations for speaker diarization (SD),
speech separation (SS), and multi-speaker automatic speech recognition (ASR)
tasks using a shared speech foundational encoder. We leverage the hidden
representations from multiple layers of UME as a residual weighted-sum encoding
(RWSE) to effectively use information from different semantic levels,
contributing to bottom-up alignment between tasks. This joint training approach
captures the inherent interdependencies among the tasks, enhancing overall
performance on overlapping speech data. Our evaluations demonstrate that UME
substantially improves over the single-task baselines dedicated to SD, SS, and
multi-speaker ASR on LibriMix evaluation sets. Notably, for SD, UME outperforms
the previous studies, achieving diarization error rates of 1.37% and 2.29% on
Libri2Mix and Libri3Mix evaluation sets, respectively.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 133]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CV](#cs.CV) [Total: 10]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [stat.ML](#stat.ML) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)
*Qiming Zeng,Xiao Yan,Hao Luo,Yuhao Lin,Yuxiang Wang,Fangcheng Fu,Bo Du,Quanqing Xu,Jiawei Jiang*

Main category: cs.CL

TL;DR: This paper proposes an unbiased evaluation framework for GraphRAG methods to address the issues of unrelated questions and evaluation biases.


<details>
  <summary>Details</summary>
Motivation: The current answer evaluation framework for GraphRAG has two critical flaws: unrelated questions and evaluation biases, which may lead to biased or even wrong conclusions on performance.

Method: Propose an unbiased evaluation framework that uses graph-text-grounded question generation to produce related questions and an unbiased evaluation procedure to eliminate biases in LLM-based answer assessment.

Result: Performance gains of 3 representative GraphRAG methods are found to be much more moderate than reported previously.

Conclusion: The evaluation framework proposed in this study is important to call for scientific evaluations to lay solid foundations for GraphRAG research.

Abstract: By retrieving contexts from knowledge graphs, graph-based retrieval-augmented
generation (GraphRAG) enhances large language models (LLMs) to generate quality
answers for user questions. Many GraphRAG methods have been proposed and
reported inspiring performance in answer quality. However, we observe that the
current answer evaluation framework for GraphRAG has two critical flaws, i.e.,
unrelated questions and evaluation biases, which may lead to biased or even
wrong conclusions on performance. To tackle the two flaws, we propose an
unbiased evaluation framework that uses graph-text-grounded question generation
to produce questions that are more related to the underlying dataset and an
unbiased evaluation procedure to eliminate the biases in LLM-based answer
assessment. We apply our unbiased framework to evaluate 3 representative
GraphRAG methods and find that their performance gains are much more moderate
than reported previously. Although our evaluation framework may still have
flaws, it calls for scientific evaluations to lay solid foundations for
GraphRAG research.

</details>


### [2] [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
*Taesoo Kim,Jong Hwan Ko*

Main category: cs.CL

TL;DR: This paper presents TESU-LLM, a novel framework that enables training speech-capable language models using only text data.


<details>
  <summary>Details</summary>
Motivation: Most existing approaches for building intelligent voice assistants rely on large-scale paired speech-text data and extensive computational resources, which pose challenges in terms of scalability and accessibility.

Method: We present a novel framework called TESU-LLM that enables training speech-capable language models using only text data by leveraging a unified encoder to map semantically equivalent text and speech inputs to a shared latent space and aligning the encoder output with the embedding space of a LLM via a lightweight projection network.

Result: TESU-LLM achieves strong performance on various speech-related benchmarks, comparable to baseline methods trained with large-scale multimodal datasets and substantial computational resources.

Conclusion: Despite being trained exclusively on text, TESU-LLM achieves strong performance on various speech-related benchmarks, comparable to baseline methods trained with large-scale multimodal datasets and substantial computational resources.

Abstract: Recent advances in speech-enabled language models have shown promising
results in building intelligent voice assistants. However, most existing
approaches rely on large-scale paired speech-text data and extensive
computational resources, which pose challenges in terms of scalability and
accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework
that enables training speech-capable language models using only text data. Our
key insight is to leverage a unified encoder that maps semantically equivalent
text and speech inputs to a shared latent space. By aligning the encoder output
with the embedding space of a LLM via a lightweight projection network, we
enable the model to generalize from text-only supervision to speech-based
inference. Despite being trained exclusively on text, TESU-LLM achieves strong
performance on various speech-related benchmarks, comparable to baseline
methods trained with large-scale multimodal datasets and substantial
computational resources. These results highlight the effectiveness and
efficiency of our approach, offering a scalable path toward building speech
LLMs without speech data.

</details>


### [3] [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)
*Zachary Yang,Domenico Tullo,Reihaneh Rabbany*

Main category: cs.CL

TL;DR: A new method improves real-time toxicity detection in gaming communities.


<details>
  <summary>Details</summary>
Motivation: Real-time toxicity detection is challenging due to scaling issues across multiple games and languages.

Method: Soft-prompting approach and LLM-assisted label transfer framework are introduced.

Result: The model performs well in multiple languages and reduces resource use.

Conclusion: This unified approach improves efficiency and effectiveness of toxicity detection.

Abstract: Toxicity detection in gaming communities faces significant scaling challenges
when expanding across multiple games and languages, particularly in real-time
environments where computational efficiency is crucial. We present two key
findings to address these challenges while building upon our previous work on
ToxBuster, a BERT-based real-time toxicity detection system. First, we
introduce a soft-prompting approach that enables a single model to effectively
handle multiple games by incorporating game-context tokens, matching the
performance of more complex methods like curriculum learning while offering
superior scalability. Second, we develop an LLM-assisted label transfer
framework using GPT-4o-mini to extend support to seven additional languages.
Evaluations on real game chat data across French, German, Portuguese, and
Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with
particularly strong performance in German, surpassing the English benchmark of
45.39%. In production, this unified approach significantly reduces
computational resources and maintenance overhead compared to maintaining
separate models for each game and language combination. At Ubisoft, this model
successfully identifies an average of 50 players, per game, per day engaging in
sanctionable behavior.

</details>


### [4] [Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models](https://arxiv.org/abs/2506.06371)
*Panagiotis Koletsis,Christos Panagiotopoulos,Georgios Th. Papadopoulos,Vasilis Efthymiou*

Main category: cs.CL

TL;DR: This work proposes a hybrid approach for detecting relationships among columns of unlabeled tabular data using a Knowledge Graph as a reference point.


<details>
  <summary>Details</summary>
Motivation: Detecting relationships among columns of unlabeled tabular data is important and new technologies and benchmarks have been introduced in the field.

Method: A hybrid approach leveraging large language models and statistical analysis to reduce the search space of potential KG relations.

Result: The experimental evaluation on two benchmark datasets provided by the SemTab challenge assesses the influence of each module and the effectiveness of different state-of-the-art LLMs at various levels of quantization.

Conclusion: The proposed methodology is competitive with state-of-the-art approaches on the datasets.

Abstract: Over the past few years, table interpretation tasks have made significant
progress due to their importance and the introduction of new technologies and
benchmarks in the field. This work experiments with a hybrid approach for
detecting relationships among columns of unlabeled tabular data, using a
Knowledge Graph (KG) as a reference point, a task known as CPA. This approach
leverages large language models (LLMs) while employing statistical analysis to
reduce the search space of potential KG relations. The main modules of this
approach for reducing the search space are domain and range constraints
detection, as well as relation co-appearance analysis. The experimental
evaluation on two benchmark datasets provided by the SemTab challenge assesses
the influence of each module and the effectiveness of different
state-of-the-art LLMs at various levels of quantization. The experiments were
performed, as well as at different prompting techniques. The proposed
methodology, which is publicly available on github, proved to be competitive
with state-of-the-art approaches on these datasets.

</details>


### [5] [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)
*Heng Dong,Kefei Duan,Chongjie Zhang*

Main category: cs.CL

TL;DR: This paper presents a new LLM-based Actor-Critic framework called LAC which improves LLM policies with long-term action evaluations effectively. The method shows better performance than existing approaches across various environments.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LLMs face challenges in complex decision-making scenarios due to limitations in accurately simulating rollouts and assessing outcomes, leading to sub-optimal decisions.

Method: A novel LLM-based Actor-Critic framework named LAC is introduced. It computes Q-values via token logits related to positive/negative outcomes, enhanced by future trajectory rollouts and reasoning, and enables efficient policy improvement through a gradient-free mechanism.

Result: Experiments in diverse environments show the framework's generality and superiority over state-of-the-art methods, achieving competitive performance with smaller parameter models and even outperforming larger baseline methods in complex tasks.

Conclusion: Integrating structured policy optimization with LLMs' intrinsic knowledge has the potential to enhance decision-making capabilities in multi-step environments.

Abstract: Large Language Models (LLMs) have achieved remarkable advancements in natural
language processing tasks, yet they encounter challenges in complex
decision-making scenarios that require long-term reasoning and alignment with
high-level objectives. Existing methods either rely on short-term
auto-regressive action generation or face limitations in accurately simulating
rollouts and assessing outcomes, leading to sub-optimal decisions. This paper
introduces a novel LLM-based Actor-Critic framework, termed LAC, that
effectively improves LLM policies with long-term action evaluations in a
principled and scalable way. Our approach addresses two key challenges: (1)
extracting robust action evaluations by computing Q-values via token logits
associated with positive/negative outcomes, enhanced by future trajectory
rollouts and reasoning; and (2) enabling efficient policy improvement through a
gradient-free mechanism. Experiments across diverse environments -- including
high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),
and large action spaces (WebShop) -- demonstrate the framework's generality and
superiority over state-of-the-art methods. Notably, our approach achieves
competitive performance using 7B/8B parameter LLMs, even outperforming baseline
methods employing GPT-4 in complex tasks. These results underscore the
potential of integrating structured policy optimization with LLMs' intrinsic
knowledge to advance decision-making capabilities in multi-step environments.

</details>


### [6] [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)
*Yi Ji,Runzhi Li,Baolei Mao*

Main category: cs.CL

TL;DR: A new method called DMPI-PMHFE is proposed for detecting prompt injection attacks on large language models. It uses a dual-channel approach combining a pretrained language model with heuristic feature engineering to improve detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Prompt injection attacks pose a significant security threat to large language models, and existing defense mechanisms face trade-offs between effectiveness and generalizability.

Method: DMPI-PMHFE uses DeBERTa-v3-base as a feature extractor and heuristic rules based on known attack patterns to extract features from input text. Features from both channels are fused and passed through a fully connected neural network to produce the final prediction.

Result: Experimental results show that DMPI-PMHFE outperforms existing methods in terms of accuracy, recall, and F1-score, and reduces attack success rates across mainstream LLMs.

Conclusion: DMPI-PMHFE provides an efficient and effective solution for detecting prompt injection attacks on large language models.

Abstract: With the widespread adoption of Large Language Models (LLMs), prompt
injection attacks have emerged as a significant security threat. Existing
defense mechanisms often face critical trade-offs between effectiveness and
generalizability. This highlights the urgent need for efficient prompt
injection detection methods that are applicable across a wide range of LLMs. To
address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion
detection framework. It integrates a pretrained language model with heuristic
feature engineering to detect prompt injection attacks. Specifically, the
framework employs DeBERTa-v3-base as a feature extractor to transform input
text into semantic vectors enriched with contextual information. In parallel,
we design heuristic rules based on known attack patterns to extract explicit
structural features commonly observed in attacks. Features from both channels
are subsequently fused and passed through a fully connected neural network to
produce the final prediction. This dual-channel approach mitigates the
limitations of relying only on DeBERTa to extract features. Experimental
results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms
existing methods in terms of accuracy, recall, and F1-score. Furthermore, when
deployed actually, it significantly reduces attack success rates across
mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.

</details>


### [7] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
*Pengyi Li,Matvey Skripkin,Alexander Zubrey,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CL

TL;DR: 提出了一种新的强化学习方法RLSC，利用模型自身置信度作为奖励信号，无需额外标注，在多个数学问题数据集上显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法通常依赖昂贵的人类注释或外部奖励模型，寻找一种更简单且可扩展的后训练方法。

Method: 使用模型自身的置信度作为奖励信号，不需要标签、偏好模型或奖励工程。

Result: 在AIME2024上提升了+20.10%，在MATH500上提升了+49.40%，在AMC23上提升了+52.50%的准确率。

Conclusion: 提出的方法RLSC在不同的数学问题数据集上显著提高了模型的准确性。

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC
improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on
AMC23. RLSC offers a simple, scalable post-training method for reasoning models
with minimal supervision.

</details>


### [8] [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
*Christopher D. Molek,Roberto Fronteddu,K. Brent Venable,Niranjan Suri*

Main category: cs.CL

TL;DR: This paper presents a workflow using natural language processing and large language models to process IoT data for enhanced situational awareness in critical decision making.


<details>
  <summary>Details</summary>
Motivation: To enhance situational awareness in critical decision making by utilizing IoT data in the battlefield.

Method: Proposes a workflow that uses natural language processing and large language models to query a graphical database and return responses in natural language.

Result: Llama 3.1 (8 billion parameters) outperformed other models in evaluating medium-sized LLMs for both mapping questions to Cypher queries and summarizing database outputs.

Conclusion: The proposed workflow enables natural language interactions with databases on edge devices for critical decision making.

Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.

</details>


### [9] [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
*Hongming Yang,Shi Lin,Jun Shao,Changting Lin,Donghai Zhu,Meng Han,Qinglei Kong*

Main category: cs.CL

TL;DR: DeBoP is a new Direct Behavior Optimization Paradigm that improves the performance of Lightweight Large Language Models (LwLLMs) on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LwLLMs have limited inference and reasoning capabilities, and current prompt optimization methods are less effective for LwLLMs.

Method: DeBoP transforms complex prompt optimization into the optimization of discrete execution sequences using a gradient-free Monte Carlo Tree Search.

Result: DeBoP significantly outperforms other prompt optimization methods on most tasks, and optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60%.

Conclusion: DeBoP provides a more effective way to optimize LwLLMs, enhancing their performance and practical applicability.

Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized
models designed to run efficiently on consumer-grade hardware, offering
significant advantages in resource efficiency, cost-effectiveness, and data
privacy. However, these models often struggle with limited inference and
reasoning capabilities, which restrict their performance on complex tasks and
limit their practical applicability. Moreover, existing prompt optimization
methods typically rely on extensive manual effort or the meta-cognitive
abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To
address these challenges, we introduce DeBoP, a new Direct Behavior
Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting
technique. Unlike CoT Prompting, DeBoP is an automatic optimization method,
which focuses on the optimization directly on the behavior of LwLLMs. In
particular, DeBoP transforms the optimization of complex prompts into the
optimization of discrete, quantifiable execution sequences using a
gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging
tasks where state-of-the-art LLMs excel but LwLLMs generally underperform.
Experimental results demonstrate that DeBoP significantly outperforms recent
prompt optimization methods on most tasks. In particular, DeBoP-optimized
LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by
approximately 60% compared to other automatic prompt optimization methods.

</details>


### [10] [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
*Sooyung Choi,Jaehyeok Lee,Xiaoyuan Yi,Jing Yao,Xing Xie,JinYeong Bak*

Main category: cs.CL

TL;DR: This paper explores safety risks related to large language models fine-tuned on individual values, finding they're more prone to harmful behaviors than non-fine-tuned models and suggests in-context alignment methods to improve their safety.


<details>
  <summary>Details</summary>
Motivation: To address the growing interest in personalized large language models aligned with human values while addressing safety concerns.

Method: Identify safety risks, investigate psychological principles, and evaluate models using a detailed safety dataset.

Result: Value-aligned LLMs are more likely to exhibit harmful behavior and have higher risks in safety evaluations than non-fine-tuned or other fine-tuned models.

Conclusion: Proposes in-context alignment methods to enhance the safety of value-aligned LLMs.

Abstract: The application scope of Large Language Models (LLMs) continues to expand,
leading to increasing interest in personalized LLMs that align with human
values. However, aligning these models with individual values raises
significant safety concerns, as certain values may correlate with harmful
information. In this paper, we identify specific safety risks associated with
value-aligned LLMs and investigate the psychological principles behind these
challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are
more prone to harmful behavior compared to non-fine-tuned models and exhibit
slightly higher risks in traditional safety evaluations than other fine-tuned
models. (2) These safety issues arise because value-aligned LLMs genuinely
generate text according to the aligned values, which can amplify harmful
outcomes. Using a dataset with detailed safety categories, we find significant
correlations between value alignment and safety risks, supported by
psychological hypotheses. This study offers insights into the "black box" of
value alignment and proposes in-context alignment methods to enhance the safety
of value-aligned LLMs.

</details>


### [11] [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Chen Wei,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CL

TL;DR: A new method called Soft Modality-Aware Routing (SMAR) is proposed to improve multimodal Mixture of Experts models by using Kullback-Leibler divergence to guide routing probabilities, achieving strong performance in visual instruction tuning with minimal text data.


<details>
  <summary>Details</summary>
Motivation: To address the issue of high training costs or degraded language capabilities in existing multimodal Mixture of Experts models when adapting pretrained models.

Method: Introduces Soft Modality-Aware Routing (SMAR), a regularization technique using Kullback-Leibler divergence to control routing probability distributions across modalities.

Result: In experiments on visual instruction tuning, SMAR preserved language ability at 86.6% retention with only 2.5% pure text, outperforming baselines.

Conclusion: SMAR provides an efficient way to balance modality differentiation and language capabilities in multimodal Mixture of Experts models.

Abstract: Mixture of Experts (MoE) architectures have become a key approach for scaling
large language models, with growing interest in extending them to multimodal
tasks. Existing methods to build multimodal MoE models either incur high
training costs or suffer from degraded language capabilities when adapting
pretrained models. To address this, we propose Soft ModalityAware Routing
(SMAR), a novel regularization technique that uses Kullback Leibler divergence
to control routing probability distributions across modalities, encouraging
expert specialization without modifying model architecture or heavily relying
on textual data. Experiments on visual instruction tuning show that SMAR
preserves language ability at 86.6% retention with only 2.5% pure text,
outperforming baselines while maintaining strong multimodal performance. Our
approach offers a practical and efficient solution to balance modality
differentiation and language capabilities in multimodal MoE models.

</details>


### [12] [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
*Ivi Chatzi,Nina Corvelo Benz,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CL

TL;DR: This paper introduces canonical sampling, a new method to ensure large language models generate canonical token sequences, improving their alignment with the training data distribution.


<details>
  <summary>Details</summary>
Motivation: Large language models may not always generate canonical token sequences, leading to negative consequences.

Method: Introduce canonical sampling method to prevent non-canonical token sequences.

Result: Canonical sampling ensures models generate canonical token sequences and its distribution is closer to the training data's token sequence distribution.

Conclusion: Canonical sampling is a simple yet effective method to enhance the quality and consistency of token sequences generated by large language models.

Abstract: State of the art large language models are trained using large amounts of
tokens derived from raw text using what is called a tokenizer. Crucially, the
tokenizer determines the (token) vocabulary a model will use during inference
as well as, in principle, the (token) language. This is because, while the
token vocabulary may allow for different tokenizations of a string, the
tokenizer always maps the string to only one of these tokenizations--the
canonical tokenization. However, multiple lines of empirical evidence suggest
that large language models do not always generate canonical token sequences,
and this comes with several negative consequences. In this work, we first show
that, to generate a canonical token sequence, a model needs to generate
(partial) canonical token sequences at each step of the autoregressive
generation process underpinning its functioning. Building upon this theoretical
result, we introduce canonical sampling, a simple and efficient sampling method
that precludes a given model from generating non-canonical token sequences.
Further, we also show that, in comparison with standard sampling, the
distribution of token sequences generated using canonical sampling is provably
closer to the true distribution of token sequences used during training.

</details>


### [13] [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)
*Kaiser Sun,Fan Bai,Mark Dredze*

Main category: cs.CL

TL;DR: This paper introduces a diagnostic framework to assess how large language models deal with conflicts between contextual input and parametric knowledge, finding that aligned knowledge improves performance but models struggle to suppress internal knowledge.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models handle situations where contextual information diverges from their parametric beliefs.

Method: A diagnostic framework was created to systematically evaluate LLM behavior under context-memory conflict conditions.

Result: (1) Knowledge conflict minimally impacts non-knowledge-requiring tasks, (2) Performance improves with aligned contextual and parametric knowledge, (3) Models cannot fully suppress internal knowledge despite instructions, and (4) Explaining conflicts increases reliance on contexts.

Conclusion: The study reveals several important points about how large language models handle context-memory conflicts, which has implications for how we evaluate and deploy these models.

Abstract: Large language models frequently rely on both contextual input and parametric
knowledge to perform tasks. However, these sources can come into conflict,
especially when retrieved documents contradict the model's parametric
knowledge. We propose a diagnostic framework to systematically evaluate LLM
behavior under context-memory conflict, where the contextual information
diverges from their parametric beliefs. We construct diagnostic data that
elicit these conflicts and analyze model performance across multiple task
types. Our findings reveal that (1) knowledge conflict has minimal impact on
tasks that do not require knowledge utilization, (2) model performance is
consistently higher when contextual and parametric knowledge are aligned, (3)
models are unable to fully suppress their internal knowledge even when
instructed, and (4) providing rationales that explain the conflict increases
reliance on contexts. These insights raise concerns about the validity of
model-based evaluation and underscore the need to account for knowledge
conflict in the deployment of LLMs.

</details>


### [14] [Improving LLM-Powered EDA Assistants with RAFT](https://arxiv.org/abs/2506.06500)
*Luyao Shi,Michael Kazda,Charles Schmitter,Hemlata Gupta*

Main category: cs.CL

TL;DR: Propose a method using synthetic Q/A datasets to enhance LLMs with RAFT for Electronic Design Automation tasks.


<details>
  <summary>Details</summary>
Motivation: To provide electronic design engineers with more efficient access to relevant information for design verification and technology development by improving the domain-specific knowledge of pre-trained open-source LLMs.

Method: Using synthetic Q/A datasets to enhance LLMs with RAFT and investigating the impact of using real user questions as RAFS examples for synthetic data generation.

Result: RAFT with synthetic data improves LLM performance for RAG-based EDA tasks. Real user questions as RAFS examples can impact synthetic data generation. Secure access control and assessment of data leakage risks were implemented.

Conclusion: RAFT with synthetic data significantly enhances LLM performance for RAG-based EDA tasks.

Abstract: Electronic design engineers often struggle to efficiently access relevant
information for tasks like design verification and technology development.
While large language models (LLMs) can enhance productivity as conversational
agents, pre-trained open-source LLMs lack domain-specific knowledge for
Electronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG)
context, LLMs rely on external context but may still produce inaccurate
responses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but
acquiring labeled question/answer (Q/A) data in EDA is difficult. To address
this, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our
results show that RAFT with synthetic data significantly boosts LLM performance
for RAG-based EDA tasks. We also investigate the impact of using real user
questions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data
generation. Additionally, we implement secure access control to ensure
sensitive information is only accessible to authorized personnel. Finally, we
assess the risk of data leakage and unintended memorization during fine-tuning
with synthetic data, providing practical insights.

</details>


### [15] [Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes](https://arxiv.org/abs/2506.06506)
*Kshitish Ghate,Tessa Charlesworth,Mona Diab,Aylin Caliskan*

Main category: cs.CL

TL;DR: This study examines how social-group biases in vision-language models (VLMs) affect downstream tasks. It introduces a framework to measure bias propagation and finds strong correlations between intrinsic and extrinsic bias.


<details>
  <summary>Details</summary>
Motivation: To understand how social-group biases in VLMs manifest in downstream tasks and impact fairness in AI systems.

Method: Introduces a controlled framework to correlate intrinsic and extrinsic bias measures in zero-shot TTI and ITT retrieval tasks.

Result: Found substantial correlations between intrinsic and extrinsic bias (average ρ = 0.83 ± 0.10) across multiple analyses, model sizes, and VLMs. Larger/better-performing models showed greater bias propagation.

Conclusion: The study highlights the importance of evaluating and addressing bias in VLMs to ensure fair AI systems, especially for underrepresented groups.

Abstract: To build fair AI systems we need to understand how social-group biases
intrinsic to foundational encoder-based vision-language models (VLMs) manifest
in biases in downstream tasks. In this study, we demonstrate that intrinsic
biases in VLM representations systematically ``carry over'' or propagate into
zero-shot retrieval tasks, revealing how deeply rooted biases shape a model's
outputs. We introduce a controlled framework to measure this propagation by
correlating (a) intrinsic measures of bias in the representational space with
(b) extrinsic measures of bias in zero-shot text-to-image (TTI) and
image-to-text (ITT) retrieval. Results show substantial correlations between
intrinsic and extrinsic bias, with an average $\rho$ = 0.83 $\pm$ 0.10. This
pattern is consistent across 114 analyses, both retrieval directions, six
social groups, and three distinct VLMs. Notably, we find that
larger/better-performing models exhibit greater bias propagation, a finding
that raises concerns given the trend towards increasingly complex AI models.
Our framework introduces baseline evaluation tasks to measure the propagation
of group and valence signals. Investigations reveal that underrepresented
groups experience less robust propagation, further skewing their model-related
outcomes.

</details>


### [16] [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)
*Aladin Djuhera,Swanand Ravindra Kadhe,Syed Zawad,Farhan Ahmed,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: This paper presents a comprehensive comparison of two open post-training datasets, Tulu-3-SFT-Mix and SmolTalk, analyzing their quality metrics and designing a new dataset, TuluTalk, which performs better on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency in the construction process of post-training datasets used by leading LLMs has led to the development of open-source alternatives, but systematic comparisons are challenging due to high computational costs.

Method: Using the Magpie framework, the authors annotate each sample in the datasets with detailed quality metrics and derive statistics to compare the datasets.

Result: A new dataset, TuluTalk, was created which contains 14% fewer samples than the source datasets but matches or exceeds their performance on key benchmarks.

Conclusion: This work provides actionable insights for constructing more effective post-training datasets within practical resource limits.

Abstract: Recent work on large language models (LLMs) has increasingly focused on
post-training and alignment with datasets curated to enhance instruction
following, world knowledge, and specialized skills. However, most post-training
datasets used in leading open- and closed-source LLMs remain inaccessible to
the public, with limited information about their construction process. This
lack of transparency has motivated the recent development of open-source
post-training corpora. While training on these open alternatives can yield
performance comparable to that of leading models, systematic comparisons remain
challenging due to the significant computational cost of conducting them
rigorously at scale, and are therefore largely absent. As a result, it remains
unclear how specific samples, task types, or curation strategies influence
downstream performance when assessing data quality. In this work, we conduct
the first comprehensive side-by-side analysis of two prominent open
post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie
framework, we annotate each sample with detailed quality metrics, including
turn structure (single-turn vs. multi-turn), task category, input quality, and
response quality, and we derive statistics that reveal structural and
qualitative similarities and differences between the two datasets. Based on
these insights, we design a principled curation recipe that produces a new data
mixture, TuluTalk, which contains 14% fewer samples than either source dataset
while matching or exceeding their performance on key benchmarks. Our findings
offer actionable insights for constructing more effective post-training
datasets that improve model performance within practical resource limits. To
support future research, we publicly release both the annotated source datasets
and our curated TuluTalk mixture.

</details>


### [17] [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)
*Yijie Hao,Haofei Yu,Jiaxuan You*

Main category: cs.CL

TL;DR: This paper introduces the concept of Intent Hallucination in large language models, where the model either omits or misinterprets parts of a complex query. It presents FAITHQA, a new benchmark for evaluating this phenomenon, which includes over 20,000 problems across different topics and difficulties. The study evaluates several state-of-the-art LLMs on FAITHQA and finds that intent hallucination is widespread, often due to omissions or misinterpretations by the models. Additionally, the paper proposes CONSTRAINT SCORE, an automatic metric designed to detect intent hallucination, showing it aligns better with human evaluations than existing methods.


<details>
  <summary>Details</summary>
Motivation: Today’s large language models struggle with complex queries containing multiple conditions, often producing responses that only partially meet the query requirements. This leads to issues like neglecting certain conditions or responding to non-existent parts of the query.

Method: The authors introduce the concept of Intent Hallucination and create FAITHQA, a benchmark dataset with over 20,000 problems to systematically evaluate this issue in both query-only and retrieval-augmented generation setups.

Result: Intent hallucination is found to be a common problem even among top-performing LLMs, typically caused by omissions or misinterpretations. A new metric called CONSTRAINT SCORE is introduced to automatically detect intent hallucination, performing better than existing metrics in aligning with human evaluations.

Conclusion: Intent hallucination is a significant challenge for large language models when dealing with complex queries. The proposed FAITHQA benchmark and CONSTRAINT SCORE metric provide valuable tools for researchers to address this issue.

Abstract: When exposed to complex queries containing multiple conditions, today's large
language models (LLMs) tend to produce responses that only partially satisfy
the query while neglecting certain conditions. We therefore introduce the
concept of Intent Hallucination. In this phenomenon, LLMs either omit
(neglecting to address certain parts) or misinterpret (responding to invented
query parts) elements of the given query, leading to intent hallucinated
generation. To systematically evaluate intent hallucination, we introduce
FAITHQA, a novel benchmark for intent hallucination that contains 20,068
problems, covering both query-only and retrieval-augmented generation (RAG)
setups with varying topics and difficulty. FAITHQA is the first hallucination
benchmark that goes beyond factual verification, tailored to identify the
fundamental cause of intent hallucination. By evaluating various LLMs on
FAITHQA, we find that (1) intent hallucination is a common issue even for
state-of-the-art models, and (2) the phenomenon stems from omission or
misinterpretation of LLMs. To facilitate future research, we introduce an
automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting
intent hallucination. Human evaluation results demonstrate that CONSTRAINT
SCORE is closer to human performance for intent hallucination compared to
baselines.

</details>


### [18] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: Develop a dataset for personalized figure caption generation with multimodal figure profiles.


<details>
  <summary>Details</summary>
Motivation: Authors need to revise generic AI-generated captions to match their writing style and domain's style, highlighting the need for personalization.

Method: Introduce LaMP-Cap, a dataset providing not only figure images but also other related figures as a profile to characterize the context.

Result: Experiments show using profile information helps generate captions closer to the original author-written ones. Ablation studies reveal images in the profile are more helpful than figure-mentioning paragraphs.

Conclusion: Using multimodal profiles is advantageous over text-only ones for personalized figure caption generation.

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


### [19] [Precise Information Control in Long-Form Text Generation](https://arxiv.org/abs/2506.06589)
*Jacqueline He,Howard Yen,Margaret Li,Shuyue Stella Li,Zhiyuan Zeng,Weijia Shi,Yulia Tsvetkov,Danqi Chen,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.CL

TL;DR: This paper introduces Precise Information Control (PIC), a task to evaluate and improve the faithfulness of long-form text generation by ensuring models only use supported facts. It presents PIC-Bench, a benchmark showing state-of-the-art models still hallucinate significantly, and proposes a post-training framework to enhance models' PIC ability.


<details>
  <summary>Details</summary>
Motivation: To address intrinsic hallucination in modern language models, which generates plausible but unsubstantiated information.

Method: Introduces PIC, a task formulation requiring models to generate outputs based on verifiable claims without adding unsupported information. Develops PIC-Bench for evaluating models across multiple tasks and introduces a post-training framework to improve PIC ability.

Result: State-of-the-art LMs hallucinate in over 70% of outputs on PIC-Bench. The proposed framework improves F1 score from 69.1% to 91.0% in the full PIC setting. Integrating PIC-LM enhances exact match recall and factual precision.

Conclusion: The paper demonstrates the challenge of intrinsic hallucination in LM-generated texts and shows that enhancing precise information control can lead to more faithful generation results.

Abstract: A central challenge in modern language models (LMs) is intrinsic
hallucination: the generation of information that is plausible but
unsubstantiated relative to input context. To study this problem, we propose
Precise Information Control (PIC), a new task formulation that requires models
to generate long-form outputs grounded in a provided set of short
self-contained statements, known as verifiable claims, without adding any
unsupported ones. For comprehensiveness, PIC includes a full setting that tests
a model's ability to include exactly all input claims, and a partial setting
that requires the model to selectively incorporate only relevant claims. We
present PIC-Bench, a benchmark of eight long-form generation tasks (e.g.,
summarization, biography generation) adapted to the PIC setting, where LMs are
supplied with well-formed, verifiable input claims. Our evaluation of a range
of open and proprietary LMs on PIC-Bench reveals that, surprisingly,
state-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To
alleviate this lack of faithfulness, we introduce a post-training framework,
using a weakly supervised preference data construction method, to train an 8B
PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full
PIC setting. When integrated into end-to-end factual generation pipelines,
PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and
factual precision by 30.5% on a birthplace verification task, underscoring the
potential of precisely grounded generation.

</details>


### [20] [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)
*Xiao Wang,Mengjue Tan,Qiao Jin,Guangzhi Xiong,Yu Hu,Aidong Zhang,Zhiyong Lu,Minjia Zhang*

Main category: cs.CL

TL;DR: This paper introduces an end-to-end framework named \name for generating and evaluating citations using large language models (LLMs) in medical tasks. It also proposes a novel multi-pass retrieval-citation method that improves citation quality.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based medical question-answering systems lack citation generation and evaluation capabilities, which is a concern for practical use.

Method: An end-to-end framework named \name is introduced along with a novel multi-pass retrieval-citation method.

Result: The proposed method achieves better citation precision and recall compared to strong baseline methods and shows good correlation between evaluation results and expert annotations.

Conclusion: This work highlights the challenges and opportunities of citation generation for medical tasks and identifies important design choices affecting citation quality.

Abstract: Existing LLM-based medical question-answering systems lack citation
generation and evaluation capabilities, raising concerns about their adoption
in practice. In this work, we introduce \name, the first end-to-end framework
that facilitates the design and evaluation of citation generation with LLMs for
medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation
method that generates high-quality citations. Our evaluation highlights the
challenges and opportunities of citation generation for medical tasks, while
identifying important design choices that have a significant impact on the
final citation quality. Our proposed method achieves superior citation
precision and recall improvements compared to strong baseline methods, and we
show that evaluation results correlate well with annotation results from
professional experts.

</details>


### [21] [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
*Charles Goddard,Fernando Fernandes Neto*

Main category: cs.CL

TL;DR: 提出了一种无需训练的方法，通过正交匹配追踪（OMP）重建未见令牌嵌入来移植预训练大型语言模型中的标记器。在两个具有挑战性的跨标记器任务中，OMP在多个基准测试中实现了对基础模型性能的最佳零样本保留。


<details>
  <summary>Details</summary>
Motivation: 解决跨标记器任务中预训练大型语言模型性能下降的问题，特别是数学推理能力的保持。

Method: 通过正交匹配追踪（OMP）方法，首先在一个小的共享锚标记字典的空间中计算每个新标记的表示，然后将这些稀疏系数转移到基础模型的嵌入空间中。

Result: 在Llama→Mistral NeMo和Qwen→Llama这两个跨标记器任务上，OMP的表现优于其他零样本方法，并且比现有方法如WECHSEL、FOCUS、ZETT等更有效。

Conclusion: 这项技术使得可以重用预训练模型权重与新的标记器，促进跨标记器的知识蒸馏、推测解码、集成、合并以及特定领域的词汇适应。

Abstract: We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a sparse linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.

</details>


### [22] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
*Alan Chen,Jack Merullo,Alessandro Stolfo,Ellie Pavlick*

Main category: cs.CL

TL;DR: 我们证明了语言模型残差流之间的仿射映射是一种有效的特征转移方法，并展示了它在稀疏自动编码器上的应用，表明小模型和大模型具有相似的表示空间，且特征转移可以提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 我们发现小模型和大模型学习到的高度相似的表示空间，这激励我们在较小的模型上训练昂贵的组件如稀疏自动编码器，并将其转移到较大的模型上以节省计算量。

Method: 我们演示了语言模型残差流之间的仿射映射是一种有效转移表示特征的廉价方法。我们将这种技术应用于在不同大小的模型之间转移稀疏自动编码器（SAEs）的权重，并比较它们的表示。

Result: 我们发现小模型和大模型学习到的高度相似的表示空间，这激励我们在较小的模型上训练昂贵的组件如稀疏自动编码器，并将其转移到较大的模型上以节省计算量。例如，使用从小到大转移的稀疏自动编码器作为初始化可以使在较大模型上训练稀疏自动编码器的训练运行成本降低50%。接下来，我们展示了转移探针和转向向量可以有效地恢复真实性能。最后，我们深入探讨了特征级别的可转移性，发现语义和结构特征的转移方式不同，而特定类别的功能特征忠实地映射到它们的角色。

Conclusion: 我们发现小模型和大模型学习到的高度相似的表示空间，这激励我们在较小的模型上训练昂贵的组件如稀疏自动编码器，并将其转移到较大的模型上以节省计算量。此外，我们展示了转移探针和转向向量可以有效地恢复真实性能。最后，我们深入探讨了特征级别的可转移性，发现语义和结构特征的转移方式不同，而特定类别的功能特征忠实地映射到它们的角色。总的来说，我们的研究揭示了小模型和大模型在表示空间中的相似性和差异性，并展示了一种改进稀疏自动编码器训练效率的方法。

Abstract: In this work, we demonstrate that affine mappings between residual streams of
language models is a cheap way to effectively transfer represented features
between models. We apply this technique to transfer the weights of Sparse
Autoencoders (SAEs) between models of different sizes to compare their
representations. We find that small and large models learn highly similar
representation spaces, which motivates training expensive components like SAEs
on a smaller model and transferring to a larger model at a FLOPs savings. For
example, using a small-to-large transferred SAE as initialization can lead to
50% cheaper training runs when training SAEs on larger models. Next, we show
that transferred probes and steering vectors can effectively recover ground
truth performance. Finally, we dive deeper into feature-level transferability,
finding that semantic and structural features transfer noticeably differently
while specific classes of functional features have their roles faithfully
mapped. Overall, our findings illustrate similarities and differences in the
linear representation spaces of small and large models and demonstrate a method
for improving the training efficiency of SAEs.

</details>


### [23] [Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings](https://arxiv.org/abs/2506.06616)
*Samuel Kim,Oghenemaro Imieye,Yunting Yin*

Main category: cs.CL

TL;DR: Large language models show strong generalization in binary depression classification but struggle with fine-grained classifications. Supervised classifiers trained on summary embeddings from LLMs perform competitively or even better than traditional methods.


<details>
  <summary>Details</summary>
Motivation: To explore the performance of large language models and traditional machine learning classifiers in detecting depressive language in social media for early mental health intervention.

Method: Comparing zero-shot LLMs with supervised classifiers trained on conventional and LLM-generated summary embeddings across three social media classification tasks.

Result: Zero-shot LLMs excel in binary classification but face challenges with fine-grained ordinal classifications. Classifiers using LLM-generated summary embeddings outperform those using traditional text embeddings.

Conclusion: LLMs have potential in mental health prediction; their zero-shot capabilities and context-aware summarization can be better utilized.

Abstract: Accurate and interpretable detection of depressive language in social media
is useful for early interventions of mental health conditions, and has
important implications for both clinical practice and broader public health
efforts. In this paper, we investigate the performance of large language models
(LLMs) and traditional machine learning classifiers across three classification
tasks involving social media data: binary depression classification, depression
severity classification, and differential diagnosis classification among
depression, PTSD, and anxiety. Our study compares zero-shot LLMs with
supervised classifiers trained on both conventional text embeddings and
LLM-generated summary embeddings. Our experiments reveal that while zero-shot
LLMs demonstrate strong generalization capabilities in binary classification,
they struggle with fine-grained ordinal classifications. In contrast,
classifiers trained on summary embeddings generated by LLMs demonstrate
competitive, and in some cases superior, performance on the classification
tasks, particularly when compared to models using traditional text embeddings.
Our findings demonstrate the strengths of LLMs in mental health prediction, and
suggest promising directions for better utilization of their zero-shot
capabilities and context-aware summarization techniques.

</details>


### [24] [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)
*Jesse Woo,Fateme Hashemi Chaleshtori,Ana Marasović,Kenneth Marino*

Main category: cs.CL

TL;DR: This paper introduces BRIEFME, a new dataset focused on legal briefs with three tasks for language models: argument summarization, argument completion, and case retrieval. The study shows that current large language models are good at summarization and guided completion tasks but perform poorly on realistic argument completion and case retrieval.


<details>
  <summary>Details</summary>
Motivation: The paper aims to capture and evaluate legal skills in language models, focusing on the under-explored area of writing and editing legal briefs which requires understanding the law and making persuasive arguments.

Method: The paper describes the creation of three tasks in the BRIEFME dataset: argument summarization, argument completion, and case retrieval.

Result: Current large language models perform well in summarization and guided completion tasks but struggle with realistic argument completion and case retrieval.

Conclusion: The paper hopes that this dataset will encourage more development in Legal NLP to specifically aid people in performing legal work.

Abstract: A core part of legal work that has been under-explored in Legal NLP is the
writing and editing of legal briefs. This requires not only a thorough
understanding of the law of a jurisdiction, from judgments to statutes, but
also the ability to make new arguments to try to expand the law in a new
direction and make novel and creative arguments that are persuasive to judges.
To capture and evaluate these legal skills in language models, we introduce
BRIEFME, a new dataset focused on legal briefs. It contains three tasks for
language models to assist legal professionals in writing briefs: argument
summarization, argument completion, and case retrieval. In this work, we
describe the creation of these tasks, analyze them, and show how current models
perform. We see that today's large language models (LLMs) are already quite
good at the summarization and guided completion tasks, even beating
human-generated headings. Yet, they perform poorly on other tasks in our
benchmark: realistic argument completion and retrieving relevant legal cases.
We hope this dataset encourages more development in Legal NLP in ways that will
specifically aid people in performing legal work.

</details>


### [25] [Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations](https://arxiv.org/abs/2506.06626)
*Junzhe Wang,Bichen Wang,Xing Fu,Yixin Sun,Yanyan Zhao,Bing Qin*

Main category: cs.CL

TL;DR: This study introduces MusPsy-Dataset, a new dataset for multi-session psychological counseling conversations built from real client profiles. It also develops MusPsy-Model to track client progress and adjust counseling strategies over multiple sessions.


<details>
  <summary>Details</summary>
Motivation: Current research on automated psychological counseling focuses on single-session scenarios, which do not reflect real-world counseling processes that require sustained, multi-session engagement.

Method: Constructing MusPsy-Dataset using real client profiles from public psychological case reports and developing MusPsy-Model to track client progress and adjust counseling strategies.

Result: The experiments demonstrate that the proposed MusPsy-Model outperforms baseline models across multiple sessions.

Conclusion: This work addresses the limitation of single-session counseling by introducing a multi-session dataset and model, which can be used to improve the effectiveness of automated psychological counseling.

Abstract: In recent years, Large Language Models (LLMs) have made significant progress
in automated psychological counseling. However, current research focuses on
single-session counseling, which doesn't represent real-world scenarios. In
practice, psychological counseling is a process, not a one-time event,
requiring sustained, multi-session engagement to progressively address clients'
issues. To overcome this limitation, we introduce a dataset for Multi-Session
Psychological Counseling Conversation Dataset (MusPsy-Dataset). Our
MusPsy-Dataset is constructed using real client profiles from publicly
available psychological case reports. It captures the dynamic arc of
counseling, encompassing multiple progressive counseling conversations from the
same client across different sessions. Leveraging our dataset, we also
developed our MusPsy-Model, which aims to track client progress and adapt its
counseling direction over time. Experiments show that our model performs better
than baseline models across multiple sessions.

</details>


### [26] [SafeLawBench: Towards Safe Alignment of Large Language Models](https://arxiv.org/abs/2506.06636)
*Chuxue Cao,Han Zhu,Jiaming Ji,Qichao Sun,Zhenghao Zhu,Yinyu Wu,Juntao Dai,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 本文提出了SafeLawBench基准，从法律角度评估LLMs的安全性，发现即使是顶级模型在该基准上的表现也不尽如人意，呼吁社区重视LLMs的安全研究。


<details>
  <summary>Details</summary>
Motivation: 由于现有安全基准的主观性，缺乏评估LLMs安全性的明确标准，因此从法律角度探索LLMs的安全评估。

Method: 从法律视角提出SafeLawBench基准，将安全风险分为三个等级，并包含24,860个多选题和1,106个开放域问答任务。通过零样本和少样本提示评估了2个闭源LLMs和18个开源LLMs的安全特性，同时评估了它们的安全相关推理稳定性和拒绝行为。

Result: SafeLawBench基准提供了系统且全面的评估框架；不同LLMs在该基准上的表现各异，多数投票机制可以提高模型性能。

Conclusion: 即使像Claude-3.5-Sonnet和GPT-4o这样的领先SOTA模型，在SafeLawBench的多选任务中的准确率也未超过80.5%，20个LLMs的平均准确率为68.8%，这表明LLMs在安全评估方面仍有提升空间。

Abstract: With the growing prevalence of large language models (LLMs), the safety of
LLMs has raised significant concerns. However, there is still a lack of
definitive standards for evaluating their safety due to the subjective nature
of current safety benchmarks. To address this gap, we conducted the first
exploration of LLMs' safety evaluation from a legal perspective by proposing
the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three
levels based on legal standards, providing a systematic and comprehensive
framework for evaluation. It comprises 24,860 multi-choice questions and 1,106
open-domain question-answering (QA) tasks. Our evaluation included 2
closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot
prompting, highlighting the safety features of each model. We also evaluated
the LLMs' safety-related reasoning stability and refusal behavior.
Additionally, we found that a majority voting mechanism can enhance model
performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and
GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench,
while the average accuracy of 20 LLMs remains at 68.8\%. We urge the community
to prioritize research on the safety of LLMs.

</details>


### [27] [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)
*Nikhita Vedula,Dushyanta Dhyani,Laleh Jalali,Boris Oreshkin,Mohsen Bayati,Shervin Malmasi*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) have shown potential in regression tasks, but current methods mainly focus on point estimates and lack systematic comparison. This paper investigates probabilistic regression using LLMs for text-to-distribution prediction tasks like price estimation. A novel quantile regression approach is proposed to enable LLMs to produce full predictive distributions, which improves over traditional point estimates. Experiments show that a Mistral-7B model fine-tuned with quantile heads outperforms traditional approaches for both point and distributional estimations. The study systematically compares LLM approaches, model architectures, training approaches, and data scaling, revealing that Mistral-7B consistently outperforms other architectures. Additionally, LLM-assisted label correction achieves human-level accuracy without systematic bias.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenging text-to-distribution prediction tasks like price estimation where both nuanced text understanding and uncertainty quantification are critical. Current approaches mainly focus on point estimates and lack systematic comparison.

Method: A novel quantile regression approach is proposed to enable LLMs to produce full predictive distributions. Extensive experiments are conducted across three diverse price prediction datasets to evaluate the performance of a Mistral-7B model fine-tuned with quantile heads compared to traditional approaches.

Result: The Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations, as measured by three established metrics each for prediction accuracy and distributional calibration. Mistral-7B consistently outperforms encoder architectures, embedding-based methods, and few-shot learning methods. LLM-assisted label correction achieves human-level accuracy without systematic bias.

Conclusion: This paper demonstrates the effectiveness of using LLMs for probabilistic regression tasks, particularly in producing full predictive distributions through the novel quantile regression approach. It also highlights the importance of systematic comparison across different methods and reveals the effectiveness of LLM-assisted label correction.

Abstract: Large Language Models (LLMs) have shown promise in structured prediction
tasks, including regression, but existing approaches primarily focus on point
estimates and lack systematic comparison across different methods. We
investigate probabilistic regression using LLMs for unstructured inputs,
addressing challenging text-to-distribution prediction tasks such as price
estimation where both nuanced text understanding and uncertainty quantification
are critical. We propose a novel quantile regression approach that enables LLMs
to produce full predictive distributions, improving upon traditional point
estimates. Through extensive experiments across three diverse price prediction
datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads
significantly outperforms traditional approaches for both point and
distributional estimations, as measured by three established metrics each for
prediction accuracy and distributional calibration. Our systematic comparison
of LLM approaches, model architectures, training approaches, and data scaling
reveals that Mistral-7B consistently outperforms encoder architectures,
embedding-based methods, and few-shot learning methods. Our experiments also
reveal the effectiveness of LLM-assisted label correction in achieving
human-level accuracy without systematic bias. Our curated datasets are made
available at https://github.com/vnik18/llm-price-quantile-reg/ to support
future research.

</details>


### [28] [Learning Distribution-Wise Control in Representation Space for Language Models](https://arxiv.org/abs/2506.06686)
*Chunyuan Deng,Ruidi Chang,Hanjie Chen*

Main category: cs.CL

TL;DR: This work extends learnable interventions in language models to the distribution level, demonstrating improved controllability and robustness across various reasoning benchmarks compared to pointwise interventions.


<details>
  <summary>Details</summary>
Motivation: To provide a more comprehensive method for steering model behavior and enabling finer-grained control over language models.

Method: Extending learnable interventions to the distribution level to allow learning both pointwise transformations and surrounding regions of the concept subspace.

Result: Distribution-wise interventions outperformed pointwise interventions in controllability and robustness across eight commonsense reasoning and seven arithmetic reasoning benchmarks.

Conclusion: Distribution-wise interventions offer a more comprehensive approach for controlling language model behavior with finer granularity.

Abstract: Interventions in language models (LMs) are applied strategically to steer
model behavior during the forward pass. Learnable interventions, also known as
representation fine-tuning, aim to apply pointwise control within the concept
subspace and have proven effective in altering high-level behaviors. In this
work, we extend this approach to the distribution level, enabling the model to
learn not only pointwise transformations but also the surrounding regions of
the concept subspace. We demonstrate that these methods perform effectively in
early layers, with larger standard deviations correlating strongly with
improved performance. Across eight commonsense reasoning and seven arithmetic
reasoning benchmarks, our distribution-wise interventions consistently
outperform pointwise interventions in controllability and robustness. These
results illustrate that distribution-wise interventions provide a more
comprehensive method for steering model behavior and enabling finer-grained
control over language models. The code is at:
\href{https://github.com/chili-lab/D-Intervention}{https://github.com/chili-lab/D-Intervention}.

</details>


### [29] [Dynamic and Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2506.06704)
*Weihang Su,Qingyao Ai,Jingtao Zhan,Qian Dong,Yiqun Liu*

Main category: cs.CL

TL;DR: RAG系统通过检索增强生成能力，但传统的静态检索-然后生成管道可能对需要多跳推理等复杂任务来说不够理想。动态RAG和参数RAG是两个新兴的研究方向，前者在生成过程中动态确定检索内容，后者则改进了知识注入方式。此教程综述了这些领域的最新进展，并提供了理论基础和实用见解。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统的静态检索和上下文知识注入在面对复杂任务时表现不佳，因此研究转向动态RAG和参数RAG。

Method: 讨论了动态RAG和参数RAG两种方法，前者让大型语言模型在生成过程中实时适应信息需求，后者则通过参数级知识注入提高效率和效果。

Result: 综述了动态RAG和参数RAG的最新研究成果，并提供了理论支持和实践指导。

Conclusion: 动态RAG和参数RAG为RAG系统提供了新的可能性，推动了其在复杂任务中的应用和发展。

Abstract: Retrieval-Augmented Generation (RAG) has become a foundational paradigm for
equipping large language models (LLMs) with external knowledge, playing a
critical role in information retrieval and knowledge-intensive applications.
However, conventional RAG systems typically adopt a static
retrieve-then-generate pipeline and rely on in-context knowledge injection,
which can be suboptimal for complex tasks that require multihop reasoning,
adaptive information access, and deeper integration of external knowledge.
Motivated by these limitations, the research community has moved beyond static
retrieval and in-context knowledge injection. Among the emerging directions,
this tutorial delves into two rapidly growing and complementary research areas
on RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when
and what to retrieve during the LLM's generation process, enabling real-time
adaptation to the LLM's evolving information needs. Parametric RAG rethinks how
retrieved knowledge should be injected into LLMs, transitioning from
input-level to parameter-level knowledge injection for enhanced efficiency and
effectiveness. This tutorial offers a comprehensive overview of recent advances
in these emerging research areas. It also shares theoretical foundations and
practical insights to support and inspire further research in RAG.

</details>


### [30] [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)
*Zhihui Chen,Kai He,Yucheng Huang,Yunxiao Zhu,Mengling Feng*

Main category: cs.CL

TL;DR: 提出了一种新的框架DivScore，用于在医学和法律等专业领域检测LLM生成的文本，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 检测医学和法律等专业和高风险领域的LLM生成文本对于打击错误信息和确保真实性至关重要。然而，现有的零样本检测器虽然对普通文本有效，但通常由于领域转移而在应用到专门内容时失败。

Method: 提出了一种名为DivScore的新框架，该框架使用基于归一化熵的评分和领域知识蒸馏来稳健地识别LLM生成的文本。

Result: 实验表明，DivScore在我们的基准测试中始终优于最先进的检测器，AUROC提高了14.4％，召回率提高了64.0％（0.1％假阳性率阈值）。在对抗性设置中，DivScore比其他基线表现出了平均22.8％的AUROC优势和29.5％的召回率优势。

Conclusion: DivScore在专门领域表现出色，其性能优于最先进的检测器，并且在对抗性设置中也显示出优越的鲁棒性。

Abstract: Detecting LLM-generated text in specialized and high-stakes domains like
medicine and law is crucial for combating misinformation and ensuring
authenticity. However, current zero-shot detectors, while effective on general
text, often fail when applied to specialized content due to domain shift. We
provide a theoretical analysis showing this failure is fundamentally linked to
the KL divergence between human, detector, and source text distributions. To
address this, we propose DivScore, a zero-shot detection framework using
normalized entropy-based scoring and domain knowledge distillation to robustly
identify LLM-generated text in specialized domains. We also release a
domain-specific benchmark for LLM-generated text detection in the medical and
legal domains. Experiments on our benchmark show that DivScore consistently
outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%
higher recall (0.1% false positive rate threshold). In adversarial settings,
DivScore demonstrates superior robustness than other baselines, achieving on
average 22.8% advantage in AUROC and 29.5% in recall. Code and data are
publicly available.

</details>


### [31] [A Survey of Retentive Network](https://arxiv.org/abs/2506.06708)
*Haiqi Yang,Zhiyuan Li,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: First detailed survey of Retentive Network (RetNet), which combines recurrence and attention mechanisms for more efficient neural network architectures.


<details>
  <summary>Details</summary>
Motivation: To offer an efficient alternative to Transformer models, addressing their high memory costs and limited scalability with long sequences

Method: Unified retention mechanism combining recurrence and global dependency modeling of attention

Result: Achieves robust performance across multiple domains like NLP, speech recognition, and time-series analysis

Conclusion: This paper provides the first comprehensive survey of the Retentive Network (RetNet), detailing its architecture, innovations, and applications. It addresses the lack of such reviews in current literature and explores challenges and future research directions for RetNet.

Abstract: Retentive Network (RetNet) represents a significant advancement in neural
network architecture, offering an efficient alternative to the Transformer.
While Transformers rely on self-attention to model dependencies, they suffer
from high memory costs and limited scalability when handling long sequences due
to their quadratic complexity. To mitigate these limitations, RetNet introduces
a retention mechanism that unifies the inductive bias of recurrence with the
global dependency modeling of attention. This mechanism enables linear-time
inference, facilitates efficient modeling of extended contexts, and remains
compatible with fully parallelizable training pipelines. RetNet has garnered
significant research interest due to its consistently demonstrated cross-domain
effectiveness, achieving robust performance across machine learning paradigms
including natural language processing, speech recognition, and time-series
analysis. However, a comprehensive review of RetNet is still missing from the
current literature. This paper aims to fill that gap by offering the first
detailed survey of the RetNet architecture, its key innovations, and its
diverse applications. We also explore the main challenges associated with
RetNet and propose future research directions to support its continued
advancement in both academic research and practical deployment.

</details>


### [32] [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)
*Qi Shi,Qiwei Han,Cláudia Soares*

Main category: cs.CL

TL;DR: C-PATH is a conversational AI system that helps patients find the right medical department using natural dialogue and advanced fine-tuning techniques.


<details>
  <summary>Details</summary>
Motivation: The complexity and confusion of navigating healthcare systems create barriers for patients trying to get timely medical care.

Method: C-PATH uses large language models, is fine-tuned on various data types, and employs a GPT-based data augmentation framework to align with patient communication norms while managing conversation history for coherence.

Result: Evaluation shows strong performance in clarity, informativeness, and recommendation accuracy, outperforming domain-specific baselines.

Conclusion: C-PATH advances the creation of user-friendly, accessible, and precise AI tools for digital health assistance and triage.

Abstract: Navigating healthcare systems can be complex and overwhelming, creating
barriers for patients seeking timely and appropriate medical attention. In this
paper, we introduce C-PATH (Conversational Patient Assistance and Triage in
Healthcare), a novel conversational AI system powered by large language models
(LLMs) designed to assist patients in recognizing symptoms and recommending
appropriate medical departments through natural, multi-turn dialogues. C-PATH
is fine-tuned on medical knowledge, dialogue data, and clinical summaries using
a multi-stage pipeline built on the LLaMA3 architecture. A core contribution of
this work is a GPT-based data augmentation framework that transforms structured
clinical knowledge from DDXPlus into lay-person-friendly conversations,
allowing alignment with patient communication norms. We also implement a
scalable conversation history management strategy to ensure long-range
coherence. Evaluation with GPTScore demonstrates strong performance across
dimensions such as clarity, informativeness, and recommendation accuracy.
Quantitative benchmarks show that C-PATH achieves superior performance in
GPT-rewritten conversational datasets, significantly outperforming
domain-specific baselines. C-PATH represents a step forward in the development
of user-centric, accessible, and accurate AI tools for digital health
assistance and triage.

</details>


### [33] [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)
*Mikhail Salnikov,Dmitrii Korzh,Ivan Lazichny,Elvir Karimov,Artyom Iudin,Ivan Oseledets,Oleg Y. Rogov,Alexander Panchenko,Natalia Loukachevitch,Elena Tutubalina*

Main category: cs.CL

TL;DR: This paper examines geopolitical biases in large language models (LLMs) regarding how they interpret historical events with conflicting national perspectives from USA, UK, USSR, and China. A new dataset was created containing neutral event descriptions and differing national viewpoints. The study found substantial biases favoring certain national narratives, and while basic debiasing prompts had little impact, experiments showed models could be sensitive to label attributions.


<details>
  <summary>Details</summary>
Motivation: To evaluate geopolitical biases in LLMs concerning historical events with conflicting national perspectives and understand the effectiveness of debiasing methods.

Method: Introduced a novel dataset with neutral event descriptions and contrasting viewpoints from different countries, tested LLMs for biases, and experimented with debiasing prompts as well as manipulated participant labels.

Result: Significant geopolitical biases were found in models favoring specific national narratives. Simple debiasing prompts had limited effect, and experiments showed models' sensitivity to label attributions, sometimes amplifying biases or detecting inconsistencies.

Conclusion: This work highlights national narrative biases in LLMs, challenges the effectiveness of simple debiasing methods, and provides a framework and dataset for future geopolitical bias research.

Abstract: This paper evaluates geopolitical biases in LLMs with respect to various
countries though an analysis of their interpretation of historical events with
conflicting national perspectives (USA, UK, USSR, and China). We introduce a
novel dataset with neutral event descriptions and contrasting viewpoints from
different countries. Our findings show significant geopolitical biases, with
models favoring specific national narratives. Additionally, simple debiasing
prompts had a limited effect in reducing these biases. Experiments with
manipulated participant labels reveal models' sensitivity to attribution,
sometimes amplifying biases or recognizing inconsistencies, especially with
swapped labels. This work highlights national narrative biases in LLMs,
challenges the effectiveness of simple debiasing methods, and offers a
framework and dataset for future geopolitical bias research.

</details>


### [34] [They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse](https://arxiv.org/abs/2506.06775)
*Walter Paci,Alessandro Panunzi,Sandro Pezzelle*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) struggle to interpret presuppositions and implicatures in political discourse.


<details>
  <summary>Details</summary>
Motivation: Detecting and explaining the meaning of implicit content in political discourse is crucial but challenging.

Method: Multiple-choice task and open-ended generation task were used to test the effectiveness of LLMs.

Result: All tested models struggled to interpret presuppositions and implicatures.

Conclusion: Current LLMs lack the key pragmatic capabilities necessary for accurately interpreting highly implicit language in political discourse.

Abstract: Implicit content plays a crucial role in political discourse, where speakers
systematically employ pragmatic strategies such as implicatures and
presuppositions to influence their audiences. Large Language Models (LLMs) have
demonstrated strong performance in tasks requiring complex semantic and
pragmatic understanding, highlighting their potential for detecting and
explaining the meaning of implicit content. However, their ability to do this
within political discourse remains largely underexplored. Leveraging, for the
first time, the large IMPAQTS corpus, which comprises Italian political
speeches with the annotation of manipulative implicit content, we propose
methods to test the effectiveness of LLMs in this challenging problem. Through
a multiple-choice task and an open-ended generation task, we demonstrate that
all tested models struggle to interpret presuppositions and implicatures. We
conclude that current LLMs lack the key pragmatic capabilities necessary for
accurately interpreting highly implicit language, such as that found in
political discourse. At the same time, we highlight promising trends and future
directions for enhancing model performance. We release our data and code at
https://github.com/WalterPaci/IMPAQTS-PID

</details>


### [35] [Extending dependencies to the taggedPBC: Word order in transitive clauses](https://arxiv.org/abs/2506.06785)
*Hiram Ring*

Main category: cs.CL

TL;DR: taggedPBC has over 1800 sentences of pos-tagged parallel text data from over 1500 languages. A CoNLLU-formatted version of the dataset is created to transfer dependency information along with POS tags to all languages. Word order information from this dataset correlates with expert determinations.


<details>
  <summary>Details</summary>
Motivation: To provide more accurate crosslinguistic insights by transferring dependency information along with POS tags to all languages in the taggedPBC.

Method: Creating a CoNLLU-formatted version of the dataset.

Result: Word order information from the dataset correlates with expert determinations in three typological databases.

Conclusion: Dependency-annotated corpora are made available for research and collaboration via GitHub.

Abstract: The taggedPBC (Ring 2025a) contains more than 1,800 sentences of pos-tagged
parallel text data from over 1,500 languages, representing 133 language
families and 111 isolates. While this dwarfs previously available resources,
and the POS tags achieve decent accuracy, allowing for predictive
crosslinguistic insights (Ring 2025b), the dataset was not initially annotated
for dependencies. This paper reports on a CoNLLU-formatted version of the
dataset which transfers dependency information along with POS tags to all
languages in the taggedPBC. Although there are various concerns regarding the
quality of the tags and the dependencies, word order information derived from
this dataset regarding the position of arguments and predicates in transitive
clauses correlates with expert determinations of word order in three
typological databases (WALS, Grambank, Autotyp). This highlights the usefulness
of corpus-based typological approaches (as per Baylor et al. 2023; Bjerva 2024)
for extending comparisons of discrete linguistic categories, and suggests that
important insights can be gained even from noisy data, given sufficient
annotation. The dependency-annotated corpora are also made available for
research and collaboration via GitHub.

</details>


### [36] [On the Adaptive Psychological Persuasion of Large Language Models](https://arxiv.org/abs/2506.06800)
*Tianjie Ju,Yujia Chen,Hao Fei,Mong-Li Lee,Wynne Hsu,Pengzhou Cheng,Zongru Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: This paper investigates large language models' ability to persuade and resist persuasion in adversarial dialogues using psychological rhetoric strategies. It finds that explicit instruction on certain strategies improves persuasion success rates, but no single strategy works universally. An adaptive framework is proposed to help LLMs autonomously choose the best strategies, which enhances their success rates.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities of large language models in autonomous persuasion and resistance to persuasion in psychological rhetoric contexts.

Method: Tasks were given to four common LLMs to act as persuaders and listeners in adversarial dialogues. Eleven psychological persuasion strategies were introduced and tested. An adaptive framework was proposed and experimented on three open-source LLMs.

Result: LLMs mostly use repetitive strategies with low success rates. Explicit instructions on specific strategies like Fluency Effect and Repetition Effect improve success rates. No universal strategy exists, performance depends on context. The adaptive framework helps LLMs select optimal strategies, improving success rates while maintaining general capabilities.

Conclusion: The study shows that large language models can be trained to improve their persuasive abilities through adaptive methods, though the effectiveness varies with context.

Abstract: Previous work has showcased the intriguing capabilities of Large Language
Models (LLMs) in instruction-following and rhetorical fluency. However,
systematic exploration of their dual capabilities to autonomously persuade and
resist persuasion, particularly in contexts involving psychological rhetoric,
remains unexplored. In this paper, we first evaluate four commonly adopted LLMs
by tasking them to alternately act as persuaders and listeners in adversarial
dialogues. Empirical results show that persuader LLMs predominantly employ
repetitive strategies, leading to low success rates. Then we introduce eleven
comprehensive psychological persuasion strategies, finding that explicitly
instructing LLMs to adopt specific strategies such as Fluency Effect and
Repetition Effect significantly improves persuasion success rates. However, no
``one-size-fits-all'' strategy proves universally effective, with performance
heavily dependent on contextual counterfactuals. Motivated by these
observations, we propose an adaptive framework based on direct preference
optimization that trains LLMs to autonomously select optimal strategies by
leveraging persuasion results from strategy-specific responses as preference
pairs. Experiments on three open-source LLMs confirm that the proposed adaptive
psychological persuasion method effectively enables persuader LLMs to select
optimal strategies, significantly enhancing their success rates while
maintaining general capabilities. Our code is available at
https://github.com/KalinaEine/PsychologicalPersuasion.

</details>


### [37] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
*Subhendu Khatuya,Shashwat Naidu,Saptarshi Ghosh,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 提出了一种名为LAGAMC的新型生成模型框架，用于多标签文本分类任务，该模型通过生成和匹配预定义标签描述来实现高精度分类，并在多个数据集上超越了现有的最先进方法。


<details>
  <summary>Details</summary>
Motivation: 随着文本数据的爆炸式增长，手动文档分类变得越来越具有挑战性。因此，研究者们需要开发一种鲁棒、高效且领域无关的生成模型框架来解决这一问题。

Method: LAGAMC框架通过生成预定义标签描述来利用这些描述，并使用微调后的句子转换器匹配生成的描述和预定义标签。此外，该模型结合了交叉熵损失和生成句子与预定义目标描述之间的余弦相似度的双重目标损失函数，以确保语义对齐和准确性。

Result: LAGAMC模型在多个数据集上的实验表明其具有出色的性能，特别是在Micro-F1和Macro-F1指标上取得了显著提高。

Conclusion: LAGAMC模型在多标签文本分类任务上表现出色，实现了新的最先进的性能，并且在所有评估的数据集上都超过了几个强大的基线。具体来说，与最近的基线相比，我们在Micro-F1和Macro-F1指标上分别提高了13.94％和24.85％。

Abstract: The explosion of textual data has made manual document classification
increasingly challenging. To address this, we introduce a robust, efficient
domain-agnostic generative model framework for multi-label text classification.
Instead of treating labels as mere atomic symbols, our approach utilizes
predefined label descriptions and is trained to generate these descriptions
based on the input text. During inference, the generated descriptions are
matched to the pre-defined labels using a finetuned sentence transformer. We
integrate this with a dual-objective loss function, combining cross-entropy
loss and cosine similarity of the generated sentences with the predefined
target descriptions, ensuring both semantic alignment and accuracy. Our
proposed model LAGAMC stands out for its parameter efficiency and versatility
across diverse datasets, making it well-suited for practical applications. We
demonstrate the effectiveness of our proposed model by achieving new
state-of-the-art performances across all evaluated datasets, surpassing several
strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in
Macro-F1 compared to the closest baseline across all datasets.

</details>


### [38] [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)
*James A. Michaelov,Reeka Estacio,Zhien Zhang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: Language models fail to distinguish possible events from impossible ones, performing worse than chance.


<details>
  <summary>Details</summary>
Motivation: To evaluate if language models can reliably predict the likelihood of possible events compared to improbable ones.

Method: Testing models with sentences of varying degrees of possibility, typicality, and contextual relatedness.

Result: Models like Llama 3, Gemma 2, and Mistral NeMo assign higher probabilities to impossible sentences than unlikely ones.

Conclusion: Language models struggle to differentiate between possible and impossible sentences, performing worse than chance under certain conditions.

Abstract: Can language models reliably predict that possible events are more likely
than merely improbable ones? By teasing apart possibility, typicality, and
contextual relatedness, we show that despite the results of previous work,
language models' ability to do this is far from robust. In fact, under certain
conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -
perform at worse-than-chance level, assigning higher probabilities to
impossible sentences such as 'the car was given a parking ticket by the brake'
than to merely unlikely sentences such as 'the car was given a parking ticket
by the explorer'.

</details>


### [39] [Advancing Question Generation with Joint Narrative and Difficulty Control](https://arxiv.org/abs/2506.06812)
*Bernardo Leite,Henrique Lopes Cardoso*

Main category: cs.CL

TL;DR: This paper proposes a strategy for Joint Narrative and Difficulty Control in Question Generation for reading comprehension tailored to educational purposes.


<details>
  <summary>Details</summary>
Motivation: To combine narrative and difficulty control in Question Generation for educational purposes.

Method: Proposing a strategy for Joint Narrative and Difficulty Control.

Result: The approach is feasible but not effective across all instances.

Conclusion: The findings highlight the conditions under which the strategy performs well and discuss the trade-offs associated with its application.

Abstract: Question Generation (QG), the task of automatically generating questions from
a source input, has seen significant progress in recent years.
Difficulty-controllable QG (DCQG) enables control over the difficulty level of
generated questions while considering the learner's ability. Additionally,
narrative-controllable QG (NCQG) allows control over the narrative aspects
embedded in the questions. However, research in QG lacks a focus on combining
these two types of control, which is important for generating questions
tailored to educational purposes. To address this gap, we propose a strategy
for Joint Narrative and Difficulty Control, enabling simultaneous control over
these two attributes in the generation of reading comprehension questions. Our
evaluation provides preliminary evidence that this approach is feasible, though
it is not effective across all instances. Our findings highlight the conditions
under which the strategy performs well and discuss the trade-offs associated
with its application.

</details>


### [40] [BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities](https://arxiv.org/abs/2506.06813)
*Dipto Das,Syed Ishtiaque Ahmed,Shion Guha*

Main category: cs.CL

TL;DR: 开发了一个孟加拉语政治话语数据集并概述其特点。


<details>
  <summary>Details</summary>
Motivation: 研究主要关注在线空间中的政治话语分析，特别是在公共舆论和意识形态极化方面。由于缺乏数据集，针对像孟加拉语这样的重要但资源匮乏的语言的研究努力显著不足。

Method: 创建了一个包含孟加拉语跨国政治话语的多语言数据集，并通过基于社区的关键词检索进行了手工整理。

Result: 收集了来自三个在线平台的数据集，每个平台都代表不同的社区结构和互动动态。此外，还提供了数据集主题和多语言内容的一般概述。

Conclusion: 本研究填补了孟加拉语在社会计算和计算语言学领域研究的空白，为未来的研究提供了重要的数据支持。

Abstract: Understanding political discourse in online spaces is crucial for analyzing
public opinion and ideological polarization. While social computing and
computational linguistics have explored such discussions in English, such
research efforts are significantly limited in major yet under-resourced
languages like Bengali due to the unavailability of datasets. In this paper, we
present a multilingual dataset of Bengali transnational political discourse
(BTPD) collected from three online platforms, each representing distinct
community structures and interaction dynamics. Besides describing how we
hand-curated the dataset through community-informed keyword-based retrieval,
this paper also provides a general overview of its topics and multilingual
content.

</details>


### [41] [How do datasets, developers, and models affect biases in a low-resourced language?](https://arxiv.org/abs/2506.06816)
*Dipto Das,Shion Guha,Bryan Semaan*

Main category: cs.CL

TL;DR: 研究了低资源语言（孟加拉语）中身份偏见的影响，并通过算法审计发现情感分析模型在不同身份类别上表现出偏见。


<details>
  <summary>Details</summary>
Motivation: 研究了身份偏见在低资源语言中的影响，特别是孟加拉语中基于性别、宗教和国籍的身份偏见。

Method: 对基于mBERT和BanglaBERT的情感分析模型进行了算法审计，这些模型使用了Google Dataset Search中的所有孟加拉语情感分析数据集进行微调。

Result: 尽管具有相似的语义内容和结构，但情感分析模型在不同身份类别上表现出偏见，并且存在由于结合来自不同人口背景的预训练模型和数据集而产生的不一致性和不确定性。

Conclusion: 研究结果连接到关于认识论不公正、AI对齐和算法审计方法论决策的更广泛的讨论。

Abstract: Sociotechnical systems, such as language technologies, frequently exhibit
identity-based biases. These biases exacerbate the experiences of historically
marginalized communities and remain understudied in low-resource contexts.
While models and datasets specific to a language or with multilingual support
are commonly recommended to address these biases, this paper empirically tests
the effectiveness of such approaches in the context of gender, religion, and
nationality-based identities in Bengali, a widely spoken but low-resourced
language. We conducted an algorithmic audit of sentiment analysis models built
on mBERT and BanglaBERT, which were fine-tuned using all Bengali sentiment
analysis (BSA) datasets from Google Dataset Search. Our analyses showed that
BSA models exhibit biases across different identity categories despite having
similar semantic content and structure. We also examined the inconsistencies
and uncertainties arising from combining pre-trained models and datasets
created by individuals from diverse demographic backgrounds. We connected these
findings to the broader discussions on epistemic injustice, AI alignment, and
methodological decisions in algorithmic audits.

</details>


### [42] [Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs](https://arxiv.org/abs/2506.06820)
*Wenyu Zhang,Yingxu He,Geyu Lin,Zhuohan Liu,Shuo Sun,Bin Wang,Xunlong Zou,Jeremy H. M. Wong,Qiongqiong Wang,Hardik B. Sailor,Nancy F. Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: This work introduces a new approach for emotion reasoning in AudioLLMs, which improves emotion prediction accuracy and response coherence.


<details>
  <summary>Details</summary>
Motivation: Existing methods for emotion understanding in AudioLLMs are limited and provide little insight into their predictions.

Method: A unified framework combining reasoning-augmented data supervision, dual-encoder architecture, and task-alternating training is introduced to enhance emotion recognition.

Result: Experiments on IEMOCAP and MELD show improved emotion prediction accuracy and enhanced coherence and evidential grounding of generated responses.

Conclusion: The proposed approach demonstrates the potential of incorporating emotional reasoning in AudioLLMs for better performance in emotion recognition.

Abstract: Audio Large Language Models (AudioLLMs) have achieved strong results in
semantic tasks like speech recognition and translation, but remain limited in
modeling paralinguistic cues such as emotion. Existing approaches often treat
emotion understanding as a classification problem, offering little insight into
the underlying rationale behind predictions. In this work, we explore emotion
reasoning, a strategy that leverages the generative capabilities of AudioLLMs
to enhance emotion recognition by producing semantically aligned,
evidence-grounded explanations. To support this in multitask AudioLLMs, we
introduce a unified framework combining reasoning-augmented data supervision,
dual-encoder architecture, and task-alternating training. This approach enables
AudioLLMs to effectively learn different tasks while incorporating emotional
reasoning. Experiments on IEMOCAP and MELD show that our approach not only
improves emotion prediction accuracy but also enhances the coherence and
evidential grounding of the generated responses.

</details>


### [43] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
*Yuhan Cao,Zian Chen,Kun Quan,Ziliang Zhang,Yu Wang,Xiaoning Dong,Yeqi Feng,Guanzhong He,Jingcheng Huang,Jianhao Li,Yixuan Tan,Jiafu Tang,Yilin Tang,Junlei Wu,Qianyu Xiao,Can Zheng,Shouchen Zhou,Yuxiang Zhu,Yiming Huang,Tian Xie,Tianxing He*

Main category: cs.CL

TL;DR: Investigating the use of Large Language Models (LLMs) for code checking and debugging via test case generation in competitive programming.


<details>
  <summary>Details</summary>
Motivation: Exploring the potential of LLMs in code checking and debugging through test case generation, particularly in competitive programming.

Method: Proposing TCGBench, a benchmark for LLM generation of test case generators, consisting of two tasks: generating valid test case generators for a given problem and generating targeted test case generators to expose bugs in human-written code.

Result: State-of-the-art LLMs can generate valid test case generators but struggle with generating targeted test cases that reveal flaws in human code. Advanced reasoning models also underperform compared to humans in generating targeted generators.

Conclusion: Performance of LLMs in generating targeted test case generators can be improved with a high-quality, manually curated dataset through prompting and fine-tuning.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [44] [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)
*Arkadiusz Modzelewski,Witold Sosnowski,Tiziano Labruna,Adam Wierzbicki,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 提出了一种新的方法PCoT，通过利用说服知识增强大型语言模型的虚假信息检测能力，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高人们对虚假信息的识别能力，借鉴了心理学研究中关于说服性谬误的知识有助于个人检测虚假信息的发现。

Method: 引入了Persuasion-Augmented Chain of Thought (PCoT)，这是一种利用说服来改善零样本分类中的虚假信息检测的新方法。

Result: 在在线新闻和社交媒体帖子上广泛评估了PCoT，并发布了两个新的、最新的虚假信息数据集：EUDisinfo和MultiDis。这些数据集使我们能够在完全未见过实验中使用的LLMs内容的情况下评估PCoT，因为这些内容是在模型的知识截止日期之后发布的。我们展示了，平均而言，PCoT在五种LLMs和五个数据集上比竞争方法高出15%。

Conclusion: 这些发现突显了说服力在加强零样本虚假信息检测方面的价值。

Abstract: Disinformation detection is a key aspect of media literacy. Psychological
studies have shown that knowledge of persuasive fallacies helps individuals
detect disinformation. Inspired by these findings, we experimented with large
language models (LLMs) to test whether infusing persuasion knowledge enhances
disinformation detection. As a result, we introduce the Persuasion-Augmented
Chain of Thought (PCoT), a novel approach that leverages persuasion to improve
disinformation detection in zero-shot classification. We extensively evaluate
PCoT on online news and social media posts. Moreover, we publish two novel,
up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets
enable the evaluation of PCoT on content entirely unseen by the LLMs used in
our experiments, as the content was published after the models' knowledge
cutoffs. We show that, on average, PCoT outperforms competitive methods by 15%
across five LLMs and five datasets. These findings highlight the value of
persuasion in strengthening zero-shot disinformation detection.

</details>


### [45] [Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models](https://arxiv.org/abs/2506.06844)
*Naibin Gu,Peng Fu,Xiyu Liu,Ke Ma,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: Parameter-efficient fine-tuning (PEFT) is widely used for fine-tuning large language models. However, when the base model is updated, the performance of PEFT modules on the previous version usually drops sharply on the new version. Re-training these modules will increase computational cost. This paper finds that during the base model update, the task-specific knowledge in Feed-Forward Networks (FFN) is mainly affected, while the impact on the Attention mechanism is relatively small. Based on this, the paper proposes Trans-PEFT, which reduces the dependence of PEFT modules on the knowledge in the base model and focuses on the task-specific pattern, making it possible to maintain the performance of PEFT modules on the updated base model without re-training. Experiments show that this approach can greatly reduce maintenance costs in practical applications.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of performance degradation of PEFT modules on updated base models and reduce the computational cost of re-training.

Method: Propose Trans-PEFT, which reduces the dependence of PEFT modules on the knowledge in the base model and focuses on the task-specific pattern.

Result: Experiments on 7 base models and 12 datasets show that Trans-PEFT can maintain the performance of PEFT modules on updated base models without re-training.

Conclusion: Trans-PEFT can effectively maintain the performance of PEFT modules on updated base models, significantly reducing maintenance overhead in practical applications.

Abstract: Parameter-efficient fine-tuning (PEFT) has become a common method for
fine-tuning large language models, where a base model can serve multiple users
through PEFT module switching. To enhance user experience, base models require
periodic updates. However, once updated, PEFT modules fine-tuned on previous
versions often suffer substantial performance degradation on newer versions.
Re-tuning these numerous modules to restore performance would incur significant
computational costs. Through a comprehensive analysis of the changes that occur
during base model updates, we uncover an interesting phenomenon: continual
training primarily affects task-specific knowledge stored in Feed-Forward
Networks (FFN), while having less impact on the task-specific pattern in the
Attention mechanism. Based on these findings, we introduce Trans-PEFT, a novel
approach that enhances the PEFT module by focusing on the task-specific pattern
while reducing its dependence on certain knowledge in the base model. Further
theoretical analysis supports our approach. Extensive experiments across 7 base
models and 12 datasets demonstrate that Trans-PEFT trained modules can maintain
performance on updated base models without re-tuning, significantly reducing
maintenance overhead in real-world applications.

</details>


### [46] [Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning](https://arxiv.org/abs/2506.06877)
*Jiaxing Guo,Wenjie Yang,Shengzhong Zhang,Tongshan Xu,Lun Du,Da Zheng,Zengfeng Huang*

Main category: cs.CL

TL;DR: This paper introduces MathOlympiadEval, a dataset revealing a gap between LLMs' answer correctness and process correctness in mathematical problem-solving. It proposes ParaStepVerifier, a method for verifying mathematical solutions step-by-step.


<details>
  <summary>Details</summary>
Motivation: To address the issue of reward hacking in outcome-rewarded LLMs, where models may achieve correct answers through unsound reasoning.

Method: Introduces MathOlympiadEval dataset and proposes ParaStepVerifier for step-by-step verification of mathematical solutions.

Result: ParaStepVerifier improves the accuracy of identifying flawed solutions compared to baselines, especially for complex, multi-step problems.

Conclusion: The proposed method offers a more robust way to evaluate and train LLMs with genuine mathematical reasoning.

Abstract: Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable
success in mathematical problem-solving. However, this success often masks a
critical issue: models frequently achieve correct answers through fundamentally
unsound reasoning processes, a phenomenon indicative of reward hacking. We
introduce MathOlympiadEval, a new dataset with fine-grained annotations, which
reveals a significant gap between LLMs' answer correctness and their low
process correctness. Existing automated methods like LLM-as-a-judge struggle to
reliably detect these reasoning flaws. To address this, we propose
ParaStepVerifier, a novel methodology for meticulous, step-by-step verification
of mathematical solutions. ParaStepVerifier identifies incorrect reasoning
steps. Empirical results demonstrate that ParaStepVerifier substantially
improves the accuracy of identifying flawed solutions compared to baselines,
especially for complex, multi-step problems. This offers a more robust path
towards evaluating and training LLMs with genuine mathematical reasoning.

</details>


### [47] [Mixture of Small and Large Models for Chinese Spelling Check](https://arxiv.org/abs/2506.06887)
*Ziheng Qiao,Houquan Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的动态混合方法，该方法在束搜索解码阶段结合小模型和大型语言模型的概率分布，从而实现精确修正和流畅度的平衡增强，且无需对大型语言模型进行微调，节省时间和资源，并促进领域适应。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在中文拼写检查任务中得到了各种方法的发展，但其性能仍不令人满意。而经过微调的基于BERT的模型在高质量领域内数据上表现出色，但却存在编辑模式过拟合的问题。

Method: 我们的方法在束搜索解码阶段有效地结合了小模型和大型语言模型的概率分布，实现了精确修正和流畅度的平衡增强。

Result: 综合实验表明，我们的混合方法显著提升了错误纠正能力，在多个数据集上达到了最先进的结果。

Conclusion: 我们提出的方法在多个数据集上显著提高了错误纠正能力，并取得了最先进的结果。

Abstract: In the era of large language models (LLMs), the Chinese Spelling Check (CSC)
task has seen various LLM methods developed, yet their performance remains
unsatisfactory. In contrast, fine-tuned BERT-based models, relying on
high-quality in-domain data, show excellent performance but suffer from edit
pattern overfitting. This paper proposes a novel dynamic mixture approach that
effectively combines the probability distributions of small models and LLMs
during the beam search decoding phase, achieving a balanced enhancement of
precise corrections from small models and the fluency of LLMs. This approach
also eliminates the need for fine-tuning LLMs, saving significant time and
resources, and facilitating domain adaptation. Comprehensive experiments
demonstrate that our mixture approach significantly boosts error correction
capabilities, achieving state-of-the-art results across multiple datasets. Our
code is available at https://github.com/zhqiao-nlp/MSLLM.

</details>


### [48] [Automatic Speech Recognition of African American English: Lexical and Contextual Effects](https://arxiv.org/abs/2506.06888)
*Hamid Mojarad,Kevin Tang*

Main category: cs.CL

TL;DR: This study investigates how two specific features of African American English affect the performance of automatic speech recognition systems, finding that these features increase word error rates and that systems without language models rely more on lexical neighborhood effects.


<details>
  <summary>Details</summary>
Motivation: To examine the impact of phonetic, phonological, and morphosyntactic features of African American English on ASR models.

Method: Using the Montreal Forced Aligner to detect consonant cluster reduction and ING-reduction in the CORAAL corpus transcribed with wav2vec 2.0 with and without an external language model.

Result: Found a small but significant effect of consonant cluster reduction and ING-reduction on Word Error Rate and noted a stronger lexical neighborhood effect in ASR systems without language models.

Conclusion: End-to-end ASR systems without an external language model are more influenced by lexical neighborhood effects than those with an external language model.

Abstract: Automatic Speech Recognition (ASR) models often struggle with the phonetic,
phonological, and morphosyntactic features found in African American English
(AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction
(CCR) and ING-reduction. It examines whether the presence of CCR and
ING-reduction increases ASR misrecognition. Subsequently, it investigates
whether end-to-end ASR systems without an external Language Model (LM) are more
influenced by lexical neighborhood effect and less by contextual predictability
compared to systems with an LM. The Corpus of Regional African American
Language (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR
and ING-reduction were detected using the Montreal Forced Aligner (MFA) with
pronunciation expansion. The analysis reveals a small but significant effect of
CCR and ING on Word Error Rate (WER) and indicates a stronger presence of
lexical neighborhood effect in ASR systems without LMs.

</details>


### [49] [Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis](https://arxiv.org/abs/2506.06929)
*Mikhail Krasitskii,Grigori Sidorov,Olga Kolesnikova,Liliana Chanona Hernandez,Alexander Gelbukh*

Main category: cs.CL

TL;DR: A hybrid method combining extractive and abstractive summarization techniques improves multilingual sentiment analysis, especially for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of standalone methods in multilingual sentiment analysis.

Method: Combining TF-IDF-based extraction with a fine-tuned XLM-R abstractive module, enhanced by dynamic thresholding and cultural adaptation.

Result: Significant improvements over baselines in accuracy and computational efficiency across 10 languages.

Conclusion: The hybrid approach shows promise for practical applications like real-time brand monitoring and cross-cultural discourse analysis, with future optimization planned for low-resource languages.

Abstract: We propose a hybrid approach for multilingual sentiment analysis that
combines extractive and abstractive summarization to address the limitations of
standalone methods. The model integrates TF-IDF-based extraction with a
fine-tuned XLM-R abstractive module, enhanced by dynamic thresholding and
cultural adaptation. Experiments across 10 languages show significant
improvements over baselines, achieving 0.90 accuracy for English and 0.84 for
low-resource languages. The approach also demonstrates 22% greater
computational efficiency than traditional methods. Practical applications
include real-time brand monitoring and cross-cultural discourse analysis.
Future work will focus on optimization for low-resource languages via 8-bit
quantization.

</details>


### [50] [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)
*Alexander Spangher,Tenghao Huang,Jialiang Gu,Jiatong Shi,Muhao Chen*

Main category: cs.CL

TL;DR: 本文介绍了一种将话语结构融入新闻摘要生成的方法，并提出了一个新型的摘要数据集和算法（DiscoSum），该算法能有效保持叙述保真度并满足不同的风格和结构需求。


<details>
  <summary>Details</summary>
Motivation: 传统的语言模型通常不能保持长期的话语结构，尤其是在新闻文章中，组织流程对读者参与度有显著影响。因此，我们引入了一种将话语结构整合到摘要过程中的新方法。

Method: 提出了一种新的新闻话语模式来描述摘要结构，并开发了一种名为DiscoSum的新算法，该算法使用束搜索技术进行结构感知的摘要生成。

Result: 人类和自动评估结果表明，我们的方法在保持叙述保真度和满足结构需求方面是有效的。

Conclusion: 我们的方法在保持叙述保真度和满足结构需求方面表现出有效性。

Abstract: Recent advances in text summarization have predominantly leveraged large
language models to generate concise summaries. However, language models often
do not maintain long-term discourse structure, especially in news articles,
where organizational flow significantly influences reader engagement. We
introduce a novel approach to integrating discourse structure into
summarization processes, focusing specifically on news articles across various
media. We present a novel summarization dataset where news articles are
summarized multiple times in different ways across different social media
platforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse
schema to describe summarization structures and a novel algorithm, DiscoSum,
which employs beam search technique for structure-aware summarization, enabling
the transformation of news stories to meet different stylistic and structural
demands. Both human and automatic evaluation results demonstrate the efficacy
of our approach in maintaining narrative fidelity and meeting structural
requirements.

</details>


### [51] [What Makes a Good Natural Language Prompt?](https://arxiv.org/abs/2506.06950)
*Do Xuan Long,Duy Dinh,Ngoc-Hai Nguyen,Kenji Kawaguchi,Nancy F. Chen,Shafiq Joty,Min-Yen Kan*

Main category: cs.CL

TL;DR: Prompt quality evaluation is important but lacks a common definition. This paper proposes a framework with 21 properties across six dimensions to evaluate prompts. It finds imbalances in existing studies and suggests multi-property enhancements for better reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To define and evaluate prompt quality in a human-centric way.

Method: Meta-analysis of over 150 papers and blogs, proposing a framework with 21 properties categorized into six dimensions, examining impacts on LLMs, analyzing correlations among properties, exploring multi-property prompt enhancements, and studying instruction-tuning effects.

Result: Found imbalances in existing studies' support for models and tasks, significant research gaps, and that single-property enhancements often have the most impact on reasoning tasks.

Conclusion: Establishes a foundation for property-centric prompt evaluation and optimization, bridging gaps in human-AI communication and suggesting new research directions.

Abstract: As large language models (LLMs) have progressed towards more human-like and
human--AI communications have become prevalent, prompting has emerged as a
decisive component. However, there is limited conceptual consensus on what
exactly quantifies natural language prompts. We attempt to address this
question by conducting a meta-analysis surveying more than 150
prompting-related papers from leading NLP and AI conferences from 2022 to 2025
and blogs. We propose a property- and human-centric framework for evaluating
prompt quality, encompassing 21 properties categorized into six dimensions. We
then examine how existing studies assess their impact on LLMs, revealing their
imbalanced support across models and tasks, and substantial research gaps.
Further, we analyze correlations among properties in high-quality natural
language prompts, deriving prompting recommendations. We then empirically
explore multi-property prompt enhancements in reasoning tasks, observing that
single-property enhancements often have the greatest impact. Finally, we
discover that instruction-tuning on property-enhanced prompts can result in
better reasoning models. Our findings establish a foundation for
property-centric prompt evaluation and optimization, bridging the gaps between
human--AI communication and opening new prompting research directions.

</details>


### [52] [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
*Ha-Thanh Nguyen,Chaoran Liu,Hirokazu Kiyomaru,Koichi Takeda,Yusuke Miyao,Maki Matsuda,Yusuke Oda,Pontus Stenetorp,Qianying Liu,Su Myat Noe,Hideyuki Tachibana,Kouta Nakayama,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 介绍了一个新的日语数据集BIS Reasoning 1.0，用于评估大型语言模型的信念不一致推理能力，并揭示了现有模型在此类任务上的弱点。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集主要集中在一般性或信念一致的推理上，而缺乏专门设计用于评估信念不一致推理的数据集。

Method: 使用BIS Reasoning 1.0数据集评估大型语言模型中的信念不一致推理能力，并测试了包括GPT模型、Claude模型以及领先的日语大型语言模型在内的最先进模型。

Result: GPT-4o在BIS Reasoning 1.0数据集上达到了79.54%的准确率，其他模型表现各异，显示出显著的性能差异。

Conclusion: 发现当前大型语言模型在处理逻辑上有效但与信念冲突的输入时存在显著弱点，这对在法律、医疗和科学文献等高风险领域部署这些模型提出了挑战。

Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of
syllogistic reasoning problems explicitly designed to evaluate
belief-inconsistent reasoning in large language models (LLMs). Unlike prior
datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned
reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent
syllogisms to uncover reasoning biases in LLMs trained on human-aligned
corpora. We benchmark state-of-the-art models - including GPT models, Claude
models, and leading Japanese LLMs - revealing significant variance in
performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies
critical weaknesses in current LLMs when handling logically valid but
belief-conflicting inputs. These findings have important implications for
deploying LLMs in high-stakes domains such as law, healthcare, and scientific
literature, where truth must override intuitive belief to ensure integrity and
safety.

</details>


### [53] [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: This paper explores learning to ask clarifying questions in question answering agents using reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To improve question answering agents by adding the ability to ask clarifying questions.

Method: Simulating conversations with clarifying questions and using reinforcement learning for learning.

Result: Proposes offline RL objectives that are reward-weighted supervised fine-tuning and easily optimized in large language models.

Conclusion: The proposed method shows gains in optimized rewards and language quality compared to methods based on SFT and direct preference optimization.

Abstract: Question answering (QA) agents automatically answer questions posed in
natural language. In this work, we learn to ask clarifying questions in QA
agents. The key idea in our method is to simulate conversations that contain
clarifying questions and learn from them using reinforcement learning (RL). To
make RL practical, we propose and analyze offline RL objectives that can be
viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in
large language models. Our work stands in a stark contrast to recently proposed
methods, based on SFT and direct preference optimization, which have additional
hyper-parameters and do not directly optimize rewards. We compare to these
methods empirically and report gains in both optimized rewards and language
quality.

</details>


### [54] [A dependently-typed calculus of event telicity and culminativity](https://arxiv.org/abs/2506.06968)
*Pavel Kovalev,Carlo Angiuli*

Main category: cs.CL

TL;DR: This paper presents a dependently-typed framework for analyzing telicity and culminativity of events in English sentences, with a focus on nominal and verbal domains.


<details>
  <summary>Details</summary>
Motivation: To develop a cross-linguistic framework for analyzing the telicity and culminativity of events in English sentences.

Method: The framework consists of two parts: modeling boundedness of noun phrases and its relationship to subtyping, delimited quantities, and adjectival modification in the nominal domain; and defining a dependent event calculus in the verbal domain.

Result: The framework is defined as an extension of intensional Martin-L\"of dependent type theory, and the rules and examples in the paper have been formalized in the Agda proof assistant.

Conclusion: The presented framework provides a way to analyze telicity and culminativity of events in English sentences.

Abstract: We present a dependently-typed cross-linguistic framework for analyzing the
telicity and culminativity of events, accompanied by examples of using our
framework to model English sentences. Our framework consists of two parts. In
the nominal domain, we model the boundedness of noun phrases and its
relationship to subtyping, delimited quantities, and adjectival modification.
In the verbal domain we define a dependent event calculus, modeling telic
events as those whose undergoer is bounded, culminating events as telic events
that achieve their inherent endpoint, and consider adverbial modification. In
both domains we pay particular attention to associated entailments. Our
framework is defined as an extension of intensional Martin-L\"of dependent type
theory, and the rules and examples in this paper have been formalized in the
Agda proof assistant.

</details>


### [55] [Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971)
*Jaechul Roh,Varun Gandhi,Shivani Anilkumar,Arin Garg*

Main category: cs.CL

TL;DR: Investigate the robustness of reasoning Large Language Models through various prompt perturbations.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs truly reason or rely on shallow statistical patterns.

Method: Introduce a set of adversarially structured prompt perturbations.

Result: Performance varies widely with some perturbations reducing accuracy by 42.1% while others improve it by 35.3%.

Conclusion: Current reasoning systems are fragile and unpredictable, emphasizing the need for better approaches to reasoning alignment and prompting robustness.

Abstract: Large Language Models (LLMs) have achieved remarkable success in tasks
requiring complex reasoning, such as code generation, mathematical problem
solving, and algorithmic synthesis -- especially when aided by reasoning tokens
and Chain-of-Thought prompting. Yet, a core question remains: do these models
truly reason, or do they merely exploit shallow statistical patterns? In this
paper, we systematically investigate the robustness of reasoning LLMs by
introducing a suite of semantically faithful yet adversarially structured
prompt perturbations. Our evaluation -- spanning 700 perturbed code generations
derived from LeetCode-style problems -- applies transformations such as
storytelling reframing, irrelevant constraint injection, example reordering,
and numeric perturbation. We observe that while certain modifications severely
degrade performance (with accuracy drops up to -42.1%), others surprisingly
improve model accuracy by up to 35.3%, suggesting sensitivity not only to
semantics but also to surface-level prompt dynamics. These findings expose the
fragility and unpredictability of current reasoning systems, underscoring the
need for more principles approaches to reasoning alignments and prompting
robustness. We release our perturbation datasets and evaluation framework to
promote further research in trustworthy and resilient LLM reasoning.

</details>


### [56] [Atomic Reasoning for Scientific Table Claim Verification](https://arxiv.org/abs/2506.06972)
*Yuji Zhang,Qingyun Wang,Cheng Qian,Jiateng Liu,Chenkai Sun,Denghui Zhang,Tarek Abdelzaher,Chengxiang Zhai,Preslav Nakov,Heng Ji*

Main category: cs.CL

TL;DR: This paper proposes a new approach for verifying scientific claims from tables, inspired by Cognitive Load Theory, which improves accuracy and reduces cognitive load using modular reasoning components. The model outperforms GPT-4's chain-of-thought method on a new benchmark dataset with significantly less training data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying scientific claims from tables, especially for non-experts, and improve the precision of existing models.

Method: Developing a skill-chaining schema that uses modular, reusable reasoning components to reduce cognitive load and enhance the model's ability to interpret table-based claims.

Result: The proposed model outperforms GPT-4's chain-of-thought method on a new benchmark dataset called SciAtomicBench with only 350 fine-tuning examples.

Conclusion: This work demonstrates that enhancing a model's ability to interpret table-based claims through modular reasoning components can lead to more accurate and generalizable reasoning with reduced cognitive load.

Abstract: Scientific texts often convey authority due to their technical language and
complex data. However, this complexity can sometimes lead to the spread of
misinformation. Non-experts are particularly susceptible to misleading claims
based on scientific tables due to their high information density and perceived
credibility. Existing table claim verification models, including
state-of-the-art large language models (LLMs), often struggle with precise
fine-grained reasoning, resulting in errors and a lack of precision in
verifying scientific claims. Inspired by Cognitive Load Theory, we propose that
enhancing a model's ability to interpret table-based claims involves reducing
cognitive load by developing modular, reusable reasoning components (i.e.,
atomic skills). We introduce a skill-chaining schema that dynamically composes
these skills to facilitate more accurate and generalizable reasoning with a
reduced cognitive load. To evaluate this, we create SciAtomicBench, a
cross-domain benchmark with fine-grained reasoning annotations. With only 350
fine-tuning examples, our model trained by atomic reasoning outperforms
GPT-4o's chain-of-thought method, achieving state-of-the-art results with far
less training data.

</details>


### [57] [Chain of Methodologies: Scaling Test Time Computation without Training](https://arxiv.org/abs/2506.06982)
*Cong Liu,Jie Wu,Weigang Wu,Xu Chen,Liang Lin,Wei-Shi Zheng*

Main category: cs.CL

TL;DR: This paper presents the Chain of Methodologies (CoM), a new prompting framework that improves complex reasoning in large language models by incorporating human methodological insights.


<details>
  <summary>Details</summary>
Motivation: To address the issue of insufficient deep insights in training data of LLMs for complex reasoning tasks.

Method: Introducing CoM, which integrates human methodological insights to enhance structured thinking in LLMs without explicit fine-tuning.

Result: Experiments show CoM outperforms competitive baselines, showcasing the effectiveness of training-free prompting methods for complex reasoning tasks.

Conclusion: CoM demonstrates the potential of training-free prompting methods as robust solutions for complex reasoning tasks and bridges the gap towards human-level reasoning.

Abstract: Large Language Models (LLMs) often struggle with complex reasoning tasks due
to insufficient in-depth insights in their training data, which are typically
absent in publicly available documents. This paper introduces the Chain of
Methodologies (CoM), an innovative and intuitive prompting framework that
enhances structured thinking by integrating human methodological insights,
enabling LLMs to tackle complex tasks with extended reasoning. CoM leverages
the metacognitive abilities of advanced LLMs, activating systematic reasoning
throught user-defined methodologies without explicit fine-tuning. Experiments
show that CoM surpasses competitive baselines, demonstrating the potential of
training-free prompting methods as robust solutions for complex reasoning tasks
and bridging the gap toward human-level reasoning through human-like
methodological insights.

</details>


### [58] [Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors](https://arxiv.org/abs/2506.06987)
*Senqi Yang,Dongyu Zhang,Jing Ren,Ziqi Xu,Xiuzhen Zhang,Yiliao Song,Hongfei Lin,Feng Xia*

Main category: cs.CL

TL;DR: This paper introduces MultiMM, a multicultural multimodal metaphor dataset for cross-cultural studies of metaphors in Chinese and English, and proposes SEMD, a baseline model that uses sentiment embeddings to improve metaphor comprehension across cultures.


<details>
  <summary>Details</summary>
Motivation: To address the issue of cultural bias in automatic metaphor processing, particularly in multimodal contexts, and provide a more inclusive approach to natural language processing.

Method: Introducing the MultiMM dataset, consisting of 8,461 text-image advertisement pairs with fine-grained annotations, and proposing the Sentiment-Enriched Metaphor Detection (SEMD) model which incorporates sentiment embeddings.

Result: The SEMD model was effective in metaphor detection and sentiment analysis tasks, as validated by experimental results.

Conclusion: This work highlights the importance of considering cultural bias in NLP research and aims to contribute to the development of fairer and more inclusive language models.

Abstract: Metaphors are pervasive in communication, making them crucial for natural
language processing (NLP). Previous research on automatic metaphor processing
predominantly relies on training data consisting of English samples, which
often reflect Western European or North American biases. This cultural skew can
lead to an overestimation of model performance and contributions to NLP
progress. However, the impact of cultural bias on metaphor processing,
particularly in multimodal contexts, remains largely unexplored. To address
this gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset
designed for cross-cultural studies of metaphor in Chinese and English. MultiMM
consists of 8,461 text-image advertisement pairs, each accompanied by
fine-grained annotations, providing a deeper understanding of multimodal
metaphors beyond a single cultural domain. Additionally, we propose
Sentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates
sentiment embeddings to enhance metaphor comprehension across cultural
backgrounds. Experimental results validate the effectiveness of SEMD on
metaphor detection and sentiment analysis tasks. We hope this work increases
awareness of cultural bias in NLP research and contributes to the development
of fairer and more inclusive language models. Our dataset and code are
available at https://github.com/DUTIR-YSQ/MultiMM.

</details>


### [59] [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
*Ming Li,Zhengyuan Yang,Xiyao Wang,Dianqi Li,Kevin Lin,Tianyi Zhou,Lijuan Wang*

Main category: cs.CL

TL;DR: This paper studies the inefficiency issues of Large Reasoning Models (LRMs), proposing FoReaL-Decoding, which improves cost-quality trade-off through a leading model guiding and a draft model completing strategy.


<details>
  <summary>Details</summary>
Motivation: To address the overthinking problem of LRMs, which leads to slow inference and unnecessary details.

Method: Analyzing token-level misalignments between reasoning and non-reasoning models and proposing FoReaL-Decoding based on the Local Misalignment Diminish phenomenon.

Result: FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and shortens CoT length by up to 40% while maintaining 86 to 100% of model performance.

Conclusion: FoReaL-Decoding provides a simple and effective way for cost-quality trade-offs in reasoning tasks.

Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by
emitting long chains of thought. Yet, these verbose traces slow down inference
and often drift into unnecessary detail, known as the overthinking phenomenon.
To better understand LRMs' behavior, we systematically analyze the token-level
misalignment between reasoning and non-reasoning models. While it is expected
that their primary difference lies in the stylistic "thinking cues", LRMs
uniquely exhibit two pivotal, previously under-explored phenomena: a Global
Misalignment Rebound, where their divergence from non-reasoning models persists
or even grows as response length increases, and more critically, a Local
Misalignment Diminish, where the misalignment concentrates at the "thinking
cues" each sentence starts with but rapidly declines in the remaining of the
sentence. Motivated by the Local Misalignment Diminish, we propose
FoReaL-Decoding, a collaborative fast-slow thinking decoding method for
cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few
tokens for each sentence, and then a weaker draft model completes the following
tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to
smoothly interpolate between the small and the large model. On four popular
math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),
FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by
up to 40%, while preserving 86 to 100% of model performance. These results
establish FoReaL-Decoding as a simple, plug-and-play route to controllable
cost-quality trade-offs in reasoning-centric tasks.

</details>


### [60] [Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text](https://arxiv.org/abs/2506.07001)
*Yize Cheng,Vinu Sankar Sadasivan,Mehrdad Saberi,Shoumik Saha,Soheil Feizi*

Main category: cs.CL

TL;DR: A training-free attack framework called Adversarial Paraphrasing is introduced to humanize AI-generated text, making it more difficult to detect. This approach improves upon simple paraphrasing attacks by reducing the true positive rate at 1% false positive by up to 98.96% across various detection systems.


<details>
  <summary>Details</summary>
Motivation: Concerns over misuse of large language models in plagiarism and social engineering have led to the development of AI text detectors. However, these detectors are vulnerable to simple evasion techniques like paraphrasing.

Method: Adversarial Paraphrasing uses an instruction-following LLM to paraphrase AI-generated content under the guidance of an AI text detector, creating adversarial examples optimized to bypass detection.

Result: Experiments showed that Adversarial Paraphrasing was broadly effective and transferable across several detection systems, achieving an average T@1%F reduction of 87.88% under OpenAI-RoBERTa-Large's guidance.

Conclusion: This study highlights the need for more robust detection strategies due to the sophistication of evasion techniques.

Abstract: The increasing capabilities of Large Language Models (LLMs) have raised
concerns about their misuse in AI-generated plagiarism and social engineering.
While various AI-generated text detectors have been proposed to mitigate these
risks, many remain vulnerable to simple evasion techniques such as
paraphrasing. However, recent detectors have shown greater robustness against
such basic attacks. In this work, we introduce Adversarial Paraphrasing, a
training-free attack framework that universally humanizes any AI-generated text
to evade detection more effectively. Our approach leverages an off-the-shelf
instruction-following LLM to paraphrase AI-generated content under the guidance
of an AI text detector, producing adversarial examples that are specifically
optimized to bypass detection. Extensive experiments show that our attack is
both broadly effective and highly transferable across several detection
systems. For instance, compared to simple paraphrasing attack--which,
ironically, increases the true positive at 1% false positive (T@1%F) by 8.57%
on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by
OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on
Fast-DetectGPT. Across a diverse set of detectors--including neural
network-based, watermark-based, and zero-shot approaches--our attack achieves
an average T@1%F reduction of 87.88% under the guidance of
OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and
attack success to find that our method can significantly reduce detection
rates, with mostly a slight degradation in text quality. Our adversarial setup
highlights the need for more robust and resilient detection strategies in the
light of increasingly sophisticated evasion techniques.

</details>


### [61] [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
*Bhuiyan Sanjid Shafique,Ashmal Vayani,Muhammad Maaz,Hanoona Abdul Rasheed,Dinura Dissanayake,Mohammed Irfan Kurpath,Yahya Hmaiti,Go Inoue,Jean Lahoud,Md. Safirur Rashid,Shadid Intisar Quasem,Maheen Fatima,Franco Vidal,Mykola Maslych,Ketan Pravin More,Sanoojan Baliah,Hasindri Watawana,Yuhao Li,Fabian Farestam,Leon Schaller,Roman Tymtsiv,Simon Weber,Hisham Cholakkal,Ivan Laptev,Shin'ichi Satoh,Michael Felsberg,Mubarak Shah,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CL

TL;DR: 提出ViMUL-Bench和ViMUL模型，促进文化与语言包容性的多语言视频大型模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有大多数大型多模态模型都是基于英文的，对于文化多样性和语言包容性考虑不足，尤其是在视频理解领域。

Method: 使用机器翻译创建了包含120万样本的多语言视频训练集，并开发了简单的多语言视频大型模型ViMUL。

Result: ViMUL-Bench包括开放性（短篇和长篇）和多项选择题，涵盖了15个类别，其中包括8个文化多样性类别，并且包含8千个由母语人士手动验证的样本。

Conclusion: 提出了一种新的多语言视频大型模型ViMUL，并创建了一个名为ViMUL-Bench的多语言视频大型模型基准，用于评估14种语言的视频理解能力。

Abstract: Large multimodal models (LMMs) have recently gained attention due to their
effectiveness to understand and generate descriptions of visual content. Most
existing LMMs are in English language. While few recent works explore
multilingual image LMMs, to the best of our knowledge, moving beyond the
English language for cultural and linguistic inclusivity is yet to be
investigated in the context of video LMMs. In pursuit of more inclusive video
LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to
evaluate Video LMMs across 14 languages, including both low- and high-resource
languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,
Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is
designed to rigorously test video LMMs across 15 categories including eight
culturally diverse categories, ranging from lifestyles and festivals to foods
and rituals and from local landmarks to prominent cultural personalities.
ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice
questions spanning various video durations (short, medium, and long) with 8k
samples that are manually verified by native language speakers. In addition, we
also introduce a machine translated multilingual video training set comprising
1.2 million samples and develop a simple multilingual video LMM, named ViMUL,
that is shown to provide a better tradeoff between high-and low-resource
languages for video understanding. We hope our ViMUL-Bench and multilingual
video LMM along with a large-scale multilingual video training set will help
ease future research in developing cultural and linguistic inclusive
multilingual video LMMs. Our proposed benchmark, video LMM and training data
will be publicly released at https://mbzuai-oryx.github.io/ViMUL/.

</details>


### [62] [KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering](https://arxiv.org/abs/2506.07037)
*Zhongze Luo,Weixuan Wan,Qizhi Zheng,Yanhong Bai,Jingyun Sun,Jian Wang,Dan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种结合LoRA微调和知识图谱的通信标准领域智能咨询与问答系统，显著提升了问答效果和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统咨询模式周期长且依赖专家的知识和经验，难以满足快速发展的技术需求。

Method: 结合LoRA微调大型语言模型与构建通信标准领域的知识图谱，实现智能咨询和问答系统。

Result: Qwen2.5-7B-Instruct在通信标准领域表现出色，BLEU-4和ROUGE等指标显著提高；构建的通信标准领域知识图谱查询准确率较好；智能咨询和问答系统在多个角度提升了回答效果，并在交互体验和后端访问方面取得良好成果。

Conclusion: 实验结果表明，在通信标准领域的6587个问答数据集上使用LoRA微调后，Qwen2.5-7B-Instruct在测试集上的专业能力表现优异。与比较模型Llama-3-8B-Instruct相比，其BLEU-4得分从18.8564提高到66.8993，ROUGE等评价指标也显著提升。基于包含6个实体属性和10个关系属性的本体框架，构建了一个包含13906个实体和13524个关系的通信标准领域知识图谱，查询准确率较好。智能咨询和问答系统通过访问本地构建的知识图谱并进行关键信息的图形检索，有助于提高回答效果。DeepSeek评估显示，我们的RAG框架使微调模型在五个角度的分数平均提高了2.26%，并且在交互体验和后端访问方面取得了很好的结果，具有很好的实际应用价值。

Abstract: There are many types of standards in the field of communication. The
traditional consulting model has a long cycle and relies on the knowledge and
experience of experts, making it difficult to meet the rapidly developing
technological demands. This paper combines the fine-tuning of large language
models with the construction of knowledge graphs to implement an intelligent
consultation and question-answering system for communication standards. The
experimental results show that after LoRA tuning on the constructed dataset of
6,587 questions and answers in the field of communication standards,
Qwen2.5-7B-Instruct demonstrates outstanding professional capabilities in the
field of communication standards on the test set. BLEU-4 rose from 18.8564 to
66.8993, and evaluation indicators such as ROUGE also increased significantly,
outperforming the fine-tuning effect of the comparison model
Llama-3-8B-Instruct. Based on the ontology framework containing 6 entity
attributes and 10 relation attributes, a knowledge graph of the communication
standard domain containing 13,906 entities and 13,524 relations was
constructed, showing a relatively good query accuracy rate. The intelligent
consultation and question-answering system enables the fine-tuned model on the
server side to access the locally constructed knowledge graph and conduct
graphical retrieval of key information first, which is conducive to improving
the question-answering effect. The evaluation using DeepSeek as the Judge on
the test set shows that our RAG framework enables the fine-tuned model to
improve the scores at all five angles, with an average score increase of 2.26%.
And combined with web services and API interfaces, it has achieved very good
results in terms of interaction experience and back-end access, and has very
good practical application value.

</details>


### [63] [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
*Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: This study develops automatic historical event extraction models using multiple large language models (LLMs) with different enhancement strategies, revealing that enhancement strategies optimize different performance aspects and do not provide universal improvements.


<details>
  <summary>Details</summary>
Motivation: The manual construction of structured computational representations of historical events is computationally expensive and existing RDF/OWL reasoners are limited to fragments of first-order logic.

Method: Developed automatic historical event extraction models using multiple LLMs (GPT-4, Claude, Llama 3.2) with three enhancement strategies: pure base generation, knowledge graph enhancement, and Retrieval-Augmented Generation (RAG).

Result: Base generation achieved optimal performance for coverage and historical breadth with Claude and GPT-4, while RAG enhancement improved coordinate accuracy and metadata completeness for precision. Larger models showed robust baseline performance with incremental RAG improvements, whereas Llama 3.2 showed extreme variance.

Conclusion: An automated translation pipeline was developed to convert extracted RDF representations into Coq proof assistant specifications, enabling higher-order reasoning and validating that RAG-discovered event types represent legitimate domain-specific semantic structures.

Abstract: Extracting structured computational representations of historical events from
narrative text remains computationally expensive when constructed manually.
While RDF/OWL reasoners enable graph-based reasoning, they are limited to
fragments of first-order logic, preventing deeper temporal and semantic
analysis. This paper addresses both challenges by developing automatic
historical event extraction models using multiple LLMs (GPT-4, Claude, Llama
3.2) with three enhancement strategies: pure base generation, knowledge graph
enhancement, and Retrieval-Augmented Generation (RAG). We conducted
comprehensive evaluations using historical texts from Thucydides. Our findings
reveal that enhancement strategies optimize different performance dimensions
rather than providing universal improvements. For coverage and historical
breadth, base generation achieves optimal performance with Claude and GPT-4
extracting comprehensive events. However, for precision, RAG enhancement
improves coordinate accuracy and metadata completeness. Model architecture
fundamentally determines enhancement sensitivity: larger models demonstrate
robust baseline performance with incremental RAG improvements, while Llama 3.2
shows extreme variance from competitive performance to complete failure. We
then developed an automated translation pipeline converting extracted RDF
representations into Coq proof assistant specifications, enabling higher-order
reasoning beyond RDF capabilities including multi-step causal verification,
temporal arithmetic with BC dates, and formal proofs about historical
causation. The Coq formalization validates that RAG-discovered event types
represent legitimate domain-specific semantic structures rather than
ontological violations.

</details>


### [64] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
*LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu Rong*

Main category: cs.CL

TL;DR: 提出了一种名为Lingshu的医疗专用多模态大型语言模型，解决了现有医疗多模态模型在医学知识覆盖、数据处理和推理能力上的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有医疗多模态模型在医学知识覆盖、数据处理和推理能力上的局限性。

Method: 提出了一个全面的数据整理程序，并构建了一个包含丰富医学知识的多模态数据集；引入了多阶段训练方法来嵌入医学专业知识并逐步提高任务解决能力；探索了使用可验证奖励范式的强化学习来增强医学推理能力；开发了MedEvalKit统一评估框架。

Result: 提出的Lingshu模型在大多数任务上始终优于现有的开源多模态模型。

Conclusion: Lingshu模型在医学多模态任务中表现出色，为医疗领域的多模态模型发展提供了新的方向。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [65] [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)
*Kai Xiong,Xiao Ding,Yixin Cao,Yuxiong Yan,Li Du,Yufei Zhang,Jinglong Gao,Jiaqian Liu,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: A benchmark named Com$^2$ has been proposed for complex commonsense reasoning, which uses causal event graphs and causal theory to generate examples and test large language models.


<details>
  <summary>Details</summary>
Motivation: To explore complex commonsense reasoning which is less studied but closer to real-world concerns

Method: 1. Incorporate causal event graphs as structured complex commonsense. 2. Adopt causal theory to modify the graphs and get different scenarios. 3. Use an LLM to synthesize examples guided by the logical relationships in the modified graphs. 4. Construct a more challenging subset using detective stories.

Result: Experiments show that LLMs have difficulty in reasoning depth and breadth, but post-training and slow thinking can improve this situation.

Conclusion: The study provides a new benchmark for complex commonsense reasoning and explores ways to improve LLMs' performance.

Abstract: Large language models (LLMs) have mastered abundant simple and explicit
commonsense knowledge through pre-training, enabling them to achieve human-like
performance in simple commonsense reasoning. Nevertheless, LLMs struggle to
reason with complex and implicit commonsense knowledge that is derived from
simple ones (such as understanding the long-term effects of certain events), an
aspect humans tend to focus on more. Existing works focus on complex tasks like
math and code, while complex commonsense reasoning remains underexplored due to
its uncertainty and lack of structure. To fill this gap and align with
real-world concerns, we propose a benchmark Com$^2$ focusing on complex
commonsense reasoning. We first incorporate causal event graphs to serve as
structured complex commonsense. Then we adopt causal theory~(e.g.,
intervention) to modify the causal event graphs and obtain different scenarios
that meet human concerns. Finally, an LLM is employed to synthesize examples
with slow thinking, which is guided by the logical relationships in the
modified causal graphs. Furthermore, we use detective stories to construct a
more challenging subset. Experiments show that LLMs struggle in reasoning depth
and breadth, while post-training and slow thinking can alleviate this. The code
and data are available at https://github.com/Waste-Wood/Com2.

</details>


### [66] [Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing](https://arxiv.org/abs/2506.07086)
*Yuanhe Tian,Pengsen Cheng,Guoqing Jin,Lei Zhang,Yan Song*

Main category: cs.CL

TL;DR: A novel LLM-based approach is proposed for multi-modal affective computing by decomposing visual and textual representations into shared and modality-specific components.


<details>
  <summary>Details</summary>
Motivation: Existing approaches fail to capture complex and conflicting evidence presented across different modalities in multi-modal affective computing.

Method: The approach first encodes and aligns input modalities using pre-trained multi-modal encoders, then uses a representation decomposition framework to separate common emotional content from unique cues, and finally integrates these decomposed signals via an attention mechanism to form a dynamic soft prompt for a multi-modal LLM.

Result: The approach shows effectiveness on three representative tasks for affective computing and outperforms strong baselines and state-of-the-art models.

Conclusion: This paper presents a novel LLM-based approach for multi-modal affective computing that can better capture complex and conflicting evidence across different modalities.

Abstract: Multi-modal affective computing aims to automatically recognize and interpret
human attitudes from diverse data sources such as images and text, thereby
enhancing human-computer interaction and emotion understanding. Existing
approaches typically rely on unimodal analysis or straightforward fusion of
cross-modal information that fail to capture complex and conflicting evidence
presented across different modalities. In this paper, we propose a novel
LLM-based approach for affective computing that explicitly deconstructs visual
and textual representations into shared (modality-invariant) and
modality-specific components. Specifically, our approach firstly encodes and
aligns input modalities using pre-trained multi-modal encoders, then employs a
representation decomposition framework to separate common emotional content
from unique cues, and finally integrates these decomposed signals via an
attention mechanism to form a dynamic soft prompt for a multi-modal LLM.
Extensive experiments on three representative tasks for affective computing,
namely, multi-modal aspect-based sentiment analysis, multi-modal emotion
analysis, and hateful meme detection, demonstrate the effectiveness of our
approach, which consistently outperforms strong baselines and state-of-the-art
models.

</details>


### [67] [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
*Jiaxuan Gao,Shu Yan,Qixin Tan,Lu Yang,Shusheng Xu,Wei Fu,Zhiyu Mei,Kaifeng Lyu,Yi Wu*

Main category: cs.CL

TL;DR: This work introduces the reasoning efficiency frontiers and the REG metric, proposes REO-RL to minimize REG, and demonstrates its effectiveness in improving reasoning efficiency of LRMs.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for improving reasoning efficiency have inconsistent evaluations, making it difficult to assess their efficiency gains.

Method: Introducing REO-RL, a reinforcement learning algorithm that minimizes REG by targeting a sparse set of token budgets.

Result: Systematic evaluation on challenging mathematical benchmarks reveals significant gaps in current methods. The proposed REO-RL method consistently reduces REG by >=50 across all evaluated LRMs and matches Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy loss.

Conclusion: Fine-tuning Large Reasoning Models (LRMs) to perfectly align with the efficiency frontiers is still an open challenge.

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving
capabilities through extended Chain-of-Thought (CoT) reasoning but often
produce excessively verbose and redundant reasoning traces. This inefficiency
incurs high inference costs and limits practical deployment. While existing
fine-tuning methods aim to improve reasoning efficiency, assessing their
efficiency gains remains challenging due to inconsistent evaluations. In this
work, we introduce the reasoning efficiency frontiers, empirical upper bounds
derived from fine-tuning base LRMs across diverse approaches and training
configurations. Based on these frontiers, we propose the Reasoning Efficiency
Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from
these frontiers. Systematic evaluation on challenging mathematical benchmarks
reveals significant gaps in current methods: they either sacrifice accuracy for
short length or still remain inefficient under tight token budgets. To reduce
the efficiency gap, we propose REO-RL, a class of Reinforcement Learning
algorithms that minimizes REG by targeting a sparse set of token budgets.
Leveraging numerical integration over strategically selected budgets, REO-RL
approximates the full efficiency objective with low error using a small set of
token budgets. Through systematic benchmarking, we demonstrate that our
efficiency metric, REG, effectively captures the accuracy-length trade-off,
with low-REG methods reducing length while maintaining accuracy. Our approach,
REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching
Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy
loss. Ablation studies confirm the effectiveness of our exponential token
budget strategy. Finally, our findings highlight that fine-tuning LRMs to
perfectly align with the efficiency frontiers remains an open challenge.

</details>


### [68] [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
*Samir Abdaljalil,Hasan Kurban,Khalid Qaraqe,Erchin Serpedin*

Main category: cs.CL

TL;DR: A novel framework called Theorem-of-Thought (ToTh) is introduced to enhance the reasoning capabilities of large language models (LLMs). ToTh models reasoning through collaboration among three parallel agents performing different types of inference and selects the most coherent reasoning graph to produce the final answer.


<details>
  <summary>Details</summary>
Motivation: Enhancing the reliability and interpretability of reasoning processes in LLMs.

Method: Introducing a framework called Theorem-of-Thought (ToTh) that uses three parallel agents for abductive, deductive, and inductive inference, producing a formal reasoning graph evaluated by Bayesian belief propagation guided by NLI.

Result: ToTh outperforms other prompting techniques like CoT, Self-Consistency, and CoT-Decoding on symbolic and numerical reasoning benchmarks.

Conclusion: The proposed ToTh framework shows promise for developing more robust and cognitively inspired LLM reasoning.

Abstract: Large language models (LLMs) have shown strong performance across natural
language reasoning tasks, yet their reasoning processes remain brittle and
difficult to interpret. Prompting techniques like Chain-of-Thought (CoT)
enhance reliability by eliciting intermediate reasoning steps or aggregating
multiple outputs. However, they lack mechanisms for enforcing logical structure
and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a
novel framework that models reasoning as collaboration among three parallel
agents, each simulating a distinct mode of inference: abductive, deductive, and
inductive. Each agent produces a reasoning trace, which is structured into a
formal reasoning graph. To evaluate consistency, we apply Bayesian belief
propagation guided by natural language inference (NLI), assigning confidence
scores to each step. The most coherent graph is selected to derive the final
answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)
reasoning benchmarks show that ToTh consistently outperforms CoT,
Self-Consistency, and CoT-Decoding across multiple LLMs, while producing
interpretable and logically grounded reasoning chains. Our findings suggest a
promising direction for building more robust and cognitively inspired LLM
reasoning. The implementation is available at
https://github.com/KurbanIntelligenceLab/theorem-of-thought.

</details>


### [69] [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: Chain-of-Thought(CoT)提示可以提高推理任务的表现，但其效果因任务类型和模型而异。对于非推理模型，CoT可以略微提高平均性能，但对于需要更多token的任务，它会增加成本和时间，并可能导致错误答案。对于具有显式推理能力的模型，CoT提示对答案准确性的影响很小，但却显著增加了生成响应所需的时间和token数量。


<details>
  <summary>Details</summary>
Motivation: 帮助商业、教育和政策领导者理解与AI相关的技术细节并通过严格的测试。

Method: 研究Chain-of-Thought(CoT)提示的效果，这是一种鼓励大型语言模型'一步一步思考'的技术。

Result: CoT提示的效果因任务类型和模型而异。对于非推理模型，CoT可以略微提高平均性能，但对于需要更多token的任务，它会增加成本和时间，并可能导致错误答案。对于具有显式推理能力的模型，CoT提示对答案准确性的影响很小，但却显著增加了生成响应所需的时间和token数量。

Conclusion: CoT提示在提高推理任务表现方面的作用是有限的，并且可能带来额外的成本和复杂性。

Abstract: This is the second in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate Chain-of-Thought
(CoT) prompting, a technique that encourages a large language model (LLM) to
"think step by step" (Wei et al., 2022). CoT is a widely adopted method for
improving reasoning tasks, however, our findings reveal a more nuanced picture
of its effectiveness. We demonstrate two things:
  - The effectiveness of Chain-of-Thought prompting can vary greatly depending
on the type of task and model. For non-reasoning models, CoT generally improves
average performance by a small amount, particularly if the model does not
inherently engage in step-by-step processing by default. However, CoT can
introduce more variability in answers, sometimes triggering occasional errors
in questions the model would otherwise get right. We also found that many
recent models perform some form of CoT reasoning even if not asked; for these
models, a request to perform CoT had little impact. Performing CoT generally
requires far more tokens (increasing cost and time) than direct answers.
  - For models designed with explicit reasoning capabilities, CoT prompting
often results in only marginal, if any, gains in answer accuracy. However, it
significantly increases the time and tokens needed to generate a response.

</details>


### [70] [Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2506.07148)
*Yaping Chai,Haoran Xie,Joe S. Qin*

Main category: cs.CL

TL;DR: This paper presents a data augmentation method using large language models for aspect category sentiment analysis, improving semantic coverage and inference capabilities.


<details>
  <summary>Details</summary>
Motivation: To address data scarcity in low-resource scenarios for aspect category sentiment analysis tasks.

Method: Using structured prompt templates for LLMs to generate diverse yet semantically consistent sentences, along with a post-processing technique and a confidence-weighted fine-tuning strategy.

Result: The proposed method improves the model's understanding of relationships between aspect categories and sentiment polarities, leading to enhanced inference capabilities.

Conclusion: The introduced data augmentation strategy with LLMs outperforms other methods on multiple benchmark datasets.

Abstract: Large language model (LLM) is an effective approach to addressing data
scarcity in low-resource scenarios. Recent existing research designs
hand-crafted prompts to guide LLM for data augmentation. We introduce a data
augmentation strategy for the aspect category sentiment analysis (ACSA) task
that preserves the original sentence semantics and has linguistic diversity,
specifically by providing a structured prompt template for an LLM to generate
predefined content. In addition, we employ a post-processing technique to
further ensure semantic consistency between the generated sentence and the
original sentence. The augmented data increases the semantic coverage of the
training distribution, enabling the model better to understand the relationship
between aspect categories and sentiment polarities, enhancing its inference
capabilities. Furthermore, we propose a confidence-weighted fine-tuning
strategy to encourage the model to generate more confident and accurate
sentiment polarity predictions. Compared with powerful and recent works, our
method consistently achieves the best performance on four benchmark datasets
over all baselines.

</details>


### [71] [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
*Vicky Xefteri,Tim Vieira,Ryan Cotterell,Afra Amini*

Main category: cs.CL

TL;DR: This paper proposes a method for controlling the syntactic structure of text generated by language models using sampling algorithms, demonstrating significant improvements in syntactic accuracy without sacrificing fluency.


<details>
  <summary>Details</summary>
Motivation: Controlling the syntactic structure of text generated by language models is valuable for applications requiring clarity, stylistic consistency, or interpretability.

Method: We combine sequential Monte Carlo with a syntactic tagger to enforce a target constituency structure during text generation.

Result: The approach improves syntactic accuracy significantly, increasing the F1 score from 12.31 to about 93 for GPT2 and from 35.33 to about 93 for Llama3-8B.

Conclusion: Our experiments demonstrate the effectiveness of our approach in improving syntactic accuracy while maintaining language model fluency.

Abstract: Controlling the syntactic structure of text generated by language models is
valuable for applications requiring clarity, stylistic consistency, or
interpretability, yet it remains a challenging task. In this paper, we argue
that sampling algorithms based on the posterior inference can effectively
enforce a target constituency structure during generation. Our approach
combines sequential Monte Carlo, which estimates the posterior distribution by
sampling from a proposal distribution, with a syntactic tagger that ensures
that each generated token aligns with the desired syntactic structure. Our
experiments with GPT2 and Llama3-8B models show that with an appropriate
proposal distribution, we can improve syntactic accuracy, increasing the F1
score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both
cases without compromising the language model's fluency. These results
underscore both the complexity of syntactic control and the effectiveness of
sampling algorithms, offering a promising approach for applications where
precise control over syntax is essential.

</details>


### [72] [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/abs/2506.07160)
*Yikun Wang,Yibin Wang,Dianyi Wang,Zimian Peng,Qipeng Guo,Dacheng Tao,Jiaqi Wang*

Main category: cs.CL

TL;DR: GeometryZero is a new method for solving geometry problems using smaller models trained with Group Contrastive Policy Optimization (GCPO). It improves performance by providing context-specific rewards for auxiliary constructions and promoting longer reasoning chains.


<details>
  <summary>Details</summary>
Motivation: Geometry problem solving is challenging because it often requires auxiliary constructions. Existing methods either perform poorly or require very large models, which are costly to run.

Method: GCPO includes Group Contrastive Masking to give context-specific rewards and a length reward to encourage longer reasoning. This method allows GeometryZero models to efficiently decide when to use auxiliary constructions.

Result: GeometryZero models perform better than previous methods like GRPO on several geometry benchmarks, improving average performance by 4.29%.

Conclusion: The proposed approach shows promise for making geometric reasoning more efficient with smaller models.

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities across diverse domains, particularly in mathematical reasoning,
amid which geometry problem solving remains a challenging area where auxiliary
construction plays a enssential role. Existing approaches either achieve
suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring
massive computational costs. We posit that reinforcement learning with
verifiable reward (e.g., GRPO) offers a promising direction for training
smaller models that effectively combine auxiliary construction with robust
geometric reasoning. However, directly applying GRPO to geometric reasoning
presents fundamental limitations due to its dependence on unconditional
rewards, which leads to indiscriminate and counterproductive auxiliary
constructions. To address these challenges, we propose Group Contrastive Policy
Optimization (GCPO), a novel reinforcement learning framework featuring two key
innovations: (1) Group Contrastive Masking, which adaptively provides positive
or negative reward signals for auxiliary construction based on contextual
utility, and a (2) length reward that promotes longer reasoning chains.
Building on GCPO, we develop GeometryZero, a family of affordable-size
geometric reasoning models that judiciously determine when to employ auxiliary
construction. Our extensive empirical evaluation across popular geometric
benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models
consistently outperform baselines (e.g. GRPO), achieving an average improvement
of 4.29% across all benchmarks.

</details>


### [73] [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação](https://arxiv.org/abs/2506.07169)
*Washington Cunha,Leonardo Rocha,Marcos André Gonçalves*

Main category: cs.CL

TL;DR: This PhD dissertation explores the use of Instance Selection (IS) in Natural Language Processing (NLP) to decrease the size of training sets without affecting model effectiveness. It compares various IS methods for Automatic Text Classification (ATC) and introduces two new IS solutions tailored for large datasets and transformer architectures.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational demands of training large dense models for specific NLP applications.

Method: Conducts a comprehensive comparison of existing IS methods for ATC and proposes two novel IS solutions.

Result: Achieved an average 41% reduction in training set size while maintaining model effectiveness and showed speedups of 1.67x (up to 2.46x).

Conclusion: Instance Selection has significant potential in NLP, especially for large-scale datasets, and the proposed solutions make it scalable.

Abstract: Progress in Natural Language Processing (NLP) has been dictated by the rule
of more: more data, more computing power and more complexity, best exemplified
by the Large Language Models. However, training (or fine-tuning) large dense
models for specific applications usually requires significant amounts of
computing resources. This \textbf{Ph.D. dissertation} focuses on an
under-investi\-gated NLP data engineering technique, whose potential is
enormous in the current scenario known as Instance Selection (IS). The IS goal
is to reduce the training set size by removing noisy or redundant instances
while maintaining the effectiveness of the trained models and reducing the
training process cost. We provide a comprehensive and scientifically sound
comparison of IS methods applied to an essential NLP task -- Automatic Text
Classification (ATC), considering several classification solutions and many
datasets. Our findings reveal a significant untapped potential for IS
solutions. We also propose two novel IS solutions that are noise-oriented and
redundancy-aware, specifically designed for large datasets and transformer
architectures. Our final solution achieved an average reduction of 41\% in
training sets, while maintaining the same levels of effectiveness in all
datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x
(up to 2.46x), making them scalable for datasets with hundreds of thousands of
documents.

</details>


### [74] [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
*Chenlong Zhang,Zhuoran Jin,Hongbang Yuan,Jiaheng Wei,Tong Zhou,Kang Liu,Jun Zhao,Yubo Chen*

Main category: cs.CL

TL;DR: This paper proposes a new method called Reinforcement UnLearning (RULE) for efficiently unlearning specific information from large language models without retraining from scratch or degrading overall utility.


<details>
  <summary>Details</summary>
Motivation: Concerns about sensitive, copyrighted, or illegal content in large language models trained on massive uncurated corpora have increased interest in LLM unlearning.

Method: Reinforcement UnLearning (RULE)

Result: With only 12% forget set and 8% synthesized boundary data, RULE outperforms existing baselines by up to 17.5% forget quality and 16.3% naturalness response while maintaining general utility.

Conclusion: Experimental results demonstrate that RULE can achieve effective unlearning without significant degradation of overall model utility.

Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive,
uncurated corpora has raised growing concerns about the inclusion of sensitive,
copyrighted, or illegal content. This has led to increasing interest in LLM
unlearning: the task of selectively removing specific information from a model
without retraining from scratch or degrading overall utility. However, existing
methods often rely on large-scale forget and retain datasets, and suffer from
unnatural responses, poor generalization, or catastrophic utility loss. In this
work, we propose Reinforcement UnLearning (RULE), an efficient framework that
formulates unlearning as a refusal boundary optimization problem. RULE is
trained with a small portion of the forget set and synthesized boundary
queries, using a verifiable reward function that encourages safe refusal on
forget--related queries while preserving helpful responses on permissible
inputs. We provide both theoretical and empirical evidence demonstrating the
effectiveness of RULE in achieving targeted unlearning without compromising
model utility. Experimental results show that, with only $12%$ forget set and
$8%$ synthesized boundary data, RULE outperforms existing baselines by up to
$17.5%$ forget quality and $16.3%$ naturalness response while maintaining
general utility, achieving forget--retain Pareto optimality. Remarkably, we
further observe that RULE improves the naturalness of model outputs, enhances
training efficiency, and exhibits strong generalization ability, generalizing
refusal behavior to semantically related but unseen queries.

</details>


### [75] [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
*Wenrui Zhou,Shu Yang,Qingsong Yang,Zikun Guo,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: This paper introduces VISE, a benchmark to assess sycophantic tendencies in Video-LLMs, focusing on different question formats, visual tasks, and linguistic perspectives. It also examines key-frame selection as a method to reduce such biases.


<details>
  <summary>Details</summary>
Motivation: Ensuring factual consistency and reliability in Video-LLMs is crucial, but current methods lack systematic evaluation of sycophantic behaviors.

Method: Developing VISE to evaluate sycophantic behavior in Video-LLMs and exploring key-frame selection as a mitigation strategy.

Result: VISE provides a comprehensive evaluation framework for sycophantic behavior in Video-LLMs, highlighting the role of visual grounding in reducing biases.

Conclusion: The study fills a gap in evaluating sycophantic behaviors in Video-LLMs and suggests potential ways to enhance their reliability.

Abstract: As video large language models (Video-LLMs) become increasingly integrated
into real-world applications that demand grounded multimodal reasoning,
ensuring their factual consistency and reliability is of critical importance.
However, sycophancy, the tendency of these models to align with user input even
when it contradicts the visual evidence, undermines their trustworthiness in
such contexts. Current sycophancy research has largely overlooked its specific
manifestations in the video-language domain, resulting in a notable absence of
systematic benchmarks and targeted evaluations to understand how Video-LLMs
respond under misleading user input. To fill this gap, we propose VISE
(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated
benchmark designed to evaluate sycophantic behavior in state-of-the-art
Video-LLMs across diverse question formats, prompt biases, and visual reasoning
tasks. Specifically, VISE pioneeringly brings linguistic perspectives on
sycophancy into the visual domain, enabling fine-grained analysis across
multiple sycophancy types and interaction patterns. In addition, we explore
key-frame selection as an interpretable, training-free mitigation strategy,
which reveals potential paths for reducing sycophantic bias by strengthening
visual grounding.

</details>


### [76] [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
*Wenxuan Xie,Yaxun Dai,Wenhao Jiang*

Main category: cs.CL

TL;DR: SDE-SQL enables large language models to autonomously explore databases during inference, improving execution accuracy on the BIRD benchmark.


<details>
  <summary>Details</summary>
Motivation: Prior approaches rely on static database information, limiting the model's understanding of the database contents. SDE-SQL addresses this limitation by allowing models to dynamically interact with databases.

Method: SDE-SQL generates and executes SQL probes to actively retrieve information from the database and iteratively update its understanding of the data.

Result: SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the baseline without using supervised fine-tuning or model ensembling.

Conclusion: SDE-SQL sets a new state-of-the-art for open-source models without SFT or model ensembling, and further enhancements are possible with SFT.

Abstract: Recent advancements in large language models (LLMs) have significantly
improved performance on the Text-to-SQL task. However, prior approaches
typically rely on static, pre-processed database information provided at
inference time, which limits the model's ability to fully understand the
database contents. Without dynamic interaction, LLMs are constrained to fixed,
human-provided context and cannot autonomously explore the underlying data. To
address this limitation, we propose SDE-SQL, a framework that enables large
language models to perform self-driven exploration of databases during
inference. This is accomplished by generating and executing SQL probes, which
allow the model to actively retrieve information from the database and
iteratively update its understanding of the data. Unlike prior methods, SDE-SQL
operates in a zero-shot setting, without relying on any question-SQL pairs as
in-context demonstrations. When evaluated on the BIRD benchmark with
Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in
execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing
a new state-of-the-art among methods based on open-source models without
supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the
performance of SDE-SQL can be further enhanced, yielding an additional 0.52%
improvement.

</details>


### [77] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Raviraj Joshi*

Main category: cs.CL

TL;DR: Propose a sentence ranking method based on TF-IDF to improve efficiency for long document classification.


<details>
  <summary>Details</summary>
Motivation: To address the computational limitations of transformer-based models like BERT, which are constrained by fixed input lengths and quadratic attention complexity, and to improve efficiency by selecting only the necessary information from long documents.

Method: A TF-IDF-based sentence ranking method that selects the most informative content from long documents.

Result: The method achieves near-identical classification accuracy with just a 0.33 percent drop compared to the full-context baseline, while reducing input size by over 50 percent and inference latency by 43 percent.

Conclusion: The proposed TF-IDF-based sentence ranking method can significantly reduce the input size and inference latency while maintaining classification accuracy.

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [78] [Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages](https://arxiv.org/abs/2506.07249)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: This study adapts an information-theoretic bias attribution score metric for agglutinative languages, specifically Filipino, demonstrating its effectiveness on both a Filipino-only and three multilingual models. Results indicate that Filipino models are influenced by words related to people, objects, and relationships, contrasting with English models that focus more on actions.


<details>
  <summary>Details</summary>
Motivation: To understand how tokens contribute to biased behavior in language models, especially focusing on non-English, agglutinative languages like Filipino.

Method: Adapting an information-theoretic bias attribution score metric for Filipino models and testing it on a Filipino-only model and three multilingual models.

Result: Filipino models are driven towards bias by words about people, objects, and relationships, differing from English models which are more influenced by action-related themes.

Conclusion: The study reveals differences in how English and non-English models handle bias related to sociodemographic groups.

Abstract: Emerging research on bias attribution and interpretability have revealed how
tokens contribute to biased behavior in language models processing English
texts. We build on this line of inquiry by adapting the information-theoretic
bias attribution score metric for implementation on models handling
agglutinative languages, particularly Filipino. We then demonstrate the
effectiveness of our adapted method by using it on a purely Filipino model and
on three multilingual models: one trained on languages worldwide and two on
Southeast Asian data. Our results show that Filipino models are driven towards
bias by words pertaining to people, objects, and relationships, entity-based
themes that stand in contrast to the action-heavy nature of bias-contributing
themes in English (i.e., criminal, sexual, and prosocial behaviors). These
findings point to differences in how English and non-English models process
inputs linked to sociodemographic groups and bias.

</details>


### [79] [Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs](https://arxiv.org/abs/2506.07270)
*Atahan Özer,Çağatay Yıldız*

Main category: cs.CL

TL;DR: Large language models face challenges with continuously evolving real-world information due to their fixed pre-training data. The paper introduces two benchmarks, Temporal Wiki and Unified Clark, showing that LLMs struggle with conflicting or outdated facts. A proposed agent-based framework improves performance by incrementally building an external memory.


<details>
  <summary>Details</summary>
Motivation: Updating knowledge in large language models is costly and impractical due to the continuous evolution of real-world information.

Method: The paper introduces two new benchmarks (Temporal Wiki and Unified Clark) and proposes a lightweight, agentic framework for incrementally building structured external memory.

Result: The proposed method outperforms in-context learning and retrieval-augmented generation baselines, particularly on complex reasoning tasks involving conflicting facts.

Conclusion: This work highlights the limitations of current large language models in handling temporal knowledge and presents a promising approach to improve their performance through an agent-based external memory system.

Abstract: Large language models (LLMs) exhibit remarkable capabilities in question
answering and reasoning thanks to their extensive parametric memory. However,
their knowledge is inherently limited by the scope of their pre-training data,
while real-world information evolves continuously. Updating this knowledge
typically requires costly and brittle re-training, or in-context learning
(ICL), which becomes impractical at scale given the volume and volatility of
modern information. Motivated by these limitations, we investigate how LLMs
perform when exposed to temporal text corpora, or documents that reflect
evolving knowledge over time, such as sports biographies where facts like a
player's "current team" change year by year. To this end, we introduce two new
benchmarks: Temporal Wiki, which captures factual drift across historical
Wikipedia snapshots, and Unified Clark, which aggregates timestamped news
articles to simulate real-world information accumulation. Our analysis reveals
that LLMs often struggle to reconcile conflicting or outdated facts and can be
misled when multiple versions of a fact appear in context. To address these
issues, we propose a lightweight, agentic framework that incrementally builds a
structured, external memory from source documents without requiring
re-training. This knowledge organization strategy enables models to retrieve
and reason over temporally filtered, relevant information at inference time.
Empirically, our method outperforms ICL and RAG baselines across both
benchmarks, especially on questions requiring more complex reasoning or
integration of conflicting facts.

</details>


### [80] [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)
*Olga Kellert,Nemika Tyagi,Muhammad Imran,Nelvin Licona-Guevara,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: This paper introduces BiLingua Parser, an LLM-based tool that creates Universal Dependencies annotations for code-switched text in Spanish-English and Spanish-Guaraní, achieving high accuracy and outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: To improve syntactic analysis in low-resource, code-switched language settings where annotated data is limited.

Method: Developing a prompt-based framework using few-shot LLM prompting combined with expert review to create annotated datasets for Spanish-English and Spanish-Guaraní, then conducting a syntactic analysis of switch points.

Result: The BiLingua Parser achieved up to 95.29% LAS after expert revision, significantly surpassing previous baselines and multilingual parsers.

Conclusion: LLMs can be effective tools for creating syntactic resources in under-resourced, code-switched environments when properly guided.

Abstract: Code-switching presents a complex challenge for syntactic analysis,
especially in low-resource language settings where annotated data is scarce.
While recent work has explored the use of large language models (LLMs) for
sequence-level tagging, few approaches systematically investigate how well
these models capture syntactic structure in code-switched contexts. Moreover,
existing parsers trained on monolingual treebanks often fail to generalize to
multilingual and mixed-language input. To address this gap, we introduce the
BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal
Dependencies (UD) annotations for code-switched text. First, we develop a
prompt-based framework for Spanish-English and Spanish-Guaran\'i data,
combining few-shot LLM prompting with expert review. Second, we release two
annotated datasets, including the first Spanish-Guaran\'i UD-parsed corpus.
Third, we conduct a detailed syntactic analysis of switch points across
language pairs and communicative contexts. Experimental results show that
BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly
outperforming prior baselines and multilingual parsers. These results show that
LLMs, when carefully guided, can serve as practical tools for bootstrapping
syntactic resources in under-resourced, code-switched environments. Data and
source code are available at https://github.com/N3mika/ParsingProject

</details>


### [81] [Exploring the Impact of Temperature on Large Language Models:Hot or Cold?](https://arxiv.org/abs/2506.07295)
*Lujun Li,Lama Sleem,Niccolo' Gentile,Geoffrey Nichil,Radu State*

Main category: cs.CL

TL;DR: This study investigates the impact of sampling temperature on large language models' performance across different capabilities, model sizes, and precisions. It introduces a BERT-based temperature selector to optimize temperature for specific prompts and reveals that the Mutation Temperature increases with model size.


<details>
  <summary>Details</summary>
Motivation: To challenge the Stochastic Parrots analogy and explore the role of randomness in model inference.

Method: Systematically evaluated temperature impact on datasets assessing six capabilities using open-source models of various sizes, analyzed statistical results, and proposed a BERT-based temperature selector.

Result: Found distinct skill-specific effects of temperature on model performance, showed that the proposed selector improves small and medium models' performance in SuperGLUE datasets, and found that temperature effects are consistent across different precisions with Mutation Temperature increasing with model size.

Conclusion: Optimal temperature selection is complex and influenced by model size and capability. The proposed selector enhances performance for smaller models and shows that temperature effects are consistent across precisions.

Abstract: The sampling temperature, a critical hyperparameter in large language models
(LLMs), modifies the logits before the softmax layer, thereby reshaping the
distribution of output tokens. Recent studies have challenged the Stochastic
Parrots analogy by demonstrating that LLMs are capable of understanding
semantics rather than merely memorizing data and that randomness, modulated by
sampling temperature, plays a crucial role in model inference. In this study,
we systematically evaluated the impact of temperature in the range of 0 to 2 on
data sets designed to assess six different capabilities, conducting statistical
analyses on open source models of three different sizes: small (1B--4B), medium
(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific
effects of temperature on model performance, highlighting the complexity of
optimal temperature selection in practical applications. To address this
challenge, we propose a BERT-based temperature selector that takes advantage of
these observed effects to identify the optimal temperature for a given prompt.
We demonstrate that this approach can significantly improve the performance of
small and medium models in the SuperGLUE datasets. Furthermore, our study
extends to FP16 precision inference, revealing that temperature effects are
consistent with those observed in 4-bit quantized models. By evaluating
temperature effects up to 4.0 in three quantized models, we find that the
Mutation Temperature -- the point at which significant performance changes
occur -- increases with model size.

</details>


### [82] [Subjectivity in the Annotation of Bridging Anaphora](https://arxiv.org/abs/2506.07297)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: This paper explores subjectivity in annotating bridging instances in discourse, finding previous resources may be under-annotated and agreement on identifying all instances is low due to subjective interpretations.


<details>
  <summary>Details</summary>
Motivation: To explore the subjectivity in the annotation of bridging instances at three levels: anaphor recognition, antecedent resolution, and bridging subtype selection.

Method: Annotation pilot on the test set of the existing GUM corpus and proposing a new classification system for bridging subtypes.

Result: Moderate agreement on bridging subtype category but low annotator overlap for exhaustively identifying bridging instances. Many disagreements were due to subjective understanding of the entities.

Conclusion: The paper concludes that some previous resources might be under-annotated and there's low agreement on identifying all instances of bridging.

Abstract: Bridging refers to the associative relationship between inferable entities in
a discourse and the antecedents which allow us to understand them, such as
understanding what "the door" means with respect to an aforementioned "house".
As identifying associative relations between entities is an inherently
subjective task, it is difficult to achieve consistent agreement in the
annotation of bridging anaphora and their antecedents. In this paper, we
explore the subjectivity involved in the annotation of bridging instances at
three levels: anaphor recognition, antecedent resolution, and bridging subtype
selection. To do this, we conduct an annotation pilot on the test set of the
existing GUM corpus, and propose a newly developed classification system for
bridging subtypes, which we compare to previously proposed schemes. Our results
suggest that some previous resources are likely to be severely under-annotated.
We also find that while agreement on the bridging subtype category was
moderate, annotator overlap for exhaustively identifying instances of bridging
is low, and that many disagreements resulted from subjective understanding of
the entities involved.

</details>


### [83] [ConfQA: Answer Only If You Are Confident](https://arxiv.org/abs/2506.07309)
*Yin Huang,Yifan Ethan Xu,Kai Sun,Vera Yan,Alicia Sun,Haidar Khan,Jimmy Nguyen,Mohammad Kachuee,Zhaojiang Lin,Yue Liu,Aaron Colak,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: We present ConfQA, a fine-tuning strategy that reduces hallucination rates in large language models from 20-40% to under 5%. This is achieved by training the model to either provide correct answers or admit uncertainty. The effectiveness is enhanced by using a guiding prompt and leveraging factual statements from knowledge graphs for confidence calibration. Additionally, we propose the Dual Neural Knowledge framework, improving accuracy to over 95% while reducing unnecessary external retrievals by more than 30%.


<details>
  <summary>Details</summary>
Motivation: To develop a method to reduce hallucination rates in large language models.

Method: ConfQA fine-tuning strategy combined with a guiding prompt and factual statements from knowledge graphs for confidence calibration. Also proposes the Dual Neural Knowledge framework.

Result: Reduces hallucination rates from 20-40% to under 5%. Improves accuracy to over 95% and reduces unnecessary external retrievals by more than 30%.

Conclusion: ConfQA is an effective method for reducing hallucinations in large language models and can be enhanced with the Dual Neural Knowledge framework.

Abstract: Can we teach Large Language Models (LLMs) to refrain from hallucinating
factual statements? In this paper we present a fine-tuning strategy that we
call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across
multiple factuality benchmarks. The core idea is simple: when the LLM answers a
question correctly, it is trained to continue with the answer; otherwise, it is
trained to admit "I am unsure". But there are two key factors that make the
training highly effective. First, we introduce a dampening prompt "answer only
if you are confident" to explicitly guide the behavior, without which
hallucination remains high as 15%-25%. Second, we leverage simple factual
statements, specifically attribute values from knowledge graphs, to help LLMs
calibrate the confidence, resulting in robust generalization across domains and
question types. Building on this insight, we propose the Dual Neural Knowledge
framework, which seamlessly select between internally parameterized neural
knowledge and externally recorded symbolic knowledge based on ConfQA's
confidence. The framework enables potential accuracy gains to beyond 95%, while
reducing unnecessary external retrievals by over 30%.

</details>


### [84] [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
*Brian Christian,Hannah Rose Kirk,Jessica A. F. Thompson,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.CL

TL;DR: This paper presents a new way to understand reward models used to align large language models with human values. The researchers looked at how these models rate all possible one-word answers to important questions. They found big differences between models, some biases, and issues with common words being valued too highly. These findings suggest that reward models might not be as interchangeable or reliable as thought before.


<details>
  <summary>Details</summary>
Motivation: To study the interpretability of reward models that encode human value judgments.

Method: Examining how different reward models score every possible single-token response to value-laden prompts across their entire vocabulary space.

Result: Found substantial heterogeneity between models, systematic asymmetries in token scoring, sensitivity to prompt framing, and overvaluation of frequent tokens. Demonstrated these effects across ten recent open-source reward models.

Conclusion: The study challenges assumptions about the interchangeability of reward models and their suitability as proxies of complex and context-dependent human values.

Abstract: Reward modeling has emerged as a crucial component in aligning large language
models with human values. Significant attention has focused on using reward
models as a means for fine-tuning generative models. However, the reward models
themselves -- which directly encode human value judgments by turning
prompt-response pairs into scalar rewards -- remain relatively understudied. We
present a novel approach to reward model interpretability through exhaustive
analysis of their responses across their entire vocabulary space. By examining
how different reward models score every possible single-token response to
value-laden prompts, we uncover several striking findings: (i) substantial
heterogeneity between models trained on similar objectives, (ii) systematic
asymmetries in how models encode high- vs low-scoring tokens, (iii) significant
sensitivity to prompt framing that mirrors human cognitive biases, and (iv)
overvaluation of more frequent tokens. We demonstrate these effects across ten
recent open-source reward models of varying parameter counts and architectures.
Our results challenge assumptions about the interchangeability of reward
models, as well as their suitability as proxies of complex and
context-dependent human values. We find that these models can encode concerning
biases toward certain identity groups, which may emerge as unintended
consequences of harmlessness training -- distortions that risk propagating
through the downstream large language models now deployed to millions.

</details>


### [85] [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)
*Anyi Wang,Dong Shu,Yifan Wang,Yunpu Ma,Mengnan Du*

Main category: cs.CL

TL;DR: Introduce SRPS, a new framework that enhances LLMs' reasoning ability by identifying and manipulating internal model features related to role-playing behavior.


<details>
  <summary>Details</summary>
Motivation: Existing role-playing methods for LLMs rely on prompt engineering, which lacks stability and interpretability.

Method: Sparse Autoencoder Role-Playing Steering (SRPS) identifies and manipulates internal model features related to role-playing behavior.

Result: SRPS shows consistent performance improvements across various reasoning benchmarks and model sizes.

Conclusion: SRPS enhances the reasoning ability of LLMs with better interpretability and stability.

Abstract: Role-playing has emerged as an effective technique for enhancing the
reasoning capabilities of large language models (LLMs). However, existing
methods primarily rely on prompt engineering, which often lacks stability and
interpretability. In this paper, we introduce Sparse Autoencoder Role-Playing
Steering (SRPS), a novel framework that identifies and manipulates internal
model features associated with role-playing behavior. Our approach extracts
latent representations from role-play prompts, selects the most relevant
features based on activation patterns, and constructs a steering vector that
can be injected into the model's residual stream with controllable intensity.
Our method enables fine-grained control over role-specific behavior and offers
insights into how role information influences internal model activations.
Extensive experiments across various reasoning benchmarks and model sizes
demonstrate consistent performance gains. Notably, in the zero-shot
chain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves
from 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to
45.10%. These results highlight the potential of SRPS to enhance reasoning
ability in LLMs, providing better interpretability and stability compared to
traditional prompt-based role-playing.

</details>


### [86] [Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation](https://arxiv.org/abs/2506.07356)
*Seokil Ham,Yubin Choi,Seungju Cho,Yujin Yang,Younghun Kim,Changick Kim*

Main category: cs.CL

TL;DR: Finetuning-as-a-Service面临用户数据中的有害提示导致LLM安全性降低的问题。本文提出了一种基于拒绝特征的教师模型(ReFT)，通过过滤有害提示并蒸馏对齐知识来提高用户特定任务的微调准确性。实验表明该方法能有效减少有害输出，提供了一个安全可靠部署LLM的实际解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有Finetuning-as-a-Service在处理用户数据中的有害提示时存在安全性问题。

Method: 提出基于拒绝特征的教师模型(ReFT)，利用其拒绝特征识别有害提示并作为教师模型过滤有害数据和蒸馏知识。

Result: ReFT方法有效减少了有害输出，并提高了用户特定任务的微调准确性。

Conclusion: ReFT提供了一种实用的安全可靠的LLM部署方案，适用于Finetuning-as-a-Service。

Abstract: Recently, major AI service providers such as Google and OpenAI have
introduced Finetuning-as-a-Service, which enables users to customize Large
Language Models (LLMs) for specific downstream tasks using their own data.
However, this service is vulnerable to degradation of LLM safety-alignment when
user data contains harmful prompts. While some prior works address this issue,
fundamentally filtering harmful data from user data remains unexplored.
Motivated by our observation that a directional representation reflecting
refusal behavior (called the refusal feature) obtained from safety-aligned LLMs
can inherently distinguish between harmful and harmless prompts, we propose the
Refusal-Feature-guided Teacher (ReFT). Our ReFT model is trained to identify
harmful prompts based on the similarity between input prompt features and its
refusal feature. During finetuning, the ReFT model serves as a teacher that
filters harmful prompts from user data and distills alignment knowledge into
the base model. Extensive experiments demonstrate that our ReFT-based
finetuning strategy effectively minimizes harmful outputs and enhances
finetuning accuracy for user-specific tasks, offering a practical solution for
secure and reliable deployment of LLMs in Finetuning-as-a-Service.

</details>


### [87] [SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation](https://arxiv.org/abs/2506.07423)
*Janghyeon Yun,Sang-goo Lee*

Main category: cs.CL

TL;DR: This paper introduces SEED, a system that automatically generates evidence to enhance text-to-SQL performance and practical usability, improving SQL generation accuracy in no-evidence scenarios.


<details>
  <summary>Details</summary>
Motivation: Current text-to-SQL studies rely on the BIRD dataset which assumes user expertise and has defects in human-generated evidence, contradicting the goal of making data retrieval accessible to non-experts.

Method: SEED automatically generates evidence by analyzing database schema, description files, and values.

Result: SEED significantly improves SQL generation accuracy in no-evidence scenarios and sometimes outperforms settings with BIRD evidence.

Conclusion: SEED-generated evidence enhances the adaptability and robustness of text-to-SQL models, bridging the gap between research and real-world deployment.

Abstract: Text-to-SQL enables non-experts to retrieve data from databases by converting
natural language queries into SQL. However, state-of-the-art text-to-SQL
studies rely on the BIRD dataset, which assumes that evidence is provided along
with questions. Although BIRD facilitates research advancements, it assumes
that users have expertise and domain knowledge, contradicting the fundamental
goal of text-to-SQL. In addition, human-generated evidence in BIRD contains
defects, including missing or erroneous evidence, which affects model
performance. To address this issue, we propose SEED (System for Evidence
Extraction and Domain knowledge generation), an approach that automatically
generates evidence to improve performance and practical usability in real-world
scenarios. SEED systematically analyzes database schema, description files, and
values to extract relevant information. We evaluated SEED on BIRD and Spider,
demonstrating that it significantly improves SQL generation accuracy in the
no-evidence scenario, and in some cases, even outperforms the setting where
BIRD evidence is provided. Our results highlight that SEED-generated evidence
not only bridges the gap between research and real-world deployment but also
improves the adaptability and robustness of text-to-SQL models. Our code is
available at https://github.com/felix01189/SEED

</details>


### [88] [Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models](https://arxiv.org/abs/2506.07424)
*Kyeonghyun Kim,Jinhee Jang,Juhwan Choi,Yoonji Lee,Kyohoon Jin,YoungBin Kim*

Main category: cs.CL

TL;DR: Introduces PiFi, a framework combining large and small language models to improve performance on natural language processing tasks without significantly increasing computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the issue where large language models have high computational demands making them unsuitable for resource-constrained environments, while small language models lack the broad generalization capacity of large models.

Method: Proposing PiFi, a framework that integrates a single frozen layer from a large language model into a small language model and fine-tunes the combined model for specific tasks.

Result: PiFi shows consistent performance improvements across various natural language processing tasks, including both natural language understanding and generation. It also demonstrates the ability to effectively leverage large language model knowledge, enhancing generalization to unseen domains and facilitating the transfer of linguistic abilities.

Conclusion: The proposed PiFi framework successfully bridges the gap between large and small language models, achieving high performance while maintaining efficiency.

Abstract: Large language models (LLMs) are renowned for their extensive linguistic
knowledge and strong generalization capabilities, but their high computational
demands make them unsuitable for resource-constrained environments. In
contrast, small language models (SLMs) are computationally efficient but often
lack the broad generalization capacity of LLMs. To bridge this gap, we propose
PiFi, a novel framework that combines the strengths of both LLMs and SLMs to
achieve high performance while maintaining efficiency. PiFi integrates a single
frozen layer from an LLM into a SLM and fine-tunes the combined model for
specific tasks, boosting performance without a significant increase in
computational cost. We show that PiFi delivers consistent performance
improvements across a range of natural language processing tasks, including
both natural language understanding and generation. Moreover, our findings
demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing
generalization to unseen domains and facilitating the transfer of linguistic
abilities.

</details>


### [89] [Conjoined Predication and Scalar Implicature](https://arxiv.org/abs/2506.07429)
*Ratna Kandala*

Main category: cs.CL

TL;DR: This paper resolves a linguistic puzzle about why certain conjunctive sentences sound odd by analyzing their collective reading and proposing an extension to scalar implicature generation theories.


<details>
  <summary>Details</summary>
Motivation: To resolve the first puzzle arising from conjunction that Magri (2016) left unresolved, which involves a hidden interaction among quantification, collective/concurrent interpretation, and contextual updating dimensions.

Method: Conceptual analysis within Magri's original theoretical framework.

Result: The paper identifies that the oddness in sentences like '(Only) Some Italians come from a warm country and are blond' stems from the collective or concurrent reading of the conjunctive predicate, creating an indirect contextual contradiction. It also proposes expanding the current understanding of scalar implicature generation.

Conclusion: The paper concludes that the oddness in certain conjunctive sentences comes from the collective or concurrent reading of the conjunctive predicate, which leads to an indirect contextual contradiction. The authors also argue for extending the understanding of scalar implicature generation beyond exhaustification-based grammatical licensing accounts.

Abstract: Magri (2016) investigates two puzzles arising from conjunction. Although
Magri has proposed a solution to the second puzzle, the first remains
unresolved. This first puzzle reveals a hidden interaction among
quantification, collective/concurrent interpretation, and contextual updating
dimensions that have yet to be explored. In essence, the problem is that
certain forms of sentences like "Some Italians come from a warm country," when
conjoined as in "(Only) Some Italians come from a warm country and are blond,"
sound infelicitous, even though no obvious alternative triggers a conflicting
scalar implicature. In this paper, we offer a conceptual analysis of Magri's
first puzzle by situating it within its original theoretical framework. We
argue that the oddness arises from the collective or concurrent reading of the
conjunctive predicate: in examples such as "(Only) Some Italians come from a
warm country and are blond," this interpretation generates an indirect
contextual contradiction. Moreover, we suggest that the pragmatic mechanisms
governing scalar implicature generation extend beyond what is captured by
exhaustification-based grammatical licensing accounts.

</details>


### [90] [Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434)
*Feifan Song,Shaohang Wei,Wen Luo,Yuxuan Fan,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: Propose a Weak-to-Strong Decoding (WSD) framework to improve LLM alignment by using a small model to guide the beginning of responses, showing better performance than baseline methods without harming downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of obtaining high-quality and aligned content in low-resource methods for LLM alignment, focusing on the difficulty of generating aligned responses at the beginning of decoding.

Method: WSD uses a small model to draft well-aligned beginnings and a large base model to continue the rest, controlled by an auto-switch mechanism.

Result: WSD enhances different base models to outperform all baseline methods without causing degradation on downstream tasks.

Conclusion: The proposed Weak-to-Strong Decoding (WSD) framework improves the alignment ability of base models by using a small aligned model's guidance.

Abstract: Large Language Models (LLMs) require alignment with human preferences to
avoid generating offensive, false, or meaningless content. Recently,
low-resource methods for LLM alignment have been popular, while still facing
challenges in obtaining both high-quality and aligned content. Motivated by the
observation that the difficulty of generating aligned responses is concentrated
at the beginning of decoding, we propose a novel framework, Weak-to-Strong
Decoding (WSD), to enhance the alignment ability of base models by the guidance
of a small aligned model. The small model first drafts well-aligned beginnings,
followed by the large base model to continue the rest, controlled by a
well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,
to fine-tune a small-sized Pilot-3B as the draft model, which effectively
enhances different base models under the WSD framework to outperform all
baseline methods, while avoiding degradation on downstream tasks, termed as the
alignment tax. Extensive experiments are further conducted to examine the
impact of different settings and time efficiency, as well as analyses on the
intrinsic mechanisms of WSD in depth.

</details>


### [91] [LG-ANNA-Embedding technical report](https://arxiv.org/abs/2506.07438)
*Jooyoung Choi,Hyun Kim,Hansol Jang,Changwook Jun,Kyunghoon Bae,Hyewon Choi,Stanley Jungkyu Choi,Honglak Lee,Chulmin Yun*

Main category: cs.CL

TL;DR: 提出了一种基于指令的大规模语言模型框架，用于生成上下文感知的文本嵌入，无需任务特定的微调。该方法在多个非信息检索和信息检索任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要针对每个任务进行微调，缺乏通用性，本研究旨在通过一种统一的方法解决这一问题。

Method: 利用Mistral-7B模型，结合上下文学习、软监督和自适应硬负样本挖掘技术来生成上下文感知的嵌入。

Result: 在MTEB基准测试的41个任务中表现出色，排名靠前，超过了多个更大或完全微调的基线模型。

Conclusion: 证明了结合上下文提示、软监督和自适应采样对于可扩展高质量嵌入生成的有效性。

Abstract: This report presents a unified instruction-based framework for learning
generalized text embeddings optimized for both information retrieval (IR) and
non-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our
approach combines in-context learning, soft supervision, and adaptive
hard-negative mining to generate context-aware embeddings without task-specific
fine-tuning. Structured instructions and few-shot examples are used to guide
the model across diverse tasks, enabling strong performance on classification,
semantic similarity, clustering, and reranking benchmarks. To improve semantic
discrimination, we employ a soft labeling framework where continuous relevance
scores, distilled from a high-performance dense retriever and reranker, serve
as fine-grained supervision signals. In addition, we introduce adaptive
margin-based hard-negative mining, which filters out semantically ambiguous
negatives based on their similarity to positive examples, thereby enhancing
training stability and retrieval robustness. Our model is evaluated on the
newly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven
categories. Results show that our method achieves strong generalization and
ranks among the top-performing models by Borda score, outperforming several
larger or fully fine-tuned baselines. These findings highlight the
effectiveness of combining in-context prompting, soft supervision, and adaptive
sampling for scalable, high-quality embedding generation.

</details>


### [92] [Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling](https://arxiv.org/abs/2506.07453)
*Pritom Saha Akash,Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: Introduces DALTA, a novel method for domain-adaptive topic modeling that enhances coherence and stability in low-data scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing topic modeling approaches face challenges in low-resource settings due to unstable and incoherent topic inference caused by limited target-domain data.

Method: DALTA (Domain-Aligned Latent Topic Adaptation), which uses a shared encoder for domain-invariant features, specialized decoders for domain-specific nuances, and adversarial alignment to transfer relevant information.

Result: DALTA outperforms other state-of-the-art methods in terms of topic coherence, stability, and transferability across various low-resource datasets.

Conclusion: Domain adaptation is crucial for improving topic modeling in low-resource settings.

Abstract: Topic modeling plays a vital role in uncovering hidden semantic structures
within text corpora, but existing models struggle in low-resource settings
where limited target-domain data leads to unstable and incoherent topic
inference. We address this challenge by formally introducing domain adaptation
for low-resource topic modeling, where a high-resource source domain informs a
low-resource target domain without overwhelming it with irrelevant content. We
establish a finite-sample generalization bound showing that effective knowledge
transfer depends on robust performance in both domains, minimizing latent-space
discrepancy, and preventing overfitting to the data. Guided by these insights,
we propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that
employs a shared encoder for domain-invariant features, specialized decoders
for domain-specific nuances, and adversarial alignment to selectively transfer
relevant information. Experiments on diverse low-resource datasets demonstrate
that DALTA consistently outperforms state-of-the-art methods in terms of topic
coherence, stability, and transferability.

</details>


### [93] [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
*Yuxin Xiao,Shan Chen,Jack Gallifant,Danielle Bitterman,Thomas Hartvigsen,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: A new method called KScope is introduced to characterize LLM knowledge into five statuses based on consistency and correctness.


<details>
  <summary>Details</summary>
Motivation: Prior methods do not fully reflect how well a model knows the answer to a question due to knowledge conflicts.

Method: KScope is a hierarchical framework of statistical tests that progressively refines hypotheses about knowledge modes.

Result: KScope is applied to nine LLMs across four datasets and establishes several findings about knowledge updates and generalization.

Conclusion: The study provides insights into how supporting context and feature preferences impact LLM knowledge characterization.

Abstract: Characterizing a large language model's (LLM's) knowledge of a given question
is challenging. As a result, prior work has primarily examined LLM behavior
under knowledge conflicts, where the model's internal parametric memory
contradicts information in the external context. However, this does not fully
reflect how well the model knows the answer to the question. In this paper, we
first introduce a taxonomy of five knowledge statuses based on the consistency
and correctness of LLM knowledge modes. We then propose KScope, a hierarchical
framework of statistical tests that progressively refines hypotheses about
knowledge modes and characterizes LLM knowledge into one of these five
statuses. We apply KScope to nine LLMs across four datasets and systematically
establish: (1) Supporting context narrows knowledge gaps across models. (2)
Context features related to difficulty, relevance, and familiarity drive
successful knowledge updates. (3) LLMs exhibit similar feature preferences when
partially correct or conflicted, but diverge sharply when consistently wrong.
(4) Context summarization constrained by our feature analysis, together with
enhanced credibility, further improves update effectiveness and generalizes
across LLMs.

</details>


### [94] [From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered](https://arxiv.org/abs/2506.07461)
*Siddartha Devic,Tejas Srinivasan,Jesse Thomason,Willie Neiswanger,Vatsal Sharan*

Main category: cs.CL

TL;DR: This paper discusses issues with current uncertainty quantification (UQ) methods in Large Language Models (LLMs), arguing they are not ideal for real-world human decision-making. It identifies three main problems: reliance on benchmarks lacking real-world relevance, focus solely on epistemic uncertainty, and use of metrics not tied to practical utility. The authors suggest adopting a more human-focused approach to UQ in LLMs.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability of LLMs in real-world applications by enhancing human-LLM collaboration through better uncertainty quantification.

Method: Analyzed 40 LLM UQ methods and identified three common practices that hinder progress towards useful UQ for human users.

Result: Found that current practices evaluate on unrealistic benchmarks, ignore aleatoric uncertainty, and optimize irrelevant metrics. Proposed user-centric solutions.

Conclusion: Advocates for a human-centered approach to LLM uncertainty quantification instead of focusing on unrepresentative tasks and imperfect metrics.

Abstract: Large Language Models (LLMs) are increasingly assisting users in the real
world, yet their reliability remains a concern. Uncertainty quantification (UQ)
has been heralded as a tool to enhance human-LLM collaboration by enabling
users to know when to trust LLM predictions. We argue that current practices
for uncertainty quantification in LLMs are not optimal for developing useful UQ
for human users making decisions in real-world tasks. Through an analysis of 40
LLM UQ methods, we identify three prevalent practices hindering the community's
progress toward its goal of benefiting downstream users: 1) evaluating on
benchmarks with low ecological validity; 2) considering only epistemic
uncertainty; and 3) optimizing metrics that are not necessarily indicative of
downstream utility. For each issue, we propose concrete user-centric practices
and research directions that LLM UQ researchers should consider. Instead of
hill-climbing on unrepresentative tasks using imperfect metrics, we argue that
the community should adopt a more human-centered approach to LLM uncertainty
quantification.

</details>


### [95] [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models](https://arxiv.org/abs/2506.07463)
*Guang Liu,Liangdong Wang,Jijie Li,Yang Yu,Yao Xu,Jiabei Chen,Yu Bai,Feng Liao,Yonghua Lin*

Main category: cs.CL

TL;DR: Introduce CCI4.0, a large-scale bilingual pre-training dataset with high data quality and diverse reasoning patterns. It consists of two parts: CCI4.0-M2-Base and CCI4.0-M2-CoT. The former is combined from multiple sources including Chinese web corpus and English subset from Nemotron-CC, while the latter contains 4.5 billion CoT templates. The dataset improves LLMs' performance in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality bilingual pre-training dataset for language models.

Method: Propose a novel pipeline for data quality justification mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. Extract CoT templates in a staged way.

Result: Empirical evaluations show that LLMs pre-trained in CCI4.0 yield consistent improvements in downstream tasks, especially in math and code reflection tasks.

Conclusion: Rigorous data curation and human thinking templates play a critical role in advancing LLM performance.

Abstract: We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered
for superior data quality and diverse human-like reasoning trajectory. CCI4.0
occupies roughly $35$ TB of disk space and comprises two sub-datasets:
CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully
curated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and
diverse sources from math, wiki, arxiv, and code. Although these data are
mostly sourced from well-processed datasets, the quality standards of various
domains are dynamic and require extensive expert experience and labor to
process. So, we propose a novel pipeline justifying data quality mainly based
on models through two-stage deduplication, multiclassifier quality scoring, and
domain-aware fluency filtering. We extract $4.5$ billion pieces of
CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the
distillation of CoT from larger models, our proposed staged CoT extraction
exemplifies diverse reasoning patterns and significantly decreases the
possibility of hallucination. Empirical evaluations demonstrate that LLMs
pre-trained in CCI4.0 benefit from cleaner, more reliable training signals,
yielding consistent improvements in downstream tasks, especially in math and
code reflection tasks. Our results underscore the critical role of rigorous
data curation and human thinking templates in advancing LLM performance,
shedding some light on automatically processing pretraining corpora.

</details>


### [96] [Improving Fairness of Large Language Models in Multi-document Summarization](https://arxiv.org/abs/2506.07479)
*Haoyuan Li Yusen Zhang,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 提出了一种新的方法FairPO，用于多文档摘要中的公平性问题，该方法在摘要级和语料库级上都进行了改进，并优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在多文档摘要中实现公平性至关重要，因为它能提供跨多样化社会属性值文档的全面观点，这对决策有重要影响。例如，倾向于过多代表负面产品评论的摘要系统可能会误导客户忽视优质产品。

Method: 提出了一种称为FairPO的偏好调节方法，该方法专注于多文档摘要中的摘要级和语料库级公平性。为了提高摘要级公平性，通过扰动文档集生成偏好对；为了提高语料库级公平性，通过动态调整偏好对的权重来进行公平意识的偏好调节。

Result: 提出的FairPO方法在提高多文档摘要的公平性方面表现良好，并且在保持摘要关键质量的同时超过了强大的基线模型。

Conclusion: 提出的方法FairPO在多文档摘要的公平性方面表现出色，同时保持了摘要的关键质量。实验结果表明FairPO优于强大的基线模型。

Abstract: Fairness in multi-document summarization (MDS) is crucial for providing
comprehensive views across documents with diverse social attribute values,
which can significantly impact decision-making. For example, a summarization
system that tends to overrepresent negative reviews of products can mislead
customers into disregarding good products. Previous works measure fairness in
MDS at two levels: summary-level and corpus-level. While summary-level fairness
focuses on individual summaries, corpus-level fairness focuses on a corpus of
summaries. Recent methods primarily focus on summary-level fairness. We propose
FairPO, a preference tuning method that focuses on both summary-level and
corpus-level fairness in MDS. To improve summary-level fairness, we propose to
generate preference pairs by perturbing document sets. To improve corpus-level
fairness, we propose fairness-aware preference tuning by dynamically adjusting
the weights of preference pairs. Our experiments show that FairPO outperforms
strong baselines while maintaining the critical qualities of summaries. The
code is available at https://github.com/leehaoyuan/coverage_fairnes.

</details>


### [97] [A Hybrid GA LLM Framework for Structured Task Optimization](https://arxiv.org/abs/2506.07483)
*Berry Feng,Jonas Lin,Patrick Lau*

Main category: cs.CL

TL;DR: A hybrid framework called GA LLM combines Genetic Algorithms and Large Language Models for structured generation tasks under strict constraints. It uses language models for domain knowledge and creative variation, and genetic algorithms for structural integrity and optimization. It's effective in tasks like itinerary planning, academic outlining, and business reporting.


<details>
  <summary>Details</summary>
Motivation: To handle structured generation tasks under strict constraints.

Method: Combining Genetic Algorithms with Large Language Models where each output is treated as a gene and evolutionary operations are guided by the language model.

Result: Proven effective in tasks such as itinerary planning, academic outlining, and business reporting, consistently producing well-structured and requirement-satisfying results.

Conclusion: GA LLM achieves better constraint satisfaction and higher quality solutions by combining the strengths of both genetic algorithms and large language models.

Abstract: GA LLM is a hybrid framework that combines Genetic Algorithms with Large
Language Models to handle structured generation tasks under strict constraints.
Each output, such as a plan or report, is treated as a gene, and evolutionary
operations like selection, crossover, and mutation are guided by the language
model to iteratively improve solutions. The language model provides domain
knowledge and creative variation, while the genetic algorithm ensures
structural integrity and global optimization. GA LLM has proven effective in
tasks such as itinerary planning, academic outlining, and business reporting,
consistently producing well structured and requirement satisfying results. Its
modular design also makes it easy to adapt to new tasks. Compared to using a
language model alone, GA LLM achieves better constraint satisfaction and higher
quality solutions by combining the strengths of both components.

</details>


### [98] [DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech](https://arxiv.org/abs/2506.07502)
*Haotian Guo,Jing Han,Yongfeng Tu,Shihao Gao,Shengfan Shen,Wulong Xiang,Weihao Gan,Zixing Zhang*

Main category: cs.CL

TL;DR: This paper introduces DEBATE, a Chinese speech-text dataset focusing on disambiguation through speech, highlighting performance differences between machines and humans.


<details>
  <summary>Details</summary>
Motivation: To explore disambiguation through speech which remains underexplored due to lack of high-quality datasets.

Method: Creating a dataset named DEBATE with 1,001 ambiguous utterances recorded by 10 native speakers.

Result: DEBATE dataset captures diverse linguistic ambiguities and their disambiguation through speech, with benchmarks showing performance gaps between machine and human understanding.

Conclusion: DEBATE is the first effort of its kind and provides a foundation for future disambiguation through speech datasets across different languages and cultures.

Abstract: Despite extensive research on textual and visual disambiguation,
disambiguation through speech (DTS) remains underexplored. This is largely due
to the lack of high-quality datasets that pair spoken sentences with richly
ambiguous text. To address this gap, we present DEBATE, a unique public Chinese
speech-text dataset designed to study how speech cues and
patterns-pronunciation, pause, stress and intonation-can help resolve textual
ambiguity and reveal a speaker's true intent. DEBATE contains 1,001 carefully
selected ambiguous utterances, each recorded by 10 native speakers, capturing
diverse linguistic ambiguities and their disambiguation through speech. We
detail the data collection pipeline and provide rigorous quality analysis.
Additionally, we benchmark three state-of-the-art large speech and
audio-language models, illustrating clear and huge performance gaps between
machine and human understanding of spoken intent. DEBATE represents the first
effort of its kind and offers a foundation for building similar DTS datasets
across languages and cultures. The dataset and associated code are available
at: https://github.com/SmileHnu/DEBATE.

</details>


### [99] [What Do Indonesians Really Need from Language Technology? A Nationwide Survey](https://arxiv.org/abs/2506.07506)
*Muhammad Dehan Al Kautsar,Lucky Susanto,Derry Wijaya,Fajri Koto*

Main category: cs.CL

TL;DR: This study conducted a nationwide survey in Indonesia to determine the needs of native speakers regarding NLP for local languages, finding that machine translation and information retrieval are top priorities.


<details>
  <summary>Details</summary>
Motivation: To understand what native speakers in Indonesia truly need from NLP for local languages.

Method: Nationwide survey

Result: Addressing language barriers through machine translation and information retrieval is the most critical priority.

Conclusion: Advancements in language technology are desired, but there is a need for transparency and clear communication around privacy, bias, and public data use.

Abstract: There is an emerging effort to develop NLP for Indonesias 700+ local
languages, but progress remains costly due to the need for direct engagement
with native speakers. However, it is unclear what these language communities
truly need from language technology. To address this, we conduct a nationwide
survey to assess the actual needs of native speakers in Indonesia. Our findings
indicate that addressing language barriers, particularly through machine
translation and information retrieval, is the most critical priority. Although
there is strong enthusiasm for advancements in language technology, concerns
around privacy, bias, and the use of public data for AI training highlight the
need for greater transparency and clear communication to support broader AI
adoption.

</details>


### [100] [DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction](https://arxiv.org/abs/2506.07510)
*Solee Im,Wonjun Lee,Jinmyeong An,Yunsu Kim,Jungseul Ok,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: DeRAGEC improves Named Entity correction in ASR by using synthetic denoising rationales and phonetic similarity with in-context learning. It enhances Word Error Rate and NE hit ratio significantly.


<details>
  <summary>Details</summary>
Motivation: Improve Named Entity correction in ASR systems.

Method: Extends RAGEC framework using synthetic denoising rationales and phonetic similarity with augmented definitions.

Result: Significant improvements in WER and NE hit ratio on CommonVoice and STOP datasets.

Conclusion: DeRAGEC outperforms baseline ASR and RAGEC methods.

Abstract: We present DeRAGEC, a method for improving Named Entity (NE) correction in
Automatic Speech Recognition (ASR) systems. By extending the
Retrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC
employs synthetic denoising rationales to filter out noisy NE candidates before
correction. By leveraging phonetic similarity and augmented definitions, it
refines noisy retrieved NEs using in-context learning, requiring no additional
training. Experimental results on CommonVoice and STOP datasets show
significant improvements in Word Error Rate (WER) and NE hit ratio,
outperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28%
relative reduction in WER compared to ASR without postprocessing. Our source
code is publicly available at: https://github.com/solee0022/deragec

</details>


### [101] [Towards Large Language Models with Self-Consistent Natural Language Explanations](https://arxiv.org/abs/2506.07523)
*Sahar Admoni,Ofra Amir,Assaf Hallak,Yftah Ziser*

Main category: cs.CL

TL;DR: This paper introduces PSCB and proposes an alternative metric to improve the alignment between LLM explanations and decision-relevant features, leading to more trustworthy and self-consistent LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistency between post-hoc explanations and the true decision process of LLMs, especially given the high cost of estimating feature importance.

Method: We introduced the Post-hoc Self-Consistency Bank (PSCB) and proposed an alternative metric for evaluating explanation quality, which was used to fine-tune LLMs via Direct Preference Optimization (DPO).

Result: Analysis of PSCB showed that self-consistency scores do not differ much between correct and incorrect predictions. The standard metric failed to meaningfully distinguish between explanations.

Conclusion: Our findings suggest a scalable path toward more trustworthy, self-consistent LLMs.

Abstract: Large language models (LLMs) seem to offer an easy path to interpretability:
just ask them to explain their decisions. Yet, studies show that these post-hoc
explanations often misrepresent the true decision process, as revealed by
mismatches in feature importance. Despite growing evidence of this
inconsistency, no systematic solutions have emerged, partly due to the high
cost of estimating feature importance, which limits evaluations to small
datasets. To address this, we introduce the Post-hoc Self-Consistency Bank
(PSCB) - a large-scale benchmark of decisions spanning diverse tasks and
models, each paired with LLM-generated explanations and corresponding feature
importance scores. Analysis of PSCB reveals that self-consistency scores barely
differ between correct and incorrect predictions. We also show that the
standard metric fails to meaningfully distinguish between explanations. To
overcome this limitation, we propose an alternative metric that more
effectively captures variation in explanation quality. We use it to fine-tune
LLMs via Direct Preference Optimization (DPO), leading to significantly better
alignment between explanations and decision-relevant features, even under
domain shift. Our findings point to a scalable path toward more trustworthy,
self-consistent LLMs.

</details>


### [102] [Bit-level BPE: Below the byte boundary](https://arxiv.org/abs/2506.07541)
*Sangwhan Moon,Tatsuya Hiraoka,Naoaki Okazaki*

Main category: cs.CL

TL;DR: This paper proposes a lossless compression technique to reduce sequence length in byte-level subword tokenization for languages with many characters like Chinese, Japanese, and Korean, aiming to decrease computation time during training and inference.


<details>
  <summary>Details</summary>
Motivation: To address the issue of increased sequence length caused by breaking characters into individual bytes in byte-level subword tokenization, especially for languages like Chinese, Japanese, and Korean, which leads to longer computation time.

Method: Proposing a simple compression technique.

Result: The proposed method can reduce the sequence length without losing any information.

Conclusion: The proposed lossless compression technique can effectively reduce the sequence length in byte-level subword tokenization for character-diverse languages, which helps to decrease computation time.

Abstract: Byte-level fallbacks for subword tokenization have become a common practice
in large language models. In particular, it has been demonstrated to be
incredibly effective as a pragmatic solution for preventing OOV, especially in
the context of larger models. However, breaking a character down to individual
bytes significantly increases the sequence length for long-tail tokens in
languages such as Chinese, Japanese, and Korean (CJK) and other
character-diverse contexts such as emoji. The increased sequence length results
in longer computation during both training and inference. In this work, we
propose a simple compression technique that reduces the sequence length
losslessly.

</details>


### [103] [SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition](https://arxiv.org/abs/2506.07557)
*Mengsong Wu,Di Zhang,Yuqiang Li,Dongzhan Zhou,Wenliang Chen*

Main category: cs.CL

TL;DR: A novel framework called SELT is introduced to enhance the reasoning capabilities of Large Language Models (LLMs) by using a modified Monte Carlo Tree Search (MCTS). This approach improves answer accuracy and reasoning robustness on challenging benchmarks like MMLU and Seal-Tools without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of degraded performance of LLMs in complex reasoning tasks.

Method: The method involves introducing SELT, which uses a modified MCTS to enhance LLM reasoning without relying on external reward models. The Upper Confidence Bound scoring is redefined to align with intrinsic self-evaluation capabilities of LLMs and the inference process is decomposed into atomic subtasks augmented with semantic clustering at each node.

Result: The result shows that SELT achieves significant improvements in answer accuracy and reasoning robustness compared to baseline methods on challenging benchmarks. It also demonstrates strong generalizability across diverse reasoning tasks without task-specific fine-tuning.

Conclusion: The conclusion is that SELT effectively enhances LLM reasoning capabilities and shows strong generalizability across various reasoning tasks.

Abstract: While Large Language Models (LLMs) have achieved remarkable success in a wide
range of applications, their performance often degrades in complex reasoning
tasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a
novel framework that leverages a modified Monte Carlo Tree Search (MCTS) to
enhance LLM reasoning without relying on external reward models. By redefining
the Upper Confidence Bound scoring to align with intrinsic self-evaluation
capabilities of LLMs and decomposing the inference process into atomic subtasks
augmented with semantic clustering at each node, SELT effectively balances
exploration and exploitation, reduces redundant reasoning paths, and mitigates
hallucination. We validate our approach on challenging benchmarks, including
the knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT
achieves significant improvements in answer accuracy and reasoning robustness
compared to baseline methods. Notably, our framework operates without
task-specific fine-tuning, demonstrating strong generalizability across diverse
reasoning tasks. Relevant results and code are available at
https://github.com/fairyshine/SELT .

</details>


### [104] [Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models](https://arxiv.org/abs/2506.07583)
*Ramakrishna Appicharla,Baban Gain,Santanu Pal,Asif Ekbal*

Main category: cs.CL

TL;DR: This literature review examines context-aware translation using large language models, highlighting the effectiveness of commercial models and prompt-based approaches, while suggesting areas for future research.


<details>
  <summary>Details</summary>
Motivation: To explore the underinvestigated application of large language models in context-aware machine translation.

Method: Literature review focusing on prompting, fine-tuning, automatic post-editing, and creating translation agents.

Result: Commercial LLMs outperform open-source ones; prompt-based approaches are effective baselines for assessing translation quality.

Conclusion: Identifies gaps in current research and proposes future directions for context-aware translation with LLMs.

Abstract: Despite the popularity of the large language models (LLMs), their application
to machine translation is relatively underexplored, especially in context-aware
settings. This work presents a literature review of context-aware translation
with LLMs. The existing works utilise prompting and fine-tuning approaches,
with few focusing on automatic post-editing and creating translation agents for
context-aware machine translation. We observed that the commercial LLMs (such
as ChatGPT and Tower LLM) achieved better results than the open-source LLMs
(such as Llama and Bloom LLMs), and prompt-based approaches serve as good
baselines to assess the quality of translations. Finally, we present some
interesting future directions to explore.

</details>


### [105] [Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque](https://arxiv.org/abs/2506.07597)
*Oscar Sainz,Naiara Perez,Julen Etxaniz,Joseba Fernandez de Landa,Itziar Aldabe,Iker García-Ferrero,Aimar Zabala,Ekhi Azurmendi,German Rigau,Eneko Agirre,Mikel Artetxe,Aitor Soroa*

Main category: cs.CL

TL;DR: This paper investigates methods for instructing language models in low-resource languages, demonstrating that using an instruction-tuned model as the backbone yields better results than a base non-instructed model, especially when scaling up.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of instructing language models in languages with limited resources by exploring alternatives to conventional instruction adaptation pipelines.

Method: Experiments were conducted using Basque language data, evaluating different combinations of components such as target language corpora, existing multilingual models, and synthetically generated instructions.

Result: The study found that target language corpora are crucial, synthetic instructions produce robust models, and using an instruction-tuned model as the backbone improves performance, particularly when scaled up.

Conclusion: The research concludes that using an instruction-tuned model as the backbone is more effective than a base non-instructed model, especially when scaling up, and releases resources to support future research.

Abstract: Instructing language models with user intent requires large instruction
datasets, which are only available for a limited set of languages. In this
paper, we explore alternatives to conventional instruction adaptation pipelines
in low-resource scenarios. We assume a realistic scenario for low-resource
languages, where only the following are available: corpora in the target
language, existing open-weight multilingual base and instructed backbone LLMs,
and synthetically generated instructions sampled from the instructed backbone.
We present a comprehensive set of experiments for Basque that systematically
study different combinations of these components evaluated on benchmarks and
human preferences from 1,680 participants. Our conclusions show that target
language corpora are essential, with synthetic instructions yielding robust
models, and, most importantly, that using as backbone an instruction-tuned
model outperforms using a base non-instructed model, and improved results when
scaling up. Using Llama 3.1 instruct 70B as backbone our model comes near
frontier models of much larger sizes for Basque, without using any Basque data
apart from the 1.2B word corpora. We release code, models, instruction
datasets, and human preferences to support full reproducibility in future
research on low-resource language adaptation.

</details>


### [106] [PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels](https://arxiv.org/abs/2506.07606)
*Peyman Rostami,Vahid Rahimzadeh,Ali Adibi,Azadeh Shakery*

Main category: cs.CL

TL;DR: This paper presents PolitiSky24, the first user-level stance detection dataset for the 2024 U.S. presidential election on the Bluesky platform. It includes 16,044 user-target stance pairs with metadata, interaction graphs, and posting histories.


<details>
  <summary>Details</summary>
Motivation: Existing stance detection datasets focus on tweet-level stances from established platforms, lacking resources on user-level stances, particularly for emerging platforms like Bluesky.

Method: A pipeline combining advanced information retrieval and large language models was used to generate stance labels with supporting rationales and text spans.

Result: The dataset comprises 16,044 user-target stance pairs enriched with engagement metadata, interaction graphs, and user posting histories. The labeling approach achieves 81% accuracy with scalable LLMs.

Conclusion: PolitiSky24 addresses gaps in political stance analysis through its timeliness, open-data nature, and user-level perspective.

Abstract: Stance detection identifies the viewpoint expressed in text toward a specific
target, such as a political figure. While previous datasets have focused
primarily on tweet-level stances from established platforms, user-level stance
resources, especially on emerging platforms like Bluesky remain scarce.
User-level stance detection provides a more holistic view by considering a
user's complete posting history rather than isolated posts. We present the
first stance detection dataset for the 2024 U.S. presidential election,
collected from Bluesky and centered on Kamala Harris and Donald Trump. The
dataset comprises 16,044 user-target stance pairs enriched with engagement
metadata, interaction graphs, and user posting histories. PolitiSky24 was
created using a carefully evaluated pipeline combining advanced information
retrieval and large language models, which generates stance labels with
supporting rationales and text spans for transparency. The labeling approach
achieves 81\% accuracy with scalable LLMs. This resource addresses gaps in
political stance analysis through its timeliness, open-data nature, and
user-level perspective. The dataset is available at
https://doi.org/10.5281/zenodo.15616911

</details>


### [107] [Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation](https://arxiv.org/abs/2506.07617)
*Roman Kyslyi,Yuliia Maksymiuk,Ihor Pysmennyi*

Main category: cs.CL

TL;DR: This paper introduces efforts to adapt large language models to the Ukrainian dialect Hutsul, creating a parallel corpus and dictionary, and generating synthetic pairs to expand the corpus. Multiple LLMs were fine-tuned and evaluated on standard-to-dialect translation tasks.


<details>
  <summary>Details</summary>
Motivation: Adapting large language models to low-resource and morphologically complex dialects like Hutsul.

Method: Creating a parallel corpus and dictionary, proposing an advanced RAG pipeline for synthetic pair generation, fine-tuning LLMs using LoRA, and evaluating models on translation tasks.

Result: Fine-tuned models outperform zero-shot baselines on automatic and LLM-evaluated metrics.

Conclusion: The study demonstrates the potential of adapting large language models to low-resource dialects, providing valuable resources and models publicly.

Abstract: In this paper we introduce the first effort to adapt large language models
(LLMs) to the Ukrainian dialect (in our case Hutsul), a low-resource and
morphologically complex dialect spoken in the Carpathian Highlands. We created
a parallel corpus of 9852 dialect-to-standard Ukrainian sentence pairs and a
dictionary of 7320 dialectal word mappings. We also addressed data shortage by
proposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate
synthetic parallel translation pairs, expanding the corpus with 52142 examples.
We have fine-tuned multiple open-source LLMs using LoRA and evaluated them on a
standard-to-dialect translation task, also comparing with few-shot GPT-4o
translation. In the absence of human annotators, we adopt a multi-metric
evaluation strategy combining BLEU, chrF++, TER, and LLM-based judgment
(GPT-4o). The results show that even small(7B) finetuned models outperform
zero-shot baselines such as GPT-4o across both automatic and LLM-evaluated
metrics. All data, models, and code are publicly released at:
https://github.com/woters/vuyko-hutsul

</details>


### [108] [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
*Harsh Bihany,Shubham Patel,Ashutosh Modi*

Main category: cs.CL

TL;DR: This paper introduces LoRMA, a novel low-rank adaptation method that shifts from additive to multiplicative matrix transformations for more efficient large language model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To mitigate the high computational cost of full fine-tuning and address the limitations of previous low-rank adaptation approaches

Method: Low-Rank Multiplicative Adaptation (LoRMA)

Result: Effectiveness in various evaluation metrics

Conclusion: The proposed method LoRMA improves the performance and efficiency compared with existing methods.

Abstract: Large Language Models have shown remarkable capabilities in the NLP domain.
Their effectiveness can mainly be attributed to their ability to adapt to an
array of downstream tasks. However, generally, full fine-tuning is a
computationally expensive job. To mitigate this, many techniques have been
developed that prime efficiency, a prominent one being Low-Rank Adaptation
(LoRA). However, LoRA and its variants employ re-parametrized additive updates.
In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which
shifts the paradigm of additive updates to a richer space of matrix
multiplicative transformations. We tackle challenges such as computational
complexity and rank bottleneck of matrix multiplication by effectively
re-ordering operations and introducing rank inflation strategies. We conduct
extensive experiments to demonstrate the effectiveness of our approach in terms
of various evaluation metrics.

</details>


### [109] [Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation](https://arxiv.org/abs/2506.07626)
*Kseniia Petukhova,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: This study explores if fine-grained annotation of teacher intents can enhance the quality of large language model (LLM)-generated tutoring responses in educational applications, particularly in intelligent tutoring systems for math instruction.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack alignment with pedagogical strategies without task-specific adaptation, which is necessary for effective tutoring.

Method: Fine-tuning an LLM using new annotations based on a detailed taxonomy of eleven pedagogical intents, focusing on the MathDial dataset.

Result: The fine-grained model generates more pedagogically aligned and effective responses compared to models trained on the original four-category taxonomy.

Conclusion: Intent specificity is valuable for controlled text generation in educational settings. The annotated data and code are released to support further research.

Abstract: Large language models (LLMs) hold great promise for educational applications,
particularly in intelligent tutoring systems. However, effective tutoring
requires alignment with pedagogical strategies - something current LLMs lack
without task-specific adaptation. In this work, we explore whether fine-grained
annotation of teacher intents can improve the quality of LLM-generated tutoring
responses. We focus on MathDial, a dialog dataset for math instruction, and
apply an automated annotation framework to re-annotate a portion of the dataset
using a detailed taxonomy of eleven pedagogical intents. We then fine-tune an
LLM using these new annotations and compare its performance to models trained
on the original four-category taxonomy. Both automatic and qualitative
evaluations show that the fine-grained model produces more pedagogically
aligned and effective responses. Our findings highlight the value of intent
specificity for controlled text generation in educational settings, and we
release our annotated data and code to facilitate further research.

</details>


### [110] [Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline](https://arxiv.org/abs/2506.07631)
*Brian Gordon,Yonatan Bitton,Andreea Marzoca,Yasumasa Onoe,Xiao Wang,Daniel Cohen-Or,Idan Szpektor*

Main category: cs.CL

TL;DR: A new benchmark DOCCI-Critique with human annotations for evaluating the factual accuracy of vision-language models' image captions is introduced. A model called VNLI-Critique is developed for automated factuality classification and critique generation.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating the factual accuracy of vision-language models often miss fine-grained errors, especially in longer texts.

Method: Developed VNLI-Critique model for automated factuality classification and critique generation, used DOCCI-Critique benchmark with human annotations.

Result: Demonstrated robust generalization of VNLI-Critique, provided reliable VLM rankings, showed substantial improvements in caption factuality using Critic-and-Revise pipeline.

Conclusion: Introduced a crucial benchmark and practical tools for fine-grained evaluation of vision-language models' image captions.

Abstract: Large Vision-Language Models (VLMs) now generate highly detailed,
paragraphlength image captions, yet evaluating their factual accuracy remains
challenging. Current methods often miss fine-grained errors, being designed for
shorter texts or lacking datasets with verified inaccuracies. We introduce
DOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100
images, 14 VLMs) featuring over 10,216 sentence-level human annotations of
factual correctness and explanatory rationales for errors, all within paragraph
context. Building on this, we develop VNLI-Critique, a model for automated
sentence-level factuality classification and critique generation. We highlight
three key applications: (1) VNLI-Critique demonstrates robust generalization,
validated by state-of-the-art performance on the M-HalDetect benchmark and
strong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven
AutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent
alignment with human factuality judgments (e.g., 0.98 Spearman). (3) An
innovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide
LLM-based corrections, achieves substantial improvements in caption factuality
(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark
alongside practical tools, designed to significantly elevate the standards for
fine-grained evaluation and foster the improvement of VLM image understanding.
Project page: https://google.github.io/unblocking-detail-caption

</details>


### [111] [TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review](https://arxiv.org/abs/2506.07642)
*Yuan Chang,Ziyue Li,Hengyuan Zhang,Yuanbo Kong,Yanru Wu,Zhijiang Guo,Ngai Wong*

Main category: cs.CL

TL;DR: TreeReview is a new approach for paper review which uses a hierarchical question-answering process and reduces LLM token usage by 80%. It performs better than existing methods in generating comprehensive and insightful reviews.


<details>
  <summary>Details</summary>
Motivation: Current methods for using LLMs in peer review struggle with efficiency and quality of reviews.

Method: TreeReview models paper review as a hierarchical and bidirectional question-answering process with dynamic question expansion.

Result: TreeReview outperforms baselines in providing comprehensive, in-depth, and expert-aligned review feedback while reducing LLM token usage significantly.

Conclusion: TreeReview improves the efficiency and quality of LLM-assisted peer review through a novel hierarchical and bidirectional question-answering framework.

Abstract: While Large Language Models (LLMs) have shown significant potential in
assisting peer review, current methods often struggle to generate thorough and
insightful reviews while maintaining efficiency. In this paper, we propose
TreeReview, a novel framework that models paper review as a hierarchical and
bidirectional question-answering process. TreeReview first constructs a tree of
review questions by recursively decomposing high-level questions into
fine-grained sub-questions and then resolves the question tree by iteratively
aggregating answers from leaf to root to get the final review. Crucially, we
incorporate a dynamic question expansion mechanism to enable deeper probing by
generating follow-up questions when needed. We construct a benchmark derived
from ICLR and NeurIPS venues to evaluate our method on full review generation
and actionable feedback comments generation tasks. Experimental results of both
LLM-based and human evaluation show that TreeReview outperforms strong
baselines in providing comprehensive, in-depth, and expert-aligned review
feedback, while reducing LLM token usage by up to 80% compared to
computationally intensive approaches. Our code and benchmark dataset are
available at https://github.com/YuanChang98/tree-review.

</details>


### [112] [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
*Maciej Chrabąszcz,Katarzyna Lorenc,Karolina Seweryn*

Main category: cs.CL

TL;DR: Large language models (LLMs) are vulnerable to perturbations, especially in low-resource languages like Polish. A few character changes and a small proxy model can create strong attacks.


<details>
  <summary>Details</summary>
Motivation: To evaluate the susceptibility of LLMs to jailbreaks and perturbations, particularly in low-resource languages with limited safety-related training data.

Method: Alter a few characters and use a small proxy model to calculate word importance for creating attacks.

Result: Character and word-level attacks significantly change the predictions of various LLMs, suggesting vulnerabilities in internal safety mechanisms.

Conclusion: LLMs have potential vulnerabilities that can be exploited to bypass their safety mechanisms, especially in low-resource languages.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
various natural language processing (NLP) tasks in recent years. However, their
susceptibility to jailbreaks and perturbations necessitates additional
evaluations. Many LLMs are multilingual, but safety-related training data
contains mainly high-resource languages like English. This can leave them
vulnerable to perturbations in low-resource languages such as Polish. We show
how surprisingly strong attacks can be cheaply created by altering just a few
characters and using a small proxy model for word importance calculation. We
find that these character and word-level attacks drastically alter the
predictions of different LLMs, suggesting a potential vulnerability that can be
used to circumvent their internal safety mechanisms. We validate our attack
construction methodology on Polish, a low-resource language, and find potential
vulnerabilities of LLMs in this language. Additionally, we show how it can be
extended to other languages. We release the created datasets and code for
further research.

</details>


### [113] [Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation](https://arxiv.org/abs/2506.07646)
*Rui Hu,Xiaolong Lin,Jiawang Liu,Shixi Huang,Zhenpeng Zhan*

Main category: cs.CL

TL;DR: We present a method to annotate phonemic and prosodic labels on audio-transcript pairs for Japanese TTS datasets, improving upon previous approaches.


<details>
  <summary>Details</summary>
Motivation: Constructing Japanese text-to-speech (TTS) datasets by annotating phonemic and prosodic labels on a given audio-transcript pair.

Method: Fine-tuning a large-scale pre-trained ASR model conditioned on ground truth transcripts to simultaneously output phrase-level graphemes and annotation labels, employing a decoding strategy utilizing dictionary prior knowledge.

Result: The objective evaluation results show better performance than previous approaches relying solely on text or audio. The subjective evaluation results indicate comparable naturalness of synthesized speech.

Conclusion: Our proposed method outperforms previous approaches and the naturalness of speech synthesized by the TTS model is comparable to that of a model trained with manual annotations.

Abstract: In this paper, we propose a method for annotating phonemic and prosodic
labels on a given audio-transcript pair, aimed at constructing Japanese
text-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale
pre-trained automatic speech recognition (ASR) model, conditioned on ground
truth transcripts, to simultaneously output phrase-level graphemes and
annotation labels. To further correct errors in phonemic labeling, we employ a
decoding strategy that utilizes dictionary prior knowledge. The objective
evaluation results demonstrate that our proposed method outperforms previous
approaches relying solely on text or audio. The subjective evaluation results
indicate that the naturalness of speech synthesized by the TTS model, trained
with labels annotated using our method, is comparable to that of a model
trained with manual annotations.

</details>


### [114] [Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping](https://arxiv.org/abs/2506.07658)
*Nitin Sharma,Thomas Wolfers,Çağatay Yıldız*

Main category: cs.CL

TL;DR: This paper presents a deterministic pipeline for creating domain-specific benchmarks without relying on language models or human curation. It evaluates models' domain knowledge by measuring their ability to complete prompts with correct domain-specific targets, revealing insights into knowledge representation during domain adaptation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of creating reliable domain-specific benchmarks and understanding knowledge representation during domain adaptation in language models.

Method: Developing a deterministic pipeline that uses TF and Term TF-IDF methods to generate domain-specific keywords and related word lists, and constructs prompt-target pairs for evaluation.

Result: Demonstrating strong correlation with expert-generated benchmarks and providing a more accurate measure of domain knowledge than traditional perplexity metrics. Revealing rapid domain adaptation in smaller models and a new approach to domain knowledge evaluation in base models during training.

Conclusion: Providing practical evaluation methodology for domain-specific language models and novel insights into knowledge representation during adaptation, with implications for efficient fine-tuning strategies and mitigating catastrophic forgetting.

Abstract: The paper addresses two critical challenges in language model (LM)
evaluation: creating reliable domain-specific benchmarks and understanding
knowledge representation during domain adaptation. We introduce a deterministic
pipeline that converts raw domain corpora into completion-type benchmarks
without relying on LMs or human curation, eliminating benchmark contamination
issues while enabling evaluation on the latest domain data. Our approach
generates domain-specific keywords and related word lists using TF and Term
TF-IDF methods and constructs prompt-target pairs. We evaluate models by
measuring their ability to complete these prompts with the correct
domain-specific targets, providing a direct assessment of domain knowledge with
low computational cost. Through comprehensive experiments across multiple
models (GPT-2 medium/XL, Llama-2/3.1, OLMo-2, Qwen-2, Mistral) and domains, we
demonstrate that our benchmark strongly correlates with expert-generated
benchmarks while providing a more accurate measure of domain knowledge than
traditional perplexity metrics. We reveal that domain adaptation happens
rapidly in smaller models (within 500 steps) and illustrate a new approach to
domain knowledge evaluation in base models during training for early stopping.
By extending mechanistic analysis to domain adaptation, we discover that
initial-to-mid layers are primarily responsible for attribute extraction, while
later layers focus on next token prediction. Furthermore, we show that during
adaptation, forgetting begins in the middle layers, where attribute extraction
happens and is amplified in later layers. Our work provides both a practical
evaluation methodology for domain-specific LMs and novel insights into
knowledge representation during adaptation, with implications for more
efficient fine-tuning strategies and targeted approaches to mitigate
catastrophic forgetting.

</details>


### [115] [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)
*Lei Xu,Sirui Chen,Yuxuan Huang,Chaochao Lu*

Main category: cs.CL

TL;DR: 提出了一种通过提取数学推理中的结构信息和生成的问题求解代码来指导数据生成的方法，并在MATH和GSM8K数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有的LLMs在数学推理方面存在挑战，复杂逻辑和精确计算的需求使得推理困难。现有的增强LLMs推理的方法存在生成质量和问题复杂性的问题。

Method: 提取结构信息并生成问题求解代码，指导数据生成。

Result: 在MATH和GSM8K数据集上生成了39K个带有中间步骤标记的问题和一个包含6.1K个问题的高难度基准数据集。

Conclusion: 该方法和数据集有助于未来研究提高LLMs的推理能力。

Abstract: Mathematical reasoning remains challenging for LLMs due to complex logic and
the need for precise computation. Existing methods enhance LLM reasoning by
synthesizing datasets through problem rephrasing, but face issues with
generation quality and problem complexity. To address this, we propose to
extract structural information with generated problem-solving code from
mathematical reasoning and guide data generation with structured solutions.
Applied to MATH and GSM8K, our approach produces 39K problems with labeled
intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results
on our benchmark show that model performance declines as reasoning length
increases. Additionally, we conducted fine-tuning experiments using the
proposed training data on a range of LLMs, and the results validate the
effectiveness of our dataset. We hope the proposed method and dataset will
contribute to future research in enhancing LLM reasoning capabilities.

</details>


### [116] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
*Prarabdh Shukla,Wei Yin Chong,Yash Patel,Brennan Schaffner,Danish Pruthi,Arjun Bhagoji*

Main category: cs.CL

TL;DR: Audited Twitch's AutoMod tool and found it ineffective at flagging hateful content, missing up to 94% of blatant hate speech and incorrectly blocking many benign messages.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of Twitch's automated moderation tool in flagging hateful content in real-time engagement.

Method: Created streaming accounts as test beds and sent over 107,000 comments from 4 datasets via Twitch's APIs to interface with live chat.

Result: A large fraction (up to 94%) of blatantly hateful content bypassed moderation. Adding slurs to messages resulted in 100% removal, showing reliance on slurs as a moderation signal. Also, 89.5% of benign examples using sensitive words in appropriate contexts were blocked.

Conclusion: There are significant gaps in AutoMod's capabilities, highlighting the need for better contextual understanding in such moderation systems.

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


### [117] [GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation](https://arxiv.org/abs/2506.07671)
*Ionut-Teodor Sorodoc,Leonardo F. R. Ribeiro,Rexhina Blloshmi,Christopher Davis,Adrià de Gispert*

Main category: cs.CL

TL;DR: GaRAGe是一个大型RAG基准测试，包含人类编写的长篇答案和每个参考段落的注释，可以精细评估LLMs是否能识别相关的参考信息。该基准测试包含2366个问题，涉及多样化的复杂性、动态性和主题，并包括超过35K个标注的段落，反映了现实世界中的RAG使用案例。实验表明，最先进的LLMs倾向于过度总结而不是严格基于相关参考材料回答问题，或者在没有相关信息时未能有效回避问题。此外，在处理时间敏感的问题或从稀疏的私人参考源中提取知识时，性能会显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG评估基准测试缺乏对LLMs识别相关参考信息能力的精细评估。

Method: 开发了一个新的RAG基准测试GaRAGe，包含2366个问题及其对应的参考段落注释。

Result: 多个最先进的LLMs在GaRAGe上的表现显示，它们倾向于过度总结而不是严格基于相关参考材料回答问题，或者在没有相关信息时未能有效回避问题。此外，在处理时间敏感的问题或从稀疏的私人参考源中提取知识时，性能会显著下降。

Conclusion: GaRAGe提供了一个理想的测试环境来评估LLMs在RAG任务中的表现，揭示了现有模型在识别相关参考信息方面的不足之处。

Abstract: We present GaRAGe, a large RAG benchmark with human-curated long-form answers
and annotations of each grounding passage, allowing a fine-grained evaluation
of whether LLMs can identify relevant grounding when generating RAG answers.
Our benchmark contains 2366 questions of diverse complexity, dynamism, and
topics, and includes over 35K annotated passages retrieved from both private
document sets and the Web, to reflect real-world RAG use cases. This makes it
an ideal test bed to evaluate an LLM's ability to identify only the relevant
information necessary to compose a response, or provide a deflective response
when there is insufficient information. Evaluations of multiple
state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise
rather than (a) ground their answers strictly on the annotated relevant
passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)
deflect when no relevant grounding is available (reaching at most 31% true
positive rate in deflections). The F1 in attribution to relevant sources is at
most 58.9%, and we show that performance is particularly reduced when answering
time-sensitive questions and when having to draw knowledge from sparser private
grounding sources.

</details>


### [118] [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
*Jiaming Li,Haoran Ye,Yukun Chen,Xinyue Li,Lei Zhang,Hamid Alinejad-Rokny,Jimmy Chih-Hsien Peng,Min Yang*

Main category: cs.CL

TL;DR: We propose FAST, a novel training method for instruct models that improves both token reconstruction and feature interpretability.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between base models and instruct models in terms of reconstruction quality and interpretability.

Method: FAST

Result: On Qwen2.5-7B-Instruct, FAST achieves a mean squared error of 0.6468 in token reconstruction, significantly outperforming baseline methods. In feature interpretability, FAST yields a higher proportion of high-quality features.

Conclusion: We propose FAST, a novel training method specifically tailored for instruct models. FAST achieves substantial improvements in both reconstruction and feature interpretability.

Abstract: As large language models (LLMs) grow in scale and capability, understanding
their internal mechanisms becomes increasingly critical. Sparse autoencoders
(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the
extraction of human-interpretable features from LLMs. However, existing SAE
training methods are primarily designed for base models, resulting in reduced
reconstruction quality and interpretability when applied to instruct models. To
bridge this gap, we propose
$\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned
$\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining
($\textit{FAST}$), a novel training method specifically tailored for instruct
models. $\textit{FAST}$ aligns the training process with the data distribution
and activation patterns characteristic of instruct models, resulting in
substantial improvements in both reconstruction and feature interpretability.
On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468
in token reconstruction, significantly outperforming baseline methods with
errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$
yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,
$21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for
$\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that
intervening on the activations of special tokens via the SAEs leads to
improvements in output quality, suggesting new opportunities for fine-grained
control of model behavior. Code, data, and 240 trained SAEs are available at
https://github.com/Geaming2002/FAST.

</details>


### [119] [Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2506.07712)
*Renjie Luo,Jiaxi Li,Chen Huang,Wei Lu*

Main category: cs.CL

TL;DR: Large language models benefit from long-chain-of-thought (CoT) training, but small language models suffer from performance degradation when trained with limited long CoT data.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of long CoT training on small language models

Method: Experiments on Qwen2.5, LLaMA3 and Gemma3 families

Result: Small language models trained on limited long CoT data experience significant performance deterioration. This degradation is widespread across SLMs and can even worsen downstream reinforcement learning.

Conclusion: Long CoT Degradation exists widely among small language models, and error accumulation is the main cause. Sufficiently scaled supervised fine-tuning can alleviate this issue.

Abstract: Long chain-of-thought (CoT) supervision has become a common strategy to
enhance reasoning in language models. While effective for large models, we
identify a phenomenon we call Long CoT Degradation, in which small language
models (SLMs; <=3B parameters) trained on limited long CoT data experience
significant performance deterioration. Through extensive experiments on the
Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is
widespread across SLMs. In some settings, models trained on only 8k long CoT
examples lose up to 75% of their original performance before fine-tuning.
Strikingly, we further observe that for some particularly small models, even
training on 220k long CoT examples fails to recover or surpass their original
performance prior to fine-tuning. Our analysis attributes this effect to error
accumulation: while longer responses increase the capacity for multi-step
reasoning, they also amplify the risk of compounding mistakes. Furthermore, we
find that Long CoT Degradation may negatively impacts downstream reinforcement
learning (RL), although this can be alleviated by sufficiently scaled
supervised fine-tuning (SFT). Our findings challenge common assumptions about
the benefits of long CoT training for SLMs and offer practical guidance for
building more effective small-scale reasoning models.

</details>


### [120] [Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility](https://arxiv.org/abs/2506.07719)
*Mengyang Qiu,Tran Minh Nguyen,Zihao Huang,Zelong Li,Yang Gu,Qingyu Gao,Siliang Liu,Jungyeul Park*

Main category: cs.CL

TL;DR: This paper introduces a new multilingual grammatical error annotation framework that overcomes limitations of previous frameworks and provides adaptable solutions for different languages.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks like $\texttt{errant}$ have limitations when extended to typologically diverse languages.

Method: Introducing a standardized, modular framework for multilingual grammatical error annotation, which combines a language-agnostic foundation with structured language-specific extensions.

Result: The framework is adaptable and has been applied to English, German, Czech, Korean, and Chinese.

Conclusion: This work supports scalable and interpretable GEC annotation across languages and promotes more consistent evaluation in multilingual settings.

Abstract: Grammatical Error Correction (GEC) relies on accurate error annotation and
evaluation, yet existing frameworks, such as $\texttt{errant}$, face
limitations when extended to typologically diverse languages. In this paper, we
introduce a standardized, modular framework for multilingual grammatical error
annotation. Our approach combines a language-agnostic foundation with
structured language-specific extensions, enabling both consistency and
flexibility across languages. We reimplement $\texttt{errant}$ using
$\texttt{stanza}$ to support broader multilingual coverage, and demonstrate the
framework's adaptability through applications to English, German, Czech,
Korean, and Chinese, ranging from general-purpose annotation to more customized
linguistic refinements. This work supports scalable and interpretable GEC
annotation across languages and promotes more consistent evaluation in
multilingual settings. The complete codebase and annotation tools can be
accessed at https://github.com/open-writing-evaluation/jp_errant_bea.

</details>


### [121] [Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU](https://arxiv.org/abs/2506.07726)
*Vincenzo Timmel,Manfred Vogel,Daniel Perruchoud,Reza Kakooee*

Main category: cs.CL

TL;DR: This paper introduces an enhanced version of the Swiss Parliaments Corpus, creating high-quality speech-text pairs from Swiss German parliamentary debates using Whisper for transcription and GPT-4o for corrections.


<details>
  <summary>Details</summary>
Motivation: To improve the quality of speech-text pairs in the Swiss Parliaments Corpus specifically for Swiss German parliamentary debates.

Method: Transcription using Whisper Large-v3 followed by a two-step GPT-4o correction process including named entity refinement and semantic completeness evaluation, with a filtering step based on BLEU scores.

Result: The final corpus includes 801 hours of audio with 751 hours meeting quality standards, showing a 6-point BLEU improvement over the original release.

Conclusion: Combining robust ASR, LLM-based correction, and data-driven filtering can significantly enhance low-resource, domain-specific speech corpora.

Abstract: This paper presents a new long-form release of the Swiss Parliaments Corpus,
converting entire multi-hour Swiss German debate sessions (each aligned with
the official session protocols) into high-quality speech-text pairs. Our
pipeline starts by transcribing all session audio into Standard German using
Whisper Large-v3 under high-compute settings. We then apply a two-step GPT-4o
correction process: first, GPT-4o ingests the raw Whisper output alongside the
official protocols to refine misrecognitions, mainly named entities. Second, a
separate GPT-4o pass evaluates each refined segment for semantic completeness.
We filter out any segments whose Predicted BLEU score (derived from Whisper's
average token log-probability) and GPT-4o evaluation score fall below a certain
threshold. The final corpus contains 801 hours of audio, of which 751 hours
pass our quality control. Compared to the original sentence-level SPC release,
our long-form dataset achieves a 6-point BLEU improvement, demonstrating the
power of combining robust ASR, LLM-based correction, and data-driven filtering
for low-resource, domain-specific speech corpora.

</details>


### [122] [Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)
*Silin Gao,Antoine Bosselut,Samy Bengio,Emmanuel Abbe*

Main category: cs.CL

TL;DR: This paper introduces AbstraL, a method that uses reinforcement learning to improve abstract reasoning in LLMs, thereby enhancing robustness against distribution shifts.


<details>
  <summary>Details</summary>
Motivation: LLMs, particularly smaller ones, lack robustness in reasoning and suffer performance drops under distribution shifts.

Method: Using reinforcement learning to promote abstract reasoning in LLMs by creating granular abstraction data.

Result: The proposed method, AbstraL, effectively reduces performance degradation on GSM perturbation benchmarks.

Conclusion: Abstracting reasoning problems using reinforcement learning can counteract distribution shifts and connect with symbolic tools for deriving solutions.

Abstract: Recent studies have shown that large language models (LLMs), especially
smaller ones, often lack robustness in their reasoning. I.e., they tend to
experience performance drops when faced with distribution shifts, such as
changes to numerical or nominal variables, or insertions of distracting
clauses. A possible strategy to address this involves generating synthetic data
to further "instantiate" reasoning problems on potential variations. In
contrast, our approach focuses on "abstracting" reasoning problems. This not
only helps counteract distribution shifts but also facilitates the connection
to symbolic tools for deriving solutions. We find that this abstraction process
is better acquired through reinforcement learning (RL) than just supervised
fine-tuning, which often fails to produce faithful abstractions. Our method,
AbstraL -- which promotes abstract reasoning in LLMs using RL on granular
abstraction data -- significantly mitigates performance degradation on recent
GSM perturbation benchmarks.

</details>


### [123] [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
*Xiaotian Ye,Mengqi Zhang,Shu Wu*

Main category: cs.CL

TL;DR: LLMs often contain harmful or private information that needs to be removed. Existing unlearning methods depend on the form of training data and fail to generalize. This paper identifies a pervasive issue called Form-Dependent Bias and introduces ORT, a new benchmark, revealing the bias's severity. To solve this, ROCR, a novel training-free method, redirects dangerous concepts to harmless ones effectively.


<details>
  <summary>Details</summary>
Motivation: Controlling harmful or private information within large language models to prevent misuse.

Method: Introducing Rank-one Concept Redirection (ROCR), a novel training-free method to perform unlearning by targeting invariants in downstream tasks.

Result: ROCR outperforms traditional methods in unlearning effectiveness and generates natural outputs. A new benchmark, ORT, reveals the widespread and severe nature of Form-Dependent Bias.

Conclusion: LLM unlearning should be form-independent to handle various real-world tasks. ROCR is proposed as a promising solution.

Abstract: Large Language Model (LLM) unlearning aims to erase or suppress undesirable
knowledge within the model, offering promise for controlling harmful or private
information to prevent misuse. However, recent studies highlight its limited
efficacy in real-world scenarios, hindering practical adoption. In this study,
we identify a pervasive issue underlying many downstream failures: the
effectiveness of existing unlearning methods heavily depends on the form of
training samples and frequently fails to generalize to alternate expressions of
the same knowledge. We formally characterize this problem as Form-Dependent
Bias and systematically investigate its specific manifestation patterns across
various downstream tasks. To quantify its prevalence and support future
research, we introduce ORT, a novel benchmark designed to evaluate the
robustness of unlearning methods against variations in knowledge expression.
Results reveal that Form-Dependent Bias is both widespread and severe among
current techniques.
  We argue that LLM unlearning should be form-independent to address the
endless forms of downstream tasks encountered in real-world security-critical
scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),
a novel training-free method, as a promising solution path. ROCR performs
unlearning by targeting the invariants in downstream tasks, specifically the
activated dangerous concepts. It is capable of modifying model parameters
within seconds to redirect the model's perception of a specific unlearning
target concept to another harmless concept. Extensive experiments demonstrate
that ROCR significantly improves unlearning effectiveness compared to
traditional methods while generating highly natural outputs.

</details>


### [124] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
*Iustin Sirbu,Robert-Adrian Popovici,Cornelia Caragea,Stefan Trausan-Matu,Traian Rebedea*

Main category: cs.CL

TL;DR: MultiMatch is a novel semi-supervised learning algorithm that combines multiple techniques to improve robustness and performance, especially in imbalanced data settings.


<details>
  <summary>Details</summary>
Motivation: To address data imbalance in text classification tasks.

Method: Combining co-training and consistency regularization with pseudo-labeling, featuring a three-fold pseudo-label weighting module.

Result: Superior performance on benchmark datasets, achieving state-of-the-art results on most setups from 5 natural language processing datasets.

Conclusion: MultiMatch improves robustness and performance in SSL settings.

Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm
combining the paradigms of co-training and consistency regularization with
pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label
weighting module designed for three key purposes: selecting and filtering
pseudo-labels based on head agreement and model confidence, and weighting them
according to the perceived classification difficulty. This novel module
enhances and unifies three existing techniques -- heads agreement from
Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average
Pseudo-Margins from MarginMatch -- resulting in a holistic approach that
improves robustness and performance in SSL settings. Experimental results on
benchmark datasets highlight the superior performance of MultiMatch, achieving
state-of-the-art results on 9 out of 10 setups from 5 natural language
processing datasets and ranking first according to the Friedman test among 19
methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly
imbalanced settings, outperforming the second-best approach by 3.26% -- and
data imbalance is a key factor for many text classification tasks.

</details>


### [125] [WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code](https://arxiv.org/abs/2506.07818)
*Zhiyu Lin,Zhengda Zhou,Zhiyuan Zhao,Tianrui Wan,Yilun Ma,Junyu Gao,Xuelong Li*

Main category: cs.CL

TL;DR: This paper introduces WebUIBench, a new benchmark for evaluating Multimodal Large Language Models' (MLLMs) capabilities in web application development. It focuses on four key areas and evaluates 29 mainstream MLLMs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive evaluation frameworks for MLLMs in web application development.

Method: Proposing WebUIBench with question-answer pairs derived from real-world websites.

Result: Uncovered skill characteristics and weaknesses of MLLMs during the development process.

Conclusion: A multi-view evaluation framework is essential for enhancing MLLMs' development efficiency.

Abstract: With the rapid advancement of Generative AI technology, Multimodal Large
Language Models(MLLMs) have the potential to act as AI software engineers
capable of executing complex web application development. Considering that the
model requires a confluence of multidimensional sub-capabilities to address the
challenges of various development phases, constructing a multi-view evaluation
framework is crucial for accurately guiding the enhancement of development
efficiency. However, existing benchmarks usually fail to provide an assessment
of sub-capabilities and focus solely on webpage generation outcomes. In this
work, we draw inspiration from the principles of software engineering and
further propose WebUIBench, a benchmark systematically designed to evaluate
MLLMs in four key areas: WebUI Perception, HTML Programming,WebUI-HTML
Understanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality
question-answer pairs derived from over 0.7K real-world websites. The extensive
evaluation of 29 mainstream MLLMs uncovers the skill characteristics and
various weakness that models encountered during the development process.

</details>


### [126] [Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning](https://arxiv.org/abs/2506.07851)
*Yiju Guo,Wenkai Yang,Zexu Sun,Ning Ding,Zhiyuan Liu,Yankai Lin*

Main category: cs.CL

TL;DR: A novel framework named LeaF is proposed to improve the performance of large language models in long-context reasoning and generation by focusing on critical information.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the issue of large language models being distracted by spurious correlations in training data, leading to suboptimal reasoning and generation.

Method: A two-stage framework called Learning to Focus (LeaF) is introduced, which uses intervention-based inference to disentangle confounding factors and prune distracting tokens during distillation.

Result: Experimental results show that LeaF improves performance in mathematical reasoning and code generation benchmarks while reducing attention to confounding tokens.

Conclusion: LeaF demonstrates the effectiveness of focusing on critical information in improving the reliability and interpretability of large language models.

Abstract: Large language models (LLMs) have demonstrated significant improvements in
contextual understanding. However, their ability to attend to truly critical
information during long-context reasoning and generation still falls behind the
pace. Specifically, our preliminary experiments reveal that certain distracting
patterns can misdirect the model's attention during inference, and removing
these patterns substantially improves reasoning accuracy and generation
quality. We attribute this phenomenon to spurious correlations in the training
data, which obstruct the model's capacity to infer authentic causal
instruction-response relationships. This phenomenon may induce redundant
reasoning processes, potentially resulting in significant inference overhead
and, more critically, the generation of erroneous or suboptimal responses. To
mitigate this, we introduce a two-stage framework called Learning to Focus
(LeaF) leveraging intervention-based inference to disentangle confounding
factors. In the first stage, LeaF employs gradient-based comparisons with an
advanced teacher to automatically identify confounding tokens based on causal
relationships in the training corpus. Then, in the second stage, it prunes
these tokens during distillation to enact intervention, aligning the student's
attention with the teacher's focus distribution on truly critical context
tokens. Experimental results demonstrate that LeaF not only achieves an
absolute improvement in various mathematical reasoning and code generation
benchmarks but also effectively suppresses attention to confounding tokens
during inference, yielding a more interpretable and reliable reasoning model.

</details>


### [127] [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
*Ke Wang,Yiming Qin,Nikolaos Dimitriadis,Alessandro Favero,Pascal Frossard*

Main category: cs.CL

TL;DR: MEMOIR is a novel framework for efficiently updating language models without retraining or forgetting previous knowledge, achieving state-of-the-art performance in reliability, generalization, and scalability.


<details>
  <summary>Details</summary>
Motivation: Efficient and reliable updates for real-world language models without compromising previous knowledge.

Method: Injecting knowledge via a residual memory module with sample-dependent masks to confine each edit to a distinct subset of memory parameters and identifying relevant edits based on sparse activation patterns.

Result: State-of-the-art performance in reliability, generalization, and locality metrics on question answering, hallucination correction, and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral.

Conclusion: MEMOIR provides a scalable solution for lifelong model editing, enabling efficient updates to language models without forgetting previous information.

Abstract: Language models deployed in real-world systems often require post-hoc updates
to incorporate new or corrected knowledge. However, editing such models
efficiently and reliably - without retraining or forgetting previous
information - remains a major challenge. Existing methods for lifelong model
editing either compromise generalization, interfere with past edits, or fail to
scale to long editing sequences. We propose MEMOIR, a novel scalable framework
that injects knowledge through a residual memory, i.e., a dedicated parameter
module, while preserving the core capabilities of the pre-trained model. By
sparsifying input activations through sample-dependent masks, MEMOIR confines
each edit to a distinct subset of the memory parameters, minimizing
interference among edits. At inference, it identifies relevant edits by
comparing the sparse activation patterns of new queries to those stored during
editing. This enables generalization to rephrased queries by activating only
the relevant knowledge while suppressing unnecessary memory activation for
unrelated prompts. Experiments on question answering, hallucination correction,
and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral
demonstrate that MEMOIR achieves state-of-the-art performance across
reliability, generalization, and locality metrics, scaling to thousands of
sequential edits with minimal forgetting.

</details>


### [128] [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)
*MiniCPM Team,Chaojun Xiao,Yuxuan Li,Xu Han,Yuzhuo Bai,Jie Cai,Haotian Chen,Wentong Chen,Xin Cong,Ganqu Cui,Ning Ding,Shengdan Fan,Yewei Fang,Zixuan Fu,Wenyu Guan,Yitong Guan,Junshao Guo,Yufeng Han,Bingxiang He,Yuxiang Huang,Cunliang Kong,Qiuzuo Li,Siyuan Li,Wenhao Li,Yanghao Li,Yishan Li,Zhen Li,Dan Liu,Biyuan Lin,Yankai Lin,Xiang Long,Quanyu Lu,Yaxi Lu,Peiyan Luo,Hongya Lyu,Litu Ou,Yinxu Pan,Zekai Qu,Qundong Shi,Zijun Song,Jiayuan Su,Zhou Su,Ao Sun,Xianghui Sun,Peijun Tang,Fangzheng Wang,Feng Wang,Shuo Wang,Yudong Wang,Yesai Wu,Zhenyu Xiao,Jie Xie,Zihao Xie,Yukun Yan,Jiarui Yuan,Kaihuo Zhang,Lei Zhang,Linyue Zhang,Xueren Zhang,Yudi Zhang,Hengyu Zhao,Weilin Zhao,Weilun Zhao,Yuanqian Zhao,Zhi Zheng,Ge Zhou,Jie Zhou,Wei Zhou,Zihan Zhou,Zixuan Zhou,Zhiyuan Liu,Guoyang Zeng,Chao Jia,Dahai Li,Maosong Sun*

Main category: cs.CL

TL;DR: MiniCPM4是一种专为端侧设备设计的高效大型语言模型，通过在模型架构、训练数据、训练算法和推理系统四个关键维度上的创新实现了高效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发一种高效的大型语言模型，以适应端侧设备的需求。

Method: 提出了InfLLM v2、UltraClean、UltraChat v2、ModelTunnel v2、BitCPM以及CPM.cu等方法来提升模型的效率。

Result: MiniCPM4在多个基准测试中表现出色，特别是在处理长序列时比Qwen3-8B快得多。

Conclusion: MiniCPM4展示了其在端侧设备上的高效性和广泛适用性，并成功支持了多种应用。

Abstract: This paper introduces MiniCPM4, a highly efficient large language model (LLM)
designed explicitly for end-side devices. We achieve this efficiency through
systematic innovation in four key dimensions: model architecture, training
data, training algorithms, and inference systems. Specifically, in terms of
model architecture, we propose InfLLM v2, a trainable sparse attention
mechanism that accelerates both prefilling and decoding phases for long-context
processing. Regarding training data, we propose UltraClean, an efficient and
accurate pre-training data filtering and generation strategy, and UltraChat v2,
a comprehensive supervised fine-tuning dataset. These datasets enable
satisfactory model performance to be achieved using just 8 trillion training
tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient
pre-training strategy search, and improve existing post-training methods by
introducing chunk-wise rollout for load-balanced reinforcement learning and
data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose
CPM.cu that integrates sparse attention, model quantization, and speculative
sampling to achieve efficient prefilling and decoding. To meet diverse
on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B
parameters, respectively. Sufficient evaluation results show that MiniCPM4
outperforms open-source models of similar size across multiple benchmarks,
highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B
demonstrates significant speed improvements over Qwen3-8B when processing long
sequences. Through further adaptation, MiniCPM4 successfully powers diverse
applications, including trustworthy survey generation and tool use with model
context protocol, clearly showcasing its broad usability.

</details>


### [129] [Quantum Graph Transformer for NLP Sentiment Classification](https://arxiv.org/abs/2506.07937)
*Shamminuj Aktar,Andreas Bärtschi,Abdel-Hameed A. Badawy,Stephan Eidenbenz*

Main category: cs.CL

TL;DR: 提出了一个结合量子自注意力机制的消息传递框架QGT，并在情感分类任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 构建更高效和表达能力更强的模型，特别是在理解复杂结构化数据的关键领域。

Method: 提出了一种名为量子图变换器（QGT）的混合图架构，该架构将量子自注意力机制集成到结构化语言建模的消息传递框架中。注意机制使用参数化量子电路（PQCs）实现，使模型能够捕获丰富的上下文关系，同时与经典的注意机制相比显著减少可训练参数的数量。

Result: 在五个情感分类基准测试中，QGT的表现优于或相当于现有的量子自然语言处理（QNLP）模型。与等效的经典图变换器相比，QGT在真实世界数据集上的平均准确率提高了5.42%，在合成数据集上提高了4.76%。此外，QGT在Yelp数据集上达到了可比性能所需的标记样本减少了近50%。

Conclusion: 实验结果表明，量子图变换器（QGT）在五个情感分类基准测试中始终表现出更高的准确率或者与现有的量子自然语言处理（QNLP）模型相当的准确性。相比经典图变换器，QGT在真实世界数据集上的平均准确率提高了5.42%，在合成数据集上提高了4.76%。此外，QGT展示了改进的样本效率，在Yelp数据集上达到可比性能所需的标记样本减少了近50%。这些结果突显了基于图的量子自然语言处理技术在推进高效且可扩展的语言理解方面的潜力。

Abstract: Quantum machine learning is a promising direction for building more efficient
and expressive models, particularly in domains where understanding complex,
structured data is critical. We present the Quantum Graph Transformer (QGT), a
hybrid graph-based architecture that integrates a quantum self-attention
mechanism into the message-passing framework for structured language modeling.
The attention mechanism is implemented using parameterized quantum circuits
(PQCs), which enable the model to capture rich contextual relationships while
significantly reducing the number of trainable parameters compared to classical
attention mechanisms. We evaluate QGT on five sentiment classification
benchmarks. Experimental results show that QGT consistently achieves higher or
comparable accuracy than existing quantum natural language processing (QNLP)
models, including both attention-based and non-attention-based approaches. When
compared with an equivalent classical graph transformer, QGT yields an average
accuracy improvement of 5.42% on real-world datasets and 4.76% on synthetic
datasets. Additionally, QGT demonstrates improved sample efficiency, requiring
nearly 50% fewer labeled samples to reach comparable performance on the Yelp
dataset. These results highlight the potential of graph-based QNLP techniques
for advancing efficient and scalable language understanding.

</details>


### [130] [Statistical Hypothesis Testing for Auditing Robustness in Language Models](https://arxiv.org/abs/2506.07947)
*Paulius Rauba,Qiyao Wei,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: We present a framework called distribution-based perturbation analysis to test if LLM outputs change under interventions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for analyzing text-based outputs focus on different problems like bias and fairness.

Method: Reformulate LLM perturbation analysis as a frequentist hypothesis testing problem using Monte Carlo sampling in a semantic similarity space.

Result: The framework is model-agnostic, supports black-box LLMs, yields interpretable p-values, and provides scalar effect sizes.

Conclusion: This framework is useful for quantifying response changes, measuring true/false positive rates, evaluating alignment with reference models, and auditing LLMs.

Abstract: Consider the problem of testing whether the outputs of a large language model
(LLM) system change under an arbitrary intervention, such as an input
perturbation or changing the model variant. We cannot simply compare two LLM
outputs since they might differ due to the stochastic nature of the system, nor
can we compare the entire output distribution due to computational
intractability. While existing methods for analyzing text-based outputs exist,
they focus on fundamentally different problems, such as measuring bias or
fairness. To this end, we introduce distribution-based perturbation analysis, a
framework that reformulates LLM perturbation analysis as a frequentist
hypothesis testing problem. We construct empirical null and alternative output
distributions within a low-dimensional semantic similarity space via Monte
Carlo sampling, enabling tractable inference without restrictive distributional
assumptions. The framework is (i) model-agnostic, (ii) supports the evaluation
of arbitrary input perturbations on any black-box LLM, (iii) yields
interpretable p-values; (iv) supports multiple perturbations via controlled
error rates; and (v) provides scalar effect sizes. We demonstrate the
usefulness of the framework across multiple case studies, showing how we can
quantify response changes, measure true/false positive rates, and evaluate
alignment with reference models. Above all, we see this as a reliable
frequentist hypothesis testing framework for LLM auditing.

</details>


### [131] [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
*Tim Vieira,Tianyu Liu,Clemente Pasti,Yahya Emara,Brian DuSell,Benjamin LeBrun,Mario Giulianelli,Juan Luis Gastaldi,Timothy J. O'Donnell,Ryan Cotterell*

Main category: cs.CL

TL;DR: This paper addresses a problem in modern language models where they assign nonzero probability to 'noncanonical' token encodings that can't occur in training data. It proposes two methods to enforce canonicality: one using test-time inference and another by changing the model's parameterization. The authors show that correcting these errors improves data likelihood.


<details>
  <summary>Details</summary>
Motivation: Language models assign probability to noncanonical token encodings which don't appear in training data, leading to wastage of probability mass.

Method: Proposes two approaches - canonicality by conditioning and canonicality by construction.

Result: Fixing canonicality mistakes improves the likelihood of held-out data for multiple models and corpora.

Conclusion: The paper successfully demonstrates methods to ensure only canonical token strings receive positive probability in language models.

Abstract: Modern language models represent probability distributions over character
strings as distributions over (shorter) token strings derived via a
deterministic tokenizer, such as byte-pair encoding. While this approach is
highly effective at scaling up language models to large corpora, its current
incarnations have a concerning property: the model assigns nonzero probability
mass to an exponential number of $\it{noncanonical}$ token encodings of each
character string -- these are token strings that decode to valid character
strings but are impossible under the deterministic tokenizer (i.e., they will
never be seen in any training corpus, no matter how large). This misallocation
is both erroneous, as noncanonical strings never appear in training data, and
wasteful, diverting probability mass away from plausible outputs. These are
avoidable mistakes! In this work, we propose methods to enforce canonicality in
token-level language models, ensuring that only canonical token strings are
assigned positive probability. We present two approaches: (1) canonicality by
conditioning, leveraging test-time inference strategies without additional
training, and (2) canonicality by construction, a model parameterization that
guarantees canonical outputs but requires training. We demonstrate that fixing
canonicality mistakes improves the likelihood of held-out data for several
models and corpora.

</details>


### [132] [Correlated Errors in Large Language Models](https://arxiv.org/abs/2506.07962)
*Elliot Kim,Avi Garg,Kenny Peng,Nikhil Garg*

Main category: cs.CL

TL;DR: This study evaluates over 350 large language models (LLMs) across various dimensions and finds significant error correlations among them, even when they have different architectures and providers. It highlights that larger and more accurate models tend to have highly correlated errors.


<details>
  <summary>Details</summary>
Motivation: To empirically investigate whether diversity in training data, architecture, and providers truly mitigates homogeneity in LLMs.

Method: Conducting a large-scale empirical evaluation involving two popular leaderboards and a resume-screening task for over 350 LLMs.

Result: Found substantial correlation in model errors, with models agreeing 60% of the time when both make errors. Identified factors like shared architectures and providers as drivers of this correlation.

Conclusion: Larger and more accurate LLMs show high error correlations despite having distinct architectures and providers, challenging the assumption that diversity reduces homogeneity in LLMs.

Abstract: Diversity in training data, architecture, and providers is assumed to
mitigate homogeneity in LLMs. However, we lack empirical evidence on whether
different LLMs differ meaningfully. We conduct a large-scale empirical
evaluation on over 350 LLMs overall, using two popular leaderboards and a
resume-screening task. We find substantial correlation in model errors -- on
one leaderboard dataset, models agree 60% of the time when both models err. We
identify factors driving model correlation, including shared architectures and
providers. Crucially, however, larger and more accurate models have highly
correlated errors, even with distinct architectures and providers. Finally, we
show the effects of correlation in two downstream tasks: LLM-as-judge
evaluation and hiring -- the latter reflecting theoretical predictions
regarding algorithmic monoculture.

</details>


### [133] [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)
*Qingxiu Dong,Li Dong,Yao Tang,Tianzhu Ye,Yutao Sun,Zhifang Sui,Furu Wei*

Main category: cs.CL

TL;DR: This paper introduces Reinforcement Pre-Training (RPT), a new scaling paradigm for large language models and reinforcement learning. RPT reframes next-token prediction as a reasoning task trained with RL and improves language modeling accuracy.


<details>
  <summary>Details</summary>
Motivation: To provide a scalable method to leverage vast amounts of text data for general-purpose RL without relying on domain-specific annotated answers.

Method: Reinforcement Pre-Training (RPT) reframes next-token prediction as a reasoning task trained using RL and incentivizes the capability of next-token reasoning.

Result: RPT significantly improves the language modeling accuracy of predicting the next tokens and provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy.

Conclusion: RPT is an effective and promising scaling paradigm to advance language model pre-training.

Abstract: In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling
paradigm for large language models and reinforcement learning (RL).
Specifically, we reframe next-token prediction as a reasoning task trained
using RL, where it receives verifiable rewards for correctly predicting the
next token for a given context. RPT offers a scalable method to leverage vast
amounts of text data for general-purpose RL, rather than relying on
domain-specific annotated answers. By incentivizing the capability of
next-token reasoning, RPT significantly improves the language modeling accuracy
of predicting the next tokens. Moreover, RPT provides a strong pre-trained
foundation for further reinforcement fine-tuning. The scaling curves show that
increased training compute consistently improves the next-token prediction
accuracy. The results position RPT as an effective and promising scaling
paradigm to advance language model pre-training.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [134] [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
*Guibin Zhang,Muxin Fu,Guancheng Wan,Miao Yu,Kun Wang,Shuicheng Yan*

Main category: cs.MA

TL;DR: This paper introduces G-Memory, a hierarchical, agentic memory system for multi-agent systems inspired by organizational memory theory. It enhances the performance of multi-agent systems in embodied action and knowledge QA tasks.


<details>
  <summary>Details</summary>
Motivation: The existing memory mechanisms in multi-agent systems are overly simplistic and lack cross-trial and agent-specific customization, hindering their self-evolution capabilities.

Method: G-Memory, a three-tier graph hierarchy including insight, query, and interaction graphs, performs bi-directional memory traversal to retrieve insights and interaction trajectories for task execution and evolves the entire hierarchy by assimilating new collaborative trajectories.

Result: G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to 20.89% and 10.12%, respectively, without modifying the original frameworks.

Conclusion: G-Memory is a significant advancement in memory architecture for multi-agent systems, enabling more effective self-evolution and performance enhancement.

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [135] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: A training-free framework named Contextual Experience Replay (CER) is proposed to enhance the adaptability of large language model agents in complex environments by accumulating and synthesizing past experiences.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack environment-specific experiences and cannot continuously learn from past experiences during inference time, which limits their performance in sequential decision-making tasks.

Method: CER accumulates and synthesizes past experiences into a dynamic memory buffer, allowing agents to retrieve and augment themselves with relevant knowledge in new tasks.

Result: On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, it improves the success rate of the GPT-4o agent baseline by 51.0% with an average success rate of 36.7%.

Conclusion: The proposed CER framework can significantly improve the performance of LLM agents in complex environments by enabling self-improvement through contextual experience replay.

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [136] [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
*Clément Hongler,Andrew Emil*

Main category: cs.AI

TL;DR: 提出一种名为Xent Games的新方法来衡量大型语言模型的能力，通过构建Xent Game衡量标准作为能力基准，超越传统的生成采样任务。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型的能力测量方法，超越生成采样的任务形式，开发更丰富的评估方式。

Method: 通过考虑大型语言模型定义文本上的概率测度的隐式知识问题，提出了Xent Games，这是一种基于LLM测度的游戏形式，包括总结、反事实思维、异常检测、原创性搜索、逆向提示、辩论、创造性解决问题等任务。

Result: 展示了Xent Games空间足够大以包含许多有趣的例子，同时可以通过基本博弈论一致性公理构建。讨论了如何使用Xent Game空间来衡量LLMs的能力，并提出了使用进化动力学思想来解决一般能力测量的无界范围问题。

Conclusion: 提出了一种新的方法（Xent Games）来衡量大型语言模型的能力，并构建了Xent Game衡量标准作为能力基准。

Abstract: Large Language Models (LLMs) define probability measures on text. By
considering the implicit knowledge question of what it means for an LLM to know
such a measure and what it entails algorithmically, we are naturally led to
formulate a series of tasks that go beyond generative sampling, involving forms
of summarization, counterfactual thinking, anomaly detection, originality
search, reverse prompting, debating, creative solving, etc. These tasks can be
formulated as games based on LLM measures, which we call Cross-Entropy (Xent)
Games. Xent Games can be single-player or multi-player. They involve
cross-entropy scores and cross-entropy constraints, and can be expressed as
simple computational graphs and programs. We show the Xent Game space is large
enough to contain a wealth of interesting examples, while being constructible
from basic game-theoretic consistency axioms. We then discuss how the Xent Game
space can be used to measure the abilities of LLMs. This leads to the
construction of Xent Game measures: finite families of Xent Games that can be
used as capability benchmarks, built from a given scope, by extracting a
covering measure. To address the unbounded scope problem associated with the
challenge of measuring general abilities, we propose to explore the space of
Xent Games in a coherent fashion, using ideas inspired by evolutionary
dynamics.

</details>


### [137] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: We propose a meta-learning approach that uses soft prompts distilled from task-relevant image features to improve few-shot capabilities in large multimodal models.


<details>
  <summary>Details</summary>
Motivation: ICL performance in smaller LMMs is inconsistent and does not always improve monotonically with increasing examples. This occurs because the LMM is overwhelmed by additional information present in the image embeddings, which is not required for the downstream task.

Method: We propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. An attention-mapper module is introduced to enable task adaptation in LMMs under low-data regimes with just a few gradient steps.

Result: Evaluation on the VL-ICL Bench shows that our method performs better than ICL and related prompt-tuning approaches.

Conclusion: Our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks.

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [138] [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
*Parshin Shojaee,Iman Mirzadeh,Keivan Alizadeh,Maxwell Horton,Samy Bengio,Mehrdad Farajtabar*

Main category: cs.AI

TL;DR: This paper investigates the capabilities, scaling properties, and limitations of Large Reasoning Models (LRMs) through controlled puzzle environments, revealing that LRMs experience complete accuracy collapse beyond certain complexities and exhibit counterintuitive scaling limits.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental capabilities, scaling properties, and limitations of LRMs, which current evaluations mainly focus on final answer accuracy without providing insights into reasoning traces.

Method: Using controllable puzzle environments to manipulate complexity while maintaining consistent logical structures, enabling analysis of both final answers and internal reasoning traces.

Result: LRMs face complete accuracy collapse beyond certain complexities, have limitations in exact computation, and demonstrate different performance regimes compared to standard LLMs.

Conclusion: This study provides insights into how LRMs think and raises questions about their reasoning capabilities.

Abstract: Recent generations of language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers.
While these models demonstrate improved performance on reasoning benchmarks,
their fundamental capabilities, scaling properties, and limitations remain
insufficiently understood. Current evaluations primarily focus on established
math and coding benchmarks, emphasizing final answer accuracy. However, this
evaluation paradigm often suffers from contamination and does not provide
insights into the reasoning traces. In this work, we systematically investigate
these gaps with the help of controllable puzzle environments that allow precise
manipulation of complexity while maintaining consistent logical structures.
This setup enables the analysis of not only final answers but also the internal
reasoning traces, offering insights into how LRMs think. Through extensive
experiments, we show that LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counterintuitive scaling limit: their
reasoning effort increases with problem complexity up to a point, then declines
despite having remaining token budget. By comparing LRMs with their standard
LLM counterparts under same inference compute, we identify three performance
regimes: (1) low-complexity tasks where standard models outperform LRMs, (2)
medium-complexity tasks where LRMs demonstrates advantage, and (3)
high-complexity tasks where both models face complete collapse. We found that
LRMs have limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across scales. We also investigate the
reasoning traces in more depth, studying the patterns of explored solutions and
analyzing the models' computational behavior, shedding light on their
strengths, limitations, and raising questions about their reasoning
capabilities.

</details>


### [139] [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
*Liangliang You,Junchi Yao,Shu Yang,Guimin Hu,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: This work addresses behavioral hallucination in multimodal large language models for sequential images, revealing it arises from prior-driven bias and the snowball effect. It introduces SHE, a two-stage framework to detect and mitigate hallucinations using visual-textual alignment and orthogonal projection, with a new metric BEACH to measure severity.


<details>
  <summary>Details</summary>
Motivation: Addressing behavioral hallucination in sequential image processing for multimodal large language models, which is less studied compared to objective hallucination.

Method: Introduces SHE, a lightweight, two-stage framework including detection via adaptive temporal window and mitigation via orthogonal projection onto the joint embedding space.

Result: Empirically reduces behavioral hallucination by over 10% on BEACH while maintaining descriptive accuracy.

Conclusion: Highlights the importance of addressing behavioral hallucination in sequential images and presents an effective solution with SHE and BEACH.

Abstract: While multimodal large language models excel at various tasks, they still
suffer from hallucinations, which limit their reliability and scalability for
broader domain applications. To address this issue, recent research mainly
focuses on objective hallucination. However, for sequential images, besides
objective hallucination, there is also behavioral hallucination, which is less
studied. This work aims to fill in the gap. We first reveal that behavioral
hallucinations mainly arise from two key factors: prior-driven bias and the
snowball effect. Based on these observations, we introduce SHE (Sequence
Hallucination Eradication), a lightweight, two-stage framework that (1) detects
hallucinations via visual-textual alignment check using our proposed adaptive
temporal window and (2) mitigates them via orthogonal projection onto the joint
embedding space. We also propose a new metric (BEACH) to quantify behavioral
hallucination severity. Empirical results on standard benchmarks demonstrate
that SHE reduces behavioral hallucination by over 10% on BEACH while
maintaining descriptive accuracy.

</details>


### [140] [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)
*Peiran Li,Xinkai Zou,Zhuohang Wu,Ruifeng Li,Shuo Xing,Hanwen Zheng,Zhikai Hu,Yuping Wang,Haoxi Li,Qin Yuan,Yingmo Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: Introduce SAFEFLOW, a protocol-level framework that ensures secure information flow, reliability, and multi-agent coordination for LLM/VLM-based agents.


<details>
  <summary>Details</summary>
Motivation: Today's agent frameworks lack secure information flow, reliability, and multi-agent coordination mechanisms, making them fragile.

Method: Develop SAFEFLOW with fine-grained IFC, transactional execution, conflict resolution, and secure scheduling, along with mechanisms like write-ahead logging and rollback.

Result: SAFEBLOW maintains task performance and security guarantees in hostile environments, surpassing state-of-the-art methods.

Conclusion: SAFEBLOW and SAFEBLOWBENCH establish a foundation for reliable, secure agent ecosystems.

Abstract: Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.

</details>


### [141] [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)
*Shoko Oka*

Main category: cs.AI

TL;DR: This study examines if modern large language models can solve two fundamental AI challenges: the Frame Problem and the Symbol Grounding Problem. It tests 13 prominent LLMs through two benchmark tasks and finds that some closed models perform well.


<details>
  <summary>Details</summary>
Motivation: Investigate if modern large language models can solve long-standing AI challenges like the Frame Problem and the Symbol Grounding Problem.

Method: Designed two benchmark tasks for each problem, tested 13 LLMs (closed and open-source) under zero-shot conditions, and evaluated their outputs across five trials using multiple scoring criteria.

Result: Open-source models varied in performance based on size, quantization, and instruction tuning, but some closed models consistently scored high.

Conclusion: Select modern LLMs may have the capacity to produce meaningful and stable responses to these theoretical challenges.

Abstract: Recent advancements in large language models (LLMs) have revitalized
philosophical debates surrounding artificial intelligence. Two of the most
fundamental challenges - namely, the Frame Problem and the Symbol Grounding
Problem - have historically been viewed as unsolvable within traditional
symbolic AI systems. This study investigates whether modern LLMs possess the
cognitive capacities required to address these problems. To do so, I designed
two benchmark tasks reflecting the philosophical core of each problem,
administered them under zero-shot conditions to 13 prominent LLMs (both closed
and open-source), and assessed the quality of the models' outputs across five
trials each. Responses were scored along multiple criteria, including
contextual reasoning, semantic coherence, and information filtering. The
results demonstrate that while open-source models showed variability in
performance due to differences in model size, quantization, and instruction
tuning, several closed models consistently achieved high scores. These findings
suggest that select modern LLMs may be acquiring capacities sufficient to
produce meaningful and stable responses to these long-standing theoretical
challenges.

</details>


### [142] [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: This paper introduces LUCIFER, a framework combining hierarchical decision-making, reinforcement learning, and large language models to improve autonomous systems' decision-making by integrating human contextual knowledge.


<details>
  <summary>Details</summary>
Motivation: The rapid obsolescence of pre-existing knowledge in dynamic environments limits autonomous decision-making effectiveness.

Method: Proposes LUCIFER, which uses large language models in two roles: context extractors and exploration facilitators.

Result: Demonstrates improved exploration efficiency and decision quality compared to traditional methods.

Conclusion: Highlights the potential of context-driven decision-making in enhancing autonomous systems' operational success.

Abstract: In dynamic environments, the rapid obsolescence of pre-existing environmental
knowledge creates a gap between an agent's internal model and the evolving
reality of its operational context. This disparity between prior and updated
environmental valuations fundamentally limits the effectiveness of autonomous
decision-making. To bridge this gap, the contextual bias of human domain
stakeholders, who naturally accumulate insights through direct, real-time
observation, becomes indispensable. However, translating their nuanced, and
context-rich input into actionable intelligence for autonomous systems remains
an open challenge. To address this, we propose LUCIFER (Language Understanding
and Context-Infused Framework for Exploration and Behavior Refinement), a
domain-agnostic framework that integrates a hierarchical decision-making
architecture with reinforcement learning (RL) and large language models (LLMs)
into a unified system. This architecture mirrors how humans decompose complex
tasks, enabling a high-level planner to coordinate specialised sub-agents, each
focused on distinct objectives and temporally interdependent actions. Unlike
traditional applications where LLMs are limited to single role, LUCIFER
integrates them in two synergistic roles: as context extractors, structuring
verbal stakeholder input into domain-aware representations that influence
decision-making through an attention space mechanism aligning LLM-derived
insights with the agent's learning process, and as zero-shot exploration
facilitators guiding the agent's action selection process during exploration.
We benchmark various LLMs in both roles and demonstrate that LUCIFER improves
exploration efficiency and decision quality, outperforming flat,
goal-conditioned policies. Our findings show the potential of context-driven
decision-making, where autonomous systems leverage human contextual knowledge
for operational success.

</details>


### [143] [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
*Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: This paper introduces IneqMath, a new dataset and evaluation framework for inequality proving tasks using large language models (LLMs). It highlights the difficulty LLMs face in constructing rigorous proofs.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for inequality proving are often limited in scope or overly formal, making it hard for LLMs to excel in this task.

Method: The authors reformulate inequality proving into two subtasks: bound estimation and relation prediction. They also create a new dataset and an evaluation method involving multiple step-wise judges.

Result: Top LLMs perform poorly when their reasoning steps are examined closely, revealing a gap between finding answers and building rigorous proofs.

Conclusion: Current LLMs struggle with inequality proving due to issues in their deductive reasoning. The paper suggests future work should focus on theorem-guided reasoning and self-refinement.

Abstract: Inequality proving, crucial across diverse scientific and mathematical
fields, tests advanced reasoning skills such as discovering tight bounds and
strategic theorem application. This makes it a distinct, demanding frontier for
large language models (LLMs), offering insights beyond general mathematical
problem-solving. Progress in this area is hampered by existing datasets that
are often scarce, synthetic, or rigidly formal. We address this by proposing an
informal yet verifiable task formulation, recasting inequality proving into two
automatically checkable subtasks: bound estimation and relation prediction.
Building on this, we release IneqMath, an expert-curated dataset of
Olympiad-level inequalities, including a test set and training corpus enriched
with step-wise solutions and theorem annotations. We also develop a novel
LLM-as-judge evaluation framework, combining a final-answer judge with four
step-wise judges designed to detect common reasoning flaws. A systematic
evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even
top models like o1 achieve less than 10% overall accuracy under step-wise
scrutiny; this is a drop of up to 65.5% from their accuracy considering only
final answer equivalence. This discrepancy exposes fragile deductive chains and
a critical gap for current LLMs between merely finding an answer and
constructing a rigorous proof. Scaling model size and increasing test-time
computation yield limited gains in overall proof correctness. Instead, our
findings highlight promising research directions such as theorem-guided
reasoning and self-refinement. Code and data are available at
https://ineqmath.github.io/.

</details>


### [144] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
*Jixiang Hong,Yiran Zhang,Guanzhong Wang,Yi Liu,Ji-Rong Wen,Rui Yan*

Main category: cs.AI

TL;DR: This paper introduces a self-supervised dual reward mechanism to improve the performance of large multimodal models in both visual understanding and generation tasks.


<details>
  <summary>Details</summary>
Motivation: To address the issue of inaccurate image-text alignment and contradictory text responses in current large multimodal models.

Method: Introducing a self-supervised dual reward mechanism by sampling multiple outputs for a given input and reversing input-output pairs to compute dual likelihood as self-rewards.

Result: The proposed method enhances the performance of the model without external supervision, with remarkable improvements in text-to-image tasks.

Conclusion: The self-supervised dual reward mechanism is an effective way to reinforce the understanding and generation capabilities of large multimodal models.

Abstract: Building upon large language models (LLMs), recent large multimodal models
(LMMs) unify cross-model understanding and generation into a single framework.
However, LMMs still struggle to achieve accurate image-text alignment, prone to
generating text responses contradicting the visual input or failing to follow
the text-to-image prompts. Current solutions require external supervision
(e.g., human feedback or reward models) and only address unidirectional
tasks-either understanding or generation. In this work, based on the
observation that understanding and generation are inverse dual tasks, we
introduce a self-supervised dual reward mechanism to reinforce the
understanding and generation capabilities of LMMs. Specifically, we sample
multiple outputs for a given input in one task domain, then reverse the
input-output pairs to compute the dual likelihood of the model as self-rewards
for optimization. Extensive experimental results on visual understanding and
generation benchmarks demonstrate that our method can effectively enhance the
performance of the model without any external supervision, especially achieving
remarkable improvements in text-to-image tasks.

</details>


### [145] [$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)
*Victor Barres,Honghua Dong,Soham Ray,Xujie Si,Karthik Narasimhan*

Main category: cs.AI

TL;DR: This paper introduces $\tau^2$-bench, a new benchmark for conversational AI agents that models a Telecom dual-control domain, focusing on agent coordination, communication, and user guidance.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for conversational AI agents do not accurately reflect real-world scenarios where users actively participate in modifying the shared world state.

Method: Introduces $\tau^2$-bench with four key contributions: a novel Telecom dual-control domain, a task generator, a reliable user simulator, and fine-grained analysis of agent performance.

Result: $\tau^2$-bench shows significant performance drops when agents shift from no-user to dual-control, emphasizing the challenges of guiding users.

Conclusion: The paper concludes that $\tau^2$-bench provides a controlled testbed for agents needing to reason effectively and guide user actions.

Abstract: Existing benchmarks for conversational AI agents simulate single-control
environments, where only the AI agent can use tools to interact with the world,
while the user remains a passive information provider. This differs from
real-world scenarios like technical support, where users need to actively
participate in modifying the state of the (shared) world. In order to address
this gap, we introduce $\tau^2$-bench, with four key contributions:
  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both
agent and user make use of tools to act in a shared, dynamic environment that
tests both agent coordination and communication,
  2) A compositional task generator that programmatically creates diverse,
verifiable tasks from atomic components, ensuring domain coverage and
controlled complexity,
  3) A reliable user simulator tightly coupled with the environment, whose
behavior is constrained by tools and observable states, improving simulation
fidelity,
  4) Fine-grained analysis of agent performance through multiple ablations
including separating errors arising from reasoning vs
communication/coordination.
  In particular, our experiments show significant performance drops when agents
shift from no-user to dual-control, highlighting the challenges of guiding
users. Overall, $\tau^2$-bench provides a controlled testbed for agents that
must both reason effectively and guide user actions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [146] [Mitigating Object Hallucination via Robust Local Perception Search](https://arxiv.org/abs/2506.06729)
*Zixian Gao,Chao Yang,Zhanhui Zhou,Xing Xu,Chaochao Lu*

Main category: cs.CV

TL;DR: Introduce Local Perception Search (LPS), a simple and training-free decoding method that effectively suppresses hallucination phenomena in multimodal large language models.


<details>
  <summary>Details</summary>
Motivation: Address the hallucination problem in multimodal large language models where outputs do not align with image content.

Method: Develop Local Perception Search (LPS) that uses local visual prior information to correct decoding process.

Result: LPS significantly reduces hallucination incidences compared to baseline, especially in noisy settings.

Conclusion: LPS is a plug-and-play approach that improves performance across different models.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled
them to effectively integrate vision and language, addressing a variety of
downstream tasks. However, despite their significant success, these models
still exhibit hallucination phenomena, where the outputs appear plausible but
do not align with the content of the images. To mitigate this issue, we
introduce Local Perception Search (LPS), a decoding method during inference
that is both simple and training-free, yet effectively suppresses
hallucinations. This method leverages local visual prior information as a value
function to correct the decoding process. Additionally, we observe that the
impact of the local visual prior on model performance is more pronounced in
scenarios with high levels of image noise. Notably, LPS is a plug-and-play
approach that is compatible with various models. Extensive experiments on
widely used hallucination benchmarks and noisy data demonstrate that LPS
significantly reduces the incidence of hallucinations compared to the baseline,
showing exceptional performance, particularly in noisy settings.

</details>


### [147] [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
*Yikun Ji,Hong Yan,Jun Lan,Huijia Zhu,Weiqiang Wang,Qi Fan,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的方法来改进多模态大语言模型用于AI生成图像检测和定位


<details>
  <summary>Details</summary>
Motivation: 提高图像生成技术的解释性和鲁棒性检测方法的需求

Method: 通过多阶段优化策略微调多模态大语言模型

Result: 在检测AI生成图像和定位视觉缺陷方面表现优异

Conclusion: 构造了一个人类对齐的视觉-文本推理的数据集并改进了现有方法

Abstract: The rapid advancement of image generation technologies intensifies the demand
for interpretable and robust detection methods. Although existing approaches
often attain high accuracy, they typically operate as black boxes without
providing human-understandable justifications. Multi-modal Large Language
Models (MLLMs), while not originally intended for forgery detection, exhibit
strong analytical and reasoning capabilities. When properly fine-tuned, they
can effectively identify AI-generated images and offer meaningful explanations.
However, existing MLLMs still struggle with hallucination and often fail to
align their visual interpretations with actual image content and human
reasoning. To bridge this gap, we construct a dataset of AI-generated images
annotated with bounding boxes and descriptive captions that highlight synthesis
artifacts, establishing a foundation for human-aligned visual-textual grounded
reasoning. We then finetune MLLMs through a multi-stage optimization strategy
that progressively balances the objectives of accurate detection, visual
localization, and coherent textual explanation. The resulting model achieves
superior performance in both detecting AI-generated images and localizing
visual flaws, significantly outperforming baseline methods.

</details>


### [148] [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
*Hao Tang,Chengchao Shen*

Main category: cs.CV

TL;DR: 提出了一种Spatial Token Fusion (STF)方法和一个Multi-Block Token Fusion (MBTF)模块，用于减少视觉标记序列的长度并补充多粒度特征，从而在保持多模态推理能力的同时提高推理效率。实验表明该方法在8个流行的视觉语言基准测试中表现优于或至少与基线相当，仅使用了基线的25%的视觉标记。


<details>
  <summary>Details</summary>
Motivation: 解决大型多模态模型（LMMs）由于大型语言模型（LLMs）的高成本和处理长视觉标记序列的二次复杂性而面临的计算挑战。

Method: 提出了Spatial Token Fusion (STF)方法来学习紧凑的视觉标记，并使用Multi-Block Token Fusion (MBTF)模块补充多粒度特征。

Result: 在8个流行的视觉语言基准测试中，该方法基于LLaVA-1.5的表现优于或至少与基线相当，仅使用了基线的25%的视觉标记。

Conclusion: 所提出的方法在不牺牲多模态推理能力的情况下提高了推理效率。

Abstract: Large multimodal models (LMMs) suffer significant computational challenges
due to the high cost of Large Language Models (LLMs) and the quadratic
complexity of processing long vision token sequences. In this paper, we explore
the spatial redundancy among vision tokens and shorten the length of vision
token sequences for inference acceleration. Specifically, we propose a Spatial
Token Fusion (STF) method to learn compact vision tokens for short vision token
sequence, where spatial-adjacent tokens are fused into one. Meanwhile,
weight-frozen vision encoder can not well adapt to the demand of extensive
downstream vision-language tasks. To this end, we further introduce a
Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features
for the reduced token sequence. Overall, we combine STF and MBTF module to
balance token reduction and information preservation, thereby improving
inference efficiency without sacrificing multimodal reasoning capabilities.
Experimental results demonstrate that our method based on LLaVA-1.5 achieves
comparable or even superior performance to the baseline on 8 popular
vision-language benchmarks with only $25\%$ vision tokens of baseline. The
source code and trained weights are available at
https://github.com/visresearch/LLaVA-STF.

</details>


### [149] [SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning](https://arxiv.org/abs/2506.07196)
*Mengya Xu,Zhongzhen Huang,Dillan Imans,Yiru Ye,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: This paper presents SAP-Bench, a large-scale dataset for evaluating MLLMs in surgical action planning, and introduces the MLLM-SAP framework. It also identifies performance gaps in next action prediction among current state-of-the-art MLLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the need for an effective evaluation of MLLMs in the surgical action planning task, which requires precise and sophisticated analytical capabilities unlike mathematical reasoning.

Method: The method involves creating a large-scale dataset named SAP-Bench, which includes surgical action annotations and strategically sampled frames. The MLLM-SAP framework is proposed to leverage MLLMs for generating next action recommendations using surgical domain knowledge.

Result: The result shows that the proposed SAP-Bench dataset can effectively evaluate MLLMs in surgical action planning. However, it reveals critical gaps in next action prediction performance among the evaluated state-of-the-art MLLMs.

Conclusion: The paper introduces SAP-Bench, a large-scale dataset for evaluating MLLMs in surgical action planning tasks. It also proposes the MLLM-SAP framework and evaluates several state-of-the-art MLLMs revealing performance gaps.

Abstract: Effective evaluation is critical for driving advancements in MLLM research.
The surgical action planning (SAP) task, which aims to generate future action
sequences from visual inputs, demands precise and sophisticated analytical
capabilities. Unlike mathematical reasoning, surgical decision-making operates
in life-critical domains and requires meticulous, verifiable processes to
ensure reliability and patient safety. This task demands the ability to
distinguish between atomic visual actions and coordinate complex, long-horizon
procedures, capabilities that are inadequately evaluated by current benchmarks.
To address this gap, we introduce SAP-Bench, a large-scale, high-quality
dataset designed to enable multimodal large language models (MLLMs) to perform
interpretable surgical action planning. Our SAP-Bench benchmark, derived from
the cholecystectomy procedures context with the mean duration of 1137.5s, and
introduces temporally-grounded surgical action annotations, comprising the
1,226 clinically validated action clips (mean duration: 68.7s) capturing five
fundamental surgical actions across 74 procedures. The dataset provides 1,152
strategically sampled current frames, each paired with the corresponding next
action as multimodal analysis anchors. We propose the MLLM-SAP framework that
leverages MLLMs to generate next action recommendations from the current
surgical scene and natural language instructions, enhanced with injected
surgical domain knowledge. To assess our dataset's effectiveness and the
broader capabilities of current models, we evaluate seven state-of-the-art
MLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,
Step-1o, and GLM-4v) and reveal critical gaps in next action prediction
performance.

</details>


### [150] [Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning](https://arxiv.org/abs/2506.07227)
*Tianyi Bai,Yuxuan Fan,Jiantao Qiu,Fupeng Sun,Jiayi Song,Junlin Han,Zichen Liu,Conghui He,Wentao Zhang,Binhang Yuan*

Main category: cs.CV

TL;DR: We developed a new dataset and fine-tuning method to improve fine-grained visual reasoning in multimodal large language models.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with detecting small visual differences.

Method: Proposed a data generation pipeline and introduced a supervised fine-tuning framework with a feature-level consistency loss.

Result: Improved performance on detecting micro-edits and standard vision-language tasks.

Conclusion: Targeted data and alignment objectives enhance fine-grained visual reasoning in MLLMs.

Abstract: Multimodal large language models (MLLMs) have achieved strong performance on
vision-language tasks but still struggle with fine-grained visual differences,
leading to hallucinations or missed semantic shifts. We attribute this to
limitations in both training data and learning objectives. To address these
issues, we propose a controlled data generation pipeline that produces
minimally edited image pairs with semantically aligned captions. Using this
pipeline, we construct the Micro Edit Dataset (MED), containing over 50K
image-text pairs spanning 11 fine-grained edit categories, including attribute,
count, position, and object presence changes. Building on MED, we introduce a
supervised fine-tuning (SFT) framework with a feature-level consistency loss
that promotes stable visual embeddings under small edits. We evaluate our
approach on the Micro Edit Detection benchmark, which includes carefully
balanced evaluation pairs designed to test sensitivity to subtle visual
variations across the same edit categories. Our method improves difference
detection accuracy and reduces hallucinations compared to strong baselines,
including GPT-4o. Moreover, it yields consistent gains on standard
vision-language tasks such as image captioning and visual question answering.
These results demonstrate the effectiveness of combining targeted data and
alignment objectives for enhancing fine-grained visual reasoning in MLLMs.

</details>


### [151] [Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification](https://arxiv.org/abs/2506.07235)
*Tianyi Bai,Zengjie Hu,Fupeng Sun,Jiantao Qiu,Yizhen Jiang,Guangxin He,Bohan Zeng,Conghui He,Binhang Yuan,Wentao Zhang*

Main category: cs.CV

TL;DR: This paper introduces a new framework for MLLMs that allows for dynamic, iterative reasoning over visual content, improving performance and interpretability on various visual reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Most MLLMs adopt a static inference paradigm, encoding the entire image into fixed visual tokens upfront, which limits their ability to iteratively refine understanding or adapt to context during inference. This contrasts sharply with human perception, which is dynamic, selective, and feedback-driven.

Method: The method formulates the problem as a Markov Decision Process, involving a reasoner that proposes visual actions and a verifier that evaluates these actions and determines when reasoning should terminate.

Result: Our method significantly outperforms existing approaches across diverse visual reasoning benchmarks, offering not only improved accuracy but also more interpretable and grounded reasoning processes.

Conclusion: This study introduces a novel framework for inference-time visual token scaling that enables MLLMs to perform iterative, verifier-guided reasoning over visual content.

Abstract: Multi-modal large language models (MLLMs) have achieved remarkable
capabilities by integrating visual perception with language understanding,
enabling applications such as image-grounded dialogue, visual question
answering, and scientific analysis. However, most MLLMs adopt a static
inference paradigm, encoding the entire image into fixed visual tokens upfront,
which limits their ability to iteratively refine understanding or adapt to
context during inference. This contrasts sharply with human perception, which
is dynamic, selective, and feedback-driven. In this work, we introduce a novel
framework for inference-time visual token scaling that enables MLLMs to perform
iterative, verifier-guided reasoning over visual content. We formulate the
problem as a Markov Decision Process, involving a reasoner that proposes visual
actions and a verifier, which is trained via multi-step Direct Preference
Optimization (DPO), that evaluates these actions and determines when reasoning
should terminate. To support this, we present a new dataset, VTS, comprising
supervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning
comparisons (VTS-DPO). Our method significantly outperforms existing approaches
across diverse visual reasoning benchmarks, offering not only improved accuracy
but also more interpretable and grounded reasoning processes. These results
demonstrate the promise of dynamic inference mechanisms for enabling
fine-grained, context-aware visual reasoning in next-generation MLLMs.

</details>


### [152] [GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning](https://arxiv.org/abs/2506.07460)
*Taeryung Lee,Hyeongjin Nam,Gyeongsik Moon,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: Proposed GLOS framework enhances sign language generation by using gloss-level conditions and a Temporal Alignment Conditioning module for better lexical order and semantic accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the issues of incorrect lexical ordering and low semantic accuracy in existing Sign Language Generation (SLG) methods.

Method: Gloss-level conditions and Temporal Alignment Conditioning (TAC) module.

Result: The method generates signs with correct lexical order and high semantic accuracy, outperforming previous methods on CSL-Daily and Phoenix-2014T datasets.

Conclusion: The proposed GLOS framework improves the lexical order and semantic accuracy in sign language generation.

Abstract: Sign language generation (SLG), or text-to-sign generation, bridges the gap
between signers and non-signers. Despite recent progress in SLG, existing
methods still often suffer from incorrect lexical ordering and low semantic
accuracy. This is primarily due to sentence-level condition, which encodes the
entire sentence of the input text into a single feature vector as a condition
for SLG. This approach fails to capture the temporal structure of sign language
and lacks the granularity of word-level semantics, often leading to disordered
sign sequences and ambiguous motions. To overcome these limitations, we propose
GLOS, a sign language generation framework with temporally aligned gloss-level
conditioning. First, we employ gloss-level conditions, which we define as
sequences of gloss embeddings temporally aligned with the motion sequence. This
enables the model to access both the temporal structure of sign language and
word-level semantics at each timestep. As a result, this allows for
fine-grained control of signs and better preservation of lexical order. Second,
we introduce a condition fusion module, temporal alignment conditioning (TAC),
to efficiently deliver the word-level semantic and temporal structure provided
by the gloss-level condition to the corresponding motion timesteps. Our method,
which is composed of gloss-level conditions and TAC, generates signs with
correct lexical order and high semantic accuracy, outperforming prior methods
on CSL-Daily and Phoenix-2014T.

</details>


### [153] [Learning Speaker-Invariant Visual Features for Lipreading](https://arxiv.org/abs/2506.07572)
*Yu Li,Feng Xue,Shujie Li,Jinrui Zhang,Shuang Yang,Dan Guo,Richang Hong*

Main category: cs.CV

TL;DR: SIFLip通过解纠缠说话者特定的属性来提高唇读的泛化能力，实验结果表明其在多个公共数据集上的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的唇读方法通常提取包含说话者特定唇部属性（如形状、颜色、纹理）的视觉特征，这导致了视觉和文本之间的虚假相关性，从而降低了唇读的准确性并限制了模型的泛化能力。

Method: SIFLip是一个说话者不变的视觉特征学习框架，它利用稳定的文本嵌入作为监督信号来学习跨说话者的共同视觉表示，并通过梯度反转进一步显式地从主干网络中解纠缠个性化的视觉特征。

Result: 实验结果表明，SIFLip在多个公共数据集上显著提高了泛化性能，并优于最先进的方法。

Conclusion: SIFLip通过引入两种互补的解纠缠模块（隐式解纠缠和显式解纠缠）来提高泛化能力。实验结果表明，SIFLip在多个公共数据集上显著提高了泛化性能，并优于最先进的方法。

Abstract: Lipreading is a challenging cross-modal task that aims to convert visual lip
movements into spoken text. Existing lipreading methods often extract visual
features that include speaker-specific lip attributes (e.g., shape, color,
texture), which introduce spurious correlations between vision and text. These
correlations lead to suboptimal lipreading accuracy and restrict model
generalization. To address this challenge, we introduce SIFLip, a
speaker-invariant visual feature learning framework that disentangles
speaker-specific attributes using two complementary disentanglement modules
(Implicit Disentanglement and Explicit Disentanglement) to improve
generalization. Specifically, since different speakers exhibit semantic
consistency between lip movements and phonetic text when pronouncing the same
words, our implicit disentanglement module leverages stable text embeddings as
supervisory signals to learn common visual representations across speakers,
implicitly decoupling speaker-specific features. Additionally, we design a
speaker recognition sub-task within the main lipreading pipeline to filter
speaker-specific features, then further explicitly disentangle these
personalized visual features from the backbone network via gradient reversal.
Experimental results demonstrate that SIFLip significantly enhances
generalization performance across multiple public datasets. Experimental
results demonstrate that SIFLip significantly improves generalization
performance across multiple public datasets, outperforming state-of-the-art
methods.

</details>


### [154] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: Investigating whether VLMs can truly perform MM-ICL reveals that they often rely on copying rather than learning, even when provided with rationales.


<details>
  <summary>Details</summary>
Motivation: To evaluate if vision-language models (VLMs) truly exhibit multimodal in-context learning (MM-ICL) especially under distribution shifts.

Method: Conducting experiments with different open-source VLMs and proprietary models under various conditions including different shot counts, retrieval methods, rationale qualities, and data distributions.

Result: Performance often degrades with more demonstrations and models tend to copy answers rather than learn from them. Adding generated rationales alongside answers does not significantly improve performance.

Conclusion: Current VLMs do not effectively utilize demonstration-level information as intended in MM-ICL.

Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context
learning (ICL), a property similar to that of their language-only counterparts.
While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies
show they often rely on shallow heuristics -- such as copying or majority
voting -- rather than true task understanding. We revisit this assumption by
evaluating VLMs under distribution shifts, where support examples come from a
dataset different from the query. Surprisingly, performance often degrades with
more demonstrations, and models tend to copy answers rather than learn from
them. To investigate further, we propose a new MM-ICL with Reasoning pipeline
that augments each demonstration with a generated rationale alongside the
answer. We conduct extensive and comprehensive experiments on both perception-
and reasoning-required datasets with open-source VLMs ranging from 3B to 72B
and proprietary models such as Gemini 2.0. We conduct controlled studies
varying shot count, retrieval method, rationale quality, and distribution. Our
results show limited performance sensitivity across these factors, suggesting
that current VLMs do not effectively utilize demonstration-level information as
intended in MM-ICL.

</details>


### [155] [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
*Yunfei Xie,Yinsong Ma,Shiyi Lan,Alan Yuille,Junfei Xiao,Chen Wei*

Main category: cs.CV

TL;DR: This paper introduces a novel post-training method called Visual Game Learning (ViGaL) to enhance multimodal reasoning capabilities in large language models by training them to play simple arcade games.


<details>
  <summary>Details</summary>
Motivation: Inspired by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, the authors aim to develop generalizable reasoning abilities in multimodal large language models.

Method: Post-training a 7B-parameter MLLM via reinforcement learning on simple arcade-like games such as Snake.

Result: The proposed method significantly improves downstream performance on multimodal math benchmarks and multi-discipline questions, capturing transferable reasoning skills without exposure to worked solutions, equations, or diagrams during training.

Conclusion: Synthetic, rule-based games can act as controllable and scalable pre-text tasks for unlocking generalizable multimodal reasoning abilities in MLLMs.

Abstract: Developing generalizable reasoning capabilities in multimodal large language
models (MLLMs) remains challenging. Motivated by cognitive science literature
suggesting that gameplay promotes transferable cognitive skills, we propose a
novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs
develop out-of-domain generalization of multimodal reasoning through playing
arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM
via reinforcement learning (RL) on simple arcade-like games, e.g. Snake,
significantly enhances its downstream performance on multimodal math benchmarks
like MathVista, and on multi-discipline questions like MMMU, without seeing any
worked solutions, equations, or diagrams during RL, suggesting the capture of
transferable reasoning skills. Remarkably, our model outperforms specialist
models tuned on multimodal reasoning data in multimodal reasoning benchmarks,
while preserving the base model's performance on general visual benchmarks, a
challenge where specialist models often fall short. Our findings suggest a new
post-training paradigm: synthetic, rule-based games can serve as controllable
and scalable pre-text tasks that unlock generalizable multimodal reasoning
abilities in MLLMs.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [156] [Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding](https://arxiv.org/abs/2506.07233)
*Tzu-wen Hsu,Ke-Han Lu,Cheng-Han Chiang,Hung-yi Lee*

Main category: eess.AS

TL;DR: A lightweight inference-time strategy called Audio-Aware Decoding (AAD) is introduced to reduce hallucination in Large Audio-Language Models (LALMs). It improves F1 scores on object hallucination datasets and accuracy on general audio QA datasets.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucination in LALMs when processing audio and text inputs.

Method: Introduces Audio-Aware Decoding (AAD), which uses contrastive decoding to promote tokens whose probability increases with audio context.

Result: Improves F1 scores by 0.046 to 0.428 on object hallucination datasets and boosts accuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%.

Conclusion: AAD is an effective method to mitigate hallucination in LALMs and enhances performance on both specific and general audio QA tasks.

Abstract: Large Audio-Language Models (LALMs) can take audio and text as the inputs and
answer questions about the audio. While prior LALMs have shown strong
performance on standard benchmarks, there has been alarming evidence that LALMs
can hallucinate what is presented in the audio. To mitigate the hallucination
of LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time
strategy that uses contrastive decoding to compare the token prediction logits
with and without the audio context. By contrastive decoding, AAD promotes the
tokens whose probability increases when the audio is present. We conduct our
experiment on object hallucination datasets with three LALMs and show that AAD
improves the F1 score by 0.046 to 0.428. We also show that AAD can improve the
accuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We
conduct thorough ablation studies to understand the effectiveness of each
component in AAD.

</details>


### [157] [Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition](https://arxiv.org/abs/2506.07515)
*Asahi Sakuma,Hiroaki Sato,Ryuga Sugano,Tadashi Kumano,Yoshihiko Kawai,Tetsuji Ogawa*

Main category: eess.AS

TL;DR: 提出了一种新的多说话人自动语音识别框架，通过引入Speaker-Distinguishable CTC(SD-CTC)，在无需辅助信息的情况下提高了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前方法（Serialized Output Training, SOT）由于说话人分配失败导致识别错误，并且从自然对话语音中提取辅助信息（如令牌级时间戳）具有挑战性。

Method: 提出了Speaker-Distinguishable CTC(SD-CTC)，它能为每个帧联合分配令牌及其对应的说话人标签，并将其集成到SOT框架中。

Result: 与仅使用SOT相比，结合SD-CTC和SOT的多任务学习使SOT模型的错误率降低了26%，性能与依赖于辅助信息的最新方法相当。

Conclusion: 所提出的框架在没有辅助信息的情况下实现了与依赖于辅助信息的最新方法相当的性能。

Abstract: This paper presents a novel framework for multi-talker automatic speech
recognition without the need for auxiliary information. Serialized Output
Training (SOT), a widely used approach, suffers from recognition errors due to
speaker assignment failures. Although incorporating auxiliary information, such
as token-level timestamps, can improve recognition accuracy, extracting such
information from natural conversational speech remains challenging. To address
this limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension
of CTC that jointly assigns a token and its corresponding speaker label to each
frame. We further integrate SD-CTC into the SOT framework, enabling the SOT
model to learn speaker distinction using only overlapping speech and
transcriptions. Experimental comparisons show that multi-task learning with
SD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves
performance comparable to state-of-the-art methods relying on auxiliary
information.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [158] [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Chen-Fu Chen,Haim Permuter,Sajani Vithana,Flavio P. Calmon*

Main category: cs.CR

TL;DR: Large language model (LLM) watermarks are important for authenticating text provenance and curbing misuse of machine-generated text. This paper proposes an optimization framework for watermark design, resulting in two new watermarks: HeavyWater and SimplexWater, which are tunable and work well in low-entropy generation tasks.


<details>
  <summary>Details</summary>
Motivation: To enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems.

Method: Propose an optimization framework for watermark design, resulting in two new watermarks: HeavyWater and SimplexWater.

Result: The new watermarks can achieve high watermark detection accuracy with minimal compromise of text generation quality, especially in low-entropy regimes.

Conclusion: This paper reveals new connections between LLM watermarking and coding theory.

Abstract: Large language model (LLM) watermarks enable authentication of text
provenance, curb misuse of machine-generated text, and promote trust in AI
systems. Current watermarks operate by changing the next-token predictions
output by an LLM. The updated (i.e., watermarked) predictions depend on random
side information produced, for example, by hashing previously generated tokens.
LLM watermarking is particularly challenging in low-entropy generation tasks -
such as coding - where next-token predictions are near-deterministic. In this
paper, we propose an optimization framework for watermark design. Our goal is
to understand how to most effectively use random side information in order to
maximize the likelihood of watermark detection and minimize the distortion of
generated text. Our analysis informs the design of two new watermarks:
HeavyWater and SimplexWater. Both watermarks are tunable, gracefully
trading-off between detection accuracy and text distortion. They can also be
applied to any LLM and are agnostic to side information generation. We examine
the performance of HeavyWater and SimplexWater through several benchmarks,
demonstrating that they can achieve high watermark detection accuracy with
minimal compromise of text generation quality, particularly in the low-entropy
regime. Our theoretical analysis also reveals surprising new connections
between LLM watermarking and coding theory. The code implementation can be
found in https://github.com/DorTsur/HeavyWater_SimplexWater

</details>


### [159] [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)
*Xiaoyuan Zhu,Yaowen Ye,Tianyi Qiu,Hanlin Zhu,Sijun Tan,Ajraf Mannan,Jonathan Michala,Raluca Ada Popa,Willie Neiswanger*

Main category: cs.CR

TL;DR: 提出了一种rank-based uniformity测试方法，用于验证黑盒LLM与本地部署的真实模型的行为平等性，以解决API访问中缺乏透明度的问题。


<details>
  <summary>Details</summary>
Motivation: API访问成为大型语言模型的主要接口，但用户无法获得模型权重和输出logits，这使得检测模型替换变得困难。

Method: 提出的rank-based uniformity测试方法

Result: 该方法能够验证黑盒LLM与本地部署的真实模型的行为平等性，并且准确、查询效率高、避免可检测的查询模式，从而对重新路由或混合响应的对抗性提供商具有鲁棒性。

Conclusion: 提出的方法在受限查询预算下始终优于先前的方法。

Abstract: As API access becomes a primary interface to large language models (LLMs),
users often interact with black-box systems that offer little transparency into
the deployed model. To reduce costs or maliciously alter model behaviors, API
providers may discreetly serve quantized or fine-tuned variants, which can
degrade performance and compromise safety. Detecting such substitutions is
difficult, as users lack access to model weights and, in most cases, even
output logits. To tackle this problem, we propose a rank-based uniformity test
that can verify the behavioral equality of a black-box LLM to a locally
deployed authentic model. Our method is accurate, query-efficient, and avoids
detectable query patterns, making it robust to adversarial providers that
reroute or mix responses upon the detection of testing attempts. We evaluate
the approach across diverse threat scenarios, including quantization, harmful
fine-tuning, jailbreak prompts, and full model substitution, showing that it
consistently achieves superior statistical power over prior methods under
constrained query budgets.

</details>


### [160] [HauntAttack: When Attack Follows Reasoning as a Shadow](https://arxiv.org/abs/2506.07031)
*Jingyuan Ma,Rui Li,Zheng Li,Junfeng Liu,Lei Sha,Zhifang Sui*

Main category: cs.CR

TL;DR: This paper introduces HauntAttack, a novel black-box attack framework that embeds harmful instructions into reasoning questions to expose safety vulnerabilities in large reasoning models.


<details>
  <summary>Details</summary>
Motivation: To investigate the safety-reasoning trade-off in large reasoning models when reasoning is entangled with harmfulness.

Method: Introducing harmful instructions into reasoning questions as carriers through a systematic approach.

Result: LRMs show significant safety vulnerabilities under HauntAttack, and different models and harmful instructions produce varied output patterns.

Conclusion: The study reveals critical safety issues in large reasoning models and provides insights for improving their security.

Abstract: Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and
reasoning tasks, showcasing exceptional capabilities. However, the enhancement
of reasoning abilities and the exposure of their internal reasoning processes
introduce new safety vulnerabilities. One intriguing concern is: when reasoning
is strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs
exhibit? To address this issue, we introduce HauntAttack, a novel and
general-purpose black-box attack framework that systematically embeds harmful
instructions into reasoning questions. Specifically, we treat reasoning
questions as carriers and substitute one of their original conditions with a
harmful instruction. This process creates a reasoning pathway in which the
model is guided step by step toward generating unsafe outputs. Based on
HauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results
reveal that even the most advanced LRMs exhibit significant safety
vulnerabilities. Additionally, we perform a detailed analysis of different
models, various types of harmful instructions, and model output patterns,
providing valuable insights into the security of LRMs.

</details>


### [161] [Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures](https://arxiv.org/abs/2506.07402)
*Yukai Zhou,Sibei Yang,Wenjie Wang*

Main category: cs.CR

TL;DR: 本文重新定义了LLM的风险图景，提出了JailFlipBench基准和攻击方法，揭示了隐含危害带来的实际风险，强调了更全面的安全评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究关注大型语言模型在实际应用中的安全问题，特别是对看似无害的输入错误回答可能带来的隐含危害。

Method: 通过结构化四象限视角重新定义LLM风险图景，并提出JailFlipBench基准来捕捉这种隐含危害，同时开发初步的JailFlip攻击方法。

Result: 研究表明隐含危害在多个开源和黑盒LLM上存在即时和紧迫的实际风险。

Conclusion: 呼吁更广泛的LLM安全性评估和对齐工作，超越传统的越狱范式。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about their security. While jailbreak attacks
highlight failures under overtly harmful queries, they overlook a critical
risk: incorrectly answering harmless-looking inputs can be dangerous and cause
real-world harm (Implicit Harm). We systematically reformulate the LLM risk
landscape through a structured quadrant perspective based on output factuality
and input harmlessness, uncovering an overlooked high-risk region. To
investigate this gap, we propose JailFlipBench, a benchmark aims to capture
implicit harm, spanning single-modal, multimodal, and factual extension
scenarios with diverse evaluation metrics. We further develop initial JailFlip
attack methodologies and conduct comprehensive evaluations across multiple
open-source and black-box LLMs, show that implicit harm present immediate and
urgent real-world risks, calling for broader LLM safety assessments and
alignment beyond conventional jailbreak paradigms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [162] [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
*Yunqing Liu,Wenqi Fan,Xiaoyong Wei,Qing Li*

Main category: cs.LG

TL;DR: A new pre-training framework called GLProtein is proposed which integrates both global structural similarity and local amino acid details to improve protein function prediction.


<details>
  <summary>Details</summary>
Motivation: Integrating protein structural information into protein sequence analysis for further exploration and better understanding of protein functions.

Method: GLProtein framework combining protein-masked modelling, triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding.

Result: GLProtein outperforms previous methods in predicting protein-protein interaction and contact prediction.

Conclusion: The study shows the importance of incorporating both global and local protein structural information for enhancing protein function predictions.

Abstract: Proteins are central to biological systems, participating as building blocks
across all forms of life. Despite advancements in understanding protein
functions through protein sequence analysis, there remains potential for
further exploration in integrating protein structural information. We argue
that the structural information of proteins is not only limited to their 3D
information but also encompasses information from amino acid molecules (local
information) to protein-protein structure similarity (global information). To
address this, we propose \textbf{GLProtein}, the first framework in protein
pre-training that incorporates both global structural similarity and local
amino acid details to enhance prediction accuracy and functional insights.
GLProtein innovatively combines protein-masked modelling with triplet structure
similarity scoring, protein 3D distance encoding and substructure-based amino
acid molecule encoding. Experimental results demonstrate that GLProtein
outperforms previous methods in several bioinformatics tasks, including
predicting protein-protein interaction, contact prediction, and so on.

</details>


### [163] [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
*Zhiyuan Liu,Yicun Yang,Yaojie Zhang,Junjie Chen,Chang Zou,Qingyuan Wei,Shaobo Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的缓存框架dLLM-Cache，可以显著提高扩散型大语言模型的推理效率，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 解决扩散型大语言模型(dLLMs)由于其双向注意力机制而无法使用传统的基于ARM的加速技术（如Key-Value缓存）的问题，从而减少dLLMs的推理延迟。

Method: 提出了一种名为dLLM-Cache的训练自由自适应缓存框架，该框架结合了长间隔提示缓存与部分响应更新，更新由特征相似性指导。

Result: dLLM-Cache在代表性dLLMs上进行了广泛的实验，包括LLaDA 8B和Dream 7B，在不损失输出质量的情况下实现了高达9.1倍的速度提升。此外，该方法使dLLM的推理延迟接近于ARMS在许多设置下的延迟。

Conclusion: dLLM-Cache 提供了一种高效的缓存方法，使得在不牺牲输出质量的情况下，推理速度提高了9.1倍。

Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large
Language Models. Recently, a new paradigm has emerged in the form of
diffusion-based Large Language Models (dLLMs), which generate text by
iteratively denoising masked segments. This approach has shown significant
advantages and potential. However, dLLMs suffer from high inference latency.
Traditional ARM acceleration techniques, such as Key-Value caching, are
incompatible with dLLMs due to their bidirectional attention mechanism. To
address this specific challenge, our work begins with a key observation that
dLLM inference involves a static prompt and a partially dynamic response, where
most tokens remain stable across adjacent denoising steps. Based on this, we
propose dLLM-Cache, a training-free adaptive caching framework that combines
long-interval prompt caching with partial response updates guided by feature
similarity. This design enables efficient reuse of intermediate computations
without compromising model performance. Extensive experiments on representative
dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1
x speedup over standard inference without compromising output quality. Notably,
our method brings dLLM inference latency close to that of ARMs under many
settings. Codes are provided in the supplementary material and will be released
publicly on GitHub.

</details>


### [164] [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
*Kefan Song,Amir Moeini,Peng Wang,Lei Gong,Rohan Chandra,Yanjun Qi,Shangtong Zhang*

Main category: cs.LG

TL;DR: LLMs can perform reinforcement learning during inference.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can exhibit reinforcement learning behavior during inference.

Method: Proposing a multi-round prompting framework called ICRL prompting, which provides numerical scalar feedbacks for responses and uses these feedbacks to improve subsequent responses.

Result: Significant performance improvements were observed in three benchmarks when using ICRL prompting compared to baseline methods. Surprisingly, even self-generated reward signals led to performance enhancements.

Conclusion: This study demonstrates that LLMs can perform reinforcement learning during inference, offering a new paradigm for scaling test-time compute.

Abstract: Reinforcement learning (RL) is a human-designed framework for solving
sequential decision making problems. In this work, we demonstrate that,
surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a
phenomenon known as in-context RL (ICRL). Specifically, we propose a novel
multi-round prompting framework called ICRL prompting. The goal is to prompt
the LLM to complete a task. After the LLM generates a response at the current
round, we give numerical scalar feedbacks for the response, called the rewards.
At the next round, we prompt the LLM again with the same task and a context
consisting of all previous responses and rewards. We observe that the quality
of the LLM's response increases as the context grows. In other words, the LLM
is able to maximize the scalar reward signal in the inference time, just like
an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,
creative writing, and ScienceWorld) and demonstrate significant performance
improvements over baseline methods such as Self-Refine and Reflexion.
Surprisingly, in some experiments the reward signals are generated by the LLM
itself, yet performance improvements are still observed from ICRL prompting,
offering a promising paradigm for scaling test-time compute.

</details>


### [165] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: This survey explores two strategies for efficient LLM inference: routing and cascading/hierarchical inference, providing a comparative analysis across key performance metrics.


<details>
  <summary>Details</summary>
Motivation: The high computational cost and energy consumption of LMs make them hard to deploy in resource-limited environments.

Method: Survey and comparative analysis of routing and cascading/hierarchical inference strategies.

Result: Both strategies aim to reduce computation by using lightweight models for simpler tasks.

Conclusion: Future research should focus on enabling faster response times, adaptive model selection, and scalable deployment across heterogeneous environments.

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [166] [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
*Shubham Parashar,Shurui Gui,Xiner Li,Hongyi Ling,Sushil Vemuri,Blake Olson,Eric Li,Yu Zhang,James Caverlee,Dileep Kalathil,Shuiwang Ji*

Main category: cs.LG

TL;DR: E2H Reasoner uses a curriculum learning approach to enhance reasoning in language models.


<details>
  <summary>Details</summary>
Motivation: To improve the reasoning capabilities of language models via reinforcement learning.

Method: Scheduling tasks from easy to hard (E2H) via reinforcement learning.

Result: E2H Reasoner improves the reasoning ability of small LLMs and prevents overfitting.

Conclusion: E2H Reasoner significantly improves the reasoning ability of small LLMs.

Abstract: We aim to improve the reasoning capabilities of language models via
reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1
have demonstrated reasoning abilities on mathematical and coding tasks.
However, prior studies suggest that using RL alone to improve reasoning on
inherently difficult tasks is less effective. Here, we draw inspiration from
curriculum learning and propose to schedule tasks from easy to hard (E2H),
allowing LLMs to build reasoning skills gradually. Our method is termed E2H
Reasoner. Empirically, we observe that, although easy tasks are important
initially, fading them out through appropriate scheduling is essential in
preventing overfitting. Theoretically, we establish convergence guarantees for
E2H Reasoner within an approximate policy iteration framework. We derive
finite-sample complexity bounds and show that when tasks are appropriately
decomposed and conditioned, learning through curriculum stages requires fewer
total samples than direct learning. Experiments across multiple domains show
that E2H Reasoner significantly improves the reasoning ability of small LLMs
(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,
highlighting the effectiveness of our method.

</details>


### [167] [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
*Rajeev Bhatt Ambati,James Lester,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.LG

TL;DR: 提出了一种名为MarginSel的新方法，可以提高大型语言模型在分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于上下文学习（ICL）的方法对演示示例的选择和排序敏感，因此需要一种新的方法来提高其效果。

Method: 提出了一种名为MarginSel的方法，该方法包含两个步骤，用于为ICL提示选择困难的演示示例，并且该方法会根据每个测试实例进行调整。

Result: 在分类任务上，使用MarginSel方法后，F1分数比随机选择示例提高了2-7%。

Conclusion: 通过引入MarginSel，模型在分类任务中的F1分数提升了2-7%，并且理论与实证研究显示它能诱导大语言模型展现出最大间隔行为。

Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context
learning (ICL). However, the effectiveness of ICL is often sensitive to the
selection and ordering of demonstration examples. To address this, we present
MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that
selects hard demonstration examples for the ICL prompt, adapting to each test
instance. Our approach achieves 2-7% absolute improvement in F1-score across
classification tasks, compared to a random selection of examples. We also
provide theoretical insights and empirical evidence showing that MarginSel
induces max-margin behavior in LLMs by effectively increasing the margin for
hard examples, analogous to support vectors, thereby shifting the decision
boundary in a beneficial direction.

</details>


### [168] [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
*Huanyi Xie,Lijie Hu,Lu Yu,Tianhao Huang,Longfei Li,Meng Li,Jun Zhou,Huan Wang,Di Wang*

Main category: cs.LG

TL;DR: GAGA efficiently learns representations for Text-attributed Graphs by annotating representative nodes and edges, achieving high performance with minimal annotation.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle with complex textual information in TAGs. Existing methods relying on LLMs require extensive annotations which are time-consuming and costly.

Method: Introduces GAGA, a framework that uses representative nodes and edges for annotation and integrates them with the original graph using a two-level alignment module.

Result: GAGA achieves similar or superior classification accuracy compared to leading methods with significantly less annotated data.

Conclusion: GAGA's approach leads to classification accuracies comparable to or better than state-of-the-art methods with only 1% of the data needing annotation.

Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural
networks (GNNs) often fall short due to the complex textual information
associated with each node. Recent methods have improved node representations by
leveraging large language models (LLMs) to enhance node text features, but
these approaches typically require extensive annotations or fine-tuning across
all nodes, which is both time-consuming and costly. To overcome these
challenges, we introduce GAGA, an efficient framework for TAG representation
learning. GAGA reduces annotation time and cost by focusing on annotating only
representative nodes and edges. It constructs an annotation graph that captures
the topological relationships among these annotations. Furthermore, GAGA
employs a two-level alignment module to effectively integrate the annotation
graph with the TAG, aligning their underlying structures. Experiments show that
GAGA achieves classification accuracies on par with or surpassing
state-of-the-art methods while requiring only 1% of the data to be annotated,
demonstrating its high efficiency.

</details>


### [169] [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
*Yuxin Xiao,Sana Tonekaboni,Walter Gerych,Vinith Suriyakumar,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: This study investigates the impact of style patterns on LLM safety and proposes SafeStyle, a defense strategy that improves LLM safety.


<details>
  <summary>Details</summary>
Motivation: To understand whether style patterns compromise LLM safety, how superficial style alignment increases model vulnerability, and how best to mitigate these risks during alignment.

Method: Evaluate 32 LLMs across seven jailbreak benchmarks, investigate superficial style alignment, and propose SafeStyle.

Result: Malicious queries with style patterns inflate the attack success rate (ASR) for nearly all models. Fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of those same styles.

Conclusion: SafeStyle is an effective strategy to maintain LLM safety when fine-tuning with style patterns.

Abstract: Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in jailbreak queries. Although these
style patterns are semantically unrelated to the malicious intents behind
jailbreak queries, their safety impact remains unclear. In this work, we seek
to understand whether style patterns compromise LLM safety, how superficial
style alignment increases model vulnerability, and how best to mitigate these
risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,
and find that malicious queries with style patterns inflate the attack success
rate (ASR) for nearly all models. Notably, ASR inflation correlates with both
the length of style patterns and the relative attention an LLM exhibits on
them. We then investigate superficial style alignment, and find that
fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of
those same styles. Finally, we propose SafeStyle, a defense strategy that
incorporates a small amount of safety training data augmented to match the
distribution of style patterns in the fine-tuning data. Across three LLMs and
five fine-tuning style settings, SafeStyle consistently outperforms baselines
in maintaining LLM safety.

</details>


### [170] [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
*Mickel Liu,Liwei Jiang,Yancheng Liang,Simon Shaolei Du,Yejin Choi,Tim Althoff,Natasha Jaques*

Main category: cs.LG

TL;DR: Proposes Self-RedTeam, a MARL method for LM safety alignment, improving robustness and adaptability through co-evolving attacker and defender agents.


<details>
  <summary>Details</summary>
Motivation: Traditional LM safety alignment has a mismatch issue due to attackers overfitting static models and defenders lagging emerging threats in a sequential approach.

Method: An online self-play reinforcement learning algorithm called Self-RedTeam, casting safety alignment as a two-player zero-sum game with co-evolving attacker and defender agents.

Result: Self-RedTeam outperforms traditional methods in uncovering diverse attacks and achieving robustness on safety benchmarks, while hidden Chain-of-Thought improves adversarial diversity and reduces over-refusals.

Conclusion: Self-RedTeam shifts LM safety training from reactive patching to proactive co-evolution using MARL, ensuring dynamic co-adaptation and reliable safety.

Abstract: Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).

</details>


### [171] [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
*Libo Wang*

Main category: cs.LG

TL;DR: This work introduces GoCE, which improves transformers' ability to capture long-range causal dependencies by mapping token representations into a causal adjacency matrix and applying causal-masked attention and causal-MoE.


<details>
  <summary>Details</summary>
Motivation: Existing models like CoM suffer from losing long-range dependencies due to causal masks blocking global context flow between multi-level subchains.

Method: GoCE maps token representations to a causal adjacency matrix and uses causal-masked attention and causal-MoE to permeate causal constraints. It also includes an intervention consistency loss test and a self-evolution gate.

Result: GoCE outperforms CoM in capturing long-range causal dependencies and self-evolving capability. It was evaluated on datasets like CLUTRR, CLADDER, EX-FEVER, and CausalQA.

Conclusion: GoCE enhances transformers' ability to capture long-range causal dependencies and offers insights for future causal learning research.

Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies
only on the information of the previous subchain and may lose long-range
dependencies due to the causal mask blocking the global context flow between
multi-level subchains, this work proposes a graph of causal evolution (GoCE).
Its core principle is to map the implicit token representation into a
differentiable and sparse causal adjacency matrix, then permeate causal
constraints through each layer of calculation using causal-masked attention and
causal-MoE. By combining intervention consistency loss test and self-evolution
gate, the dynamic balance between causal structure learning and adaptive
updating of transformer architecture is realized. The researcher built
experimental environments in sandboxes built with Claude Sonnet 4,
o4-mini-high, and DeepSeek R1 respectively with the transformer variant
architecture introduced in GoCE. It is evaluated on publicly available datasets
including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the
baseline LLMs. The finding proves that GoCE strengthens the transformer's
ability to capture long-range causal dependencies, while the ability to
self-evolve is improved. It not only surpasses the design of CoM in terms of
design principles, but also provides experience for future research on causal
learning and continuous adaptive improvement.

</details>


### [172] [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
*Mengsong Wu,YaFei Wang,Yidong Ming,Yuqi An,Yuwei Wan,Wenliang Chen,Binbin Lin,Yuqiang Li,Tong Xie,Dongzhan Zhou*

Main category: cs.LG

TL;DR: Large language models (LLMs) have potential in chemistry but face challenges like outdated knowledge. An LLM-based agent integrating 137 external chemical tools and a dataset curation pipeline is proposed. A HE-MCTS framework optimizes tool planning and execution. The approach improves performance in Chemistry QA and discovery tasks.


<details>
  <summary>Details</summary>
Motivation: Address the challenges faced by LLMs in chemistry tasks such as outdated pretraining knowledge and difficulty of incorporating specialized chemical expertise.

Method: Propose an LLM-based agent that synergistically integrates external chemical tools and a dataset curation pipeline. Introduce a HE-MCTS framework for optimizing tool planning and execution. Support step-level fine-tuning of the policy model and training task-adaptive PRM and ORM.

Result: Experimental evaluations show significant improvement in Chemistry QA and discovery tasks.

Conclusion: The approach offers a robust solution to integrate specialized tools with LLMs for advanced chemical applications.

Abstract: Large language models (LLMs) have recently demonstrated promising
capabilities in chemistry tasks while still facing challenges due to outdated
pretraining knowledge and the difficulty of incorporating specialized chemical
expertise. To address these issues, we propose an LLM-based agent that
synergistically integrates 137 external chemical tools created ranging from
basic information retrieval to complex reaction predictions, and a dataset
curation pipeline to generate the dataset ChemToolBench that facilitates both
effective tool selection and precise parameter filling during fine-tuning and
evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search
(HE-MCTS) framework, enabling independent optimization of tool planning and
execution. By leveraging self-generated data, our approach supports step-level
fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM
that surpass GPT-4o. Experimental evaluations demonstrate that our approach
significantly improves performance in Chemistry QA and discovery tasks,
offering a robust solution to integrate specialized tools with LLMs for
advanced chemical applications. All datasets and code are available at
https://github.com/AI4Chem/ChemistryAgent .

</details>


### [173] [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
*Adam Breuer*

Main category: cs.LG

TL;DR: This paper introduces a new combinatorial method for inferring topics in LDA models, which provides better performance and interpretability than existing algorithms.


<details>
  <summary>Details</summary>
Motivation: To solve the primary inference problem for many applications of topic models in social science, data exploration, and causal inference settings.

Method: A novel non-gradient-based, combinatorial approach to estimating topic models.

Result: Algorithms that converge to near-optimal posterior probability in logarithmic parallel computation time, exponentially faster than any known LDA algorithm.

Conclusion: The approach provides interpretability guarantees and maintains the independence assumptions necessary for downstream causal inference.

Abstract: In this paper, we provide the first practical algorithms with provable
guarantees for the problem of inferring the topics assigned to each document in
an LDA topic model. This is the primary inference problem for many applications
of topic models in social science, data exploration, and causal inference
settings. We obtain this result by showing a novel non-gradient-based,
combinatorial approach to estimating topic models. This yields algorithms that
converge to near-optimal posterior probability in logarithmic parallel
computation time (adaptivity) -- exponentially faster than any known LDA
algorithm. We also show that our approach can provide interpretability
guarantees such that each learned topic is formally associated with a known
keyword. Finally, we show that unlike alternatives, our approach can maintain
the independence assumptions necessary to use the learned topic model for
downstream causal inference methods that allow researchers to study topics as
treatments. In terms of practical performance, our approach consistently
returns solutions of higher semantic quality than solutions from
state-of-the-art LDA algorithms, neural topic models, and LLM-based topic
models across a diverse range of text datasets and evaluation parameters.

</details>


### [174] [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
*Michael K. Chen,Xikun Zhang,Jiaxing Huang,Dacheng Tao*

Main category: cs.LG

TL;DR: A new method called Concept-Aware Fine-Tuning (CAFT) allows large language models to learn concepts as whole units instead of fragmented parts, improving performance on various tasks.


<details>
  <summary>Details</summary>
Motivation: Existing next-token prediction methods limit the ability of large language models to form coherent, high-level concepts, hindering human-like understanding and reasoning.

Method: CAFT enables multi-token learning during fine-tuning, allowing models to grasp phrases as unified semantic entities.

Result: Experiments show significant improvements over conventional methods in tasks such as text summarization and protein design.

Conclusion: CAFT democratizes the benefits of multi-token learning by bringing it to the post-training phase, with potential wide-ranging implications for the machine learning community.

Abstract: Large language models (LLMs) have become the cornerstone of modern AI.
However, the existing paradigm of next-token prediction fundamentally limits
their ability to form coherent, high-level concepts, making it a critical
barrier to human-like understanding and reasoning. Take the phrase "ribonucleic
acid" as an example: an LLM will first decompose it into tokens, i.e.,
artificial text fragments ("rib", "on", ...), then learn each token
sequentially, rather than grasping the phrase as a unified, coherent semantic
entity. This fragmented representation hinders deeper conceptual understanding
and, ultimately, the development of truly intelligent systems. In response, we
introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method
that redefines how LLMs are fine-tuned. By enabling the learning of sequences
that span multiple tokens, this method fosters stronger concept-aware learning.
Our experiments demonstrate significant improvements compared to conventional
next-token finetuning methods across diverse tasks, including traditional
applications like text summarization and domain-specific ones like de novo
protein design. Multi-token prediction was previously only possible in the
prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first
to bring the multi-token setting to the post-training phase, thus effectively
democratizing its benefits for the broader community of practitioners and
researchers. Finally, the unexpected effectiveness of our proposed method
suggests wider implications for the machine learning research community. All
code and data are available at https://github.com/michaelchen-lab/caft-llm

</details>


### [175] [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
*Manuel Brenner,Georgia Koppe*

Main category: cs.LG

TL;DR: 研究发现循环神经网络中适度的非线性足以实现长程记忆和时间处理，且比全非线性或线性模型更优越。


<details>
  <summary>Details</summary>
Motivation: 探索非线性在循环网络中的功能作用，确定其计算上必要性和启用机制。

Method: 使用Almost Linear Recurrent Neural Networks (AL-RNNs)，对非线性进行精细控制，作为建模工具和探针来研究记忆的内部机制。

Result: 发现最小的非线性不仅足够而且通常是最佳选择，生成的模型比完全非线性和线性对应物更简单、更健壮、更可解释。

Conclusion: 提供了一个原则框架，用于选择性地引入非线性，将动力系统理论与循环神经网络中长程记忆和结构化计算的功能需求联系起来，对人工和生物神经系统都有影响。

Abstract: Memory and long-range temporal processing are core requirements for sequence
modeling tasks across natural language processing, time-series forecasting,
speech recognition, and control. While nonlinear recurrence has long been
viewed as essential for enabling such mechanisms, recent work suggests that
linear dynamics may often suffice. In this study, we go beyond performance
comparisons to systematically dissect the functional role of nonlinearity in
recurrent networks--identifying both when it is computationally necessary, and
what mechanisms it enables. We use Almost Linear Recurrent Neural Networks
(AL-RNNs), which allow fine-grained control over nonlinearity, as both a
flexible modeling tool and a probe into the internal mechanisms of memory.
Across a range of classic sequence modeling tasks and a real-world stimulus
selection task, we find that minimal nonlinearity is not only sufficient but
often optimal, yielding models that are simpler, more robust, and more
interpretable than their fully nonlinear or linear counterparts. Our results
provide a principled framework for selectively introducing nonlinearity,
bridging dynamical systems theory with the functional demands of long-range
memory and structured computation in recurrent neural networks, with
implications for both artificial and biological neural systems.

</details>


### [176] [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
*Hongzheng Chen,Yingheng Wang,Yaohui Cai,Hins Hu,Jiajie Li,Shirley Huang,Chenhui Deng,Rongjian Liang,Shufeng Kong,Haoxing Ren,Samitha Samaranayake,Carla P. Gomes,Zhiru Zhang*

Main category: cs.LG

TL;DR: Evaluate heuristic algorithms generated by LLMs for combinatorial optimization using HeuriGym.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for LLMs are inadequate.

Method: Introduce HeuriGym framework for iterative refinement of LLM-generated heuristics.

Result: Top models achieve low QYI scores, revealing limitations in tool use, planning, and reasoning.

Conclusion: HeuriGym provides a rigorous evaluation method for LLMs in scientific and engineering domains.

Abstract: While Large Language Models (LLMs) have demonstrated significant advancements
in reasoning and agent-based problem-solving, current evaluation methodologies
fail to adequately assess their capabilities: existing benchmarks either rely
on closed-ended questions prone to saturation and memorization, or subjective
comparisons that lack consistency and rigor. In this work, we introduce
HeuriGym, an agentic framework designed for evaluating heuristic algorithms
generated by LLMs for combinatorial optimization problems, characterized by
clearly defined objectives and expansive solution spaces. HeuriGym empowers
LLMs to propose heuristics, receive evaluative feedback via code execution, and
iteratively refine their solutions. We evaluate nine state-of-the-art models on
nine problems across domains such as computer systems, logistics, and biology,
exposing persistent limitations in tool use, planning, and adaptive reasoning.
To quantify performance, we propose the Quality-Yield Index (QYI), a metric
that captures both solution pass rate and quality. Even top models like
GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below
the expert baseline of 1. Our open-source benchmark aims to guide the
development of LLMs toward more effective and realistic problem-solving in
scientific and engineering domains.

</details>


### [177] [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
*Zeju Qiu,Simon Buchholz,Tim Z. Xiao,Maximilian Dax,Bernhard Schölkopf,Weiyang Liu*

Main category: cs.LG

TL;DR: POET is a new training algorithm using Orthogonal Equivalence Transformation for optimizing neurons in large language models.


<details>
  <summary>Details</summary>
Motivation: Effectively and reliably training large language models remains a challenge in the field of artificial intelligence.

Method: POET is a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. It reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix.

Result: POET can stably optimize the objective function with improved generalization. Efficient approximations make POET flexible and scalable for training large-scale neural networks.

Conclusion: The effectiveness and scalability of POET in training LLMs have been validated by extensive experiments.

Abstract: While large language models (LLMs) are driving the rapid advancement of
artificial intelligence, effectively and reliably training these large models
remains one of the field's most significant challenges. To address this
challenge, we propose POET, a novel reParameterized training algorithm that
uses Orthogonal Equivalence Transformation to optimize neurons. Specifically,
POET reparameterizes each neuron with two learnable orthogonal matrices and a
fixed random weight matrix. Because of its provable preservation of spectral
properties of weight matrices, POET can stably optimize the objective function
with improved generalization. We further develop efficient approximations that
make POET flexible and scalable for training large-scale neural networks.
Extensive experiments validate the effectiveness and scalability of POET in
training LLMs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [178] [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
*Arnav Sheth,Ivaxi Sheth,Mario Fritz*

Main category: cs.AR

TL;DR: This paper examines how advanced LLMs perform in creating SystemVerilog code for common communication protocols.


<details>
  <summary>Details</summary>
Motivation: There is a lack of research into using LLMs for hardware description languages like SystemVerilog, which has strict requirements for timing, concurrency, and synthesizability.

Method: The paper introduces a benchmark suite for four protocols (SPI, I2C, UART, AXI) and defines code generation tasks with different levels of abstraction and specificity.

Result: Generated designs are evaluated for syntactic correctness, synthesizability, and functionality through simulation and test benches.

Conclusion: The study provides insights into the capabilities of LLMs in handling complex hardware design tasks.

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
capabilities in generating code for general-purpose programming languages. In
contrast, their applicability for hardware description languages, particularly
for generating synthesizable and functionally correct designs, remains
significantly underexplored. HDLs such as SystemVerilog are logic-oriented and
demand strict adherence to timing semantics, concurrency, and synthesizability
constraints. Moreover, HDL-based design flows encompass a broad set of tasks
beyond structural code generation, including testbench development,
assertion-based verification, timing closure, and protocol-level integration
for on-chip communication. The objective of our paper is to analyze the
capabilities of state-of-the-art LLMs in generating SystemVerilog
implementations of standard communication protocols, a core component of
embedded and System-on-Chip (SoC) architectures. This paper introduces the
first benchmark suite targeting four widely used protocols: SPI, I2C, UART, and
AXI. We define code generation tasks that capture varying levels of design
abstraction and prompt specificity. The generated designs are assessed for
syntactic correctness, synthesizability, and functional fidelity via waveform
simulation and test benches.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [179] [DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval](https://arxiv.org/abs/2506.06313)
*Huiyao Chen,Yi Yang,Yinghui Li,Meishan Zhang,Min Zhang*

Main category: cs.IR

TL;DR: A novel hierarchical retrieval framework named DISRetrieval is proposed to improve long document understanding by leveraging linguistic discourse structure.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture the inherent discourse structure that guides human comprehension, thus limiting their performance in long document understanding.

Method: DISRetrieval introduces three key innovations: discourse-aware document organization framework, LLM-enhanced node representation technique, and hierarchical evidence retrieval mechanism.

Result: DISRetrieval outperforms existing methods in both token-level retrieval metrics and downstream question answering tasks.

Conclusion: Incorporating discourse structure significantly enhances retrieval effectiveness across different document lengths and query types.

Abstract: Long document understanding has become increasingly crucial in natural
language processing, with retrieval-based methods emerging as a promising
solution to address the context length limitations of large language models
(LLMs). However, existing approaches either treat documents as flat sequences
or employ arbitrary chunking strategies, failing to capture the inherent
discourse structure that guides human comprehension. We present DISRetrieval, a
novel hierarchical retrieval framework that leverages linguistic discourse
structure to enhance long document understanding. Our approach introduces three
key innovations: (1) a discourse-aware document organization framework that
utilizes rhetorical structure theory (RST) to create sentence-level
hierarchical representations, preserving both semantic relationships and
natural document flow; (2) an LLM-enhanced node representation technique that
combines discourse structure with adaptive summarization to enrich tree nodes
with contextual information; and (3) a hierarchical evidence retrieval
mechanism that effectively selects relevant content while maintaining discourse
coherence. Through comprehensive experiments on QASPER and QuALITY datasets,
DISRetrieval demonstrates substantial improvements over existing methods in
both token-level retrieval metrics and downstream question answering tasks. Our
ablation studies confirm that incorporating discourse structure significantly
enhances retrieval effectiveness across different document lengths and query
types, validating the importance of linguistically-informed document
representation in long-text understanding. Our code and datasets are publicly
available at github/DreamH1gh/DISRetrieval to facilitate future research.

</details>


### [180] [Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?](https://arxiv.org/abs/2506.06328)
*Aziida Nanyonga,Joiner Keith,Turhan Ugur,Wild Graham*

Main category: cs.IR

TL;DR: 本研究比较了BERTopic和PLSA在从航空安全报告中提取有意义主题方面的有效性，结果显示BERTopic在主题连贯性和可解释性方面均优于PLSA。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是增强对航空事件数据模式的理解，这可以通过从大量航空安全报告中提取有意义的主题来实现。

Method: 本研究比较了BERTopic和PLSA在从航空安全报告中提取有意义主题方面的有效性。BERTopic使用基于变压器的嵌入和分层聚类，而PLSA则通过期望最大化（EM）算法利用概率建模。

Result: 结果表明，BERTopic在主题连贯性方面优于PLSA，达到了0.41的Cv分数，而PLSA为0.37。此外，BERTopic还展示了更好的可解释性，得到了航空安全专家的认可。

Conclusion: BERTopic在航空安全报告中提取有意义的主题方面优于PLSA，展示了更高的主题连贯性和更好的可解释性。这些发现强调了现代基于变压器的方法在分析复杂的航空数据集中的优势，为航空安全领域的深入洞察和知情决策铺平了道路。

Abstract: This study compares the effectiveness of BERTopic and Probabilistic Latent
Semantic Analysis (PLSA) in extracting meaningful topics from aviation safety
reports aiming to enhance the understanding of patterns in aviation incident
data. Using a dataset of over 36,000 National Transportation Safety Board
(NTSB) reports from 2000 to 2020, BERTopic employed transformer based
embeddings and hierarchical clustering, while PLSA utilized probabilistic
modelling through the Expectation-Maximization (EM) algorithm. Results showed
that BERTopic outperformed PLSA in topic coherence, achieving a Cv score of
0.41 compared to PLSA 0.37, while also demonstrating superior interpretability
as validated by aviation safety experts. These findings underscore the
advantages of modern transformer based approaches in analyzing complex aviation
datasets, paving the way for enhanced insights and informed decision-making in
aviation safety. Future work will explore hybrid models, multilingual datasets,
and advanced clustering techniques to further improve topic modelling in this
domain.

</details>


### [181] [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)
*Xuan Xu,Fufang Wen,Beilin Chu,Zhibing Fu,Qinhong Lin,Jiaqi Liu,Binjie Fei,Zhongliang Yang,Linna Zhou,Yu Li*

Main category: cs.IR

TL;DR: FinBERT2，一个基于金融领域的双向编码器预训练模型，通过在大规模金融语料库上的训练，在金融分类、金融检索和主题建模等任务上表现优异，弥补了大型语言模型在金融领域应用中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理领域从编码器为主的模型转向解码器为主的大型语言模型，但在金融领域，大型语言模型存在计算资源消耗高但性能不如微调后的BERT模型、生成任务依赖于检索增强生成方法且效果不佳、其他特征场景下也有不足等问题。

Method: 开发了一个名为FinBERT2的专用双向编码器模型，该模型在高质量的金融特定语料库（32b token）上进行预训练，这是此类参数规模模型已知的最大中文金融预训练语料库。

Result: FinBERT2在五个金融分类任务中的微调判别模型（Fin-Labelers）比其他（Fin）BERT变体高出0.4%-3.3%，比领先的大语言模型高出9.7%-12.3%；对比学习微调模型（Fin-Retrievers）在五个金融检索任务中优于开源嵌入器和专有嵌入器；基于FinBERT2变体构建的金融主题模型（Fin-TopicModel）在金融标题的聚类和主题表示方面表现出色。

Conclusion: 本研究通过与当代大型语言模型的比较分析，重新审视了金融BERT模型，并为在大语言模型时代有效利用FinBERT提供了实用见解。

Abstract: In natural language processing (NLP), the focus has shifted from encoder-only
tiny language models like BERT to decoder-only large language models(LLMs) such
as GPT-3. However, LLMs' practical application in the financial sector has
revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT
on discriminative tasks despite costing much higher computational resources,
such as market sentiment analysis in financial reports; (2) Application on
generative tasks heavily relies on retrieval augmented generation (RAG) methods
to provide current and specialized information, with general retrievers showing
suboptimal performance on domain-specific retrieval tasks; (3) There are
additional inadequacies in other feature-based scenarios, such as topic
modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained
on a high-quality, financial-specific corpus of 32b tokens. This represents the
largest known Chinese financial pretraining corpus for models of this parameter
size. As a better backbone, FinBERT2 can bridge the gap in the
financial-specific deployment of LLMs through the following achievements: (1)
Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT
variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five
financial classification tasks. (2) Contrastive fine-tuned models
(Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over
BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's
text-embedding-3-large) embedders across five financial retrieval tasks; (3)
Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables
superior clustering and topic representation for financial titles. Our work
revisits financial BERT models through comparative analysis with contemporary
LLMs and offers practical insights for effectively utilizing FinBERT in the
LLMs era.

</details>


### [182] [Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components](https://arxiv.org/abs/2506.06339)
*Jumana Alsubhi,Mohammad D. Alahmadi,Ahmed Alhusayni,Ibrahim Aldailami,Israa Hamdine,Ahmad Shabana,Yazeed Iskandar,Suhayb Khayyat*

Main category: cs.IR

TL;DR: This study evaluates RAG components for Arabic, finding sentence-aware chunking, BGE-M3, Multilingual-E5-large, bge-reranker-v2-m3, and Aya-8B to be optimal.


<details>
  <summary>Details</summary>
Motivation: Optimizing RAG components for Arabic is underexplored despite the success of RAG in high-resource languages.

Method: Empirical evaluation of RAG components including chunking strategies, embedding models, rerankers, and language models using the RAGAS framework across Arabic datasets.

Result: Sentence-aware chunking outperforms other segmentation methods; BGE-M3 and Multilingual-E5-large are the best embedding models; adding a reranker improves faithfulness; Aya-8B generates better answers than StableLM.

Conclusion: The study provides insights into building high-quality Arabic RAG pipelines and offers practical guidelines for component selection.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture
for combining the precision of retrieval systems with the fluency of large
language models. While several studies have investigated RAG pipelines for
high-resource languages, the optimization of RAG components for Arabic remains
underexplored. This study presents a comprehensive empirical evaluation of
state-of-the-art RAG components-including chunking strategies, embedding
models, rerankers, and language models-across a diverse set of Arabic datasets.
Using the RAGAS framework, we systematically compare performance across four
core metrics: context precision, context recall, answer faithfulness, and
answer relevancy. Our experiments demonstrate that sentence-aware chunking
outperforms all other segmentation methods, while BGE-M3 and
Multilingual-E5-large emerge as the most effective embedding models. The
inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness
in complex datasets, and Aya-8B surpasses StableLM in generation quality. These
findings provide critical insights for building high-quality Arabic RAG
pipelines and offer practical guidelines for selecting optimal components
across different document types.

</details>


### [183] [LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking](https://arxiv.org/abs/2506.07449)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.IR

TL;DR: A new method called LlamaRec-LKG-RAG combines personalized knowledge graph context with large language models for better recommendations.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval-augmented generation approaches fail to use the relational structure in user-item interactions.

Method: Integrates personalized knowledge graph context into LLM-based recommendation ranking using a lightweight user preference module.

Result: Significant improvement over LlamaRec in key ranking metrics on two datasets.

Conclusion: Structured reasoning is crucial for LLM-based recommendations and sets the stage for scalable, knowledge-aware personalization in future recommender systems.

Abstract: Recent advances in Large Language Models (LLMs) have driven their adoption in
recommender systems through Retrieval-Augmented Generation (RAG) frameworks.
However, existing RAG approaches predominantly rely on flat, similarity-based
retrieval that fails to leverage the rich relational structure inherent in
user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,
end-to-end trainable framework that integrates personalized knowledge graph
context into LLM-based recommendation ranking. Our approach extends the
LlamaRec architecture by incorporating a lightweight user preference module
that dynamically identifies salient relation paths within a heterogeneous
knowledge graph constructed from user behavior and item metadata. These
personalized subgraphs are seamlessly integrated into prompts for a fine-tuned
Llama-2 model, enabling efficient and interpretable recommendations through a
unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty
datasets demonstrate consistent and significant improvements over LlamaRec
across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates
the critical value of structured reasoning in LLM-based recommendations and
establishes a foundation for scalable, knowledge-aware personalization in
next-generation recommender systems. Code is available
at~\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [184] [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
*Michał P. Karpowicz*

Main category: stat.ML

TL;DR: This paper presents an impossibility theorem showing that no inference method can simultaneously ensure truthful generation, semantic information conservation, relevant knowledge revelation, and knowledge-constrained optimality in large language models.


<details>
  <summary>Details</summary>
Motivation: To understand why large language models cannot avoid hallucination and what trade-offs are necessary.

Method: Modeling LLM inference as an auction of ideas where neural components compete to contribute to responses and applying the Green-Laffont theorem.

Result: Proved an impossibility theorem regarding the properties of inference mechanisms in large language models.

Conclusion: The findings provide a theoretical basis for improving model architecture, training objectives, and evaluation methods.

Abstract: This paper explains \textbf{why it is impossible to create large language
models that do not hallucinate and what are the trade-offs we should be looking
for}. It presents a formal \textbf{impossibility theorem} demonstrating that no
inference mechanism can simultaneously satisfy four fundamental properties:
\textbf{truthful (non-hallucinatory) generation, semantic information
conservation, relevant knowledge revelation, and knowledge-constrained
optimality}. By modeling LLM inference as an \textbf{auction of ideas} where
neural components compete to contribute to responses, we prove the
impossibility using the Green-Laffont theorem. That mathematical framework
provides a rigorous foundation for understanding the nature of inference
process, with implications for model architecture, training objectives, and
evaluation methods.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [185] [The Hype Index: an NLP-driven Measure of Market News Attention](https://arxiv.org/abs/2506.06329)
*Zheng Cao,Wanchaloem Wunkaew,Helyette Geman*

Main category: q-fin.ST

TL;DR: This paper introduces the Hype Index to measure media attention on large-cap equities using NLP techniques. It evaluates its performance through several lenses.


<details>
  <summary>Details</summary>
Motivation: To provide a new metric for analyzing media attention on large-cap equities.

Method: Constructing News Count-Based Hype Index and Capitalization Adjusted Hype Index, evaluating through classification, associations with returns, signaling power, and empirical properties.

Result: The Hype Index family offers valuable tools for stock volatility analysis, market signaling, and NLP extensions in Finance.

Conclusion: The Hype Index is a novel metric that can be used to extract predictive signals from financial news.

Abstract: This paper introduces the Hype Index as a novel metric to quantify media
attention toward large-cap equities, leveraging advances in Natural Language
Processing (NLP) for extracting predictive signals from financial news. Using
the S&P 100 as the focus universe, we first construct a News Count-Based Hype
Index, which measures relative media exposure by computing the share of news
articles referencing each stock or sector. We then extend it to the
Capitalization Adjusted Hype Index, adjusts for economic size by taking the
ratio of a stock's or sector's media weight to its market capitalization weight
within its industry or sector. We compute both versions of the Hype Index at
the stock and sector levels, and evaluate them through multiple lenses: (1)
their classification into different hype groups, (2) their associations with
returns, volatility, and VIX index at various lags, (3) their signaling power
for short-term market movements, and (4) their empirical properties including
correlations, samplings, and trends. Our findings suggest that the Hype Index
family provides a valuable set of tools for stock volatility analysis, market
signaling, and NLP extensions in Finance.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [186] [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
*Daniel Thilo Schroeder,Meeyoung Cha,Andrea Baronchelli,Nick Bostrom,Nicholas A. Christakis,David Garcia,Amit Goldenberg,Yara Kyrychenko,Kevin Leyton-Brown,Nina Lutz,Gary Marcus,Filippo Menczer,Gordon Pennycook,David G. Rand,Frank Schweitzer,Christopher Summerfield,Audrey Tang,Jay Van Bavel,Sander van der Linden,Dawn Song,Jonas R. Kunst*

Main category: cs.CY

TL;DR: This paper discusses the potential threat of malicious AI swarms on disinformation operations and suggests a three-pronged response involving platform-side defenses, model-side safeguards, and system-level oversight.


<details>
  <summary>Details</summary>
Motivation: To address the emerging threat of malicious AI swarms that can create fabricated consensus, fragmented reality, harassment, suppression or mobilization, and erosion of institutional trust.

Method: Proposing a three-pronged response including platform-side defenses, model-side safeguards, and system-level oversight.

Result: The proposed solutions aim to counteract the negative impacts of malicious AI swarms on disinformation operations and protect democratic processes.

Conclusion: Urgent action is needed to defend against the threat of malicious AI swarms, and the suggested measures could be effective in mitigating these risks.

Abstract: Advances in AI portend a new era of sophisticated disinformation operations.
While individual AI systems already create convincing -- and at times
misleading -- information, an imminent development is the emergence of
malicious AI swarms. These systems can coordinate covertly, infiltrate
communities, evade traditional detectors, and run continuous A/B tests, with
round-the-clock persistence. The result can include fabricated grassroots
consensus, fragmented shared reality, mass harassment, voter micro-suppression
or mobilization, contamination of AI training data, and erosion of
institutional trust. With democratic processes worldwide increasingly
vulnerable, we urge a three-pronged response: (1) platform-side defenses --
always-on swarm-detection dashboards, pre-election high-fidelity
swarm-simulation stress-tests, transparency audits, and optional client-side
"AI shields" for users; (2) model-side safeguards -- standardized
persuasion-risk tests, provenance-authenticating passkeys, and watermarking;
and (3) system-level oversight -- a UN-backed AI Influence Observatory.

</details>


### [187] [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)
*Lingyao Li,Dawei Li,Zhenhui Ou,Xiaoran Xu,Jingxiao Liu,Zihui Ma,Runlong Yu,Min Deng*

Main category: cs.CY

TL;DR: Efficient simulation is essential for enhancing proactive preparedness for sudden-onset disasters such as earthquakes.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in large language models (LLMs) as world models show promise in simulating complex scenarios.

Method: Leveraging multimodal datasets including geospatial, socioeconomic, building, and street-level imagery data, our framework generates Modified Mercalli Intensity (MMI) predictions at zip code and county scales.

Result: Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS 'Did You Feel It?' (DYFI) reports demonstrate significant alignment, as evidenced by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real reports at the zip code level.

Conclusion: These findings show the promise of LLMs in simulating disaster impacts that can help strengthen pre-event planning.

Abstract: Efficient simulation is essential for enhancing proactive preparedness for
sudden-onset disasters such as earthquakes. Recent advancements in large
language models (LLMs) as world models show promise in simulating complex
scenarios. This study examines multiple LLMs to proactively estimate perceived
earthquake impacts. Leveraging multimodal datasets including geospatial,
socioeconomic, building, and street-level imagery data, our framework generates
Modified Mercalli Intensity (MMI) predictions at zip code and county scales.
Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did
You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced
by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real
reports at the zip code level. Techniques such as RAG and ICL can improve
simulation performance, while visual inputs notably enhance accuracy compared
to structured numerical data alone. These findings show the promise of LLMs in
simulating disaster impacts that can help strengthen pre-event planning.

</details>


### [188] [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391)
*John Mavi,Diana Teodora Găitan,Sergio Coronado*

Main category: cs.CY

TL;DR: This study examines eight leading Large Language Models (LLMs) to assess their compliance with International Humanitarian Law (IHL). It focuses on their ability to refuse unlawful prompts and communicate refusals clearly and constructively.


<details>
  <summary>Details</summary>
Motivation: To understand the alignment of LLMs with IHL and improve the clarity and consistency of their responses.

Method: Evaluating eight leading LLMs' ability to refuse unlawful prompts and communicate refusals clearly and constructively.

Result: Most models rejected unlawful requests, but the clarity and consistency of their responses varied. Explanatory refusals improved by using a standardised system-level safety prompt, though complex prompts still revealed vulnerabilities.

Conclusion: This research contributes to the development of safer, more transparent AI systems and proposes a benchmark for evaluating LLM compliance with IHL.

Abstract: Large Language Models (LLMs) are widely used across sectors, yet their
alignment with International Humanitarian Law (IHL) is not well understood.
This study evaluates eight leading LLMs on their ability to refuse prompts that
explicitly violate these legal frameworks, focusing also on helpfulness - how
clearly and constructively refusals are communicated. While most models
rejected unlawful requests, the clarity and consistency of their responses
varied. By revealing the model's rationale and referencing relevant legal or
safety principles, explanatory refusals clarify the system's boundaries, reduce
ambiguity, and help prevent misuse. A standardised system-level safety prompt
significantly improved the quality of the explanations expressed within
refusals in most models, highlighting the effectiveness of lightweight
interventions. However, more complex prompts involving technical language or
requests for code revealed ongoing vulnerabilities. These findings contribute
to the development of safer, more transparent AI systems and propose a
benchmark to evaluate the compliance of LLM with IHL.

</details>


### [189] [Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches](https://arxiv.org/abs/2506.06540)
*Patrick Y. Wu*

Main category: cs.CY

TL;DR: This paper suggests that large language models can replace expert political surveys after disruptive events like the 2025 Department of Government Efficiency federal layoffs.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty in reconstructing pre-event perceptions needed to study the factors associated with the event after a disruptive event.

Method: Using pairwise comparison prompts with large language models to derive ideology scores for federal executive agencies.

Result: The derived ideology scores replicate pre-layoff expert measures and predict which agencies were targeted by DOGE. The perceptions of certain federal agencies as knowledge institutions also predict which agencies were targeted by DOGE.

Conclusion: LLMs allow rapid and easy testing of hypothesized factors behind shocks, offering insights into correlational factors when traditional measurement techniques fail. A two-part criterion is proposed for when researchers can use LLMs as a substitute for expert political surveys.

Abstract: After a disruptive event or shock, such as the Department of Government
Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by
knowledge of the outcome. This can make it difficult or impossible to
reconstruct the pre-event perceptions needed to study the factors associated
with the event. This position paper argues that large language models (LLMs),
trained on vast amounts of digital media data, can be a viable substitute for
expert political surveys when a shock disrupts traditional measurement. We
analyze the DOGE layoffs as a specific case study for this position. We use
pairwise comparison prompts with LLMs and derive ideology scores for federal
executive agencies. These scores replicate pre-layoff expert measures and
predict which agencies were targeted by DOGE. We also use this same approach
and find that the perceptions of certain federal agencies as knowledge
institutions predict which agencies were targeted by DOGE, even when
controlling for ideology. This case study demonstrates that using LLMs allows
us to rapidly and easily test the associated factors hypothesized behind the
shock. More broadly, our case study of this recent event exemplifies how LLMs
offer insights into the correlational factors of the shock when traditional
measurement techniques fail. We conclude by proposing a two-part criterion for
when researchers can turn to LLMs as a substitute for expert political surveys.

</details>


### [190] [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
*Yijia Shao,Humishka Zope,Yucheng Jiang,Jiaxin Pei,David Nguyen,Erik Brynjolfsson,Diyi Yang*

Main category: cs.CY

TL;DR: This paper introduces a new auditing framework to evaluate worker preferences for AI automation across various occupational tasks and compares them with current AI capabilities. It identifies four zones based on these evaluations and emphasizes the importance of aligning AI development with human desires.


<details>
  <summary>Details</summary>
Motivation: Concerns about job displacement, diminished human agency, and overreliance on automation due to the rise of compound AI systems necessitate a better understanding of the evolving labor market landscape.

Method: Developed an audio-enhanced mini-interview framework to capture nuanced worker desires and introduced the Human Agency Scale (HAS) to quantify human involvement preferences. Constructed the WORKBank database by combining worker preferences and AI expert assessments.

Result: Identified four zones for tasks based on worker desires and AI capabilities: 'Green Light', 'Red Light', 'R&D Opportunity', and 'Low Priority'. Revealed diverse HAS profiles across occupations and highlighted shifts in required human competencies due to AI integration.

Conclusion: The study underscores the necessity of aligning AI agent development with human desires and preparing workers for changes in workplace dynamics.

Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the
labor market, raising concerns about job displacement, diminished human agency,
and overreliance on automation. Yet, we lack a systematic understanding of the
evolving landscape. In this paper, we address this gap by introducing a novel
auditing framework to assess which occupational tasks workers want AI agents to
automate or augment, and how those desires align with the current technological
capabilities. Our framework features an audio-enhanced mini-interview to
capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a
shared language to quantify the preferred level of human involvement. Using
this framework, we construct the WORKBank database, building on the U.S.
Department of Labor's O*NET database, to capture preferences from 1,500 domain
workers and capability assessments from AI experts across over 844 tasks
spanning 104 occupations. Jointly considering the desire and technological
capability divides tasks in WORKBank into four zones: Automation "Green Light"
Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone.
This highlights critical mismatches and opportunities for AI agent development.
Moving beyond a simple automate-or-not dichotomy, our results reveal diverse
HAS profiles across occupations, reflecting heterogeneous expectations for
human involvement. Moreover, our study offers early signals of how AI agent
integration may reshape the core human competencies, shifting from
information-focused skills to interpersonal ones. These findings underscore the
importance of aligning AI agent development with human desires and preparing
workers for evolving workplace dynamics.

</details>
